\documentclass[times, review, 10pt]{elsarticle}
% \documentclass[preprint,11pt]{elsarticle}
% \documentclass[preprint,12pt]{elsarticle}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{a4wide}
% \usepackage[a4paper, total={7in,7.5in}]{geometry}
\usepackage{mathrsfs}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{color}
\usepackage{lineno}
\usepackage{makecell} 
\usepackage{pdflscape}
\usepackage{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{setspace}
\doublespacing
\usepackage{blindtext}
\usepackage{multirow}
\usepackage{notoccite} %citation number ordering
\usepackage{lscape} %landscape table
\usepackage{caption} %add a newline in the table caption
\usepackage{mwe}%%%%%%%%%%%%%%%
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{mathrsfs}
\usetikzlibrary{shapes,arrows}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\RNum}[1]{\lowercase\expandafter{\romannumeral #1\relax}}
\newcommand{\RNumU}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
% \usepackage[english]{babel}
% \newtheorem{theorem}{Theorem}
\usepackage{natbib}
\journal{Elsevier}

\begin{document}
\begin{frontmatter}
\title{TRKM: Twin Restricted Kernel Machines for Classification and Regression}
\author[inst1]{A. Quadir}
\ead{mscphd2207141002@iiti.ac.in}
\author[inst1]{M. Tanveer\corref{Correspondingauthor}}
\ead{mtanveer@iiti.ac.in}

% \author[]{for the Alzheimer’s Disease Neuroimaging
% Initiative\corref{ADNI citation}}
\affiliation[inst1]{organization={Department of Mathematics, Indian Institute of Technology Indore},%Department and Organization
            addressline={Simrol}, 
            city={Indore},
            postcode={453552}, 
            state={Madhya Pradesh},
            country={India}}
            \cortext[Correspondingauthor]{Corresponding author}
%             \cortext[ADNI citation]{This study used data from the Alzheimer’s Disease Neuroimaging Initiative
% (ADNI) (adni.loni.usc.edu). The ADNI investigators were responsible for the design and implementation of the study, but they did not take part in the analysis or the writing of this publication.}
\begin{abstract}
Restricted kernel machines (RKMs) have considerably improved generalization in machine learning. Recent advancements explored various techniques within the RKM framework, integrating kernel functions with least squares support vector machines (LSSVM) to mirror the energy function of restricted Boltzmann machines (RBM), leading to enhanced performance. However, RKMs may face challenges in generalization when dealing with unevenly distributed or complexly clustered data. Additionally, as the dataset size increases, the computational burden of managing high-dimensional feature spaces can become substantial, potentially hindering performance in large-scale datasets. To address these challenges, we propose twin restricted kernel machine (TRKM). TRKM combines the benefits of twin models with the robustness of the RKM framework to enhance classification and regression tasks. By leveraging the Fenchel-Young inequality, we introduce a novel conjugate feature duality, allowing the formulation of classification and regression problems in terms of dual variables. This duality provides an upper bound to the objective function of the TRKM problem, resulting in a new methodology under the RKM framework. The model uses an energy function similar to that of RBM, incorporating both visible and hidden variables corresponding to both classes. Additionally, the kernel trick is employed to map data into a high-dimensional feature space, where the model identifies an optimal separating hyperplane using a regularized least squares approach. Extensive experiments conducted on a diverse range of datasets from UCI, and KEEL confirm the superiority of TRKM over existing baseline models, demonstrating its effectiveness in handling complex data structures with improved robustness and efficiency. Furthermore, we implemented the proposed TRKM model to the brain age estimation dataset. Experimental results highlight the model's effectiveness in predicting brain age prediction.
\end{abstract}

\begin{keyword}
Restricted Boltzmann machines, Kernel methods, Restricted kernel machines, Twin support vector machine, Brain age estimation.
\end{keyword}
\end{frontmatter}


\section{Introduction}
Support vector machines (SVMs) \cite{cortes1995support} have emerged as a powerful tool for solving classification problems. SVMs are based on statistical learning theory and the principle of maximizing the margin to identify the optimal hyperplane that separates classes, achieved by solving a quadratic programming problem (QPP). SVM has been extensively applied to a wide range of real-world challenges, such as remote sensing \cite{pal2005support}, EEG signal classification \cite{richhariya2018eeg}, diagnosis of Alzheimer's disease \cite{quadir2024granular}, feature extraction \cite{li2008joint}, and so on. SVM offers significant advantages by employing the structural risk minimization (SRM) principle, which enhances generalization and reduces errors during the training phase. However, it faces challenges in real-world applications due to its high computational complexity. Least squares SVM (LSSVM) was proposed by \citet{suykens1999least} to address the computational burden of SVM. Unlike SVM, LSSVM uses a quadratic loss function rather than a hinge loss function, enabling the use of equality constraints in the classification problem. As a result, the solution can be obtained by solving a system of linear equations, thereby avoiding the need to solve large QPP. LSSVM has significantly lower computation time than SVM, making it more efficient for large-scale problems. Over the past decade, significant progress has been made in enhancing the accuracy of SVM. One notable development is the generalized eigenvalue proximal SVM (GEPSVM) introduced by \citet{mangasarian2005multisurface}, and the twin SVM (TSVM), proposed by \citet{khemchandani2007twin}. GEPSVM addresses the generalized eigenvalue problem, avoiding the need to solve a QPP. However, TSVM is four times faster than standard SVM because it solves two smaller QPPs rather than a single large QPP \cite{khemchandani2007twin, tanveer2022comprehensive} and establishing it as a notable and superior alternative. In order to keep each hyperplane near to the data points of one class and at least one unit away from the data points of the other class, TSVM generates a pair of non-parallel hyperplanes. Recently, several variants of TSVM have been introduced, such as the enhanced feature based granular ball TSVM (EF-GBTSVM) \cite{quadir2024enhanced}, least squares TSVM (LSTSVM) \cite{kumar2009least}, intuitionistic fuzzy universum TSVM for imbalanced data (IFUTSVM-ID)\cite{INquadir2024intuitionistic}, IF generalized eigenvalue proximal SVM (IF-GEPSVM) \cite{quadir2024intuitionistic} multiview learning with twin parametric margin SVM (MvTPMSVM) \cite{quadir2024multiview}, multiview SVM with wave loss (Wave-MvSVM) \cite{quadir2024enhancing} and many more.

While SVM and its variants are effectively applied to regression tasks, they are referred to as support vector regression (SVR) \cite{basak2007support}. In contrast to SVM, SVR generates a tolerance margin \( \epsilon \) and seeks for the best hyperplane to minimize the error within it. Thus, SVR identifies a function where the error can be up to \( \epsilon \) distance, meaning any error within \( \epsilon \) deviation from the true values is considered acceptable. SVR learns at a relatively slow pace because it requires solving a QPP. Researchers have developed many variants of SVR to boost its performance by lowering computational complexity and increasing accuracy \cite{drucker1996support, akhtar2024enhancing}. SVR involves high computational costs, which led to the development of the efficient twin SVR (TSVR) \cite{peng2010tsvr}. TSVR utilizes \( \epsilon \)-insensitive upper and lower bounds to refine and optimize the final regression function. Its formulation is more computationally efficient, as it requires solving two smaller QPPs, leading to a substantial increase in speed compared to SVR. Further, several variants of TSVR have been proposed, including twin projection SVR \cite{peng2014twin}, \( \epsilon \)-TSVR \cite{shao2013varepsilon}, and twin support vector quantile regression (TSVQR) \cite{ye2024twin}, and so on. The previously described variations solve a pair of QPPs, which may need a significant amount of memory and time. \citet{zhao2013twin} proposed twin least squares SVR (TLSSVR), to address this high computational burden. TLSSVR replaces the inequality constraints of TSVR with equality constraints, which allows the model to be trained by solving a system of linear equations.

Both linear SVM and SVR, along with their variants, generate only linear decision boundaries. Consequently, if the data is not linearly separable, these models may have difficulty accurately classifying the data points. In contrast, kernel-based SVM \cite{scholkopf2002learning} and SVR \cite{basak2007support} have made a significant impact across various application domains by effectively managing complex data structures, enabling them to handle non-linear relationships and improve classification and regression performance \cite{shawe2004kernel, bishop2006pattern}. Kernel functions implicitly allow SVM and SVR to operate in a high-dimensional feature space, which permits them to capture non-linear relationships between features. This flexibility makes kernel-based SVM and SVR highly versatile, effectively handling a broad spectrum of data types and structures \cite{cristianini2000introduction}. Despite their robust mathematical foundations, kernel methods face challenges when scaling to large datasets. Additionally, selecting the appropriate kernel function and tuning its parameters is a complex task, requiring careful consideration and optimization to achieve the best performance. Additionally, \citet{suykens2017deep} introduced restricted kernel machine (RKM) for classification and regression, aiming to integrate kernel methods with neural network approaches. This development extends the utility of kernel techniques, enabling them to tackle more complex real-world problems effectively.RKM provides a representation of LSSVM that mimics the energy function of the restricted Boltzmann machine (RBM) \cite{hinton2006fast} by employing the Legendre-Fenchel duality \cite{rockafellar1974conjugate}. Although RKM has been effectively used in generative models \cite{pandey2021generative, pandey2022disentangled}, classification \cite{houthuys2021tensor}, and disentangled representations \cite{tonin2021unsupervised}. RKM generates a non-linear separating hyperplane that can efficiently handle non-linear relationships by using the kernel method to convert data into a high-dimensional feature space. However, as the dataset size increases, the computational demands of managing high-dimensional spaces can become significant, leading to potential performance issues in large-scale datasets. 
Motivated by the advantages of TSVM and LSTSVM, and considering the computational and generalization challenges RKM faces, we propose the twin restricted kernel machine (TRKM). This model integrates the principles of RKM with the benefits of twin methods, offering an improved approach for classification and regression tasks. By leveraging the kernel trick and addressing the computational complexities associated with large datasets, TRKM aims to enhance performance while maintaining efficiency. To the best of our knowledge, this is the first time that the RKM framework is being utilized in a twin variant for both classification and regression, aimed at improving robustness and efficiency in handling complex data structures. The main contributions of our work are as follows:
\begin{enumerate}
    \item Building on the strengths of TSVM and LSTSVM, and addressing the computational and generalization challenges encountered with RKM, we proposed the twin restricted kernel machine (TRKM). TRKM model combines the foundational principles of RKM with the advantages of twin methods, offering a robust solution for both classification and regression tasks. 
    \item TRKM adopts an energy function akin to that used in restricted Boltzmann machines (RBM), integrating both visible and hidden variables within a non-probabilistic framework to capture complex patterns.
    \item In TRKM, the kernel trick is utilized to project data into a high-dimensional feature space. Within this space, the proposed model identifies a hyperplane that optimally separates the training instances through a regularized least squares approach, enhancing the model's performance and adaptability.
    \item We introduce a novel conjugate feature duality based on the Fenchel-Young inequality, enabling the expression of classification and regression problems in terms of conjugate dual variables. This duality provides an upper bound on the objective function of the TRKM problem for both classification and regression, resulting in a new methodology within the RKM framework for these tasks.
    \item We conducted experiments on $36$ real-world datasets from UCI \cite{dua2017uci}and KEEL \cite{derrac2015keel} repository. Numerical experiments and statistical analyses reveal that the proposed TRKM model outperforms the baseline models.
    \item We conducted experiments on the brain age dataset to assess the effectiveness of the proposed TRKM model. The empirical findings provide strong evidence of the TRKM model's suitability for the early detection of AD using brain age as a biomarker.
\end{enumerate}

\section{Related Works}
This section presents the notations used in this paper, along with a brief overview of TSVM, TSVR, and the proposed RKM model for classification and regression.

\subsection{Notations}
Consider a classification dataset denoted by \(\mathscr{T}\), consisting of pairs \((x_i, y_i)\) where \(x_i \in \mathbb{R}^{1 \times m}\) is a feature vector with \(m\) features and \(y_i \in \{+1, -1\}\) is the class label, with \(+1\) indicating the positive class and \(-1\) indicating the negative class. The positive class feature vectors are collected into an \(n_1 \times m\) matrix \(A\), where \(n_1\) is the number of positive samples, and the negative class feature vectors are collected into an \(n_2 \times m\) matrix \(B\), where \(n_2\) is the number of negative samples. Here, \(n = n_1 + n_2\) represents the total number of samples in the dataset.

Consider a regression dataset denoted by \(\mathscr{M} \), consisting of pairs \((u_i, t_i)\) where \(u_i \in \mathbb{R}^{1 \times m}\) is a feature vector with \(m\) features and \(t_i \in \mathbb{R}\) is the corresponding continuous output value. The feature vectors \(u_i\) are organized into an \(n \times m\) matrix \(X\), where \(n\) is the total number of samples, and the output values \(t_i\) are organized into an \(n \times 1\) vector \(Y\). Here, \(n\) represents the total number of samples in the regression dataset.

\subsection{Twin Support Vector Machine (TSVM)}
In TSVM \cite{khemchandani2007twin}, two non-parallel hyperplanes are generated. Each hyperplane passes through the samples of its respective class and aims to maximize the margin between the hyperplanes and the samples of the opposing class. The optimization problem for TSVM can be formulated as follows:
\begin{align}
\label{eq:1}
      & \underset{w_1,b_1}{min} \hspace{0.2cm} \frac{1}{2} \|\mathcal{K}(A, C^T)w_1 + e_1b_1\|^2 + d_1e_2^T\xi_2 \nonumber \\
     & s.t. \hspace{0.2cm} -(\mathcal{K}(B, C^T)w_1 + e_2b_1) + \xi_2 \geq e_2, \nonumber \\
     & \hspace{0.8cm} \xi_2 \geq 0,
\end{align}
and
\begin{align}
\label{eq:2}
    & \underset{w_2,b_2}{min} \hspace{0.2cm} \frac{1}{2} \|\mathcal{K}(B, C^T)w_2 + e_2b_2\|^2 + d_2e_1^T\xi_1 \nonumber \\
     & s.t. \hspace{0.2cm} (\mathcal{K}(A, C^T)w_2 + e_1b_2) + \xi_1 \geq e_1, \nonumber \\
     & \hspace{0.8cm} \xi_1 \geq 0,
\end{align}
where \( d_1 \) and \( d_2 \) (\( > 0 \)) are penalty parameters, \( e_1 \) and \( e_2 \) are column vectors of ones with appropriate dimensions, $\mathcal{K}$ is kernel function and \( \xi_1 \) and \( \xi_2 \) are slack vectors, respectively. 

\subsection{Twin Support Vector Regression (TSVR)}
TSVR extends the concept of TSVM to regression problems. In TSVR, two separate regression functions are learned, each focusing on approximating different parts of the data distribution. By solving two QPPs simultaneously, TSVR aims to find two distinct regression functions that minimize the prediction error while maintaining a balance between the models. The optimization problem of TSVR is given as follows:
\begin{align}
\label{eq:5}
      & \underset{w_1,b_1}{min} \hspace{0.2cm} \frac{1}{2} \|Y - e\epsilon_1 - (\mathcal{K}(X, X^T)w_1 + e_1b_1)\|^2 + d_1e^T\xi_2 \nonumber \\
     & s.t. \hspace{0.2cm} Y-(\mathcal{K}(X, X^T)w_1 + eb_1) + \xi_2 \geq e \epsilon_1, \nonumber \\
     & \hspace{0.8cm} \xi_2 \geq 0,
\end{align}
and
\begin{align}
\label{eq:6}
    & \underset{w_2,b_2}{min} \hspace{0.2cm} \frac{1}{2} \|Y + e\epsilon_2 - (\mathcal{K}(X, X^T)w_2 + eb_2)\|^2 + d_2e^T\xi_1 \nonumber \\
     & s.t. \hspace{0.2cm} (\mathcal{K}(X, X^T)w_2 + e_1b_2) - Y + \xi_1 \geq e\epsilon_2, \nonumber \\
     & \hspace{0.8cm} \xi_1 \geq 0,
\end{align}
where \( d_1 \) and \( d_2 \) (\( > 0 \)) are regularization parameters, $\xi_1$ and $\xi_2$ are slack vectors, respectively. Therefore, the regression function of a nonlinear TSVR can be expressed as:
\begin{align}
\label{eq:7}
    f(x) &= \frac{1}{2}(f_1(x) + f_2(x)) \nonumber \\
   & = \frac{1}{2}((w_1+w_2)^T\mathcal{K}(x, X^T)) +\frac{1}{2}(b_1 + b_2).
\end{align}
\subsection{Restricted Kernel Machine (RKM)}
Here, we present an overview of the RKM model, as described by \citet{suykens2017deep}, which is closely related to the well-known LSSVM model \cite{suykens1999least}. RKM utilizes the kernel trick to transform the data into a high-dimensional feature space, enabling the construction of a linear separating hyperplane. The optimization problem for RKM is given as follows:
\begin{align}
\label{eq:8}
      J = & \frac{\gamma}{2} Tr(w^Tw) + \sum_{i=1}^N (1-(\phi(x_i)^Tw+b)y_i)h_i - \frac{\eta}{2} \sum_{i=1}^Nh_i^2,
\end{align}
where \(\gamma\) and \(\eta\) are the regularization parameters, \(b\) is the bias term, and \(h\) represents the hidden features. The solution to equation \eqref{eq:8} is obtained by taking the partial derivatives of \(J\) with respect to (w.r.t.) \(w\), \(b\), and \(h_i\), and then setting these derivatives to zero. For a detailed derivation, refer to \citet{suykens2017deep}. The solution to the optimization problem is given by:
\begin{align}
    \begin{bmatrix}
\mathbf{1}_N & \frac{1}{\gamma} \mathcal{K} + \eta \mathbf{I}_N \\
0  & \mathbf{1}_N^T
\end{bmatrix}
\begin{bmatrix}
b \\
h
\end{bmatrix}
= 
\begin{bmatrix}
y \\
0
\end{bmatrix},
\end{align}
where $\mathcal{K}$ is the kernel matrix, \(\mathbf{1}_N\) denotes a column vector with all its entries equal to one, $\mathbf{I}_N$ is the identity matrix and $h \in \mathbb{R}^{N \times 1}$.

\section{Proposed Twin Restricted Kernel Machines for Classification (TRKM-C)}
This section provides a detailed explanation of the proposed TRKM for classification (TRKM-C). We begin by outlining the general mathematical framework of the TRKM-C. TRKM-C presents a distinctive approach to kernel methods by integrating both visible and hidden variables. This method is analogous to the energy function found in RBM \cite{hinton2006fast}, thereby establishing a connection between kernel methods and RBM. TRKM-C can be associated with the energy form expression of an RBM, interpreted through hidden $h_1$ ($h_2$) and visible units $v_1$ ($v_2$) corresponding to $+1$ ($-1$) class. Similar to RBM, TRKM-C is characterized by a scalar-valued energy function in both the training and prediction phases. Consistent with kernel methods, the optimal solutions for this objective function are obtained by solving a linear system or performing matrix decomposition in both the training and prediction phases.

Assume that the function \( \phi: x_i \rightarrow \phi(x_i) \) maps the training samples from the input space to a high-dimensional feature space during both the training and prediction phases. The formulation of TRKM-C for the first hyperplane is given as follows:
\begin{align}
\label{eq:9}
     \underset{w_1, \xi_1}{\min}~J_1(w_1, \xi_1) = & \underset{w_1, \xi_1}{\min}~\frac{\gamma_1}{2} Tr(w_1^Tw_1)  + e_2^T(\phi(B)w_1 + e_2b_1) +\frac{1}{2\eta_1}\xi_1^T\xi_1 \nonumber \\
   & \hspace{0.07cm} \textbf{s.t.}  \hspace{0.2cm} \xi_1 = e_1 - \phi(A)w_1 - e_1 b_1, 
\end{align}
where $w_1$ is the interconnection matrix, \( \xi_1 \) is the error vector, \( b_1 \) denotes the bias term, the vectors $e_1$ and $e_2$ are ones of the suitable dimensions, and \( \gamma_1 \) and \( \eta_1 \) are the tunable parameters, respectively. The connection between LSSVM and RBM in TRKM-C is formed by applying the Legendre-Fenchel conjugate for quadratic functions \cite{rockafellar1974conjugate}. The TRKM-C establishes an upper bound for \( J_1 \) by incorporating the hidden layer representations \( h_1 \) as follows:
\begin{align}
\label{eq:10}
    \frac{1}{2\eta_1}\xi_1^T\xi_1 \geq \xi_1^Th_1 - \frac{\eta_1}{2}h_1^Th_1, \hspace{0.2cm} \forall \hspace{0.1cm} \xi_1, h_1.
\end{align}
Combining \eqref{eq:9} and \eqref{eq:10} leads to
\begin{align}
\label{eq:11}
    J_1 \geq ~& \xi_1^T h_1 - \frac{\eta_1}{2} h_1^Th_1 + \frac{\gamma_1}{2} Tr(w_1^Tw_1) + e_2^T(\phi(B)w_1 + e_2b_1) \nonumber \\ 
   &   \textbf{s.t.} \hspace{0.2cm} \xi_1 = e_1 - \phi(A)w_1 - e_1 b_1.
\end{align}
The tight upper bound of $J_1$ can be obtained by incorporating the constraints as follows:
\begin{align}
\label{eq:12}
    J_1 \geq &  (e_1 - \phi(A)w_1 - e_1 b_1)^T h_1 - \frac{\eta_1}{2} h_1^Th_1 + \frac{\gamma_1}{2} Tr(w_1^Tw_1) + e_2^T(\phi(B)w_1 + e_2b_1) \nonumber \\
    & =  \hat{J}_1.
\end{align}
Now, examine the stationary points of $\hat{J}_1$ by taking the gradients w.r.t. $w_1$, $h_1$, and $b_1$ as follows:
\begin{align}
    & \frac{\partial \hat{J}_1}{\partial w_1} = 0 \implies w_1 = \frac{1}{\gamma_1} (\phi(A)^Th_1 - \phi(B)^Te_2), \label{eq:13} \\
    & \frac{\partial \hat{J}_1}{\partial h_1} = 0 \implies \eta_1 h_1 = e_1 - \phi(A)w_1 - e_1 b_1, \label{eq:14}  \\
    & \frac{\partial \hat{J}_1}{\partial b_1} = 0 \implies h_1^Te_1 = e_2^te_2. \label{eq:15}
\end{align}
Substituting the weight vector \( w_1 \) from Eq \eqref{eq:13} into \eqref{eq:14}, we obtain:
\begin{align}
\label{eq:16}
    \frac{1}{\gamma_1} \phi(A)\phi(A)^Th_1 - \frac{1}{\gamma_1} \phi(A)\phi(B)^Te_2 + \eta h_1 + b_1e_1 = e_1.
\end{align}
By calculating the stationary points of the objective function, we obtain the following system of linear equations:
\begin{align}
\label{eq:17}
\left[\begin{array}{c|c } 
	\frac{1}{\gamma_1} \mathcal{K}(A, A^T) + \eta_1 I & e_1  \\  
	\hline 
	 e_1^T    & 0  
\end{array}\right]
    \begin{bmatrix}
    h_1 \\ b_1 
    \end{bmatrix}  =  \begin{bmatrix} e_1 + \frac{1}{\gamma_1} \mathcal{K}(A, B^T)e_2 \\   e_2^Te_2   \end{bmatrix},
\end{align}
where $I$ represents a matrix of ones of appropriate dimension, and $\mathcal{K}$ represents the kernel function.

The optimization problem of TRKM-C for the second hyperplane is given as follows:
\begin{align}
\label{eq:18}
     \underset{w_2, \xi_2}{\min}~J_2(w_2, \xi_2) = & \underset{w_2, \xi_2}{\min}~\frac{\gamma_2}{2} Tr(w_2^Tw_2)  - e_1^T(\phi(A)w_2 + e_1b_2) +\frac{1}{2\eta_2}\xi_2^T\xi_2 \nonumber \\
   & \hspace{0.07cm} \textbf{s.t.}  \hspace{0.2cm} \xi_2 = -e_2 - \phi(B)w_2 - e_2 b_2.
\end{align}
The tight upper bound of $J_2$ can be obtained by incorporating the constraints as follows:
\begin{align}
\label{eq:19}
    J_2 \geq &  (-e_2 - \phi(B)w_2 - e_2 b_2)^T h_2 - \frac{\eta_2}{2} h_2^Th_2 + \frac{\gamma_2}{2} Tr(w_2^Tw_2) - e_1^T(\phi(A)w_2 + e_1b_2) \nonumber \\
    & =  \hat{J}_2.
\end{align}
Analogously, $w_2$ corresponding to the $-1$ class can be calculated by the subsequent equation:
\begin{align}
\label{eq:20}
    w_2 = \frac{1}{\gamma_2} (\phi(B)^Th_2 + \phi(A)^Te_1).
\end{align}
By determining the stationary points of the objective for the \(-1\) class, we obtain the following linear problem:
\begin{align}
\label{eq:21}
\left[\begin{array}{c|c } 
	\frac{1}{\gamma_2} \mathcal{K}(B, B^T) + \eta_2 I & e_2  \\  
	\hline 
	 e_2^T    & 0  
\end{array}\right]
    \begin{bmatrix}
    h_2 \\ b_2
    \end{bmatrix}  = - \begin{bmatrix} e_2 + \frac{1}{\gamma_2} \mathcal{K}(B, A^T)e_1 \\   e_1^Te_1   \end{bmatrix}.
\end{align}

Once the optimal values of $h_1$ ($b_1$) and $h_2$ ($b_2$) are calculated for the $+1$ and $-1$ class, respectively. To predict the label of a new sample \( x \), the following decision function can be used as follows:
\begin{align}
\label{eq:22}
    \text{class}(x) =  sign\left(f_1(x) + f_2(x)\right),
\end{align}
where
\begin{align}
    f_1(x) = \frac{1}{\gamma_1}[\mathcal{K}(x,A^T)h_1 - \mathcal{K}(x,B^T)e_2] + b_1, \label{eq:23}
\end{align}
and
\begin{align}
    f_2(x) = \frac{1}{\gamma_2}[\mathcal{K}(x,B^T)h_2 + \mathcal{K}(x,A^T)e_1] + b_2. \label{eq:24} 
\end{align}
Algorithm \ref{TRKM-C training and prediction} provides a concise description of the proposed TRKM-C algorithm.

\section{Proposed Twin Restricted Kernel Machines for Regression (TRKM-R)}
This section provides a detailed detailed formulation of the proposed twin restricted kernel machines for regression (TRKM-R). TRKM-R can be related to the energy form expression of an RBM by interpreting it in terms of the hidden and visible units. This connection enables us to interpret TRKM-R using a framework similar to RBMs, where the energy function captures the interactions between hidden and visible units. Given these parallels with RBMs and the absence of hidden-to-hidden connections, we refer to this specific interpretation of the model as a TRKM representation for regression. The first optimization problem of TRKM-R can be obtained as:
\begin{align}
\label{eq:25}
     \underset{w_1, \xi_1}{\min}~J_1(w_1, \xi_1) = & \underset{w_1, \xi_1}{\min}~\frac{\gamma_1}{2} Tr(w_1^Tw_1)  -  e^T(\phi(X)w_1 + eb_1) +\frac{1}{2\eta_1}\xi_1^T\xi_1 \nonumber \\
   & \hspace{0.07cm} \textbf{s.t.}  \hspace{0.2cm} Y = \phi(X)w_1 + eb_1 - \xi_1. 
\end{align}
We derived the upper bound for \( J \) by applying the same property used in the classification problem, resulting in:
\begin{align}
    &J_1 \geq \xi_1^T h_1 - \frac{\eta_1}{2} h_1^Th_1 + \frac{\gamma_1}{2} Tr(w_1^Tw_1) - e^T(\phi(X)w_1 + eb_1) \nonumber \\ 
   &  \textbf{s.t.} \hspace{0.2cm}  Y = \phi(X)w_1 + eb_1 - \xi_1.
\end{align}
The upper bound can be obtained by substituting the constraints:
\begin{align}
    J_1 \geq &  (\phi(X)w_1 + eb_1 - Y)^T h_1 - \frac{\eta_1}{2} h_1^Th_1 + \frac{\gamma_1}{2} Tr(w_1^Tw_1) - e^T(\phi(X)w_1 + eb_1) \nonumber \\
   & =  \hat{J_1}.
\end{align}
The stationary points of $\hat{J_1}$ are given by 
\begin{align}
    & \frac{\partial \hat{J}_1}{\partial w_1} = 0 \implies w_1 = \frac{1}{\gamma_1} (\phi(X)^Te - \phi(X)^th_1), \label{eq:28} \\
    & \frac{\partial \hat{J}_1}{\partial h_1} = 0 \implies \eta_1 h_1 = \phi(X)w_1 + e b_1 - Y, \label{eq:29}  \\
    & \frac{\partial \hat{J}_1}{\partial b_1} = 0 \implies h_1^Te = e^Te. \label{eq:30}
\end{align}
On employing $w_1$ in \eqref{eq:29}, we get
\begin{align}
\label{eq:31}
    \frac{1}{\gamma_1} \phi(X)\phi(X)^Te - \frac{1}{\gamma_1} \phi(X)\phi(X)^Th_1 - \eta h_1 + eb_1 = Y,
\end{align}
Using Eqs \eqref{eq:31} and \eqref{eq:30}, we can find the solution by solving the following system of linear equations:
\begin{align}
\label{eq:32}
\left[\begin{array}{c|c } 
	\frac{1}{\gamma_1} \mathcal{K}(X, X^T) + \eta_1 I & -e  \\  
	\hline 
	 e^T    & 0  
\end{array}\right]
    \begin{bmatrix}
    h_1 \\ b_1 
    \end{bmatrix}  =  \begin{bmatrix} -Y + \frac{1}{\gamma_1} \mathcal{K}(X, X^T)e \\   e^Te   \end{bmatrix}.
\end{align}
The second optimization problem of TRKM-R can be obtained as follows:
\begin{align}
\label{eq:33}
     \underset{w_2, \xi_2}{\min}~J_2(w_2, \xi_2) = & \underset{w_2, \xi_2}{\min}~\frac{\gamma_2}{2} Tr(w_2^Tw_2)  +  e^T(\phi(X)w_2 + eb_2) +\frac{1}{2\eta_2}\xi_2^T\xi_2 \nonumber \\
   & \hspace{0.07cm} \textbf{s.t.}  \hspace{0.2cm} Y = \phi(X)w_1 + eb_2 + \xi_2. 
\end{align}
By applying the constraints, we derive the following accurate upper bound for \( J_2 \):
\begin{align}
\label{eq:34}
    J_2 \geq &  (Y - \phi(X)w_2 - eb_2 )^T h_2 - \frac{\eta_2}{2} h_2^Th_2 + \frac{\gamma_2}{2} Tr(w_2^Tw_2) + e^T(\phi(X)w_2 + eb_2) \nonumber \\
    & =  \hat{J}.
\end{align}
By taking the gradient of \eqref{eq:34}, we determine the weight vector associated with the second optimization problem:
\begin{align}
    w_2 = \frac{1}{\gamma_2} (\phi(X)^Th_2 - \phi(X)^Te).
\end{align}
Similarly, we can derive the following system of linear equations:
\begin{align}
\label{eq:36}
\left[\begin{array}{c|c } 
	\frac{1}{\gamma_2} \mathcal{K}(X, X) + \eta_2 I & e  \\  
	\hline 
	 e^T    & 0  
\end{array}\right]
    \begin{bmatrix}
    h_2 \\ b_2 
    \end{bmatrix}  =  \begin{bmatrix} Y + \frac{1}{\gamma_2} \mathcal{K}(X, X^T)e \\   e^Te   \end{bmatrix}.
\end{align}
Once the optimal values of $h_1$ ($b_1$) and $h_2$ ($b_2$) are calculated. Then, we construct the estimated regressor as follows:
\begin{align}
\label{eq:37}
    f(x) =  \left(f_1(x) + f_2(x)\right)/2,
\end{align}
where
\begin{align}
    f_1(x) = \frac{1}{\gamma_1} \mathcal{K}(x, X^T)(e - h_1) + b_1, \label{eq:38}
\end{align}
and
\begin{align}
    f_2(x) = \frac{1}{\gamma_2} \mathcal{K}(x, X^T)(h_2 - e) + b_2.  \label{eq:39}
\end{align}
Algorithm \ref{TRKM-R training and prediction} provides a concise description of the proposed TRKM-R algorithm.

\begin{algorithm}
\caption{Training and prediction of the proposed TRKM-C.}
\label{TRKM-C training and prediction}
\textbf{Require:} Let $\{x_i\}_{i=1}^n$ be the input training dataset, $A$ and $B$ represent the matrix of $+1$ and $-1$ classes and the trade-off parameters $\gamma_1$, $\gamma_2$, $\eta_1$ and $\eta_2$, respectively.
\begin{algorithmic}[1]
\STATE Find the kernel matrix $\mathcal{K}(A, A^T)$, $\mathcal{K}(A, B^T)$, $\mathcal{K}(B, B^T)$, and $\mathcal{K}(B, A^T)$.\\
\STATE Calculate $h_1$, $b_1$ and $h_2$, $b_2$ corresponding to $+1$ and $-1$ class using Eqs \eqref{eq:17} and \eqref{eq:21}.\\
\STATE Find the decision function with dual representation using Eqs \eqref{eq:23} and \eqref{eq:24}.
\STATE The classification of a test sample \(x\) into class \(+1\) or \(-1\) is determined using Eq. \eqref{eq:22}.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Training and prediction of the proposed TRKM-R.}
\label{TRKM-R training and prediction}
\textbf{Require:} Let $X$ be the input training dataset, $Y$ represent the vector that contains the input data's target values, and the trade-off parameters $\gamma_1$, $\gamma_2$, $\eta_1$ and $\eta_2$, respectively.
\begin{algorithmic}[1]
\STATE Find the kernel matrix $\mathcal{K}(X, X^T)$.\\
\STATE Calculate $h_1$, $b_1$ and $h_2$, $b_2$ using Eqs \eqref{eq:32} and \eqref{eq:36}.\\
\STATE Find the decision function with dual representation using Eqs \eqref{eq:38} and \eqref{eq:39}.
\STATE The predicted value of the new samples $x$  is determined using Eq \eqref{eq:37}.
\end{algorithmic}
\end{algorithm}

\section{Experiments and Results}
In this section, we conduct an extensive evaluation of the proposed TRKM model by performing experiments on UCI and KEEL datasets and comparing its performance with that of leading state-of-the-art models. Moreover, we use the Brain Age prediction dataset to evaluate the proposed models.

\subsection{Experimental Setup}
The experiments are conducted in Python 3.11 on Windows 11 running on a PC with system configuration Intel® Xeon® Gold 6226R CPU and 128 GB of RAM. The dataset is divided randomly in a $70:30$ proportion, where $70\%$ is allocated for training the model, and the remaining $30\%$ is used for testing. we use five-fold cross-validation combined with a grid search method to fine-tune the model's hyperparameters, selecting the following ranges: \( \eta_i = \gamma_i = \{10^{-5}, 10^{-4}, \ldots, 10^5\} \) for \( i = 1, 2 \). We employ the Gaussian kernel, defined as \( \mathcal{K}(x_i, x_j) = e^{-\frac{1}{2\sigma^2} \| x_i - x_j \|^2} \). The range \(\{2^{-5}, 2^{-4}, \ldots, 2^5\}\) is used to choose the Gaussian kernel parameter \(\sigma\). For TRKM-C and TRKM-R, we adopt equal penalty parameters, $i.e.$ $\eta_1 = \eta_2$ and $\gamma_1 = \gamma_2$, respectively. To assess the performance of the proposed TRKM-R model, we employ four metrics, including mean absolute error ($MAE$), root mean squared error ($RMSE$), negative error ($Neg~Error$), and positive error ($Pos~Error$). The following are the specific definitions of these metrics:
\begin{align}
   & RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(x_i) - y_i)^2}, \\
    & MAE = \frac{1}{n} \sum_{i=1}^n \lvert (f(x_i) - y_i)\rvert, 
\end{align}
\begin{align}
    & Pos~Error = \frac{1}{n}\sum_{i=1, f(x_i)\leq y_i }^n  \lvert (f(x_i) - y_i)\rvert, \\
    & Neg~Error = \frac{1}{n}\sum_{i=1, f(x_i) > y_i }^n  \lvert (f(x_i) - y_i)\rvert.
\end{align}
 
\subsection{Experimental Results and Statistical Analysis on UCI and KEEL Datasets for Classification}

\begin{table*}[htp]
\centering
    \caption{Classification ACC of the proposed TRKM-C and the baseline models across the real world datasets i.e. UCI and KEEL.}
    \label{Average ACC and average rank for UCI and KEEL datasets}
    \resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcccccc}
\hline
Model $\rightarrow$ & SVM \cite{cortes1995support} & TSVM \cite{khemchandani2007twin} & Pin-GTSVM \cite{tanveer2019general} & RKM \cite{suykens2017deep} & GBTSVM \cite{GBquadir2024granular} & TRKM-C$^{\dagger}$ \\ 
Dataset $\downarrow$ &    ACC (\%)  &  ACC (\%)  &  ACC (\%) &  ACC (\%) &  ACC (\%)  &   ACC (\%)    \\ 
$(\#Samples \times \#Feature)$  &   $(d_1, \sigma)$  &   $(d_1, d_2, \sigma)$  &   $(d_1, d_2, \tau, \sigma)$  &  $(\gamma, \eta, \sigma)$ &  $(d_1, d_2, \sigma)$  &   $(\gamma_1, \eta_1, \sigma)$   \\ \hline
aus & $56.25$ & $81.25$ & $79.33$ & $88.94$ & $87.98$ & $87.5$ \\
$(690 \times 15)$  & $(10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 2^{-5})$ & $(10^{2}, 10^{4}, 0.1, 2^{3})$ & $(10^{1}, 10^{3}, 2^{4})$ & $(10^{-1}, 1, 2^{5})$ & $(10^{-1}, 10^{-3}, 2^{2})$ \\
breast\_cancer & $74.42$ & $67.44$ & $63.95$ & $75.58$ & $62.79$ & $77.91$ \\
$(286 \times 10)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-2}, 2^{-4})$ & $(10^{2}, 10^{-2}, 1, 2^{-4})$ & $(10^{1}, 10^{-5}, 2^{4})$ & $(10^{1}, 10^{1}, 2^{5})$ & $(10^{-1}, 10^{-1}, 2^{2})$ \\
checkerboard\_Data & $56.25$ & $81.25$ & $79.33$ & $86.94$ & $87.98$ & $87.5$ \\
$(690 \times 15)$ & $(10^{-5}, 2^{-5})$ & $(10^{-2}, 10^{-2}, 2^{-5})$  &  $(10^{-1}, 10^{-5}, 0.5, 2^{-3})$  & $(10^{1}, 10^{3}, 2^{4})$ & $(10^{-1}, 1, 2^{5})$  & $(10^{-1}, 10^{-3}, 2^{2})$ \\
chess\_krvkp & $52.35$ & $90.41$ & $90.41$ & $98.27$ & $97.08$ & $98.75$ \\
$(3196 \times 37)$ & $(10^{-3}, 2^{3})$ & $(10^{-1}, 10^{-1}, 2^{-5})$  & $(10^{-5}, 10^{-5}, 0.1, 2^{4})$ & $(10^{-1}, 10^{-5}, 2^{5})$ & $(1, 1, 2^{5})$ &  $(10^{-2}, 10^{-5}, 2^{5})$ \\
crossplane130  & $51.28$ & $100$ & $97.44$ & $100$ & $100$ & $100$ \\
$(130 \times 3)$ & $(10^{-3}, 2^{4})$ & $(10^{-5}, 10^{-5}, 2^{-5})$  & $(10^{-1}, 10^{-5}, 1, 2^{-2})$  & $(10^{-5}, 10^{4}, 2^{-3})$ & $(10^{-5}, 10^{-5}, 2^{5})$ & $(10^{-5}, 10^{-5}, 2^{-5})$  \\
ecoli-0-1\_vs\_5 & $94.44$ & $97.22$ & $95.83$ & $96.81$ & $98.89$ & $97.74$ \\
$(240 \times 7)$ & $(1, 1)$ & $(10^{-5}, 10^{-4}, 2^{-3})$  &  $(10^{-3}, 10^{-5}, 0.75, 2^{4})$ & $(10^{2}, 10^{-5}, 2^{1})$  & $(10^{-4}, 10^{-5}, 2^{5})$ & $(10^{-1}, 10^{-5}, 2^{2})$ \\
ecoli-0-1\_vs\_2-3-5 & $91.89$ & $90.59$ & $94.59$ & $94.59$ & $90.81$ & $93.24$ \\
$(244 \times 8)$ & $(1, 1)$ &  $(10^{-4}, 10^{-3}, 2^{5})$ & $(10^{-5}, 10^{-5}, 0.1, 2^{3})$  &  $(10^{2}, 10^{2}, 2^{4})$ & $(10^{-2}, 1, 2^{2})$  &  $(10^{-5}, 10^{4}, 2^{-2})$ \\
ecoli-0-1-4-7\_vs\_2-3-5-6 & $87.13$ & $96.04$ & $95.05$ & $93.55$ & $88.12$ & $96.67$ \\
$(336 \times 8)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 1, 2^{-1})$  & $(10^{-5}, 10^{-5}, 0.1, 2^{2})$  & $(10^{-4}, 10^{2}, 2^{-1})$ & $(1, 10^{-1}, 2^{2})$ & $(10^{-5}, 10^{-5}, 2^{-5})$ \\
ecoli-0-1-4-7\_vs\_5-6 & $91$ & $96$ & $96$ & $98.36$ & $94$ & $98.46$ \\
$(332 \times 7)$ & $(10^{1}, 1)$ & $(10^{-3}, 10^{-2}, 2^{2})$  & $(10^{-1}, 10^{-5}, 0.5, 2^{-3})$  & $(10^{-5}, 10^{-5}, 1)$ & $(1, 1, 2^{5})$  & $(10^{-5}, 10^{-5}, 2^{-5})$  \\
ecoli-0-1-4-6\_vs\_5 & $98.81$ & $100$ & $100$ & $98.81$ & $97.62$ & $98.81$ \\
$(280 \times 7)$ & $(1, 1)$ &  $(10^{-5}, 10^{-5}, 2^{-5})$  & $(10^{-5}, 10^{-5}, 0.1, 2^{-3})$  & $(10^{-5}, 10^{1}, 2^{-1})$ & $(1, 1, 2^{5})$ & $(10^{-5}, 10^{-5}, 2^{-1})$ \\
haber & $82.61$ & $75.35$ & $69.57$ & $76.09$ & $77.17$ & $80.43$ \\
$(306 \times 4)$ & $(10^{-5}, 2^{-5})$ &  $(1, 10^{-1}, 2^{-3})$  & $(10^{-1}, 10^{1}, 1, 2^{-3})$ & $(10^{-1}, 10^{1}, 1)$ & $(10^{1}, 1, 2^{3})$  & $(10^{-4}, 10^{-1}, 2^{3})$ \\
haberman & $81.52$ & $75.35$ & $69.57$ & $76.09$ & $77.17$ & $80.43$ \\
$(306 \times 4)$ & $(10^{-1}, 1)$ &  $(10^{-1}, 1, 2^{-2})$ & $(10^{1}, 10^{-1}, 1, 2^{-5})$  & $(10^{-1}, 10^{1}, 1)$ & $(1, 10^{1}, 2^{2})$ & $(10^{-3}, 10^{-1}, 2^{3})$ \\
haberman\_survival & $82.61$ & $79.35$ & $69.57$ & $76.09$ & $79.35$ & $80.43$ \\
$(306 \times 4)$ & $(10^{-5}, 2^{-5})$ &  $(1, 10^{-1}, 2^{-5})$ & $(10^{-1}, 10^{1}, 1, 2^{5})$ &  $(10^{-1}, 10^{1}, 1)$ & $(10^{1}, 10^{1}, 2^{5})$ & $(10^{-4}, 10^{-1}, 2^{3})$ \\
heart-stat & $56.79$ & $70.37$ & $70.37$ & $87.65$ & $79.01$ & $85.19$ \\
$(270 \times 14)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-1}, 2^{-5})$  & $(10^{-5}, 1, 0.25, 2^{-5})$ & $(10^{1}, 10^{3}, 2^{4})$ & $(10^{-5}, 10^{-5}, 2^{5})$ &  $(10^{-2}, 10^{-2}, 2^{3})$ \\
led7digit-0-2-4-5-6-7-8-9\_vs\_1 & $81.95$ & $93.98$ & $93.98$ & $94.74$ & $94.74$ & $94.98$ \\
$(443 \times 8)$ & $(1, 1)$ &  $(10^{-5}, 10^{-4}, 2^{-3})$ &  $(10^{-5}, 1, 0.1, 2^{-5})$ & $(10^{1}, 10^{-5}, 2^{2})$ & $(10^{-5}, 10^{-5}, 2^{5})$  & $(10^{-2}, 10^{-4}, 2^{-5})$ \\
mammographic & $52.94$ & $79.93$ & $77.82$ & $78.74$ & $81.31$ & $79.24$ \\
$(961 \times 6)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-1}, 2^{-5})$  & $(10^{3}, 10^{-5}, 1, 2^{-5})$ & $(10^{-1}, 10^{-5}, 2^{4})$ & $(10^{-1}, 10^{-1}, 2^{5})$ & $(10^{-5}, 10^{-2}, 2^{-2})$ \\
monks\_3 & $46.11$ & $75.21$ & $95.81$ & $49.1$ & $80.24$ & $80.84$ \\
$(554 \times 7)$ & $(10^{-5}, 2^{-5})$ &  $(1, 10^{-1}, 2^{-2})$ & $(10^{1}, 1, 0.25, 2^{-3})$ & $(10^{5}, 10^{2}, 2^{-5})$ & $(1, 10^{1}, 2^{5})$ & $(10^{2}, 10^{-4}, 2^{-2})$ \\
mushroom & $63.41$ & $70.65$ & $80.51$ & $81.77$ & $84.91$ & $96.41$ \\
$(8124 \times 22)$ & $(10^{-3}, 2^{1})$  & $(10^{2}, 10^{3}, 2^{5})$  & $(10^{1}, 10^{2}, 0.25, 2^{-3})$ & $(10^{-1}, 10^{-5}, 2^{1})$ & $(1, 10^{-1}, 2^{3})$ & $(10^{-4}, 10^{-2}, 2^{5})$ \\
musk\_1 & $53.15$ & $83.15$ & $53.15$ & $92.31$ & $91.61$ & $92.31$ \\
$(476 \times 167)$ & $(10^{-5}, 2^{-5})$ &  $(10^{-5}, 10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 0.1, 2^{-1})$ & $(10^{1}, 10^{-5}, 2^{5})$ & $(10^{-1}, 1, 2^{5})$ & $(10^{-1}, 10^{-5}, 2^{5})$ \\
new-thyroid1 & $87.69$ & $98.46$ & $96.92$ & $98.46$ & $95.38$ & $96.92$ \\
$(215 \times 16)$ & $(10^{-5}, 2^{-5})$ & $(10^{-2}, 10^{-2}, 2^{-3})$ & $(1, 10^{1}, 0.75, 2^{-5})$ & $(1, 10^{3}, 2^{1})$ & $(10^{-5}, 10^{-5}, 2^{4})$ & $(10^{-2}, 10^{-5}, 2^{3})$ \\
oocytes\_merluccius\_nucleus\_4d & $64.82$ & $76.22$ & $79.48$ & $75.06$ & $77.2$ & $79.15$ \\
$(1022 \times 42)$ & $(10^{-5}, 2^{-5})$ & $(10^{-2}, 10^{-1}, 2^{-5})$  & $(1, 1, 0.25, 2^{-4})$ & $(10^{-1}, 10^{-1}, 2^{5})$ & $(1, 1, 2^{4})$  & $(10^{-1}, 10^{-2}, 2^{2})$ \\
ozone & $96.58$ & $96.58$ & $96.58$ & $87.96$ & $94.09$ & $97.58$ \\
$(2536 \times 6)$ & $(10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 2^{-5})$   & $(10^{-5}, 10^{-5}, 0.1, 2^{-3})$ & $(10^{-1}, 1, 2^{5})$ & $(10^{1}, 1, 2^{5})$ & $(10^{-2}, 10^{-2}, 2^{3})$  \\
ringnorm & $90.42$ & $92.65$ & $93.06$ & $89.07$ & $96.94$ & $96.44$ \\
 $(7400 \times 21)$ & $(1, 2^{1})$ &  $(10^{-5}, 10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 0.1, 2^{-3})$ & $(10^{-2}, 10^{-5}, 2^{1})$ & $(10^{1}, 1, 2^{5})$ & $(10^{-5}, 10^{-5}, 1)$  \\
shuttle-6\_vs\_2-3 & $95.65$ & $97.1$ & $97.1$ & $100$ & $98.55$ & $98.55$ \\
$(230 \times 10)$ & $(10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-4}, 2^{-4})$   & $(10^{-5}, 10^{-5}, 0.1, 2^{-4})$ & $(10^{-5}, 10^{5}, 2^{3})$ & $(10^{-2}, 10^{-1}, 2^{4})$ & $(10^{-5}, 10^{4}, 1)$ \\
spambase & $62.2$ & $84.79$ & $76.76$ & $79.37$ & $90.88$ & $98$ \\
$(4601 \times 58)$ & $(10^{-1}, 1)$ & $(10^{-2}, 1, 2^{-3})$  &  $(10^{3}, 10^{4}, 0.5, 2^{3})$ & $(10^{2}, 10^{2}, 2^{4})$ & $(10^{1}, 10^{1}, 2^{5})$ & $(10^{-5}, 10^{3}, 2^{2})$ \\
spectf & $80.25$ & $79.42$ & $83.95$ & $83.95$ & $85.19$ & $85.19$ \\
$(267 \times 45)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-1}, 2^{-5})$  & $(10^{-5}, 10^{-4}, 0.75, 2^{-3})$ & $(10^{1}, 1, 2^{3})$ & $(10^{-1}, 10^{-3}, 2^{4})$ & $(10^{-4}, 10^{-5}, 2^{1})$ \\
tic\_tac\_toe & $66.32$ & $95$ & $100$ & $100$ & $98.96$ & $99.65$ \\
$(958 \times 10)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-2}, 2^{-2})$   & $(10^{-5}, 10^{-5}, 0.1, 2^{2})$   & $(10^{-5}, 10^{3}, 2^{-2})$ & $(10^{1}, 10^{1}, 2^{4})$ & $(10^{-1}, 10^{-5}, 2^{5})$  \\
vehicle1 & $75.98$ & $80.31$ & $77.95$ & $88.58$ & $81.5$ & $81.89$ \\
$(846 \times 19)$ & $(10^{-5}, 2^{-5})$ & $(10^{-2}, 10^{-2}, 2^{-5})$ & $(10^{-2}, 1, 0.5, 2^{3})$ & $(10^{-1}, 1, 2^{5})$ & $(1, 1, 2^{5})$ & $(10^{-3}, 1, 2^{2})$ \\
vehicle2 & $77.95$ & $76.46$ & $96.06$ & $98.82$ & $94.49$ & $95.28$ \\
$(846 \times 19)$ & $(1, 1)$ & $(10^{-2}, 10^{-1}, 2^{-2})$ & $(10^{-2}, 1, 0.5, 2^{5})$ & $(10^{-1}, 10^{-1}, 2^{5})$ & $(10^{-1}, 10^{-1}, 2^{5})$ & $(10^{-5}, 10^{-5}, 1)$ \\
vertebral\_column\_2clases & $69.89$ & $89.25$ & $81.72$ & $89.25$ & $88.17$ & $87.1$ \\
$(310 \times 7)$ & $(10^{-5}, 2^{-5})$ & $(10^{-2}, 10^{-2}, 2^{-5})$  & $(1, 1, 0.25, 2^{-3})$ & $(1, 10^{5}, 2^{5})$ & $(10^{-1}, 10^{-1}, 2^{4})$ & $(10^{-3}, 1, 2^{1})$ \\
wpbc & $77.97$ & $77.97$ & $77.97$ & $77.97$ & $76.27$ & $76.27$ \\
$(194 \times 34)$ & $(10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 2^{-5})$ & $(10^{-5}, 10^{-5}, 0.1, 2^{5})$ & $(10^{1}, 10^{-1}, 2^{4})$ & $(10^{-5}, 10^{-5}, 2^{5})$ & $(10^{-3}, 1, 2^{4})$ \\
yeast-0-2-5-6\_vs\_3-7-8-9 & $91.39$ & $84.04$ & $94.37$ & $94.04$ & $94.04$ & $92.72$ \\
$(1004 \times 9)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-1}, 2^{1})$ & $(10^{-1}, 10^{1}, 0.75, 2^{-2})$  & $(10^{2}, 10^{3}, 2^{4})$ & $(1, 1, 2^{5})$  & $(10^{-5}, 10^{3}, 2^{-1})$ \\
yeast-0-3-5-9\_vs\_7-8 & $87.5$ & $53.15$ & $95.7$ & $97.35$ & $90.79$ & $95.7$ \\
$(1004 \times 9)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-5}, 2^{-5})$ & $(10^{2}, 10^{3}, 0.1, 2^{2})$ & $(10^{3}, 10^{4}, 2^{5})$ & $(10^{-5}, 10^{-5}, 2^{5})$  &  $(1, 10^{-2}, 2^{1})$\\
yeast-0-5-6-7-9\_vs\_4 & $91.19$ & $82.45$ & $88.68$ & $91.82$ & $91.19$ & $92.45$ \\
$(528 \times 9)$ & $(10^{-5}, 2^{-5})$ &  $(10^{-3}, 10^{-2}, 2^{-4})$  & $(10^{5}, 10^{5}, 0.1, 2^{5})$ & $(10^{1}, 1, 2^{2})$ & $(10^{-2}, 10^{-2}, 2^{5})$ & $(10^{-5}, 10^{2}, 2^{-1})$ \\
yeast-2\_vs\_4 & $85.81$ & $94.19$ & $94.19$ & $96.77$ & $97.42$ & $97.32$ \\
$(514 \times 9)$ & $(10^{-5}, 2^{-5})$ & $(10^{-1}, 10^{-1}, 2^{-5})$ & $(10^{-2}, 1, 0.5, 2^{2})$ & $(10^{1}, 10^{-1}, 2^{3})$ & $(10^{-1}, 10^{-1}, 2^{5})$ & $(10^{-4}, 10^{2}, 1)$ \\
yeast3 & $89.24$ & $92.38$ & $91.26$ & $93.95$ & $92.83$ & $93.5$ \\ 
$(1484 \times 9)$ & $(10^{-5}, 2^{-5})$  &  $(10^{-2}, 10^{-1}, 2^{-3})$ & $(10^{3}, 1, 0.5, 2^{4})$  & $(10^{1}, 10^{-1}, 2^{2})$ & $(1, 10^{2}, 2^{5})$ & $(10^{-5}, 10^{-5}, 2^{-1})$ \\ \hline
Avergae ACC & $76.27$ & $84.83$ & $85.95$ & $88.52$ & $88.74$ & $\textbf{90.85}$ \\ \hline
Avergae Rank & $4.98$ & $4$ & $3.86$ & $2.77$ & $3.16$ & $\textbf{2.20}$ \\  \hline
\multicolumn{7}{l}{$^{\dagger}$ represents the proposed models.} 
\end{tabular}}
\end{table*}


In this subsection, we offer a thorough comparison of the proposed TRKM-C model against the SVM \cite{cortes1995support}, TSVM \cite{khemchandani2007twin}, Pin-GTSVM \cite{tanveer2019general}, RKM \cite{suykens2017deep}, and GBTSVM \cite{GBquadir2024granular} models. $36$ benchmark datasets from the KEEL and UCI repositories are used in this comparison. Optimal parameters and the average classification accuracy (ACC) of the proposed TRKM-C, along with the baseline SVM, TSVM, Pin-GTSVM, RKM, and GBTSVM models, are presented in Table \ref{Average ACC and average rank for UCI and KEEL datasets}. The ACC comparison demonstrates that our proposed TRKM-C model outperforms the baseline SVM, TSVM, Pin-GTSVM, RKM, and GBTSVM models on most of the datasets. The proposed TRKM-C model provides an average ACC of $90.85\%$,  whereas the average ACC of SVM, TSVM, Pin-GTSVM, RKM, and GBTSVM models are $76.27\%$, $84.83\%$, $85.95\%$, $88.52\%$, and $88.74$ respectively. The proposed TRKM-C model achieves a higher average ACC compared to the existing models. Average accuracy can sometimes be misleading if a model performs exceptionally well on one dataset but poorly across others. This can skew the results and may not provide a complete picture of the model's performance. To gauge the effectiveness and performance of the models, we employ the ranking method outlined by \citet{demvsar2006statistical}. In this method, classifiers are ranked such that models with better performance receive lower ranks, while those with poorer performance are assigned higher ranks. The rank of the \( j^{th} \) model on the \( i^{th} \) dataset is expressed as \( \mathfrak{R}_{j}^{i} \) in order to evaluate \( p \) models over \( N \) datasets. The average rank of the model is determined as \( \mathfrak{R}_j = \frac{1}{N} \sum_{i=1}^{N} \mathfrak{R}_{j}^i \). The average rank of the proposed TRKM-C along with the baselines SVM, TSVM, Pin-GTSVM, RKM, and GBTSVM models are $2.20$, $4.98$, $4.00$, $3.86$, $2.77$ and $3.16$, respectively. The lowest average rank is attained by the proposed TRKM-C model, indicating the most favorable position compared to the existing models. The Friedman test \cite{demvsar2006statistical} is then used to determine whether the models differ significantly from one another. Under the null hypothesis of the Friedman test, it is assumed that all models have the same average rank, indicating equivalent performance. The Friedman test follows a chi-squared distribution (\( \chi^2_F \)) with \( (p - 1) \) degrees of freedom. Its calculation is given by:  $\chi^2_F = \frac{12N}{p(p+1)}\left[\sum_{j}\mathfrak{R}_j^2 - \frac{p(p+1)^2}{4}\right].$ The $F_F$ statistic is calculated using the formula: $F_F = \frac{(N - 1) \chi^2_F}{N(p - 1) - \chi^2_F}$, where the $F$-distribution has \( (p - 1) \) and \( (N - 1) \times (p - 1) \) degrees of freedom. For \( p = 6 \) and \( N = 36 \), we obtain \( \chi^2_F = 48.95 \) and \( F_F = 13.0732 \) at the $5\%$ significance level. According to the statistical $F$-distribution table, \( F_F(5, 175) = 2.2657 \). Since $F_F > 2.2657$, the null hypothesis is rejected. As a result, there is a statistically significant disparity among the models under comparison. The Nemenyi post hoc test is then used to assess the models' pairwise differences. The critical difference (C.D.) is calculated using the formula \(\text{C.D.} = q_\alpha \sqrt{\frac{p(p + 1)}{6N}}\), where \( q_\alpha \) is the two-tailed Nemenyi test critical value obtained from the distribution table. With reference to the $F$-distribution table, the computed C.D. is $1.2567$ at the $5\%$ significance level, with \(q_\alpha = 2.850 \). The proposed TRKM-C model's average rank differences with the SVM, TSVM, Pin-GTSVM, RKM, and GBTSVM models are as follows: $2.78$, $1.80$, $1.66$, $0.57$ and $0.96$, respectively. The Nemenyi post hoc test confirms that the proposed TRKM-C model is significantly superior to the baseline SVM, TSVM, and Pin-GTSVM models. The proposed TRKM-C model shows a statistically significant improvement over the RKM and GBTSVM models. Based on the lowest average rank attained by the TRKM-C model, we conclude that the proposed TRKM-C model excels in overall performance and ranking compared to existing models.
\begin{table}[htp]
\centering
    \caption{Comparison of Win-Tie-Loss Results on UCI and KEEL classification datasets.}
    \label{win tie loss sign test}
    \resizebox{0.9\textwidth}{!}{
    % \resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\hline
\textbf{}   & SVM \cite{cortes1995support}         &  TSVM \cite{khemchandani2007twin}    & Pin-GTSVM \cite{tanveer2019general}  & RKM \cite{suykens2017deep} & GBTSVM \cite{GBquadir2024granular} \\ \hline
TSVM \cite{khemchandani2007twin} & {[}$24, 2, 10${]} &  &  &  &  \\
Pin-GTSVM \cite{tanveer2019general} & {[}$28, 3, 5${]} & {[}$11, 9, 16${]} &  &  &  \\
RKM \cite{suykens2017deep} & {[}$29, 2, 5${]} & {[}$22, 4, 10${]} & {[}$25, 4, 7${]} &  &  \\
GBTSVM \cite{GBquadir2024granular} & {[}$27, 1, 8${]} & {[}$26, 2, 8${]} & {[}$22, 0, 14${]} & {[}$14, 3, 19${]} &  \\
TRKM-C$^{\dagger}$ & {[}$31, 1, 4${]} & {[}$30, 1, 5${]} & {[}$26, 2, 8${]} & {[}$20, 3, 13${]} & {[}$24, 4, 8${]} \\ \hline
\multicolumn{6}{l}{$^{\dagger}$ represents the proposed models.}
\end{tabular}}
\end{table}
Furthermore, to evaluate the model's performance, we use the pairwise win-tie-loss sign test. Under the null hypothesis, the test assumes that two models perform equally well, each expected to be the best in half of the $N$ datasets. A model is considered significantly superior if it surpasses the competition on approximately \( N/2 + 1.96 \sqrt{N/2} \) datasets. The two models are distributed equally if the number of ties between them is even. One tie is eliminated and the remaining ties are divided equally among the classifiers if the number of ties is odd. For \( N = 36 \), a model needs to secure at least $24$ wins to demonstrate a significant difference from the other models. The proposed TRKM-C model is evaluated in comparison to the baseline models in Table \ref{win tie loss sign test}. The entry $[x, y, z]$ in Table \ref{win tie loss sign test} shows that, when compared to the model listed in the column, the model listed in the row wins $x$ times, ties $y$ times, and loses $z$ times. The proposed TRKM-C model demonstrates a statistically significant difference compared to the baseline models, with the exception of RKM. The TRKM-C model consistently demonstrates its superior performance over the RKM model, as evidenced by its higher winning percentage. The results indicate that the proposed TRKM-C model considerably surpasses the performance of the baseline models.

\subsection{Sensitivity Analysis on Real World UCI and KEEL Datasets for Classification}
A comprehensive evaluation of the proposed TRKM-C model's robustness requires analyzing its sensitivity to different hyperparameters. We conduct sensitivity analyses focusing on the following aspects: (1) $\eta$ versus $\gamma$, and (2) \(\eta\) versus $\sigma$. We experiment with different ranges for each hyperparameter and assess their impact on the model's performance.
\begin{enumerate}
    \item A complete assessment of the robustness of the proposed TRKM-C model requires examining its sensitivity to the hyperparameters \(\eta\) and \(\gamma\). This thorough investigation aids in determining the setup that optimizes predictive ACC and improves the model's performance on unknown inputs. Fig \ref{Effect of parameters eta and gamma} highlights significant fluctuations in model ACC across various \(\eta\) and \(\gamma\) values, demonstrating the sensitivity of the model to these hyperparameters. From Figs. \ref{1a} and \ref{1b}, the TRKM-C model performs best inside the ranges of \(\eta\) from \(10^{-1}\) to \(10^{5}\) and \(\gamma\) from \(10^{-5}\) to \(10^{-1}\). Also, Figs. \ref{1c} and \ref{1d} show that the model achieves maximum ACC when \(\eta\) and \(\gamma\) are within \(10^{-5}\) to \(10^{3}\). Therefore, we recommend using \(\eta\) and \(\gamma\) within the range of \(10^{-5}\) to \(10^{-1}\) for optimal results. However, fine-tuning may be required depending on the specific characteristics of the dataset to achieve the best generalization performance with the TRKM-C model.
    \item We assess the performance of the proposed TRKM-C model by altering the values of \(\eta\) and \(\sigma\). Fig. \ref{Effect of parameters eta and sigma} shows significant variations in model ACC across different combinations of \(\eta\) and \(\sigma\), underscoring the sensitivity of the proposed TRKM-C model to these hyperparameters. We can see from the results shown in Figs. \ref{2a} and \ref{2b} that the proposed TRKM-C model performs well within \(\gamma\) ranges of \(10^{-3}\) to \(10^5\).  Fig \ref{2c} shows an increase in testing ACC within the \(\eta\) range of \(10^{-1}\) to \(10^{5}\) and the \(\sigma\) range of \(2^{-3}\) to \(2^{3}\). Similarly, Fig. \ref{2d} shows that testing ACC increases as \(\eta\) ranges from \(10^{-5}\) to \(10^{-1}\) across all ranges of \(\sigma\). Therefore, it is essential to carefully select the hyperparameters for the TRKM-C model to attain optimal generalization performance.
\end{enumerate}

\begin{figure*}[ht!]
\begin{minipage}{.246\linewidth}
\centering
\subfloat[haber]{\label{1a}\includegraphics[scale=0.20]{img/eta_vs_gamma_haber.jpg}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[monk3]{\label{1b}\includegraphics[scale=0.20]{img/eta_vs_gamma_monk3.jpg}}
\end{minipage}
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[new-thyroid1]{\label{1c}\includegraphics[scale=0.20]{img/eta_vs_gamma_new-thyroid1.jpg}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[pima]{\label{1d}\includegraphics[scale=0.20]{img/eta_vs_gamma_pima.jpg}}
\end{minipage}
\caption{The impact of changing the parameters \(\eta\) and \(\gamma\) on the ACC values of the proposed TRKM-C model.}
\label{Effect of parameters eta and gamma}
\end{figure*}


\begin{figure*}[ht!]
\begin{minipage}{.246\linewidth}
\centering
\subfloat[haber]{\label{2a}\includegraphics[scale=0.20]{img/eta_vs_sigma_haber.jpg}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[monk2]{\label{2b}\includegraphics[scale=0.20]{img/eta_vs_sigma_monk2.jpg}}
\end{minipage}
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[ripley]{\label{2c}\includegraphics[scale=0.20]{img/eta_vs_sigma_ripley.jpg}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.246\linewidth}
\centering
\subfloat[sonar]{\label{2d}\includegraphics[scale=0.20]{img/eta_vs_sigma_sonar.jpg}}
\end{minipage}
\caption{The impact of changing the parameters $\eta$ and $\sigma$ on the ACC values of the proposed TRKM-C model.}
\label{Effect of parameters eta and sigma}
\end{figure*}


\begin{table*}[ht!]
\centering
    \caption{$RMSE$, $MAE$, $Pos~Error$, and $Neg~Error$ values of the proposed TRKM-R and the baseline models across regression datasets.}
    \label{Average RMSE and average rank for UCI datasets}
    \resizebox{0.9\linewidth}{!}{
\begin{tabular}{llccccc}
\hline
Dataset $\downarrow$ Model $\rightarrow$ &  & SVR \cite{basak2007support} & TSVR \cite{peng2010tsvr} & TSVQR \cite{ye2024twin} & RKM \cite{suykens2017deep} & TRKM-R$^{\dagger}$ \\ 
$(\#Samples \times \#Feature)$ &   &   &   &  &  &   \\ \hline
Abalone & $RMSE$ & $0.00619696$ & $0.38767592$ & $0.07666791$ & $0.00044776$ & $0.00003633$ \\
$(4117 \times 7)$ & $MAE$ & $0.00448455$ & $0.25410483$ & $6.07666478$ & $0.00000301$ & $0.00002449$ \\
 & $Pos~Error$ & $0.00327$ & $0.26615116$ & $1.20577705$ & $0.00000309$ & $0.00001841$ \\
 & $Neg~Error$ & $0.0063161$ & $0.00007567$ & $6.07666478$ & $0.00000293$ & $0.00002719$ \\ \hline
Airfoil\_Self\_Noise & $RMSE$ & $0.28273723$ & $0.00088726$ & $0.25334222$ & $0.00033556$ & $0.00054951$ \\
$(1503 \times 5)$ & $MAE$ & $0.20323196$ & $0.28901812$ & $0.20892536$ & $0.00000518$ & $0.00053706$ \\
 & $Pos~Error$ & $0.30709839$ & $0.22327934$ & $0.32777245$ & $0.00000503$ & $0.00053843$ \\
 & $Neg~Error$ & $0.13613576$ & $0.76480796$ & $0.17763229$ & $0.00000535$ & $0.00033233$ \\  \hline
auto-original & $RMSE$ & $0.02208954$ & $0.33117149$ & $0.66636073$ & $0.00039196$ & $0.00026814$ \\
$(392 \times 7)$ & $MAE$ & $0.01923475$ & $0.0577438$ & $0.52216993$ & $0.00022567$ & $0.00018836$ \\
 & $Pos~Error$ & $0.02076571$ & $0.03669568$ & $0.58268115$ & $0.00032394$ & $0.00022018$ \\
 & $Neg~Error$ & $0.00786152$ & $0.24246004$ & $0.25812095$ & $0.00082211$ & $0.00010973$ \\   \hline
Auto-price & $RMSE$ & $0.24931086$ & $0.38767592$ & $0.80669028$ & $0.00047829$ & $0.00043308$ \\
$(159 \times 15)$ & $MAE$ & $0.19605144$ & $0.07397557$ & $0.50818611$ & $0.00062508$ & $0.00031579$ \\
 & $Pos~Error$ & $0.2362126$ & $0.12724069$ & $0.57794927$ & $0.00018158$ & $0.00016123$ \\
 & $Neg~Error$ & $0.07556795$ & $0.32188751$ & $0.32036222$ & $0.00013476$ & $0.00038605$ \\   \hline
bodyfat & $RMSE$ & $0.04248758$ & $0.1320877$ & $0.7909357$ & $0.0003355$ & $0.00074434$ \\
$(252 \times 14)$ & $MAE$ & $0.03476774$ & $0.06042572$ & $0.62823812$ & $0.00084039$ & $0.00060215$ \\
 & $Pos~Error$ & $0.03072008$ & $0.08347705$ & $0.59351638$ & $0.00079833$ & $0.00064703$ \\
 & $Neg~Error$ & $0.03712887$ & $0.06934175$ & $0.65488504$ & $0.00057252$ & $0.00036282$ \\  \hline
cpu\_pref & $RMSE$ & $0.04585881$ & $0.27085771$ & $1.40835874$ & $0.00476195$ & $0.0013733$ \\   
$(209 \times 9)$ & $MAE$ & $0.03447315$ & $0.08627122$ & $0.5221133$ & $0.00033116$ & $0.00098835$ \\
 & $Pos~Error$ & $0.02909847$ & $0.09582179$ & $0.68427341$ & $0.00044175$ & $0.00152845$ \\
 & $Neg~Error$ & $0.04264265$ & $0.03893273$ & $0.21990582$ & $0.0002631$ & $0.00086121$ \\  \hline
Daily\_Demand\_Forecasting\_Orders & $RMSE$ & $0.03665407$ & $0.09586042$ & $1.08419387$ & $0.00294797$ & $0.00195928$ \\   
$(60 \times 12)$ & $MAE$ & $0.02895825$ & $0.06542136$ & $0.67679833$ & $0.00109839$ & $0.00109826$ \\
 & $Pos~Error$ & $0.03217199$ & $0.11739298$ & $0.86489816$ & $0.00176665$ & $0.00180286$ \\
 & $Neg~Error$ & $0.01770995$ & $0.03737439$ & $0.30059867$ & $0.00043014$ & $0.00039365$ \\  \hline
gas\_furnace2 & $RMSE$ & $0.03371765$ & $0.13730804$ & $0.66500792$ & $0.00003184$ & $0.00023341$ \\
$(293 \times 6)$ & $MAE$ & $0.02870417$ & $0.6032496$ & $0.54300641$ & $0.00002163$ & $0.00014806$ \\
 & $Pos~Error$ & $0.03374757$ & $0.91916708$ & $0.63810399$ & $0.00020692$ & $0.00004956$ \\
 & $Neg~Error$ & $0.0255287$ & $0.07110268$ & $0.49128667$ & $0.00002272$ & $0.00015791$ \\   \hline
machine & $RMSE$ & $0.04578493$ & $0.25929231$ & $1.42404944$ & $0.00636192$ & $0.00264685$ \\
$(209 \times 9)$ & $MAE$ & $0.03507087$ & $0.00006935$ & $0.57988418$ & $0.00041585$ & $0.00191738$ \\
 & $Pos~Error$ & $0.03397687$ & $0.00006611$ & $0.94066641$ & $0.00056675$ & $0.00251774$ \\
 & $Neg~Error$ & $0.03684863$ & $0.02890076$ & $0.25190033$ & $0.00253411$ & $0.00143709$ \\  \hline
wpbc & $RMSE$ & $0.00139195$ & $0.86978316$ & $0.95779329$ & $0.00023423$ & $0.00019688$ \\
$(194 \times 34)$ & $MAE$ & $0.00133402$ & $0.71006265$ & $0.63237936$ & $0.00060265$ & $0.00009768$ \\
 & $Pos~Error$ & $0.00125402$ & $0.66320246$ & $1.31818329$ & $0.00075996$ & $0.00009044$ \\
 & $Neg~Error$ & $0.00219808$ & $0.45318878$ & $0.3771965$ & $0.00005219$ & $0.00010569$ \\  \hline
Average $RMSE$ &  & $0.07662296$ & $0.28725999$ & $0.81334001$ & $0.0016327$ & $\textbf{0.00084411}$ \\  \hline  
Average rank &  & $3.2$ & $4$ & $4.8$ & $1.7$ & $\textbf{1.3}$  \\  \hline
\multicolumn{7}{l}{$^{\dagger}$ represents the proposed models.}
\end{tabular}}
\end{table*}

\subsection{Experiments on Real World Regression Datasets}
In this subsection, we evaluate the performance of the proposed TRKM-R model using $10$ benchmark regression datasets from the UCI repository \cite{dua2017uci} against baseline SVR \cite{basak2007support}, TSVR \cite{peng2010tsvr}, TSVQR \cite{ye2024twin}, and RKM \cite{suykens2017deep} models. Table \ref{Average RMSE and average rank for UCI datasets} presents the detailed results of the proposed TRKM-R model and the existing models (SVR, TSVR, TSVQR, and RKM), evaluated using metrics such as $RMSE$, $MAE$, $Pos~Error$, and $Neg~Error$. $RMSE$, an important statistic whose lower values indicate better model performance, is the primary focus of the evaluation. In $7$ of the $10$ datasets, the proposed TRKM-R model achieves the lowest $RMSE$ values, indicating superior performance. For the remaining $3$ datasets, TRKM-R ranks second in terms of $RMSE$. This consistent performance across a wide range of datasets highlights the strength of the TRKM-R model. The effectiveness of the TRKM-R model is further confirmed by its average $RMSE$ values. The average $RMSE$ values for the existing SVR, TSVR, TSVQR, and RKM models are $0.07662296$, $0.28725999$, $0.81334001$, and $0.0016327$, respectively. In comparison, the proposed TRKM-R model achieves a superior average $RMSE$ of $0.00084411$, outperforming the baseline models. To evaluate model performance accurately, it is important to rank each model separately for every dataset, rather than relying solely on the average $RMSE$ values. Table \ref{Average RMSE and average rank for UCI datasets} displays the average ranking of the proposed TRKM-R model compared to the baseline models. Models are ranked based on $RMSE$, with the lowest $RMSE$ receiving the highest rank. The average ranks of TRKM-R model and the existing SVR, TSVR, TSVQR, and RKM models are $1.3$, $3.2$, $4$, $4.8$, and $1.7$, respectively. The rankings show the TRKM-R model's improved performance, decisively outperforming the baseline models.
We conducted the Friedman test \cite{demvsar2006statistical} and the Nemenyi post hoc test to further evaluate the efficacy of the proposed TRKM-R model. The significance of the performance variations between the models is statistically evaluated using the Friedman test. For $p=5$ and $N=10$, we obtained $\chi_F^2 = 35.44$ and $F_F = 69.947$. The $F_F$ statistic follows an $F$-distribution with degrees of freedom $(4, 36)$. According to the statistical $F$-distribution table, the critical value for $F_F(4,36)$ at a $5\%$ significance level is $2.6335$. Since the calculated $F_F$ value exceeds $2.6335$, the null hypothesis is rejected, indicating significant differences exist among the models. To identify significant differences in the pairwise comparisons between the models, the Nemenyi post-hoc test is used. We compute the $C.D.$ as $1.1364$, which means that for the average rankings in Table \ref{Average RMSE and average rank for UCI datasets} to be deemed statistically significant, there must be a minimum difference of $1.1364$ between them. The average rank distinctions between the TRKM-R model and the existing SVR, TSVR, TSVQR, and RKM models are $1.90$, $2.70$, $3.50$, and $0.40$, respectively. The proposed TRKM-R model is statistically better than other baseline models, except RKM, according to the Nemenyi post hoc test. The lower ranks of the proposed TRKM-R model indicate its stronger generalization capability compared to the existing RKM model. The combination of elevated average $RMSE$ and consistent performance across multiple statistical tests provides strong evidence that the proposed TRKM-R model exceeds the performance of the existing baseline models in terms of generalization.

\subsection{Evaluation on  Brain Age Prediction}
In this subsection, we perform experiments on a brain age estimation dataset. The dataset comprises structural MRI scans from a total of 976 subjects, which were sourced from two prominent repositories: the OASIS dataset\footnote{\url{https://sites.wustl.edu/oasisbrains/}} and the IXI dataset\footnote{\url{https://brain-development.org/ixi-dataset/}}. The OASIS dataset provides MRI data for aging studies, while the IXI dataset includes diverse brain imaging data from different age groups. This comprehensive dataset enables robust evaluation and validation of the proposed TRKM-R model for predicting brain age. The dataset consists of $30$ patients diagnosed with Alzheimer's disease (AD), $876$ cognitively healthy (CH) individuals, and $70$ patients with mild cognitive impairment (MCI) \cite{ganaie2022brain}. For training the brain age predictor, $90\%$ of the cognitively healthy subjects were randomly selected, totaling $788$ individuals with a mean age of $47.40 \pm 19.69$ years. For model validation, we used the remaining cognitively healthy subjects, amounting to $88$ individuals with a mean age of $48.17 \pm 17.73$ years, along with the MCI patients ($70$ subjects with a mean age of $76.21 \pm 7.18$ years) and AD patients ($30$ subjects with a mean age of $78.03 \pm 6.91$ years).
\begin{figure*}[ht!]
\begin{center}
\begin{minipage}{.32\linewidth}
\centering
\subfloat[SVR]{\includegraphics[scale=0.33]{img/Train_SVR.png}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.32\linewidth}
\centering
\subfloat[TSVR]{\includegraphics[scale=0.33]{img/Train_TSVR.png}}
\end{minipage}
% \par\medskip
\begin{minipage}{.32\linewidth}
\centering
\subfloat[TSVQR]{\includegraphics[scale=0.33]{img/Train_TSVQR.png}}
\end{minipage}
\end{center}
\par\medskip
\par\medskip
\begin{center}
\begin{minipage}{.32\linewidth}
\centering
\subfloat[RKM]{\includegraphics[scale=0.33]{img/Train_RKM.png}}
\end{minipage}
\begin{minipage}{.32\linewidth}
\centering
\subfloat[TRKM-R]{\includegraphics[scale=0.33]{img/Train_Twin_RKM.png}}
\end{minipage}
\end{center}
\caption{Real age versus estimated brain age on the training data using different prediction models, with the identity line represented by a black line.}
\label{The plot shows estimated brain age versus real age on the training data.}
\end{figure*}
The structural MRI scans were pre-processed using the CAT12 package\footnote{\url{http://dbm.neuo.uni-jena.de/}} and SPM12 software\footnote{\url{https://www.fil.ion.ucl.ac.uk/spm/}}. Initially, the MRI scans are divided into their fundamental components: white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). This paper specifically concentrates on the GM data. The GM images are registered to the Montreal Neurological Institute (MNI) template using a diffeomorphic registration algorithm, after which they are modulated to conform to the template. A Gaussian smoothing filter with a full-width half-maximum (FWHM) of $4$ millimetres is used to further process the smoothed GM pictures after they have been resampled to an isotropic spatial resolution of $8$ millimetres. As a result of this process, each subject yields approximately $3,700$ GM voxel values. To evaluate the prediction accuracy of the models, we compute several metrics: root mean square error ($RMSE$), mean absolute error ($MAE$), coefficient of determination (\( R^2 \)), brain-age delta (\( \Delta \)), and the $95\%$ confidence interval (CI). After training each model on the training set, it is then used to estimate brain age for the independent test sets.

\subsubsection{Experimental Results on the Training Set}

\begin{table*}[ht!]
\centering
    \caption{Comparison of the proposed TRKM-R model's performance against baseline models using training data of Brain Age estimation.}
    \label{Average RMSE and average rank for brain training}
    \resizebox{0.9\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
Model $\rightarrow$ & SVR \cite{basak2007support} & TSVR \cite{peng2010tsvr} & TSVQR \cite{ye2024twin} & RKM \cite{suykens2017deep} & TRKM-R$^{\dagger}$ \\ \hline
$MAE$ & $7.41$ & $6.83$ & $6.27$ & $4.79$ & $3.27$ \\
$RMSE$ & $8.79$ & $7.11$ & $6.79$ & $5.33$ & $3.92$ \\
Mean brain age delta & $0$ & $0$ & $0$ & $0$ & $0$ \\
$95\%$ CI Values & $[-0.54,  0.54]$ & $[-0.52,  0.52]$ & $[-0.54,  0.54]$ & $[-0.44,  0.44]$ & $[-0.29,  0.29]$ \\
$R^2$ Score & $0.88$ & $0.88$ & $0.88$ & $0.92$ & $0.96$ \\ \hline
\multicolumn{6}{l}{$^{\dagger}$ represents the proposed models.}
\end{tabular}}
\end{table*}

Table \ref{Average RMSE and average rank for brain training} shows the performance of the proposed TRKM-R and the baseline models on the training set. The prediction accuracy shows that the SVR model has an $MAE$ of $7.41$ years, followed by the TSVR, TSVQR, and RKM models are $6.83$, $6.27$, and $4.79$ years, and the proposed TRKM-R with $3.27$ years. $RMSE$ values show a similar pattern, with the SVR model at $8.79$ years, the TSVR, TSVQR, and RKM models at $7.11$, $6.79$, and $5.33$ years, respectively, and the proposed TRKM model achieving $3.92$ years. As illustrated in Table \ref{Average RMSE and average rank for brain training}, the proposed TRKM-R model outperforms the existing SVR, TSVR, TSVQR, and RKM models. For all prediction models in the training set, the average brain age delta is zero. The training set's predicted outcomes for each model failed to demonstrate a statistically significant age dependence (\(P > 0.05\)). Fig. \ref{The plot shows estimated brain age versus real age on the training data.} illustrates the relationship between the actual age and estimated brain age for different prediction models in the training set. As shown in Fig. \ref{The plot shows estimated brain age versus real age on the training data.}, the proposed TRKM-R significantly outperformed the baseline models. The SVR, TSVR, and TSVQR models have similar prediction \( R^2 \) scores (\( R^2 = 0.88 \)), while the RKM model has an \( R^2 \) of 0.92. In contrast, the proposed TRKM model achieved a significantly higher \( R^2 \) of 0.96 on the same data.

\subsubsection{Experimental Results on Independent Test Sets}

\begin{table*}[htp]
\centering
    \caption{Comparison of the proposed TRKM-R model's performance against baseline models using testing Data (AD, CH, and MCI subjects).}
    \label{Average RMSE and average rank for brain datasets}
    \resizebox{0.9\linewidth}{!}{
\begin{tabular}{llccccc}
\hline
Dataset $\downarrow$ Model $\rightarrow$ &  & SVR \cite{basak2007support} & TSVR \cite{peng2010tsvr} & TSVQR \cite{ye2024twin} & RKM \cite{suykens2017deep} & TRKM-R$^{\dagger}$ \\ \hline
AD & $MAE$ & $8.38$ & $8.82$ & $7.85$ & $9.67$ & $7.67$ \\
 & $RMSE$ & $12.64$ & $11.46$ & $11.96$ & $8.89$ & $8.49$ \\
 & Mean brain age delta & $7.64$ & $5.01$ & $6.96$ & $6.55$ & $5.75$ \\
 & $95\%$ CI Values & $[8.22,  13.06]$ & $[8.26,  12.77]$ & $[6.54,  13.38]$ & $[8.85,  14.25]$ & $[3.85,  9.25]$ \\
 & $R^2$ Score & $0.24$ & $0.3$ & $0.12$ & $0.21$ & $0.31$ \\ \hline
CH & $MAE$ & $14.77$ & $9.33$ & $12.45$ & $7.84$ & $5.32$ \\
 & $RMSE$ & $7.65$ & $5.94$ & $8.81$ & $6.12$ & $5.74$ \\
 & Mean brain age delta & $0.78$ & $1.03$ & $-3.9$ & $1.18$ & $1.18$ \\
 & $95\%$ CI Values & $[-1.54,  2.98]$ & $[-1.58,  1.51]$ & $[-1.15,  1.66]$ & $[-2.89,  1.35]$ & $[-2.72,  0.36]$ \\
 & $R^2$ Score & $0.87$ & $0.84$ & $0.97$ & $0.59$ & $0.84$ \\ \hline
MCI & $MAE$ & $8.82$ & $8.6$ & $7.84$ & $8.98$ & $7.31$ \\
 & $RMSE$ & $9.69$ & $11.84$ & $14.14$ & $9.03$ & $7.03$ \\
 & Mean brain age delta & $4.82$ & $4.31$ & $4.14$ & $4.97$ & $3.97$ \\
 & $95\%$ CI Values & $[-5.53,  3.11]$ & $[-5.75,  2.38]$ & $[-5.85,  2.42]$ & $[-5.13,  2.08]$ & $[-5.13,  2.08]$ \\
 & $R^2$ Score & $0.32$ & $0.22$ & $0.13$ & $0.23$ & $0.97$ \\ \hline
 \multicolumn{7}{l}{$^{\dagger}$ represents the proposed models.}
\end{tabular}}
\end{table*}

\begin{figure*}[ht!]
\begin{center}
\begin{minipage}{.30\linewidth}
\centering
\subfloat[SVR]{\includegraphics[scale=0.17]{img/Test_SVR.png}}
\end{minipage}
% \par\medskip
% \par\medskip
\begin{minipage}{.30\linewidth}
\centering
\subfloat[TSVR]{\includegraphics[scale=0.17]{img/Test_TSVR.png}}
\end{minipage}
% \par\medskip
\begin{minipage}{.30\linewidth}
\centering
\subfloat[TSVQR]{\includegraphics[scale=0.17]{img/Test_TSVQR.png}}
\end{minipage}
\end{center}
\par\medskip
\par\medskip
\begin{center}
\begin{minipage}{.30\linewidth}
\centering
\subfloat[RKM]{\includegraphics[scale=0.17]{img/Test_RKM.png}}
\end{minipage}
\begin{minipage}{.30\linewidth}
\centering
\subfloat[TRKM-R]{\includegraphics[scale=0.17]{img/Test_Twin_RKM.png}}
\end{minipage}
\end{center}
\caption{Estimated actual age versus brain age on the independent test sets features red markers and regression lines for CH subjects, blue markers, and regression lines for MCI subjects, and green markers and regression lines for AD subjects. An identity line, shown in black, serves as a reference for comparison across different prediction models.}
\label{The plot shows estimated brain age versus real age on the independent test}
\end{figure*}

Table \ref{Average RMSE and average rank for brain datasets} shows the performance of TRKM-R model against the baseline models on independent test sets. TRKM-R model attained an $MAE$ of $5.32$ years on cognitively healthy subjects, significantly outperforming the SVR, TSVR, TSVQR, and RKM models, which had $MAEs$ of $14.77$, $9.33$, $12.45$ and $7.84$ years, respectively. In the independent group of cognitively healthy individuals, the mean brain age delta was approximately zero across all prediction models. The analysis of independent test sets reveals that the predicted brain age for cognitively healthy individuals did not exhibit significant age dependency across different models (\( P > 0.05 \)), consistent with the training set results. In contrast, both AD and MCI groups showed a positive brain age delta across all models, indicating that their predicted brain age was higher than their actual age. For the MCI group, the mean brain age delta values ranged from \( \Delta = 3.97 \) years (TRKM-R) to \( \Delta = 4.97 \) years (RKM), while for the AD group, the values ranged from \( \Delta = 5.75 \) years (TRKM-R) to \( \Delta = 7.64 \) years (SVR). These results suggest that the TRKM-R model offers a lower brain age delta compared to most baseline models, reflecting better performance in brain age estimation. Fig. \ref{The plot shows estimated brain age versus real age on the independent test} illustrates the correlation between predicted and actual brain ages, where different markers and regression lines for CH, MCI, and AD subjects show how well the models estimate brain age across these conditions.

All prediction models revealed a positive brain age delta for both MCI and AD subjects, as shown in Table \ref{Average RMSE and average rank for brain datasets}. This positive delta indicates that these individuals, on average, show an accelerated brain aging process compared to age-matched healthy controls. A higher brain age delta correlates with increased brain atrophy and deterioration in cognitive functions such as memory, attention, and problem-solving skills in individuals with AD \cite{beheshti2018association}. This implies that MCI and AD patients generally experience more substantial brain atrophy than their healthy peers.

Specifically, individuals with AD exhibit a greater brain age delta compared to those with MCI, suggesting that AD is associated with more pronounced brain atrophy. The greater delta observed in AD patients relative to MCI patients points to a more rapid progression of brain aging in AD \cite{beheshti2018association}. Furthermore, as illustrated in Fig. \ref{The plot shows estimated brain age versus real age on the independent test}, younger patients with MCI or AD show a larger brain age discrepancy than older patients within the same diagnostic category. This finding aligns with other studies suggesting that early-onset AD patients typically exhibit a larger brain age gap compared to those with late-onset AD. The proposed TRKM-R model achieved an $MAE$ of $5.32$ years when applied to cognitively healthy subjects \cite{beheshti2021disappearing}.

\section{Conclusion}
In this paper, we proposed a novel twin restricted kernel machine (TRKM) model for classification and regression. TRKM model effectively tackles the challenges related to generalization in RKMs, especially when working with unevenly distributed or complexly clustered data. By integrating the strengths of twin models and leveraging the conjugate feature duality based on the Fenchel-Young inequality, TRKM offers a robust and efficient framework for both classification and regression tasks. We evaluated the proposed TRKM model using benchmark datasets from UCI and KEEL repositories and compared its performance against five state-of-the-art models for classification and regression. The results emphasize the outstanding performance of the TRKM model, which emerged as the top-performing model, achieving an average accuracy improvement of up to $0.42\%$ over the second-highest baseline model. Also, our proposed TRKM model demonstrates exceptional performance for regression tasks. Statistical analyses—encompassing ranking, the Friedman test, the Nemenyi post hoc test, and the win-tie-loss sign test—demonstrate that our proposed model significantly outperforms the baseline models in terms of robustness. Additionally, the efficacy of our proposed TRKM model in brain age prediction is validated, showing superior performance compared to the baseline models. While the proposed model has demonstrated exceptional performance on single-view datasets, its effectiveness in multiview scenarios has not yet been assessed. Future research should focus on adapting the TRKM model for multiview problems and exploring methods to reduce computational complexity while extending the model's applicability to datasets with multiple views. Another potential direction is to explore the integration of more advanced kernel functions or the development of adaptive kernel strategies that can dynamically adjust to the underlying data distribution, enhancing performance in highly heterogeneous datasets. Additionally, exploring the application of TRKM in unsupervised learning tasks, such as clustering and dimensionality reduction, could reveal new insights and broaden its utility. 
\section*{Acknowledgement}
This study receives support from the Science and Engineering Research Board (SERB) through the Mathematical Research Impact-Centric Support (MATRICS) scheme Grant No. MTR/2021/000787. The authors gratefully acknowledge the invaluable support provided by the Indian Institute of Technology Indore.
\bibliography{refs.bib}
\bibliographystyle{unsrtnat}
\end{document}