\section{Related Works}
This section presents the notations used in this paper, along with a brief overview of TSVM, TSVR, and the proposed RKM model for classification and regression.

\subsection{Notations}
Consider a classification dataset denoted by \(\mathscr{T}\), consisting of pairs \((x_i, y_i)\) where \(x_i \in \mathbb{R}^{1 \times m}\) is a feature vector with \(m\) features and \(y_i \in \{+1, -1\}\) is the class label, with \(+1\) indicating the positive class and \(-1\) indicating the negative class. The positive class feature vectors are collected into an \(n_1 \times m\) matrix \(A\), where \(n_1\) is the number of positive samples, and the negative class feature vectors are collected into an \(n_2 \times m\) matrix \(B\), where \(n_2\) is the number of negative samples. Here, \(n = n_1 + n_2\) represents the total number of samples in the dataset.

Consider a regression dataset denoted by \(\mathscr{M} \), consisting of pairs \((u_i, t_i)\) where \(u_i \in \mathbb{R}^{1 \times m}\) is a feature vector with \(m\) features and \(t_i \in \mathbb{R}\) is the corresponding continuous output value. The feature vectors \(u_i\) are organized into an \(n \times m\) matrix \(X\), where \(n\) is the total number of samples, and the output values \(t_i\) are organized into an \(n \times 1\) vector \(Y\). Here, \(n\) represents the total number of samples in the regression dataset.

\subsection{Twin Support Vector Machine (TSVM)}
In TSVM \cite{khemchandani2007twin}, two non-parallel hyperplanes are generated. Each hyperplane passes through the samples of its respective class and aims to maximize the margin between the hyperplanes and the samples of the opposing class. The optimization problem for TSVM can be formulated as follows:
\begin{align}
\label{eq:1}
      & \underset{w_1,b_1}{min} \hspace{0.2cm} \frac{1}{2} \|\mathcal{K}(A, C^T)w_1 + e_1b_1\|^2 + d_1e_2^T\xi_2 \nonumber \\
     & s.t. \hspace{0.2cm} -(\mathcal{K}(B, C^T)w_1 + e_2b_1) + \xi_2 \geq e_2, \nonumber \\
     & \hspace{0.8cm} \xi_2 \geq 0,
\end{align}
and
\begin{align}
\label{eq:2}
    & \underset{w_2,b_2}{min} \hspace{0.2cm} \frac{1}{2} \|\mathcal{K}(B, C^T)w_2 + e_2b_2\|^2 + d_2e_1^T\xi_1 \nonumber \\
     & s.t. \hspace{0.2cm} (\mathcal{K}(A, C^T)w_2 + e_1b_2) + \xi_1 \geq e_1, \nonumber \\
     & \hspace{0.8cm} \xi_1 \geq 0,
\end{align}
where \( d_1 \) and \( d_2 \) (\( > 0 \)) are penalty parameters, \( e_1 \) and \( e_2 \) are column vectors of ones with appropriate dimensions, $\mathcal{K}$ is kernel function and \( \xi_1 \) and \( \xi_2 \) are slack vectors, respectively. 

\subsection{Twin Support Vector Regression (TSVR)}
TSVR extends the concept of TSVM to regression problems. In TSVR, two separate regression functions are learned, each focusing on approximating different parts of the data distribution. By solving two QPPs simultaneously, TSVR aims to find two distinct regression functions that minimize the prediction error while maintaining a balance between the models. The optimization problem of TSVR is given as follows:
\begin{align}
\label{eq:5}
      & \underset{w_1,b_1}{min} \hspace{0.2cm} \frac{1}{2} \|Y - e\epsilon_1 - (\mathcal{K}(X, X^T)w_1 + e_1b_1)\|^2 + d_1e^T\xi_2 \nonumber \\
     & s.t. \hspace{0.2cm} Y-(\mathcal{K}(X, X^T)w_1 + eb_1) + \xi_2 \geq e \epsilon_1, \nonumber \\
     & \hspace{0.8cm} \xi_2 \geq 0,
\end{align}
and
\begin{align}
\label{eq:6}
    & \underset{w_2,b_2}{min} \hspace{0.2cm} \frac{1}{2} \|Y + e\epsilon_2 - (\mathcal{K}(X, X^T)w_2 + eb_2)\|^2 + d_2e^T\xi_1 \nonumber \\
     & s.t. \hspace{0.2cm} (\mathcal{K}(X, X^T)w_2 + e_1b_2) - Y + \xi_1 \geq e\epsilon_2, \nonumber \\
     & \hspace{0.8cm} \xi_1 \geq 0,
\end{align}
where \( d_1 \) and \( d_2 \) (\( > 0 \)) are regularization parameters, $\xi_1$ and $\xi_2$ are slack vectors, respectively. Therefore, the regression function of a nonlinear TSVR can be expressed as:
\begin{align}
\label{eq:7}
    f(x) &= \frac{1}{2}(f_1(x) + f_2(x)) \nonumber \\
   & = \frac{1}{2}((w_1+w_2)^T\mathcal{K}(x, X^T)) +\frac{1}{2}(b_1 + b_2).
\end{align}
\subsection{Restricted Kernel Machine (RKM)}
Here, we present an overview of the RKM model, as described by \citet{suykens2017deep}, which is closely related to the well-known LSSVM model \cite{suykens1999least}. RKM utilizes the kernel trick to transform the data into a high-dimensional feature space, enabling the construction of a linear separating hyperplane. The optimization problem for RKM is given as follows:
\begin{align}
\label{eq:8}
      J = & \frac{\gamma}{2} Tr(w^Tw) + \sum_{i=1}^N (1-(\phi(x_i)^Tw+b)y_i)h_i - \frac{\eta}{2} \sum_{i=1}^Nh_i^2,
\end{align}
where \(\gamma\) and \(\eta\) are the regularization parameters, \(b\) is the bias term, and \(h\) represents the hidden features. The solution to equation \eqref{eq:8} is obtained by taking the partial derivatives of \(J\) with respect to (w.r.t.) \(w\), \(b\), and \(h_i\), and then setting these derivatives to zero. For a detailed derivation, refer to \citet{suykens2017deep}. The solution to the optimization problem is given by:
\begin{align}
    \begin{bmatrix}
\mathbf{1}_N & \frac{1}{\gamma} \mathcal{K} + \eta \mathbf{I}_N \\
0  & \mathbf{1}_N^T
\end{bmatrix}
\begin{bmatrix}
b \\
h
\end{bmatrix}
= 
\begin{bmatrix}
y \\
0
\end{bmatrix},
\end{align}
where $\mathcal{K}$ is the kernel matrix, \(\mathbf{1}_N\) denotes a column vector with all its entries equal to one, $\mathbf{I}_N$ is the identity matrix and $h \in \mathbb{R}^{N \times 1}$.