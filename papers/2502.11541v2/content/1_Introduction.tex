\section{Introduction}

Large Language Models (LLMs) have made remarkable advancements and are being wildly applied across various domains \cite{zhao2024surveylargelanguagemodels,minaee2024large,hadi2024large,yin2024survey,wang2024survey}. The instruction-following ability is fundamental and important, as it enables LLMs to generate appropriate responses to given instructions and solve corresponding tasks~\cite{openai2024gpt4technicalreport}. 
% As LLMs continue to evolve,
% The instructions have become more complex with many additional constraints to deal with more difficult tasks. 
% However,
While recent LLMs perform comparatively well on simple instructions, their response quality to complex instructions with elaborate constraints often falls under expectation, with some of the constraints omitted \cite{he2024complexsimpleenhancingmulticonstraint, jiang-etal-2024-followbench}, which hinders their application in more real-world complex scenarios.

To enhance the complex instruction following,
the core challenge 
% in enhancing the complex instruction-following capabilities 
is the scarcity of high-quality complex instruction data~\cite{lou2024large}. Most existing instruction datasets are constructed based on existing NLP datasets or question-answering websites with simple constraints \cite{camel2023,alpaca,OpenOrca,longpre2023flan}. To cope with the scarcity of complex instruction data, previous work such as Evol-Instruct \cite{xu2024wizardlm}, Conifer \cite{sun2024conifer}, and Self-Correct \cite{palmeira-ferraz-etal-2024-self-correction} have been proposed to construct complex instructions and responses. However, \textit{these methods typically rely on a high-performance proprietary model (e.g.,  GPT-4) to distill the complex instruction-following ability, which is expensive and can not be scaled up in real-world applications}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/intro-fig.pdf}
    \caption{An illustrative comparison between our method and Self-Reward. Note that Self-Reward cannot create effective contrast for complex instruction-following, resulting in suboptimal optimization. }
    \label{fig: intro}
    \vspace{-1mm}
\end{figure}

Recently, the research community has paid attention to self-alignment, to break the data bottleneck without relying on a stronger model~\cite{wang2024comprehensivesurveyllmalignment, zhang2024surveyselfplaymethodsreinforcement}. Self-Reward \cite{yuan2024selfrewardinglanguagemodels} proposes to utilize the model itself to both generate responses and evaluate the results, which can be incorporated into DPO training. ISHEEP \cite{liang2024isheepselfalignmentllmscratch} proposes an automatic loop to self-assess and self-filter instruction data. \textit{Despite their effectiveness, these methods are targeted at general instruction-following ability. The self-alignment of complex instruction-following ability remains unexplored.}

In this paper, to address the above limitations, we propose a novel \textbf{Mu}lti-granularity \textbf{S}elf-\textbf{C}ontrastive Training framework (\textbf{MuSC}) in Figure~\ref{fig: intro}, which mainly comprises the following components:

1) \textbf{Coarse-grained Contrast: Constraint-aware Preference Data Construction}.  
To improve the model's comprehension of constraint-level distinctions, we construct preference pairs that reflect the disparities in constraint fulfillment. We achieve this by breaking down each complex instruction into atomic constraints and selectively omitting a subset to form negative instructions. The chosen response, derived from the original instruction, is paired with the rejected response, generated from the negative instruction, as a contrastive pair. Notably, no external models are utilized in this construction process.

%We first break down each original complex instruction \texttt{Ins} into individual constraints and randomly drop some of the constraints.
%Then, we regenerate a new complex instruction as a negative sample based on the remaining constraints \texttt{Ins$_{drop}$} to form a contrastive instruction pair (\texttt{Ins}, \texttt{Ins$_{drop}$}). 
%Finally, we build the corresponding responses (\texttt{Resp}, \texttt{Resp$_{drop}$}) for these pairs.  Notably, no other model is used in the above construction process. 


2) \textbf{Fine-grained Contrast: Token-aware Preference Optimization}. For complex instructions, the responses often involve multiple tokens that contribute differently to fulfilling the instruction's constraints. Therefore, we introduce a token-aware optimization framework that integrates dynamic token-level weights based on the model’s confidence. By focusing on tokens that deviate from the constraints, this approach effectively identifies and corrects tokens where the model fails to satisfy the instruction’s requirements, leading to more contextually appropriate responses.
% 2) \textbf{Output-granularity Contrast: Token-aware Preference Optimization}. For complex instructions with lengthy responses, different tokens should not be treated monotonically during preference optimization. Thus, we introduce fine-grained token-level preference optimization by assigning dynamic weights to different tokens. As it is non-trivial to obtain token-level rewards, we adopt the model's confidence scores as the weights, which are then calibrated between contrastive samples.
% to locate the tokens in need of optimization. 

Moreover, we need to mention that our MuSC can be applied on both pre-existing complex instruction datasets, or newly generated instruction datasets created by data synthesis methods (e.g.,  Self-Instruct \cite{selfinstruct}). 

Our contribution can be summarized as follows:

\vspace{-2mm}

\begin{itemize}[itemsep=1mm, parsep=0pt]
    % \item 
% \end{itemize}
    \item We propose a novel Multi-granularity Self-Contrastive Training (MuSC) framework, which creates effective contrast on both coarse and fine granularity, to enhance the complex instruction following abilities.
    % We propose to construct preference data for complex instruction following, by detaching and recombining complex instructions;
    \item For coarse-grained contrast, we construct constraint-aware preference data with instruction decomposition-recombination. For fine-grained contrast, we adopt dynamic token-level weight with confidence guidance for better preference optimization.
    \item We evaluate our framework on open-source LLMs, and achieve significant improvements on both complex and general instruction following benchmarks, without the help of a larger model or human supervision.
    % Our method achieves significant improvement on both complex and simple instruction benchmarks, without relying on a larger model;
\end{itemize}

\vspace{-1mm}

