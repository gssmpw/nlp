\section{Experiments}
\label{sec:experiments}
\subsection{Set-up}
\paragraph{Models.}We  conduct experiments on two models: LLaMA-3-8B-Instruct \cite{dubey2024llama} and Qwen2-7B-Instruct \cite{yang2024qwen2}. Both models have undergone alignment to possess fundamental instruction-following ability.

% Our experiment is conducted under two scenarios: 1) \textbf{PreInst}, where we use the pre-existing instructions from \citet{xu2024wizardlm} as start point; 2) \textbf{SelfInst}, where we synthesize initial instructions based on self-Instruct \cite{selfinstruct}. 


\paragraph{Setting.} 
The experiments are carried out in two distinct settings:

\vspace{-2mm}

\begin{enumerate}[itemsep=1mm, parsep=0pt]
\item \textbf{Pre-Existing Instructions (PreInst):} We leverage pre-existing complex instructions as a starting point for the model. We randomly select 2,000 instructions from the dataset of WizardLM \cite{xu2024wizardlm}.
\item \textbf{Self-Generated Instructions (SelfInst):} In this setting, we generate instructions using the Self-Instruct method \cite{selfinstruct}, based on 10 high-quality samples from \citet{qin2024infobench} as in-context examples. Compared with \textbf{PreInst}, this setting is more challenging as we need to construct the complex instructions from scratch.
\end{enumerate}

\vspace{-2mm}

% Our experiment is conducted under two settings: 1) \textbf{PreInst}, where there pre-exist complex instructions and we perform decomposition directly. In that case, we randomly select 2K instructions from \citet{xu2024wizardlm} as the start point. 2) \textbf{SelfInst}, where there is no pre-existing complex instructions, and we synthesize instructions from scratch based on Self-Instruct \cite{selfinstruct}. We curated 10 high-quality demonstrations from the dataset of \citet{qin2024infobench} as the in-context demonstrations for Self-Instruct.
% We curated 10 high-quality demonstrations from the dataset of \citet{qin2024infobench}, which includes intricate human-crafted instructions and constraints. 


\begin{table*}[t]
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{c|ccccccccc}
% \hline
\toprule
\multirow{2}{*}{\textbf{Setting}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c}{\textbf{FollowBench}} & \multicolumn{1}{c|}{\textbf{ComplexBench}} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
                                   & \multicolumn{1}{c|}{}                                 & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{HSR}        & \textbf{SSR}       & \multicolumn{1}{c|}{\textbf{Overall}}      & \textbf{LC (\%)}   & \textbf{Avg. Len}   \\
                                   \midrule
                                   % \hline
\multicolumn{10}{l}{\textit{Results on LLaMA-3-8B-Instruct}}                                                                                                                                                                                                             \\ 
% \hline
\midrule
\multicolumn{2}{c|}{LLaMA-3-8B-Instruct}                                              & 0.64          & 0.24          & 0.34          & 62.39               & 73.07              & \multicolumn{1}{c|}{61.49}                 & 21.07              & 1702                \\
% \hline
\midrule
\multirow{6}{*}{SelfInst}          & \multicolumn{1}{c|}{Self-Reward}                      & 0.65          & 0.26          & 0.35          & 61.20               & 72.22              & \multicolumn{1}{c|}{62.45}                 & 19.21              & 1824                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ BSM}               & 0.68          & 0.28          & 0.39          & 64.30               & 73.84              & \multicolumn{1}{c|}{64.13}                 & 19.03              & 1787                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ GPT-4}             & 0.66          & 0.25          & 0.37          & 62.18               & 73.34              & \multicolumn{1}{c|}{64.05}                 & 19.55              & 1767                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{Self-Correct}                  & 0.52          & 0.20          & 0.27          & 54.38               & 67.19              & \multicolumn{1}{c|}{55.91}                 & 7.97               & 1919                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{ISHEEP}                           & 0.60          & 0.29          & 0.40          & 62.77               & 72.86              & \multicolumn{1}{c|}{62.67}                 & 22.00              & 1707                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{\textbf{MuSC}}                    & \textbf{0.70} & \textbf{0.32} & \textbf{0.44} & \textbf{66.71}      & \textbf{74.84}     & \multicolumn{1}{c|}{\textbf{65.98}}        & \textbf{23.87}     & 1708                \\
                                   \midrule
                                   % \hline
\multirow{7}{*}{PreInst}           & \multicolumn{1}{c|}{Self-Reward}                      & 0.66          & 0.27          & 0.37          & 60.88               & 72.17              & \multicolumn{1}{c|}{62.03}                 & 19.93              & 1789                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ BSM}               & 0.68          & 0.29          & 0.40          & 63.96               & 73.78              & \multicolumn{1}{c|}{64.3}                  & 20.98              & 1829                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ GPT-4}             & 0.66          & 0.26          & 0.37          & 64.02               & 73.26              & \multicolumn{1}{c|}{63.52}                 & 18.02              & 1804                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{Self-Correct}                  & 0.60          & 0.23          & 0.32          & 60.11               & 70.94              & \multicolumn{1}{c|}{60.79}                 & 6.20               & 1593                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{ISHEEP}                           & 0.67          & 0.29          & 0.40          & 63.54               & 73.21              & \multicolumn{1}{c|}{62.92}                 & 20.23              & 1703                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{SFT}                              & 0.56          & 0.20          & 0.26          & 50.06               & 66.48              & \multicolumn{1}{c|}{53.93}                 & 10.00              & 1079                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{\textbf{MuSC}}                    & \textbf{0.69} & \textbf{0.30} & \textbf{0.42} & \textbf{66.90}      & \textbf{75.11}     & \multicolumn{1}{c|}{\textbf{64.73}}        & \textbf{23.74}     & 1631                \\ 
                                   \midrule
                                   % \hline
\multicolumn{10}{l}{\textit{Results on Qwen2-7B-Instruct}}                                                                                                                                                                                                                    \\
\midrule
% \hline
\multicolumn{2}{c|}{Qwen2-7B-Instruct}                                                     & 0.74          & 0.36          & 0.49          & 59.81               & 71.69              & \multicolumn{1}{c|}{67.24}                 & 15.53              & 1688                \\ \midrule
\multirow{5}{*}{SelfInst}          & \multicolumn{1}{c|}{Self-Reward}                      & 0.75          & 0.38          & 0.50          & 55.36               & 69.71              & \multicolumn{1}{c|}{66.98}                 & 16.81              & 1756                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ BSM}               & 0.75          & 0.38          & 0.50          & 57.83               & 70.53              & \multicolumn{1}{c|}{67.02}                 & 16.94              & 1710                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{Self-Correct}                  & 0.67          & 0.28          & 0.38          & 51.98               & 67.89              & \multicolumn{1}{c|}{64.41}                 & 14.01              & 1497                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{ISHEEP}                           & 0.76          & 0.40          & 0.52          & 57.01               & 69.88              & \multicolumn{1}{c|}{67.32}                 & 16.99              & 1619                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{\textbf{MuSC}}                    & \textbf{0.78} & \textbf{0.42} & \textbf{0.54} & \textbf{62.60}      & \textbf{72.57}     & \multicolumn{1}{c|}{\textbf{69.39}}        & \textbf{20.08}     & 1595                \\ 
                                   \midrule
                                   % \hline
\multirow{6}{*}{PreInst}           & \multicolumn{1}{c|}{Self-Reward}                      & 0.75          & 0.37          & 0.49          & 56.45               & 70.00              & \multicolumn{1}{c|}{66.45}                 & 15.98              & 1796                \\
                                   & \multicolumn{1}{c|}{Self-Reward w/ BSM}               & 0.75          & 0.37          & 0.49          & 58.02               & 70.62              & \multicolumn{1}{c|}{67.43}                 & 17.17              & 1764                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{Self-Correct}                  & 0.66          & 0.28          & 0.37          & 49.47               & 66.35              & \multicolumn{1}{c|}{64.32}                 & 14.46              & 1737                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{ISHEEP}                           & 0.77          & 0.41          & 0.52          & 55.52               & 69.62              & \multicolumn{1}{c|}{67.13}                 & 16.52              & 1627                \\ 
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{SFT}                              & 0.72          & 0.35          & 0.46          & 47.36               & 64.67              & \multicolumn{1}{c|}{65.89}                 & 9.52               & 979                 \\
                                   % \cline{2-10} 
                                   & \multicolumn{1}{c|}{\textbf{MuSC}}                    & \textbf{0.79} & \textbf{0.44} & \textbf{0.55} & \textbf{62.73}      & \textbf{73.09}     & \multicolumn{1}{c|}{\textbf{70.00}}        & \textbf{20.29}     & 1613                \\
                                   \bottomrule
                                   % \hline
\end{tabular}}
\caption{Experiment results of different groups of methods on instruction following benchmarks. For more detailed results on each benchmark, please refer to Appendix \ref{sec:app-results}.}
\label{tab:main}
\end{table*}

\paragraph{Evaluation.} 
We mainly perform evaluations on three complex instruction-following benchmarks: \textbf{CFBench}~\cite{zhang2024cfbenchcomprehensiveconstraintsfollowingbenchmark}, \textbf{FollowBench}~\cite{jiang-etal-2024-followbench} and \textbf{ComplexBench} \cite{wen2024benchmarkingcomplexinstructionfollowingmultiple}. We also conduct evaluations on one general instruction benchmark: \textbf{AlpacaEval2}~\cite{dubois2024length}. Note that all benchmarks require GPT-4 for judgment, and we use GPT-4o-0513\footnote{\url{platform.openai.com/docs/models/gp##gpt-4o}} as the evaluator for all of them.
% Instructions in these benchmarks are typically harder and often contains constraints, and are evaluated in the way of checklist.
% \begin{itemize}[itemsep=1mm, parsep=0pt]
%     \item \textbf{CFBench} \cite{zhang2024cfbenchcomprehensiveconstraintsfollowingbenchmark} is a comprehensive constraints following benchmark, which integrates multi-dimensional assessment criteria with requirement prioritization.
%      \item \textbf{FollowBench} \cite{jiang-etal-2024-followbench} is a fine-grained constraints following benchmark with multi-level difficulties.
%     \item \textbf{ComplexBench} \cite{he2024complexsimpleenhancingmulticonstraint} is a benchmark that breaks down a complex constraint into simpler criteria across categories.
% \end{itemize}


% \begin{itemize}
%     \item \textbf{AlpacaEval2} \cite{dubois2024length} is a LLM-based general-purposed alignment evaluation benchmark. The score is the length-adjusted win-rate against the reference model.
% \end{itemize}



\paragraph{Baselines.} We mainly compared our method against the following self-alignment methods:

\vspace{-2mm}
\begin{itemize}[itemsep=1mm, parsep=0pt]
  \item \textbf{Self-Reward} \cite{yuan2024selfrewardinglanguagemodels}: This method leverages the model to first generate multiple responses and then construct rewards.
  \item \textbf{Self-Reward + BSM}: Based on Self-Reward, this method performs fine-grained evaluation based on BSM \cite{saha2024branchsolvemergeimproveslargelanguage}.
  \item \textbf{Self-Correct} \cite{palmeira-ferraz-etal-2024-self-correction}: This method generates initial output and then corrects it to construct preference data.
  \item \textbf{ISHEEP} \cite{liang2024isheepselfalignmentllmscratch}: This method self-creates additional instruction-output pair, which are filtered for supervised fine-tuning.
\end{itemize}
\vspace{-2mm}

\subsection{Main Results}


% % Please add the following required packages to your document preamble:
% \begin{table*}[t]
% \resizebox{1.0\textwidth}{!}{
% \begin{tabular}{ccc|ccc|cccc}
% \hline
% \multirow{2}{*}{\textbf{Model}}                                                  & \multirow{2}{*}{\textbf{Scenario}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{CF-Bench}}                              & \multicolumn{4}{c}{\textbf{ComplexBench}}                                                 \\
%                                                                                  &                                    &                                  & \textbf{CSR}         & \textbf{ISR}         & \textbf{PSR}          & \textbf{And}         & \textbf{Chain}       & \textbf{Selection}   & \textbf{Overall}     \\ \hline
% \multirow{18}{*}{\begin{tabular}[c]{@{}c@{}}LLaMA-3-\\ 8B-Instruct\end{tabular}} & \multicolumn{2}{c|}{Baseline}                                         & 0.64                 & 0.24                 & 0.34                  & 0.7253               & 0.5722               & 0.5355               & 0.6149               \\ \cline{2-10} 
%                                                                                  & \multirow{8}{*}{SelfInstruct}      & Self-Reward                      & 0.65                 & 0.26                 & 0.35                  & 0.7392               & 0.5823               & 0.5407               & 0.6245               \\
%                                                                                  &                                    & w/ BSM                           & 0.68                 & 0.28                 & 0.39                  & 0.7608               & 0.5801               & \textbf{0.5662}         & 0.6413               \\
%                                                                                  &                                    & w/ GPT-4                         &                      &                      &                       &                      &                      &                      &                      \\ \cline{3-10} 
%                                                                                  &                                    & Simple2Complex                   & 0.52                 & 0.20                 & 0.27                  & 0.6934               & 0.4985               & 0.4691               & 0.5591               \\
%                                                                                  &                                    & w/ GPT4                          & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \cline{3-10} 
%                                                                                  &                                    & ISHEEP                           & 0.60                 & 0.29                 & 0.40                  & 0.7427               & 0.5779               & 0.5463               & 0.6267               \\ \cline{3-10} 
%                                                                                  &                                    & SelfContrast                     & 0.70                 & 0.32                 & 0.43                  & 0.7665               & 0.5887               & 0.5494               & 0.6394               \\
%                                                                                  &                                    & w/Confidence                     & \textbf{0.71}        & \textbf{0.34}        & \textbf{0.45}         & \textbf{0.7788}      & \textbf{0.6345}      & \textbf{0.5596}      & \textbf{0.6598}      \\ \cline{2-10} 
%                                                                                  & \multirow{9}{*}{EvolInstruct}      & Self-Reward                      & 0.66                 & 0.27                 & 0.37                  & 0.7469               & 0.5694               & 0.5309               & 0.6203               \\
%                                                                                  &                                    & w/ BSM                           & 0.68                 & 0.29                 & 0.40                  & 0.7700               & 0.5758               & \textbf{0.5647}      & 0.6430               \\
%                                                                                  &                                    & w/ GPT-4                         & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} &                      \\ \cline{3-10} 
%                                                                                  &                                    & Simple2Complex                   & 0.60                 & 0.23                 & 0.32                  & 0.7330               & 0.5565               & 0.5202               & 0.6079               \\
%                                                                                  &                                    & w/ GPT4                          & 0.32                 & 0.11                 & 0.13                  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \cline{3-10} 
%                                                                                  &                                    & ISHEEP                           & 0.67                 & 0.29                 & 0.40                  & 0.7577               & 0.5637               & 0.5483               & 0.6292               \\ \cline{3-10} 
%                                                                                  &                                    & SFT                              & 0.56                 & 0.2                  & 0.26                  & 0.6970               & 0.4577               & 0.4409               & 0.5393               \\ \cline{3-10} 
%                                                                                  &                                    & SelfContrast                     & 0.70                 & 0.30                 & 0.41                  & 0.7752               & \textbf{0.6001}               & 0.5499               & 0.6458               \\
%                                                                                  &                                    & w/Confidence                     & \textbf{0.71}        & \textbf{0.34}        & \textbf{0.44}         & \textbf{0.7757}      & 0.5923          & \textbf{0.5591}      & \textbf{0.6473}      \\ \hline
% \multirow{14}{*}{\begin{tabular}[c]{@{}c@{}}Qwen2-\\ 7B-Instruct\end{tabular}}   & \multicolumn{2}{c|}{Baseline}                                         & 0.74                 & 0.36                 & 0.49                  & 0.7890                 & 0.6258            & 0.5897               & 0.6724               \\ \cline{2-10} 
%                                                                                  & \multirow{6}{*}{SelfInstruct}      & Self-Reward                      & 0.75                 & 0.38                 & 0.50                  & 0.7911               & 0.6302               & 0.5775               & 0.6698               \\
%                                                                                  &                                    & w/ BSM                           & 0.75                 & 0.38                 & 0.50                  & 0.7958               & 0.6237               & 0.5785               & 0.6702               \\ \cline{3-10} 
%                                                                                  &                                    & Simple2Comlex                    & 0.67                 & 0.28                 & 0.38                  & 0.7731               & 0.5959               & 0.5504               & 0.6441               \\ \cline{3-10} 
%                                                                                  &                                    & ISHEEP                           & 0.76                 & 0.40                 & 0.52                  & 0.7891               & 0.6195               & 0.5964               & 0.6732               \\ \cline{3-10} 
%                                                                                  &                                    & SelfContrast                     & 0.77                 & 0.41                 & 0.53                  & 0.8153               & 0.6445               & \textbf{0.6169}      & \textbf{0.6970}      \\
%                                                                                  &                                    & w/Confidence                     & \textbf{0.78}        & \textbf{0.42}        & \textbf{0.54}         & \textbf{0.8189}      & \textbf{0.6545}      & 0.5979      & 0.6939      \\ \cline{2-10} 
%                                                                                  & \multirow{7}{*}{EvolInstruct}      & Self-Reward                      & 0.75                 & 0.37                 & 0.49                  & 0.7896               & 0.6137               & 0.5764               & 0.6645               \\
%                                                                                  &                                    & w/ BSM                           & 0.75                 & 0.37                 & 0.49                  & 0.7973               & 0.6295               & 0.5841               & 0.6743               \\ \cline{3-10} 
%                                                                                  &                                    & Simple2Comlex                    & 0.66                 & 0.28                 & 0.37                  & 0.7706               & 0.6016               & 0.5463               & 0.6432               \\ \cline{3-10} 
%                                                                                  &                                    & ISHEEP                           & 0.77                 & 0.41                 & 0.52                  & 0.7870               & 0.6445               & 0.5754               & 0.6713               \\ \cline{3-10} 
%                                                                                  &                                    & SFT                              & 0.72                 & 0.35                 & 0.46                  & 0.7854               & 0.6016               & 0.5739               & 0.6589               \\ \cline{3-10} 
%                                                                                  &                                    & SelfContrast                     & 0.79                 & 0.44                 & 0.56                  & 0.8035               & 0.6602               & 0.5964               & 0.6892               \\
%                                                                                  &                                    & w/Confidence                     & \textbf{0.80}        & \textbf{0.46}        & \textbf{0.58}         & \textbf{0.8092}      & \textbf{0.6688}      & \textbf{0.6138}      & \textbf{0.7000}      \\ \hline
% \end{tabular}}
% \caption{Experiment results of different groups of methods on complex instruction following benchmarks.}
% \label{tab:main}
% \end{table*}

\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{cc|cccccc|cc}
\toprule
\multirow{2}{*}{\textbf{Setting}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c}{\textbf{FollowBench}} & \textbf{ComplexBench} & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
                                   &                                  & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{HSR}        & \textbf{SSR}       & \textbf{Overall}      & \textbf{LC (\%)}   & \textbf{Avg. Len}   \\ \midrule
\multicolumn{2}{c|}{LLaMA-3-8B-Ultrachat-200K}                         & 0.54          & 0.18          & 0.25          & 33.64               & 51.37              & 44.89                 & 5.94               & 861                 \\ \midrule
\multirow{4}{*}{PreInst}           & Self-Reward w/BSM                     & 0.58          & 0.2           & 0.28          & 39.24               & 56.82              & 53.69                 & 9.64               & 1099                \\
                                   & Self-Correct                     & 0.32          & 0.08          & 0.01          & 18.59               & 36.88              & 36.38                 & 3.47               & 575                 \\
                                   & ISHEEP                           & 0.57          & 0.19          & 0.26          & 42.22               & 58.87              & 52.95                 & 10.35              & 1283                \\ %\cline{2-10} 
                                   & \textbf{MuSC}                    & \textbf{0.66} & \textbf{0.25} & \textbf{0.34} & \textbf{48.93}      & \textbf{61.54}     & \textbf{56.24}        & \textbf{11.13}     & 1112                \\ \midrule
\multicolumn{2}{c|}{LLaMA-3-8B-Tulu-330K}                              & 0.56          & 0.20          & 0.27          & 36.26               & 54.66              & 54.52                 & 6.00               & 992                 \\ \midrule
\multirow{4}{*}{PreInst}           & Self-Reward w/BSM                   & 0.60          & 0.22          & 0.30          & 41.71               & 58.68              & 55.84                 & 9.71               & 1307                \\
                                   & Self-Correct                     & 0.31          & 0.08          & 0.10          & 24.67               & 39.94              & 40.44                 & 3.21               & 623                 \\
                                   & ISHEEP                           & 0.61          & 0.22          & 0.30          & 43.16               & 59.21              & 58.20                 & 10.77              & 1402                \\ %\cline{2-10} 
                                   & \textbf{MuSC}                    & \textbf{0.70} & \textbf{0.28} & \textbf{0.40} & \textbf{51.26}      & \textbf{63.94}     & \textbf{62.98}        & \textbf{13.20}     & 1341                \\ \bottomrule
\end{tabular}}
\caption{Experiment results of different methods on supervised fine-tuned models.}
\label{tab:sft}
\end{table*}

As demonstrated in Table \ref{tab:main}, our proposed MuSC achieves significant improvement across both complex and general instruction-following benchmarks. The improvement is consistent among different settings, verifying its scalability. By creating preference data with both constraint-aware and token-aware contrast, the model effectively learns to address all constraints lying in the instructions. 

% On the other hand, as the responses lies in the decoding space while maintain a reasonable edit distance between each other. This verifies the three aspects we proposed for creating contrast samples.

The results of Self-Reward underperform our method, even with the help of branched evaluation \cite{saha2024branchsolvemergeimproveslargelanguage}. This is because of the limited evaluation capability of the model, especially when evaluating its own response to complex instructions. Moreover, as different responses generated from the same model to the same instruction typically do not vary significantly, it is difficult to create effective contrast samples with real negativity.

The improvement of I-SHEEP also underperforms, likely due to its reliance on supervised fine-tuning for optimization. Previous research also suggests that learning from negative samples is more effective than learning solely from positive ones \cite{yang2024doesnegativesamplingmatter}. The results of Self-Correct degrades a large margin, which might be due to the inability of the model for self-correction on complex instructions \cite{palmeira-ferraz-etal-2024-self-correction}.

% However, when comparing the result of Self-Reward and I-SHEEP, we can tell that I-SHEEP performs slightly better. This might be due to the rejected samples created from Self-Reward is noised, leading to degradation.

On general instruction benchmarks, our method also achieves significant improvement. This aligns with the previous research, which suggests that the improvement on complex instruction-following is beneficial for the overall instruction-following ability \cite{xu2024wizardlm, elmadany-etal-2023-orca}.

% The improvement of Confidence weight is comparatively marginal. However, we would like to argue that the introduction of confidence does not require an external annotator, neither does it introduce further training expense, only deriving the weight as a by-product of model decoding. Therefore, incorporating the confidence score as weight is a highly efficient method.