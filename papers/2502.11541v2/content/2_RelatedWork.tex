\section{Related Work}
\noindent\textbf{Complex Instruction-Following}.
As one of the cores of LLM intelligence, how to improve the model's instruction-following capability is important. The earliest works, such as Alpaca \cite{alpaca}, Vicuna \cite{vicuna2023}, and Camel \cite{camel2023}, used instruction data generated by proprietary models to supervise fine-tuning of open-source models, 
significantly enhancing their instruction-following capabilities. However, these methods mainly focus on general instruction following, while complex instruction following still remains challenging.
% Complex instructions refer to instructions that include multiple and interrelated constraints, which require the model to produce lengthy and multi-facet response. Existing instruction data generally includes few complex instructions, leading to incapability of model's complex instruction following ability \cite{jiang-etal-2024-followbench}. 
To cope with this challenge, a lot of methods~\cite{yin2023dynosaur,lou2023muffin,he2024complexsimpleenhancingmulticonstraint,sun2024parrot,chen2024dog,dong2024self} have been proposed to construct complex instruction data. 
The earliest work is Evol-Instruct \cite{xu2024wizardlm}, which proposed to utilize GPT-4 to expand the instructions from both depth and width, thereby generating complex instructions and corresponding constraints. 
% Complex2Simple \cite{he2024complexsimpleenhancingmulticonstraint} suggests that corrections made by powerful models on outputs from less capable models yield better results than direct generation by the powerful models alone, and the derived contrast data is utilized for preference optimization.
Conifer \cite{sun2024conifer} proposed a progressive learning strategy designed to help smaller models incrementally enhance their abilities.
% However, these methods all rely on GPT-4, essentially distilling the instruction-following capabilities of a stronger model into a smaller one, limiting their applicability. How to improve the complex instruction-following capabilities without relying on a stronger model remains a problem.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/main.pdf}
    \caption{The pipeline of our proposed MuSC. The process starts with constraint-aware preference data construction, which includes instruction decomposition, constraint dropout, instruction recombination and response generation. Next, the token-aware DPO is performed based on calibrated confidence to achieve token-level alignment.}
    \label{fig: intro-2}
    \vspace{-1mm}
\end{figure*}

\noindent\textbf{Self-Alignment}.
Self-alignment refers to aligning the model to human preference without relying on a more advanced model or external supervision.
% This is crucial for the continuous evolvement of LLMs, especially in the context of exhaustion of high-quality data.
As an early study, Self-Rewarding \cite{yuan2024selfrewardinglanguagemodels} proposed the model itself to both generate responses and evaluate the results. 
% They first utilize evaluation fine-tuning to enhance the model's evaluation capabilities. Based on that, preference data was obtained through self-evaluation for DPO training,and the model's instruction-following capability is optimized in an iterative manner.
Following this work, many works~\cite{liu-etal-2024-direct,chen2024dog,chen2024bootstrapping,pang2024self,meng2024simpo} are conducted to obtain supervision data by the model itself. Meta-Rewarding \cite{wu2024metarewardinglanguagemodelsselfimproving} advanced the concept by improving the model's instruction and evaluation capabilities simultaneously. 
% They also include length information in evaluation to avoid length bias. 
\citet{liu-etal-2024-direct} employed diverse prompts to guide the model to generate various responses.
% and then obtain preference pairs by comparing the probability scores under harmful and harmless prompts. 
% I-SHEEP \cite{liang2024isheepselfalignmentllmscratch} proposed using model answers as seed data to obtain multiple replies, then selecting high-scoring replies to add to instruction data for iterative training.
Despite the progress these methods have made, they all target general instruction following. For complex instructions with multiple constraints, the response will be lengthy and multi-facet, resulting in challenges for the self-evaluation process. 

% Thus, how to perform self-alignment on complex instruction-following remains unexplored. 