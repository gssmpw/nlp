\section{Approach}
The pipeline of MuSC is shown in Figure~\ref{fig: intro-2}.
\subsection{Constraint-aware Preference Data}

Reinforcement-learning methods, such as PPO \cite{schulman2017proximalpolicyoptimizationalgorithms} and DPO \cite{rafailov2024directpreferenceoptimizationlanguage}, have achieved notable success in LLM optimization. Research has shown that learning from negative samples is significantly more efficient than learning solely from positive samples \cite{yang2024doesnegativesamplingmatter}. However, these methods are limited by the need for high-quality preference data, which is particularly scarce for complex instructions.

% as there is few complex instruction data with preference annotations available.

To construct effective preference data for complex instruction following, we propose a novel data construction method, with the following steps\footnote{Please refer to Appendix \ref{sec:impl} for implementation details.}:

\vspace{-2mm}

\begin{enumerate}[itemsep=1mm, parsep=0pt]
    \item Instruction Decomposition: A complex instruction is typically a combination of multiple atomic constraints. We decompose the complex instruction into individual atomic constraints, denoted as \texttt{Cons}.
    \item Constraint Dropout: From the decomposed constraints \texttt{Cons}, we randomly eliminate $\alpha\%$ of the constraints to form \texttt{Cons$_{drop}$}.
    \item Instruction Recombination: We recombine both the original and the dropped constraints \texttt{Cons} and \texttt{Cons$_{drop}$}, to create chosen and rejected instructions: \texttt{Ins} and \texttt{Ins$_{drop}$}.
    \item Response Generation: Based on \texttt{Ins} and \texttt{Ins$_{drop}$}, we generate the chosen response \texttt{Resp} and the rejected response \texttt{Resp$_{drop}$}.
\end{enumerate}

\vspace{-1mm}

Previous research has suggested that the construction of effective preference pairs for optimization is non-trivial \cite{ivison2024unpackingdpoppodisentangling}. Our data construction pipeline is guided by three principles:

\vspace{-1mm}

\begin{itemize}[itemsep=1mm, parsep=0pt]
    \item \textbf{Negativity}: The rejected response should deviate from the instruction by omitting some constraints. Our method generates the rejected instruction based on corrupted constraints, ensuring that the rejected response deviates from the original complex instruction.
    \item \textbf{Consistency}: The rejected response should reside within the model's decoding space \cite{guo2024directlanguagemodelalignment}. In our method, the rejected instruction is simply a recombination of the original instructions, ensuring the response falls within the decoding space, which is crucial for the optimization process.
    \item \textbf{Contrastiveness}: Chosen and rejected responses should be with a rational edit distance, to form an effective contrast \cite{jiang2024bridgingmodelingcorrelationspairwise}. By reconstructing both chosen and rejected instructions using the same method, we ensure that the derived samples do not deviate too far from each other.
\end{itemize}

\vspace{-2mm}

With constructed data satisfying both \textbf{negativity}, \textbf{consistency} and \textbf{contrastiveness}, we form a solid foundation for effective alignment. Moreover, our method does not require a stronger model or human supervision, ensuring its scalability.

Our self-construction method can be applied in different scenarios. On one hand, it can be directly applied on pre-existing complex instruction dataset. On the other hand, if there is no existing complex queries, we can adapt the Self-Instruct \cite{selfinstruct} method by first generating constraints and then generating instructions. In that case, the decomposition step can be omitted.

\subsection{Token-aware Preference Optimization}

A well-known issue with DPO is its uniform treatment of all tokens in both chosen and rejected examples \cite{wu2023finegrained, cao2024sparserewardsenhancingreinforcement, li20242ddposcalingdirectpreference}. However, different tokens within responses carry varying significance. Especially in scenarios involving complex instructions, the responses tend to be lengthy and multi-facet. On one hand, not all tokens in the rejected response are erroneous and should be disapproved. On the other hand, chosen response may also contain tokens that fail to meet specific constraints, therefore should not be unanimously approved.

Despite previous researchers have explored fine-grained supervision signals, the signals either come from a stronger model \cite{cao2024sparserewardsenhancingreinforcement, li20242ddposcalingdirectpreference} or human annotation \cite{wu2023finegrained, lightman2023let}. However, in our case, it is difficult for the model to provide accurate supervision for its own response, especially when dealing with multifaceted instructions and the evaluation is at token-level. Therefore, we propose Confidence-Guided Token-aware DPO, which obtains token-level supervision based on model confidence.

\subsubsection{Preliminary: Token-level DPO}
\label{preliminary}
Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} proposes a direct optimization objective that satisfies the optimal preference policy without using a reward model:

% , as one of the most popular alignment methods, 

% \begin{small}
% \begin{align*}
% & \mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = \\
% & -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{ref}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{ref}(y_l \mid x)} \right) \right]
% \end{align*}
% \end{small}
% \begin{small}

\vspace{-8mm}

\begin{align}
& \mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = \nonumber\\
& -{E}_{\left(x, y^w, y^l \right) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{ref}(y_w \mid x)} \right. \right. \nonumber\\
& \left. \left. - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{ref}(y_l \mid x)} \right) \right], 
\label{eq:dpo}
\end{align}

\vspace{-3mm}

\noindent where $\pi_\theta$ and $\pi_{ref}$ represent the policy model and the reference model, respectively. 

Subsequently, based on the theories of \citet{Levine2018ReinforcementLA}, \citet{rafailov2024rqlanguagemodel} derived the form of DPO in token-level Markov Decision Process\footnote{Please refer to Appendix \ref{app: prel} for more details.}, where dynamic weight can be easily integrated for different tokens\footnote{Please refer to Appendix \ref{app: change_beta} for the mathematical proof of the dynamic token weight in preference optimization}, with the loss function as follows:

\vspace{-8mm}

\begin{align}
& \mathcal{L}_{TDPO}(\pi_{\theta},D) = \nonumber \\
& -\mathbb{E}_{(\tau_w,\tau_l)\sim D} \log \sigma ( \beta \sum_{t=0}^{N-1} r_t^w \log \frac{\pi_{\theta}(\mathbf{a}_t^w|\mathbf{s}_t^w)}{\pi_{ref}(\mathbf{a}_t^w|\mathbf{s}_t^w)} \nonumber \\
& -\beta \sum_{t=0}^{M-1} r_t^l \log \frac{\pi_{\theta}(\mathbf{a}_t^l|\mathbf{s}_t^l)}{\pi_{ref}(\mathbf{a}_t^l|\mathbf{s}_t^l)}),
\label{eq: rdpo}
\end{align}

\vspace{-3mm}

\noindent where $\tau^w$ and $\tau^l$ represent the winning and losing trajectories, with $N$ and $M$ as the token numbers, and $r_t$ represents the weight for the $t$-th token.

% \begin{align}
% & \mathcal{L}_{TDPO}(\pi_\theta; \pi_{ref}) = \\
% & -\mathbb{E} \left[ \log \sigma \left( \beta \sum_{t=0}^{N-1} \log \frac{\pi_\theta(a_w^t \mid s_w^t)}{\pi_{ref}(a_w^t \mid s_w^t)} \right. \right. \nonumber \\
% & - \beta \sum_{t=0}^{M-1} \log \frac{\pi_\theta(a_l^t \mid s_l^t)}{\pi_{ref}(a_l^t \mid s_l^t)} \right) \right].  \nonumber
% \end{align}

\subsubsection{Calibrated Confidence as Token Weight}
\label{sec:entropy}

While Section ~\ref{preliminary} provide theoretical support for token-level DPO, it is non trivial to derive token-level supervision. In this work, we propose to use the calibrated confidence as supervision.

Given an instruction $x$, we obtain the entropy of probability distribution over target vocabulary of size $V$ at each decoding step as the weights:
\vspace{-2mm}
% $$ p(y_t) = p(y_t|y_{<t}, x, \theta)$$
\begin{equation} 
\textrm{Ent}(y_t|x^{w}, \theta) = - \sum_{v=1}^{V}p(y_t^v)\mathrm{log}p(y_t^v),
\vspace{-1mm}
\end{equation}

%However, the token probability should not be used as confidence score, as the model would always select from the most probable tokens during decoding regardless of their quality, leading to biased evaluation. 

\noindent where $p(y_t)$ represents the conditional distribution $p(y_t|x, y_{<t}, \theta)$, and $\theta$ represents model parameters. If the majority of the probability mass is concentrated on a limited number of vocabulary words, it indicates that the model is confident and the token is more likely to be aligned with the instruction~\cite{fomicheva2020unsupervised}. Conversely, if the probabilities resemble a uniform distribution, the resulting token is expected to be misaligned.

% where selecting any word from the vocabulary is equally probable, 

While there are other attributes that could also impact the confidence score (such as fluency), in this work, we want to focus our optimization on instruction alignment. Therefore, we apply calibration to the entropy to derive the token-level supervision for chosen and rejected samples respectively:

\vspace{-4mm}

\begin{align*} r_t =
\left\{  
  \begin{array}{ll}
    \textrm{Ent}(y_t|x^{w}, \theta) / \textrm{Ent}(y_t|x^{l}, \theta),\ \text{for}\ y_t\ \text{in}\ y^w, \\
    \textrm{Ent}(y_t|x^{l}, \theta) / \textrm{Ent}(y_t|x^{w}, \theta),\ \text{for}\ y_t\ \text{in}\ y^l,
  \end{array}
\right.
\end{align*}

\vspace{-4mm}
\begin{equation}
    r_t = \mathbf{min}(\Gamma, r_t),
\end{equation}
\vspace{-4mm}

% $$r_t^{w} = \textrm{Ent}(y_t^{w}|x^{w}, \theta) / \textrm{Ent}(y_t^{w}|x^{l}, \theta)$$
% $$r_t^{l} = \textrm{Ent}(y_t^{l}|x^{l}, \theta) / \textrm{Ent}(y_t^{l}|x^{w}, \theta)$$

\noindent where the chosen sample $(x^{w}, y^{w})$ refers to $(\texttt{Ins}, \texttt{Resp})$, and the rejected sample $(x^{l}, y^{l})$ refers to $(\texttt{Ins}_{drop}, \texttt{Resp}_{drop})$, and $\Gamma$ is an upper-bound to avoid extreme cases to disrupt training, and we set $\Gamma$ as 2 in this work.

The rationale is straightforward: for a given token in the response, if it exhibits high confidence under $x^{w}$ and low confidence under $x^{l}$, this suggests that the token aligns well with $x^{w}$ but does not fit $x^{l}$, potentially reflecting the dropped constraint. Therefore, the token requires a larger weight if it is in the rejected response (or a smaller weight if it is in the chosen response).

% The same logic can be applied to the chosen response $y^{w}$, with a larger confidence gap reflects a potential constraint mismatch.

Calibrated confidence guided token-aware DPO allows for more targeted optimization, focusing on tokens that highlight the constraint mismatch on complex instructions, instead of unanimously optimizing all tokens, thereby improving the efficiency of complex instruction alignment.