\clearpage
\appendix
\onecolumn
\section{Implementation Details}
\subsection{Token-aware Preference Data Construction}
\label{sec:impl}
For all models that used for preference data construction, we adopt the following prompts presented in Figure \ref{fig: prompt-decom}, \ref{fig: prompt-selfinst}, \ref{fig: prompt-recomb}, \ref{fig: prompt-sub}, \ref{fig: prompt-neg} and \ref{fig: prompt-sub}. We set the temperate as 0.5 for all steps to ensure diversity. To ensure the data quality, we filter instructions with less than three constraints and more than ten constraints. We also filter preference pairs with the same chosen and rejected responses. 

For constraint dropout, we set the dropout ratio $\alpha$ to 0.3 to ensure that negative examples are sufficiently negative, meanwhile not deviate too much from the positive sample. We avoid dropout on the first constraint, as it often establishes the foundation for the task, and dropping the first one would make the recombined instruction overly biased.

\subsection{Token-aware Preference Optimization}
\label{sec:impl-dpo}
Our experiments are based on Llama-Factory \cite{zheng2024llamafactory}, and we trained all models on 8 A100-80GB SXM GPUs. The \texttt{per\_device\_train\_batch\_size} was set to 1, \texttt{gradient\_accumulation\_steps} to 8, leading to an overal batch size as 64, and we used bfloat16 precision. The learning rate is set as 1e-06 with cosine decay,and each model is trained with 2 epochs. We set $\beta$ to 0.2 for all DPO-based experiments, $\beta$ as 3.0 and $\gamma$ as 1.0 for all SimPO-based experiments, $\beta$ as 1.0 for all IPO-based methods referring to the settings of \citet{meng2024simpo}. All of the final loss includes 0.1x of the SFT loss.

\section{The Influence of Noising Scheme}
\label{app:noising}

Previous work has proposed various noising strategies in contrastive training \cite{lai-etal-2021-saliency-based}. While we leverage Constraint-Dropout for negative sample generation, to make a fair comparison with other strategies, we implement the following strategies: 1) Constraint-Negate: Leverage the model to generate an opposite constraint. 2) Constraint-Substitute: Substitute the constraint with an unrelated constraint.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{figures/drop_ratio.png}
\caption{The variation of results on CFBench and AlpacaEval2 with different dropout ratios.}
\label{fig:drop_ratio}
\end{figure}

As shown in Table \ref{tab:detail-noising}, both the negation and substitution applied on the constraints would lead to performance degradation. After a thoroughly inspect of the derived data, we realize that instructions derived from both dropout and negation would lead to instructions too far from the positive instruction, therefore the derived negative response would also deviate too much from the original instruction. An effective negative sample should fulfill both negativity, consistency and contrastiveness, and constrait-dropout is a simple yet effective method to achieve this goal.

We also provide the variation of the results on CF-Bench and AlpacaEval2 with different constraint dropout ratios. As shown in Figure \ref{fig:drop_ratio}, with the dropout ratio increased from 0.1 to 0.5, the results on CF-Bench firstly increases and then slightly decreases. On the other hand, the results on AlpacaEval2 declines a lot with a higher dropout ratio. This denotes that a suboptimal droout ratio is essential for the balance between complex instruction and general instruction following abilities, with lower ratio may decrease the effectiveness of general instruction alignment, while higher ratio may be harmful for complex instruction alignment. Finally, we set the constraint dropout ratio as 0.3 in all experiments.

\begin{table*}[tt]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{cc|ccccc|ccccc}
\toprule
\multirow{3}{*}{\textbf{Scenario}} & \multirow{3}{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{Meta-LLaMA-3-8B-Instruct}}                                    & \multicolumn{5}{c}{\textbf{Qwen-2-7B-Instruct}}                                          \\
                                   &                                  & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c|}{\textbf{AlpacaEval2}} & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
                                   &                                  & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{LC\%}      & \textbf{Avg.Len}     & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{LC\%}      & \textbf{Avg.Len}    \\ \midrule
\multirow{6}{*}{PreInst}           & baseline                         & 0.64          & 0.24          & 0.34          & 21.07              & 1702                 & 0.74          & 0.36          & 0.49          & 15.53              & 1688                \\ \cline{2-12} 
                                   & Constraint-Drop               & \textbf{0.71} & \textbf{0.34} & \textbf{0.45} & \textbf{23.43}     & 1682           & \textbf{0.79} & \textbf{0.43}  & \textbf{0.54}          & \textbf{19.31}     & 1675                \\
                                   & Constraint-Negate             & 0.68          & 0.28          & 0.39          & 18.94              & 1688                 & 0.75          & 0.37          & 0.50          & 17.82              & 1663                \\
                                   & Constraint-Substitute             & 0.68          & 0.28          & 0.40          & 20.48              & 1706                 & 0.76          & 0.39          & 0.51          & 19.05              & 1709                \\ \bottomrule
\end{tabular}}
\caption{Experiment results of different noising strategies on instruction following benchmarks.}
\label{tab:detail-noising}
\end{table*}

\section{Mathematical Derivations}
\subsection{Preliminary: DPO in the Token Level Marcov Decision Process}
\label{app: prel}
% In the most classic RLHF methods, the optimization goal is typically expressed as an entropy bonus using the following KL-constrained:

% \begin{align}
% &
% \max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \mathcal{D}_{KL}[\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)||\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)]]
% % \label{eq: rlhf_obj}
% \\
% &
% =\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \frac{\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)}{\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}]
% % \nonumber
% \\
% &
% =\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} [ \sum_{t=0}^{T} ( r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t) ) + \beta \mathcal{H}(\pi_\theta) | \mathbf{s}_0 \sim \rho(\mathbf{s}_0) ]
% % \nonumber
% \label{eq: rlhf_objective}
% \end{align}

As demonstrated in \citet{rafailov2024rqlanguagemodel}, the Bradley-Terry preference model in token-level Marcov Decision Process (MDP) is:

\begin{equation}
p^*\left(\tau^w \succeq \tau^l\right)=\frac{\exp \left(\sum_{i=1}^N r\left(\mathbf{s}_i^w, \mathbf{a}_i^w\right)\right)}{\exp \left(\sum_{i=1}^N r\left(\mathbf{s}_i^w, \mathbf{a}_i^w\right)\right)+\exp \left(\sum_{i=1}^M r\left(\mathbf{s}_i^l, \mathbf{a}_i^l\right)\right)}
\label{eq: tdpo_bt}
\end{equation}

\label{app: tdpo}
The formula using the $Q$-function to measure the relationship between the current timestep and future returns:

% From $r$ to $Q^*$
\begin{equation}
Q^*(s_t, a_t) =
\begin{cases} 
r(s_t, a_t) + \beta \log \pi_{ref}(a_t | s_t) + V^*(s_{t+1}), & \text{if } s_{t+1} \text{ is not terminal} \\
r(s_t, a_t) + \beta \log \pi_{ref}(a_t | s_t), & \text{if } s_{t+1} \text{ is terminal}
\end{cases}
\label{eq: t_return}
\end{equation}

Derive the total reward obtained along the entire trajectory based on the above definitions:
\begin{align}
& \sum_{t=0}^{T-1} r(s_t, a_t)
 = \sum_{t=0}^{T-1} ( Q^*(s_t, a_t) - \beta \log \pi_{\text{ref}}(a_t | s_t) - V^*(s_{t+1}) )
\label{eq: r_sum}
\end{align}

Combining this with the fixed point solution of the optimal policy \cite{Ziebart2010ModelingPA, Levine2018ReinforcementLA}, we can further derive:
\begin{align}
\sum_{t=0}^{T-1} r(s_t, a_t)
& = Q^*(s_0, a_0) - \beta \log \pi_{ref}(a_0 | s_0) 
+ \sum_{t=1}^{T-1} ( Q^*(s_t, a_t) - V^*(s_t) - \beta \log \pi_{\text{ref}}(a_t | s_t) )
\\
& = Q^*(s_0, a_0) - \beta \log \pi_{ref}(a_0 | s_0) + \sum_{t=1}^{T-1} \beta \log \frac{\pi^*(a_t | s_t)}{\pi_{\text{ref}}(a_t | s_t)}
% \nonumber
\\
& = V^*(s_0) + \sum_{t=0}^{T-1} \beta \log \frac{\pi^*(a_t | s_t)}{\pi_{\text{ref}}(a_t | s_t)}
% \nonumber
\end{align}

By substituting the above result into Eq. \ref{eq: tdpo_bt}, we can eliminate $V^*(S_0)$ in the same way as removing the partition function in DPO, obtaining the Token-level BT model that conforms to the MDP:
% By substituting the above result into equation \ref{eq: tdpo_bt}, we can obtain the Token-level BT model that conforms to the Markov Decision Process:

\begin{equation}
p_{\pi^*}\left(\tau^w \succeq \tau^l\right)=\sigma\left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^*\left(\mathbf{a}_t^w \mid \mathbf{s}_t^w\right)}{\pi_{\mathrm{ref}}\left(\mathbf{a}_t^w \mid \mathbf{s}_t^w\right)}-\sum_{t=0}^{M-1} \beta \log \frac{\pi^*\left(\mathbf{a}_t^l \mid \mathbf{s}_t^l\right)}{\pi_{\mathrm{ref}}\left(\mathbf{a}_t^l \mid \mathbf{s}_t^l\right)}\right)
\end{equation}

Thus, the Loss formulation of DPO at the Token level is:
\begin{equation}
\mathcal{L}\left(\pi_\theta, \mathcal{D}\right)=-\mathbb{E}_{\left(\tau_w, \tau_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\left(\sum_{t=0}^{N-1} \beta \log \frac{\pi^*\left(\mathbf{a}_t^w \mid \mathbf{s}_t^w\right)}{\pi_{\mathrm{ref}}\left(\mathbf{a}_t^w \mid \mathbf{s}_t^w\right)}\right)-\left(\sum_{t=0}^{M-1} \beta \log \frac{\pi^*\left(\mathbf{a}_t^l \mid \mathbf{s}_t^l\right)}{\pi_{\mathrm{ref}}\left(\mathbf{a}_t^l \mid \mathbf{s}_t^l\right)}\right)\right)\right]
\end{equation}

\subsection{Proof of Dynamic Token Weight in Token-level DPO}
\label{app: change_beta}

In classic RLHF methods, the optimization objective is typically formulated with an entropy bonus, expressed through a Kullback-Leibler (KL) divergence constraint as follows:

\begin{align}
&
\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \mathcal{D}_{KL}[\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)||\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)]]
% \label{eq: rlhf_obj}
\\
&
=\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \frac{\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)}{\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}]
% \nonumber
\label{eq: rlhf_objective}
\end{align}

This can be further rewritten by separating the terms involving the reference policy and the entropy of the current policy:

$$\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} [ \sum_{t=0}^{T} ( r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t) ) + \beta \mathcal{H}(\pi_\theta) | \mathbf{s}_0 \sim \rho(\mathbf{s}_0) ]$$

When the coefficient $\beta$ is treated as a variable that depends on the timestep $t$ \cite{li20242ddposcalingdirectpreference}, the objective transforms to:

\begin{align}
&
\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [( r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)) - \beta_t \log \pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)]
\end{align}

\noindent where $\beta_t$ depends solely on $\mathbf{a}_t$ and $\mathbf{s}_t$. Following the formulation by \citet{Levine2018ReinforcementLA}, the above expression can be recast to incorporate the KL divergence explicitly:

\begin{align}
&
\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [( r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)) - \beta_t \log \pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)]
\end{align}

\noindent where the value function  $V(\mathbf{s}_t)$ is defined as:

\begin{align}
V(\mathbf{s}_t) = \beta_t \log \int_{\mathcal{A}} [\exp\frac{r(\mathbf{s}_t, \mathbf{a}_t)}{\beta_t} \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)] \, d\mathbf{a}_t
\end{align}

When the KL divergence term is minimized—implying that the two distributions are identical—the expectation in Eq. \eqref{eq: rlhf_objective} reaches its maximum value. Therefore, the optimal policy satisfies:

\begin{align}
\pi_\theta(\mathbf{a}_t | \mathbf{s}_t) = \frac{1}{\exp(V(\mathbf{s}_t))} \exp\left(\frac{r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}{\beta_t}\right)
\end{align}

Based on this relationship, we define the optimal Q-function as:

\begin{equation}
Q^*(s_t, a_t) =
\begin{cases} 
r(s_t, a_t) + \beta_t \log \pi_{ref}(a_t | s_t) + V^*(s_{t+1}), & \text{if } s_{t+1} \text{ is not terminal} \\
r(s_t, a_t) + \beta_t \log \pi_{ref}(a_t | s_t), & \text{if } s_{t+1} \text{ is terminal}
\end{cases}
\label{eq: t_return}
\end{equation}

Consequently, the optimal policy can be expressed as:
% $Q(\mathbf{s}_t, \mathbf{a}_t) = r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{\text{ref}}(\mathbf{a}_t | \mathbf{s}_t)$, thus we can obtain the solution for the optimal policy:
\begin{align}
\pi_\theta(\mathbf{a}_t | \mathbf{s}_t) = e^{(Q(\mathbf{s}_t, \mathbf{a}_t) - V(\mathbf{s}_t))/\beta_t}
\label{eq: fixed_point_2}
\end{align}

By taking the natural logarithm of both sides, we obtain a log-linear relationship for the optimal policy at the token level, which is expressed with the optimial Q-function:
\begin{align}
\beta_t \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) = Q_\theta(\mathbf{s}_t, \mathbf{a}_t) - V_\theta(\mathbf{s}_t)
\end{align}


This equation establishes a direct relationship between the scaled log-ratio of the optimal policy to the reference policy and the reward function $r(\mathbf{s}_t, \mathbf{a}_t)$:

\begin{align}
\beta_t \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{t+1}) - V^*(\mathbf{s}_t)
\end{align}

Furthermore, following the definition by \citet{rafailov2024rqlanguagemodel}'s definition, two reward functions $r(\mathbf{s}_t, \mathbf{a}_t)$ and $r'(\mathbf{s}_t, \mathbf{a}_t)$ are considered equivalent if there exists a potential function $\Phi(\mathbf{s})$, such that:

\begin{align}
r'(\mathbf{s}_t, \mathbf{a}_t) =r(\mathbf{s}_t, \mathbf{a}_t) + \Phi(\mathbf{s}_{t+1})  - \Phi(\mathbf{s}_{t})
\end{align}

This equivalence implies that the optimal advantage function remains invariant under such transformations of the reward function. Consequently, we derive why the coefficient $beta$ in direct preference optimization can be variable, depending on the state and action, thereby allowing for more flexible and adaptive policy optimization in RLHF frameworks.

% In the most classic RLHF methods, the optimization goal is typically expressed as an entropy bonus using the following KL-constrained:
% \begin{align}
% &
% \max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \mathcal{D}_{KL}[\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)||\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)]]
% % \label{eq: rlhf_obj}
% \\
% &
% =\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [r(\mathbf{s}_t, \mathbf{a}_t) - \beta \log \frac{\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)}{\pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}]
% % \nonumber
% \\
% &
% =\max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} [ \sum_{t=0}^{T} ( r(\mathbf{s}_t, \mathbf{a}_t) + \beta \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t) ) + \beta \mathcal{H}(\pi_\theta) | \mathbf{s}_0 \sim \rho(\mathbf{s}_0) ]
% % \nonumber
% \label{eq: rlhf_objective}
% \end{align}


% When $\beta$ is considered as a variable dependent on $t$, Eq. \ref{eq: rlhf_objective} is transformed into:
% \begin{align}
% &
% \max_{\pi_\theta} \mathbb{E}_{a_t \sim \pi_\theta(\cdot | \mathbf{s}_t)} \sum_{t=0}^{T} [( r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)) - \beta_t \log \pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)]
% \end{align}

% \noindent where $\beta_t$ depends solely on $\mathbf{a}_t$ and $\mathbf{s}_t$. Then, according to \citet{Levine2018ReinforcementLA}, the above formula can be rewritten in a form that includes the KL divergence:
% \begin{align}
% &
% =\mathbb{E}_{\mathbf{s}_t} [ -\beta_t D_{KL}\left( \pi_\theta(\mathbf{a}_t | \mathbf{s}_t) \bigg\| \frac{1}{\exp(V(\mathbf{s}_t))} \exp\left(\frac{r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}{\beta_t}\right) \right) + V(\mathbf{s}_t) ]
% \label{eq: rlhf_objective_2}
% \end{align}

% \noindent where $V(\mathbf{s}_t) = \beta_t \log \int_{\mathcal{A}} [\exp\frac{r(\mathbf{s}_t, \mathbf{a}_t)}{\beta_t} \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)] \, d\mathbf{a}_t$. When the KL divergence term is minimized, meaning the two distributions are the same, the above expectation reaches its maximum value. That is:
% \begin{align}
% \pi_\theta(\mathbf{a}_t | \mathbf{s}_t) = \frac{1}{\exp(V(\mathbf{s}_t))} \exp\left(\frac{r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{ref}(\mathbf{a}_t | \mathbf{s}_t)}{\beta_t}\right)
% \end{align}

% Based on this, we define that:
% \begin{equation}
% Q^*(s_t, a_t) =
% \begin{cases} 
% r(s_t, a_t) + \beta_t \log \pi_{ref}(a_t | s_t) + V^*(s_{t+1}), & \text{if } s_{t+1} \text{ is not terminal} \\
% r(s_t, a_t) + \beta_t \log \pi_{ref}(a_t | s_t), & \text{if } s_{t+1} \text{ is terminal}
% \end{cases}
% \label{eq: t_return}
% \end{equation}

% Thus we can obtain the solution for the optimal policy:
% % $Q(\mathbf{s}_t, \mathbf{a}_t) = r(\mathbf{s}_t, \mathbf{a}_t) + \beta_t \log \pi_{\text{ref}}(\mathbf{a}_t | \mathbf{s}_t)$, thus we can obtain the solution for the optimal policy:
% \begin{align}
% \pi_\theta(\mathbf{a}_t | \mathbf{s}_t) = e^{(Q(\mathbf{s}_t, \mathbf{a}_t) - V(\mathbf{s}_t))/\beta_t}
% \label{eq: fixed_point_2}
% \end{align}

% By log-linearizing the fixed point solution of the optimal policy at the token level, we obtain:
% \begin{align}
% &
% \beta_t \log \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t) = Q_\theta(\mathbf{s}_t, \mathbf{a}_t) - V_\theta(\mathbf{s}_t)
% \end{align}

% Then, combining with Eq. \ref{eq: t_return}:
% \begin{align}
% \beta_t \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)} = r(\mathbf{s}_t, \mathbf{a}_t) + V^*(\mathbf{s}_{t+1}) - V^*(\mathbf{s}_t).
% \end{align}

% Thus, we can establish the relationship between $\beta_t \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}$ and $r(\mathbf{s}_t, \mathbf{a}_t)$. 

% According to \citet{rafailov2024rqlanguagemodel}'s definition, two reward functions $r(\mathbf{s}_t, \mathbf{a}_t)$ and $r'(\mathbf{s}_t, \mathbf{a}_t)$ are equivalent if there exists a potential function $\Phi(\mathbf{s})$, such that $r'(\mathbf{s}_t, \mathbf{a}_t) =r(\mathbf{s}_t, \mathbf{a}_t) + \Phi(\mathbf{s}_{t+1})  - \Phi(\mathbf{s}_{t})$. We can conclude that the optimal advantage function is $\beta_t \log \frac{\pi^*(\mathbf{a}_t \mid \mathbf{s}_t)}{\pi_{\text{ref}}(\mathbf{a}_t \mid \mathbf{s}_t)}$.

\section{Detailed Experiment Results}
\label{sec:app-results}
In this section, we presented detailed experiment results which are omitted in the main body of this paper due to space limitation. The detailed experiment results of different methods on ComplexBench, FollowBench and AlpacaEval2 are presented in Table \ref{tab:complexbench}, \ref{tab:alpaca-eval} and \ref{tab:followbench}. The detailed results for the ablative studies of confidence metrics is presented in Table \ref{tab:detail-confidence}. The detailed results for the ablative studies of confidence metrics is presented in Table \ref{tab:detail-noising}. We also present a case study in Table \ref{tab:case-study}, which visualize the token-level weight derived from calibrated confidence score.


\begin{table*}[ht]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{cc|cccc|cccc}
\hline
\multirow{3}{*}{\textbf{Scenario}} & \multirow{3}{*}{\textbf{Method}} & \multicolumn{8}{c}{\textbf{ComplexBench}}                                                                                                         \\
                                   &                                  & \multicolumn{4}{c}{\textbf{Meta-Llama3-8B-Instruct}}                    & \multicolumn{4}{c}{\textbf{Qwen2-7B-Instruct}}                          \\
                                   &                                  & \textbf{Overall} & \textbf{And}   & \textbf{Chain} & \textbf{Selection} & \textbf{Overall} & \textbf{And}   & \textbf{Chain} & \textbf{Selection} \\ \hline
\multicolumn{2}{c|}{baseline}                          & 61.49            & 57.22          & 57.22          & 53.55              & 67.24            & 62.58          & 62.58          & 58.97              \\ \hline
\multirow{6}{*}{SelfInst}          & Self-Reward                      & 62.45            & 58.23          & 58.23          & 54.07              & 66.98            & 63.02          & 63.02          & 57.75              \\
                                   & w/ BSM                           & 64.13            & 58.01          & 58.01          & 56.62              & 67.02            & 62.37          & 62.37          & 57.85              \\
                                   & w/ GPT-4                         & 64.05            & 59.44          & 59.44          & 54.78              & —                & —              & —              & —                  \\ \cline{2-10} 
                                   & Self-Correct                     & 55.91            & 49.85          & 49.85          & 46.91              & 64.41            & 59.59          & 59.59          & 55.04              \\
                                   & ISHEEP                           & 62.67            & 57.79          & 57.79          & 54.63              & 67.32            & 61.95          & 61.95          & 59.64              \\ \cline{2-10} 
                                   & \textbf{MuSC}                    & \textbf{65.98}   & \textbf{63.45} & \textbf{63.45} & \textbf{55.96}     & \textbf{69.39}   & \textbf{65.45} & \textbf{65.45} & \textbf{59.79}     \\ \hline
\multirow{7}{*}{PreInst}           & Self-Reward                      & 62.03            & 56.94          & 56.94          & 53.09              & 66.45            & 61.37          & 61.37          & 57.64              \\
                                   & w/ BSM                           & 64.30            & 57.58          & 57.58          & 56.47              & 67.43            & 62.95          & 62.95          & 58.41              \\
                                   & w/ GPT-4                         & 63.52            & 59.08          & 59.08          & 53.91              & —                & —              & —              & —                  \\ \cline{2-10} 
                                   & Self-Correct                     & 60.79            & 55.65          & 55.65          & 52.02              & 64.32            & 60.16          & 60.16          & 54.63              \\
                                   & ISHEEP                           & 62.92            & 56.37          & 56.37          & 54.83              & 67.13            & 64.45          & 64.45          & 57.54              \\
                                   & SFT                              & 53.93            & 45.77          & 45.77          & 44.09              & 65.89            & 60.16          & 60.16          & 57.39              \\ \cline{2-10} 
                                   & \textbf{MuSC}                    & \textbf{64.73}   & \textbf{59.23} & \textbf{59.23} & \textbf{55.91}     & \textbf{70.00}   & \textbf{66.88} & \textbf{66.88} & \textbf{61.38}     \\ \hline
\end{tabular}}
\label{tab:complexbench}
\caption{Detailed experiment results of different methods on ComplexBench.}
\label{tab:complexbench}
\end{table*}

\begin{table*}[ht]
\centering
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{cc|ccc|ccc}
\hline
\multirow{3}{*}{\textbf{Scenario}} & \multirow{3}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{FollowBench}}                                                               \\
                                   &                                  & \multicolumn{3}{c}{\textbf{Meta-Llama3-8B-Instruct}} & \multicolumn{3}{c}{\textbf{Qwen2-7B-Instruct}}  \\
                                   &                                  & \textbf{HSR}     & \textbf{SSR}     & \textbf{CSL}   & \textbf{HSR}   & \textbf{SSR}   & \textbf{CSL}  \\ \hline
\multicolumn{2}{c|}{baseline}                                         & 62.39            & 73.07            & 2.76           & 59.81          & 71.69          & 2.46          \\ \hline
\multirow{6}{*}{SelfInst}          & Self-Reward                      & 61.20            & 72.22            & 2.56           & 55.36          & 69.71          & 2.34          \\
                                   & w/ BSM                           & 64.30            & 73.84            & 2.80           & 57.83          & 70.53          & 2.41          \\
                                   & w/ GPT-4                         & 62.18            & 73.34            & 2.66           & —              & —              & —             \\ \cline{2-8} 
                                   & Self-Correct                     & 54.38            & 67.19            & 2.02           & 51.98          & 67.89          & 2.16          \\
                                   & ISHEEP                           & 62.77            & 72.86            & 2.52           & 57.01          & 69.88          & 2.36          \\ \cline{2-8} 
                                   & \textbf{MuSC}                    & \textbf{66.71}   & \textbf{74.84}   & \textbf{2.92}  & \textbf{62.60} & \textbf{72.57} & \textbf{2.82} \\ \hline
\multirow{7}{*}{PreInst}           & Self-Reward                      & 60.88            & 72.17            & 2.64           & 56.45          & 70.00          & 2.44          \\
                                   & w/ BSM                           & 63.96            & 73.78            & 2.66           & 58.02          & 70.62          & 2.42          \\
                                   & w/ GPT-4                         & 64.02            & 73.26            & 2.64           & —              & —              & —             \\ \cline{2-8} 
                                   & Self-Correct                     & 60.11            & 70.94            & 2.70           & 49.47          & 66.35          & 1.98          \\
                                   & ISHEEP                           & 63.54            & 73.21            & 2.64           & 55.52          & 69.62          & 2.28          \\
                                   & SFT                              & 50.06            & 66.48            & 2.04           & 47.36          & 64.67          & 1.96          \\ \cline{2-8} 
                                   & \textbf{MuSC}                    & \textbf{66.90}   & \textbf{75.11}   & \textbf{2.99}  & \textbf{62.73} & \textbf{73.09} & \textbf{2.86} \\ \hline
\end{tabular}}
\caption{Detailed experiment results of different methods on FollowBench.}
\label{tab:followbench}
\end{table*}

\begin{table*}[ht]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{cc|cccccc}
\hline
\multirow{3}{*}{\textbf{Scenario}} & \multirow{3}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{AlpacaEval2}}                                                                          \\
                                   &                                  & \multicolumn{3}{c}{\textbf{Meta-Llama3-8B-Instruct}}    & \multicolumn{3}{c}{\textbf{Qwen2-7B-Instruct}}          \\
                                   &                                  & \textbf{LC (\%)} & \textbf{WR (\%)} & \textbf{Avg. Len} & \textbf{LC (\%)} & \textbf{WR (\%)} & \textbf{Avg. Len} \\ \hline
\multicolumn{2}{c|}{baseline}                                         & 21.07            & 18.73            & 1702              & 15.53            & 13.70            & 1688              \\ \hline
\multirow{6}{*}{SelfInst}          & Self-Reward                      & 19.21            & 19.18            & 1824              & 16.81            & 15.66            & 1756              \\
                                   & w/ BSM                           & 19.03            & 18.34            & 1787              & 16.94            & 15.09            & 1710              \\
                                   & w/ GPT-4                         & 19.55            & 18.53            & 1767              & —                & —                & —                 \\ \cline{2-8} 
                                   & Self-Correct                     & 7.97             & 9.34             & 1919              & 14.01            & 10.92            & 1497              \\
                                   & ISHEEP                           & 22.00            & 19.50            & 1707              & 16.99            & 14.04            & 1619              \\ \cline{2-8} 
                                   & \textbf{MuSC}                    & \textbf{23.87}   & \textbf{20.91}   & \textbf{1708}     & \textbf{20.08}   & \textbf{15.67}   & \textbf{1595}     \\ \hline
\multirow{7}{*}{PreInst}           & Self-Reward                      & 19.93            & 19.04            & 1789              & 15.98            & 15.62            & 1796              \\
                                   & w/ BSM                           & 20.98            & 20.75            & 1829              & 17.17            & 16.21            & 1764              \\
                                   & w/ GPT-4                         & 18.02            & 17.74            & 1804              & —                & —                & —                 \\ \cline{2-8} 
                                   & Self-Correct                     & 6.20             & 5.81             & 1593              & 14.46            & 14.02            & 1737              \\
                                   & ISHEEP                           & 20.23            & 17.86            & 1703              & 16.52            & 13.36            & 1627              \\
                                   & SFT                              & 10.00            & 6.22             & 1079              & 9.52             & 5.25             & 979               \\ \cline{2-8} 
                                   & \textbf{MuSC}                    & \textbf{23.74}   & \textbf{19.53}   & \textbf{1631}     & \textbf{20.29}   & \textbf{15.91}   & \textbf{1613}     \\ \hline
\end{tabular}}
\caption{Detailed experiment results of different methods on AlpacaEval2.}
\label{tab:alpaca-eval}
\end{table*}

\begin{table}[ht]
\centering
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{cc|ccccc|ccccc}
\toprule
\multirow{3}{*}{\textbf{Scenario}} & \multirow{3}{*}{\textbf{Method}} & \multicolumn{5}{c|}{\textbf{Meta-Llama-3-8B-Instruct}}                                    & \multicolumn{5}{c}{\textbf{Qwen-2-7B-Instruct}}                                          \\
                                   &                                  & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c|}{\textbf{AlpacaEval2}} & \multicolumn{3}{c}{\textbf{CF-Bench}}         & \multicolumn{2}{c}{\textbf{AlpacaEval2}} \\
                                   &                                  & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{LC (\%)}   & \textbf{Avg. Len}       & \textbf{CSR}  & \textbf{ISR}  & \textbf{PSR}  & \textbf{LC (\%)}   & \textbf{Avg. Len}      \\ \midrule
\multirow{6}{*}{PreInst}           & Baseline                         & 0.64          & 0.24          & 0.34          & 21.07                & 1702               & 0.74          & 0.36          & 0.49          & 15.53                & 1688              \\ \cline{2-12} 
                                   % & MuSC w/o conf                  & 0.70          & 0.30          & 0.41          & 21.19                & 1703               & 0.79          & 0.44          & 0.56          & 18.91                & 1604              \\ \cline{2-12} 
                                   & w/ perplexity                    & 0.70          & 0.32          & 0.43          & 22.99                & 1744               & 0.79          & 0.43          & 0.54          & 19.31                & 1675              \\
                                   & w/ PMI                           & 0.69          & 0.29          & 0.41          & 21.92                & 1713               & 0.78          & 0.43          & 0.55          & 17.42                & 1651              \\
                                   & w/ KLDiv                         & 0.69          & 0.31          & 0.42          & 21.86                & 1686               & 0.78          & 0.42          & 0.54          & 18.73                & 1614              \\
                                   & \textbf{w/ entropy}                       & \textbf{0.71} & \textbf{0.34} & \textbf{0.44} & \textbf{23.74}       & 1631               & \textbf{0.80} & \textbf{0.46} & \textbf{0.58} & \textbf{20.38}       & 1683              \\ \bottomrule
\end{tabular}}
\caption{Detailed experiment results of different confidence metrics as the fine-grained weight.}
\label{tab:detail-confidence}
\end{table}

\setlength{\fboxsep}{0pt}

\begin{table}[t]
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l}
\hline
\textbf{Instruction 1}: Identify the incorrect word in the given sentence: "The group acquired the skill to ski\\ in a suitable manner." Rewrite the identified incorrect word correctly, ensuring the new sentence\\ adheres to proper capitalization, punctuation, and grammar rules. Include at least one synonym of\\ the incorrect word to test the model's understanding of context and vocabulary, and ensure the synon-\\ ym is used appropriately in the context of the sentence. \\\hline
\textbf{Chosen Response 1}: \colorbox{red!38}{\strut \:The}\colorbox{red!30}{\strut \:group}\colorbox{red!12}{\strut \:acquired}\colorbox{red!50}{\strut \:the}\colorbox{red!25}{\strut \:skill}\colorbox{red!50}{\strut \:to}\colorbox{red!38}{\strut \:ski}\colorbox{red!50}{\strut \:in}\colorbox{red!50}{\strut \:a}\colorbox{red!0}{\strut \:proficient}\colorbox{red!25}{\strut \:manner}\colorbox{red!50}{\strut \:.}\colorbox{red!35}{\strut \:The}\colorbox{red!12}{\strut \:incorrect}\colorbox{red!25}{\strut \:word}\\\colorbox{red!50}{\strut \:is}\colorbox{red!38}{\strut \:"suitable"}\colorbox{red!50}{\strut \:.}\colorbox{red!45}{\strut \:A}\colorbox{red!18}{\strut \:synonym}\colorbox{red!50}{\strut \:for}\colorbox{red!38}{\strut \:"suitable"}\colorbox{red!50}{\strut \:is}\colorbox{red!5}{\strut \:"appropriate"}\colorbox{red!50}{\strut \:,}\colorbox{red!40}{\strut \:but}\colorbox{red!50}{\strut \:it}\colorbox{red!25}{\strut \:doesn't}\colorbox{red!10}{\strut \:fit}\colorbox{red!50}{\strut \:the}\colorbox{red!32}{\strut \:context}\colorbox{red!50}{\strut \:of}\colorbox{red!50}{\strut \:the}\colorbox{red!28}{\strut \:sentence}\colorbox{red!50}{\strut \:.}\\\colorbox{red!8}{\strut \:"Proficient"}\colorbox{red!50}{\strut \:is}\colorbox{red!50}{\strut \:a}\colorbox{red!18}{\strut \:better}\colorbox{red!20}{\strut \:synonym}\colorbox{red!50}{\strut \:in}\colorbox{red!45}{\strut \:this}\colorbox{red!25}{\strut \:context}\colorbox{red!50}{\strut \:as}\colorbox{red!45}{\strut \:it}\colorbox{red!50}{\strut \:means}\colorbox{red!5}{\strut \:skilled}\colorbox{red!50}{\strut \:or}\colorbox{red!32}{\strut \:expert}\colorbox{red!50}{\strut \:.} \\\hline

\textbf{Rejected Response 1}: \colorbox{red!14}{\strut \:The}\colorbox{red!25}{\strut \:word}\colorbox{red!43}{\strut \:"acquired"}\colorbox{red!32}{\strut \:seems}\colorbox{red!36}{\strut \:incorrect}\colorbox{red!0}{\strut \:in}\colorbox{red!0}{\strut \:the}\colorbox{red!27}{\strut \:given}\colorbox{red!23}{\strut \:sentence}\colorbox{red!0}{\strut \:.}\colorbox{red!0}{\strut \:A}\colorbox{red!39}{\strut \:better}\colorbox{red!20}{\strut \:word}\colorbox{red!0}{\strut \:to}\\\colorbox{red!16}{\strut \:use}\colorbox{red!0}{\strut \:in}\colorbox{red!0}{\strut \:this}\colorbox{red!27}{\strut \:context}\colorbox{red!23}{\strut \:would}\colorbox{red!0}{\strut \:be} \colorbox{red!50}{\strut \:"learned"}\colorbox{red!0}{\strut \:.}\colorbox{red!18}{\strut \:Rewritten}\colorbox{red!20}{\strut \:sentence}\colorbox{red!0}{\strut \::}\colorbox{red!9}{\strut \:"The}\colorbox{red!23}{\strut \:group}\colorbox{red!30}{\strut \:learned}\colorbox{red!0}{\strut \:the}\colorbox{red!25}{\strut \:skill}\colorbox{red!0}{\strut \:to}\colorbox{red!9}{\strut \:ski}\colorbox{red!0}{\strut \:in}\colorbox{red!0}{\strut \:a}\\\colorbox{red!36}{\strut \:suitable}\colorbox{red!23}{\strut \:manner}\colorbox{red!0}{\strut \:."} \\\hline

% Create a compelling tagline that effectively highlights the superior efficiency and optimized performance of a product, service, or process. The tagline should communicate the key benefits of streamlined workflows and cost-effective solutions. It should be catchy and memorable, enhancing brand recognition and fostering positive associations with the company's commitment to innovation and excellence. Ensure that the tagline resonates with the target audience and is concise and clear, avoiding unnecessary complexity or jargon. \\\hline

% \colorbox{red!26}{\strut \:Effortlessly}\colorbox{red!32}{\strut \:Optimized}\colorbox{red!50}{\strut \::}\colorbox{red!0}{\strut \:Streamlining}\colorbox{red!15}{\strut \:Efficiency}\colorbox{red!50}{\strut \:,}\colorbox{red!9}{\strut \:Maximizing}\colorbox{red!21}{\strut \:Value} \\\hline

% \colorbox{red!50}{\strut \:Unleash}\colorbox{red!42}{\strut \:Unmatched}\colorbox{red!25}{\strut \:Efficiency}\colorbox{red!0}{\strut \::}\colorbox{red!44}{\strut \:Experience}\colorbox{red!0}{\strut \:the}\colorbox{red!39}{\strut \:Power}\colorbox{red!0}{\strut \:of}\colorbox{red!39}{\strut \:Streamlined}\colorbox{red!28}{\strut \:Solutions} \\\hline

\textbf{Instruction 2}: Rewrite the given sentence in C\# syntax, using an idiom or metaphor to express dis-\\ belief, and ensure the sentence maintains its original meaning while being grammatically correct in\\ C\#. The sentence to be rewritten is: "The idea that pigs can fly seems more plausible than his expla-\\nation for being late."\\\hline

\textbf{Chosen Response 2}:\colorbox{red!50}{\strut \:csharp\textbackslash n}\colorbox{red!50}{\strut \:using}\colorbox{red!50}{\strut \:System}\colorbox{red!50}{\strut \:;\textbackslash n}\colorbox{red!50}{\strut \:class}\colorbox{red!44}{\strut \:Program\textbackslash n}\colorbox{red!50}{\strut \:\{\textbackslash n}\colorbox{red!50}{\strut \:static}\colorbox{red!50}{\strut \:void}\colorbox{red!44}{\strut \:Main}\colorbox{red!50}{\strut \:(}\colorbox{red!50}{\strut \:string}\colorbox{red!39}{\strut \:[]}\colorbox{red!50}{\strut \:args}\\\colorbox{red!50}{\strut \:\textbackslash n)}\colorbox{red!44}{\strut \:\{\textbackslash n}\colorbox{red!50}{\strut \:Console.WriteLine}\colorbox{red!50}{\strut \:(}\colorbox{red!36}{\strut \:"His}\colorbox{red!19}{\strut \:explanation}\colorbox{red!50}{\strut \:for}\colorbox{red!50}{\strut \:being}\colorbox{red!36}{\strut \:late}\colorbox{red!50}{\strut \:is}\colorbox{red!50}{\strut \:so}\colorbox{red!22}{\strut \:baffling}\colorbox{red!50}{\strut \:that}\colorbox{red!31}{\strut \:it's}\colorbox{red!0}{\strut \:akin}\colorbox{red!50}{\strut \:to}\colorbox{red!31}{\strut \:saying}\colorbox{red!39}{\strut \:pigs}\\\colorbox{red!50}{\strut \:can}\colorbox{red!36}{\strut \:fly}\colorbox{red!50}{\strut \:,}\colorbox{red!19}{\strut \:making}\colorbox{red!50}{\strut \:it}\colorbox{red!33}{\strut \:more}\colorbox{red!8}{\strut \:believable}\colorbox{red!50}{\strut \:in}\colorbox{red!25}{\strut \:comparison}\colorbox{red!44}{\strut \:.}\colorbox{red!50}{\strut \:)}\colorbox{red!50}{\strut \:;\textbackslash n}\colorbox{red!50}{\strut \:\}\textbackslash n}\colorbox{red!50}{\strut \:\}} \\\hline
\textbf{Rejected Response 2}:\colorbox{red!50}{\strut \:Wow}\colorbox{red!0}{\strut \:,}\colorbox{red!15}{\strut \:it's}\colorbox{red!30}{\strut \:truly}\colorbox{red!39}{\strut \:hard}\colorbox{red!0}{\strut \:to}\colorbox{red!20}{\strut \:believe}\colorbox{red!0}{\strut \:that}\colorbox{red!9}{\strut \:pigs}\colorbox{red!33}{\strut \:flying}\colorbox{red!20}{\strut \:could}\colorbox{red!0}{\strut \:be}\colorbox{red!11}{\strut \:more}\colorbox{red!39}{\strut \:plausible}\colorbox{red!0}{\strut \:than}\\\colorbox{red!15}{\strut \:my}\colorbox{red!22}{\strut \:explanation}\colorbox{red!4}{\strut \:for}\colorbox{red!0}{\strut \:being}\colorbox{red!11}{\strut \:late}\colorbox{red!4}{\strut \:!}\\\hline

\end{tabular}}
\caption{Visualization of dynamic weights derived for chosen and rejected responses, based on our proposed calibrated entropy score. We select two samples from the datasets as an illustration.}
\label{tab:case-study}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prompt-decom.png}
    \caption{The prompt template used for instruction decomposition.}
    \label{fig: prompt-decom}
    \vspace{-1mm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prompt-recomb.png}
    \caption{The prompt template used for constraint recombination.}
    \label{fig: prompt-recomb}
    \vspace{-1mm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prompt-selfinst.png}
    \caption{The prompt template used for self-instruct.}
    \label{fig: prompt-selfinst}
    \vspace{-1mm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prompt-sub.png}
    \caption{The prompt template used for constraint substitution.}
    \label{fig: prompt-sub}
    \vspace{-1mm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/prompt-neg.png}
    \caption{The prompt template used for constraint negation.}
    \label{fig: prompt-neg}
    \vspace{-1mm}
\end{figure}