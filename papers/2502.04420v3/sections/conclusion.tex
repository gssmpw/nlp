\section{Conclusion}
KVTuner enables efficient and adaptive layer-wise mixed-precision KV cache quantization via sensitivity-aware optimization techniques. It systematically reduces KV cache quantization errors by prioritizing key cache precision while balancing memory efficiency and inference accuracy. Experimental results demonstrate that KVTuner achieves nearly lossless compression at 3.25-bit for Llama-3.1-8B-Instruct and 4-bit for sensitive Qwen2.5-7B-Instruct.  
KVTuner also demonstrates that employing longer CoTs with lower and mixed precision KV cache quantization yields superior performance compared to shorter CoTs utilizing higher precision KV cache. This improvement is evident in both memory efficiency and accuracy, particularly in the context of mathematical reasoning tasks. KVTuner also greatly narrows the performance difference between the simple per-token-asym and accurate KIVI quantization modes, even when using overall similar low-precision settings.







