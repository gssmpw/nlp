\section{Experimental results}
The detailed experimental settings are available in Section \ref{sec:experimental_settings}. The intra-layer and inter-layer KV precision pairs pruning results of various LLMs are available in Appendix \ref{sec:intra_layer_inter_layer_pruning_kvcache_quantization_precision_pairs}. The proposed pruning algorithm can significantly reduce the search space to ${S_p}^G$ and speedup convergence of MOO search. The final model accuracy on mathematical reasoning datasets and the throughput improvement validate the effectiveness of KVTuner.

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c c r l c}
\toprule
\multicolumn{1}{c}{Model name} & L & Key quant. mode & \multicolumn{1}{c}{KV cache precision pairs} & \multicolumn{1}{c}{Layer ids} \\ \hline
\multirow{3}{*}{Llama-3.1-8B-Instruct} & \multirow{3}{*}{32} & per-token-asym & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & 0 \\ \cline{3-5}
 &  & \multirow{2}{*}{per-channel-asym} & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2  & 0\\
 &  &  &  KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & 1, 2, 3, 7, 29, 31 \\
 \hline
\multirow{3}{*}{Mistral-7B-Instruct-v0.3} & \multirow{3}{*}{32} & per-token-asym & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0 \\ \cline{3-5}
 & & \multirow{2}{*}{per-channel-asym} & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0 \\
  & & & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & $G_1$ \\\hline
  \multirow{4}{*}{Qwen2.5-3B-Instruct} & \multirow{4}{*}{36} & \multirow{2}{*}{per-token-asym} & KV8, K8V4, \textcolor{blue}{\textit{K8V2}}, K4V2, KV2 & 0  \\
  & & & KV8, K8V4, \textcolor{blue}{\textit{K8V2}}, KV4, K4V2, KV2 & 18, 27, 29\\ \cline{3-5}
 & & \multirow{2}{*}{per-channel-asym} & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0, 1, 2, 4, 34, 35\\
 & & & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2  & 3, 6, 11, 13, 23 \\ \hline
\multirow{4}{*}{Qwen2.5-7B-Instruct}  & \multirow{4}{*}{28} & \multirow{2}{*}{per-token-asym} & KV8, K8V4, \textcolor{blue}{\textit{K8V2}}, K4V2, KV2 & 0 \\ 
 & & & KV8, K8V4, \textcolor{blue}{\textit{K8V2}}, KV4, K4V2, KV2 & 3, 13, 27 \\ \cline{3-5}
 & & per-channel-asym & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0, 1, 2, 3 \\
 & & &  KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & 6 \\ \hline
 \multirow{3}{*}{Qwen2.5-14B-Instruct} & \multirow{3}{*}{48} & per-token-asym & \multicolumn{2}{c}{None} \\ \cline{3-5}
 & & \multirow{2}{*}{per-channel-asym} & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0, 1, 2, 3, 4  \\ 
  & & & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & 5, 6, 8, 9, 12 \\ \hline
  \multirow{4}{*}{Qwen2.5-32B-Instruct} & \multirow{4}{*}{64} & per-token-asym &  \multicolumn{2}{c}{None}  \\ \cline{3-5}
 & & \multirow{3}{*}{per-channel-asym} &  KV8, \textcolor{red}{\textbf{K4V8}}, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 0, 1, 2, 3, 4, 11  \\
  & & & KV8, \textcolor{red}{\textbf{K4V8}}, KV4, K4V2, KV2 & $G_2$ \\
  & & & KV8, K8V4, KV4, \textcolor{red}{\textbf{K2V4}}, KV2 & 63\\
\bottomrule
\end{tabular}
}
\caption{Intra-layer KV cache quantization precision pair pruning results of special transformer layers. The pruned Pateto efficient KV cache precision pairs in most layers are \{KV8, K8V4, KV4, K4V2, KV2\}, so we omit them in the table. Value is always quantized with the per-token-asym mode.  $G_1$ of Mistral-7B-Instruct-v0.3 is 2$\sim$4, 6, 7$\sim$10, 14, 18, 27, 29. $G_2$ of Qwen2.5-32B-Instruct is $5\sim10, 12, 14, 16, 18\sim21, 23, 26\sim28, 32$.}
\label{tab:intra_layer_kvcache_quantization_precision_pairs}
% \vspace{-1.5em}
\end{table}

\subsection{Pareto-optimal KV Cache Precision Pair Search}
\begin{figure}
    \centering
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/optuna_search/optuna_llama3_adaptive_new_1k_per_channel.pdf}
        \vspace{-1em}
        \caption{Llama-3.1-8B-Instruct with KIVI}
        \label{fig:pareto_frontier_per_channel_asym_gsm8k_limit_200_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figs/optuna_search/optuna_qwen2_7b_adaptive_new_1k_per_token.pdf}
        \vspace{-1em}
        \caption{Qwen2.5-7B-Instruct with per-token-asym}
        \label{fig:pareto_frontier_per_token_asym_gsm8k_limit_200_qwen2.5_7b_instruct}
    \end{subfigure}
    % \vspace{-1em}
    \caption{Pareto frontier on the first 200 GSM8K 4-shot prompts.}
    \label{fig:pareto_frontier_comparison}
    % \vspace{-2.5em}
\end{figure}
\textbf{KIVI.} The mixed precision KIVI quantization mode can maintain high accuracy. As shown in Figure \ref{fig:pareto_frontier_per_channel_asym_gsm8k_limit_200_Llama-3.1-8B-Instruct}, 
KVTuner with KIVI effectively maintains Llama-3.1-8B-Instruct performance while reducing the equivalent quantization precision to 3.06-bit.
In addition, KVTuner also finds out four settings including lower-precision 4.91-bit in the Pareto frontier whose memory usage and accuracy are better than KV8. Most sampled settings are close to the Pareto frontier, indicating that Llama-3.1-8B-Instruct is more robust to low-precision KV quantization. These demonstrate that KVTuner increases the flexibility of KV cache quantization and can achieve lower precision and even better precision than uniform KV precision. 

\textbf{Per-token-asym.} According to Figure \ref{fig:pareto_frontier_per_token_asym_gsm8k_limit_200_qwen2.5_7b_instruct}, when using the per-token-asym quantization mode on the sensitive Qwen2.5-7B-Instruct model, the Pareto frontier identified by KVTuner consistently outperforms uniform precision quantization. Especially, KVTuner can achieve KV8 accuracy with the equivalent 3.92-bit KV precision, while the uniform KV4 accuracy significantly degrades to around $0\%$. 
Therefore, even leveraging the simple and commonly used per-token-asym mode \cite{lin2024qserve,sheng2023flexgen}, KVTuner can reduce the memory footprint with the maintained accuracy of models with high knowledge density.

\subsection{LLM Mathematical and Scientific Reasoning Accuracy}
\label{sec:llm_mathmetical_reasoning_accuracy}
Apart the in-context few-shot GSM8K datasets, we also  utilize them as the internal reasoning steps in a multi-turn way to imitate Openai o1 like reasoning systems in Table \ref{tab:gsm8k_cot_and_multiturn_kvtuner_results_kivi}.
KIVI-2 and KIVI-4 result in dramatic accuracy loss in Qwen2.5-\{3B, 7B\}-Instruct due to their high sensitivity to low-precision KV quantization. KVTuner with KIVI can nearly losslessly quantizate KV cache to 3.25-bit, 3.17-bit, and 5.96-bit of the three models, respectively, further reducing the memory footprint compared with KIVI-4 and KIVI-8. In addition, we find out an interesting observation: KVTuner enables longer context and lower KV precision for better CoT and multi-turn mathematical reasoning accuracy than short-context and original BF16 precision KV. Most LLMs benefit from longer CoT and KVTuner enables nearly lossless lower-precision KV quantization.
\begin{table}[ht]
\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{ c | r r r r r r | r }
\toprule
\multirow{2}{*}{Precision} &  \multicolumn{3}{c}{Few-shot CoT} & \multicolumn{3}{c}{Few-shot CoT as Multiturn} &  \multirow{2}{*}{Average} \\ 
& 4-shot & 8-shot & 16-shot & 4-shot & 8-shot & 16-shot & \\ \hline
\multicolumn{8}{c}{\textbf{Llama-3.1-8B-Instruct}} \\ \hline
BF16 & 0.7635 & 0.7741 & 0.7854 & 0.8355 & 0.8309 & 0.8332 & 0.8038 \\ \hline
KIVI-8 & 0.7733 & 0.7748 & 0.7756 & 0.8347 & 0.8317 & 0.8294 & 0.8033 \\
KIVI-4 & 0.7566 & 0.7718 & 0.7839 & 0.8370 & 0.8241 & 0.8332 & 0.8011 \\
KIVI-2 & 0.6073 & 0.6080 & 0.5929 & 0.6649 & 0.6543 & 0.6687 & 0.6327 \\ \hline
KVTuner-C4.90 & 0.7506 & 0.7665 & 0.7657 & 0.8173 & 0.8188 & 0.8378 & 0.7928 \\
KVTuner-C3.25 & 0.7483 & 0.7566 & 0.7604 & 0.8362 & 0.8256 & 0.8279 & 0.7925 \\ \hline
\multicolumn{8}{c}{\textbf{Qwen2.5-3B-Instruct}} \\ \hline
BF16 & 0.6020 & 0.6490 & 0.7020 & 0.5679 & 0.6005 & 0.6490 & 0.6284 \\ \hline
KIVI-8 & 0.5974 & 0.6619 & 0.7096 & 0.5648 & 0.5989 & 0.6346 & 0.6279 \\
KIVI-4 & 0.6156 & 0.6550 & 0.7066 & 0.5732 & 0.6073 & 0.6414 & 0.6332 \\
KIVI-2 & 0.0546 & 0.0576 & 0.0675 & 0.047 & 0.0478 & 0.0591 & 0.0556 \\ \hline
KVTuner-C3.44 & 0.5989 & 0.6429 & 0.7089 & 0.5701 & 0.5997 & 0.6475 & 0.6280 \\
KVTuner-C3.17 & 0.6065 & 0.6444 & 0.6998 & 0.5512 & 0.5891 & 0.6406 & 0.6219 \\ \hline
\multicolumn{8}{c}{\textbf{Qwen2.5-7B-Instruct}} \\ \hline
BF16 & 0.8059 & 0.8287 & 0.8218 & 0.7081 & 0.7339 & 0.7544 & 0.7755 \\ \hline
KIVI-8 & 0.8021 & 0.8271 & 0.8302 & 0.7066 & 0.7354 & 0.7506 & 0.7753 \\
KIVI-4 & 0.0735 & 0.1137 & 0.1554 & 0.0667 & 0.0705 & 0.1463 & 0.1043 \\
KIVI-2 & 0.0379 & 0.0402 & 0.0356 & 0.0326 & 0.0258 & 0.0235 & 0.0326 \\ \hline
KVTuner-C5.96 & 0.8218 & 0.8309 & 0.8150 & 0.6907 & 0.7248 & 0.7513 & 0.7724 \\
KVTuner-C3.92 & 0.5959 & 0.6664 & 0.6558 & 0.5588 & 0.6156 & 0.6035 & 0.6160 \\ \hline
\end{tabular}
}
% \vspace{-2mm}
\caption{Mathematical reasoning accuracy comparison of different KV cache precision settings with the KIVI quantization mode on the GSM8K few-shot CoT and CoT as multi-turn dataset.}
\label{tab:gsm8k_cot_and_multiturn_kvtuner_results_kivi}
\vspace{-1em}
\end{table}


We extend our evaluation to the GPQA dataset with few-shot CoTs, as detailed in Table \ref{tab:gpqa_extended_results}.
KVTuner successfully enables lower than 4-bit, such as 3.59-bit, KV cache quantization with minimal performance degradation across various models. These results demonstrate the effectiveness of our method in maintaining high mathematical reasoning accuracy while significantly reducing memory usage.


\begin{table}
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{ c | r r r | r || c | r r r | r }
\toprule
\multirow{2}{*}{Precision} &  \multicolumn{3}{c}{GPQA Extended} &  \multirow{2}{*}{Average} & \multirow{2}{*}{Precision} &  \multicolumn{3}{c}{GPQA Extended} &  \multirow{2}{*}{Average} \\
& 5-shot & 10-shot & 20-shot &  & & 5-shot & 10-shot & 20-shot &  \\ \hline
\multicolumn{5}{c}{\textbf{Llama-3.1-8B-Instruct}} & \multicolumn{5}{c}{\textbf{Mistral-7B-Instruct-v0.3}} \\ \hline
BF16 & 0.3095 & 0.3114 & 0.2985 & 0.3065 & BF16 & 0.2930 & 0.2784 & 0.2766 & 0.2827 \\ \hline
KV8 & 0.3242 & 0.3022 & 0.3059 & 0.3108 & KV8 & 0.2985 & 0.2839 & 0.2784 & 0.2869 \\
KV4 & 0.3095 & 0.3168 & 0.3077 & 0.3113 & KV4 & 0.3040 & 0.2839 & 0.3022 & 0.2967 \\
KV2 & 0.1996 & 0.2198 & 0.2473 & 0.2222 & KV2 & 0.2857 & 0.2106 & 0.2344 & 0.2436 \\ \hline
KVTuner-C5.43 & 0.3187 & 0.3077 & 0.3187 & 0.3150 & KVTuner-C5.38 & 0.3004 & 0.2839 & 0.2912 & 0.2918 \\
KVTuner-C3.59 & 0.3223 & 0.3205 & 0.3059 & 0.3162 & KVTuner-C3.78 & 0.3260 & 0.2857 & 0.3040 & 0.3052 \\ \midrule
\multicolumn{5}{c}{\textbf{Qwen2.5-3B-Instruct}} & \multicolumn{5}{c}{\textbf{Qwen2.5-7B-Instruct}} \\ \hline
BF16 & 0.3059 & 0.3095 & 0.3150 & 0.3101 & BF16 & 0.3168 & 0.3352 & 0.3297 & 0.3272 \\ \hline
KV8 & 0.3095 & 0.3059 & 0.3187 & 0.3114 & KV8 & 0.3242 & 0.3333 & 0.3407 & 0.3327 \\
KV4 & 0.2564 & 0.2711 & 0.2692 & 0.2656 & KV4 & 0.0586 & 0.0641 & 0.0751 & 0.0659 \\
KV2 & 0.0971 & 0.0806 & 0.1026 & 0.0934 & KV2 & 0.2216 & 0.1941 & 0.1996 & 0.2051 \\ \hline
KVTuner-C5.06 & 0.2985 & 0.3040 & 0.3278 & 0.3101 & KVTuner-C5.0 & 0.3315 & 0.3297 & 0.3187 & 0.3266 \\
KVTuner-C3.64 & 0.2949 & 0.3059 & 0.2985 & 0.2998 & KVTuner-C4.0 & 0.3333 & 0.3223 & 0.3205 & 0.3254 \\ \bottomrule
\end{tabular}
}
% \vspace{-2mm}
\caption{Scientific reasoning accuracy comparison of different KV cache precision settings with the per-token-asym KV quantization mode on the GPQA Extended dataset.}
\label{tab:gpqa_extended_results}
\vspace{-1em}
\end{table}


\subsection{Throughput}
We measure the maximum throughput and the corresponding batch size under specific input prompt length with the implementation of the KIVI GPU kernel. 
The Llama2-7B model and the output length 128 are used to align with KIVI. 
As shown in Table \ref{tab:max_batch_size_throughput}, for context length ranging from 512 to 4096, the high accuracy mode KVTuner-C6 can achieve $26.7\%\sim38.3\%$ throughput improvement and the high efficiency KVTuner-C3 can achieve $58.3\%\sim76.4\%$ throughput improvement than KV8. %\vspace{-1em}
\begin{table}[h]
\centering
\resizebox{0.65\columnwidth}{!}{
\begin{tabular}{ r r r r r r }
\toprule
$S$ & FP16 & KV8 & KVTuner-C6 & KV4  & KVTuner-C3 \\ \hline
512 & 224 {\scriptsize \textcolor{gray}{(8)}} & 394 {\scriptsize \textcolor{gray}{(32)}} & 529 {\scriptsize \textcolor{gray}{(60)}} & 618 {\scriptsize \textcolor{gray}{(64)}} & 679 {\scriptsize \textcolor{gray}{(64)}} \\ \hline
1K & 129 {\scriptsize \textcolor{gray}{(8)}} & 214 {\scriptsize \textcolor{gray}{(20)}} & 296 {\scriptsize \textcolor{gray}{(32)}} & 332 {\scriptsize \textcolor{gray}{(32)}} & 365 {\scriptsize \textcolor{gray}{(32)}} \\ \hline
2K & 92 {\scriptsize \textcolor{gray}{(8)}} & 106 {\scriptsize \textcolor{gray}{(12)}} & 143 {\scriptsize \textcolor{gray}{(16)}} & 168 {\scriptsize \textcolor{gray}{(16)}} & 187 {\scriptsize \textcolor{gray}{(16)}}   \\ \hline
4K & 49 {\scriptsize \textcolor{gray}{(4)}} & 60 {\scriptsize \textcolor{gray}{(8)}} & 76 {\scriptsize \textcolor{gray}{(8)}} & 83 {\scriptsize \textcolor{gray}{(8)}} & 95 {\scriptsize \textcolor{gray}{(12)}}   \\ \hline
8K & OOM {\scriptsize \textcolor{gray}{(4)}} & 43 {\scriptsize \textcolor{gray}{(4)}} & 44 {\scriptsize \textcolor{gray}{(8)}} & 45 {\scriptsize \textcolor{gray}{(4)}} & 62 {\scriptsize \textcolor{gray}{(8)}}   \\ 
\bottomrule
\end{tabular}
}
\caption{Maximum throughput {\scriptsize\textcolor{gray}{(and the corresponding batch size)}} of Llama2-7B on a Nvidia L40 48GB GPU with optimized KIVI KV cache quantization and dequantization CUDA kernels.}
\label{tab:max_batch_size_throughput}
\vspace{-2em}
\end{table}

\subsection{Detailed Analysis}
\label{sec:detailed_analysis_layer_wise_selection}

By analyzing the detailed configurations in the Pareto frontier identified for Llama-3.1-8B-Instruct, we observe that:
% \vspace{-4mm}

\begin{itemize}
\item  In most cases, all layer groups adopt a quantization configuration where the precision of the key is higher than the precision of the value. This supports our earlier observation from uniform quantization that the key plays a more critical role in quantization.
% \vspace{-2mm}

\item In other cases, in certain specialized layer groups, the value is set at a higher precision than the key for certain specialized layer groups. This aligns with the patterns identified in Table \ref{tab:intra_layer_kvcache_quantization_precision_pairs}, which highlight specific layer groups may require higher precision for values.
% \vspace{-2mm}

\item KVTuner tends to allocate higher precision to groups with larger quantization errors. Reducing the quantization precision of the key for a crucial group of layers can significantly degrade the performance. For instance, in Llama-3.1-8B-Instruct, the layer group [$8\sim11$, $14\sim17$, 20, 30] is particularly sensitive to the reduction of the key precision, and if the precision of the key is reduced from 4-bit to 2-bit, the performance would drop from 0.67 to 0.495.
\end{itemize}
% \vspace{-2mm}

\subsection{Ablation Studies}

\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{figs/optuna_search/optuna_llama3_brute_force_new_1k_per_token.pdf}
    % \vspace{-1.5em}
    \caption{Pareto frontier of Llama-3.1-8B-Instruct with the per-token-asym KV quantization mode and without the proposed two-stage search space pruning on the first 200 GSM8k 4-shot prompts}
    \label{fig:pareto_frontier_per_token_asym_gsm8k_limit_200_llama3_instruct_brute_force}
    % \vspace{-1em}
\end{figure}

According to Figure \ref{fig:pareto_frontier_per_token_asym_gsm8k_limit_200_llama3_instruct_brute_force}, when using the per-token-asym quantization mode on the Llama-3.1-8B-Instruct model, the search results deteriorate significantly if the proposed intra-layer and inter-layer search space pruning algorithms are not applied. In comparison with the counterpart with search space pruning as pre-processing in Figure \ref{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-llama3}, this highlights search space pruning is helpful for MOO search convergence and maintaining quantization performance.
