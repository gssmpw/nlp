\section{Method}
\label{sec:kvtuner}
KVTuner is an adaptive tuning framework for hardware-friendly mixed-precision KV cache quantization. It optimizes layer-wise KV precision pairs by considering their inherent sensitivity properties, aiming to achieve a better trade-off between inference efficiency and model accuracy.

Instead of making online decisions about fine-grained token or page-level KV cache quantization precision for improved model accuracy, we conduct offline search to identify the Pareto-optimal quantization precision settings for coarse-grained KV cache in each transformer layer using multi-objective optimization algorithms. Here, we refer to the entire low-bit KV cache being quantized with a specific precision pair, such as K8V4 or K4V2. This approach ensures that no additional overhead is introduced during dynamic quantization and online inference.
Due to the flexibility introduced by layer-wise KV cache quantization precision tuning, KVTuner is able to accommodate more hardware and accuracy constraints of different deployed LLMs compared to uniform 8-bit or even lower precision quantization. Moreover, KVTuner accelerates LLM inference and reduces memory footprint, while still maintaining lossless or slightly lossy final model generation.

\subsection{Problem Formulation}
The offline layer-wise KV precision pair tuning problem can be formulated as a discrete combinatorial optimization task, considering hardware limitations and accuracy loss constraints. It can be solved using multi-objective optimization algorithms.
We aim to minimize the quantized KV cache memory usage across all transformer layers while minimizing the final model accuracy loss, subject to the maximum $M$ memory and $\Delta A$ accuracy loss constraints:
\begin{equation}
\!\!\!\min_{\mathbf{P}}\left(f_m(\mathbf{P}), f_a(\mathbf{P})\right) \;  \mbox{s.t.} \; f_m(\mathbf{P}) \leq M, f_a(\mathbf{P}) \leq \Delta A,
\end{equation}
where the search space $\mathbf{P} \in S^{L}$ is the KV cache precision pairs in $L$ layers. The layer-wise search space $S$ is defined as the KV cache precision pair ($P^l_k$, $P^l_v$) in the $l$-th layer. $f_m(\mathbf{P}) = \frac{\sum(\mathbf{P})}{2L}$ captures the average equivalent quantization bits of all KV cache, $f_a(\mathbf{P}) = A_{LLM}(KV_{half}) - A_{LLM}(KV_{\mathbf{P}})$ measures the final LLM accuracy loss with the KV precision as $\mathbf{P}$ compared with LLM inference using 16-bit half precision KV cache. For instance, we can limit the average KV cache quantization precision to 2.5-bit, while optimizing the equivalent quantization precision and inference accuracy.

\subsection{Framework}
To reduce the overhead of online fine-grained KV cache mixed-precision quantization tuning, we propose offline calibration of the optimal coarse-grained KV cache quantization precision pairs for each layer or head using multi-objective optimization algorithms \cite{akiba2019optuna, zhang2007moea}. These pre-calibrated settings are then directly applied during online quantization. The efficiency of offline calibration is crucial for practical applications due to the large combinatorial search space of KV cache quantization pairs across multiple transformer layers. Therefore, as demonstrated in Figure \ref{fig:kvtuner_framework}, we propose the intra-layer and inter-layer search space pruning algorithms to accelerate the search process while preserving optimization opportunities. After the efficient preprocessing, the final LLM inference accuracy is utilized to search the Pareto optimal layer-wise KV precision pairs $\mathbf{P}$ capturing complex dependencies of the nonlinear error accumulation.
\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figs/kvtuner_framework.pdf}\vspace{-2mm}    
    \caption{The layer-wise KV cache quantization tuning framework KVTuner with two-stage search space pruning for efficient MOO search using the final memory and model accuracy.}
    \label{fig:kvtuner_framework}\vspace{-2em}
\end{figure}

\subsection{Automatic KV Cache Quantization Precision Pair Search}
As analyzed in Section \ref{sec:kvcache_sensitivity_to_quantization_mode_precision} and \ref{sec:why_key_are_more_important_in_kvcache_quantization}, the model-wise and layer-wise sensitivity to KV cache quantization mode and precision is the inherent model property and is independent of the input prompts. Therefore, we can search for the optimal layer-wise KV cache quantization precision pairs offline to eliminate the additional online decision-making overhead with high generalization. If the candidate layer-wise KV precision pairs are $\{2, 4, 8\} \times \{2, 4, 8\}$, then the number of possible combinations is $9^{L}$, where the $L$ is the number of transformer layers. For example, the Llama-3.1-8B-Instruct model with 32 layers has about $3.4 \times 10^{30}$, which is intractable. Therefore, we design the following two-level search space pruning algorithm to reduce $\mathbf{P}$ from ${S}^{L}$ to ${S_p}^{G}$, where $S_p$ is the pruned candidate set in a group and $G$ is the number of clustered layer groups.


\subsubsection*{Intra-layer KV Cache Quantization Precision Pair Pruning}
KV cache quantization errors in each layer accumulate across both the model layers and generation token dimensions. Therefore, we must control the layer-wise error by pruning KV cache quantization pairs to limit the final model error. For all candidate KV cache quantization pairs in each layer, we prune those that are not part of the Pareto frontier, considering both the equivalent KV cache quantization precision and the relative attention output errors. For example, the precision pairs KV8, K8V4, KV4, K4V2, and KV2 are Pareto efficient for most layers in Llama-3.1-8B-Instruct in Figure \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b}, except for the 0-th layer, where K4V8 results in smaller errors than K8V4.

\subsubsection*{Inter-layer Clustering}
Although the above intra-layer pruning already significantly reduces the search space to ${S_p}^L$ such as $5^{32} \approx 2.3\times 10^{22}$ in Llama-3.1-8B-Instruct, it is still too computationally costly for searching. Therefore, we further propose the inter-layer clustering algorithm based on relative attention output errors and the pruned candidate KV quantization pairs to ${S_p}^{G}$ such as $5^6=15625$.
The initial step involves partitioning layers based on distinct candidate sets of pruned KV cache quantization precision pairs. These candidate sets serve as indicators of how individual layers respond differently to specific KV cache quantization precision configurations.
The subsequent step involves clustering layers that share the same candidate set, using quantization sensitivity as the clustering metric. This sensitivity is quantified with the relative attention output errors produced by the pruned precision pairs.

\subsubsection*{Calibration Dataset Design}
To effectively evaluate different quantization settings, we develop an approach that amplifies KV cache quantization error accumulation and distinguishes the performance of KV precision pairs during the calibration process. This approach utilizes dequantized KV cache for self-attention computation during the prefilling stage, enabling error accumulation across model layers. 
Furthermore, we utilize long-context generation and challenging calibration datasets such as mathematical reasoning. 
In these tasks, minor errors propagating in decoding steps may result in intermediate generation token flipping and substantial mistakes in final answers as demonstrated by Table \ref{tab:error_accumulation_kivi_low_bit_example}.