\section{Introduction}
Large language models (LLMs) and multi-modality large models can comprehend and generate text, audio, image, and video like humans, showing the strong capability of assisting and interacting with humans.
LLM inference efficiency such as throughput and latency is critical to enhance user experience and reduce cost.
To improve the inference efficiency of LLMs, previously processed KV tokens are cached to avoid redundant recomputation. However, the memory usage of the KV cache linearly grows with the number of batch size and sequence length, so the KV cache becomes the new bottleneck of LLM serving systems with large batching requests and long context. 
Valuable long context generation applications include multi-turn dialogues, long document understanding, and OpenAI o1-like level-2 reasoning. Commercial companies are releasing their supports for long context generation and KV cache-based services like prompt caching for better capability and efficiency  \cite{OpenAI2024PromptCache, DeepSeek2024ContextCache}. 
Efficient KV cache management and compression can accelerate LLM inference and reduce hardware resource consumption, making it a foundational technique for advancing both enterprise-scale LLM deployment and personalized AI agents. 

\begin{figure}
\centering
    \begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{figs/token_attention_distribution_shift_figsize_54/Qwen2.5-7B-Instruct_kvquant_token_attention_error_fp16_gsm8k_from_train_nshot_0_prompt_00_layer_00_head_02.pdf}
    \caption{Layer-0 query head-2}
    \label{fig:attention_distribution_Qwen2.5-7B-Instruct_gsm8k_zeroshot_first_prompt_layer_2}
    \end{subfigure}\hspace{5mm}
    \begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{figs/token_attention_distribution_shift_figsize_54/Qwen2.5-7B-Instruct_kvquant_token_attention_error_fp16_gsm8k_from_train_nshot_0_prompt_00_layer_21_head_04.pdf}
    \caption{Layer-21 query head-4}
    \label{fig:attention_distribution_Qwen2.5-7B-Instruct_gsm8k_zeroshot_first_prompt_layer_21}
    \end{subfigure}
    \caption{Token-level attention score of the 79-th query token to previous key tokens with the per-token-asym key cache quantization (Qwen2.5-7B-Instruct, GSM8k). Low-precision KV quantization (4-bit and 2-bit) causes significant distribution shifts, resulting in errors of missing or incorrect critical key identification.}
    \label{fig:token_level_attention_distribution_shift_per_token_asym_quant_Qwen2.5-7B-Instruct}
    % \vspace{-5mm}
\end{figure}

KV cache quantization is one of the most stable and easily deployable KV cache compression methods to reduce the memory footprint and improve throughput \cite{yuan2024kvcompression}. INT8/FP8 KV cache with dynamic asymmetric token-wise (per-token-asym) or channel-wise (per-channel-asym) quantization can achieve lossless compression in most practical applications. However, lower-bit KV cache quantization easily leads to model accuracy degradation. 

Intra-layer mixed precision KV quantization methods retain important KV tokens with high precision to reduce KV cache quantization errors and quantize other cache in the same layer with uniformly low precision such as 2-bit. KIVI \cite{liu2024kivi}, IntactKV \cite{liu2024intactkv}, and KVQuant \cite{hooper2024kvquant} statically keep prefix and initial KV cache blocks with high precision. 
They need specially designed operators for hardware like GPUs and require more careful KV cache management. Besides, the assumption that the static prefix and recent KV is important may not always hold as demonstrated in Figure \ref{fig:token_level_attention_distribution_shift_per_token_asym_quant_Qwen2.5-7B-Instruct}, where low-precision quantization (4-bit and 2-bit) leads to dramatic attention distribution shift in sensitive models like Qwen2.5-7B-Instruct. Existing static and uniform KV precision methods including KIVI 4-bit cannot effectively handle these non-sparse retrieval heads. The only viable and efficient solution is to increase KV cache quantization precision of the whole model or some critical and sensitive layers.

In contrast, fine-grained methods, such as QAQ \cite{dong2024qaq}, MiKV \cite{yang2024mikv}, and ZipCache \cite{he2024zipcache}, dynamically identify critical KV cache and update their precision on-the-fly to improve accuracy. 
However, they cannot be easily integrated with flash attention \cite{dao2022flashattention} and vLLM \cite{kwon2023vllm}, because of the intra-layer fine-grained KV cache precision difference and additional deployment efforts. In addition, the online computation and control flow logic for critical token identification introduce overhead and do not fit into static graph-based inference acceleration methods.% like torch.compile.
  

There are still several issues to improve the inference throughput and maximum supported context length with KV cache quantization under constrained hardware resources: 
1) 
Can we further almost losslessly compress KV cache with hardware-friendly and mixed precision quantization in a plug-and-play way?
2) Are there any other inherent model properties such as attention patterns \cite{tang2024razorattention, xiao2024duoattention} that can help better trade-off memory reduction and model accuracy?
3) There are normally multiple deployed LLMs in the industrial service systems and Artificial Intelligence (AI) agents. How to adaptively tune the KV cache quantization precision considering the accuracy requirement of requests and the LLM sensitivity to KV cache quantization?

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{p{0.55in}|p{9in}}\hline
\toprule
KV cache precision & Results with 15-th input prompt \RaggedRight{\textit{In a dance class of 20 students, 20$\%$ enrolled in contemporary dance, 25$\%$ of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?}} \\
\hline
FP16 & \RaggedRight{Let X be the number of students enrolled in hip-hop dance. The number of students enrolled in contemporary dance is 20$\%$ of 20 = 4. The number of students enrolled in jazz dance is 25$\%$ of the remaining = 25$\%$ of (20 - 4) = 25$\%$ of 16 = 4.
Therefore, the number of students enrolled in hip-hop dance is \colorbox{green!30}{20 - 4 - 4 = <<20-4-4=12>>12}. The percentage of the entire students enrolled in hip-hop dance is \colorbox{green!30}{12/20 x 100$\%$ = 60$\%$}. \#\#\#\# 60$\%$.} \\
KIVI-4 & \RaggedRight{Let X be the number of students enrolled in hip-hop dance. The number of students enrolled in contemporary dance is 20$\%$ of 20 = 4. The number of students enrolled in jazz dance is 25$\%$ of the remaining = 25$\%$ of (20 - 4) = 25$\%$ of 16 = 4. Therefore, the number of students enrolled in hip-hop dance is \colorbox{green!30}{20 - 4 - 4 = <<20-4-4=12>>12}. The percentage of the entire students enrolled in hip-hop dance is \colorbox{green!30}{12/20 x 100$\%$ = 60$\%$}.  \#\#\#\# 60$\%$.} \\
KIVI-2 & \RaggedRight{Let X be the number of students \colorbox{blue!30}{who} enrolled in hip-hop dance. The number of students \colorbox{blue!30}{who} enrolled in contemporary dance is 20$\%$ of 20 = 4. The number of students \colorbox{blue!30}{who} enrolled in jazz dance is 25$\%$ of \colorbox{blue!30}{16} = 4. Therefore, the total number of students \colorbox{blue!30}{who} enrolled in hip-hop dance is \colorbox{red!30}{20 + 4 + 4 = 28}. The percentage of the entire students \colorbox{blue!30}{who} enrolled in hip-hop dance is \colorbox{red!30}{28/20 = <<28/20=14>>14$\%$}. \#\#\#\# 14.} \\\bottomrule
\end{tabular}
}
% \vspace{-1em}
\caption{The error accumulation caused by low-bit KV cache quantization (KIVI-2) potentially leads to wrong responses of mathematical reasoning tasks. The input prompt is combined with 15-shot CoT from GSM8k training dataset for Llama2-13B-chat-hf model generation. The official KIVI CUDA kernel and code are used to generate the output.}
\label{tab:error_accumulation_kivi_low_bit_example}
% \vspace{-1em}
\end{table*}

To address these issues, we thoroughly study the sensitivity of LLM transformer layers to KV cache quantization and find out that error accumulation caused by KV cache quantization is strongly correlated with attention patterns in Section \ref{sec:error_accumulation} and \ref{sec:attention_patterns_layer_sensitivity}. According to our observation of the sensitivity of key and value cache in the same layer in Section \ref{sec:kvcache_sensitivity_to_quantization_mode_precision} and \ref{sec:why_key_are_more_important_in_kvcache_quantization} and the layer-wise difference of transformer layers in Section \ref{sec:layer_wise_sensitivity_kvcache_quant}, we propose to quantize coarse-grained key and value cache in the same layer with different precision and automatically search for the optimal layer-wise KV cache quantization precision pairs based on the inherent importance of intermediate layers in Section \ref{sec:kvtuner}. During online serving, the offline calibrated layer-wise KV cache quantization precision pairs are directly loaded without any additional overhead to improve inference throughput and latency.
Our contributions are summarized as follows:
% \vspace{-4mm}
\begin{itemize}
    \item We study the underlying mechanism of why key cache normally is more important than value cache. The LLM accuracy degradation with low-bit key cache quantization is mainly caused by error accumulation and the layer-wise attention error distribution shift. We find out that the sensitivity of LLMs and intermediate layers to KV cache quantization is the model property and independent of input prompts. 
    % \vspace{-2mm}
    \item We propose to automatically search for the hardware-friendly layer-wise KV cache precision pairs such as K8V4 and K4V2 with multi-objective optimization (MOO) under certain memory or accuracy constraints for efficient online inference. The intra-layer pruning and inter-layer clustering are used to significantly reduce the search space and the offline tuning cost. 
    % \vspace{-2mm}
    \item We empirically demonstrate that our mixed-precision KV tuning framework KVTuner can achieve almost lossless KV cache quantization with equivalent 4-bit even 3.25-bit precision in mathematical reasoning tasks for most LLMs including the sensitive Qwen2.5-7B-Instruct with 38.3\% inference throughput improvement.
\end{itemize}
