\appendix
\onecolumn
% \section{Appendix}
\section{Proof of Lemma \ref{lemma_attention_pattern_kvquant_error}}
\label{sec:proof_of_lemma_kvquant_error_attention_patterns}
Lemma \ref{lemma_attention_pattern_kvquant_error} claims that only attention heads with sparse and concentrated patterns demonstrate consistent robustness to low-precision KV cache quantization. Its proof is below.
\begin{proof}
Given the query token $\boldsymbol{q} \in \mathbb{R}^{1  \times D}$ and key cache $\boldsymbol{K} \in \mathbb{R}^{D \times S}$, the attention score without errors is $a_i = \frac{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i)}{\sum_{j=1}^{S} \mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j)}$. The key asymmetric uniform quantization error $\Delta \boldsymbol{K} \in \mathbb{R}^{S \times D} \sim \mathcal{N}(0, \,\sigma^{2}) $ follows normal distribution, where $\sigma = \frac{max(\boldsymbol{K})-min(\boldsymbol{K})}{2^B-1}$. Therefore, low precision quantization leads to exponential larger quantization errors. Then, the $i$-th attention score with key errors is 

\begin{equation}
\hat{a}_i = \frac{\mbox{exp}(\boldsymbol{q} (\boldsymbol{K}_i + \Delta \boldsymbol{K}_i))}{\sum_{j=1}^{S} \mbox{exp}(\boldsymbol{q} (\boldsymbol{K}_j + \Delta \boldsymbol{K}_j))} 
= \frac{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i) \mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i))}{\sum_{j=1}^{S} \mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j) \mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_j)}
= \frac{ \mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i))}{\sum_{j=1}^{S} \mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j)  \frac{\mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_j)}{\mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i)} }.
\end{equation}

If the key quantization error vector $\Delta \boldsymbol{K}_j$ with low quantization 
precision $B$ is noticeable, the inner product of query and error vector $\boldsymbol{q} \Delta \boldsymbol{K}_j$ is also not ignorable. There are two cases where $\hat{a}_i$ equals to $a_i$ for all tokens. In other words, the attention distribution before and after key quantization are identical.

Case 1) $\frac{\mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_j)}{\mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i)}=1$, where each key token quantization errors have the same inner product result with the query token $\boldsymbol{q} \Delta \boldsymbol{K}_i=\boldsymbol{q} \Delta \boldsymbol{K}_j$ which normally does not happen. 

Case 2) There is a dominating key token $i$. If $j\neq i$,  $\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i) \gg \mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j)$ and $\frac{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j)}{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i)} \approx 0$, then
\begin{equation}
\hat{a}_i = \frac{ \mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i))}{\sum_{j=1}^{S} \mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_j)  \frac{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_j)}{\mbox{exp}(\boldsymbol{q} \boldsymbol{K}_i)}} \approx \frac{ \mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i))}{\mbox{exp}(\boldsymbol{q} \Delta \boldsymbol{K}_i))} = 1.
\end{equation}
Other dominated key token thus has the attention score $\hat{a}_j=0$.
The exactly identical attention distribution with a dominating key token may be a special case, but it indicates that attention heads with a small amount of dominated key tokens, which have highly attention scores and result in sparse and concentrated attention patterns, are consistently robust to low-precision KV cache quantization.
\end{proof}

\section{Effects of KV Cache Quantization Modes and Precision}
\label{sec:kvquant_mode_precision}
In this section, we analyze the effects of KV cache quantization mode and precision. 
We collect the full precision query tensor in the decoding phase and KV cache in both prefilling and decoding stages of the Llama-3.1-8B-Instruct model when processing the first 20 mathematical GSM8K zero-shot prompts without KV cache quantization. After that, we quantize KV cache along the channel or token dimension with uniform precision to compute errors of KV cache and attention score and output vectors of each self-attention layer as defined in Section \ref{sec:kvcache_quantization_errors}, caused by KV cache quantization without any error accumulation.
Finally, we average the simulated errors over different prompts and all layers in Table \ref{tab:kvcache_quantization_error_per_channel_per_token_asym} to study the inherent sensitivity of KV cache to quantization mode and precision.

The non-accumulated relative attention output errors $e_o$ of INT8 KV cache quantization with the per-token-asym or per-channel-asym are lower than $3\%$. Minor single-token errors may cause slight shifts in intermediate attention patterns and final output distributions, yet these shifts are typically insufficient to alter the generated output tokens.
However, when implementing extremely low-precision 2-bit KV2 cache quantization, the relative key quantization error $e_a$ increases to $40.1\%$ or $77.5\%$, which may lead to substantial attention distribution shift for non-sparse retrieval heads as demonstrated in Figure \ref{fig:token_level_attention_distribution_shift_per_token_asym_quant_Llama-3.1-8B-Instruct}. $e_o$ increases dramatically to $81.4\%$ with the per-channel-asym mode even $96.2\%$ with the per-token-asym mode. The noticeable errors may thus lead to noticeable token flipping and generation errors as in Table \ref{tab:error_accumulation_kivi_low_bit_example}.

The relative key error $e_k$ of the INT8 per-token-asym key quantization is 0.012280, which is $2.5 \times$ larger than the per-channel-asym counterpart 0.004869. Dynamically asymmetric quantization along the channel dimension leads to significantly smaller error of both key cache and attention score compared with token dimension quantization, indicating that key cache is strongly sensitive to quantization dimensions. The phenomenon can be explained with the strong channel-wise outliers of key cache \cite{liu2024kivi, hooper2024kvquant}. While value cache can not benefit from switching the quantization dimension, as the relative value errors of the channel or token dimensions over different precision are quite close. 

%
% KV cache quantization errors without error accumualtion of Llama-3.1-8B-Instruct
%
\begin{table}[H]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c c c c c c}
\toprule
KV cache precision & KV quant mode & Relative $e_k$ &  Relative $e_v$ & $e_a$ & Relative $e_o$ \\ \hline
\multirow{2}{*}{KV8} & per-channel-asym & 0.004869 & 0.007754 & 0.000013 & 0.027686  \\
    & per-token-asym & 0.012280 & 0.007865 & 0.000018 & 0.014589 \\    \hline
\multirow{2}{*}{KV4} & per-channel-asym & 0.080991 & 0.125457 & 0.000172 & 0.158429  \\
    & per-token-asym & 0.196476 & 0.126894 & 0.000251 & 0.206909 \\    \hline
\multirow{2}{*}{KV2} & per-channel-asym & 0.401151 & 0.604678 & 0.000868 & 0.814023 \\
    & per-token-asym & 0.774668 & 0.607898 & 0.001166 & 0.961792 \\
\bottomrule
\end{tabular}
}
\caption{Key and value cache quantization relative error analysis of different precision and quantization methods. We collect BF16 KV cache of 20 prompts from the GSM8K zero-shot dataset with Llama-3.1-8B-Instruct and then perform offline quantization to compute the mean error between BF16 and dequantized KV cache.} 
\label{tab:kvcache_quantization_error_per_channel_per_token_asym}
\end{table}

As shown in Figure \ref{fig:layer_wise_relative_key_value_quantization_error_Llama-3.1-8B-Instruct_gsm8k}, there are clear layer-wise diversities of KV quantization errors $e_k$ and $e_o$ with different quantization modes including per-token-asym and per-channel-asym and different precision like INT8, INT4, and INT2. In addition, changing the quantization dimension or mode can result in the significant distribution shift of layer-wise key quantization error. For example, the most sensitive layer with the per-token-asym quantization mode is layer-29, while it changes to layer-11 and layer-13 with the per-token-asym mode. Statically retaining the first or last several layers with more sparse budgets \cite{tang2024quest} may not general well in KV cache quantization. Therefore, we need an automatic KV cache quantization tuning framework to adaptively adopt to these layer-wise differences and configuration modifications.

\begin{figure}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym.pdf}
    \caption{K8 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym.pdf}
    \caption{K4 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym.pdf}
    \caption{K2 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_8_bit_per_channel_asym.pdf}
    \caption{K8 per-channel-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_4_bit_per_channel_asym.pdf}
    \caption{K4 per-channel-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_key_error_Llama-3.1-8B-Instruct_k_2_bit_per_channel_asym.pdf}
    \caption{K2 per-channel-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_8_bit_per_token_asym.pdf}
    \caption{V8 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_4_bit_per_token_asym.pdf}
    \caption{V4 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_2_bit_per_token_asym.pdf}
    \caption{V2 per-token-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_8_bit_per_channel_asym.pdf}
    \caption{V8 per-channel-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_4_bit_per_channel_asym.pdf}
    \caption{V4 per-channel-asym}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_relative_value_error_Llama-3.1-8B-Instruct_v_2_bit_per_channel_asym.pdf}
    \caption{V2 per-channel-asym}
    \end{subfigure}
    \caption{Layer-wise relative key errors $e_k$ and value errors $e_v$ of \textbf{Llama-3.1-8B-Instruct} with the \textbf{per-token-asym and per-channel-asym quantization} modes, KV cache precision as 8, 4, and 2-bit, and the same settings in Table \ref{tab:kvcache_quantization_error_per_channel_per_token_asym}.}
    \label{fig:layer_wise_relative_key_value_quantization_error_Llama-3.1-8B-Instruct_gsm8k}
\end{figure}

\section{Experimental Settings}
\label{sec:experimental_settings}
KVTuner is an automatic KV cache quantization precision tuning framework and can be applied to any quantization mode. We choose two representative and efficient KV cache quantization algorithms KIVI \cite{liu2024kivi} and per-token-asym with uniform KV8, KV4, or KV2 precision pairs in all layers as baselines. Specifically, for the KIVI quantization method, we set the residual length to 32 and the group size to 32.
KVTuner is currently implemented based on huggingface transformers, but it can be applied to inference frameworks such as vLLM \cite{kwon2023vllm}, Megatron \cite{shoeybi2019megatron}, LMDeploy \cite{2023lmdeploy}, and SGLang \cite{zheng2024sglang}. To ensure compatibility, we integrate KV cache quantization methods including KIVI, per-token-asym, and KVTuner within the lm-evaluation-harness \cite{eval-harness}, allowing for seamless adaptation and reproducibility of KVTuner.

We select three popular and recently released LLMs series Llama3.1 \cite{dubey2024llama3.1}, Mistral-v0.3 \cite{jiang2023mistral}, and Qwen2.5 \cite{yang2024qwen2.5}. Among them, Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, and Qwen2.5-7B-Instruct represent the most studied model size. To cover more LLMs application scenarios with different scales, Qwen2.5-3B-Instuct and its quantized version Qwen2.5-3B-Instruct-AWQ are selected for personal devices with limited GPU memory, while Qwen2.5-\{14B, 32B\}-Instuct with larger model scale and better performance are also tested. We also test Qwen2.5-Math-7B-Instruct for mathematical reasoning tasks.

We cover 5 general AIGC and 2 mathematical reasoning tasks available in lm-evaluation-harness 
. 1) \textbf{General tasks}: CEVAL\cite{huang2024ceval}, MMLU \cite{hendrycks2020mmlu}, TriviaQA \cite{joshi2017triviaqa}, RACE \cite{lai2017race}, and TruthfulQA \cite{lin2021truthfulqa}. 2) \textbf{Math, science, and logic tasks}: GSM8K \{0-shot, 4-shot, 8-shot, 16-shot\} \cite{cobbe2021gsm8k}, GSM8K multi-round with lm-evaluation-harness \cite{eval-harness}, GPQA \cite{rein2023gpqa}. 

For the final layer-wise KV cache quantization precision pair searching with multi-objective optimization, we use the open-sourced and widely used Optuna framework \cite{akiba2019optuna} and MOEA/D \cite{zhang2007moea} algorithm. In which case, we treat the LLM inference accuracy under different layer-wise KV precision pairs and input prompts as block-box. The intra-layer and inter-layer search space pruning only takes several minutes but significantly improves sampling efficiency of the downstream Optuna.

We first preprocess the available quantization precision options for each layer group and store them in an array. The indices of this array are then treated as integer parameters, which are optimized by Optuna through multi-objective optimization. The first objective is to maximize the accuracy on the first 200 samples of the GSM8K dataset, while the second objective is to minimize the equivalent quantization precision or memory usage of KV cache.
For each combination of model and quantization mode, we set a soft constraint on the equivalent precision at 4-bit and 6-bit for optuna, conducting 200 search iterations for each setting. The total time cost of offline KV cache precision pair tuning with Optuna mainly depends on the hardware and operator implementation efficiency.

\section{Search Space Pruning and Multi-objective Searching Results}
\subsection{Intra-Layer and Inter-Layer Search Space Pruning Results}
\label{sec:intra_layer_inter_layer_pruning_kvcache_quantization_precision_pairs}
\subsubsection{Intra-Layer Pareto Optimal KV Cache Precision Pair Pruning}
The intra-layer KV cache quantization precision pair pruning based on Pareto frontier are available in Table \ref{tab:intra_layer_kvcache_quantization_precision_pairs}. The calibration dataset is the first 20 prompts from the zeroshot GSM8K dataset. The Pareto optimal KV cache precision pairs in most layers are the key-first set \{KV8, K8V4, KV4, K4V2, KV2\}, indicating that the observation that key cache is more important than value cache holds. 

When both key and value cache are quantized along the token dimension, only the first layer in Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 prefers other KV precision pairs and all layers in Qwen2.5-\{14B, 32B\}-Instruct select the key-first set. In contrast, K8V2 outperforms KV4 in four important layers of Qwen2.5-\{3B, 7B\}-Instruct, indicating that uniform 4-bit key quantization may lead to model accuracy degradation as in Table  \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_qwen2.5}. 

When utilizing the KIVI-like key per-channel-asym  and value per-token-asym quantization mode, more layers show diverse preference of Pareto optimal KV cache quantization precision pairs. In these layers, K4V8 and K2V4 outperform K8V4 and K4V2, which means that lower precision key is better than lower precision value in terms of attention errors. It indicates that per-channel key quantization can effectively reduce quantization errors. 

\subsubsection{Inter-Layer Clustering based on Attention Errors}
After the intra-layer KV cache quantization precision pair pruning, we apply the inter-layer clustering among the layers with the same precision pair set. The clustering algorithm is DBSCAN  \cite{ester1996dbscan} with the hyper-parameter epsilon=0.05 and min\_samples=2. As demonstrated in Table \ref{tab:inter_layer_grouping}, we successfully reduce the exponential component of search space size from the number of transformer layers $L$ e.g. 28$\sim$64 to the number of clustered layer groups $G$ e.g. 4$\sim$8. Utilizing the two-level search space pruning, the total number of combinations of candidate KV cache precision pairs is significantly reduced from $9^L$ to $5^G$ or $6^G$. In Llama-3.1-8B-Instruct, $9^L=9^{32} \approx 3.4\times10^{30}$, while $5^G=5^6=15625$.

In the layer-wise relative attention output errors with per-token-asym KV cache quantization of Llama-3.1-8B-Instruct in Figure \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b}, the highly sensitive layers include layer-\{0, 1, 2, 3, 4, 23, 24, 25, 27, 28, 29\}, while the insensitive layers include layer-\{8, 9, 10, 11, 13, 14, 15, 20, 30\}. Layers in these two classes are correctly clustered into different groups.  Similar phenomenon can also be observed in Qwen2.5-7B-Instruct per-token-asym and KIVI-like quantization modes in Figure \ref{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_qwen2.5_7b} and \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_k_bit_per_channel_asym__v_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}, respectively. Therefore, we can conclude that the proposed multi-objective Pareto frontier based intra-layer pruning and inter-layer clustering algorithms successfully reduce the search space by considering the inherent layer-wise sensitivities.

\begin{table}
\centering
\resizebox{\textwidth}{!}{
% \begin{tabular}{ c r c c c }
\begin{tabular}{ p{1.5in} p{0.3in} p{1in} p{0.4in} p{4.5in} }
\toprule
\multicolumn{1}{c}{Model name} & L & Key quant. mode & G & \multicolumn{1}{c}{Grouped layer ids}  \\ \hline
\multirow{4}{*}{Llama-3.1-8B-Instruct} & \multirow{4}{*}{32} & \multirow{2}{*}{per-token-asym} & \multirow{2}{*}{6} & \{0\}, \{1$\sim$4, 7, 13, 18, 25, 27, 31\}, \{5, 6, 12, 21, 26, 28\}, \{8$\sim$11, 14$\sim$17, 20, 30\}, \{19, 22\}, \{23, 24, 29\} \\ \cline{3-5}
&  & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{6} & \{0\}, \{1$\sim$3, 7, 29, 31\}, \{4, 25, 27\}, \{5, 21, 23, 24\}, \{6, 8$\sim$12, 14$\sim$16, 18$\sim$20, 22, 26, 28, 30\}, \{13, 17\} \\  \hline
\multirow{4}{*}{Mistral-7B-Instruct-v0.3} & \multirow{4}{*}{32} & \multirow{1}{*}{per-token-asym} & \multirow{1}{*}{5} & \{0\}, \{1, 2\}, \{3, 4, 23, 31\}, \{5, 6\}, \{7$\sim$22, 24$\sim$30\} \\ \cline{3-5}
& & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{8} & \{0, 1, 31\}, \{2$\sim$4\}, \{6, 27, 29\}, \{7, 8, 10, 18\}, \{9, 14\}, \{5, 21$\sim$26, 28, 30\}, \{11$\sim$13, 15, 17, 19, 20\}, \{16\} \\  \hline
\multirow{4}{*}{Qwen2.5-3B-Instruct} & \multirow{4}{*}{36}  & \multirow{2}{*}{per-token-asym} & \multirow{2}{*}{8} & \{0\}, \{1, 3$\sim$6, 8, 9, 12, 13, 15, 20\}, \{2, 14, 23, 35\}, \{7, 11, 16, 25, 28, 32\}, \{10, 19, 24, 26, 33\}, \{17, 30, 31, 34\}, \{21, 22\}, \{18, 27, 29\}  \\ \cline{3-5}
& & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{8} &  \{0, 1\}, \{2, 4\}, \{34, 35\}, \{3, 6, 11, 13, 23\}, \{5, 7, 25, 32, 33\}, \{8, 16, 18, 21, 22, 24, 26, 27, 30\}, \{9, 10, 14, 15, 17, 19, 20, 29, 31\}, \{12, 28\} \\  \hline
\multirow{4}{*}{Qwen2.5-7B-Instruct} & \multirow{4}{*}{28} & \multirow{2}{*}{per-token-asym} & \multirow{2}{*}{8} & \{0\}, \{1, 2, 4, 5, 25\}, \{6, 19\}, \{7, 10, 11, 15, 23\}, \{8, 24\}, \{9, 12, 16$\sim$18, 21, 22, 26\}, \{14, 20\}, \{3, 13, 27\} \\ \cline{3-5}
& & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{7} & \{0, 2\}, \{1, 3\}, \{4, 5, 12, 22$\sim$25\}, \{7, 9, 10, 13, 14, 16, 18$\sim$21, 27\}, \{8, 26\}, \{11, 15, 17\}, \{6\} \\  \hline
\multirow{4}{*}{Qwen2.5-14B-Instruct} & \multirow{4}{*}{48} & \multirow{2}{*}{per-token-asym} & \multirow{2}{*}{6} & \{0$\sim$2, 6, 11, 12, 19, 23$\sim$25, 41\}, \{3$\sim$5, 8\}, \{7, 10, 15\}, \{9, 13, 14, 31, 38, 39\}, \{16$\sim$18, 20, 21, 27, 28, 30, 32$\sim$37, 40, 42$\sim$44, 46, 47\}, \{22, 26, 29, 45\} \\ \cline{3-5}
& & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{7} & \{0, 2\}, \{1, 3, 4\}, \{5, 6, 8, 9, 12\}, \{7, 10, 13, 15$\sim$21, 23, 24, 26$\sim$33, 35$\sim$38, 44$\sim$47\}, \{11, 25, 41, 42\}, \{14, 39, 40, 43\}, \{22, 34\} \\  \hline
\multirow{4}{*}{Qwen2.5-32B-Instruct} & \multirow{4}{*}{64} & \multirow{2}{*}{per-token-asym} & \multirow{2}{*}{4} & \{0, 2, 11, 12, 15, 33, 54, 57\}, \{1, 5, 7$\sim$10, 13, 14, 17$\sim$32, 34$\sim$53, 55, 56, 58$\sim$63\}, \{3, 4\}, \{6, 16\} \\ \cline{3-5}
& & \multirow{2}{*}{per-channel-asym} & \multirow{2}{*}{5} & \{0$\sim$4\}, \{11\}, \{5$\sim$10, 12, 14, 16, 18$\sim$23, 26$\sim$28, 32\}, \{13, 15, 17, 22, 24, 25, 29$\sim$31, 33$\sim$62\}, \{63\} \\ 
\bottomrule
\end{tabular}
}
\caption{Inter-layer grouping results by clustering among the layers with the same pruned intra-layer KV cache quantization precision pairs.}
\label{tab:inter_layer_grouping}
\end{table}

\subsection{Searched layer-wise KV precision pairs}
The final searched layer-wise mixed precision KV cache quantization precision pairs of different LLMs and KV quantization modes are available in Table \ref{tab:detailed_config}. Some clustered layer groups in Table \ref{tab:inter_layer_grouping} choose the same KV cache quantization pairs under the given memory consumption and/or accuracy degradation constraints. The number of utilized KV cache quantization precision pairs is reduced from $6\sim8$ to $2\sim 5$ in the tested Llama-3.1-8B-Instruct, Qwen2.5-3B-Instruct, and Qwen2.5-7B-Instruct models. In addition, the significantly diverse layer-wise KV precision pair distribution in Table \ref{tab:detailed_config} indicates that there are not clear heuristic rules based on layer depths to identify layer importance and sensitivity to KV cache quantization. Therefore, we need to measure the model accuracies considering their complicated nonlinear dependencies to layer-wise KV cache precision pairs and utilize accuracies to distinguish the whole model level KV cache precision pair combinations.
\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{p{1.7in} p{1.3in} p{0.7in} p{0.8in} p{3.5in}}
\toprule
\multicolumn{1}{c}{Model name} 
 & \multicolumn{1}{c}{Quant. mode} 
 & \multicolumn{1}{c}{Equivalent precision} 
 & \multicolumn{1}{c}{Quant. precision} 
 & \multicolumn{1}{c}{Layer ids} \\
\midrule

\multirow{14}{*}{Llama-3.1-8B-Instruct}
 & \multirow{5}{*}{per-token-asym}
    & \multirow{3}{*}{3.59}
       & K4V8 & 0 \\
 & & & KV4 & 5, 6, 8--12, 14--17, 20, 21, 26, 28, 30 \\
 & & & K4V2 & 1--4, 7, 13, 18, 19, 22--25, 27, 29, 31 \\
\cline{3-5}
 & 
   & \multirow{2}{*}{5.44}
     & K8V4 & 1--4, 7--11, 13--18, 20, 23--25, 27, 29--31 \\
 & & & KV4 & 0, 5, 6, 12, 19, 21, 22, 26, 28 \\
\cline{2-5}
  & \multirow{9}{*}{KIVI}
    & \multirow{4}{*}{3.25}
       & K8V4 & 13, 17 \\
 & & & KV4 & 1--3, 7, 29, 31 \\
 & & & K4V2 & 5, 6, 8--12, 14--16, 18--24, 26, 28, 30 \\
 & & & KV2 & 0, 4, 25, 27 \\
\cline{3-5}
 & 
   & \multirow{5}{*}{4.90}
       & K8V4 & 4, 6, 8--12, 14--16, 18--20, 22, 25--28, 30 \\
 & & & KV4 & 1--3, 7, 29, 31 \\
 & & & K4V2 & 5, 21, 23, 24 \\
 & & & K2V4 & 0 \\
 & & & KV2 & 13, 17 \\
\hline

\multirow{17}{*}{Qwen2.5-3B-Instruct}
 & \multirow{7}{*}{per-token-asym}
    & \multirow{4}{*}{4.00}
     & K8V4 & 17, 18, 27, 29--31, 34 \\
 & & & K8V2 & 0 \\
 & & & KV4 & 7, 10, 11, 19, 21, 22, 24--26, 28, 32, 33 \\
 & & & K4V2 & 1--6, 8, 9, 12, 13--16, 20, 23, 35 \\
\cline{3-5}
 & 
   & \multirow{3}{*}{5.06}
      & KV8 & 0 \\
 & & & K8V4 & 1, 3--6, 8--10, 12, 13, 15, 17--20, 24, 26, 27, 29--31, 33, 34 \\
 & & & K4V2 & 2, 7, 11, 14, 16, 21--23, 25, 28, 32, 35 \\
\cline{2-5}
  & \multirow{10}{*}{KIVI}
    & \multirow{5}{*}{3.17}
      & K4V8 & 0--1 \\
 & & & K4V2 & 3, 5--11, 13--27, 29--33 \\
 & & & KV4 & 34--35 \\
 & & & K2V4 & 2, 4 \\
 & & & KV2 & 12, 28 \\
\cline{3-5}
 & 
   & \multirow{5}{*}{3.44}
      & KV8 & 0--1 \\
 & & & KV4 & 3, 5--7, 11, 13, 23, 25, 32, 33 \\
 & & & K4V2 & 8--10, 14--22, 24, 26, 27, 29--31 \\
 & & & K2V4 & 34--35 \\
 & & & KV2 & 2, 4, 12, 28 \\
 \hline


\multirow{15}{*}{Qwen2.5-7B-Instruct}
  & \multirow{7}{*}{per-token-asym}
    & \multirow{4}{*}{4.00}
       & KV8 & 0 \\
 & & & K8V2 & 3, 13, 27 \\
 & & & KV4 & 6, 7, 9--12, 14--23, 26 \\
 & & & K4V2 & 1, 2, 4, 5, 8, 24, 25 \\
\cline{3-5}
 & 
   & \multirow{3}{*}{5.00}
       & K8V4 & 8, 9, 12, 14, 16--18, 20--22, 24, 26 \\
 & & & K8V2 & 0, 3, 13, 27 \\
 & & & KV4 & 1, 2, 4--7, 10, 11, 15, 19, 23, 25 \\
\cline{2-5}
 & \multirow{8}{*}{KIVI}
    & \multirow{3}{*}{3.92}
       & KV8 & 0, 2, 6, 11, 15, 17 \\
 & & & KV4 & 4, 5, 8, 12, 22--26 \\
 & & & KV2 & 1, 3, 7, 9, 10, 13, 14, 16, 18--21, 27 \\
\cline{3-5}
 & 
   & \multirow{5}{*}{5.96}
       & KV8 & 0, 2, 7, 9, 10, 13, 14, 16, 18--21, 27 \\
 & & & K8V4 & 4, 5, 12, 22, 23, 24, 25 \\
& & & K4V2 & 11, 15, 17 \\
 & & & K2V4 & 1, 3 \\
 & & & KV2 & 6, 8, 26 \\
\bottomrule

\end{tabular}
}
\caption{Detailed searched layer-wise KV cache quantization precision pairs of different LLMs and KV cache quantization modes by KVTuner.}
\label{tab:detailed_config}
\end{table}

\subsection{Pareto Frontier with the GSM8K Calibration Dataset}
We use the open-sourced package optuna \cite{akiba2019optuna} with the MOEA/D algorithm \cite{zhang2007moea} for the final search with the model memory usage and inference accuracy of the first 200 4-shot GSM8K prompts. The multi-objective search of the Pareto optimal layer-wise KV cache quantization precision pairs of Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, Qwen2.5-3B-Instruct, and Qwen2.5-7B-Instruct with the KIVI and per-token-asym quantization modes are available in Figure \ref{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200} and \ref{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200}. In order to validate the effectiveness of the proposed two-stage intra-layer and inter-layer search space pruning, we disable the pre-processing process and directly use the original full $S^L$ search space in Figure \ref{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-brute-force}.

For each combination of model and quantization mode, we set a soft constraint on the equivalent precision at 4-bit and 6-bit for optuna, conducting 200 search iterations for each setting. The search results are then merged for visualization. In cases where the two-stage intra-layer and inter-layer search space pruning is not applied, we set the maximum equivalent quantization precision to 6-bit and similarly perform 200 search iterations. Specifically, for the KIVI quantization method, we set the residual length to 32 and the group size to 32.

Note that for Qwen-2.5-7B with the KIVI quantization mode, the result from 200 search iterations appeared abnormal. Therefore, we extended the search to 500 iterations to obtain the final result.

\begin{figure*}
    \centering
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_llama3_adaptive_new_1k_per_channel.pdf}
    \caption{Llama-3.1-8B-Instruct}
    \label{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200-llama3}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_mistral_7b_adaptive_new_1k_per_channel.pdf}
    \caption{Mistral-7B-Instruct-v0.3}
    \label{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200-mistral}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_adaptive_new_1k_per_channel.pdf}
    \caption{Qwen2.5-3B-Instruct}
    \label{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200-qwen2.5-3b}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    % \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_7b_adaptive_new_1k_per_channel.pdf}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_7b_adaptive_new_1k_per_channel_fix.pdf}
    \caption{Qwen2.5-7B-Instruct}
    \label{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200-qwen2.5-7b}
    \end{subfigure}
    \caption{Pareto frontier of different models with the \textbf{KIVI} quantization mode on the first 200 data slices of the 4-shot GSM8K dataset.}
\label{fig:pareto-frontiers-per-channel-asym-gsm8k-limit-200}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_llama3_adaptive_new_1k_per_token.pdf}
    \caption{Llama-3.1-8B-Instruct}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-llama3}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_mistral_7b_adaptive_new_1k_per_token.pdf}
    \caption{Mistral-7B-Instruct-v0.3}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-mistral}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_adaptive_new_1k_per_token.pdf}
    \caption{Qwen2.5-3B-Instruct}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-qwen2.5-3b}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_7b_adaptive_new_1k_per_token.pdf}
    \caption{Qwen2.5-7B-Instruct}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-qwen2.5-7b}
    \end{subfigure}
    \caption{Pareto frontier of different models with the \textbf{per-token-asym} quantization mode on the first 200 data slices of the 4-shot GSM8K dataset.}
\label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_llama3_brute_force_new_1k_per_token.pdf}
    \caption{Llama-3.1-8B-Instruct}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-llama3-brute-force}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
    \includegraphics[width=\textwidth]{figs/optuna_search/optuna_qwen2_brute_force_new_1k_per_token.pdf}
    \caption{Qwen2.5-3B-Instruct}
    \label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-mistral-brute-force}
    \end{subfigure}
    \caption{Pareto frontier of different models with the \textbf{per-token-asym} quantization model on the first 200 data slices of the 4-shot GSM8K dataset without intra-layer and inter-layer search space pruning.}
\label{fig:pareto-frontiers-per-token-asym-gsm8k-limit-200-brute-force}
\end{figure*}

\section{Correlation of Model- and Layer-wise KV Cache Quantization Sensitivity with Attention Patterns}
\label{sec:correlation_kvcache_sensitivity_attention_patterns}
According to the layer-wise attention score errors of Llama-3.1-8B-Instruct in Figure \ref{fig:kvcache_simulated_quant_attention_score_error_layer_wise_key_842bit_per_token_asym_llama3.1_8b} and Qwen2.5-7B-Instruct in Figure \ref{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_qwen2.5_7b}, we can observe the clear layer-wise difference in the same LLM. In this section, we try to explain the reason of the difference from the attention pattern perspective as in Figure \ref{fig:selected_layer_wise_attention_patterns_llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt} and \ref{fig:selected_layer_wise_attention_patterns_Qwen2.5-7BB-Instruct_gsm8k_zeroshot_first_prompt}. In which, we visualize block level attention scores of the first 4 heads with block size 4 in the prefilling and decoding stages, and horizontal and vertical axes represent the key and query dimensions respectively. Yellow, green, and purple points indicate high, medium, and low attention scores, respectively. We find out that the more complex and dynamic attention patterns usually lead to larger attention score errors and sensitivity to KV cache quantization of intermediate transformer layers and the whole LLMs.

Take Llama-3.1-8B-Instruct as an example, layer 12 and 13 are in the group with high attention score errors, while layer 0 and 31 are in the medium error group and layer 2 and 23 are in the low error group. Analyzing the attention patterns of these layer in the below Figure \ref{fig:selected_layer_wise_attention_patterns_llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt}, we can conclude that heads in the layer 12 and 13 have dynamic and non-sparse attention patterns, which are called as retrieval heads \cite{tang2024razorattention, xiao2024duoattention}. In contrast, heads in layer 0, 2, 23 and 31 have more static attention patterns like attention sink and recent window, which are called as streaming heads \cite{xiao2024duoattention, xiao2023streamingllm}.

Compared with Llama-3.1-8B-Instruct which has the high ratio of heads with static attention patterns, Qwen2.5-7B-Instruct consists of many heads with mixture of dynamic retrieval heads and other static patterns. It may explain why Qwen2.5-7B-Instruct is more unstable to KV cache quantization as in Table \ref{tab:word_perplexity_kvcache_quant_kivi_hqq_dataset_wikitext}. Layer 5, 12, 21, and 27 have similar attention patterns, but the relative strength of retrieval and streaming heads leads to the difference of layer-wise sensitivity to KV cache quantization.

However, the sensitivity to KV cache quantization is the inherent model property which can be learned offline. Therefore, it is necessary to apply layer-wise mixed precision KV cache quantization and maintain high precision of key cache than value cache with multi-objective optimization KV precision pair tuning as proposed in this work. KVTuner thus makes equivalent 4-bit and even lower KV cache quantization nearly lossless in the sensitive models like Qwen2.5-7B-Instruct.

\begin{figure}
    \centering
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_0_block_size_16.pdf}
    \caption{Layer-0 with recent attention patterns (medium attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_0}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_2_block_size_16.pdf}
    \caption{Layer-2 with attention sinks (low attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_2}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_12_block_size_16.pdf}
    \caption{Layer-12 with retrieval heads (high attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_12}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_13_block_size_16.pdf}
    \caption{Layer-13 with retrieval heads (high attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_13}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_23_block_size_16.pdf}
    \caption{Layer-23 with attention sink (low attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_23}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/llama3.1-8b-instruct_gsm8k_prompt_0/layer_31_block_size_16.pdf}
    \caption{Layer-31 with mixture of retrieval and streaming heads (medium attention score errors)}
    \label{fig:attention_pattern_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_31}
    \end{subfigure}
    \caption{Selected layer-wise attention patterns of \textbf{Llama-3.1-8B-Instruct} model and the first prompt in the \textbf{0-shot GSM8K} dataset. Many layers and heads of Llama-3.1-8B-Instruct have simple and streaming attention patterns which highly concentrated and sparse attention scores. As a result, the attention score errors in these layers are medium or low. In contrast, layers with retrieval or mixed attention patterns, whose attention scores are non-sparse, normally show high attention score errors. \textit{We also observe that the attention patterns of query heads in the same group and sharing the same key cache are highly similar, which may indicate that we can apply attention head group-wise KV cache management for better accuracy.}}
\label{fig:selected_layer_wise_attention_patterns_llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_0_block_size_8.pdf}
    \caption{Layer-0 with mixture of recent window, re-access, and retrieval heads (high attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_0}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_1_block_size_8.pdf}
    \caption{Layer-1 with mixture of recent window and re-access patterns (medium attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_1}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_5_block_size_8.pdf}
    \caption{Layer-5 with mixture of retrieval and streaming heads (low attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_5}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_12_block_size_8.pdf}
    \caption{Layer-12 with mixture of retrieval and streaming heads (medium attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_12}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_21_block_size_8.pdf}
    \caption{Layer-21 with mixture of retrieval heads and attention sinks (medium attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_26}
    \end{subfigure}
    \begin{subfigure}{0.72\columnwidth}
    \includegraphics[width=\columnwidth]{figs/attention_maps/qwen2.5-7b-instruct_gsm8k_prompt_0/layer_27_block_size_8.pdf}
    \caption{Layer-27 with mixture of retrieval heads and attention sinks (high attention score errors)}
    \label{fig:attention_pattern_qwen2.5-7b-instruct_gsm8k_zeroshot_first_prompt_layer_27}
    \end{subfigure}
    \caption{Selected layer-wise attention patterns of \textbf{Qwen2.5-7B-Instruct} model and the first prompt in the \textbf{0-shot GSM8K} dataset. Most layers and heads of Qwen2.5-7B-Instruct have complex attention patterns, such as retrieval, and mixture of retrieval and recent or attention sink patterns. These non-sparse and non-concentrated attention patterns result in the high sensitivity of Qwen2.5-7B-Instruct to KV cache compression including low-precision quantization and even model weight and activation quantization.}
\label{fig:selected_layer_wise_attention_patterns_Qwen2.5-7BB-Instruct_gsm8k_zeroshot_first_prompt}
\end{figure}

\section{More KV Cache Quantization Results}
\subsection{More KV Cache Quantization Results on GSM8K}
\begin{table}
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ c c | r r r | r r r | r  }
    \toprule
     \multirow{2}{*}{Quant. method} & \multirow{2}{*}{Precision} &  \multicolumn{3}{c}{Few-shot CoT} & \multicolumn{3}{c}{Few-shot as multiturn} &  \multirow{2}{*}{Average} \\ 
    & & 4-shot & 8-shot & 16-shot & 4-shot & 8-shot & 16-shot & \\ \hline
\multicolumn{9}{c}{\textbf{Llama-3.1-8B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
    & BF16 & 0.7635 & 0.7741 & 0.7854 & 0.8355 & 0.8309 & 0.8332 & 0.8038 \\ \hline
\multirow{5}{*}{Per-token-asym}
    & KV8 & 0.7635 & 0.7710 & 0.7908 & 0.8340 & 0.8302 & 0.8279 & 0.8029 \\
    & KV4 & 0.7240 & 0.7506 & 0.7354 & 0.8211 & 0.8180 & 0.8097 & 0.7765 \\
    & KV2 & 0.0174 & 0.019 & 0.0250 & 0.0167 & 0.019 & 0.0197 & 0.0195 \\ \cline{2-9}
    & KVTuner-C5.44 & 0.7604 & 0.7726 & 0.7726 & 0.8287 & 0.8385 & 0.8309 & 0.8006\\
    & KVTuner-C3.59 & 0.7210 & 0.7316 & 0.7407 & 0.8021 & 0.8014 & 0.7991 & 0.7660 \\\hline
\multirow{5}{*}{KIVI}
    & KIVI-8 & 0.7733 & 0.7748 & 0.7756 & 0.8347 & 0.8317 & 0.8294 & 0.8033 \\
    & KIVI-4 & 0.7566 & 0.7718 & 0.7839 & 0.8370 & 0.8241 & 0.8332 & 0.8011 \\
    & KIVI-2 & 0.6073 & 0.6080 & 0.5929 & 0.6649 & 0.6543 & 0.6687 & 0.6327 \\ \cline{2-9}
    & KVTuner-C4.90 & 0.7506 & 0.7665 & 0.7657 & 0.8173 & 0.8188 & 0.8378 & 0.7928 \\
    & KVTuner-C3.25 & 0.7483 & 0.7566 & 0.7604 & 0.8362 & 0.8256 & 0.8279 & 0.7925 \\\hline
\multicolumn{9}{c}{\textbf{Qwen2.5-3B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
    & BF16 & 0.6020 & 0.6490 & 0.7020 & 0.5679 & 0.6005 & 0.6490 & 0.6284 \\ \hline
\multirow{5}{*}{Per-token-asym}
    & KV8 & 0.5959 & 0.6573 & 0.7081 & 0.5686 & 0.6080 & 0.6323 & 0.6284 \\
    & KV4 & 0.1888 & 0.1721 & 0.2312 & 0.2229 & 0.2616 & 0.2464 & 0.2205 \\
    & KV2 & 0.0099 & 0.0121 & 0.0106 & 0.0106 & 0.0091 & 0.0129 & 0.0109 \\ \cline{2-9}
    & KVTuner-C5.06 & 0.6058 & 0.6664 & 0.6823 & 0.5914 & 0.6133 & 0.6490 & 0.6347 \\
    & KVTuner-C4.00 & 0.6156 & 0.6482 & 0.6672 & 0.5815 & 0.6118 & 0.6422 & 0.6278 \\\hline
\multirow{5}{*}{KIVI}
    & KIVI-8 & 0.5974 & 0.6619 & 0.7096 & 0.5648 & 0.5989 & 0.6346 & 0.6279 \\
    & KIVI-4 & 0.6156 & 0.6550 & 0.7066 & 0.5732 & 0.6073 & 0.6414 & 0.6332 \\
    & KIVI-2 & 0.0546 & 0.0576 & 0.0675 & 0.047 & 0.0478 & 0.0591 & 0.0556 \\ \cline{2-9}
    & KVTuner-C3.44 & 0.5989 & 0.6429 & 0.7089 & 0.5701 & 0.5997 & 0.6475 & 0.6280 \\
    & KVTuner-C3.17 & 0.6065 & 0.6444 & 0.6998 & 0.5512 & 0.5891 & 0.6406 & 0.6219 \\ \hline
\multicolumn{9}{c}{\textbf{Qwen2.5-7B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
    & BF16 & 0.8059 & 0.8287 & 0.8218 & 0.7081 & 0.7339 & 0.7544 & 0.7755 \\ \hline
\multirow{5}{*}{Per-token-asym}
    & KV8 & 0.7998 & 0.8203 & 0.8196 & 0.7134 & 0.7384 & 0.7354 & 0.7712 \\
    & KV4 & 0.0106 & 0.0121 & 0.0121 & 0.003 & 0.003 & 0.0061 & 0.0078 \\
    & KV2 & 0.0068 & 0.0099 & 0.0076 & 0.0083 & 0.0106 & 0.0106 & 0.0090 \\ \cline{2-9}
    & KVTuner-C5.00 & 0.7885 & 0.8302 & 0.8203 & 0.6914 & 0.7445 & 0.7468 & 0.7703 \\
    & KVTuner-C4.00 & 0.7847 & 0.8112 & 0.7726 & 0.6929 & 0.7331 & 0.7407 & 0.7559 \\\hline
\multirow{5}{*}{KIVI}
    & KIVI-8 & 0.8021 & 0.8271 & 0.8302 & 0.7066 & 0.7354 & 0.7506 & 0.7753 \\
    & KIVI-4 & 0.0735 & 0.1137 & 0.1554 & 0.0667 & 0.0705 & 0.1463 & 0.1043 \\
    & KIVI-2 & 0.0379 & 0.0402 & 0.0356 & 0.0326 & 0.0258 & 0.0235 & 0.0326 \\ \cline{2-9}
    & KVTuner-C5.96 & 0.8218 & 0.8309 & 0.8150 & 0.6907 & 0.7248 & 0.7513 & 0.7724 \\
    & KVTuner-C3.92 & 0.5959 & 0.6664 & 0.6558 & 0.5588 & 0.6156 & 0.6035 & 0.6160 \\ \bottomrule
\end{tabular}
}
\label{tab:merged_gsm8k_per_token_asym}
\caption{Mathematical reasoning accuracy comparison of different KV cache precision settings with the KIVI and per-token-asym quant. mode on the GSM8K few-shot CoT and CoT as multiturn dataset. Notably, for the Qwen2.5-3B-Instruct model using KIVI quantization mode, all configurations within the 4-bit to 6-bit equivalent precision range exhibit lower accuracy on the calibration dataset compared to a configuration with an equivalent precision of 3.44-bit. As a result, we choose this 3.44-bit configuration as the highest-accuracy representative for cases where the equivalent precision is constrained to $\le$6-bit.
\textit{We observe that KVTuner significantly reduces the performance gap between the per-token-asym and KIVI quantization modes.}}
\end{table}


\subsection{More KV Cache Quantization Results on General and Mathematical Reasoning Datasets}
The experimental results of the selected 5 LLMs on the general and mathematical reasoning datasets with uniform KV cache quantization precision pairs are available in Table \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_llama_mistral} and \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_qwen2.5}. To simulate the Openai o1 like long CoT reasoning process, the few-shot CoTs in the GSM8K dataset are treated as a multi-turn conversation, which is enabled with the flags \textit{fewshot\_as\_multiturn} and \textit{apply\_chat\_template} in lm-evaluation-harness. In which cases, questions are provided as user content and answers are provided as assistant responses instead of directly using the given standard answers. Table \ref{tab:kvcache_quant_results_kivi_gsm8k_cot_as_multiturn} summarizes the results of 8 LLMs including Qwen2.5-\{14, 32B\}-Instruct under the \textit{fewshot\_as\_multiturn} setting.

There are limited long output mathematical reasoning datasets in lm-evaluation-harness \cite{eval-harness} and the evaluation of the long context generation is an open question. Therefore, we enable KV cache quantization in both the prefilling and decoding stages to amplify the effects to final generation results caused by KV cache quantization error accumulation, which makes distinguishing different quantization methods easier. For the KIVI quantization mode, we utilize the HQQ quantizer from HuggingFace's implementation, with both the residual length and group size set to 32. 
% For the per-token-asym quantization mode, we employ a vanilla quantizer, which can be retrieved from our codebase.

According to Table \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_llama_mistral}, \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_qwen2.5},  and \ref{tab:kvcache_quant_results_kivi_gsm8k_cot_as_multiturn}, most LLMs, including Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, Qwen2.5-\{14B, 32B\}-Instruct, are robust to low-bit KV cache quantization. Although error accumulation caused by KV cache quantization starts from the prefilling stage, the high KV cache quantization precision pair KV8 with KIVI or per-token-asym quantization mode are still generally lossless, except Qwen2.5-Math-7B-Instruct. The uniform KV cache quantization precision pairs KV4 or even K4V2 with the KIVI quantization mode can achieve nearly lossless $4\times$ or even $5.3\times$ KV cache compression, respectively. KV4 with the simple per-token-asym mode also results in negligible accuracy loss in Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 as shown in Table \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_llama_mistral}. KIVI does outperform the per-token-asym quantization mode in the low-precision settings such as KV4, K4V2, and KV2, especially in Qwen2.5-3B-Instruct-AWQ and Qwen2.5-7B-Instruct as demonstrated in Figure \ref{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_qwen2.5}. 

As shown in Figure \ref{tab:kvcache_quant_results_kivi_gsm8k_cot_as_multiturn}, the larger Qwen2.5-\{14B,32B\}-Instruct models are robust than the smaller Qwen2.5-\{3B, 7B, Math-7B\}-Instruct and the weight quantized Qwen2.5-3B-Instruct-AWQ models. In addition, comparing Qwen2.5-3B-Instruct-AWQ and Qwen2.5-3B-Instruct, we can conclude that model weight quantization with AWQ does not affect the model-level sensitivity to KV cache quantization. The increasing GSM8K accuracy with the longer CoTs under the half precision BF16 KV cache setting indicates that most Qwen2.5 models benefit from longer CoTs. We also obverse that 16-shot CoTs with K4V2 KV cache precision outperforms the 4-shot CoTs with BF16 KV cache precision on the larger Qwen2.5-\{14B,32B\}-Instruct models. It indicates that longer CoT with lower and mixed precision KV cache outperforms uniform precision counterparts as in Section \ref{sec:llm_mathmetical_reasoning_accuracy}. In other words, mixed precision key cache quantization with higher precision key can achieve both memory usage and inference accuracy improvement than equal precision key and value cache quantization.

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ c c | r r r r r r r r r | r  }
\toprule
\multirow{2}{*}{Quant. method} & \multirow{2}{*}{Precision} &  \multirow{2}{*}{CEVAL} & \multirow{2}{*}{MMLU} & \multirow{2}{*}{TriviaQA} & \multirow{2}{*}{RACE} & \multirow{2}{*}{TruthfulQA} & \multicolumn{4}{c}{GSM8K} & \multirow{2}{*}{Average} \\  \cline{8-11}
& & & & & & & 0-shot & 4-shot & 8-shot & 16-shot & \\ 
\hline
\multicolumn{11}{c}{\textbf{Llama-3.1-8B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
& BF16 & 0.5386 & 0.6802 & 0.5161 & 0.4469 & 0.6267 & 0.2866 & 0.7635 & 0.7741 & 0.7854 & 0.6020 \\\hline
\multirow{8}{*}{KIVI} & KV8 & 0.5416 & 0.6798 & 0.5162 & 0.4469 & 0.6304 & 0.2752 & 0.7597 & 0.7657 & 0.7809 & 0.5996 \\
& K8V4 & 0.5394 & 0.6792 & 0.5138 & 0.4498 & 0.6450 & 0.2858 & 0.7695 & 0.7794 & 0.7923 & 0.6060 \\
& K8V2 & 0.4807 & 0.6381 & 0.4989 & 0.4383 & 0.6499 & 0.2358 & 0.7074 & 0.7036 & 0.7195 & 0.5636 \\
& K4V8 & 0.5327 & 0.6694 & 0.5144 & 0.4488 & 0.5851 & 0.2623 & 0.7566 & 0.7566 & 0.7710 & 0.5885 \\
& KV4 & 0.5245 & 0.6689 & 0.5135 & 0.4498 & 0.6132 & 0.2782 & 0.746 & 0.7589 & 0.7680 & 0.5912 \\
& K4V2 & 0.4703 & 0.6236 & 0.5016 & 0.4450 & 0.5373 & 0.2464 & 0.6694 & 0.6694 & 0.6854 & \colorbox{blue!30}{0.5387} \\
& K2V4 & 0.3247 & 0.4628 & 0.4761 & 0.3675 & 0.4639 & 0.0978 & 0.1122 & 0.1054 & 0.0842 & \colorbox{blue!30}{0.2772} \\
& KV2 & 0.2771 & 0.3600 & 0.4584 & 0.3301 & 0.3182 & 0.0508 & 0.0432 & 0.0318 & 0.0250 & \colorbox{blue!30}{0.2105} \\\hline
\multirow{8}{*}{Per-token-asym}
& KV8 & 0.5342 & 0.6800 & 0.5175 & 0.4459 & 0.6206 & 0.2805 & 0.7657 & 0.7809 & 0.7801 & 0.6006 \\
& K8V4 & 0.5386 & 0.6776 & 0.4709 & 0.4450 & 0.6169 & 0.3154 & 0.7733 & 0.7688 & 0.7847  & 0.5990 \\
& K8V2 & 0.4792 & 0.6183 & 0.4984 & 0.4239 & 0.5887 & 0.1501 & 0.6391 & 0.6262 & 0.6550 & 0.5199 \\
& K4V8 & 0.5163 & 0.6579 & 0.5123 & 0.4411 & 0.6781 & 0.2517 & 0.7180 & 0.7293 & 0.7240 & 0.5810\\
& KV4 & 0.5141 & 0.6570 & 0.4849 & 0.4325 & 0.6340 & 0.2782 & 0.7240 & 0.7202 & 0.7157 & 0.5734 \\
& K4V2 & 0.4413 & 0.5910 & 0.4779 & 0.4306 & 0.5447 & 0.1289 & 0.5709 & 0.5633 & 0.5519 & \colorbox{blue!30}{0.4778} \\
& K2V4 & 0.2400 & 0.2350 & 0.0249 & 0.2593 & 0.3268 & 0.0212 & 0.0159 & 0.0296 & 0.0212 & \colorbox{blue!30}{0.1304} \\
& KV2 & 0.2444 & 0.2338 & 0.0052 & 0.2478 & 0.2277 & 0.0227 & 0.0174 & 0.0197 & 0.0273 & \colorbox{blue!30}{0.1162}\\\hline
\multicolumn{11}{c}{\textbf{Mistral-7B-v0.3}} \\ \hline
\multirow{1}{*}{BF16}
& BF16 & 0.3923 & 0.5911 & 0.6081 & 0.4057 & 0.4296 & 0.0766 & 0.3389 & 0.3753 & 0.3601 & 0.3975 \\\hline
\multirow{8}{*}{KIVI}
& KV8 & 0.3945 & 0.5901 & 0.6072 & 0.4115 & 0.4259 & 0.0735 & 0.3412 & 0.3639 & 0.3624 & 0.3967 \\
& K8V4 & 0.3945 & 0.5909 & 0.6068 & 0.4067 & 0.4394 & 0.0781 & 0.3457 & 0.3723 & 0.3669 & 0.4001 \\
& K8V2 & 0.3819 & 0.5776 & 0.6042 & 0.4086 & 0.4370 & 0.0675 & 0.3404 & 0.3518 & 0.3609 & 0.3922 \\
& K4V8 & 0.3990 & 0.5875 & 0.6069 & 0.4048 & 0.4308 & 0.0697 & 0.3442 & 0.3563 & 0.3738 & 0.3970 \\
& KV4 & 0.3945 & 0.5886 & 0.6074 & 0.4105 & 0.4455 & 0.0751 & 0.3434 & 0.3662 & 0.3586  & 0.3989 \\
& K4V2 & 0.3752 & 0.5753 & 0.6035 & 0.4000 & 0.4223 & 0.0705 & 0.3434 & 0.3397 & 0.3616 & 0.3879 \\
& K2V4 & 0.3128 & 0.4926 & 0.5982 & 0.3847 & 0.3917 & 0.0637 & 0.0978 & 0.0910 & 0.0773 & \colorbox{blue!30}{0.2789} \\
& KV2 & 0.2905 & 0.4571 & 0.5920 & 0.3885 & 0.4688 & 0.0478 & 0.0766 & 0.0644 & 0.0516 & \colorbox{blue!30}{0.2708} \\\hline
\multirow{8}{*}{Per-token-asym}
& KV8 & 0.3900 & 0.5892 & 0.6071 & 0.4067 & 0.4284 & 0.072 & 0.3419 & 0.3745 & 0.3571 & 0.3963 \\
& K8V4 & 0.3967 & 0.5897 & 0.6040 & 0.4057 & 0.4357 & 0.0751 & 0.3533 & 0.3715 & 0.3707 & 0.4003 \\
& K8V2 & 0.3692 & 0.5760 & 0.5797 & 0.4029 & 0.3929 & 0.0675 & 0.3328 & 0.3381 & 0.3548 & 0.3793 \\
& K4V8 & 0.3871 & 0.5862 & 0.6070 & 0.4077 & 0.4259 & 0.0629 & 0.3450 & 0.3578 & 0.3692 & 0.3943 \\
& KV4 & 0.3871 & 0.5865 & 0.5994 & 0.4048 & 0.4321 & 0.072 & 0.3450 & 0.3556 & 0.3685 & 0.3946 \\
& K4V2 & 0.3618 & 0.5672 & 0.5774 & 0.4086 & 0.3623 & 0.0599 & 0.3048 & 0.3389 & 0.3571 & 0.3709 \\
& K2V4 & 0.2786 & 0.4360 & 0.4688 & 0.3914 & 0.3268 & 0.0303 & 0.0334 & 0.0281 & 0.0212 & \colorbox{blue!30}{0.2238} \\
& KV2 & 0.2741 & 0.3926 & 0.4045 & 0.4019 & 0.2999 & 0.0281 & 0.0265 & 0.0167 & 0.0220 & \colorbox{blue!30}{0.2074} \\
% \hline
\bottomrule
\end{tabular}
}
\caption{Final generation accuracy comparison of different KV cache quantization modes and precisions and Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 on the AIGC and mathematical datasets. KV cache quantization is enabled during both prefilling and decoding stages to amplify the effects of error accumulation.}
\label{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_llama_mistral}
\end{table}

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ c c | r r r r r r r r r  | r }
\toprule
\multirow{2}{*}{Quant. method} & \multirow{2}{*}{Precision} &  \multirow{2}{*}{CEVAL} & \multirow{2}{*}{MMLU} & \multirow{2}{*}{TriviaQA} & \multirow{2}{*}{RACE} & \multirow{2}{*}{TruthfulQA} & \multicolumn{4}{c}{GSM8K} & \multirow{2}{*}{Average} \\  \cline{8-11}
& & & & & & & 0-shot & 4-shot & 8-shot & 16-shot & \\ \hline
\multicolumn{11}{c}{\textbf{Qwen2.5-3B-Instruct-AWQ}} \\ \hline
\multirow{1}{*}{BF16}
& BF16 & 0.7125 & 0.6382 & 0.2299 & 0.3904 & 0.4700 & 0.4867 & 0.5815 & 0.6353 & 0.6861 & 0.5367 \\\hline
\multirow{8}{*}{KIVI}
& KV8 & 0.7073 & 0.6389 & 0.2283 & 0.3885 & 0.4761 & 0.4882 & 0.5762 & 0.6361 & 0.6816 &  0.5357 \\
& K8V4 & 0.7080 & 0.6388 & 0.2321 & 0.3895 & 0.4871 & 0.4852 & 0.5625 & 0.6315 & 0.6823 & 0.5352 \\
& K8V2 & 0.6872 & 0.6204 & 0.2225 & 0.3856 & 0.4847 & 0.4928 & 0.5368 & 0.6058 & 0.6641 & 0.5222 \\
& K4V8 & 0.7125 & 0.6275 & 0.2326 & 0.3923 & 0.4761 & 0.4814 & 0.5580 & 0.6096 & 0.6550 & 0.5272 \\
& KV4 & 0.7013 & 0.6249 & 0.2322 & 0.4048 & 0.4627 & 0.4761 & 0.5474 & 0.6240 & 0.6368 & 0.5234 \\
& K4V2 & 0.6709 & 0.6038 & 0.2216 & 0.3885 & 0.4700 & 0.4519 & 0.5284 & 0.5732 & 0.6171 & \colorbox{blue!30}{0.5028} \\
& K2V4 & 0.3566 & 0.3626 & 0.1986 & 0.2995 & 0.4186 & 0.0197 & 0.0099 & 0.0099 & 0.0091 & \colorbox{blue!30}{0.1872} \\
& KV2 & 0.3507 & 0.3203 & 0.1983 & 0.2727 & 0.4308 & 0.0136 & 0.0144 & 0.0144 & 0.0136 & \colorbox{blue!30}{0.1810} \\\hline
\multirow{8}{*}{Per-token-asym}
& KV8 & 0.7043 & 0.6379 & 0.2248 & 0.3866 & 0.4798 & 0.4913 & 0.5823 & 0.6331 & 0.6740 & 0.5349 \\
& K8V4 & 0.6969 & 0.6364 & 0.2402 & 0.3837 & 0.4676 & 0.4784 & 0.5671 & 0.6209 & 0.6717 & 0.5292 \\
& K8V2 & 0.4926 & 0.4979 & 0.0100 & 0.3732 & 0.4749 & 0.3616 & 0.3798 & 0.4200 & 0.4640 & \colorbox{blue!30}{0.3860} \\
& K4V8 & 0.2489 & 0.2306 & 0.0000 & 0.2258 & 0.1591 & 0.0008 & 0 & 0 & 0.0008 & \colorbox{blue!30}{0.0962} \\
& KV4 & 0.2377 & 0.2325 & 0.0000 & 0.2220 & 0.1469 & 0 & 0 & 0.0015 & 0.0015 & \colorbox{blue!30}{0.0936} \\
& K4V2 & 0.2600 & 0.2323 & 0.0000 & 0.2258 & 0.0979 & 0.0038 & 0 & 0 & 0 & \colorbox{blue!30}{0.0911} \\
& K2V4 & 0.2318 & 0.2335 & 0.0001 & 0.2201 & 0.1677 & 0.0023 & 0.0083 & 0.0045 & 0.0099 & \colorbox{blue!30}{0.0976} \\
& KV2 & 0.2489 & 0.2372 & 0.0000 & 0.2249 & 0.1310 & 0.0023 & 0.0053 & 0.0106 & 0.0061 & \colorbox{blue!30}{0.0963} \\\hline
\multicolumn{11}{c}{\textbf{Qwen2.5-7B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
& BF16 & 0.7949 & 0.7178 & 0.3239 & 0.4612 & 0.5104 & 0.7233 & 0.8059 & 0.8287 & 0.8218 & 0.6653 \\\hline
\multirow{8}{*}{KIVI}
& KV8 & 0.7949 & 0.7174 & 0.3235 & 0.4603 & 0.5092 & 0.721 & 0.7915 & 0.8249 & 0.8302 & 0.6637 \\
& K8V4 & 0.7979 & 0.7174 & 0.3222 & 0.4651 & 0.5104 & 0.7119 & 0.7915 & 0.8180 & 0.8226 & 0.6619 \\
& K8V2 & 0.7734 & 0.7035 & 0.3165 & 0.4459 & 0.4994 & 0.6581 & 0.7832 & 0.8059 & 0.8105 & 0.6440 \\
& K4V8 & 0.5780 & 0.5024 & 0.2757 & 0.3311 & 0.3660 & 0.0136 & 0.0076 & 0.0038 & 0.003 & \colorbox{blue!30}{0.2312} \\
& KV4 & 0.5802 & 0.5028 & 0.2761 & 0.3206 & 0.3758 & 0.0182 & 0.0068 & 0.0038 & 0.003 & \colorbox{blue!30}{0.2319} \\
& K4V2 & 0.5245 & 0.4704 & 0.2754 & 0.3167 & 0.3745 & 0.0152 & 0.0099 & 0.0053 & 0.0038 & \colorbox{blue!30}{0.2217} \\
& K2V4 & 0.2719 & 0.2645 & 0.2742 & 0.2507 & 0.2399 & 0.0053 & 0.0015 & 0.0008 & 0.0008 & \colorbox{blue!30}{0.1455} \\
& KV2 & 0.2756 & 0.2568 & 0.2741 & 0.2632 & 0.2338 & 0.0099 & 0.0038 & 0.0023 & 0 & \colorbox{blue!30}{0.1466} \\\hline
\multirow{8}{*}{Per-token-asym}
& KV8 & 0.7883 & 0.7119 & 0.3192 & 0.4593 & 0.4957 & 0.7149 & 0.8044 & 0.8052 & 0.8203 & 0.6577 \\
& K8V4 & 0.7920 & 0.7117 & 0.2978 & 0.4545 & 0.5018 & 0.7111 & 0.7847 & 0.8044 & 0.8067 & 0.6516 \\
& K8V2 & 0.7169 & 0.6757 & 0.1127 & 0.4488 & 0.4957 & 0.577 & 0.7233 & 0.7453 & 0.7513 & \colorbox{blue!30}{0.5830} \\
& K4V8 & 0.2192 & 0.2305 & 0.0000 & 0.2220 & 0.0318 & 0 & 0 & 0 & 0 & \colorbox{blue!30}{0.0782} \\
& KV4 & 0.2400 & 0.2327 & 0.0000 & 0.2115 & 0.0171 & 0.0008 & 0.0015 & 0 & 0 & \colorbox{blue!30}{0.0782}\\
& K4V2 & 0.2400 & 0.2301 & 0.0001 & 0.2172 & 0.0245 & 0.0023 & 0.0008 & 0 & 0 & \colorbox{blue!30}{0.0794} \\
& K2V4 & 0.2273 & 0.2347 & 0.0001 & 0.2077 & 0.0575 & 0.0061 & 0.0068 & 0.0015 & 0.0015 & \colorbox{blue!30}{0.0826} \\
& KV2 & 0.2489 & 0.2376 & 0.0000 & 0.2230 & 0.1346 & 0.0045 & 0.003 & 0.0076 & 0.0015 & \colorbox{blue!30}{0.0956} \\\hline
\multicolumn{11}{c}{\textbf{Qwen2.5-Math-7B-Instruct}} \\ \hline
\multirow{1}{*}{BF16}
& BF16 & 0.4881 & 0.5383 & 0.0074 & 0.3464 & 0.4015 & 0.4109 & 0.8863 & 0.8870 & 0.8840 & 0.5389 \\\hline
\multirow{8}{*}{KIVI}
& KV8 & 0.4844 & 0.5379 & 0.0072 & 0.3397 & 0.3966 & 0.4041 & 0.8878 & 0.8878 & 0.8772 & 0.5359 \\
& K8V4 & 0.4874 & 0.5361 & 0.0071 & 0.3445 & 0.4002 & 0.4102 & 0.8886 & 0.8870 & 0.8840 & 0.5383 \\
& K8V2 & 0.4606 & 0.5291 & 0.0071 & 0.3426 & 0.4162 & 0.4139 & 0.8779 & 0.8802 & 0.8696 & 0.5330 \\
& K4V8 & 0.4428 & 0.5061 & 0.0073 & 0.2660 & 0.4100 & 0.0834 & 0.1501 & 0.2024 & 0.1259 & \colorbox{blue!30}{0.2438} \\
& KV4 & 0.4368 & 0.5070 & 0.0074 & 0.2718 & 0.4284 & 0.0879 & 0.1516 & 0.1895 & 0.1236 & \colorbox{blue!30}{0.2449} \\
& K4V2 & 0.4294 & 0.4862 & 0.0069 & 0.2699 & 0.4100 & 0.0819 & 0.1145 & 0.1433 & 0.1024 & \colorbox{blue!30}{0.2272} \\
& K2V4 & 0.2712 & 0.2780 & 0.0059 & 0.2230 & 0.3941 & 0.0152 & 0.0061 & 0.0023 & 0.0008 & \colorbox{blue!30}{0.1330} \\
& KV2 & 0.2741 & 0.2757 & 0.0057 & 0.2220 & 0.3501 & 0.0167 & 0.0023 & 0.003 & 0 & \colorbox{blue!30}{0.1277} \\\hline
\multirow{8}{*}{Per-token-asym}
& KV8 & 0.3975 & 0.5905 & 0.6064 & 0.4038 & 0.4308 & 0.0728 & 0.3457 & 0.3685 & 0.3571 & 0.3970 \\
& K8V4 & 0.3878 & 0.5891 & 0.6035 & 0.4010 & 0.4443 & 0.0735 & 0.3450 & 0.3616 & 0.3632 & 0.3966 \\
& K8V2 & 0.3522 & 0.5590 & 0.5452 & 0.3971 & 0.3452 & 0.0462 & 0.3116 & 0.3397 & 0.3359 & 0.3591 \\
& K4V8 & 0.3804 & 0.5822 & 0.6016 & 0.4010 & 0.3831 & 0.0667 & 0.3252 & 0.351 & 0.3381 & 0.3810 \\
& KV4 & 0.3767 & 0.5803 & 0.5967 & 0.4038 & 0.3953 & 0.0622 & 0.3093 & 0.3146 & 0.3404 & 0.3755 \\
& K4V2 & 0.3470 & 0.5463 & 0.5372 & 0.3943 & 0.4211 & 0.0462 & 0.2631 & 0.2752 & 0.2911 & \colorbox{blue!30}{0.3468} \\
& K2V4 & 0.2429 & 0.2401 & 0.0262 & 0.2900 & 0.2693 & 0.0121 & 0.0038 & 0.0045 & 0.0083 & \colorbox{blue!30}{0.1219} \\
& KV2 & 0.2363 & 0.2351 & 0.0110 & 0.2766 & 0.1787 & 0.0121 & 0.0061 & 0.0091 & 0.0091 & \colorbox{blue!30}{0.1082} \\
\bottomrule
\end{tabular}
}
\caption{Final generation accuracy comparison of different KV cache quantization modes and precisions and Qwen2.5 LLMs on the AIGC and mathematical datasets. KV cache quantization is enabled during both prefilling and decoding stages to amplify the effects of error accumulation.}
\label{tab:kvcache_quant_results_aigc_gsm8k_datasets_quant_modes_models_both_pd_enabled_kvquant_qwen2.5}
\end{table}

\begin{table}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ c | r r r r | r r r r }
\toprule
\multirow{2}{*}{Precision} & \multicolumn{3}{c}{GSM8K} & \multirow{2}{*}{Average} & \multicolumn{3}{c}{GSM8K} & \multirow{2}{*}{Average} \\ 
  & 4-shot & 8-shot & 16-shot & & 4-shot & 8-shot & 16-shot &  \\ \hline
\multicolumn{5}{c}{\textbf{Llama-3-8B-Instruct}} & \multicolumn{4}{c}{\textbf{Qwen2.5-7B-Instruct}} \\ \hline
BF16 & 0.7794 & 0.8006 & 0.7847 & 0.7882 & 0.6998 & 0.7377 & 0.7506 & 0.7294 \\ \hline
KV8 & 0.7801 & 0.8006 & 0.7824 & 0.7877 & 0.6801 & 0.7369 & 0.7460 & 0.7210\\
K8V4 & 0.7688 & 0.7862 & 0.7809 & 0.7786  & 0.6793 & 0.724 & 0.7468 & 0.7167 \\ 
K8V2 & 0.7566 & 0.7763 & 0.7642 & 0.7657 & 0.6801 & 0.7491 & 0.7437 & 0.7243 \\ 
K4V8 & 0.7445 & 0.7695 & 0.7498 & 0.7546 & 0.0076 & 0.0038 & 0.0053 & \colorbox{blue!30}{0.0056} \\ 
KV4 & 0.7422 & 0.7688 & 0.7384 & 0.7498 & 0.0038 & 0.0053 & 0.0023 & \colorbox{blue!30}{0.0038}  \\ 
K4V2 & 0.7346 & 0.7437 & 0.7430 & \colorbox{blue!30}{0.7404} & 0.0061 & 0.0023 & 0.0038 & \colorbox{blue!30}{0.0041} \\ 
K2V4 & 0.0152 & 0.0167 & 0.0159 &  \colorbox{blue!30}{0.0159} & 0.0045 & 0.0045 & 0.0023 & \colorbox{blue!30}{0.0038} \\ 
KV2  & 0.0159 & 0.0144 & 0.0152 & \colorbox{blue!30}{0.0152} & 0.0023 & 0.0015 & 0.003 & \colorbox{blue!30}{0.0023} \\  \hline
\multicolumn{5}{c}{\textbf{Mistral-7B-Instruct-v0.3}} & \multicolumn{4}{c}{\textbf{Qwen2.5-Math-7B-Instruct}} \\ \hline
BF16 & 0.5019 & 0.4890 & 0.4973 & 0.4961 & 0.8901 & 0.8666 & 0.8658 & 0.8742 \\ \hline
KV8 & 0.5042 & 0.4890 & 0.4966 & 0.4966 & 0.8901 & 0.8658 & 0.8666 & 0.8742 \\
K8V4 & 0.5064 & 0.4890 & 0.4913 & 0.4956 & 0.8931 & 0.8688 & 0.8628 & 0.8749 \\ 
K8V2 & 0.4837 & 0.4663 & 0.4632 & 0.4711 & 0.8719 & 0.8741 & 0.8491 & 0.8650 \\
K4V8 & 0.4754 & 0.4701 & 0.4534 & 0.4663 & 0.0500 & 0.0576 & 0.0697 & \colorbox{blue!30}{0.0591} \\ 
KV4 & 0.4875 & 0.4754 & 0.4822 & 0.4817 & 0.0455 & 0.0516 & 0.0796 & \colorbox{blue!30}{0.0589} \\ 
K4V2 & 0.4428 & 0.4503 & 0.4579 & \colorbox{blue!30}{0.4503} & 0.0425 & 0.0516 & 0.0607 & \colorbox{blue!30}{0.0516} \\ 
K2V4 & 0.0258 & 0.0288 & 0.0250 & \colorbox{blue!30}{0.0265} & 0.0023 & 0 & 0  & \colorbox{blue!30}{0.0008} \\ 
KV2  & 0.0190 & 0.0220 & 0.0208 & \colorbox{blue!30}{0.0206} & 0.0023 & 0.0008 & 0.0015 & \colorbox{blue!30}{0.0015}  \\ \hline
\multicolumn{5}{c}{\textbf{Qwen2.5-3B-Instruct}} & \multicolumn{4}{c}{\textbf{Qwen2.5-14B-Instruct}} \\  \hline
BF16 & 0.5732 & 0.5997 & 0.6459 & 0.6063 & 0.7536 & 0.7862 & 0.8180 & 0.7859 \\  \hline
KV8 & 0.583 & 0.6035 & 0.6353 & 0.6073 & 0.7491 & 0.7877 & 0.8158 &  0.7842 \\
K8V4 & 0.5603 & 0.5967 & 0.6513 & 0.6028 & 0.7551 & 0.7953 & 0.8264 & 0.7923\\ 
K8V2 & 0.5133 & 0.5481 & 0.5997 & \colorbox{blue!30}{0.5537} & 0.743 & 0.7733 & 0.8029 & 0.7731\\
K4V8 & 0.5118 & 0.5057 & 0.5049 & \colorbox{blue!30}{0.5075} & 0.7430 & 0.7779 & 0.7998 & 0.7736\\ 
KV4 & 0.5080 & 0.4845 & 0.4837 & \colorbox{blue!30}{0.4921} & 0.7339 & 0.7908 & 0.8112 & 0.7786\\ 
K4V2 & 0.4587 & 0.4124 & 0.4170 & \colorbox{blue!30}{0.4294} & 0.7475 & 0.7733 & 0.7953 & 0.7720\\ 
K2V4 & 0.0083 & 0.0061 & 0.0136 & \colorbox{blue!30}{0.0093} & 0.0220 & 0.0144 & 0.0174 & \colorbox{blue!30}{0.0179}\\ 
KV2  & 0.0061 & 0.0076 & 0.0076 & \colorbox{blue!30}{0.0071} & 0.0288 & 0.0152 & 0.0167 & \colorbox{blue!30}{0.0202}\\ \hline
\multicolumn{5}{c}{\textbf{Qwen2.5-3B-Instruct-AWQ}} & \multicolumn{4}{c}{\textbf{Qwen2.5-32B-Instruct}} \\ \hline
BF16 & 0.5656 & 0.6209 & 0.6399 & 0.6088 & 0.7619 & 0.7809 & 0.7961 & 0.7796 \\ \hline
KV8 & 0.5686 & 0.6149 & 0.6550 & 0.6128 & 0.7650 & 0.7877 & 0.8021 & 0.7849 \\
K8V4 & 0.5747 & 0.608 & 0.6406 & 0.6078 & 0.7726 & 0.7801 & 0.7998 & 0.7842 \\ 
K8V2 & 0.5466 & 0.5694 & 0.6149 & \colorbox{blue!30}{0.5770} & 0.7384 & 0.7703 & 0.7877 & 0.7655 \\
K4V8 & 0.4845 & 0.4564 & 0.4443 & \colorbox{blue!30}{0.4617} & 0.7597 & 0.7794 & 0.8135 & 0.7842 \\ 
KV4 & 0.4845 & 0.4807 & 0.4352 & \colorbox{blue!30}{0.4668} &  0.7680 & 0.7718 & 0.8097 & 0.7832 \\ 
K4V2 & 0.4177 & 0.3730 & 0.3518 & \colorbox{blue!30}{0.3808} & 0.7559 & 0.7733 & 0.7801 & 0.7698 \\ 
K2V4 & 0.0114 & 0.0091 & 0.0053 & \colorbox{blue!30}{0.0086} & 0.0379 & 0.0281 & 0.0311 & \colorbox{blue!30}{0.0324} \\ 
KV2  & 0.0167 & 0.0114 & 0.0129 & \colorbox{blue!30}{0.0137} & 0.0258 & 0.0136 & 0.0311 & \colorbox{blue!30}{0.0235}\\ 
\bottomrule
\end{tabular}
}
\caption{\textbf{KIVI-HQQ} KV cache quantization results of different precision and LLM models on the GSM8K few-shot CoTs as multiturn conversation dataset.}
\label{tab:kvcache_quant_results_kivi_gsm8k_cot_as_multiturn}
\end{table}


\section{Layer-wise Attention Score and Relative Output Error}
\label{sec:layer_wise_attention_relative_output_error_appendix}

In this section, we visualize more layer-wise attention errors with KV cache quantization covering different LLMs, datasets, and KV cache quantization mode and precision. We select the first 20 prompts from the mathematical reasoning dataset GSM8K \cite{cobbe2021gsm8k} and the AIGC multi-turn conversation dataset multiturn-softage \cite{SoftAge_AI2024Multi_turn_softage}. Tested LLMs include Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Mistral-7B-Instruct-v0.3. The layer-wise sensitivity to KV cache quantization of different LLMs are consistent to different prompts and datasets.
Key cache quantization generally leads to the layer-wiser attention output error distribution shift. When the layer-wise attention error distribution significantly changes, the final model accuracy also dramatically degrades. For example, the perplexity and final generation accruracy of Qwen2.5-7B-Instruct dramatically degrades when the key quantization precision decreases to 4-bit and 2-bit with the KIVI or per-token-asym quantization mode as demonstrated in Table \ref{tab:word_perplexity_kvcache_quant_kivi_hqq_dataset_wikitext},  \ref{tab:gsm8k_cot_and_multiturn_kvtuner_results_kivi}, and \ref{tab:gpqa_extended_results}. The attention distribution of it also significantly shifts as shown in Figure \ref{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_qwen2.5_7b}, \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_kv_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}, and \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_k_bit_per_channel_asym__v_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}. The most 

As visualized in Figure \ref{fig:selected_layer_wise_attention_patterns_Qwen2.5-7BB-Instruct_gsm8k_zeroshot_first_prompt}, most layers of Qwen2.5-7B-Instruct have a high ratio of non-sparse retrieval heads, which are sensitive to low-precision key cache quantization as analyzed in Section \ref{sec:attention_patterns_layer_sensitivity}. As a result, 4-bit or 2-bit key quantization leads to noticeable errors of attention score and critical KV identification in these layers with medium attention errors such as layer-1, 12, and 21. 


%
% Llama-3.1-8B-Instruct attention score error and output relative error on GSM8K
%
\begin{figure*}
    \centering
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.014}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k8v8_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.100}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k8v4_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.401}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k8v2_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.168}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k4v8_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.207 }
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k4v4_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.453}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k4v2_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 0.882}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k2v8_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 0.892 }
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k2v4_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\textwidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{KV2 $e_o$: 0.962}
    \label{fig:full_kvcache_simulated_quant_error_layer_wise_k2v2_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \caption{Layer-wise relative attention output error $e_o$ of \textbf{per-token-asym} KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Llama-3.1-8B-Instruct} model and the first 20 prompts in the \textbf{0-shot GSM8K} dataset. When the key quantization precision decreases to 2-bit, the layer-wise relative attention output error distribution significantly shifts. Especially, the errors of layer-3 and layer-1 are significantly larger than other layers.}
\label{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b}
\end{figure*}

%
% Llama-3.1-8B-Instruct attention score error and output relative error on multiturn softage KV per-token-asym
%
\begin{figure*}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K8 $e_a$: $5.0\times 10^{-6}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K4 $e_a$: $6.7\times 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2 $e_a$: $3.26\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.017}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.110}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.418}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.199}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.240}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.484}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 1.092}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 1.103}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2V2 $e_o$: 1.148}
    \label{fig:kvcache_simulated_quant_error_layer_wise_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \caption{Layer-wise attention score errors $e_a$ and relative attention output error $e_o$ of \textbf{per-token-asym} KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Llama-3.1-8B-Instruct} model and the first 20 prompts in the \textbf{AIGC multiturn softage} dataset. When the key quantization precision decreases to 2-bit, the layer-wise relative attention output error distribution significantly shifts. Especially, the errors of layer-3, layer-1, and layer-27 are significantly larger than other layers.}
\label{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b_multiturn_softage}
\end{figure*}

%
% Llama-3.1-8B-Instruct attention score error and output relative error on multiturn softage K per-channel-asym V per-token-asym
%
\begin{figure*}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_8_bit_per_channel_asym_v_8_bit_per_channel_asym.pdf}
    \caption{K8 $e_a$: $4.0\times 10^{-6}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8_per_channel_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_4_bit_per_channel_asym_v_4_bit_per_channel_asym.pdf}
    \caption{K4 $e_a$: $6.7\times 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4_per_channel_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_2_bit_per_channel_asym_v_2_bit_per_channel_asym.pdf}
    \caption{K2 $e_a$: $3.26\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2_per_channel_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.008}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_per_channe_asym_v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.110}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_channel_asym_v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_8_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.418}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_channel_asym_v2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.138}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.187}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_4_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.484}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 1.092}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v8_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 1.103}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v4_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Llama-3.1-8B-Instruct_k_2_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2V2 $e_o$: 1.148}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v2_per_token_asym_Llama3.1-8B-Instruct_multirurn_softage}
    \end{subfigure}
    \caption{Layer-wise attention score errors $e_a$ and relative attention output error $e_o$ of \textbf{key per-channel-asym and value per-token-asym} quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Llama-3.1-8B-Instruct} model and the first 20 prompts in the \textbf{AIGC multiturn softage} dataset. When the key quantization precision decreases to 2-bit, the layer-wise relative attention output error distribution significantly shifts. Especially, the errors of layer-2 and 27 are significantly larger than other layers.}
\label{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_k_bit_per_channel_asym__v_per_token_asym_llama3.1_8b_multiturn_softage}
\end{figure*}

%
% Qwen2.5-7B-Instruct attention score error and output relative error on GSM8K with KV per-token-asym
%

\begin{figure}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K8 $e_a$: $1.74\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8v8_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K4 $e_a$: $1.54\times 10^{-3}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4v4_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2 $e_a$: $4.68\times 10^{-3}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2v2_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.033}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v8_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.117}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v4_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.446}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v2_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.292}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v8_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.324 }
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v4_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.557}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v2_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 0.948}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v8_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 0.958 }
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v4_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{KV2 $e_o$: 1.038}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v2_per_token_asym_Qwen2.5-7B-Instruct}
    \end{subfigure}
    \caption{Layer-wise attention score $e_a$ and relative attention output error $e_o$ of \textbf{per-token-asym} KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Qwen2.5-7B-Instruct} model and the first 20 prompts in the \textbf{0-shot GSM8K} dataset. When the key quantization precision decreases to 4-bit or 2-bit, the layer-wise relative attention output error distribution significantly shifts. It also explains the performance degradation of Qwen2.5-7B-Instruct in the wikitext and other datasets. Especially, the errors of layer-3 and 13 are significantly larger than other layers. Note that in the 8-bit key cache quantization precision, only the first layer-0 and last layer-27 show significantly high errors, while in the 4-bit and 2-bit key cache quantization precision, the attention output errors of layer-3, 7, 10, 13, and 23 become noticeable compared with the first and last layers. Although these layers have relative simpler attention patterns as demonstrated in Figure \ref{fig:selected_layer_wise_attention_patterns_Qwen2.5-7BB-Instruct_gsm8k_zeroshot_first_prompt}, the low-precision 4-bit and 2-bit key cache quantization results in significantly token-level attention distribution shift.}
\label{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_qwen2.5_7b}
\end{figure}

%
% Qwen2.5-7B-Instruct attention score error and output relative error on multiturn softage KV per-token-asym
%
\begin{figure*}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K8 $e_a$: $1.8\times 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K4 $e_a$: $1.68\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2 $e_a$: $5.00\times 10^{-3}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.031}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.110}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_token_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.427}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_token_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.280}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_token_asym_v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.310}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_token_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.531}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_token_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 0.901}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_token_asym_v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 0.909}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_token_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2V2 $e_o$: 0.961}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_token_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \caption{Layer-wise attention score $e_a$ and relative attention output error $e_o$ of \textbf{per-token-asym} KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Qwen2.5-7B-Instruct} model and the first 20 prompts in the \textbf{AIGC multiturn softage} dataset. The layer-wise attention error shift is similar to Figure \ref{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_qwen2.5_7b}, indicating that the layer-wise sensitivity to KV cache quantization is independent of the input prompts and even domains.}
\label{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_kv_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}
\end{figure*}

%
% Qwen2.5-7B-Instruct attention score error and output relative error on multiturn softage K per-channel-asym V per-token-asym
%
\begin{figure*}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_8_bit_per_channel_asym_v_8_bit_per_channel_asym.pdf}
    \caption{K8 $e_a$: $8.0\times 10^{-6}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8_per_channel_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_4_bit_per_channel_asym_v_4_bit_per_channel_asym.pdf}
    \caption{K4 $e_a$: $8.3\times 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4_per_channel_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_score_Qwen2.5-7B-Instruct_k_2_bit_per_channel_asym_v_2_bit_per_channel_asym.pdf}
    \caption{K2 $e_a$: $3.92\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2_per_channel_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.017}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_per_channel_v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.101}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_channel_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_8_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.424}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8_bit_per_channel_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.131}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.174}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_4_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.454}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4_bit_per_channel_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_channel_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 0.679}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v8_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_channel_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 0.696}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v4_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/multiturn_softage_attention_errors/kvquant_attention_relative_output_error_Qwen2.5-7B-Instruct_k_2_bit_per_channel_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2V2 $e_o$: 0.838}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2_bit_per_channel_asym_v2_per_token_asym_Qwen2.5-7B-Instruct_multirurn_softage}
    \end{subfigure}
    \caption{Layer-wise attention score $e_a$ and relative attention output error $e_o$ of \textbf{key per-channel-asym and value per-token-asym} quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Qwen2.5-7B-Instruct} model and the first 20 prompts in the \textbf{AIGC multiturn softage} dataset. Key quantization along the channel dimension significantly affects the distribution of critical layers for 4-bit and 2-bit precision compared with those in Figure \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_kv_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}. The averaged attention output errors $e_o$ under the same KV precision pairs also dramatically reduced. }
\label{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_k_bit_per_channel_asym__v_per_token_asym_Qwen2.5-7B-Instruct_multiturn_softage}
\end{figure*}

%
% Mistral-7B-Instruct-v0.3 attention score error and output relative error on GSM8K
%
\begin{figure}
    \centering
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Mistral-7B-Instruct-v0.3_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K8: $5.90\times 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8v8_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Mistral-7B-Instruct-v0.3_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K4: $8.51\times 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k4v4_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Mistral-7B-Instruct-v0.3_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2: $4.04\times 10^{-3}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2v2_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{KV8 $e_o$: 0.013}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v8_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_8_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K8V4 $e_o$: 0.102}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v4_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_8_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K8V2 $e_o$: 0.411}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k8v2_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_4_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K4V8 $e_o$: 0.149}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v8_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{KV4 $e_o$: 0.191 }
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v4_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_4_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K4V2 $e_o$: 0.453}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k4v2_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_2_bit_per_token_asym_v_8_bit_per_token_asym.pdf}
    \caption{K2V8 $e_o$: 0.823}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v8_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_2_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K2V4 $e_o$: 0.837 }
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v4_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \begin{subfigure}{0.25\columnwidth}
    \includegraphics[width=\columnwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_relative_output_error_Mistral-7B-Instruct-v0.3_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{KV2 $e_o$: 0.939}
    \label{fig:kvcache_simulated_quant_error_layer_wise_k2v2_per_token_asym_Mistral-7B-Instruct-v0.3}
    \end{subfigure}
    \caption{Layer-wise attention score $e_a$ and relative attention output error $e_o$ of \textbf{per-token-asym} KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the \textbf{Mistral-7B-Instruct-v0.3} model and the first 20 prompts in the \textbf{0-shot GSM8K} dataset. When the key quantization precision decreases to 2-bit, the layer-wise relative attention output error distribution significantly shifts. Especially, the errors of layer-1, 2, 3, and 4 are significantly larger than other layers.}
\label{fig:kvcache_simulated_quant_attention_score_relative_output_error_layer_wise_per_token_asym_Mistral-7B-Instruct-v0.3}
\end{figure}