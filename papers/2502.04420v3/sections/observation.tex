\section{Background}
\subsection{Transformer and KV Cache}
In LLMs, there are multiple intermediate transformer layers stacked and executed to generate final output responses. For the $l$-th transformer layer, given $i$-th D-dimensional input hidden state $\boldsymbol{x}_i^l \in \mathbb{R}^{D}$, the $l$-th query, key, and value feedforward neural network layers generate $\boldsymbol{q}_i^l = \boldsymbol{W}_q^l \boldsymbol{x}_i^l$, $\boldsymbol{k}_i^l = \boldsymbol{W}_k^l \boldsymbol{x}_i^l$, and $\boldsymbol{v}_i^l = \boldsymbol{W}_v^l \boldsymbol{x}_i^l$ with the corresponding weight matrices $\boldsymbol{W}_q^l$, $\boldsymbol{W}_k^l$, and $\boldsymbol{W}_v^l$, respectively. Then the self-attention scores $\boldsymbol{a}_i^l$ are computed with the current query embedding and all key embeddings until the $i$-th step. Finally, the $l$-th self-attention layer generates the output state $\boldsymbol{o}_i^l$, which is forwarded to downstream sub-layers in the $l$-th transformer layer, with the softly weighted value embeddings $\boldsymbol{V}^l$ using the attention scores $\boldsymbol{a}_i^l$:
\begin{equation}\label{eq:attention_score}
\boldsymbol{a}_i^l = \mbox{softmax}\left(\frac{\boldsymbol{q}_i^l {\boldsymbol{K}^l}^\top}{\sqrt{D}}\right), \; \boldsymbol{o}_i^l =  \boldsymbol{a}_i^l \boldsymbol{V}^l, 
\end{equation}
where $\boldsymbol{K}^l \!\!=\!\! \text{concat}(\boldsymbol{K}_{:i-1}^l, \boldsymbol{k}_{i}^l)$ and  $\boldsymbol{V}^l \!\!=\!\! \text{concat}(\boldsymbol{V}_{:i-1}^l, \boldsymbol{v}_{i}^l)$ are the key and value embeddings generated in the prefilling and decoding stage in $l$-th transformer layer until $i$-th step. They will still be re-used in subsequent generation steps for self-attention computation. Therefore, we need to store them as KV cache in each layer independently to remove the additional computational cost of KV cache re-computation.

\subsection{KV Cache Quantization}
\label{sec:kvcache_quantization_errors}
Although storing KV cache can reduce the re-computation cost, the KV cache may become the new inference memory and latency bottleneck in the large batch size and long context scenario. 
KV cache quantization can effectively address these problems.
The round-to-nearest $B$-bit quantization and dequantization along the channel or token dimension to input $\boldsymbol{X} \in \mathbb{R}^{S \times D}$ are defined as
\begin{equation}
Q(\boldsymbol{X}) = \mbox{round}\left(\frac{\boldsymbol{X} - \boldsymbol{z}}{\boldsymbol{s}}\right), \; \hat{\boldsymbol{X}} = Q(\boldsymbol{X}) \cdot \boldsymbol{s} + \boldsymbol{z},
\end{equation}
where the offset $\boldsymbol{z} = \min(\boldsymbol{X})$ and the scale $\boldsymbol{s} = \frac{\max(\boldsymbol{X}) - \min(\boldsymbol{X})}{2^B - 1}$. We measure the relative KV cache and attention output errors and the absolute attention score error as 
$e_k^l = \text{mean}\left(\frac{|\boldsymbol{K}^l - \hat{\boldsymbol{K}}^l|}{|\boldsymbol{K}^l|}\right)$,
$e_v^l = \text{mean}\left(\frac{|\boldsymbol{V}^l - \hat{\boldsymbol{V}}^l|}{|\boldsymbol{V}^l|}\right)$,
$e_a^l = \text{mean}(|\boldsymbol{a}^l - \hat{\boldsymbol{a}}^l|)$, and
$e_o^l = \text{mean}\left(\frac{|\boldsymbol{o}^l - \hat{\boldsymbol{o}}^l|}{|\boldsymbol{o}^l|}\right)$, 
where the attention score with dequantized key cache $\hat{\boldsymbol{a}}_i^l = \text{softmax}\left(\frac{\boldsymbol{q}_i^l {\boldsymbol{\hat{K}}^l}^\top}{\sqrt{D}}\right)$ and the attention output with dequantized KV cache $\hat{\boldsymbol{o}}_i^l =  \hat{\boldsymbol{a}}_i^l \hat{\boldsymbol{V}}^l$.


\section{Observation}
\subsection{Error Accumulation}
\label{sec:error_accumulation}
Due to the sequential nature of LLMs along both the model layer and token sequence dimensions, the previous layer output with KV cache quantization errors is the input of the current layer and the previous step model output token with errors is the input of the input and subsequent transformer layers. Therefore, KV cache quantization leads to two-dimensional error accumulation. The error in the $l$-th layer and $i$-th token $e_{i}^{l}$ depends on previous $1\sim l-1$ layers and $1\sim i-1$ steps, as defined in
\begin{equation}
e_i^l= f_e(\boldsymbol{e}_{i}^{1:l-1}, \boldsymbol{e}_{i-1}^{1:L}, \cdots, \boldsymbol{e}_{1}^{1:L}).
\end{equation}

The KV cache quantization error of a single token and layer may be ignorable. However, the error accumulation over the whole model and long context length is noticeable and may lead to token flipping and generation error, which is similar to model quantization \cite{lee2024fliptoken}. The error accumulation caused by low-precision KV cache quantization is a general problem in domain knowledge QA, AI Generated Contents (AIGC), coding, and mathematical reasoning tasks, which may lead to critical factual errors and loss of instruction following ability.

Accumulated errors and intermediate token flipping can render the entire mathematical and logical reasoning process ineffective, resulting in unnecessary computational overhead in long-context reasoning models like OpenAI o1.
As demonstrated in Table \ref{tab:error_accumulation_kivi_low_bit_example}, KIVI-4 has exactly the same response with half-precision KV cache of an example from the GSM8K 15-shot dataset, while the first three generated sentences with low-precision KIVI-2 are highly similar to original generation except for minor differences. Additionally, there is a small token flipping from $-$ to $+$, which leads to the arithmetic operation error in the fourth sentence with KIVI-2. The wrong $20+4+4=28$ instead of $20-4-4=12$ finally leads to the arithmetic error 28/20 = <<28/20=14>>14$\%$ and the completely wrong final answer $14$. 


\subsection{Sensitivity to Quantization Mode and Precision}
\label{sec:kvcache_sensitivity_to_quantization_mode_precision}
KV cache quantization errors strongly correlate with the quantization mode and precision as in Table \ref{tab:intra_layer_kvcache_quantization_precision_pairs}.
In terms of relative key error $e_k$, the per-channel-asym quantization mode consistently outperforms the per-token-asym counterpart under the same precision for key cache, because key cache has strong channel-wise outliers \cite{liu2024kivi, hooper2024kvquant}, more detailed experiment results can be found in Table \ref{tab:kvcache_quantization_error_per_channel_per_token_asym}. Therefore, for specific KV cache, the quantization mode modification may lead to the shift of importance of key and value to attention output errors.
As shown in Table \ref{tab:intra_layer_kvcache_quantization_precision_pairs}, the Pareto-optimal intra-layer KV cache quantization precision pairs significantly differ between these two modes. Therefore, the KV cache precision pairs need to be adapted to quantization modes. More detailed experimental settings and results are available in Appendix \ref{sec:kvquant_mode_precision} and \ref{sec:intra_layer_inter_layer_pruning_kvcache_quantization_precision_pairs} due to space limitations.

\subsection{Why Key Cache Is More Important?}
\label{sec:why_key_are_more_important_in_kvcache_quantization}
We discover the diverse model and transformer layer sensitivity to KV cache quantization mode and pairs, which is mainly caused by attention distribution shift as in Figure \ref{fig:token_level_attention_distribution_shift_per_token_asym_quant_Qwen2.5-7B-Instruct}. In this section, we thus analyze the reason why key cache is normally more important than value cache from both the empirical and theoretical perspectives. 


\begin{table}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c r r r r r r r r r}
\toprule
Model & KV8 & K8V4 & K8V2 & K4V8 & KV4 & K4V2 & K2V8 & K2V4 & KV2 \\ \hline
Llama3-8B-Instruct & 9.95 & 9.94 & 10.04 & 9.99 & 9.99 & 10.11 & \colorbox{blue!30}{31.92} & 31.48 & 37.29 \\ \hline
Llama2-7B-chat-hf & 11.60 & 11.60 & 11.67 & 11.61 & 11.62 & 11.67 & \colorbox{blue!30}{13.86} & 13.92 & 14.92 \\ \hline
Llama2-13B-chat-hf & 10.04 & 10.05 & 10.08 & 10.06 & 10.07 & 10.11 & \colorbox{blue!30}{13.30} & 13.37 & 14.25 \\ \hline
Mistral-7B-Instruct-v0.3 & 8.28 & 8.27 & 8.35 & 8.31 & 8.29 & 8.44 & \colorbox{blue!30}{12.61} & 12.71  & 15.18  \\ \hline
Qwen2.5-3B-Instruct & 10.60 & 10.59 & 11.36 & 11.11 & 11.11 & 12.28 & \colorbox{blue!30}{147.03} & 151.30 & 251.89 \\ \hline
Qwen2.5-7B-Instruct & 9.56 & 9.39 & 9.45 & \colorbox{blue!30}{220.83} & 235.03 & 149.15 & \colorbox{blue!30}{1866.33} & 1831.33 & 4016.10\\ \hline
Qwen2.5-Math-7B-Instruct & 168.92 & 169.60 & 175.34 & \colorbox{blue!30}{588.34} & 599.02 & 725.10 & \colorbox{blue!30}{1746.07} & 1760.31 & 1829.26 \\ \hline
Qwen2.5-14B-Instruct & 6.65 & 6.67 & 7.19 & 6.81 & 6.83 & 7.32 & \colorbox{blue!30}{16.05} & 16.37 & 18.22 \\ \hline
Qwen2.5-32B-Instruct & 6.68 & 6.85 & 6.34 & 6.47 & 6.52 & 6.43 & \colorbox{blue!30}{9.13} & 9.20 & 9.56 \\
\bottomrule
\end{tabular}
}
\caption{Word-perplexity of different KV cache quantization precision pairs with the huggingface transformers KIVI-HQQ implementation on the wikitext dataset and lm-eval-harness.
}
\label{tab:word_perplexity_kvcache_quant_kivi_hqq_dataset_wikitext}
% \vspace{-3mm}
\end{table}

\begin{figure}[htbp]
\centering
    \begin{subfigure}{.315\linewidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_8_bit_per_token_asym_v_8_bit_per_token_asym.pdf}    
    \caption{K8 $e_a$ $1.8 \!\!\times\!\! 10^{-5}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k8v8_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{.315\linewidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    % {figs/kvquant_attention_score_Llama-3.1-8B-Instruct_k_4_bit_per_token_asym_v_4_bit_per_token_asym.pdf}
    \caption{K4 $e_a$ $2.5 \!\!\times\!\! 10^{-4}$}
    \label{fig:kvcache_simulated_quant_attention_score_error
    _layer_wise_k4v4_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    \begin{subfigure}{.315\linewidth}
    \includegraphics[width=\textwidth]{figs/layer_wise_kvquant_attention_score_output_errors/kvquant_attention_score_Llama-3.1-8B-Instruct_k_2_bit_per_token_asym_v_2_bit_per_token_asym.pdf}
    \caption{K2 $e_a$ $1.2 \!\!\times\!\! 10^{-3}$}
    \label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_k2v2_per_token_asym_Llama-3.1-8B-Instruct}
    \end{subfigure}
    % \vspace{-1em}
    \caption{Layer-wise attention score error of per-token-asym KV cache quantization with simulated offline quantization and dequantization (without error accumulation) of the Llama-3.1-8B-Instruct model and the first 20 prompts in the zero-shot GSM8K dataset.}
    % \vspace{-1em}
\label{fig:kvcache_simulated_quant_attention_score_error_layer_wise_key_842bit_per_token_asym_llama3.1_8b}
\end{figure}
%\subsubsection{Empirical error analysis}
\textbf{Intermediate attention errors.} Following the settings in Table \ref{tab:kvcache_quantization_error_per_channel_per_token_asym}, we visualize the simulated layer-wise attention score errors of Llama-3.1-8B-Instruct with the per-token-asym KV cache quantization mode in Figure \ref{fig:kvcache_simulated_quant_attention_score_error_layer_wise_key_842bit_per_token_asym_llama3.1_8b}. More results of diverse LLMs and datasets are available in Appendix \ref{sec:layer_wise_attention_relative_output_error_appendix}. Decreasing the key cache quantization precision from 8-bit to 4-bit and from 4-bit to 2-bit leads to $13.9\times$ and $4.6\times$ average attention score error degradation in Figure \ref{fig:kvcache_simulated_quant_attention_score_error_layer_wise_key_842bit_per_token_asym_llama3.1_8b}, respectively. It may result in attention distribution shift in the token levels of specific sensitive heads as in Figure \ref{fig:token_level_attention_distribution_shift_per_token_asym_quant_Qwen2.5-7B-Instruct} and thus degrade the final accuracy. A similar phenomenon occurs in the final output token probability when implementing KV cache eviction \cite{adnan2024keyformer}. 

As shown in Table \ref{tab:kvcache_simulated_quant_attention_output_relative_error}, the relative attention output errors of high-precision key cache quantization with the same overall memory usage e.g. K4V2 is significantly lower than the high-precision value quantization e.g. K2V4, which empirically validates that key cache is more important than value cache during KV cache quantization of intermediate transformer layers. More detailed experiment setting and results can be found in Figure \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b} and \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b_multiturn_softage}.

\begin{table}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ c | c c c c c c c c c }
        \toprule
        Precision & KV8 & K8V4 & K8V2 & K4V8 & KV4 & K4V2 & K2V8 & K2V4 & KV2 \\
        \midrule
        Relative Attention Output Error ($e_o$) & 0.014 & 0.100 & 0.401 & 0.168 & 0.207 & 0.453 & 0.882 & 0.892 & 0.962 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Layer-wise relative attention output error ($e_o$) of per-token-asym KV Quant. method on Llama-3.1-8B-Instruct on the first 20 prompts from the GSM8K dataset.}
    \label{tab:kvcache_simulated_quant_attention_output_relative_error}
\end{table}

\textbf{Final generation errors.} 
We also study the final LLM generation performance with error accumulation enabled during decoding. Low-precision KV cache in all intermediate layers are quantized with the same KV precision pairs such as K8V4 and K4V2.
We utilize the KIVI implementation with the HQQ backend in huggingface transformers v4.46.2 \cite{wolf2020transformers}, which supports popular LLMs with different scales and proposes, and measure the word-perplexity with lm-evaluation-harness \cite{eval-harness} in Table \ref{tab:word_perplexity_kvcache_quant_kivi_hqq_dataset_wikitext}. 

As shown in Table \ref{tab:word_perplexity_kvcache_quant_kivi_hqq_dataset_wikitext}, both KV8 and K8V4 quantization demonstrate similar perplexity levels across all models. Similarly, KV4 and K4V2 quantization demonstrate comparable patterns. These results suggest that we can achieve equivalent performance using either 6-bit (K8V4) or 3-bit (K4V2) KV cache quantization while maintaining accuracy levels similar to those of KV8 or KV4 quantization, respectively. In contrast, K4V8 and K2V4 quantizations lead to substantial increases in perplexity scores, resulting in significant degradation of generation quality. 
A noticeable decline in generation quality occurs when reducing the precision of the key cache rather than the value cache. The 5-bit K8V2 precision pair achieves performance equal to or better than the higher 6-bit K4V8 precision pair while achieving an additional 12.5\% reduction in memory usage.
These LLMs demonstrate varying levels of sensitivity to KV cache quantization. Most models experience significant perplexity increases only with int2 key cache quantization, with two notable exceptions: Qwen2.5-\{7B, Math-7B\}-Instruct. These two LLMs are sensitive even to int4 key cache quantization, indicating a lower tolerance for precision reduction.
Based on these findings, we conclude that the key cache plays a more critical role than the value cache during quantization. This characteristic can be leveraged to optimize memory usage while maintaining model effectiveness.


\subsection{KV Quantization Errors vs Attention Patterns}
\label{sec:attention_patterns_layer_sensitivity}
As shown in Figure  \ref{fig:token_level_attention_distribution_shift_per_token_asym_quant_Llama-3.1-8B-Instruct}, heads with high KV cache quantization errors typically exhibit non-sparse attention patterns. The sparsity patterns of the attention heads are correlated with the head-wise and layer-wise sensitivity to KV cache quantization,  Highly sparse streaming heads are generally more robust to KV cache quantization than retrieval heads. The proof of Lemma \ref{lemma_attention_pattern_kvquant_error} is available in Appendix \ref{sec:proof_of_lemma_kvquant_error_attention_patterns}.
% \todo{CHECK THE PROOF}
\begin{lemma}
Only attention heads with sparse and concentrated patterns demonstrate consistent robustness to low-precision KV cache quantization.
\label{lemma_attention_pattern_kvquant_error}
\end{lemma}


The optimal strategy to mitigate attention shift and enhance accuracy is to increase key quantization precision, specifically reducing $\boldsymbol{q}\Delta \boldsymbol{K}$ in highly sensitive layers. This approach is recommended when dynamic fine-grained token or page-level KV cache quantization for better accuracy is not feasible, as such methods remain challenging to implement on existing hardware.

\begin{figure}[H]\vspace{-2mm}
\centering
    \begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{figs/token_attention_distribution_shift_figsize_54/Llama-3.1-8B-Instruct_kvquant_token_attention_error_fp16_gsm8k_from_train_nshot_0_prompt_00_layer_02_head_02.pdf}
    \caption{Layer-2 streaming head}
    \label{fig:attention_distribution_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_2}
    \end{subfigure}\hspace{3mm}
    \begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{figs/token_attention_distribution_shift_figsize_54/Llama-3.1-8B-Instruct_kvquant_token_attention_error_fp16_gsm8k_from_train_nshot_0_prompt_00_layer_13_head_02.pdf}
    \caption{Layer-13 retrieval head}
    \label{fig:attention_distribution_Llama3.1-8B-Instruct_gsm8k_zeroshot_first_prompt_layer_13}
    \end{subfigure}
    % \vspace{-1em}
    \caption{Token-level attention distribution shift with the per-token-asym key cache quantization(Llama-3.1-8B-Instruct, GSM8k)}
    \label{fig:token_level_attention_distribution_shift_per_token_asym_quant_Llama-3.1-8B-Instruct}
    % \vspace{-4mm}
\end{figure}



\subsection{Layer-wise Sensitivity to KV Cache Quantization}
\label{sec:layer_wise_sensitivity_kvcache_quant}
According to the layer-wise attention score and relative output errors of different prompts and KV cache quantization precision pairs of Llama-3.1-8B-Instruct in Figure \ref{fig:kvcache_simulated_quant_attention_score_error_layer_wise_key_842bit_per_token_asym_llama3.1_8b} and \ref{fig:kvcache_simulated_quant_attention_output_relative_error_layer_wise_per_token_asym_llama3.1_8b}, transformer layers sensitive to KV cache quantization remain consistent across different input prompts. The observed shifts in layer-wise error distribution primarily stem from variations in key cache quantization precision. Both Qwen2.5-7B-Instruct and Mistral-7B-Instruct-v0.3 exhibit similar behavioral patterns in this respect. Further analysis results can be found in Appendix \ref{sec:layer_wise_attention_relative_output_error_appendix}. We can thus conclude that layer-wise sensitivity to KV cache quantization is an inherent characteristic of LLMs. 

KV cache quantization errors are accumulated over both the model layer and generation sequence dimensions, and the sensitive layer will further amplify errors and lead to dramatic model performance degradation. 
We can perform an offline search to identify the optimal coarse-grained KV cache quantization configuration, determining the most effective precision pairs for each layer, particularly for sensitive layers, to achieve a balance between memory reduction and generation efficiency without incurring any overhead during online inference.