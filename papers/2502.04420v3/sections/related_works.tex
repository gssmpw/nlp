\section{Related Work}
KV cache management and compression methods include paged KV cache \cite{kwon2023vllm}, prefilling-decoding (PD) disaggregation \cite{qin2407mooncake},  quantization \cite{liu2024kivi, liu2024intactkv, hooper2024kvquant, zhang2024qhitter, yang2024mikv, he2024zipcache, he2024zipvl,dong2024qaq}, eviction \cite{zhang2024h2o, ge2023fastgen, liu2024scissorhands, li2024snapkv, adnan2024keyformer}, merging \cite{zhang2024cam, wang2024model, wan2024look-m, liu2024minicache}, low-rank decomposition \cite{kang2024gear, sun2024shadowkv}, offloading \cite{sheng2023flexgen, zhang2024pqcache}, prefetching \cite{lee2024infinigen}, and retrieval \cite{tang2024quest}.
Among them, KV cache quantization is orthogonal to most other KV cache management and compression methods, so it has been integrated with eviction, retrieval, and transferring \cite{tang2024quest, liu2024cachegen}.

Model and activation quantization methods such as GPTQ \cite{frantar2022gptq}, SmoothQuant \cite{xiao2023smoothquant}, AWQ \cite{lin2024awq}, SpinQuant \cite{liu2024spinquant}, and QServe \cite{lin2024qserve} are also used to reduce model memory usage and inference latency with low-bit computation units. Model pruning and layer skipping reduce computational cost by directly pruning unimportant layers or heads \cite{ma2023llmpruner, zeng2023learning, elhoushi2024layerskip}.

Speculative decoding is another promising direction for lossless LLM inference acceleration by reducing the LLM inference iteration times and KV cache memory movement cost in the memory-bounded decoding stage. LLMs verify multiple tokens speculated with smaller models \cite{li2024eagle}, self-partial layers \cite{cai2024medusa, liu2024deepseekv3, gloeckle2024mtp, stern2018blockwise}, or other training-free algorithms \cite{zhao2024lookahead} in one forward step. In addition, Triforce \cite{sun2024triforce} is proposed to integrate KV cache compression with hierarchical speculative decoding to improve long context generation efficiency.