\section{Introduction}
\label{sec:introduction}
 Vector data is common in various scientific and business domains, and its prevalence is expected to grow in the future with the proliferation of learned embeddings~\cite{conf/icde/echihabi2021,palpanas2015data}. The volume and dimensionality of this data, which can exceed multiple terabytes and thousands of dimensions, make its analysis very challenging~\cite{palpanas2015data}. A critical component of these data analysis tasks is vector search ~\cite{conf/icde/echihabi2021,palpanas2015data,conf/sigmod/echihabi2020,DBLP:conf/wims/EchihabiZP20}. It supports recommendation~\cite{conf/kdd/wang2018,amazon}, information retrieval~\cite{conf/williams2014}, clustering~\cite{journal/JMLR/bubeck2009,journal/pattrecog/Warren2005},
classification~\cite{DBLP:conf/icdm/PetitjeanFWNCK14} and outlier detection~\cite{discord,norma,series2graph,landmines,nba} in many fields including bioinformatics, computer vision, security, finance and medicine. 
In data integration, vector search is used for entity resolution~\cite{journal/pvldb/ebraheem2018}, missing values imputation~\cite{retro}, 
and data discovery~\cite{journal/pvldb/zhu2016}. %~\cite{conf/icde/castro2018,journal/pvldb/miller18}. 
Besides, it is exploited in software engineering~\cite{journal/pacml/uri2019,conf/icsec/nguyen2016} 
to automate API mappings and in cybersecurity to profile network usage and detect malware~\cite{cybersecurity}. More recently, vector search has been playing a crucial role in improving the performance and interpretability of Large Language Models and reducing their hallucinations~\cite{retrieval-diffusion-models,dense-passage-retrieval,seq2seq,rag-nlp}. 

A vector search algorithm over a dataset ${S}$ of $n$ $d$-dimensional vectors returns answers in ${S}$ that are similar to a given input vector $V_Q$. 
The brute-force approach (a.k.a. a sequential or serial scan) compares $V_Q$ to every single element in ${S}$. The time complexity of this approach is $O\left(nd\right)$, which becomes impractical when dealing with large collections of data with high dimensionality. 
State-of-the-art approaches typically improve this time complexity by reducing the dimensionality $d$ using concise and precise summarization techniques and/or decreasing the number of processed vectors $n$, by developing efficient indexing data structures and search algorithms that prune unnecessary comparisons to $V_Q$. 
Some approaches compute exact answers, while others $\epsilon$- and $\delta$-$\epsilon$-approximate answers (with deterministic and probabilistic guarantees, respectively, on the accuracy of the answers), or ng-approximate answers (without any theoretical guarantees, but high accuracy in practice)~\cite{hydra1,lernaeanhydra2}. 
One popular vector search algorithm is k-nearest neighbor (k-NN) search, which returns the $k$ vectors in ${S}$ closest to $V_Q$ according to a similarity measure, such as the Euclidean distance. 

%A straightforward approach to retrieving $k$-NN answers involves comparing $V_Q$ to every single element in ${S}$. This method is often referred to as the brute-force approach, a serial scan or a sequential scan. The time complexity of this approach is $O(nd)$, which becomes impractical when dealing with large collections of data with high dimensionality. State-of-the-art approaches typically improve this time complexity by reducing the dimensionality $d$ using concise and precise summarization techniques and/or decreasing the number of processed vectors $n$, by developing efficient indexing data structures and search algorithms that prune unnecessary comparisons to $V_Q$. 

%Despite these disadvantages, graph-based approaches remain the methods of choice for many real applications such as recommendation systems~\cite{graphrec2,graphrec1,alibabaknngml,faiss} that require a very low query latency (a few milliseconds per query on billion-scale collections), and can tolerate a lack of theoretical guarantees on the quality of the answers as long as a high recall ($\ge$ 0.90) can be achieved empirically.  

The efficiency of exact vector search has significantly improved over the last decade~\cite{hydra1, isaxfamily, dumpy, sing, oddysey}. 
However, exact techniques still do not address the query latency constraints of many analytical tasks. Therefore, 
%applications do not %require exact $k$-NN answers and
a large body of work has been dedicated to approximate vector search, which trades off accuracy for efficiency~\cite{lernaeanhydra2,dpg,DBLP:journals/debu/00070P023}. These approaches are based on scans~\cite{vafile,va+file}, trees~\cite{conf/vldb/Wang2013,isax2+,parisplus,coconut,localpairbundle,dpisax,ulisse,messi,flann,hdindex,seanet,wang2024dumpyos,leafi}, graphs~\cite{hnsw,nsg,vamana,hcnng, efanna,SPTAG2}, inverted indexes~\cite{gist,conf/icassp/jegou2011,journal/iccv/xia2013,journal/pami/babenko15}, hashing~\cite{conf/stoc/indyk1998,conf/vldb/sun14,srs}, or a combination of these data structures~\cite{ieh,efanna,SPTAG4,hcnng,elpis,lshapg,DBLP:journals/pvldb/WeiPLP24}. Over the last decade, graph-based techniques have emerged as the method of choice for vector search in many real applications that can relax theoretical guarantees to achieve a query latency of a few milliseconds on terabyte-scale collections~\cite{graphrec2,graphrec1,alibabaknngml,faiss}. 

Figure~\ref{fig:use_case} illustrates vector search in an image retrieval use case. We produce embeddings for ImageNet~\cite{imagenet} using a ResNet50 model~\cite{resnet}. 
We report the time at which a best-so-far (bsf) answer is found (i.e., the image in the database most similar to the query) by vector search techniques from different families: ELPIS~\cite{elpis} and EFANNA~\cite{efanna} for ng-approximate search, QALSH~\cite{qalsh} for $\delta$-$\epsilon$-approximate search, and a serial scan for exact search. We observe that: (i) ELPIS returns the same answer as the serial scan and QALSH over three orders of magnitude faster, which explains the popularity of graph-based vector search in many real applications; and (ii) not all graph-based methods have the same performance, e.g. ELPIS is 3x faster than EFANNA, which motivates this experimental study to help the community better understand the strengths and limitations of the existing graph-based vector search techniques.

\begin{figure}[htb] 
		%\captionsetup{justification=centering}
		\includegraphics[width=0.8\columnwidth]{../img-png/figures/ENDTOENDEXAMPLE2.png}
		\caption{
  Image retrieval using different vector search methods. Each row shows the bsf answer returned by each method (y-axis) at a given timestamp (x-axis). The graph-based approach ELPIS: (i) returns the same answer as the serial scan and QALSH over three orders of magnitude faster; and
  (ii) is 3x faster than the graph-based method EFANNA. 
  }        
		\label{fig:use_case}
 \end{figure}

Graph-based methods typically structure a dataset ${S}$ as a proximity graph $G\left({V},{E}\right)$ where ${V}$ is the set of $n$ vertices representing the $n$ data points in ${S}$, and ${E}$ is the set of edges that connect similar vertices.
%  in the graph connect to $R$ similar vertices, where $R$ is the graph's out-degree. 
Answering a query $V_Q$ usually consists of running an ng-approximate beam search algorithm that warms up the results queue with an initial list of candidate nodes in $G$ known as seed nodes, selects one of them as an entry node, i.e., the node that will be traversed first, and performs a greedy graph traversal following the edges of the traversed nodes to return all nodes that are similar to $V_Q$. State-of-the-Art (SotA) graph-based vector search methods, such as HNSW~\cite{hnsw}, NSG~\cite{nsg}, ELPIS~\cite{elpis}, Vamana~\cite{vamana}, and NSSG~\cite{nssg} adopt the same beam search algorithm to retrieve ng-approximate answers from the graph. % structure. 
However, they differ %primarily 
in how they construct the graph and initiate the beam search. 


%are utilized, with the search starting from a single or a few seed nodes. This guides the query to converge to similar nodes using a beam search approach, as described in Algorithm 1, resulting in the nearest neighbor answers.


A number of studies have evaluated vector search methods. Some focus on exact search~\cite{hydra1} while others cover approximate search with and without guarantees~\cite{journal/tkde/li19, conf/sisap/martin17, journal/pvld/naidan2015}. 
Given the increased interest in graph-based vector search, a recent experimental evaluation~\cite{graph-survey-vldb} has been dedicated entirely to this family of techniques. However, the results of this study are inconclusive, because the experimental evaluation was conducted on very small datasets, not exceeding 1M vectors (we will show in Section~\ref{sec:experiments} that trends change as dataset size increases, e.g. some methods that are efficient on 1M datasets do not perform well on large collections and vice-versa).  
%\karima{summarize all the key findings that illustrate how our study improves upon the vldb survey}).
Besides, our study proposes a new taxonomy that classifies SotA graph-based vector search methods according to five key design paradigms. It also highlights the impact of these design choices on the indexing and search performance, and how this impact changes with dataset size. 
In parallel, some benchmarks~\cite{aumuller2017ann,neurips-2021-ann-competition} also evaluate vector search methods, but they do not focus on graph-based approaches, and thus, do not shed light on why the best methods have superior query performance. In contrast, our study covers real datasets reaching one billion vectors from different domains such as deep network embeddings, computer vision, neuroscience, and seismology, includes more recent approaches, and uncovers interesting insights that have not been reported before, such as the impact of seed selection and neighborhood diversification on indexing and search performance. 

In this paper, we make the following contributions.

\noindent$\bullet$ We identify five main paradigms exploited by state-of-the-art vector search methods: %to achieve efficient indexing and query answering: 
(1) Seed Selection (SS) determines the node(s) in the graph where the search initiates. 
(2) Neighborhood Propagation (NP) approximates the k-NN graph, where each node is connected to its exact k nearest neighbors, by propagating neighborhood lists between connected nodes.
(3) Incremental Insertion (II) builds a proximity graph incrementally connecting vertices with short links to their closest neighbors and long links that model small-world phenomena. 
(4) Neighborhood Diversification (ND) sparsifies the graph by pruning edges that lead to redundant directions at each node.
%reduce the number of edges by sparisifying the graph and delete node edges that take to redudent directions to imprvoe the efficiency and reduce the effort wasted on non promising directions durign search. 
%\cite{kleinberg2000,kleinberg2002}
(5) Divide-and-Conquer (DC) splits a large dataset into partitions and builds individual graphs separately. 
We provide a brief overview of these five paradigms, and study in depth the two key paradigms that have a strong impact on query performance (as shown in Section~\ref{sec:experiments}), namely SS and ND. 

\noindent$\bullet$ We propose a new taxonomy that categorizes graph-based vector search methods according to the five paradigms described earlier, and highlights the chronological development of the different techniques and their influence map. 

\noindent$\bullet$ We provide a brief survey of the SotA graph-based vector search methods. We describe their design principles and pinpoint their strengths and limitations.

\noindent$\bullet$ We conduct an exhaustive experimental evaluation of twelve SotA methods using synthetic and real-world datasets from neuroscience, computer vision and seismology, with sizes reaching up to 1 billion vectors. We provide a rich discussion of the results. 
We confirm some conventional wisdom, e.g., methods that construct graphs through incremental insertion exhibit superior query performance and scalability compared to other existing methods~\cite{elpis,graph-survey-vldb}. We also explain results that differ from the literature, in particular with respect to the most recent study on the same topic~\cite{graph-survey-vldb}. For instance, in the recent study, SPTAG-BKT~\cite{SPTAG4} and HNSW~\cite{hnsw} rank low in terms of query and indexing efficiency, respectively. 
However, our experiments show that SPTAG-BKT is among the most efficient techniques in query answering on small-sized datasets (1M-25GB), and that many methods lag behind HNSW in terms of indexing time, including on small datasets. 
In addition, contrary to previous works~\cite{elpis}, we demonstrate that Vamana~\cite{vamana} is also competitive with ELPIS and HNSW~\cite{elpis,hnsw}. 
%We attribute these different results to the use of the most efficient publicly available implementations in this paper.

%\karima{are you sure this is the only reason especially w.r.t the vldb survey, how about dataset size?}~\ilias{vldb survey used different implementations than the official one}.

%are due to because we use one priority queue instead of two. 
%\karima{\bf Let's move this discussion in green to section 4}.
%\ilias{Our finding that using the heuristic beam search as implemented in HNSW to run with two max-heap PQs to keep track of promising nodes to visit and to keep during search is not efficient and adopting a single sorted linear buffer as priority queue to store both promising nodes and bsf nodes inline with each node ids and distance to the query, shows to make the search faster due to its hardware friendliness. 1.2(25GB)-2.2x(1B) faster.\textbf{Should I add a plot for that??}} \\

%\ilias{SPTAG has two versions, one with Balanced Kmeans trees, and anotehr wth kdtrees. both are costly for indexing, during search SPTAG-BKT give a good performance. nevertheless, couldn't scale to 100GB due to its huge time indexing requirement}.

%although some methods perform well on 1m they dont' scale on larger. NSG ranks among the best for them but it does not scale beyond 25GB because indexing is memory errors .

%some methods that did not well, do well in our case on the same dataset sizes SPTAG. because of the implementation. we use the most up to date implementations.

%HNSW ranks as the high indexing times, in our case experiments hnAW is among the lowest. as dataset size increases, theIn 1M,
%1M: hnsw is low rank 
%1M they use efconstruction is 800 for sift1M, whereas 300 is enough to reach good accuracy. beam width priority queue.
%they used 50 and we used 40 out-degree. M

%usually M=20, M0=40, they used M=40, M0=50. M0 is the base layer. recommended by original paper.

%other methods that have low indexing time in the survey NSG and DPG.

\noindent$\bullet$ We share new insights about vector search that have never been published before.
For instance, (i) we show that not all ND techniques are equally effective, and we identify the one that is most effective for large dataset sizes; and (ii) we illustrate how different SS strategies affect query answering and index building (some methods perform a beam search during graph construction).
%and we provide recommendations on the best strategy for some scenarios.

%, where random sampling of multiple seeds is the optimal choice for medium-scale datasets (25GB-100GB), and that selecting seeds using a hierarchical structure on top of the graph leads to superior performance for billion-scale datasets. 

%\karima{iii and ii seem both to relate to seed selection? revise this paragraph}. 

\noindent$\bullet$ Based on the results of the comprehensive experimental study, we pinpoint some promising research directions in this field: 
(i) novel graph structures that can scale to large data collections should be proposed for methods based on NP and ND strategies;
(ii) more effective and data-adaptive seed selection strategies should be developed to further improve indexing and search performance on large-scale datasets; 
(iii) a deeper theoretical understanding of ND approaches could pave the way for the development of enhanced methods that can adapt to data distributions; and 
(iv) devising techniques (e.g., clustering, neighborhood diversification) tailored to DC-based methods has the potential to improve their performance. 
%\karima{improve this paragraph.}
%(iv) developing more sophisticated graph construction methods, by exploiting summarization techniques, could lead to better indexing and search performance; 

%~\karima{\bf Link each one of these to a plot or paragraph in section 4.}
%\karima{\bf we will propose more directions once the discussion in section 4 is complete.}\ilias{
%A deeper theoretical understanding of Neighborhood Diversification (ND) approaches and their impact on graph structure for ng-approximate similarity search could pave the way for the development of enhanced ND methods that can adapt to data distributions(A potential for learning-based ND techniques? or combination of multiple ND techniques within the same graph?)
%Developing more sophisticated graph construction methods, exploiting summarization techniques, could lead to better index and search performance.
%Further, devising efficient methods (e.g., clustering, ND for small clusters) tailored to Divide-and-Conquer (DC) graph-based construction methods might improve their performance).
%}

\begin{comment}
\ilias{
In real-world skewed distributions, ND methods enhance both the efficiency and accuracy of graph-based structures. Among these, RND and MOND methods yield the best performance, followed by RRND, and then the baseline approach, where nodes connect to their approximate nearest neighbors without diversifying connections. This performance gap widens with increasing dataset size.
The selection of entry points can significantly impact the efficiency of both search and indexing in methods that find the NN candidate neighbors for each node through beam search on the graph structure. Random sampling of multiple seeds is the optimal choice for medium-scale datasets of 25GB and 100GB. However, for 1B vectors, Stacked NSW demonstrates superior efficiency.

The state-of-the-art ND-based graph structure delivers the best performance for query search, with notable performance for methods that belong to the two flavors of ND and DC-based graphs, like SPTAG~\cite{SPTAG4} and Elpis~\cite{elpis} on 25GB, with the lowest indexing footprint. 
Most methods struggle to index large datasets of 100GB and above, while only the insertion/ND-based methods HNSW~\cite{hnsw} and Elpis~\cite{elpis} scale to over 1B vector datasets. The majority of other ND-based methods fail to scale, primarily due to their dependence on refining an NP graph(for NP/ND-based methods like DPG~\cite{dpg} AND NSG~\cite{nsg}), which requires high indexing time and footprint, or the high indexing time of the construction methods themselves(i.e SPTAG~\cite{SPTAG4}, Vamana~\cite{vamana}, HCNNG~\cite{hcnng}).

Improving the scalability of ND methods is crucial. Through our studies, we propose several directions that may contribute to the enhancement of graph methods' scalability and performance: \noindent$\bullet$ The insertion construction approach for ng approximate similarity search methods emerges as an optimal choice, offering an excellent trade-off between indexing and search performance. Moreover, insertion-based graph methods facilitate straightforward operations for the insertion and deletion of nodes in the graph.
\noindent$\bullet$ ND-based methods, which refine an already existing graph through ND methods, should rely on a more efficient approximate \textit{knn} graph that scales to large datasets.
\noindent$\bullet$ DC methods exhibit good search performance, however, methods like ELPIS~\cite{elpis} depend on multithreading execution to deliver satisfactory performance during search, which may not be feasible for single-machine setups. Implementing more efficient graph pruning methods to reduce the number of required threads may significantly improve search performance.
\noindent$\bullet$ Implementing an efficient structure to retrieve entry points could be crucial for the next generation of graph-based methods. SPTAG-BKT~\cite{SPTAG4} demonstrates good performance on 25GB datasets, yet it demands high indexing time (30x the time needed for indexing Elpis 25GB), due to the construction of multiple BKTrees exploited during the search to retrieve seeds for beam search.
}
\end{comment}



%\end{enumerate}


%our objective is to evaluate the ten popular SOTA graph-based similarity search methods, in order to better understand the key design choices that impact the scalability of both indexing and search algorithms.

% as well as the main key design that differentiate the best methods for graph-based similarity search, the seeds selection and neighborhood diversification. We investigate how each method performs, and how the different key design affect the performance of graph index during search, across various real-world datasets, ranging in size fro 1 million to 100 million data points. Through our analysis, we aim to shed light on the effectiveness of different seeds selection and neighborhood diversification strategy, and their impact on the performance of nearest neighbor search algorithms on different dataset scale. In this study:






%As datasets grow in scale and intricacy, traditional data processing and analysis techniques often fall short, unable to efficiently manage, let alone extract value from, these immense data repositories. At the intersection of addressing these challenges lies similarity search.



%Graph-based methods commonly employ beam search to obtain nearest neighbor (NN) answers, but they vary in their approach to selecting entry points, also known as seeds selection\footnote{in the following paragraphs, we use seeds and entry nodes interchangeably}. Some methods attempt to sample points from the dataset and construct a lighter one or multiple indices. They then perform a lightweight search to obtain the closest seeds before starting an intensive search on the graph using beam search. For instance, HNSW proposes the use of a multi-resolution nsw graph stacked hierarchically, while methods like SPTAG, EFANNA, and HCNNG utilize multiple KD-trees or BK-trees to obtain initial seeds, IEH uses hash table.

%In contrast, other methods like DPG, NSG, VAMANA and NSSG believe that such a process may not be advantageous in real-world scenarios. Instead, they choose predefined representative points, such as centroids, or randomly sampled points before each search. However, none of these methods have provided sufficient theoretical or empirical evidence to support their choices for entry point selection.
%\ilias{how to sample representative sample? methods like HVS proposes to exploit all points in embedded PQ space of the corresponding voronoi cells, can we do the same with SAX and how this can be compared to HVS??}
%\ilias{can we learn this sampling as filter ??}
%\ilias{can we cluster the list of neighborhood into multiple subsets? so we can only compare with the one set of neighbors ?? how can this be applied for the case of rng sparse graph?? what is the potential of such approach to be paralleled ??}
%\ilias{can we use learning, or principal component vectors, to divid the neighbors into subeset depending on the directions ???}


%In this paper, our objective is to compare different methods for entry points in both approximate and exact nearest neighbor graphs. We investigate how each method performs across various real-world datasets, ranging in size from 1 million to 100 million data points. Through our analysis, we aim to shed light on the effectiveness of different entry point selection strategies and their impact on the performance of nearest neighbor search algorithms.
\noindent{\bf Scope}: The goal of this study is to conduct an experimental evaluation of the SotA graph-based methods designed for in-memory dense vector search. A theoretical analysis of the indexing and search algorithms is an open research question (no existing graph-based vector search method provides this analysis), and is out-of-scope. The extension to the out-of-core scenario and the theoretical analysis are both part of our future work. 
%\tp{looks good; do we want to state this here, or in the conclusions?}
