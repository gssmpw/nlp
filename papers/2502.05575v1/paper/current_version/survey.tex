\section{Graph-Based Vector Search}
\label{sec:survey}
We now present an overview of the main SotA graph-based $ng$-approximate vector search methods. 
We outline the base data structures and algorithms in this field, and identify five main paradigms exploited by the SotA approaches. % to improve upon these baselines. 
We propose a new taxonomy that categorizes these approaches along the five paradigms, highlighting also their chronological development and influence map. 
%Finally, we describe these approaches in more detail pinpointing their key design principles, strengths and limitations.

\subsection{A Primer}

%\noindent{\bf Base Graphs.}} 
A proximity graph is a graph $G\left({V},{E}\right)$ in which two vertices $V_i$ and $V_j$ are connected by an edge if and only if they satisfy particular geometric requirements, namely the \textit{neighborhood criterion}~\cite{shamos1975closest}.
%whereas proximity refers to the spacial distance. 
A proximity graph can be constructed using different distances such as the dot product~\cite{mipsg}, nevertheless the Euclidean distance remains the most popular one~\cite{edelsbrunner87}. One of the earliest proximity graphs in the literature is the Delaunay Graph (\textit{DG}). It is a planar dual graph for the Voronoi Diagram~\cite{vd95}, where each vertex is the center of its own voronoi cell, and two vertices are linked if and only if their corresponding voronoi cells share at least one edge. A DG 
%G({V},{E})$
% constructed over $\textit{S}$ 
satisfies the Delaunay Triangulation :$\forall q,p,r \in {V}, \left(q,p\right), \left(q,r\right), \left(r,p\right) \in {E}$
%if no other vertex from ${V}$ is inside the circle passing through $q, p, r$,
if the circumcircle of the triangle $q, p, r$ is empty~\cite{aurenhammer2013voronoi}.
%\noindent{\bf Base Search Algorithm.}} 
A beam search~\cite{reddy77bm} (Algorithm~\ref{alg:beamsearch}) on a \textit{DG} can find the exact nearest neighbors~\cite{dobkin1990delaunay} when the dataset has a high dimensionality or the beam search uses a large beam width.
%~\ilias{(the paper indicate that DG can be almost as good as complete graph(fully  connected graph, where every node has a connection to every node), and since DG is guaranteed to be a connected graph by nature, which mean  every node is connected/has path to every other node, with large enough BW, we can guarantees the results to be always exact)}. \karima{\bf I don't understand. Does this mean that a beam search on a DG is NOT guaranteed to find the exact nearest neighbors unless the BW is large enough?? If yes, any indication of how large this BW should be?}\ilias{beam search is guaranteed to find solution on complete graph where each node is connected to each node, in the paper i cited, they prove that dg can be almost as good as complete graph when the dimension grows high, which is beams search c} \karima{\bf This is still not clear. Does the paper quantify "almost" and "high"?} 
However, using a DG in high dimensions is impractical, because the graph becomes almost fully connected as the dimensionality $d$ grows~\cite{dobkin1990delaunay}. 
Thus, SotA methods build alternative graph structures and use beam search to support efficient query answering
~\cite{gabriel69,matula80,toussaint02}. 

%Beam search, originally proposed in 1977~\cite{reddy77bm}, is a fundamental algorithm employed in various fields such as speech recognition, machine translation, and natural language processing. It involves exploring a limited number of hypotheses, termed the "beam," to enhance efficiency compared to exhaustive search methods. The algorithm sacrifices completeness and optimality for efficiency, as it does not guarantee the best solution. The time complexity of beam search depends on several parameters, including the beam width and the maximum allowed path length within the graph. In proximity graphs, such as those used in spatial data analysis, beam search can be adapted to efficiently explore nearby nodes or edges, contributing to its versatility in different applications.



%mention that there is no work so far on 

%:
%\[
%\text{Time Complexity} = O(B \times R \times D)
%\]

%where:
%\begin{itemize}
%    \item \( B \) represents the beam width,

%    \item \( R \) denotes the average out-degree,
%    \item \( D \) is the diameter of the graph.
%\end{itemize}
%}


\begin{algorithm}[tb]
{\small
\caption{Beam Search (G, $V_Q$, \textit{s}, \textit{k}, \textit{L})}\label{alg:beamsearch}
\KwIn{Graph \textit{G}, query vector $V_Q$, initial seeds s, result size \textit{k}, beam width $\textit{L}\geq \textit{k}$ }
\KwOut{\textit{k} approximate nearest neighbors to $V_Q$}
 initialize candidate set $\textit{C} \longleftarrow {\textit{s}}$\; \label{alg:beam:line:init}
 initialize visited list $\textit{R} \longleftarrow \emptyset$\; \label{alg:beam:line:init2}
 \While{$\textit{C}\backslash\textit{R} \neq \emptyset$ }{
  let $p^{*} \longleftarrow \operatorname*{argmin}_{V_i \in \textit{C}\backslash\textit{R}} dist\left(V_Q, V_i\right)$\;\label{alg:beam:line:best}

  update $\textit{C} \longleftarrow \textit{C} \cup N_{out}\left(p^*\right)  $\;\label{alg:beam:line:updatecand}
  update $\textit{R} \longleftarrow  \textit{R}\cup p^{*}$\;\label{alg:beam:line:updaterslt}
  \If{$\vert \textit{C} \vert > \textit{L}$}{
   update $\textit{C}$ to retain closest \textit{L} points to $V_Q$\;
   }
 }
 return the $\textit{k}$ candidates in  $\textit{C}$ closest to $V_Q$\;\label{alg:beam:line:return}
 } % font size
\end{algorithm}

\subsection{Main Paradigms}

We provide a brief overview of the five main paradigms exploited by SotA methods.
%to improve the search efficiency in practical applications: 
%(1) SS, (2) NP, (3) II (II), (4) ND, and  
%(5) DC. %Decomposition (DC). 
Then, we describe in more detail the two  paradigms that have the greatest impact on query performance (as will be demonstrated in Section~\ref{sec:experiments}).

\noindent{\bf{Seed Selection (SS)}} chooses initial nodes to visit during search. It is also used during index building by approaches that exploit a beam search during the construction of the index to decide which edges to build. Some methods simply select one or more seed(s) randomly, while others use special data structures, e.g., a K-D Tree.


\noindent{\bf{Neighborhood Propagation (NP)}} refines a pre-existing graph following a user-defined number of iterations, a.k.a. NNDescent~\cite{nndescent}. 
During each iteration, the potential neighbors of a given node are sourced both from its immediate neighbors and the neighbors of its neighbors. Then, the node only keeps the $m$ closest neighbors, where $m$ is a user-parameter. %\karima{\bf So, with NP, edges are added but not removed?}\ilias{yes, for each node, we construct the list of candidate neighbors by adding the nodes neighbors, and the neighbors of its neighbors, then the node keep K closest candidates and update its neighborhood list} 
The pre-existing graph could be a random graph 
%as is the case with KGraph~\cite{kgraph} 
or some other type of graph.
%, which could be a random graph—as is the case with KGraph~\cite{kgraph}—or a graph constructed utilizing various other approximate similarity search techniques, such as tree-based methods in Efanna~\cite{efanna} or hash-based methods in IEH~\cite{ieh}. The refinement process involves several iterations of the neighborhood propagation procedure, also known as NNDescent~\cite{nndescent}, wherein a node’s potential neighbors are sourced from both its immediate neighbors and the neighbors of its neighbors.
%Among the techniques that employ this strategy, 
% which propose to build graph index using a NP-based graphs(i.e Kgraph and Efanna) as a base graph.


%\noindent{\bf{NP-Based Methods:}} The neighborhood propagation (NP) approach encompasses methods that refine a pre-existing graph, which could be a random graph—as is the case with KGraph~\cite{kgraph}—or a graph constructed utilizing various other approximate similarity search techniques, such as tree-based methods in Efanna~\cite{efanna} or hash-based methods in IEH~\cite{ieh}. The refinement process involves several iterations of the neighborhood propagation procedure, also known as NNDescent~\cite{nndescent}, wherein a node’s potential neighbors are sourced from both its immediate neighbors and the neighbors of its neighbors.
%Among the techniques that employ this strategy, KGraph~\cite{kgraph} stands out for pioneering the use of the neighborhood propagation approach in building approximate KNN graph structures, setting a precedent that has influenced numerous subsequent methods, including IEH~\cite{ieh}, EFANNA~\cite{efanna}, and others, such as  DPG~\cite{dpg}, NSG~\cite{nsg} and NSSG~\cite{nssg} which propose to build graph index using a NP-based graphs(i.e Kgraph and Efanna) as a base graph.

\noindent{\bf{Incremental Insertion (II)}} refers to building a graph by inserting one vertex at a time. 
Each vertex is connected using bi-directional edges to its nearest neighbors and some distant vertices. The neighbors are selected using a beam search on the already inserted portion of the graph. At the end of graph construction, some vertices retain early connections which act as long-range links. 
%, and the distant vertices are selected among the early connections of the current vertex. 
This approach was first proposed in the object-based peer-to-peer overlay network VoroNet~\cite{voronet}, with the idea of adding long-range links being inspired from Kleinberg's small-world model~\cite{kleinberg2000, kleinberg2002}, with the difference that the latter selects the long-range links randomly. 
%State-of-the-art II-based similarity search methods such as NSW~\cite{nsw14} and HNSW~\cite{hnsw} inspired by VoroNet also build long-range links from early inserted nodes, with HNSW also including long-range links from the hierarchical structure it builds on top of NSW. \karima{\bf Ilias, is this last sentence correct?}

%{The NSW has long range edge from early inserted nodes, HNSW has long range edges from its early inserted node and stacked NSW, but none of theses methods actually gives any guanrantees that does long range edges will not be pruned in process of graph construction, only in the HNSW where the stacked nsw structure is kept}

% to identify a list of candidate neighbors for each new node, followed by the addition of bi-directional edges. Notably$X$ in HNSW~\cite{hnsw} (and by extension, ELPIS~\cite{elpis}), the list of neighbors undergoes pruning through the use of neighborhood diversification techniques. This dual categorization of these methods, appearing in both insertion-based and neighborhood diversification (ND) based families, is evident in Figure~\ref{fig:roadmap}.
%Insertion-based methods like HNSW and Elpis have shown strong capabilities in indexing large datasets~\cite{hydra2,ann-benchmark-journal,elpis}.


%\noindent{\bf{Insertion-Based Methods}}  such as NSW~\cite{nsw14}, HNSW~\cite{hnsw}, and ELPIS~\cite{elpis} form their graph indices utilizing an approach based on sequential node insertion. This methodology has its roots in the object-based peer-to-peer overlay network VoroNet~\cite{voronet}, which was influenced by Kleinberg's small-world model~\cite{kleinberg2000, kleinberg2002}. In this particular model, each node establishes connections with its immediate neighbors and additionally secures a link with a distant neighbor. VoroNet further refined the small-world graph construction process by adopting an incremental approach, where early connections serve as long-range edges to facilitate efficient graph traversal.
%In general, insertion-based graph methods incrementally add nodes to the graph, ensuring each new node is connected to its nearest neighbors. This is achieved by employing beam search on the already inserted portion of the graph to identify a list of candidate neighbors for each new node, followed by the addition of bi-directional edges. Notably, in HNSW~\cite{hnsw} (and by extension, ELPIS~\cite{elpis}), the list of neighbors undergoes pruning through the use of neighborhood diversification techniques. This dual categorization of these methods, appearing in both insertion-based and neighborhood diversification (ND) based families, is evident in Figure~\ref{fig:roadmap}.
%Insertion-based methods like HNSW and Elpis have shown strong capabilities in indexing large datasets~\cite{hydra2,ann-benchmark-journal,elpis}.

% and can support insertion/deletion of nodes much easily.

\noindent{\bf{Neighborhood Diversification (ND)}} was first introduced by the Relative Neighborhood Graph (RNG)~\cite{rng}. It aims to sparsify the graph by pruning unnecessary edges while preserving connectivity. For each node, ND exploits the geometrical properties of the graph to remove edges to neighbors that lead to redundant regions or directions
%\karima{(Ilias check this)}\ilias{Regions based on RND and RRND, MOND for direction; nevertheless, we proove that RRND and MOND are both approximations of RND in archive}.
This indirectly causes the creation of long-range links allowing nodes to maintain diversified neighborhood lists, which reduces the number of comparisons during search.
%is a process that aims at improving the navigability of a graph by connecting vertices not only to their closest neighbors but also to some farther nodes. This is typically achieved by eliminating triangular connections within the graph, which results in a node's neighborhood list potentially including more distant neighbors that do not form triangular connections with the node. 
%\karima{This comment was not addressed: This paragraph should be moved before Incremental Insertion since the latter refers to distant nodes. Shouldn't the discussion on Kleinberg be added here rather than in II?}
%\ilias{Kleinberg's work focused on small-world networks, where a few edges are added randomly to achieve logarithmic convergence. The II model introduced the concept of long-range edges during the random insertion of nodes, forming long edges between some nodes due to their insertion order. This concept has not been discussed in ND, first introduced by Toussaint in RNG~\cite{rng}. the long-range edges result indirectly from pruning close neighbors, allowing nodes to connect to more farthest neighbors with available edges, thus acquiring long range edges.}

\begin{comment}
\noindent{\bf{Neighborhood diversification (ND)}} is a process that aims at diversifying \karima{\bf explain what is meant by diversify} the list of candidate neighbors for a node during the index building phase\ilias{to contains not only the closest neighbors to the node, but may include few fatherest nodes that improve the navigability through the graph during search}. This is achieved by eliminating triangular connections within the graph, which results in a node's neighborhood list potentially including more distant neighbors that do not form triangular connections with the node. ND-based graph methods leverage ND strategies such as relative neighborhood diversification, based on the Relative Neighborhood Graph (RNG)~\cite{rng}, or its approximations, to select neighbors for each node from its top \( k \)  candidate approximate nearest neighbors. The concept of 'relatively close' neighbors, foundational to RNG, was introduced by Lankford in 1969~\cite{lankford69}. According to this principle, two vertices \( (V_i, V_j) \) from the set \( {V}^2 \) are considered relatively close if and only if 
$
\text{dist}(V_i,V_j) \leq \max[\text{dist}(V_i,V_w),\text{dist}(V_w,V_j)] \quad \text{for all} \quad w = 1,...,n, w\neq i,j.$
This idea has been effectively utilized in recent approximate graph-based similarity search algorithms as a heuristic to refine edges. The aim is to minimize the required number of comparisons and distance calculations to reach the query's nearest neighbor (NN) region during beam search, enhancing the search's efficiency and accuracy.
\end{comment}
%These methods employ ND strategies such as relative neighborhood~\cite{rng} diversification or its approximations, for selecting neighbors for each node out of the list of its top $k$ approximate nearest neighbors. The concept of 'relatively close' neighbors, which forms the foundation of Relative Neighborhood Graph~\cite{rng}, was first introduced by Lankford in 1969~\cite{lankford69}. According to this principle, two vertices \( (V_i, V_j) \) from the set \( {V}^2 \) are considered relatively close if and only if \( \text{dist}(V_i,V_j) \leq \max[\text{dist}(V_i,V_w),\text{dist}(V_w,V_j)] \) for all \( w = 1,...,n, w\neq i,j \).

\noindent{\bf{Divide-and-Conquer (DC)}} is a strategy that splits a dataset into multiple, possibly overlapping, partitions, then builds a separate graph on each partition. 
Some approaches such as SPTAG~\cite{SPTAG4} and HCNNG~\cite{hcnng} combine the individual graphs into one large graph, on which a beam search is performed, while ELPIS~\cite{elpis} maintains the graphs separate and searches them in parallel.

%Next, we will discuss in depth two key paradigms that have a strong impact on query performance (Section~\ref{sec:experiments}), namely, SS and ND.

\subsection{Seed Selection}
\label{sec:ss}
While SotA graph-based vector search methods adopt diverse strategies for constructing the graph, they virtually all use beam search for query answering (Algorithm~\ref{alg:beamsearch}).
%The Beam Search initiated with a set of entry points \( s \) of size $ |s| \geq 1$, the algorithm maintains a candidate set \( C \) and a visited list \( R \), progressively exploring the graph by iteratively selecting the closest non-visited point to \( V_Q \) from \( C \), updating \( C \) with its neighbors, and marking it as visited. The beam width \( L \), ensuring a balance between accuracy and computational efficiency, limits the size of \( C \) at each step, focusing the search on the most promising candidates. The process continues until all candidates are visited, concluding with the return of the \( k \) points closest to \( V_Q \) from the candidate set. This approach ensures a guided and efficient traversal of the graph, making it a practical choice for graph-based ANNs task.
because it usually retrieves good answers if the graph is well-connected. However, choosing the right nodes to visit first has an impact on how quickly good answers are found. The longer the graph traversal, the higher the number of visited nodes, and the slower the search.
%Although using a proximity graph However, despite the high likelihood of reaching NN answers in a well-connected graph using the beam search procedure, the efficacy of the search may significantly be influenced by the choice of starting points, which can lead to varying amounts of distance calculations required to retrieve NN answers.
%Even if in a given well connected graph we have high chance to reach the NN answers following the beam search procedure on a graph, however, the points where we start our search may affect the search efficiency, leading to either inquire more or less distance calculations to retrieve NN anwers.
%but they vary in their approach to selecting entry points, also known as seeds selection\footnote{in the following paragraphs, we use seeds and entry nodes interchangeably}. 
Several methods build one or more index(es), in addition to the graph, on top of a sample of data points. These additional indexes are used during query answering to find the entry points in the graph.
%For instance, HNSW~\cite{hnsw} employs a hierarchical, multi-resolution NSW graphs, whereas SPTAG~\cite{SPTAG4}, EFANNA~\cite{efanna}, and HCNNG~\cite{hcnng} leverage multiple K-D Trees~\cite{kdtree} or BK-trees~\cite{bkmtree} to acquire initial seeds, and IEH~\cite{ieh} resorts to hash tables.
%In contrast, methods like DPG~\cite{dpg}, NSG~\cite{nsg}, Vamana~\cite{vamana}, and NSSG~\cite{nssg} argue that such an approach may not yield benefits in practice, opting instead for predefined representative points, such as the medoid of the graph (the node with the minimum sum of distances to others), or points randomly sampled before each search.
However, to the best of our knowledge, none of these methods have provided sufficient theoretical or empirical evidence to support their choices for seed selection. In this paper, we conduct an in-depth study of the different seed selection techniques proposed in the literature:
%
%\begin{enumerate}
%\item 

%\noindent (1) \textbf{Stacked-NSW (SN)}: \ilias{Inspired by skip lists~\cite{skiplist}, HNSW~\cite{hnsw} constructs hierarchical multi-resolution graphs~\cite{nsw14} for seed selection. Each level contains a diversified NSW graph built using a sample of nodes from the level below, with the lowest layer sampling from the entire dataset. Stacked NSW is constructed by assigning each node a maximum level L generated following the formula:
%\( L = \frac{-\ln(\xi)}{\ln\left(M/2\right)} \) \cite{hnsw} (Eq. 1), 
%\setlength{\abovedisplayskip}{0.15cm}
%\setlength{\belowdisplayskip}{0cm}
%\begin{equation}
%\centering
%L = \frac{-\ln(\xi)}{\ln\left(M/2\right)} 
%\hspace{0.2cm} \cite{hnsw} \hspace{1cm} (\text{Eq. 1}) \tag*{}
%\end{equation}
%The parameter $\xi$ is uniformly distributed between 0 and 1, with $\ln()$ as the natural logarithm. $M$ denotes the maximum out-degree and controls the probability of a node’s top layer. Nodes with $L > 0$ are inserted incrementally from layer $L$ down to 1. 
%Higher $M$ reduces both nodes in and the number of layers, while lower $M$ increases them. During querying, the algorithm performs a greedy search from a fixed entry point at the top layer, descending through layers via the closest nodes until reaching the bottom, where the final node serves as the seed. %Due to space constraints, our analysis of $M$'s impact on node and layer distribution is provided in the supplementary material~\cite{url/GASS}.
%}


\noindent (1) \textbf{Stacked-NSW (SN)}: Inspired by skip lists~\cite{skiplist}, HNSW~\cite{hnsw} constructs hierarchical multi-resolution graphs~\cite{nsw14} for seed selection. Each level contains a diversified NSW graph built using a sample of nodes from the level below, with the lowest layer sampling from the entire dataset. Stacked NSW is constructed by assigning each node a maximum level L. Nodes with L>0 are incrementally inserted into the graphs from layer L down to 1. 
The maximum level L is: % generated using the formula: 
%\( L = \frac{-\ln(\xi)}{\ln\left(M/2\right)} \) \cite{hnsw} (Eq. 1), 
%setlength{\abovedisplayskip}{-0.1cm}
% \setlength{\belowdisplayskip}{0cm}
%\vspace{cm}
\begin{equation}
\hspace{3cm}
L = \frac{-\ln \left(\xi\right)}{\ln\left(M\div2\right)} 
 \hspace{2cm} (\text{Eq. 1 \cite{hnsw} }) \tag*{}
\end{equation}
where $\xi$ is a uniformly random number between 0 and 1, %$\ln()$ denotes the natural logarithm function, 
and $M$ is the maximum out-degree in the graphs, controlling the probability distribution of a node's maximum layer. 
A high M decreases the number of nodes represented in hierarchical layers as well as the number of layers, while a lower M allocates more nodes to the hierarchical layers, thus generating more hierarchical layers. 
During query answering, it starts a greedy search from a fixed entry point at the top layer, descending layer by layer, each time starting from the closest node found in the previous layer, until reaching the bottom layer. The node selected in the bottom layer and the nodes connected to it serve as seed nodes.

%Where $\xi$ represents a uniformly random number between 0 and 1, $\ln()$ denotes the natural logarithm function, and $M$ stands for the maximum out-degree in the graphs, and also controlling the probability distribution of a node's maximum layer. A high M decreases the number of nodes represented in hierarchical layers as well as the number of layers, while a lower M allocates more nodes to the hierarchical layers, thus generating more hierarchical layers. During query answering, it starts a greedy search from a fixed entry point at the top layer, descending layer by layer, each time starting from the closest node found in the previous layer, until reaching the bottom layer. The node selected in the bottom layer and the nodes connected to it serve as seed nodes.
     

    %KEY IDEA : As M increases in HNSW, lower-layer node concentration rises, reducing levels to conserve distance calculations for base layer beam search. Conversely, decreasing MM allocates more nodes to the top layer, enhancing seed quality and improving performance, particularly on sparse graphs.
    
%\item 
\noindent (2) \textbf{K-D Trees (KD)}: Utilized by EFANNA~\cite{efanna}, SPTAG-KDT~\cite{SPTAG2}, and HCNNG~\cite{hcnng}, this technique involves constructing single or multiple K-D Tree(s)~\cite{kdtree} on a dataset sample. During search, a set of seed points is retrieved by running a depth-first search traversal (DFS) on the K-D Tree structure(s), warming up the set of candidate answers. The node closest to the query is selected as an entry node.

\noindent (3) \textbf{LSH}: Utilized by IEH~\cite{ieh}, this strategy constructs an LSH index
%~\cite{add citation} 
on a sample of the dataset and uses it during search to return a set of seed points, using one as an entry node. %~\karima{\bf Check if this is correct}


%\item 
\noindent (4) \textbf{Medoid (MD)}: 
%Adopted by DPG~\cite{dpg}, NSG~\cite{nsg}, and Vamana~\cite{vamana}, 
This strategy fixes the medoid node as entry point during query answering and uses its neighbors as seeds.

%\item 
\noindent (5) \textbf{Single Fixed Random Entry Point (SF)}: A random node is selected and fixed as the entry point for all searches. This node and the nodes connected to it are used as seeds.
%\karima{\bf Ilias, add references to methods that use this strategy.}\ilias{I add it as a base line to compare with medoid}\karima{\bf Ilias, I don't understand your answer. This was never used in the literature?}\ilias{no, this was a baseline to compare medoid with, which shows that using one single point as entry node is not efficient, weither its representative(medoid) or random}
    
%\item 
\noindent (6) \textbf{K-Sampled Random Seeds (KS)}: 
For each query, \( k \) random nodes are selected to warm up the set of candidate answers. This approach is supported in DPG~\cite{dpg}, NSG~\cite{nsg}, and Vamana~\cite{vamana} which choose the medoid as the entry point and enhance the list of initial seeds with the random nodes. %~\karima{\bf There seems to be a contradiction here. We just said that Vamana and NSG use the medoid as entry node and its neighbors as initial candidates.}~\ilias{vamana and nsg use medoid + k random sampling to fill the candidate set in Alg.1 line~\ref{alg:beam:line:init}} ~\karima{\bf There is still a contradiction with the Medoid description which says the neighbors medoids are the initial candidates. Rephrase Medoid in this case.}
%\karima{\bf Ilias, same comment as before, how is the entry point selected? is it the first NN?}\ilias{technically, we give the set of of initial seeds s(k sampled random points) to algorithm1 and run the beam search, the beam search select from this set the most promising candidate from which it starts search, we have to note that beam search may select other node from the k points during search if the already selected node guide the search to local minimum, that we generally increase in the beamwidth during search to reach good accuracy, and the same reason why KS doesn't perform the best in large scale dataset in our experiments }
%\end{enumerate}

\noindent (7) \textbf{Balanced K-means Trees (KM)}: Utilized by SPTAG-BKT~\cite{SPTAG2}, this technique constructs Balanced K-means Trees (BKT) ~\cite{bkmtree} on a dataset sample. During search, seed points are retrieved via depth-first search (DFS) on the BKT structure(s).



\subsection{Neighborhood Diversification}
\label{sec:nd}
 The goal of Neighborhood Diversification (ND) is to create a sparse graph structure, where nodes are connected both to their close neighbors and to some further nodes. This is because traversing a graph in which nodes are only connected to their close neighbors incurs many unnecessary comparisons before reaching the promising regions. This method first appeared in the Relative Neighborhood Graph (RNG)~\citep{rng,toussaint02}, which builds an undirected graph from scratch or by pruning an existing Delaunay Graph, then removes the longest edge in each triangle of three connected points.
 This approach, and other variations that exploit different geometric properties of the graph, have been adapted for directed graphs by several graph-based ng-approximate vector search methods ~\citep{hnsw,dpg,nsg,nssg,vamana,SPTAG4,elpis}. We identify three main ND strategies used by these methods: Relative Neighborhood Diversification (RND), Relaxed Relative Neighborhood Diversification (RRND) and Maximum-Oriented Neighborhood Diversification (MOND).
Note that the ND strategy is different from Kleinberg's small world network graph~\cite{kleinberg2000, kleinberg2002}. The long-range links in the latter are achieved through a random selection of nodes, while in ND they result indirectly from pruning the close neighbors of each node.
 %Neighborhood diversification constitutes a versatile step within ND-based graph construction. Where, for a query node $X_q$ and a list of candidate neighbors $C_q$, the core objective is to strategically select specific candidate neighbors $X_i$ for inclusion in the set of $X_q$'s neighbors $R_q$. At its heart, ND diversifies these neighbor selections, optimizing the efficiency during search to traverse the graph, while maintaining the notion of proximity within it.
Assume: %Consider the following:

\begin{itemize}
  \setlength\itemindent{1em}  % Adjust indentation here
  \item \(X_q\) is the query node, i.e. the node to be inserted in the graph.
  \item \(R_q\), the list of current closest neighbors to \(X_q\).
  \item \(C_q\), the list of candidate neighbors to \(X_q\) not yet in \(R_q\).
  \item \(X_j\) is a candidate from \(C_q\) considered for inclusion in \(R_q\).
  \item \(X_i\) is a node already in the set \(R_q\).
  \item \(\text{dist} \left(X_i, X_j\right)\) is the Euclidean distance between \(X_i\) and \(X_j\).
\end{itemize}
\begin{defn}[RND]\label{def:rnd}
The node \(X_j\) is added to \(R_q\) if and only if the following condition holds:
	\[
	\forall X_i \in R_q, \, \text{dist}\left(X_q, X_j\right) < \text{dist}\left(X_i, X_j\right) \quad (\text{Eq. 2})
	\]
\end{defn}

\begin{defn}[RRND]\label{def:rrnd}
For a relaxation factor \(\alpha \geq 1\), the node \(X_j\) is added to \(R_q\) if and only if the following condition holds:
	\[
	\forall X_i \in R_q, \, \text{dist}\left(X_q, X_j\right) < \alpha \cdot \text{dist}\left(X_i, X_j\right) \quad (\text{Eq. 3})
	\]
	%Where \(\alpha\) is the relaxation factor (e.g., \(\alpha = 1.5\)).
\end{defn}

\begin{defn}[MOND]\label{def:mond}
For an angle \(\theta \geq 60^\circ\), the node \(X_j\) is added to \(R_q\) if and only if the following condition holds:
 	\[
	\forall X_i \in R_q, \, \angle\left(X_j X_q X_i\right) > \theta \quad (\text{Eq. 4})
	\]
\end{defn}

Figure ~\ref{fig:ND:example} shows the results after applying different ND approaches on the candidate neighborhood list \( C_q = \{ X_1, X_2, X_3, X_4 \} \) for node \( X_q \).
RND (Figure~\ref{fig:ND:RND}) was first used in the context of vector search by HNSW~\cite{hnsw}. Other methods adopted RND including NSG~\cite{nsg}, SPTAG~\cite{SPTAG4} and ELPIS~\cite{elpis}.  Per Definition~\ref{def:rnd}, RND operates over the candidate neighborhood list by adding $X_j$ to $R_q$ if its distance to $X_q$ is smaller than the distance between $X_j$ and any neighbors $X_i$ of $X_q$. 
In the example of Figure~\ref{fig:ND:RND}, $X_q$ is going to connect to $X_1$ since it is the closest neighbor ($\sigma = dist\left(X_q,X_1\right)$). Both $X_2$ and $X_3$ are pruned from the $X_q$ neighborhood list as they are closer to $X_1$ than $X_q$, and $X_4$ will be added to $X_q$ eventually since dist($X_q$,$X_4$) $\leq$ dist($X_1$,$X_4$).
In contrast, RRND (Figure~\ref{fig:ND:RRND}), proposed by Vamana~\cite{vamana}, introduces a relaxation factor $\alpha$ (with $\alpha \geq 1.5$) where $X_j$ is added to $R_q$ if its distance to $X_q$ is smaller than $\alpha$ times its distance to $X_i$, a neighbor of $X_q$ in $R_q$, relaxing the property to prune less candidate neighbors (hence $X_2$ will not be pruned in this case, but $X_3$ is pruned due to its proximity to $X_2$). 
When $\alpha = 1$, RRND is reduced to RND.
Finally, MOND (Figure~\ref{fig:ND:MOND}) was proposed by DPG~\cite{dpg} and used in NSSG~\cite{nssg} as well. 
It aims at maximizing angles in the graph topology (cf. Definition~\ref{def:mond}), guiding the selection process via the angle threshold $\theta$ (e.g., $\theta = 60^\circ$). MOND prunes the candidate neighbors of a node to favor edges pointing in different directions ($X_2$ is pruned since $\angle X_1X_qX_2$ < $60^\circ$, while $X_q$ connects with $X_3$ since  $\angle X_1X_qX_3$ > $60^\circ$ and with $X_4$ since  $\angle X_1X_qX_4$ < $60^\circ$ and  $\angle X_3X_qX_4$ < $60^\circ$). 
Note that any nodes pruned by RRND and MOND will eventually be pruned by RND, but not vice versa. 
%are approximations to Equation 2 (RND) \karima{what do we mean by approximation?}\ilias{all edges pruned by RND and MOND will be pruned eventually by RND, but not vice versa}, introducing a relaxation factor \( \alpha \) and an angle threshold \( \theta \) respectively. 
Refer to~\cite{url/GASS} for a detailed proof.
%\ilias{I changed the name of the nodes in the figure. For a node $X_q$, a set of candidate neighbors $C_q={X_i,X_j,X_k,X_l}$ and set of neighbors $R_q$ of size 2(maximum outdegree). $X_q$ is connected to $X_i$ in the three scenarios since $X_i$ is the closest node to $X_q$, thus $X_i \in R_q$. For the next candidate neighbors, RND prunes both $X_j$ and $X_k$, and add $X_l$ to $R_q$ since both $X_j$ and $X_k$ are closer to  $X_i$  than $X_q$. Whereas Relaxed RND (RRND) connects $X_q$ with add $X_j$}
%I changed the name of the nodes in the figure, for a node $X_q$, a set of candidate neighbors $C_q={X_i,X_j,X_k,X_l}$. $X_q$, is connected to $X_i$, in the three scenarios since $X_$, is the closest node to A, RND prunes both C and D out of the list and connect A to E, since both D and C are closer to B than A. When we relaxed this requirement in RRND, A connects to C. In the case of MOND, C is pruned since the angle $\angle CAB < \theta$ since MOND maximize the angle between the neighbors. D is selected since $\angle DAB > \theta$.
%\karima{\bf Ilias, explain the intuition here, how cos is different from using the ED dist}
%: $\forall X_j \in R_q, \cos(\angle X_iX_qX_j) < \cos(\theta)$, where $\theta$ delineates the angle threshold (e.g., $\theta = 60^\circ$). Figure\ref{rng:ND:exp} shows the difference between the three ND approaches. For a node A, a set of candidate neighbors {B,C,D,E} and a maximum outdegree of 2. A is connected to B in the three scenarios since B is the closest node to A, RND prunes both C and D out of the list and connect A to E, since both D and C are closer to B than A. When we relaxed this requirement in RRND, A connects to C. In the case of MOND, C is pruned since the angle $\angle CAB < \theta$ since MOND maximize the angle between the neighbors. D is selected since $\angle DAB > \theta$.

%In the domain of graph-based approximate similarity search, we explore Neighborhood Diversification (ND) techniques that redefine how nodes connect within graphs. These methods are tailored to enhance the diversity and efficiency of node relationships, making the performance ND graph-based approximate similarity search methods rise to another level. In this section, we'll delve into three fundamental ND approaches: the Relative Neighborhood Graph (RNG), recognized for selecting 'Relative Close Neighbors,' its more versatile counterpart, 'Relaxed RNG,' and the 'Angle-based' method. Each of these methods introduces unique strategies, contributing significantly to the evolving landscape of graph-based approximate similarity search


%  RNG: Relative Neighborhood Diversification (RN-Div)
% Relaxed RNG: RRNG (Relaxed Relative Neighborhood Diversification)
%Angle-based: MOND (Max Orientation Neighborhood Diversification)

\newcommand{\subfigwidth}{0.27\columnwidth} 
\newcommand{\dottedheight}{3.5cm} 
\newcommand{\vdottedline}[1]{
	\begin{tikzpicture}
	\draw[dotted] (0,0) -- (0,#1);
	\end{tikzpicture}
}
\begin{figure}[tb]
	\begin{subfigure}[b]{\subfigwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{../img-png/figures/rnd.png}
		\caption{RND}
		\label{fig:ND:RND}
	\end{subfigure}
	% Vertical dotted line
	\vdottedline{\dottedheight}
	\begin{subfigure}[b]{\subfigwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{../img-png/figures/rrnd.png}
		\caption{RRND}
		\label{fig:ND:RRND}
	\end{subfigure}
	% Vertical dotted line
	\vdottedline{\dottedheight}
	\begin{subfigure}[b]{\subfigwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=\textwidth]{../img-png/figures/mond.png}
		\caption{MOND}
		\label{fig:ND:MOND}
	\end{subfigure}
    \caption{Neighborhood diversification approaches}
    \label{fig:ND:example}
\end{figure}


\subsection{A Taxonomy}
Figure~\ref{fig:roadmap} depicts the SotA graph-based approaches, classified based on the five design paradigms: SS, NP, II, ND, and DC.
%Seed Selection (SS), Neighborhood Propagation (NP), Incremental Insertion (II), Neighborhood Diversification (ND), and a Divide-and-Conquer (DC) protocol. 
The taxonomy also reflects the chronological development of the methods. 
Directed arrows %between methods 
indicate the influence of one method on another. Within the ND category, distinctions are made between different strategies, i.e., No Neighborhood Diversification (NoND), RND, RRND, and MOND (cf. Section~\ref{sec:nd}). 
We identify the SS strategy of each method: KS, KD, SN, MD, LSH, and KM (SF is not used by any SotA method, but we consider it as an alternative strategy). 
Additionally, some methods use more than one strategy (e.g. NSG and VAMANA use KS and MD), or offer the flexibility to use different strategies (e.g., SPTAG can use either KD or KM). 
Note that a method can exploit one or more paradigms; e.g., HNSW uses incremental node insertion and prunes each node's neighbors using the RND approach, thereby being classified as both II and ND. %in contrast to NP-based approaches. 
%This is due to their substantial improvements compared to early NP-based methods that were NP-based
%\karima{Explain the meaning of + and |. Provide a more detailed description of this figure}.
KGraph~\cite{kgraph} was the first to use NP to approximate the exact k-NN graph (k-NNG) (with quadratic complexity), and influenced numerous subsequent methods, including IEH~\cite{ieh} and EFANNA~\cite{efanna}. In parallel, NSW~\cite{nsw11} introduced the II strategy for graph construction.
%, marking the beginning of insertion-based methods. 
HNSW~\cite{hnsw} and DPG~\cite{dpg} leveraged ND to enhance NSW and KGraph~\cite{kgraph}, respectively.
%~\karima{\bf Check that this is correct}. 
The good performance of HNSW and DPG encouraged more methods to adopt the ND paradigm, including NGT~\cite{ngt_library}, NSG~\cite{nsg} and SSG~\cite{nssg}, which apply ND on the NP-based graph EFANNA~\cite{efanna}. SPTAG~\cite{SPTAG4} combined DC with ND. 
%Vamana~\cite{vamana}, although inspired by NSG \karima{give more details, what ideas from NSG were used by Vamana} , starts with a random graph as the base graph.
Vamana~\cite{vamana} adopts NSG's idea of constructing the graph through beam search 
 %\karima{why beam search? It is not a paradigm.}\ilias{since its similar across the methods; our initial focus was on evaluating four distinct paradigms (NP, II, ND, DC) for graph construction, with SS considered as a separate key component.then, we included SS too due to its importance in the last revision}
 and ND. 
 However, Vamana constructs its graph by refining an initial base random graph in two rounds of pruning, using RRND and RND. Inspired by HNSW, Vamana and NGT proposed variants %to their original algorithms 
 that support incremental graph building~\cite{diskanncode, ngt_library}, but we classify them as ND-based per the ideas proposed in the original papers. HCNNG~\cite{hcnng} was influenced by SPTAG~\cite{SPTAG4} and adopted a DC approach for constructing the graph without adopting ND. ELPIS~\cite{elpis} also adopted a DC strategy but leveraged both II and ND. 
HVS~\cite{hvs} and LSHAPG~\cite{lshapg} both propose new seed selection structures for HNSW, with the latter additionally adopting a new probabilistic rooting approach. Note that earlier approaches, except from NSW, were mainly NP-based; however, recent studies have focused on devising methods that leverage the ND, II, and DC paradigms because they lead to superior performance 
%\cite{aumuller2017ann, shimomura2021survey, graph-survey-vldb} \karima{Ilias, what are these citations for?}\ilias{for the trend in recent methods} 
(cf. Section~\ref{sec:experiments}).

%~\karima{\bf This is not clear}. 
%~\karima{\bf Check that this is correct}.

%State-of-the-art graph-based similarity search methods typically use the same beam search algorithm and they differ mainly in how they build the graph structure. 
\begin{figure}[tb] 
		\captionsetup{justification=centering}
%		\centering
%\hspace*{-0.5cm}
		\includegraphics[width=0.9\columnwidth]{../img-png/figures/RoadMAPGANNS.png}
		%\vspace*{-0.2cm}
        \caption{Graph-based ANN indexing paradigms}
		\label{fig:roadmap}
	
 \end{figure}


\subsection{State-of-the-Art Approaches}
%\subsubsection{KGraph}%2011%O(n^1.14)
\noindent{\bf KGraph}~\cite{kgraph} reduces the construction cost of an exact k-NNG, which has a quadratic worst-case complexity. It constructs an approximate k-NNG by refining a random initial graph with an empirical cost of \( O\left(n^{1.14}\right) \)~\cite{nndescent}. 
This refinement process, also known as NNDescent~\cite{nndescent} (Neighborhood Propagation), aims at improving the approximation of the \( k \)-NN graph by assuming that the neighbors of a vertex \( u \) are more likely to be neighbors of each other. 
The process iterates over all graph vertices \( u \in {V} \): 
for each vertex \( u \) and pair \( (x,y) \) of its neighbors, it adds \( x \) to the neighbors of \( y \) and vice-versa, keeping the closest \( k \) neighbors of \( u \).
%\subsubsection{Iterative Expanding Hashing (IEH)} 

 \noindent{\bf Navigable Small World (NSW)}~\cite{nsw11,nsw14} is an approximation of a Delaunay graph which guarantees the small world property~\cite{watts98}, i.e. the number of hops $L$ between two randomly chosen vertices grows to the logarithm of graph size $n$ such that $L \propto Log\left(n\right)$.
An {\it NSW} graph is based on the VoroNet graph~\cite{voronet}, an extention of Kleinberg's variant of Watts-Strogatz's small world model graph~\cite{kleinberg2000,kleinberg2002},  The VoroNet graph is built incrementally by inserting a randomly picked vertex to the graph and connecting it to 2d+1 
neighbors selected using a beam search on the existing vertices in the graph.
Once this process completes, the first built edges would serve as long-range edges to quickly converge toward nearest neighbors~\cite{voronet}. The resulting graph was proved to guarantee the small world network property~\cite{voronet,beaumont07}. 

 \noindent{\bf Iterative Expanding Hashing (IEH)}~\cite{ieh} follows the same process as KGraph to construct an approximate k-NNG; however, it refines an initial graph where the candidates for each node are generated using a hashing function.
%~\karima{\bf This is not clear}. 
Two extensions of IEH have been proposed to better leverage advanced hashing methods for generating initial candidates: IEH-LSH~\cite{iehlsh} and IEH-ITQ~\cite{iehitq}. All these methods use NNDescent to finalize the graph connections.
%\subsubsection{EFANNA}

 \noindent{\bf EFANNA}~\cite{efanna} selects seeds similarly to KGraph~\cite{kgraph} and IEH~\cite{ieh} and refines candidates using NNdescent. It builds an approximate $k$-NNG by selecting initial neighbors of each node using randomized truncated K-D Trees \cite{dasgupta2008random} and refining the graph using NNDescent~\cite{nndescent}. 
During search, EFANNA uses the pre-built trees to select seeds, then
%$s$, 
runs a beam search on the graph index.

%\subsubsection{Navigable Small World (NSW)}
%Expanding upon the VoroNet graph, initially conceptualized for peer-to-peer network solutions, Krylov et al. adapted it to support similarity search~\cite{nsw11,nsw14}. In this adaptation, NSW initiates multiple greedy search processes from multiple randomly selected vertices. Each new vertex is bi-directionally connected to its k approximate nearest neighbors. However, a side effect of this approach is that the first inserted elements transform into hubs, possessing a high out-degree by the completion of the graph construction. The number of these hubs increases logarithmically with the addition of more elements to the graph, leading to an elevated number of distance calculations during search. 
%\karima{Ilias, add here the discussion about the ND strategy being used in NSW instead of the strategy described in II.}\ilias{the NSW doesn't use ND, but exploit the early edges not pruned as long range edges; to keep these edges to not be pruned NSW use a large outdegree, which resulted in a high number of comparisons(in nutshell, nsw aims for a small diameter(small worldness) by exploiting these long range edges, but the price was to keep a high outdegree; thus in HNSW they added ND diversification to pruned the edges leading to redundant region) }
%\karima{This description seems related to the discussion of ND. Is this correct? If yes, move all this background to the ND subsection.}\ilias{no it's not related}

%\subsubsection{Hierarchical Navigable Small World (HNSW)} 
\noindent{\bf Hierarchical Navigable Small World (HNSW)}~\cite{hnsw} improves the scalability of NSW~\cite{nsw11,nsw14}  by proposing RND to sparsify the graph and a hierarchical seed selection strategy (SN) 
to shorten the search path during index building and query answering. Each hierarchical layer includes all nodes in the layer above it, with the bottom (a.k.a. {\it base}) layer containing all points of the dataset \( {S} \), 
%In the index construction, each new element \( x \in {S} \) is randomly assigned a maximum layer \( l_0 \leq l_i \leq l_{max} \), and is then added to layers \( l_i, \ldots, l_0 \) in an iterative manner. In each layer, 
HNSW builds an NSW graph incrementally. However, HNSW diverges from NSW in that it refines the candidate nearest neighbors, identified through beam search on the nodes already in that layer using RND. 
%Subsequently, the node is bi-directionally connected to its approximate nearest neighbors to maintain a good connectivity with early inserted nodes, and if the connections of a neighbor exceed the maximum out-degree (which is \( M \) in the hierarchical layers and \( 2*M \) in the bottom layer), HNSW applies the RND method to reduce the node neighborhood list to the appropriate maximum out-degree.
During query answering, HNSW utilizes SN to quickly find an entry point in the base layer to start the beam search.

%the region of the graph containing nodes close to the query using greedy search and to find a closer entry node to the query, from which HNSW starts the beam search on base layer.

%\subsubsection{Diversified Proximity Graph (DPG)}
\noindent{\bf Diversified Proximity Graph (DPG)}~\cite{dpg} 
%builds an approximate KNN graph structure based on Kgraph~\cite{kgraph}. DPG enhances the KgrapFh sparsity by pruning the edges of each vertex using different neighborhood diversification method than HNSW, where DPG maximize the average angle between the remaining outgoing edges of each node. namely Maximum Oriented Neighborhood Diversification(MOND) to 
%following the MOND approach described in section 3.3, however in the implementation, dpg uses RND rather than MOND as they explained in the article. 
%introduces a new method for constructing an approximate KNN graph, extending Kgraph~\cite{kgraph} 
extends KGraph~\cite{kgraph} by diversifying the neighborhoods of its nodes through edge orientation, a technique we refer to as Maximum-Oriented Neighborhood Diversification (MOND) in Section 3.4.
%~\karima{\bf 3.4? Add labels rather than hard-coded section numbers}. 
MOND’s main objective is to maximize the angles between neighboring nodes, contributing to a sparsed graph structure. This process is iteratively applied to all nodes. After that, the directed graph is transformed into an undirected one, enhancing its connectivity. Nevertheless, note that DPG's publicly available implementation~\cite{dpgrepo} utilizes RND rather than MOND for neighborhood diversification.

\noindent{\bf NGT}~\cite{ngt_library} is an approximate nearest neighbor (ANN) search library developed by Yahoo Japan. It offers two construction methods: one extends KNN graphs with reverse edges, forming bi-directed KNN graphs~\cite{ngtpanng1}, while the other incrementally builds graphs similar to HNSW with a range-based search strategy~\cite{ngtpanng2}.  In this study, we consider the former~\cite{ngtpanng1}. Additionally, the library includes methods that employ quantization for highly efficient search.
NGT maintains efficiency by pruning neighbors via RND and using Vantage-Point Trees~\cite{vptree} to select seed nodes for accurate query results.

\noindent{\bf Navigating Spreading-out Graph (NSG)}~\cite{nsg}, similarly to DPG, builds an approximate k-NNG first. But, unlike DPG, it builds an EFANNA graph rather than a KGraph. It then diversifies the graph using RND. 
%adopted by HNSW~\cite{hnsw} to diversify the graph. 
%For each node $v \in {V}$, NSG performs a beam search starting from the approximate medoid on the EFANNA graph~\cite{efanna} to get the set of nodes visited during the search as candidate nearest neighbors. The set is refined according to the RND approach~\cite{nsg}. 
%Using the set of visited nodes helps NSG construct long-range edges. NSG adds bidirectional connections between the nodes and its neighbors after the pruning, and uses RND to limit the neighbors out-degree from  exceeding the maximum allowed out-degree. 
At the end, NSG creates a depth-first search tree to verify the connectivity of the graph. If there is a vertex that is disconnected from the tree, NSG connects it to the nearest node in the tree to ensure graph connectivity.

\noindent{\bf SPTAG}~\cite{SPTAG4} is a library for approximate vector search proposed by Microsoft. 
SPTAG follows a DC approach and is based on multiple existing works.  
%The first proposed version SPTAG-KDT~\cite{SPTAG1,SPTAG2} 
It selects small dataset samples on which it builds either K-D Trees~\cite{kdtree} or Balanced K-means Trees~\cite{bkmtree}. These strutures will be used for seed selection during query answering. Then it clusters the full dataset using multiple hierarchical random divisions of TP Trees~\cite{tptree}, builds an exact k-NN graph on each cluster (i.e., leaf) and refines each graph using ND. The graphs are merged into one large graph index for query processing. 

%\subsubsection{Vamana}
\noindent{\bf Vamana}~\cite{vamana} is similar to NSG in considering the set of visited nodes when building long-range edges within the graph. However, instead of using EFANNA~\cite{efanna}, Vamana uses a randomly generated graph with node degree~$\geq~log\left(n\right)$ to ensure the initial graph connectivity~\cite{erconnect}. 
Then, for each node, Vamana runs a beam search on the graph structure to get the visited node list $R$, which will be refined in the first round using RRND. After adding bi-directional edges to selected neighbors, the neighbors that exceed the maximum allowed out-degree will refine their neighborhood list following an RND process. Then, Vamana repeats the same refinement process a second time to improve the graph quality, this time using RRND with $\alpha \geq 1$ to increase the connectivity within the graph.
%~\karima{\bf What does graph connectivity do with RRND?}.

%\subsubsection{SSG} 
\noindent{\bf SSG}~\cite{nssg} integrates the MOND approach from DPG~\cite{dpg} and closely follows the steps of NSG~\cite{nsg} and DPG~\cite{dpg} in index building from a foundational graph. Instead of performing a search for each node to acquire candidates, SSG~\cite{nssg} employs a breadth-first search on each node to assemble candidate neighbors through local expansion on a base graph (EFANNA). When the maximum size for the candidate neighbors is achieved, SSG reduces the neighbors in the list by enforcing the MOND diversification strategy, pruning the candidate nodes forming an angle smaller than a user-defined parameter $\theta$ with the already existing neighbors of the concerned node. After iteratively applying this method to all nodes, SSG~\cite{nssg} enhances connectivity by constructing multiple DFS trees from various random points, in contrast to NSG's~\cite{nsg} singular DFS approach.

\noindent{\bf Hierarchical Clustering-based Nearest Neighbor Graph (HCNNG)}~\cite{hcnng} was inspired by SPTAG. It employs hierarchical clustering to randomly divide the dataset into multiple subsets. This subdivision process is executed several times, resulting in a collection of intersecting subsets. On each subset, HCNNG constructs a Minimum Spanning Tree (MST) graph. 
Following this, the vertices and edges from all the MSTs are merged to form a single, connected graph. 
To facilitate the search process, HCNNG constructs multiple K-D Trees~\cite{kdtree}, to identifying entry points during query search. 
%\subsubsection{SPTAG}
%parameters: number of trees and sizes. kdt and bkt on each sample for SS. cluster the data using tpt trees on the full dataset. then merge the clusters to build the graph. exact+RND. mrged into one graph

\noindent{\bf HVS}~\cite{hvs} extends HNSW's base layer by refining the construction of hierarchical layers. Instead of random selection, nodes are assigned to layers based on local density to better capture data distribution. Each layer forms a Voronoi diagram %, with nodes representing Voronoi cells, 
and uses multi-level quantization, increasing dimensionality by a factor of 2 in each lower layer.
%\(2^i\) at each lower layer, and i is the number of the layer. 
Search at the base layer is similar to that of HNSW.
%, ensuring accurate nearest neighbor retrieval while maintaining efficiency across layers.

\noindent{\bf LSHAPG}~\cite{lshapg} combines HNSW graphs with multiple hash tables based on the LSB-Tree structure~\cite{lsb} to enhance search efficiency. It leverages $L$ hash tables to retrieve seeds for beam search on the base layer, unlike HNSW, which selects a single seed through SN. LSHAPG also utilizes these hash tables for probabilistic rooting during search, pruning neighbors based on the projected distance %between its projection and the query’s projection 
before evaluating and pruning the raw vectors.
%to reduces the computational cost of neighbor pruning during beam search.


%\karima{Is a K-D Tree built on each random samples first, then a graph is constructed for each sample?}\ilias{No, it's build on a sample from the dataset}, which will be combined to yield a more accurate  \karima{why is it more accurate?}~\ilias{because it uses exact graph, however that version wasn't efficient in index building, especially when the ND methods appeared (DPG and HNSW), thus it becomes more clear that building or approximating the exact KNN graph is not one of the promising keys to improve efficiency of graph based ANN( more effort was invested since then in the ND approaches, where efanna's authors proposed N SG, then vamana and SSG appeared, in same time SPTAG also adopted the ND in its graph, and so on...)}
%approximate neighborhood graph 
%\karima{Ilias, the previous sentence is still not clear. How are k-NNGs are K-D Trees related to the samples? Are subdivisions equivalent to samples?} \ilias{No, to build the dktree, sptag select the sample from the whole dataset, we precise the sample size s <= N(#points) during indexing; before running the clustering using TPT; }, and also apply a neighborhood propagation scheme similar to NNDescent to further enhance the connectivity. 
%During search, the K-D Tree structures~\cite{bentley1975multidimensional} are used to find the initial seeds to start the beam search on the graph iteratively. 
%In a recent optimized version~\cite{SPTAG3}, SPTAG replaces hierarchical clustering  \karima{K-D Tree?}\ilias{ no, the random hierarchical clustering, the KD-tree are used for seed selections} with Trinary-Projection Trees (TPT)~\cite{tptree} and refines the exact k-NNG with RND
%~\karima{\bf RND?}. 
%Furthermore, SPTAG-BKT enhances the existing version by replacing the K-D Tree structures~\cite{kdtree} with Balanced K-means Tree structures~\cite{bkmtree}
%\karima{add citation. Is Balanced K-means equivalent to Balanced K-means Trees?}
%\ilias{not really, the balanced k-means tree algorithm uses balanced kmeans recursively to create a tree structure } 
%to avoid inaccurate distance-bound estimation in the high-dimensional space.

%\subsubsection{Hierarchical Clustering-based Nearest Neighbor Graph (HCNNG)}

%During search, HCNNG uses the K-D Trees to retrieve seeds, subsequently performing a guided beam search across the graph to locate the nearest neighbors.

%\subsubsection{ELPIS}
\noindent{\bf ELPIS}~\cite{elpis} is a DC-based approach that splits the dataset into subsets using the Hercules EAPCA tree~\cite{hercules}, where each leaf corresponds to a different subset, then builds in parallel a graph-based index for each leaf using HNSW~\cite{hnsw}. During search, ELPIS first selects heuristically an initial leaf %based on a DFS traversal of the EAPCA-based tree
and executes a beam search on its respective graph. 
The retrieved set of answers feed the search priority queues for the other leaves. 
Only a subset of leaves is selected based on the answers and the lower-bounding distances of the query to the EAPCA summarization of each leaf.  Then, ELPIS initiates multiple concurrent beam searches on the graph structures of the candidate leaves. 
%This enables ELPIS to efficiently prune unnecessary distance calculations compared to a cold start. 
Finally, ELPIS aggregates all results from candidate clusters and returns the top-k answers. 