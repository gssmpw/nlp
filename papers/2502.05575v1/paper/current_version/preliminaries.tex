\section{Preliminaries}
\label{sec:preliminaries}

%\subsection{Definitions}
\label{subsec:preliminaries-definitions}
The $ng$-approximate vector search problem is typically modeled as an $ng$-approximate $k$-NN search problem in high-dimensional vector space. 
Data points are represented as $d$-dimensional vectors in ${R}^d$, and the \textit{dissimilarity} between the points is measured using the Euclidean distance \textit{dist}. We consider a dataset collection 
%this line causes txfont ==> '=' 
$S$ = $\{V_{C_1}, V_{C_2}, \ldots, V_{C_n}\}$
 of $n$ $d$-dimensional points and a query vector $V_Q$.




\begin{defn} \label{def:knnquery}
    Given a positive integer $k$, an \textbf{exact k-NN query} over a dataset $S$ retrieves the set of vectors $A$ = $\{ \{V_{C_1}, \ldots, V_{C_k}\} \subseteq S$ \text{|} 
    $\forall V_C \in A $
    and 
    %this causes the issue
    $\forall V_{C'} \notin A$, 
    $\text{dist}\left(V_Q, V_C\right) \leq \text{dist}\left(V_Q, V_{C'}\right) \}$
    ~\cite{lernaeanhydra2}.
\end{defn}


\begin{defn} \label{def:appmatch} (An equivalent definition has appeared in~\cite{lernaeanhydra2}.)
	Given a positive integer $k$, an \textbf{ng-approximate k-NN query} over a dataset ${S}$ retrieves the set ${A}$ = $ \{V_{C_1},...,V_{C_k}\} \subseteq {S}$ in a heuristic manner, i.e., there are no theoretical guarantees about the quality of the answers. 
\end{defn}    




We use the terms vector search, similarity search, $ng$-approximate search%, approximate $k$-NN search
, and approximate search %and ANN search 
interchangeably.


\subsection{Approximate Vector Search Methods}
Vector search has been heavily studied  for over fifty years. 
Proposed approaches are either based on scans, trees, graphs, hashing, inverted indexes, or on hybrid designs combining two or more of these data structures. 
Vector search methods often rely on summarization to reduce the complexity of the search. In this section, we provide an overview of the main summarization techniques exploited by state-of-the-art approaches, then describe the fundamental concepts of each family, discussing its strengths and limitations.

\noindent{\bf{Summarization Techniques}} \textit{Random projections} 
project the original high-dimensional vector into a lower dimensional space using a random matrix. The pairwise distances are guaranteed to be nearly preserved with high probability if the dimension of the projected space is large enough~\cite{conf/map/johnson84}. \textit{Quantization} compresses a vector of infinite values to a finite set of codewords that constitute the codebook. \emph{Scalar} quantization maps each vector dimension independently, whereas \emph{vector} quantization maps the whole vector at once exploiting inter-dimension correlations~\cite{journal/tit/gray1998}.  \emph{Product} quantization~\cite{gist} divides the vector and operates a vector quantizer on the subvectors. {\it Optimized Product Quantization} (OPQ) optimizes the space decomposition of a product quantizer by decorrelating the dimensions. 
The \textit{Karhunen-Lo\`{e}ve Transform (KLT)}~\cite{karhunen1947ueber,loeve1948functions} performs a linear transformation that adapts to a given signal to decorrelate the dimensions, then applies scalar quantization. Recent work has shown that some summarizations that were developed for data series are equally efficient for generic vectors~\cite{lernaeanhydra2,hercules}. 
In particular, 
\textit{Piecewise Aggregate Approximation} (PAA)~\cite{journal/kais/Keogh2001} and {\it Adaptive Piecewise Constant Approximation} (APCA)~\cite{journal/acds/Chakrabarti2002} segment a vector into equal or variable length segments, respectively, and summarize each segment with its mean value. 
{\it Extended APCA} (EAPCA)~\cite{conf/vldb/Wang2013} improves upon APCA by using both the mean and standard deviation to summarize each segment. {\it Symbolic Aggregate Approximation} (SAX)~\cite{conf/dmkd/LinKLC03} transforms a vector using PAA and discretizes the values using a scalar quantizer. 
{\it Deep Embedding Approximation} (DEA) learns summarizations using the SEAnet deep neural network~\cite{seanetjournal}.


\noindent{\bf{Tree-based indexes}} organize the data using a tree structure and have been the method of choice for exact vector search for data series and generic high-dimensional vectors~\cite{conf/sigmod/Guttman1984,conf/icmd/Beckmann1990,journal/edbt/Schafer2012,conf/vldb/Wang2013,zoumpatianos2016ads,coconutjournal,ulissejournal,twinsubsequences,dpisaxjournal,localgeolocatetedsimilarity,parisplus,messijournal,sing,phd-workshop-karima,seanetjournal,dumpy,oddysey,fresh}. 
Some works have proposed using tree indexes to support approximate search with~\cite{mtree-pac,lernaeanhydra2} or without guarantees~\cite{flann,hdindex,lernaeanhydra2,dumpy}. 
The tree-based methods that support ng-approximate search either build multiple randomized K-D Trees that are searched in parallel 
~\cite{flann}, segment the space into smaller dimensions indexed by an RDB tree~\cite{hdindex}, or use heuristics to select candidates from some leaf node(s)~\cite{lernaeanhydra2,dumpy}. 
The ng-approximate search accuracy/efficiency tradeoff is tuned with user-defined parameters specific to each method, e.g., specifying the maximum number of leaves to visit during search.

\noindent{\bf{Scan-based approaches}} have been designed primarily for exact search. Sequential scans compare the query to each single candidate in a dataset. 
Skip-sequential scans summarize the high-dimensional data in a filter file that fits in memory and only search the original data, typically stored out-of-core, if a candidate cannot be pruned using the filter file. 
Some scan-based methods can also support ANN search with~\cite{lernaeanhydra2} or without guarantees~\cite{lernaeanhydra2,vafile,va+file}. These methods tune the ng-approximate search accuracy/efficiency tradeoff with a user-defined parameter that specifies the maximum number of vectors to be visited. 

\noindent{\bf{Inverted indexes}} typically compress the original high-dimensional vector data using quantization to reduce the space overhead and improve efficiency. 
Popular methods such as the Inverted Multi-Index (IMI)~\cite{journal/pami/babenko15}, and the IVF-PQ~\cite{gist} organize the data by associating lists of data points (a.k.a. posting lists) with their proximity to a codeword. 
These codewords are representative points, e.g. the medoid/centroid of the posting list, derived after clustering the original data, collectively forming a codebook. The
ng-approximate search algorithm accesses the codebook to retrieve points linked to the closest codeword(s).
The accuracy/efficiency tradeoff is tuned with a user-defined parameter that specifies the maximum number of posting lists to be explored during the search. 
This tradeoff is also affected by the index building parameters such as the codebook size and the quantization bit budget. 

\noindent{\bf{Hash-based approaches}} belong primarily to the family of locality-sensitive hashing (LSH)~\cite{conf/stoc/indyk1998,lsh-survey}. 
They are designed to support $\delta$-$\epsilon$-approximate vector search~\cite{lernaeanhydra2} by exploiting hash functions that group with high-probability similar data points into the same bucket and non-similar data points into different buckets. 
Numerous LSH variants exist, each with its unique strengths. 
Among these, the SRS~\cite{srs} and QALSH~\cite{qalsh} methods stand out. SRS provides efficient search results with a relatively small index size, while QALSH enhances bucketing precision by using the incoming query as a reference point. 
The $\delta$-$\epsilon$-approximate search accuracy/efficiency tradeoff is tuned with user-defined parameters that specify the approximation error and probability thresholds. 
This tradeoff is also affected by the index building parameters, e.g., the number of hash tables and the number of random projections.

\noindent{\bf{Graph-Based approaches}} support $ng$-approximate vector search. They often employ a proximity graph structure~\cite{gabriel69,toussaint02,schvaneveldt1989network}, where each vertex represents a data point, and edges connect similar vertices. Two vertices are linked if the data points they represent are close in their respective spaces, usually determined by a common distance measure such as the Euclidean distance~\cite{edelsbrunner87}. When searching for a specific query point, the process typically begins from a set of initial points or seeds. These seeds can be chosen randomly or based on certain criteria.  The search uses one of these seeds as an entry node and the others as initial candidate answers.
It then progresses by visiting the neighboring vertices of the current node in a best-first, greedy manner, concluding when no better matches are identified~\cite{reddy77bm}. State-of-the-art graph-based methods, including KGraph~\cite{kgraph}, EFANNA~\cite{efanna}, HNSW~\cite{hnsw}, DPG~\cite{dpg}, NSG~\cite{nsg}, SPTAG~\cite{tptree}, SSG~\cite{nssg}, Vamana~\cite{vamana}, HCNNG~\cite{hcnng}, ELPIS~\cite{elpis} and others~\cite{nsw14,ieh,fanng} 
use the same search algorithm~\cite{reddy77bm} but differ in how they construct the graph and select seed points.

\noindent{\bf{Hybrid approaches}} combine ideas from the different families described earlier. IEH~\cite{ieh} retrieves initial neighbors to build the graph using hash-based methods, such as LSH~\cite{iehlsh} and ITQ~\cite{iehitq}. EFANNA~\cite{efanna} builds an initial graph using randomized K-D Trees where nodes are connected to the neighbors returned by K-D Trees, and exploits these trees during search to find initial candidates. 
SPTAG~\cite{SPTAG4} divides the dataset using multiple Trinary-Projection Trees (TP Trees)~\cite{tptree} before merging multiple graphs constructed for each of the subsets, and constructs multiple K-D Trees or BKTrees to retrieve initial seeds. 
HCNNG~\cite{hcnng} divides the dataset using random hierarchical clusterings, and builds multiple Minimum Spanning Tree (MST) structures on the different subsets. 
The graph is built by merging the multiple MSTs. 
HCNNG also exploits K-D Trees to retrieve seeds to initiate the search. 
ELPIS~\cite{elpis} builds a Hercules tree~\cite{hercules} to divide the dataset into subsets, and constructs an HNSW~\cite{hnsw} graph on each subset. 
During search, ELPIS prunes subsets using the EAPCA~\cite{conf/vldb/Wang2013} lower-bounding distance. 

\noindent{\bf Summary.} Tree-based approaches typically demonstrate efficient indexing performance, boasting low index construction time and memory usage~\cite{lernaeanhydra2,elpis,tptree}. 
Many of these methods support various search flavors, from exact to approximate, with some offering guarantees. However, tree-based methods often fall short in search efficiency, especially when faced with challenging query workloads~\cite{lernaeanhydra2}. 
Methods based on inverted indexes are less scalable in terms of indexing time and footprint than tree-based indexes but can answer queries faster. 
However, achieving high query accuracy typically requires a significant indexing time and space overhead~\cite{lernaeanhydra2}. 
Hash-based methods provide theoretical guarantees on query accuracy and are the only SotA approximate methods that support theoretical guarantees on query performance. 
However, building the index requires a significant overhead both in space and time, often requiring different indexes to be pre-built to support different guarantees during search. 
Graph-based approaches, while expensive in terms of indexing time and memory usage, and lacking guarantees on query acccuracy, showcase impressive empirical query accuracy and efficiency~\cite{lernaeanhydra2, elpis, lin19, dpg}. 

