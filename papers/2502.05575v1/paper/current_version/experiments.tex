\section{Experimental Evaluation}
\label{sec:experiments}
%In this section, we conduct a comprehensive experimental evaluation of
We experimentally evaluate twelve state-of-the-art graph-based vector search methods, 
%We start by describing the experimental framework. 
%Then, we evaluate the different methods 
based on the two key paradigms described in Section~\ref{sec:survey}, i.e., SS and ND. 
To single out the effect of each strategy, we first implement a basic II-based method, where nodes are inserted incrementally and each node $i$ acquires its list of candidate neighbors through a beam search on the current partial graph of already inserted nodes.  Then, we implement each strategy independently on the resulting graph. 
Finally, we assess the indexing and query-answering performance of these methods on a variety of real and 
synthetic datasets.
All artifacts are available in~\cite{url/GASS}.
\subsection{Framework}

\noindent{\bf Setup.} 
Methods were compiled with GCC 8.2.0 under Ubuntu Linux 20.04 (Rocky Linux 8.5 on HPC) using default compilation flags and optimization level 3. Experiments were conducted on an Intel Xeon Platinum 8276 server with 4 sockets, each with 28 cores and 1 thread per core.
%, totaling 112 cores. 
The CPU cache configuration is: 32KB L1d, 32KB L1i, 1,024KB L2, and 39,424KB L3 cache. The server includes a 1.5TB RAM via 24x 64GiB DDR4 DIMMs.

%and queries are executed in asynchronous mode.  %Each index is built using 48 threads and query workloads are executed in asynchronous mode using 48 threads {\bf double check}. . 
%Our focus is to minimize the latency at high recall levels (0.9–0.999) so all algorithms were carefully tuned to achieve an optimal trade-off between latency and high accuracy. For reproducibility, the specific parameter choices, based on both our experiments and prior literature, are provided in~\cite{url/GASS}. We also cleared the caches between query workloads and disabled certain optimizations primarily used during query search (e.g., cache pre-warming and L2 normalization as used by some approaches like NSG and Vamana). For each method, we measure both the time and the recall (following the standard metrics in the existing similarity search literature [84,7,81,e,f,g,h]). For each method and search parameter (such as beam width in the case of beam search), we ran the experiment six times; we excluded the two best and two worst performances and reported the mean of the remaining performances. }

%\karima{\noindent{\bf Procedure.} During timed experiments, the server was used exclusively to ensure accurate time measurements. Experiments are carried out in two steps: indexing building and query answering. Methods are allowed a maximum of 48 hours to build a single index and queries are executed in asynchronous mode.  %Each index is built using 48 threads and query workloads are executed in asynchronous mode using 48 threads {\bf double check}. . 
%Our focus is to minimize the latency at high recall levels (0.9–0.999) so all algorithms were carefully tuned to achieve an optimal trade-off between latency and high accuracy. For reproducibility, the specific parameter choices, based on both our experiments and prior literature, are provided in~\cite{url/GASS}. We also cleared the caches between query workloads and disabled certain optimizations primarily used during query search (e.g., cache pre-warming and L2 normalization as used by some approaches like NSG and Vamana). For each method, we measure both the time and the recall (following the standard metrics in the existing similarity search literature [84,7,81,e,f,g,h]). For each method and search parameter (such as beam width in the case of beam search), we ran the experiment six times; we excluded the two best and two worst performances and reported the mean of the remaining performances. 
%}



\noindent{\bf Algorithms.} We cover the following methods: HNSW~\cite{hnsw}, NSG~\cite{nsg}, Vamana~\cite{vamana}, DPG~\cite{dpg}, EFANNA~\cite{efanna}, HCNNG~\cite{hcnng}, KGraph~\cite{kgraph}, NGT~\cite{ngt_library}, DPG~\cite{dpg}, and two versions of SPTAG~\cite{SPTAG4} (SPTAG-BKT and SPTAG-KDT, using BKT and K-D Trees, respectively). We also include ELPIS~\cite{elpis} and LSHAPG~\cite{lshapg}, new techniques not evaluated in the latest survey~\cite{graph-survey-vldb}. IEH~\cite{ieh} and FANNG~\cite{fanng} are excluded due to suboptimal performance~\cite{graph-survey-vldb, nsg}, and HVS~\cite{hvs} due to difficulties running the official implementation~\cite{hvsgithub}. We use the most efficient publicly available C/C++ implementations for each algorithm, leveraging multithreading and SIMD vectorization to optimize performance.  We also carefully inspected all code bases and, as is common in the literature ~\cite{graph-survey-vldb, diskanncode, nsgcode,ssgcode,ngtcode,sptagcode}, disabled the optimizations that would lead to an unfair evaluation such as cache pre-warming in Vamana and L2-normalized Euclidean distance in NSG, EFANNA, and Vamana. Since all methods except ELPIS and HNSW use a single linear buffer as a priority queue, we modified the original implementations of these two algorithms (which used two max-heap priority queues)~\cite{url/Elpis,url/hnsw}. The modifications to each code base are documented in~\cite{url/GASS}.



\noindent{\bf Datasets.} 
We use seven real-world datasets from various domains, including deep network embeddings, computer vision, neuroscience, and seismology:  
(i) \emph{Deep}~\cite{url/data/deep1b} contains 1 billion 96-dimensional vectors extracted from the final layers of a convolutional neural network;  
(ii) \emph{Sift}~\cite{conf/icassp/jegou2011,url/data/sift} consists of 1 billion 128-dimensional SIFT vectors representing image feature descriptors;  
(iii) \emph{SALD}~\cite{url/data/eeg} provides neuroscience MRI data with 200 million 128-dimensional data series;  
(iv) \emph{Seismic}~\cite{url/data/seismic} contains 100 million 256-dimensional time series representing earthquake recordings from seismic stations worldwide;  
(v) \emph{Text-to-Image}~\cite{url/data/text2image} offers 1 billion 200-dimensional image embeddings from Se-ResNext-101 along with 50 million DSSM-embedded text queries for cross-modal retrieval under domain shifts;  
(vi) \emph{GIST}~\cite{gist} contains 1 million 960-dimensional vectors, using GIST descriptors~\cite{gistdesc} to capture spatial structure and color layout of images;  and
(vii) \emph{ImageNet1M}, a new dataset that we generated from the original ImageNet~\cite{imagenet}, producing embeddings of 1 million original vectors using the ResNet50 model~\cite{resnet}, with PCA applied to reduce dimensionality to 256. We select subsets of different sizes from the Sift, Deep, SALD and Seismic datasets, and we refer to each subset with the name of the dataset followed by the subset size in GBs (e.g., Deep25GB). 
We refer to the 1-million and 1-billion datasets with the 1M and 1B prefixes, respectively. 
To evaluate the methods on datasets with different distributions, we generate three random 25GB datasets RandPow0, % (uniform distribution~\cite{url/Elpis}), 
RandPow5 and RandPow50, each with 256 dimensions, following the power law distribution~\cite{powerlaw} using three power law exponents: 0 (uniform~\cite{url/power-law}), 5 and 50 (very skewed).
The power law distribution models many real world phenomena (including in economics, physics, %biology, 
social networks, etc.). 
It is a relationship of type $Y= kX^a$, where $Y$ and $X$ are variables of interest, $a$ is the power law exponent and $k$ is a constant. 
The skewness of a dataset distribution increases with $a$. 
When $a = 0$, the dataset is evenly distributed.

\noindent{\bf Dataset Complexity.} 
The complexity of the datasets in our experimental evaluation is assessed using Local Intrinsic Dimensionality (LID)~\cite{lid15,DBLP:journals/is/AumullerC21} and Local Relative Contrast (LRC)~\cite{rc,DBLP:journals/is/AumullerC21}.  
The LID and LRC for a query point \( x \) are  defined as follows: 
\begin{displaymath}
\hspace{1.25cm}
\text{LID}(x) = -\left(\frac{1}{k} \sum_{i=1}^{k} \log \frac{\text{dist}_i(x)}{\text{dist}_{k}(x)}\right)^{-1} \hspace{0.7cm}(\text{Eq. 5}) \tag*{}
\end{displaymath}

\begin{equation}
\hspace{1.3cm}
\text{LRC}(x) = \frac{\text{dist}_{\text{mean}}(x)}{\text{dist}_k(x)}
\hspace{2.8cm} (\text{Eq. 6}) \tag*{}
\end{equation}
where $dist_i(x)$ is the Euclidean distance between point $x$ and its $i$-th true nearest neighbor, $k$ is the number of nearest neighbors, and $\text{dist}_{mean}(x)$ is the average distance of $x$ to all other points in the dataset.
LID represents the intrinsic dimensionality of the data
%, on which the data points lie 
(and can be significantly lower than the original dimensionality of the data space): the lower the LID, the easier the search is.
LRC is an intuitive measure of separability of the nearest neighbor of a query from the rest of the points in the dataset: the higher the LRC, the easier the search is. Figure~\ref{fig:datacomp} shows that the LID and LRC results are consistent. The graphs were produced using a subset of 1M points randomly sampled from each dataset, and $k=100$. 
Note that the orange horizontal lines in Figures~\ref{fig:datacomp:lid} and~\ref{fig:datacomp:rc} denote the mean LID and LRC values of each dataset, respectively. %, which corresponds to the Relative Contrast (RC)~\cite{DBLP:journals/is/AumullerC21} of the dataset. 
The Pow0, Pow5, Pow50, Seismic and \karima{Text2Img}~datasets have the highest LID and lowest LRC values, indicating that they are hard datasets for vector search~\cite{DBLP:journals/is/AumullerC21}. 
\karima{Sift, Deep and ImageNet}, on the other hand, are the easiest datasets in our workload, with the lowest LID and highest LRC values.


\begin{figure}[htb]
    \centering
    \begin{subfigure}{0.39\columnwidth}
    \centering
			\captionsetup{justification=centering}	
        \includegraphics[width=\textwidth]{../img-png/Experiments/data/query_lid_plot.png}
        \caption{\karima{Local Intrinsic Dimensionality (LID): low values indicate easy search}}
        \label{fig:datacomp:lid}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}{0.39\columnwidth}
    \centering
			\captionsetup{justification=centering}	
        \includegraphics[width=\textwidth]{../img-png/Experiments/data/query_rc_plot.png}
        \caption{\karima{Local Relative Contrast (LRC): high values indicate easy search}}
        \label{fig:datacomp:rc}
    \end{subfigure}
    \caption{\karima{Dataset Complexity}}
    \label{fig:datacomp}
\end{figure}


%Lower LID values, indicate simpler dataset structures conducive to efficient and accurate kNN querying. Conversely, datasets with higher LID values, suggest increased complexity and may necessitate more sophisticated kNN methods. 

\noindent{\bf Queries.}  Query sets include 100 vectors processed sequentially, not in batches, mimicking a real-world scenario where queries are unpredictable~\cite{itsawreport,DBLP:conf/edbt/GogolouTPB19,conf/sigmod/gogolou20}. \karima{Results with 1 million queries are extrapolated from 100 query sets. For Deep, Sift, GIST, and Text-to-Image, queries are randomly sampled from available query workloads. For SALD, ImageNet, and Seismic, 100 queries are randomly selected from the datasets and excluded from the index-building phase.}
%The ground-truths for each query workload are found using an exact search.
%In one experiment, we use queries of different difficulty for the Deep dataset, labeled with a percentage (1\%-10\%).  These queries were generated by randomly selecting vectors from the dataset and perturbing them using  different levels of Gaussian noise ($\mu$ = 0, $\sigma^2$ = 0.01-0.1)~\cite{johannesjoural2018}. The percentage indicates the value of $\sigma^2$.
For hardness experiments, we use Deep query vectors of varying complexity, denoted as a percentage ranging from 1\% to 10\%. These vectors were obtained by adding Gaussian noise (\(\mu = 0\), \(\sigma^2\) ranging from 0.01 to 0.1) to randomly choose dataset vectors, with the percentage reflecting the \(\sigma^2\) value~\cite{johannesjoural2018}.
The 100-query workloads for the power law distribution datasets are generated following the same distributions using different seeds. % as its respective dataset. 
Unless otherwise stated, all experiments were conducted with 10-NN queries per the standard in the community~\cite{neurips-2021-ann-competition,url/Elpis,aumuller2017ann}.
%\karima{Ilias, didn't we run the 100-NN experiments? If yes, let's mention this and put the 100-NN plots in the archive.}\ilias{we have run the 100NN experiments in ELPIS, but not in GASS, If we mention so I need to run extra experiments to update the numbers, eventually I will need to run the 100NN for ND}, 
We use 100-NN queries to evaluate dataset complexity because a higher k improves the estimation of LID and LRC (Eqs. 5-6), and to assess the different SS strategies because the higher the k the more overhead for seed selection.

\noindent\textbf{Measures.} We measure %and report 
the {\it wall clock time} and {\it distance calculations} for both indexing and query answering. We also measure the accuracy of each k-NN query using {\it Recall} which quantifies the fraction of the true nearest neighbors that the query $S_Q$ successfully returns.  

\karima{\noindent {\bf Procedure.} We tune each method to achieve the best trade-offs in accuracy/efficiency.  
Then, we carry experiments in two steps: indexing building and query answering, with caches cleared before each step and kept warm during the same query workload. Methods were allowed at most 48 hours to build a single index. During timed experiments, the server was used exclusively to ensure accurate measurements. 
 For each query workload, we ran the experiment six times; we excluded the two best and worst, and reported the mean of the remaining performances. For reproducibility, all parametrization details are provided in~\cite{url/GASS}.}
 

%\subsubsection{Neighborhood Diversification}
\subsection{Neighborhood Diversification}
\label{subsec:experiments-ND}
We now evaluate the ND strategies covered in Section~\ref{sec:survey}, i.e., RND, RRND, and MOND against a baseline without ND (NoND). 
We apply each strategy individually to an II-based graph, where each node is inserted sequentially and linked with a pruned list of neighbors, determined via a beam search with maximum out-degree $R=60$ and beam width $L=800$. Bi-directional edges are added to neighbors, and the neighborhood list is pruned to size $R$ using the same ND strategy.
Graphs are built on Deep and Sift (25GB, 100GB and 1B). 
For RRND and MOND, we run experiments with different values of $\alpha$ ($1-2$) and $\theta$ ($50^\circ-80^\circ$), respectively, and use $\alpha = 1.3$ and $\theta = 60^\circ$ because they lead to the best performance. 
Then, we execute workloads with 100 queries against each dataset, and measure the accuracy/efficiency tradeoff using the recall and the number of distance calculations incurred during the search. The results in Figure~\ref{fig:ND:search:real} indicate that both RND and MOND consistently outperform, followed by RRND. NoND is the worst performer overall. As the dataset size increases, the performance gap between NoND and ND methods widens, particularly at high Recall (Figures~\ref{fig:ND:sift1b}, \ref{fig:ND:deep1b}). 
This is due to the higher number of hops needed to find the answers and the density of the neighborhoods in the NoND nodes since no pruning was applied. 
These results indicate the key role played by the ND paradigm in improving query-answering performance and the superiority of the RND and MOND strategies.
\newcommand{\sffive}{0.27\columnwidth}
\begin{figure}[tb]
	\captionsetup{justification=centering}
	\centering	
		\begin{subfigure}{\columnwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=0.35\columnwidth]{../img-png/Experiments/RNG/legend.png}
			\label{fig:ND:legend}
		\end{subfigure}\\
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_DEEP25GB.png}
		\caption{{DEEP25GB}}
		\label{fig:ND:deep25GB}	
		\end{subfigure}	
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_DEEP100GB.png}
		\caption{{DEEP100GB}}
		\label{fig:ND:deep100GB}
		\end{subfigure}	
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_DEEP1B.png}
		\caption{{DEEP1B}}%need to be updated
		\label{fig:ND:deep1b}	
  \end{subfigure}	
  \\
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_SIFT25GB.png}
   \caption{{SIFT25GB}}
		\label{fig:ND:sift25GB}
		\end{subfigure}	
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_SIFT100GB.png}
             \caption{{SIFT100GB}}
		      \label{fig:ND:sift100GB}
		\end{subfigure}	
		\begin{subfigure}{\sffive}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/RNG/DC_SIFT1B.png}
	%\vspace*{-0.4cm}
   \caption{{SIFT1B}}
		\label{fig:ND:sift1b}
		\end{subfigure}	
		\caption{{ND methods performance on real-world datasets}}
	
\label{fig:ND:search:real}
 \end{figure}

In Table~\ref{tab:pruning_ratio}, we report the pruning ratios of the three neighborhood ND methods on the Deep and Sift 25GB datasets. 
 The pruning ratio quantifies the percentage reduction in the size of the candidate neighbor list before and after the diversification step. Higher pruning ratios indicate more aggressive pruning, which directly affects the graph size and memory usage. RND achieves the highest pruning ratios, MOND provides moderate pruning, and RRND exhibits the least pruning. 
 As a result, RND leads to smaller graph sizes and reduced memory requirements, while RRND creates larger graphs with higher memory usage.
\begin{comment}
\begin{figure}[tb]
	\captionsetup{justification=centering}
	\centering	
    \includegraphics[width=0.6\textwidth]{../img-png/Experiments/RNG/pruningratio_n.png}
    		\caption{{Pruning Ratios of ND Methods on Deep and Sift Datasets}}
\label{fig:ND:search:pr}
 \end{figure}    
\end{comment}
\begin{table}[tb]
{\normalsize
\centering
%\renewcommand{\arraystretch}{1} 
\begin{tabular}{@{}lccc@{}}
\toprule
         & \textbf{RND} & \textbf{MOND} & \textbf{RRND} \\ 
\midrule
\textbf{Deep} & 20\%        & 2\%          & 0.6\%         \\ 
\textbf{Sift} & 25\%        & 4\%          & 0.7\%         \\ 
\bottomrule
\end{tabular}
} 
\caption{Pruning ratios of ND methods on Deep and Sift datasets.}
\label{tab:pruning_ratio}
\end{table}








\subsection{Seed Selection}
%In the experiments that follow, we evaluate four seed selection strategies for the beam search algorithm: Stacked NSW (SN), Medoid (MD), Single Fixed Random Entry Point (SF), K-Sampled Random Seeds (KS), and KD-trees (KD). 
In these experiments, we focus on the four most common SS strategies for the beam search algorithm: SN~\cite{hnsw,elpis}, MD~\cite{nsg,vamana}, KS~\cite{kgraph,nsw11,dpg,vamana,nssg}, and KD~\cite{efanna,SPTAG4,hcnng} (KM and LSH were excluded because they are not among the commonly used seed selection strategies in graph-based methods). We consider the baseline method SF which has not been used in the literature before. 
In these experiments, we focus on the four most common SS strategies for the beam search algorithm: SN~\cite{hnsw,elpis}, MD~\cite{nsg,vamana}, KS~\cite{kgraph,nsw11,dpg,vamana,nssg}, and KD~\cite{efanna,SPTAG4,hcnng} (KM and LSH were excluded because they are not among the commonly used seed selection strategies in graph-based methods). We consider the baseline method SF which has not been used in the literature before. 
These strategies are compared using the same insertion-based graph structure that exploits RND pruning since this is the best baseline from the results in Section~\ref{subsec:experiments-ND}.  
We run 100 queries for each strategy on the Deep and Sift datasets with sizes 25GB, 100GB, and 1B. We extrapolate the results to 1M queries and report the number of distance calculations to achieve a 0.99 accuracy in Figure~\ref{fig:ss:search}. We observe that SN and KS are the most efficient strategies across all scenarios, while SF and MD are the least efficient overall. The KD strategy is competitive on 25GB and 100GB Deep and Sift datasets but its performance deteriorates on billion-scale datasets. 
KS outperforms SN on dataset sizes 25GB and 100GB; however, this trend reverses with the 1B size dataset. 
The difference in distance calculations between SN and KS on the 25GB and 1B datasets is $\sim$1M and $\sim$10M, respectively. 
As the dataset size increases, it becomes imperative to sample more nodes (beyond the beam width utilized during search in KS) to obtain a representative sample of the dataset, thereby enhancing the likelihood of initiating the search closer to the graph region where the query resides (SN adapts its size logarithmically with the growth of the dataset, leading to a better representation of the dataset). Figure~\ref{fig:ss:search} also illustrates that both MD and SF are among the least performing strategies, with MD performing better than SF on Deep and vice-versa on Sift. This indicates neither MD nor SF are effective and robust seed selection strategies. 


We now study the effect of SS strategies on indexing performance. We focus on the two best strategies KS and SN and study their effect on the same baseline based on II and RND~\cite{nsw11,dpg,hnsw,nsg,nssg,vamana,elpis,SPTAG4}. This is because these methods are the most impacted by the SS strategy used, since they perform a beam search, which includes a seed selection step, at the insertion of each node.
We build an index using each strategy on Deep1M and Deep25GB and measure distance calculations. %incurred in each case. 
We calculate the distance overhead of SN compared to KS, and we estimate the number of additional 100-NN queries that the KS-based graph can answer, with 0.99 recall, before the SN-based graph completes its construction.  
We observe (Table~\ref{tab:ss:idx}) that the SN-based graph requires 182 million and 22.3 billion more distance calculations than the KS-based graph on Deep1M and Deep25GB respectively. Furthermore, the KS-based graph can answer ~45K and 1.17 million queries on Deep1M and Deep25GB respectively before the SN-based graph finishes construction.

\newcommand{\sfsix}{0.27\columnwidth}
\begin{figure}[tb]
	\captionsetup{justification=centering}
	\centering	
 		\begin{subfigure}{0.015\columnwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/dc.png}
   \vspace{0.24in}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/DEEP_25GB_100.png}
		\caption{{Deep25GB}}
		\label{fig:ss:deep25gb}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/DEEP_100GB_100.png}
		\caption{{Deep100GB}}
		\label{fig:ss:deep100gb}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/DEEP_1B_100.png}
		\caption{{Deep1B}}
		\label{fig:ss:deep1b}
		\end{subfigure}	
        
 		\begin{subfigure}{0.015\columnwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/dc.png}
   \vspace{0.24in}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/SIFT_25GB_100.png}
		\caption{{Sift25GB}}
		\label{fig:ss:sift25gbß}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/SIFT_100GB_100.png}
		\caption{{Sift100GB}}
		\label{fig:ss:sift100gb}
		\end{subfigure}	
		\begin{subfigure}{\sfsix}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/EP/SIFT_1B_100.png}
		\caption{{Sift1B}}
		\label{fig:ss:sift1b}
		\end{subfigure}	
\caption{The impact of SS Methods on query answering}
\label{fig:ss:search}
 \end{figure}
 


\begin{table}[tb]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
%\textbf{Metric} 
& \textbf{Deep1M} & \textbf{Deep25GB} \\ 
\midrule
\textbf{Dist. Calculations (SN)} & 4.3 billion & 1.49 trillion \\
\textbf{Dist. Calculations (KS)} & 4.1 billion & 1.46 trillion \\
\midrule
\textbf{Overhead (SN vs. KS)} & 182 million & 22.3 billion \\
\textbf{Additional Queries} & 44,959 & 1,165,870 \\
\bottomrule
\end{tabular}
\caption{The impact of SS methods on Indexing Performance}
\label{tab:ss:idx}
\end{table}


\subsection{Indexing Performance}
We evaluate twelve state-of-the-art vector search methods, varying dataset sizes and reporting total indexing time and memory footprint. For brevity, we present results only for the Deep dataset as trends are consistent across other datasets. Full results are in~\cite{url/GASS}.  We use subsets of Deep ranging from 1 million to 1 billion vectors (equivalent to ~350GB). Indexes are built to allow a 0.99 recall efficiently. \karima{ Initial experiments on 1 million vectors include all methods. Methods that could not scale to larger datasets are excluded from subsequent experiments. Specifically, HCNNG, SPTAG-BKT, NGT, and SPTAG-KDT take over 24 hours to build indexes on 25GB datasets and exceed 48 hours on 100GB datasets.
% under high-efficiency settings. 
KGraph, DPG, EFANNA, and LSHAPG delivered unsatisfactory results on 25GB, so they were not included in larger datasets.} Furthermore, KGraph and EFANNA require over 300GB and 1.4TB of RAM for 25GB and 100GB datasets, respectively. As DPG, NSG, and SSG rely on KGraph and EFANNA, they were also excluded from larger datasets.

\noindent{\bf Indexing Time.} Figure~\ref{fig:elpis:idx:time} demonstrates that II-based approaches have the lowest indexing time across dataset sizes. In particular, the II and DC-based approach ELPIS, is 2.7x faster than HNSW and 4x faster than NSG for both 1M and 25GB dataset sizes, \karima{while HNSW is 1.4x faster than NGT}. Note that NSG's indexing time includes both the construction of its base graph, EFANNA, which is time-intensive, and the refinement with NSG. SPTAG-BKT and SPTAG-KDT exhibit high indexing times, requiring over 25 hours to index the Deep25GB dataset-24 times more than ELPIS, the fastest method. This inefficiency in SPTAG arises from its design, which involves constructing multiple TP Trees and graphs, becoming increasingly costly with larger datasets. On datasets with 100GB and 1B vectors ($\approx$350GB), only HNSW, ELPIS, and Vamana scale with acceptable indexing time, with ELPIS being 2 and 2.7 times faster than HNSW and Vamana, respectively.


\noindent\textbf{Indexing Footprint.}
Figure~\ref{fig:elpis:idx:footprint:memory} reports the memory footprint for each index, including the raw data. To perform the evaluation, we record the peak virtual memory usage during construction\footnote{Readings are taken from the proc pseudo filesystem’s Virtual Memory Peak.}. SPTAG-BKT and SPTAG-KDT demonstrate efficient memory utilization (1M and 25GB) despite having the highest indexing time. 
%\karima{give some intuition to explain this result}\ilias{I believe this is primarily attributable to its efficient implementation, although I cannot assert with complete certainty}. 
%They exhibit the lowest indexing memory footprint for 1M and 25GB datasets.
For larger datasets, ELPIS has the lowest indexing memory footprint, occupying up to 40\% less memory than HNSW and 30\% less than Vamana during indexing. This is because ELPIS needs a smaller maximum out-degree and beam width compared to its competitors.
% to efficiently build the index. 
HNSW has a higher indexing memory footprint due to its use of a graph layout optimized for direct access to node edges through a large contiguous block allocation~\cite{url/hnsw}. This layout offers a time advantage over adjacency lists by reducing memory indirections and cache misses. However, it can result in quadratic memory growth when using a large maximum out-degree on large-scale datasets.
In Figure \ref{fig:elpis:idx:footprint:disk}, we compare the size of method indices, including the raw data. The figure shows that certain methods, such as EFANNA, HCNNG, KGraph, and consequently NSG, SSG, and DPG (which use one of these base graphs), exhibit a significantly larger memory footprint relative to their final index size. For instance, HCNNG consumes substantial memory during indexing, requiring over 200GB for Deep25GB (Fig. \ref{fig:elpis:idx:footprint:memory}) due to merging multiple MST from numerous samples generated during hierarchical clusterings. In contrast, its final index size is less than 50GB (Fig. \ref{fig:elpis:idx:footprint:disk}).

\begin{figure*}[!htb]
	\captionsetup{justification=centering}
	\centering	
	\begin{minipage}{\textwidth}
		\begin{subfigure}{\textwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/legendall.png}
		\end{subfigure}
	\end{minipage}
	\begin{minipage}{0.19\textwidth}				
	%	\captionsetup{justification=centering}
			\centering
		\begin{subfigure}{\textwidth}
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/Idx_footprint_datasets/idx_time_deep_n.png}
		\end{subfigure}	
		\caption{\karima{Indexing Time }}
		\label{fig:elpis:idx:time}
	\end{minipage}	
	\begin{minipage}{0.19\textwidth}				
	%	\captionsetup{justification=centering}
		\begin{subfigure}{\textwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/Idx_footprint_datasets/idx_footprint_deep_n.png}
		\end{subfigure}	
		\caption{\karima{Indexing Memory Footprint}}
		\label{fig:elpis:idx:footprint:memory}
	\end{minipage}		
	\begin{minipage}{0.19\textwidth}				
	%	\captionsetup{justification=centering}
		\begin{subfigure}{\textwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/Idx_footprint_datasets/idx_indexsize_deep_n.png}
		\end{subfigure}	
		\caption{\karima{Indexing Disk Footprint}}
		\label{fig:elpis:idx:footprint:disk}
	\end{minipage}		
	\begin{minipage}{0.19\textwidth}				
		%	\captionsetup{justification=centering}
		\begin{subfigure}{\textwidth}
			\centering
			\captionsetup{justification=centering}	
			\includegraphics[width=\textwidth]{../img-png/Experiments/Idx_footprint_datasets/search_footprint_deep_n.png}
		\end{subfigure}	
		\caption{\karima{Query Memory Footprint}}
		\label{fig:elpis:query:footprint:memory}
	\end{minipage}	
	\begin{minipage}{0.19\textwidth}
		\centering
		\begin{subfigure}{\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/Idx_footprint_datasets/qrs_beamwidth_n.png}
		\end{subfigure} 
		\caption{\karima{Query Beam Width}}
		\label{fig:elpis:query:beam-width}
	\end{minipage}
\end{figure*}

%%%%%%%
\subsection{Search Performance}
\karima{
We now evaluate the search performance of the different methods. All methods were included in the 1M experiments. Some methods were excluded in 25GB plots (KGraph, DPG, SPTAG-KDT, HCNNG, and EFANNA) for the sake of clarity, as their search was significantly slower than the best baselines. Full results are in~\cite{url/GASS}.
Other methods were omitted from the 100GB and 1B dataset sizes due to various limitations. The indexes for SPTAG and NGT could not be built on the larger datasets within 48 hours. EFANNA was excluded due to its high footprint, and likewise for methods based on it such as NSG and SSG.
% could not be built for larger dataset size. 
Finally, KGraph, DPG, and LSHAPG were excluded due to unsatisfactory results on 1M and 25GB.
}

%thus  also excluded certain approaches  from certain 25GB datasets due to their significantly higher query times compared to most evaluated methods. Including all methods would clutter the figure and obscure key performance differences among the top contenders. (See full results in archive).}

\noindent\textbf{Query Memory Footprint and Beam Width.}
Figure~\ref{fig:elpis:query:footprint:memory} indicates that Vamana, followed by ELPIS, have the lowest memory footprint during search. Even though ELPIS has a smaller index size, it adopts a contiguous memory storing during search, which increases the index footprint when loaded into memory. Besides, Figure~\ref{fig:elpis:query:beam-width} shows that Elpis requires the smallest beam width to reach similar query accuracy. Having a very high beam width indicates that the beam search requires to visit a wider area to and making more distance calculations to retrieves the NN answers.

\noindent\textbf{Real Datasets.} 
%Analyzing different real datasets in Figures~\ref{fig:elpis:query:performance:25GB},~\ref{fig:elpis:query:performance:100GB}, and ~\ref{fig:elpis:query:performance:1B}, ND-based and DC-based as well as II-based methods SPTAG-BKT, ELPIS,  HNSW, Vamana and NSG emerge as the leading performers. NSG excels on Sift1M and Deep1M datasets, and SPTAG-BKT shows best performance in SALD1M and SALD25GB. 
%ELPIS dominates the performance on the larger datasets. 
\karima{On datasets with 1M vectors (Figure~\ref{fig:elpis:query:performance:1M}), ELPIS and NSG/SSG perform best on Sift1M, achieving the highest performance for 0.99 and lower recall, respectively. For Seismic1M, HCNNG and ELPIS share the top rank. NGT, SSG, and NSG excel on Deep1M, while HCNNG leads on SALD1M at the highest recall, followed by SPTAG and NSG at lower recall levels. In ImageNet1M, NSG/SSG and HNSW rank as the top performers. 
Across most scenarios, SSG and NSG show similar performance. However, LSHAPG demonstrates limitations, requiring more computation to achieve high accuracy. Its probabilistic rooting prunes promising neighbors, requiring a larger beam width and tighter $L$-bsf lower-bound distance during search.}
%ELPIS returns as best performing Seismic 1M, achieving better accuracy than HCNNG. 
\karima{When moving to 25GB datasets (Figure~\ref{fig:elpis:query:performance:25GB}), SSG, NSG, NGT and HCNNG experience a drop in performance, and ELPIS takes the lead with the best overall performance together with SPTAG-BKT for SALD25GB .} %ELPIS leverages its multi-graph search, optimized for query latency. As dataset size increases, ELPIS becomes increasingly advantageous compared to competitor methods.
It is worth noting that none of the methods achieved an accuracy over 0.8 on the Seismic dataset, leading us to report results for these lower recall values. 
The significant indexing footprint of NSG prevented us from extending its evaluation to larger datasets, as constructing the EFANNA graph (which NSG depends on) requires more memory than the available 1.4TB.
%checked updqted number from 10NN; 
For hard query workloads in Figure~\ref{fig:search:query:performance:25GB:hard} we compare the best-performing methods from the two most performing graph paradigms, ND-based and DC-based methods, including HNSW, NSG, ELPIS and SPTAG-BKT. 
SPTAG-BKT achieves the overall best performance for 1\% noise query set, as we increase the noise up to 10\%, SPTAG-BKT's performance deteriorates, which we can relate to SPTAG BKT structures failing to identify good seed points. At the same time, the other competitors gain an advantage, with ELPIS taking the lead. 
When analyzing very large datasets of 1 billion vectors, Figure~\ref{fig:elpis:query:performance:1B} shows the superiority of ELPIS which is up to an order of magnitude faster at achieving 0.95 accuracy, thanks to its design that supports multi-threading for single query answering
%\karima{Is this the only reason why ELPIS wins in this scenario?}.
This trend is consistent across subsets ranging from 100GB (Figure~\ref{fig:elpis:query:performance:100GB}) to 250GB (detailed results are reported in~\cite{url/GASS}).


\noindent{\bf Data Distributions.} We assess top performers representing different paradigms (EFANNA, Vamana, SSG, HNSW, ELPIS, and SPTAG-BKT) on challenging datasets (Fig. ~\ref{fig:datacomp}). Results (Figs. ~\ref{fig:query:performance:25GB:rand:pow1:10NN} and ~\ref{fig:query:performance:25GB:rand:pow50:10NN}) indicate that ELPIS consistently achieves high accuracy across skewness levels (0 to 50), outperforming other methods. As skewness increases, search becomes easier so most graph-based approaches improve but ELPIS maintains its superiority.
% tested  on synthetic datasets with varying skewness from a power-law distribution 
 
\begin{comment}
\ilias{I mention this in discussion}
The majority of graph-based approaches exhibit an interesting trend \karima{what is this trend?}, not only on the synthetic datasets but also on the Seismic and Sald real datasets, which present significant difficulties (Figure ~\ref{fig}) and challenging queries (Figure ~\ref{fig:search:query:performance:25GB}) \karima{fix the figure numbers} \karima{what are these difficulties and why are the queries challenging?}. We observe that the DC-based approaches outperform ND-based approaches in query performance, as the latter often become trapped in local minima \karima{in which plot do we see this? what are the methods?}. In contrast, DC-based approaches like SPTAG and ELPIS both
achieve better performance on hard datasets and out-of-distribution queries. This is because they both effectively navigate various regions of the data space: SPTAG constructs more refined connections during its building of multiple graphs, and ELPIS leverages multi-cluster search capabilities.
\end{comment}


%In scenarios with varied synthetic data distributions, ELPIS consistently showcases outstanding performance, particularly in the $RandPow0$ dataset, as illustrated in Figure~\ref{fig:query:performance:25GB:rand:pow1:10NN}. Analyzing the $RandPow0$ dataset, which presents a more challenging landscape, it becomes apparent that rival methods struggle to achieve a recall higher than 0.7. This can be attributed to their search algorithms possibly getting trapped in local minima. In contrast, ELPIS leverages its multi-cluster search capability to navigate through various regions of the data space, facilitating the retrieval of answers.


\newcommand{\soneM}{0.27}
\begin{figure}[!htb]
    \centering
    \begin{minipage}{\textwidth}
        \begin{subfigure}{\textwidth}
            \centering
            \captionsetup{justification=centering}	
            \includegraphics[width=\textwidth]{../img-png/Experiments/legendall.png}
        \end{subfigure}
    \end{minipage}
    
    \begin{minipage}{\textwidth}
        \centering
        \captionsetup{justification=centering}
        \captionsetup[subfigure]{justification=centering}
        \begin{subfigure}{0.025\textwidth}
            \raisebox{1.2cm}{\includegraphics[width=\textwidth]{../img-png/Experiments/time.png}} 
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/deep_10nn.png}
            \caption{\karima{\textbf{Deep}}} 
            \label{fig:elpis:query:performance:1M:deep:10NN}
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/sift_10nn.png}
            \caption{\karima{\textbf{Sift}}} 
        \label{fig:elpis:query:performance:1M:sift:10NN}
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/sald_10nn.png}
            \caption{\karima{\textbf{SALD}}} 
            \label{fig:elpis:query:performance:1M:sald:10NN}
        \end{subfigure}
        \\
        \begin{subfigure}{0.025\textwidth}
            \raisebox{1.2cm}{\includegraphics[width=\textwidth]{../img-png/Experiments/time.png}} 
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/seismic_10nn.png}
            \caption{\karima{\textbf{Seismic}}} 
            \label{fig:elpis:query:performance:1M:seismic:10NN}
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/gist_10nn.png}
            \caption{\karima{\textbf{Gist}}} 
            \label{fig:elpis:query:performance:1M:gist:10NN}
        \end{subfigure}
        \begin{subfigure}{\soneM\textwidth}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/imagenet_10nn.png}
            \caption{\karima{\textbf{Imagenet}}} 
            \label{fig:elpis:query:performance:1M:imagenet:10NN}
        \end{subfigure}
        
        \caption{\karima{Query performance on 1M vectors}}
        \label{fig:elpis:query:performance:1M}
    \end{minipage}
\end{figure}

\newcommand{\soneMs}{0.19}
\begin{figure}[!htb]
    \centering    
    \begin{minipage}{\textwidth}
        \begin{subfigure}{\textwidth}
            \centering
            \captionsetup{justification=centering}	
            \includegraphics[width=\textwidth]{../img-png/Experiments/legendall.png}
        \end{subfigure}
    \end{minipage}
    
	\begin{minipage}{0.6\textwidth}
		\captionsetup{justification=centering}
		\captionsetup[subfigure]{justification=centering}
        \begin{subfigure}{0.029\textwidth}
            \raisebox{1cm}{\includegraphics[width=\textwidth]{../img-png/Experiments/time.png}} 
        \end{subfigure}
		\begin{subfigure}{0.314\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/deep_10nn.png}
			\caption{\karima{Deep}}  
			\label{fig:elpis:query:performance:25GB:deep:10NN}
		\end{subfigure}
		\begin{subfigure}{0.314\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/sald_10nn.png}
			\caption{\karima{SALD}}  
			\label{fig:elpis:query:performance:25GB:sald:10NN}
		\end{subfigure}
		\begin{subfigure}{0.314\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/seismic_10nn.png}
			\caption{\karima{Seismic}}  
			\label{fig:elpis:query:performance:25GB:seismic:10NN}
		\end{subfigure}	

                \begin{subfigure}{0.029\textwidth}
            \raisebox{1cm}{\includegraphics[width=\textwidth]{../img-png/Experiments/time.png}} 
        \end{subfigure}
		\begin{subfigure}{0.314\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/sift_10nn.png}
			\caption{\karima{Sift}}  
			\label{fig:elpis:query:performance:25GB:sift:10NN}
		\end{subfigure}
	    \begin{subfigure}{0.314\textwidth}
		\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/pow1_10nn.png}
		\caption{\karima{\textbf{RandPow0}}} 
		\label{fig:query:performance:25GB:rand:pow1:10NN}
	\end{subfigure}
	    \begin{subfigure}{0.314\textwidth}
		\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/pow50_10nn.png}
		\caption{\karima{\textbf{RandPow50}}} 
		\label{fig:query:performance:25GB:rand:pow50:10NN}
	\end{subfigure}		
		\caption{{\karima{25GB datasets}}}
		\label{fig:elpis:query:performance:25GB}
	\end{minipage}
	\begin{minipage}{\soneMs\textwidth}
		\captionsetup{justification=centering}
		\captionsetup[subfigure]{justification=centering}
		\begin{subfigure}{\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/100/deep_10nn.png}
			\caption{Deep}  
		\label{fig:elpis:query:performance:100GB:deep:10NN}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
			\includegraphics[width=\textwidth]{../img-png/Experiments/search/100/sift_10nn.png}
			\caption{Sift}  
			\label{fig:elpis:query:performance:100GB:sift:10NN}
		\end{subfigure}
		\caption{{100GB datasets}}	
		\label{fig:elpis:query:performance:100GB}
	\end{minipage}
	\begin{minipage}{\soneMs\textwidth}
		\captionsetup{justification=centering}
		\captionsetup[subfigure]{justification=centering}
		\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/deep1p_10nn.png}
		\caption{\textbf{\karima{1\% noise}}} 
		\label{fig:search:query:performance:25GB:hard:1p}
		\end{subfigure}
		\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{../img-png/Experiments/search/25/deep10p_10nn.png}
		\caption{\textbf{\karima{10\% noise}}} 
		\label{fig:search:query:performance:25GB:hard:10p}
		\end{subfigure}
	\caption{\karima{Varying workloads}}
		\label{fig:search:query:performance:25GB:hard}
	\end{minipage}
\end{figure}



\karima{\noindent{\bf Implementation Impact.} 
We evaluate the performance of original implementations of the best performing methods on 1B experiments, i.e., Vamana, HNSW, and ELPIS against optimized methods from the ParlayANN library~\cite{parlayann} (Vamana\_Opt, HNSW\_Opt, and HCNNG\_Opt) on Deep1B.  %Additionally, we assess the search performance under single-thread execution, as beam search—used by all methods—lacks efficient parallel implementations on single graphs, except for ELPIS, which leverages multi-threading across multiple graphs. 
Figure~\ref{fig:optimized_impl} indicates that Vamana\_Opt and HNSW\_Opt are faster for recall below 0.97 compared to their original counterparts, due to more efficient data structures~\cite{parlayann, parlayanncode}. However, at higher recall, this advantage diminishes as distance computations dominate; %making pruning efficiency more critical. 
HCNNG\_Opt is competitive with Vamana and HNSW, while ELPIS maintains a performance lead. 
%However, under single-thread constraints, ELPIS suffers significant degradation as it must sequentially execute multiple beam searches, making it inefficient with insufficient threads per query.
}

\begin{figure}[h]
  \centering      
  \begin{minipage}{\columnwidth}
        \includegraphics[width=\columnwidth]{../img-png/Experiments/search/parlayann/legend.png}
        \end{minipage}
 \begin{minipage}{0.73\columnwidth}
 \centering
    \begin{minipage}{\textwidth}
        \centering
        \captionsetup{justification=centering}
        \begin{subfigure}{0.32\textwidth}
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1B/text2img_10nn.png}
            \caption{\karima{Text2Image}}
            \label{fig:elpis:query:performance:1B:t2i:10NN}
        \end{subfigure}
        \begin{subfigure}{0.305\textwidth}
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1B/deep_10nn.png}
            \caption{\karima{Deep}}
            \label{fig:elpis:query:performance:1B:deep:10NN}
        \end{subfigure}
        \begin{subfigure}{0.305\textwidth}
            \centering
            \captionsetup{justification=centering}
            \includegraphics[width=\textwidth]{../img-png/Experiments/search/1B/sift_10nn.png}
            \caption{\karima{Sift}}
            \label{fig:elpis:query:performance:1B:sift:10NN}
        \end{subfigure}
    \end{minipage}
    \caption{\karima{1B Datasets}}
    \label{fig:elpis:query:performance:1B}
    \end{minipage}
    \begin{minipage}{0.26\columnwidth}
        \centering
        \captionsetup{justification=centering}
        \includegraphics[width=\textwidth]{../img-png/Experiments/search/parlayann/10_Time_n.png}
        \caption{\karima{Optimized Implementations (Deep1B)}}
        \label{fig:optimized_impl}
    \end{minipage}
\end{figure}


\begin{comment}
    \begin{figure}[h]
  \centering      
  \begin{minipage}{0.64\columnwidth}
        \includegraphics[width=0.8\columnwidth]{../img-png/Experiments/search/parlayann/legend.png}

\begin{minipage}{0.05\textwidth}
            \captionsetup{justification=centering}
        \captionsetup[subfigure]{justification=centering}
        \begin{subfigure}{\textwidth}
            \vspace*{-.4in}
            \includegraphics[width=\columnwidth]{../img-png/Experiments/time.png}
        \end{subfigure}
    \end{minipage}
 \begin{minipage}{0.46\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{../img-png/Experiments/search/1B/text2img_10nn.png}
        \caption{Text2Image1B}
        \label{fig:text2image}
    \end{minipage}
    \begin{minipage}{0.46\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{../img-png/Experiments/search/parlayann/10_Time_n.png}
        \caption{Optimized Implementations}
        \label{fig:optimized_impl}
    \end{minipage}
  \end{minipage}
    \begin{minipage}{0.33\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{../img-png/Experiments/search/1M/performance_scatter_plot.png}
        \caption{Parameter Tuning}
        \label{fig:parameters_tuning}
    \end{minipage}
\end{figure}

\end{comment}
