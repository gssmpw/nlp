\section{Related Work}
We review related work on machine learning-based detection of code vulnerability in terms of different representations of code: (a) code as graphs, (b) code as trees, and (c) code as text.  


Graph-based methods typically capture the structural characteristics of various types of graphs, such as control-flow graphs (CFGs), data-flow graphs (DFGs), and data dependence graphs (DDGs), using techniques such as graph neural networks (GNNs), convolutional neural networks (CNNs), or recurrent neural networks (RNNs), to construct models for detecting vulnerabilities. VulDeePecker ____ models C/C++ source code as a graph, leveraging graph-level and node-level features for vulnerability prediction. $\mu$VulDeePecker ____ is an enhanced version incorporating data and control dependencies from system dependency graphs (SDGs). Including code attention and localized information enables the model to effectively detect 40 different types of vulnerabilities. DeepTective ____  combines gated recurrent units (GRUs) and graph convolutional networks (GCNs) to leverage both syntactic and semantic information for detecting SQLi, XSS, and OSCI vulnerabilities in PHP code. Velvet ____ integrates graph-based and sequence-based neural networks to capture the local and global context of code and identify vulnerable patterns. DEVIGN ____ uses a joint graph representation of code snippets by merging ASTs, CFGs, and DFGs for detecting vulnerabilities in C programs with high accuracy. FUNDED ____ creates a method-level graph representation using nine types of edges to ASTs, and uses a pre-trained word2vec network updated by a GRU for node embeddings, achieving an average accuracy of 92\%. HGVul ____ utilizes the code property graph combined with natural code sequence as a graph representation, which is fed into three different GNN models, GCN, GAT, and GGNN, achieving an F1 score of 88.3\%. EL-VDetect ____ integrates serialization-based and graph-based neural networks with an attention mechanism to capture code semantics and achieves 90.72\% accuracy on a real-world dataset. Wang et al. ____ utilize contract graphs for smart contract vulnerability detection, achieving an average accuracy of 89\%. AMPLE ____ is a vulnerability detection framework with graph simplification and enhanced graph representation learning that aims to capture global vulnerable information. Cao et al. ____ propose a statement-level vulnerability detection approach based on flow-sensitive graph neural networks (FS-GNN) to capture implicit memory-related vulnerability patterns. DeepWuKong ____ is another statement-level vulnerability detector that utilizes GATs, GCNs, and k-GNNs on sliced PDGs. These methods utilize GNN models to detect vulnerabilities, but their capacity to model diverse relationships between code elements contributing to different vulnerabilities is limited.

Tree-based approaches extract features by traversing the ASTs and their variants. Dong et al. ____ utilized code sequences extracted from ASTs as semantic features and the frequency as token features to build a fully connected neural network for detecting vulnerabilities in Android binary executables. Wang et al. ____ preserved three types of nodes, and converted them into code sequences. These sequences are mapped into high-dimension vectors to train deep belief networks (DBNs) for detecting software weakness. POSTER ____ and Lin et al. ____ discover vulnerabilities in the function level by building a bidirectional LSTM network with ASTs-based sequences. Dam et al. ____ built a sequence to sequence the LSTM network to learn the semantic and syntactic features from ASTs in Java methods. SySeVR ____ divided programs into small pieces and generated multiple representations from ASTs to exhibit the syntax and semantics characteristics of vulnerabilities. These approaches are incapable of capturing intricate program structural properties (branches or parallel statements).


Text-based methods exploit NLP models for vulnerability detection by treating source code as text. Peng et al. ____ used n-gram models and Wilcoxon rank-sum optimization on Java source code vectors. Hovsepyan et al. ____ and Pang et al. ____ used SVM with bag-of-words (BOW) and n-grams representations of Java code. Scandariato et al. ____ used text-mining for vulnerability prediction. Lee et al. ____ developed an Instruction2vec model on assembly codes. Yamaguchi et al. ____ applied NLP approaches to API symbol prediction. Luo et al. ____ used natural language syntax with BERT for Java code. Russell et al. ____ used CNNs and RNNs on embedded source representations. Le et al. ____ proposed a sequential autoencoder for feature extraction on binary code. Sestili et al. ____ used one-hot vectors and memory networks for buffer overflow detection. These methods focus on semantic information but may overlook the structural features. Shabtai et al. ____ applied principal component analysis to the ASTs of source code to identify vulnerable code. Similarly, Mokhov et al. ____ used numerous methods from WEKA ____ and the ASTs as characteristics to construct prediction models.

In addition to vulnerability detection, there are other source code representations for program analysis. CodeBERT ____ is a bimodal pre-trained model by adding programming language to base BERT ____, which supports downstream NL-PL applications. SourcererCC ____ transformed programs into regularized token sequences and ordered by an optimized inverted index for code clone detection. Based on program CFGs and PDGs, Allamanis et al. ____ applied Gated Graph Neural Networks to predict variable names and detect variable misuses, and DeepSim ____ encoded flows into a semantic matrix for measuring code functional similarity.