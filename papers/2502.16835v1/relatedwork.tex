\section{Related Work}
We review related work on machine learning-based detection of code vulnerability in terms of different representations of code: (a) code as graphs, (b) code as trees, and (c) code as text.  


Graph-based methods typically capture the structural characteristics of various types of graphs, such as control-flow graphs (CFGs), data-flow graphs (DFGs), and data dependence graphs (DDGs), using techniques such as graph neural networks (GNNs), convolutional neural networks (CNNs), or recurrent neural networks (RNNs), to construct models for detecting vulnerabilities. VulDeePecker \cite{Li2018} models C/C++ source code as a graph, leveraging graph-level and node-level features for vulnerability prediction. $\mu$VulDeePecker \cite{zou2019} is an enhanced version incorporating data and control dependencies from system dependency graphs (SDGs). Including code attention and localized information enables the model to effectively detect 40 different types of vulnerabilities. DeepTective \cite{rabheru2022hybrid}  combines gated recurrent units (GRUs) and graph convolutional networks (GCNs) to leverage both syntactic and semantic information for detecting SQLi, XSS, and OSCI vulnerabilities in PHP code. Velvet \cite{ding2022velvet} integrates graph-based and sequence-based neural networks to capture the local and global context of code and identify vulnerable patterns. DEVIGN \cite{zhou2019devign} uses a joint graph representation of code snippets by merging ASTs, CFGs, and DFGs for detecting vulnerabilities in C programs with high accuracy. FUNDED \cite{Wang2020} creates a method-level graph representation using nine types of edges to ASTs, and uses a pre-trained word2vec network updated by a GRU for node embeddings, achieving an average accuracy of 92\%. HGVul \cite{song2022hgvul} utilizes the code property graph combined with natural code sequence as a graph representation, which is fed into three different GNN models, GCN, GAT, and GGNN, achieving an F1 score of 88.3\%. EL-VDetect \cite{sun2023enhanced} integrates serialization-based and graph-based neural networks with an attention mechanism to capture code semantics and achieves 90.72\% accuracy on a real-world dataset. Wang et al. \cite{liu2021combining} utilize contract graphs for smart contract vulnerability detection, achieving an average accuracy of 89\%. AMPLE \cite{wen2023vulnerability} is a vulnerability detection framework with graph simplification and enhanced graph representation learning that aims to capture global vulnerable information. Cao et al. \cite{cao2022mvd} propose a statement-level vulnerability detection approach based on flow-sensitive graph neural networks (FS-GNN) to capture implicit memory-related vulnerability patterns. DeepWuKong \cite{cheng2021deepwukong} is another statement-level vulnerability detector that utilizes GATs, GCNs, and k-GNNs on sliced PDGs. These methods utilize GNN models to detect vulnerabilities, but their capacity to model diverse relationships between code elements contributing to different vulnerabilities is limited.

Tree-based approaches extract features by traversing the ASTs and their variants. Dong et al. \cite{dong2018defect} utilized code sequences extracted from ASTs as semantic features and the frequency as token features to build a fully connected neural network for detecting vulnerabilities in Android binary executables. Wang et al. \cite{wang2016} preserved three types of nodes, and converted them into code sequences. These sequences are mapped into high-dimension vectors to train deep belief networks (DBNs) for detecting software weakness. POSTER \cite{lin2017} and Lin et al. \cite{lin2018cross} discover vulnerabilities in the function level by building a bidirectional LSTM network with ASTs-based sequences. Dam et al. \cite{dam2017automatic} built a sequence to sequence the LSTM network to learn the semantic and syntactic features from ASTs in Java methods. SySeVR \cite{li2021sysevr} divided programs into small pieces and generated multiple representations from ASTs to exhibit the syntax and semantics characteristics of vulnerabilities. These approaches are incapable of capturing intricate program structural properties (branches or parallel statements).


Text-based methods exploit NLP models for vulnerability detection by treating source code as text. Peng et al. \cite{peng2015building} used n-gram models and Wilcoxon rank-sum optimization on Java source code vectors. Hovsepyan et al. \cite{hovsepyan2012} and Pang et al. \cite{pang2015} used SVM with bag-of-words (BOW) and n-grams representations of Java code. Scandariato et al. \cite{scandariato2014} used text-mining for vulnerability prediction. Lee et al. \cite{lee2017learning} developed an Instruction2vec model on assembly codes. Yamaguchi et al. \cite{yamaguchi2011} applied NLP approaches to API symbol prediction. Luo et al. \cite{luo2022Journal} used natural language syntax with BERT for Java code. Russell et al. \cite{Russell2018} used CNNs and RNNs on embedded source representations. Le et al. \cite{le2018maximal} proposed a sequential autoencoder for feature extraction on binary code. Sestili et al. \cite{sestili2018towards} used one-hot vectors and memory networks for buffer overflow detection. These methods focus on semantic information but may overlook the structural features. Shabtai et al. \cite{shabtai2009} applied principal component analysis to the ASTs of source code to identify vulnerable code. Similarly, Mokhov et al. \cite{mokhov2015} used numerous methods from WEKA \cite{holmes1994weka} and the ASTs as characteristics to construct prediction models.

In addition to vulnerability detection, there are other source code representations for program analysis. CodeBERT \cite{feng2020codebert} is a bimodal pre-trained model by adding programming language to base BERT \cite{Devlin2018}, which supports downstream NL-PL applications. SourcererCC \cite{sajnani2016sourcerercc} transformed programs into regularized token sequences and ordered by an optimized inverted index for code clone detection. Based on program CFGs and PDGs, Allamanis et al. \cite{allamanis2017learning} applied Gated Graph Neural Networks to predict variable names and detect variable misuses, and DeepSim \cite{zhao2018deepsim} encoded flows into a semantic matrix for measuring code functional similarity.