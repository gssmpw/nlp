% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{mdframed}
\usepackage{fancyvrb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{float}
% \setlength{\abovecaptionskip}{3pt} % Reduce space above caption
% \setlength{\belowcaptionskip}{0.1pt} % Reduce space below caption

\newcommand{\todo}[1]{\textcolor{green}{[todo: #1]}}
\newcommand{\jw}[1]{\textcolor{cyan}{[Jason: #1]}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Turning Conversations into Workflows:  A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents}
%{Decoding Procedures from Conversations: A Framework for Extracting and Evaluating Dialog Workflows}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{\quad Prafulla Kumar Choubey \quad Xiangyu Peng \quad Shilpa Bhagavath \\
 {\bf Caiming Xiong  \quad Shiva Kumar Pentyala \quad Chien-Sheng Wu}\\
Salesforce AI Research \\
\small{
   \textbf{Correspondence:} \href{mailto:pchoubey@salesforce.com}{pchoubey@salesforce.com}} \\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. 
Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\% in average macro accuracy over the baseline.
% Extensive experiments on the ABCD dataset, along with the SynthABCD dataset, demonstrate that our QA-CoT technique improves significantly workflow extraction by 12.16\% in average macro accuracy from  the baseline. 
Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.


% Procedural knowledge is crucial for developing advanced NLP systems in tasks such as planning, instruction generation, and decision-making. Despite its importance, procedural knowledge extraction remains underexplored compared to factual and conceptual knowledge. To reduce this gap, we propose a new framework for extracting and evaluating procedural knowledge from conversations. Our framework adapts the ABCD dataset~\cite{chen-etal-2021-action} and synthesizes a new dataset, SynthABCD, based on its workflows. We develop two automatic evaluation strategies—QA-based and End-to-End (E2E)—to assess the accuracy of procedural steps and their effectiveness in resolving customer issues. Both strategies strongly align with human evaluations. Additionally, we evaluate three conversation selection strategies for extracting procedures—random selection, maximizing coverage through diverse conversations, and focusing on similar conversations for consistency. We also propose a QA-CoT approach that uses question-answer pairs to generate structured chain-of-thought, to improve the extraction of accurate procedural knowledge. \todo{[old]}

%Furthermore, we evaluate three conversation selection strategies: (1) random selection, (2) selecting diverse conversations to maximize coverage, and (3) focusing on conversations with similar procedures to enhance consistency. Lastly, we explore reflection and chain-of-thought prompting methods to extract procedural knowledge from selected conversations, and propose a QA-CoT approach that generates structured thought through question-answer pairs, which is then used to derive the final procedural knowledge.
\end{abstract}

\section{Introduction}

LLMs are changing customer service by helping automated bots respond more accurately and consistently to customer inquiries. These bots use well-structured workflows to guide their interactions and ensure efficient issue resolution and adherence to company policies~\cite{brown2023automate}. However, high-quality workflows are scarce, making it challenging to maintain agent consistency and quality.

Traditional workflow creation methods often rely on manual efforts~\cite{BOHUS2009332,mosig2020star,mehri-eskenazi-2021-schema,chen-etal-2021-action,zhao-etal-2023-anytod,robino2025conversationroutinespromptengineering}, which can quickly become outdated and fail to capture the full nuances of real-world interactions. Notably, common procedural knowledge is embedded within past customer-agent interactions. For instance, Fig. \ref{fig:example} presents a segment of a workflow derived based on two conversations representing different customer scenarios. Yet extracting and structuring this information is unexplored. Automating the extraction of workflows from conversations offers a promising solution by transforming implicit knowledge into structured guidelines for both human agents and AI systems. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.92\linewidth]{latex/example.png}
    \caption{An example showing the derivation of a workflow from historical conversations. Full workflow is shown in Figs. \ref{fig:retriever_1_2} and \ref{fig:eval_1_2}.}
    \label{fig:example}
\end{figure}



% Knowledge mining from unstructured texts has traditionally focused on identifying named entities, events, and relationships and constructing factual knowledge graphs \cite{10.1145/1089815.1089817,10.5555/1214993,singhal2012introducing,wan2023gptre,xu2023large,Khraisha2023CanLL,qi2023mastering}. 
% Procedural knowledge, which involves necessary steps in the correct order to accomplish some tasks, is another important tenet of knowledge with many realistic applications, such as agent planning \cite{Huang2022LanguageMA}, customer support automation \cite{min2023workflowguided}, workflow automation \cite{gallanti1985}, and synthetic customer-agent chat generation \cite{du2024dflowdiversedialogueflow}. 
% Despite its broad applications, existing work on procedural knowledge extraction relies on well-written documents \cite{agarwal2020extracting, dunn2022structured, dannenfelser2024into}, and the task of extracting task-relevant steps from natural language interactions (e.g., conversations) remains under-explored.

% Extracting procedural knowledge from conversations is essential, as high-quality process documents are scarce, and the most recent and nuanced insights often emerge exclusively in natural language interactions.
% Once these conversational procedural knowledge or workflows are extracted, they can be used as structured guidelines for humans and AI, detailing permissible actions across various scenarios.
% Reconstructing procedural knowledge provides several advantages, such as, enhancing the reliability and scalability of automated customer service bots, and they can offer consistent and predictable responses to similar queries, improving the overall customer experience. Additionally, it help reduce redundancy, streamline the resolution process, and simplify the analysis of failure cases.

In this paper, we propose a novel two-step pipeline for automatically extracting dialog workflows from customer-agent interactions. These conversations often contain noise, such as agents combining multiple steps or sequencing them incorrectly. Moreover, customer interactions usually cover only a subset of the established rules, making it difficult to capture a complete workflow.
To tackle these challenges, our pipeline first enhances the retrieval process by identifying key procedural components (Fig.~\ref{procedural-elements})—such as intent, slot-values, and resolution steps—and selecting the most relevant conversations while filtering out non-compliant ones (Fig.~\ref{fig:retriever2}). 

In the second step, we introduce a structured QA-based chain-of-thought (QA-CoT) prompting technique to systematically extract workflows from large conversation datasets. Extracting workflows at scale is challenging, as it requires capturing all critical decision points and ensuring their correct representation within the workflow.
Our QA-CoT approach addresses this by simulating an interactive exchange between a Guide and an Implementer agent (Fig.~\ref{qa-cot-example}). The Guide systematically examines key aspects of the workflow, including preconditions, decision points, and step rationales, while the Implementer responds with insights derived from historical conversations. This structured QA process helps the LLM focus on each decision point individually, ensuring that all essential procedural elements are accurately incorporated into the final workflow.


\begin{figure}[]
\tiny
\begin{mdframed}
\begin{verbatim}
"intent": "Customer wants to refund an order before shipping.", 
"slot_values": {
    "full_name": "Chloe Zhang", 
    "username": "czhang94", 
    "email": "czhang94@email.com", 
    "order_id": "1553732700", 
    "refund_method": "original credit card", 
    "shipping_status": "not shipped"
}, 
"resolution_steps": [
    "Agent asked for the customer's full name.", 
    "Agent requested the username, email, and order ID.",
    "Agent asked how the customer would like the refund processed.",
    "Agent inquired about the account ID.", 
    "Agent asked about the shipping status of the order."
]
\end{verbatim}
\end{mdframed}
\caption{An example of procedural elements extracted from a conversation by the GPT-4o mini LLM.} 
\label{procedural-elements}
\end{figure}

% In this paper, we introduce a new framework to enable and evaluate workflow extraction from conversations. 
% We adapt the ABCD dataset~\cite{chen-etal-2021-action}, originally designed to ground conversations in predefined dialogue workflows, to function in reverse.
% While these conversations generally follow the expected workflows, they also contain noise due to human factors in the data collection process, such as agents merging steps or executing them out of sequence. %\jw{do we have an insight on the noise ratio? If so, add numbers here.} 
% In our automated analysis using an LLM, we observe that 28.47\% of conversations in the ABCD data validation set fail to follow at least one step from the reference workflow correctly.
% Furthermore, each conversation typically covers only a subset of the steps in a workflow, making it difficult to infer a comprehensive workflow from a limited samples. To complement the ABCD dataset, we also generate a synthesized yet clean SynthABCD dataset using LLMs, and it enables a controlled analysis of task feasibility.
% % and explores the upper potential of LLMs in reconstructing workflows from conversations under ideal conditions, with high-quality and noise-free dialogues.

% Another key contribution of our work is the evaluation strategy we designed to reliably evaluate procedural knowledge. %Evaluating procedures involves verifying both the accuracy of individual steps and their correct sequence within a workflow. In an initial study, we asked human annotators to evaluate whether each step in the reference procedures was accurately represented in the predicted procedures, using a set of 10 LLM-extracted procedures. The results showed moderate inter-annotator agreement, with a Cohen's $\kappa$ score of 0.45, highlighting the difficulties even humans face in identifying nuanced differences when judging procedural accuracy.
% % In our initial study, simply prompting LLMs to compare the similarity between two procedures led to low human agreement \jw{any numbers we can provide?}, as LLMs struggled to capture nuanced differences and provide meaningful judgments.
% % To ensure reliable evaluation of this task, w
% We propose two strategies with high human agreement: QA-based and End-to-End (E2E). %\jw{good names? let's stay consistent.}.
Another key contribution of our work is a robust multi-step evaluation pipeline for dialog workflows, featuring an end-to-end method that aligns closely with human assessments. We simulate interactions between a service agent bot (equipped with the predicted workflow and system data) and a customer bot (provided intent and user-specific details). After the conversation ends, we evaluate task success by comparing the outcome achieved using the predicted workflow with the expected outcome derived from the ground truth workflow (see Fig.~\ref{fig:eval2} for an example). % \jw{can we add an example expected outcome v.s. generated outcome to illustrate this concept better?}.
% We measure the task success rate with a given expected outcome (e.g., issue full refund) allowing us to observe the outcomes of using different workflows as guidance during execution. %We show examples of QA pairs extracted from procedures, along with the steps used to build the SynthABCD dataset and the E2E evaluation framework, in \cref{fig:eval-fw}.

% \todo{Another highlight of our framework is a novel conversation selection strategy combined with an agentic system for extracting procedures from historical conversations. The conversation selection strategy first utilizes an LLM to extract key procedural elements—such as customer issues, slot values, and resolution steps—from each conversation. It then selects conversations based on the embedding centrality of these extracted procedural elements, ensuring that the most representative and frequently occurring steps are prioritized. The agentic system involves the interaction of two LLM agents to collaboratively generate a structured chain of thought \citep{wei2022chain}, which serves as the foundation for the final procedure reconstruction by the LLM.}

We evaluate our proposed QA-CoT method against common prompting strategies using the ABCD~\cite{chen-etal-2021-action} and an LLM-generated SynthABCD datasets. ABCD originally grounds conversations on predefined dialog workflows, but we re-purpose it for extracting workflows from conversations. Additionally, to eliminate human-induced errors in ABCD conversations, we generate SynthABCD using LLMs, ensuring error-free conversations while complying with the same workflows. This allows for a controlled and reliable evaluation under an idealized setting.

Our QA-CoT outperforms other prompting methods across different LLMs including GPT-4o, Opus, Sonnet, and Gemini. For instance, on the ABCD dataset, it achieves an 11.81\% gain over the baseline with GPT-4o.
Furthermore, incorporating explicit QA-CoT reasoning also improves the performance of the state-of-the-art reasoning LLMs, including OpenAI o1, o1-mini, o3-mini, as well as DeepSeek-R1 \cite{guo2025deepseek} on both datasets.

%By selecting conversations based on centrality, we improve E2E accuracy by prioritizing common steps. Additionally, the agentic framework enhances procedure extraction by structuring the LLM’s reasoning process. One of our main findings from evaluations is that all models, including ours, perform significantly worse on the ABCD dataset. Even the best model on ABCD has roughly 32\% lower macro E2E accuracy than the basic model on SynthABCD, highlighting the challenges of extracting procedures from noisy, real-world conversations.

%Our approach outperforms traditional prompting techniques in extracting procedures. %, offering more accurate and comprehensive workflow generation.
%benchmarking on different knowledge procedure extraction strategies. We implement X conversation retrieval methods, utilize X LLMs to extract procedures, and experiment with various LLM debates and ensemble approaches. Our findings show that \jw{<add main takeaways from retrieval setup, LLM choice, and ensemble choice>}. In our best setup on the ABCD dataset, we achieve only \jw{X\%} QA-based accuracy and \jw{X\%} End-to-End task success rate, highlighting the challenge of this task and the need for further research.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/wf_proc.png}
        \caption{}
        \label{fig:retriever1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/eg1-retriever.png}
        \caption{}
        \label{fig:retriever2}
    \end{subfigure}
    \caption{(a) An example workflow with procedural elements for 3 of the 10 possible distinct sub-flows. Sub-flows and their procedural elements are color-coded to match. (b) Steps in our proposed procedural element-based retriever. An example of complete procedural elements extracted from a conversation is shown in Fig. \ref{procedural-elements}.}
    \label{fig:retriever_1_2}
\end{figure*}


\begin{figure*}[!tb]
\tiny % Adjust font size to better fit within page width
\begin{mdframed}
\begin{minipage}{\linewidth} % Ensures content does not overflow
\begin{Verbatim}[commandchars=\\\{\}]
..
\textbf{Guide:} What additional details are required to validate the customer's purchase?
\textbf{Implementer:} The agent must ask for the username, email address, and order ID to validate the purchase.
\textbf{Guide:} How does the customer's membership level impact the return process?
\textbf{Implementer:} The customer's membership level determines the return policy duration: 
                    - \textbf{Guest membership} requires returns within \textbf{30 days} with receipt.
                    - \textbf{Bronze membership} allows returns within \textbf{90 days} or with original packaging.
                    - \textbf{Silver membership} permits returns \textbf{within six months}.
                    - Gold members have unlimited time for returns.
\textbf{Guide:} What should the agent do if the return is past the allowable period for the customer's membership level?
\textbf{Implementer:} If the return period has passed, the agent should check if the customer has the original receipt and packaging. If not, they politely 
inform the customer that a return cannot be processed.
..
\end{Verbatim}
\end{minipage}
\end{mdframed}
\caption{A snippet of the QA chain-of-thought generated by the GPT-4o model for the $return\_color$ intent using conversations from the ABCD data. The extracted QA pairs highlight key preconditions based on membership level.}
\label{qa-cot-example}
\end{figure*}

% \begin{figure*}[tb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{latex/wf.png}
%     \caption{Diagram illustrating an example workflow and the steps in our proposed End-to-End (E2E) evaluation framework. Step 1 presents scenarios extracted from the workflow. Step 2 showcases the extracted system and user information, along with the expected outcome for each scenario.} % The E2E evaluation framework utilizes both Step 1 and Step 2 to simulate customer and service bot interactions.}
%     \label{fig:eval-fw}
% \end{figure*}


%conclusion: By using the accuracy of answers as a measure, we enable more reliable comparisons between different models and ensure a fairer and more standardized evaluation of workflow reconstruction tasks.
\section{Related Work}
% \cite{BOHUS2009332} managing chat through schema or dialog flows

% \todo{evaluation}

% Knowledge mining from unstructured texts has traditionally focused on identifying named entities, events, and relationships and constructing factual knowledge graphs \cite{10.1145/1089815.1089817,10.5555/1214993,singhal2012introducing,wan2023gptre,xu2023large,Khraisha2023CanLL,qi2023mastering}. 
% Existing methods for extracting procedural knowledge predominantly rely on well-structured textual sources, such as manuals and documentation \cite{agarwal2020extracting,dunn2022structured,dannenfelser2024into}. In contrast, the task of extracting task-relevant steps from natural language interactions remains an under-explored challenge.
% % Procedural knowledge, which involves necessary steps in the correct order to accomplish some tasks, is another important tenet of knowledge with many realistic applications, such as agent planning \cite{Huang2022LanguageMA}, customer support automation \cite{min2023workflowguided}, workflow automation \cite{gallanti1985}, and synthetic customer-agent chat generation \cite{du2024dflowdiversedialogueflow}. 
% % Despite its broad applications, existing work on procedural knowledge extraction relies on 
% % well-written documents \cite{agarwal2020extracting, dunn2022structured, dannenfelser2024into}, and the task of extracting task-relevant steps from natural language interactions (e.g., conversations) remains under-explored.

%Prior work has primarily relied on human-authored dialog workflows \citep{mosig2020star,chen-etal-2021-action}. While dialog workflows are a specialized form of procedural knowledge, most 
Dialog workflows are a specialized form of procedural knowledge. While workflow extraction has received little attention, automatic procedural extraction has been widely studied, primarily focusing on ``how-to'' documents \citep{10.1145/2187980.2188194,maeta-etal-2015-framework,Chu2017DistillingTK,Park2018LearningPF} and instructional videos \citep{ushiku-etal-2017-procedural,10.5555/3504035.3504965,xu-etal-2020-benchmark}.
These works typically model linear sequences of explicitly stated actions, aiming to either predict procedural steps or generate summaries of task execution \citep{Koupaee2018WikiHowAL}.
% , which typically feature a linear sequence of actions clearly stated within the text. These studies primarily aimed at either predicting the sequence of actions or providing a summary of the procedural steps \citep{Koupaee2018WikiHowAL}.
In contrast, our work tackles dialog workflows, where actions are often implicit and depend on previous steps, user inputs, and system responses. This introduces decision-dependent variability, making extraction significantly more challenging than predicting fixed procedural sequences.

Specific to dialog systems, there has been extensive research on studying structures in task-oriented dialogs~\cite{Jurafsky1997SwitchboardSS,Chotimongkol2008LearningTS,shi2019unsuperviseddialogstructurelearning,xu2020discoveringdialogstructuregraph,chen2021dsbertunsuperviseddialoguestructurelearning,Nath2021TSCAND,wang2021modellinghierarchicalstructuredialogue,rony2022dialokgknowledgestructureawaretaskoriented,lu2022unsupervisedlearninghierarchicalconversation,qiu2022structureextractiontaskorienteddialogues,Yin_2023,pryor2024usingdomainknowledgeguide,burdisso2024dialog2flow}, focusing on how dialogs evolve using dialog acts, intent-slot pairs, or turn-level dependencies. Our work parallels workflow discovery \cite{hattami2023workflow, raimondo2023improving,min2023workflowguided}, which aims to predict the optimal next dialog action from the conversation's current state and all available actions. However, unlike this, we focus on extracting global workflows applicable across conversations for a specific intent. This increases the complexity of the task, as the model must filter out noisy actions, and consolidate multiple potential actions sequences from different conversations. Additionally, we also propose a new QA-CoT prompting method for workflow extraction and introduce a robust  end-to-end evaluation framework to assess the accuracy of the extracted workflows.
 
% \section{Task Definition and Dataset Creation}
\section{Dialog Workflow Extraction }
% \jw{reminder to update Fig 4 based on our discussion.}
% Task-oriented dialogue systems depend on predefined workflows that outline steps to resolve customer issues in different scenarios.
The dialog workflow extraction task aims to identify and organize all essential steps required to resolve customer issues across various scenarios into structured guidelines. For instance, Fig. \ref{fig:retriever1} illustrate a workflow for customer complaints about a bill they never purchased, while Fig. \ref{fig:eval1} presents the corresponding state machine diagram, highlighting different sub-flows. It covers scenarios such as assisting gold, silver, guest, or bronze members; addressing cases with or without system errors; and accommodating customers who provide either their full name or account details.
In this example, we aim to generate a workflow that covers all variables, including membership levels, system errors, and available customer information.
%These workflows must comprehensively cover potential resolution paths to handle diverse and complex cases effectively. 

% \section{Procedure Extraction System}
We focus on two key aspects to improve dialog workflows extraction: retrieval, which involves selecting the most relevant historical conversations for the task, and extraction, which uses a structured QA-CoT prompting approach to derive workflows from the selected conversations.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/eval_1.png}
        \caption{}
        \label{fig:eval1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/eval_2.png}
        \caption{}
        \label{fig:eval2}
    \end{subfigure}
    \caption{(a) Flowchart of the workflow in Fig. \ref{fig:retriever1}, illustrating 10 possible customer scenarios [Step 1, E2E pipeline], along with an example of user information, system information, and the success criteria for one scenario [Steps 2 and 3]. (b) Simulation of the user (system) bot based on intent and user information (system information and the predicted workflow) [Step 4], followed by a final evaluation of dialogue success [Step 5].}
    \label{fig:eval_1_2}
\end{figure*}

\subsection{Procedural Element-based Retrieval}
Our goal is to extract all valid step sequences for resolving customer issues, ensuring comprehensive scenario coverage while filtering out incorrect or noisy conversations. Since our focus is on the underlying process instead of surface-level dialogs, we propose a two-step selection strategy.
First, we extract key procedural elements—such as intent, relevant slot values, and the resolution steps taken by the agent—from each conversation using GPT-4o-mini (example in Figs.~\ref{procedural-elements} and ~\ref{fig:retriever1}, prompt in Fig. \ref{prompt:conv-selection-procedure}).
% First, we extract essential procedural elements (example in Figure~\ref{procedural-elements})—the customer issue, relevant slots and values, and the agent’s actions—from each conversation using GPT-4o-mini (prompt shown in \cref{prompt:conv-selection-procedure}). 
This ensures selection is based on procedural content rather than surface-level dialog.

Next, we embed each conversation using its extracted key procedural elements with the OpenAI text-embedding-3-small model, then we cluster and compute the centroid for each intent. We then select the top-K conversations closest to each centroid using cosine similarity, ensuring the chosen conversations represent the most commonly followed resolution steps (Fig.~\ref{fig:retriever2}). By focusing on key procedural elements rather than full conversations, we retain meaningful process information while filtering out noise. 
As we demonstrate in $\S$\ref{result:retriever}, directly clustering full conversation embeddings captures extraneous details and conversational variability, leading to significantly lower performance compared to procedural element embeddings.
% \jw{can we point our early experiments on clustering conversation into appendix here to strengthen our claim? We need stronger support or motivation to support your design, more than just these few words about we believe so.} \jw{saw Sec 5.2 you mentioned that, better to still give a hint here to motivate ours and link to 5.2}

While this approach effectively captures frequent resolution steps, it may reduce diversity. Given the inherent noise in conversations, distinguishing between true procedural variations and spurious deviations remains a challenge for LLMs. Our strategy prioritizes common procedural patterns while minimizing noise, enabling more reliable workflow extraction.
% \subsection{option 1}
% We explore three strategies for conversation retrieval: random selection, selecting conversations with similar procedures, and selecting diverse conversations to maximize procedural coverage. To implement similarity-based selection, we propose a two-step strategy to identify representative conversations for each intent. First, we extract key procedural elements—such as the customer issue, slots and values, and the agent's actions—from each conversation by prompting an LLM (prompt shown in \cref{prompt:conv-selection-procedure}). To select similar conversations, we compute the centroid of embeddings for these procedural elements across all conversations addressing the same issue and select the top-K conversations closest to the centroid based on cosine similarity. This approach focuses on most common procedural information while avoiding noise from irrelevant dialogue details.

% To enable diversity-based selection, we similarly rely on embeddings of procedural elements rather than entire conversations. First, we remove noisy conversations by filtering out those in the bottom percentiles based on their cosine similarity to the centroid of procedural elements. From the remaining conversations, we select the conversation least similar to the centroid as a starting point. Subsequently, we iteratively select additional conversations that are least similar to the centroid of the previously selected procedural elements. 
% \subsection{option 2}
% We propose a two-step strategy to select representative conversations for each intent. First, we extract key elements: the customer issue, slots and values, and the agent's actions from each conversation by prompting the GPT-4o-mini model (prompt shown in \cref{prompt:conv-selection-procedure}). 
% Then, we compute the centroid of embeddings for these elements across all conversations on the same issue and select the top-K conversations closest to the centroid based on cosine similarity. Using embeddings of key elements, rather than entire conversations, ensures that selection focuses on meaningful procedural information, avoiding noise from irrelevant dialogue details.
% This selection approach enables us to extract the most frequently used steps accurately, though it comes at the expense of diversity. Given the inherent noise in conversations, distinguishing between noise and true diversity is a significant challenge for LLMs. Therefore, we adopt a strategy that emphasizes extracting common procedural patterns while minimizing noise.

\subsection{QA-CoT for Workflows Extraction}

% \iffalse
% We propose extracting procedures from conversations by generating a structured chain-of-thoughts as a sequence of QA pairs that capture key decision points and procedural logic.

% \paragraph{QA-CoT Generation} 
% % We prompt an LLM to generate question-answer exchanges between a Guide and an Implementer agent (prompt shown in Fig.~\ref{prompt:qa-cot}). The Guide formulates targeted questions to understand the steps taken in a given scenario based on historical conversations. It focuses on clarifying preconditions, decision points, and the logic behind each step, ensuring that all possible customer contexts and edge cases are addressed. The Implementer reviews historical conversations to answer the Guide's questions, providing detailed explanations about the actions taken at each step and their rationale.


% We prompt an LLM to simulate question-answer exchanges between a Guide and an Implementer (example in Fig.~\ref{qa-cot-example}, prompt in Fig.~\ref{prompt:qa-cot}). The Guide formulates targeted questions to analyze the steps taken in a given scenario, focusing on preconditions, decision points, and underlying logic. It ensures that all possible customer contexts and edge cases are addressed. The Implementer reviews historical conversations and provides detailed responses, explaining the rationale behind each action and how steps were executed. \jw{this part doesn't provide much details comparing to line 73 to 84. The motivation is missing and think about why people should believe in this. Ideally, can we change the flow to a) The naive solution to extract workflow is just use this A prompt with retrieved chats as input, but we observe XYZ reasons and drawbacks while doing this in our early experiments. b) thus, motivated by XYZ work (let's cite some QA-guided or multi-agent discussion collaboration/debate work), we design a Guide agent with this prompt B, where the goal of the agent is to do XYZ, and the other Implementer agent with this prompt C, the goal is to do XYZ. c) how do we design the conversation, how many turns, any reflection or revision. d) After that, we use this prompt D to combine original chat + the augmented chat from agents to generate procedure, and with that we are observing X\% better as shown in Section X or Table X.}

% \paragraph{Procedure Extraction} 
% The generated QA pairs form a structured chain-of-thought, serving as the foundation for workflow reconstruction. By combining these QA exchanges with historical conversations, we prompt the LLM to reconstruct complete workflows (prompt shown in Fig.~\ref{prompt:procedural-extraction}).
% Our QA-CoT enables the LLM to systematically map decision points to their corresponding actions. This ensures that different procedural variations are covered, and generates workflows that comprehensively represent all possible resolution paths for an intent.
% \fi




A naive approach to extracting workflows is to use simple prompting (Fig.\ref{prompt:procedural-extraction-basic}) with retrieved conversations. However, this often results in workflows that lack fine-grained details. For instance, in the example workflow shown in Fig.\ref{fig:retriever1}, the generated workflow might rigidly follow a sequence by checking for errors first and then asking the membership level even when a system error occurs.


To address these limitations, we propose QA-CoT, a structured framework inspired by multi-agent debate and collaboration systems~\cite{du2023improvingfactualityreasoninglanguage,zhang2024chainagentslargelanguage,musumeci2024llmbasedmultiagentgeneration,hu2025debatetowritepersonadrivenmultiagentframework}. In QA-CoT, a Guide-Implementer interaction generates QA pairs (example in Fig.~\ref{qa-cot-example}, prompt in Fig.~\ref{prompt:qa-cot}) to capture fine-grained procedural knowledge. 
The Guide formulates targeted questions based on past conversations, focusing on clarifying preconditions, decision points, and logical dependencies. For instance, in the example workflow shown in Fig.~\ref{fig:retriever1}, the Guide would explicitly ask: ``What is the next step if there is a system error?'' and ``What is the next step if there is no system error?'' The Implementer, using the same past conversations, provides detailed responses, such as ``asking for the membership level'' or ``processing a refund'' based on the scenario. To improve workflow extraction, we augment historical conversations with the Guide-Implementer exchange and prompt the LLM (Fig.~\ref{prompt:procedural-extraction}) to generate workflows that incorporate fine-grained procedural details.

We experimented with two approaches for simulating this interaction. The first is multi-turn prompting, where the Guide and Implementer take turns. The Guide asks a question, and the Implementer provides a response. This process continues until the Guide has no further clarification questions or a maximum of 25 turns is reached. The second approach generates the entire exchange in a single pass. Empirically, the single-pass method yields superior results with GPT-4o (results in Table~\ref{tab:multi-step-vs-single-step-qa}), while also being computationally efficient. Given our large-scale experiments across 8 LLMs and two datasets, we adopt the single-pass strategy.


\section{Dialog Workflow Evaluation}
% \section{Evaluation Framework for Procedural Knowledge}
Evaluating workflow requires careful attention to the relationships and dependencies between constituent steps to ensure the predicted workflow aligns with the expected decision-making process. We encountered significant challenges in performing this evaluation at the workflow level, primarily due to the complex dependencies between steps that are difficult to assess consistently. % \jw{Can we add a hint here about: In our early study, if we give any SOTA LLM like GPT4 or Claude3.5 one ground truth workflow and one machine generate workflow that contains errors, they can only recognize the error in X\% of samples, having high precision but low recall. --> With this, then we can motivate why we tried several ways to evaluate and in the end we found E2E correlates the best with human}. 
In an initial study, we asked human annotators to evaluate whether each step in the ground-truth workflows was accurately represented in the predicted workflows, using a set of 10 LLM-extracted workflows. The results showed moderate inter-annotator agreement, with a Cohen's $\kappa$ score of 0.45, highlighting the difficulties even humans face in identifying nuanced procedural differences.


To address these challenges, we propose an end-to-end (E2E, Fig.\ref{fig:eval_1_2}) evaluation framework that works in five stages\footnote{We also explores alternative evaluation strategies such as QA-based evaluation~\cite{fabbri2021qafacteval}. These strategies proved less effective than our proposed E2E approach and are discussed in detail in Appendix~\ref{sec:alternative-evaluation}.}:

\begin{itemize}[leftmargin=*]%, itemsep=0.3pt, topsep=0.3pt]
    \item \textit{Decomposing Workflows into Sub-flows}: We first decompose each workflow into its constituent sub-flows. As shown in Fig. \ref{fig:eval1} (Step 1), this process involves extracting all 10 possible paths from START to END, which result from variations in membership levels, the type of user input (full name or account ID), and the occurrence of system errors. This approach ensures comprehensive coverage of all decision branches in our evaluation (prompt shown in Fig. \ref{prompt:synthabcd-e2e-step1}).
    \item \textit{Generating User and Agent Bot Information}: Next, we map each sub-flow to the relevant user and system information required for bot simulation. In the subflow highlighted in red in Fig. \ref{fig:eval1} (Step 2), the user bot should provide only their full name, username, order ID, and membership level (e.g., silver), while the agent bot has access to internal systems that confirm no error occurred on the company’s end (prompt shown in Fig. \ref{prompt:e2e-step2}).
    \item \textit{Defining Success Criteria}: We further link each sub-flow to a success criterion, which represents the expected outcome of the conversation if the agent follows the ground-truth workflow. In the sub-flow highlighted in red in Fig. \ref{fig:eval1} (Step 3), the agent is expected to approve the user's refund request (prompt shown in Fig. \ref{prompt:e2e-step2}).
    \item \textit{Dialog Simulation}: Next, we simulate interactions between a customer bot and a service agent bot, where the customer bot conveys intent and provides user-specific details, while the agent bot utilizes system data and acts according to the predicted workflow. The customer bot responds strictly to explicit agent's requests or seeks alternatives when unable to provide the requested information. Meanwhile, the agent bot executes the predicted workflow by requesting data (e.g., asking for a username), performing actions (e.g., issuing refunds, checking system information), or terminating the conversation if no further steps are possible. The interaction ends when the agent either completes the workflow successfully or cannot proceed due to missing information (Fig. \ref{fig:eval2}, prompts in Figs. \ref{prompt:customer-bot-e2e} and \ref{prompt:service-bot-e2e}).
    \item \textit{Success Evaluation}: Once the conversation ends, we evaluate whether the simulated interaction meets the success criteria. For instance, in Fig. \ref{fig:eval2}, the interaction fails because the user bot was unable to provide the account ID (prompt shown in Fig. \ref{prompt:e2e-success}).
\end{itemize}


We evaluated the accuracy of our E2E simulation pipeline and found that the overall accuracy across steps 1 to 4 was 94.81\%. Moreover, step 5 exhibited strong alignment with human evaluation, achieving a Cohen’s Kappa score of 0.92. These results suggest that our E2E evaluation closely mirrors human judgments (for details, see $\S$\ref{appendix:e2e-human}).
% %%%%%%%%%%%%
% \iffalse

% evaluation framework that first decomposes workflows into constituent sub-flows. For each subflow, it extracts user and agent bot profiles and simulates a conversation based on these profiles and the predicted policy. Finally, it verifies whether the conversation reaches a successful resolution. This approach streamlines the evaluation process for both human and automatic evaluators, enhancing consistency and reliability\footnote{We also explore alternative evaluation strategies that proved less effective than E2E, discussed in details in Appendix~\ref{sec:alternative-evaluation}.}.


% % Often, predicted workflows combine multiple rules in a single step, making it harder to accurately evaluate the conditional logic\footnote{We discuss several alternative evaluation strategies in Appendix~\ref{sec:alternative-evaluation}.}. 
% % To address these challenges, we introduce an End-to-End (E2E) evaluation framework to assess the effectiveness of a predicted workflow in resolving customer issues.


% \subsection{End-to-End (E2E) Simulation Framework}
% E2E method directly measures how well an automatically simulated service bot, based on the predicted policy, performs in addressing real customer problems. It mimics real-world interactions, 
% where the service bot has access to the issue-resolution workflow and internal system information, while the customer bot is provided  with details such as issue description, personal information (e.g., name, address), and order specifics.

% \jw{I feel Sec 4.1 to Sec 5.1 is hard for people to understand. I suggest we make this revision: a) let's make 4.1 and 4.2 neat by removing the dependency of knowing how you handle ABCD datasets. In Sec 4.1 just make this clear: what is the input of service bot, what is the input of customer bot, how to you let them chat and what's the stopping criteria, how do you measure whether the chat meets the expectation, and what's human agreement on that. And, very important, give an example to illustrate this.}

% \jw{I suggest move 4.2 into section 5 after you already introduce data. 4.2 now has too many dataset details.}

% \jw{Everything that is not in the cleaned Sec 4.1 can move to section 5.1 where you will introduce not just SynthABCD, but how do you process ABCD to get what you need in Sec 4.1}

% %we propose an End2End (E2E) evaluation framework, which measures how many customer scenarios the predicted workflow handles correctly. This approach also mimics real-world use cases by simulating an agent bot that operates based on predicted workflows and assessing its success in resolving customer issues.

% % We propose two frameworks for reliable and objective evaluation of predicted workflows. %\todo{explain what each of them measures}
% % % \jw{move some intro info here, let's see how to merge}
% % QA-based evaluation strategy provides an objective means to evaluate each step in predicted workflows. %, demonstrating a high Pearson correlation of XX between human evaluators. Additionally, applying the same two-step process for automatic evaluation using an LLM yields a strong correlation (YY) between automatic and human assessments. These results suggest that our evaluation protocol offers a scalable, consistent, and efficient approach for assessing predicted workflows.
% % End2End evaluation strategy measures how many customer scenarios can a predicted polcy correctly handle.%directly using the predicted policy to simulate conversations between a service agent bot and a customer bot and measure task success by calculating the percentage of conversations, based on predicted workflows, that result in the same outcome as the corresponding gold workflows. 
% %To conduct this evaluation, we introduce a new simulation framework that utilizes the ground truth workflow to identify all potential customer scenarios. For each scenario, we extract the following elements: issue description, customer profile information, system information, and expected outcome. We use the issue description and customer profile to simulate the customer bot, and the system information along with the predicted policy to simulate the agent bot. The task is considered successful if the conversation leads to the expected outcome. Our human analysis demonstrates a high agreement, with an XX kappa score, between human evaluations and the results from our proposed simulation framework.

% % \subsection{QA-based Evaluation Framework}
% % % Evaluating a workflow requires more than just checking whether the predicted workflow includes all the necessary steps from the reference gold workflow. It 
% % We propose a QA-based evaluation metric to ensure the accuracy of each step in a procedure by verifying its correct dependence on preconditions, such as the outcome of a previous step. For example, if the ground truth workflow includes a step to `ask shipment status' followed by either `ask membership level if the product has shipped' or `issue refund if the product has not shipped,' our goal is to verify that the predicted workflow reflects this conditional logic accurately. To achieve this, we decompose reference workflows into atomic questions, such as: `Q: What is the next step after asking shipment status if the product has not shipped? A: issue refund,' and `Q: What is the next step after asking shipment status if the product has shipped? A: ask membership level.' By breaking down the workflow into discrete questions that focus on evaluating each decision point, we simplify the evaluation process, ensuring consistency and clarity in assessing the correctness of the predicted workflows.
% % For example, if the ground truth workflow includes a step to "ask shipment status" followed by either "ask membership level if the product has shipped" or "issue refund if the product has not shipped," our goal is to verify that the predicted workflow reflects this conditional logic accurately. This evaluation requires careful attention to the relationships and dependencies between steps to ensure the predicted workflow aligns with the expected decision-making process.
% % We encountered significant challenges in performing this evaluation at the workflow level, primarily due to the complex dependencies between steps that are difficult to evaluate consistently. Often, predicted workflows might combine multiple rules in a single step, making it harder to assess the conditional logic accurately. Additionally, workflows may include policies that are not always explicit actions but rather instructions such as "go to step [N] after this step," further complicating the evaluation. To address these challenges and simplify the evaluation process, we decompose the reference gold workflows into atomic questions. For example, for the above example, we create questions such as: "Q: What is the next step after asking shipment status if the product has not shipped? A: issue refund," and "Q: What is the next step after asking shipment status if the product has shipped? A: ask membership level." By breaking down the workflow into discrete questions that focus on evaluating each decision point, we simplify make the evaluation process, ensuring consistency and clarity in assessing the correctness of the predicted workflows.
% % \paragraph{Human Evaluation Study:} To establish the utility of our proposed QA evaluation protocol, we conduct a human study on workflows for 11 complex intents, each accompanied by 4 predicted workflows. The evaluation process began with decomposing each workflow into a series of QA pairs. For the gold reference workflows, one author wrote the QA pairs, while a second author verified their quality. This dual-step validation ensures that the questions were clear and accurately represented the underlying steps and conditional logic inherent in the workflows. From the 11 workflows, we obtain a total of 141 QA pairs. Additionally, the 4 predicted workflows for each intent collectively yield 564 QA pairs, providing a comprehensive dataset for evaluation.
% % Next, two authors independently labeled the correctness of each QA pair based on the corresponding predicted workflow. This independent labeling process resulted in a Cohen's $\kappa$ score of 0.742, indicating substantial agreement between the annotators. 
% % For QA pairs labeled as correct and incorrect, we achieve an F1 score of 0.937 for correct labels, demonstrating high agreement on identifying correct steps, and 0.82 for incorrect labels, indicating slightly lower agreement in identifying errors. This further indicates that detecting errors by reasoning on previous steps and their outcomes is challenging, even for humans.
% % To assess the overall alignment between the predicted and reference workflows, we calculate a final score for each predicted workflow based on the percentage of accurately annotated QA pairs. This score shows a correlation score of 0.835 between human annotators, indicating strong consistency in their evaluations.   [moved to human eval section]
% % \paragraph{Automated Evaluation}
% % Conducting large-scale human evaluation, while reliable, is not scalable and difficult to conduct consistently. So, w
% % We use LLM for both generating QA pairs from reference workflows and evaluating their accuracy with respect to predicted workflows. This automation allowed us to scale the evaluation process efficiently while maintaining reasonable reliability. % Using GPT-4o for both decomposing workflows into QA pairs and assessing their correctness, achieves a correlation score of 0.708 with human annotations. This correlation is notably high, especially when compared to prior studies that evaluated only the factual coverage in summarization tasks, which reported lower correlation scores, such as 0.37 \cite{huang-etal-2024-embrace} and 0.75 \cite{laban-etal-2024-summary}. Our two-step QA-based workflow evaluation surpasses or is close to these benchmarks while evaluating not just coverage of steps but also their sequential inter-dependencies. %These findings underscore that our proposed evaluation method, leveraging LLMs, provides a robust and scalable means of evaluating predicted workflows.
% % The prompts for converting workflows to QA and evaluating QA accuracy are shown in \cref{prompt:worfklow-to-qa} and \ref{prompt:qa-evaluation} respectively.

% % \subsection{End-to-End (E2E) Simulation Framework}
% % We introduce the End-to-End (E2E) evaluation to assess the effectiveness of a predicted workflow in resolving customer issues. This method directly measures how well an automatically simulated service bot, based on the predicted policy, performs in addressing real customer problems. It mimics real-world interactions, 
% % where the service bot has access to the issue-resolution workflow and internal system information, while the customer bot provides details such as issue description, personal information (e.g., name, address), and order specifics.

% To simulate this interaction, we first prompt an LLM to map the ABCD workflows to
% all possible customer scenarios. As shown in step 1 (Fig. \ref{fig:eval-fw}), this involves extracting all 10 paths from START to END in the workflow. Covering these paths is crucial for capturing all procedural variations present in the original workflows and to allow us to control both the service and customer bots to accurately follow a sub-workflow. Next, for each scenario, we prompt the LLM to generate customer details, system information, and expected outcomes (step 2 in Fig.\ref{fig:eval-fw}). 
% Using these metadata, we simulate interactions between a customer bot and a service agent bot. The customer bot provides issue descriptions and customer-specific details, while the agent bot operates based on the predicted policy and system data. During the interaction, the customer bot only responds to information explicitly requested by the agent bot or seeks alternatives when it lacks requested information. The agent bot executes the predicted workflow, requesting data (e.g., asking username), performing actions (e.g., issuing refunds or checking system information), or concluding the conversation if no further steps can be taken. The interaction ends when the agent bot either completes all steps in the predicted workflow or is unable to proceed due to missing information.
% After the conversation ends, we use the LLM to evaluate whether the simulated interaction concluded successfully by comparing it to the expected outcome. %and the gold reference policy. 
% The prompts used in steps 1, step 2 and evaluating conversation success are shown in Figs. \ref{prompt:synthabcd-e2e-step1}, \ref{prompt:e2e-step2} and \ref{prompt:e2e-success} respectively.

% % \todo{We perform extensive human evaluation, finding high agreement between human and both QA and E2E evaluation frameworks. Details are provided in \cref{sec:human-evaluation}.}
% % \paragraph{Human Evaluation Study:} We manually evaluated the automated generation of customer information, system data, and expected outcomes across different scenarios. For the 224 correctly extracted scenarios out of 231 possible scenarios (\cref{sec:synthabcd}), GPT-4o generated these details accurately in 219 cases. This demonstrates that we can simulate customer and service bots with high accuracy and coverage using LLMs like GPT-4o.

% % Next, to evaluate the LLM's ability to predict conversation success, two authors manually labeled successful completions for 105 simulated conversations. This manual evaluation yielded a high inter-annotator agreement, achieving a Cohen’s $\kappa$ of 0.966. Additionally, the agreement between the LLM's evaluation and human judgment was similarly strong, achieving a Cohen’s $\kappa$ of 0.922. These results highlight the robustness of the full E2E evaluation framework and its effectiveness in evaluating procedural knowledge. [moved to human eval section]

% \fi
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental Settings}
% We talk about task setup officially, as well as details on how to transfomer ABCD, and also SynthABCD.
\subsection{Dataset}
% \todo{Data Compliance Eval: keep here or eval section?}
We use two datasets in our evaluations: the human-developed ABCD and the LLM-synthesized SynthABCD. The ABCD dataset is inherently noisy ($\S$~\ref{sec:data-compliance}), as conversations were generated through human interactions between a customer and a support agent. This process introduced inconsistencies, such as customers providing information before being prompted and agents deviating from workflows by reordering, skipping, or merging steps. While these variations reflect real-world interactions, they complicate workflow extraction. To address these challenges, we also create SynthABCD, a synthetic dataset derived from ABCD workflows using an LLM. SynthABCD reduces noise by strictly following predefined workflows, enabling a more accurate evaluation of LLMs' capabilities.

In the first step of the SynthABCD synthesis, we use the same step 1 from E2E evaluation framework to generate all possible sub-flows. %Each scenario corresponds to a specific customer context or path within the workflow. 
Next, we pair each subflow with the corresponding workflow to guide the LLM in simulating several conversations between a customer and an agent for each subflow.
Additional data details, such as the intents used for ABCD and SynthABCD and human evaluation for SynthABCD, are discussed in $\S$\ref{sec:appendix-data}. 

\subsection{Conversations Retrievers}
We evaluate our procedural element similarity-based retrieval strategy (Proc-Sim) against three baselines: random selection, procedural element diversity-based retrieval (Proc-Div), and conversation similarity-based retrieval (Conv-Sim). 
For Conv-Sim, we use the same approach as Proc-Sim, except that we use full conversation embeddings rather than procedural element embeddings.
For Proc-Div, we want to enrich diversity of selected conversations, by first removing noisy conversations by excluding those in the lowest 10th percentile based on cosine similarity to the centroid of procedural element embeddings. Then, from the remaining conversations, we select one conversation furthest from the centroid as the initial candidate. Then, we iteratively select additional conversations that are maximally distant from the centroid of the procedural elements of the already selected conversations~\cite{GONZALEZ1985293}. %\jw{can we cite some work about this baseline?}.  


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/GPT-4o_retriever.png}
        \label{fig:f1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/gemini_retriever.png}
        \label{fig:f2}
    \end{subfigure}

    \vspace{1em} % Add some space between rows

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/opus_retriever.png}
        \label{fig:f3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/sonnet_retriever.png}
        \label{fig:f4}
    \end{subfigure}
    
    \caption{Performance comparison of conversation selection strategies (Proc-Sim, Conv-Sim, Random, and Proc-Div) with varying numbers of selected conversations. Results are shown for four LLMs: gpt-4o, gemini-1.5-pro, opus-3, and sonnet-3.5, using E2E macro (solid lines) and micro (dashed lines) accuracies.
    }
    \label{fig:retriever}
\end{figure*}


\subsection{Workflows Generation Method}
We compare the performance of our proposed QA-CoT model against several prompting techniques. %Prompts are shown in Figs. \ref{prompt:procedural-extraction-basic}-\ref{prompt:procedural-extraction-ensemble} in appendix. 
\begin{itemize}[leftmargin=*]%, itemsep=0.3pt, topsep=0.3pt]
    \item Basic: We use a single-step prompt over selected conversations to generate workflows directly (prompt in Fig.~\ref{prompt:procedural-extraction-basic}). 
    
    \item Reflection~\cite{shinn2023reflexionlanguageagentsverbal}: We build upon Basic by identifying and addressing coverage gaps in the workflow. After generating the initial workflow using basic prompting, a reflection prompt is applied to identify missing steps or inconsistencies, which are then refined to improve the workflow iteratively (prompts in Figs.~\ref{prompt:procedural-extraction-reflect} and~\ref{prompt:procedural-extraction-reflect-generate}).

    \item Plan~\cite{wang-etal-2023-plan}: This approach first prompts the LLM to devise a structured plan for extracting workflows given the conversations and then applies a second LLM prompt to execute the plan and generate workflows from the same set of conversations (prompts in Figs.~\ref{prompt:procedural-extraction-plan} and~\ref{prompt:procedural-extraction-plan-generating}).

    \item Ensemble: We start by generating four workflows using basic prompting, each based on a different random order of conversations. The LLM then evaluates these workflows, checking step-by-step correctness based on cross-consistency among them, and produces a final consolidated workflow by selecting the most consistent steps across all versions~\cite{chen2023universalselfconsistencylargelanguage} (prompt in Fig.~\ref{prompt:procedural-extraction-ensemble}).

    
    \item QA-CoT+Reflect: We apply a reflection prompt to the QA chain-of-thought generated through the Guide-Implementer interaction, identifying and refining missing questions before generating the final workflow (prompt in Fig.~\ref{prompt:procedural-extraction-qa-reflect}).
\end{itemize}

% \todo{QA-CoT + reflection and prompts}
%Our proposed method, QA-CoT, employs a QA chain-of-thought approach to guide procedure extraction. 
% On the ABCD dataset, we also include a variant, QA-CoT (w/all), which leverages all available conversations for QA generation while using only the selected conversations for the actual workflows generation \jw{I don't understand what is this setup and why we need this, and why only on ABCD. Is this important? If not, consider removing this. If it is important, make it clear.}. 
For consistency, all numbers are averaged using two workflows per intent by randomizing the order of conversations in all experiments.  We evaluate performance using macro accuracy (arithmetic average of the percentage of sub-flows correct per intent), micro accuracy (percentage of total subflows correct across all intents), and \#utt, which represents the average number of utterances in simulated conversations.
% \todo{Prafulla arxiv}
% Lastly, for each LLM, we select the best-performing strategy between QA-CoT and QA-CoT (w/all), and then apply reflection prompting on the QA outputs (QA-CoT+Reflect) \jw{need more motivation and explanation how Reflect is added with QA-COT}.

% \jw{Where did you introduce Metrics? You should say how do you define Macro/Micro and \#utt somewhere here?}


\section{Results and Analysis}




\begin{table*}
\centering
\small
\begin{tabular}{l|rrr|rrr|rrr|rrr} \hline
& \multicolumn{3}{c|}{GPT-4o} & \multicolumn{3}{c|}{gemini}  & \multicolumn{3}{c|}{opus}  & \multicolumn{3}{c}{sonnet} \\ \hline
% & \multicolumn{2}{c}{End-to-End} & QA & \multicolumn{2}{c}{End-to-End} & QA & \multicolumn{2}{c}{End-to-End} & QA & \multicolumn{2}{c}{End-to-End} & QA \\
Method    & Macro  & Micro  &  \#utt   & Macro  & Micro  &  \#utt  & Macro  & Micro & \#utt    & Macro  & Micro  &   \#utt       \\ \hline
 \multicolumn{13}{c}{ABCD} \\ \hline
Basic & 46.74 &	44.96 & 8.69 & 42.80	& 36.51	& 7.89 & 45.15 & 40.82 & 9.99 & 45.68 & 41.0 & 8.86 \\
Reflect  & 44.43 & 42.09 & 8.51  & 37.14 & 35.61 & 8.51 & 50.89 & 45.86 & 10.47 & 51.80  & 49.46 & 10.57 \\ %\hline
Plan & 37.29 & 38.30 & 7.71 & 41.0 & 38.12 & 6.75 & 35.77 & 32.91 & 7.90 & 37.56 & 34.71 & 7.39  \\
 % Universal & 40.96 & 41.18 & 8.32 & 38.81 & 30.57 & 7.20 & 31.20 & 25.53 & 7.67 & 44.40 & 41.72 & 9.69\\ 
Ensemble & 46.13 & 45.32 & 8.66 & 38.71 & 38.67 & 8.42 & 41.80 & 34.35 & 8.51 & 46.47 &	42.44 &	9.29 \\ \hline
QA-CoT  &  \textbf{58.55} & \textbf{56.29} & 10.60  & \textbf{48.09} & \textbf{46.40}  & 8.40  & \textbf{52.36} &	\textbf{55.75} &	11.92 &  \textbf{56.28} & \textbf{53.44} & 11.49 \\
  % \ \ \ w/all & 50.16 & 49.46 & 10.03 & 46.58 & 46.22 & 8.92 & \textbf{59.83} & \textbf{61.51} & 11.97 & \textbf{58.25} & \textbf{54.85} & 10.24 \\
 + Reflect &  55.45 & 55.39 & 10.73 & 43.10  & 46.22 & 8.36 & 56.84 & 56.65 & 11.54 & 54.44 &	54.67 &	10.56 \\ 
 % Universal & 52.76 & 48.56 & 9.02 & \textbf{49.72} & 46.04 & 8.77 & 49.83 & 46.22 & 10.93 & 43.26 & 42.45 & 9.75 \\ 
 \hline
 \multicolumn{13}{c}{SynthABCD} \\ \hline
Basic & 68.91 & 61.95 & 8.71  & 74.73 & 75.65 & 8.59  & 91.43 & 91.52 & 12.29 & 51.23 & 45.43 & 7.49  \\
Reflect  & 57.42	& 52.61	& 8.24  & 61.90  & 60.43 & 9.08  & \textbf{91.85} & \textbf{92.82} & 12.47 & 49.14 &	41.08 &	7.68  \\ %\hline
Plan & 78.40 & 68.91 & 8.55 & 72.77 & 70.65 & 7.97 & 74.85 & 70.21 & 9.07 & 73.02 & 70.65 & 9.32 \\
% Universal & 71.64 & 64.13 & 8.87 & 85.08 & 83.26 & 9.28 & 56.15 & 48.69 & 7.99 & 61.23 & 58.47 & 8.65 \\
Ensemble & 84.31 & 81.30 & 9.62 & 81.13 & 79.78 & 10.41 & 78.70 & 73.26 & 9.21 & 55.97 & 45.43 & 7.99 \\ \hline
QA-CoT  & \textbf{86.53} & \textbf{86.08} & 11.96 & \textbf{84.04} & \textbf{81.96} & 9.70 & 89.73 & 90.21 & 12.27 & \textbf{88.38} & \textbf{86.73} & 10.46 \\
 + Reflect & \textbf{87.87} & \textbf{89.13} & 12.28 & 76.59 & 73.91 & 9.89  & 87.12 & 85.21 & 11.83 & 82.59 & 85.65 & 11.08 \\ 
 % Universal & \textbf{88.66} & \textbf{89.56} & 11.72 & 74.93 & 71.08 & 9.80 & 91.41 & 92.82 & 11.49 & 80.90 & 80.43 & 10.26 \\ 
 \hline
\end{tabular}
\caption{Performance comparison of different workflow extraction systems.} \label{tab:generator-e2e}
\end{table*}

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\hline
 & Macro  & Micro  &  \#utt \\ \hline
Ground-truth Workflow    & 96.93 & 96.97 &  10.58  \\ \hline
\end{tabular}
\caption{Performance when ground-truth workflows are used to simulate the system bot.}
\label{tab:evaluation-gold-e2e}
\end{table}

\subsection{Conversations Retrieval} \label{result:retriever}
% In \cref{fig:retriever}, we compare our similarity (Cos Similar) based conversation selection strategy with random and diversity (Cos Diverse) based conversation selection strategies. To enable diversity-based selection, we similarly rely on embeddings of procedural elements rather than entire conversations. First, we remove noisy conversations by filtering out those in the bottom percentiles based on their cosine similarity to the centroid of procedural elements. From the remaining conversations, we select the conversation least similar to the centroid as a starting point. Subsequently, we iteratively select additional conversations that are least similar to the centroid of the previously selected procedural elements. 
For each conversation retrieval approach, we select subsets containing 25, 50, 75, 100, and all conversations and prompt four LLMs (GPT-4o, gemini-1.5-pro, opus-3, and sonnet-3.5) to extract workflows. The performance of each strategy is evaluated using arithmetic macro accuracy (solid lines) and micro accuracy (dashed lines), as shown in Fig. \ref{fig:retriever}. % and \ref{fig:retriever-anthropic}.

Our results show that \textbf{selecting a subset of conversations consistently leads to better performance than using all available conversations}, highlighting that simply applying any LLM to the entire dataset is not the most effective approach. GPT-4o and Sonnet-3.5 achieve peak performance with 75 conversations, Opus-3 peaks at 25, and Gemini-1.5-Pro performs best with 100. %, emphasizing that carefully selecting a relevant subset of conversations is crucial for optimizing procedure extraction.

Among different retrieval strategies, \textbf{Proc-Sim is the most effective for three of the four LLMs, and it is better than Conv-Sim}, suggesting that relying on conversations is less effective than explicit procedural elements. 
Gemini-1.5-Pro is the only model that achieves the best performance under the random selection strategy, implying that it is less sensitive to conversation noise. % Can we claim that?}. % it's difficult to explain this, so let's keep it
In contrast, Proc-Div consistently yields worse results, indicating that prioritizing diversity introduces noise from real-world conversations. With a more diverse but noisy set of conversations, the model may miss the correct procedure and instead incorporate outlier or incorrect steps, leading to reduced performance.

% Similarly, the Conv-Sim method performs worse than Proc-Sim, suggesting that relying on conversations alone is less effective, as procedural details are often implicit. In contrast, Proc-Sim leverages explicit procedural elements, leading to significantly better performance. These results highlight the importance of extracting and utilizing procedural elements for selecting conversations.

To systematically evaluate different prompting techniques and LLMs for workflow extraction, we adopt the best-performing retrieval strategy and the number of conversations for each LLM{ separately (e.g., GPT-4o we use Proc-Sim with 75 samples). We use macro-accuracy for measuring performance, which accounts for differences in workflow complexity by treating each workflow equally, rather than skewing results toward those with more sub-flows. 


\subsection{Workflows Generation}
As shown in Table \ref{tab:generator-e2e}, all baselines—Reflection, Plan, and Ensemble—fail to consistently outperform basic prompting across LLMs. 
Notably, Reflection and Ensemble incorporate additional refinement steps to improve workflow extraction; however, these refinements do not consistently enhance performance and may introduce noise. Similarly, Reflection prompting on QA-CoT fails to improve performance, reinforcing that refinement steps may not always help workflow extraction.


On the other hand, \textbf{QA-CoT outperforms almost all baselines on both ABCD and SynthABCD datasets}, showing that augmented context with Guide and Implementer agents is helpful for workflow generation. On average, QA-CoT improves macro accuracy by 8.73\% on ABCD and 15.59\% on SynthABCD datasets. The most significant improvement is seen on SynthABCD with the Sonnet model, where QA-CoT boosts macro accuracy by 37.15\%. On ABCD, it achieves the highest gain of 11.81\% for GPT-4o model.
%On the SynthABCD dataset, QA-CoT gives the most significant improvement for the Sonnet model, boosting macro accuracy by 37.15. On the ABCD dataset, QA-CoT achieves the highest macro accuracy gain of 11.81\% for the GPT-4o model. 
These results highlight QA-CoT's effectiveness in enhancing workflow extraction across different LLMs.
There is one outlier: Opus model for SynthABCD, where reflect prompting performs better. Notably, Opus achieves near-optimal performance on SynthABCD, with a macro accuracy of 91.43\% in this ideal setting, while the upper bound, attained when the agent bot follows the ground-truth workflow, is 96.93\% (Table~\ref{tab:evaluation-gold-e2e}). Unlike predicted workflows, which may introduce errors, the ground-truth workflow guarantees strictly correct actions, representing the theoretical performance ceiling.
 %\jw{need some explanation of Table 2?}. 

% Reflection prompting, which incorporates self-generated feedback, does not consistently improve performance across LLMs and underperforms compared to basic prompting with both GPT-4o and Gemini. This suggests that the reflection step may cause the model to focus on missed patterns, capturing both irrelevant noise and useful scenarios, leading to decreased accuracy. We observe a similar trend with the ensemble approach, which performs better than basic prompting only for the Sonnet model but does not offer consistent improvements across other LLMs. Plan-based prompting performs the worst overall, consistently underperforming relative to basic prompting, indicating that explicitly formulating a plan through simple prompting before workflow extraction does not enhance procedural accuracy.
% Additionally, reflection prompting generates more utterances and introduces extra (irrelevant) steps into the workflow. 
% The ensemble approach generally performs similar to basic prompting.

We also find that all LLMs exhibit notably higher performance when using SynthABCD, showcasing the strong ability of LLMs to effectively extract workflows from error-free conversations. This further emphasizes the importance of developing accurate conversation selection strategies, which can substantially enhance performance.
%On the ABCD dataset, our proposed model consistently outperforms baseline methods in E2E evaluation. Among the models tested—GPT-4o and Gemini—the GPT-4o-based model achieves the highest overall performance on both datasets.
% Lastly, these results highlight that procedural tasks with real data remain challenging, as even the most advanced LLMs are able to handle only roughly half of the scenarios correctly.

Lastly, we observe that ground-truth workflows have an average of 10.58 utterances per conversation (Table~\ref{tab:evaluation-gold-e2e}), whereas model-predicted workflows tend to produce longer interactions despite lower accuracy. For instance, the best-performing predicted workflows, generated by Opus on SynthABCD, have an average of 1.89 utterances higher (\#utt of 12.47) but a macro accuracy 5.08 points lower than the ground-truth workflows. 

Our results also show a positive correlation between macro accuracy and utterance count, suggesting that higher-quality workflows lead to longer conversations. If a predicted workflow splits a correct step into multiple sequential steps, E2E evaluation still treats both as equivalent since the simulated conversation meets the success criteria. To distinguish such cases, we recommend using utterance count (\#utt) as a measure of efficiency, selecting the workflow that achieves the goal in fewer steps.

% we  observe that LLM-generated workflows tend to be less efficient, leading to a higher average number of utterances compared to the reference workflow (10.58).
% For example, the least-efficient Opus generates an average of 12.47 utterances, Sonnet produces 11.49, Gemini 10.41, and GPT-4o 12.28—all while performing much lower than the gold workflow. \jw{I'm not very persuaded by this claim. First, 10.58 is it average on ABCD or SynthABCD? I think the most insightful claim here is: From the results, you can basically see that Macro has positive correlation than #utt, that is, the better the workflow generator is, the longer the conversation is. Which is not too surprising because better workflow generator might cover more rules for user-customer to discuss. I think the only thing here is to show if we know the "ideal" #utt here which is 10.58, then our method here does not penalize those over-wordy or over-stepped workflow.} %This suggests that model-generated workflows introduce unnecessary steps, making them less streamlined and efficient.

\begin{table*}
\centering
\small
\begin{tabular}{l|rrr|rrr|rrr|rrr} \hline
& \multicolumn{3}{c|}{o1} & \multicolumn{3}{c|}{o1-mini}  & \multicolumn{3}{c|}{o3-mini}  & \multicolumn{3}{c}{DeepSeek R1} \\ \hline
Method    & Macro  & Micro  &  \#utt   & Macro  & Micro  &  \#utt  & Macro  & Micro & \#utt    & Macro  & Micro  &   \#utt       \\ \hline
 \multicolumn{13}{c}{ABCD} \\ \hline
Basic & 36.72 & \textbf{35.97} & 7.70 & 44.20 & 40.46 & 9.73 & 57.06 & 57.19 & 9.52 & 46.10 & 40.10 & 8.42 \\
QA-CoT  & \textbf{39.0} & 33.09 & 8.50 & \textbf{60.74} & \textbf{64.74} & 12.05 & \textbf{61.20} & \textbf{61.51} & 11.26 &  \textbf{47.24} & \textbf{44.06} & 7.89 \\
 \hline
 \multicolumn{13}{c}{SynthABCD} \\ \hline
Basic & 59.57 & 52.39 & 7.65 & 69.57 & 67.39 & 10.56 & 79.50 & 78.91 & 9.14 & 65.26 & 51.52 & 7.50  \\
QA-CoT  &  \textbf{80.60} & \textbf{76.95} & 9.62 & \textbf{87.16} & \textbf{87.17} & 12.43 & \textbf{89.49} & \textbf{90.21} & 10.31 & \textbf{66.95} & \textbf{55.65} & 8.41 \\
 \hline
\end{tabular}
\caption{End-to-End performance comparison of different reasoning LLMs.} \label{tab:reasoning-e2e}
\end{table*}

\subsection{Performance of Reasoning LLMs}
% \todo{which data, etc.}
In Table \ref{tab:reasoning-e2e}, we show performance of four reasoning LLMs—DeepSeek-R1, o1, o1-mini, and o3-mini—on both the ABCD and SynthABCD datasets. We use 75 conversations selected using Proc-Sim retriever.
Our results show that \textbf{simply prompting a reasoning LLM does not consistently improve performance}. Among the evaluated models, only o3-mini demonstrates a significant improvement over non-reasoning LLMs on ABCD, achieving performance levels close to QA-CoT prompting with non-reasoning models.
Furthermore, employing \textbf{QA-CoT with reasoning LLMs consistently improves performance over basic prompting across both datasets}. This finding highlights that explicit QA-based chain-of-thought prompting can even benefit reasoning LLMs in extracting dialog workflows.

To better understand the limitations in performance improvement, we analyze both the thinking process and final outputs of DeepSeek-R1 on the ABCD and SynthABCD datasets (Fig. \ref{deepseek-think-example}). Our analysis reveals that while the thinking tokens capture the general process, they lack fine-grained preconditions. For instance, instead of explicitly detailing the relationship between the number of days and membership type in determining return eligibility, the model generates a generic thinking such as ``determine return eligibility based on purchase date.'' This omission potentially leads to incomplete workflow extraction, as critical decision criteria—such as a 6-month return window for silver members versus a 30-day window for guest members (QA-CoT with GPT-4o includes these fine-grained details as shown in Fig.~\ref{qa-cot-example})—are not captured.
% With QA-CoT, we observe a slight performance improvement on both the ABCD and SynthABCD datasets despite generating similar thinking tokens, as the QA pairs explicitly reinforce fine-grained details that is otherwise overlooked.




% \begin{table}
% \centering
% \small
% \begin{tabular}{l|rrrr} \hline
%   & GPT-4o  & gemini  &  opus   & sonnet  \\ \hline
%  \multicolumn{5}{c}{ABCD} \\ \hline
% Basic &  65.11 & \textbf{65.96} & 57.76 & 62.28 \\
%  + Reflect  & \textbf{68.78} & 58.62 & \textbf{66.94} & 62.28 \\ \hline
% QA-CoT  &   60.02 & 64.12 & 56.77 & 68.36  \\
%   \ \ \ w/all &  62.01 & 65.81 & 63.27 & \textbf{69.63} \\
%  + Reflect &  60.02 & 59.32 & 61.58 & 64.40 \\ \hline
%  \multicolumn{5}{c}{SynthABCD} \\ \hline
% Basic &  82.51 & \textbf{81.52} & 85.46 & 85.22 \\
%  + Reflect  &  82.75 & 75.36 & \textbf{86.94} & 84.97   \\ \hline
% QA-CoT  & 83.49 & 79.80 & 82.26 & \textbf{88.17} \\
%  + Reflect & \textbf{86.45} & 78.32 & 82.01 & 82.26 \\ \hline
% \end{tabular}
% \caption{QA performance comparison of different procedure extraction system.} \label{tab:generator-qa}
% \end{table}




% All models demonstrate significantly higher performance across both E2E and QA evaluation frameworks with SynthABCD, highlighting the capabilities of LLMs to effectively extract procedures from compliant conversations. This suggests that developing more accurate conversation selection strategies can significantly improve performance. In \cref{sec:appendix-error-synthabcd}, we conduct an additional controlled study by synthesizing noisy conversations, revealing that using a limited number of error-free conversations per scenario is the most effective strategy for enhancing procedural knowledge extraction.

% On the ABCD dataset, our proposed model consistently outperforms the baseline methods in E2E evaluation. Among our models—based on GPT-4o, Gemini, and Opus—the GPT-4o-based model achieves the highest overall performance. Reflection prompting, which uses self-generated feedback, improves QA evaluation performance but underperforms in E2E evaluation, even falling below basic prompting. This is likely because the reflection step causes the model to focus on missed patterns, which include both noise and useful scenarios from conversations. Additionally, we observe that while the reflection method has lower E2E accuracy, it generates more utterances than basic prompting, further indicating that it introduces extra (irrelevant) steps into the workflow. 

% These results highlight that procedural tasks with real data remain challenging, as even the most advanced LLMs are able to handle only roughly half of the scenarios correctly.


% \begin{table}
% \centering
% \small
% \begin{tabular}{l|rrrr} \hline
% & \multicolumn{3}{c}{End-to-End} & QA \\
% Method    & Macro  & Micro  & \#utt. &     Micro        \\ \hline
% \textit{Random} (25)  & 33.55	& 30.03	& 8.13	& 65.25 \\
% \textit{Random} (50) & 34.89	& 31.29	& 8.46	& 62.57 \\
% \textit{Random} (100) & 34.08	& 31.29	& 8.47	& 59.03 \\
% \textit{Conversation Sim.}  & 28.83  & 24.78  & 7.00          & 59.60       \\
% \textit{Our (Proposed)}      & \textbf{47.17}  & \textbf{47.12}  & 8.68          & 61.01       \\ 
% \textit{LLM-Select}      & 37.53  & 32.82  & 7.86          & \textbf{66.80}       \\\hline
% \end{tabular}
% \caption{Performance comparison of different conversation selection strategy. Both conversation sim. and llm-select methods use 50 conversations. All methods use the basic prompting model to extract workflows.} \label{table:retriever}
% \end{table}

% \subsection{Conversation Selection Strategy}
% In \cref{table:retriever}, we compare our conversation selection strategy with three methods: 
% \begin{itemize}[leftmargin=*]
% \setlength\itemsep{0.01em}
%     \item \textit{Random}: We randomly select 25, 50, or 100 conversations for each intent. % without considering procedural relevance.
%     \item \textit{Conversation Sim.}: We follow the same iterative approach as our proposed method but uses embedding for entire conversations rather than key procedural elements like customer issues, slot values, and resolution steps.
%     \item \textit{LLM-Select}: We prompt GPT-4o to select representative conversations covering diverse customer scenarios. 
% \end{itemize}

% \begin{table*}
% \centering
% \small
% \begin{tabular}{c|c|rrrr} \hline
% \# Clean Samples & Noisy Samples & \multicolumn{3}{c}{End-to-End} & QA \\
%  &    & Macro  & Micro  & \#utt. &     Micro        \\ \hline
% 2 & $\times$ &  84.76	& 83.91	& 11.24	&  80.78 \\
% 5 & $\times$ &  81.66 & 75.43 & 10.40 & 75.12 \\
% 2 & $\checkmark$ & 77.59 & 74.13 & 10.37 & 78.57 \\
% 0 & $\checkmark$ & 61.81 & 57.17 & 10.42 & 65.27 \\
% \hline
% \end{tabular}
% \caption{Effect of noise on procedural knowledge extraction. The table compares the effects of clean and noisy synthetic conversations on performance in procedural knowledge extraction.} \label{table:synth-analysis-error}
% \end{table*}

% Selecting 25, 50, or 100 conversations randomly results in similar E2E accuracies, but QA accuracy decreases as the number of selected conversations increases. This indicates that simply adding more conversations is not an effective strategy for procedural extraction, as it can introduce noise and reduce the overall quality of the selected dataset.

 
% Conversation Sim. performs the worst across all metrics, highlighting that directly using conversations is not an effective strategy, as procedural details are often implicit and not explicitly stated. In contrast, our approach extracts key procedural elements (issue, slot-values, resolution steps), which explicitly represent procedures, resulting in significantly better performance.


% LLM-Select achieves the highest QA accuracy (66.80), as it leverages LLM prompts to identify representative conversations that cover all scenarios, allowing it to correctly capture a greater number of steps. However, our method performs best on E2E evaluation, achieving the highest accuracies. This is because our centroid-similarity approach focuses on selecting the most common procedures, which ensures better performance on E2E tasks by correctly identifying frequent steps, even though it sacrifices diversity and performs slightly worse on QA accuracy.


% \subsection{SynthABCD: Analyzing Effects of Noise}
% To further analyze the impact of conversational quality on workflow extraction, we extended the SynthABCD dataset by generating conversations with intentional errors using GPT-4o. These errors were introduced by prompting the model to simulate common agent mistakes, such as combining multiple steps incorrectly or reordering steps within a workflow. For comparison, we also varied the number of clean samples per scenario, generating datasets with either 2 or 5 clean conversations for each customer scenario.
% We show the evaluation results in \cref{table:synth-analysis-error}.

% Increasing the number of clean conversations per scenario from 2 to 5 led to a decline in model performance across both evaluation frameworks. This result indicates that introducing multiple samples from the same scenario creates information redundancy, which appears to negatively impact the procedure extraction process.

% Incorporating noisy samples into the dataset caused a significant drop in performance, with the most pronounced decline observed when the dataset consisted solely of noisy conversations.

% These findings highlight the importance of developing an effective conversation selection strategies for dialog workflow extraction from real-world interactions. Ensuring diversity and quality in selected conversations is essential for accurate workflow predictions.

\section{Error Analysis}
We manually reviewed all eight QA-CoT systems to find where they failed to handle all scenarios correctly (example in Fig. \ref{fig:appendix-error}). One common mistake was \textbf{confusing system-available information with details that should be requested from the user}. This issue was especially common in shipping-related tasks, where models wrongly assumed they should check the shipping status in the system instead of asking the customer. o1 and DeepSeek-R1 made this mistake in five intents, while o3-mini was the only model that avoided it. This error did not come from the conversation data but from the LLM’s built-in knowledge, as it appeared in both the ABCD and SynthABCD datasets.
Another common issue was \textbf{models tend to ignore alternative options}. Instead of allowing a choice, they often predicted only one option. For example, when a workflow required asking for either the ``full name or the account ID'', models typically picked just one (e.g., ask ``full name'') instead of keeping both possibilities.
A third major issue was \textbf{failing to follow conditional logic at branching points}. For example, a workflow might require asking for the membership level only if there is no system error. However, models often ignored this condition and predicted a fixed sequence, combining steps incorrectly instead of following the intended logic.

We also observed two other patterns that did not impact performance but affected efficiency. First, models often \textbf{merged multiple steps into a single step}, particularly in the early stages of workflows. For instance, a workflow might specify sequential steps such as verifying the account ID or full name first, followed by email ID, order ID, and username. Instead of keeping these as separate steps, models frequently combined them, which reduces the average number of utterances required to resolve an issue. Second, when the original workflow allowed for multiple conditions to proceed—such as purchase date within 90 days, original receipt, or original packaging—models tended to convert this flexibility into a strict sequence. For example, they predicted checking the purchase date first, then asking about the receipt if the date exceeded 90 days, and so on, rather than allowing any of the conditions to be met.
While this does not reduce accuracy, it negatively impacts efficiency by increasing the number of utterances compared to the ground-truth workflow. %We show an example of policy predicted by the GPT-4o model in Fig. \ref{fig:appendix-error}. \todo{replace with multiple examples}



% We manually analyzed the predicted workflows for 11 intents from 4 systems to identify common errors. %We provide full details of our human evaluation in \cref{sec:human-evaluation}.
% Our evaluation reveals several common errors in workflow extraction. One frequent issue is merging multiple steps into a single step, particularly in the early stages of workflows. For instance, a workflow may specify sequential steps such as verifying the account ID or full name first, followed by email ID, order ID, and username. However, the model often conflates these into a single step. 
% Another common error involves incorrectly handling alternative options, where the model predicts only one option instead of allowing a choice, such as `ask for full name or account ID.' At branching points, the model often fails to respect conditional logic, such as asking for membership level only if there is no system error, instead predicting a fixed sequence that combines conditions incorrectly. Another frequent mistake occurs when the original procedure allows taking the next step based on one of several conditions, such as `purchase date within 90 days,' `original receipt,' or `original packaging.' Instead of maintaining this flexibility, the predicted workflow often converts this into a sequential process. For example, the model predicts asking for the purchase date first; if it exceeds 90 days, it proceeds to ask about the receipt, and so on. Overall, all models generally extract most steps correctly, they struggle with maintaining the correct order, respecting conditional logic, and covering all possible procedural paths. 

\section{Conclusion}
We introduced a novel framework for extracting and evaluating dialog workflows from historical conversations. Our proposed QA-CoT prompting outperforms standard prompting strategies, achieving a 12.16\% improvement in average macro-accuracy. Furthermore, the strong alignment of our E2E evaluation with human assessments underscores the robustness and reliability of our evaluation framework. We believe that our work offers a robust foundation for structured workflow extraction and evaluation, and will encourage future research to develop more effective methods. % for automatically extracting service agent workflows.
% In this paper, we presented a novel framework for extracting and evaluating procedural knowledge from conversations. Using the ABCD dataset, we adapted its workflows and generated SynthABCD with LLMs, leveraging both datasets for procedural extraction tasks. To evaluate procedural knowledge extraction, we proposed two evaluation frameworks that align closely with human judgments. Additionally, we introduced a novel conversation-selection strategy and an agentic framework to enhance procedural knowledge extraction. Our proposed model consistently outperformed baseline methods.  

% Our analysis on both the ABCD and SynthABCD datasets emphasizes the critical role of selecting appropriate conversations, offering an effective approach to improving procedural knowledge extraction systems in future work.


\section{Limitations}
Our workflow extraction and evaluation methods are designed for Service AI agents. Although they work well for this domain, applying them to more complex domains may require some adjustments. Additionally, our conversation selection method groups conversations by intent. An extra step for intent classification may be needed when intent labels are unavailable. 

Our E2E evaluation aligns closely with the human judgment, making it a reliable way to assess the accuracy of workflows. However, for more complex workflows in other domains, human evaluation can provide additional insights to further refine the evaluation framework.

SynthABCD offers a controlled testing environment, ensuring consistent benchmarking. Although it does not capture the full complexity of real-world interactions, models that perform well on SynthABCD provide a good starting point. To improve robustness, we recommend additional testing on real-world datasets such as ABCD to ensure that the models perform well in diverse scenarios.
% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

% \clearpage
\appendix 


\begin{figure*}[t]
\tiny % Adjust font size to better fit within page width
\begin{mdframed}
\begin{minipage}{\linewidth} % Ensures content does not overflow
\begin{Verbatim}[commandchars=\\\{\}]
..
<think>
Okay, let's tackle this problem. The user wants me to identify the steps necessary to resolve a customer's issue based on the provided 
conversations. They specified that the steps should be essential, consolidated, and formatted as a numbered list with pre-conditions, 
required inputs, and specific conditions.

First, I need to go through all the conversations to understand the common steps agents take to resolve the return requests. Since there 
are 75 conversations, I'll look for patterns and repeated actions.

Looking at the conversations, the first thing agents usually do is ask for the customer's name or account ID to pull up their account.
Then, they request the username, email address, and order ID to validate the purchase. \textbf{Next, they check the membership level (guest,} 
\textbf{bronze, silver, gold) because the return policy varies based on that. For example, guests might have a shorter return window,}
\textbf{while gold members have unlimited returns.}

\textbf{Agents then verify the purchase date. If the purchase is beyond the standard return period for the customer's membership level, they}
\textbf{ask if the customer has the receipt or original packaging. If they do, the return is processed. Without the receipt, returns might}
\textbf{be denied, especially for guests or lower-tier members.}

After confirming eligibility, the agent collects the customer's address to generate a shipping label. Finally, they ask how the 
customer wants to process the return: by mail, in-store, or at a drop-off center.

I need to make sure all these steps are included, but without breaking them down by individual conversation. Also, pre-conditions 
like membership level affecting return eligibility and required inputs like receipt or address should be mentioned.

Let me list out the steps:

1. Collect customer's full name or account ID to access their account.
2. Obtain username, email address, and order ID to validate the purchase.
\textbf{3. Determine the customer's membership level (guest, bronze, silver, gold) to apply the appropriate return policy.}
\textbf{4. Verify the purchase date to check if it's within the allowed return period for their membership level.}
\textbf{5. If the purchase is outside the standard return period, check if the customer has the receipt or original packaging (pre-condition} 
\textbf{for returns beyond standard period).}
6. Collect the customer's shipping address to generate a return label.
7. Confirm the return method (mail, in-store, drop-off center) and provide instructions.

I should check if any steps are missing. For example, in some cases, if the customer doesn't have a receipt and it's past the return 
period, the agent can't process the return. That's covered in step 5. Also, the return method is always asked, so step 7 is necessary. 
Each step includes required inputs (like address) and conditions (membership level affecting return window).

Yes, this covers all the essential steps without unnecessary details. The list is consolidated and formatted as per the instructions.
</think>
..
\end{Verbatim}
\end{minipage}
\end{mdframed}
\caption{A snippet of <think> tokens generated by the DeepSeek-R1 model for the $return\_color$ intent using conversations from the ABCD data. Compared to the QA-CoT (Fig.~\ref{qa-cot-example}), think tokens lack fine-grained conditions based on membership level.}
\label{deepseek-think-example}
\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{latex/er1.png}
    \caption{Example showing errors in the extracted workflow. Instead of two steps `First, ask the customer for their Full Name or Account ID to pull up account.' and `After getting the Full Name or Account ID, ask the customer for their Email Address, Order ID, and Username to validate purchase.', it combines them into one and also exclude \textbf{Account ID}. It also merge ask `membership level' and `purchase date' while the ground-truth workflow requires asking membership level first and then asking `purchase date' or `original receipt' or `original packaging' or `address' depending on the membership level.}
    \label{fig:appendix-error}
\end{figure*}

\section{Human Evaluations of E2E Framework} \label{appendix:e2e-human}
We assess the effectiveness of our E2E framework using 18 distinct intents from the ABCD dataset\footnote{All our annotators hold graduate degrees in computer science with over 8 years of experience in NLP in industry or academia.}. First, we manually evaluate the automatic scenario construction process against ground-truth workflows (Step 1, Fig.~\ref{fig:eval1}). Across these 18 intents, humans identified 231 possible scenarios, while GPT-4o generated 230, with 224 found to be correct.
Next, we evaluate the mapping of scenarios to customer information, system information, and success criteria (Steps 2 and 3, Fig.~\ref{fig:eval1}). Among the 224 correct scenarios, GPT-4o successfully generated all required details in 219 cases. Together for all three steps of the process, GPT-4o achieved an overall accuracy of 94.81\% (219 out of 231 scenarios), highlighting its effectiveness in capturing and mapping essential workflow details for user and agent bots simulation.

Finally, to evaluate the LLM's ability to predict conversation success, two annotators manually labeled successful completions for 105 simulated conversations. The manual evaluation yielded a high inter-annotator agreement, with a Cohen’s $\kappa$ of 0.966. Additionally, the agreement between the LLM's evaluation and human judgments was strong, achieving a Cohen’s $\kappa$ of 0.922. These results prove the robustness of our E2E evaluation framework in evaluating workflows.



\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\hline
 & Macro  & Micro  &  \#utt \\ \hline
Multi-turn QA    & 52.50 & 49.28 &  9.28  \\ \hline
Single-turn QA    & \textbf{58.55} & \textbf{56.29} &  10.60  \\ \hline
\end{tabular}
\caption{Performance of multi-turn prompting and single-pass prompting for simulating the Guide-Implementer interaction. The single-pass approach achieves better accuracy.}
\label{tab:multi-step-vs-single-step-qa}
\end{table}


\section{Dataset} \label{sec:appendix-data}

\subsection{ABCD Dataset}
The original ABCD dataset includes 55 intents, but we only focus on 21 complex intents that involve multi-step workflows and discard the remaining 34, which include both non-procedural issues—such as FAQs about products, pricing, timings, membership, or features—and simpler, linear workflows. These 21 complex intents feature multiple sub-flows for handling different scenarios (e.g., different membership levels in Fig. \ref{fig:eval1}).



% \begin{figure*}[]
%     \centering
%     \includegraphics[width=0.99\linewidth]{latex/formatting.png}
%     \caption{Example of transforming instruction from the ABCD guidelines into atomic steps in our workflow.}
%     \label{fig:guideline-example}
% \end{figure*}


\subsection{SynthABCD Dataset} \label{appendix:synthabcd}
Among the 21 intents used in our E2E study, three workflows are duplicates of each other, and two others are also duplicates, resulting in a total of 18 distinct workflows. We focus on these 18 intents. To introduce variability, we create 50 random user profiles with varying attributes such as name, profession, and city of residence, generating two distinct synthetic conversations for each scenario.
The prompts used for converting workflows into scenarios and for simulating conversations are provided in Fig. \ref{prompt:synthabcd-e2e-step1} and Fig. \ref{prompt:synthabcd-conv}, respectively.

Our SynthABCD pipeline employs the same automatic scenario construction process as the E2E framework. Across the 18 intents, human annotation identified 231 possible scenarios, while the GPT-4o model generated 230 scenarios. Of these, 224 were correctly extracted, yielding an accuracy of 96.97\%. This high accuracy—with only one scenario missed and six incorrect extractions—demonstrates the effectiveness of our conversation synthesis process and its applicability to new workflows.
All intents used in our evaluation are reported in Table \ref{table:appendix-data-stats}.


\begin{table*}[t]
\centering
\small
\begin{tabular}{l|rccc} \hline
% & & \multicolumn{3}{c} {E2E Evaluation}   \\
Intent & Type & \#Secnarios & ABCD & SynthABCD  \\ \hline
  refund\_initiate &  simple & 1 & & \\
  refund\_update &  simple & 1 & & \\
  refund\_status &  simple & 1 &  & \\
  return\_stain & complex & 18 & $\checkmark$ & $\checkmark$   \\
  return\_size & complex & 18 & $\checkmark$ &    \\
  return\_color & complex & 18 & $\checkmark$ &    \\
  status\_mystery\_fee & complex & 10 & $\checkmark$ & $\checkmark$    \\
  status\_delivery\_time & complex & 10 & $\checkmark$ & $\checkmark$  \\
  status\_payment\_method & complex & 8 & $\checkmark$ & $\checkmark$   \\
  status\_quantity & complex  & 8 & $\checkmark$ & $\checkmark$   \\
  manage\_upgrade & complex  & 14 & $\checkmark$ & $\checkmark$   \\
  manage\_downgrade & complex & 26 & $\checkmark$ & $\checkmark$  \\
  manage\_create & complex &  26 &$\checkmark$ & $\checkmark$  \\
  manage\_cancel & complex & 26 &  $\checkmark$ & $\checkmark$ \\
  recover\_username &  simple & 1 & &  \\
  recover\_password &  simple & 1 & &  \\
  reset\_2fa &  simple & 1 & &  \\
  status\_service\_added & complex  & 10 & $\checkmark$ & $\checkmark$    \\
  status\_service\_removed & complex   & 8 & $\checkmark$ & $\checkmark$    \\
  status\_credit\_missing & complex   & 6 &  $\checkmark$ & $\checkmark$    \\
  manage\_change\_address &  simple & 1 & &  \\
  manage\_change\_name &  simple & 1  & &  \\
  manage\_change\_phone &  simple & 1  & & \\
  manage\_payment\_method &  simple & 1  & &  \\
  bad\_price\_competitor &  simple & 1  & & \\
  bad\_price\_yesterday &  simple & 1  & &   \\
  out\_of\_stock\_general &  simple  & 1 & &   \\
  out\_of\_stock\_one\_item &  simple  & 1 & &   \\
  promo\_code\_out\_of\_date & complex   & 12 & $\checkmark$ & $\checkmark$    \\
  promo\_code\_invalid & complex  & 12 & $\checkmark$ & \\
  mistimed\_billing\_already\_returned & complex  & 10 & $\checkmark$ & $\checkmark$   \\
  mistimed\_billing\_never\_bought & complex  & 10 & $\checkmark$ & $\checkmark$  \\
  missing &  simple  & 1 & & \\
  cost & complex  & 8 &  $\checkmark$ & $\checkmark$    \\
  status\_due\_amount &  simple  & 1&  &   \\
  status\_due\_date &  simple  & 1&  &   \\
  manage\_pay\_bill &  simple  & 1&  &  \\
  manage\_extension & complex   & 8 & $\checkmark$ & $\checkmark$    \\
  manage\_dispute\_bill & complex   & 12 & $\checkmark$ & $\checkmark$  \\
  credit\_card &  simple  & 1& &  \\
  shopping\_cart &  simple   & 1& &   \\
  search\_results &  simple   & 1& &   \\
  slow\_speed &  simple   & 1& &  \\
  \hline
\end{tabular}
\caption{Intents used in E2E evaluation framework for the ABCD and SynthABCD datasets. return\_stain, return\_size and return\_color share the same workflow, as do promo\_code\_out\_of\_date and promo\_code\_invalid. Therefore, we only use one from each group in SynthABCD.} \label{table:appendix-data-stats}
\end{table*}


\subsection{Data Quality} \label{sec:data-compliance}
\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\hline
\textbf{Dataset} & \textbf{F} & \textbf{NA} & \textbf{NF} & \textbf{NC Cnv.}  \\ \hline
ABCD & 39.0\% & 57.07\% & 3.93\% & 28.57\% \\
SynthABCD & 43.56\% & 56.21\% & 0.22\% & 2.71\% \\ \hline
\end{tabular}
\caption{Data quality evaluation of ABCD and SynthABCD datasets. F: followed, NA: not applicable, NF: not followed, NC Cnv.: non-compliant conversation}
\label{tab:data-quality}
\end{table}

We evaluate the compliance of conversations in both the ABCD validation set and SynthABCD with their corresponding ground-truth workflows using the GPT-4o model (Prompt in Fig. \ref{prompt:compliance}). For each conversation, we assess whether each step in the workflow is followed, deemed not applicable, or not followed. Additionally, we calculate the percentage of non-compliant conversations, which refers to the conversations that fail to comply with at least one step in the ground-truth workflows. 

The data quality evaluation results in Table \ref{tab:data-quality} show key differences between the ABCD and SynthABCD datasets. In ABCD, 39.0\% of workflow steps are followed, compared to 43.56\% in SynthABCD.  Additionally, the percentage of unfollowed steps (NF) is significantly lower in SynthABCD (0.22\%) compared to ABCD (3.93\%), demonstrating the effectiveness of controlled conversation synthesis using the LLM. This reduction in noise aligns with SynthABCD’s objective of enhancing data consistency. Consequently, SynthABCD achieves 97.29\% compliant conversations, a substantial improvement over ABCD’s 71.43\%, reflecting its higher consistency.


\begin{figure}[]
    \centering
    \includegraphics[width=0.99\linewidth]{latex/eg.png}
    \caption{Illustration of End-to-End (E2E) accuracy evaluation. For this workflow, there are 10 scenarios based on membership level, system error, and whether the user can provide their full name or account ID. In the case of error B, none of the scenarios can be completed successfully, resulting in an accuracy of 0. For error A, only 50\% of scenarios where the user provides an account ID can be completed, yielding an accuracy of 0.5. For error C, 2 scenarios fail, including those for gold members with either account ID or full name, resulting in an accuracy of 0.8.}
    \label{fig:eval-e2e}
\end{figure}

\section{Alternative Evaluation Frameworks for Dialog Workflows} \label{sec:alternative-evaluation}
In addition to our primary End-to-End (E2E) evaluation, we explored several alternative evaluation frameworks to assess step-level correctness in workflow reconstruction. While these approaches provide fine-grained insights, they fail to capture the holistic effectiveness of a workflow and exhibit low agreement with human evaluations. Notably, methods that evaluate steps in isolation overlook dependencies across the entire process, potentially leading to an over- or under-estimation of practical performance. As illustrated in Fig.~\ref{fig:eval-e2e}, not all errors have the same impact on workflow completion—some may block all scenarios (e.g., error B with 0\% accuracy), while others only affect a subset of cases (e.g., error C reducing accuracy to 0.8). Step-level evaluation treats all errors as equal, failing to reflect their true effect on workflow execution. In contrast, E2E evaluation remains the most reliable metric, as it closely aligns with human judgments and directly measures real-world utility. Since workflows are ultimately assessed based on their ability to guide complete processes, we adopt E2E evaluation as our primary strategy.

\subsection{Evaluation Methods}

\subsubsection{Automatic QA Evaluator} 

We explored a QA-based evaluation framework that measures the accuracy of each step in a workflow by verifying its correct dependence on preconditions, such as the outcome of a previous step \cite{fabbri2021qafacteval}. For example, if the ground truth workflow includes a step to ask shipment status' followed by either ask membership level if the product has shipped' or `issue refund if the product has not shipped,' our goal is to verify that the predicted workflow reflects this conditional logic accurately.
To achieve this, we decompose ground-truth workflows into atomic questions, such as:
Q: What is the next step after asking shipment status if the product has not shipped? A: issue refund.
Q: What is the next step after asking shipment status if the product has shipped? A: ask membership level.
By structuring evaluation in this way, we ensure a step-wise correctness check while maintaining consistency. We use the GPT-4o model both for generating QA pairs from ground-truth workflows and for evaluating the predicted workflows. %This automation enables efficient scaling of the evaluation process. %The prompts for converting workflows to QA and evaluating QA accuracy are shown in \cref{prompt:worfklow-to-qa} and \ref{prompt:qa-evaluation}, respectively.


\subsubsection{Embedding Similarity}
Embedding similarity method quantifies structural and semantic similarity using cosine distance between workflow representations. Specifically, we represent both the ground-truth and predicted workflows as embedding vectors and define the similarity score as:  

\begin{equation}
S_{\text{embedding}} = \frac{1}{\text{cosine\_distance}(\mathbf{w}_{\text{ref}}, \mathbf{w}_{\text{pred}})}
\end{equation}

where \( \mathbf{w}_{\text{ref}} \) and \( \mathbf{w}_{\text{pred}} \) are the vector representations of the ground-truth and predicted workflows, respectively. A higher score indicates greater alignment between the two workflows.  
To obtain these embeddings, we use OpenAI text-embedding-3-small model.


\subsubsection{Edit Distance}
Edit-distance-based evaluation quantifies structural differences by computing the minimum number of operations required to transform the predicted workflow into the ground-truth workflow. We define three types of operations:  

\begin{itemize}
    \item \textbf{Insertion}: Adding a missing step to align with the ground-truth workflow.
    \item \textbf{Deletion}: Removing an extraneous step that does not appear in the ground-truth workflow.
    \item \textbf{Reordering}: Adjusting the sequence of steps to match the ground-truth workflow.
\end{itemize}  

To compute edit distance, we use the GPT-4o that aligns the predicted workflow to the ground-truth workflow using these operations. The final score is defined as:  

\begin{equation}
S_{\text{edit}} = \frac{1}{\text{number\_of\_edits}}
\end{equation}

where a higher score indicates fewer modifications needed for alignment, implying better prediction accuracy.  


\subsubsection{Step-Accuracy Evaluation}  
We use the GPT-4o model to label each step in the ground-truth workflow as either correctly or incorrectly covered with respect to the predicted workflow. The final score is computed as:  

\begin{equation}
S_{\text{step}} = \frac{\text{number\_of\_correct\_steps}}{\text{total\_number\_of\_steps}}
\end{equation}

where a higher score indicates that a greater proportion of steps in the ground-truth workflow are correctly captured by the predicted workflow.

\subsubsection{Likert-Scale Evaluation}  
We use the GPT-4o model to assign a similarity score between 1 and 100 based on structural and semantic alignment between predicted and ground-truth workflows. The score is computed as:  

\subsection{Human Evaluation Setup}
To evaluate the alignment of alternative evaluation protocols, we conduct a human study on workflows for 11 complex intents, each accompanied by 4 predicted workflows. The evaluation process began with decomposing each workflow into a series of QA pairs. For the gold ground-truth workflows, one author wrote the QA pairs, while a second author verified their quality. This dual-step validation ensures that the questions were clear and accurately represented the underlying steps and conditional logic inherent in the workflows. From the 11 workflows, we obtain a total of 141 QA pairs. Additionally, the 4 predicted workflows for each intent collectively yield 564 QA pairs, providing a comprehensive dataset for evaluation.

Next, two authors independently labeled the correctness of each QA pair based on the corresponding predicted workflow. This independent labeling process resulted in a Cohen's $\kappa$ score of 0.742, indicating substantial agreement between the annotators. 
For QA pairs labeled as correct and incorrect, we achieve an F1 score of 0.937 for correct labels, demonstrating high agreement on identifying correct steps, and 0.82 for incorrect labels, indicating slightly lower agreement in identifying errors. This further indicates that detecting errors by reasoning on previous steps and their outcomes is challenging, even for humans.


To assess the overall alignment between the predicted and ground-truth workflows, we calculate a final score for each predicted workflow based on the percentage of correctly answered questions. This score shows a correlation score of 0.835 between human annotators, indicating strong consistency in their evaluations. 
We further analyze correlations between step-level evaluation methods and human judgments, as reported in Table~\ref{tab:evaluator-correlation}. Among automated evaluation methods, the QA-based metric achieves the highest correlation (0.708) with human annotations, indicating its relative effectiveness in step-level evaluation. However, these correlation scores remain lower than those observed with End-to-End (E2E) evaluation, reinforcing our decision to prioritize E2E evaluation as the primary metric.  

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\hline
\textbf{Automatic Evaluation} & \textbf{Correlation} \\ \hline
embedding             & 0.412                \\
edit-distance               & 0.587                \\
step-accuracy              & 0.631                \\
likert-scale             & 0.678                \\
QA-based (Proposed)                 & \textbf{0.708 }               \\ \hline
\end{tabular}
\caption{Correlation between different automatic evaluation metrics and human judgements.}
\label{tab:evaluator-correlation}
\end{table}





\section{Prompts}


% \begin{figure*}[!h]
% \tiny
% \begin{mdframed}
% You are given a workflow describing a step-by-step process for resolving a customer issue. Your task is to transform the workflow into a set of question-answer pairs.

% STEP-BY-STEP INSTRUCTIONS TO CONVERT A WORKFLOW IN TO QA PAIRS

% 1. **Write a Question:** Convert each rule to one or more questions based on the preceding steps. Clearly mention pre-conditions in each question.

% 2. **Write its answer**: Answer each question based on the rules.

% 3. **Final Output**:

%    - Ensure the question-answer pairs flow logically and maintain the sequence of the original process. 
   
%    - Ensure the question-answer pairs comprehensively cover every step.
   
%    - Each question-answer pair should address only one key information in a step. Generate multiple questions for a single step if necessary.
   
%    - Provide the output in JSON format.

% \begin{verbatim}
% {
%     "QA_pairs": [list of question answer pairs formatted as {"question": "a question", "answer": "answer for the question"}]
% }
% \end{verbatim}


% Below are some examples:

% Example Workflow 1:

% \{rules\_1\}

% Example Question Answer pairs set 1:

% \{qa\_1\}

% Example Workflow 2:

% \{rules\_2\}

% Example Question Answer pairs set 2:

% \{qa\_2\}

% Example Workflow 3:

% \{rules\_3\}

% Example Question Answer pairs set 3:

% \{qa\_3\}

% Example Workflow 4:

% \{rules\_4\}

% Example Question Answer pairs set 4:

% \{qa\_4\}

% Example Workflow 5:

% \{rules\_5\}

% Example Question Answer pairs set 5:

% \{qa\_5\}

% Now, generate all question answer pairs for the following process. Remember to follow the json format.

% \{candidate\_rules\}
% \end{mdframed}
% \caption{The prompt for converting workflow to QA pairs [QA Extraction].} \label{prompt:worfklow-to-qa}
% \end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Extract intent, slot values and resolution steps from the customer service chat provided under <CONVERSATION> section below.

STEP-BY-STEP INSTRUCTIONS

1. Exclude any non-essential statements such as greetings, apologies, and expressions of gratitude.

2. Be concise, complete, and truthful to the facts mentioned in the conversation.

3. Include any all the numberical and categorical details such as personal names, addresses, phone numbers etc.

4. Follow the exact sequential order in which the agent took different actions in "resolution\_step".

5. Respond using the JSON format:

\begin{verbatim}
{
    "intent": "What is the customer issue? Use max 50 characters.",
    "slot_values": {
        "dictionary of slots and values collected from the customer or provided to the customer by the agent formatted as {slot: value}"
    },
    "resolution_steps": ["List of steps/ actions taken by the agent to resolve the customer's issue."]
}
\end{verbatim}

Example Conversation:

Customer: Hi, I'd like to return an item I purchased.

Agent: Hi! I’m happy to help with that. Could you please provide me with your order ID so I can pull up your account?

Customer: Sure, it’s 123456.

Agent: Thank you. Can you confirm the reason for your return?

Customer: The item arrived damaged.

Agent: I’m sorry to hear that. We’ll get this sorted for you right away. Would you prefer a replacement or a refund?

Customer: I’d like a refund, please.

Agent: Got it. I’ve initiated the return process for a refund. You’ll receive an email with a prepaid return label.Once we receive the item, we’ll process your refund within 3–5 business days. Can I help you with anything else?

Customer: No, that’s all. Thanks!

Agent: You’re welcome! If you need further assistance, feel free to reach out. Have a great day!

Example Output:
\begin{verbatim}
{
    "intent": "Customer wants to return a damaged item.",
    "slot_values": {
        "order_id": "123456",
        "return_reason": "damaged item",
        "refund_preference": "refund"
    },
    "resolution_steps": [
        "Agent asked for the order ID.",
        "Agent asked the reason for the return.",
        "Agent asked whether the customer wants a replacement or refund.",
        "Agent initiated the return process for a refund.",
        "Agent informed the customer about receiving a prepaid return label via email.",
        "Agent explained that the refund will be processed within 3–5 business days after receiving the item."
    ]
}
\end{verbatim}
\end{mdframed}
\caption{The prompt for extracting key elements: issue description, slots-values and resolution steps from ABCD conversations [Conversation Selection].} \label{prompt:conv-selection-procedure}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Identify all branching conditions in the provided policy. Do not include common steps. Generate one branch per line.

Example Policy 1:

\{policy\_1\}

Example Output 1:

\{output\_1\}

Example Policy 2:

\{policy\_2\}

Example Output 2:

\{output\_2\}

\end{mdframed}
\caption{The prompt for mapping workflows to scenarios [SynthABCD and E2E Evaluation].} \label{prompt:synthabcd-e2e-step1}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are given below a dialog policy that a customer service agent uses to resolve a customer's issue.

\#\# Policy

{policy}

Read the policy carefully and simulate a conversation between an user and an agent that follows the below sub-policy.

\{subflow\}

\# Instruction

- Generate conversation that strictly follow the provided sub-policy.

- Make sure that the conversation is started by user. 

User's details

Name: \{user\_name\}

Profession: \{user\_profession\}

Address: \{city\}

- Make sure that agent only asks the information in subflow or common steps in the dialog policy. Avoid any unnecessary information.

Example Policy 1:

\{example\_policy\}

Example Subflow 1:

\{example\_subflow\}

Example Output 1:

\{example\_conv\}

\end{mdframed}
\caption{The prompt for generating conversations given a workflow and a scenario [SynthABCD].} \label{prompt:synthabcd-conv}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are given a dialog workflow and a specific user scenario. Generate detailed list of user information, system information and the corresponding expected outcome for the given scenario based on the workflow. Respond following the below json format.

\begin{verbatim}
{
    "user information": ["detailed list of information that the user provide to the agent to complete the conversation in the given scenario"],
    "system information": ["detailed list of information that the agent check in system to complete the conversation in the given scenario"],
    "outcome": "expected outcome reflecting the resolution in the given scenario"
}
\end{verbatim}


Example Policy 1:

\{policy\_1\}

Example Scenario 1:

\{scenario\_1\}

Example Output 1:

\{output\_1\}

Example Policy 2:

\{policy\_2\}

Example Scenario 2:

\{scenario\_2\}

Example Output 2:

\{output\_2\}

Example Policy 3:

\{policy\_3\}

Example Scenario 3:

\{scenario\_3\}

Example Output 3:

\{output\_3\}

Example Policy 4:

\{policy\_4\}

Example Scenario 4:

\{scenario\_4\}

Example Output 4:

\{output\_4\}

\end{mdframed}
\caption{The prompt for extracting user information, system information and success criteria given a scenario and a workflow [E2E evaluation].} \label{prompt:e2e-step2}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are given a dialog policy and corresponding criteria for successful completion of conversation with a customer.

Given a conversation between an agent and a customer, check if the conversation ends successfully or not.

Dialog Policy:

\{policy\}

Success Criteria:

\{outcome\}

Conversation:

\{conv\}

Respond in json following the format below.

\begin{verbatim}
{
    "successful": "yes/no"
    "explanation": "explain how the conversation went?"
}
\end{verbatim}
\end{mdframed}
\caption{The prompt for evaluating successful completion of a conversation between the customer and service bot [E2E evaluation].} \label{prompt:e2e-success}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are a customer talking to an agent to have your issue resolved. You are given the issue description and the slots-values that you share with the agent. Respond to the agent and provide them them the requested information if possible.

STEP-BY-STEP INSTRUCTIONS:

- Identify the information requested by the agent.

- Check the provided issue description and the slots-values and identify if you can provide the requested information to the agent.

- If you can, respond to the agent with the requested information.

- If you don't have the requested information, respond that "you don't have the requested information and ask if there is any other information you can provide".

- Only provide the information that is requested by the user.

\#\# Issue:

\{issue\}

\#\# Slots-Values that you can provide to the agent:

{info}

Make up free form slot-values, e.g., user name, full name, order ID, account ID, email and address. 

\#\# Conversation History:

\{history\}
\end{mdframed}
\caption{The prompt for simulating a customer bot [E2E Evaluation].} \label{prompt:customer-bot-e2e}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are a customer service agent trying to solve a customer's issue. You are given the dialog policy, conversation history and the system information. Respond to the customer by strictly following the provided dialog policy.

STEP-BY-STEP INSTRUCTIONS:

- Read the provided conversation history and identify the current dialog state.

- Match dialog state with the dialog policy, and identify the next step to address the customer's issue.

- If the next step involves checking system information (e.g., checking system for an error), check the available system information and inform that to the customer. If the required system information is not available, reply "DONE".

- If the next step involves requesting information from customer (e.g., username), ask customer for the required information.

- If the next step involves taking some action (e.g., issuing refund), inform the customer that you have succesfully taken that action.

- Respond to the customer based on the identified next step.

- If the policy does not describe the next step based on the current dialog state, conclude the conversation by generating "DONE".

- If you can not take the next step for any reason, conclude the conversation by generating "DONE".

- If the issue has been successfully resolved, conclude the conversation by generating "DONE".

- Avoid repeating yourself or requesting information that has already been mentioned in the conversation history.

\#\# Dialog Policy

```

\{policy\}

```

\#\# System Information

\{info\}

\#\# Conversation History

\{history\}

\end{mdframed}
\caption{The prompt for simulating a service agent bot [E2E Evaluation].} \label{prompt:service-bot-e2e}
\end{figure*}


% \begin{figure*}[!h]
% \tiny
% \begin{mdframed}
% You are tasked with evaluating the correctness of a set of answers to questions based on a dialog workflow. Follow the step-by-step instructions below to ensure precise evaluation.


% STEP-BY-STEP INSTRUCTIONS

% 1. **Identify the relevant snippet**
    
%     For each question, locate the corresponding snippet in the workflow that satisfies all the conditions specified in the question. Ensure every condition in the question is checked to determine whether a matching snippet exists. **Examples**:
    
%         - If the question specifies a slot-value (e.g., gold membership or shipping status: in transit), locate the snippet addressing the next step specifically for that slot-value. If no such snippet exists, mark it as "No snippet found."
        
%         - If the question refers to the next step following a specific step (e.g., step A), ensure the workflow includes step A. If step A is absent, mark it as "No snippet found."
     
% 2. **Validate the answer**

%     After identifying a relevant snippet:
    
%        - Check for snippet inclusion: Ensure the snippet contains the answer provided.
       
%        - Determine correctness: determine whether the answer is correct or incorrect.
       
%            - First, confirm whether the snippet describes the type of next step specified in the answer (e.g., checking system info, requesting customer info, or taking an action).
           
%            - Then, verify whether the specific slots or actions mentioned in the answer align with those in the snippet.

% 3. **Final label**

%     Assign the final label:
    
%        - If no matching snippet is found, label the answer as "incorrect."
       
%        - If both the next step type and slots/actions match between the snippet and the answer, label the answer as "correct."
       
%        - If either the next step type or slots/actions differ, label the answer as "incorrect."
       
% Use this process to evaluate each question-answer pair systematically and consistently. Respond in **json format**. For each question-answer pair, output the snippet from the workflow that answers the question, reasoning and the label following the below format.

% \begin{verbatim}
% {
%     "question id": {
%         "snippet": "text snippet from the workflow that answers the question",
%         "reasoning": "explain your reason why the answer is correct or incorrect",
%         "label": "correct/ incorrect label"
%     }
% }  
% \end{verbatim}
% \end{mdframed}
% \caption{The prompt for QA-based evaluation [QA Evaluation].} \label{prompt:qa-evaluation}
% \end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are a quality assurance manager tasked to assess whether an agent followed the established agent guidelines for resolving a customer's issue. The guidelines offer detailed, rule-based instructions for agents to be followed in a step-by-step manner. However, agents may not consistently adhere to these instructions. Your task is to evaluate the agent's compliance by comparing the steps taken in the provided conversation against the established guidelines.

**Important: Approach your task step-by-step. Carefully evaluate each rule in the guidelines.**

\#\# Step-by-Step Instruction

- For each rule in guidelines, determine whether that rule is applicable to conversation or not.

  - Some rules are context-specific, they define actions contingent on preceding action outcomes, making them applicable only in specific conversational situations.
  
  - Other rules are universally applicable, dictating actions that consistently follow a preceding action, regardless of the outcome.

- If the rule is applicable, check whether the agent followed the prescribed steps accurately or not.

  - Evaluate the accuracy of both the action, and the order of the action.

- Here are some rule for handling specific scenarios

  - If the customer has already provided certain details (such as Full Name, Account ID, refund amount, item details, etc.) either voluntarily or as requested by the agent in a prior step:
  
    - Do not penalize the agent for not following guidelines by not requesting the same details again. 
    
    - Do not penalize the agent for not following guidelines even if the agent request for the same details again.
    
  - If a rule includes multiple actions, the order of those actions is irrelevant and should not be penalized. For instance, [ask 'Email Address', 'Order ID', and 'Username'] or  [ask 'Order ID', 'Email Address', and 'Username'] are equivalent.

- Generate output in the JSON format given below, offering both the response and an explanation for each rule.

\begin{verbatim}
{
    "Rule_1":{
    "response": 'followed'/ 'not applicable'/ 'not followed',
    "explanation": ''
    },
    "Rule_2":{
    "response": 'followed'/ 'not applicable'/ 'not followed',
    "explanation": ''
    },
    ..
}
\end{verbatim}

\end{mdframed}
\caption{The prompt for conversation's compliance evaluation given the ground-truth workflow [Data Quality].} \label{prompt:compliance}
\end{figure*}



% \begin{figure*}[!h]
% \tiny
% \begin{mdframed}
% You are a guide agent. Ask guiding questions to build the dialog workflow from provided conversations.

% Some example of guiding questions:

% 1. What is the next step after the customer has provided email address and phone number?

% 2. What is the next step if the customer complains about low fuel in the car?

% Output "DONE" if you don't have any new question.

% Below are all the known question answer (QA) pairs. Only ask questions that are not covered by the below QA pairs. Only generate one question at a time.

% \end{mdframed}
% \caption{The prompt for Guide Agent [Procedure Extraction].} \label{prompt:guide-agent}
% \end{figure*}


% \begin{figure*}[!h]
% \tiny
% \begin{mdframed}
% You are an implementer agent. Answer the question asked by the Guide agent by looking at the provided conversations. Only answer the question asked by the guide agent.
% \end{mdframed}
% \caption{The prompt for Implementer Agent [Procedure Extraction].} \label{prompt:implementer-agent}
% \end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
You are a QA simulator consisting of two agents: a Guide and an Implementer. Both agents are tasked with collaboratively reconstructing the process of resolving customer issues by analyzing historical conversations.

Below are the profiles for Guide and Implementer:

\#\#\# Guide:

- Asks targeted questions to understand the steps taken in a given scenario.

- Focuses on clarifying the preconditions, decision points, and the logic behind the steps.

- Ensures that all possible customer contexts and edge cases are addressed.

\#\#\# Implementer:

- Review past conversations to answer the questions.

- Provides detailed explanations about the actions taken at each step and their rationale.

\#\#\# Below is an example discussion between a guide and an implementer agent:

Guide: What is the first step an agent should take when booking a restaurant?

Implementer: ask name and phone number

Guide: What is the next step after asking the name and phone number? 

Implementer: ask preferred date and time for the reservation

Guide: What is the next step after asking the preferred date and time for the reservation? 

Implementer: ask party size

Guide: What is the next step after asking the party size? 

Implementer: ask for any special requests

Guide: What is the next step after asking about any special request? 

Implementer: check availability

Guide: What is the next step if restaurant is booked at requested date and time? 

Implementer: offer alternative time and date

Guide: What is the next step if restaurant is available at requested date and time?

Implementer: book and inform the customer

Let's think step-by-step and generate a discussion between the Guide and the Implementer based on below conversations.
\end{mdframed}
\caption{The prompt for generating question-answer-based chain-of-thoughts [Workflow Extraction: QA-CoT].} \label{prompt:qa-cot}
\end{figure*}

\begin{figure*}[!h]
\tiny
\begin{mdframed}
Identify the steps necessary to resolve the customer's issue based on the provided conversations and the discussion between Guide and Implementer Agents.

- Include only the essential actions needed for issue resolution, excluding unnecessary steps.

- Use the discussion between Guide and Implementer Agents to identify important steps in the workflow.

- Create a unified, consolidated list of steps without breaking them down by individual conversation.

- Formatting Instructions:

    1. Use simple and concise language for each step, mentioning any pre-conditions where applicable.
    
    2. Organize the steps in a numbered list for clarity.
    
    3. Include relevant details, such as required inputs and specific conditions, for each step.
\end{mdframed}
\caption{The prompt for extracting workflows given historical conversations and the discussion between the guide and implementer agents [Workflow Extraction: QA-CoT].} \label{prompt:procedural-extraction}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Identify the steps necessary to resolve the customer's issue based on the provided conversations.

- Include only the essential actions needed for issue resolution, excluding unnecessary steps.

- Create a unified, consolidated list of steps without breaking them down by individual conversation.

- Formatting Instructions:

    1. Use simple and concise language for each step, mentioning any pre-conditions where applicable.
    
    2. Organize the steps in a numbered list for clarity.
    
    3. Include relevant details, such as required inputs and specific conditions, for each step.
\end{mdframed}
\caption{The prompt for basic prompting strategy. [Workflow Extraction Baseline: Basic].} \label{prompt:procedural-extraction-basic}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Reflect on the following past chats and assess how well the dialog workflow covers the different scenarios mentioned. Your goal is to focus on the most representative scenarios discussed in the chats, excluding outlier conversations where agent may have taken wrong steps. For each valid scenario, consider whether the workflow addresses it comprehensively, and if not, identify the gaps.
        
Evaluate Coverage Gaps

    - Where does the workflow fail to provide guidance or actions for specific scenarios presented in the past chats?
    
    - Are there any special cases (e.g., different membership tiers, exceptions, or advanced user queries) that the workflow misses entirely?
    
    - Ignore outlier conversations where agent actions are inconsistent with most other conversations from the same scenario.
    
    - For each gap identified, explain how the current workflow might be extended or modified to address these uncovered cases.
    
Suggestions for Enhancing Coverage

    - What additions or modifications could be made to the workflow to cover the uncovered scenarios more effectively?

\end{mdframed}
\caption{The prompt for identifying coverage gap in a workflow given historical conversations. [Workflow Extraction Baseline: Reflect].} \label{prompt:procedural-extraction-reflect}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Improve the below dialog workflow based on the provided conversations and the LLM feedback.

- Include only the essential actions needed for issue resolution, excluding unnecessary steps.

- Create a unified, consolidated list of steps without breaking them down by individual conversation.

- Ignore feedback that is specific to an individual customer and does not address a general dialog scenario.

- Formatting Instructions:

    1. Use simple and concise language for each step, mentioning any pre-conditions where applicable.
    
    2. Organize the steps in a numbered list for clarity.
    
    3. Include relevant details, such as required inputs and specific conditions, for each step.
\end{mdframed}
\caption{The prompt for improving coverage given conversations and LLM-feedback. [Workflow Extraction Baseline: Reflect].} \label{prompt:procedural-extraction-reflect-generate}
\end{figure*}

\begin{figure*}[!h]
\tiny
\begin{mdframed}
Your task is to identify the steps necessary to resolve the customer's issue based on the provided conversations. 

\{convs\}

Let's first understand the problem and devise a plan to solve the problem.
\end{mdframed}
\caption{The prompt for generating plan from historical conversations. [Workflow Extraction Baseline: Plan].} \label{prompt:procedural-extraction-plan}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Identify the steps necessary to resolve the customer's issue based on the provided conversations and the plan.

- Include only the essential actions needed for issue resolution, excluding unnecessary steps.

- Use the plan to identify important steps in the workflow.

- Create a unified, consolidated list of steps without breaking them down by individual conversation.

- Formatting Instructions:

    1. Use simple and concise language for each step, mentioning any pre-conditions where applicable.
    
    2. Organize the steps in a numbered list for clarity.
    
    3. Include relevant details, such as required inputs and specific conditions, for each step.
    
- Only generate the final guideline, outlining important steps required for solving customer's issue.
\end{mdframed}
\caption{The prompt for generating workflows from plan and historical conversations. [Workflow Extraction Baseline: Plan].} \label{prompt:procedural-extraction-plan-generating}
\end{figure*}


\begin{figure*}[!h]
\tiny
\begin{mdframed}
Aggregate and improve upon the following four dialog workflows used to resolve the customer's issue. 

Workflow 1:

\{wf1\}

Workflow 2:

\{wf2\}

Workflow 3:

\{wf3\}

Workflow 4:

\{wf4\}

Use the provided example conversations to decide which of the mentioned workflows is correct at each step or to identify any steps that are missing from all workflows.

\{convs\}

STEP-BY-STEP INSTRUCTIONS:

- Identify all the steps necessary to resolve the customer's issue.

- Include only the essential actions needed for issue resolution, excluding unnecessary steps.

- Create a unified, consolidated list of steps without breaking them down by individual conversation.

- Formatting Instructions:

    1. Use simple and concise language for each step, mentioning any pre-conditions where applicable.
    
    2. Organize the steps in a numbered list for clarity.
    
    3. Include relevant details, such as required inputs and specific conditions, for each step.
\end{mdframed}
\caption{The prompt for ensemble prompting strategy. [Workflow Extraction Baseline: Ensemble].} \label{prompt:procedural-extraction-ensemble}
\end{figure*}



\begin{figure*}[!h]
\tiny
\begin{mdframed}
Correct the given LLM-generated discussion between two collaborative agents, the Guide and the Implementer. 

Reflect on the given discussion:

    - Evaluate whether the discussion cover all the steps comprehensively.
    

    
    - Look for discussion parts that may belong to outlier conversations (e.g., rare scenarios or edge cases) and determine if they should be part of the main workflow.

Correct and Complete:

    - Add any missing discussions to bridge gaps and ensure logical continuity.
    
    - Reorganize or exclude discussion about steps in outlier conversations if they do not fit the main workflow.
    
    - Clearly specify the conditions under which any alternative steps or edge cases arise.
    
Provide the corrected and complete discussion following the format of original discussion between the guide and implementer agents. Do not add any justifications.
\end{mdframed}
\caption{The prompt used for refining the QA chain-of-thought [Workflow Extraction Baseline: QA-CoT+Reflect].} \label{prompt:procedural-extraction-qa-reflect}
\end{figure*}


% 

\end{document}
