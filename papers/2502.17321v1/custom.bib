@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{fabbri2021qafacteval,
  title={QAFactEval: Improved QA-based factual consistency evaluation for summarization},
  author={Fabbri, Alexander R and Wu, Chien-Sheng and Liu, Wenhao and Xiong, Caiming},
  journal={arXiv preprint arXiv:2112.08542},
  year={2021}
}

@article{dannenfelser2024into,
  title={Into the single cell multiverse: an end-to-end dataset for procedural knowledge extraction in biomedical texts},
  author={Dannenfelser, Ruth and Zhong, Jeffrey and Zhang, Ran and Yao, Vicky},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{agarwal2020extracting,
  title={Extracting procedural knowledge from technical documents},
  author={Agarwal, Shivali and Atreja, Shubham and Agarwal, Vikas},
  journal={arXiv preprint arXiv:2010.10156},
  year={2020}
}

@article{dunn2022structured,
  title={Structured information extraction from complex scientific text with fine-tuned large language models},
  author={Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
  journal={arXiv preprint arXiv:2212.05238},
  year={2022}
}

@inproceedings{chen-etal-2021-action,
    title = "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems",
    author = "Chen, Derek  and
      Chen, Howard  and
      Yang, Yi  and
      Lin, Alexander  and
      Yu, Zhou",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.239",
    doi = "10.18653/v1/2021.naacl-main.239",
    pages = "3002--3017",
    abstract = "Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8{\%} absolute accuracy) still exists to reach human-level performance on ABCD.",
}

@inproceedings{zhao-etal-2023-anytod,
    title = "{A}ny{TOD}: A Programmable Task-Oriented Dialog System",
    author = "Zhao, Jeffrey  and
      Cao, Yuan  and
      Gupta, Raghav  and
      Lee, Harrison  and
      Rastogi, Abhinav  and
      Wang, Mingqiu  and
      Soltau, Hagen  and
      Shafran, Izhak  and
      Wu, Yonghui",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1006/",
    doi = "10.18653/v1/2023.emnlp-main.1006",
    pages = "16189--16204",
    abstract = "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models."
}

@inproceedings{mehri-eskenazi-2021-schema,
    title = "Schema-Guided Paradigm for Zero-Shot Dialog",
    author = "Mehri, Shikib  and
      Eskenazi, Maxine",
    editor = "Li, Haizhou  and
      Levow, Gina-Anne  and
      Yu, Zhou  and
      Gupta, Chitralekha  and
      Sisman, Berrak  and
      Cai, Siqi  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Wu, Yan  and
      Li, Junyi Jessy",
    booktitle = "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    month = jul,
    year = "2021",
    address = "Singapore and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigdial-1.52/",
    doi = "10.18653/v1/2021.sigdial-1.52",
    pages = "499--508",
    abstract = "Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in dialog research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in dialog. Ablation experiments are also presented to demonstrate the efficacy of SAM."
}

@article{brown2023automate,
  title = {Automate This! — Power Up Your Einstein Bot with Flow},
  author = {Kristi Brown},
  year = {2023},
  month = {June},
  url = {https://admin.salesforce.com/blog/2023/automate-this-power-up-your-einstein-bot-with-flow}
}

@article{GONZALEZ1985293,
title = {Clustering to minimize the maximum intercluster distance},
journal = {Theoretical Computer Science},
volume = {38},
pages = {293-306},
year = {1985},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(85)90224-5},
url = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
author = {Teofilo F. Gonzalez},
keywords = {Algorithms, clustering, NP-completeness, approximation algorithms, minimizing the maximum intercluster distance},
abstract = {The problem of clustering a set of points so as to minimize the maximum intercluster distance is studied. An O(kn) approximation algorithm, where n is the number of points and k is the number of clusters, that guarantees solutions with an objective function value within two times the optimal solution value is presented. This approximation algorithm succeeds as long as the set of points satisfies the triangular inequality. We also show that our approximation algorithm is best possible, with respect to the approximation bound, if P ≠ NP.}
}

@misc{shinn2023reflexionlanguageagentsverbal,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2303.11366}, 
}

@misc{zhang2024chainagentslargelanguage,
      title={Chain of Agents: Large Language Models Collaborating on Long-Context Tasks}, 
      author={Yusen Zhang and Ruoxi Sun and Yanfei Chen and Tomas Pfister and Rui Zhang and Sercan Ö. Arik},
      year={2024},
      eprint={2406.02818},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02818}, 
}

@misc{musumeci2024llmbasedmultiagentgeneration,
      title={LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain}, 
      author={Emanuele Musumeci and Michele Brienza and Vincenzo Suriani and Daniele Nardi and Domenico Daniele Bloisi},
      year={2024},
      eprint={2402.14871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14871}, 
}

@misc{hu2025debatetowritepersonadrivenmultiagentframework,
      title={Debate-to-Write: A Persona-Driven Multi-Agent Framework for Diverse Argument Generation}, 
      author={Zhe Hu and Hou Pong Chan and Jing Li and Yu Yin},
      year={2025},
      eprint={2406.19643},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.19643}, 
}

@misc{du2023improvingfactualityreasoninglanguage,
      title={Improving Factuality and Reasoning in Language Models through Multiagent Debate}, 
      author={Yilun Du and Shuang Li and Antonio Torralba and Joshua B. Tenenbaum and Igor Mordatch},
      year={2023},
      eprint={2305.14325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14325}, 
}

@inproceedings{wang-etal-2023-plan,
    title = "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
    author = "Wang, Lei  and
      Xu, Wanyu  and
      Lan, Yihuai  and
      Hu, Zhiqiang  and
      Lan, Yunshi  and
      Lee, Roy Ka-Wei  and
      Lim, Ee-Peng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.147/",
    doi = "10.18653/v1/2023.acl-long.147",
    pages = "2609--2634",
    abstract = "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with {\textquotedblleft}\textit{Let`s think step by step}{\textquotedblright} as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at \url{https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting}."
}

@misc{chen2023universalselfconsistencylargelanguage,
      title={Universal Self-Consistency for Large Language Model Generation}, 
      author={Xinyun Chen and Renat Aksitov and Uri Alon and Jie Ren and Kefan Xiao and Pengcheng Yin and Sushant Prakash and Charles Sutton and Xuezhi Wang and Denny Zhou},
      year={2023},
      eprint={2311.17311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.17311}, 
}

@article{mosig2020star,
  	   author = {Johannes E. M. Mosig and Shikib Mehri and Thomas Kober},
        title = "{STAR: A Schema-Guided Dialog Dataset for Transfer Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.11853},
archivePrefix = {arXiv},
       eprint = {2010.11853},
 primaryClass = {cs.CL},
}

@article{BOHUS2009332,
    title = {The RavenClaw dialog management framework: Architecture and systems},
    journal = {Computer Speech \& Language},
    volume = {23},
    number = {3},
    pages = {332-361},
    year = {2009},
    issn = {0885-2308},
    doi = {https://doi.org/10.1016/j.csl.2008.10.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230808000545},
    author = {Dan Bohus and Alexander I. Rudnicky},
    keywords = {Dialog management, Spoken dialog systems, Error handling, Focus shifting, Mixed-initiative},
    abstract = {In this paper, we describe RavenClaw, a plan-based, task-independent dialog management framework. RavenClaw isolates the domain-specific aspects of the dialog control logic from domain-independent conversational skills, and in the process facilitates rapid development of mixed-initiative systems operating in complex, task-oriented domains. System developers can focus exclusively on describing the dialog task control logic, while a large number of domain-independent conversational skills such as error handling, timing and turn-taking are transparently supported and enforced by the RavenClaw dialog engine. To date, RavenClaw has been used to construct and deploy a large number of systems, spanning different domains and interaction styles, such as information access, guidance through procedures, command-and-control, medical diagnosis, etc. The framework has easily adapted to all of these domains, indicating a high degree of versatility and scalability.}
}

@misc{qiu2022structureextractiontaskorienteddialogues,
      title={Structure Extraction in Task-Oriented Dialogues with Slot Clustering}, 
      author={Liang Qiu and Chien-Sheng Wu and Wenhao Liu and Caiming Xiong},
      year={2022},
      eprint={2203.00073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.00073}, 
}

@misc{wang2021modellinghierarchicalstructuredialogue,
      title={Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System}, 
      author={Jianhong Wang and Yuan Zhang and Tae-Kyun Kim and Yunjie Gu},
      year={2021},
      eprint={2006.06814},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.06814}, 
}

@article{Nath2021TSCAND,
  title={TSCAN : Dialog Structure discovery using SCAN},
  author={Apurba Nath and Aayush Kubba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.06426},
  url={https://api.semanticscholar.org/CorpusID:235829653}
}

@misc{rony2022dialokgknowledgestructureawaretaskoriented,
      title={DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation}, 
      author={Md Rashad Al Hasan Rony and Ricardo Usbeck and Jens Lehmann},
      year={2022},
      eprint={2204.09149},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.09149}, 
}

@misc{lu2022unsupervisedlearninghierarchicalconversation,
      title={Unsupervised Learning of Hierarchical Conversation Structure}, 
      author={Bo-Ru Lu and Yushi Hu and Hao Cheng and Noah A. Smith and Mari Ostendorf},
      year={2022},
      eprint={2205.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.12244}, 
}
@misc{xu2020discoveringdialogstructuregraph,
      title={Discovering Dialog Structure Graph for Open-Domain Dialog Generation}, 
      author={Jun Xu and Zeyang Lei and Haifeng Wang and Zheng-Yu Niu and Hua Wu and Wanxiang Che and Ting Liu},
      year={2020},
      eprint={2012.15543},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2012.15543}, 
}

@misc{pryor2024usingdomainknowledgeguide,
      title={Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic}, 
      author={Connor Pryor and Quan Yuan and Jeremiah Liu and Mehran Kazemi and Deepak Ramachandran and Tania Bedrax-Weiss and Lise Getoor},
      year={2024},
      eprint={2403.17853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.17853}, 
}

@misc{chen2021dsbertunsuperviseddialoguestructurelearning,
      title={DSBERT:Unsupervised Dialogue Structure learning with BERT}, 
      author={Bingkun Chen and Shaobing Dai and Shenghua Zheng and Lei Liao and Yang Li},
      year={2021},
      eprint={2111.04933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.04933}, 
}

@inproceedings{Yin_2023, series={WWW ’23},
   title={CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation},
   url={http://dx.doi.org/10.1145/3543507.3583285},
   DOI={10.1145/3543507.3583285},
   booktitle={Proceedings of the ACM Web Conference 2023},
   publisher={ACM},
   author={Yin, Congchi and Li, Piji and Ren, Zhaochun},
   year={2023},
   month=apr, pages={1539–1550},
   collection={WWW ’23} }


@misc{shi2019unsuperviseddialogstructurelearning,
      title={Unsupervised Dialog Structure Learning}, 
      author={Weiyan Shi and Tiancheng Zhao and Zhou Yu},
      year={2019},
      eprint={1904.03736},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.03736}, 
}

@inproceedings{Chotimongkol2008LearningTS,
  title={Learning the Structure of Task-Oriented Conversations from the Corpus of In-Domain Dialogs},
  author={Ananlada Chotimongkol},
  year={2008},
  url={https://api.semanticscholar.org/CorpusID:2968702}
}

@inproceedings{Jurafsky1997SwitchboardSS,
  title={Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual},
  author={Dan Jurafsky and Elizabeth Shriberg},
  year={1997},
  url={https://api.semanticscholar.org/CorpusID:53899804}
}

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed H and Le, Quoc V and Zhou, Denny and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{laban-etal-2024-summary,
    title = "Summary of a Haystack: A Challenge to Long-Context {LLM}s and {RAG} Systems",
    author = "Laban, Philippe  and
      Fabbri, Alexander  and
      Xiong, Caiming  and
      Wu, Chien-Sheng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.552",
    doi = "10.18653/v1/2024.emnlp-main.552",
    pages = "9885--9903",
    abstract = "LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthesize Haystacks of documents, ensuring that specific insights repeat across documents. The {``}Summary of a Haystack{''} (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have precise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly reproducible automatic evaluation that can score summaries on two aspects {--} Coverage and Citation. We generate Haystacks in two domains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56{\%}) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20{\%} on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.",
}

@inproceedings{huang-etal-2024-embrace,
    title = "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
    author = "Huang, Kung-Hsiang  and
      Laban, Philippe  and
      Fabbri, Alexander  and
      Choubey, Prafulla Kumar  and
      Joty, Shafiq  and
      Xiong, Caiming  and
      Wu, Chien-Sheng",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.32",
    doi = "10.18653/v1/2024.naacl-long.32",
    pages = "570--593",
    abstract = "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40{\%} of the diverse information on average.",
}

@article{10.1145/1089815.1089817,
author = {Mooney, Raymond J. and Bunescu, Razvan},
title = {Mining knowledge from text using information extraction},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/1089815.1089817},
doi = {10.1145/1089815.1089817},
abstract = {An important approach to text mining involves the use of natural-language information extraction. Information extraction (IE) distills structured data or knowledge from unstructured text by identifying references to named entities as well as stated relationships between such entities. IE systems can be used to directly extricate abstract knowledge from a text corpus, or to extract concrete data from a set of documents which can then be further analyzed with traditional data-mining techniques to discover more general patterns. We discuss methods and implemented systems for both of these approaches and summarize results on mining real text corpora of biomedical abstracts, job announcements, and product descriptions. We also discuss challenges that arise when employing current information extraction technology to discover knowledge in text.},
journal = {SIGKDD Explor. Newsl.},
month = {jun},
pages = {3–10},
numpages = {8}
}

@misc{du2024dflowdiversedialogueflow,
      title={DFlow: Diverse Dialogue Flow Simulation with Large Language Models}, 
      author={Wanyu Du and Song Feng and James Gung and Lijia Sun and Yi Zhang and Saab Mansour and Yanjun Qi},
      year={2024},
      eprint={2410.14853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14853}, 
}

@article{doi:10.1177/001316446002000104,
author = {Jacob Cohen},
title ={A Coefficient of Agreement for Nominal Scales},
journal = {Educational and Psychological Measurement},
volume = {20},
number = {1},
pages = {37-46},
year = {1960},
doi = {10.1177/001316446002000104},
URL = { 
        https://doi.org/10.1177/001316446002000104
},
eprint = { 
        https://doi.org/10.1177/001316446002000104
}
}
@inproceedings{gallanti1985,
author = {Gallanti, M. and Guida, Giovanni and Spampinato, Luca and Stefanini, Alberto},
year = {1985},
month = {01},
pages = {345-352},
title = {Representing Procedural Knowledge in Expert Systems: An Application to Process Control.},
volume = {1},
journal = {IJCAI}
}
@article{Huang2022LanguageMA,
  title={Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author={Wenlong Huang and P. Abbeel and Deepak Pathak and Igor Mordatch},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07207},
  url={https://api.semanticscholar.org/CorpusID:246035276}
}

@misc{qi2023mastering,
      title={Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment}, 
      author={Ji Qi and Kaixuan Ji and Xiaozhi Wang and Jifan Yu and Kaisheng Zeng and Lei Hou and Juanzi Li and Bin Xu},
      year={2023},
      eprint={2310.10590},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{burdisso2024dialog2flow,
  title={Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction},
  author={Burdisso, Sergio and Madikeri, Srikanth and Motlicek, Petr},
  journal={arXiv preprint arXiv:2410.18481},
  year={2024}
}

\@article{Khraisha2023CanLL,
  title={Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages},
  author={Qusai Khraisha and Sophie Put and Johanna Kappenberg and Azza Warraitch and Kristin Hadfield},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.17526},
  url={https://api.semanticscholar.org/CorpusID:264490974}
}
@misc{wan2023gptre,
      title={GPT-RE: In-context Learning for Relation Extraction using Large Language Models}, 
      author={Zhen Wan and Fei Cheng and Zhuoyuan Mao and Qianying Liu and Haiyue Song and Jiwei Li and Sadao Kurohashi},
      year={2023},
      eprint={2305.02105},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2023large,
      title={Large Language Models for Generative Information Extraction: A Survey}, 
      author={Derong Xu and Wei Chen and Wenjun Peng and Chao Zhang and Tong Xu and Xiangyu Zhao and Xian Wu and Yefeng Zheng and Enhong Chen},
      year={2023},
      eprint={2312.17617},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@electronic{singhal2012introducing,
  added-at = {2022-11-26T03:41:36.000+0100},
  author = {Singhal, Amit},
  biburl = {https://www.bibsonomy.org/bibtex/23de8f9e39e3b0c5df9709b2548fbd189/ferdinandshi111},
  interhash = {091175fd22ba89545528f5182c81eb88},
  intrahash = {3de8f9e39e3b0c5df9709b2548fbd189},
  keywords = {Graph Knowledge},
  note = 2012,
  timestamp = {2022-11-26T03:41:36.000+0100},
  title = {Introducing the Knowledge Graph: things, not strings},
  url = {https://www.blog.google/products/search/introducing-knowledge-graph-things-not/},
  year = 2012
}

@book{10.5555/1214993,
author = {Jurafsky, Daniel and Martin, James H.},
title = {Speech and Language Processing (2nd Edition)},
year = {2009},
isbn = {0131873210},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@misc{robino2025conversationroutinespromptengineering,
      title={Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems}, 
      author={Giorgio Robino},
      year={2025},
      eprint={2501.11613},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.11613}, 
}

@inproceedings{He2021GALAXYAG,
  title={GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection},
  author={Wanwei He and Yinpei Dai and Yinhe Zheng and Yuchuan Wu and Zhen Cao and Dermot Liu and Peng Jiang and Min Yang and Feiling Huang and Luo Si and Jian Sun and Yongbin Li},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:244714676}
}

@misc{min2023workflowguided,
      title={Workflow-Guided Response Generation for Task-Oriented Dialogue}, 
      author={Do June Min and Paloma Sodhi and Ramya Ramakrishnan},
      year={2023},
      eprint={2311.08300},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raimondo2023improving,
      title={Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans}, 
      author={Stefania Raimondo and Christopher Pal and Xiaotian Liu and David Vazquez and Hector Palacios},
      year={2023},
      eprint={2306.01729},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hattami2023workflow,
      title={Workflow Discovery from Dialogues in the Low Data Regime}, 
      author={Amine El Hattami and Stefania Raimondo and Issam Laradji and David Vazquez and Pau Rodriguez and Chris Pal},
      year={2023},
      eprint={2205.11690},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{maeta-etal-2015-framework,
    title = "A Framework for Procedural Text Understanding",
    author = "Maeta, Hirokuni  and
      Sasada, Tetsuro  and
      Mori, Shinsuke",
    booktitle = "Proceedings of the 14th International Conference on Parsing Technologies",
    month = jul,
    year = "2015",
    address = "Bilbao, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-2206",
    doi = "10.18653/v1/W15-2206",
    pages = "50--60",
}

@inproceedings{ushiku-etal-2017-procedural,
    title = "Procedural Text Generation from an Execution Video",
    author = "Ushiku, Atsushi  and
      Hashimoto, Hayato  and
      Hashimoto, Atsushi  and
      Mori, Shinsuke",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1033",
    pages = "326--335",
    abstract = "In recent years, there has been a surge of interest in automatically describing images or videos in a natural language. These descriptions are useful for image/video search, etc. In this paper, we focus on procedure execution videos, in which a human makes or repairs something and propose a method for generating procedural texts from them. Since video/text pairs available are limited in size, the direct application of end-to-end deep learning is not feasible. Thus we propose to train Faster R-CNN network for object recognition and LSTM for text generation and combine them at run time. We took pairs of recipe and cooking video, generated a recipe from a video, and compared it with the original recipe. The experimental results showed that our method can produce a recipe as accurate as the state-of-the-art scene descriptions.",
}

@article{Chu2017DistillingTK,
  title={Distilling Task Knowledge from How-To Communities},
  author={Cuong Xuan Chu and Niket Tandon and Gerhard Weikum},
  journal={Proceedings of the 26th International Conference on World Wide Web},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:17184682}
}

@article{Park2018LearningPF,
  title={Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks},
  author={Hogun Park and Hamid R. Motahari Nezhad},
  journal={Companion Proceedings of the The Web Conference 2018},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:13825603}
}

@article{Koupaee2018WikiHowAL,
  title={WikiHow: A Large Scale Text Summarization Dataset},
  author={Mahnaz Koupaee and William Yang Wang},
  journal={ArXiv},
  year={2018},
  volume={abs/1810.09305},
  url={https://api.semanticscholar.org/CorpusID:53046555}
}

@inproceedings{10.5555/3504035.3504965,
author = {Zhou, Luowei and Xu, Chenliang and Corso, Jason J.},
title = {Towards automatic learning of procedures from web instructional videos},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The potential for agents, whether embodied or software, to learn by observing other agents performing procedures involving objects and actions is rich. Current research on automatic procedure learning heavily relies on action labels or video subtitles, even during the evaluation phase, which makes them infeasible in real-world scenarios. This leads to our question: can the human-consensus structure of a procedure be learned from a large set of long, unconstrained videos (e.g., instructional videos from YouTube) with only visual evidence? To answer this question, we introduce the problem of procedure segmentation—to segment a video procedure into category-independent procedure segments. Given that no large-scale dataset is available for this problem, we collect a large-scale procedure segmentation dataset with procedure segments temporally localized and described; we use cooking videos and name the dataset YouCook2. We propose a segment-level recurrent network for generating procedure segments by modeling the dependencies across segments. The generated segments can be used as pre-processing for other tasks, such as dense video captioning and event parsing. We show in our experiments that the proposed model outperforms competitive baselines in procedure segmentation.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {930},
numpages = {9},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@inproceedings{xu-etal-2020-benchmark,
    title = "A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos",
    author = "Xu, Frank F.  and
      Ji, Lei  and
      Shi, Botian  and
      Du, Junyi  and
      Neubig, Graham  and
      Bisk, Yonatan  and
      Duan, Nan",
    editor = "Castellucci, Giuseppe  and
      Filice, Simone  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Specia, Lucia",
    booktitle = "Proceedings of the First International Workshop on Natural Language Processing Beyond Text",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.nlpbt-1.4",
    doi = "10.18653/v1/2020.nlpbt-1.4",
    pages = "30--40",
    abstract = "Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.",
}

@inproceedings{10.1145/2187980.2188194,
author = {Schumacher, Pol and Minor, Mirjam and Walter, Kirstin and Bergmann, Ralph},
title = {Extraction of procedural knowledge from the web: a comparison of two workflow extraction approaches},
year = {2012},
isbn = {9781450312301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187980.2188194},
doi = {10.1145/2187980.2188194},
abstract = {User generated Web content includes large amounts of procedural knowledge (also called how to knowledge). This paper is on a comparison of two extraction methods for procedural knowledge from the Web. Both methods create workflow representations automatically from text with the aim to reuse the Web experience by reasoning methods. Two variants of the workflow extraction process are introduced and evaluated by experiments with cooking recipes as a sample domain. The first variant is a term-based approach that integrates standard information extraction methods from the GATE system. The second variant is a frame-based approach that is implemented by means of the SUNDANCE system. The expert assessment of the extraction results clearly shows that the more sophisticated frame-based approach outperforms the term-based approach of automated workflow extraction.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {739–747},
numpages = {9},
keywords = {experience web, experience reuse},
location = {Lyon, France},
series = {WWW '12 Companion}
}

