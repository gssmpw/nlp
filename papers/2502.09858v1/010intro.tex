\vspace{-2mm}
\section{Introduction}

A hypothesis is a theory or an explanation based on limited evidence. It forms the backbone of decision-making, information acquisition, and discovery across domains~\cite{thompson2023scope}. For example, a robot evaluates different hypotheses to decide what action to take next. A scientist decides which experiments to run to evaluate a hypothesis/theory. The marketing strategy decisions are guided by the hypothesized effect on increasing customer retention. Similarly, policymakers may rely on hypotheses about the outcomes of proposed interventions. \

Given their profound implications, it is important to validate hypotheses with supporting evidence. This need has grown increasingly urgent with the recent surge in hypotheses generated by Large Language Models (LLMs)~\cite{wang2023hypothesis,zhou2024hypothesis}. While these systems exhibit remarkable creativity and diversity, the plausibility of their generated hypotheses can vary significantly due to potential hallucinations~\cite{huang2023survey}. Moreover, the sheer volume of LLM-generated hypotheses makes it impractical to invest in each one immediately. Therefore, obtaining a reliable, scalable understanding of the quality of these hypotheses is essential to fully unlock their potential.


Having said this, many real-world hypotheses are abstract natural language statements that are difficult to directly evaluate~\cite{thompson2023scope,godfrey2009theory}. For example, while we might hypothesize that ``a gene causes a disease,'' it is infeasible to test this statement directly as it stands. Instead, it must be translated into specific, measurable implications that can be experimented rigorously~\cite{jun2022hypothesis}. 
Yet, even for a single hypothesis, the space of potential supportive implications  is vast, highlighting the need for frameworks that can automate this evaluation process. 
Notably, such frameworks must also be statistically rigorous, avoiding false verifications of hypotheses that are not true~\cite{neyman1928use,neyman1933testing,fisher1936design}. 
Without such control, research efforts risk being misdirected, resources wasted, and harmful conclusions drawn, ultimately undermining progress and trust. Overall, this raises a critical question: \textit{How can we rigorously validate free-form hypotheses at scale?}

\vspace{-0.5em}
\xhdr{Present work}
We introduce \mname, a novel framework for rigorous and automated validation of free-form natural language hypotheses using LLM agents. Inspired by Karl Popper's principle of falsification~\cite{popper2005logic}, \mname systematically challenges hypotheses by sequentially testing their measurable implications through diverse experiments, ranging from data analysis and simulations to real-world experiments and interventions.

To automate this process, \mname\ employs two specialized LLM agents with complementary roles. The \emph{Experiment Design Agent} leverages reasoning capabilities and domain knowledge to identify a measurable implication (sub-hypothesis) of the main hypothesis and design a falsification experiment. Notably, this sub-hypothesis needs to be falsifiable with clear null and alternative definitions. 
Once designed, the \emph{Experiment Execution Agent} implements the experiments, which may involve data collection, simulations, statistical analyses, or real-world procedures. This agent ultimately produces a p-value that summarizes the outcome of the falsification experiment.

To maintain statistical rigor, \mname\ introduces a novel sequential testing framework that aggregates evidence from multiple, potentially dependent LLM-generated tests while strictly controlling the Type-I error rate (i.e., the probability of incorrectly rejecting a true null hypothesis). Individual p-values are converted into e-values~\cite{vovk2021values}, enabling the aggergation of cumulative evidence. By adaptively combining these e-values, \mname\ determines whether to reject the hypothesis, conduct further experiments, or terminate the validation process. The framework's ability to make dynamic, statistically sound decisions is ensured by the \emph{any-time validity} property of the combined e-values~\cite{grunwald2020safe}.
By iteratively testing adaptively solicited implications of a hypothesis, \mname\ systematically explores its flexibility while adhering to rigorous statistical principles. This provides a scalable and automated approach to hypothesis validation.

We instantiated \mname\ across six diverse domains, including biology, sociology, and economics. In our implementation, \mname\ designs falsification experiments by leveraging large-scale, hypothesis-free datasets and executes them with a Python code environment. The process involves systematic data identification, preprocessing, analysis, and statistical evaluation, ultimately generating sequentially valid p-values. Our results demonstrate that \mname\ effectively controls the Type-I error rate while achieving significant power improvements over existing methods. Additionally, an expert user study involving nine PhD-level biostatisticians and computational biologists found that \mname\ matched human performance in hypothesis validation tasks while reducing validation time by an order of magnitude. 








