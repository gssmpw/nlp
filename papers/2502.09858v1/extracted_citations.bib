@misc{baek2024researchagentiterativeresearchidea,
      title={ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models}, 
      author={Jinheon Baek and Sujay Kumar Jauhar and Silviu Cucerzan and Sung Ju Hwang},
      year={2024},
      eprint={2404.07738},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.07738}, 
}

@misc{gu2024bladebenchmarkinglanguagemodel,
      title={BLADE: Benchmarking Language Model Agents for Data-Driven Science}, 
      author={Ken Gu and Ruoxi Shang and Ruien Jiang and Keying Kuang and Richard-John Lin and Donghe Lyu and Yue Mao and Youran Pan and Teng Wu and Jiaqian Yu and Yikun Zhang and Tianmai M. Zhang and Lanyi Zhu and Mike A. Merrill and Jeffrey Heer and Tim Althoff},
      year={2024},
      eprint={2408.09667},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.09667}, 
}

@inproceedings{honovich-etal-2023-instruction,
    title = "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
    author = "Honovich, Or  and
      Shaham, Uri  and
      Bowman, Samuel R.  and
      Levy, Omer",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.108",
    doi = "10.18653/v1/2023.acl-long.108",
    pages = "1935--1952",
    abstract = "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7{\%} of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8{\%} of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.",
}

@misc{ifargan2024autonomousllmdrivenresearchdata,
      title={Autonomous LLM-driven research from data to human-verifiable research papers}, 
      author={Tal Ifargan and Lukas Hafner and Maor Kern and Ori Alcalay and Roy Kishony},
      year={2024},
      eprint={2404.17605},
      archivePrefix={arXiv},
      primaryClass={q-bio.OT},
      url={https://arxiv.org/abs/2404.17605}, 
}

@article{li2024critical,
  title={CriticAL: Critic Automation with Language Models},
  author={Li, Michael Y and Vajipey, Vivek and Goodman, Noah D and Fox, Emily B},
  journal={arXiv preprint arXiv:2411.06590},
  year={2024}
}

@misc{li2024mlrcopilotautonomousmachinelearning,
      title={MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents}, 
      author={Ruochen Li and Teerth Patel and Qingyun Wang and Xinya Du},
      year={2024},
      eprint={2408.14033},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.14033}, 
}

@misc{lu2024aiscientistfullyautomated,
      title={The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery}, 
      author={Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha},
      year={2024},
      eprint={2408.06292},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.06292}, 
}

@article{majumder2024discoverybench,
  title={Discoverybench: Towards data-driven discovery with large language models},
  author={Majumder, Bodhisattwa Prasad and Surana, Harshit and Agarwal, Dhruv and Mishra, Bhavana Dalvi and Meena, Abhijeetsingh and Prakhar, Aryan and Vora, Tirth and Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
  journal={arXiv preprint arXiv:2407.01725},
  year={2024}
}

@misc{si2024llmsgeneratenovelresearch,
      title={Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers}, 
      author={Chenglei Si and Diyi Yang and Tatsunori Hashimoto},
      year={2024},
      eprint={2409.04109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.04109}, 
}

@misc{tian2024scicoderesearchcodingbenchmark,
      title={SciCode: A Research Coding Benchmark Curated by Scientists}, 
      author={Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
      year={2024},
      eprint={2407.13168},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.13168}, 
}

@misc{wang2024hypothesissearchinductivereasoning,
      title={Hypothesis Search: Inductive Reasoning with Language Models}, 
      author={Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2309.05660},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.05660}, 
}

@misc{wang2024scimonscientificinspirationmachines,
      title={SciMON: Scientific Inspiration Machines Optimized for Novelty}, 
      author={Qingyun Wang and Doug Downey and Heng Ji and Tom Hope},
      year={2024},
      eprint={2305.14259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14259}, 
}

@misc{yang2024largelanguagemodelsautomated,
      title={Large Language Models for Automated Open-domain Scientific Hypotheses Discovery}, 
      author={Zonglin Yang and Xinya Du and Junxian Li and Jie Zheng and Soujanya Poria and Erik Cambria},
      year={2024},
      eprint={2309.02726},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.02726}, 
}

