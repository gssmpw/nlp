\section{Related Work}

We discuss here related works that are closest to   \mname and provide extended discussion on other related works in Appendix~\ref{appedix:related}. LLMs have been widely explored for hypothesis generation, with works focusing on domain-specific ideas \citep{wang2024scimonscientificinspirationmachines, baek2024researchagentiterativeresearchidea, yang2024largelanguagemodelsautomated} and comparisons between AI-generated and expert proposals \citep{si2024llmsgeneratenovelresearch}. Beyond idea generation, some studies refine hypotheses \citep{honovich-etal-2023-instruction, wang2024hypothesissearchinductivereasoning} or ground them in datasets \citep{majumder2024discoverybench}, yet few systematically test free-form hypotheses under rigorous statistical controls. While certain works evaluate LLM-driven experimental protocols \citep{tian2024scicoderesearchcodingbenchmark, gu2024bladebenchmarkinglanguagemodel} or integrate hypothesis and code generation \citep{li2024mlrcopilotautonomousmachinelearning, lu2024aiscientistfullyautomated, ifargan2024autonomousllmdrivenresearchdata, majumder2024discoverybench}, they often lack strong error control. Unlike these, \mname conducts robust statistical validation of both LLM- and human-generated hypotheses through a sequential falsification framework, ensuring reliability. Although \citet{li2024critical} also uses hypothesis testing as a way to challenge language models, \mname uniquely targets free-form natural language hypotheses and offers rigorous error control. 







