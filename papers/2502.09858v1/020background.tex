
\begin{figure*}[t!]
    \centering
    \captionsetup{font=small}
\includegraphics[width=\linewidth]{figs/popper_agent_illustration.pdf}
\vspace{-2mm}
    \caption{\textbf{Illustration of \mname.} Given a hypothesis and a pre-defined significance level $\alpha\in (0,1)$, \mname constructs sequential experiments to falsify the hypothesis. Each iteration proceeds as follows. First, an experiment design agent proposes a falsification experiment, which is refined through a self-critique process considering factors such as causality, data availability, and redundancy. The experiment is then evaluated by an LLM-as-a-judge relevance checker to ensure its alignment with the main hypothesis. If deemed relevant, the test is implemented by a ReAct-based experiment execution agent which obtains a p-value. P-values from multiple falsification experiments are aggregated into sequential e-values using a sequential testing framework. If the aggregated e-value exceeds $1/\alpha$, we declare sufficient evidence to reject the null hypothesis. Otherwise, the process continues with the next falsification test.} 
    \label{fig:method}
    \vspace{-4mm}
\end{figure*}

\vspace{-2mm}
\section{\mname: a general framework for automated hypothesis validation}

\subsection{Background and Problem Formulation}

Following \citet{majumder2024discoverybench,thompson2023scope}, we broadly define hypothesis $H$ as a statement that defines relationships ($r$) between a set of variables ($\mathcal{V}$) under contexts ($c$). For example, in the hypothesis $H$ ``Gene VAV1 regulates IL2 production in immune tissue'', $\mathcal{V}$= \{``VAV1'',  ``IL2 production''\}, $r$ = ``regulate'', and $c$ = ``in the immune tissue''.   
To formalize the discussion, the hypothesis $H$ is associated with a null hypothesis $H_0$. $H_0$  describes a family $\mathcal{P}_0$ of distributions that generate the data under the null, i.e., in uninteresting situations (such as ``Gene VAV1 does not regulate IL2 production''). In this way, $H_0$ being incorrect is of interest (the alternative hypothesis).  
Hypothesis validation aims to test the null hypothesis $H_0$ and suggest evidence for the alternative. 

The hypothesis validation task is defined as $f: H \rightarrow \{0,1\}$, where $0$ stands for unvalidated and $1$ stands for validated (claiming the alternative). Given a hypothesis $H$, a system or a program $f$ designs and performs experiments and generates an answer in $\{0,1\}$. We denote $\hat{y}$ as the predicted validation status. An experiment is typically associated with the collection (or retrieval) and processing of datasets denoted as $\mathcal{D}$.
An LLM agent $A$ is broadly defined as a program that takes in instructions in natural language and performs actions $\mathcal{T}$ with reasoning capabilities to solve the task given the instruction and outputs a natural language answer~\cite{yao2023react}. %

For rigorous hypothesis validation, we adopt the classical Type-I error control as our primary criterion. The Type-I error is the probability of the system incorrectly claiming an ``interesting'' finding (e.g., enriched gene expression) when the null hypothesis is true. 
Formally, the Type-I error rate is  $\sup_{\mathbb{P}\in \mathcal{P}_0}\mathbb{P}(\hat{y}=1)$, where the probability is over the data and the validation system. 
To ensure rigor, our goal is to control the Type-I error at a pre-defined level $\alpha \in (0,1)$.
Another important concept is the power of the validation system, which we define as $\mathbb{P}(\hat{y}=1)$ where $\mathbb{P}$ is the data distribution. 
While power--the ability to detect true effects--is important, its improvement is meaningful only when  Type-I error control is ensured. Without this foundation, increased power risks invalid conclusions. 

