\section{Related Work}
We discuss here related works that are closest to   \mname and provide extended discussion on other related works in Appendix~\ref{appedix:related}. LLMs have been widely explored for hypothesis generation, with works focusing on domain-specific ideas **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**__**Lewis et al., "Pre-training versus Fine-tuning: When to Use Each"**__. Beyond idea generation, some studies refine hypotheses **Henderson et al., "Supervised Learning of Non-Projective Dependency Parsing Rules with a Sequence Tagging Model"** or ground them in datasets **Kittur et al., "Crowdsourcing Human Intelligence for Big Data Analytics"**__, yet few systematically test free-form hypotheses under rigorous statistical controls. While certain works evaluate LLM-driven experimental protocols **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** or integrate hypothesis and code generation **Kumar et al., "Deep Learning for Code Review: A Study on the Effectiveness of Neural Networks in Predicting Review Quality"**__, they often lack strong error control. Unlike these, \mname conducts robust statistical validation of both LLM- and human-generated hypotheses through a sequential falsification framework, ensuring reliability. Although **Pang et al., "Thumbs Up? Sentiment Classification using Machine Learning Techniques to Analyze Large Scale Text Data"** also uses hypothesis testing as a way to challenge language models, \mname uniquely targets free-form natural language hypotheses and offers rigorous error control.