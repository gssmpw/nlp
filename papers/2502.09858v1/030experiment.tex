

\section{Experiments}
We evaluate \mname in terms of Type-I error control, power improvements, expert user studies, ablations, human annotations, and failure analysis.

\vspace{-0.5em}


\xhdr{Evaluation setup} 
We assess Type-I error by creating \emph{negative examples} through random column-wise permutations in each dataset, ensuring the null hypothesis holds. 
For DiscoveryBench, we generate as many negative examples as positive ones. 
For the target validation benchmark (with only 20 positives), we create 50 negatives. 
We measure Type-I error by the proportion of ``reject'' decisions ($\hat{y}=1$) on negative examples and Power by the proportion of ``reject'' decisions on positive examples. 
We set a nominal Type-I error level $\alpha=0.1$. 
Unless noted otherwise, we use Claude-Sonnet-3.5 as our LLM, with a maximum of 3 tests on DiscoveryBench and 5 on target validation (due to more complex hypotheses in the latter scenario).





\vspace{-0.5em}
\xhdr{Baselines \& variations}
We group comparing methods into two categories. 
(1)~\textit{Baselines.} Since this is a novel application with no direct references, we compare against three general-purpose task resolvers: \textit{CodeGen}~\cite{ridnik2024code}, which generates code; \textit{ReAct}~\cite{yao2023react}, which iteratively combines reasoning and coding; and \textit{Self-refine}~\cite{madaan2024self}, which refines CodeGen outputs via a critic. None include specialized mechanisms for statistical rigor. 
We also evaluated an enhanced \textit{CodeGen-o1} with improved reasoning. (2)~\textit{Variations of \mname.} These include \textit{Fisher}, which uses p-values and Fisher's combined test~\cite{fisher1970statistical} instead of e-values; \textit{LLM-Likelihood Ratio}, which relies on an LLM to estimate the (optimal) likelihood ratio~\cite{zheng2023judging} rather than a p-to-e calibrator; \textit{\mname-NoReleCheck}, omitting the relevance checker; and \textit{\mname-CodeGen}, which substitutes ReAct with direct code generation for statistical tests.



\vspace{-0.5em}

\subsection{Results}

\vspace{-0.5em}

\xhdr{\mname achieves Type-I error control}
Table~\ref{tab:main_result} reports the Type-I error rates and several key observations are in order. 
First, most baselines fail to consistently control the Type-I error, while \mname remains below the nominal level across all datasets. 
This underscores the necessity of principled statistical design in LLM-driven hypothesis validation; 
without such rigor, the flexibility of LLM agents can inflate Type-I errors. 
Second, the comparison against Fisher's combined test highlights the benefits of e-values in aggregating evidence. 
Third, the LLM-Likelihood Ratio method lacks calibration, overly conservative for TargetVal-IL2 and too liberal for DiscoveryBench and TargetVal-IFNG, illustrating the need for strict statistical control rather than relying solely on LLM-based estimations. 
Finally, removing the relevance checker (\mname-NoReleCheck) significantly raises the Type-I error due to irrelevant and misleading tests. 
Together, these results establish \mname as a robust framework for agentic hypothesis validation.

\vspace{-0.5em}
\xhdr{\mname has significant power improvement} 
Table~\ref{tab:main_result} shows the power across three benchmarks. 
First, we exclude any method with an uncontrolled Type-I error (gray-shaded in the table), as their power estimates are invalid. 
Among methods that do control the Type-I error, \mname consistently achieves the highest power: on DiscoveryBench, it delivers 66.5\% greater power than ReAct, and on TargetVal-IL2, it outperforms Self-Refine by a factor of 3.17. 
This highlights the strength of \mname's iterative testing mechanism, which continually accumulates evidence to improve validation. 
Second, \mname  with the ReAct coding agent outperforms \mname-CodeGen in power - even with a lower Type-I error. The likely cause is that its reasoning module enables more effective falsification tests. 
Overall, these results confirm the  ability of \mname to balance high power with error control, making it a reliable and efficient approach to hypothesis validation.


\vspace{-0.5em}

\xhdr{\mname compares with human experts}
We recruited nine computational biologists and bioinformaticians (either PhD holders or candidates) to perform hypothesis validation on TargetVal-IL2 (details in Appendix~\ref{appendix:human_study}). 
Figure~\ref{fig:human_study} shows that the Type-I error and power of \mname closely match those of the human participants, with no statistically significant differences given the small sample size. 
Notably, \mname completed tasks 9.7 times faster, generated 3.6 times more lines of code, and performed 2.5 times more statistical tests, underscoring its efficiency gains. 
Qualitative analysis (the right half of Figure~\ref{fig:human_study}, where the numbers represent the amount of distinct statistical tests in each category) revealed substantial overlap between human experts and \mname in both biological falsification experiments (e.g., correlation in gene expression levels, network interactions, eQTL tests) and statistical methods (e.g., permutation, $t$-test, chi-squared), reinforcing the soundness of \mname in automating validation tasks.


\begin{figure}[!t]
\captionsetup{font=small}
    \centering
    \includegraphics[width=\linewidth]{figs/human_study.pdf}
    \vspace{-8mm}
    \caption{
    \textbf{Expert human study.}
\mname achieved similar power and Type-I error rates to human experts while significantly reducing task completion time.
It also generated more lines of code and conducted more statistical tests.
Qualitatively, \mname and human experts exhibited substantial overlap in both the designed falsification experiments and the statistical methods employed.
    }
    \label{fig:human_study}
\end{figure}


\vspace{-0.5em}
\xhdr{Performance varies across a wide range of LLMs}
Since \mname must propose meaningful falsification tests and compute valid p-values (per Assumptions~\ref{assump:hypo} and~\ref{assump:evalue}), it requires strong reasoning and coding capabilities. We evaluated several LLMs on DiscoveryBench and TargetVal-IL2, including closed-source models (Claude Haiku~3.5, Sonnet~3.5, GPT-4o, o1) and the open-source Llama~3.3~70B. Table~\ref{tab:var-llms} shows that higher-capability models are critical: Claude~Haiku~3.5 has a high Type-I error, whereas Llama, GPT-4o, Sonnet, and o1 maintained reasonable error control. Among them, o1 performed best on DiscoveryBench, and GPT-4o excelled in power for DiscoveryBench, whereas Sonnet led on TargetVal-IL2. These results emphasize the importance of robust reasoning and coding skills for effective hypothesis validation and highlight nuanced performance trade-offs.


\begin{table}[!t]
\captionsetup{font=small}
    \centering
    \caption{\textbf{Evaluation of various LLM backbones with \mname. }}
    \vspace{-2mm}
    \adjustbox{width = 0.5\textwidth}{
    \begin{tabular}{l|cc|cc}
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c|}{Type I Error ($\alpha$ = 0.1)} & \multicolumn{2}{c}{Power} \\ \cmidrule{2-5}
    & \small DiscoveryBench & \small TargetVal-IL2 & \small DiscoveryBench & \small TargetVal-IL2 \\
    \midrule
    Claude-Haiku-3.5
        & 0.230\std{0.079} & 0.780\std{0.120}
        & \textcolor{gray}{0.844}\std{0.017} & \textcolor{gray}{0.835\std{0.113}} \\
    Llama 3.3 70B
        & 0.147\std{0.036} & 0.116\std{0.020}
        & \textcolor{gray}{0.690\std{0.027}} & 0.515\std{0.078} \\
    GPT-4o
        & 0.143\std{0.039} & 0.096\std{0.043}
        & \textcolor{gray}{0.730\std{0.054}} & 0.385\std{0.102} \\
    Claude-Sonnet-3.5
        & 0.103\std{0.020} & 0.082\std{0.046}
        & 0.638\std{0.066} & \textbf{0.580*}\std{0.125} \\
    o1
        & \textbf{0.091*}\std{0.015} & \textbf{0.031*}\std{0.015}
        & \textbf{0.654*}\std{0.019} &  0.336\std{0.121}\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \label{tab:var-llms}
\end{table}



\begin{figure*}[t!]
    \centering
\captionsetup{font=small}
    \includegraphics[width=\linewidth]{figs/popper_characterization.pdf}
    \vspace{-5mm}
    \caption{\textbf{Characterization of \mname.} (1) \mname designs biologically relevant falsification experiments. (2) It performs multiple logical steps to execute the experiment. (3) It employs a wide range of statistical tests. (4) Progression of cumulative e-values across multiple iterations of falsification tests. More details are available in Appendix~\ref{appendix:test_analysis}.}
    \label{fig:test_distribution}
    \vspace{-2mm}
\end{figure*}

\subsection{Analysis and Discussion}\label{sec:analysis}
\vspace{-0.5em}

\xhdr{Qualitative characterization}
We characterize the trajectories of \mname in Figure~\ref{fig:test_distribution} (procedure described in Appendix~\ref{appendix:test_analysis}). In TargetVal, we observe that \mname designed experiments that span a broad set of biological tests, including protein-protein interaction networks, expression correlation analyses, eQTL regulatory tests, loss-of-function studies, and genetic perturbation tests. 
During each iteration, the execution agent typically performs up to 14 distinct steps: dataset inspection, preprocessing, model fitting, error handling, statistical testing, visualization, and summarization. 
Notably, \mname carefully selects statistical methods based on modeling assumptions (e.g., chi-squared, hypergeometric, Fisher's, and permutation tests) and often includes well-chosen negative controls. 
Interestingly, non-parametric tests are most frequent, making them robust to various data distributions. 
Visualizing the e-value trajectories reveals that evidence against the null accumulates quickly under the alternative while remaining below the nominal threshold under the null, underscoring the rigor and power of sequential testing.


\vspace{-0.5em}

\xhdr{Sensitivity analysis}
Figure~\ref{fig:sensitivity} presents the robustness of \mname under different settings. 
First, we varied the significance level \(\alpha\) and found that \mname consistently maintained Type-I error control. 
Second, we examined the effect of increasing the budget (maximum number of tests). 
While Type-I error remained well-controlled, the power rose with additional tests, indicating that \mname can accumulate more diverse evidence when given more computational resources. 
These results demonstrate the scalability of e-values to both small and large numbers of sequential tests, allowing \mname to achieve higher discovery rates as resources increase.

\begin{figure}[!t]
    \centering
\captionsetup{font=small}
    \includegraphics[width=\linewidth]{figs/popper_sensitivity_analysis.pdf}
    \vspace{-10mm}
    \caption{\textbf{Sensitivity analysis.} (1) Empirical Type-I error at various nominal levels $\alpha$. (2) Power and Type-I error at various budgets as a function of the number of maximum tests.}
    \vspace{-5mm}
    \label{fig:sensitivity}
\end{figure}

\vspace{-0.5em}

\xhdr{Human annotations of falsification test quality}
To assess the implication strength of LLM-generated falsification tests, three authors independently rated 90 randomly selected proposals using the same rubric provided to the ReleCheck agent (Appendix~\ref{lst:relevance_checker_prompt}). 
After calibration, the annotators achieved a high inter-rater agreement (Kendall's $W=0.91$). 
The agent's ratings correlated strongly with human judgments (Spearman's $\rho=0.55$, $p=5\times10^{-6}$), 
though it slightly overestimated the relevance of the implications: 
it labeled 85\% of proposals as ``strongly implied," compared to a 77\% pass rate among human evaluators. 
These findings indicate that while the ReleCheck agent aligns reasonably well with human perspectives, further calibration and domain-specific expertise are needed to enhance the reliability of falsification test selection.

\vspace{-0.5em}
\xhdr{Error analysis}
We analyzed potential failure modes in \mname's hypothesis validation workflow. Using an LLM to categorize errors followed by human inspections, we identified the top reasons for failure: misinterpreted p-values (35.9\%), ineffective falsification experiment design (28.1\%), falsification test breaks implication (17.2\%), and incorrect test implementation (8.6\%). Hallucination was minimal (0.8\%). More details are provided in Appendix~\ref{appendix:error_analysis}. Overall, while agentic automation holds promise, our findings highlight areas needing further improvement, guiding future work on more robust hypothesis validation pipelines.















