Under limited budget constraints, it is impractical to save all task vectors for a large number of tasks $T$. Although one could argue to only save one final merged model, we lose flexibility of task vector composition, especially when we want to only merge or unlearn a part of the knowledge from a certain task in the future. Besides, the addition performance drop as the number of tasks grows is reported in \citet{ilharco2022editing} in practice. Therefore, we propose using \textbf{task vector bases} to reduce the number of task vectors to save while retaining task information maximally and preserving the flexibility of model composition.

\begin{definition}[Task Vector Bases]
    Given $T$ task vectors, task vector bases are a new set of $d$-dimensional vectors $\{\tau_{B_1}, \dots, \tau_{B_m}\}$, where $1 < m < T$, created by applying certain transformations to the original $T$ task vectors.
    \label{def:basis}
\end{definition}

\subsection{Bases Creation}
\label{sec:bases_creation}

From \Cref{thm:task-addition}, we want $\epsilon$ to be small, thereby creating near-orthogonal bases. As observed in the green box of \Cref{fig:twice-task-orthogonality}, non-related tasks are nearly orthogonal, while similar task vectors represent certain interpretable skills. Based on this intuition, we define the transformation in \Cref{def:basis} by grouping similar task vectors using clustering algorithms to remove redundant information across tasks.

\begin{algorithm}[tb]
   \caption{Task Vector Bases Creation}
   \label{alg:task_vector_basis}
\begin{algorithmic}[1]

   \STATE {\bfseries Input:} task vectors $\tau_1, \dots, \tau_T$, inner merge methods $\texttt{merge}_\text{in}()$, threshold $\delta_1 < 0$
   \STATE $M \in \mathbb{R}^{T\times T}, M_{ij} = \cos(\tau_i,\tau_j)$ // \textit{Task clustering}
   \STATE $k = \arg\max_{1 < i \leq {T-1}} \{\lambda^{(i+1)}_{L_{|M|}} - \lambda^{(i)}_{L_{|M|}}\}$ 
   \STATE $\mathcal{C}_1, \dots, \mathcal{C}_k = $ \texttt{spectral\_cluster}$(|M|, k)$
\STATE{$\mathcal{C} = \{\}$ // \textit{Separate positive and negative directions}}
\STATE{\textbf{for} $i \in [k]$ \textbf{do}}
   \STATE \quad Extract submatrix $M_{\mathcal{C}_i}$ from $M$
   \STATE{ \quad \textbf{if} $\min{M_{\mathcal{C}_i}} < \delta_1$ \textbf{then}}
      \STATE \quad \quad $M_{\mathcal{C}_i} \mathrel{+}= |\min{M_{\mathcal{C}_i}}|$ 
      \STATE \quad \quad $\mathcal{C}_{pos}^i, \mathcal{C}_{neg}^i = $ \texttt{spectral\_cluster}$(M_{\mathcal{C}_i}, 2)$
      \STATE {\quad \quad $\mathcal{C} = \mathcal{C} \cup \{\mathcal{C}_{pos}^i, \mathcal{C}_{neg}^i\}$}
   \STATE{\quad \textbf{else} $\mathcal{C} = \mathcal{C} \cup \{\mathcal{C}_i\}$}
   \STATE $\mathcal{C}_{B_1}, \dots, \mathcal{C}_{B_m} = \mathcal{C}$  
   \STATE{\textbf{for}  $i \in [m]$ \textbf{do}}
   \STATE \quad $\tau_i = \texttt{merge}_\text{in}(\theta_0, \mathcal{C}_{B_i})$ // \textit{Create basis by merging}
   \STATE {\bfseries Return:} $\{\tau_1, \dots, \tau_{B_m}\}$
\end{algorithmic}
\end{algorithm}

\begin{table}[tb]
\caption{Bases Arithmetic. Here $i$ is a task id. There are several ways to interpret similarity search given bases and the target task.}
\label{tab:basis_arithmetic}
\centering
\scalebox{0.85}{
\begin{tabular}{@{}ll@{}}
\toprule
Arithmetic            & Expression \\ \midrule
Addition for multitask             &  $\theta^* = \texttt{merge}_\text{out}(\theta_0, \{\tau_1, \dots, \tau_{B_m}\})$.         \\
Negation for unlearning $i$       &    Find $B_j$ the most similar to $i$,    \\
       &    $\theta^* = \theta_0 - \alpha\tau_{B_j}$.       \\
OOD Generalization on $i$ &     Find $U = \{B_{j_1},\dots, B_{j_l}\}$ similar to $i$,   \\ 
 &    $\theta^* = \texttt{merge}_\text{out}(\theta_0, \{\tau_{B_{j_1}}, \dots, \tau_{B_{j_l}}\})$     \\
&    if $U = \emptyset$, store $\tau_i$ as new basis.    \\\bottomrule
\end{tabular}
}
\end{table}


In \Cref{alg:task_vector_basis}, we first create the similarity matrix of task vectors and pass this matrix into spectral clustering \citep{ng2001spectral} to group the tasks. Next, we can apply any existing merging method as $\texttt{merge}_\texttt{in}$ to create task vector bases. When cosine similarity becomes strongly negative, we first use the absolute value of the similarity for clustering and further partition the positive and negative groups with an additional clustering step. However, the scenario where task vectors have nearly opposite directions rarely occurs in practice for natural tasks unless specific adversarial tasks are defined. Therefore, lines 5-9 are typically optional. We choose spectral clustering for memory efficiency. Unlike $k$-means \citep{lloyd1982least}, which requires storing the entire set of $O(Td)$ task vectors during the update, spectral clustering only requires storing an $O(T^2)$ matrix for eigenvalue computation, which is more advantageous since $d \gg T$. Additionally, in line 3 of \Cref{alg:task_vector_basis}, we can determine the number of bases by inspecting the eigenspectrum of the Laplacian matrix $L_{|M|}$, without the need for additional hyperparameter tuning. This approach reduces the $T$ task vector addition problem to saving only $m$ task vector bases, significantly reducing the memory footprint.




\subsection{Bases Arithmetic}
\label{sec:bases_arithmetic}
 
We show how to do arithmetic with task vector bases in \Cref{tab:basis_arithmetic}, with only slight modifications from standard task arithmetic operations discussed in \Cref{sec:bounds}. There are two points that need special attention. First, we observe the usage of $\texttt{merge}_\text{out}$, which is another iteration of model merging. We can replace $\texttt{merge}_\text{out}$ easily with dataless methods such as Model Soup \cite{pmlr-v162-wortsman22a} or TIES \cite{yadav2024ties}, while for more advanced methods, we provide an example in \Cref{sec:localize-and-stitch} to show possible modifications that fit in our framework. Second, there are multiple ways to do the similarity search in \Cref{tab:basis_arithmetic}. The naive method is to fine tune $\theta_i$ till convergence to get $\tau_i$ and simply compute the cosine similarity between $\tau_i$ and bases vectors, but then one could argue to directly use $\tau_i$ for unlearning and OOD generalization. Fortunately, in \Cref{sec:intermediate}, we see without training $i$ till convergence, intermediate checkpoints even with one step of gradient update still reflects important task information. Also, as we will see in \Cref{sec:memory_70} since bases can be interpreted as some skill: for example, one bases represents digit classification and the target task is MNIST, we can use such heuristics of task similarity knowledge without any explicit computation of $\tau_i$. In these two scenarios, since $\tau_i$ is either far from the optimal or does not exist, it can be more beneficial to use stored knowledge in the bases.

