\begin{figure}[tb]
    \centering
\includegraphics[width=0.6\linewidth]{figure/basis_intro_clusteronly.pdf}
    \caption{Saving one task vector for each task can lead to high memory cost and suboptimal merging performance as the number of tasks increases. Instead, we can construct the affinity graph of task vectors for clustering, identify the clusters, and then apply existing merging methods on them to create task vector bases.}
    \label{fig:basis_intro}
\vspace*{-1em}
\end{figure}

Task vector \citep{ilharco2022editing} is a practical model editing and merging technique that navigates the weight space of pretrained models. These vectors provide a direction that enhances task-specific knowledge in parameter space by subtracting the pretrained model weights from the updated parameters after fine-tuning a specific task. Beyond their simplicity, task vectors have an intriguing property: they can be manipulated through arithmetic operations such as addition and negation, allowing for the composition of models with tailored behaviors or hybrid task capabilities.

However, existing approaches mainly rely on heuristics and lack theoretical justifications~\citep{yang2024model}, leaving key questions about their principles and limitations unanswered. The foundations of model merging techniques are nascent. Some existing analyses make impractical assumptions, such as disjoint task support \citep{ortiz2024task,xiong2024multi} or convexity \citep{zhou2024cross, tao2024task}, which fail to provide a convincing explanation of the underlying mechanisms of task arithmetic. 

Furthermore, the scalability of task vector-based approaches for model editing faces several challenges. One major issue is that merging models trained on multiple tasks often results in a performance drop compared to training models independently or using multi-task joint training, especially as the number of tasks increases \citep{ilharco2022editing, yang2024model}. Additionally, existing model merging methods are inefficient, often requiring substantial memory to store task vectors, which are the same size as the pretrained model. For example, merging 72 fine-tuned ViT-B/32 \citep{dosovitskiy2020image} models can require over 200GB of memory~\citep{li2024scalable, yang2024model}. There were attempts that address this problem either through model sparsification \citep{he2024localize, wang2024localizing}, alternative optimization algorithms \citep{li2024scalable}, or assuming all fine-tuned weights lie in a thin Gaussian shell when training task is the same \citep{jang2025model}. However, it is unclear whether the same statement applies to task arithmetic where models are finetuned from diverse tasks. This limits the applicability of such techniques, particularly when handling large-scale models or resource-constrained environments.

In this work, we address these limitations with the following contributions. \emph{First}, we provide an in-depth theoretical analysis to explain when and why task arithmetic is successful. We establish the connections between key assumptions on task vector norms, task vector similarity, local smoothness, and the generalization error bounds of different task arithmetic operations. We additionally validate our assumptions and theorems with empirical evidence from previous literature and our new setups. \emph{Second}, as shown in \Cref{fig:basis_intro}, we leverage the similarity between different task vectors and propose a two-step framework to learn a task space with bases: first clustering the task vectors with their similarity matrix, and then merging clustered task vectors as bases with any model merging methods. Through extensive experiments, we show how to do bases arithmetic, including addition, negation and out-of-distribution task generalization with a significant reduction of memory requirement. Our task vector bases approach enables scalable and efficient model editing while preserving strong performance across multiple tasks by fully leveraging the advantage of existing task arithmetic methods in the past literature. 