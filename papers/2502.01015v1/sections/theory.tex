We will next introduce several practical assumptions based on which we provide our theoretical analysis to explain the benefits of task vectors in model merging. To complement our theoretical analysis, we also provide empirical evidence to verify our theoretical statements in~\Cref{sec:experiments}.

\subsection{Assumptions}

\begin{assumption}[Fine-tuning Regime]
    We assume that $\forall i\in [T], \frac{\partial\mathcal{L}_i(\theta_i)}{\partial{\theta}} = \mathbf{0}$ and $\exists C > 0$ such that $\norm{\tau_i}^2 \leq C$.
\label{ass:finetuning}
\end{assumption}
\Cref{ass:finetuning} is often met in practice since $\theta_i$ is fine-tuned from the pre-trained model $\theta_0$ on the particular downstream task $\calD_i$ until convergence. Furthermore, during the fine-tuning regime, the change of model parameters is relatively small. Through a sparsity localization technique,~\citet{he2024localize} show that it is sufficient to only fine-tune 1\%$\sim$5\% of the model parameters for competitive performances.

\begin{assumption}[Task Vector Near Orthogonality] There exists a universal constant $\epsilon > 0$ such that $\forall i\neq j, |\cos(\tau_i, \tau_j)| \leq \epsilon$.
\label{ass:orthogonality}
\end{assumption}

A small $\epsilon$ in \Cref{ass:orthogonality} holds when any pair of tasks are not related to each other, which happens when task vectors are restricted to be sparse~\citep{he2024localize}. Cross-task generalization when tasks are similar to each other are better understood~\citep{tripuraneni2020theory,hu2024revisiting}. 
\begin{remark}
From a technical perspective, if $\tau_i$ and $\tau_j$ are independent standard Gaussian random vectors, then $\mathbb{E}[|\cos(\tau_i, \tau_j)|] = \sqrt{2/\pi d}$ and $\text{Var}(|\cos(\tau_i, \tau_j)|) \approx (1 - \frac{2}{\pi})/d$, so $|\cos(\tau_i, \tau_j)| \to 0$ when $d \to \infty$ by Chebyshevâ€™s inequality. The upper bound of the task vector norm $C$ is also dependent on the $d$. Since fine-tuning only slightly changes each parameter, let $\tau = (\tau^1, \cdots, \tau^d)$, and each entry of the task vector has an $O(1)$ change during the fine-tuning. Then, $\norm{\tau}^2 = O(d)$.
\end{remark}

\begin{assumption}[Local Smoothness]
    Any fine tuning loss function $\calL$ is $L_i$-locally smooth w.r.t.\ model parameters at $\theta_i$, which means for any $\theta \in \Theta$ such that $\|\theta - \theta_i\|^2 = O(C), \mathcal{L}(\theta) - \mathcal{L}(\theta_i) \leq \left<\theta - \theta_i, \frac{\partial{\mathcal{L}(\theta_i)}}{\partial\theta}\right> + \frac{L_i}{2}\norm{\theta - \theta_i}^2.$
    Note that $\theta_i$ is the fine-tuned model trained on $\mathcal{D}_i$ and $L_i = \norm{\mathbf{H}(\theta_i)}_2$ is the spectral norm of the Hessian matrix of $\mathcal{L}$, evaluated locally at $\theta_i$. We hide the subscript of $L_i$ when the context is clear.
\end{assumption}

\begin{assumption}[Coefficients]
\label{ass:coeff}
    Let $\alpha_1, \dots, \alpha_T$ be the coefficients used to scale the task vector in task arithmetic. We assume $\alpha_i \geq 0, \forall i$ and $\sum_{i\in[T]}\alpha_i = 1$.
\end{assumption}


\subsection{Task Arithmetic Bounds}
\label{sec:bounds}
With assumptions in the previous section, by first-order Taylor expansion w.r.t. the fine tuned model parameter, we can get the following statements for different types of task arithmetic. We defer all proof details to \Cref{sec:bounds-details}.

\begin{restatable}[Task Addition for Multitask Learning]{theorem}{add}
    Let task addition $\theta^* = \theta_0 + \sum_{i=1}^T\alpha_i\tau_i$ be the model parameter used for multitask learning, then $\forall i\in[T], \mathcal{L}_i(\theta^*) - \mathcal{L}_i(\theta_i) \leq 2L_iC(1 + \epsilon).$
\label{thm:task-addition}
\end{restatable}
\Cref{thm:task-addition} shows that as long as the task vectors reside in the fine-tuning regime and task vectors are nearly orthogonal, then a single model obtained by model merging simultaneously performs comparably well on all the tasks. This bound does not depend on the number of tasks $T$. The local smoothness constant $L_i$ in the generalization bound implies that a flatter minima is preferred in model merging, which also partially explains the empirical success of the Fisher weighted averaging method~\citep{matena2022merging} as $\mathbf{H}$ is also the Fisher information matrix when $\ell$ is the cross-entropy loss which is a log-likelihood.

\begin{restatable}[Task Negation for Unlearning]{theorem}{negate}
Let $\theta_i^* = \theta_0 - \alpha_i\tau_i$ be the model parameter used for unlearning task $i$. Then $\forall j\neq i$, $\mathcal{L}_j(\theta_i^*) - \mathcal{L}_j(\theta_0)\leq L_jC\left(\frac{3}{2} + \epsilon\right)$. 
\end{restatable}
Since $C$ is small due to fine-tuning, $\mathcal{L}_j(\theta_i^*) \approx \mathcal{L}_j(\theta_0)$, which means that the negation of a task for forgetting will not adversely impact the performance of other orthogonal tasks, which has been shown empirically in \citet{ilharco2022editing}, in contrast to other classic unlearning methods like gradient ascent.

\begin{restatable}[Out-of-Distribution Generalization]{theorem}{generalize}
    Given a collection of source task vectors $\mathcal{S} = \{\tau_1, \tau_2, \dots, \tau_T\}$ and a target task vector with $\|\tau_\mathrm{tar}\|^2\leq C$. If $\exists i\in[T]$ such that $\langle \tau_\mathrm{tar}, \tau_i\rangle \geq \beta C$ for $0 < \beta \leq 1$, then there exists a merging scheme $\alpha_i, i\in[T]$ such that for the merged model $\theta^* = \theta_0 + \sum_{i=1}^T \alpha_i \tau_i, \mathcal{L}_\mathrm{tar}(\theta^*) \leq \mathcal{L}_\mathrm{tar}(\theta_\mathrm{tar}) + L_\mathrm{tar}C(1-\beta).$  
\label{thm:task-generalization}
\end{restatable}
This implies when $\beta$, which roughly corresponds to the cosine similarity of the two task vectors, is large enough, the gap between $\mathcal{L}_\mathrm{tar}(\theta^*)$ and $\mathcal{L}_\mathrm{tar}(\theta_\mathrm{tar})$ is small, so we can use the combination of similar task vectors to achieve similar generalization performance for tasks that are out-of-distribution (OOD) w.r.t. the source models.
