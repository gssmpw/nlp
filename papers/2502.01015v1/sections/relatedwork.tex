\paragraph{Theory of Task Arithmetic}
Task arithmetic was believed to be related to the hypothesis that fine-tuning over-parameterized models behaves like a neural tangent kernel \citep{jacot2018neural}, but this hypothesis contradicts the experimental results in \citet{ortiz2024task} where there exists a nontrivial gap between linear and standard non-linear fine-tuning generalization performance. Therefore, \citet{ortiz2024task} proposed a formal definition of the task arithmetic property, which has been widely adopted by subsequent work \citep{xiong2024multi} for analysis, yet under the assumption of disjoint task support, which does not hold in the standard image classification merging benchmark. Another hypothesis is related to the linear mode connectivity \citep{garipov2018loss} phenomenon \citep{frankle2020linear, neyshabur2020being}, which concerns the same pretrained model finetuned on the same task with different SGD noise due to hyperparameter difference or data shuffling. The same phenomenon was observed to be layer-wise on modern model architectures \citep{adilova2023layerwise}, and further generalized to cross-task linearity \citep{zhou2024cross} in the context of model merging, where modes are related to different input tasks. \citet{zhou2024cross} provides a first-order Taylor expansion analysis to prove the existence of cross-task linearity relying on the convexity assumption. Similarly, the convexity assumption also appears in more recent theoretical analyses \citep{tao2024task} that relates task arithmetic with one-shot federated learning theories, which is not required in our framework. Additionally, \citet{zhou2024metagpt} assumes mean-squared error as the loss function to analytically compute the optimal vector merging weights. In summary, all existing theories rely on assumptions such as disjoint task support and loss convexity that do not hold in practical models. In contrast, we do not assume loss convexity and further relax the disjoint task support assumption. We provide both discussion and empirical evidence that support our assumptions. For the literature review of other model merging methods, please refer to \Cref{sec:ta-methods}.

\paragraph{Task Grouping}
Since it is widely believed that jointly training similar tasks together improves accuracy performance or convergence \citep{caruana1993multitask} in the multi-task learning problem, identifying task groupings is a well-explored area. Classic methods include convex formulations of task clustering \citep{jacob2008clustered}, latent task basis learning \citep{kumar2012learning}, and gradient-based methods \citep{fifty2021efficiently}, which are more suitable for modern deep learning. When task vectors are used for addition in multi-task settings, many techniques from the multitask grouping literature can be adapted to our context. Another interesting recent work proposed the LoRA \citep{hu2021lora} Adapter library \citep{ostapenko2024towards}, which uses a similar clustering approach to enable more modular large language models, along with a router-based adapter reuse algorithm. Such advantage was also proved to be successful for LoRA merging \citep{zhao2024merging}, which can be seen as a special case under our bases framework. Note that we focus on the broader context of task arithmetic to examine how creating a task vector bases, or library, impacts the performance of multitask, unlearning, and domain generalization capabilities, regardless of fine tuning strategy used for task vectors. 