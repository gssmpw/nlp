\section{Proof Details of \Cref{sec:bounds}}
\label{sec:bounds-details}
\add*
\begin{proof}
Note that since $\theta^* = \theta_0 + \sum_{i=1}^T \alpha_i\tau_i$, so
\begin{equation*}
    \|\theta^* - \theta_0\|^2 = \left\|\sum_{i=1}^T\alpha_i \tau_i\right\|^2 \leq \left(\sum_{i=1}^T\alpha_i \|\tau_i\|\right)^2 \leq C,
\end{equation*}
which means that $\theta^*$ is within the fine-tuning regime and satisfies the local smoothness assumption. Hence, if $x\sim\mathcal{D}_i$,
\begin{align*}
    \gL_i(\theta^*) - \mathcal{L}_i(\theta_i) &\leq \left<\theta^* - \theta_i, \frac{\partial\mathcal{L}_i(x, \theta_i)}{\partial{\theta}}\right> + \frac{L_i}{2}\norm{\theta^* - \theta_i}^2\\
    &= \left<\sum\limits_{j=1}^T\alpha_j\tau_j - \tau_i, \cancel{\frac{\partial\mathcal{L}_i(x, \theta_i)}{\partial{\theta}}}\right> + \frac{L_i}{2} \norm{\sum\limits_{j=1}^T\alpha_j\tau_j - \tau_i}^2\\
    \norm{\sum\limits_{j=1}^T\alpha_j\tau_j - \tau_i}^2 &= \sum\limits_{j=1}^T\alpha_j^2\norm{\tau_j}^2 + 2\sum_{j\neq k}\alpha_j\alpha_k\left<\tau_j, \tau_k\right> - 2\left<\sum\limits_{j=1}^T\alpha_j\tau_j,\tau_i\right> +\norm{\tau_i}^2\\
    &\leq \sum\limits_{j=1}^T\alpha_j^2\norm{\tau_j}^2 + 2\sum_{j\neq k}\alpha_j\alpha_k\left|\left<\tau_j, \tau_k\right>\right| + 2\sum\limits_{j=1}^T\alpha_j\left|\left<\tau_j, \tau_i\right>\right| + \norm{\tau_i}^2\\
    &= \sum\limits_{j=1}^T\alpha_j^2\norm{\tau_j}^2 + 2\sum_{j\neq k}\alpha_j\alpha_k\left|\left<\tau_j, \tau_k\right>\right| + 2\sum\limits_{j\neq i}^T\alpha_j\left|\left<\tau_j, \tau_i\right>\right| + 2\alpha_i\norm{\tau_i}^2 + \norm{\tau_i}^2\\
    &\leq \left(\sum\limits_{j=1}^T\alpha_j^2+2\alpha_i+1\right)C + 2\left(\sum_{j\neq k}\alpha_j\alpha_k+\sum\limits_{j\neq i}^T\alpha_j\right)\epsilon C.
\end{align*}
To further upper bound the coefficients of the above two terms, we have
\begin{equation*}
    \sum\limits_{j=1}^T\alpha_j^2+2\alpha_i+1 \leq \sum_{j=1}^T \alpha_j + 2 + 1 = 4,
\end{equation*}
and
\begin{equation*}
    2\left(\sum_{j\neq k}\alpha_j\alpha_k+\sum\limits_{j\neq i}^T\alpha_j\right) \leq 2\left(\left(\sum_{j=1}^T\alpha_j\right)\left(\sum_{k=1}^T\alpha_k\right)+\sum\limits_{j =1}^T\alpha_j\right) = 4.
\end{equation*}
To conclude, we have 
\begin{equation*}
           \gL_i(\theta^*) - \mathcal{L}_i(\theta_i) \leq \frac{L_iC}{2}  \left(1+2\alpha_i+\sum\limits_{j=1}^T\alpha_j^2\right) + \frac{L_i \epsilon C}{2}\left(\sum_{j\neq k}\alpha_j\alpha_k+\sum\limits_{j\neq i}^T\alpha_j\right) \leq 2L_i C(1 + \epsilon).\qedhere
\end{equation*}
\end{proof}


\negate*
\begin{proof}
First, note that 
\begin{equation*}
    \gL_j(\theta_i^*) - \gL_j(\theta_0) \leq \gL_j(\theta_i^*) - \gL_j(\theta_j) + \gL_j(\theta_j)- \gL_j(\theta_0) \leq \gL_j(\theta_i^*) - \gL_j(\theta_j) + |\gL_j(\theta_0) - \gL_j(\theta_j)|
\end{equation*}
We will upper bound the last two terms separately. To bound $\gL_j(\theta_i^*) - \gL_j(\theta_j)$, note that $\|\theta_i^* - \theta_j\|^2 = \|\alpha_i\tau_i + \tau_j\|^2 \leq \left(\alpha_i \|\tau_i\| + \|\tau_j\|\right)^2 \leq 4C$, due to the local smoothness of $\gL_j$ around $\theta_j$ and the fact that $\partial\gL_j(\theta_j)/\partial \theta = \mathbf{0}$, we have
\begin{equation*}
    \gL_j(\theta_i^*) - \gL_j(\theta_j) \leq \frac{L_j}{2}\|\theta_i^* - \theta_j\|^2 = \frac{L_j}{2}\|\alpha_i \tau_i + \tau_j\|^2\leq \frac{L_j}{2}\left(\alpha_i^2 C + 2\langle \tau_i,\tau_j\rangle + C\right) \leq L_j C (1 + \epsilon).
\end{equation*}
Similarly, for the second term, we have 
\begin{equation*}
    |\gL_j(\theta_0) - \gL_j(\theta_j)| \leq \frac{L_j}{2}\|\theta_0 - \theta_j\|^2 \leq \frac{L_jC}{2}.
\end{equation*}
Combine both above, leading to
\begin{equation*}
    \gL_j(\theta_i^*) - \gL_j(\theta_0)  \leq L_j C (1 + \epsilon) + \frac{L_jC}{2} = L_jC \left(\frac{3}{2} + \epsilon\right),
\end{equation*}
as desired.
\end{proof}

\generalize*
\begin{proof}
Let $i^* = \argmax_{i\in[T]}\langle \tau_\mathrm{tar}, \tau_i\rangle$ and choose $\alpha_{i^*} = 1$, $\alpha_j = 0,~\forall j\neq i^*$. Clear $\langle \tau_\mathrm{tar}, \tau_{i^*}\rangle \geq \beta C$ and $\theta^* = \theta_0 + \tau_{i^*}$. It is easy to check that $\|\theta^* - \theta_\mathrm{tar}\| \leq 4C$ so by the local smoothness assumption of $\gL_\mathrm{tar}$, we have
\begin{align*}
    \gL_\mathrm{tar}(\theta^*) - \gL_\mathrm{tar}(\theta_\mathrm{tar}) &\leq \frac{L_\mathrm{tar}}{2}\|\theta^* - \theta_\mathrm{tar}\|^2 \\
    &= \frac{L_\mathrm{tar}}{2}\|\tau_{i^*} - \tau_\mathrm{tar}\|^2 \\
    &\leq \frac{L_\mathrm{tar}}{2}\left( C - 2\langle \tau_\mathrm{tar}, \tau_{i^*}\rangle + C\right) \\
    &\leq L_\mathrm{tar}C(1- \beta).\qedhere
\end{align*}
\end{proof}

\section{Additional Related Work}
\label{sec:ta-methods}

\subsection{Single-task Merging Methods}
\label{sec:single-task}
Prior to Task Arithmetic \citep{ilharco2022editing}, researchers discussed how to combine models fine-tuned on the same task, with some minor differences due to hyperparameter changes, as an alternative to ensembles, starting with model soup \citep{pmlr-v162-wortsman22a}. Since fine-tuned models capture more domain-specific skills while pretrained models contain more generic knowledge, WiSE-FT \citep{wortsman2021robust} proposed merging the pretrained model and the fine-tuned model via linear interpolation, achieving balanced or even optimal performance on both in-domain and out-of-distribution generalization metrics. \citet{izmailov2018averaging} introduced stochastic weight averaging, which includes intermediate checkpoints before model convergence for model merging. Several close variants, such as exponentially moving averaging \citep{szegedy2016rethinking} and LAtest Weight Averaging \citep{kaddour2022stop, sanyal2023early}, have been explained theoretically under a unified framework \citep{wang2024unified}.

\subsection{Multi-task Merging Methods}
The major difference from \Cref{sec:single-task} is that all methods discussed in this subsection focus on the setting that one pretrained model is fine tuned on many different tasks. Task arithmetic \citep{ilharco2022editing} can be seen as the generalization of the single-task model merging method, model soup \citep{pmlr-v162-wortsman22a}, where task vectors are simply averaged. In \citet{ilharco2022editing}, however, the scaling coefficients $\alpha$ are allowed to be tuned. Since then, several ideas have been proposed to improve task arithmetic. First, since tuning $\alpha$ is time-consuming, popular approaches such as Fisher merging \citep{matena2022merging}, RegMean \citep{jin2022dataless}, AdaMerging \citep{yang2023adamerging}, Evol \citep{akiba2024evolutionary} aim to find better methods to automatically adjust scaling coefficients for improved task arithmetic performance. Second, instead of using standard fine-tuning to obtain $\tau$, alternative fine-tuning methods, such as tangent space fine-tuning \citep{ortiz2024task} and parameter-efficient fine-tuning methods \citep{zhang2023composing, tang2023parameter, stoica2024model}, are employed in task arithmetic to disentangle task information for better merging. Third, to reduce task vector conflicts, task vectors can be sparsified into different subspaces by localization \citep{he2024localize, yadav2024ties, yu2024language, davari2025model, tang2023concrete, wang2024localizing}, i.e., by masking out useless parameters for each task. Note this category of work has the additional benefit on memory efficiency due to the storage of sparse model weights. Finally, inspired by the Mixture-of-Experts \citep{shazeer2017outrageously} mechanism, task vector merging performance can be enhanced by learned routers that dynamically merge task-specific and task-shared information \citep{lu2024twin, tang2024merging}. For more details on the latest task arithmetic methods and their applications, we refer readers to the model merging survey \citep{yang2024model}. 

\section{Illustration of Memory Efficiency of Task Vector Bases}

In \Cref{fig:memo_comparison}, we illustrate the memory advantage of using task vector bases methods, from the perspective of the OOD generalization application. First, in the left figure, we observe that all storage space is used up with $T$ task vectors before clustering. In the second step, since using $m$ bases provides comparable accuracy performance to using $T$ vectors, we only need to store $m$ copies of the checkpoint. When new tasks are very different from the existing memory, as shown in \Cref{tab:basis_arithmetic}, we allow $T - m$ new bases to store additional information. Thus, by reducing redundant task vectors to task vector bases, we save space for storing new vectors during domain generalization. This approach is especially beneficial when $T$ original task vectors are highly similar, and future target tasks are orthogonal to the old knowledge.

\begin{figure}[!ht]
    \centering
\includegraphics[width=0.5\linewidth]{figure/memory_illustration.pdf}
    \caption{Workflow of using task vector bases for generation. The light gray box represents all available memory and we assume $T$ task vectors uses up the full memory as shown in the left. Colorful arrows represents different task vector bases. We use clustering and merging from left to middle figure for the bases creation step in \Cref{alg:task_vector_basis}, and use the OOD generalization rule in \Cref{tab:basis_arithmetic} from middle to right which involves saving new bases into the memory. If we simply use the left figure for OOD generalization, we do not allow additional new bases which can lead to trivial performance based on \Cref{tab:continual_dtd}.}
    \label{fig:memo_comparison}
\end{figure}



\section{Details for Computer Vision Experiments}
\label{sec:cv_details}
\subsection{Task Vector Norm $C$}
\label{sec:norm_details}
\begin{table}[!ht]
\centering
\begin{tabular}{l l c c}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Ratio}\% & \textbf{Norm Task Vector} \\
\midrule
laion2b\_e16 & MNIST     & 0.46 & 2.18 \\
             & EuroSAT   & 0.45 & 2.16 \\
             & Cars      & 0.54 & 2.52 \\
             & DTD       & 0.39 & 1.81 \\
             & GTSRB     & 0.49 & 2.32 \\
             & RESISC45  & 0.54 & 2.55 \\
             & SUN397    & 0.65 & 3.03 \\
             & SVHN      & 0.56 & 2.64 \\
\midrule
laion2b\_s34b\_b79k & MNIST     & 0.42 & 2.30 \\
                    & EuroSAT   & 0.42 & 2.27 \\
                    & Cars      & 0.48 & 2.59 \\
                    & DTD       & 0.33 & 1.79 \\
                    & GTSRB     & 0.45 & 2.44 \\
                    & RESISC45  & 0.48 & 2.63 \\
                    & SUN397    & 0.58 & 3.18 \\
                    & SVHN      & 0.51 & 2.76 \\
\bottomrule
\end{tabular}
\caption{Ratio and Norm Task Vector for Different Models and Datasets}
\label{tab:ratio_norm_task}
\end{table}

Two openclip checkpoints are details can be found from \url{https://github.com/mlfoundations/open\_clip/blob/main/docs/PRETRAINED.md} where laion2b\_s34b\_b79k is reported to be trained with larger batch size and learning rate, while two models share the same training data LAION-2B \citep{schuhmann2022laionb}. In \Cref{tab:ratio_norm_task}, we reported the task vector norm and the ratio of task vector norm over the pretrained model norm, which is very small across datasets and models.

We elaborate on the connection of small task vector norm $C$ requirement with previous literature. \citet{ilharco2022editing} in its Figure 7 demonstrates that the performance of merging task vectors derived from intermediate checkpoints, far before model convergence, is close to the performance of merging converged task vectors. These intermediate checkpoints typically have smaller norms due to fewer optimization steps, so a small $C$ appears sufficient for the success of task addition. On the other hand, Figure 6 of \citet{ilharco2022editing} also indicated that a smaller learning rate is more important for task addition than for standard single-task fine-tuning, which implies that a smaller $C$ is also necessary. 

\subsection{Identifying Task Groups with Less Training}
\label{sec:intermediate}
\begin{figure}[!ht]
    \centering
    % Four images in a single row
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \text{\scriptsize Step 0}
        \includegraphics[width=\textwidth]{figure/step0_long.pdf} % Replace with your image path
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \text{\scriptsize Step 1}
        \includegraphics[width=\textwidth]{figure/step1_long.pdf} % Replace with your image path
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \text{\scriptsize Epoch 0}
        \includegraphics[width=\textwidth]{figure/epoch0_long.pdf} % Replace with your image path
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \text{\scriptsize Epoch 1}
        \includegraphics[width=\textwidth]{figure/epoch1_long.pdf} % Replace with your image path
    \end{minipage}
    \caption{Evolution of the similarity matrix for eight vision tasks during fine-tuning, showing the first two steps of batch updates and two full epochs for each task.}
    \label{fig:4_images_inline}
\end{figure}

When we do not have access to all single-task task vectors at the beginning and the number of tasks $T$ is large, the most time-consuming step in generating the task similarity matrix, as shown in \Cref{fig:nlp_cluster}, is fine-tuning on each task. In \Cref{fig:4_images_inline}, we observe how the task similarity matrix evolves as we increase the number of training steps on an 8-task standard vision benchmark. If we apply spectral clustering to all four matrices—Step 0, Step 1, and Epoch 0—share the same clustering result: [(MNIST, SVHN), (SUN397, DTD), (EuroSAT, RESISC45), (Cars), (GTSRB)]. Starting from Epoch 1, the clusters converge to the same result: [(MNIST, SVHN, GTSRB), (DTD), (EuroSAT, RESISC45), (SUN397), (Cars)]. All matrices in \Cref{fig:4_images_inline} exhibit similar patterns, especially for the two major task groups: digit classification for MNIST, SVHN, and satellite image classification for EuroSAT and RESISC45. Over time, the model learns new skills, such as how traffic sign recognition in GTSRB involves digit identification, similar to MNIST and SVHN. However, these intermediate checkpoints, even with just one batch or one epoch update, are able to reflect useful information about task vectors for creating task bases, eliminating the need for converged task models in \Cref{tab:basis_arithmetic}.


\subsection{Bases Addition}
\label{sec:cv_exp_details}

We report the full table of \Cref{tab:TA_mergeB} in \Cref{tab:TA_mergeB_details} for all possible partitions of MNIST, SVHN, EuroSAT, and RESISC45. We can draw several conclusions. First, our natural partition is almost always among the best for a 2-2 partition. In the context of multitask learning, the common wisdom is that similar tasks can be trained to leverage mutually shared information for better performance \citep{caruana1993multitask}. Additionally, by \cref{thm:task-addition}, at the second $\texttt{merge}_\text{out}$ step, it is also beneficial to combine two task vector bases with fewer conflicts or smaller $\epsilon$. These two conditions are automatically satisfied by the clustering process. Second, MS-ER is consistently better than the average of all $k = 2$ and $k \in [1,4]$ partitions. By grouping, we can achieve better-than-average addition performance while using a small memory budget. Note that there is no consistent partition that always achieves the best performance for all $\texttt{merge}_\text{in}$ methods.


In \Cref{tab:mean_mergeA_detail} and \Cref{tab:merge_twice_detail}, we cannot directly use non-dataless merging methods for the outer merging method $\texttt{merge}_\text{out}$ as we pointed out in \Cref{sec:bases_arithmetic}. For example, after merging MS at step one, it is unnecessary to jointly fine-tune MS from scratch at step two or recompute the same Fisher-based weights again. For the remaining dataless methods, we report the merging performance for different $\texttt{merge}_\text{out}$ methods, while fixing Mean as $\texttt{merge}_\text{in}$ in \Cref{tab:mean_mergeA_detail}, and fixing both steps of merging methods across different sizes of CLIP architectures in \Cref{tab:merge_twice_detail}. We observe some similar patterns as in \Cref{tab:TA_mergeB_details} across merging methods and architectures. Note that when the task number is $4$, if we use the merging algorithm Mean twice, task vector weight when $k = 1$ will be the same as all other 2-2 partitions. Therefore, we see the same numbers for Mean rows in these situations. Although in both tables, MS-ER is not always the best clustering among all 2-2 partitions, we can still see that the MS-ER natural partition is better than average metrics. Finally, by comparing each row, we can say the absolute task vector bases method performance depends on the effectiveness of chosen merging methods. This is more obvious in \Cref{tab:TA_mergeB_details} where joint fine tuning methods including MTL and Tangent surpass other basic methods like Mean and Topk by a large margin.


\begin{table*}[!ht]
\caption{Merging MNIST, SVHN, EuroSAT, RESISC45 fixing Task Arithmetic as outer merging method $\texttt{merge}_\text{out}$ for different inner merging methods $\texttt{merge}_\text{in}$. We included all possible partitions in this table, where each dataset is represented by its first letter, and the partition can be reflected by the location of -. For example, cluster algorithms produces the grouping of MS-ER. When $k = 1$, it reduces to one step of merging methods listed at the leftmost column of $\texttt{merge}_\text{in}$, and when $k = 4$, it reduces to one step of $\texttt{merge}_\text{out}$ applied to all task vectors. We bold the natural clustering \colorbox{mygray}{MS-ER} if this cell value of Abs. Avg. Acc. is greater than both Avg. $k=2$ and $k \in [1,4]$.}
\label{tab:TA_mergeB_details}
\centering
\scalebox{0.625}{
\begin{tabular}{@{}lccccccccccccccccc@{}}
\toprule
     & \multicolumn{2}{c}{Avg.} & $k = 1$ & \multicolumn{7}{c}{$k = 2$}                             & \multicolumn{6}{c}{$k = 3$}                           & $k = 4$   \\ \cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-11}\cmidrule(lr){12-17}\cmidrule(lr){18-18}
     $\texttt{merge}_\text{in}$ & $k = 2$    & $k \in [1,4]$   & MSER  & \cellcolor{mygray}{MS-ER} & MR-ES & \multicolumn{1}{c|}{ME-SR} & M-ESR & S-MER & E-MSR & R-MSE & M-S-ER & S-R-EM & M-R-ES & E-S-RM & M-E-RS & E-R-MS & M-S-E-R \\ \midrule
MTL  & 95.45    & 93.85         & 98.05 & \cellcolor{mygray}{\textbf{95.52}} & 95.27 & \multicolumn{1}{c|}{94.40} & 97.26 & 95.99 & 96.60 & 93.12 & 91.74  & 90.36  & 92.25  & 92.81  & 93.38  & 92.42  & 88.62   \\
TIES & 88.68    & 88.48         & 90.74 & \cellcolor{mygray}{\textbf{90.43}} & 90.17 & \multicolumn{1}{c|}{90.36} & 83.69 & 86.26 & 89.70 & 90.17 & 83.27  & 88.83  & 86.32  & 90.10  & 88.00  & 90.58  & 88.62   \\
Fisher &
  82.41 & 83.67 & 72.88 & \cellcolor{mygray}{\textbf{86.37}} & 81.45 & \multicolumn{1}{c|}{84.21} & 73.60 & 83.92 & 85.72  & 81.62 & 82.33 & 88.36 & 79.63 & 90.51 & 85.78 & 90.09 &
  88.62 \\
Tangent &
  88.50 & 88.75 & 88.61 & \cellcolor{mygray}{\textbf{90.05}} & 90.05 & \multicolumn{1}{c|}{90.05} & 85.07 & 90.36 & 83.56 & 90.38 & 89.39 & 92.15 & 88.75 & 88.57 & 85.23 & 89.00 & 90.01\\
Topk &
  82.17 & 83.05 & 85.87 & \cellcolor{mygray}{\textbf{85.69}} & 86.16 & \multicolumn{1}{c|}{85.95} & 75.50 & 78.55 & 81.20 & 82.14 & 76.43 & 84.49 & 79.97 & 87.83 & 84.01 & 83.34 
 & 88.62 \\
Mean & 85.65    & 86.44         & 87.41 & \cellcolor{mygray}{\textbf{88.65}} & 88.65 & \multicolumn{1}{c|}{88.65} & 79.54 & 83.13 & 84.26 & 86.68 & 83.14  & 88.34  & 85.39  & 88.75  & 86.25  & 89.14  & 88.62   \\ \bottomrule
\end{tabular}
}
\end{table*}



\begin{table}[!ht]
\caption{Merging MNIST, SVHN, EuroSAT, RESISC45 fixing model soup as inner merging method $\texttt{merge}_\text{in}$ for different outer merging methods $\texttt{merge}_\text{out}$.}
\label{tab:mean_mergeA_detail}
\centering
\scalebox{0.625}{
\begin{tabular}{@{}lccccccccccccccccc@{}}
\toprule
     & \multicolumn{2}{c}{Avg.} & $k = 1$ & \multicolumn{7}{c}{$k = 2$}                             & \multicolumn{6}{c}{$k = 3$}                           & $k = 4$   \\ \cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-11}\cmidrule(lr){12-17}\cmidrule(lr){18-18}
     $\texttt{merge}_\text{out}$ & $k = 2$    & $k \in [1,4]$   & MSER  & \cellcolor{mygray}{MS-ER} & MR-ES & \multicolumn{1}{c|}{ME-SR} & M-ESR & S-MER & E-MSR & R-MSE & M-S-ER & S-R-EM & M-R-ES & E-S-RM & M-E-RS & E-R-MS & M-S-E-R \\ \midrule
Mean &
  84.74 & 85.56 & 87.42 & \cellcolor{mygray}{\textbf{87.42}} & 87.42 & \multicolumn{1}{c|}{87.42} & 79.50 & 83.13 & 84.10 & 84.22 & 83.12 & 86.61 & 84.22 & 88.18 & 85.91 & 87.25 & 87.42 \\
TIES & 85.57    & 86.48         & 83.46 & \cellcolor{mygray}{\textbf{89.60}} & 89.26 & \multicolumn{1}{c|}{89.02} & 79.86 & 82.50 & 84.56 & 84.18 & 83.04 & 87.67 & 85.61 & 89.48 & 87.34 & 90.90 & 90.70   \\
Topk & 68.24    & 68.13         & 68.71 & \cellcolor{mygray}{\textbf{69.01}} & 68.12 & \multicolumn{1}{c|}{67.96} & 68.71 & 69.62 & 67.63 & 66.65 & 69.14 & 67.88 & 67.60 & 68.23 & 67.82 & 67.09 & 67.84 \\ \bottomrule
\end{tabular}
}
\end{table}


\begin{table}[!ht]
\caption{Merging MNIST, SVHN, EuroSAT, RESISC45 with the same $\texttt{merge}_\text{in}$ and $\texttt{merge}_\text{out}$ across various CLIP architectures.}
\label{tab:merge_twice_detail}
\centering
\scalebox{0.625}{
\begin{tabular}{@{}lccccccccccccccccc@{}}
\toprule
     & \multicolumn{2}{c}{Avg.} & $k = 1$ & \multicolumn{7}{c}{$k = 2$}                             & \multicolumn{6}{c}{$k = 3$}                           & $k = 4$   \\ \cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-11}\cmidrule(lr){12-17}\cmidrule(lr){18-18}
     ViT/B-16 & $k = 2$    & $k \in [1,4]$   & MSER  & \cellcolor{mygray}{MS-ER} & MR-ES & \multicolumn{1}{c|}{ME-SR} & M-ESR & S-MER & E-MSR & R-MSE & M-S-ER & S-R-EM & M-R-ES & E-S-RM & M-E-RS & E-R-MS & M-S-E-R \\ \midrule
Mean &
  86.76 & 87.48 & 89.23 & \cellcolor{mygray}{\textbf{89.23}} & 89.23 & \multicolumn{1}{c|}{89.23} & 82.97 & 85.41 & 84.25 & 86.99 & 85.57 & 89.25 & 87.05 & 89.01 & 87.10 & 88.39 & 89.23 \\
TIES & 84.14    & 86.21         & 89.23 & \cellcolor{mygray}{\textbf{90.65}} & 90.73 & \multicolumn{1}{c|}{90.83} & 78.09 & 75.97 & 79.88 & 82.82 & 81.35 & 89.28 & 87.66 & 88.85 & 86.55 & 89.75 & 91.55   \\
Topk & 71.82    & 71.57         & 72.98 & \cellcolor{mygray}{\textbf{72.50}} & 72.26 & \multicolumn{1}{c|}{72.14} & 71.98 & 71.62 & 72.03 & 70.21 & 71.45 & 70.47 & 70.81 & 71.56 & 71.86 & 70.96 & 70.75  \\ \midrule
ViT/B-32 & $k = 2$    & $k \in [1,4]$   & MSER  & \cellcolor{mygray}{MS-ER} & MR-ES & \multicolumn{1}{c|}{ME-SR} & M-ESR & S-MER & E-MSR & R-MSE & M-S-ER & S-R-EM & M-R-ES & E-S-RM & M-E-RS & E-R-MS & M-S-E-R \\ \midrule
Mean &
  84.74 & 85.56 & 87.42 & \cellcolor{mygray}{\textbf{87.42}} & 87.42 & \multicolumn{1}{c|}{87.42} & 79.50 & 83.13 & 84.10 & 84.22 & 83.12 & 86.61 & 84.22 & 88.18 & 85.91 & 87.25 & 87.42 \\
TIES & 88.24   & 88.81         & 85.31 & \cellcolor{mygray}{\textbf{89.28}} & 89.35 & \multicolumn{1}{c|}{90.32} & 82.90 & 86.26 & 91.36 & 88.19 & 85.75 & 89.92 & 87.64 & 91.96 & 90.47 & 92.78 & 90.70   \\
Topk & 68.86    & 68.57         & 70.60 & \cellcolor{mygray}{68.56} & 69.23 & \multicolumn{1}{c|}{69.37} & 69.62 & 69.69 & 68.33 & 67.21 & 67.84 & 67.96 & 68.04 & 68.36 & 68.59 & 67.04 & 68.04  \\ \midrule
ViT/L-14 & $k = 2$    & $k \in [1,4]$   & MSER  & \cellcolor{mygray}{MS-ER} & MR-ES & \multicolumn{1}{c|}{ME-SR} & M-ESR & S-MER & E-MSR & R-MSE & M-S-ER & S-R-EM & M-R-ES & E-S-RM & M-E-RS & E-R-MS & M-S-E-R \\ \midrule
Mean &
  91.56 & 91.92 & 92.98 & \cellcolor{mygray}{\textbf{92.98}} & 92.98 & \multicolumn{1}{c|}{92.98} & 88.75 & 92.40 & 89.11 & 91.70 & 91.57 & 93.94 & 91.41 & 92.84 & 90.19 & 92.06 & 92.98 \\
TIES & 90.35    & 91.63         & 92.98 & \cellcolor{mygray}{\textbf{94.17}} & 94.15 & \multicolumn{1}{c|}{94.67} & 86.82 & 88.61 & 85.88 & 88.12 & 91.55 & 94.53 & 91.51 & 93.26 & 90.28 & 92.81 & 95.18   \\
Topk & 79.36    & 79.33         & 80.02 & \cellcolor{mygray}{\textbf{79.88}} & 79.40 & \multicolumn{1}{c|}{79.48} & 78.87 & 79.21 & 80.12 & 78.56 & 78.97 & 78.57 & 78.81 & 79.45 & 79.46 & 80.12 & 79.05  \\ \bottomrule
\end{tabular}
}
\end{table}


\subsection{Bases OOD Generalization}

\begin{figure}[!ht]
    \centering
\includegraphics[width=0.8\linewidth]{figure/top1_continual_similarity_short.pdf}
    \caption{Average cosine similarity of held-out task vectors with the most similar task vector basis for all possible MSER partitions in the setting of \Cref{tab:TA_mergeB}.}
    \label{fig:continue_similarity}
\end{figure}

We report the cosine similarity of the most similar task vector bases among all 15 MSER partitions for each dataset and each $\texttt{merge}_\text{in}$ method in \Cref{fig:continue_similarity}. Following our intuition, for most merging methods, GTSRB has the highest similarity due to the overlap between traffic sign identification and digit classification. In contrast, other datasets such as Cars, DTD, and SUN397 do not have a strong relationship with the stored task vector bases. Based on \Cref{tab:basis_arithmetic}, it is more recommended to create new task vector bases for each of these datasets.

We include the expanded version of \Cref{tab:bases_generalization} in \Cref{tab:continual_detail} and \Cref{tab:continual_dtd}. When the target task is GTSRB in \Cref{tab:continual_detail}, we can observe several patterns. First, in most cases, GTSRB identifies bases containing at least one of MNIST or SVHN. Second, we observe that applying another merging method $\texttt{merge}_\text{out}$ is necessary. In our case, we use task arithmetic and report the performance with and without tuning $\alpha$. Without weight tuning, the generalization performance can be worse than the pretrained model performance. Finally, we also added the MSER column, where only 1 final merged checkpoint is stored in memory. Compared to using bases, we can further improve domain generalization performance by using this well-combined basis that contains all old task information. However, due to the significant loss of unlearning capability shown in \Cref{tab:bases_negation}, we do not recommend to only save MSER as basis. The equivalent solution for improved OOD generalization performance is to further merge MS and ER only in the OOD generalization step.

In \Cref{tab:continual_dtd}, however, when we replace the target task with DTD, we do not observe significant improvement using any of the MSER partitions. This is expected because, first, based on the clustering results in \Cref{sec:intermediate}, DTD is not grouped with any of the MSER tasks, and second, in \Cref{fig:continue_similarity}, DTD’s cosine similarity score is consistently low.

\begin{table}[!ht]
\caption{Detailed version of \Cref{tab:bases_generalization} when the target task is GTSRB.}
\label{tab:continual_detail}
\centering
\begin{tabular}{lccccccccc}
\toprule
 $\texttt{merge}_\text{in}$ &
  \begin{tabular}[c]{@{}c@{}}Success Rate\\ of finding M/S\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Avg.\\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Avg. \\ $\alpha$ tuned\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MS-ER \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}\cellcolor{mygray}{MS-ER }\\ \cellcolor{mygray}{$\alpha$ tuned}\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}M-S-E-R \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}M-S-E-R \\ $\alpha$ tuned\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSER \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSER \\ $\alpha$ tuned\end{tabular} \\ \midrule
MTL     & 14/15 & 28.72 & 43.21 & 23.68 & \cellcolor{mygray}{\textbf{41.02}} & 22.69 & 37.91 & 30.61 & 44.77 \\
TIES    & 15/15 & 33.43 & 41.16 & 38.85 & \cellcolor{mygray}{\textbf{43.15}} & 22.69 & 37.91 & 42.94 & 45.59 \\
Fisher  & 15/15 & 34.32 & 40.76 & 36.08 & \cellcolor{mygray}{\textbf{41.31}} & 22.69 & 37.91 & 38.35 & 41.80 \\
Tangent & 14/15 & 40.35 & 40.65 & 38.93 & \cellcolor{mygray}{39.21} & 40.67 & 41.59 & 41.77 & 41.77 \\
Topk    & 15/15 & 33.82 & 40.14 & 40.37 & \cellcolor{mygray}{\textbf{40.37}} & 22.69 & 37.91 & 43.68 & 43.68 \\
Mean    & 15/15 & 38.79 & 41.85 & 38.17 & \cellcolor{mygray}{\textbf{41.97}} & 22.69 & 37.91 & 44.00 & 44.13 \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[!ht]
\caption{Detailed version of \Cref{tab:bases_generalization} when the target task is DTD.}
\label{tab:continual_dtd}
\centering
\begin{tabular}{lcccccccc}
\toprule
 $\texttt{merge}_\text{in}$ &
  \begin{tabular}[c]{@{}c@{}}Avg. \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Avg. \\ $\alpha$ tuned\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MS-ER \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MS-ER\\ $\alpha$ tuned\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}M-S-E-R \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}M-S-E-R \\ $\alpha$ tuned\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSER \\ $\alpha = 1$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSER \\ $\alpha$ tuned\end{tabular} \\ \midrule
MTL          & 33.13 & 54.75 & 38.61 & 54.73 & 46.27 & 55.00 & 27.18 & 54.73 \\
TIES         & 46.08 & 54.82 & 48.35 & 54.73 & 46.27 & 55.00 & 45.85 & 54.73 \\
Fisher       & 46.86 & 54.84 & 48.08 & 54.89 & 46.27 & 55.00 & 42.65 & 54.73 \\
Tangent & 52.94 & 55.08 & 51.70 & 55.10 & 50.85 & 55.42 & 41.77 & 41.77 \\
Topk         & 48.72 & 54.89 & 54.09 & 55.00 & 46.27 & 55.00 & 53.61 & 54.78 \\
Mean         & 49.09 & 54.75 & 48.19 & 54.73 & 46.27 & 55.00 & 50.53 & 54.73 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Model Merging Methods Settings}


\begin{figure}[!ht]
    \centering
\includegraphics[width=0.5\linewidth]{figure/tangent_matrix.pdf}
    \caption{Cosine similarity matrix when we use tangent linearized fine tuning.}
    \label{fig:tangent_matrix}
\end{figure}


For all model merging methods in \Cref{tab:TA_mergeB}, we follow the exact same hyperparameters as in the corresponding model merging method literature. We add some additional comments below.

\paragraph{Multitask Learning}
We keep all the fine-tuning hyperparameters the same as in Task Arithmetic \citep{ilharco2022editing}, with the only difference being that when we jointly train on more than one dataset, the total number of training epochs is the sum of all epochs used in \citet{ilharco2022editing} for training independently on each dataset. Therefore, the total training time spent on all data points in the 8 datasets is the same for both independent and multi-task training.

\paragraph{TIES} We uses the dataless version of TIES \citep{yadav2024ties}, so $\alpha$ is always set to be $0.4$ as recommended.

\paragraph{Tangent} For all tangent method rows in our tables, we jointly tune the corresponding datasets in one cluster, similar to MTL, and replace the standard non-linear fine-tuning with the linearized fine-tuning methods in \citep{ortiz2024task}. For fine-tuning-based methods in model merging, we note that they can still be used for clustering. For example, when we use \Cref{fig:tangent_matrix} as the affinity matrix input, the clustering result is: [(MNIST, SVHN), (SUN397, DTD, Cars), (EuroSAT), (RESISC45), (GTSRB)], where we can still observe alignment with both intermediate clustering results and the final converged clustering results in \Cref{sec:intermediate}. Additionally, the same similarity metric is used for LoRA modules in other contexts, as seen in \citet{ostapenko2024towards}.

\paragraph{Topk} is inspired from the dataless version of \citet{he2024localize} by only keeping the top 5\% magnitude of the vector entries. To merge the sparse vectors, the update rule of the mask for task $i$ is $\gamma_i \leftarrow \gamma_i/T$.


\section{Details for Language Experiments}
\label{sec:nlp_details}


\subsection{Merging Performance on the Medium-Scale Benchmark}
\label{sec:medium_nlp_details}
We include the full version of \Cref{tab:medium_nlp} in \Cref{sec:medium_nlp_details}. Since we employ a strong $\texttt{merge}$ algorithm, Localize-and-Stitch (L\&S), within our framework, we observe the advantage of the two bases L\&S versions over several important baselines, including Task Arithmetic, TIES \citep{yadav2024ties}, Fisher merging \citep{matena2022merging}, RegMean \citep{jin2022dataless}, AdaMerging \citep{yang2023adamerging}, and Consensus methods \citep{wang2024localizing}. Even though Mask Sharing achieves the lowest performance among all L\&S variants due to its focus on memory efficiency, its multitask performance still outperforms all other non-L\&S baselines.

\begin{table}[tb]
    \centering
    \caption{Model merging generalization comparison full table expanded on \Cref{tab:medium_nlp}, except for last two shaded rows, all numbers are from \citet{he2024localize}. We only report data-based merging algorithms for the best merging performance.}
    \label{tab:medium_nlp_details}
    \scalebox{0.88}{
        \begin{tabular}{l|cccccccccccc|c}
            \toprule
            Task                                                                                      & SST-2 & CR    & MR    & MPQA  & TREC  & SUBJ  & QNLI  & SNLI  & MNLI  & RTE   & MRPC$^\dagger$  & QQP   & Avg. \\
            \midrule
            Task Arithmetic & 88.5 & 88.2 & 80.3 & 82.9 & 32.0 & 61.0 & 62.0 & 56.1 & 49.5 & 65.6 & 82.8 & 62.3 & 67.5  \\
            TIES  & 88.6 & 88.0 & 85.2 & 83.5 & 22.6 & 48.2 & 54.8 & 35.9 & 39.7 & 59.4 & 79.4 & 60.3 & 62.1  \\
            Fisher merging                                                   & 90.0 & 89.8 & 83.7 & 75.8 & 26.0 & 54.6 & 54.2 & 72.5 & 65.2 & 65.6 & 83.3 & 67.7 & 69.0          \\
            RegMean                                                            & 89.7 & 89.7 & 84.7 & 82.6 & 73.0 & 79.1 & 55.9 & 68.3 & 56.8 & 63.8 & 79.4 & 64.2 & 73.9          \\
            AdaMerging                                                      & 85.0 & 86.1 & 77.8 & 81.5 & 23.0 & 59.5 & 61.2 & 54.1 & 40.4 & 54.7 & 82.2 & 58.8 & 63.7          \\
            Consensus TA  & 89.2 & 89.4 & 86.6 & 88.2 & 37.6 & 59.6 & 71.4 & 66.5 & 52.0 & 63.0 & 88.1 & 66.4 & 71.5\\
            Consensus TIES & 89.8 & 89.3 & 84.7 & 86.2 & 31.4 & 63.1 & 68.9 & 60.4 & 46.1 & 63.0 & 86.2 & 64.4 & 69.5 \\
            \midrule
            Localize-and-Stitch                                                              & 89.6 & 89.6 & 84.9 & 82.8 & 78.2 & 82.0 & 73.4 & 62.1 & 58.0 & 63.3 & 82.0 & 65.1 & 75.9 \\
            \cellcolor{mygray}{\quad - Mask Sharing} & \cellcolor{mygray}{90.4} & \cellcolor{mygray}{89.4} & \cellcolor{mygray}{84.8} & \cellcolor{mygray}{76.4} & \cellcolor{mygray}{89.4} & \cellcolor{mygray}{90.9} & \cellcolor{mygray}{53.0} & \cellcolor{mygray}{58.2} & \cellcolor{mygray}{51.9} & \cellcolor{mygray}{59.2} & \cellcolor{mygray}{77.7} & \cellcolor{mygray}{67.3} & \cellcolor{mygray}{74.1}  \\
            \cellcolor{mygray}{\quad - Stitch Twice} & \cellcolor{mygray}{88.3} & \cellcolor{mygray}{88.5} & \cellcolor{mygray}{83.5} & \cellcolor{mygray}{78.5} & \cellcolor{mygray}{78.6} & \cellcolor{mygray}{85.1} & \cellcolor{mygray}{54.3} & \cellcolor{mygray}{74.1} & \cellcolor{mygray}{62.8} & \cellcolor{mygray}{67.5} & \cellcolor{mygray}{81.4} & \cellcolor{mygray}{74.2} & \cellcolor{mygray}{\textbf{76.4}} 
            \\
            \bottomrule
        \end{tabular}}
\end{table}

\subsection{Hyperparameters}
We follow the fine-tuning setting of \citet{gao2020making} by reformulating a 64-shot classification problem into a masked language modeling problem with appropriate templates and prompts.

During fine-tuning, we fix the language modeling head and all embedding layers. For other parameters, we use the AdamW \citep{loshchilov2017decoupled} optimizer and a linear learning rate scheduler with a learning rate of $2 \times 10^{-5}$, training for 10 epochs with a batch size of 4. For each class label in the dataset, we randomly sample 64 data points from the training set and evaluate the model on the original test set if the label exists. If not, we use the validation set for evaluation. The maximum sequence length is 512 tokens, and unknown text labels are added as new tokens. We run all experiments on NVIDIA RTX A6000 GPUs with 48GB memory.

\subsection{Language Tasks in the Large-Scale Benchmark}
Now we list all datasets selected after filtering out those with performance lower than the majority baseline.

\paragraph{QNLI} \citep{wang2018glue} is a question-answering inference task. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{MRPC} Microsoft Research Paraphrase Corpus \citep{dolan2005automatically} is a task to determine whether the input pair of sentences are semantically equivalent. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: similar: Yes, not\_similar: No. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{CoLA} The Corpus of Linguistic Acceptability \citep{warstadt2019neural} is the task to determine whether the input sentence is grammatically correct. The prompt template is: \textless{}S1\textgreater{} This is {[}MASK{]}. The label mapping is: acceptable: Yes, not\_acceptable: No. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{MNLI} MultiNLI \citep{williams2017broad} is a natural language inference task. We use the matched subset for evaluation. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, neutral: Maybe, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{RTE} The Recognizing Textual Entailment \citep{wang2018glue} dataset is a natural language inference task. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{SST2} Stanford Sentiment Treebank-2 \citep{socher2013recursive} is a sentiment analysis task with binary labels. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: positive: great, negative: terrible. The Huggingface link is: \url{https://huggingface.co/datasets/nyu-mll/glue}.

\paragraph{SST5} is the 5-label version of Stanford Sentiment Treebank \citep{socher2013recursive} sentiment analysis dataset. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: positive: great, somewhat\_positive: good, neutral: okay, somewhat\_negative: bad, negative: terrible. The Huggingface link is: \url{https://huggingface.co/datasets/SetFit/sst5}.

\paragraph{SNLI} Stanford Natural Language Inference \citep{bowman2015large} is a natural language inference task. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, neutral: Maybe, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/stanfordnlp/snli}.

\paragraph{TREC} Text REtrieval Conference \citep{li2002learning} is a text classification task that classifies question into $6$ categories. The prompt template is: {[}MASK{]}: \textless{}S1\textgreater{}. The label mapping is: ABBR: Expression, ENTY: Entity, DESC: Description, HUM: Human, LOC: Location, NUM: number. The Huggingface link is: \url{https://huggingface.co/datasets/CogComp/trec}.

\paragraph{SUBJ} Subjectivity \citep{pang2004sentimental} dataset is a task to determine whether given text is subjective. The prompt template is: \textless{}S1\textgreater{} This is {[}MASK{]}. The label mapping is: subjective: subjective, objective: objective. The dataset link is: \url{http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz}.

\paragraph{CR} Customer Review \citep{hu2004mining} is a binary sentiment analysis task. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: positive: great, negative: terrible. The dataset link is \url{https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar}.

\paragraph{MPQA} Multi-Perspective Question Answering \citep{wiebe2005annotating} is a binary sentiment analysis task. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: positive: great, negative: terrible. The dataset link is \url{https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar}.

\paragraph{MR} Movie Reviews \citep{pang2004sentimental} is a binary sentiment analysis task. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: positive: great, negative: terrible. The dataset link is \url{https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar}.

\paragraph{AG News} \citep{zhang2015character} is a topic classification task that classifies news article into 4 categories. The prompt template is: \textless{}S1\textgreater{} This is about {[}MASK{]} news. The label mapping is: World: international, Sports: sports, Business: business, Sci/Tech: science. The Huggingface link is: \url{https://huggingface.co/datasets/fancyzhx/ag_news}.

\paragraph{Yelp} \citep{zhang2015character} contains yelp reviews and is a sentiment analysis task. The prompt template is: \textless{}S1\textgreater{} This place is {[}MASK{]}. The label mapping is: 1-star: poor, 2-star: fair, 3-star: good, 4-star: great, 5-star: excellent. The Huggingface link is: \url{https://huggingface.co/datasets/Yelp/yelp_review_full}.


\paragraph{IMDb} \citep{maas2011learning} is a movie review binary sentiment analysis dataset. The prompt template is: \textless{}S1\textgreater{} This movie is {[}MASK{]}. The label mapping is: positive: great, negative: terrible. The Huggingface link is \url{https://huggingface.co/datasets/stanfordnlp/imdb}.

\paragraph{Yahoo! Answers} \citep{zhang2015character} is a question-answer topic classification dataset. The prompt template is: \textless{}S1\textgreater{} This is related to {[}MASK{]}. The question title, question content, and the best answer are concatenated to be S1. The label mapping is: Society\_\&\_Culture : society, Science\_\&\_Mathematics: science, Health: health, Education\_\&\_Reference: education, Computers\_\&\_Internet: computer, Sports: sports, Business\_\&\_Finance: finance, Entertainment\_\&\_Music: entertainment, Family\_\&\_Relationships: relationship, Politics\_\&\_Government: government. The Huggingface link is \url{https://huggingface.co/datasets/community-datasets/yahoo_answers_topics}.

\paragraph{ANLI} \citep{nie2019adversarial} are three natural language inference tasks collected iteratively and adversarially. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, neutral: Maybe, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/facebook/anli}.

\paragraph{CB} CommitmentBank \citep{de2019commitmentbank} is a natural language inference task. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. The label mapping is: entailment: Yes, neutral: Maybe, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/aps/super_glue}.

\paragraph{WiC} Word-in-Context \citep{pilehvar2018wic} is a binary classification task to answer whether the same word is used in the same way in two sentences. The prompt template is: \textless{}S1\textgreater{} \textless{}S2\textgreater{} Does \textless{}WORD\textgreater{} have the same meaning in both sentences? {[}MASK{]}. The label mapping is: Yes: Yes, No: No. The Huggingface link is: \url{https://huggingface.co/datasets/aps/super_glue}.

\paragraph{ETHICS Commonsense} is the commonsense subset of the ETHICS \citep{hendrycks2020aligning} dataset, which is a binary classification task to determine whether the behavior of the text matches commonsense moral. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: 0: acceptable, 1: unacceptable. The Huggingface link is: \url{https://huggingface.co/datasets/hendrycks/ethics}.

\paragraph{ETHICS Deontology} is the deontology subset of the ETHICS \citep{hendrycks2020aligning} dataset, which is a binary classification task to determine whether the excuse is reasonable given the scenario. The prompt template is: \textless{}S1\textgreater{} \textless{}S2\textgreater{} This is a {[}MASK{]} excuse. The label mapping is: 0: great, 1: terrible. The Huggingface link is: \url{https://huggingface.co/datasets/hendrycks/ethics}.

\paragraph{ETHICS Justice} is the justice subset of the ETHICS \citep{hendrycks2020aligning} dataset, which is a binary classification task to determine whether the text follows the principle of justice. The prompt template is: \textless{}S1\textgreater{} It was {[}MASK{]}. The label mapping is: 0: unfair, 1: fair. The Huggingface link is: \url{https://huggingface.co/datasets/hendrycks/ethics}.

\paragraph{Logiqa2.0 NLI} \citep{10174688} is a set of natural language inference tasks specifically designed for evaluating logical reasoning. The prompt template is: \textless{}S1\textgreater{}? {[}MASK{]} , \textless{}S2\textgreater{}. S1 is the concatenation of major and minor premise. The label mapping is: entailment: Yes, not\_entailment: No. The Huggingface link is: \url{https://huggingface.co/datasets/baber/logiqa2}. 

\paragraph{Amazon Reviews} is a collection of Amazon reviews with rating labels. The prompt template is: \textless{}S1\textgreater{} This product is {[}MASK{]}.  The label mapping is: 1-star: poor, 2-star: fair, 3-star: good, 4-star: great, 5-star: excellent. The Huggingface link is: \url{https://huggingface.co/datasets/SetFit/amazon_reviews_multi_en}. 

\paragraph{TweetEval Emotion} \citep{mohammad2018semeval} is a subset of TweetEval \citep{barbieri2020tweeteval} dataset, which is a 4-label emotion classification task for a collection of tweets. The prompt template is: \textless{}S1\textgreater{} This person feels {[}MASK{]}. The label mapping is: anger: angry, joy: happy, optimism: optimistic, sadness: sad. The Huggingface link is: \url{https://huggingface.co/datasets/cardiffnlp/tweet_eval}.

\paragraph{TweetEval Hate} \citep{basile-etal-2019-semeval} is a subset of TweetEval \citep{barbieri2020tweeteval} dataset, which is a binary hate speech detection task for a collection of tweets. The prompt template is: \textless{}S1\textgreater{} The sentence is {[}MASK{]}. The label mapping is: non-hate: neutral, hate: aggressive. The Huggingface link is: \url{https://huggingface.co/datasets/cardiffnlp/tweet_eval}.

\paragraph{TweetEval Offensive} \citep{zampieri2019semeval} is a subset of TweetEval \citep{barbieri2020tweeteval} dataset, which is a binary offensive text detection task for a collection of tweets. The prompt template is: \textless{}S1\textgreater{} It is {[}MASK{]}. The label mapping is: non-offensive: polite, offensive: offensive. The Huggingface link is: \url{https://huggingface.co/datasets/cardiffnlp/tweet_eval}.

\paragraph{TweetEval Sentiment} \citep{rosenthal2017semeval} is a subset of TweetEval \citep{barbieri2020tweeteval} dataset, which is a sentiment analysis task for a collection of tweets. The prompt template is: \textless{}S1\textgreater{} This is {[}MASK{]}. The label mapping is: negative: terrible, neutral: okay, positive: great. The Huggingface link is: \url{https://huggingface.co/datasets/cardiffnlp/tweet_eval}.

\paragraph{TweetEval Irony} \citep{van2018semeval} is a subset of TweetEval \citep{barbieri2020tweeteval} dataset, which is a binary ironic text detection task for a collection of tweets. The prompt template is: \textless{}S1\textgreater{} The sentence is {[}MASK{]}. The label mapping is: non-irony: genuine, irony: sarcastic.  The Huggingface link is: \url{https://huggingface.co/datasets/cardiffnlp/tweet_eval}.


\paragraph{Rotten Tomatoes} \citep{pang2005seeing} is a movie review dataset that contains the binary sentiment label of rotten tomatoes movie reviews. The prompt template is: \textless{}S1\textgreater{} This is {[}MASK{]}. The label mapping is: negative: terrible, positive: great. The Huggingface link is: \url{https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes}.

\paragraph{DBpedia14} \citep{zhang2015character} is a topic classification dataset that contains 14 labels. The prompt template is: \textless{}S1\textgreater{} This is about {[}MASK{]}. The label mapping is: Company: company, EducationalInstitution: school, Artist: artist, Athlete: sports, OfficeHolder: politics, MeanOfTransportation: transportation, Building: building, NaturalPlace: nature, Village: town, Animal: animal, Plant: plant, Album: music, Film: movie, WrittenWork: book. The Huggingface link is: \url{https://huggingface.co/datasets/fancyzhx/dbpedia_14}. 

\paragraph{Emotion} \citep{saravia-etal-2018-carer} is a sentiment analysis dataset with 6 types of emotions of tweets. The prompt template is: \textless{}S1\textgreater{} This person feels {[}MASK{]}. The label mapping is: joy: happy, sadness: sad, anger: anger, fear: scared, love: love, suprised: shock. The Huggingface link is: \url{https://huggingface.co/datasets/dair-ai/emotion}.

\paragraph{20Newsgroups} \citep{LANG1995331} is a topic classification dataset that contains 20 labels for news type. The prompt template is: \textless{}S1\textgreater{} This is about {[}MASK{]} news. The label mapping is: alt.atheism: atheism, comp.graphics: graphics, comp.os.ms-windows.misc: windows, comp.sys.ibm.pc.hardware: ibm, comp.sys.mac.hardware: mac, comp.windows.x: windowsX, rec.autos: car, rec.motorcycles: motorcycle, rec.sport.baseball: baseball, rec.sport.hockey: hockey, sci.crypt: cryptography, sci.electronics: electronics, sci.med: health, sci.space: space, misc.forsale: purchase, talk.politics.misc: politics, talk.politics.guns: gun, talk.politics.mideast: mideast, talk.religion.misc: religion, soc.religion.christian: christian. The Huggingface link is: \url{https://huggingface.co/datasets/SetFit/20_newsgroups}.

\paragraph{Folio} \citep{han2022folio} is a logic reasoning benchmark with first order logic annotations, and can be reformulated into the 2-sentence natural language inference format. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]}, \textless{}S2\textgreater{}. The label mapping is: False: No, Uncertain: Maybe, True: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/tasksource/folio}.

\paragraph{Doc NLI} \citep{yin-etal-2021-docnli} is a document level natural language inference task. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]}, \textless{}S2\textgreater{}. The label mapping is: not\_entailment: No, entailment: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/tasksource/doc-nli}.

\paragraph{WANLI} Worker-AI Collaboration for NLI \citep{liu-etal-2022-wanli} is a natural language inference task. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]}, \textless{}S2\textgreater{}. The label mapping is: not\_entailment: No, neutral: Maybe, entailment: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/alisawuffles/WANLI}.

\paragraph{VitaminC} \citep{schuster-etal-2021-get} is a task to determine if given evidence supports claims. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]}, \textless{}S2\textgreater{}. The label mapping is: REFUTES: No, NOT\_ENOUGH\_INFO: Maybe, SUPPORTS: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/tals/vitaminc}.

\paragraph{bAbI} \citep{weston2015towards} is a set of 20 NLP toy tasks that can be reformulated into natural language inference format. The prompt template is: \textless{}S1\textgreater{} {[}MASK{]}, \textless{}S2\textgreater{}. The label mapping is: not\_entailment: No, entailment: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/tasksource/babi_nli}.

\paragraph{Fake News} is a fake news detection dataset that contains binary labels. The prompt template is \textless{}S1\textgreater{} It was {[}MASK{]} news. The label mapping is: 0: fake, 1: real. The Huggingface link is: \url{https://huggingface.co/datasets/GonzaloA/fake_news}.

\paragraph{Human-vs-Machine} \citep{sivesind_2023} is a classification problem to differentiate human and machine generated text. The prompt template is \textless{}S1\textgreater{} It was written by {[}MASK{]}. The label mapping is: human-produced: human, machine-generated: machine. The Huggingface link is: \url{https://huggingface.co/datasets/NicolaiSivesind/human-vs-machine}.

\paragraph{AI-Human-Text} is a classification problem to differentiate human and machine generated text. The prompt template is \textless{}S1\textgreater{} It was written by {[}MASK{]}. The label mapping is: human-produced: human, machine-generated: machine. The Huggingface link is: \url{https://huggingface.co/datasets/andythetechnerd03/AI-human-text}.

\paragraph{WoS} Web of Science \citep{kowsari2017HDLTex} is a topic classification problem that contains 7 labels for paper abstract. The prompt template is: \textless{}S1\textgreater{} This is about {[}MASK{]}. The label mapping is: Computer Science: CS, Electrical\_Engineering: ECE, Psychology: Psychology, Mechanical\_Engineering: MechE, Civil\_Engineering: CivilE, Material\_Engineering: MaterialE. The Huggingface link is: \url{https://huggingface.co/datasets/river-martin/web-of-science-with-label-texts}.

\paragraph{PiC} Phrase Similarity \citep{pham2022PiC} is the binary classification task to determine whether two phrases are semantically equivalent given the context. The prompt template is: \textless{}S1\textgreater{} \textless{}S2\textgreater{} Does \textless{}PHRASE1\textgreater{} and \textless{}PHRASE2\textgreater{} have the same meaning? {[}MASK{]}. The label mapping is: negative: No, positive: Yes. The Huggingface link is: \url{https://huggingface.co/datasets/PiC/phrase_similarity}.

\paragraph{ART} \citep{Bhagavatula2020Abductive} is the multiple choice task of given observations, determine the most plausible hypothesis from the two. The prompt template is: \textless{}S1\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{} Question: which hypothesis is correct? Answer: {[}MASK{]}. The label mapping is: 1: A, 2: B. \textless{}S1\textgreater{} is the concatenation of two observations, and choices are two hypotheses. The Huggingface link is: \url{https://huggingface.co/datasets/allenai/art}.

\paragraph{ARC} \citep{allenai:arc} is the multiple choice question designed for advanced artificial intelligence reasoning, with Easy and Challenge splits. The prompt template is: \textless{}Question\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{}
C: \textless{}Choice3\textgreater{}
D: \textless{}Choice4\textgreater{}
E: \textless{}Choice5\textgreater{} Answer: {[}MASK{]}. The label mapping is: 0: A, 1: B, 2: C, 3: D, 4: E. The Huggingface link is: \url{https://huggingface.co/datasets/allenai/ai2_arc}.

\paragraph{HellaSwag} \citep{zellers2019hellaswag} is a multiple choice question task to deterimine which is the best ending of the sentence. The prompt template is: \textless{}S1\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{} C: \textless{}Choice3\textgreater{} D: \textless{}Choice4\textgreater{} Answer: {[}MASK{]}. The label mapping is: 0: A, 1: B, 2: C, 3: D. The Huggingface link is: \url{https://huggingface.co/datasets/Rowan/hellaswag}.

\paragraph{PiQA} Physical Interaction: Question Answering \citep{Bisk2020} is a multiple choice question task that evaluates physical commonsense knowledge. The prompt template is:  \textless{}S1\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{} Answer: {[}MASK{]}. The label mapping is: 0: A, 1: B.  The Huggingface link is: \url{https://huggingface.co/datasets/ybisk/piqa}.

\paragraph{SWAG} Situations With Adversarial Generations \citep{zellers2018swagaf} is a multiple choice question task that evaluates both skill of natural language inference and physical knowledge reasoning. The prompt template is: \textless{}S1\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{} C: \textless{}Choice3\textgreater{} D: \textless{}Choice4\textgreater{} Answer: {[}MASK{]}. The label mapping is: 0: A, 1: B, 2: C, 3: D. The Huggingface link is: \url{https://huggingface.co/datasets/allenai/swag}.

\paragraph{SiQA} Social Interaction QA \citep{sap2019socialiqa} is a multiple choice question task that evaluates social common sense knowledge. The prompt template is: \textless{}Context\textgreater{} Question: \textless{}Question\textgreater{} A: \textless{}Choice1\textgreater{} B: \textless{}Choice2\textgreater{} C: \textless{}Choice3\textgreater{} 
D: \textless{}Choice4\textgreater{} Answer: {[}MASK{]}. The label mapping is: 0: A, 1: B, 2: C, 3: D. The Huggingface link is: \url{https://huggingface.co/datasets/allenai/social_i_qa}.
