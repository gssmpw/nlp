
\subsection{Models and Datasets}
This paper includes both computer vision and natural language processing experiments. In the vision experiments, we use CLIP \citep{radford2021learning} and ViT \citep{dosovitskiy2020image} models from OpenCLIP \citep{ilharco_gabriel_2021_5143773}, along with the following 8 datasets: MNIST \citep{lecun1998gradient}, EuroSAT \citep{helber2019eurosat}, Cars \citep{krause20133d}, GTSRB \citep{stallkamp2012man}, RESISC45 \citep{cheng2017remote}, DTD \citep{cimpoi2014describing}, SUN397 \citep{xiao2010sun}, and SVHN \citep{netzer2011reading}. The language experiments are based on RoBERTa \citep{liu2019roberta} models, tested on a medium scale benchmark with 12 tasks \citep{panigrahi2023task, he2024localize} and a larger scale collection of 70 tasks, the latter with further details provided in \Cref{sec:nlp_details}.

\subsection{Empirical Verification of Theoretical Results}
We primarily focus on \Cref{thm:task-addition} and examine the relationship between the loss gap and key constants.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/mnist-against-others.pdf}
    \vspace{-2em}
    \caption{Task vector similarity vs. $\mathcal{L}_\text{MNIST}(\theta^*) - \mathcal{L}_\text{MNIST}(\theta_\text{MNIST})$, where $\theta^* = \theta_0 + 0.5\tau_\text{MNIST} + 0.5\tau_\text{task}$. This figure includes two different set of CLIP ViT/B-32 task vectors. The pink shade includes the high similarity high loss gap region, and the green shade is the low similarity low loss gap region. This implies larger task similarity $\epsilon$ is harmful for addition.}
    \label{fig:mnist-againt-others}
\end{figure}

\paragraph{Task Vector Orthogonality $\epsilon$}
To verify how task vector similarity $\epsilon$ impacts the performance, we conduct the experiment shown in \cref{fig:mnist-againt-others}. We merge the MNIST task vector with each of the other task vectors, all having similar norms ranging from $[2,3)$ (see \cref{tab:ratio_norm_task} in the Appendix for details), and set the scaling coefficient $\alpha$ to $0.5$. In this setting, we approximately control all constants in \cref{thm:task-addition}, including $L$, $\alpha$, and $C$, and observe that highly similar tasks, such as digit classification in MNIST, SVHN, and GTSRB, lead to larger loss gaps or worse performance for MNIST compared to less related tasks.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/twice-task-orthogonality-flattened-short.pdf}
    \vspace{-0.5cm}
    \caption{Task vector similarity matrix for two checkpoints. The left green box represents the task similarity for vectors all derived from fine-tuning pretrained model 1. The right pink box represents the similarity values for task vectors from two different checkpoints, which corresponds to small $\epsilon$ in the 50/50 row of \Cref{tab:twice-task-orthogonality}.}
    \label{fig:twice-task-orthogonality}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/distance_two_checkpoints_annot.pdf}
    \vspace{-0.8cm}
    \caption{Task vector norms in different settings. Since the distance of two pretrained models ($\approx 314$) are much larger than the distance between the pretrained model and their own fine tuned model ($\approx 2$), in \Cref{tab:twice-task-orthogonality} if we subtract pretrained 2 from any finetune 1, $\norm{\tau}$'s upper bound $C$ is huge, leading to the merging failure. For visualization purpose, we only show two randomly selected dimensions, while the numbers for $C_1,C_2, \norm{\tau}$ are directly computed from high-dimensional vectors based on data.}
    \label{fig:task-vector-norm-avg}
\end{figure}

\begin{table}[tb]
\caption{Task vector mixing absolute average accuracy, which is the average of all task test performances evaluated with the merged model. Numbers 1 and 2 refer to the identities of the pretrained checkpoints. “50/50” represents the experiment where 50\% of the own task vector is mixed with 50\% of task vectors derived from the other pretrained model. The task vector $\tau$ is defined as $\theta_i - \theta_0$.}
\label{tab:twice-task-orthogonality}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
$\theta_i$ \textbackslash $\ \theta_0$ & pretrained 1 & pretrained 2 \\ \midrule
 finetune from 1             & \textbf{70.83}        & 51.71        \\
 50 / 50                   & 59.92        & 61.71        \\
 finetune from 2             & 54.21        & \textbf{71.09}        \\ \bottomrule
\end{tabular}
\end{table}


\paragraph{Interaction between $C$ and $\epsilon$}
We provide additional evidence that both $C$ and $\epsilon$ must be constrained for the success of task vector addition. In \cref{tab:twice-task-orthogonality}, we collected task vectors for 8 tasks from two CLIP checkpoints pretrained with different hyperparameters. From this table, we observe that successful task addition reveals the identity of the task vectors. For optimal merging performance, we should only add task vectors fine-tuned from the same checkpoint, as any mixture of task vectors from different checkpoints will cause a significant performance drop. The above empirical observation consolidates our~\Cref{ass:finetuning} that task vectors should reside in the same fine-tuning regime. To elaborate, from \cref{fig:twice-task-orthogonality}, although all $\epsilon$ values in the pink box are very low, task addition still fails due to the large $C$ value. From \cref{fig:task-vector-norm-avg}, we see that with different hyperparameters, the two pretrained models are situated in two local convex basins, and the distance between the two checkpoints is much larger than the task vectors (for more discussion, see \Cref{sec:norm_details}). Thus, if we create task vectors by subtracting the wrong pretrained checkpoint, the large $C$ value leads to the failure of task addition.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/dtd_loss_gap_by_dataset.pdf}
    \vspace{-0.8cm}
    \caption{$\mathcal{L}_\text{DTD}(\theta^*) - \mathcal{L}_\text{DTD}(\theta_\text{DTD})$ by merging $\tau_\text{DTD}$ with other task vectors, setting scaling coefficient as $0.5$. Two colored pretrained checkpoints have different local smoothness values.}
    \label{fig:dtd_smoothness}
\end{figure}

\paragraph{Local Smoothness $L$}
The local smoothness $L$ is specific to each pretrained model due to differences in their optimization trajectories. Since, as shown in \Cref{fig:mnist-againt-others} and \Cref{fig:task-vector-norm-avg}, the differences in $C$ and $\epsilon$ (not $\theta_i$) between two pretrained models are small. In \Cref{fig:dtd_smoothness}, we merge the DTD task vector with each of the other task vectors and compare the loss gap between two checkpoints. Because it is not feasible to load $\mathbf{H}(\theta_i) \in \mathbb{R}^{d \times d}$ directly onto the GPU, we estimate $L$ using the power iteration method \citep{mises1929praktische} to reduce the largest eigenvalue problem to a Hessian-vector product computation. As seen in \Cref{fig:dtd_smoothness}, larger local smoothness consistently leads to a larger gap from the optimal loss term across datasets, resulting in worse merging performance.


\subsection{Vision Experiments with Task Vector Bases}
\label{sec:basis-exp-cv}

\begin{table}[tb]
\vspace{-0.8cm}
\caption{Addition of MNIST, SVHN, EuroSAT, RESISC45 with Task Arithmetic as $\texttt{merge}_\text{out}$ for different $\texttt{merge}_\text{in}$. MS-ER represents the partition of (MNIST, SVHN) and (EuroSAT, RESISC45).}
\centering
\label{tab:TA_mergeB}
\begin{tabular}{@{}lccc@{}}
\toprule
 $\texttt{merge}_\text{in}$ & Avg. $k = 2$ & Avg. All $k$ & \cellcolor{mygray}{MS-ER} \\ \midrule
MTL & 95.45  & 93.85 & \cellcolor{mygray}{\textbf{95.52}} \\
TIES & 88.68 & 88.48 & \cellcolor{mygray}{\textbf{90.43}} \\
Fisher & 82.41 & 83.67 & \cellcolor{mygray}{\textbf{86.37}} \\
Tangent & 88.50 & 88.75 & \cellcolor{mygray}{\textbf{90.05}} \\
Topk & 82.17 & 83.05 & \cellcolor{mygray}{\textbf{85.69}} \\
Mean & 85.65 & 86.44 & \cellcolor{mygray}{\textbf{88.65}} \\ \bottomrule
\end{tabular}
\end{table}


\paragraph{Bases Addition} 
\label{sec:cv_add}
For a detailed analysis of the behavior of task vector bases methods, we first fix a small subset of tasks where the separation between digit classification (MNIST, SVHN) and satellite image classification (EuroSAT, RESISC45) is obvious and matches the optimal clustering result that reduces the memory storage by half. With only $4$ task vectors, we can enumerate all possible partitions of tasks and understand the position of the merging performance using this natural partition.

In \Cref{tab:TA_mergeB}, we fix the outer merging method $\texttt{merge}_\text{out}$ for various inner merging methods $\texttt{merge}_\text{in}$, including multi-task joint training (MTL) by mixing all task datasets together during fine-tuning, Fisher merging \citep{matena2022merging}, tangent fine-tuning \citep{ortiz2024task}, TIES \citep{yadav2024ties} that resolves the sign conflicts of different task vectors, Topk (similar to the dataless version of \citet{he2024localize}) that only keeps the top 5\% magnitude parameters, and the Mean of all task vectors \citep{pmlr-v162-wortsman22a}. We do not report routing-based methods due to the additional introduction of router parameters. These methods represent the most popular categories of model merging. We can observe that MS-ER is consistently better than the average of all $k = 2$ and $k \in [1,4]$ partitions, across different merging methods. By grouping, we can achieve better-than-average addition performance while using a small memory budget.

\paragraph{Bases Negation} After the first step of merging in \Cref{tab:TA_mergeB}, due to memory budget, we reduce the number of saved checkpoints. If we want to unlearn part of the knowledge under memory constraints, \Cref{tab:bases_negation} shows the preferred bases for this task. In \Cref{tab:bases_negation}, the goal is to unlearn MNIST from the pretrained model, so we tune $\alpha$ to maintain the control metric at 95\% of the pretrained model performance, evaluated on ImageNet. Following the setup in \Cref{tab:TA_mergeB}, we save the MS and ER bases with two copies of checkpoints. The alternative extreme is to store only one fully merged model (MSER) from \Cref{tab:TA_mergeB} to minimize memory usage. However, since MSER mixes unrelated task vectors, it is usually less effective than subtracting a MS basis when the goal is to unlearn MNIST-specific information. For some $\texttt{merge}_\text{in}$ methods, such as TIES and Mean, we observe that MS performance is better than directly subtracting the task vector trained on MNIST, likely due to the overlap of skills between MNIST and SVHN. This demonstrates that the memory-efficient task vector bases arithmetic can be highly flexible as well.

\begin{table}[tb]
\caption{Unlearning MNIST from pretrained model by negating bases constructed from MNIST, SVHN, EuroSAT, and RESISC45. For Tangent, directly subtracting $\tau_\text{MNIST}$ has the target performance of 0.11. For others, this target performance is 15.68.}
\label{tab:bases_negation}
\centering
\scalebox{0.9}{
\begin{tabular}{@{}lcccc@{}}
\toprule
                             & \multicolumn{2}{c}{MS}                          & \multicolumn{2}{c}{MSER}               \\ \cmidrule(l){2-5} 
$\texttt{merge}_\text{in}$ & \cellcolor{mygray}{Target ($\downarrow$)}          & \color[HTML]{9B9B9B} {Control}                       & Target ($\downarrow$) & \color[HTML]{9B9B9B} {Control}                        \\ \midrule
MTL                          & \cellcolor{mygray}{\textbf{21.53}} & {\color[HTML]{9B9B9B} 63.13} & 31.91 & {\color[HTML]{9B9B9B} 63.60} \\
TIES                         & \cellcolor{mygray}{\textbf{12.17}} & {\color[HTML]{9B9B9B} 63.18} & 22.27 & {\color[HTML]{9B9B9B} 62.76} \\
Fisher                       & \cellcolor{mygray}{17.83}          & {\color[HTML]{9B9B9B} 63.49} & \textbf{16.74} & {\color[HTML]{9B9B9B} 63.09} \\
Tangent & \cellcolor{mygray}{\textbf{0.12}} & {\color[HTML]{9B9B9B} 63.00} & 3.80 & {\color[HTML]{9B9B9B} 63.31} \\
Topk                         & \cellcolor{mygray}{\textbf{19.53}} & {\color[HTML]{9B9B9B} 64.36} & 22.89 & {\color[HTML]{9B9B9B} 62.44} \\
Mean                         & \cellcolor{mygray}{\textbf{10.36}} & {\color[HTML]{9B9B9B} 63.21} & 20.61 & {\color[HTML]{9B9B9B} 64.27} \\ \bottomrule
\end{tabular}
}

\end{table}


\begin{table}[tb]
\caption{Generalization performance on GTSRB and DTD with different $\texttt{merge}_\text{in}$ methods by using bases constructed from MNIST, SVHN, EuroSAT, and RESISC45. The pretrained model performance 36.48 on GTSRB and 54.73 on DTD.}
\label{tab:bases_generalization}
\centering
\scalebox{0.9}{
\begin{tabular}{@{}lcccc@{}}
\toprule
        & \multicolumn{2}{c}{GTSRB}       & \multicolumn{2}{c}{DTD}                                     \\ \cmidrule(l){2-5} 
$\texttt{merge}_\text{in}$ & \cellcolor{mygray}{MS-ER} & M-S-E-R & {\color[HTML]{9B9B9B} MS-ER} & {\color[HTML]{9B9B9B} M-S-E-R} \\ \midrule
MTL     & \cellcolor{mygray}{\textbf{41.02}} & 37.91          & {\color[HTML]{9B9B9B} 54.73} & {\color[HTML]{9B9B9B} 55.00} \\
TIES    & \cellcolor{mygray}{\textbf{43.15}} & 37.91          & {\color[HTML]{9B9B9B} 54.73} & {\color[HTML]{9B9B9B} 55.00} \\
Fisher  & \cellcolor{mygray}{\textbf{41.31}} & 37.91          & {\color[HTML]{9B9B9B} 54.89} & {\color[HTML]{9B9B9B} 55.00} \\
Tangent & \cellcolor{mygray}{39.21}          & \textbf{41.59} & {\color[HTML]{9B9B9B} 55.10} & {\color[HTML]{9B9B9B} 55.42} \\
Topk    & \cellcolor{mygray}{\textbf{40.37}} & 37.91          & {\color[HTML]{9B9B9B} 55.00} & {\color[HTML]{9B9B9B} 55.00} \\
Mean    & \cellcolor{mygray}{\textbf{41.97}} & 37.91          & {\color[HTML]{9B9B9B} 54.73} & {\color[HTML]{9B9B9B} 55.00} \\ \bottomrule
\end{tabular}
}

\end{table}


\paragraph{Bases OOD Generalization}
After saving task vector bases from MNIST, SVHN, EuroSAT, and RESISC45 in \Cref{sec:cv_add}, we test generalization metrics on unseen tasks in our vision benchmark. We set a similarity threshold to only use the most similar task vector bases for GTSRB and DTD generalization in \Cref{tab:bases_generalization}, with $\texttt{merge}_\text{out}$ set to Task Arithmetic. Since we use only one basis for generalization, this reduces to tuning $\alpha$ for the selected basis on the validation set. For GTSRB, which involves traffic sign recognition, we expect overlap with the digit classification skills in MNIST and SVHN. In \Cref{tab:bases_generalization}, we observe that the performance of the MS-ER column, corresponding to the bases method, is generally higher than the M-S-E-R column, where full task arithmetic flexibility is retained. For OOD generalization, using all task-specific information related to the target task proves beneficial. However, since DTD is not related to any stored bases, its OOD generalization performance is almost equivalent to the pretrained model. This validates \Cref{thm:task-generalization} and shows that when an OOD task is not too far away from seen tasks, leveraging bases knowledge is more effective than relying solely on a task vector trained on a single task, with the added benefit of memory savings.




\subsection{Language Experiments with Task Vector Bases}
\label{sec:basis-exp-nlp}
\subsubsection{Results for Large Scale Benchmark}
\begin{figure}[!ht]
    \centering
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/k3_nlp_recolor.png} 
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/k10_nlp_recolor.png} 
    \end{minipage}
    \caption{Task similarity matrix of merging 70 natural language tasks when $k = 3$ (left) and $k = 10$ (right), ordered by cluster index based on spectral clustering. We use orange boxes to highlight the clusters with more than one task. When $k = 3$, the clusters from top to bottom are: 1-sentence tasks with multiple choice questions (MCQ), 2-sentence tasks, and AI-vs-human text classification. When $k = 10$, the white boxes represent natural language inference (NLI) tasks, bAbI \citep{weston2015towards}, AI-vs-human, topic classification, two sentiment analysis groups, and MCQ.}
    \label{fig:nlp_cluster}
\end{figure}

\paragraph{Saving Memory Storage with Bases}
\label{sec:memory_70}
We collect 70 natural language understanding tasks from Huggingface \citep{wolf2019huggingface} datasets that can be reformulated as classification problems. With a larger number of tasks, we can conduct a stress test to determine how much we can compress the memory budget using \Cref{alg:task_vector_basis}. Following \citet{gao2020making, he2024localize}, we design appropriate prompt templates and labels for masked language modeling fine-tuning of RoBERTa-base, ensuring that each fine-tuned model’s performance exceeds the majority vote baseline. As a result, each of the collected models is meaningful, and since all tasks can be evaluated with accuracy, the merged metrics are directly comparable.

We first examine the result of clustering in \Cref{fig:nlp_cluster}, where we show the task cosine similarity matrix ordered by the spectral clustering results. We plot two matrices: $k = 3$, which represents the number of clusters selected based on the largest eigenvalue gap, and $k = 10$, which further partitions tasks into more fine-grained groups. Consequently, The properties of the task vectors change according to the prompt template difference. When $k = 3$, the tasks are split based on the number of input sentences, and when $k = 10$, there are more fine-grained clusters that reflect the task distribution, such as the separation of bAbI-NLI tasks, which reformulate tasks from \citet{weston2015towards} as NLI problems, and other types of NLI tasks. In summary, the generated task clusters in step 2 of \Cref{fig:basis_intro} are reasonable, as they reflect certain interpretable skills learned by all models. And when we use $k = 3$ as the algorithm suggested, we reduce the storage of 70 task vectors into 3 bases, saving \textbf{95.71\%} of the memory budget.
%Based on \Cref{tab:prompts}, we observe a clear distinction between each category of prompts. 

\begin{table*}[!ht]
    \centering
    \caption{Merged accuracy/F1$^\dagger$ and storage cost on the $12$-task language benchmark. First two rows contain numbers in \citet{he2024localize}.}
    \label{tab:medium_nlp}
    \scalebox{0.68}{
        \begin{tabular}{l||cccccccccccc|c||cc|c}
            \toprule
            Task                                                                                      & SST-2 & CR    & MR    & MPQA  & TREC  & SUBJ  & QNLI  & SNLI  & MNLI  & RTE   & MRPC$^\dagger$  & QQP   & Avg. & \#Masks & Sparsity\% & Storage (GB) \\
            \midrule
            Task Arithmetic & 88.5 & 88.2 & 80.3 & 82.9 & 32.0 & 61.0 & 62.0 & 56.1 & 49.5 & 65.6 & 82.8 & 62.3 & 67.5 & $\times$ 12 & 100 & 3.90 \\
            Localize-and-Stitch                                                              & 89.6 & 89.6 & 84.9 & 82.8 & 78.2 & 82.0 & 73.4 & 62.1 & 58.0 & 63.3 & 82.0 & 65.1 & 75.9 & $\times$ 12 & 10.9 & 0.42 \\
            \cellcolor{mygray}{\quad - Mask Sharing} & \cellcolor{mygray}{90.4} & \cellcolor{mygray}{89.4} & \cellcolor{mygray}{84.8} & \cellcolor{mygray}{76.4} & \cellcolor{mygray}{89.4} & \cellcolor{mygray}{90.9} & \cellcolor{mygray}{53.0} & \cellcolor{mygray}{58.2} & \cellcolor{mygray}{51.9} & \cellcolor{mygray}{59.2} & \cellcolor{mygray}{77.7} & \cellcolor{mygray}{67.3} & \cellcolor{mygray}{74.1} & \cellcolor{mygray}{$\times$ 4} & \cellcolor{mygray}{11.6} & \cellcolor{mygray}{\textbf{0.15}} \\
            \cellcolor{mygray}{\quad - Stitch Twice} & \cellcolor{mygray}{88.3} & \cellcolor{mygray}{88.5} & \cellcolor{mygray}{83.5} & \cellcolor{mygray}{78.5} & \cellcolor{mygray}{78.6} & \cellcolor{mygray}{85.1} & \cellcolor{mygray}{54.3} & \cellcolor{mygray}{74.1} & \cellcolor{mygray}{62.8} & \cellcolor{mygray}{67.5} & \cellcolor{mygray}{81.4} & \cellcolor{mygray}{74.2} & \cellcolor{mygray}{\textbf{76.4}} & \cellcolor{mygray}{$\times$ 12} & \cellcolor{mygray}{7.93} & \cellcolor{mygray}{0.31}  \\
            \bottomrule
        \end{tabular}}
\end{table*}


\paragraph{Generalization Performance for Merged Model} Given the task clusters, we now evaluate the performance of using task vector bases. In \Cref{fig:bases_random}, we plot the performance of the task vector bases against 5 randomly selected partitions of tasks for the same $k$. To avoid the computational overhead of selecting $\alpha$, we set both the inner and outer merging methods to Mean \citep{pmlr-v162-wortsman22a}. From the results, we observe that our basis method with $k = 3$, selected based on the spectral gap, achieves the best performance overall, outperforming the Mean baseline when $k = 1$, as well as the pretrained model and majority vote baselines. Furthermore, we observe a performance degradation of the bases method after $k = 3$, and a gradual performance increase for random partitions, although all random partition metrics are worse than the Model Soup baseline. This supports the importance of using the eigengap trick to select $k$ in \Cref{alg:task_vector_basis}, which effectively balances the task addition conflict for two rounds of merging. Based on \Cref{fig:bases_random}, our bases method surpasses model merging baseline performances in the large $T$ challenging setting.


\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/nlp_bases_random.pdf}

    \caption{Absolute average accuracy for each number of bases $k$, comparing the task vector bases method and random partition of tasks. When $k = 1$, it reduces to one step of model merging. The pretrained model $\theta_0$ performance is $42.15$, and using majority vote for each task’s performance is $40.47$, both of which are far below the two lines in this figure.}
    \label{fig:bases_random}
\end{figure}


\subsubsection{Results for Medium Scale Benchmark}

Since most merging methods require $\alpha$ tuning or additional training, testing them on the $70$-task benchmark is time-consuming. To better understand how bases methods compare with other popular baselines, we conduct experiments on a medium-scale benchmark with $12$ language tasks.

\paragraph{Bases Addition with Localize-and-Stitch} \label{sec:localize-and-stitch}
(L\&S) \citet{he2024localize} divides model merging into two steps: first, localizing the task information by learning a sparse vector mask, and second, stitching sparse models with mask normalization, ensuring that $\alpha$ is a probability vector like \Cref{ass:coeff}. Compared to other model merging methods, \citet{he2024localize} offers the dual advantage with significantly improved multitask performance due to reduced conflict from localization, and drastically lowered memory costs by saving only a small proportion of nonzero weights. Thus, to maximally boost all metrics, we investigate how to integrate L\&S into our bases creation framework which requires two steps of merging. As described in \Cref{sec:bases_creation}, after clustering sparse localized task vectors, we can directly apply L\&S as $\texttt{merge}_\text{in}$ within each cluster. For $\texttt{merge}_\text{out}$, it requires special design and we propose two modifications to L\&S. 

The first approach, \textbf{Mask Sharing}, creates a shared binary mask within each task cluster by averaging the task masks and rounding the averaged mask entries. Formally, if each task $i$ has an associated task-specific mask $\gamma_i \in \{0,1\}^d$, and the cluster size is $N$, these clustered tasks share one mask $\bar\gamma \leftarrow \texttt{round}(\sum_{i}\gamma_i/N)$. This approach significantly reduces memory storage, requiring only $m < T$ copies of sparse models, though it may slightly increase mask sparsity. However, this method sacrifices some task-specific information, leading to task conflicts during merging and suboptimal generalization performance in \Cref{tab:medium_nlp}. 

The second approach, \textbf{Stitch Twice}, first localizes tasks and stitches by mask normalization within cluster. Afterward, in $\texttt{merge}_\text{out}$, we round within-cluster-normalized masks as updated binary task masks, and use them as input for the second iteration of stitching across all $T$ tasks. Formally, in the first iteration, for $l \in [d]$, the $l$-th entry of the mask for task $i$ is $\gamma_i^l \leftarrow \texttt{round}(\gamma_i^l/\sum_{j=1}^N\gamma_j^l)$, and the second iteration of mask update is $\gamma_i^l \leftarrow \gamma_i^l/\sum_{j=1}^T\gamma_j^l$. Inspired by the improvements shown in \Cref{fig:bases_random}, this two-step process automatically readjusts task vector weights with appropriate task relationships learned from clustering. Unlike Mask Sharing, Stitch Twice requires $T$ copies of sparse models, since each task retains its own mask. However, in \Cref{tab:medium_nlp}, Stitch Twice improves generalization performance from L\&S (and more competitive baselines, see \Cref{sec:medium_nlp_details}) while also surprisingly reducing memory costs due to increased mask sparsity from double normalization. 

In summary, we can integrate L\&S into our proposed bases framework with minimal efforts, achieving both enhanced generalization and improved memory efficiency, highlighting the broad applicability of our proposal.




