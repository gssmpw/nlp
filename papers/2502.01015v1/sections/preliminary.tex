\paragraph{Problem Setting}
Let $\ell : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ be the loss function, and $h : \mathcal{X} \times \Theta \to \mathcal{Y} \subseteq \mathbb{R}$ be the classifier. When the context is clear, we omit some arguments for $\ell$ and $h$. We consider the initial pre-trained model parameter $\theta_0 \in \mathbb{R}^d$, which is fine-tuned on $T$ tasks to yield fine-tuned parameters $\{ \theta_1, \dots, \theta_T \}$ with respect to the loss functions $\{ \ell_1, \dots, \ell_T \}$. For $n$ training samples drawn from the $i$-th task distribution $\mathcal{D}_i$, we denote the population risk evaluated at $\theta$ as $\mathcal{L}_i(\theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}_i}[\ell_i(h(x, \theta),y)]$.

\paragraph{Task Arithmetic and Applications}
Task vectors are defined as $\tau_i := \theta_i - \theta_0, \forall i \in[T]$. \citet{ilharco2022editing} discovered two basic arithmetic operations for task vectors. Let $\alpha$ be the scaling coefficient. Task vector addition is achieved by adding weighted task vectors together, $\theta^* = \theta_0 + \sum_{i=1}^T \alpha_i \tau_i$, for multitask learning. Task vector negation involves subtracting a task vector from the pretrained model for machine unlearning, which is $\theta^* = \theta_0 - \alpha \tau$. Built on addition and negation, we can extend to more complex operations for other applications such as domain generalization. In \citet{ilharco2022editing}, $\alpha$ is tuned on a validation set within $0$ to $1$, and all task vectors share the same $\alpha$ to save hyperparameter tuning efforts, although this is not necessarily true for other merging methods. 