\section{Related Works}
\label{related_sec}
\paragraph{Zero-shot Object Customization.}
____ introduced AnyDoor to tackle the ZSOIC problem, utilizing a large pre-trained diffusion model, Stable Diffusion ____, with a dual-backbone architecture inspired by ControlNet ____. Despite achieving strong performance, AnyDoor relies on a resource-intensive architecture with significant computational demands, making the image generation process inefficient. Similarly, ____ proposed MimicBrush, a Unet-based latent diffusion model that integrates a reference U-Net into an imitative U-Net, offering finer part-level control but requiring a large model size and heavy computational resources. UniCanvas ____ also employs a substantial framework built upon customized text-to-image generation with Stable Diffusion, introducing a novel affordance-aware editing pipeline but further amplifying computational overhead with its parallel generative branches. While these approaches improve object customization applications, their reliance on oversized CNN-based architectures limits practical usage and applicability, especially on hardware with limited resources. To address these shortcomings, we propose \texttt{E-MD3C}, a lightweight transformer-based diffusion model that generates high-quality images while significantly reducing resource requirements (see Fig. \ref{fig:efficiency_bar}).

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{./figs2/DreamBooth}
  \vspace{-10pt}
  \caption{\textbf{Object Composition}. The 3$^{\text{rd}}$ and 4$^{\text{th}}$ columns show outputs from the existing method and our model. Our model generates images in just over 2 seconds, compared to 7 seconds for the existing approach.}
  \label{fig:DreamBooth}
\vspace{-14pt}
\end{figure}

\paragraph{Denoising Diffusion Transformers.}
The CNN-based U-Net architecture ____ has long been the foundational framework for diffusion models and remains a prevalent choice for various diffusion-based generation tasks ____. However, a transformative shift occurred with the introduction of DiT ____, which incorporated the transformer-based ViT architecture ____ into latent diffusion models. This innovation demonstrated superior scalability and consistently outperformed traditional U-Net-based designs. Building on this progress, ____ further advanced diffusion transformers, achieving state-of-the-art results in class-conditional image generation on ImageNet through advanced contextual representation learning. Other works, such as Stable Diffusion v3 ____, PixArt ____ and Sana ____, have leveraged DiT for text-to-image generation, highlighting the strong potential of transformer architectures in generative tasks. Although prior studies primarily focus on general-purpose or text-driven generative tasks, our research investigates the application of diffusion transformers to the specialized and complex task of zero-shot object-level image customization (ZSOIC). ZSOIC involves extracting and integrating intricate attributes from a source image, including object appearance, identity, and background, into a unified target image, which poses unique challenges in achieving cohesive synthesis ____ (See Fig. \ref{fig:DreamBooth}).

\begin{figure*} %[!htbp]
  \centering
  \includegraphics[width=1\linewidth]{./figs2/proposal_2025}
  \vspace{-16pt}
  \caption{Zero-shot object customization and its practical applications. Images are generated using our \texttt{E-MD3C} model.}
  \label{fig:task}
  \vspace{-12pt}
\end{figure*}

\paragraph{Mask Prediction Modeling.}
Mask-based vision models, drawing inspiration from mask language models like BERT ____, have demonstrated exceptional scalability and performance across diverse tasks. Prominent examples include MAE ____ for self-supervised learning (SSL) and models such as MaskGIT ____, Muse ____, and MAGVIT ____, which learn discrete token distributions for image generation. These methods adopt non-autoregressive modeling with a cross-entropy loss to predict token indices within a codebook. Diverging from this paradigm, MDT ____ introduced an asymmetric masking schedule that enhances contextual representation learning in denoising diffusion transformers, achieving superior class-conditional image generation on ImageNet. Building on this foundation, ____ proposed X-MDPT, demonstrating the potential of masked diffusion transformers for pose-guided human image generation with improved performance and efficiency. Similarly, MDT-A2G ____ applied masked diffusion transformers to gesture generation, while QA-MDT ____ extended this framework for music generation. Further advancements include masked diffusion transformers for audio generation, as explored by MDSGen ____, achieving impressive results and inspiring subsequent research. In this work, we focus on leveraging the strengths of masked diffusion transformers ____ for the task of zero-shot object-level image customization, introducing \texttt{E-MD3C}, a disentangled design with multiple conditioning mechanisms to enable effective and targeted object generation with various applications as shown in Fig. \ref{fig:task}. 

\begin{figure*}%[!htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{./figs2/arch2}
  \vspace{-8pt}
  \caption{Overview of the \texttt{E-MD3C} framework for zero-shot object customization. During training, 30\% of patched tokens are masked, and the noisy input is processed by the Diffusion Transformer, conditioned on a collected vector ($D=1024$) via AdaLN modulation ____. A mask prediction objective models token relationships. The \textcolor{red}{red arrow} $\color{red}\rightarrow$ is training-only, the \textcolor{black}{black arrow} $\color{black}\rightarrow$ is used for both training and inference, and the \textcolor{green}{green arrow} $\color{green}\rightarrow$ is inference-only.}
  \label{fig:arch}
  \vspace{-12pt}
\end{figure*}