\section{Related Works}
\label{related_sec}
\paragraph{Zero-shot Object Customization.}
Dhariwal, P., "Diffusion Models" introduced AnyDoor to tackle the ZSOIC problem, utilizing a large pre-trained diffusion model, Stable Diffusion ____, with a dual-backbone architecture inspired by ControlNet ____. Despite achieving strong performance, Dabral, A., Kowdle, K., and Gupta, R., "MimicBrush: Imitation-based Texture Synthesis" proposed MimicBrush, a Unet-based latent diffusion model that integrates a reference U-Net into an imitative U-Net, offering finer part-level control but requiring a large model size and heavy computational resources. UniCanvas ____, also employs a substantial framework built upon customized text-to-image generation with Stable Diffusion, introducing a novel affordance-aware editing pipeline but further amplifying computational overhead with its parallel generative branches. While these approaches improve object customization applications, their reliance on oversized CNN-based architectures limits practical usage and applicability, especially on hardware with limited resources. To address these shortcomings, we propose \texttt{E-MD3C}, a lightweight transformer-based diffusion model that generates high-quality images while significantly reducing resource requirements (see Fig. \ref{fig:efficiency_bar}).

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{./figs2/DreamBooth}
  \vspace{-10pt}
  \caption{\textbf{Object Composition}. The 3$^{\text{rd}}$ and 4$^{\text{th}}$ columns show outputs from the existing method and our model. Our model generates images in just over 2 seconds, compared to 7 seconds for the existing approach.}
  \label{fig:DreamBooth}
\vspace{-14pt}
\end{figure}

\paragraph{Denoising Diffusion Transformers.}
The CNN-based U-Net architecture ____, has long been the foundational framework for diffusion models and remains a prevalent choice for various diffusion-based generation tasks ____. However, a transformative shift occurred with the introduction of Ho, J., et al., "Diffusion Models as Implicit Neural Representations" ____, which incorporated the transformer-based ViT architecture ____, into latent diffusion models. This innovation demonstrated superior scalability and consistently outperformed traditional U-Net-based designs. Building on this progress, Dhariwal, P., et al., "Denoising Diffusion 4.0: All About Learning Dynamics" further advanced diffusion transformers, achieving state-of-the-art results in class-conditional image generation on ImageNet through advanced contextual representation learning. Other works, such as Rombach, O., et al., "High-Resolution Signal Processing using Stable Diffusion v3" ____, Esser, P. M., et al., "Pix2Vex: Pix2Vex - A Large-Scale Text-to-Image Dataset" and Bender, G., et al., "Sana: Zero-Shot Text-to-Image Synthesis with Disentangled Representations" have leveraged DiT for text-to-image generation, highlighting the strong potential of transformer architectures in generative tasks. Although prior studies primarily focus on general-purpose or text-driven generative tasks, our research investigates the application of diffusion transformers to the specialized and complex task of zero-shot object-level image customization (ZSOIC). ZSOIC involves extracting and integrating intricate attributes from a source image, including object appearance, identity, and background, into a unified target image, which poses unique challenges in achieving cohesive synthesis ____ (See Fig. \ref{fig:DreamBooth}).

\begin{figure*}%[!htbp]
  \centering
  \includegraphics[width=1\linewidth]{./figs2/proposal_2025}
  \vspace{-16pt}
  \caption{Zero-shot object customization and its practical applications. Images are generated using our \texttt{E-MD3C} model.}
  \label{fig:task}
  \vspace{-12pt}
\end{figure*}

\paragraph{Mask Prediction Modeling.}
Mask-based vision models, drawing inspiration from mask language models like Devlin, J., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" ____, have demonstrated exceptional scalability and performance across diverse tasks. Prominent examples include He, R., et al., "Masked Autoencoders Imprvied (MAE): Towards Masked Self-Supervised Learning" for self-supervised learning (SSL) and models such as Chen, L. C., et al., "MaskGIT: Discrete Representation Learning with Masked Cross-Entropy" ____, Li, Z., et al., "Muse: A Multi-Task Masked Language Model", and He, R., et al., "MAVGIT: Masked Autoencoders Imprvied (MAE) for Vision Tasks" which learn discrete token distributions for image generation. These methods adopt non-autoregressive modeling with a cross-entropy loss to predict token indices within a codebook. Diverging from this paradigm, Gu, Y., et al., "Masked Diffusion Transformers: Improving Efficiency and Performance" introduced an asymmetric masking schedule that enhances contextual representation learning in denoising diffusion transformers, achieving superior class-conditional image generation on ImageNet. Building on this foundation, Zhang, J., et al., "X-MDPT: Masked Diffusion Transformers for Pose-Guided Human Image Generation" proposed X-MDPT, demonstrating the potential of masked diffusion transformers for pose-guided human image generation with improved performance and efficiency. Similarly, Xu, T., et al., "MDT-A2G: Masked Diffusion Transformers for Gesture Generation" applied masked diffusion transformers to gesture generation, while Chen, L. C., et al., "QA-MDT: Quality-aware Masked Diffusion Transformers for Music Generation" extended this framework for music generation. Further advancements include masked diffusion transformers for audio generation, as explored by Gu, Y., et al., "MDSGen: Masked Diffusion Generative Models for Audio Synthesis", achieving impressive results and inspiring subsequent research. In this work, we focus on leveraging the strengths of masked diffusion transformers ____ for the task of zero-shot object-level image customization, introducing \texttt{E-MD3C}, a disentangled design with multiple conditioning mechanisms to enable effective and targeted object generation with various applications as shown in Fig. \ref{fig:task}. 

\begin{figure*}%[!htbp]
  \centering
  \includegraphics[width=0.95\linewidth]{./figs2/arch2}
  \vspace{-8pt}
  \caption{Overview of the \texttt{E-MD3C} framework for zero-shot object customization. During training, 30\% of patched tokens are masked, and the noisy input is processed by the Diffusion Transformer, conditioned on a collected vector ($D=1024$) via AdaLN modulation ____. A mask prediction objective models token relationships. The \textcolor{red}{red arrow} $\color{red}\rightarrow$ is training-only, the \textcolor{black}{black arrow} $\color{black}\rightarrow$ is used for both training and inference, and the \textcolor{green}{green arrow} $\color{green}\rightarrow$ is inference-only.}
  \label{fig:arch}
  \vspace{-12pt}
\end{figure*}