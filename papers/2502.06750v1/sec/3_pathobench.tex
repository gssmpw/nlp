\clearpage
\noindent\textbf{\large{Standardizing benchmarking with Patho-Bench}} \label{sec5}

We introduce Patho-Bench\footnote{\url{https://github.com/mahmoodlab/patho-bench}}, a Python package for large-scale model evaluation, which can manage thousands of experiments with efficient parallelism. In addition, we publicly release a unified set of tasks with clean labels and predefined train-test splits\footnote{\url{https://huggingface.co/datasets/MahmoodLab/patho-bench}}. Patho-Bench is the most extensive and diverse public benchmark for computational pathology released to date.
% train-test splits across a diverse range of downstream datasets, covering major WSI-level and patient-level pathology tasks. 

\subsection{Downstream tasks and data splits.}
We curated canonical train-test splits for 42 publicly available WSI-level and patient-level tasks, which we categorized into six families: morphological subtyping, tumor grading, molecular subtyping, mutation prediction, treatment response and assessment, and survival prediction. A description of each task family is provided in \Cref{tab:tasks}. Detailed information on each dataset and task is provided in \cite{vaidya2024amolecular}.
% We open-source the labels and train-test splits associated with the 42 public datasets, and provide links for users to download the raw WSI files from the original sources.
% We will maintain this public resource and continue expanding it with more datasets and tasks over time.

\begin{table}[ht]
    \centering
    \caption{\textbf{Overview of families of tasks in Patho-Bench (42 public tasks in total).}}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabularx}{\textwidth}{lXc}
        \toprule
        \textbf{Task Family} & \textbf{Description} & \textbf{\# Tasks} \\
        \midrule
        Morphological subtyping & Classifying different disease subtypes & 4 \\
        Tumor grading & Assigning a grade based on cellular differentiation and growth patterns & 2 \\
        Molecular subtyping & Predicting molecular alterations as tested with immunohistochemistry & 3 \\
        Mutation prediction & Predicting genetic mutations in tumors as tested with next-generation sequencing & 21 \\
        Treatment response and assessment & Evaluating how patients respond to treatment & 6 \\
        Survival prediction & Predicting time-to-event for patient survival outcomes & 6 \\
        \bottomrule
    \end{tabularx}
    \label{tab:tasks}
\end{table}

Each task is associated with two artifacts: A CSV file and a YAML file. The CSV file contains one row per slide, with columns indicating the patient and slide identifier, task label, and train-test assignments for each fold. The YAML file contains additional task metadata such as whether it is a patient-level or slide-level task, the number of samples, number of folds, and task-dependent canonical performance metric (e.g., balanced accuracy, area under the receiver operating characteristic curve, quadratic weighted kappa, or concordance index). Most tasks use 5-fold cross-validation, while some tasks use 50-fold Monte Carlo sampling due to having very few samples. For a small number of tasks that already have canonical single-fold train-test splits reported in the literature, we use their official splits. Otherwise, we maintain a train-test ratio of 80\%:20\% in each split. We have deliberately refrained from assigning validation samples in the public splits, leaving it up to the user to decide whether a validation set is suitable for their task and how to reassign any training samples toward a validation set. 

\subsection{Evaluation frameworks.}
In Patho-Bench, models can be evaluated using three different parametric evaluation strategies (linear probing, Cox proportional-hazards regression, and supervised finetuning) and one non-parametric one (case retrieval). Supervised finetuning uses frozen patch-level features, while the remaining evaluation frameworks use frozen slide- or patient-level features. All evaluation frameworks in Patho-Bench use Trident-extracted patch features. Patho-Bench incorporates the slide-level feature extraction abilities of Trident while also adding the ability to extract patient-level features using the data split CSVs.
% Patho-Bench can be used for evaluating both patch and slide encoder foundation models from Trident.

\subsection{Parallelization.}
A major challenge of running large benchmarks is the combinatorial experimentation space. For instance, running benchmarks for five FMs across 50 tasks and three evaluation frameworks per task adds up to already 750 individual experiments. This does not even consider hyperparameter sweeps, multiple folds of training and testing, or few shot variations of each task, where benchmarking a single model can easily scale to hundreds of thousands of training and testing runs. It is infeasible to loop over all possible experimental configurations in a serial manner, and manually parallelizing experiments takes a significant amount of effort and is prone to human error. To mitigate this challenge, Patho-Bench can run a set of experiments using task-level parallelization. The user can easily define a parallelization strategy in a configuration file, which will automatically launch and monitor all requested experiments in the terminal using the popular Linux utility Tmux. For experiments requiring GPUs, Patho-Bench will automatically perform load balancing across available GPUs. At the end of a sweep, all experiment results are automatically gathered into a single output file.
% For most computing hardware available at the level of an academic lab, there are also limits to the number of parallel processes that can be run simultaneously before running into hardware bottlenecks.

In designing Patho-Bench, we acknowledged that users are likely to prefer varying levels of complexity and that some users may not need the full parallelized implementation. Therefore, for each type of evaluation, we expose both high-level scripts for high-throughput experiment sweeps as well as low-level modules that enable users to run single experiments. The implementation is highly modular, making it easy to reuse and adapt for specialty use cases. Users can also incorporate Patho-Bench task labels and data splits into their own evaluation pipelines.
% By open-sourcing Patho-Bench, we hope to make comprehensive and reproducible benchmarking pipelines as easy as possible for the community.
