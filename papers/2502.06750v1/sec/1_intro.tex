\clearpage
\noindent\textbf{\large{Introduction}} \label{sec:intro}

The scale of available histology data is rapidly expanding as more medical institutions transition to fully digitized workflows, supported by large-scale retrospective tissue slide scanning. Many labs and institutions now have access to petabytes of data accounting for millions of diagnostic slides. This shift has significantly advanced AI applications in pathology, evolving from early studies with hundreds of slides\cite{bejnordi2017diagnostic} to datasets with tens of thousands\cite{bulten2022artificial}, and now, with the advent of foundation models (FMs)\cite{chen2024towards,vorontsov2024foundation,xu2024whole}, to training on millions. Foundation models offer a shared basis for developing task-specific models at minimal cost, enabling adaptation to various clinically relevant tasks such as predicting histologic subtypes, molecular biomarkers, and treatment response directly from the tissue morphology\cite{kather2020pan,lu2021ai}.

However, current open-source tools for whole-slide image (WSI) processing are not designed for scaling to very large repositories with support for multiple stains including hematoxylin and eosin (H\&E), immunohistochemistry and special stains\cite{pocock2022tiatoolbox,elnahhas2025stamp}. With a global acceleration in the number of publicly available foundation models in pathology, it also becomes increasingly complex to understand their strengths and weaknesses compared to existing models. This is compounded by the lack of reusable open-source code with standardized train-test data splits on diverse downstream tasks. 

To advance the field, the community needs new tools for foundation model evaluation based on very large benchmarks. Model assessment must be based on several metrics aggregated across many tasks, hyperparameter combinations and evaluation strategies, such as linear probing, supervised fine-tuning, and case retrieval. To address these challenges, we release a set of software packages and downstream tasks that aim to standardize foundation model benchmarking: \textbf{Trident}, a package for whole-slide image processing with support for state-of-the-art patch-level and slide-level foundation models\cite{wang2024chief,shaikovski2024prism,vaidya2024amolecular,ding2024titan}, \textbf{Patho-Bench}, a library for benchmarking FMs under several evaluation strategies, and \textbf{Patho-Bench data splits}, with labels for 42 clinically relevant pathology tasks.


% DEPRECATED


% In addition, with more and more models released, keeping track of all models is becoming increasingly challenging. This observation is compounded by the fact that there is a global lack of standardized benchmark in the community. 

% Computational pathology aims to develop models for making predictions from high-resolution digitized tissue sections, known as whole-slide images (WSIs). WSIs are far larger than images encountered in general-purpose computer vision workflows; in fact, they are too large to be opened at full resolution on most modern computers. Therefore, researchers have focused on developing artificial intelligence (AI) models for compressing WSIs into information-dense vectors known as embeddings. These models are an example of foundation models (FMs) \cite{bommasaniOpportunitiesRisksFoundation2022} that act as a shared basis for the development of task-specific models. FMs for pathology can be adapted for a wide array of clinically relevant downstream tasks, including the prediction of histologic subtypes, molecular biomarkers, and response treatment, among others.

% The field has converged on a dual-stage pipeline for encoding WSIs: first, extracting embeddings from WSI patches, and then processing the collection of patch embeddings to obtain a single embedding representing the entire slide. WSIs are typically divided into high-resolution patches of a few hundred pixels wide, e.g., 256$\times$256 at 0.5 microns per pixel (mpp), resulting in thousands of patches per slide. Two opportunities for foundation model development emerge from this paradigm: building patch-level encoders, which extract vector embeddings from raw patch images, and building slide-level encoders, which aggregate patch-level embeddings into a slide-level embedding. There has been an explosion of interest at both levels, with earlier research focusing on patch-level foundation models and more recent work focusing on slide-level foundation models\cite{dingMultimodalWholeSlide2024, shaikovskiPRISMMultiModalGenerative2024, wangPathologyFoundationModel2024, xuWholeslideFoundationModel2024}.
% At least four major slide-level foundation models were released in 2024 alone .

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{figs/workflow.png}
%     \caption{\textbf{Foundation models in a typical computational pathology workflow.} First, the tissue-containing regions (outlined in green) must be identified and divided into small patches (shown in red). Patch and slide encoders serve to compress the data from a large whole-slide image into more compact representations (vector embeddings). These encoders aim to retain important diagnostic and prognostic information. The quality of the slide embedding (and, therefore, the intermediate encoders) is determined by benchmarking on a wide variety of downstream tasks, such as subtyping, survival prediction, and grading.}
%     \label{fig:workflow}
% \end{figure}

% keep this paragraph:

% Advances in foundation models for pathology have accelerated, with new models frequently released. With each new contribution, it is important for the community to understand its strengths and weaknesses compared to existing models. Moreover, during model development, it is useful for researchers to access performance benchmarks so that the field continually pushes forward the frontier of FM performance on the most challenging pathology tasks. 
% Currently, most labs are developing their own in-house pipelines for model benchmarking.
% % to check
% This is due to several factors, including the lack of reusable open-source code, the lack of standardized train-test data splits for downstream tasks, and the intrinsic complexity of working with gigapixel whole-slide images.
% % to change
% To address these challenges, we release a set of software packages and downstream tasks that aim to standardize the FM benchmarking pipeline and enable users to run benchmarking efforts at the scale needed for the foundation modeling era.

% experimentation compared with evaluating task-specific ("narrow") models. A relatively small number of benchmarks is typically sufficient to cover the input-output distribution of narrow models and provide a trustworthy estimate of model performance after deployment. In contrast, foundation models are inherently generalist and intended for potential future applications on an undefined number of possible downstream tasks. Evaluating a foundation model requires a much wider array of proxy tasks that aim to cover the input-output distribution as broadly as possible.
% Consequently, the experimentation infrastructure for benchmarking a foundation model can be substantially more complex than for benchmarking a task-specific model.

% Talk about CLAM and how it has been used by so many people
% There are large disparities in how benchmarking is actually conducted
% Everyone is using their own preprocssing and eval setup
% We are releasing these tools to standardize these steps
% Focus on need for standardization