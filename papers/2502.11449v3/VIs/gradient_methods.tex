\subsubsection{Mirror Gradient Algorithm}

The canonical class of first-order methods  for VIs is the class of \mydef{mirror gradient algorithms} \cite{nemirovskij1983problem} which are parameterized by a kernel function $\kernel: \set \to \R$ which induces a a Bregman divergence $\divergence[\kernel]: \set \times \set \to \R$ that defines the update function $\kordermethod^{\mathrm{MG}}$ of the algorithm as $\kordermethod^{\mathrm{MG}}(\othervartuple,  \vioper(\othervartuple)) \doteq \argmin\limits_{\vartuple \in \set} \left\{ \innerprod[{\vioper(\othervartuple)}][{ \vartuple - \othervartuple}] + \frac{1}{2 \learnrate[][ ]} \divergence[\kernel][\vartuple][{\othervartuple}]\right\} 
$. When the kernel function is chosen s.t. $\kernel(\vartuple) \doteq \frac{1}{2}\norm[\vartuple]^2$, the mirror gradient method $\kordermethod^{\mathrm{MG}}$ reduces to the well-known \mydef{projected gradient} method $\kordermethod^{\mathrm{PG}}$  \cite{cauchy1847methode}, i.e., $\kordermethod^{\mathrm{PG}}(\othervartuple,  \vioper(\othervartuple)) \doteq \proj[\outerset] \left[ \othervartuple - \learnrate[ ][ ] \vioper(\othervartuple) \right]$.


\if 0
\begin{algorithm}
\caption{Mirror Gradient Algorithm}\label{alg:VI_mirror_proj_method}
\textbf{Input:} $\set, \vioper,  \kernel, \numhorizons, \learnrate[ ][ ], \vartuple[][][0]$\\
\textbf{Output:} $\{\vartuple[][][\numhorizon]\}_{\numhorizon}$
\begin{algorithmic}[1]
\For {$\numhorizon = 1, \hdots, \numhorizons $}
    \State $\vartuple[][][\numhorizon +1] \gets 
    \argmin\limits_{\vartuple \in \set} \left\{ \innerprod[{\vioper(\vartuple[][(\numhorizon)])}][{ \vartuple - \vartuple[][(\numhorizon)]}] + \frac{1}{2 \learnrate[][ ]} \divergence[\kernel][\vartuple][{\vartuple[][(\numhorizon)]}]\right\}  $
\EndFor
\State \Return $\{\vartuple[][][\numhorizon]\}_{\numhorizon}$
\end{algorithmic}
\end{algorithm}

, the Bregman divergence corresponds to the Euclidean square norm, i.e., $\divergence[\kernel][\vartuple][{\othervartuple}] \doteq \norm[{\vartuple - \othervartuple}]^2$, in which case 


\begin{algorithm}
\caption{Project Gradient Algorithm}\label{alg:VI_proj_method}
\textbf{Input:} $\set, \vioper,  \numhorizons, \learnrate[ ][ ], \vartuple[][][0]$\\
\textbf{Output:} $\{\vartuple[][][\numhorizon]\}_{\numhorizon}$
\begin{algorithmic}[1]
\State Initialize $\vartuple[][][1] \in \set$ arbitrarily
\For {$\numhorizon = 1, \hdots, \numhorizons $}
    \State $\vartuple[][][\numhorizon +1] \gets 
    \proj[\outerset] \left[ \vartuple[][][\numhorizon] - \learnrate[ ][ ] \vioper(\vartuple[][][\numhorizon]) \right]
    = \argmin\limits_{\vartuple \in \set} \left\{ \innerprod[{\vioper(\vartuple[][(\numhorizon)])}][{ \vartuple - \vartuple[][(\numhorizon)]}] + \nicefrac{1}{2 \learnrate[][ ]} \| \vartuple - \vartuple[][(\numhorizon)] \|^2 \right\}  $
\EndFor
\State \Return $\{\vartuple[][][\numhorizon]\}_{\numhorizon}$
\end{algorithmic}
\end{algorithm}
\fi

Unfortunately, while the average of the iterates of the mirror gradient method can be shown to converge to a strong solution asymptotically for monotone and Lipschitz-continuous VIs, it is in general only possible to prove polynomial-time computation of a $\varepsilon$-weak solution in such VIs which does not necessarily imply convergence to a $\varepsilon$-strong solution (see, for instance, Proposition 8 and Appendix D of \citet{liu2021first}).
More importantly, in general the sequence of iterates generated by the mirror gradient method is not guaranteed to converge, as shown by \Cref{example:monotone_non_convergence} in \Cref{sec_app:vi_examples}.


\subsubsection{Global Convergence of the Mirror Extragradient Algorithm}

As the iterates of the mirror gradient method do not asymptotically  converge to a strong or weak solution, and it is not possible to obtain polynomial-time computation of a $\varepsilon$-strong solution by averaging the iterates, we now introduce a novel class of first order methods, namely the class of \mydef{mirror extragradient algorithms} (\Cref{alg:VI_mirror_extragrad}) which similar to the class of mirror gradient methods are parameterized by a kernel function $\kernel: \set \to \R$ that induces a Bregman divergence $\divergence[\kernel]: \set \times \set \to \R$ defining the update function $\kordermethod^{\mathrm{MEG}}$ of the algorithm which can be written in terms of $\kordermethod^{\mathrm{MG}}$ as follows: $\kordermethod^{\mathrm{MEG}}(\othervartuple,  \vioper(\othervartuple)) \doteq  \kordermethod^{\mathrm{MG}}\left(\othervartuple,  \vioper(\kordermethod^{\mathrm{MG}}( \othervartuple, \vioper(\othervartuple)))\right)$. 


\input{algos/mirror_extragrad}


The mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad}) generalizes the well-known extragradient algorithm which is known to asymptotically converge to a strong solution \cite{popov1980modification}, and allows for the polynomial-time computation of a $\varepsilon$-strong solution \cite{nemirovski2004prox, golowich2020eglast, cai2022tight}.
In particular, when the kernel function for the mirror extragradient method is chosen s.t. $\kernel(\vartuple) \doteq \frac{1}{2}\norm[\vartuple]^2$, the mirror gradient method reduces to the extragradient method. 

\if 0
\begin{algorithm}
\caption{Extragradient Algorithm}\label{alg:VI_extragrad}
\textbf{Input:} $\set, \vioper,  \numhorizons, \learnrate[ ][ ], \vartuple[][][0]$\\
\textbf{Output:} $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$
\begin{algorithmic}[1]
\For {$\numhorizon = 1, \hdots, \numhorizons $}
    \State $\vartuple[][][\numhorizon + 0.5] \gets 
    \proj[\outerset] \left[ \vartuple[][][\numhorizon] - \learnrate[ ][ ] \vioper(\vartuple[][][\numhorizon]) \right]
    = \argmin\limits_{\vartuple \in \set} \left\{ \innerprod[{\vioper(\vartuple[][(\numhorizon)])}][{ \vartuple - \vartuple[][(\numhorizon)]}] + \nicefrac{1}{2 \learnrate[][ ]} \| \vartuple - \vartuple[][(\numhorizon)] \|^2 \right\}  $
    \State $\vartuple[][][\numhorizon + 1] \gets 
    \proj[\outerset] \left[ \vartuple[][][\numhorizon] - \learnrate[ ][ ] \vioper(\vartuple[][][\numhorizon + 0.5]) \right]
    = \argmin\limits_{\vartuple \in \set} \left\{ \innerprod[{\vioper(\vartuple[][(\numhorizon + 0.5)])}][{ \vartuple - \vartuple[][(\numhorizon)]}] + \nicefrac{1}{2 \learnrate[][ ]} \| \vartuple - \vartuple[][(\numhorizon)] \|^2 \right\}  $
\EndFor
\Return $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$
\end{algorithmic}
\end{algorithm}
\fi 

A seminal result by \citet{nemirovski2004prox} shows that the average of the iterates output by the extragradient algorithm are a $\varepsilon$-strong solution for any monotone VI with a Lipschitz-continuous optimality operator when the algorithm is run for $\numhorizons \in O(\nicefrac{1}{\varepsilon})$ time-steps. Additionally, \citet{golowich2020eglast, cai2022tight} show that in the same setting, best iterate convergence to a $\varepsilon$-strong solution in $O(\nicefrac{1}{\varepsilon^2})$ operations. More recently, \citet{huang2023beyond} have extended the same polynomial-time computation result to VIs which satisfy the weaker Minty condition rather than monotonicity assumption. We extend at present this result to mirror extragradient algorithm with the following theorem. We introduce the result in its full generality as we will subsequently be applying this general result in \Cref{section:walrasian_economies}. The following result states that a $\varepsilon$-strong solution of any VI $(\set, \vioper)$ which satisfies the Minty condition given that the VI $(\set, \vioper)$ is \mydef{pathwise Bregman continuous} over the outputs of the mirror extragradient method $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon]\}_{\numhorizon}$, i.e., there exists $\lsmooth \geq 0$, s.t. for all $\numhorizon \in [\numhorizons]$, $\frac{1}{2}\|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2 \leq \lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$. As we will show in \Cref{section:walrasian_economies}, this weaker pathwise Bregman-continuity condition can be useful in the analysis of price-adjustment processes, and has been in the past used in the analysis of price-adjustment processes (see, for instance, \citet{fisher-tatonnement}). We include all omitted results and proofs in \Cref{sec_app:vis}.




\begin{restatable}[Mirror Extragradient Method Convergence]{theorem}{thmmirrorextragradglobal}\label{thm:mirror_extragradient_global_convergence}
    Let $(\set, \vioper)$ be a VI  satisfying the Minty condition with $\set$ non-empty, compact, and convex; and $\kernel$ a $1$-strongly-convex and $\kernelsmooth$-Lipschitz-smooth kernel function. Consider the mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad})  run with the VI $(\set, \vioper)$, the kernel function $\kernel$, a step size $\learnrate[ ][ ] > 0$, a time horizon $\numhorizons \in \N$, and outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$. Suppose that there exists $\lsmooth \in (0, \frac{1}{\sqrt{2}\learnrate[ ][ ]}]$, s.t. $\frac{1}{2}\|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2 \leq \lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$. Then, the following bound holds:\\
    $
     \min_{k = 0, \hdots, \numhorizons} \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k + 0.5] - \vartuple \rangle \leq  \frac{2 (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}
    $
    where $\vartuple[][*] \in \mvi(\set, \vioper)$ is a weak solution of the VI $(\set, \vioper)$.
    
    In addition, let $\bestiter[\vartuple][\numhorizons] \in \argmin_{\vartuple[][][k+0.5] : k = 0, \hdots, \numhorizons} \divergence[\kernel](\vartuple[][][k+0.5], \vartuple[][][k])$, then, for some choice of time horizon $\numhorizons \in O(\frac{\kernelsmooth^2\diam(\set)^2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}{\learnrate[ ][ ]^2\varepsilon^2})$,  $\bestiter[\vartuple][\numhorizons]$ is a $\varepsilon$-strong solution of $(\set, \vioper)$, and $\lim_{\numhorizon \to \infty} \vartuple[][][\numhorizon +0.5] = \lim_{\numhorizon \to \infty} \vartuple[][][\numhorizon] \in \svi(\set, \vioper)$ is a strong solution of the VI $(\set, \vioper)$.\footnote{    First, note that the assumption that $\kernel$ is $1$-strongly-convex is without loss of generality since any $\mu$-strongly-convex kernel $\kernel^\prime$ can be converted to a $1$-strongly-convex kernel $\frac{1}{\mu}\kernel^\prime$.
% 
    Second, while we present our convergence result using the pathwise Bregman-continuity assumption for generality, we note that the pathwise Bregman-continuity assumption is guaranteed to hold whenever $\vioper$ is Bregman-continuous. Further, since if $\kernel$ is $1$-strongly-convex, Lipschitz-continuity of $\vioper$ implies its Bregman continuity, hence, a direct corollary of our result is that the mirror extragradient algorithm can compute a $\varepsilon$-strong solution in $O(\nicefrac{1}{\varepsilon^2})$ evaluation of $\vioper$ for Lipschitz-continuous VIs which satisfy the Minty condition.}
\end{restatable}
% 
% With our main theorem in place, some remarks are in order.
% 
% \begin{remark}\label{remark:mirror_extragrad_global_convergence_thm}

% \end{remark}

\if 0
\begin{definition}[Bregman-continuity]
    Given a \mydef{continuity modulus} $\lipschitz \geq 0$, a function $\obj: \set \to \R$ is $\lipschitz$-Bregman-continous iff $\divergence[\kernel](\obj(\vartuple), \obj(\othervartuple)) \leq \lipschitz \divergence[\kernel](\vartuple, \othervartuple)$ 
\end{definition}

\begin{corollary}[Convergence to weak solution]
    Convergence to $\varepsilon$-weak solution under monotonicity, and to weak solution under pseudo-monotonicity
\end{corollary}
\fi


\if0
\begin{lemma}
For the extragradient method, assume that $\vioper$ is Lipschitz continuous with constant $\lsmooth$, and $t \leq \frac{1}{\sqrt{2\lsmooth}}$, the following inequality holds:
\begin{align}
\langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle + \frac{1}{4\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2 \leq \frac{1}{2\learnrate[ ][ ]} \left[ \|\vartuple[][][k] - \vartuple\|^2 - \|\vartuple[][][k+1] - \vartuple\|^2 \right] \enspace .    
\end{align}
\end{lemma}
\begin{proof}
By the first order optimality conditions of $\vartuple[][][k+0.5]$, we have for all $\vartuple \in \set$:
% 
\begin{align*}
\langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle + \frac{1}{\learnrate[ ][ ]} \langle \vartuple[][][k+0.5] - \vartuple[][][k], \vartuple - \vartuple[][][k+0.5] \rangle \geq 0.
\end{align*}

Substituting $\vartuple = \vartuple[][][k+1]$ above, we have:
\begin{align*}
\langle \vioper(\vartuple[][][k]), \vartuple[][][k+1] - \vartuple[][][k+0.5] \rangle 
&\geq \frac{1}{\learnrate[ ][ ]} \langle \vartuple[][][k+0.5] - \vartuple[][][k], \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
&= \frac{1}{2\learnrate[ ][ ]} \left( \|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2 + \|\vartuple[][][k+1] - \vartuple[][][k+0.5]\|^2 - \|\vartuple[][][k+1] - \vartuple[][][k]\|^2 \right).
\end{align*}

On the other hand, by the optimality condition at $\vartuple[][][k+1]$, we have for all $\vartuple \in \set$:
\begin{align*}
\langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle + \frac{1}{\learnrate[ ][ ]} \langle \vartuple[][][k+1] - \vartuple[][][k], \vartuple - \vartuple[][][k+1] \rangle \geq 0 \enspace .
\end{align*}

Hence, for all $\vartuple \in \set$:
\begin{align*}
\langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle 
&\geq \frac{1}{\learnrate[ ][ ]} \langle \vartuple[][][k+1] - \vartuple[][][k], \vartuple[][][k+1] - \vartuple \rangle \\
&= \frac{1}{2\learnrate[ ][ ]} \left( \|\vartuple[][][k+1] - \vartuple\|^2 + \|\vartuple[][][k] - \vartuple\|^2 - \|\vartuple[][][k+1] - \vartuple[][][k]\|^2 \right) \enspace.
\end{align*}


Continue with the above inequality, for any given $\vartuple \in \set$, we have:

\begin{align}
    &\frac{1}{2\learnrate[ ][ ]} \left( \|\vartuple[][][k+1] - \vartuple \|^2 + \|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2 - \|\vartuple[][][k] - \vartuple \|^2 \right)\\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle \\
    &= \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &= \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \langle \vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \notag \\
    &\quad + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\| \cdot \|\vartuple[][][k+0.5] - \vartuple[][][k+1]\| \notag \\
    &\quad + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \frac{t \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2}{2} \notag \\
    &\quad + \frac{\|\vartuple[][][k+0.5] - \vartuple[][][k+1]\|^2}{2\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \frac{t\lsmooth^2 \|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2}{2} \notag \\
    &\quad + \frac{\|\vartuple[][][k+0.5] - \vartuple[][][k+1]\|^2}{2\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle.
\end{align}

Since \(\learnrate[ ][ ] \leq \frac{1}{\sqrt{2} \lsmooth}\), and with (23), we have
\begin{align}
    \frac{1}{2\learnrate[ ][ ]} \left( \|\vartuple[][][k+1] - \vartuple\|^2 + \|\vartuple[][][k+1] - \vartuple[][][k]\|^2 - \|\vartuple[][][k] - \vartuple\|^2 \right) 
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \frac{\|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2}{4\learnrate[ ][ ]} \notag \\
    &\quad + \frac{1}{2\learnrate[ ][ ]} \left(-\|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2 - \|\vartuple[][][k+1] - \vartuple[][][k]\|^2 \right).
\end{align}

Canceling out terms, we simplify the above inequality into
\begin{align}
    \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle + \frac{1}{4t} \|\vartuple[][][k+0.5] - \vartuple[][][k]\|^2 
    &\leq \frac{1}{2t} \left( \|\vartuple[][][k] - \vartuple \|^2 - \|\vartuple[][][k+1] - \vartuple \|^2 \right).
\end{align}
\end{proof}

\begin{theorem}
    
\end{theorem}
\fi



\subsubsection{Local Convergence of the Mirror Extragradient Algorithm}

Unfortunately, beyond VIs for which the Minty condition holds, it seems implausible to devise a first-order method that converges to strong solutions. To see this, we refer the reader to \Cref{example:non_convergence_non_minty} in \Cref{sec_app:vi_examples}.
This is perhaps not surprising, since the computation of a $\vepsilon$-strong solution for Lipschitz-continuous VIs is in general a PPAD-complete problem \cite{kapron2024computational}. 
Nevertheless, it is still of interest to investigate under what conditions can one guarantee the local convergence of the mirror extragradient algorithm to a strong solution. As we will show, we can guarantee local convergence by assuming the algorithm is initialized close enough to a \emph{local} weak (or Minty) solution \citet{aussel2024variational}. 
Given a VI $(\set, \vioperset)$, and a \mydef{locality parameter} $\vdelta \geq 0$, a \mydef{$\vdelta$-local weak solution} of the VI is a $\vartuple[][*] \in \set$ that satisfies for all $\vartuple \in \set \cap \closedball[\vdelta][{\vartuple[][*]}], \vioper(\vartuple) \in \vioperset(\vartuple)$, $\innerprod[{\vioper(\vartuple[][])}][{\vartuple - \vartuple[][*]}] \leq 0$
%   % For any $\vdelta > 0$, a $(0, \vdelta)$-local strong (resp. weak) solution is simply called a \mydef{local strong solution} (resp. \mydef{local weak solution}).
We denote the set of $\vdelta$-local weak solutions of a VI $(\set, \vioperset)$ by $\lmvi[][\vdelta](\set, \vioperset)$. With this definition in place, we now present our local convergence result for the mirror extragradient algorithm. As we will not be directly applying this result, for simplicity we present it under the assumption of Lipschitz-continuity, and then turn our attention to our Walrasian economies application.
% 
% The aim of $(\vepsilon, \vdelta)$-local strong/weak solution is to bridge the gap between the intractability results of \citet{daskalakis2020complexity} which make it impossible to compute a global solution, and the convergence results of \citet{liu2021first} which use a notion of convergence which does not allow one to clearly quantify the trade-offs, namely between the approximation parameter $\vepsilon$ and the locality parameter $\vdelta$, in seeking to compute a solution to a VI. 
% 
\begin{restatable}[Mirror Extragradient Method Local Convergence]{theorem}{thmvimirrorextragradlocal}\label{thm:vi_mirror_extragrad_local}
    Let $(\set, \vioper)$ be a $\lsmooth$-Lipschitz-continuous VI, $\kernel$ a $1$-strongly-convex and $\kernelsmooth$-Lipschitz-smooth kernel function, and let $\learnrate[ ][ ] \in  \left(0, \frac{1}{\sqrt{2}\lsmooth}\right]$. Suppose that $\|\vioper\|_\infty \leq \lipschitz < \infty$, and that for some $\vartuple[][*] \in \lmvi[][\delta](\set, \vioper)$ $\delta$-local weak solution, the initial iterate $\vartuple[][][0] \in \set$ is chosen so that $\sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]} \leq \delta - \learnrate[ ][ ] \lipschitz$.
     
     Consider the mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad})  run with the VI $(\set, \vioper)$, the kernel function $\kernel$, the step size $\learnrate[ ][ ]$, initial iterate $\vartuple[][][0]$, an arbitrary time horizon $\numhorizons \in \N$, and outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$. Then, we have:
    $
     \min_{k = 0, \hdots, \numhorizons} \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k + 0.5] - \vartuple \rangle \leq  \frac{\sqrt{2} (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\delta}{\sqrt{\numhorizons}}
    $

    In addition, let $\bestiter[\vartuple][\numhorizons] \in \argmin_{\vartuple[][][k+0.5] : k = 0, \hdots, \numhorizons} \divergence[\kernel](\vartuple[][][k+0.5], \vartuple[][][k])$. Then, for some choice of $\numhorizons \in O(\nicefrac{1}{\varepsilon^2})$,  $\bestiter[\vartuple][\numhorizons]$ is a $\varepsilon$-strong solution of $(\set, \vioper)$.
\end{restatable}