Before we start our analysis of the mirror extragradient method (\Cref{alg:VI_mirror_extragrad}), we first prove the following technical lemma on Bregman divergences.

\begin{lemma}[Bregman Triangle Lemma]\label{lemma:bregman_triangle}
    Consider the Bregman divergence $\divergence[\kernel]: \set \times \set \to \R$ associated with a differentiable kernel function $\kernel: \set \to \R$. Let $\x, \y, \z \in \set$, we then have:
    \begin{align}
        \divergence[\kernel][\x][\z] + \divergence[\kernel][\y][\x] - \divergence[\kernel][\y][\z] = \langle \nabla \kernel(\x) - \nabla \kernel(\z), \x - \y \rangle.
    \end{align}
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:bregman_triangle}]

For all $\x, \y, \z \in \set$, we have:
\begin{align*}
& \divergence[\kernel][\x][\z] + \divergence[\kernel][\y][\x] - \divergence[\kernel][\y][\z]\\
&= \left[\kernel(\x) - \kernel(\z) - \langle \nabla \kernel(\z), \x - \z \rangle \right] +  \left[\kernel(\y) - \kernel(\x) - \langle \nabla \kernel(\x), \y - \x \rangle \right]
- \left[ \kernel(\y) - \kernel(\z) - \langle \nabla \kernel(\z), \y - \z \rangle \right]\\
&= - \langle \nabla \kernel(\z), \x - \z \rangle  - \langle \nabla \kernel(\x), \y - \x \rangle
+ \langle \nabla \kernel(\z), \y - \z \rangle\\
&= \langle \nabla \kernel(\z) - \nabla \kernel(\x), \y - \x \rangle.
\end{align*}
\end{proof}


\subsection{Global Convergence of the Mirror Extragradient Algorithm}

With the above technical lemma in hand, we are now ready to prove a progress lemma for the mirror extragradient method, which describes how the algorithm progresses from one iteration to another. Note that under the Minty condition, the following lemma implies  convergence to a weak solution since setting $\vartuple \doteq \vartuple[][*] \in \mvi(\set, \vioper)$, we obtain $\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] > \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k + 1]}]$ for all $k \in \N$ (i.e., the distance to the weak solution $\vartuple[][*]$ is strictly decreasing). In addition, note that under the assumptions of the lemma, the VI $(\set, \vioper)$ is continuous, hence also implying convergence to a strong solution since any weak solution is also a strong solution in continuous VIs.

\begin{lemma}[Mirror Extragradient Progress]\label{lemma:mirror_extragrad_progress}
Consider the mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad}), run with a VI $(\set, \vioper)$ where $\set$ in non-empty, compact, and convex, and a 1-strongly-convex kernel function $\kernel$, a step size $\learnrate[ ][ ] > 0$, a time horizon $\numhorizons \in \N$, and outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$. Suppose that there exists $\lsmooth \geq 0$, s.t. $\frac{1}{2}\|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2 \leq \lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$. Then, for all $k \in \N$ and $\vartuple \in \set$, the following inequality holds for its outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$:
\begin{align}
\divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] \geq  \learnrate[ ][ ]\langle \vioper(\vartuple[][][k+0.5]),\vartuple[][][k+0.5]  - \vartuple \rangle + \left( 1 - (\learnrate[ ][ ]\lsmooth)^2 \right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
\end{align}
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:mirror_extragrad_progress}]
By the first order optimality conditions of $\vartuple[][][k+0.5]$, we have for all $\vartuple \in \set$:
% 
\begin{align*}
\langle \vioper(\vartuple[][][k]) + \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k]), \vartuple - \vartuple[][][k+0.5] \rangle \geq 0.
\end{align*}

Substituting $\vartuple = \vartuple[][][k+1]$ above, we have:
\begin{align}
\langle \vioper(\vartuple[][][k]), \vartuple[][][k+1] - \vartuple[][][k+0.5] \rangle 
&\geq \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k]) - \grad \kernel(\vartuple[][][k+0.5]),  \vartuple[][][k+1] - \vartuple[][][k+0.5] \rangle \notag \\
&= \frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] + \divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k+0.5]}] - \divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k]}] \right). \label{eq:mirror_extra_opt1}
\end{align}
where the last line was obtained by \Cref{lemma:bregman_triangle}.

On the other hand, by the optimality condition at $\vartuple[][][k+1]$, we have for all $\vartuple \in \set$:
\begin{align*}
\langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle + \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k+1]) - \grad \kernel(\vartuple[][][k]), \vartuple - \vartuple[][][k+1] \rangle \geq 0 \enspace .
\end{align*}

Hence, for all $\vartuple \in \set$:
\begin{align*}
\langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle 
&\geq \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k])  - \grad \kernel(\vartuple[][][k+1]), \vartuple - \vartuple[][][k+1] \rangle \\
&= \frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+1]}][{ \vartuple[][][k]}] + \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1 ]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right) \enspace.
\end{align*}
where the last line was once again obtained by \Cref{lemma:bregman_triangle}.

Continuing with the above inequality, for any given $\vartuple \in \set$, we have:
% 
\begin{align}
    &\frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+1]}][{ \vartuple[][][k]}] + \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right) \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+1] \rangle \\
    &= \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &= \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \langle \vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \notag \\
    &\quad + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\| \cdot \|\vartuple[][][k+0.5] - \vartuple[][][k+1]\| \notag \\
    &\quad + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle 
\end{align}
where the final line follows by the Cauchy-Schwarz inequality \cite{cauchy1821cours, schwarz1884ueber}.

Recall that by the arithmetic mean-geometric mean inequality, $\forall x, y \in \R_+$, $\frac{x+y}{2} \geq \sqrt{xy}$. Hence, applying the inequality with $x = \learnrate[ ][ ] \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2$ and $y = \nicefrac{1}{\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k+1]\|^2$
\begin{align}
     &\frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+1]}][{ \vartuple[][][k]}] + \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right) \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \frac{\learnrate[ ][ ] \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2}{2} \notag \\
    &\quad + \frac{\|\vartuple[][][k+0.5] - \vartuple[][][k+1]\|^2}{2\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \learnrate[ ][ ]\lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] \notag \\
    &\quad + \frac{\|\vartuple[][][k+0.5] - \vartuple[][][k+1]\|^2}{2\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle 
\end{align}
where the last line was obtained by the assumption that there exists $\lsmooth \geq 0$, s.t. $\frac{1}{2}\|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2 \leq \lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$.

Additionally, note that by strong convexity of $\kernel$, we have $\forall \vartuple, \othervartuple \in \set$, $\divergence[\kernel][{\vartuple}][{\othervartuple}] \geq \nicefrac{1}{2}\| \vartuple - \othervartuple\|^2$. Hence, continuing we have:
\begin{align}
    &\frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+1]}][{ \vartuple[][][k]}] + \divergence[\kernel][{\vartuple}][{\vartuple[][][k + 1]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right) \\
    % &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \learnrate[ ][ ] \divergence[\kernel][{\vioper(\vartuple[][][k+0.5])}][{\vioper(\vartuple[][][k])}] \notag \\
    % &\quad + \frac{\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k+1]}]}{\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle \\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \learnrate[ ][ ]\lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] \notag \\
    &\quad + \frac{\divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k+0.5]}]}{\learnrate[ ][ ]} + \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple[][][k+1] \rangle.
\end{align}


Plugging \Cref{eq:mirror_extra_opt1} into the above, we have:
\begin{align*}
    &\frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+1]}][{ \vartuple[][][k]}] + \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right)\\ 
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \learnrate[ ][ ]\lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] \notag \\
    &\quad + \frac{\divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k+0.5]}]}{\learnrate[ ][ ]} - \frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] + \divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k+0.5]}] - \divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k]}] \right)\\
    &\leq \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \left(\learnrate[ ][ ]\lsmooth^2 - \frac{1}{\learnrate[ ][ ]}\right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] + \frac{1}{\learnrate[ ][ ]} \divergence[\kernel][{\vartuple[][][k+1]}][{\vartuple[][][k]}] 
\end{align*}

Canceling out terms, we simplify the above inequality into:
\begin{align}
    \frac{1}{\learnrate[ ][ ]} \left(\divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] \right)\leq  \langle \vioper(\vartuple[][][k+0.5]), \vartuple - \vartuple[][][k+0.5] \rangle + \left(\learnrate[ ][ ]\lsmooth^2 - \frac{1}{\learnrate[ ][ ]}\right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
\end{align}

Multiplying both sides by $-\learnrate[ ][ ] < 0$, we obtain the lemma statement.

\end{proof}


While the above lemma implies asymptotic convergence to a strong solution, to show polynomial-time computation of a $\varepsilon$-strong solution, we have to bound the progress of the intermediate iterates $\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$ as a function of the time horizon algorithm. In the proof of the following theorem we show that we can bound this quantity, assuming that the kernel function is in addition $\kernelsmooth$-Lipschitz-smooth.




\thmmirrorextragradglobal*
\begin{proof}[Proof of \Cref{thm:mirror_extragradient_global_convergence}]
    Taking \Cref{lemma:mirror_extragrad_progress} with $\vartuple \doteq \vartuple[][*] \in \mvi(\set, \vioper)$, we have:
    \begin{align*}
        \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] &\geq  \learnrate[ ][ ] \underbrace{\langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple[][*] \rangle}_{\geq 0} + \left(1 - (\learnrate[ ][ ]\lsmooth)^2 \right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] \\
        &\geq \left(1  - (\learnrate[ ][ ]\lsmooth)^2 \right) \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]
    \end{align*}

    Multiplying both sides by $\left(1  - (\learnrate[ ][ ]\lsmooth)^2 \right)^{-1} > 0$, we have:
    \begin{align}
        \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] \right)
    \end{align}
    
    Summing up for $k  = 0, \hdots, \numhorizons$:
    \begin{align}
        \sum_{k = 0}^{\numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]  &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \sum_{k = 0}^{\numhorizons} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] \right)\\
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons+1]}] \right)\\
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] 
    \end{align}
    Dividing both sides by $\numhorizons$, we have:
    \begin{align}
        \frac{1}{\numhorizons}\sum_{k = 0}^{\numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] &\leq \frac{1}{\numhorizons\left(1 - (\learnrate[ ][ ]\lsmooth)^2\right)} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] \right)\\
        \min_{k = 0, \hdots, \numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]  &\leq \frac{1}{\numhorizons\left(1 - (\learnrate[ ][ ]\lsmooth)^2\right)} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] \right)\label{eq:intermediate_progress_bound1}
    \end{align}

    We can transform this convergence into a convergence in terms of the primal gap function. Now, recall by the first order optimality conditions of $\vartuple[][][k+0.5]$, we have for all $\vartuple \in \set$:
% 
    \begin{align*}
    \langle \vioper(\vartuple[][][k]) + \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k]), \vartuple - \vartuple[][][k+0.5] \rangle \geq 0.
    \end{align*}
Re-organizing, for all $\vartuple \in \set$, and $k \in \N$ we have:
\begin{align}
         \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle &\leq \frac{1}{\learnrate[ ][ ]} \left\| \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k])\right\| \left\|\vartuple[][][k+0.5] - \vartuple \right\|\\
         &\leq \frac{\diam(\set)}{\learnrate[ ][ ]} \left\| \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k])\right\|\\
         &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \left\| \vartuple[][][k+0.5] - \vartuple[][][k]\right\|
    \end{align}
    where the last line follow from $h$ being $\kernelsmooth$-Lipschitz-smooth.

Now, with the above inequality in hand, notice that for all $\vartuple \in \set$ and $k \in \N$, we have:
\begin{align*}
    \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k+0.5] - \vartuple \rangle &= \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle + \langle \vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle \\
    &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k]\| + \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\| \cdot \|\vartuple[][][k+0.5] - \vartuple\|\\
    &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k]\| + \lsmooth \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]} \cdot \|\vartuple[][][k+0.5] - \vartuple\|\\
    &\leq \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}
\end{align*}
where the penultimate line was obtained by the assumption that there exists $\lsmooth \geq 0$, s.t. $\frac{1}{2}\|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\|^2 \leq \lsmooth^2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]$, and the last line from the strong convexity of $\kernel$, which means that we have $\forall \vartuple, \othervartuple \in \set$, $\divergence[\kernel][{\vartuple}][{\othervartuple}] \geq \nicefrac{1}{2}\| \vartuple - \othervartuple\|^2$. 

Now, let $k^* \in \argmin_{k = 0, \hdots, \numhorizons} \divergence[\kernel](\vartuple[][][k+0.5], \vartuple[][][k])$, we then have for all $\vartuple \in \set$:
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^*+0.5] - \vartuple \rangle 
    % &\leq  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \|\vartuple[][][k^*+0.5] - \vartuple[][][k^*]\|\\
    % &=  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \min_{k= 0, \hdots, \numhorizons} \|\vartuple[][][k^*+0.5] - \vartuple[][][k^*]\|\\
    &\leq \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \sqrt{2 \divergence[\kernel][{\vartuple[][][{k^*+0.5}]}][{\vartuple[][][{k^*}]}]}\\
    &= \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right)  \sqrt{2 \min_{k= 0, \hdots, \numhorizons}\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}
\end{align*}
% 


\if 0
Or equivalently, we have:
\begin{align*}
    \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle &\leq \diam(\set) \left( \frac{1}{\learnrate[ ][ ]} + \lsmooth \right) \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}
\end{align*}
\fi

Now, plugging \Cref{eq:intermediate_progress_bound1} in the above, we have  for all $\vartuple \in \set$:
% 
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle 
    % &\leq  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \min_{k = 0, \hdots, \numhorizons} \sqrt{2\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}\\
    % &\leq  \sqrt{2}\diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right)  \sqrt{\min_{k = 0, \hdots, \numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}\\
    &\leq \frac{\sqrt{2} \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right)}{\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}
\end{align*}

Now, by the assumption that $\learnrate[ ][ ] \leq \frac{1}{\sqrt{2}\lsmooth} \leq \frac{1}{\lsmooth}$, we have:
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle &\leq   \frac{\sqrt{2} \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \frac{1}{\learnrate[ ][ ]} \right)}{\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{(1 + \kernelsmooth)\sqrt{2} \diam(\set)}{\learnrate[ ][ ]\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &=   \frac{(1 + \kernelsmooth)\sqrt{2} \diam(\set)}{\learnrate[ ][ ]\sqrt{1 - (\nicefrac{1}{\sqrt{2}})^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{2(1 + \kernelsmooth)  \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{2 (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}
\end{align*}

That is, we have:
% 
\begin{align*}
    \min_{k = 0, \hdots, \numhorizons} \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle 
    &\leq \max_{\vartuple \in \set}  \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle \\
    &\leq \frac{2 (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}
\end{align*}

In addition, for any $\varepsilon \geq 0$, letting $\frac{2 (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}} \leq \varepsilon$, and solving for $\numhorizons$, we have:
% 
\begin{align*}
    \frac{2 (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}} &\leq \varepsilon\\
    \frac{4 (1 + \kernelsmooth)^2 \diam(\set)^2}{\learnrate[ ][ ]^2} \frac{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}{\varepsilon^2} &\leq \numhorizons
\end{align*}

That is, $\bestiter[\vartuple][\numhorizons] \in \argmin_{\vartuple[][][k+0.5] : k = 0, \hdots, \numhorizons} \divergence[\kernel](\vartuple[][][k+0.5], \vartuple[][][k])$ is a $\varepsilon$-strong solution after $\frac{4 (1 + \kernelsmooth)^2 \diam(\set)^2}{\learnrate[ ][ ]^2} \frac{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}{\varepsilon^2}$ iterations of the mirror extragradient algorithm.

   Finally, notice that we have $\lim_{k \to \infty} \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k + 0.5] - \vartuple \rangle = \lim_{\numhorizons \to \infty} \min_{k = 0, \hdots, \numhorizons} \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k + 0.5] - \vartuple \rangle = 0$ and $\lim_{k \to \infty} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]= \lim_{\numhorizons \to \infty} \min_{k = 0, \hdots, \numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] = 0$. Hence, $\lim_{\numhorizon \to \infty} \vartuple[][][\numhorizon + 0.5] = \lim_{\numhorizon \to \infty} \vartuple[][][\numhorizon] = \vartuple[][**]$ is a strong solution of the VI $(\set, \vioper)$. 
\end{proof}


\subsection{Local Convergence of the Mirror Extragradient Algorithm}


To understand how a local weak solution can provide us with local convergence, recall by \Cref{lemma:mirror_extragrad_progress} the iterates of the mirror extragradient algorithm satisfy the following for all $\numhorizon \in \N$:
% 
\begin{align*}
\divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] \geq  \learnrate[ ][ ]\langle \vioper(\vartuple[][][k+0.5]),\vartuple[][][k+0.5]  - \vartuple \rangle + \left( 1 - (\learnrate[ ][ ]\lsmooth)^2 \right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
\end{align*}

Suppose that the kernel function $\kernel$ is strictly convex, and that the algorithm has not yet converged, i.e., $\vartuple[][][k+0.5] \neq \vartuple[][][k]$, then $\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] > 0$, and we can drop the term. Re-organizing the expressions, we then have:
\begin{align*}
\divergence[\kernel][{\vartuple}][{\vartuple[][][k]}] > \divergence[\kernel][{\vartuple}][{\vartuple[][][k+1]}] +   \learnrate[ ][ ] \langle \vioper(\vartuple[][][k+0.5]),\vartuple[][][k+0.5]  - \vartuple \rangle 
\end{align*}


Now, notice that if we can ensure that for all $k \in \N_+$, there exists a $\vartuple[][*] \in \svi(\set, \vioper)$ s.t. $\langle \vioper(\vartuple[][][k+0.5]),\vartuple[][][k+0.5]  - \vartuple[][*] \rangle \geq 0$, then we have: 
$\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] \geq \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}]$, implying that $\vartuple[][][k] \to \vartuple[][*]$. converges to a strong solution. Since we cannot assume the existence of a weak solution (i.e., the Minty condition), the next best way to ensure that there $\vartuple[][*] \in \svi(\set, \vioper)$ s.t. $\langle \vioper(\vartuple[][][k+0.5]),\vartuple[][][k+0.5]  - \vartuple[][*] \rangle \geq 0$, is to initialize the algorithm with an initial iterate $\vartuple[][][0] \in \set$ which is $O(\delta)$-close to a $ \delta$-local weak solution $\vartuple[][*] \in \lmvi[][\vdelta](\set, \vioper)$\footnote{Note that a local weak solution is guaranteed to be strong solution by Proposition 3.1 of \citet{aussel2024variational}.} for some $\delta \geq 0$, and ensure that all subsequent intermediary iterates $\{\vartuple[][][k + 0.5]\}_{k \in \N_{++}}$ remain $\delta$-close to $\vartuple[][*]$.

To ensure this, we have to first bound the distance between the intermediary $\{\vartuple[][][k + 0.5]\}_{k \in \N_{+}}$ and terminal $\{\vartuple[][][k]\}_{k \in \N_{+}}$ iterates. The following lemma provides us with such a bound.

\begin{lemma}[Distance bound on intermediate iterates]\label{lemma:extragrad_intermediate_iterate_dist}
        Let $(\set, \vioper)$ be a $\lsmooth$-Lipschitz-continuous VI satisfying the Minty condition, and $\kernel$ a $1$-strongly-convex and $\kernelsmooth$-Lipschitz-smooth kernel function. Consider the mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad})  run with the VI $(\set, \vioper)$, the kernel function $\kernel$, any step size $\learnrate[ ][ ] \geq 0$, for any time horizon $\numhorizons \in \N$, and outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$. We then have:
    \begin{align}
        \norm[{\vartuple[][][k+0.5] - \vartuple[][][k]}] \leq \learnrate[ ][ ] \lipschitz
    \end{align}
    where $\lipschitz \doteq \max_{\vartuple \in \set} \| \vioper(\vartuple)\|$.
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:extragrad_intermediate_iterate_dist}]
    Note that for all $k \in \N_+$, by the first order optimality conditions of $\vartuple[][][k+0.5]$, we have for all $\vartuple \in \set$:
    % 
    \begin{align*}
    \langle \vioper(\vartuple[][][k]) + \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k]), \vartuple - \vartuple[][][k+0.5] \rangle \geq 0.
    \end{align*}
    
    Substituting $\vartuple = \vartuple[][][k]$ above, we have:
    \begin{align}
    \langle \vioper(\vartuple[][][k]), \vartuple[][][k] - \vartuple[][][k+0.5] \rangle 
    &\geq \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k]) - \grad \kernel(\vartuple[][][k+0.5]), \vartuple[][][k] - \vartuple[][][k+0.5] \rangle \notag \\
    &= \frac{1}{\learnrate[ ][ ]} \left( \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] + \divergence[\kernel][{\vartuple[][][k]}][{\vartuple[][][k+0.5]}] \right).
    \end{align}

    where the last line was obtained by \Cref{lemma:bregman_triangle}.
    
    Re-organizing:
    
    \begin{align}
    \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
    &\leq \learnrate[ ][ ] \langle \vioper(\vartuple[][][k]), \vartuple[][][k] - \vartuple[][][k+0.5] \rangle -\divergence[\kernel][{\vartuple[][][k]}][{\vartuple[][][k+0.5]}] .\\
    &\leq \learnrate[ ][ ] \langle \vioper(\vartuple[][][k]), \vartuple[][][k] - \vartuple[][][k+0.5] \rangle -\nicefrac{1}{2} \norm[{\vartuple[][][k] - \vartuple[][][k+0.5]}]^2\\
    &\leq \learnrate[ ][ ] \norm[{ \vioper(\vartuple[][][k])}] \norm[{\vartuple[][][k] - \vartuple[][][k+0.5]}] - \nicefrac{1}{2}\norm[{\vartuple[][][k] - \vartuple[][][k+0.5]}]^2\\
    &\leq \learnrate[ ][ ] \lipschitz \norm[{\vartuple[][][k] - \vartuple[][][k+0.5]}] - \nicefrac{1}{2} \norm[{\vartuple[][][k] - \vartuple[][][k+0.5]}]^2
    \end{align}

    Since for all $z \in \R$, $a b \in \R_+$, we have $az - bz^2 \leq \nicefrac{a^2}{4b}$:
    \begin{align}
        \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
        &\leq \frac{\learnrate[ ][ ]^2 \lipschitz^2}{2}
    \end{align}

    Additionally, note that by strong convexity of $\kernel$, we have $\forall \vartuple, \othervartuple \in \set$, $\divergence[\kernel][{\vartuple}][{\othervartuple}] \geq \nicefrac{1}{2}\| \vartuple - \othervartuple\|^2$ or equivalently $\sqrt{2\divergence[\kernel][{\vartuple}][{\othervartuple}]} \geq \| \vartuple - \othervartuple\|^2$. Hence, continuing:
    \begin{align}
        \norm[{\vartuple[][][k+0.5] - \vartuple[][][k]}] \leq \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] }
        &\leq \learnrate[ ][ ] \lipschitz
    \end{align}
\end{proof}

With the above Lemma in hand, we can show that if the initial iterate starts close enough to some local weak solution, then the intermediate iterates will remain within this $\vdelta$-ball for the remainder of the algorithm for an appropriate choice of step size.

\begin{lemma}[Mirror Extragradient Iterates Remain Local]\label{lemma:mirror_extragrad_iterates_remain_local}
        Let $(\set, \vioper)$ be a $\lsmooth$-Lipschitz-continuous VI  satisfying the Minty condition, and $\kernel$ a $1$-strongly-convex kernel function. Define  $\lipschitz \doteq \max_{\vartuple \in \set} \| \vioper(\vartuple)\|$. Suppose that for some $\vartuple[][*] \in \lmvi[][\delta](\set, \vioper)$ $\delta$-local weak solution, the initial iterate $\vartuple[][][0] \in \set$ is chosen so that $\sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]} \leq \delta - \learnrate[ ][ ] \lipschitz$. Consider the mirror extragradient algorithm (\Cref{alg:VI_mirror_extragrad})  run with the VI $(\set, \vioper)$, the kernel function $\kernel$, a step size $\learnrate[ ][ ] \geq 0$, initial iterate $\vartuple[][][0]$, some time horizon $\numhorizons \in \N$, and outputs $\{\vartuple[][][\numhorizon + 0.5], \vartuple[][][\numhorizon + 1]\}_{\numhorizon}$. Then. for all $\numhorizon \in [\numhorizons]$, we have 
        \begin{align*}
          &\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizon]}] \leq \nicefrac{1}{2}(\delta - \learnrate[ ][ ] \lipschitz)^2  &\text{and} &\norm[{\vartuple[][][\numhorizon + 0.5] - \vartuple[][*]}] \leq \delta \enspace .
        \end{align*}
\end{lemma}

\begin{proof}[Proof of \Cref{lemma:mirror_extragrad_iterates_remain_local}]
    We will prove the claim by induction on $\numhorizon \in \N_+$.

    
    
    \paragraph{Base case: $\numhorizon = 0$ }

    \begin{align*}
        \norm[{\vartuple[][][0.5] - \vartuple[][*]}]
        &= \norm[{\vartuple[][][0.5] - \vartuple[][][0] + \vartuple[][][0] - \vartuple[][*]}]\\
        &\leq \norm[{\vartuple[][][0.5] - \vartuple[][][0]}] + \norm[{\vartuple[][][0] - \vartuple[][*]}]\\
        &\leq \norm[{\vartuple[][][0.5] - \vartuple[][][0]}] + \sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}\\
        &\leq \learnrate[ ][ ] \lipschitz + (\delta - \learnrate[ ][ ] \lipschitz) && \text{(\Cref{lemma:extragrad_intermediate_iterate_dist})}\\
        &\leq \delta
    \end{align*}

    \paragraph{Inductive step:} Suppose that for all $\numhorizon = 0, \hdots, \numhorizons$, $\norm[{\vartuple[][][\numhorizon + 0.5] - \vartuple[][*]}] \leq \delta$ and $\sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizon]}]} \leq \delta - \learnrate[ ][ ] \lipschitz$ (or equivalently, $\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizon]}] \leq \nicefrac{1}{2}(\delta - \learnrate[ ][ ] \lipschitz)^2$). We will show that $\norm[{\vartuple[][][\numhorizons + 1.5] - \vartuple[][*]}] \leq \delta$ and $\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons +1 ]}] \leq \nicefrac{1}{2}(\delta - \learnrate[ ][ ] \lipschitz)^2$.

    By \Cref{lemma:mirror_extragrad_progress}, we have:
    \begin{align}
        \divergence[\kernel][{\vartuple}][{\vartuple[][][\numhorizons]}] - \divergence[\kernel][{\vartuple}][{\vartuple[][][\numhorizons+1]}] \geq  \learnrate[ ][ ]\langle \vioper(\vartuple[][][\numhorizons+0.5]),\vartuple[][][\numhorizons+0.5]  - \vartuple \rangle + \left( 1 - (\learnrate[ ][ ]\lsmooth)^2 \right)\divergence[\kernel][{\vartuple[][][\numhorizons+0.5]}][{\vartuple[][][\numhorizons]}]
    \end{align}

    Substituting in $\vartuple \doteq \vartuple[][*] \in \lmvi[][\delta](\set, \vioper)$, we have:
    \begin{align*}
        \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons+1]}]&\geq  \learnrate[ ][ ] \underbrace{\langle \vioper(\vartuple[][][\numhorizons+0.5]),\vartuple[][][\numhorizons+0.5]  - \vartuple[][*] \rangle}_{\geq 0} + \left( 1 - (\learnrate[ ][ ]\lsmooth)^2 \right) \underbrace{\divergence[\kernel][{\vartuple[][][\numhorizons+0.5]}][{\vartuple[][][\numhorizons]}]}_{\geq 0}\\
        &\geq 0
    \end{align*}
    Re-organizing, and using the inductive assumption that $\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons]}] \leq \nicefrac{1}{2}(\delta - \learnrate[ ][ ] \lipschitz)^2$ we have:
    \begin{align}
        \nicefrac{1}{2}(\delta - \learnrate[ ][ ] \lipschitz)^2 \geq \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons]}] \geq \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons+1]}]
    \end{align}

    Now, notice that we have:
    \begin{align*}
        \norm[{\vartuple[][][\numhorizons + 0.5] - \vartuple[][*]}] 
        &= \norm[{\vartuple[][][\numhorizons + 0.5] - \vartuple[][][\numhorizons] + \vartuple[][][\numhorizons] - \vartuple[][*]}]\\
        &\leq \norm[{\vartuple[][][\numhorizons + 0.5] - \vartuple[][][\numhorizons]}] + \norm[{\vartuple[][][\numhorizons] - \vartuple[][*]}]\\
        &\leq \norm[{\vartuple[][][\numhorizons + 0.5] - \vartuple[][][\numhorizons]}] + \sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons]}]}\\
        &\leq \learnrate[ ][ ] \lipschitz + (\delta - \learnrate[ ][ ] \lipschitz) && \text{(\Cref{lemma:extragrad_intermediate_iterate_dist})}\\
        &\leq \delta
    \end{align*}
\end{proof}

With the above lemma in hand, modifying the proof of \Cref{thm:mirror_extragradient_global_convergence} slightly, we can show local convergence to a strong solution when the initial iterate of the algorithm is initialized close enough to a local solution.


\thmvimirrorextragradlocal*
\begin{proof}[Proof of \Cref{thm:vi_mirror_extragrad_local}]
Taking \Cref{lemma:mirror_extragrad_progress} with $\vartuple \doteq \vartuple[][*]$, where $\vartuple[][*]$ is given as in the Theorem statement, then by \Cref{lemma:mirror_extragrad_iterates_remain_local} we have for all $k \in \N$:
    \begin{align*}
        \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] &\geq  \learnrate[ ][ ] \underbrace{\langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple[][*] \rangle}_{\geq 0} + \left(1 - (\learnrate[ ][ ]\lsmooth)^2 \right)\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] \\
        &\geq \left(1  - (\learnrate[ ][ ]\lsmooth)^2 \right) \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]
    \end{align*}

    Multiplying both sides by $\left(1  - (\learnrate[ ][ ]\lsmooth)^2 \right)^{-1} > 0$, we have:
    \begin{align}
        \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] 
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] \right)
    \end{align}
    
    Summing up for $k  = 0, \hdots, \numhorizons$:
    \begin{align}
        \sum_{k = 0}^{\numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]  &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \sum_{k = 0}^{\numhorizons} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][k+1]}] \right)\\
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] - \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][\numhorizons+1]}] \right)\\
        &\leq \frac{1}{1 - (\learnrate[ ][ ]\lsmooth)^2} \divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] 
    \end{align}
    Dividing both sides by $\numhorizons$, we have:
    \begin{align}
        \frac{1}{\numhorizons}\sum_{k = 0}^{\numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}] &\leq \frac{1}{\numhorizons\left(1 - (\learnrate[ ][ ]\lsmooth)^2\right)} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] \right)\\
        \min_{k = 0, \hdots, \numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]  &\leq \frac{1}{\numhorizons\left(1 - (\learnrate[ ][ ]\lsmooth)^2\right)} \left(\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}] \right)\label{eq:intermediate_progress_bound}
    \end{align}

    We can transform this convergence into a convergence in terms of the primal gap function. Now, recall by the first order optimality conditions of $\vartuple[][][k+0.5]$, we have for all $\vartuple \in \set$:
% 
    \begin{align*}
    \langle \vioper(\vartuple[][][k]) + \frac{1}{\learnrate[ ][ ]} \langle \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k]), \vartuple - \vartuple[][][k+0.5] \rangle \geq 0.
    \end{align*}
Re-organizing, for all $\vartuple \in \set$, and $k \in \N$ we have:
    \begin{align}
         \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle &\leq \frac{1}{\learnrate[ ][ ]} \left\| \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k])\right\| \left\|\vartuple[][][k+0.5] - \vartuple \right\|\\
         &\leq \frac{\diam(\set)}{\learnrate[ ][ ]} \left\| \grad \kernel(\vartuple[][][k+0.5]) - \grad \kernel(\vartuple[][][k])\right\|\\
         &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \left\| \vartuple[][][k+0.5] - \vartuple[][][k]\right\|
    \end{align}
    where the last line follow from $h$ being $\kernelsmooth$-Lipschitz-smooth.

Now, with the above inequality in hand, notice that for all $\vartuple \in \set$ and $k \in \N$, we have:
\begin{align*}
    \langle \vioper(\vartuple[][][k + 0.5]), \vartuple[][][k+0.5] - \vartuple \rangle &= \langle \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle + \langle \vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k]), \vartuple[][][k+0.5] - \vartuple \rangle \\
    &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k]\| + \|\vioper(\vartuple[][][k+0.5]) - \vioper(\vartuple[][][k])\| \cdot \|\vartuple[][][k+0.5] - \vartuple\|\\
    &\leq \frac{\diam(\set)\kernelsmooth}{\learnrate[ ][ ]} \|\vartuple[][][k+0.5] - \vartuple[][][k]\| + \lsmooth \|\vartuple[][][k+0.5] - \vartuple[][][k]\| \cdot \|\vartuple[][][k+0.5] - \vartuple\|\\
    &\leq \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \|\vartuple[][][k+0.5] - \vartuple[][][k]\|
\end{align*}
where the penultimate line follows from the $\lsmooth$-Lipschitz-continuity of $\vioper$, and the strong convexity of $\kernel$, which means that we have $\forall \vartuple, \othervartuple \in \set$, $\divergence[\kernel][{\vartuple}][{\othervartuple}] \geq \nicefrac{1}{2}\| \vartuple - \othervartuple\|^2$..

Now, let $k^* \in \argmin_{k = 0, \hdots, \numhorizons} \|\vartuple[][][k+0.5] - \vartuple[][][k]\|$, we then have:
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^*+0.5] - \vartuple \rangle &\leq  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \|\vartuple[][][k^*+0.5] - \vartuple[][][k^*]\|\\
    &=  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \min_{k= 0, \hdots, \numhorizons} \|\vartuple[][][k^*+0.5] - \vartuple[][][k^*]\|\\
    &\leq \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \min_{k= 0, \hdots, \numhorizons} \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}
\end{align*}
% 
where the last line follows from  

\if 0
Or equivalently, we have:
\begin{align*}
    \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle &\leq \diam(\set) \left( \frac{1}{\learnrate[ ][ ]} + \lsmooth \right) \sqrt{2 \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}
\end{align*}
\fi

Now, plugging \Cref{eq:intermediate_progress_bound} in the above, we have:
% 
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle &\leq  \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right) \min_{k = 0, \hdots, \numhorizons} \sqrt{2\divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}\\
    &\leq  \sqrt{2}\diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right)  \sqrt{\min_{k = 0, \hdots, \numhorizons} \divergence[\kernel][{\vartuple[][][k+0.5]}][{\vartuple[][][k]}]}\\
    &\leq   \frac{\sqrt{2} \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \lsmooth \right)}{\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}
\end{align*}

Now, by the assumption that $\learnrate[ ][ ] \leq \frac{1}{\sqrt{2}\lsmooth} \leq \frac{1}{\lsmooth}$, we have:
\begin{align*}
    \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle &\leq   \frac{\sqrt{2} \diam(\set) \left( \frac{\kernelsmooth}{\learnrate[ ][ ]} + \frac{1}{\learnrate[ ][ ]} \right)}{\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{(1 + \kernelsmooth)\sqrt{2} \diam(\set)}{\learnrate[ ][ ]\sqrt{1 - (\learnrate[ ][ ]\lsmooth)^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &=   \frac{(1 + \kernelsmooth)\sqrt{2} \diam(\set)}{\learnrate[ ][ ]\sqrt{1 - (\nicefrac{1}{\sqrt{2}})^2}} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{2(1 + \kernelsmooth)  \diam(\set)}{\learnrate[ ][ ]} \frac{\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]}}{\sqrt{\numhorizons}}\\
    &= \frac{\sqrt{2} (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\delta}{\sqrt{\numhorizons}}
\end{align*}
where the last line follows from the assumption that $\sqrt{2\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]} \leq \delta - \learnrate[ ][ ] \lipschitz$ which implies $\sqrt{\divergence[\kernel][{\vartuple[][*]}][{\vartuple[][][0]}]} \leq \frac{\delta}{\sqrt{2}}$.

That is, we have:
% 
\begin{align*}
    \max_{\vartuple \in \set} \langle \vioper(\vartuple[][][k+0.5]), \vartuple[][][k+0.5] - \vartuple \rangle \leq \langle \vioper(\vartuple[][][k^* + 0.5]), \vartuple[][][k^* +0.5] - \vartuple \rangle \leq \frac{\sqrt{2} (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\delta}{\sqrt{\numhorizons}}
\end{align*}

In addition, for any $\varepsilon \geq 0$, letting $\frac{\sqrt{2} (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\delta}{\sqrt{\numhorizons}} \leq \varepsilon$, and solving for $\numhorizons$, we have:
% 
\begin{align*}
    \frac{\sqrt{2} (1 + \kernelsmooth) \diam(\set)}{\learnrate[ ][ ]} \frac{\delta}{\sqrt{\numhorizons}} &\leq \varepsilon\\
    \frac{2 (1 + \kernelsmooth)^2 \diam(\set)^2}{\learnrate[ ][ ]^2} \frac{\delta^2}{\varepsilon^2} &\leq \numhorizons
\end{align*}

That is, $\bestiter[\vartuple][\numhorizons] \in \argmin_{\vartuple[][][k+0.5] : k = 0, \hdots, \numhorizons} \|\vartuple[][][k+0.5] - \vartuple[][][k]\|$ is a $\varepsilon$-strong solution after $\frac{2 (1 + \kernelsmooth)^2 \diam(\set)^2}{\learnrate[ ][ ]^2} \frac{\delta}{\varepsilon^2}$ iterations of the mirror extragradient algorithm.
\end{proof}