\section{Energy Term}
To model human motion using Gaussian sequences, we first employ the color energy $E_{\text{color}}$ during the training process:
\begin{equation}
E_{\text{color}} = (1 - \lambda_{\text{color}}) \mathcal{L}_1 + \lambda_{\text{color}} \mathcal{L}_{\text{D-SSIM}},
\end{equation}
where $\mathcal{L}_1$ is the $L_1$ photometric loss and $\mathcal{L}_{\text{D-SSIM}}$ is the D-SSIM term. We then employ a smooth regularizer to constrain the joint Gaussians motion locally as-rigidas-possible(ARAP):
\begin{equation}
\begin{aligned}
    E_{\text{smooth}} =& \sum_{i} \sum_{k \in \mathcal{N}(i)} w_{i,k} \left\| R \left( q_{i,t}^{j} * {q_{i,t-1}^{j}}^{-1} \right) \right. \\
    & \quad \left. \left( p_{k,t-1}^{j} - p_{i,t-1}^{j} \right) - \left( p_{k,t}^{j} - p_{i,t}^{j} \right) \right\|_2^2,
\end{aligned}
\end{equation}

where $\mathcal{N}(i)$represents the set of neighboring joint Gaussian kernels of $i$, $R(\cdot)$ converts quaternion back into a rotation matrix and $w_{i,k}$ corresponds to the blending weights according to DualGS~\cite{jiang2024robust}.
Furthermore, we apply the temporal regularizer $E_{\text{temp}}$ to promote coherent appearances by constraining Gaussian attributes ($C_{i,t}, \sigma_{i,t}, s_{i,t}$) to be consistent with the previous frame:
\begin{equation}
    E_{\text{temp}} = \sum_{a \in \{\mathcal{C}, \sigma, s\}}\lambda_a \| a_{i,t} - a_{i,t-1} \|_2^2,
\end{equation}

Together with $E_{\text{normal}}$ we mentioned above, the overall energy term in our 4D Gaussian modeling is expressed as:
\begin{equation}
    E = \lambda_{\text{color}}E_{\text{color}}+\lambda_{\text{smooth}}E_{\text{smooth}}+\lambda_{\text{temp}}E_{\text{temp}}+\lambda_{\text{normal}}E_{\text{normal}},
\end{equation}
which helps us obtain accurate geometric information while modeling human performance sequences.



\section{Base Color Decomposition}
To compute base color, we ignore the effect of indirect illumination caused by reflections on the surface of the human body, the rendering equation can be approximated as:
 \begin{footnotesize}  
\begin{equation}
	L_{o}\left(x,\boldsymbol{\omega}_{o}\right) \approx \int_{\Omega} f_{r}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right)  V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) L_{i}^{\text{env}}\left(\boldsymbol{\omega}_{i}\right) (\mathbf{n} \cdot \boldsymbol{\omega}_{i}) \mathrm{d} \boldsymbol{\omega}_{i},
\end{equation}
\end{footnotesize}
where $x$ and $\mathbf{n}$ are the surface point and its normal vector, $f_r$ is the Bidirectional Reflectance Distribution Function (BRDF), $L_o\left(x,\boldsymbol{\omega}_{o}\right)$ denotes the outgoing radiance in direction $\boldsymbol{\omega}_o$.

We decompose the BRDF term $f_r$ into two components, the diffuse term $f_{r_d}$ and the specular term $f_{r_s}$, where we use the Lambertian model for $f_{r_d}$ and Cook-Torrance microfacet specular shading model~\cite{cook1982reflectance} for $f_{r_s}$:
\begin{align}
 	f_{r}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right) &= f_{r_d}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right) + f_{r_s}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right), \\
    f_{r_d}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right) &= 
    \frac{1-F}{\pi} \rho(x), \\
    f_{r_s}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right) &= \frac{D G F}{4(\mathbf{n}\cdot \boldsymbol{\omega}_{i})(\mathbf{n}\cdot \boldsymbol{\omega}_{o})},
\end{align}
where $D$, $G$, $F$ are respectively the GGX normal distribution function (NDF), the geometric attenuation function, and the approximated Fresnel term. 
We then split $L_o$ into two parts:
\begin{align}
	L_{o}\left(x,\boldsymbol{\omega}_{o}\right) &= \int_{\Omega} \left(f_{r_d} + f_{r_s} \right) \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i} , \\
    &= \int_{\Omega} f_{r_d}  \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i} +  \int_{\Omega}  f_{r_s}  \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i} ,\\
    \label{eq_Lo}
     &=  \frac{\rho(x)}{\pi} \int_{\Omega}  \left(1-F\right) \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i} +  \int_{\Omega}  f_{r_s}  \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i},
\end{align}
where $\mathcal{L} = V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) L_{i}^{\text{env}}\left(\boldsymbol{\omega}_{i}\right) (\mathbf{n} \cdot \boldsymbol{\omega}_{i})$.

The $D$ and $G$ in $f_{r_s}$ are both irrelevant with the base color $\rho$. For the Fresnel term $F$, we use the Schlick Fresnel approximation~\cite{Schlick1994AnIB}:
\begin{equation}
     F =  F_0 + (1 - F_0) \left(1- \left(\mathbf{h} \cdot \boldsymbol{\omega}_o\right)\right)^5,
 \end{equation}
where $\mathbf{h}=\frac{\boldsymbol{\omega}_o+\boldsymbol{\omega}_i}{\left \| \boldsymbol{\omega}_o+\boldsymbol{\omega}_i \right \|}$, $F_0$ represents the specular reflectance at normal incidence and is achromatic for dielectrics and chromatic for metals. Since the human body is mostly composed of dielectric materials, we assume $F_0=0.04$, so that $F$ is also irrelevant with $\rho$. 
Consequently, we can solve the base color $\rho$ from Eq.~\ref{eq_Lo}:
\begin{equation}
     \rho(x) = \frac{\pi \left(L_o(x, \boldsymbol{\omega}_{o}) - \int_\Omega f_{r_s}  \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i}\right)}{  \int_\Omega 
    \left(1-F\right)  \mathcal{L} \mathrm{d} \boldsymbol{\omega}_{i}}.
\end{equation}


\section{Additional Evaluations}
In this section, we present more details on the qualitative comparison conducted on synthetic human data, including normal, roughness, base color, AO, and relight rendering effects in Fig.~\ref{fig_comparison_full}.

\section{Applications}
In Fig.~\ref{fig_vr}, we showcase the application of watching an actor dancing in a scene with varying lighting. Viewers can immerse themselves in a realistic environment with dramatic lighting changes, observing the actor's dance performance seamlessly integrated into the surroundings. We can capture, process, and generate high-fidelity human 4D assets with our dome cameras and BEAM pipeline, reproducing outstanding performances in different scenes and lightings.

At the same time, our compatibility with the traditional CG pipeline allows us to edit scenes in the Unity plugin we developed or integrate multiple human performances, as shown in Fig.~\ref{fig_unity_plugin}. We can align multiple performances through CG platform, edit the scene and lighting, and present stunning visual effects of the performances.




\begin{figure*} [ht]
 \centering
 \includegraphics[width=\textwidth]{figs/comparison_more.jpg}
 \captionof{figure}{We present additional comparison results on synthetic humans, demonstrating the high-quality relighting capabilities of our method.}
 \label{fig_comparison_full}
\end{figure*}
\begin{figure*} [ht]
 \centering
 \includegraphics[width=\textwidth]{figs/vr.jpg}
 \captionof{figure}{We demonstrate our VR application on PICO, enabling an immersive experience with relightable volumetric video.}
 \label{fig_vr}
\end{figure*}


\begin{figure*} [ht]
 \centering
 \includegraphics[width=\textwidth]{figs/unity.jpg}
 \captionof{figure}{We showcase our custom Unity plugin for relightable 4D Gaussian assets. Our relightable 4D Gaussians are CG-friendly, supporting scene and lighting environment editing within Unity.}
 \label{fig_unity_plugin}
\end{figure*}
