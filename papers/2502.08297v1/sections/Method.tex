

\section{Method}

Given multi-view video inputs, we aim to produce relightable 4D Gaussian sequences with physically-based rendering (PBR) materials, enabling realistic rendering under various lighting. PBR typically requires multiple material components, such as base color, metallic, roughness, normal, and ambient occlusion (AO), as shown in Fig.\ref{fig_pbr_attributes}. In our approach, 
as the metallic attribute is negligible for human bodies, we set it to zero and focus on optimizing the remaining properties: normal, roughness, AO, and base color. The complete pipeline is illustrated in Fig.\ref{fig_pipeline}.









\subsection{Gaussian Modeling and Geometry Optimization}
\label{sec_3_1}

To obtain temporally consistent geometric information (depth and normal maps) for physical-based rendering, we seamlessly combine the Gaussian-based performance tracking~\cite{jiang2024robust} with the geometry-aware rasterizer~\cite{zhang2024rade} within a coarse-to-fine optimization framework. Specifically, our framework uses a dual Gaussian representation to separately model global motion and visual appearance through joint and skin Gaussians. Each skin Gaussian is anchored to multiple joint Gaussians and is warped across frames based on the tracking results of these joint Gaussians. The optimization process integrates a photometric loss $E_{\text{color}}$, a rigid term $E_{\text{rigid}}$, and a temporal regularization term $E_{\text{temp}}$. 

To further enhance the optimization of Gaussian geometry, we introduce an additional normal consistency loss $E_{\text{normal}}$. During rasterization, we assume that the intersection points between rays and Gaussians correspond to the maxima in Gaussian values. The depth of a Gaussian is defined as the depth of the intersection point, while the normal of the intersection plane is taken as the Gaussian normal. This rasterizing process generates precise depth maps and normal maps $N_r$. Using the obtained depth map, we compute the normal map $N_d$ based on a local plane assumption and then measure the normal consistency loss $E_{\text{normal}}$ between $N_d$ and $N_r$ as follows:








\begin{equation}
E_{\text{normal}} = \sum_i \omega_i (1 - N_r^\top N_d),
\end{equation}
where $i$ indexes the intersected splats along the ray, and $\omega_i=\alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)$ represents the blending weight of the intersection point.

The total energy term in our 4D Gaussian modeling and geometric optimization framework is expressed as:
\begin{equation}
    E = \lambda_{\text{color}}E_{\text{color}}+\lambda_{\text{smooth}}E_{\text{smooth}}+\lambda_{\text{temp}}E_{\text{temp}}+\lambda_{\text{normal}}E_{\text{normal}},
\end{equation}
During dynamic training, $E_{\text{norm}}$ is introduced after the appearance optimization over 7,000 iterations, followed by an additional 5,000 iterations dedicated to optimizing the normals. For other energy terms not discussed here, please refer to DualGS~\cite{jiang2024robust}. The hyperparameters are set as follows: $\lambda_{\text{color}} = 1$, $ \lambda_{\text{smooth}} = 0.001$, $\lambda_{\text{temp}} = 0.00005$, $\lambda_{\text{normal}} = 0.03$.















\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/pbr_attributes.png}
  \captionof{figure}{Illustration of Gaussian properties. We showcase the normal, roughness, AO and base color maps required for relighting, which are rasterized from our relightable 4D Gaussians.} 
  \label{fig_pbr_attributes}
  \vspace{-15pt}
\end{figure}



\subsection{PBR Materials Decomposition}
\label{sec_3_2}

To enable physical-based rendering within the Gaussian-based paradigm, we extend 3D Gaussian attributes by incorporating a roughness value and two additional third-order spherical harmonic (SH) properties for AO and base color. And we utilize a step-by-step approach to disentangle these properties.















\paragraph{Roughness.} To efficiently assign a roughness value to each Gaussian, we utilize a generative model that offers significantly faster performance compared to inverse rendering methods. Specifically, we feed the canonical mesh and multi-view images into the Material Diffusion module of CLAY~\cite{zhang2024clay} to generate a roughness texture for the mesh. Each valid pixel of the texture is then mapped back to the world coordinate system, allowing us to assign each Gaussian the roughness value of its nearest pixel.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/rendering.jpg}
  \captionof{figure}{We present the results of different rendering techniques. Real-time rendering focuses on efficiency,  while offline rendering delivers more realistic shadows and occlusion effects.} 

  \vspace{-12pt}
  \label{fig_rendering}
\end{figure}

\begin{figure*} [ht]
  \centering
  \includegraphics[width=\textwidth]
  {figs/more_results.jpg}
  \vspace{-10pt}
  \captionof{figure}{Gallery of our results. We present some real-time rendering results under HDRI settings, which deliver high-fidelity rendering of human performances across challenging motions and complex clothing textures.} 
  \vspace{-10pt}
  \label{fig_moreres}
\end{figure*}

\paragraph{Ambient Occlusion and Base Color}  
For AO and base color, we first estimate these two attributes in the input views for each frame to bake 2D material maps, and then optimize into the corresponding dense skin Gausssians.

Ambient Occlusion $\mathcal{A}(x)$ is an approximation of global illumination, which models the diffuse shadows produced by close, potentially small occluders within a constrained computational budget:
\begin{equation}
\label{equ_AO}
\mathcal{A}(x) = \frac{1}{\pi} \int_{\Omega} V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) (\mathbf{n} \cdot \boldsymbol{\omega}_{i}) \mathrm{d} \boldsymbol{\omega}_{i},
\end{equation}
where $V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right)$ is the visibility term at 3D point $x$ in direction $\boldsymbol{\omega}_i$, $\mathbf{n}$ is the normal of the surface at point $x$, $\Omega$ is the hemisphere centered in $x$ and having $\mathbf{n}$ as its axis. 

For the base color, we use a simplified Disney BRDF model~\cite{burley2012physically} composed of a Lambertian diffuse term and a Cook-Torrance specular term~\cite{cook1982reflectance}, with the outgoing radiance $L_{o}$ being a linear combination of these two components. Since for dielectric materials the diffuse part is proportional to the base color while the specular part is independent of it, the rendering equation~\cite{kajiya1986rendering} can be written as: 
\begin{equation}
    L_o\left(x,\boldsymbol{\omega}_{o}\right) = \rho(x) L_{o}^D\left(x,\boldsymbol{\omega}_{o}\right) + L_{o}^S\left(x,\boldsymbol{\omega}_{o}\right),
\end{equation}
where $L_o\left(x,\boldsymbol{\omega}_{o}\right)$ is the outgoing radiance at $x$ in direction $\boldsymbol{\omega}_{o}$, computed by mapping image pixel colors to linear space. $\rho$ is the base color, $L_{o}^D$ is the diffuse part residue and $L_{o}^S$ is the specular part.
To simplify the rendering equation, our computation of the base color disregards the indirect illumination effects caused by surface reflections on the human body. Consequently, $L_{o}^D$ and $L_{o}^S$ are expressed as follows:
\begin{align}
\label{equ_LoD}
     L_{o}^D\left(x,\boldsymbol{\omega}_{o}\right) &= \frac{1}{\pi  } \int_\Omega 
     \left(1-F\right)  \mathcal{L}(x,\boldsymbol{\omega}_i) \mathrm{d} \boldsymbol{\omega}_{i},  \\
    \label{equ_LoS}
     L_{o}^S\left(x,\boldsymbol{\omega}_{o}\right) &= \int_\Omega f_{r_s}\left(x,\boldsymbol{\omega}_{i}, \boldsymbol{\omega}_{o}\right)  \mathcal{L}(x,\boldsymbol{\omega}_i) \mathrm{d} \boldsymbol{\omega}_{i},
\end{align}
where $\mathcal{L}(x,\boldsymbol{\omega}_i) = V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) L_{i}^{\text{env}}\left(\boldsymbol{\omega}_{i}\right) (\mathbf{n} \cdot \boldsymbol{\omega}_{i})$, $F$ is the approximated Fresnel term, $f_{r_s}$ is the specular term in the BRDF, $L_{i}^{\text{env}}$ can be queried from the environment map. To capture the environment map of our multi-view, well-lit dome setup, we position a DSLR camera at the center and take bracketed exposure photographs from multiple directions. These photographs are then processed using PTGui~\cite{new_house_internet_services_bv_ptgui_2025} to generate a high dynamic range (HDR) panoramic image, enabling precise calculation of the incoming radiance $L_i^{\text{env}}$ from the environment.

\begin{figure*} [ht]
  \centering
  \includegraphics[width=\textwidth]{figs/comparison_new.jpg}
  \captionof{figure}{Qualitative comparisons of our method against R-3DGS~\cite{gao2025relightable}, GS-IR~\cite{liang2024gs} and D-2DGS$^{\ast}$~\cite{zhang2024dynamic}. D-2DGS$^{\ast}$ refers to the results of inverse rendering applied to mesh sequences obtained from D-2DGS to decouple PBR materials. Our method achieves the highest relighting quality. For more detailed comparison results, please refer to the appendix.}
  \label{fig_comparison1}
\end{figure*}

We observe that both 2D AO and base color need the visibility term $V^{\text{env}}$. To accurately compute $V^{\text{env}}$, we 
adopt Gaussian ray tracer from 3DGRT~\cite{3dgrt2024}, and compute intersections based on the maximum Gaussian response.  For efficient computation, a proxy icosahedron mesh is employed to leverage hardware acceleration, with a two-level BVH constructed at both the mesh and instance levels. We accumulate the alpha values of the Gaussians at the intersection points without sorting them:
\begin{equation}
    \alpha = \sum_{i=1}^{N} O_i G_i(x_i), 
\end{equation}
where $O_i$ is the opacity of the $i_{th}$ Gaussian, $G_i(x_i)$ is the response of the Gaussian kernel at $x_i$ which makes the response be the maximum value along the ray. We set $V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) = 0$ only when $\alpha$ is greater than a threshold $T$ (we choose $T$ as 0.9999); otherwise, $V^{\text{env}}\left(x,\boldsymbol{\omega}_{i}\right) = 1$. Ray tracing stops once the accumulated alpha value exceeds the threshold $T$. To avoid occlusion from Gaussians directly above the original depth, we offset the ray origin $o$ along the normal direction: $o=x+\epsilon \mathbf{n}$.


Finally, we use the Monte Carlo method to solve the integrals in Eq.~\ref{equ_AO},~\ref{equ_LoD},~\ref{equ_LoS}. We render the AO image with 50 samples per pixel and the base color image with 100 samples per pixel, then apply Intel Open Image Denoise~\cite{IntelOIDN2025} for denoising. 









 





To optimize the AO and base color material maps into the corresponding dense skin Gaussians, we employ the generated training view material maps as ground truth supervision, while keeping all other geometry-related attributes fixed.
The optimization process commences by initializing the AO attributes with zeros and the base color attributes with RGB values. We then train 5000 iterations to obtain both AO and base color attributes separately. Besides, we incorporate the same regularization term to the base color to ensure temporal consistency.
\subsection{Physically Based Rendering}
\label{sec_3_3}

By leveraging our relightable 4D Gaussians, we can seamlessly integrate the 4D assets into traditional CG engines, supporting both real-time and offline rendering workflows.
For real-time rendering, we utilize the deferred shading to deliver immersive and efficient visualization across diverse settings. For offline rendering, we employ ray tracing, which excels in handling shadows and occlusion relationships, ensuring high-quality results.
\paragraph{Real-time Rendering.}
We implement real-time rendering using deferred shading techniques~\cite{deering1988deferred}, based on the High Definition Render Pipeline (HDRP) in Unity. Specifically, we rasterize our 4D Gaussian sequence with extra PBR attributes, including base color, AO, normals, roughness, and depth maps, and store them in the GBuffer. This GBuffer is then integrated into the original forward transparent stage of HDRP for rendering semi-transparent objects. We also leverage shadow mapping from the HDRP rendering pipeline to perform shadow calculations for Gaussians under different light types. Our approach enables real-time rendering at 100 FPS in 1080P for volumetric videos.




\paragraph{Offline Rendering.} 

For offline rendering, we sort the intersection points of the Gaussian during ray tracing, and use alpha blending to determine the exact intersection with the entire Gaussian object, simultaneously acquiring the normal and material attributes at that point. This allows full compatibility with the widely used path tracing pipeline. To demonstrate the relighting quality of real-time rendering and offline rendering techniques, we showcase results for both under an environment map, as depicted in Fig.~\ref{fig_rendering}. Notably, the offline rendering effectively handles the shadows caused by occlusions in the Gaussian representations.






















































 






