\section{Introduction}
Volumetric video captures dynamic 3D scenes from multiple angles, allowing interactive viewing from any perspective. 
This technology is crucial for creating immersive experiences in virtual and augmented reality, enhancing storytelling, education, cultural preservation, and telepresence with lifelike, interactive content. However, traditional volumetric video is often limited by fixed lighting conditions captured during recording, which can clash with dynamic or virtual environments, reducing realism and flexibility. Relightable volumetric video overcomes this limitation by enabling post-capture relighting. This allows for seamless integration into dynamic lighting environments and offers creative control over visual aesthetics.

The prevailing workflow~\cite{collet2015high, dou2016fusion4d, lawrence2024project, garon2016real} for producing relightable volumetric videos in the industry still relies on tracked mesh sequences and texture videos, which can be seamlessly integrated into standard CG pipelines to support relighting under various lighting conditions. However, the intricate reconstruction process often introduces artifacts such as holes and noise, and the quality of relighting remains constrained, frequently resulting in visible imperfections.
Neural advancements~\cite{srinivasan2021nerv, zhang2021nerfactor, yao2022neilf} focus on enabling relighting capabilities using neural factorization within implicit MLPs representations. However, these approaches often face challenges in balancing training efficiency, rendering speed, and output quality, ultimately failing to deliver satisfactory results.
Recently, 3D Gaussian Splatting (3DGS)\cite{gaussiansplatting}, an efficient point-based representation, has achieved photo-realistic rendering at unprecedented frame rates. While dynamic variants\cite{jiang2024robust, li2023spacetime, vcube} can produce high-quality volumetric videos, they fail to produce the detailed geometry necessary for essential operations like relighting.
Although efforts~\cite{gao2025relightable, shi2023gir, liang2024gs, jiang2024gaussianshader} have been made to integrate physically-based rendering into the 3DGS pipeline, these methods are often computationally expensive and limited to static scenarios. These limitations severely restrict their applicability in industrial workflows, hindering the efficient production of 4D content.


In this paper, we introduce BEAM, a novel pipeline that bridges 4D Gaussians with accurate physically-based rendering (PBR) for producing relightable volumetric videos from multi-view RGB footage. Our key idea is to robustly recover detailed geometry and decouple the PBR properties (e.g., ambient occlusion, roughness, and base color) using a carefully selected suite of techniques, i.e., rasterization, performance tracking, and ray tracing, all within a Gaussian-based paradigm. As a result, BEAM enables lifelike dynamic scenes that can be seamlessly and CG-friendly integrated into various platforms under diverse lightings (see Fig.~\ref{fig:teaser}). 

We first recover detailed and spatial-temporally consistent geometries from multiview video input, which organically combines the Gaussian-based performance tracking~\cite{jiang2024robust} with the geometry-aware Gaussian rasterization~\cite{zhang2024rade}. While the former excels at motion tracking and the latter at static geometry recovery, we unify them in a coarse-to-fine optimization framework. Specifically, we employ coarse joint Gaussians to track non-rigid motion and dense skin Gaussians to preserve intricate geometry details. We adopt a robust optimization process that integrates normal consistency, photometric consistency, and temporal regularization to enhance geometric accuracy and smoothness. This enables accurate depth and normal recovery from the dense Gaussians using the geometry-aware rasterizer~\cite{zhang2024rade}, providing a robust foundation for material decomposition and relighting.

We further decouple the dense 4D Gaussians to recover detailed material properties, enabling high-quality physically-based rendering grounded in the rendering equation~\cite{kajiya1986rendering} and simplified Disney BRDF~\cite{burley2012physically}.
Assuming human-centric scenes with negligible metallic components, we focus on accurately associating roughness, ambient occlusion (AO), and base color properties with the Gaussians, ensuring realistic and adaptable rendering under diverse lighting conditions.
To achieve this, we adopt a step-by-step approach to disentangle these properties.  
Specifically, we first generate a roughness texture using the material diffusion module in previous work~\cite{zhang2024clay} with multi-view conditioning, which is associated with the dense Gaussians through UV projection. Then, for the AO and base color, we adopt a 2D-to-3D strategy, where these attributes are estimated in the input views to bake 2D material maps, and then optimize into the corresponding dense Gaussians in the 3D space. This strategy effectively reduces noise and smooths the disentanglement to improve relighting quality. For further 2D AO and based color decomposition, the lighting environment during capturing can be estimated using off-the-shelf tool~\cite{new_house_internet_services_bv_ptgui_2025}, while the geometry attributes and roughness are obtained in previous stages. Thus, by carefully re-examining and simplifying the rendering equation~\cite{kajiya1986rendering}, we identify a critical insight: both 2D AO and base color can be accurately derived by accumulated visibility information for specific points along specific directions during ray tracing. We tailor the Gaussian-based ray tracer~\cite{3dgrt2024} to compute such visibility, with a novel alpha blending strategy based on the dense Gaussians. This strategy efficiently captures visibility information, forming the foundation for estimating AO and base color maps in the input viewpoints.

Once the material properties are baked into our dense dynamic Gaussians, these 4D assets seamlessly integrate with traditional CG engines, supporting flexible rendering workflows. For real-time rendering, we adopt deferred shading to deliver immersive and efficient visualizations, while offline rendering leverages ray tracing to precisely capture complex shadows and occlusions.
We further develop a Unity plugin enabling seamless integration of 4D assets into various platforms for real-time, lifelike interactions under diverse lighting conditions. This innovation opens new possibilities for storytelling, interactive entertainment, and creative visualization, offering users an immersive journey into dynamic, relightable volumetric worlds.













	

	
  	




















	
	
	
	
	
	


