




















\section{Experiments} 

To demonstrate our relighting capabilities, we capture human performances with rich textures and challenge body movements using 81 calibrated industrial cameras, enabling high-quality rendering under diverse lighting conditions.
Our pipeline is implemented based on 3DGS~\cite{gaussiansplatting} and trained on a single NVIDIA GeForce RTX 3090. Our method achieves a processing time of 12 minutes per frame for 4D Gaussians modeling and optimization and 4 minutes for materials estimation and baking. The generated relightable 4D Gaussian sequences are fully compatible with VR platforms and CG engines, enhancing immersive experiences during playback and editing. 







\subsection{Comparison}



We compare our method against state-of-the-art static Gaussian relighting techniques, R-3DGS~\cite{gao2025relightable}, GS-IR~\cite{liang2024gs}, both of which reconstruct geometry frame by frame. For dynamic comparison, we first obtain the mesh sequences using D-2DGS~\cite{zhang2024dynamic} and then apply the path replay backpropagation method~\cite{vicini_path_2021} for inverse rendering to decouple attributes, such as base color and roughness for each frame individually. As shown in Fig.~\ref{fig_comparison1}, the normals and AO decoupled by Relightable-3DGS are blurry, resulting in significant relighting artifacts. GS-IR struggles to reconstruct smooth normals and effectively separate the base color from other attributes. While D-2DGS with inverse rendering produces smoother normals and more accurate attribute decoupling, the relighting results are prone to losing high-frequency details. In contrast, our method produces smooth normals and accurately decouples the AO and base color, enabling high-fidelity relighting results.

For quantitative comparison, we conduct evaluations on synthetic data to generate ground truth images under predefined lighting conditions. We use Blender to simulate the similar capture perspectives of our dome system and render human meshes from RenderPeople\cite{RenderPeople} into corresponding viewpoints using the CYCLES engine.
we employ PSNR, SSIM, and LPIPS as metrics. To ensure a fair and precise comparison, we compute the metrics for AO, base color, and relighting results across two synthetic sequences, each consisting of 150 frames. In addition, we compute these metrics within the bounding box of the human region. As shown in Tab.~\ref{tab:comparison_full}, our method surpasses the other techniques in all metrics evaluated.

\begin{table}[t]
    \caption{
        Quantitative comparison with SOTA relighting
 methods on our blender-rendered synthetic dataset. \colorbox{best1}{Green} and \colorbox{best2}{yellow} cell colors indicate
 the best and the second-best results.
    }
    \label{tab:comparison_full}
    \centering

    \addtolength{\tabcolsep}{-2pt}
    \resizebox{\linewidth}{!}{

        \begin{tabular}{l|cccccccccc}  %
            \hline
            & \multicolumn{3}{c}{\textbf{AO}} & \multicolumn{3}{c}{\textbf{Base Color}} & \multicolumn{3}{c}{\textbf{Relighting}}                                                                                                                                 \\
            Method                              & PSNR$\uparrow$                          & SSIM$\uparrow$                          & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIPS$\downarrow$ \\
            \hline
            
            R-3DGS         & 20.67      & 0.791   & 0.307         & 20.65         & \colorbox{best2}{0.853}        & 0.166       & \colorbox{best2}{24.66}     & 0.855      & \colorbox{best2}{0.105}            \\
            GS-IR      & 18.95     & 0.850    & 0.345    & 13.35     & 0.702      &  0.252    & 23.05      & \colorbox{best2}{0.858}  & 0.177     \\
            D-2DGS$^{\ast}$    & \colorbox{best2}{24.19}     & \colorbox{best2}{0.906 }       & \colorbox{best2}{0.232}    & \colorbox{best2}{21.04}        &  0.839       & \colorbox{best2}{0.145}     & 23.23      & 0.857    & 0.198           \\
            Ours      & \colorbox{best1}{25.32}       & \colorbox{best1}{0.924}     & \colorbox{best1}{0.168}    & \colorbox{best1}{21.47}    & \colorbox{best1}{0.906}   & \colorbox{best1}{0.084}            & \colorbox{best1}{25.57}         & \colorbox{best1}{0.895}          & \colorbox{best1}{0.086}             \\
            \hline
        \end{tabular}
    }
    \addtolength{\tabcolsep}{2pt}

\end{table}











\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/ablation.jpg}
  \captionof{figure}{Ablation on parameter settings for materials decomposition. Our full model provides accurate AO and base color representations, resulting in high-quality relighting with minimal artifacts.} 
  \label{fig_ablation_study}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/ablation_number.jpg}
  \captionof{figure}{Ablation study on the number of geometry-aware Gaussians. With $\sim140,000$ Gaussians, BEAM achieves high rendering quality and geometric reconstruction accuracy, while ensuring efficient training and rendering.} 
  \label{fig_ablation_study_point}
\end{figure}

\subsection{Evaluation}
\paragraph{Materials Decomposition.}
We conduct a qualitative ablation study on materials decomposition to the impact of different variables on AO, base color, and the final relighting results. For the offset variable in the Gaussians surface estimation during ray tracing, we present the results without the offset and with an excessively large offset (0.1), in the first and second columns of Fig.~\ref{fig_ablation_study}. We found that without an offset, AO and base color contained many black artifacts, and the Gaussian surface estimation suffered significant degradation. On the other hand, an offset of 0.1 led to overestimation, causing incorrect lighting decoupling and producing overly bright artifacts. Additionally, regarding the SH order required to represent AO and base color, as illustrated in the third column of Fig.~\ref{fig_ablation_study}, using a zero-order SH Gaussian resulted in blurriness. In contrast, our full ray tracing and material optimization pipeline, with an offset set to 0.02 and using third-order SH degree with geometry-aware Gaussians, provides accurate calculations and representations of the Gaussian's AO and base color attributes.
\paragraph{The Number of Gaussians.}
We evaluate the impact of varying the number of geometry-aware Gaussians on both rendering quality and mesh quality across 100 frames in our virtual dataset, using PSNR and Chamfer Distance respectively. As shown in Fig.~\ref{fig_ablation_study_point}, using $\sim140,000$ geometry-aware Gaussians strikes an optimal balance between rendering quality and geometric reconstruction accuracy, without introducing excessive redundancy. This point count ensures efficient training and is well-suited for applications such as VR and AR.

\subsection{User Study}
We conduct a user study to evaluate the temporal reconstruction quality and normal consistency of our 4D Gaussians. 
We assess our method on our captured dataset, comparing it against R-3DGS, GS-IR, as well as D-2DGS, and ask 30 users to select the best option. For GS-IR and R-3DGS, we train Gaussian models for 200 frames individually and concatenate the per-frame normal maps. 
For our method and D-2DGS, we rasterize a normal map sequence from a 200-frame dynamic Gaussian sequence. 
In terms of temporal reconstruction quality, 95.65\% of users prefer our method, while 87\% choose our approach for normal consistency.
These preference results clearly indicate a significant advantage of our method over the competing approaches, demonstrating its superior performance in both aspects.






































    

    









				




