\section{\textbf{Reivew of frequency-domain scattered wavefield solutions with vanilla PINN}}\label{method_review}
Modeling seismic wave propagation in acoustic, isotropic media with constant density involves solving the frequency-domain acoustic wave equation, which is given by \citep{Aki1980QuantitativeST}: 
\begin{equation}\label{eq1} 
\omega^2 m(\mathbf{x}) u (\mathbf{x},\mathbf{x}_s,\omega) +  
\nabla^2 u (\mathbf{x},\mathbf{x}_s,\omega) = s(\mathbf{x},\mathbf{x}_s,\omega),
\end{equation} 
where $\omega$ denotes the angular frequency, $\mathbf{x} = (x,z)$ is spatial coordinates in a 2-D medium, $\mathbf{x}_s=(x_s,z_s)$ represents the source position, $m(\mathbf{x}) = 1/v^2(\mathbf{x})$ is the spatially varying squared slowness corresponding to velocity $v(\mathbf{x})$, $u(\mathbf{x}, \mathbf{x}_s,\omega)$ is the complex wavefield in the frequency domain, $s(\mathbf{x},\mathbf{x}_s,\omega)$ represents the source term, and $\nabla^2$ is the Laplacian operator with respect to the spatial coordinates. 

Recently, PINNs have been proposed as an effective tool for solving wave equations by embedding the governing equations into the training process of NNs \citep{raissi2019physics}. PINNs enable the approximation of wavefield solutions without requiring labeled data, utilizing automatic differentiation to enforce the physical laws described by the wave equation. However, the singularity in Equation (\ref{eq1}) caused by a point representation of the source poses challenges for PINNs, as it hinder the convergence speed and reduce the accuracy of the wavefield solution. 

To overcome this issue, \cite{alkhalifah2021wavefield} proposed reformulating the equation using a scattered wavefield approach based on scattering theory. This leads to the following perturbation equation:
\begin{equation}\label{eq2} 
\omega^2 m(\mathbf{x}) \delta u(\mathbf{x},\mathbf{x}_s,\omega) + \nabla^2 \delta u(\mathbf{x},\mathbf{x}_s,\omega) = -\omega^2 \delta m(\mathbf{x}) u_0(\mathbf{x},\mathbf{x}_s,\omega), 
\end{equation}
where $\delta u(\mathbf{x},\mathbf{x}_s,\omega) = u(\mathbf{x},\mathbf{x}_s,\omega) - u_0(\mathbf{x},\mathbf{x}_s,\omega)$ is the scattered wavefield, $u_0(\mathbf{x},\mathbf{x}_s,\omega)$ is the known background wavefield corresponding to a homogeneous medium with constant velocity $v_0$, and $\delta m(\mathbf{x}) = m(\mathbf{x}) - m_0$ is the slowness perturbation with $m_0 = 1/v_0^2$. 

In a homogeneous and infinite background medium, the background wavefield $u_0(\mathbf{x}, \mathbf{x}_s,\omega)$ can be analytically computed using:
\begin{equation}\label{eq3} 
u_0(\mathbf{x},\mathbf{x}_s,\omega) = \frac{\mathrm{i}}{4} H_0^{(2)}\left( \frac{\omega}{v_0} \left| \mathbf{x} - \mathbf{x}_s \right| \right), 
\end{equation}
where $H_0^{(2)}$ denotes the zero-order Hankel function of the second kind, $\mathrm{i}$ is the imaginary unit, and $\left| \cdot \right|$ represents the Euclidean distance. This equation allows us to quickly obtain the analytical wavefield solution at arbitrary spatial positions. 

By leveraging this reformulation, \cite{song2021solving} employed a vanilla PINN to directly predict the scattered wavefield $\delta u(\mathbf{x},\mathbf{x}_s,\omega)$, where the inputs are the spatial coordinates $\mathbf{x}$ and source position $\mathbf{x}_s$, and the outputs are the real part $\delta u_R(\mathbf{x},\mathbf{x}_s,\omega)$ and the imaginary part $\delta u_I(\mathbf{x},\mathbf{x}_s,\omega)$ of the scattered wavefield. The vanilla PINN is constructed as a fully connected NN, which can be represented by the mapping:
\begin{equation}\label{eq4} 
\delta u(\mathbf{x},\mathbf{x}_s,\omega) = \delta u_R(\mathbf{x},\mathbf{x}_s,\omega) + \mathrm{i} \delta u_I(\mathbf{x},\mathbf{x}_s,\omega) = \text{NN}_\theta(\mathbf{x},\mathbf{x}_s,\omega), 
\end{equation}
where $\text{NN}_\theta(\mathbf{x},\mathbf{x}_s,\omega)$ represents the PINN parameterized by weights and biases $\boldsymbol{\theta}$.  

The architecture of a vanilla PINN includes multiple fully connected hidden layers, which can be represented by:
\begin{equation}\label{eq5} 
\begin{aligned} 
h^{(1)} &= f[W_l \cdot (\mathbf{x}, \mathbf{x}_s) + b_l], \quad l = 1, \\
h^{(l)} &= f[W_l \cdot h^{(l)} + b_l], \quad l = 2, \ldots, L-1, \\
h^{(L)} &= W_L \cdot h^{(L)} + b_L, 
\end{aligned} 
\end{equation}
where $h^{(l)}$ represents the activations in layer $l$, $W_l$ and $b_l$ are the weights and biases of layer $l$, and $f[\cdot]$ is the activation function, often chosen to be a sine or another nonlinear function to introduce nonlinearity. The output layer directly provides the real part $\delta u_R(\mathbf{x},\mathbf{x}_s,\omega)$ and the imaginary part $\delta u_I(\mathbf{x},\mathbf{x}_s,\omega)$ of the scattered wavefield, without applying any activation function (linear layer). 

The training of the PINN is guided by a physics-informed loss function:
\begin{equation}\label{eq6}
\mathcal{L}_{p} = \frac{1}{N}\sum\limits_{j=1}^{N} \left ( {\left|(\omega^2 m^j + \nabla^2)\delta u_R^j + \omega^2 \delta m^j u_{R0}^j \right|_2^2 } \right. 
\left. {+ \left|(\omega^2 m^j + \nabla^2)\delta u_I^j + \omega^2 \delta m^j u_{I0}^j \right|_2^2 }\right),
\end{equation}
where $N$ is the number of training points sampled from the spatial domain, and $u_{R0}$ and $u_{I0}$ are the real and imaginary parts of the background wavefield $u_0(\mathbf{x}, \mathbf{x}_s, \omega)$, respectively. Since we use the network to approximate the nonlinear relationship from the input spatial position to the scattered wavefield values at the input spatial position, we can use automatic differentiation to compute the required spatial derivatives in the loss function. This provides a key advantage where we eliminate the need for grid-based discretization, allowing the network to learn a solution that inherently satisfies the differential operators across the entire domain. 

However, when we represent the scattered wavefield for large-scale, complex velocity models or with high-frequency, it typically requires deeper and wider NN architectures \citep{alkhalifah2021wavefield}. This is because large-scale complex velocity models exhibit significant spatial heterogeneity and discontinuities, which imposes higher demands on the network's representational capacity. High-frequency wavefields contain intricate details and rapid variations that shallow networks struggle to effectively capture. However, larger networks require storing a substantial number of parameters, and during training, the activation values and gradients at each layer consume significant memory resources. In addition, increasing the network's depth and width, which leads to a greater number of parameters, makes the optimization process more complex by not only adding to the computational burden but also increasing the risk for gradient vanishing or explosion, resulting in inefficient training and difficulty in convergence. 

On the other hand, since PINNs are fundamentally designed to approximate functions based on a PDE loss corresponding to specific parameters (like velocity model, frequency or source location), they face limitations when generalizing to new parameters. For instance, if the velocity model or the frequency changes, the previously trained PINN may fail to accurately predict the wavefield for these new conditions. The lack of generalization stems from the fact that the network representation is tightly coupled to the specific training velocity model or frequency. This necessitates retraining for each new scenario, which is computationally inefficient. 

To address these challenges, in the following section, we employ singular value decomposition (SVD) to represent the network with fewer, more effective parameters, thereby significantly addressing the burden of representing high-frequency wavefields and large-scale velocity models on network size. We, also, utilize a meta-training framework to improve the generalization capabilities of the PINN. Our method specifically aims to overcome the issues related to adapting the network to new velocity models and different frequencies, by training the network from a learned initialization, thereby significantly improving efficiency and scalability. 

\section{\textbf{Method}}\label{method}
\subsection{Low-rank decomposition of the network parameters}
In the context of machine learning, weight matrices in NNs often exhibit significant redundancy. This means that they can be effectively approximated by lower-rank representations without a notable loss in accuracy. This property is particularly advantageous given the growing size of modern NNs, as reducing the dimensionality of weight matrices can yield substantial savings in memory and computational cost, while also improving generalization by mitigating overfitting. 

To address the redundancy, we consider a low-rank decomposition of the weight matrices $W_l$ of each layer $l$ in the PINN, the decomposition can be expressed as:
\begin{equation}\label{eq7}
W_l = U_l \cdot V_l^\text{T}, \quad l = 1, \ldots, L,
\end{equation}
where $U_l \in \mathbb{R}^{m \times k}$ and $V_l \in \mathbb{R}^{n \times k}$, with $k \ll \min(m, n)$. Here, $k$ denotes the rank of the approximation. This factorization significantly reduces the number of parameters because the combined size of $U_l$ and $V_l$ is much smaller than the original size of $W_l$, for small k. Consequently, this decomposition reduces the computational burden and improves the networkâ€™s efficiency, enabling faster training and inference. 

While the conventional low-rank decomposition reduces the number of parameters, it lacks the structured interpretability required for modeling frequency-dependent wavefields. To address this limitation, we further extend it to a more informative low-rank decomposition, Singular Value Decomposition (SVD). SVD factorizes the weight matrix $W_l$ into three components:
\begin{equation}\label{eq8}
W_l = U_l \cdot \Theta_l \cdot V_l^\text{T},
\end{equation}
where $\Theta_l$ is a diagonal matrix containing singular values:
\begin{equation}\label{eq9}
\Theta_l = \text{diag}(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k}) = 
\begin{bmatrix}
\sigma_{l,1} & 0 & \cdots & 0 \\
0 & \sigma_{l,2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{l,k}
\end{bmatrix}.\\
\end{equation}

Compared to the conventional low-rank decomposition, SVD introduces an additional diagonal matrix $\Theta_l$, which contains only $k$ singular values. The slight increase in parameter count is negligible relative to the memory and computational savings achieved by the decomposition. 

Despite the small increase in parameters, SVD brings significant advantages in terms of representation and interpretability. First, it provides a more structured decomposition that separates the weight matrix into orthonormal bases ($U_l$ and $V_l$) and singular values ($\Theta_l$). This structured factorization separates the geometric transformations (captured by \(U_l\) and \(V_l\)) from their scaling (captured by \(\Theta_l\)), enabling a more interpretable representation of the network's internal operations. As a result, it provides a more detailed understanding of how different components of the network contribute to the overall transformation. 

Moreover, the singular values, $\sigma_{l,i}$, $i=1,\ldots,k$, dominate the contribution of each rank-1 component to the transformation performed by the layer. By associating these singular values with frequency-dependent information, we can directly encode and control the frequency components of the scattered wavefield within the network. This is particularly important as the scattered wavefield inherently varies with frequency. Thus, by applying SVD to the weight matrices of the PINN, we can not only reduce the parameter count but also achieve a direct link between the network's internal representation and the frequency content of the wavefield. This enhanced representation improves the generalization capability of the model, particularly for new frequency and velocity scenarios, without requiring retraining from scratch. \\

In the following section, we will introduce how to explicitly link the frequency of scattered wavefield to the singular values of the weight matrix. This approach allows us to incorporate frequency information more directly into the network, thereby enhancing its adaptability and efficiency in simulating scattered wavefields at different frequencies. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.98\textwidth]{Figure/fig1.png}
\caption{
Illustration of the proposed LRPINN with frequency embedding. 
(a) \textbf{Frequency embedding hypernetwork}: This network maps the input frequency (\(\textit{freq}\)) to frequency-dependent singular values (\(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k}\)) for each layer through a series of linear layers with sine and GELU activations. These singular values are used to construct diagonal matrices (\(\Theta_l\)) that dynamically adapt to the input frequency. 
(b) \textbf{Low-rank PINN}: The inputs (spatial location $\mathbf{x} =(x,z)$ and source location $\mathbf{x}_s =(x_s,z_s)$) are encoded using sine activations, and each layer's weight matrix (\(W_l\)) is factorized into \(U_l \cdot \Theta_l \cdot V_l^\text{T}\), incorporating the frequency-dependent singular values from (a). The network outputs the real (\(\delta u_R(\mathbf{x}, \mathbf{x}_s, \omega)\)) and imaginary (\(\delta u_I(\mathbf{x}, \mathbf{x}_s, \omega)\)) components of the scattered wavefield.}
\label{fig1}
\end{figure}

\subsection{Low-rank PINN with frequency embedding}
To address the challenges of modeling scattered wavefields across a wide range of frequencies, we propose an innovative low-rank PINN (LRPINN) architecture that incorporates frequency embedding. By explicitly linking the frequency of the scattered wavefield to the singular values of each layer's weight matrix, the proposed framework achieves enhanced adaptability and scalability for different frequency scenarios. 

The architecture is illustrated in Figure~\ref{fig1}, which consists of two main components:
\begin{enumerate}
    \item \textbf{Frequency embedding hypernetwork (FEH)} (Figure \ref{fig1}a): \\
    The FEH is designed to link the input frequency $\textit{freq}$ to the singular values of each layerâ€™s weight matrix. Specifically, the FEH begins by feeding the frequency into a series of linear layers followed by sine activation functions to extract frequency-specific features. This helps the network capture the complex relationships between frequency and the wavefield characteristics. The output from these linear layers with sine activation is then processed through additional linear layers followed by gaussian error linear unit (GELU) activation functions, resulting in the singular values for each hidden layer. For each layer $l$, the network produces a vector $\sigma_l=(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k})$, which is then used to construct the diagonal singular value matrix: 
    \begin{equation}
    \Theta_l = \text{diag}(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k}).
     \end{equation}
    The singular values produced by the FEH, denoted by $\Theta_l$, are frequency-dependent and they dynamically update the low-rank decomposition of each hidden layerâ€™s weight matrix in the LRPINN. 
    \item \textbf{LRPINN with frequency-aware weights} (Figure \ref{fig1}b): \\
    The second component of the proposed architecture is the LRPINN, which represents the scattered wavefield by utilizing low-rank decomposed weights that are modulated based on the input frequency. The spatial coordinates $(\mathbf{x}, \mathbf{x}_s)$ are first passed through a linear layer followed by sine activation functions to encode spatial features effectively. Each hidden layer of the LRPINN is represented by a weight matrix $W_l$, which is factorized using SVD as shown in Equation \ref{eq8}. Here, $U_l \in \mathbb{R}^{m \times k}$ and $V_l \in \mathbb{R}^{n \times k}$ are learned matrices, and $\Theta_l$ is the frequency-dependent diagonal matrix of singular values produced by the FEH. We emphasize that, for further simplification, we here discard the bias of each hidden layer. In other words, the input of each hidden layer is directly multiplied by the constructed $W_l$ without adding the bias, but is directly activated. After passing through the sequence of layers, the network produces the real and imaginary components of the scattered wavefield, $\delta u_R(\mathbf{x}, \mathbf{x}_s, \omega)$ and $\delta u_I(\mathbf{x}, \mathbf{x}_s, \omega)$, respectively, which represent the final predictions of our LRPINN.

\end{enumerate}

By embedding the frequency information into the singular values of each layer, our proposed architecture allows the weight matrix $W_l$ to change based on the frequency of the input, effectively adapting the representation of each hidden layer to match the characteristics of the given frequency. Meanwhile, the low-rank representation reduces the number of parameters needed to represent the weight matrices and, thus, improves the network's ability to represent the scattered wavefield across different frequencies effectively and efficiently. Furthermore, the frequency embedding provides an efficient way to adjust these parameters based on the frequency of the input data. For example, we can adaptively adjust the network weights by removing smaller singular values and their corresponding matrices ($\delta u_R(\mathbf{x}, \mathbf{x}_s, \omega)$ and $\delta u_I(\mathbf{x}, \mathbf{x}_s, \omega)$) based on their magnitudes, as we will see later. This strategy helps make the network more compact and computationally efficient. 


\subsection{Meta-learning enhanced LRPINN}
While the proposed LRPINN with frequency embedding demonstrates significant improvements in efficiency and adaptability, its performance is highly sensitive to the quality of the initial weight matrices. Random initialization, which is commonly used, often results in slow convergence and suboptimal solutions. This issue is particularly pronounced in low-rank settings, as the reduced parameterization limits the network's capacity to explore the solution space effectively during optimization. Consequently, achieving optimal performance with the LRPINN can be challenging without a well-designed initialization strategy. 

To address this limitation, we, in this subsection, propose using meta-learning \citep{finn2017model} to provide a robust initialization for the LRPINN, where we call this framework as Meta-LRPINN. Meta-LRPINN consists of two key stages: meta-training and meta-testing, as outlined in Algorithm \ref{alg1} and Algorithm \ref{alg2}, respectively. 

The goal of the meta-training stage is to find an optimal initialization for LRPINN parameters $\boldsymbol{\theta}$ and FEH parameters $\boldsymbol{\epsilon}$ that allows for fast adaptation across a range of velocity $v(\mathcal{T})$ and frequency $\textit{freq}(\mathcal{T})$ distributions. Here, $mathcal{T}$ represents all training data set, which can be considered as all velocity models and their corresponding frequencies. This stage involves a double-loop optimization strategy, consisting of an inner loop for task-specific adaptation and an outer loop for meta-optimization. 

The inner loop begins by copying the initialization parameters, which is denoted by ($\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'$), from the previous iteration. These parameters are then updated to adapt specifically to a sampled support dataset ($\mathcal{T}_i^s$) and corresponding frequency ($\textit{freq}^s$), i.e., a velocity model and the corresponding frequency sampled from the training data set. The task-specific loss $\mathcal{L}_{T_i^s, \textit{freq}^s}$ is computed based on the support dataset, and gradient descent is applied to adjust the parameters:
\begin{equation}\label{eq11}
(\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i') = (\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i') - lr_{\text{inner}} \cdot \nabla_{\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'} \mathcal{L}_{\mathcal{T}_i^s, \textit{freq}^s}(G_{\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'}),
\end{equation}
where $lr_{\text{inner}}$ is the inner Loop learning rate, and $G_{\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'}$ represents a parameterized function with the copied parameters ($\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'$). The updated parameters $(\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i')$ are specific to the current support dataset and the corresponding frequency, representing the model's adaptation to the given velocity and frequency. 

In the outer loop, the adapted parameters from the inner loop $(\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i')$ are directly evaluated on the corresponding query dataset $T_i^q$ and the corresponding frequency $\textit{freq}^q$. The query dataset loss $\mathcal{L}_{\mathcal{T}_i^q, \textit{freq}^q}$ measures how well the task-specific adaptation generalizes to new velocity and frequency. This loss is computed for all sampled support-query pairs, and the total query loss is accumulated:
\begin{equation}\label{eq12}
\mathcal{L}_{\text{sum}} = \sum_i \mathcal{L}_{\mathcal{T}_i^q, \textit{freq}^q}(G_{\boldsymbol{\theta}_i', \boldsymbol{\epsilon}_i'}).
\end{equation}
The accumulated loss is then used to compute the gradients with respect to the global initialization parameters ($\boldsymbol{\theta}, \boldsymbol{\epsilon}$), which are updated using gradient descent:
\begin{equation}\label{eq13}
(\boldsymbol{\theta}, \boldsymbol{\epsilon}) \leftarrow (\boldsymbol{\theta}, \boldsymbol{\epsilon}) - lr_{\text{meta}} \cdot \nabla_{\boldsymbol{\theta}, \boldsymbol{\epsilon}} \mathcal{L}_{\text{sum}},
\end{equation}
where $lr_{\text{meta}}$ is the outer loop learning rate. This update ensures that the initialization parameters ($\boldsymbol{\theta}, \boldsymbol{\epsilon}$) are optimized to perform well across all tasks (velocity models and corresponding frequencies). 

The inner loop and outer loop are repeated for multiple iterations. In each iteration, we sample $N$ pairs of support-query datasets along with the corresponding $(\textit{freq}^s, \textit{freq}^q)$ pairs from the given velocity and frequency distributions, which can be considered as $N$ tasks. Each iteration will then iterate over these $N$ pairs of datasets. This iterative process allows the model to learn a robust initialization that generalizes across tasks, enabling effective task-specific adaptation during the meta-testing stage. 

In the meta-testing stage, the meta-trained initialization is used to adapt the LRPINN to specific velocity models $v$ and frequency $\textit{freq}$, leveraging the knowledge learned during meta-training phase. Specifically, first, the FEH, which accept the test frequency $\textit{freq}$, predicts the singular values for each layer of the LRPINN: 
\begin{equation} \label{eq14}
\sigma_l = (\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k}) = \text{FEH}(\textit{freq}). 
\end{equation} 
After this step, the FEH is pruned to reduce computational complexity and, also, further improve computational efficiency, leaving only the LRPINN fine-tuned for the specific task. Next, the orthogonal matrices $(U_l, V_l)$ are extracted from the LRPINNâ€™s meta-initialized weights and combined with the predicted singular values $\sigma_l$ to construct the weight matrices dynamically: 
\begin{equation}\label{eq15}
W_l = U_l \cdot \Theta_l \cdot V_l^\text{T}, 
\end{equation} 
where $\Theta_l = \text{diag}(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k})$, and the orthogonal matrices $U_l$ and $V_l$ and the singular values $\sigma_l$ are set as learnable parameters. These learnable parameters $(U_l, V_l, \sigma_l)$ are then fine-tuned using gradient descent: 
\begin{equation}\label{eq16}
(U_l, V_l, \sigma_l) \leftarrow (U_l, V_l, \sigma_l) - lr \cdot \nabla_{U_l, V_l, \sigma_l} \mathcal{L}_{v, \textit{freq}}F_{U_l, V_l, \sigma_l}, 
\end{equation} 
where $lr$ is the meta-testing learning rate, and $F_{U_l, V_l, \sigma_l}$ is a parameterized function to represent the LRPINN with the learnable parameters $(U_l, V_l, \sigma_l)$. 

Compared to our previous work, Meta-PINN \citep{cheng2025meta}, the Meta-LRPINN presented here leverages not only the prior knowledge of the velocity distribution, but also the frequency distribution to learn an optimized initialization, in our new SVD based representation of PINN weights. This initialization allows the network to achieve faster convergence and better performance across new velocity models and various frequencies. The SVD representation, on the other hand, allows for a low-rank representation of the PINN network weights, which effectively enhances computational efficiency and reduces memory consumption. Furthermore, Meta-LRPINN can refine the low-rank representation by pruning less significant components while simplifying the model by removing the FEH. This approach maintains the efficiency benefits of the LRPINN while further improving its adaptability and generalization capabilities. 

\begin{algorithm}
\caption{Meta-LRPINN: Meta-Training}\label{alg1}
\textbf{Input:} ${v(\mathcal{T})}$: The full velocity dataset. \\
\textbf{Input:} ${\textit{freq}(\mathcal{T})}$: The full frequency distribution. \\
\textbf{Input:} ${lr_{inner}, lr_{meta}}$: Learning rates for inner and outer loops, respectively. \\
\textbf{Input:} ${iter}$: The number of iterations in the support dataset. \\
\textbf{-------------------------------- Meta-training stage -----------------------------} \\
\textbf{Output:} Meta-based initialization of the LRPINN and FEH
\begin{algorithmic}
\State 1: Randomly initialize LRPINN parameters $\boldsymbol{\theta}$ and FEH parameters $\boldsymbol{\epsilon}$
\State 2: \textbf{while} all velocity model ${v(\mathcal{T})}$ and frequencies ${\textit{freq}(\mathcal{T})}$ \textbf{do}
\State 3: \quad Sample batches of support $\mathcal{T}_i^s$ and query $\mathcal{T}_i^q$ datasets from the full dataset $v(\mathcal{T})$
\State 4: \quad Sample the corresponding frequencies $\textit{freq}^s$ and $\textit{freq}^q$ from ${\textit{freq}(\mathcal{T})}$ to \\
\quad \quad \quad \quad \quad \quad the sampled batches of support and query datasets, respectively
\State 5: \quad \textbf{for} every $(\mathcal{T}_i^s, \textit{freq}^s)$ and $(\mathcal{T}_i^q, \textit{freq}^q)$ \textbf{do}
\State 6: \quad \quad Copy the Meta-LRPINN model $G_{\boldsymbol{\theta}^{'}, \boldsymbol{\epsilon}^{'}} = G_{\boldsymbol{\theta}, \boldsymbol{\epsilon}}$
\State 7: \quad \quad \textbf{for} ${i}$ \textbf{in} ${iter}$ \textbf{do}
\State 8: \quad \quad \quad \quad Evaluate $\nabla_{\boldsymbol{\theta}^{'}, \boldsymbol{\epsilon}^{'}} \mathcal{L}_{\mathcal{T}_i^s, \textit{freq}^s} \left( G_{\boldsymbol{\theta}^{'}, \boldsymbol{\epsilon}^{'}} \right)$ with respect to \\
\quad \quad \quad \quad \quad \quad \quad \quad \quad the sampled support dataset $\mathcal{T}_i^s$ and frequency $\textit{freq}^s$
\State 9: \quad \quad \quad \quad Compute adapted parameters with gradient descent: \\
\quad \quad \quad \quad \quad \quad \quad \quad \quad $(\boldsymbol{\theta}_i^{'}, \boldsymbol{\epsilon}_i^{'})  = (\boldsymbol{\theta}_i^{'}, \boldsymbol{\epsilon}_i^{'}) - lr_{inner} \cdot \nabla_{\boldsymbol{\theta}_i^{'}, \boldsymbol{\epsilon}_i^{'}} \mathcal{L}_{\mathcal{T}_i^s, \textit{freq}^s} \left( G_{\boldsymbol{\theta}_i^{'}, \boldsymbol{\epsilon}_i^{'}} \right)$
\State 10: \quad \quad \textbf{end for}
\State 11: \quad \quad Evaluate $ \mathcal{L}_{\mathcal{T}_i^q, \textit{freq}^q}( G_{\boldsymbol{\theta}_i^{'}, \boldsymbol{\epsilon}_i^{'}})$ with respect to the sampled query dataset $\mathcal{T}_i^q$ and frequency $\textit{freq}^q$
\State 12: \quad \textbf{end for}
\State 13: \quad Sum the loss of all samples on the query dataset $\mathcal{T}_i^q$ and the corresponding frequencies $\textit{freq}^q$: \\
\quad \quad \quad \quad \quad \quad \quad \quad $\mathcal{L}_{sum} = \sum_{\mathcal{T}_i^q, \textit{freq}^q} \mathcal{L}_{\mathcal{T}_i^q, \textit{freq}^q} ( G_{{\boldsymbol{\theta}}_i^{'}, {\boldsymbol{\epsilon}}_i^{'}})$
\State 14: \quad Update the Meta-LRPINN $(\boldsymbol{\theta}, \boldsymbol{\epsilon}) \leftarrow (\boldsymbol{\theta}, \boldsymbol{\epsilon}) - lr_{meta} \cdot \nabla_{\boldsymbol{\theta}, \boldsymbol{\epsilon}} \mathcal{L}_{sum}$
\State 15: \textbf{end while}
\State 16: \textbf{Return:} Meta-LRPINN parameters $(\boldsymbol{\theta}, \boldsymbol{\epsilon})$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Meta-LRPINN: Meta-Testing}\label{alg2}
\textbf{Input:} $v$: Test velocity model. \\
\textbf{Input:} $\textit{freq}$: Test frequency of the scattered wavefield. \\
\textbf{Input:} $lr$: Learning rate for meta-testing stage. \\
\textbf{Input:} ${iter}$: The number of iterations in the meta-testing stage. \\
\textbf{-------------------------------- Meta-testing stage -----------------------------} \\
\textbf{Output:} Velocity- and frequency-specific Meta-LRPINN model
\begin{algorithmic}
\State 1: Load Meta-LRPINN initialization parameters $(\boldsymbol{\theta}, \boldsymbol{\epsilon})$ from Meta-training stage
\State 2: Input frequency $\textit{freq}$ to FEH to obtain \\
\quad \quad \quad \quad the singular values of each layer $l$ of LRPINN: $(\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k}) = \text{FEH}(\textit{freq})$
\State 3: Turn predicted singular values $\sigma_{l} = (\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k})$ into learnable parameters and prune FEH
\State 4: Extract the matrices $U_l \in \mathbb{R}^{m \times k}$ and $V_l \in \mathbb{R}^{n \times k}$ from Meta-LRPINN initialization parameters $\boldsymbol{\theta}$ 
\State 5: Set matrices $U_l \in \mathbb{R}^{m \times k}$ and $V_l \in \mathbb{R}^{n \times k}$ to be learnable
\State 6: \textbf{for} ${epoch}$ \textbf{in} ${iter}$ \textbf{do}
\State 7: \quad \quad Construct diagonal matrices $\Theta_l$ from the learnable singular values $\sigma_{l}$
\State 8: \quad \quad Calculate the weight matrix $W_l$ of each layer of LRPINN: $W_l = U_l \cdot \Theta_l \cdot V_l^\text{T}$
\State 9: \quad \quad Evaluate $\nabla_{U_l, V_l, \sigma_{l}} \mathcal{L}_{v, \textit{freq}} \left( F_{U_l, V_l, \sigma_{l}} \right)$ with respect to \\
\quad \quad \quad \quad \quad \quad the test velocity model and the specific frequency
\State 10: \quad \quad Update the learnable parameters with gradient descent: \\
\quad \quad \quad \quad \quad \quad \quad \quad \quad $(U_l, V_l, \sigma_{l}) = (U_l, V_l, \sigma_{l}) - lr \cdot \nabla_{U_l, V_l, \sigma_{l}} \mathcal{L}_{v, \textit{freq}} \left( F_{U_l, V_l, \sigma_{l}} \right)$
\State 11: \textbf{end for}
\State 12: \textbf{Return:} Velocity- and frequency-specific Meta-LRPINN model
\end{algorithmic}
\end{algorithm}

\subsection{Adaptive rank reduction}\label{method4}
As described in the previous subsections, we chose SVD over traditional low-rank decomposition because SVD provides unique advantages, including a more interpretable representation of the network's internal operations. In SVD, the singular values can be seen as scaling factors for the geometric transformations represented by the orthogonal matrices $U_l$ and $V_l$. This characteristic indicates that larger singular values correspond to significant contributions, while smaller singular values contribute marginally to the overall transformation. 

For seismic wavefields, larger singular values are likely associated with low-frequency components, while smaller singular values may capture high-frequency details. This insight allows us to perform adaptive rank reduction in scenarios where certain wavefield characteristics, such as low-frequency representations, are prioritized. By adaptively reducing the rank of SVD based on the magnitude of the singular values, we can further improve the computational efficiency and memory usage during the meta-testing stage. 

Specifically, during the meta-testing stage, we can first obtain a set of singular values $\sigma_l = (\sigma_{l,1}, \sigma_{l,2}, \dots, \sigma_{l,k})$ for each layer $l$ of the LRPINN by inputting the frequency ($\textit{freq}$) into the FEH, as shown in Equation \ref{eq14}. We then adaptively prune smaller singular values based on their magnitudes. Let $ \tau $ represent a threshold for pruning, such that only singular values satisfying $|\sigma_{l,i}| \geq \tau$ are retained. Mathematically, the pruned singular values can be represented as:
\begin{equation}\label{eq17}
\sigma_l' = \{\sigma_{l,i} \mid |\sigma_{l,i}| \geq \tau, \, i = 1, 2, \dots, k\},
\end{equation}
where the threshold $\tau$ can be adjusted based on task requirements, allowing for a balance between model complexity and representational capacity. Actually, in our implementation, instead of directly choosing a fixed threshold $\tau$, the threshold is determined by the desired percentage of retained ranks. For example, if we aim to retain 50\% of the rank, we sort the singular values by their absolute magnitudes and select the largest 50\%:
\begin{equation}\label{eq18}
\text{indices}(\sigma_l') = \text{top-k}(|\sigma_l|, \lfloor r \cdot k \rfloor),
\end{equation}
where $r$ is the retention ratio (e.g., $r=0.5$ for 50\%), $k$ is the total number of singular values, and $\text{top-k}$ returns the indices of the $\lfloor r \cdot k \rfloor$ largest absolute values. The retained singular values are then given by:
\begin{equation}\label{eq19}
\sigma_l' = \{\sigma_{l,i} \mid i \in \text{indices}(\sigma_l')\}.
\end{equation}

The corresponding rows and columns in the orthogonal matrices $U_l$ and $V_l$ that align with the pruned singular values are also removed:
\begin{equation}\label{eq20}
U_l' = U_l[:, \text{indices}(\sigma_l')], \quad V_l' = V_l[:, \text{indices}(\sigma_l')],
\end{equation}
where $\text{indices}(\sigma_l')$ denotes the indices of the retained singular values in $\sigma_l$. Using the pruned components, the weight matrix $W_l$ for each layer is reconstructed dynamically as:
\begin{equation}\label{eq21}
W_l' = U_l' \cdot \Theta_l' \cdot (V_l')^\text{T},
\end{equation}
where $\Theta_l' = \text{diag}(\sigma_l')$ is the diagonal matrix containing the retained singular values. 

By removing smaller singular values and their corresponding dimensions in $U_l$ and $V_l$, the number of learnable parameters is reduced, significantly decreasing computational and memory costs during meta-testing. This adaption is particularly beneficial in scenarios where low-frequency representations dominate or computational resources are limited.

\subsection{Loss functions}
For our Meta-LRPINN, in addition to the original physical and regularization losses from the Meta-PINN framework, we introduce an additional loss term to ensure the orthogonality of the matrices $U_l$ and $V_l$ in the SVD decomposition, which has the form as follows: 
\begin{equation}\label{eq22} 
\mathcal{L}_{\text{ort}} = \sum_{l=1}^L \left( |U_l^\text{T} U_l - \mathbf{I}|_F^2 + |V_l^\text{T} V_l - \mathbf{I}|_F^2 \right), 
\end{equation} 
where $L$ is the total number of layers, $\mathbf{I}$ is the identity matrix, and $|\cdot|_F$ denotes the Frobenius norm. This term strives to make $U_l$ and $V_l$ orthonormal, which is crucial for the validity of the SVD-based low-rank representation. 

Thus, the total loss combines the three components: 
\begin{equation}\label{eq23} 
\mathcal{L}_{\text{total}} = \lambda_{scale} \cdot (\lambda_p \cdot \mathcal{L}_{\text{physics}} + \lambda_r \cdot\mathcal{L}_{\text{reg}} + \lambda_{ort} \cdot \mathcal{L}_{\text{ort}}),
\end{equation}
where the physical loss has been presented in Equation \ref{eq6}, the regularization loss can be found in our previous work \citep{cheng2025meta}, $\lambda_p$, $\lambda_r$, and $\lambda_{ort}$ are the hyperparameters controlling the weight of the corresponding losses, and $\lambda_{scale}$ denotes a scaling factor that is employed to adjust the magnitude of the total loss value. We emphasize that we use this loss function to optimize network parameters in both meta-training and meta-testing stages. To make the meta-training process more stable, we set $\lambda_{scale}$ to 0.1. In the meta-testing stage, we inherit this setting. Meanwhile, to demonstrate the generalizablity of our Meta-LRPINN, we do not fine-tune the optimal configurations of $\lambda_p$, $\lambda_r$, and $\lambda_{ort}$. Instead, they are all set to 1.

By incorporating the orthogonality loss, the proposed loss function not only enforces the physical constraints of the wavefield and prevents overfitting, but also ensures the correctness of the SVD-based low-rank representation. This comprehensive loss design enhances the robustness and interpretability of the LRPINN while maintaining computational efficiency.