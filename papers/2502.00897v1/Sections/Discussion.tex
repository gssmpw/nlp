\section{\textbf{Discussion}}\label{discussion}
In this section, we will analyze and evaluate some key components of the proposed Meta-LRPINN framework to provide a deeper understanding of its performance and contributions. First, we discuss the role of meta-learning in providing an effective initialization for LRPINN. Second, we explore the impact of rank size on the performance of meta-LRPINN, highlighting how different rank configurations influence the accuracy, convergence, and computational efficiency. Third, we analyze the decision to prune the frequency embedding hypernetwork (FEH) during the meta-testing stage. Lastly, we examine the generalization capability of Meta-LRPINN to out-of-distribution frequencies, given that the meta-trained initialization is learned from a specific range of velocity and frequency distributions. 

\subsection{The contributions of meta-learning}
To evaluate the contribution of meta-learning to our Meta-LRPINN framework, we test the performance of randomly initialized LRPINN. Using the layered velocity model as an example, we consider three different frequencies, 3 Hz, 6 Hz, and 12 Hz, and train LRPINN directly from random initialization. The results are shown in Figure \ref{fig16}, where the PDE loss and accuracy curves for the three frequencies are presented, with panel (a) depicting the physical loss curves and panel (b) showing the accuracy curves. 

The results highlight the difficulty of optimizing LRPINN without meta-learned initialization. The PDE loss for the randomly initialized LRPINN across all three frequencies remains exceedingly high, ranging between $10^{12}$ and $10^{16}$, indicating the network struggles to minimize the physical loss during training. Similarly, the accuracy curves reveal that the error relative to the numerical reference is large, with values between 1500 and 3000 across all three frequencies. This is in stark contrast to the results shown in Figure \ref{fig3} for Meta-LRPINN with meta-learned initialization, where the loss and accuracy curves quickly converge to low values. These results demonstrate the critical role of meta-learning in providing robust initialization for LRPINN. By leveraging meta-learning, the optimization process is significantly stabilized, and the training efficiency is greatly enhanced, particularly for challenging tasks such as representing multi-frequency wavefields in variable velocity models.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Figure/fig16.png}
\caption{The physical loss and accuracy curves of LRPINN initialized with random parameters for three different frequencies: 3 Hz (blue), 6 Hz (orange), and 12 Hz (yellow). (a) The physical loss curves. (b) The accuracy curves.}
\label{fig16}
\end{figure}


\subsection{The effect of rank size}
To explore the influence of rank on the performance of Meta-LRPINN, we conduct an experiment where the rank is fixed during both the meta-training and meta-testing phases, as opposed to using the rank-adaptive reduction strategy in previous numerical examples. Specifically, we train Meta-LRPINNs with different ranks, including 10, 25, 50, 100, and 200, using the same meta-training configuration, with the only difference being the rank size. These models are then evaluated on the layered velocity model during the meta-testing phase for three frequencies: 3 Hz, 6 Hz, and 12 Hz. For each frequency, a separate network is trained. The results are presented in Figure~\ref{fig17}, which shows the physical loss and accuracy curves for each rank size across the three frequencies. Each column corresponds to a specific frequency, with rows displaying the physical loss (left) and accuracy (right) curves. 

From the physical loss curves, we observe no universal trend across all ranks and frequencies. However, certain patterns emerge. The rank-100 model consistently achieves the fastest convergence and the lowest physical loss across all frequencies. This is followed by the rank-200 and rank-50 models, which also exhibit competitive loss values. The performance of the rank-10 and rank-25 models varies: at 3 Hz, the rank-10 model performs worse than rank-25, while at 6 Hz and 12 Hz, rank-10 outperforms rank-25 in terms of both convergence speed and final loss values. This indicates that at higher frequencies, a smaller rank (e.g., rank-10) introduces a form of regularization that aids in convergence, although this effect diminishes for very high ranks such as 100 and 200, where the increased parameterization compensates for the lack of regularization. 

The accuracy curves provide additional insights into the impact of rank size. At 3 Hz, we can observe a clear trend where the convergence speed improve as the rank size increases. However, the final accuracy of all rank configurations holds almost the same. At 6 Hz, this trend is largely preserved, except that the rank-10 model demonstrates slightly faster accuracy convergence than rank-25. At 12 Hz, the rank-100 model achieves the best accuracy and convergence speed, followed by rank-200 and rank-50. The rank-10 and rank-25 models perform relatively poor, but rank-10 exhibits slightly stronger accuracy improvement than rank-25. This indicates that at higher frequencies, maintaining a sufficient rank is critical for capturing the detailed characteristics of the wavefield. 

These results reveal several important insights about the impact of rank size on Meta-LRPINN performance:
\begin{itemize}
    \item \textbf{Higher ranks are essential for high-frequency wavefields}: The results confirm that for high-frequency wavefields (e.g., 12 Hz), smaller ranks fail to effectively capture the detailed characteristics of the wavefield, leading to lower accuracy and slower convergence. This highlights the importance of maintaining sufficient rank for high-frequency applications.
    \item \textbf{Over-parameterization risks with a very high rank}: While increasing the rank generally improves accuracy and convergence, excessively large ranks (e.g., rank-200) introduce redundant parameters, leading to over-parameterization. This can make optimization more challenging, resulting in slightly lower accuracy compared to a more optimal rank, as observed in the comparison between rank-100 and rank-200.
    \item \textbf{Advantages of rank-adaptive reduction}: When comparing these results to the adaptive rank reduction experiments (see Figure \ref{fig12}), we can observe that a rank-100 configuration during meta-training, combined with adaptive rank reduction during meta-testing (e.g., 50\% reduction), outperforms models meta-trained and tested with a fixed rank of 50. This demonstrates the potential of adopting a flexible rank strategy: using an appropriate rank during meta-training and applying rank-adaptive reduction during meta-testing can not only enhance computational efficiency but also maintain competitive accuracy and convergence speed.
\end{itemize}




\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Figure/fig17.png}
\caption{Comparison of physical loss and accuracy curves between different rank sizes on layered model: rank = 10 (blue), 25 (orange), 50 (yellow), 100 (purple), and 200 (green). The rows correspond to different frequencies (top: 3 Hz, middle: 6 Hz, bottom: 12 Hz). For each frequency, the left column shows the physical loss curves, and the right column presents the accuracy curves (measured as MSE against the numerical reference).}
\label{fig17}
\end{figure}


\subsection{The effect of pruning vs. retaining FEH during meta-testing phase}
In the previous numerical examples section, when we perform the meta-testing phase, we choose to prune the FEH to further simplify the network and reduce computational overhead. To investigate the impact of this decision, we here compare the performance of retaining FEH and pruning FEH during the meta-testing phase. We also use the layered velocity model as a example and evaluate three frequencies (3 Hz, 6 Hz, and 12 Hz) by training separate networks for each configuration. 

Figure~\ref{fig18} illustrates the physical loss and accuracy curves for both strategies, where the rows from top to bottom correspond to frequencies 3, 6, and 12 Hz, and the left and right columns of each row represent the physical loss and accuracy curves, respectively. The physical loss curves suggest that pruning FEH generally achieves faster convergence compared to retaining FEH in the low- and mid-frequency cases (3 Hz and 6 Hz). Specifically, at both 3 and 6 Hz, the pruned FEH model converges much fast than the retained FEH model. At 12 Hz, the two approaches show similar convergence speeds. For the accuracy curves, at 3 Hz, the accuracy curves for the two strategies nearly overlap. At 6 Hz, pruning FEH achieves faster accuracy improvement in the early training stages, but the retained FEH model eventually achieves slightly higher accuracy in the later stages. At 12 Hz, pruning FEH outperforms the retained FEH model in terms of both convergence speed and final accuracy. 

Therefore, we can see that pruning FEH generally accelerates convergence, particularly in the early stages of training, making it a computationally efficient choice for meta-testing phase. While retaining FEH maintains the network’s full parameterization, the added complexity does not consistently result in better performance. Pruning FEH, on the other hand, reduces computational costs without significant sacrifices and, in some cases, even outperforms retaining FEH. As a result, we recommend directly pruning FEH during the meta-testing phase, as we did in our numerical examples.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Figure/fig18.png}
\caption{Comparison of physical loss and accuracy curves between pruning FEH (blue) and retaining FEH (orange) during the meta-testing phase for 3 Hz (top row), 6 Hz (middle row), and 12 Hz (bottom row). The left column displays physical loss curves, while the right column shows accuracy curves.}
\label{fig18}
\end{figure}


\subsection{The performance for out of distribution}
To evaluate the generalization capability of the meta-trained initialization provided by Meta-LRPINN, we test its performance on a frequency outside the range used during the meta-training phase. While the meta-training stage is conducted on frequencies within the range of 2–15 Hz, we select 18 Hz as an out-of-distribution frequency to examine the model's ability to adapt to unseen scenarios. Using the same layered velocity model as in previous experiments, we initialize the meta-testing phase with the meta-trained weights and assess the wavefield representation at 18 Hz. For comparison, we include two benchmarks: Meta-PINN and vanilla PINN. 

Figure~\ref{fig19} compares the performance of our Meta-LRPINN, Meta-PINN, and vanilla PINN, with panel (a) showing the physical loss curves and panel (b) displaying the accuracy curves. We can see that our Meta-LRPINN shows a clear advantage in reducing PDE loss right from the start, indicating that the meta-trained initialization enables the model to quickly establish an effective optimization direction even for a frequency not encountered during meta-training. In terms of accuracy, Meta-LRPINN initially improves rapidly within just a few epochs, but its accuracy subsequently declines. Meta-PINN, despite demonstrating a faster initial decline in PDE loss compared to Meta-LRPINN, ultimately fails to convert that into meaningful accuracy gains. Instead, its accuracy increases over time, implying that the model converges to a trivial solution rather than genuinely improving the wavefield representation. Meanwhile, vanilla PINN’s curves for both PDE loss and accuracy remain essentially flat, underscoring its difficulty in effectively modeling high-frequency wavefields without a tailored initialization and extended optimization effort. Overall, these observations highlight the capability of our Meta-LRPINN in improving adaptability and optimization efficiency at out-of-distribution frequencies.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Figure/fig19.png}
\caption{Test the performance of our Meta-LRPINN (blue) on an out-of-distribution frequency (18 Hz), where we compare it with Meta-PINN (orange) and vanilla PINN (yellow). (a) The physical loss curves. (b) The accuracy curves.}
\label{fig19}
\end{figure}

% \subsection{Model capacity, computational cost, and memory usage}

% \begin{table}[htbp]
%     \centering
%     \caption{Comparison of model capacity, computational cost, and memory usage for Meta-LRPINN (with different ranks), Meta-PINN, and Vanilla PINN. Computational cost is measured by the training time for 100 epochs. Memory usage represents the peak GPU memory required during training.}
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{Model} & \textbf{Number of parameters} & \textbf{Training time (100 epochs)} & \textbf{Memory usage (GB)} \\
%         \hline
%         Vanilla PINN/Meta-PINN & 618242 & 15.2 S & 12.5 \\
%         Meta-LRPINN Rank 10 & 40382 & 12.66 S & 8.5 \\
%         Meta-LRPINN Rank 25 & 98072 & 12.68 s & 9.0 \\
%         Meta-LRPINN Rank 50 & 194222 & 12.80 s & 10.0 \\
%         Meta-LRPINN Rank 100 & 386522 & 13.37 S & 11.5 \\
%         Meta-LRPINN Rank 200 & 771122 & 92.12 s & 13.0 \\
%         \hline
%     \end{tabular}
%     \label{tab:model_comparison}
% \end{table}
