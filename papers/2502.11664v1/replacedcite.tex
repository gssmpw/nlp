\section{Related Work}
\subsection{Video Large Language Models}
Recent advancements in Video-LLMs have significantly enhanced video processing by integrating multiple modalities and employing instruction fine-tuning. Notable innovations include Video-ChatGPT ____, which introduced video instruction tuning for text generation, and VideoChat ____ and VideoChat2 ____, which improved modality alignment via cross-attention and multi-stage bootstrapping etc. Other models, such as Chat-UniVi ____ and LLaMA-VID ____, focus on efficient video representations through techniques like token compression and dual-token methods that separate context and content. Additionally, PLLaVA ____ explores the use of image-pretrained LLaVA models for video tasks, utilizing simple spatial pooling techniques.

\subsection{Multimodal Position Embedding}
Most Video-LLMs inherit the default design from LLMs by using Rotary Position Embedding (RoPE) ____ for positional encoding. RoPE encodes relative distance information as absolute position embeddings, offering key advantages like no additional training parameters and improved performance in various tasks ____. It is widely used in modern LLMs due to its ability to extrapolate context length, extending a model's window size without the need for expensive retraining. However, RoPE's 1D design, effective for text, overlooks the spatiotemporal structure of video data, limiting its suitability for Video-LLMs. To address this, several approaches have adapted RoPE for video ____. For instance, RoPE-2D ____ extends the encoding to capture spatial relationships in video frames, while RoPE-3D ____ divides the channel dimension into three groups to better represent the spatiotemporal dimensions.

However, these approaches still face issues like Positional Attention Bias and Cross-Modal Positional Discontinuity, which are discussed in Section \ref{sec:motivation}. Our VRoPE method addresses these limitations, offering more accurate and robust positional encoding tailored for Video-LLMs.