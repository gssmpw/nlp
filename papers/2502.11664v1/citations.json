[
  {
    "index": 0,
    "papers": [
      {
        "key": "maaz2023video",
        "author": "Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz",
        "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2023videochat",
        "author": "Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu",
        "title": "Videochat: Chat-centric video understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2024mvbench",
        "author": "Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others",
        "title": "Mvbench: A comprehensive multi-modal video understanding benchmark"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jin2024chat",
        "author": "Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li",
        "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2024llama",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "Llama-vid: An image is worth 2 tokens in large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xu2024pllava",
        "author": "Xu, Lin and Zhao, Yilin and Zhou, Daquan and Lin, Zhijie and Ng, See Kiong and Feng, Jiashi",
        "title": "Pllava: Parameter-free llava extension from images to videos for video dense captioning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "su2024roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "su2024roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kexuefm-10040",
        "author": "Su, Jianlin",
        "title": "A Brief Reflection on Multimodal Position Embedding."
      },
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "agrawal2024pixtral",
        "author": "Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and De Monicault, Baudouin and Garg, Saurabh and Gervet, Theophile and others",
        "title": "Pixtral 12B"
      },
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  }
]