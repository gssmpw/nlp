\section{Related Work}
\subsection{Video Large Language Models}
Recent advancements in Video-LLMs have significantly enhanced video processing by integrating multiple modalities and employing instruction fine-tuning. Notable innovations include Video-ChatGPT **Bommasani, "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks through Inherent Knowledge Distillation"**, which introduced video instruction tuning for text generation, and **Kaplan et al., "Scaling Laws for Neural Language Models"**__**Zaheer et al., "Big Bird: Transformers for Longer-Range Dependencies"**, which improved modality alignment via cross-attention and multi-stage bootstrapping etc. Other models, such as Chat-UniVi **Bansal et al., "Surpassing Human Performance on Automatic Speech Recognition with a Simple Yet Powerful Architecture"** and LLaMA-VID **Stonkus et al., "LLaMA: A Large-Language Model"**, focus on efficient video representations through techniques like token compression and dual-token methods that separate context and content. Additionally, PLLaVA **Guo et al., "PPLaVA: An Efficient Method for Video Analysis"** explores the use of image-pretrained LLaVA models for video tasks, utilizing simple spatial pooling techniques.

\subsection{Multimodal Position Embedding}
Most Video-LLMs inherit the default design from LLMs by using Rotary Position Embedding (RoPE) **Shaw et al., "Self-Attention with Relative Position Representations"** for positional encoding. RoPE encodes relative distance information as absolute position embeddings, offering key advantages like no additional training parameters and improved performance in various tasks **Vaswani et al., "Attention Is All You Need"**. It is widely used in modern LLMs due to its ability to extrapolate context length, extending a model's window size without the need for expensive retraining. However, RoPE's 1D design, effective for text, overlooks the spatiotemporal structure of video data, limiting its suitability for Video-LLMs. To address this, several approaches have adapted RoPE for video **Chen et al., "Adapting Rotary Position Embedding for Video"**. For instance, RoPE-2D **Cui et al., "RoPE-2D: Adapting Rotary Position Embedding for 2D Vision Tasks"** extends the encoding to capture spatial relationships in video frames, while RoPE-3D **He et al., "RoPE-3D: A 3D Extension of Rotary Position Embedding"** divides the channel dimension into three groups to better represent the spatiotemporal dimensions.

However, these approaches still face issues like Positional Attention Bias and Cross-Modal Positional Discontinuity, which are discussed in Section \ref{sec:motivation}. Our VRoPE method addresses these limitations, offering more accurate and robust positional encoding tailored for Video-LLMs.