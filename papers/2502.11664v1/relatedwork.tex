\section{Related Work}
\subsection{Video Large Language Models}
Recent advancements in Video-LLMs have significantly enhanced video processing by integrating multiple modalities and employing instruction fine-tuning. Notable innovations include Video-ChatGPT \cite{maaz2023video}, which introduced video instruction tuning for text generation, and VideoChat \cite{li2023videochat} and VideoChat2 \cite{li2024mvbench}, which improved modality alignment via cross-attention and multi-stage bootstrapping etc. Other models, such as Chat-UniVi \cite{jin2024chat} and LLaMA-VID \cite{li2024llama}, focus on efficient video representations through techniques like token compression and dual-token methods that separate context and content. Additionally, PLLaVA \cite{xu2024pllava} explores the use of image-pretrained LLaVA models for video tasks, utilizing simple spatial pooling techniques.

\subsection{Multimodal Position Embedding}
Most Video-LLMs inherit the default design from LLMs by using Rotary Position Embedding (RoPE) \cite{su2024roformer} for positional encoding. RoPE encodes relative distance information as absolute position embeddings, offering key advantages like no additional training parameters and improved performance in various tasks \cite{su2024roformer}. It is widely used in modern LLMs due to its ability to extrapolate context length, extending a model's window size without the need for expensive retraining. However, RoPE's 1D design, effective for text, overlooks the spatiotemporal structure of video data, limiting its suitability for Video-LLMs. To address this, several approaches have adapted RoPE for video \cite{kexuefm-10040, wang2024qwen2}. For instance, RoPE-2D \cite{agrawal2024pixtral, wang2024qwen2} extends the encoding to capture spatial relationships in video frames, while RoPE-3D \cite{wang2024qwen2} divides the channel dimension into three groups to better represent the spatiotemporal dimensions.

However, these approaches still face issues like Positional Attention Bias and Cross-Modal Positional Discontinuity, which are discussed in Section \ref{sec:motivation}. Our VRoPE method addresses these limitations, offering more accurate and robust positional encoding tailored for Video-LLMs.