
\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\appendix
\input{sec/figs/user_interface}


\section{Human Annotation}

Prior to the final manual annotation, we undertook several steps to ensure the quantity, quality, and efficiency of the annotation process. In building the dataset, we focused on the diversity and complexity of prompts, creating annotated data across multiple dimensions and categories. We meticulously selected candidates for the annotation task and provided them with in-depth training sessions. We then designed a user-friendly interface and a comprehensive annotation program to streamline the process. Additionally, we developed a detailed annotation guide that covers all aspects of the task and highlights essential precautions. To further ensure consistency and accuracy among annotators, we conducted multiple rounds of annotation and implemented random sampling quality checks.
\subsection{Annotator selection}
The effectiveness of annotated data hinges significantly on the capabilities of the individuals responsible for the annotation. As such, at the onset of the annotation initiative, we placed a strong emphasis on meticulously selecting and training a team of annotators to ensure their competence and objectivity. This process entailed administering a comprehensive evaluation to potential candidates, which was designed to gauge their proficiency in ten critical domains: domain expertise, tolerance for visually disturbing content, attention to detail, communication abilities, reliability, cultural and linguistic competence, technical skills, ethical awareness, aesthetic discernment, and motivation.

Given that the models under assessment could generate videos containing potentially distressing or inappropriate material, candidates were informed of this possibility in advance. Their participation in the evaluation was predicated on their acknowledgment of this condition, and they were assured of their right to withdraw at any time.
Based on the results of the evaluation and the candidates' professional backgrounds, we sought to form a team that was diverse and well-rounded in terms of expertise and capabilities across the ten assessed areas. Ultimately, we selected a group of 10 annotators—comprising five men and five women, all of whom possessed bachelor's degrees. We conducted follow-up interviews with the selected annotators to reaffirm their suitability for the annotation task.
\subsection{Annotation Guideline}
When it comes to assessing human preferences in videos, the evaluation process is structured around three fundamental aspects: \textbf{semantic consistency}, \textbf{motion smoothness}, and \textbf{video faithfulness}. Each of these dimensions is meticulously examined independently to provide a comprehensive understanding of the video's overall quality. \textbf{Semantic consistency} ensures that the video's content aligns logically and contextually, while \textbf{motion smoothness} evaluates the smoothness and naturalness of movements within the video. \textbf{Video faithfulness}, on the other hand, assesses how well the video maintains a realistic and high-quality appearance. By breaking down the evaluation into these distinct yet interconnected aspects, we can achieve a more nuanced and accurate assessment of human preferences in video content.

\begin{itemize}
    \item \textbf{Semantic Consistency} The consistency between the text and the video refers to whether elements such as the topic category (e.g., people or animals), quantity, color, scene description, and style mentioned in the text align with what is displayed in the video.
    
    \item \textbf{Motion Smoothness} The motion smoothness refers to whether the movements depicted are smooth, coherent, and logically consistent, particularly in terms of their adherence to physical laws and real-world dynamics. This includes evaluating if the actions unfold in a seamless and believable manner, without abrupt or unrealistic transitions, and whether they align with the expected behavior of objects or individuals in the given context.
    
    \item \textbf{Video Faithfulness} Faithfulness refers to the accuracy and realism of the visual content in the video. It examines whether depictions of people, animals, or objects closely match their real-world counterparts, without unrealistic distortions like severed limbs, deformed features, or other anomalies. High fidelity ensures the visual elements are believable and consistent with reality, enhancing the video's authenticity..
\end{itemize}
\paragraph{Pointwise Annotation} For each dimension, annotators assign one of three ratings:
\begin{itemize}
    \item \textbf{Good}: No observable flaws; fully satisfies the criterion.
    \item \textbf{Normal}: Minor imperfections that do not severely impact perception.
    \item \textbf{Bad}: Significant violations that degrade quality or realism.
\end{itemize}
Annotations are performed independently across dimensions to avoid bias.
\paragraph{Pairwise Annotation}
Given two videos $\mathcal{V}_A$ and $\mathcal{V}_B$, annotators compare them under each dimension:
\begin{itemize}
    \item For \textbf{semantic consistency}, determine which video better aligns with the intended context.
    \item For \textbf{motion smoothness}, identify the video with more natural and continuous motion.
    \item For \textbf{video faithfulness}, select the video exhibiting higher realism and fewer artifacts.
\end{itemize}
A ranked preference ($\mathcal{V}_A \succ \mathcal{V}_B$ or $\mathcal{V}_B \succ \mathcal{V}_A$) is recorded separately for each dimension. Ties are permitted only if differences are indistinguishable.



\subsection{Annotation training and interface}
We convened a comprehensive training session centered on our detailed user guidelines to ensure that the annotation team fully comprehends our goals and standards. During the training, we covered the purpose, precautions, standards, workload, and remuneration related to the annotation process. Additionally, we formally communicated to the annotators that the human annotation is strictly for research purposes and that the annotated data might be made publicly available in the future. We reached a mutual agreement with the annotators regarding the standards, workload, remuneration, and intended use of the annotated data. The criteria for assessing video faithfulness, text-video alignment, and motion smoothness are universal, which means that individual standards should not vary significantly. Consequently, we annotated several sample videos using our carefully designed annotation platform to ensure consistency among annotators. An overview of the developed annotation platform is shown in~\cref{fig:interface}. Through this training, we also equipped the annotators with the necessary knowledge to conduct impartial and detailed human evaluations of video faithfulness, text-video alignment, and motion smoothness.

\input{sec/tables/vbench_compare_all_appendix}
\section{Additional Quantitative Analysis}
\subsection{All evaluation dimension results}
We present the evaluation metrics of VBench benchmark in 16 different dimensions in the Tab.~\ref{tab:vbench_compare_all}. The results indicate that the performance shows a continuous upward trend in multiple training iterations, indicating that the training is dynamically stable. Although small fluctuations are observed, they can be ignored and will not affect the overall positive progress of the indicator.

\input{sec/tables/critic_model_acc}

\subsection{Impact of Critic Model Scale on Accuracy}
To evaluate the impact of model scale on critic performance, we experimented with two ViLA-based critic models of 13B and 40B parameters. Fine-tuned on paired and pointwise annotated datasets, these models were assessed on validation tasks, as shown in Tab.~\ref{results_critic}.  

Results show that increasing the model size from 13B to 40B significantly improves accuracy, with the 40B model achieving 80.73\% in paired ranking and 86.57\% in pointwise scoring—2.92\% and 2.62\% higher than the 13B model. This suggests that larger critic models better capture human preferences by representing complex multimodal relationships.  

Our ablation study further highlights the importance of model scale. While smaller models perform reasonably well, larger models significantly boost accuracy, benefiting downstream reinforcement learning tasks. These findings suggest that scaling the critic model is a simple yet effective way to enhance preference alignment and improve video generation quality.

\input{sec/tables/reward_modelsize}
\subsection{Impact of Critic Model Size on RL}
We conducted a comparative analysis of critic models of different sizes in the iterative preference optimization training process. As shown in Tab.~\ref{results_critic_size}, larger critic models provide more precise reward estimations, leading to improved alignment with human preferences. This enhanced reward accuracy contributes to better optimization during preference alignment training, ultimately resulting in superior model performance. Our findings highlight the importance of scaling the critic model to achieve more reliable preference optimization and reinforce the benefits of leveraging larger critic architectures for improved alignment outcomes.

\section{Additional Qualitative Comparison}
We provide a more detailed qualitative comparison in Fig. \ref{fig:appendix_case}, which demonstrates the superiority of our model over the baseline. This visualization highlights key improvements in fidelity and overall quality.By comparing outputs side by side, we illustrate the advantages of our model in handling complex scenarios, reinforcing its effectiveness in real-world applications.
\section{Societal Impacts}
The emergence of advanced text-to-video generation pipelines and optimized training strategies marks a major breakthrough in AI-driven content creation. These innovations significantly lower barriers to video production, making it more efficient, cost-effective, and widely accessible. Industries such as education, entertainment, marketing, and accessibility technology stand to benefit immensely, as creators can generate high-quality videos with minimal effort and resources.

However, alongside these advancements come ethical concerns, particularly regarding potential misuse. One of the most pressing risks is the creation of highly realistic yet misleading content, such as deepfakes, which could be exploited for misinformation, reputational harm, or social manipulation. Additionally, automated video curation may unintentionally reinforce biases present in training datasets, leading to the perpetuation of stereotypes or discriminatory narratives.

On a larger scale, the widespread adoption of AI-driven video generation could disrupt traditional media industries, impacting professions like video editing, animation, and scriptwriting. The proliferation of hyper-realistic AI-generated content also raises concerns about media authenticity, making it increasingly difficult for audiences to distinguish between real and synthetic footage. Furthermore, if access to these technologies remains limited to well-funded organizations, existing digital divides could widen, leaving smaller creators at a disadvantage.

To mitigate these risks, proactive measures must be taken. Implementing transparency features—such as metadata labels to distinguish AI-generated content—can enhance accountability. Ensuring diverse and ethically curated datasets is crucial to reducing bias and promoting fairness in outputs. Policymakers, industry leaders, and researchers must work together to develop regulatory frameworks that balance innovation with ethical responsibility. Additionally, providing educational resources and affordable access to AI tools can help democratize their benefits, preventing technological disparities.

The rapid evolution of generative AI necessitates ongoing ethical oversight. Continuous evaluation of its societal impact, coupled with adaptive safeguards, is essential to minimizing harm while maximizing positive outcomes. Addressing these challenges requires a multidisciplinary approach, fostering a responsible AI ecosystem that empowers creators without compromising ethical integrity.  

\section{Limitations}

While our approach represents a significant advancement in text-to-video generation, several challenges persist. Addressing these issues presents valuable opportunities for future research and development. Below, we outline key limitations and potential solutions:  

\paragraph{Computational Constraints}
Despite improvements in training efficiency, large-scale text-to-video models still demand substantial computational power. This limitation may hinder accessibility, particularly for individuals and organizations with limited resources. Future research should focus on developing lightweight architectures, model compression techniques, and efficient inference strategies to enhance accessibility in resource-constrained environments.  

\paragraph{Dataset Representativeness and Cultural Adaptability} 
The effectiveness of our model is highly dependent on the quality and diversity of its training dataset. While significant efforts were made to curate a representative dataset, certain cultural contexts, niche topics, and underrepresented communities may not be adequately covered. Additionally, biases present in pretraining data could lead to unintended stereotypes or inappropriate outputs. Expanding dataset coverage, improving curation techniques, and integrating fairness-aware training strategies will be crucial to enhancing contextual accuracy and mitigating bias.  

\paragraph{Multimodal Hallucination and Interpretability}
Since our evaluation models are fine-tuned multimodal large language models (MLLMs), they are susceptible to hallucination—generating content that appears plausible but is factually incorrect or cannot be inferred from the input text and images. Furthermore, MLLMs inherit biases from their pretraining data, which may lead to inappropriate responses. While careful curation of supervised fine-tuning (SFT) data helps alleviate these issues, they are not fully resolved. In addition to these concerns, MLLMs also suffer from opacity, interpretability challenges, and sensitivity to input formatting, making it difficult to understand and control their decision-making processes. Enhancing model explainability and robustness remains a crucial area for future research.  

\paragraph{Human Annotation Limitations} 
Human evaluation plays a critical role in assessing model performance, but it is inherently subjective and influenced by annotator biases, perspectives, and inconsistencies. Errors or noisy labels may arise during annotation, potentially affecting evaluation reliability. To address this, we conducted multiple rounds of trial annotations and random sampling quality inspections to improve inter-annotator consistency. Additionally, we designed user-friendly annotation guidelines and interfaces to streamline the process and enhance accuracy. However, variations in annotation methodologies across different platforms and annotators can still lead to discrepancies, limiting the generalizability of our results. Moreover, human annotation remains time-consuming and resource-intensive, constraining scalability. Future efforts could explore semi-automated evaluation approaches or crowdsourced methods to improve efficiency while maintaining quality.  

\paragraph{Ethical Considerations and Regulatory Compliance}
Ensuring responsible AI deployment remains an ongoing challenge, even with safeguards in place to prevent misuse. The rapid advancement of AI-generated media demands continuous revisions to ethical policies and regulatory frameworks. Proactively engaging with policymakers, industry leaders, and researchers is crucial for developing guidelines that uphold accountability, transparency, and fair usage. Embedding metadata or digital watermarks in AI-generated content can improve traceability and help differentiate synthetic media from authentic sources. Furthermore, raising public awareness and education on AI-generated content help users recognize potential risks. Developers should prioritize responsible AI innovation by ensuring algorithmic fairness while minimizing bias. Additionally, fostering cross-industry collaboration and promoting global standards will contribute to a more secure and trustworthy AI ecosystem.
\input{sec/figs/appendix_case}