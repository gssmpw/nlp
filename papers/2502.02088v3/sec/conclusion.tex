
\section{Conclusion}
In this paper, we present a novel Iterative Preference Optimization framework for improvement video foundation models from the pespective of post training.
%This paper presents Iterative Preference Optimization (IPO), a novel post-training framework for text-to-video (T2V) generation. 
In particular, IPO collects a human preference dataset and trains a critic model for automatically labeling the generated videos from base model. Given this, IPO efficiently optimizes the base model in an iterative manner. By this way, IPO can effectively align the base model with human preference, leading to enhanced video results in motion smoothness, subject consistency and aesthetic quality, etc. In addition, we adapt IPO with different optimization strategies: DPO and KTO, both achieve impressive improvements. This further verifies the flexibility of IPO. Thorough experiments on VBench demonstrates the efficacy of IPO. 

%IPO iteratively enhances models using preference datasets, criticism models, and iterative learning, improving subject consistency, motion smoothness, and aesthetic quality. As the first to apply iterative preference optimization to T2V, IPO leverages human feedback to refine video quality, making it more aligned with user expectations. Experiments show that IPO enables a 2B-parameter model to outperform a 5B-parameter model on VBench.