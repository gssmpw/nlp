\section{METHOD}
\label{sec:method} 

The proposed Iterative Preference Optimization framework consists of three major parts: (1) A human preference dataset for training critic model; (2) A critic model for automatically labeling generated videos; (3) An iterative optimization process for enhancing the base text-to-video models. Fig.~\ref{fig:method_pipeline} illustrates the whole pipeline of IPO. We will explain details of each part in the following sections. 

%Our approach consists of three main steps: (1) \textbf{Human Preference Annotation}, where we collect human feedback on video-text pairs using pairwise ranking and pointwise scoring; (2) \textbf{Critic Model Training}, which uses the annotated data to train a VLM-based reward model that predicts human preferences; and (3) \textbf{T2V Model Alignment}, where we iteratively align the T2V model based on the critic model. Our alignment method is flexible, supporting both pointwise (KTO) and pairwise (DPO) preference alignment. Iterative preference optimization offers key advantages over offline learning, allowing the integration of offline algorithms into an iterative framework, resulting in significant performance gains. Specifically, we explore two alignment methods: \textbf{KTO}, which uses pointwise scoring, and \textbf{DPO}, which employs pairwise ranking. Figure \ref{fig:method_pipeline} illustrates this pipeline, where each step refines the model iteratively based on human feedback.

% Our alignment method is versatile and general-purpose, supporting both pointwise (KTO) and pairwise (DPO) preference alignment. Iterative preference optimization offers significant advantages over offline learning methods, enabling the integration of various offline algorithms into an iterative framework, leading to substantial performance improvements. Specifically, we explore two distinct alignment methods: the \textbf{KTO algorithm}, which uses pointwise scoring, and the \textbf{DPO algorithm}, which leverages pairwise ranking. Figure \ref{fig:method_pipeline} illustrates this pipeline, where each step refines the model iteratively based on human feedback.

\subsection{Human Preference Dataset}
% \subsection{Datasets Annotation Pipeline} 
\label{collect dataset}

\paragraph{Video-text Collection} 
% Our annotated dataset is constructed using the open-source model CogVideoX-2B \cite{yang2024cogvideox}. The process begins by creating a diverse set of prompts for video generation, covering various character types, actions, scenes (both realistic and abstract), animals, plants, vehicles, and video styles. These elements are randomly combined and refined into coherent prompts using the Qwen2.5 \cite{qwen2.5} large language model. To ensure diversity and avoid the generation of repetitive sentences, we fine-tune the model's output by adjusting key parameters such as temperature, top-k, and top-p. Using these prompts, the video generation model produces three distinct variants for each video by applying different random seeds. Throughout the training process, we iteratively regenerate data using an optimized strategy to progressively expand and enhance our preference-aligned dataset.
% Our annotated dataset is constructed using the open-source model CogVideoX-2B \cite{yang2024cogvideox}. We first generate diverse prompts covering various character types, actions, scenes, animals, plants, vehicles, and video styles. These prompts are randomly combined and refined using the Qwen2.5 \cite{qwen2.5} model, with temperature, top-k, and top-p adjustments to enhance diversity. As shown in Figure.~\ref{fig:dataset_statis}, our dataset includes more than ten distinct categories, such as Human, Architecture, Vehicles, Animals, and others, as well as over ten different actions performed by humans and animals.
% .For each prompt, the video generation model produces four variants with different random seeds. The dataset is iteratively expanded through an optimized regeneration strategy to improve preference alignment.
The generated video dataset is created using the CogVideoX-2B model \cite{yang2024cogvideox}. We generate diverse prompts covering various character types, which are then combined and refined using the Qwen2.5~\cite{qwen2.5}. To increase diversity, we adjust parameters like temperature, top-k, and top-p. As shown in Fig.~\ref{fig:dataset_statis}, the dataset includes over ten categories, such as Human, Architecture, Vehicles, and Animals, as well as more than ten different actions of human and animal. For each prompt, the video generation model produces four variants with different random seeds. The dataset is iteratively expanded through an optimized regeneration strategy to improve preference alignment.



\vspace{0.5\baselineskip}
\noindent\textbf{Human Annotation}  
When aligning human feedback, we focus on three dimensions, inspired by the text-to-image evaluation work Evalalign~\cite{tan2024evalalign}: consistency between text and video, video faithfulness, and additional motion smoothness. To capture human preferences, we employ two annotation methods: \textbf{Pairwise Ranking} and \textbf{Pointwise Scoring}. 
%Figure.~\ref{fig:dataset_statis} shows the distribution statistics of two different annotation results.
For pairwise ranking, annotators are shown two videos generated from the same prompt and asked to evaluate them based on predefined criteria. They are required to: (1) provide detailed natural language feedback, and (2) identify specific quality issues in each video. The annotated comparisons are then organized to create training samples.
{\small
\[
\left\{
\begin{array}{l}
    \texttt{<Video1>, <Video2>, Question,} \\
    \texttt{Evaluation Dimension,} \\
    {\color{blue} \text{Ranking~Answer}}, {\color{blue} \text{Reason}}
\end{array}
\right\}
\]
}
% \[
% \left\{
% \begin{array}{l}
%     \texttt{<Video1>, <Video2>, Question,} \\
%     \texttt{Evaluation Dimension,} \\
%     {\color{blue} \text{Ranking~Answer}}, {\color{blue} \text{Reason}}
% \end{array}
% \right\}
% \]

For pointwise scoring, each video is assessed across three dimensions, with scores categorized into three levels: \textbf{Good}, \textbf{Normal}, and \textbf{Poor}. Similar to the ranking process, annotators provide detailed justifications for their scores to ensure transparency and consistency. The annotated evaluations are then organized to create training samples.
{\small
\[
\left\{
\begin{array}{l}
    \texttt{<Video>, Question,} \\
    \texttt{Evaluation Dimension,} \\
    {\color{blue} \text{Scoring~Answer}}, {\color{blue} \text{Reason}}
\end{array}
\right\}
\]
}

Each dataset is annotated by at least three individuals. The final annotation is determined through voting, selecting the one with the highest consistency among reviewers.


\subsection{Critic Model Training}

% Large Visual-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. These models, which are pre-trained on vast amounts of multimodal data, possess a strong ability to capture complex relationships between textual and visual information. This makes them particularly well-suited for tasks like reward modeling, where the model must align its understanding of multimodal content with human preferences. By leveraging VLMs as reward models, we can exploit their ability to generalize across different types of inputs and improve the quality of video generation through more accurate preference predictions.
Large Visual-Language Models (VLMs) exhibit strong generalization across diverse tasks due to pre-training on extensive multimodal data. Their capability to capture complex text-image relationships makes them well-suited for critic modeling, where alignment with human preferences is crucial. Leveraging VLMs as critic models enhances video generation by improving preference predictions through their robust generalization ability.


% Our approach to reward modeling is versatile, supporting both \textit{pointwise} and \textit{pairwise} methods for preference alignment. The pointwise reward model computes a score for each individual video-text pair, evaluating the quality of the generated video in isolation. This method is trained using the autoregressive loss function, as explained below. On the other hand, the pairwise reward model compares two generated videos and determines which one better aligns with the given text, offering a different but complementary way of assessing video generation quality.
% Our critic modeling approach supports both \textit{pointwise} and \textit{pairwise} methods for preference alignment. The pointwise model assigns a quality score to each video-text pair independently, trained using an autoregressive loss. In contrast, the pairwise model compares two generated videos to determine which better aligns with the text, providing a complementary evaluation of video quality.


Fig. \ref{fig:method_pipeline} illustrates our supervised fine-tuning process. The training starts by passing video frames through an image encoder to extract image tokens. These tokens, together with the textual caption, question, and answer, are then fed into a pre-trained Visual-Language Model (VLM). Formally, each training sample consists of a triplet: \textbf{multimodal input} $M$, which includes both the video and its corresponding textual description; \textbf{question} $Q$; and \textbf{answer} $A$.

The optimization objective is derived from the autoregressive loss function commonly used in LLM training, but applied specifically to the answer as:
%. The loss function is formulated as:
\vspace{-8pt}
\small
\begin{align}
    L(\phi) = -\sum^N_{i=1} \log p(A_i \mid Q, M, A_{<i}; \phi),
\end{align}
\normalsize
%\vspace{-8pt}
where $N$ is the length of the ground-truth answer. This loss encourages the model to get answers that align with provided questions and multimodal context.
% Depending on the preference alignment method, the model receives different inputs. In the \textbf{pointwise} case, the input consists of a single video-text pair, where the model evaluates the quality of one generated video based on the given question and answer. In the \textbf{pairwise} case, the input consists of two video-text pairs, and the model is tasked with comparing the two videos to determine which one better aligns with the textual context. The optimization process differs accordingly, with the pointwise model optimizing for individual video quality and the pairwise model focusing on relative comparison.
Depending on the method, the model receives different inputs. In the \textbf{pointwise} case, it evaluates a single video-text pair. In the \textbf{pairwise} case, it compares two pairs to determine which aligns better with the text. The optimization focuses on individual quality in pointwise and relative comparison in pairwise.

\subsection{T2V Alignment}

\subsubsection{Alignment Optimization}
%1. RLHF
In Reinforcement Learning with Human Feedback (RLHF), the primary objective is to optimize the model to maximize the expected reward given context \( c \). This is achieved by incorporating a regularization term that penalizes deviations from a reference distribution, ensuring the optimized model remains consistent with prior knowledge or previously trained models. The optimization objective is as:
\small
\begin{multline}
    \max_{\pi_\theta} \mathbb{E}_{x_0 \sim \pi_\theta(x_0|c)} \left[ r(x_0, c)
    - \beta \log \frac{\pi_\theta(x_0|c)}{\pi_{ref}(x_0|c)} \right] ,
    \label{eq:objective}
\end{multline}
\normalsize
where \( r(x_0, c) \) denotes the reward associated with a state \( x_0 \) given context \( c \), \( \pi_\theta(x_0|c) \) is the policy of the model parameterized by \( \theta \), and \( \pi_{ref}(x_0|c) \) is the reference policy. The hyperparameter \( \beta \) controls the strength of the regularization, typically set as a fixed constant. This objective function aims to balance reward maximization with regularization, which prevents the model from straying too far from the reference distribution, ensuring stability and generalization~\cite{zhang_2023_ltbook}. 
%Further elaboration on this concept is presented in~\cite{zhang_2023_ltbook}, 
And the loss function for the policy \( \pi_\theta(\cdot|c) \) is formulated in terms of the expected value over the reward and the KL-divergence penalty as following:
%. The loss functional is expressed as:
\vspace{-4pt}
\small
\begin{multline}
\mathbb{E}_{x_0 \sim \pi_\theta(x_0|c)} \Big[ -r(x_0, c) - \beta \log \frac{\pi_{ref}(x_0|c)}{\pi_\theta(x_0|c)} \Big] = \\
\beta \mathbb{KL} \left( \pi_\theta(x_0|c) \Big\Vert \pi_{ref}(x_0|c) \exp \left( \frac{1}{\beta} r(x_0, c) \right) \right),
\end{multline}
\normalsize
%Minimizing this loss functional leads to the optimal policy \( \pi^*(x_0|c) \), which can be derived as:
Minimizing this loss leads to optimal policy \( \pi^*(x_0|c) \) as:
\vspace{-4pt}
\small
\begin{equation}
\pi^*(x_0|c) \propto \pi_0(x_0|c) \exp \left( \frac{1}{\beta} r(x_0, c) \right),
\end{equation}
\normalsize
This aligns with Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} that translates the optimization goal into training the optimal policy \( \pi^*(x_0|c) \). The key advantage of this formulation is that it incorporates both reward maximization and regularization into a unified framework, thereby enabling robust learning.
%and improved generalization across diverse environments.
%2.real data joint training to sovle problem that XXX. 
To improve the model’s alignment with human feedback and output quality, we incorporate real-world data into the alignment training process. By using authentic, human-generated examples, the model learns to reduce hallucinations and better align with human preferences. This also enhances the model's robustness and generalization, preventing training divergence and ensuring reliable performance in diverse real-world tasks.
%TODO: Equation 

\subsubsection{Pointwise Optimization}
% 1.what reward 
% 2.pointwise objective

% For pointwise preference alignment, we adopt Diffusion-KTO \cite{ethayarajh2024ktomodelalignmentprospect}, which is based on prospect theory and extends the Kahneman-Tversky Optimization (KTO) framework. Unlike traditional preference-based methods, KTO introduces \textit{human-aware loss functions}, optimizing a nonlinear utility function rather than directly maximizing expected rewards. A key advantage of KTO is that it only requires binary-labeled data, avoiding the need for explicit preference pairs, and is particularly sensitive to negative samples.

% In Diffusion-KTO, the utility of an action is computed relative to a reference point $Q_{\text{ref}}$, defined as the Kullback-Leibler (KL) divergence between the learned policy $\pi_{\theta}$ and the reference policy $\pi_{\text{ref}}$. This enables the model to adjust its behavior based on deviations from a predefined baseline. The optimization objective for Diffusion-KTO is:
% \small
% \begin{multline}
%     L_{\text{kto}}(\theta)
%     = \underset{\pi_{\theta}}{\text{max}}\mathbb{E}_{x_0 \sim \mathcal{D},t\sim \text{Uniform([0,T])} 
%     } \\
%     [U(w(x_0)(\beta \log \frac{\pi_\theta(x_{t-1}|x_t)}{\pi_{\text{ref}}(x_{t-1}|x_t)}-Q_{\text{ref}}))],
%     \label{eq:dkto_loss}
% \end{multline}
% \normalsize
% where $w(x_0) = \pm1$ determines whether a state $x_0$ is considered desirable or undesirable. The reference point $Q_{\text{ref}}$ is defined as:
% \small
% \begin{equation}
% Q_{\text{ref}} = \beta \mathbb{D}_{\text{KL}}[\pi_{\theta}(a|s) ||\pi_{\text{ref}}(a|s)],
% \end{equation}
% \normalsize
% which measures the KL divergence between the learned policy and the reference policy. By leveraging this adaptive reference mechanism, Diffusion-KTO effectively calibrates the policy toward human-aligned decision-making.

% % To optimize the policy, we use the loss function defined in Eq.~\ref{eq:dkto_loss}. However, one of the challenges in KTO training is estimating the average reward $Q$. When the batch size is small, the Q-value estimation becomes unreliable, leading to unstable training dynamics. We address this issue by setting the batch size to 128, which provides a more stable estimation of $Q$ and results in improved performance.

% % Furthermore, to enhance stability and improve the generalization ability of the model, we incorporate real-world data from the VidGen-1M dataset~\cite{tan2024vidgen1mlargescaledatasettexttovideo} into the training process. Specifically, we introduce a negative log-likelihood (NLL) loss term on real samples, which regularizes the model and prevents divergence from natural video distributions. The final training objective is expressed as:
% To optimize the policy, we use the loss function from Eq.~\ref{eq:dkto_loss}. A key challenge in KTO training is estimating the average reward $Q$, which becomes unreliable with small batch sizes, leading to unstable training. To mitigate this, we set the batch size to 128, ensuring more stable $Q$ estimation and better performance.

% Additionally, we incorporate real-world data from the VidGen-1M dataset~\cite{tan2024vidgen1mlargescaledatasettexttovideo} to improve stability and generalization. We introduce a negative log-likelihood (NLL) loss term on real samples to regularize the model and prevent divergence from natural video distributions. The final training objective is:
% \small
% \begin{equation}
% \begin{aligned}
% \mathcal{L}(\theta) &= L_{\text{kto}}(\theta)+ \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}} \bigl[-\log p_{\theta}(y|x)\bigr],
% \end{aligned}
% \end{equation}
% \normalsize
% where $\lambda$ controls the contribution of real data regularization. This hybrid training approach ensures that the model maintains a balance between maximizing its utility function and adhering to real-world video distributions, ultimately leading to improved alignment with human preferences.

For pointwise preference alignment, we adopt Diffusion-KTO~\cite{ethayarajh2024ktomodelalignmentprospect}.
%, which extends the Kahneman-Tversky Optimization (KTO) framework based on prospect theory. 
Unlike traditional preference-based methods, KTO optimizes a nonlinear utility function using binary-labeled data, making it particularly sensitive to negative samples. This approach avoids the need for explicit preference pairs and introduces human-aware loss functions.

In Diffusion-KTO, the utility of an action is computed relative to a reference point $Q_{\text{ref}}$, defined as the 
%Kullback-Leibler (KL) 
KL divergence between the learned policy $\pi_{\theta}$ and the reference policy $\pi_{\text{ref}}$. The optimization objective is:
\small
\begin{multline}
    L_{\text{kto}}(\theta) = \underset{\pi_{\theta}}{\text{max}} \mathbb{E}_{x_0 \sim \mathcal{D}, t \sim \text{Uniform([0,T])}} \\
    [U(w(x_0)(\beta \log \frac{\pi_\theta(x_{t-1}|x_t)}{\pi_{\text{ref}}(x_{t-1}|x_t)} - Q_{\text{ref}}))],
    \label{eq:dkto_loss}
\end{multline}
\normalsize
where $w(x_0) = \pm1$ indicates whether a state $x_0$ is desirable or undesirable. The reference point $Q_{\text{ref}}$ is defined as:
\small
\begin{equation}
Q_{\text{ref}} = \beta \mathbb{D}_{\text{KL}}[\pi_{\theta}(a|s) || \pi_{\text{ref}}(a|s)].
\end{equation}
\normalsize
%which measures the KL divergence between the learned policy and the reference policy. 
This adaptive reference mechanism allows Diffusion-KTO to calibrate the policy toward human-aligned decision-making.
To optimize the policy, we use the loss function from Eq.~\ref{eq:dkto_loss}. A challenge in KTO training is estimating the average reward $Q$, which becomes unstable with small batch sizes. To mitigate this, we set the batch size as 128 for stable $Q$ estimation.
To further improve stability and generalization, we incorporate real-world data from the VidGen-1M dataset~\cite{tan2024vidgen1mlargescaledatasettexttovideo}. We add a negative log-likelihood (NLL) loss term on real samples to regularize the model and prevent divergence from natural video distributions. The final training objective is:
\small
\begin{equation}
\mathcal{L}(\theta) = L_{\text{kto}}(\theta) + \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}} \bigl[-\log p_{\theta}(y|x)\bigr],
\end{equation}
\normalsize
where $\lambda$ controls the weight of real data regularization. This hybrid approach balances the model’s utility maximization with adherence to real-world video distributions, enhancing its alignment with human preferences.



\subsubsection{Pairwise Optimization}
% 1.what reward 
% 2.pairwise objective
For pairwise preference alignment, we adopt Diffusion-DPO \cite{wallace2023diffusionmodelalignmentusing}, an extension of Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage} to diffusion models. Diffusion-DPO utilizes inverse decomposition to replace the standard policy $\pi_\theta$ and reference policy $\pi_{\text{ref}}$, enabling efficient preference optimization within the diffusion framework.

In Diffusion-DPO, the forward process $q(x_{1:T}|x_0)$ approximates the reverse process $p_\theta(x_{1:T}|x_0)$, redefining data likelihood in diffusion models. This leads to a loss function that facilitates robust preference optimization. The objective function for Diffusion-DPO is  as:
\small
\begin{multline}
    L_{\text{dpo}}(\theta) = -\mathbb{E}_{
    (x^w_0, x^l_0) \sim D, t \sim \mathcal{U}(0,T), x^w_{t} \sim q(x^w_{t}|x^w_0), x^l_{t} \sim q(x^l_{t}|x^l_0)
    } \\
    \log\sigma \left( -\beta T \omega(\lambda_t) \left( \| \epsilon^w - \epsilon_\theta(x_{t}^w,t)\|^2_2 - \|\epsilon^w - \epsilon_{\text{ref}}(x_{t}^w,t)\|^2_2 \right. \right. \\
    - \left. \left( \| \epsilon^l - \epsilon_\theta(x_{t}^l,t)\|^2_2 - \|\epsilon^l - \epsilon_{\text{ref}}(x_{t}^l,t)\|^2_2 \right) \right),
\end{multline}
\normalsize
where $x^w$ denotes the preferred sample, $x^l$ the rejected sample, and $\omega(\lambda_t)$ is a weighting function \cite{ho2020denoisingdiffusionprobabilisticmodels, vaediffusion, rafailov2024directpreferenceoptimizationlanguage}.

During training, we use a selective sampling strategy to construct high-quality preference pairs by selecting the highest-ranked (preferred) and lowest-ranked (rejected) samples based on critic model evaluations. This ensures clearer preference distinctions and enhances learning. These pairs $(y^{w}, y^{l})$ form the final training data.

The optimization objective combines the Diffusion-DPO loss with two auxiliary negative log-likelihood (NLL) terms. The first, weighted by $\lambda_1{=}0.2$, regularizes the preferred samples to align with human preferences. The second, weighted by $\lambda_2{=}0.1$, anchors the model to real video distributions to prevent overfitting. The final loss is:
\small
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) &= L_{\text{dpo}}(\theta) \\
&\quad + \lambda_1 \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{sample}}} \bigl[-\log p_{\theta}(y^w|x)\bigr] \\
&\quad + \lambda_2 \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}} \bigl[-\log p_{\theta}(y|x)\bigr],
\end{aligned}
\end{equation}
\normalsize
where $\mathcal{D}^{\mathrm{sample}}$ represents the set of preference pairs derived from the critic model’s evaluation, and $\mathcal{D}^{\mathrm{real}}$ denotes the real video data used for regularization.


\subsubsection{Iterative Optimization}
%1. why Iterative Optimization
%2. what is Iterative Optimization

% Iterative optimization offers several advantages, particularly when dealing with complex tasks like preference alignment in reinforcement learning. Instead of directly approximating the optimal policy $\pi_\theta$ through a single step, which is challenging and computationally expensive, iterative optimization divides the problem into more manageable stages. This stepwise approach helps refine the model progressively, achieving incremental improvements in a controlled manner. By breaking down the overall task, each iteration can focus on refining the model's alignment with human preferences, making the overall optimization process more tractable and efficient.
Iterative optimization is advantageous for complex tasks like preference alignment in reinforcement learning. Rather than approximating the optimal policy $\pi_\theta$ in a single step, which is computationally challenging, it decomposes the problem into manageable stages. This stepwise refinement enables incremental improvements, progressively aligning the model with human preferences in a more efficient and controlled manner. A key benefit of iterative optimization is that it enables more stable training. Rather than attempting to directly approximate the reference distribution $\pi_{\text{ref}} \exp\left(\frac{1}{\beta} r(x_0, c)\right)$ in a single step, the process proceeds in multiple stages. In each iteration, we approximate the distribution using the following formula:
\vspace{-8pt}
\small
\begin{equation}
\pi_{\text{ref}}^{(k)} = \pi_{\text{ref}}^{(k-1)} \exp\Big(\frac{r^k}{\beta_{k}} \Big),
\end{equation}
\normalsize
% where $r^k$ is the reward function at the $k$-th iteration, and $\beta_k$ is a hyperparameter that controls the rate of optimization at each stage. Initially, we set $r^0 = 0$ to start the optimization process with a neutral reward function. As the iterations progress, the reward function is refined, and the model converges more effectively toward an optimal policy.
where, $r^k$ denotes the reward function at the $k$-th iteration, and $\beta_k$ is a hyperparameter controlling the optimization rate at each stage. We initialize $r^0 = 0$ to start with a neutral reward function. As iterations progress, the reward function is refined, guiding the model toward an optimal policy.

Iterative refinement improves training control, enabling the model to gradually align with human feedback while avoiding early overfitting or divergence. By utilizing intermediate distributions at each stage, the method accommodates more complex feedback signals, leading to a more robust model that generalizes well across diverse real-world scenarios.
Additionally, iterative optimization mitigates issues like catastrophic forgetting and instability, which can occur when attempting to approximate the optimal policy in a single step. Each iteration builds on the previous one, ensuring stable and reliable convergence toward the target policy.
In this way, iterative preference optimization (IPO) enhances preference-based optimization by providing better training control, faster convergence, and improved generalization, making it a valuable tool for optimizing policies based on human feedback.
% This iterative refinement allows for better control over the training process, enabling the model to gradually improve its alignment with human feedback without the risk of overfitting or divergence early on in the training. Additionally, by using intermediate distributions at each stage, the method can handle more complex feedback signals, ultimately leading to a more robust model that generalizes well to diverse real-world scenarios.

% Moreover, iterative optimization helps mitigate challenges such as catastrophic forgetting and instability that can arise when attempting to approximate the optimal policy in one go. It ensures that each step builds on the previous one, allowing for a more stable and reliable convergence toward the target policy.

% In summary, by breaking down the optimization process into multiple iterations, iterative preference optimization (IPO) improves both the tractability and effectiveness of preference-based optimization. This staged approach leads to better control over the training process, faster convergence, and improved generalization, making it an essential tool for optimizing policies based on human feedback.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this section, we present our proposed general iterative preference optimization (IPO) framework, which is designed to support both pairwise and pointwise preference alignment optimization algorithms. For offline preference alignment algorithms, the effectiveness is often hindered by the limitations of offline datasets, as highlighted by RSO~\cite{Liu_Zhao_RSO2023}, particularly when the offline data distribution diverges from the target distribution. Taking Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} as a representative example, which can be extended to other algorithms, DPO can be interpreted as imposing constraints on the resulting policy through the dataset \( D \). Informally, for DPO to converge to the optimal policy \( \pi^* \), it requires an infinite dataset \( D \) to comprehensively cover the entire query-response space. However, in practice, training on a finite dataset \( D \) inevitably fails to encompass the full query-response space, leading to suboptimal convergence and performance limitations, which our IPO framework aims to address by iteratively refining the alignment process and enhancing the coverage and quality of the policy optimization.


% ~\cite{li2023policy} demonstrates that reward models may exhibit superior generalization capabilities compared to policies in terms of sample complexity, meaning that, given a fixed number of samples, reward models achieve higher preference classification accuracy. To leverage this insight, we first train a critic model capable of performing both pairwise ranking and pointwise scoring. During the training process, no additional human feedback is queried; instead, the trained critic model is utilized to label the samples generated by the model in subsequent training iterations. However, since the model is initially poorly aligned with human feedback, the generated results often suffer from low quality, leading to inefficient sampling. This results in collected samples that may not adequately approximate the target distribution, ultimately causing performance degradation when training on these samples. To address this issue and enhance algorithmic efficiency, we propose an iterative preference optimization (IPO) framework, which progressively refines the model's alignment with human preferences, enabling a more effective and gradual achievement of the final optimization goal.

% In RLHF, the optimization goal is to maximize the expected reward given the condition $c$, while increasing the KL-divergence as a reward penalty to keep the optimized model close to the reference distribution.

% \begin{multline}
%     \max_{\pi_\theta} \mathbb{E}_{x_0 \sim \pi_\theta(x_0|c)} \left[ r(x_0, c)
%     - \beta \log \frac{\pi_\theta(x_0|c)}{\pi_{ref}(x_0|c)} \right] ,
%     \label{eq:objective}
% \end{multline}

% where the hyperparameter $\beta$ controls regularization and is usually a fixed value.

% In \cite{zhang_2023_ltbook}, a loss functional is defined in terms of  $\pi_\theta(\cdot|c)$, expressed as 
% \begin{multline}
% \mathbb{E}_{x_0 \sim \pi(x_0|c)} \Big[-r(x_0,c) - \beta \log \frac{\pi_{ref}(x_0|c)}{\pi_\theta(x_0|c)}\Big] =  \\
% \beta \mathbb{KL}\Big( \pi_\theta(x_0|c) \Big\Vert \pi_{ref}(x_0|c)\exp\Big(\frac{1}{\beta}r(x_0,c)\Big) \Big), 
% \end{multline}
% the value that minimizes the loss functional is
% $
% \pi^*(x_0|c) \propto\pi_0(x_0|c)\exp\Big(\frac{1}{\beta}r(x_0,c)\Big) 
% $.
% This is consistent with the results of DPO, which directly transforms the optimization objective into training $\pi^*(x_0|c)$.

% The conclusion drawn is that the reinforcement learning process, when combined with the maximum expected reward under KL regularization, aims to approximate $\pi^*(x_0|c)$ during the learning process. However, in practical scenarios, obtaining unlimited offline datasets to achieve this goal is infeasible, and the model often achieves only limited improvement and constrained generalization when trained on finite and suboptimal offline datasets. While it is necessary to utilize the Critic model to annotate the results generated by the model, the efficiency and effectiveness of data generation often lead to suboptimal outcomes in a single training iteration, highlighting the need for iterative refinement to enhance performance and generalization capabilities.


% Inspired by ～\cite{ouyang2022traininglanguagemodelsfollow},We introduce an iterative preference optimization method where, instead of directly approximating $\pi_\theta$ by $\pi_{ref} \exp(\frac{1}{\beta} r(x_0,c))$, which is hard, we divide the process into multiple stages by examining a series of distributions.By approximating $\pi_{ref}\exp(\frac{1}{\beta_{i-1}} r)$ with $\pi_{ref}\exp(\frac{1}{\beta_{i-1}} r)$ at each iteration, stepwise improvement is easier to achieve.

% Inspired by~\cite{ouyang2022traininglanguagemodelsfollow}, we introduce an iterative preference optimization method that, rather than directly approximating $\pi_\theta$ by $\pi_{\text{ref}} \exp\left(\frac{1}{\beta} r(x_0, c)\right)$—a challenging task—divides the process into multiple stages by examining a series of intermediate distributions. By approximating $\pi_{\text{ref}} \exp\left(\frac{1}{\beta_{i-1}} r\right)$ with $\pi_{\text{ref}} \exp\left(\frac{1}{\beta_{i-1}} r\right)$ at each iteration, we achieve stepwise improvements, making the optimization process more tractable and effective.
% % $$
% % \pi_{ref} \to \pi_{ref} \exp(\frac{1}{\beta_1} r) \to \cdots \to \pi_{ref} \exp(\frac{1}{\beta_N} r) \to  \pi^*
% % $$
% %$
% %\begin{document}

% \[
% \pi_{ref}^{(0)} = \pi_{ref},
% \]
% \[
% \pi_{ref}^{(1)} = \pi_{ref}^{(0)} \exp\Big(\frac{1}{\beta_1} r\Big),
% \]
% \[
% \pi_{ref}^{(2)} = \pi_{ref}^{(1)} \exp\Big(\frac{1}{\beta_2} r\Big),
% \]
% \[
% \vdots
% \]
% \[
% \pi^* = \pi_{ref}^{(N)} \exp\Big(\frac{1}{\beta_2} r\Big).
% \]
% %\end{document}
% %$

% To validate the effectiveness and generalizability of the proposed approach, we introduce two distinct offline preference alignment methodologies: Diffusion-DPO, which leverages pairwise ranking data to align models with human preferences, and Diffusion-KTO, which utilizes pointwise scoring data to optimize alignment objectives, both designed to enhance the performance and adaptability of the framework across diverse scenarios.

% \paragraph{Diffusin-DPO Objective}
% Diffusion-DPO~\cite{wallace2023diffusionmodelalignmentusing} extends DPO~\cite{rafailov2024directpreferenceoptimizationlanguage} by introducing inverse decomposition to replace $\pi_\theta$ and $\pi_{ref}$.  The forward process $q(x_{1:T}|x_0)$ serves as an approximation for the reverse process $p_\theta(x_{1:T}|x_0)$. Ultimately, the objective function is formulated after these adjustments. By redefining data likelihood in the context of diffusion models, Diffusion-DPO introduces a straightforward and effective loss formulation, enabling robust and efficient preference optimization.
% \begin{multline}
%     L_{diffusion-dpo}(\theta)
%     = \\
%     -\mathbb{E}_{
%     (x^w_0, x^l_0) \sim D, t\sim \mathcal{U}(0,T), %
%     x^w_{t}\sim q(x^w_{t}|x^w_0),x^l_{t} \sim q(x^l_{t}|x^l_0)
%     } \\
%     \log\sigma \left(-\beta T \omega(\lambda_t) \left(\right. \right.\\
%     \| \epsilon^w -\epsilon_\theta(x_{t}^w,t)\|^2_2 - \|\epsilon^w - \epsilon_\text{ref}(x_{t}^w,t)\|^2_2 \\
%     \left. \left.  - \left( \| \epsilon^l -\epsilon_\theta(x_{t}^l,t)\|^2_2 - \|\epsilon^l - \epsilon_\text{ref}(x_{t}^l,t)\|^2_2\right)
%     \right)\right)\label{eq:loss-dpo-1}
% \end{multline}
% where $x^w$ is the chosen sample, $x^l$ is the rejected sample,  $\omega(\lambda_t)$ a weighting 
% function~\cite{ho2020denoisingdiffusionprobabilisticmodels,vaediffusion,rafailov2024directpreferenceoptimizationlanguage}).

% \paragraph{Diffusin-KTO Objective}
% Inspired by prospect theory, KTO proposes the concept of human-aware loss functions by maximizing the utility function and defining reference points. KTO does not require preference data and can directly use data marked with binary signals to train the algorithm, making it more sensitive to negative samples.
% Instead of optimizing the expected reward, Diffusion-KTO adopts a nonlinear utility function that calculates the utility of an action based on its value $Q(a,s)$ relative to a reference point $Q_\text{ref}$.
% \begin{multline}
% %\begin{equation}
%     L_{diffusion-kto}(\theta)
%     = \underset{\pi_{\theta}}{\text{max}}\mathbb{E}_{x_0 \sim \mathcal{D},t\sim \text{Uniform([0,T])} 
%     } \\
%     [U(w(x_0)(\beta \log \frac{\pi_\theta(x_{t-1}|x_t)}{\pi_{\text{ref}}(x_{t-1}|x_t)}-Q_{\text{ref}}))]
%     \label{eq:dkto_loss}
% %\end{equation}
% \end{multline}
% where $w(x_0)= \pm1$, $x_0$ represents a state that is considered either desirable or undesirable. The quantity $Q_{ref}$ is introduced as  $\beta \mathbb{D}_{\text{KL}}[\pi_{\theta}(a|s) ||\pi_{\text{ref}}(a|s)]$, which measures the Kullback-Leibler divergence between the policy $\pi_\theta$ and the reference policy $\pi_{\text{ref}}$.

% Iterative Preference Optimization (IPO) framework, each iteration training consists of three key components: video generation, critic, and preference learning. The implementation details will be introduced in the next section.

% The Iterative Preference Optimization (IPO) framework consists of three key components in each training iteration: Video generation, Critic model annotation, and Preference learning. Below, we outline the steps involved in each stage of the process:

% \begin{itemize}
% \setlength{\itemsep}{2pt} 
% \item \textit{\textbf{Video Generation.}}.
%  We use T2V model generate at least three random videos for the same prompt by setting different seeds to to form a dataset $\mathcal{V} = $\{(x_k, y_k_1, y_k_2, y_k_3,...)\}_{k=1}^N$ $, where $x_k$ represents the prompt and $y_k$ represents the corresponding video generated by the model.
 
% \item \textit{\textbf{Critic model annotation}}.
%  In order to obtain better ranking results and eliminate the bias of the reward model, we compare the generated videos in pairs. At the same time, we will swap the order of the two videos. The results are consistent and self-consistent. We will add the paired dataset $\mathcal{D} = $\{(x_k, y_k_w, y_k_l)\}_{k=1}^N$ $，where $y_k_w$ represents the selected samples and $y_k_l$ represents the rejected samples.


% \item  \textit{\textbf{Preference learning}.} 
% During training, we select the highest and the worst ranked pairs as the final paired data $(y^{w}, y^{l})$, and do not use the data in between, in order to obtain higher quality paired data.
% We use the diffusion-dpo loss. 
% To further stabilize DPO training, we incorporate two additional negative log-likelihood (NLL) loss terms, each scaled by a factor of 0.1, following the approach of Pang et al. (2024). These terms are applied to the selected sequences—one to the chosen sample and the other to the real video data. This strategy ensures the preservation of the desired output format and prevents a decline in the log-probability of the selected responses (Pang et al., 2024; Pal et al., 2024).



% \begin{equation}
% \begin{aligned}
% \mathcal{L}(\theta) = L_\text{DPO-Diffusion}(\theta) \\
% &+ 
% \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{sample}}}
% \left[-\log p_{\theta}(y^w|x)\right] \\
% &+
% \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}}
% \left[-\log p_{\theta}(y|x)\right]，
% \end{aligned}
% \end{equation}


%\end{itemize}