\section{Background}
\label{Background}

\subsection{Diffusion Models}
% Diffusion models\cite{ddpm,sohl2015deep} are latent variable models with discrete-time forward diffusion process where $q({\x_0})$ is the data distribution.
% % $\alpha$ is a scalar function, with latent variables $\{{\x_t}\}_{t=1}^T$. The forward process is defined as:
% % \begin{equation}
% %     q(\x_t|\x_0)=\calN(\x_t;\alpha_t\x_0, \sigma^2_t\I).
% % \end{equation}
Diffusion models have achieved remarkable results in the field of image and video generation. Denoising Diffusion Probabilistic Models (DDPM) consists of two processes, the forward denoising process and the reverse denoising process. Given the initial data distribution $x_0 \sim q(x)$, Gaussian noise is gradually added to the data distribution, and the denoising process lasts t times. The generative model can then be trained using the Evidence Lower Bound (ELBO) objective to learn the inverse process $p_\theta(x_{t-1}|x_t)$:
 \begin{equation}
    \mathcal{L_{\text{DDPM}}}=\mathbb{E}_{x_0, t, \epsilon}[\lambda(t)\lVert\epsilon_t-\epsilon_\theta(x_t,t)\rVert^2]
\end{equation}

where $\epsilon \sim \mathcal{N}(0, I)$,
$ $\lambda_t=\alpha_t^2/\sigma_t^2$ is a signal-to-noise ratio~\cite{vaediffusion}, $\omega(\lambda_t)$ is a time-dependent and pre-specified weighting function~\cite{ddpm,song2019generative}).

\subsection{Direct Preference Optimization} In our online training based on pairwise data method, diffusion-dpoï½ž\cite{} is used, which is a preference alignment model based on dpo~\cite{} that is more suitable for diffusion.

DPO is a method to directly optimize user or expert preferences. It collects user preference data on model output and directly optimizes the model parameters, making the model output more in line with user preferences, avoiding complex reinforcement learning algorithms.
\begin{equation}
    L_{\text{DPO}}(\theta)\!=\underset{\pi_{\theta}}{\text{max }}\mathbb{E}_{x^w,x^l,c \sim \mathcal{D}}[\log \sigma(\beta \log \frac{\pi_\theta(x^w|c)}{\pi_{\text{ref}}(x^w|c)}-\beta \log \frac{\pi_\theta(x^l|c)}{\pi_{\text{ref}}(x^l|c)})]
    \label{eq:dpo}
\end{equation}
% \paragraph{Diffusin-DPO Objective}
% Diffusion-DPO replaces $p_\theta$ and $\pref$ with inverse decomposition, and uses Jensen's inequality and the convexity of the function ~$-\!\log\sigma$ to push the expectation outside. After some simplification, the following bounds are obtained. The forward $q(\x_{1:T}|\x_0)$ approximates the inverse process $p_\theta(\x_{1:T}|\x_0)$. After simplification, the final objective function is obtained. Diffusion-DPO redefines the concept of data likelihood under the diffusion model and derives a simple but effective loss target, thereby achieving stable and efficient preference training.
% \begin{multline}
%     L(\theta)
%     = -\mathbb{E}_{
%     (x^w_0, x^l_0) \sim \calD, t\sim \mathcal{U}(0,T), %
%     x^w_{t}\sim q(x^w_{t}|x^w_0),x^l_{t} \sim q(x^l_{t}|x^l_0)
%     } \\
%     \log\sigma \left(-\beta T \omega(\lambda_t) \left(\right. \right.\\
%     \| \epsilon^w -\epsilon_\theta(x_{t}^w,t)\|^2_2 - \|\epsilon^w - \epsilon_\text{ref}(x_{t}^w,t)\|^2_2 \\
%     \left. \left.  - \left( \| \epsilon^l -\epsilon_\theta(x_{t}^l,t)\|^2_2 - \|\epsilon^l - \epsilon\text{ref}(x_{t}^l,t)\|^2_2\right)
%     \right)\right)\label{eq:loss-dpo-1}
% \end{multline}
% where $x^*_t =  \alpha_t\x^*_0+\sigma_t\epsilon^*, \epsilon^* \sim N(0,I)$, $\omega(\lambda_t)$ a weighting function (constant in practice~\cite{ddpm,vaediffusion}).

% \paragraph{Diffusin-KTO Objective}
% Inspired by prospect theory, KTO proposes the concept of human-aware loss functions by maximizing the utility function and defining reference points. KTO does not require preference data and can directly use data marked with binary signals to train the algorithm, making it more sensitive to negative samples.
% Instead of optimizing the expected reward, Diffusion-KTO adopts a nonlinear utility function that calculates the utility of an action based on its value $Q(a,s)$ relative to a reference point $Q_\text{ref}$.
% \begin{equation}
%     \underset{\pi_{\theta}}{\text{max }}\mathbb{E}_{x_0 \sim \mathcal{D},t\sim \text{Uniform([0,T])} } [U(w(x_0)(\beta \log \frac{\pi_\theta(x_{t-1}|x_t)}{\pi_{\text{ref}}(x_{t-1}|x_t)}-Q_{\text{ref}}) )  ]
%     \label{eq:dkto_loss}
% \end{equation}

% where $w(x_0)= \pm1$ if image $x_0$ is desirable or undesirable. We set $Q_{\text{ref}}= \beta \mathbb{D}_{\text{KL}}[\pi_{\theta}(a|s) ||\pi_{\text{ref}}(a|s)]$. Empirically, this is calculated by computing $\max(0,\frac{1}{m}\sum \log \frac{\pi_\theta(a'|s')}{\pi_{\text{ref}(a'|s')}})$ over a batch of unrelated pairs of $(s',a')$ following the KTO  setup \cite{ethayarajh2024kto} in \cref{eq:kto_loss_llm}.
