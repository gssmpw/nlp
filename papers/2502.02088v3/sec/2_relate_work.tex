\section{Related Work}
\label{sec:related_work}

\noindent\textbf{Text-to-video generation}
Recent advances in video generation have been largely driven by diffusion models~\cite{ho2020denoisingdiffusionprobabilisticmodels,Sohl-Dickstein_Weiss_Maheswaranathan_Ganguli_2015,Song_Meng_Ermon_2020,Zhang_Tao_Chen_2022}, achieving remarkable improvements in diversity, fidelity, and quality~\cite{chen2023seineshorttolongvideodiffusion,esser2023structurecontentguidedvideosynthesis,ho2022videodiffusionmodels,liu2023dualstreamdiffusionnettexttovideo,ruan2023mmdiffusionlearningmultimodaldiffusion,wang2024swapattentionspatiotemporaldiffusions,xing2023simdasimplediffusionadapter}. Scaling model size and training data further enhance performance~\cite{Ho_Chan_Saharia_Whang_Gao_Gritsenko_Kingma_Poole_Norouzi_Fleet_et,Singer_Polyak_Hayes_Yin_An_Zhang_Hu_Yang_Ashual_Gafni_et,Wu_Huang_Zhang_Li_Ji_Yang_Sapiro_Duan_2021}. In Text-to-Video (T2V) generation, pre-training from scratch requires substantial computational resources and large-scale datasets~\cite{gen3,luma,kling,brooks2024video,kong2024hunyuanvideo}. Current methods~\cite{blattmann2023alignlatentshighresolutionvideo,wang2023modelscopetexttovideotechnicalreport,guo2024animatediffanimatepersonalizedtexttoimage} extend text-to-image models by incorporating spatial and temporal attention modules, often using joint image-video training~\cite{ma2024lattelatentdiffusiontransformer}, with~\cite{guo2024animatediffanimatepersonalizedtexttoimage} proposing a plug-and-play temporal module for personalized image models. State-of-the-art pixel quality and temporal consistency have been achieved by~\cite{Zhang_Wang_Zhang_Zhao_Yuan_Qin_Wang_Zhao_Zhou_Group,Chen_Zhang_Cun_Xia_Wang_Weng_Shan_2024}, while diffusion models based on the DiT structure~\cite{brooks2024video,yang2024cogvideox,Chen_Zhang_Cun_Xia_Wang_Weng_Shan_2024} enhance spatiotemporal coherence and scalability, highlighting the potential of Transformer-based architectures. Despite of their success, they still cannot meet requirements in real scenarios due to lack of alignment with human preference.

%Further improvements address spatial-temporal discrepancies in two-stage training, with~\cite{lin2024opensoraplanopensourcelarge,kong2024hunyuanvideo,yang2024cogvideox} employing 3D full attention mechanisms to enhance performance and bridge spatial-temporal gaps.
% Recent advances in video generation have been predominantly driven by diffusion models~\cite{Ho_Jain_Abbeel_Berkeley,Sohl-Dickstein_Weiss_Maheswaranathan_Ganguli_2015,Song_Meng_Ermon_2020,Zhang_Tao_Chen_2022}, which have demonstrated remarkable improvements across multiple dimensions, including diversity, fidelity, and image quality~\cite{chen2023seineshorttolongvideodiffusion,esser2023structurecontentguidedvideosynthesis,ho2022videodiffusionmodels,liu2023dualstreamdiffusionnettexttovideo,ruan2023mmdiffusionlearningmultimodaldiffusion,wang2024swapattentionspatiotemporaldiffusions,xing2023simdasimplediffusionadapter}, with the scaling of training data and model size further enhancing their performance~\cite{Ho_Chan_Saharia_Whang_Gao_Gritsenko_Kingma_Poole_Norouzi_Fleet_et,Singer_Polyak_Hayes_Yin_An_Zhang_Hu_Yang_Ashual_Gafni_et,Wu_Huang_Zhang_Li_Ji_Yang_Sapiro_Duan_2021}. In the domain of Text-to-Video (T2V) generation, pre-training models from scratch demands substantial computational resources and access to extensive human-curated video datasets, highlighting the critical importance of leveraging large-scale datasets for effective model training~\cite{gen3,luma,kling,sora,kong2024hunyuanvideo}. Current methodologies, such as those presented in~\cite{blattmann2023alignlatentshighresolutionvideo,wang2023modelscopetexttovideotechnicalreport,guo2024animatediffanimatepersonalizedtexttoimage}, build upon text-to-image models by incorporating spatial attention mechanisms, introducing temporal attention modules, and fine-tuning them on video datasets, often employing joint image and video training strategies~\cite{ma2024lattelatentdiffusiontransformer}, with~\cite{guo2024animatediffanimatepersonalizedtexttoimage} proposing a plug-and-play temporal module that facilitates video generation from personalized image models. State-of-the-art results in terms of pixel quality and temporal consistency have been achieved by methods such as~\cite{Zhang_Wang_Zhang_Zhao_Yuan_Qin_Wang_Zhao_Zhou_Group,Chen_Zhang_Cun_Xia_Wang_Weng_Shan_2024}, while recent diffusion models based on the Dit structure, such as~\cite{sora,yang2024cogvideox,Chen_Zhang_Cun_Xia_Wang_Weng_Shan_2024}, have demonstrated exceptional performance, advancing the frontiers of T2V generation by enhancing spatiotemporal coherence and scalability, showcasing the potential of Transformer-based diffusion architectures. Further advancements have been made in addressing the discrepancy between spatial and temporal modules in two-stage training, with works such as~\cite{lin2024opensoraplanopensourcelarge,kong2024hunyuanvideo,yang2024cogvideox} employing 3D full attention mechanisms to improve overall performance and effectively bridge the gap between spatial and temporal components.
% The training of large models usually consists of two stages: pre-training and post-training. There is relatively little research on post-training of text-to-video.~\cite{yuan2023instructvideoinstructingvideodiffusion}proposed a temporal decaying reward (TAR) based on the assumption that the central frame should be given the most importance, while the emphasis gradually decreases towards the peripheral frames. This strategic distribution of importance across frames ensures a more stable and visually coherent video generation process.~\cite{li2024t2vturbov2enhancingvideogeneration}highlighted the vast design space of energy functions to enhance ODE solvers and demonstrated its potential by enhancing T2V model training by extracting motion priors from training videos.~\cite{prabhudesai2024videodiffusionalignmentreward}is a method that uses the gradient of a reward model to adjust a base video diffusion model. A wide range of pre-trained vision models are used to align various video diffusion models. Finally, alignment using VADER can easily generalize to cues not seen during training.


\vspace{0.5\baselineskip}
\noindent\textbf{Post-training for human preference alignment}
Post-training is a common technique exploited in Large Language Models (LLMs)s~\cite{ouyang2022traininglanguagemodelsfollow, ramamurthy2023reinforcementlearningnotnatural, zheng2023secretsrlhflargelanguage}. Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow} learns a reward function from comparison data to align the policy model. Rank Responses for Human Feedback (RRHF)~\cite{yuan2023rrhfrankresponsesalign} extends supervised fine-tuning, while Sequence Likelihood Calibration (SLiC)~\cite{zhao2023slichfsequencelikelihoodcalibration} leverages offline RL data. Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} improves stability and efficiency by directly optimizing human preference data, and Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024ktomodelalignmentprospect} aligns models via a prospect-theoretic approach. Rejection Sampling Optimization (RSO)~\cite{liu2024statisticalrejectionsamplingimproves} enhances alignment using statistical rejection sampling. Prior works have introduced this technique into enhancement of image foundation models, such as~\cite{clark2024directlyfinetuningdiffusionmodels, prabhudesai2024aligningtexttoimagediffusionmodels, xu2023imagerewardlearningevaluatinghuman}. Diffusion-DPO~\cite{wallace2023diffusionmodelalignmentusing} extends DPO for diffusion models, optimizing model likelihood, while Diffusion-KTO~\cite{li2024aligningdiffusionmodelsoptimizing} maximizes expected human utility without costly pairwise preference data. The usage of post-training in video generation is still rare. Current works mainly focus on the design of reward models. 
%The training of large models consists of pre-training and post-training, with limited research on post-training for text-to-video (T2V) generation. In this context,
\cite{yuan2023instructvideoinstructingvideodiffusion} introduced a temporal decaying reward (TAR) mechanism, prioritizing central frames for stable and coherent video generation. Meanwhile,~\cite{li2024t2vturbov2enhancingvideogeneration} optimized ordinary differential equation (ODE) solvers by exploring energy function designs, enhancing T2V training through motion prior extraction. VADER~\cite{prabhudesai2024videodiffusionalignmentreward} fine-tunes video diffusion models using reward model gradients and pre-trained vision models, achieving strong generalization even for unseen cues, thereby improving T2V adaptability and performance. However, the above strategies can only achieve minor improvements and not fully explore the potential of post-training. 
% The training of large models typically involves two key stages: pre-training and post-training, with relatively limited research focusing on the post-training phase for text-to-video (T2V) generation. In this context,~\cite{yuan2023instructvideoinstructingvideodiffusion} introduced a temporal decaying reward (TAR) mechanism, grounded in the assumption that the central frame of a video should be assigned the highest importance, while the emphasis gradually diminishes towards the peripheral frames. This strategic allocation of importance across frames ensures a more stable and visually coherent video generation process. Meanwhile,~\cite{li2024t2vturbov2enhancingvideogeneration} explored the extensive design space of energy functions to enhance ordinary differential equation (ODE) solvers, demonstrating its potential by improving T2V model training through the extraction of motion priors from training videos. Additionally, VADER~\cite{prabhudesai2024videodiffusionalignmentreward} proposed a method that leverages the gradient of a reward model to fine-tune a base video diffusion model, utilizing a wide range of pre-trained vision models to align various video diffusion models. This alignment approach, facilitated by VADER, exhibits strong generalization capabilities, even for cues not encountered during training, thereby advancing the adaptability and performance of T2V systems.
% \NNnote{Bram -- can we couple lines on RLAIF  --constitutional AI etc.}

\if 0
\vspace{0.5\baselineskip}
\noindent\textbf{Aligning with human preferences}
Aligning human preferences in large language models (LLMs) has been extensively studied, with various post-training approaches achieving significant progress~\cite{ouyang2022traininglanguagemodelsfollow, ramamurthy2023reinforcementlearningnotnatural, zheng2023secretsrlhflargelanguage}. Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow} learns a reward function from comparison data to align the policy model. Rank Responses for Human Feedback (RRHF)~\cite{yuan2023rrhfrankresponsesalign} extends supervised fine-tuning, while Sequence Likelihood Calibration (SLiC)~\cite{zhao2023slichfsequencelikelihoodcalibration} leverages offline RL data. Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} improves stability and efficiency by directly optimizing human preference data, and Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024ktomodelalignmentprospect} aligns models via a prospect-theoretic approach. Rejection Sampling Optimization (RSO)~\cite{liu2024statisticalrejectionsamplingimproves} enhances alignment using statistical rejection sampling. Preference alignment has also been applied to diffusion models, achieving notable improvements. DraFT~\cite{clark2024directlyfinetuningdiffusionmodels} fine-tunes models based on differentiable reward functions, while~\cite{prabhudesai2024aligningtexttoimagediffusionmodels} aligns text-to-image models via end-to-end backpropagation. Other works optimize models using reward scores~\cite{xu2023imagerewardlearningevaluatinghuman} but face efficiency and stability issues. DPOK~\cite{fan2023dpokreinforcementlearningfinetuning} combines policy optimization with KL regularization to fine-tune diffusion models, and DDPO~\cite{black2024trainingdiffusionmodelsreinforcement} reframes denoising as a multi-step decision problem. Diffusion-DPO~\cite{wallace2023diffusionmodelalignmentusing} extends DPO for diffusion models, optimizing model likelihood, while Diffusion-KTO~\cite{li2024aligningdiffusionmodelsoptimizing} maximizes expected human utility without costly pairwise preference data. The iterative preference optimization algorithm in this work further enhances methods like Diffusion-DPO and Diffusion-KTO, improving stability and alignment effectiveness.
\fi

% Aligning human preferences through post-training has been widely studied for large language models (LLMs), with several approaches~\cite{ouyang2022traininglanguagemodelsfollow, ramamurthy2023reinforcementlearningnotnatural, zheng2023secretsrlhflargelanguage} making notable advancements. Reinforcement Learning from Human Feedback (RLHF)~\cite{ouyang2022traininglanguagemodelsfollow} trains a reward function from comparison data on model outputs to align the policy model. Rank Responses for Human Feedback (RRHF)~\cite{yuan2023rrhfrankresponsesalign}, a simpler alternative to Proximal Policy Optimization (PPO), extends supervised fine-tuning and reward model training. Sequence Likelihood Calibration (SLiC)~\cite{zhao2023slichfsequencelikelihoodcalibration} achieves alignment using human feedback data collected for different models, akin to offline RL data. Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} simplifies alignment by directly training on human preference data, improving stability and performance while reducing computational costs. Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024ktomodelalignmentprospect} maximizes utility based on the Kahneman-Tversky model, rather than maximizing log-likelihood of preferences. Rejection Sampling Optimization (RSO)~\cite{liu2024statisticalrejectionsamplingimproves} introduces statistical rejection sampling to derive preference data from the optimal policy, enhancing alignment with human preferences.


% Several methods have successfully integrated preference alignment with diffusion models, driving significant progress in the field. DraFT~\cite{clark2024directlyfinetuningdiffusionmodels} fine-tunes models to maximize differentiable reward functions, such as human preference scores, while~\cite{prabhudesai2024aligningtexttoimagediffusionmodels} aligns models with downstream reward functions via end-to-end backpropagation through the denoising process. Similarly,~\cite{xu2023imagerewardlearningevaluatinghuman} optimizes models based on reward scores, but these approaches often face inefficiency and instability. To overcome these challenges, DPOK~\cite{fan2023dpokreinforcementlearningfinetuning} frames fine-tuning as a reinforcement learning problem, combining policy optimization with KL regularization and policy gradients to update pre-trained text-to-image diffusion models. DDPO~\cite{black2024trainingdiffusionmodelsreinforcement} reframes denoising as a multi-step decision problem, using policy gradients to enhance cue-image alignment without additional data collection or human annotation. Diffusion-DPO~\cite{wallace2023diffusionmodelalignmentusing} optimizes models using human comparison data, reformulating Direct Preference Optimization (DPO) to improve model likelihood and differentiable objectives. Diffusion-KTO~\cite{li2024aligningdiffusionmodelsoptimizing} introduces a novel approach for aligning text-to-image models by maximizing expected human utility, removing the need for costly pairwise preference data. The iterative preference optimization algorithm in this work further enhances offline learning methods like Diffusion-DPO and Diffusion-KTO, addressing instability and performance issues while advancing alignment effectiveness.

