


% \input{sec/tables/vbench_offline_online}

\begin{table}[t!]\small
\centering
\vspace{-3mm}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{Method} & Quality & Semantic & Total \\
& Score & Score & Score \\
\midrule
CogVideoX-2B & 82.18 & 75.83 & 80.91 \\
CogVideoX-5B & 82.75 & 77.04 & 81.61 \\
CogVideoX-2B-IPO-3 & \textbf{83.92} & \textbf{78.00} & \textbf{82.74} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Comparison of IPO with baselines CogVideoX models.}
\label{sota_comparison}
\end{table}

\input{sec/tables/sftloss}


\section{Experiments}


\input{sec/tables/vbench_iter}
%\input{sec/tables/reward_modelsize}
\input{sec/figs/dpo_2B_case_1}
%This section details the experimental setup, results, and ablation studies to evaluate the effectiveness of the proposed iterative preference optimization framework for video generation using human feedback during post-training.  



\subsection{Experimental Setup}

\subsubsection{Critic Model Training} \label{critic_training}
% To develop a comprehensive model capable of handling both paired data sorting and individual video scoring tasks, we fine-tuned the pre-trained VILA\cite{lin2023vila}, leveraging its exceptional capabilities in image and video instruction following, comprehension, and multi-image processing, which make it particularly suitable for training our Critic model. During training, we implemented key modifications, including restructuring the prompt format to incorporate detailed reasoning chains before point-by-point scoring or pairwise ranking, and constructing explicit chains of thought to enhance the model's decision-making process. This approach enables the model to provide comprehensive rationales prior to delivering its final output, improving both the accuracy of results and the quality of natural language feedback.
To unify paired data sorting and individual video scoring, we fine-tune the pre-trained VILA~\cite{lin2023vila}, leveraging its strengths in video comprehension and instruction following. Key modifications include integrating reasoning chains before scoring or ranking and enhancing decision-making with explicit thought processes. This improves result accuracy and the quality of natural language feedback.  

% We initialized the model using the VILA 13B/40B pre-trained checkpoint and fine-tuned it for 5 epochs on our labeled dataset with a learning rate of 2e-5, distributed across 32 GPUs with a batch size of 4 per GPU. To mitigate inconsistencies from scaling and center cropping, we implemented pad training. For better video content understanding, we extracted 16 uniformly sampled frames from each video sequence, keeping other hyperparameters aligned with VILA's defaults. This setup optimizes both computational efficiency and model performance, ensuring strong video-text alignment.
We initialize the model with the VILA 13B/40B pre-trained checkpoint and fine-tune it for 5 epochs on our labeled dataset using a learning rate of $2\text{e-}5$. Training is distributed across 32 GPUs with a batch size of 4 per GPU. To reduce inconsistencies from scaling and center cropping, we adopt pad training. For improved video understanding, we extract 16 uniformly sampled frames per video while keeping other hyperparameters consistent with VILAâ€™s defaults. This setup balances computational efficiency and model performance, ensuring strong video-text alignment.  



\subsubsection{T2V Alignment Training}
% Our experiments are conducted on a Critic model constructed based on the diffusion model, which evaluates video quality through semantic and temporal consistency. The Critic model incorporates both metrics derived from human feedback and pre-defined objective criteria to ensure comprehensive evaluation. 

% The training process leverages both DPO and KTO methods within the proposed online reinforcement learning framework. The DPO method aligns the generated videos with human feedback by minimizing divergence between the learned policy and a reference policy, while KTO directly maximizes the utility of the generated video instead of maximizing the preference log-likelihood. Both methods were implemented with a batch size of 128, learning rate of 2e-5, and trained for up to three rounds of iterative optimization. Each round involves a combination of human feedback-guided rewards and the original diffusion model loss to balance consistency and realism.
We use a critic model based on diffusion framework to evaluate video quality through semantic and temporal consistency, combining human feedback and objective criteria. We train IPO with DPO and KTO, separately.
%Training follows an iterative preference optimization framework with DPO and KTO. 
%DPO minimizes divergence from a reference policy, while KTO maximizes video utility. 
using a batch size of 128, a learning rate of $2\text{e-}5$, and three iterations. %combining human feedback-guided rewards with the diffusion model loss to balance consistency and realism.



\subsection{Results}

\subsubsection{Quantitative Results}


Quantitative results are evaluated using the VBench metric, which measures video generation quality in terms of temporal coherence, semantic alignment, and overall plausibility. As shown in Tab.~\ref{sota_comparison}, our experiments reveal that the optimized $2B$ model outperforms the baseline $5B$ model across all metrics. The optimized model shows significant improvements in temporal coherence, enhancing frame transitions and minimizing abrupt changes. It also demonstrates stronger semantic alignment, better capturing the intended meaning and context of input prompts. These results highlight the effectiveness of our post-training strategy in addressing key limitations of the baseline model. The overall VBench score further confirms that the optimized $2B$ model achieves superior video quality while maintaining computational efficiency, making it a practical solution for optimizing smaller-scale models.

\subsubsection{Qualitative Results}
% \begin{figure*}[ht]
%     \centering
%     % Placeholder for visual comparisons
%     \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}} % Placeholder box
%     \caption{Qualitative comparison of video generation results. The optimized $2B$ model demonstrates superior semantic fidelity, temporal consistency, and overall realism compared to the baseline $5B$ model. Visualization includes examples of generated videos for different input prompts.}
%     \label{fig:qualitative_comparison}
% \end{figure*}
% To further analyze the benefits of our approach, we provide qualitative comparisons between the baseline and optimized models. Visual results demonstrate that the optimized $2B$ model produces videos with higher semantic fidelity, accurately capturing the intent of input prompts and reflecting their underlying context. Additionally, the optimized model ensures greater temporal consistency, effectively minimizing artifacts and sudden transitions between frames, which are often observed in the baseline outputs. Furthermore, the generated videos exhibit enhanced realism, with reduced incoherent structures and improved plausibility, making the outputs more visually appealing and aligned with human perception. These qualitative improvements highlight the effectiveness of our optimization process in addressing key challenges related to semantic and temporal consistency, significantly elevating the overall quality of generated videos.
To further assess the benefits of our approach, we provide qualitative comparisons between the baseline and optimized models. Visual results, shown in Fig.~\ref{fig:visual_case}, reveal that the optimized $2B$ model generates videos with better semantic fidelity, accurately capturing the intent of input prompts and their context. It also ensures improved temporal consistency by minimizing artifacts and sudden transitions between frames, common in the baseline outputs. Additionally, the optimized videos exhibit enhanced realism, with fewer incoherent structures and greater plausibility, making them more visually appealing and aligned with human perception. This further demonstrate the effectiveness of IPO.

%These improvements demonstrate the effectiveness of our optimization process in addressing key challenges and enhancing the overall quality of generated videos.



\subsection{Ablation Studies}
%We conduct a series of ablation studies to validate the contributions of key components in our framework. Each experiment evaluates performance using the VBench metric to ensure consistency in evaluation.
%\input{sec/tables/critic_model_acc}

% \subsubsection{Impact of Critic Model Scale on Accuracy}
% To evaluate the impact of model scale on the performance of the critic model, we conducted experiments with two different scales of ViLA-based critic models: 13B and 40B parameters. The critic models were fine-tuned on a combination of pairwise and pointwise annotated datasets, designed to align with human preferences effectively. Table~\ref{results_critic} presents the accuracy of these models on validation datasets for both pairwise ranking and pointwise scoring tasks.
% The results indicate that increasing the model scale from 13B to 40B leads to significant improvements in both pairwise and pointwise accuracy. Specifically, the 40B model achieves 86.14\% accuracy in pairwise ranking and 97.57\% in pointwise scoring, outperforming the 13B model by 2.42\% and 2.24\%, respectively. This demonstrates that larger-scale critic models are better at capturing and generalizing human preferences, likely due to their increased capacity to represent complex relationships in multimodal data.
% The ablation study highlights the importance of model scale in improving the critic model's performance. While smaller models can achieve reasonable results, larger models provide a notable boost in accuracy, which is crucial for downstream reinforcement learning tasks. These findings suggest that scaling up the critic model can be a straightforward yet effective way to enhance its alignment capabilities and improve the overall quality of video generation.



\subsubsection{Incorporating Real Video Data}

To explore the impact of real video data during post-training, we integrated real samples into the training process and applied the original diffusion model loss with reinforcement learning objectives. This modification greatly improves generation quality by enhancing temporal coherence, reducing flickering, and strengthening frame consistency. Furthermore, the inclusion of real-world examples boosts semantic alignment, guiding the model to generate more realistic motion dynamics and contextual details.

As shown in Tab.~\ref{results_realdata}, the introduction of real video data (indicated by the inclusion of \texttt{sft loss}) leads to measurable improvements across multiple VBench metrics, including temporal flicker, motion smoothness, and aesthetic quality. Notably, the optimized model achieves higher scores in semantic consistency and dynamic degree, indicating a significant enhancement in the realism and coherence of generated videos. These results underscore the effectiveness of integrating real-world data with reinforcement learning to refine video generation outputs, enabling the model to produce videos with superior visual and temporal fidelity.

\input{sec/tables/vbench_dpo_kto}

\subsubsection{Comparison of different RL Methods}
To evaluate the performance of the proposed online reinforcement learning framework, we conducted experiments using two different feedback alignment methods: Knowledge Transfer Optimization (KTO) and Direct Policy Optimization (DPO). The results, presented in Tab.~\ref{difference_rl}, demonstrate that our framework consistently improves video generation quality across multiple metrics, regardless of the specific reinforcement learning method employed. All evaluation metrics across different dimensions are provided in the supplementary materials, further supporting this comprehensive improvement.

Both KTO and DPO lead to significant enhancements over the baseline \texttt{CogVideoX-2B} model, showing improvements in temporal flicker reduction, motion smoothness, and dynamic degree. These results validate the generality and robustness of the proposed online reinforcement learning framework, which effectively integrates human feedback to optimize video generation outputs. For example, the inclusion of either KTO or DPO raises the \textit{Aesthetic Quality} and \textit{Dynamic Degree} metrics compared to the baseline model, underscoring the benefits of aligning the generated outputs with human preferences.

Between the two methods, KTO achieves consistently better performance than DPO across most metrics. Notably, KTO yields a higher \textit{Dynamic Degree} score (68.83 vs. 68.76) and improved \textit{Motion Smoothness} (97.93 vs. 98.03), suggesting that KTO is more effective at producing realistic and temporally coherent videos. Additionally, KTO demonstrates greater stability during training, as evidenced by its higher overall scores across \textit{Subject Consistency} and \textit{Background Consistency}, which measure adherence to semantic and contextual details. These findings highlight the advantages of the KTO method in leveraging multi-step optimization, enabling the generation of more plausible and visually consistent video outputs compared to DPO. This confirms our IPO is flexible to adapt to different optimization methods. In addition, KTO exhibits superior performance and stability, making it a preferred choice.

%In summary, the experimental results confirm that the proposed online reinforcement learning framework is effective under different optimization methods. Among these, KTO exhibits superior performance and stability, making it a preferred choice for improving video generation quality through human feedback alignment.

\subsubsection{Effect of Multi-Round Training}
The impact of multi-round reinforcement learning is assessed by training models with 1, 2, and 3 rounds of iterative optimization, where a single round represents the traditional offline post-training approach. Results in Tab.~\ref{results_mulitiround} show that performance improves with each additional round, as multi-round training better aligns the model with human feedback and refines generation quality. These findings confirm that iterative reinforcement learning effectively enhances both temporal and semantic consistency.

\input{sec/figs/human_evaluation}
% \begin{figure}[!t]

%     \centering
%     \includegraphics[width=0.9\linewidth]{sec/figs/human_evaluation.pdf}
%     \vspace{-3mm}
%     \caption{Human evaluation of IPO and CogVideoX-2B.}
%     \label{fig:human_evaltion}
%     \vspace{-5mm}
% \end{figure}

\subsection{Human Evaluation}
Fig.~\ref{fig:human_evaltion} presents the results of a human evaluation comparing the IPO model with the baseline CogVideo-2B
%in generating videos for the same prompt. 
The result shows that our IPO outperforms the baseline in all dimensions. 
%, demonstrating comprehensive improvements across various aspects. 
In the "Consistency" dimension, the two models performed equally in around 40.5 percent of the cases, indicating that the baseline excels in maintaining text consistency. However, in the "Motion" and "Faithfulness" dimensions, the baseline's performance is average, while the IPO model demonstrates a significant improvement, particularly in generating more natural motion and better alignment with the given text. All evaluation metrics across different dimensions are provided in the \textbf{supp. materials}





% \input{sec/figs/dpo_2B_case_1}






