\begin{abstract}

Video foundation models have achieved significant advancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet requirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the perspective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Preference Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from preference feedback, which helps improve generated video quality in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which  enables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can efficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual labeling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark.The source code and model checkpoints, along with implementation details and resources, are openly available at \href{https://github.com/SAIS-FUXI/IPO}{this GitHub repository}.

% We will release our source codes, models as well as dataset to advance future research and applications.


%we train a reward model with manually labeled data for criticizing generated videos from both pairwise ranking and pointwise scoring. 
%Text-to-video generation has made significant strides in recent years, but existing models still struggle with video quality, temporal consistency, and motion coherence. 
%To address these issues, we propose an iterative reinforcement learning framework that incorporates human feedback. 
%Our approach supports two independent optimization methods: (1) \textit{Direct Preference Optimization} (DPO), which optimizes policies using a reward model trained on pairwise and pointwise annotations, and (2) \textit{Diffusion-KTO}, which aligns models with binary feedback signals, such as likes or dislikes, without requiring a complex reward model. This flexible framework enables efficient optimization and significantly improves video generation performance.
%Experiments demonstrate that our optimized models achieve state-of-the-art performance on the VBench benchmark, with our 2B parameter model surpassing the performance of 5B parameter models. Additionally, we release annotated datasets, trained reward models, and human-aligned text-to-video generation models to advance future research and applications.

%The key: We propose to improve t2v models from the perspective of post-training, instead of network architecture, parameter scale-up or loss design.
%The new PO: an iterative framework, rather than single-round
% Algin the model with human preference.
% 

\end{abstract}