% \section{Introduction}
% \label{sec:intro}

% In the past year, great progress has been made in text-to-video generation based on diffusion models.
% Excellent open source video generation models have emerged, such as CogvideoX~\cite{yang2024cogvideox},HunyuanVideo~\cite{kong2024hunyuanvideo},SVD~\cite{Blattmann2023svd}.There are also some commercial closed-source models, such as Sora. However, there are still many problems with the quality, consistency, and motion of video generation.These models are only pretraing, without post-training to align with human feedback.In the LLM training task, methods such as RLHF, DPO, and RAFT greatly improved the performance of the model.Diffusion-dpo,kto achieves significant results in aligning text-to-image models with human preferences.However, these alignment methods have not been fully explored for text-to-video tasks.Pairs are often sampled from an SFT strategy and labeled by humans, but this is still not strictly MLE for the preference model since the sampling distribution does not match π∗. In practice, it is very challenging to directly sample human-preferred pairs from π∗.As training progresses, the training data may no longer match the distribution of the current policy model, and the calculated reward may also be biased，at the same time, there will be a problem that the rewards of chosen and rejected will rise and fall synchronously during the training process, resulting in a decrease in the effect.If we use the current best strategy to collect some data in real time during training and generate new response data pairs, we can enable the reward function to give effective signals in high-reward areas, thereby obtaining a better model.If we adopt this online training strategy, we need a reward model that can score the model in real time, which is also difficult to obtain in the field of video generation.

% In this work, we proposed an online learning algorithm to address the above issues. In each learning, we can use the best strategy to synthesize data, and then annotate the synthesized data in real time through the reward model. Combined with the dpo and kto offline alignment algorithms, after optimization, it has a significant improvement over the original dpo and kto. At the same time, we introduced the reals model for video annotation, which is a multi-modal large model based on COT that can perform pairwise and pointwise annotation at the same time.

% Our main contributions are as follows:
% \begin{itemize}
% \item ~\textbf{\textit{reward model}}:We have launched a reward model based on a multimodal large model, which can score single videos and sort paired videos at the same time. In addition, our reward model will give natural language feedback, point out the specific problems of the generated video, and give the scoring or sorting results through COT;
% \item ~\textbf{\textit{human feedback datasets}}:We annotate the videos generated by the model. We not only score or rank the model, but also give specific reasons and provide richer natural language feedback, which can be used for more post-training tasks；
% \item ~\textbf{\textit{text-2-video Alignment Model}}:We explored and applied the DPO and KTO alignment algorithms in video generation.
% \item ~\textbf{\textit{Online training}}:We propose an online training framework that supports different offline alignment methods to be converted to online models, reducing model overfitting and over-optimization problems and significantly improving algorithm performance.
% \end{itemize}


\section{Introduction}
\label{sec:intro}

Text-to-video generation has drawn lots of attention due to its great potentials in movie products, video effects and user-generated contents, etc. Recent works~\cite{brooks2024video,kling,hailuo} introduce the Transformer architecture into diffusion models and improve the video generation quality by simply scaling up the parameter size, \emph{e.g.,} CogVideoX-5B~\cite{yang2024cogvideox}, Mochi-10B~\cite{genmo2024mochi} and HunyuanVideo-13B~\cite{kong2024hunyuanvideo}. Although achieving impressive performance, they still face challenges for generating user-satisfied videos with consistent subject, smooth motion and high-aesthetic picture. In addition, their large model sizes make them slow to produce videos. These drawbacks limit their applications in practice.

To align generation results with human preference, post-training techniques have shown their effectiveness in fields of Large Language Models (LLMs) and Text-to-Image models (T2Is). For instance, Direct Preference Optimization (DPO)~\cite{rafailov2024directpreferenceoptimizationlanguage} exploits pairwisely ranked data to make LLMs know the language style liked by human. While Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024ktomodelalignmentprospect} uses pointwisely binary data to reinforce T2Is to learn how to maximumly align the objective with human expectation. These techniques promote generative models to derive user-satisfied contents and give us inspiration for improving video generation models.

Thus, we propose to align Text-to-Video models (T2Vs) with human preference from the perspective of post-training in this paper. In particular, we present the Iterative Preference Optimization (IPO) framework for enhancing T2Vs. IPO introduces preference priors for finetuning T2Vs in the form of reinforcement learning, which considers subject consistency, motion smoothness and aesthetic quality, etc. This leads to improved video generations that are well aligned with human preference. In addition, as show in Fig.~\ref{fig:intro}, difference from prior single-round methods, IPO adopts a multi-round optimization strategy to reinforce T2Vs in an iterative manner, which strengths the models continuously. In this way, IPO can effectively improve T2Vs without need of large-scale dataset and expensive supervised finetuning.

%Specifically, 
%To address these issues, we propose an iterative reinforcement learning framework that combines human feedback and multi-round policy optimization to significantly enhance video generation models in terms of quality, consistency, and motion coherence. 
Specifically, IPO consists of three core parts as shown Fig.~\ref{fig:method_pipeline}: the preference dataset, the critic model and the iterative learning framework. IPO first collects a preference dataset for training the critic model. To get this dataset, IPO predefines several categories, including human, animal, action, etc. Then, IPO randomly combines these categories and uses LLM to extend them as prompts for generating videos with T2V models. Given these videos, IPO manually annotates them with two kinds of labels: one is pairwise ranking label that depicts preference comparison of two videos; another is pointwise scoring label that describes video quality as good or bad in a binary manner. After collecting the preference dataset, IPO uses it to train a critic model for justifying the quality of input videos. Concretely, IPO exploits Multi-modality Large Language Models (MLLMs) to achieve the above goal considering their powerful capability for recognizing video contents. In addition, IPO combines instruction-tuning to learn the critic model, making it able to generate pairwise and pointwise labels with different prompts. Given the critic model, IPO plays as a plugin for preference optimization of T2Vs without requirements of retraining or relabeling. For a specific T2V model, IPO can either use Diffusion-DPO~\cite{wallace2023diffusionmodelalignmentusing} with pairwise ranking data or Diffusion-KTO~\cite{li2024aligningdiffusionmodelsoptimizing} with pointwise scoring data. In addition, IPO adopts the critic model for automatically labeling videos from the reinforced T2Vs, which can be used as prior information for further improving T2Vs. In the iterative way, IPO can consistently improve T2Vs for generating high-quality videos.

Comprehensive experiments on benchmark VBench demonstrate that the proposed IPO can effectively and efficiently reinforce T2Vs to produce user-satisfied videos with consistent subject, smooth motion as well as high-aesthetic quality. In addition, given CogVideoX as the video foundation model, IPO can improve a 2B model to surpass the 5B counterpart. Our contributions can be summarise into three folds: First, we introduce a new preference optimization framework into text-to-video generation, which well align generations with human preference. Second, we present a novel iterative post-training framework for enhancing foundation models in a multi-round manner, differing from prior single-round methods. Third, we set new state-of-the-art on the benchmark VBench both in quantity and quality. To advance future researches in the area of video generation, we will release the preference dataset, the codes, the critic models as well as T2V models.

%We release annotated datasets, trained reward models, and human-aligned text-to-video generation models to advance future research and applications in the field.

%in our method consists of two key components: (1) a reward model trained on human-annotated data, incorporating both \textit{Pairwise} ranking and \textit{Pointwise} fine-grained scoring to comprehensively capture human preferences; and (2) iterative policy optimization using two strategies: \textit{Direct Preference Optimization} (DPO)~\cite{wallace2023diffusionmodelalignmentusing} for direct reward-based optimization, and \textit{Diffusion-KTO}~\cite{li2024aligningdiffusionmodelsoptimizing}, which leverages per-sample binary feedback signals (e.g., likes or dislikes) to align the model with human preferences without requiring a complex reward model. This lightweight optimization mechanism improves training efficiency significantly.

%and     Text-to-video generation has emerged as an important research area in recent years, with significant advancements driven by diffusion-based models. Several state-of-the-art open-source models, such as \textit{CogVideoX}~\cite{yang2024cogvideox}, \textit{HunyuanVideo}~\cite{kong2024hunyuanvideo},  \textit{SVD}~\cite{Blattmann2023svd}, \textit{Mochi}~\cite{genmo2024mochi}, \textit{Videocrafter}~\cite{chen2023videocrafter1} as well as commercial closed-source systems like \textit{Sora}~\cite{sora},\textit{Sora}~\cite{kling} , \textit{Hailuo}~\cite{hailuo}have demonstrated the potential of generating videos from textual input. Despite these advances, existing models still face substantial challenges in generating videos with high visual quality, temporal consistency, and coherent motion~\cite{lin2024opensoraplanopensourcelarge}. These limitations stem from the reliance on pretraining techniques, which are unable to align the models with human preferences effectively due to the lack of post-training optimization.

%In contrast, Reinforcement Learning with Human Feedback (RLHF)~\cite{Christiano_Leike_Brown_Martic_Legg_Amodei_2017}~\cite{Ziegler_Stiennon_Wu_Brown_Radford_Amodei_Christiano_Irving_2019}~\cite{ouyang2022traininglanguagemodelsfollow} has shown remarkable success in improving alignment with human expectations in other domains. For instance, RLHF has been widely adopted in large language models (LLMs) to refine generated text quality, and it has significantly enhanced text-to-image generation by aligning outputs with human preferences~\cite{clark2024directlyfinetuningdiffusionmodels}~\cite{prabhudesai2024aligningtexttoimagediffusionmodels}~\cite{fan2023dpokreinforcementlearningfinetuning}. These successes demonstrate the power of leveraging human feedback to guide complex generation models. However, despite its potential, RLHF has yet to be explored in text-to-video generation. This gap presents an exciting opportunity to enhance video generation by integrating human preferences into the training pipeline.

%While some works have attempted to leverage Offline Reinforcement Learning (Offline RL) to optimize video generation models, several limitations remain: (1) Offline RL relies on static datasets and lacks the capability for active exploration of novel strategies; (2) it struggles to capture diverse and complex human preferences, particularly in multidimensional generation tasks; (3) policy optimization is prone to distributional shifts, leading to unstable performance on unseen data; (4) training efficiency is often reduced due to the need for additional constraints or regularization to mitigate distributional shifts.



%Our contributions are summarized as follows. First, we introduce a reward model training framework that supports both \textit{Pairwise} ranking and \textit{Pointwise} scoring, effectively enhancing the model's ability to represent human preferences comprehensively. Second, we develop an iterative reinforcement learning framework that supports two optimization methods: DPO (Direct Preference Optimization) and Diffusion-KTO. These methods offer flexibility in aligning models with human preferences, with DPO relying on a reward model and Diffusion-KTO leveraging lightweight binary feedback signals without requiring a complex reward model. Third, our optimized model achieves significant performance improvements on the VBench benchmark, outperforming all comparable models of the same scale, with our 2B parameter model even surpassing the performance of 5B parameter models. Finally, we release annotated datasets, trained reward models, and human-aligned text-to-video generation models to advance future research and applications in the field.