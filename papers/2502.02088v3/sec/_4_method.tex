\section{Implementation details}
\label{sec:detail} 
% Although RLHF methods have been successful in fine-tuning language models, adapting them to diffusion models has been challenging based on the fact that sampling from diffusion models is inefficient and training diffusion models is expensive. 
% Therefore, our goal is to derive a simple yet effective approach that can directly optimize diffusion models to align with human preferences.
% 

% \se{i think you need to start by sating that's the setting. you have a reference diffusion model, you have a dataset of prompt,image1,image2 and annotations on the winning one, and the goal is to align the model to the human preferences}

In this section, we present our iterative preference optimization framework, along with its implementation details and pipeline. An overview of the algorithm is illustrated in Fig. \ref{fig:Figure_pipeline}. Specifically, the data collection and annotation process are described in detail in Section \ref{collect dataset}, while Section \ref{reward_training} elaborates on the training methodology for the Critic model. Finally, Section \ref{pipeline_training} introduces a comprehensive framework for aligning human feedback, encompassing the training of DPO on paired data and KTO on single data.
% \textcolor{green}{



% In order to obtain a comprehensive model that can handle paired data sorting and single video scoring, we fine-tune the pre-trained model Vila\cite{lin2023vila}, which already has strong image and video instruction following capabilities, comprehension capabilities, and multi-image processing capabilities, which are essential for training our reward model. During training, restructure the prompts and add detailed reasons before scoring point by point or ranking in pairs. Construct a chain of thoughts. After training, the model will give detailed reasons before giving an answer, improving the accuracy of the final result while providing richer natural language feedback.

% In our experiments, we start from the Vila 13B/40B pre-trained checkpoint and fine-tune it for 5 epochs on the labeled dataset to train the reward model. We use a learning rate of 2e-5, 32 GPUs, and a batch size of 4 per GPU. In addition, to reduce the impact of scaling and center cropping preprocessing on the consistency of text and video, we use pad training. In order to enhance the understanding of video actions, 16 frames are extracted from each video. Other parameters are the same as Vila default settings.

\subsection{Iterative training pipeline} \label{pipeline_training}


% \begin{algorithm}[t]
% \small
% \DontPrintSemicolon
%     \KwInput{Prompt Set $\mathcal{M} = \{x_0,\cdots,x_n \}$, Video Diffusion Model $\mathcal{G}(\cdot)$, Video Reward Model $\mathcal{R}(\cdot)$, Curriculum Update Interval $K$} 
%    \KwOutput{Preference-aligned Video Diffusion Model $\mathcal{G}^*(\cdot)$} 
%     $\mathrm{step} = 0$ \\
%     \For{$x_i$ \text{in} $\mathcal{M}$}{
%         \textcolor{commentGreen}{// Online Preference Sample Generation}
%         $\mathcal{V} = \{y_1, y_2, \cdots, y_N \} \sim \mathcal{G}(x_i)$ 
%         $\mathcal{S} = \{\mathcal{R}(y_1), \mathcal{R}(y_2), \cdots, \mathcal{R}(y_N) \}$
%         $\tilde{y}_{w} = \mathcal{V}_{max_{i} \mathcal{R}(y_i)}; $
%         $\tilde{y}_{l} = \mathcal{V}_{min_{i} \mathcal{R}(y_i)}$\\
%         $\mathcal{L}_{\textrm{DPO}} = \mathbb{E}\left[\log \sigma\left(\beta \log \frac{\mathcal{G}_{\theta}\left(\tilde{y}_{w} \mid x\right)}{\mathcal{G}_{\mathrm{ref}\left(\tilde{y}_{w} \mid x\right)}}-\beta\log \frac{\mathcal{G}_{\theta}\left(\tilde{y}_{l} \mid x\right)}{\mathcal{G}_{\mathrm{ref}}\left(\tilde{y}_{l} \mid x\right)}\right)\right]$

%         \textcolor{commentGreen}{// Update $\mathcal{G}_{\theta}$ with DPO loss}
    
%         $\mathcal{G}_{\theta} \leftarrow \mathcal{G}_{\theta} + \nabla_{\mathcal{G}_{\theta}}\mathcal{L}_{\mathrm{DPO}} $ 

%         $\mathrm{step} = \mathrm{step} + 1$

%         \textcolor{commentGreen}{// Curriculum Preference Update}
        
%         \If{$(\mathrm{step}$ $\bmod$ $K)$ $=$ 0}{
%          $\mathcal{G}_{\mathrm{ref}} = \mathcal{G}_{\theta}$

%         }
       
%     }

% \caption{Online Video Preference Optimization (OnlineVPO). }
% \label{algo:online_preferecence}
% \end{algorithm}
% \vspace{-0.2cm}

Iterative preference optimization provides substantial advantages over offline learning methods. The iterative learning framework we propose is a \textbf{versatile} and \textbf{general-purpose} approach, capable of seamlessly integrating various offline learning algorithms into an iterative setting, thereby achieving significant performance enhancements. Specifically, we investigate two distinct offline preference alignment methods: the \textbf{KTO algorithm}, which utilizes pointwise scoring, and the \textbf{DPO algorithm}, which is based on pairwise ranking.

\paragraph{Iterative DPO} In the iterative preference optimization process employing Direct Preference Optimization (DPO), the system follows a structured workflow: during each iteration, the current optimal policy model generates multiple video outputs for a given prompt. These generated videos are subsequently evaluated and ranked by a critic model based on predefined quality metrics. The ranked outputs then serve as training data for preference alignment, where the policy model is fine-tuned to better align with the desired preferences. This cyclic process of generation, evaluation, and optimization continues iteratively, progressively enhancing the model's performance through successive refinement cycles.
\begin{itemize}
\setlength{\itemsep}{2pt}
\item \textit{\textbf{Video Generation}.} We use CogvideoX-2B as the initial checkpoint, and generate at least three random videos for the same prompt by setting different seeds to to form a dataset $\mathcal{V}$ = $\{(x_k, y_{k_1}, y_{k_2}, y_{k_3},...)\}_{k=1}^N$,  where $x_k$ represents the prompt and $y_k$ represents the corresponding video generated by the model. 

\item \textit{\textbf{Pairwise Ranking}}.
To enhance ranking reliability and mitigate potential biases in the Critic model, we implement a robust pairwise comparison protocol with position swapping during inference: for each video pair, we conduct two reciprocal evaluations by alternating their presentation order, retaining only consistent results as valid entries for the preference dataset. Formally, we construct the pairwise dataset as $\mathcal{D} = \{(x_k, y_{k_w}, y_{k_l})\}_{k=1}^N$, where $y_{k_w}$ and $y_{k_l}$ denote the preferred (winning) and dispreferred (losing) samples, respectively. This rigorous validation process ensures higher data quality and reduces positional bias in the collected preference pairs.

\item  \textit{\textbf{Preference learning}.} 
During training, we employ a selective sampling strategy to construct high-quality preference pairs by extracting only the highest-ranked and lowest-ranked samples from the critic model's evaluations, forming the final paired data $(y^{w}, y^{l})$ while deliberately excluding intermediate-ranked samples to ensure clearer preference distinctions. For optimization, we utilize the diffusion-DPO loss as the primary objective function and introduce two auxiliary negative log-likelihood (NLL) loss terms following ~\cite{pang2024iterativereasoningpreferenceoptimization}: the first term, weighted by 0.2, regularizes the preferred samples, while the second term, scaled by 0.1, anchors the model to real video data distributions. This dual regularization strategy preserves the structural integrity and format of generated outputs while preventing undesirable degradation in the log-probability of selected responses, as demonstrated in ~\cite{pal2024smaugfixingfailuremodes}, ~\cite{grattafiori2024llama3herdmodels}, and ~\cite{pang2024iterativereasoningpreferenceoptimization}.
% \begin{equation}
% \begin{aligned}
% \mathcal{L}(\theta) = L_\text{DPO-Diffusion}(\theta) \\
% &+ 
% \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{sample}}}
% \left[-\log p_{\theta}(y^w|x)\right] \\
% &+
% \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}}
% \left[-\log p_{\theta}(y|x)\right]ï¼Œ
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) &= L_{diffusion-dpo}(\theta) \\
&\quad + \lambda_1 \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{sample}}} \bigl[-\log p_{\theta}(y^w|x)\bigr] \\
&\quad + \lambda_2 \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}} \bigl[-\log p_{\theta}(y|x)\bigr],
\end{aligned}
\end{equation}
\end{itemize}

\paragraph{Iterative KTO} KTO~\cite{ethayarajh2024ktomodelalignmentprospect} is an offline learning method that leverages pointwise scoring data, eliminating the need for preference-based datasets. By directly utilizing binary-labeled data, KTO effectively trains algorithms while demonstrating heightened sensitivity to negative samples. This makes it particularly adept at handling imbalanced datasets with uneven distributions of positive and negative samples. Furthermore, KTO achieves significant performance improvements even without relying on the SFT stage. In scenarios where data is imbalanced, paired sorting data is unavailable, or preference data is noisy, KTO stands out as a more effective and robust solution.
\begin{itemize}
\setlength{\itemsep}{2pt}
\item ~\textbf{\textit{Video Generation}.} Similar to the DPO training framework, we adopt an identical video sampling strategy in this study. In alignment with the KTO~\cite{ethayarajh2024ktomodelalignmentprospect} methodology, empirical evidence suggests that sampling multiple videos for a given prompt yields superior results compared to single-video sampling. Therefore, we maintain consistency with the DPO sampling protocol throughout our implementation.

\item \textit{\textbf{Pointwise scoring}}.
The Critic model is employed to evaluate and assign scores to all generated text-video pairs 
$(y, x)$. Each data sample is categorized into three distinct quality levels: "Good", "Normal", and "Bad". Based on this classification, we designate samples labeled as "Good" and "Normal" as positive instances, while those marked as "Bad" are treated as negative instances for subsequent analysis and model training.
% The reward model is used to score all the generated text and video data $(y, x)$. Each sample has three different scores: "Good", "Normal" and "Bad". For "Good" and "Normal", we label them as positive samples, and "Bad" as negative samples.

\item  \textit{\textbf{Preference learning}.} 
We use the Eq.~\ref{eq:dkto_loss} for training. KTO needs to estimate the average reward Q during training. When the batch size is small, the Q value is not accurately estimated and the model training will be unstable. We set the batch size to 128 and achieved good results. To stabilize the training, we also sampled data from the real data VidGen-1M~\cite{tan2024vidgen1mlargescaledatasettexttovideo} and added negative log-likelihood loss.
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) &= L_{diffusion-kto}(\theta) \\
&\quad + \lambda \cdot \mathbb{E}_{(\textbf{\textit{y}}, x) \sim \mathcal{D}^{\mathrm{real}}} \bigl[-\log p_{\theta}(y|x)\bigr],
\end{aligned}
\end{equation}

\end{itemize}

Both DPO and KTO employ an iterative learning framework that cyclically executes three core steps: (1) strategic data sampling, (2) model-based preference labeling, and (3) alignment optimization. The process incorporates early stopping based on validation metrics while adaptively optimizing strategies for each component to maximize training efficiency and model performance.