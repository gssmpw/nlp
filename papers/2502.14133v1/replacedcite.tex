\section{Related Works}
Our work builds on research in LLM interpretability and classifier regularization. Efforts to explain LLMs have leveraged activation monitoring____, probing____, and basis decomposition____ to address neuron polysemanticity____. Recent advances use sparse autoencoders____ to extract monosemantic features, improving interpretability and enabling control over model behaviors, with applications extending to translation____, circuit detection____, and scaling to larger LLMs____. We extend this line by fine-tuning sparse autoencoders on domain data to mitigate dead neurons and enhance task-specific feature reconstruction. In classifier regularization, traditional methods like weight decay____ and dropout____ prevent overfitting, while modern approaches such as spectral normalization____ and focal loss____ refine model generalization. Our work differs by leveraging sparse autoencoders to achieve semantical regularization on embedding-based classifier training. 
Appendix~\ref{appd:related_work} provides more discussions on related works.