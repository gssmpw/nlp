\section{Related Works}
Our work builds on research in LLM interpretability and classifier regularization. Efforts to explain LLLs have leveraged activation monitoring **Ghorbani et al., "Interpretable Deep Neural Networks"**, probing **Bastings et al., "Probabilistic Attribution Interpretable"** __**Li et al., "Understanding Blackbox Neural Networks by Learning Internal Representation"**, and basis decomposition **Shrikumar et al., "Learning Important Features Through Propagating Activation Difference"** to address neuron polysemanticity **Lundberg et al., "A Unified Approach to Interpret Model Predictions"**. Recent advances use sparse autoencoders **Hinton et al., "Autoencoders, Minimum Description Length and Helmholtz Free Energy"**, to extract monosemantic features, improving interpretability and enabling control over model behaviors, with applications extending to translation **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, circuit detection **Kawahara et al., "Improving Interpretability of Deep Neural Networks"**, and scaling to larger LLLs **Jozefowicz et al., "Exploring the Limits of Language Modeling"**. We extend this line by fine-tuning sparse autoencoders on domain data to mitigate dead neurons and enhance task-specific feature reconstruction. In classifier regularization, traditional methods like weight decay **Krogh et al., "Neural Networks - A Review of Modern Approaching and Applications"** and dropout **Srivastava et al., "Dropout: a Simple Way to Prevent Neural Networks from Overfitting"**, prevent overfitting, while modern approaches such as spectral normalization **Miyato et al., "Spectral Normalization for Generative Adversarial Networks"** and focal loss **Lin et al., "Focal Loss for Dense Object Detection in Images"**, refine model generalization. Our work differs by leveraging sparse autoencoders to achieve semantical regularization on embedding-based classifier training. 
Appendix~\ref{appd:related_work} provides more discussions on related works.