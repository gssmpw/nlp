\section{Related Works}
Our work builds on research in LLM interpretability and classifier regularization. Efforts to explain LLMs have leveraged activation monitoring~\citep{wang2023label}, probing~\citep{belinkov2018evaluating,jawahar2019does,rogers2021primer}, and basis decomposition~\citep{elhage2022toy,olah2020zoom} to address neuron polysemanticity~\cite{arora2018linear}. Recent advances use sparse autoencoders~\citep{brickentowards,cunningham2023sparse} to extract monosemantic features, improving interpretability and enabling control over model behaviors, with applications extending to translation~\citep{dumas2024llamas}, circuit detection~\citep{marks2024sparse}, and scaling to larger LLMs~\citep{templeton2024scaling,gao2024scaling,lieberum2024gemma}. We extend this line by fine-tuning sparse autoencoders on domain data to mitigate dead neurons and enhance task-specific feature reconstruction. In classifier regularization, traditional methods like weight decay~\citep{hoerl1970ridge} and dropout~\citep{srivastava2014dropout} prevent overfitting, while modern approaches such as spectral normalization~\citep{yoshida2017spectral} and focal loss~\citep{ross2017focal} refine model generalization. Our work differs by leveraging sparse autoencoders to achieve semantical regularization on embedding-based classifier training. 
Appendix~\ref{appd:related_work} provides more discussions on related works.