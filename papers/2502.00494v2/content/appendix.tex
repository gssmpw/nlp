\newpage
\appendix
\onecolumn





\section{Experimental Setup}
\label{appendix:experiments}

\begin{table}[t]
	\caption{Settings of three CML scenarios.}
	\label{tab:settings}
	\small
	\begin{tabular}{llc|cc|cc|cc}
		\hline
		\multicolumn{1}{c}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}CML\\ type\end{tabular}}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}CML \\ algorithm\end{tabular}}}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{Client 1}} & \multicolumn{2}{c|}{\textbf{Client 2}} & \multicolumn{2}{c}{\textbf{Client 3}} \\ \cline{4-9} 
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} &  & \textbf{data blocks} & \textbf{\begin{tabular}[c]{@{}c@{}}data\\ size\end{tabular}} & \textbf{data blocks} & \textbf{\begin{tabular}[c]{@{}c@{}}data\\ size\end{tabular}} & \textbf{data blocks} & \textbf{\begin{tabular}[c]{@{}c@{}}data\\ size\end{tabular}} \\ \hline
		HFL & FedAVG & \begin{tabular}[c]{@{}c@{}}Apartments\\ for\\ Rent\end{tabular} & \begin{tabular}[c]{@{}c@{}}(1) Northeast,\\ (2) Mid-Atlantic,\\ (3) Southeast,\\ (4) East Central\end{tabular} & 4215 & \begin{tabular}[c]{@{}c@{}}(1) Great Lakes,\\ (2) Midwest Palins,\\ (3) Central Plains,\\ (4) Southern Plains\end{tabular} & 1722 & \begin{tabular}[c]{@{}c@{}}(1) Mountain States,\\ (2) Northwest,\\ (3) Pacific Coast,\\ (4) Southwest\end{tabular} & 2063 \\
		VFL & SplitNN & \begin{tabular}[c]{@{}c@{}}Bank \\ Marketing\end{tabular} & \begin{tabular}[c]{@{}c@{}}(1) age,\\ (2) job,\\ (3) marital,\\ (4) education\end{tabular} & 8000 & \begin{tabular}[c]{@{}c@{}}(1) default,\\ (2) balance,\\ (3) housing,\\ (4) loan\end{tabular} & 8000 & \begin{tabular}[c]{@{}c@{}}(1) contact, \\ (2) day\_of\_week,\\ month\\ (3) campaign,\\ pdays, previous\\ (4) poutcome\end{tabular} & 8000 \\
		HyFL & FedMD & CDC & \begin{tabular}[c]{@{}c@{}}(1) Sex, Age,\\ Education, Income,\\ AnyHealthcare,\\ NoDocbcCost,\\ (2) Smoker,\\ HvyAlcoholConsump,\\ (3) PhysActivity,\\ DiffWalk,\\ (4) Fruits, Veggies\end{tabular} & 2666 & \begin{tabular}[c]{@{}c@{}}(1) Sex, Age,\\ Education, Income,\\ AnyHealthcare,\\ NoDocbcCost,\\ (2) highBP, highChol,\\ (3) CholCheck,\\ (4) Stroke,\\ HeartDiseaseorAttack\end{tabular} & 2666 & \begin{tabular}[c]{@{}c@{}}(1) Sex, Age,\\ Education, Income,\\ AnyHealthcare,\\ NoDocbcCost,\\ (2) GenHlth,\\ MentHlth,\\ PhysHlth\\ (3) BMI\end{tabular} & 2666 \\ \hline
	\end{tabular}
\end{table}

% \textbf{Datasets, CML algorithms \& models.}
As shown in Table \ref{tab:settings}, we perform data valuation in the following three CML scenarios.
\begin{itemize}[leftmargin=*]
	\item Horizontal federated learning (HFL): In HFL, clients possess data blocks with the same feature space but different sample spaces. In this scenario, we use the \textit{Apartments for Rent} dataset~\citep{apartment_for_rent_classified_555}, which contains rental data from various states. 
	We assume three regional rental companies as clients and divide the dataset into 12 data blocks based on the geographic regions of the apartments in the US, assigning these blocks to the clients. 
	These clients utilize the FedAVG algorithm~\citep{mcmahan2017communication} to perform HFL, collaboratively training a multilayer perceptron (MLP) to predict rental prices.
	\item Vertical federated learning (VFL): In VFL, clients possess different features of the same samples. 
	We use the \textit{Bank Marketing} dataset~\citep{bank_marketing_222} and partition its features into 12 data blocks based on their content, assigning them to three finance-related companies as clients.
	These clients then perform VFL using the split learning algorithm~\cite{gupta2018distributed} to train a SplitNN-based binary classification model for target customer detection.
	\item Hybrid federated learning (HyFL): HyFL allows clients to have data with both different sample spaces and feature spaces. 
	We consider three medical institutions as clients, each with a non-overlapping patient group and distinct diagnostic tests. 
	They own 11 data blocks from the \textit{CDC} medical dataset~\citep{cdc_health_indicators}, where each data block contains patient data related to a specific diagnostic test at a particular institution. 
	These institutions use the FedMD algorithm~\citep{li2019fedmd} to collaboratively train an MLP-based ensemble model for diabetes prediction.
\end{itemize}


\section{Proofs}

\begin{proof}[Proof of Lemma \ref{lem:data_value_form}]
	For every data subset $\datasubset \subseteq \granddataset$, we define the basis game $\delta_\datasubset(T) =$ by 
	\begin{align*}
		\delta_\datasubset(T) =
		\begin{cases}
			1, & T=S, \\
			0, & T\neq S.
		\end{cases}
	\end{align*}
	The set $\{\delta_\datasubset \mid \datasubset \subseteq \granddataset\}$ is a natural basis for $\mathcal{G}(\granddataset)$.
	Then, consider a linear data valuation metric $\phi(\granddataset, v)$.
	For any two utility functions $v_1$ and $v_2$ and scalars $w_1, w_2 \in \mathbb{R}$, we have:
	\begin{align*}
		\phi_i(\granddataset, w_1v_1+w_2v_2) = w_1\phi_i(\granddataset, v_1) + w_2\phi_i(\granddataset, v_2).
	\end{align*}
	Therefore, $\phi_i(\granddataset, \cdot): \mathcal{G}(\granddataset) \rightarrow \mathbb{R}$ is a linear functional on the vector space $\mathcal{G}(\granddataset)$.
	Then, because any utility function $v\in \mathcal{G}(\granddataset)$ can be written as $v=\sum_{\datasubset \subseteq \granddataset} v(\datasubset)\cdot \delta_\datasubset$, we have:
	\begin{align*}
		\phi_i(\granddataset, v) = \phi_i(\granddataset, \sum_{\datasubset \subseteq \granddataset} v(\datasubset)\cdot \delta_\datasubset) = \sum_{\datasubset \subseteq \granddataset} v(\datasubset)\cdot \phi_i(\granddataset, \delta_\datasubset).
	\end{align*}
	By defining $\beta_i(\datasubset) \coloneqq \phi_i(\granddataset, \delta_\datasubset)$, we conclude that Lemma \ref{lem:data_value_form} is true.
	
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:attack_success}]
	According to Definition \ref{def:overvaluation}, under a data overvaluation attack, we have
	\begin{align*}
		& \empiricaldatavalue_{i}(\granddataset, v) = \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_i(\datasubset) \cdot v(\reportedsubset) \\
		= & \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_i(\datasubset) \cdot v(\reportedsubset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) > 0} \beta_i(\datasubset) \cdot v(\reportedsubset) \\
		& + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) > 0} \beta_i(\datasubset) \cdot v(\reportedsubset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_i(\datasubset) \cdot v(\reportedsubset)\\
		\geq & \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_i(\datasubset) \cdot v(\stoi\cup \reportedstominusi) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) > 0} \beta_i(\datasubset) \cdot v(\stoi\cup \reportedstominusi) \\
		& + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) > 0} \beta_i(\datasubset) \cdot v(\stoi\cup \reportedstominusi) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_i(\datasubset) \cdot v(\stoi\cup \reportedstominusi)\\
		= & \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_i(\datasubset) \cdot v(\stoi \cup \reportedstominusi) = \empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi).
	\end{align*}
	Similarly, under a data overvaluation attack, we have
	\begin{align*}
		& \empiricaldatavalue_{-i}(\granddataset, v) = \beta_{-i}(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_{-i}(\datasubset) \cdot v(\reportedsubset) \\
		= & \beta_{-i}(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_{-i}(\datasubset) \cdot v(\reportedsubset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) > 0} \beta_{-i}(\datasubset) \cdot v(\reportedsubset) \\
		& + \sum_{\datasubset\subset \granddataset, \beta_{i}(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) > 0} \beta_{-i}(\datasubset) \cdot v(\reportedsubset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_{-i}(\datasubset) \cdot v(\reportedsubset)\\
		\leq & \beta_{-i}(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_{-i}(\datasubset) \cdot v(\stoi\cup \reportedstominusi) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) > 0} \beta_{-i}(\datasubset) \cdot v(\stoi\cup \reportedstominusi) \\
		& + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) > 0, \beta_{-i}(\mathcal{S}) > 0} \beta_{-i}(\datasubset) \cdot v(\stoi\cup \reportedstominusi) + \sum_{\datasubset\subset \granddataset, \beta_i(\mathcal{S}) \leq 0, \beta_{-i}(\mathcal{S}) \leq 0} \beta_{-i}(\datasubset) \cdot v(\stoi\cup \reportedstominusi)\\
		= & \beta_{-i}(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_{-i}(\datasubset) \cdot v(\stoi \cup \reportedstominusi) = \empiricaldatavalue_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi).
	\end{align*}
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:characterization1}]
	$\Rightarrow$: Under Assumption \ref{ass:optimal_dataset}, for any game $(\granddataset, v)$, for any client $i$, and for any reported data subsets $\{\reportedstoi\mid \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset) \}$, we have
	\begin{align*}
		&\mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v)] = \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)]\\
		= & \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] +\sum_{\datasubset\subset \granddataset, \stoi \neq D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] \\
		= & \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] \\
		\leq &\beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)] \\
		= &\beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)] +\sum_{\datasubset\subset \granddataset, \stoi \neq D_i} \beta_i(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)]\\
		= & \mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi = \stoi)].
	\end{align*}
	Similarly, under Assumption \ref{ass:optimal_dataset}, for any game $(\granddataset, v)$, for any client $i$, and for any reported data subsets $\{\reportedstoi\mid \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset) \}$, we have
	\begin{align*}
		&\mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v)] = \beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)]\\
		= & \beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] +\sum_{\datasubset\subset \granddataset, \stoi \neq D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] \\
		= & \beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\reportedstoi \cup \reportedstominusi)] \\
		\geq &\beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)] \\
		= &\beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\datasubset\subset \granddataset, \stoi = D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)] +\sum_{\datasubset\subset \granddataset, \stoi \neq D_i} \beta_{-i}(\datasubset) \cdot\! \mathbb{E}[v(\stoi \cup \reportedstominusi)]\\
		= & \mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi = \stoi)].
	\end{align*}
	$\Leftarrow$: If $\exists \datasubset \subset \granddataset$ such that if $\stoi = D_i$, we have $\beta_i(\datasubset) < 0$, or such that if $\stoi \neq D_i$, we have $\beta_i(\datasubset) \neq 0$, we can always construct a utility function $v$ such that $\mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v)] > \mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi = \stoi)]$. 
	Also, if $\exists \datasubset \subset \granddataset$ such that if $\stoi = D_i$, we have $\beta_{-i}(\datasubset) > 0$, or such that if $\stoi \neq D_i$, we have $\beta_{-i}(\datasubset) \neq 0$, we can always construct a utility function $v$ such that $\mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v)] < \mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi = \stoi)]$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:characterization2}]
	$\Rightarrow$: Under Assumption \ref{ass:optimal_dataset}, for any game $(\granddataset, v)$, for any client $i$, and for any reported data subsets $\{\reportedstoi\mid \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset) \}$, we have
	\begin{align*}
		&\mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v)] = \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\mathcal{C}\subset \mathbb{N}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})]\\
		= & \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\mathcal{C}\subset \mathbb{N}, i\in \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})] +\sum_{\mathcal{C}\subset \mathbb{N}, i\notin \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})] \\
		\leq & \beta_i(\granddataset) \cdot v(\granddataset) +\sum_{\mathcal{C}\subset \mathbb{N}, i\in \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(D_i \cup (\cup_{i'\in \mathcal{C} \setminus \{i\}} \widehat{D}_{i'}))] +\sum_{\mathcal{C}\subset \mathbb{N}, i\notin \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})]\\
		= & \mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v \!\mid\! \forall \datasubset \!\subset \!\granddataset,\! \reportedstoi\! =  \!\stoi)]
	\end{align*}
	Because $\phi$ is efficient, we have $\beta_{-i}(D_{\mathcal{C}}) = - \beta_{i}(D_{\mathcal{C}})$ for all $\mathcal{C} \subset \mathbb{N}$.
	Therefore, under Assumption \ref{ass:optimal_dataset}, for any game $(\granddataset, v)$, for any client $i$, and for any reported data subsets $\{\reportedstoi\mid \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset) \}$, we have
	\begin{align*}
		&\mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v)] = \beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\mathcal{C}\subset \mathbb{N}} \beta_{-i}(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})]\\
		= & -\beta_i(\granddataset) \cdot v(\granddataset) -\sum_{\mathcal{C}\subset \mathbb{N}, i\in \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})] -\sum_{\mathcal{C}\subset \mathbb{N}, i\notin \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})] \\
		\geq & -\beta_i(\granddataset) \cdot v(\granddataset) -\sum_{\mathcal{C}\subset \mathbb{N}, i\in \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(D_i \cup (\cup_{i'\in \mathcal{C} \setminus \{i\}} \widehat{D}_{i'}))] -\sum_{\mathcal{C}\subset \mathbb{N}, i\notin \mathcal{C}} \beta_i(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})]\\
		= & \beta_{-i}(\granddataset) \cdot v(\granddataset) +\sum_{\mathcal{C}\subset \mathbb{N}, i\in \mathcal{C}} \beta_{-i}(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(D_i \cup (\cup_{i'\in \mathcal{C} \setminus \{i\}} \widehat{D}_{i'}))] +\sum_{\mathcal{C}\subset \mathbb{N}, i\notin \mathcal{C}} \beta_{-i}(D_{\mathcal{C}}) \cdot\! \mathbb{E}[v(\widehat{D}_{\mathcal{C}})]\\
		= & \mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v \!\mid\! \forall \datasubset \!\subset \!\granddataset,\! \reportedstoi\! =  \!\stoi)]
	\end{align*}
	$\Leftarrow$: According to Theorem \ref{thm:characterization1}, if a data valuation metric $\phi$ is linear and BIC, we have
	\begin{align*}
		\phi_i(\granddataset, v) = \sum_{\datasubset \subseteq \granddataset} \beta_i(\datasubset) \cdot v(\datasubset) = \sum_{\datasubset \subseteq \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot v(\datasubset)
	\end{align*}
	Then, because $\phi$ is efficient, we have 
	\begin{align*}
		&\phi_i(\granddataset, v) = \sum_{\datasubset \subseteq \granddataset, \stoi = D_i} \beta_i(\datasubset) \cdot v(\datasubset) = \sum_{\mathcal{C} \subseteq \mathbb{N}} \beta_i(D_{\mathcal{C}}) \cdot v(D_{\mathcal{C}}) + \sum_{\datasubset \subseteq \granddataset, \stoi = D_i, \exists i' \in \mathbb{N}(\datasubset)\setminus \{i\}, D^{\datasubset}_{i'} \neq D_{i'}} \beta_i(\datasubset) \cdot v(\datasubset) \\
		= &\sum_{\mathcal{C} \subseteq \mathbb{N}} \beta_i(D_{\mathcal{C}}) \cdot v(D_{\mathcal{C}}) - \sum_{\datasubset \subseteq \granddataset, \stoi = D_i, \exists i' \in \mathbb{N}(\datasubset)\setminus \{i\}, D^{\datasubset}_{i'} \neq D_{i'}} \beta_{-i}(\datasubset) \cdot v(\datasubset) = \sum_{\mathcal{C} \subseteq \mathbb{N}} \beta_i(D_{\mathcal{C}}) \cdot v(D_{\mathcal{C}}),
	\end{align*}
	where $\beta_i(D_{\mathcal{C}})$ must be non-negative for all $\forall \mathcal{C} \subset \mathbb{N}$ with $i\in \mathcal{C}$ to ensure BIC.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:tsv_bic}]
	The client-level TSV can be written as:
	\begin{align*}
		& \tsv_i(\granddataset, v) = \sum_{\mathcal{C} \subseteq \mathbb{N} \setminus \{i\}} \weightsv(\mathcal{C} \mid \mathbb{N})\big(v(\clientleveldatasubset \cup \{D_i\}) - v(\clientleveldatasubset)\big) =  \sum_{\mathcal{C} \subseteq \mathbb{N}} \beta^{TSV}_i(D_\mathcal{C}) \cdot v(D_\mathcal{C}),
	\end{align*}
	where $\beta^{TSV}_i(D_\mathcal{C}) = \begin{cases}
		\weightsv(\mathcal{C} \mid \mathbb{N}), & i\notin \mathcal{C},\\
		- \weightsv(\mathcal{C} \mid \mathbb{N}), & i\in \mathcal{C}.
	\end{cases}$. 
	Because $\beta^{TSV}_i(D_\mathcal{C}) \geq 0$ for all $\mathcal{C} \subset \mathbb{N}$ with $i\in \mathcal{C}$, according to Theorem \ref{thm:characterization2}, $\tsv$ satisfies BIC.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:tsv_unique}]
	Because $\tsv_i(\granddataset, v) = \sv_i(\clientlevelgranddataset, v)$, according to Theorem \ref{thm:sv_unique}, $\tsv_i$ uniquely satisfies LIN, DUM-C, SYM-C and EFF-C, where EFF-C means $\sum_{i\in \mathbb{N}} \phi_i(\granddataset, v) = v(\granddataset)$.
	Then, because $\tsv_{i,j}(\granddataset, v) = \sv_j(D_i, v^{\tsv_i})$, according to Theorem \ref{thm:sv_unique}, $\tsv_{i,j}$ uniquely satisfies LIN, DUM-IB, SYM-IB and EFF-IB, where EFF-IB means $\forall i\in \mathbb{N}, \sum_{j\in [M_i]} \phi_{i,j}(\granddataset, v) = \phi_i(\granddataset, v)$.
	Therefore, $\tsv$ uniquely satisfies EFF, LIN, DUM-C, DUM-IB, SYM-C, and SYM-IB.
\end{proof}