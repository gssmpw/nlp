

\section{Truthful Data Valuation for CML}
In this section, we first characterize the subclass of data valuation metrics that can prevent the data overvaluation attack and then select a special metric from this class, named \textit{Truth-Shapley} (Truthful Shapley value). 

\subsection{Characterization of Truthful Data Valuation}
From Definition \ref{def:overvaluation}, we know that the issue of data overvaluation arises from strategic clients untruthfully reporting their data subset $\reportedstoi$, which is highly analogous to the problem of untruthful bidding in auctions. 
Specifically, for each client $i$, if we regard their reported data $\reportedstoi, \forall \datasubset \subset \granddataset$ as their bid and the empirical data value $\empiricaldatavalue_i$ as their payoff, the problem of preventing data overvaluation can be viewed as ensuring a truthful auction. 

\begin{definition}[Bayesian Incentive Compatibility for Truthful Data Valuation]
\label{def:truthfulness}
    A data valuation metric $\phi$ is Bayesian incentive compatible (BIC) if for any game $(\granddataset, v)$, for any client $i$, and for any reported data subsets $\{\reportedstoi\mid \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset) \}$, we have
    \begin{align}
    \label{formula:expected_data_value_i}
        &\mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset}[\empiricaldatavalue_{i}(\granddataset, v)] \leq \\
        \label{formula:expected_data_value_i_max}
         & \mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset}[\empiricaldatavalue_{i}(\granddataset, v \!\mid\! \forall \datasubset \!\subset \!\granddataset,\! \reportedstoi\! =  \!\stoi)],\\
        \label{formula:expected_data_value_minusi}
        &\mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset}[\empiricaldatavalue_{-i}(\granddataset, v)] \geq\\
        \label{formula:expected_data_value_minusi_max}
         & \medmath{\mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset}[\empiricaldatavalue_{-i}(\granddataset, v \!\mid \!\forall \datasubset \subset \granddataset, \reportedstoi \!= \! \stoi)]},
    \end{align}
    where $\sigma_i(\cdot \mid \datasubset)$ denotes the distribution of $\reportedstominusi$ estimated by client $i$ in their belief.
\end{definition}

Accordingly, we draw on the concept of \textit{Bayesian incentive compatibility} (BIC) from auction theory~\cite{d1982bayesian}, defined in Definition \ref{def:truthfulness}, to ensure truthful data valuation.
Intuitively, BIC ensures that for each client $i$, truthfully reporting their data $\reportedstoi =  \stoi$ is the optimal strategy that not only maximizes their expected data value in Formula (\ref{formula:expected_data_value_i}) but also minimizes the sum of the other clients' data values in Formula (\ref{formula:expected_data_value_minusi}). 
Note that the expected data values in Formulas (\ref{formula:expected_data_value_i})-(\ref{formula:expected_data_value_minusi_max}) are based on client 
$i$’s prior beliefs $\{\sigma_i(\cdot \mid \datasubset)\}_{\forall \datasubset \subset \granddataset}$ about the other clients' reported data. 
This implies that truthful reporting is subjectively optimal based on their beliefs, rather than objectively optimal.
For simplicity, we will omit the conditional subscript of the expectation operator $\mathbb{E}$ where there is no ambiguity.

\begin{assumption}[Subjectively Optimal Data]
\label{ass:optimal_dataset}
    For any data subset $\datasubset \subset \granddataset$ with $\stoi = D_i$, and for any $\reportedstoi$, we have
    \begin{align*}
        &\mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset} [v(\stoi \cup \reportedstominusi)] \\
        \geq & \mathbb{E}_{\reportedstominusi \sim \sigma_i(\cdot \mid \datasubset), \forall \datasubset \subset \granddataset} [v(\reportedstoi \cup \reportedstominusi)].
    \end{align*}
\end{assumption}


Next, we characterize the subclass of linear data valuation metrics that ensure BIC, based on Assumption \ref{ass:optimal_dataset}.
Intuitively, Assumption \ref{ass:optimal_dataset} means that according to client $i$’s belief, $D_i$ is the dataset known to them that can best optimize the model’s utility.
This is also why client $i$ has used $D_i$ to train the grand model $\mathcal{A}(\granddataset)$ in the CML.
Then, according to Lemma \ref{lem:data_value_form}, for any linear data valuation metric $\phi$, Formulas (\ref{formula:expected_data_value_i})-(\ref{formula:expected_data_value_minusi}) can be expressed in the following form:
\begin{align*}
    & \medmath{\mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v)] \!=\!  \beta_i(\granddataset) \!\cdot\! v(\granddataset) \!+\! \sum_{\datasubset\subset \granddataset} \!\beta_i(\datasubset) \!\cdot\! \mathbb{E}[v(\reportedstoi \!\cup\! \reportedstominusi)]},\\
    & \medmath{\mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v)] \!=\!  \beta_{-i}(\granddataset) \!\cdot\! v(\granddataset) \!+\! \sum_{\datasubset\subset \granddataset} \!\beta_{-i}(\datasubset) \!\cdot\! \mathbb{E}[v(\reportedstoi \!\cup\! \reportedstominusi)]}.
\end{align*}
Therefore, for any data subset $\datasubset \subset \granddataset$, if $\stoi \neq D_i$, we should ensure that $\beta_i(\datasubset) = \beta_{-i}(\datasubset)= 0$ such that varying $\reportedstoi$ has no impact on client $i$'s (the other clients') expected data value $\mathbb{E}[\empiricaldatavalue_{i}(\granddataset, v)]$ ($\mathbb{E}[\empiricaldatavalue_{-i}(\granddataset, v)]$), thereby ensuring no incentive to report $\reportedstoi \neq \stoi$.
If $\stoi = D_i$, we can set $\beta_i(\datasubset) \geq 0$ and $\beta_{-i}(\datasubset) \leq 0$ to incentivize client $i$ to optimize the utility $\mathbb{E}[v(\widehat{D}_i \cup \reportedstominusi)]$;
because of Assumption \ref{ass:optimal_dataset}, reporting their subjectively optimal data $\widehat{D}_i = D_i$ is their best strategy.
Consequently, we derive the following characterization of linear and BIC data valuation metrics.
\begin{theorem}[Characterization 1]
\label{thm:characterization1}
    Consider a linear data valuation metric $\phi_i(\granddataset, v) \coloneqq \sum_{\datasubset \subseteq \granddataset} \beta_i(\datasubset) \cdot v(\datasubset)$ where $\beta_i: 2^{\granddataset} \rightarrow \mathbb{R}$.
    Under Assumption \ref{ass:optimal_dataset}, $\phi$ satisfies BIC iff $\beta_i$ satisfies that: $\forall \datasubset \subset \granddataset$, if $\stoi = D_i$, $\beta_i(\datasubset) \geq 0$ and $\beta_{-i}(\datasubset) \leq 0$;
    otherwise, $\beta_i(\datasubset) = \beta_{-i}(\datasubset) = 0$.
\end{theorem}

\subsection{Truth-Shapley}
Next, we attempt to select a strong member from the subclass of linear and BIC data valuation metrics. 
Our idea is to satisfy the four axioms enjoyed by the SV as much as possible, even though, according to Theorem \ref{thm:sv_unique}, full compliance is impossible. 
The first axiom we prioritize is EFF, as it ensures that the model utility $v(\granddataset)$ is fully attributed to all data blocks.
Consequently, we propose Theorem 4.4, which characterizes linear, efficient, and BIC valuation metrics.

% However, even if client $i$ data value $\empiricaldatavalue_i$ decreases when they untruthfully report data subsets, they may still achieve a greater reward by simultaneously reducing the data values $\empiricaldatavalue_{-i}$ of the others. 
% This issue can be addressed by ensuring the EFF property, such that when $\empiricaldatavalue_i$ decreases, $\empiricaldatavalue_{-i}$ must increase.
% Finally, Theorem \ref{thm:characterization2} characterizes BIC, linear, and efficient data valuation metrics.

\begin{theorem}[Characterization 2]
\label{thm:characterization2}
    Consider a linear, efficient data valuation metric $\phi_i(\granddataset, v) \coloneqq \sum_{\datasubset \subseteq \granddataset} \beta_i(\datasubset) \cdot v(\datasubset)$ where $\beta_i: 2^{\granddataset} \rightarrow \mathbb{R}$.
    Under Assumption \ref{ass:optimal_dataset}, $\phi$ satisfies BIC iff: $\phi_i(\granddataset, v) \equiv \sum_{\mathcal{C} \subseteq \mathbb{N}} \beta_i(D_{\mathcal{C}}) \cdot v(D_{\mathcal{C}})$ where $\beta_i(D_{\mathcal{C}}) \geq 0$ for all $ \mathcal{C} \subset \mathbb{N}$ with $i\in \mathcal{C}$.
\end{theorem}

Based on Theorem \ref{thm:characterization2}, we know that under a data valuation metric satisfying LIN, EFF, and BIC, each client's client-level data value $\phi_i$ should be determined only by the utilities $v(D_\mathcal{C})$ of all combinations $D_\mathcal{C}$ of client's full datasets $D_1,\dots,D_N$. 
Accordingly, we propose Truth-Shapley (simply TSV) $\tsv$, which uses an SV-style approach to (1) compute the client-level data value $\tsv_i$ based on the clients' full datasets and then (2) divide $\tsv_i$ among individual data blocks to derive $\tsv_{i,1}, \dots, \tsv_{i, M_i}$.

Specifically, let $\clientleveldatasubset = \{D_i\}_{\forall i \in \mathcal{C}}$ for all $\mathcal{C} \subseteq \mathbb{N}$ and $\mathbb{D}_{-i} = \mathbb{D}_{\mathbb{N}} \setminus \{D_i\}$.
Note that $\clientleveldatasubset$ is mathematically distinct from $D_{\mathcal{C}}$, but it holds the same physical meaning and thus corresponds to the same utility $v(\clientleveldatasubset) = v(D_{\mathcal{C}})$.
Then, we apply the approach of the SV to calculate the client-level TSV:
\begin{align*}
    &\tsv_i(\granddataset, v) \coloneqq \sv_i(\clientlevelgranddataset, v)\\
    = & \sum_{\mathcal{C} \subseteq \mathbb{N} \setminus \{i\}} \weightsv(\mathcal{C} \mid \mathbb{N})\big(v(\clientleveldatasubset \cup \{D_i\}) - v(\clientleveldatasubset)\big),
    % = & \sum_{\mathcal{C} \subseteq \mathbb{N}} \beta^{TSV}_i(D_\mathcal{C})\cdot v(D_\mathcal{C}),\\
\end{align*}
where $\weightsv(\mathcal{C} \mid \mathbb{N}) \coloneqq  \frac{|\mathcal{C}|!(|\mathbb{N}|- |\mathcal{C}| - 1)!}{|\mathbb{N}|!}$.
Next, we employ the SV again to calculate the block-level TSV:
\begin{align*}
    &\tsv_{i,j}(\granddataset, v) \coloneqq \sv_j(D_i, v^{\tsv_i})\\
    = & \! \sum_{\datasubset \subseteq D_i \setminus \{D_{i,j}\}} \weightsv(\datasubset \!\mid\! D_i)\big(v^{\tsv_i}(\datasubset \cup \{D_{i,j}\}) \!-\! v^{\tsv_i}(\datasubset )\big),
    % = & \sum_{\datasubset \subseteq D_i \setminus \{D_{i,j}\}} \weightsv(\datasubset)\big(\tsv_i(\mathbb{D}_{-i} \cup \{D^{\datasubset \cup \{D_{i,j}\}}_i\}) - \tsv_i(\mathbb{D}_{-i} \cup \{\stoi\})\big)
\end{align*}
where $\weightsv(\mathcal{S} \mid D_i) \coloneqq  \frac{|\mathcal{S}|!(|D_i|- |\mathcal{S}| - 1)!}{|D_i|!}$ and $v^{\tsv_i}(\datasubset )\coloneqq \tsv_i(\mathbb{D}_{-i} \cup \{\stoi\}, v)$.
Intuitively, the utility $v^{\tsv_i}(\datasubset)$ represents the client-level TSV $\tsv_i$ when client $i$ contributes dataset $\stoi$.
Consequently, the block-level TSV $\tsv_{i,j}$ measures the expected marginal contribution of the data block $D_{i,j}$ to improving the client-level TSV $\tsv_i$.
% In this way, even if client $i$ misreports data subsets $\reportedstoi \neq \stoi, \forall \stoi \subset D_i$, it only alters their block-level TSVs without affecting their client-level TSV.
Due to the use of the SV-style approach for defining both $\tsv_i$ and $\tsv_{i, j}$, we ensure that Truth-Shapley is linear, efficient, and perfectly complies with the characterization in Theorem \ref{thm:characterization2}.
Therefore, we conclude that it satisfies BIC.

\begin{table*}[t]
\centering
\caption{Performance of data valuation metrics on the reward allocation task. The suffix (o/v) denotes that the valuation metric has undergone a data overvaluation attack. The percentage in parentheses represents the relative increase/decrease in comparison to $\phi_i$.}
\label{tab:reward_alloc}
\small
\begin{tabular}{@{}l|ccc|ccc|ccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Valuation\\ Metric\end{tabular}}}} & \multicolumn{3}{c|}{\textbf{HFL (FedAVG)}} & \multicolumn{3}{c|}{\textbf{VFL (SplitNN)}} & \multicolumn{3}{c}{\textbf{HyFL (FedMD)}} \\ \cmidrule(l){2-10} 
\multicolumn{1}{c|}{} & \textbf{\begin{tabular}[c]{@{}c@{}}incr. (rate)\\ in $\empiricaldatavalue_i$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}decr. (rate)\\ in $\empiricaldatavalue_{-i}$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}val.\\ err.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}incr. (rate)\\ in $\empiricaldatavalue_i$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}decr. (rate)\\ in $\empiricaldatavalue_{-i}$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}val.\\ err.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}incr. (rate)\\ in $\empiricaldatavalue_i$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}decr. (rate)\\ in $\empiricaldatavalue_{-i}$\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}val.\\ err.\end{tabular}} \\ \midrule
\textit{SV (o/v)} & 0.42 (139\%) & 0.42 (69.7\%) & 5.65 & 0.08 (106\%) & 0.08 (53.2\%) & 3.69 & 0.16 (210\%) & 0.16 (106\%) & 56.87 \\
\textit{LOO (o/v)} & 1.99 (53265\%) & 0.0 (0.0\%) & 3.01 & 0.37 (5539\%) & 0.0 (0.0\%) & 2.91 & 0.70 (9738\%) & 0.0 (0.0\%) & 3.18 \\
\textit{BV (o/v)} & 0.16 (348\%) & 0.11 (119\%) & 4.13 & 0.04 (80.8\%) & 0.03 (33.1\%) & 3.71 & 0.08 (262\%) & 0.10 (165\%) & 62.46 \\
\textit{BSV (o/v)} & 0.003 (15.8\%) & 0.002 (5.0\%) & 3.92 & 0.0 (0.0\%) & 0.0 (0.0\%) & 0.0 & 0.0 (0.0\%) & 0.0 (0.0\%) & 0.0 \\
\textit{TSV (o/v)} & 0.0 (0.0\%) & 0.0 (0.0\%) & 0.0 & 0.0 (0.0\%) & 0.0 (0.0\%) & 0.0 & 0.0 (0.0\%) & 0.0 (0.0\%) & 0.0 \\ \bottomrule
\end{tabular}
\end{table*}

\begin{theorem}
\label{thm:tsv_bic}
    Truth-Shapley $\tsv$ satisfies BIC.
\end{theorem}

From Theorem 2.2, we know that Truth-Shapley cannot simultaneously satisfy DUM, SYM, LIN, and EFF. 
However, we find that apart from LIN and EFF, Truth-Shapley satisfies the following axioms:
\begin{itemize}[leftmargin=*]
    \item \textit{Client-level dummy actions (DUM-C)}: If for all $\mathcal{C}\subseteq \mathbb{N} \setminus \set[i]$, we have $v(D_{\mathcal{C}}\cup D_{i}) - v(D_{\mathcal{C}}) = v(D_{i})$, then $\phi_{i}(\granddataset, v) = v(D_{i})$.
    \item \textit{Inner Block-level dummy actions (DUM-IB)}: If for all $\mathcal{S}\subseteq D_{i} \setminus \set[D_{i,j}]$, we have $v(\mathcal{S}\cup \set[D_{i,j}]) - v(\mathcal{S}) = v(\set[D_{i,j}])$, then $\phi_{i,j}(\granddataset, v) = v(\set[D_{i,j}])$.
    \item \textit{Client-level symmetry (SYM-C)}: For any two clients $i_1, i_2$, if for any subset of the other clients $\mathcal{C} \subseteq \mathbb{N} \setminus \{i_1, i_2 \}$, we have $v(D_{\mathcal{C}} \cup D_{i_1}) = v(D_{\mathcal{C}} \cup D_{i_2})$, then we have $\phi_{i_1}(\granddataset, v) =\phi_{i_2}(\granddataset, v)$.
    \item \textit{Inner Block-level symmetry (SYM-IB)}: For any client $i$, if for any two data blocks $D_{i, j_1}, D_{i, j_2}$, and for any subset of their other blocks $\mathcal{S} \subseteq D_i \setminus \{D_{i, j_1}, D_{i, j_2} \}$, we have $v(\mathcal{S} \cup \set[D_{i, j_1}]) = v(\mathcal{S} \cup \set[D_{i, j_2}])$, then we have $\phi_{i, j_1}(\granddataset, v) =\phi_{i, j_2}(\granddataset, v)$.
\end{itemize}
DUM-C and SYM-C are variants of DUM and SYM tailored for client-level data valuation, ensuring a fair allocation of client-level data values among all clients. 
Similarly, DUM-IB and SYM-IB are variants of DUM and SYM specific for block-level data valuation, ensuring a fair distribution of block-level data values among a single client's data blocks.
More importantly, Truth-Shapley uniquely satisfies EFF, LIN, DUM-C, DUM-IB, SYM-C, and SYM-IB, highlighting its distinctiveness among all BIC data valuation metrics.

\begin{theorem}
\label{thm:tsv_unique}
    Truth-Shapley is the unique data valuation metric that satisfies EFF, LIN, DUM-C, DUM-IB, SYM-C, and SYM-IB.
\end{theorem}

