\section{Discussion}
\label{sec:discuss}
\textbf{Poisoning attacker.}
Assumption 4.2 is the core assumption of this paper, which implicitly assumes that the attacker's dataset $D_i$ is not a poisoning dataset. This assumption is based on the premise that client $i$ aims to maximize their reward and thus will not poison the grand model $\mathcal{A}(\granddataset)$, as doing so would reduce the reward derived from monetizing/utilizing $\mathcal{A}(\granddataset)$.
However, in certain scenarios, client $i$ may pursue dual objectives: both attacking the grand model and conducting data overvaluation. Addressing this dual-objective scenario requires further exploration.


\textbf{Computational efficiency.}
Similar to computing the SV, computing Truth-Shapley is time-consuming, as it requires $O(2^{N+\max_i M_i})$ times of model retraining. 
Since Truth-Shapley utilizes the SV-style approach to define both its client-level data value and block-level data value, existing techniques for accelerating SV computation can be applied to computing these two levels of data value.
Also, designing more efficient acceleration methods specifically for Truth-Shapley is a promising direction for future research.


\textbf{Extension of data overvaluation attack.}
The data overvaluation attack proposed in Definition \ref{def:overvaluation} allows client $i$ to manipulate the utility $v(\datasubset)$ of a data subset $\datasubset \subset \granddataset$ by misreporting client $i$'s data blocks $\reportedstoi$. 
Similarly, client $i$ can achieve the same objective by violating the training algorithm $\mathcal{A}$.
For example, client $i$ can decrease $v(\datasubset)$ by performing a gradient ascent attack during model training.
Truth-Shapley remains resistant to this extension of data overvaluation attack with a slight modification to Assumption 4.2: we assume that, in client $i$â€™s belief, following algorithm $\mathcal{A}$ maximizes the expected utility for any $\datasubset \subset \granddataset$.