

% \begin{table*}[t]
% \centering
% \caption{Model accuracy under different data valuation metrics in HFL.}
% \label{tab:data_sel_hfl}
% \small
% \begin{tabular}{@{}l|ccccc|cccccc@{}}
% \toprule
% \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{5}{c|}{\textbf{No. of selected blocks}} & \multicolumn{6}{c}{\textbf{Selection threshold}} \\
% \multicolumn{1}{c|}{} & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{7} & \textbf{9} & \textbf{0\%} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \textbf{8\%} & \textbf{10\%} \\ \midrule
% \textit{Shapley value} & $86.1$ & $90.1$ & $92.2$ & $92.7$ & \textbf{$93.1$} & $91.6$ & $91.6$ & $91.7$ & \textbf{$92.8$} & \textbf{$92.8$} & $90.5$ \\
% \textit{Shapley value (o/v)} & $75.7 (\downarrow)$ & $88.7 (\downarrow)$ & $90.6 (\downarrow)$ & $91.6 (\downarrow)$ & \textbf{$92.9 (\downarrow)$} & $91.9 (\uparrow)$ & \textbf{$92.2 (\uparrow)$} & $91.6 (\downarrow)$ & $90.5 (\downarrow)$ & $88.3 (\downarrow)$ & $87.9 (\downarrow)$ \\ \midrule
% \textit{LOO} & $78.1$ & $86.1$ & $90.8$ & $93.3$ & \textbf{$93.5$} & \textbf{$92.0$} & $91.5$ & $90.7$ & $90.9$ & $90.5$ & $90.0$ \\
% \textit{LOO (o/v)} & $80.4 (\uparrow)$ & $88.1 (\uparrow)$ & $90.2 (\downarrow)$ & \textbf{$91.4 (\downarrow)$} & $89.9 (\downarrow)$ & $91.9 (\downarrow)$ & \textbf{$92.2 (\uparrow)$} & $91.6 (\uparrow)$ & $90.5 (\downarrow)$ & $88.3 (\downarrow)$ & $87.9 (\downarrow)$ \\ \midrule
% \textit{Beta Shapley} & $91.9$ & $87.2$ & $90.7$ & $91.9$ & \textbf{$92.3$} & $91.6$ & $91.6$ & $91.6$ & $91.4$ & \textbf{$92.2$} & $87.6$ \\
% \textit{Beta Shapley (o/v)} & $76.5 (\downarrow)$ & $88.4 (\uparrow)$ & $90.8 (\uparrow)$ & $89.4 (\downarrow)$ & \textbf{$92.0 (\downarrow)$} & $91.6$ & $91.5 (\downarrow)$ & $91.7 (\uparrow)$ & \textbf{$92.5 (\uparrow)$} & $89.0 (\downarrow)$ & $85.6 (\downarrow)$ \\ \midrule
% \textit{Banzhaf value} & $77.1$ & $92.2$ & $93.6$ & \textbf{$94.2$} & $93.3$ & $94.0$ & \textbf{$94.3$} & $94.2$ & $94.1$ & $93.8$ & $93.5$ \\
% \textit{Banzhaf value (o/v)} & $76.4 (\downarrow)$ & $85.4 (\downarrow)$ & $89.9 (\downarrow)$ & $91.6 (\downarrow)$ & \textbf{$92.9 (\downarrow)$} & $92.4 (\downarrow)$ & $92.6 (\downarrow)$ & \textbf{$92.9 (\downarrow)$} & $91.8 (\downarrow)$ & $91.4 (\downarrow)$ & $90.5 (\downarrow)$ \\ \midrule
% \textit{Truth-Shapley} & $91.3$ & $87.9$ & $90.9$ & $91.9$ & \textbf{$92.4$} & $91.6$ & $91.5$ & \textbf{$91.8$} & \textbf{$91.8$} & $91.5$ & $88.8$ \\
% \textit{Truth-Shapley (o/v)} & $91.3$ & $87.9$ & $90.9$ & $91.9$ & \textbf{$92.4$} & $91.6$ & $91.5$ & \textbf{$91.8$} & \textbf{$91.8$} & $91.5$ & $88.8$ \\ \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}[t]
% \centering
% \caption{Model accuracy under different data valuation metrics in VFL.}
% \label{tab:data_sel_vfl}
% \small
% \begin{tabular}{@{}l|ccccc|cccccc@{}}
% \toprule
% \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{5}{c|}{\textbf{No. of selected blocks}} & \multicolumn{6}{c}{\textbf{Selection threshold}} \\
% \multicolumn{1}{c|}{} & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{7} & \textbf{9} & \textbf{0\%} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \textbf{8\%} & \textbf{10\%} \\ \midrule
% \textit{Shapley value} & 64.3 & 69.9 & 71.9 & \textbf{72.2} & 72.1 & 71.6 & 71.9 & 72.0 & \textbf{72.5} & \textbf{72.5} & 71.0 \\
% \textit{Shapley value (o/v)} & 62.8($\downarrow$) & 69.6($\downarrow$) & 71.4($\downarrow$) & \textbf{72.3($\uparrow$)} & 71.9($\downarrow$) & 71.7($\uparrow$) & \textbf{71.8($\downarrow$)} & 71.6($\downarrow$) & 71.6($\uparrow$) & 71.1($\downarrow$) & 69.8($\downarrow$) \\ \midrule
% \textit{LOO} & 63.5 & 68.4 & 70.4 & 71.4 & \textbf{71.7} & 70.8 & 70.7 & \textbf{71.1} & 71.0 & 70.6 & 70.2 \\
% \textit{LOO (o/v)} & 63.2($\downarrow$) & 66.9($\downarrow$) & 68.8($\downarrow$) & 70.0($\downarrow$) & \textbf{70.3($\downarrow$)} & 71.6($\uparrow$) & \textbf{71.6($\uparrow$)} & \textbf{71.6($\uparrow$)} & 71.1($\uparrow$) & 69.1($\downarrow$) & 61.4($\downarrow$) \\ \midrule
% \textit{Beta Shapley} & 64.3 & 69.9 & 71.5 & \textbf{72.0} & 71.7 & 71.4 & 71.5 & 71.9 & 72.1 & \textbf{72.5} & 71.4 \\
% \textit{Beta Shapley (o/v)} & 64.3($\downarrow$) & 69.8($\downarrow$) & 71.5 & \textbf{72.1($\uparrow$)} & 71.6($\downarrow$) & 71.4 & 71.6($\uparrow$) & 71.9 & \textbf{72.9($\uparrow$)} & 72.4($\downarrow$) & 71.7($\uparrow$) \\ \midrule
% \textit{Banzhaf value} & 64.3 & 69.8 & 71.7 & \textbf{72.5} & 72.0 & 71.9 & 72.1 & 72.3 & \textbf{72.6} & 72.0 & 70.5 \\
% \textit{Banzhaf value (o/v)} & 62.8($\downarrow$) & 69.6($\downarrow$) & 71.0($\downarrow$) & \textbf{72.4($\downarrow$)} & 72.3($\uparrow$) & 71.6($\uparrow$) & 71.8($\downarrow$) & \textbf{72.2($\downarrow$)} & 72.1($\downarrow$) & 70.5($\downarrow$) & 68.9($\downarrow$) \\ \midrule
% \textit{Truth-Shapley} & 64.3 & 69.8 & 71.8 & 72.2 & \textbf{72.3} & 71.8 & 72.0 & 72.3 & \textbf{72.4} & 72.0 & 71.4 \\
% \textit{Truth-Shapley (o/v)} & 64.3 & 69.8 & 71.8 & 72.2 & \textbf{72.3} & 71.8 & 72.0 & 72.3 & \textbf{72.4} & 72.0 & 71.4 \\ \bottomrule
% \end{tabular}
% \end{table*}

% \begin{table*}[t]
% \centering
% \caption{Model accuracy under different data valuation metrics in HFL.}
% \label{tab:data_sel_hyfl}
% \small
% \begin{tabular}{@{}l|ccccc|cccccc@{}}
% \toprule
% \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{5}{c|}{\textbf{No. of selected blocks}} & \multicolumn{6}{c}{\textbf{Selection threshold}} \\
% \multicolumn{1}{c|}{} & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{7} & \textbf{9} & \textbf{0\%} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \textbf{8\%} & \textbf{10\%} \\ \midrule
% \textit{Shapley value} & 68.2 & \textbf{73.0} & 72.7 & 72.0 & 71.9 & 72.1 & 72.0 & 72.0 & 72.0 & 72.0 & \textbf{73.0} \\
% \textit{Shapley value (o/v)} & 65.5($\downarrow$) & 69.7($\downarrow$) & 71.8($\downarrow$) & \textbf{72.7($\uparrow$)} & 71.9 & \textbf{72.3($\uparrow$)} & 72.0 & 71.7($\downarrow$) & 70.8($\downarrow$) & 70.3($\downarrow$) & 70.3($\downarrow$) \\ \midrule
% \textit{LOO} & 67.7 & 72.8 & \textbf{73.1} & 72.8 & 73.0 & 72.9 & \textbf{73.2} & \textbf{73.2} & \textbf{73.2} & 73.1 & 73.1 \\
% \textit{LOO (o/v)} & 70.3($\downarrow$) & 71.3($\downarrow$) & 71.5($\downarrow$) & 71.7($\downarrow$) & \textbf{71.9($\downarrow$)} & 72.0($\uparrow$) & 72.0($\downarrow$) & \textbf{72.1($\downarrow$)} & 72.0($\downarrow$) & 72.0($\downarrow$) & 56.5($\downarrow$) \\ \midrule
% \textit{Beta Shapley} & 68.3 & \textbf{72.6} & \textbf{72.6} & 71.8 & 71.9 & \textbf{72.0} & 71.9 & 71.9 & 71.9 & 71.6 & 71.6 \\
% \textit{Beta Shapley (o/v)} & 68.3($\downarrow$) & 72.4($\downarrow$) & \textbf{72.5($\downarrow$)} & 71.8 & 71.9 & \textbf{72.0} & 71.9 & 71.8($\downarrow$) & 71.8($\downarrow$) & 71.7($\uparrow$) & 71.8($\uparrow$) \\ \midrule
% \textit{Banzhaf value} & 68.1 & 73.0 & \textbf{73.8} & 73.3 & 72.3 & 72.4 & 73.2 & \textbf{73.5} & 73.4 & 73.3 & 73.1 \\
% \textit{Banzhaf value (o/v)} & 64.7($\downarrow$) & 69.2($\downarrow$) & 71.9($\downarrow$) & 72.8($\downarrow$) & \textbf{72.7($\uparrow$)} & 72.7($\uparrow$) & \textbf{72.9($\downarrow$)} & 72.8($\downarrow$) & 72.7($\downarrow$) & 71.2($\downarrow$) & 70.4($\downarrow$) \\ \midrule
% \textit{Truth-Shapley} & 68.2 & \textbf{73.0} & 72.7 & 72.0 & 71.9 & 72.0 & 71.9 & 72.0 & 71.9 & 72.2 & \textbf{73.4} \\
% \textit{Truth-Shapley (o/v)} & 68.2 & \textbf{73.0} & 72.7 & 72.0 & 71.9 & 72.0 & 71.9 & 72.0 & 71.9 & 72.2 & \textbf{73.4} \\ \bottomrule
% \end{tabular}
% \end{table*}


\begin{table*}[t]
\centering
\caption{Performance of data valuation metrics on the data selection task where data blocks with top-K data values are selected. The suffix (o/v) denotes that the valuation metric has undergone a data overvaluation attack. If the model accuracy decreases after the attack, we highlight it in bold.}
\label{tab:data_sel_by_nb}
\small
\begin{tabular}{@{}l|cccccccccccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Valuation\\ Metric\end{tabular}}}} & \multicolumn{4}{c|}{\textbf{HFL (FedAVG)}} & \multicolumn{4}{c|}{\textbf{VFL (SplitNN)}} & \multicolumn{4}{c}{\textbf{HyFL (FedMD)}} \\ \cmidrule(l){2-13} 
\multicolumn{1}{c|}{} & \multicolumn{12}{c}{\textbf{model accuracy (\%) when the number of selected blocks =}} \\
\multicolumn{1}{c|}{} & \textbf{2} & \textbf{4} & \textbf{6} & \multicolumn{1}{c|}{\textbf{8}} & \textbf{2} & \textbf{4} & \textbf{6} & \multicolumn{1}{c|}{\textbf{8}} & \textbf{2} & \textbf{4} & \textbf{6} & \textbf{8} \\ \midrule
\textit{SV} & 89.89 & 91.06 & 93.09 & \multicolumn{1}{c|}{93.40} & 66.35 & 70.64 & 72.25 & \multicolumn{1}{c|}{71.90} & 71.04 & 73.43 & 71.68 & 72.08 \\
\textit{SV (o/v)} & \textbf{83.51} & \textbf{88.54} & \textbf{91.87} & \multicolumn{1}{c|}{\textbf{92.61}} & 67.02 & \textbf{70.27} & \textbf{71.84} & \multicolumn{1}{c|}{72.01} & \textbf{68.11} & \textbf{69.97} & 72.27 & 72.18 \\ \midrule
\textit{LOO} & 83.98 & 91.20 & 93.32 & \multicolumn{1}{c|}{93.72} & 66.39 & 69.72 & 70.75 & \multicolumn{1}{c|}{71.50} & 70.56 & 72.89 & 72.90 & 72.71 \\
\textit{LOO (o/v)} & \textbf{83.85} & \textbf{89.17} & \textbf{91.78} & \multicolumn{1}{c|}{\textbf{92.63}} & \textbf{60.37} & \textbf{67.93} & \textbf{70.00} & \multicolumn{1}{c|}{\textbf{71.07}} & \textbf{67.41} & \textbf{69.75} & 73.00 & 73.15 \\ \midrule
\textit{BV} & 87.58 & 93.33 & 94.14 & \multicolumn{1}{c|}{93.97} & 66.40 & 70.74 & 72.25 & \multicolumn{1}{c|}{72.23} & 71.05 & 73.16 & 73.61 & 72.55 \\
\textit{BV (o/v)} & \textbf{85.02} & \textbf{89.82} & \textbf{92.89} & \multicolumn{1}{c|}{\textbf{93.19}} & 67.03 & \textbf{70.35} & \textbf{72.04} & \multicolumn{1}{c|}{\textbf{72.12}} & \textbf{68.24} & \textbf{70.46} & \textbf{72.61} & \textbf{72.54} \\ \midrule
\textit{BSV} & 90.85 & 88.55 & 91.29 & \multicolumn{1}{c|}{92.46} & 66.37 & 70.62 & 72.25 & \multicolumn{1}{c|}{71.63} & 71.06 & 73.26 & 71.68 & 72.08 \\
\textit{BSV (o/v)} & \textbf{90.58} & 91.95 & \textbf{90.83} & \multicolumn{1}{c|}{\textbf{91.70}} & 66.37 & 70.62 & 72.25 & \multicolumn{1}{c|}{71.63} & 71.06 & 73.26 & 71.68 & 72.08 \\ \midrule
\textit{TSV} & 90.58 & 91.15 & 92.13 & \multicolumn{1}{c|}{92.51} & 66.37 & 70.42 & 72.21 & \multicolumn{1}{c|}{72.17} & 71.07 & 73.55 & 71.79 & 72.08 \\
\textit{TSV (o/v)} & 90.58 & 91.15 & 92.13 & \multicolumn{1}{c|}{92.51} & 66.37 & 70.42 & 72.21 & \multicolumn{1}{c|}{72.17} & 71.07 & 73.55 & 71.79 & 72.08 \\ \bottomrule
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{Performance of data valuation metrics on the data selection task where data blocks with sufficient contribution rate are selected. The suffix (o/v) denotes that the valuation metric has undergone a data overvaluation attack. If the model accuracy decreases after the attack, we highlight it in bold.}
\label{tab:data_sel_by_value}
\small
\begin{tabular}{@{}l|cccccccccccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Valuation\\ Metric\end{tabular}}}} & \multicolumn{4}{c|}{\textbf{HFL (FedAVG)}} & \multicolumn{4}{c|}{\textbf{VFL (SplitNN)}} & \multicolumn{4}{c}{\textbf{HyFL (FedMD)}} \\ \cmidrule(l){2-13} 
\multicolumn{1}{c|}{} & \multicolumn{12}{c}{\textbf{model accuracy (\%) when selecting blocks with contribution rate $>=$}} \\
\multicolumn{1}{c|}{} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \multicolumn{1}{c|}{\textbf{8\%}} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \multicolumn{1}{c|}{\textbf{8\%}} & \textbf{2\%} & \textbf{4\%} & \textbf{6\%} & \textbf{8\%} \\ \midrule
\textit{SV} & 91.37 & 91.76 & 93.01 & \multicolumn{1}{c|}{93.23} & 71.82 & 71.95 & 72.22 & \multicolumn{1}{c|}{72.26} & 72.01 & 72.10 & 72.12 & 71.88 \\
\textit{SV (o/v)} & 91.80 & 91.88 & \textbf{91.04} & \multicolumn{1}{c|}{\textbf{88.73}} & \textbf{71.44} & \textbf{71.51} & \textbf{71.49} & \multicolumn{1}{c|}{\textbf{70.99}} & 72.05 & \textbf{71.97} & \textbf{70.69} & \textbf{70.35} \\ \midrule
\textit{LOO} & 91.73 & 91.97 & 92.73 & \multicolumn{1}{c|}{92.87} & 69.88 & 70.45 & 70.32 & \multicolumn{1}{c|}{70.29} & 72.90 & 72.96 & 73.00 & 73.03 \\
\textit{LOO (o/v)} & \textbf{89.09} & \textbf{88.85} & \textbf{88.90} & \multicolumn{1}{c|}{\textbf{88.49}} & 70.17 & \textbf{69.88} & \textbf{69.52} & \multicolumn{1}{c|}{\textbf{69.16}} & \textbf{70.99} & \textbf{69.46} & \textbf{69.20} & \textbf{69.11} \\ \midrule
\textit{BV} & 94.23 & 94.21 & 94.29 & \multicolumn{1}{c|}{94.03} & 71.82 & 72.24 & 72.26 & \multicolumn{1}{c|}{72.06} & 73.18 & 73.41 & 73.19 & 72.97 \\
\textit{BV (o/v)} & \textbf{92.70} & \textbf{92.67} & \textbf{92.53} & \multicolumn{1}{c|}{\textbf{92.14}} & 71.97 & \textbf{71.76} & \textbf{71.77} & \multicolumn{1}{c|}{\textbf{71.16}} & 73.30 & \textbf{73.05} & \textbf{72.96} & \textbf{72.77} \\ \midrule
\textit{BSV} & 91.37 & 91.36 & 91.80 & \multicolumn{1}{c|}{92.06} & 71.30 & 71.77 & 71.94 & \multicolumn{1}{c|}{72.21} & 71.92 & 72.01 & 72.03 & 71.68 \\
\textit{BSV (o/v)} & 91.37 & 91.38 & \textbf{91.77} & \multicolumn{1}{c|}{\textbf{91.04}} & 71.30 & 71.77 & 71.94 & \multicolumn{1}{c|}{72.21} & 71.92 & 72.01 & 72.03 & 71.68 \\ \midrule
\textit{TSV} & 91.39 & 91.76 & 92.48 & \multicolumn{1}{c|}{92.04} & 71.98 & 72.14 & 72.17 & \multicolumn{1}{c|}{71.85} & 71.99 & 72.10 & 72.07 & 72.08 \\
\textit{TSV (o/v)} & 91.39 & 91.76 & 92.48 & \multicolumn{1}{c|}{92.04} & 71.98 & 72.14 & 72.17 & \multicolumn{1}{c|}{71.85} & 71.99 & 72.10 & 72.07 & 72.08 \\ \bottomrule
\end{tabular}
\end{table*}







\section{Experiments}
% \subsection{Setup}
\textbf{Research questions.}
We conduct experiments to answer the following questions:
Is the generalized data overvaluation attack effective against various data valuation metrics? When the attack occurs, how do the metrics perform in reward allocation and data selection?

\textbf{Baselines.}
We include four SOTA \textbf{linear} data valuation metrics as baselines: the SV, the LOO, Beta Shapley (BSV)~\citep{kwon2022beta}, and Banzhaf value (BV)~\citep{wang2023data}.
All of these metrics satisfy LIN, making the data overvaluation attack applicable to them. 
For Beta Shapley, parameters for the beta distribution need to be specified; we select the Beta(4, 1) distribution, which demonstrates good performance in the original paper.

\textbf{Datasets, CML algorithms \& models.}
We use the \textit{Apartments for Rent}~\citep{apartment_for_rent_classified_555}, \textit{Bank Marketing}~\citep{bank_marketing_222}, and \textit{CDC Healthcare}~\citep{cdc_health_indicators} datasets for horizontal federated learning (HFL, using the FedAVG algorithm~\citep{mcmahan2017communication}), vertical federated learning (VFL, using the SplitNN algorithm~\citep{gupta2018distributed}), and hybrid federated learning (HyFL, using the FedMD algorithm~\citep{li2019fedmd}), respectively. 
HFL allows clients to have data with different sample spaces, VFL allows different feature spaces, and HyFL permits both to differ. 
Each dataset is partitioned into 11 to 12 data blocks based on its content and assigned to three clients for CML.
Then, the clients run one round of the CML algorithm to train $\mathcal{A}(\granddataset)$ and then evaluate each data subset for data valuation.
More details can be found in Appendix \ref{appendix:experiments}.



\textbf{Implementation of data overvaluation.}
We conduct 90 experimental runs, where in each run, we randomly select a client $i$ to perform the generalized data overvaluation attack.
In HFL, for all $\datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset)$, when $\beta_i(\datasubset) > 0$ while $\beta_{-i}(\datasubset) \leq 0$, client $i$ reports $\reportedstoi = D_i$ to increase $v(\datasubset)$;
when $\beta_i(\datasubset) < 0$ while $\beta_{-i}(\datasubset) \geq 0$,
client $i$ performs data poisoning over $\stoi$ to decrease $v(\datasubset)$.
In VFL and HyFL, since it is difficult to increase $v(\datasubset)$ by augmenting $\stoi$, client $i$ only poisons $\stoi$ when $\beta_i(\datasubset) < 0$ and $\beta_{-i}(\datasubset) \geq 0$.

\subsection{Reward Allocation}
\label{sec:reward_alloc}
Table \ref{tab:reward_alloc} presents the performance of various data valuation metrics in the reward allocation task. 
Since the reward of client $i$ depends on both client $i$'s empirical data value $\empiricaldatavalue_i$ and the sum of the other clients' data values $\empiricaldatavalue_{-i}$, we measure the impact of the data overvaluation attack on reward allocation by assessing the \textit{increase in $\empiricaldatavalue_i$} and the \textit{decrease in $\empiricaldatavalue_{-i}$} caused by the attack.
Additionally, we compute the \textit{valuation error}, defined as the normalized mean squared error between the vectors $[\empiricaldatavalue_1,\dots,\empiricaldatavalue_N]$ and $[\phi_1,\dots,\phi_N]$, to measure the robustness of different data valuation metrics against data overvaluation.

In HFL, Truth-Shapley is completely resistant to the data overvaluation attack, whereas other data valuation metrics are significantly affected. 
For SV, BV, and BSV, data overvaluation not only increases the attacker's data value $\empiricaldatavalue_i$ but also substantially reduces the total data value $\empiricaldatavalue_{-i}$ of other clients. 
Notably, since SV satisfies the EFF axiom, the increase in $\empiricaldatavalue_i$ is exactly equal to the decrease in $\empiricaldatavalue_{-i}$.
However, in both the VFL and HyFL scenarios, the effect of data overvaluation is relatively weaker. 
This is because, in these CML settings, the feature heterogeneity of data blocks limits the opportunities for data overvaluation. 
Specifically, when $\beta_i(\datasubset) > 0$ and $\beta_{-i}(\datasubset) \leq 0$, it is difficult to augment $\reportedstoi$ using other data blocks to enhance $v(S)$; 
instead, data overvaluation is only achieved through data poisoning when $\beta_i(\datasubset) < 0$ and $\beta_{-i}(\datasubset) \geq 0$. 
In particular, data overvaluation fails entirely against BSV because there is no subset $\datasubset$ satisfying $\beta_i(\datasubset) < 0$ and $\beta_{-i}(\datasubset) \geq 0$.

Overall, data overvaluation can significantly impact reward allocation outcomes, and only Truth-Shapley can fully prevent such an attack. 
Although BSV also performs well in VFL and HyFL settings, its performance in HFL reveals a potential vulnerability: it may still be possible to bypass BSVâ€™s defense by implementing data augmentation when $\beta_i(\datasubset) > 0$ and $\beta_{-i}(\datasubset) \leq 0$.


\subsection{Data Selection}
For data selection, we evaluate data valuation metrics based on two selection criteria. 
In Table \ref{tab:data_sel_by_nb}, we perform CML using the top $K$ data blocks ranked by their data values. 
In Table \ref{tab:data_sel_by_value}, we calculate the contribution rate of each data block, defined as the ratio of its block-level data value to the sum of all data blocks' data values. 
Only data blocks with a contribution rate no less than a predefined threshold are selected for CML.

As shown in Tables \ref{tab:data_sel_by_nb} and \ref{tab:data_sel_by_value}, in HFL, data overvaluation attacks against Truth-Shaply cannot affect the model accuracy at all, as it can fully resist such attacks. 
In contrast, under other data valuation metrics, since data overvaluation significantly alters both the absolute values of block-level data values and their relative rankings, the model accuracy declines significantly in most cases.
Additionally, in VFL and HyFL, the situation is generally similar, except in the case of BSV, where data overvaluation does not succeed as shown in Table \ref{tab:reward_alloc}.

In summary, under data valuation metrics that do not satisfy BIC, the data overvaluation attack can significantly impact data selection outcomes. 
This not only harms model accuracy but also leads to unfair opportunities for participating in CML, indirectly affecting clients' potential rewards.
Notably, even in the absence of the attack, Truth-Shapley remains as effective as other metrics in selecting data blocks.
