\section{Introduction}

As data regulations become increasingly stringent, it is becoming more challenging for enterprises to collect sufficient high-quality data for machine learning (ML). 
To address this issue, collaborative ML (CML), such as federated learning~\citep{mcmahan2017communication}, has emerged as a promising solution that enables enterprises to train accurate ML models without directly sharing data. 
Given that enterprises' datasets are often highly heterogeneous, a critical task in CML is data valuation, that is, how to reasonably evaluate the contribution of different heterogeneous datasets to model performance improvement. 
Based on the datasets' data values, i.e., the outcome of data valuation, enterprises can select higher-quality data to further enhance model performance and fairly allocate rewards among themselves, such as the revenue made by deploying the model.

In the literature, \textit{marginal contribution}-based valuation metrics, represented by the leave-one-out (LOO)~\citep{cook1977detection} and the Shapley value (SV)~\citep{shapley1953value}, have been widely adopted for data valuation in CML. 
These metrics evaluate data value by measuring the impact of including or excluding a dataset on model performance. 
For example, the SV requires iterating over all possible combinations of the datasets and computing the model utility improvement contributed by each dataset to each combination, leading to significant computational costs for repeated model retraining. 
Consequently, extensive research efforts (e.g., \citep{ghorbani2019data, jia2019towards, jia2019efficient, kwon2021efficient}) have been devoted to improving the computational efficiency of data valuation to enhance its practicality.
However, existing studies overlook a critical trust vulnerability: \textit{During model retraining, clients may misreport their datasets to untruthfully overvalue them, thereby maximizing their gains in data selection and reward allocation.}\footnote{Since clients in CML do not share raw data, detecting data misreporting behavior during model retraining is challenging.}
This gap motivates us to conduct the first exploration of \textit{data overvaluation and truthful data valuation}.

% \subsection{Our Contributions}
In this paper, we propose a novel attack method targeting data valuation: the data overvaluation attack. 
This attack enables strategic clients to misreport their datasets to significantly inflate their data value, thereby gaining an unfair advantage in subsequent data selection and reward allocation tasks. 
Notably, the attack works against all linear data valuation metrics, which cover most of the state-of-the-arts (SOTAs) including the LOO and the SV. 
Our experimental results demonstrate that the data overvaluation attack can increase an attackerâ€™s SV by up to \textbf{210\%} and even boost their LOO value by \textbf{four orders of magnitude}.

Next, we explore how to ensure \textit{truthful data valuation}.
We theoretically characterize the subclass of \textit{linear} data valuation metrics that can resist the data overvaluation attack. 
This characterization is fundamental since most of mainstream data valuation metrics are linear, inlcuding the LOO, the SV, Beta Shapley~\citep{kwon2022beta}, and Banzhaf value~\cite{wang2023data}.
From this subclass, we identify a novel valuation metric, named \textit{Truth-Shapley}. 
Similar to the SV, Truth-Shapley uniquely satisfies a set of promising axioms for valuation, thereby ensuring effective data selection and fair reward allocation. 
Therefore, regardless of whether a data overvaluation attack occurs, Truth-Shapley serves as an excellent choice for robust and effective data valuation.

We summarize our contributions as follows.
\begin{itemize}[leftmargin=*]
    \item First, we propose the data overvaluation attack.
    This attack reveals the unexplored vulnerability of existing data valuation metrics to strategic data manipulation and thus opens up a new research direction toward truthful data valuation for CML.
    \item Second, we theoretically analyze and characterize the necessary and sufficient conditions for linear data valuation metrics to ensure truthful data valuation. This characterization facilitates a rigorous assessment of their robustness against data overvaluation.
    \item Third, we propose Truth-Shapley, which is the unique data valuation metric that can prevent the data overvaluation attack while satisfying some key axioms for valuation.
    \item Finally, we conduct extensive experiments in three representative CML scenarios: horizontal FL, vertical FL, and hybrid FL. 
    Our results demonstrate the vulnerability of existing data valuation metrics to the data overvaluation attack and validate the robustness and effectiveness of Truth-Shapley in data selection and reward allocation.
\end{itemize}