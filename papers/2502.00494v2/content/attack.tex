\section{Data Overvaluation Attack}
In this section, we first provide the \textit{data overvaluation} attack against the SV and then generalize it for other metrics.

\subsection{Data Overvaluation against the SV}
Although the SV fairly allocates data values to honest clients, strategic clients can manipulate their SVs by misreporting data subsets for model retraining.
As shown in Equation (\ref{eq:sv}), the SV $\sv_{i, j}$ enumerates all subsets $\datasubset$ of the grand dataset $\granddataset$; 
for each subset $\datasubset \subset \granddataset$, a model $\mathcal{A}(\datasubset)$ is retrained, and its utility $v(\datasubset)$ is evaluated for calculating $\sv_{i, j}$. 
Let $\stoi$ denote the set of data blocks in $\datasubset$ that belong to client $i$, $\stominusi$ denote the others data blocks in $\datasubset$, i.e., $\stominusi = \datasubset / \stoi$, and $\clientsins$ denote the set of clients who have at least one data block in $\datasubset$. 
Then, for each subset $\datasubset \subset \granddataset$ with $i \in \clientsins$, since $v(\datasubset)$ can also be expressed as $v(\stoi \cup \stominusi)$, client $i$ can vary $v(\datasubset)$ by misreporting $\stoi$, thereby manipulating their block-level SVs $\sv_{i, j}$.

Similarly, client $i$ can also manipulate their client-level SV $\sv_{i}$ by altering the model utility $v(\datasubset)$.
Specifically, the client-level SV $\sv_{i}=\sum_{j\in[M_i]}\sv_{i,j}$ can be written in the following form:
\begin{align*}
     &\sv_{i}(\granddataset, v) = \sum_{\datasubset\subseteq \granddataset} \betasv(\datasubset) \cdot v(\datasubset), \text{ where} \\
      & \betasv(\datasubset) \\
      \coloneqq &
   \begin{cases}
            \scriptstyle(|\stoi||\granddataset| - |D_{i}||\datasubset|) \cdot \frac{(|\datasubset |- 1)! (|\granddataset|-|\datasubset| - 1)!}{|\granddataset|!}, & \datasubset \subset \granddataset, \datasubset \neq \emptyset, \\
            |D_i|/ |\granddataset|, & \datasubset = \granddataset, \\
            - |D_i|/ |\granddataset|, & \datasubset = \emptyset.
   \end{cases}
\end{align*}
Because $\frac{\partial \sv_i}{\partial (v(\datasubset))} = \betasv(\datasubset)$, when $\betasv(\datasubset) > 0$, increasing $v(\datasubset)$ can enhance $\sv_{i}$; when $\betasv(\datasubset) < 0$, decreasing $v(\datasubset)$ improves $\sv_{i}$; when $\betasv(\datasubset) = 0$, changing $v(\datasubset)$ reduces $\sv_{i}$ has no effect on $\sv_{i}$.

\begin{algorithm}[t]
    \caption{Data Overvaluation against the SV}
    \label{alg:data_overvaluation}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} dataset $\granddataset$, grand model's utility $v(\granddataset)$
    \STATE {\bfseries Attacker:} client $i$
   \FOR{ each subset $\datasubset\subset \granddataset$}
        \IF{$i\in \clientsins$}
            \IF{$\betasv(\datasubset) > 0$}
                \STATE Client $i$: Positively augment $\stoi$ to obtain a reported dataset $\reportedstoi$.
            \ELSIF{$\betasv(\datasubset) < 0$}
                \STATE Client $i$: Negatively augment $\stoi$ to generate a reported dataset $\reportedstoi$.
            \ELSE
                \STATE Client $i$: Honestly use $\reportedstoi = \stoi$.
            \ENDIF
        \ENDIF
        \STATE Clients $\clientsins$: Use dataset $\reportedsubset=\cup_{i'\in \clientsins}  \widehat{D}^{\datasubset}_{i'}$ to train a model $\mathcal{A}(\reportedsubset)$.
        \STATE Server: Evaluate the model's utility $v(\reportedsubset)$.
   \ENDFOR
   \STATE Server: Calculate $\empiricalsv_{i,j}(\granddataset, v), \forall i, \forall j$ and return them.
   % \RETURN $\empiricalsv_{i,j}(\granddataset, v), \forall i$
\end{algorithmic}
\end{algorithm}

\textbf{Algorithm.}
Based on the above analysis, we propose Algorithm \ref{alg:data_overvaluation} to implement data overvaluation against the SV, considering a strategic client $i$ as an attacker.
Let $\reportedstoi$ denote the version of $\stoi$ reported by client $i$ and $\reportedstominusi$ denote the version of $\stominusi$ reported by the other clients for evaluating the model utility $v(\datasubset)$.
That means, instead of truthfully using dataset $\stoi$, client $i$ may untruthfully employ dataset $\reportedstoi \neq \stoi$ to increase the model utility from $v(\datasubset)$ to $v(\reportedsubset)$, where $\reportedsubset = \reportedstoi \cup \reportedstominusi$.
In Algorithm \ref{alg:data_overvaluation}, to compute the SV, we iterate over every subset $\datasubset \subset \granddataset$ and evaluate its corresponding model utility (Lines 3--18).
Note that the grand model's utility $v(\granddataset)$ is given as the algorithm's input, as the grand model $\mathcal{A}(\granddataset)$ has already been trained before data valuation. 
When a subset $\datasubset$ includes client $i$'s data blocks, if $\betasv(\datasubset)$ is nonzero, client $i$ has the incentive to manipulate $v(\datasubset)$; thus, in this case, client $i$ positively/negatively augments their dataset $\stoi$ to derive a new dataset $\reportedstoi$ (Lines 6 and 8), resulting in an enhanced/reduced utility $v(\reportedsubset)$ (Lines 13--14).
After evaluating all the model utilities, the server calculates client $i$'s \textit{empirical} block-level SVs as:
% \begin{align*}
%     &\sv_{i,j}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi) \\
%     = & \sum_{\datasubset\subset \granddataset \setminus \set[D_{i,j}], i \in \clientsins} \weightsv(\datasubset) \big(v(\reportedsplustoi \cup \splustominusi)  - v(\reportedstoi \cup \stominusi)\big) \\
%     & + \sum_{\datasubset\subset \granddataset \setminus \set[D_{i,j}], i \notin \clientsins} \weightsv(\datasubset) \big(v(\reportedsplustoi \cup \splustominusi) -v(\datasubset)\big) \\
%     & + \frac{1}{|\granddataset|} \big(v(\granddataset)  - v(\widehat{D}^{\granddataset / \{D_{i,j}\}}_{i} \cup D^{\granddataset / \{D_{i,j}\}}_{-i})\big).
% \end{align*}

\begin{align*}
    \empiricalsv_{i,j}(\granddataset, v)\coloneqq & \sum_{\datasubset\subset \granddataset \setminus \set[D_{i,j}]} \weightsv(\datasubset) \big(v(\reportedsubset^{+})  - v(\reportedsubset)\big) \\
    & + \weightsv(\granddataset^{-}) \big(v(\granddataset)  - v(\widehat{D}_{\mathbb{N}}^{-})\big)
\end{align*}
where $\reportedsubset^{+}$ is the reported version of $\datasubset^{+}$, $\granddataset^{-} = \granddataset \setminus \{D_{i,j}\}$, and $\widehat{D}_{\mathbb{N}}^{-}$ is the reported version of $\granddataset^{-}$.
Consequently, client $i$ obtains their empirical client-level SV as:
% \begin{align*}
%     &\sv_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi) \\
%     = &\sum_{\datasubset\subseteq \granddataset, i\notin \clientsins} \betasv(\datasubset) \cdot v(\datasubset) \\
%     &+ \sum_{\datasubset\subseteq \granddataset, i \in \clientsins} \betasv(\datasubset) \cdot v(\reportedstoi \cup \stominusi), 
% \end{align*}
% which is guaranteed to be no less than the real SV $\sv_i(\granddataset, v)$ under certain circumstances as shown in Lemma \ref{lem:overvalued}.
\vspace{-3pt}
\begin{align*}
    \empiricalsv_{i}(\granddataset, v) = & \betasv(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \betasv(\datasubset) \cdot v(\reportedsubset).
\end{align*}
If $\forall \datasubset \subset \granddataset$ with $\betasv(\datasubset) > 0$, we have $v(\reportedstoi \cup \reportedstominusi) \geq v(\stoi \cup \reportedstominusi)$, and $\forall \datasubset \subset \granddataset$ with $\betasv(\datasubset) < 0$, we have $v(\reportedstoi \cup \reportedstominusi) \leq v(\stoi \cup \reportedstominusi)$, then $\empiricalsv_{i}(\granddataset, v)$ is guaranteed to be no less than the empirical SV $\empiricalsv_i(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$ where client $i$ truthfully reports their data contributions $\reportedstoi = \stoi$, i.e.,
\begin{align*}
     &\empiricalsv_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi) \\
    = & \betasv(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \betasv(\datasubset) \cdot v(\stoi \cup \reportedstominusi).
\end{align*}
This implies that client $i$ may increase its (empirical) client-level SV $\empiricalsv_{i}$ through a data overvaluation attack. 
Moreover, since the SV satisfies the EFF axiom, the total SV $\empiricalsv_{-i}$ of the other clients will decrease accordingly.

% \begin{lemma}
% \label{lem:overvalued}
%     When $v(\reportedstoi \cup \reportedstominusi) \geq v(\stoi \cup \reportedstominusi), \forall \datasubset \subset \granddataset, \betasv(\datasubset) > 0$ and $v(\reportedstoi \cup \reportedstominusi) \leq v(\stoi \cup \reportedstominusi), \forall \datasubset \subset \granddataset, \betasv(\datasubset) < 0$, we have $\empiricalsv_{i}(\granddataset, v) \geq \empiricalsv_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$ and $\empiricalsv_{-i}(\granddataset, v) \leq \empiricalsv_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$.
% \end{lemma}


\subsection{Generalization}
In addition to the SV, we further generalize the data overvaluation attack to manipulate all linear data valuation metrics. 
% This generalization is highly fundamental because linearity is essential for fair and efficient data valuation and is satisfied by most mainstream valuation metrics.

\begin{lemma}
\label{lem:data_value_form}
    If a data valuation metric $\phi$ satisfies LIN, then for each client $i$, there exists $\beta_i: 2^{\granddataset} \rightarrow \mathbb{R}$ such that
    \begin{align*}
        \phi_{i}(\granddataset, v) \equiv \sum_{\datasubset \subseteq \granddataset} \beta_i(\datasubset) \cdot v(\datasubset).
    \end{align*}
\end{lemma}

Specifically, Lemma \ref{lem:data_value_form} indicates that any linear data value $\phi_i$ can be expressed as a weighted sum of model utilities $\{v(\datasubset)\}_{\datasubset\subseteq \granddataset}$. 
Consequently, similar to the case of the SV, for each subset $\datasubset \subset \granddataset$, and for each client $i\in\clientsins$, when $\beta_i(\datasubset)$ is positive (negative), they can increase (decrease) $v(\datasubset)$ to enhance their linear data value $\phi_i$.
However, unlike the SV, some data valuation metrics such as Beta Shapley and Banzhaf value do not satisfy EFF, meaning that an increase in $\phi_{i}$ does not necessarily lead to a decrease in $\phi_{-i}$.
As a result, client $i$ may not always receive a higher reward.
Therefore, when $\beta_{i}(\datasubset)$ is positive (negative), we should also ensure that $\beta_{-i}(\datasubset)=\sum_{i' \in \mathbb{N}(\datasubset) \setminus \{i\}} \beta_{i'}(\datasubset)$ is non-positive to prevent $\phi_{-i}$ from increasing.

Hence, we generalize the data overvaluation attack in Definition \ref{def:overvaluation}.
As demonstrated in Lemma \ref{lem:attack_success}, by (successfully) implementing the attack, a strategic client $i$ can derive an inflated empirical data value $\empiricaldatavalue_{i}(\granddataset, v) \geq \empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$, where
\begin{align*}
    &\empiricaldatavalue_{i}(\granddataset, v) = \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_i(\datasubset) \cdot v(\reportedsubset),\\
    &\empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi) \\
    = & \beta_i(\granddataset) \cdot v(\granddataset) + \sum_{\datasubset\subset \granddataset} \beta_i(\datasubset) \cdot v(\stoi \cup \reportedstominusi),
\end{align*}
and have the sum of the other clients' data values decreased, i.e., $\phi_{-i}(\granddataset, v) \leq \phi_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$.

\begin{definition}[Data Overvaluation Attack]
\label{def:overvaluation}
    Consider a linear data valuation metric $\phi$. 
    A data overvaluation attack against data value $\phi_i(\granddataset, v)$ is to report data subsets $\{\reportedstoi \mid \forall \datasubset \subset \granddataset, i\in \mathbb{N}(\datasubset) \}$ where $\exists \reportedstoi \neq \stoi$ to achieve the following goal: $\forall \datasubset \subset \granddataset, i \in \mathbb{N}(\datasubset)$, if $\beta_i(\mathcal{S}) > 0$ and $\beta_{-i}(\mathcal{S}) \leq 0$, we have $v(\reportedstoi\cup \reportedstominusi) > v(\stoi\cup \reportedstominusi)$; if $\beta_i(\mathcal{S}) < 0$ and $\beta_{-i}(\mathcal{S}) \geq 0$, we have $v(\reportedstoi\cup \reportedstominusi) < v(\stoi\cup \reportedstominusi)$; otherwise, $v(\reportedstoi\cup \reportedstominusi) = v(\stoi\cup \reportedstominusi)$.
    % We say that a client $i$ performs a data overvaluation attack against $\phi(\granddataset, v)$ when they report a dataset $\reportedstoi \neq \stoi$ such that $v(\reportedstoi\cup \reportedstominusi) > v(\stoi\cup \reportedstominusi)$ if $\beta_i(\mathcal{S}) > 0$, or that $v(\reportedstoi\cup \reportedstominusi) < v(\stoi\cup \reportedstominusi)$ if $\beta_i(\mathcal{S}) < 0$, where $\datasubset \subset \granddataset$, and $i \in \mathbb{N}(\datasubset)$.
    % For any subset $\datasubset \subset \granddataset$, and for any client $i \in \mathbb{N}(\datasubset)$, a data overvaluation attack by client $i$ over subset $\datasubset$ is to report $\reportedstoi \neq \stoi$ such that $v(\reportedstoi\cup \reportedstominusi) > v(\stoi\cup \reportedstominusi)$ if $\beta_i(\mathcal{S}) > 0$, or that $v(\reportedstoi\cup \reportedstominusi) < v(\stoi\cup \reportedstominusi)$ if $\beta_i(\mathcal{S}) < 0$.
\end{definition}

\begin{lemma}
\label{lem:attack_success}
    If the goal is achieved, a data overvaluation attack against a linear data value $\phi_i(\granddataset, v)$ ensures that $\empiricaldatavalue_{i}(\granddataset, v) \geq \empiricaldatavalue_{i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$ while $\empiricaldatavalue_{-i}(\granddataset, v) \leq \empiricaldatavalue_{-i}(\granddataset, v \mid \forall \datasubset \subset \granddataset, \reportedstoi =  \stoi)$.
\end{lemma}



% An important question is: If client $i$ manipulates a data block $D_{i, j}$ in data valuation, can the server easily observe it? 
% In a centralized learning scenario, since client $i$ should upload their raw data, the server can compare $D_{i,j}'$ with $D_i$ to detect if it is manipulated, where $D_{i,j}'$ is a reported version of $D_{i,j}$.
% However, in a decentralized learning scenario, client $i$ needs to upload a transformed representation $\theta_{D_{i,j}}$ of $D_{i,j}$ to the server.
% Consequently, it is generally challenging for the server to detect the manipulation by comparing $\theta_{D_i}$ and $\theta_{\set[D_{i,j}']}$ considering the complexity of the transformation, where $\theta_{D_{i,j}'}$ is a reported version of $\theta_{D_{i,j}}$.
% Therefore, the data overvaluation attack, as defined below, is non-trivial.

