\section{Preliminaries}

\subsection{Data Valuation Problem}
We consider a CML scenario where $N$ clients (i.e., data owners) $\clientset=\set[1,\dots, N]$ collaboratively train an ML model under the coordination of a server (e.g., a model buyer or a broker in the data market).
Each client $i\in \clientset$ possesses a dataset $D_i$ that includes $M_i$ data blocks.
A data block $D_{i, j} \in D_i$ could be a subset of the samples in $D_i$, a subset of the features in $D_i$, or even a subset of the features from a subset of the samples in $D_i$.
% Without loss of generality, we assume that each dataset $D_i$ consists of $M$ ($M \geq M_i, \forall i$) data blocks $D_{i,1},\dots, D_{i, M}$ where $D_{i,1},\dots, D_{i, M_i}$ are non-empty, and the other blocks are empty.
We write $\clientsubset \subseteq \clientset$ to denote a subset of clients and $D_{\clientsubset}$ to denote the set of data blocks possessed by clients $\clientsubset$, i.e., $D_{\clientsubset} = \cup_{i \in \clientsubset} D_i = \{D_{i,j} \mid i\in \clientsubset, j\in[M_i]\}$.
Given all clients' data blocks, the server utilizes a CML algorithm $\mathcal{A}$ to train an ML model $\mathcal{A}(\granddataset)$ on the grand dataset $\granddataset$. 



% Then, the server utilizes a CML algorithm $\mathcal{A}$ to train an ML model $\Theta_{D_\clientset} = \mathcal{A}(\granddataset)$ on the grand dataset $\granddataset$, which is referred to as the \textit{global model}. 
% Due to data owners' increasing privacy concerns in practice, we assume that Algorithm $\mathcal{A}$ is a decentralized CML algorithm, such as a federated learning (FL) algorithm~\citep{mcmahan2017communication}, that runs as follows.
% \begin{itemize}[leftmargin=*]
%     \item \textit{Local training}: Each client $i$ employs a learning algorithm $\mathcal{A}_i$ to train a data representation $\theta_{D_i}=\mathcal{A}_i(D_i)$ of $D_i$ and then submits $\theta_{D_i}$ to the server.
%     Note that $\theta_{D_i}$ can represent any form of data contribution from $D_i$, which could be an ML model/gradient, or even multiple models/gradients obtained during multiple rounds of model training.
%     \item \textit{Global fusion}: Given data presentations $\theta_{D_1},\dots,\theta_{D_N}$ submitted by all clients, the server calls a fusion algorithm $\mathcal{A}_0$ to obtain the global model $\Theta_{D_\clientset}=\Theta_{\cup_{i\in\clientset}D_i}=\mathcal{A}_0(\theta_{D_1},\dots,\theta_{D_N})$.
% \end{itemize}
% Note that the above abstraction covers most existing decentralized CML algorithms, including FedAvg~\citep{mcmahan2017communication}, SplitNN~\citep{gupta2018distributed}, and FedMD~\cite{li2019fedmd}. 
% Additionally, the results of this paper can be easily applied to multi-round CML: we just need to consider each client's submissions in multiple rounds as a single data representation.

% Specifically, in Algorithm $\mathcal{A}$, each client $i$ does not upload their local dataset $D_i$ but instead submits a transformed representation of $D_i$, such as a local model or a local gradient trained on the local dataset in FL, to the server to construct the global model.


After model training, the server performs \textit{data valuation} to evaluate each data block $D_{i,j}$'s \textit{block-level data value} $\phi_{i,j}(\granddataset, v)$, which reflects the contribution of $D_{i,j}$ to improving the utility $v(\granddataset)$ of the global model $\mathcal{A}(\granddataset)$.
We write $\phi_{i,j}(\granddataset, v)$ as $\phi_{i,j}$ for simplicity when there is no ambiguity.
Then, each client $i$'s \textit{client-level data value} $\phi_i(\granddataset, v)$ or simply $\phi_i$ is the sum of their data blocks' data values, i.e., $\phi_i = \sum_{j\in[M_i]} \phi_{i,j}$. 
Consequently, the data valuation problem is to design a data valuation metric $\phi$, defined as follows, to determine data values for all data blocks involved in the CML.
\begin{definition}[Data Valuation]
    A utility metric $v: 2^{\granddataset}\rightarrow \mathbb{R}$ maps a subset of data blocks $S \subseteq \granddataset$ to the utility $v(S)$ of the model $\mathcal{A}(S)$ with $v(\emptyset) = 0$. 
    A data valuation metric $\phi: \granddataset \times \mathcal{G}(\granddataset)$ allocates data values $\{\phi_{i,j}(\granddataset, v) \mid i\in \clientset, j\in [M_i]\}$, where $\mathcal{G}(\granddataset) = \{v\mid v:2^{\granddataset} \rightarrow \mathbb{R}\}$.
\end{definition}

Data valuation facilitates the following two downstream tasks, which ensure fairness and incentivize clients to participate in CML:
\begin{itemize}[leftmargin=*]
    \item \textit{Data selection}: The server selects data blocks $D_{i,j}$ with high(er) block-level data values $\phi_{i,j}$ to enhance the performance of CML next time~\citep{cohen2005feature, nagalapatti2021game}, which is critical when there exist clients who contribute trivial data or outliers.
    \item \textit{Reward allocation}: The server allocates rewards $R_i(\phi_1,\dots,\phi_N)$ to each client $i$ based on their client-level data values $\phi_i$.
    The rewards may be revenue obtained from commercializing the global model (i.e., monetary rewards~\citep{nguyen2022trade}) or customized models with differing utility (i.e., model rewards~\citep{sim2020collaborative}).
    % Clients with higher client-level data values obtain more valuable rewards.
    Each client $i$'s reward $R_i(\phi_1,\dots,\phi_N)$ increases with their own data value $\phi_i$ and decreases with the sum of the other clients' data values $\phi_{-i} = \sum_{i'\in \mathbb{N} \setminus \{i\}} \phi_{i'}$. 
    Therefore, we assume that each client $i$ is selfish and rational, aiming to maximize $\phi_i$ while minimizing $\phi_{-i}$.
\end{itemize}


\subsection{Shapley Value for Data Valuation}
\label{sec:sv}
As a classic metric for contribution evaluation in cooperative game theory, the Shapley value (SV) has been widely adopted for data valuation in CML. 
It calculates the average contribution of each participant to a coalition. 
In our scenario, as each data block can be regarded as a participant, the SV determines the data value $\sv_{i,j}$ of each data block $D_{i,j}$ as follows.
\begin{align}
\label{eq:sv}
    &\medmath{\sv_{i,j}(\granddataset, v) \coloneqq \sum_{\mathcal{S}\subseteq \granddataset \setminus \set[D_{i,j}]} \weightsv(\mathcal{S}) \big(v(\splus) - v(\mathcal{S})\big)}
\end{align}
where $\weightsv(\mathcal{S}) \coloneqq  \frac{|\mathcal{S}|!(|\granddataset|- |\mathcal{S}| - 1)!}{|\granddataset|!}$ and $\splus = \mathcal{S} \cup \{D_{i,j}\}$.
Specifically, the SV enumerates all possible subsets $\mathcal{S}$ of $\granddataset$ excluding the data block $D_{i,j}$.
The term $\big(v(\splus) - v(\mathcal{S})\big)$ quantifies the utility improvement achieved by adding $D_{i,j}$ to subset $\mathcal{S}$, and computing $v(\mathcal{S})$ and $v(\mathcal{S}^+)$ requires model retraining.
$\weightsv(\mathcal{S})$ is a coefficient that weights the importance of $\mathcal{S}$.
The data value $\sv_{i,j}$ thus is the weighted aggregation of all utility improvements attributed to $D_{i,j}$.

The SV is considered an ideal solution to data valuation because it has been proven be to the unique valuation metric that satisfies the following axioms~\citep{shapley1953value}.
\begin{itemize}[leftmargin=*]
    \item \textit{Linearity (LIN)}: The server can linearly combine the data values evaluated on any two utility metrics $v_1$ and $v_2$, i.e., $\phi_{i,j}(\granddataset, v_1+v_2) = \phi_{i,j}(\granddataset, v_1)  + \phi_{i,j}(\granddataset, v_2)$.
    \item \textit{Efficiency (EFF)}: The sum of all data blocks' data values equals the utility improved by the grand dataset $\granddataset$, i.e., $\sum_{i\in \clientset} \sum_{j\in[M_i]} \phi_{i,j}(\granddataset, v) = v(\granddataset)$.
    \item \textit{Dummy actions (DUM)}: If a data block $D_{i,j}$ does not have any synergy with the other blocks, its data value $\sv_{i,j}$ equals the utility $v(D_{i,j})$ of the model trained only on itself.
    That is, if for all $\mathcal{S}\subseteq \granddataset \setminus \set[D_{i,j}]$, we have $v(\mathcal{S}\cup D_{i, j}) - v(\mathcal{S}) = v(D_{i, j})$, then $\phi_{i,j}(\granddataset, v) = v(D_{i,j})$.
    % \item \textit{Client-level symmetry (SYM-C)}: If two clients' data have the same effect on the model utility, they should obtain the same client-level data values.
    % Formally, if for two clients $i_1, i_2 \in \clientset$ and for any subset of clients $\clientsubset\subseteq \clientset \setminus \set[i_1, i_2]$, we have $v(D_{\clientsubset} \cup D_{i_1}) = v(D_{\clientsubset} \cup D_{i_2})$, then $\sv_{i_1}(\granddataset, v) =\sv_{i_2}(\granddataset, v)$.
    \item \textit{Symmetry (SYM)}: If two data blocks have the same effect on the model utility, they should obtain the same block-level data values.
    In other words, for two data blocks $D_{i_1,j_1}, D_{i_2,j_2} \in \granddataset$, if for any subset of data blocks $\mathcal{S} \subseteq \granddataset \setminus \set[D_{i_1,j_1}, D_{i_2,j_2}]$, we have $v(\mathcal{S} \cup D_{i_1,j_1}) = v(\mathcal{S} \cup D_{i_2,j_2})$, then we have $\phi_{i_1,j_1}(\granddataset, v) =\phi_{i_2, j_2}(\granddataset, v)$.
\end{itemize}


\begin{theorem}[Uniqueness of SV~\citep{shapley1953value}]
\label{thm:sv_unique}
    The SV $\sv$ is the unique data valuation metric that satisfies DUM, SYM, LIN, and EFF.
\end{theorem}



