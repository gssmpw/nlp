%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xspace}
\usepackage{pgf}
\usepackage{import}
\usepackage{multirow}
\usepackage{tcolorbox}
\def\mathdefault#1{#1}
\everymath=\expandafter{\the\everymath\displaystyle}


%%
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage[colorlinks=true, allcolors=blue]{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{enumitem}
\usepackage{cleveref}
\usepackage[table]{xcolor}
\usepackage{booktabs}

\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{natbib}


\usepackage{etoolbox}

% Create unnumbered hypothesis environment
\newtheorem*{pg-hypothesis}{The Policy Gradient Hypothesis}

% Adjusted anchor placement with vertical offset
\AtBeginEnvironment{pg-hypothesis}{%
    \vspace*{-2ex}% Pull anchor point up by 2ex
    \phantomsection\label{hyp:policy_gradient}%
    \vspace*{2ex}% Restore original spacing
}

% TWO reference commands for manual control
\newcommand{\policyhyp}{\hyperref[hyp:policy_gradient]{the policy gradient hypothesis}} % No space
\newcommand{\policyhypesp}{\hyperref[hyp:policy_gradient]{the policy gradient hypothesis\hspace{0.33em}}} % With space



\newcommand{\ev}[1]{{\textcolor{blue}{[Ev: #1]}}}
\newcommand{\sam}[1]{{\textcolor{red}{[Sam: #1]}}}
\newcommand{\somo}[1]{{\textcolor{green}{[Somo: #1]}}}
\newcommand{\nat}[1]{{\textcolor{orange}{[Nathan: #1]}}}
\newcommand{\gabri}[1]{{\textcolor{purple}{[** Gabri: #1 **]}}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Reevaluating Policy Gradient Methods for Imperfect-Information Games}

\NewDocumentCommand \PKGName {} {\texttt{exp-a-spiel}\xspace}

\usepackage{inconsolata}

\begin{document}

\twocolumn[
\icmltitle{Reevaluating Policy Gradient Methods for Imperfect-Information Games
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In the past decade, motivated by the putative failure of naive self-play deep reinforcement learning (DRL) in adversarial imperfect-information games, researchers have developed numerous DRL algorithms based on fictitious play (FP), double oracle (DO), and counterfactual regret minimization (CFR). In light of recent results of the magnetic mirror descent algorithm, we hypothesize that simpler generic policy gradient methods like PPO are competitive with or superior to these FP, DO, and CFR-based DRL approaches. To facilitate the resolution of this hypothesis, we implement and release the first broadly accessible exact exploitability computations for four large games. Using these games, we conduct the largest-ever exploitability comparison of DRL algorithms for imperfect-information games. Over 5600 training runs, we find that FP, DO, and CFR-based approaches fail to outperform generic policy gradient methods.
\end{abstract}



\section{Introduction}
\label{sec:intro}

An imperfect-information game (IIG) is one in which there is information asymmetry between players or in which two players act simultaneously.
Two-player zero-sum IIGs---those in which two players with opposite incentives compete against one another---are among the most well-studied IIGs.
This is in part due to the fact that, for these games, there is a sensible and tractable objective: minimizing the amount by which a player can be exploited by a worst-case adversary.

A natural approach to model-free deep reinforcement learning (DRL) in such games is to deploy a single-agent algorithm in self-play.
However, because imperfect information induces cyclical best response dynamics, such an approach can fail catastrophically, yielding policies that are maximally
exploitable.
So as to avoid this outcome, most existing literature focuses on adapting pre-existing tabular algorithms with established equilibrium convergence guarantees, such as fictitious play (FP) \citep{fp51,robinson_fp51}, double oracle (DO) \citep{do03}, and counterfactual regret minimization (CFR) \citep{cfr07}, to DRL.

Unfortunately, constructing effective DRL approaches from these tabular algorithms has proven challenging.
FP and DO require expensive best response computations at each iteration (i.e., each iteration requires solving an entire reinforcement learning problem) and can exhibit slow convergence as a function of iteration number \citep{do_lower24}.
While CFR converges more quickly, its immediate adaptation to model-free reinforcement learning requires importance weighting over trajectories \citep{mccfr09}, resulting in high variance feedback to which function approximation is poorly suited.
Moreover, none of FP, DO and CFR enjoy last-iterate convergence, creating a layer of indirection to extract the desired policy.

Recently, \citet{mmd23} demonstrated the promise of an alternative algorithm, a policy gradient (PG) method called magnetic mirror descent (MMD).
They showed that MMD achieves competitive performance with CFR in tabular settings, while retaining the deep learning compatibility native to reinforcement learning algorithms.

In this work, we expound the relationship between MMD and generic deep PG methods such as PPO \citep{ppo17}.
Like MMD, the improvement steps of these other generic PG methods 1) maximize expected value, 2) regularize the policy, and 3) control the size of the update.
These parallels lead us to the question:
Given that MMD performs well as a DRL algorithm for IIGs, shouldn't these other generic PG methods perform well, too?

We believe that the answer to this question is yes.

To further this idea, we put forth the following hypothesis:
\begin{quote}\centering\itshape
    With proper tuning, generic PG methods are\\ competitive with or superior to FP, DO, and CFR-based DRL approaches in IIGs.
\end{quote}

The confirmation of this hypothesis would have large ramifications both for researchers and practitioners. It would force researchers who currently dismiss approaches like PPO to revisit fundamental beliefs. And, for practitioners, it would justify the use of simpler PG methods, potentially offering both cleaner implementations and improved performance.


However, assessing the above hypothesis is not straightforward. Doing so demands exact exploitability computations for games large enough to be representative of settings to which DRL is relevant, but small enough that these computations are tractable in reasonable time. Unfortunately, due to the complexity of implementing these computations for large games, few are publicly available, even in OpenSpiel \citep{openspiel19}---the standard library for DRL in IIGs. This issue has plagued the field, forcing researchers to resort to tenuous metrics and making progress difficult to measure.

We address this issue by implementing exact exploitability computation for four large games---3x3 Dark Hex, Phantom Tic-Tac-Toe, 3x3 Abrupt Dark Hex, and Abrupt Phantom Tic-Tac-Toe---ourselves.
These games have 10s of billions of nodes in their game trees---over one million times more than
many games for which exploitability is typically reported, such as Leduc Hold'em \citep{leduc05}.

Armed with these benchmarks, we undertake the largest-ever exploitability comparison of DRL algorithms for IIGs, spanning 5600 total runs across 350 hyperparameter configurations, requiring over 278,000 CPU hours, and including NFSP \citep{nfsp16}, PSRO \citep{psro17}, ESCHER \citep{escher23}, R-NaD \citep{deepnash22}, MMD \citep{mmd23}, PPO \citep{ppo17} and PPG \citep{ppg20}.
Over these runs, all of which are included in the paper, NFSP, PSRO, ESCHER, and R-NaD fail to outperform the generic PG methods (MMD, PPO, PPG).

We release our code for computing exact exploitability and head-to-head values, which require under 2 minutes on commodity hardware, with OpenSpiel-compatible Python bindings, as well as efficient implementations of tabular solvers and best responses. We encourage the community to use these tools both to test our hypothesis and as benchmarks for future research.

\section{Preliminaries}

To set context, we give a formalization of IIGs and exploitability, and an overview of model-free DRL approaches to IIGs.

\subsection{Game Formalism}
IIGs are often formalized as perfect-recall extensive-form games (EFGs) \citep{kuhn2003lectures}.
Our formalism is equivalent to perfect-recall EFGs, but possesses greater superficial similarity to traditional reinforcement learning notation. We notate a game as a tuple \[\langle \mathbb{S}, \mathbb{A}, \mathbb{O},  \mathbb{I}, \mathcal{R}, \mathcal{T}, \mathcal{O}, \mathcal{C}, t_{\text{max}} \rangle,\]
where
\begin{itemize}[nosep,leftmargin=*]
    \item $\mathbb{S}$ is the space of game states;
    \item $\mathbb{A}$ is the space of actions;
    \item $\mathbb{O}$ is the space of observations;
    \item $\mathbb{I} = \cup_t (\mathbb{O} \times \mathbb{A})^t \times \mathbb{O}$ is the space of information states;
    \item $\mathcal{T} \colon \mathbb{S} \times \mathbb{A} \to \Delta(\mathbb{S})$ is the transition function;
    \item $\mathcal{R} \colon \mathbb{S} \times \mathbb{A} \to \mathbb{R}$ is the reward function for player 1 (and the cost function for player 2);
    \item $\mathcal{O} \colon \mathbb{S} \to \mathbb{O}$ is the observation function, which determines the observation given to the acting player;
    \item $\mathcal{C} \colon \mathbb{S} \to \{1, 2\}$ is the player choice function, which determines the acting player;
    \item $t_{\text{max}}$ is the time step after which the game terminates.
\end{itemize}
The players interact with the game using policies $\pi_i \colon \mathbb{I} \to \Delta(\mathbb{A})$ for $i \in \{0, 1\}$ mapping information states to distributions over actions.

The objective of player 1 and loss of player 2 is the expected return:
\[\mathcal{J}(\pi) = \mathbb{E}_{\pi} \left[G\right] = \mathbb{E}_{\pi} \left[ \sum_{t=1}^{t_{\text{max}}} \mathcal{R}(S^t, A^t)\right].\]

However, directly optimizing the expected return is only possible given a particular opponent or distribution of opponents.
In practice, identifying this distribution may be prohibitively expensive or infeasible---such a distribution may not even exist.

The standard resolution to this issue is the pursuit of minimax guarantees.
The metric associated to minimax guarantees is called exploitability:
\[\text{expl}(\pi) = \frac{\max_{\pi_0'}\mathcal{J}(\pi_0', \pi_1) - \min_{\pi_1'}\mathcal{J}(\pi_0, \pi_1')}{2}.\]
Exploitability is the expected return of a worst-case opponent, called a best response, assuming it plays half of the time as player 1 and half of the time as player 2. Joint policies with exploitability zero are Nash equilibria.

Exploitability is an attractive resolution both because: 1) if exploitability zero is achieved, no opponent can do better than tie in expectation (assuming the player plays each role half of the time); and 2) in complex games, it tends to be the case that even achieving low, positive exploitability leads to reliably winning against opponents in practice.

\subsection{Algorithmic Approaches}
\label{sec:algorithms}

Existing model-free DRL approaches to two-player zero-sum IIGs are largely based on one of naive self-play, best responses, CFR, and regularization.

\subsubsection{Naive self-play} Applying model-free DRL algorithms to two-player zero-sum IIGs in self-play is appealing for its simplicity; however, it is easy to show that doing so naively can lead to catastrophic results.
Some value-based algorithms cannot even express non-deterministic policies.
This makes it impossible for them to achieve low-exploitability in many games (e.g., in rock-paper-scissors, deterministic policies are maximally exploitable).
While PG methods can at least express non-deterministic policies, their learning dynamics generally cycle, diverge or exhibit chaotic behavior, rather than converge to Nash equilibria \citep{vortices19}.

\subsubsection{Best Responses} A next-simplest alternative is to leverage tabular game-theoretic algorithms that rely on best-response computations.
These game-theoretic algorithms support a plug-and-play workflow with model-free DRL, by substituting the exact best response for an approximate best response computed via DRL. 
And, unlike naive self-play, they come with convergence guarantees.

Fictitious play (FP) \citep{fp51,robinson_fp51} is one such game-theoretic algorithm.
At each iteration $t$, FP computes a best response $\pi_t$ to the average of the previous iterations $\bar{\pi}_{t-1} = \text{avg}(\pi_1, \dots, \pi_{t-1})$.
This average is over the sequence form of the policy, meaning that it is equivalent to a policy that uniformly samples $T \sim \text{uniform}(\{1, \dots, t-1\})$ at the start of each game and then plays according to $\pi_T$ until the game's conclusion.
The exploitability of the average $\bar{\pi}_t$ of a sequence of policies generated by FP converges to zero as $t$ grows large.

Double oracle (DO) \citep{do03} is another such game-theoretic algorithm. Like FP, DO works by iteratively computing best responses.
However, rather than computing best responses to the average of the previous iterates, DO computes best responses $\pi_t$ to Nash equilibria $\sigma_t^{\ast}$ of a so-called metagame.
This metagame is a one-shot game in which players select policies $\pi^{(1)}, \pi^{(2)}$ from among the previous best responses $\{\pi_1, \dots, \pi_{t-1}\}$ and receive as payoff the expected value $\mathcal{J}(\pi^{(1)}, \pi^{(2)})$ of playing these policies against one another.
A policy of this metagame is a mixture $\sigma_t$ over previous best responses $\{\pi_1, \dots, \pi_{t-1}\}$ that is enacted by sampling $\pi_T \sim \sigma_t$ once at the start of each game and then playing according to $\pi_T$ until the game's conclusion.
A policy of this metagame $\sigma_t$  is unexploitable if none of the previous best responses $\{\pi_1, \dots, \pi_{t-1}\}$ exploits it.
The exploitability of a sequence of metagame Nash equilibria is guaranteed to converge to zero as $t$ grows large.

Unfortunately, both FP and DO can suffer from slow convergence.
This is true not only in practice, but also according to the exponential best-known complexity for FP and exponential lower bound for DO \citep{do_lower24}.
Requiring a large number of iterations is especially concerning in the context of DRL because each iteration requires solving an entire reinforcement learning problem.
Despite numerous works on DRL variants of both FP \citep{nfsp16,nfsp-em19,alphastar19,nfsp-adapt23,nfsp-eve23,nfsp-plt23} and DO \citep{psro17,psro-pipe20,xdo21,iterative-psro21,anytimepsro22,odo22,epsro22,psro-nontrans23,rmdo23,sppsro24,sapsro24,fusion-psro24}, it is not clear that this issue has been addressed.

\subsubsection{Counterfactual Regret Minimization} Counterfactual regret minimization (CFR) \citep{cfr07}, widely regarded as the gold standard for tabularly solving IIGs, generally converges faster than FP or DO. 
Instead of relying on best responses, CFR is grounded in the principle of regret minimization.
A key result of regret minimization is that, in two-player zero-sum games, the average strategy of two regret minimizing learners converges to a Nash equilibrium. 
CFR applies this principle by independently minimizing regret at each information state.
By appropriately weighting feedback, CFR ensures regret minimization across the entire game, thus guaranteeing convergence to a Nash equilibrium.

Because CFR is more involved than FP and DO, it does not admit the same kind of plug-and-play workflow with single-agent model-free DRL. Instead, new methods \citep{dream20,armac20,escher23} have been designed specifically around CFR. Unfortunately, these new methods come with new weaknesses. The most straightforward method \citep{dream20}, for instance, relies on importance sampling over trajectories, which leads to high variance in feedback, especially in games with many steps. This makes it challenging for function approximators to learn. Although \citet{escher23} resolve this issue, their resolution necessitates a uniform behavioral policy, making it difficult to achieve sufficient coverage of relevant trajectories.

\subsubsection{Regularization} Motivated by the limitations of FP, DO, and CFR-based methods, \citet{fforel21} proposed an alternative approach centered around regularization. Unlike previous methods, their approach is designed to converge in the last iterate, aligning it more closely with typical DRL techniques. \citet{fforel21}'s work has been influential, serving as inspiration for several subsequent studies \citep{fu2022actorcritic,deepnash22,mmd23,ftrl-ent23,reg-enough24}. Among them, \citet{mmd23} introduced a policy gradient algorithm called magnet mirror descent (MMD), characterized by the following update rule:
\begin{align}
\pi_{t+1} = \underset{\pi}{\text{argmax}} \underset{A \sim \pi}{\mathbb{E}} q(A) - \alpha \text{KL}(\pi, \rho) - \frac{1}{\eta}\text{KL}(\pi, \pi_t), \label{eq:mmd}
\end{align}
where $q(A)$ is the value of action $A$, $\alpha$ is a regularization temperature, $\eta$ is a stepsize, $\rho$ is a magnet policy, and $\text{KL}$ is KL divergence.
In a surprising result, \citet{mmd23} demonstrated that MMD, implemented as a standard tabular self-play reinforcement learning algorithm, achieves performance competitive with that of CFR in IIGs.
\citet{mmd23} also give empirical evidence that MMD is performant as a DRL algorithm.

\section{The Policy Gradient Hypothesis}
\label{sec:hypothesis}

The surprising performance of magnetic mirror descent invites reflection.
Consider the functions served by the components of \Cref{eq:mmd}:
\begin{itemize}[leftmargin=*]
    \item $\mathbb{E}_{a \sim \pi} q_t(A)$ maximizes the expected return.
    \item $- \alpha \text{KL}(\pi, \rho)$ regularizes the updated policy (if $\rho$ is uniform, this functions similarly to an entropy bonus).
    \item $- \frac{1}{\eta} \text{KL}(\pi, \pi_t)$ constrains the update size.
\end{itemize}
Far from being unique to MMD, these functions have long been present in deep PG methods, such as TRPO \citep{trpo15}, PPO \citep{ppo17}, and PPG \citep{ppg20}. Indeed, as with MMD, the update rules of these algorithms include terms that maximize expected value and encourage entropy; they also possess mechanisms that, by various means, control the size of the update.

Yet, despite this shared ethos, generic PG methods---typically PPO---appear in literature only as underperforming baselines.
This raises the question: Why has MMD performed well, while these generic PG methods have not?

There are multiple answers worth considering.
One possibility is that the differences between MMD and other generic PG methods are material.
Unlike most PG methods, which use some combination of gradient clipping and forward KL divergence, MMD employs reverse KL divergence to regulate update size.
In tabular settings, where competitive performance requires high precision, this distinction is manifestly important.

However, in the context of deep learning, it is less obvious that this variation matters.
Empirical research 
has raised questions about the extent to which these different approaches to controlling update size vary in performance \citep{Engstrom2020Implementation,revisiting20,forwardreverse22}, and suggested that none of them actually succeed in controlling update size \citep{Ilyas2020A}.

A second possibility is that these generic PG methods are fully capable of performing well, but have not been run with good hyperparameters.
Ever-present reasons that baselines tend to perform poorly, such as the expense and difficulty of tuning (and, not least, the disinclination of researchers toward showing baselines outperforming their algorithmic contributions), could be at work.

But there are also several domain-specific factors that make this second possibility particularly plausible:
\begin{itemize}[leftmargin=*]
    \item There is a putative belief that generic PG methods do not work in IIGs and that more game theoretically involved algorithms are required.
    Due to this belief, tuning may have been confounded by confirmation bias.
    \item Works that report generic PG method baselines often rely on head-to-head results. The intransitive nature of these results increases the complexity of tuning, and may thereby have hampered it.
    \item The hyperparameter regime for which \citet{mmd23} showed MMD to be effective involves more entropy regularization than is typically used for PG methods in single-agent problems.
    Thus, it is possible effective hyperparameter configurations were overlooked due to their regime being too far removed from those of single-agent settings.
\end{itemize}

Based on these reasons, this work posits that generic PG methods are fully capable of performing well, a position we formalize under the following hypothesis.
\begin{tcolorbox}
\begin{pg-hypothesis}
Appropriately tuned policy gradient methods that share an ethos with magnetic mirror descent are competitive with or superior to model-free deep reinforcement learning approaches based on fictitious play, double oracle, or counterfactual regret minimization in two-player zero-sum imperfect-information games.
\end{pg-hypothesis}
\end{tcolorbox}

The confirmation of \policyhypesp
 would have ramifications on both research and practice.
On the research side, it would vindicate generic PG methods, which have generally either been dismissed as unsound or relegated to the role of sacrificial baseline.
Practically, it would lead to simpler implementations and potentially significant performance improvements in applications where FP, DO, and CFR-based DRL algorithms are currently employed.
These applications include autonomous vehicles \citep{nfsp_av18}, network security \citep{psro_scale_free20,nfsp_network21}, robot confrontation \citep{psro_robot22}, eavesdropping \citep{nfsp_eavesdrop22}, radar (anti-)jamming \citep{nfsp_radar22,cfr_jamming22,nfsp_radar23,nfsp_antijamming24}, intrusion response \citep{nfsp_security23}, aerial combat \citep{nfsp_dogfight23,nfsp_dogfight24}, pursuit-evasion games \citep{psro_pursuit23,psro_pursuit24}, racing \citep{zheng2024racing}, language model alignment \citep{psro_lm24}, and language model red teaming \citep{psro_red_team24}.


\section{Benchmarks}
\label{sec:games}


Unfortunately, evaluating DRL algorithms for adversarial IIGs is not well standardized.
Due to the difficulty of efficiently implementing exploitability for the large IIGs, there is a dearth of accessible implementations.
As a result, only a small number of works \citep{dream20,armac20,nfsp-eve23} make the effort to report exploitability in large games.

The rest of literature predominantly resorts to some combination of two unsatisfactory metrics. The first is exploitability in the same small EFGs that served as benchmarks for tabular solvers, such as Leduc Hold'em \citep{leduc05}. Benchmarking in these small games is unsatisfactory because success requires learning a precise near-Nash policy for a handful of repeatedly revisited information states---a fundamentally different challenge from large games, where success requires learning a policy that is strong (but not necessarily numerically close to Nash) for a vast number of unvisited information states. The second is head-to-head evaluations in large games. These evaluations are unsatisfactory because the intransitive dynamics typically present in imperfect-information games, such as rock-beats-scissors-beats-paper-beats-rock, muddle interpretations of relative strength. Exacerbating the issue, there is no community coordination on head-to-head opponents, making direct cross-paper comparisons impossible.

This predicament has created an undesirable state of affairs. Many works claim ``state-of-the-art'' performance, but even experts in specific algorithm classes are often unsure which instances are actually effective.

To address this predicament, we set out to implement efficient computations for four large games---Phantom Tic-Tac-Toe (PTTT), 3x3 Dark Hex (DH3), Abrupt Phantom Tic-Tac-Toe (APTTT), and 3x3 Abrupt Dark Hex (ADH3)---ourselves.
These games are imperfect-information twists on the perfect-information games Tic-Tac-Toe and 3x3 Hex.

In Tic-Tac-Toe, which is played on a 3-by-3 grid with square cells, the objective is to get three-in-a-row horizontally, vertically, or diagonally.
If the board is filled without either player having three-in-a-row, the game ends in a draw.
An example game is shown in \Cref{fig:ex_games} (top) in which the first moving player (red) wins via the diagonal.

In 3x3 Hex, which is played on a rhombus with hexagonal cells, the first moving player's objective is to connect the top edge of the board to bottom edge, while the second moving player's objective is to connect the left edge of the board to the right edge.
Draws are not possible because the board provably cannot be filled without a player winning \citep[Hex Theorem]{gale1979game}.
An example game is shown in \Cref{fig:ex_games} (bottom) in which the first moving player (red) wins by connecting the top and bottom edges.


\begin{figure}
    \centering
    % \subfigure[The first-moving player (red) wins by getting three-in-a-row along the diagonal.]
    {
    \includegraphics[width=0.117\linewidth]{figures/ttt/null.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S3D0S.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S3D0S8D.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S3D0S8D6S.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S3D0S8D6S5D.pdf}~
    \includegraphics[width=0.117\linewidth]{figures/ttt/4S3D0S8D6S5D2S.pdf}
    }

    \vspace{3mm}
    % \subfigure[The first moving player (red) wins by connecting the top and bottom edges. \label{fig:ttt:hex}]
    {
    \includegraphics[width=0.16\linewidth]{figures/hex/null.pdf}\hspace{-1.0mm}
    \includegraphics[width=0.16\linewidth]{figures/hex/2S.pdf}\hspace{-1.0mm}
    \includegraphics[width=0.16\linewidth]{figures/hex/2S4D.pdf}\hspace{-1.0mm}
    \includegraphics[width=0.16\linewidth]{figures/hex/2S4D5S.pdf}\hspace{-1.0mm}
    \includegraphics[width=0.16\linewidth]{figures/hex/2S4D5S8D.pdf}\hspace{-1.0mm}
    \includegraphics[width=0.16 \linewidth]{figures/hex/2S4D5S8D7S.pdf}
    }
    \caption{A game of Tic-Tac-Toe (top) and 3x3 Hex (bottom).\vspace{-1em}}
    \label{fig:ex_games}
\end{figure}

In the \textit{dark} and \textit{phantom} variants of these games, actions are concealed from the non-acting player, creating imperfect information. 
These imperfect-information variants have two rulesets---classical and \textit{abrupt}---which differ in how they handle the situation in which the acting player attempts to place a piece on a cell occupied by their opponent's piece.
In the classical ruleset, the acting player must select a different cell.
In abrupt, the acting player loses their turn.


These games have at least three significant strengths as benchmarks:
\begin{enumerate}[leftmargin=*]
    \item OpenSpiel \citep{openspiel19} support: 
    DH3, PTTT, and ADH3 are already implemented in OpenSpiel, making it easy for other researchers to use them.
    To address APTTT's absence, we provide our own game implementation.
    \item Precedent: Existing work has used PTTT and ADH3 for head-to-head evaluations \citep{mmd23,escher23,ftrl-ent23,reg-enough24}, offering external evidence for their relevance.
    \item Size: These games allow model-free DRL algorithms to be trained in minutes or hours on commodity hardware. But, as detailed in \Cref{table:game-vals}, they have 10s of billions of game states, making them quite large by the standards of game solving benchmarks.
\end{enumerate}

\begin{table}[tbhp]
\centering
  \rowcolors{2}{gray!25}{white}
  \renewcommand{\arraystretch}{1.2}
  \centering
  \caption{Fundamental game quantities. Positive Nash values (i.e., expected values for player 1 at Nash equilibria) mean that player 1 possesses a structural advantage. The Nash value of 1 for DH3 implies a guaranteed winning strategy for player 1, which we describe in \Cref{appen:dh3-solution}.} \label{table:game-vals}
  \small
\begin{tabular}{lrrc} 
    % \rowcolor{gray!50}
    \toprule
    \bf Game & \bf States & \bf Info. states & \bf Nash value $\pm$ error \\
    \midrule
    DH3 & 19.12B & 6.07M &   1.00000 $\pm$ 0.00000 \\
    ADH3 & 29.31B & 27.33M & 0.38443 $\pm$ 0.00021 \\
    % \midrule
    PTTT & 19.93B & 5.99M & 0.66665 $\pm$ 0.00026 \\
    APTTT & 27.12B & 23.31M &  0.55017 $\pm$ 0.00014 \\
    \bottomrule
\end{tabular}
\end{table}

The size of these games makes it non-trivial to implement exploitability computations. To accommodate their magnitude, we utilize sequence-form representations, which are approximately 1000 times more compact than the explicit game trees, alongside dynamic payoff matrix generation to circumvent explicit matrix storage. This memory-efficient approach enables usage on commodity hardware.

To optimize runtime, we distribute computation across multiple threads after the first two moves, using 18 buffers to prevent concurrency conflicts across the 81 possible opening sequences. Depending on the machine and game, this implementation delivers exploitability computation (and head-to-head evaluation) times of roughly 30 to 90 seconds.

\section{Experiments}
\label{sec:experiments}

While our high-performance exploitability computations open the door to evaluating model-free DRL for IIGs more rigorously, the novelty of these benchmarks necessitates particular care in experimental design and interpretation. Simply showing that tuned PG methods perform ``well'' on these benchmarks would not suffice to evidence \policyhyp, as there are no numbers for FP, DO, or CFR-based DRL algorithms against which to compare. The obvious recourse---which we take---is to report our own results for FP, DO, and CFR-based DRL algorithms. However, since we have hypothesized these algorithms will underperform, our role in tuning them should be carefully scrutinized. To aid readers in this endeavor, we provide detailed documentation of our algorithm implementations, hyperparameter tuning procedure, and hyperparameter tuning results, in addition to final results.

\subsection{Algorithms and Implementation}

We choose algorithms to evaluate using two criteria.
First, that the collective set of algorithms include representatives from FP-based algorithms, DO-based algorithms, CFR-based algorithms, and generic PG methods, so as to make our experiments sufficiently comprehensive to provide evidence for or against \policyhyp.
Second, that algorithms be implemented by a reliable external source in a fashion requiring as little adaptation as possible for OpenSpiel \citep{openspiel19} compatibility, so as to reduce the probability of an implementation error.

In accordance with these criteria, the algorithms we include are NFSP \citep{nfsp16}, PSRO \citep{psro17}, ESCHER \citep{escher23}, R-NaD \citep{deepnash22}, PPO \citep{ppo17}, PPG \citep{ppg20}, and MMD \citep{mmd23}.
This selection includes one FP-based algorithm (NFSP), one DO-based algorithm (PSRO), one CFR-based algorithm (ESCHER), three generic PG methods (PPO, PPG, MMD), and one non-standard PG method (R-NaD) inspired by \citet{fforel21}.
Algorithm implementations were sourced from OpenSpiel \citep{openspiel19}, except PPG and ESCHER, which we sourced from CleanRL \citep{cleanrl22} and the official ESCHER repository \citep{escher23}, respectively.
These choices are summarized in \Cref{tab:implementations}.

\begin{table}[tbhp]
  \rowcolors{2}{gray!25}{white}
  % \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Algorithms and implementation sources.}
    \begin{tabular}{lll}
        \textbf{Algorithm} & \textbf{Class} & \textbf{Implementation} \\
        \toprule
        % \midrule
        NFSP & FP & OpenSpiel\\
        PSRO & DO & OpenSpiel\\
        ESCHER & CFR & Official\\
        R-NaD & Non-Standard PG & OpenSpiel\\
        PPO & Generic PG & OpenSpiel\\
        PPG & Generic PG & CleanRL\\
        MMD & Generic PG & Adapted from PPO\\
        \bottomrule
    \end{tabular}
    \label{tab:implementations}
\end{table}

Of the algorithm implementation code that we wrote or modified, some of the larger potential sources of error include moving NFSP from TensorFlow \citep{tf15} to PyTorch \citep{pytorch19}, adding multi-agent support for PPO and PPG, and implementing MMD as a modification of PPO. To corroborate the correctness of these pieces of code, we reproduce the NFSP exploitability results for Leduc Hold'em from OpenSpiel, as detailed in  \Cref{app:algo_repro_nfsp}, and the MMD approximate exploitability results for ADH3 and PTTT from \citet{mmd23}, as detailed in \cref{app:algo_repro_mmd}. We enable external verification of the correctness of these pieces of code, and the rest of our algorithm implementations, by open sourcing our codebase.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/expo.pdf}
    \vspace{-8mm}
    \caption{Exploitability results. For each game-algorithm pair, the box-and-whisker depicts the distribution of final exploitability over the runs from the hyperparameter tuning launch (left) and evaluation launch (right) with square-root y-axis scale. R-NaD, NFSP, ESCHER, and PSRO failed to outperform generic PG methods (MMD, PPO, PPG).}
    \label{fig:exploitabilities_best_box}
\end{figure*}

\subsection{Design}

We design our training runs as a series of two launches: a hyperparameter tuning launch followed by a maximization-bias-free evaluation launch.

For the hyperparameter tuning launch, for each combination of the 4 games and 7 algorithms, we test 50 hyperparameter configurations for 10 million steps over 3 seeds. To create these configurations, we modify the default positive real-valued hyperparameters by independent randomly sampled powers of 2---multiplying for the vast majority of hyperparameters, but exponentiating for some hyperparameters upper bounded by 1. We take the default values for hyperparameters from the implementation source in almost all cases. The most notable exceptions to this are the network architecture and optimizer, for which we impose a 3-layer 512-hidden unit fully connected network and Adam \citep{adam15} on every algorithm, and the entropy coefficient, for which we apply the value from \citet{mmd23} to all generic PG methods. We provide a comprehensive list of hyperparameters, default values, and links to the specific lines of code of the external sources from which those default values were taken in \Cref{app:algs_hparams}.

For the evaluation launch, we run the 5 strongest hyperparameter configurations from the first launch, as measured by lowest average final exploitability, with 10 fresh seeds, for 10 million steps, for each combination of the 4 games and 7 algorithms. From these evaluation runs, we report both exploitabilities and head-to-head comparisons. For the head-to-head comparisons, we select the model with the lower median final exploitability over the 10 seeds for each hyperparameter configuration, and compare these selected models for each pair of algorithms for each game.


\subsection{Results}

We summarize three of the main findings from our experiments: exploitability performance, head-to-head performance, and the effect of the entropy coefficient on the performance of generic PG methods.

\paragraph{Exploitability} \Cref{fig:exploitabilities_best_box} plots the distribution of final exploitabilities for each game and algorithm in pairs of box-and-whisker plots for the hyperparameter tuning launch (left) and the evaluation launch (right). Over these runs, ESCHER was uniformly non-competitive. PSRO was largely non-competitive, showing poor performance in ADH3, PTTT, and APTTT and having only sporadic success in DH3, for which several hyperparameter configurations found Nash equilibria on some seeds but not others. NFSP and R-NaD approached or matched the performance of generic PG methods in select cases, but typically underperformed them. The generic PG methods performed the strongest, and were roughly on par with one another.




% 2. head to head
\paragraph{Head-to-Head Comparisons} \cref{fig:h2h} plots head-to-head comparisons for each pair of algorithms. Relative performance patterns are similar to those for exploitability: The generic PG methods matched or defeated other algorithms while maintaining rough parity among themselves, R-NaD either approached the performance of the generic PG methods or underperformed them, while NFSP and, especially, ESCHER and PSRO were non-competitive.

\begin{figure*}
    \centering
    % \resizebox{.45\linewidth}{!}{\import{figures/h2h}{dh3.pgf}}
    % \resizebox{.45\linewidth}{!}{\import{figures/h2h}{adh3.pgf}}
    % \resizebox{.45\linewidth}{!}{\import{figures/h2h}{pttt.pgf}}
    % \resizebox{.45\linewidth}{!}{\import{figures/h2h} {apttt.pgf}}
    \NewDocumentCommand \addplot {m} {
        \scalebox{.97}{
        % \begin{tikzpicture}%
            % \node at (0,0) {%
            \includegraphics[height=5.1cm]{#1}%
            % };
            % \draw[red,line width=.5mm] (-1.84,0.54) -- (1.85, 0.54);
            % \draw[red,very thick] (-0.28, -1.58) -- (-0.28, 2.12);
            % \draw[black] (-1.84, -1.58) rectangle (1.85, 2.12);
        % \end{tikzpicture}
        }
        \hspace{-3.6mm}
    }
    \addplot {figures/h2h/dh3.pdf}%
    \addplot {figures/h2h/adh3_no_ykey.pdf}%
    \addplot {figures/h2h/pttt_no_ykey.pdf}%
    \addplot {figures/h2h/apttt_no_ykey.pdf}%
    \vspace{-4mm}
    \caption{Head-to-head evaluations. The number in each cell is the expected utility of the row algorithm against the column algorithm, assuming each plays half of the games as the first moving player. R-NaD, NFSP, ESCHER, and PSRO failed to outperform generic PG methods (MMD, PPO, PPG), which are segregated by the dashed red line.}
    \label{fig:h2h}
\end{figure*}



\begin{figure}
    \centering
    % \includegraphics[width=0.95\linewidth]{figures/results/score_vs_entropy_concat.png}
    \includegraphics[width=1\linewidth]{figures/ent-line-depressed.pdf}
    \vspace{-5mm}
    \caption{Average (dash-dotted green) and median (solid purple) exploitability of generic policy gradient methods across all games as a function of entropy coefficient, with shaded interquartile range and square-root x-axis. Vertical dashed red lines show the default entropy coefficient values for PPO in widely used DRL libraries. Results broken down by game are shown in \cref{fig:score_vs_entropy_sep}.
    }
    \label{fig:ent_analysis}
    \vspace{-5mm}
\end{figure}

\paragraph{Entropy Coefficient Analysis} In \Cref{sec:hypothesis}, we suggest that one possible reason for the poor performance of generic PG methods in existing literature is that the entropy coefficients required to achieve good performance were too far removed from typical choices. To investigate, we plot exploitability as a function of entropy coefficient in \Cref{fig:ent_analysis}. The results are consistent with this possible explanation: Not only do unilateral changes in entropy coefficient produce drastic changes in exploitability, but the entropy coefficients with the best average performance are between 0.05 and 0.2, larger than any of the default entropy coefficients for PPO in Stable Baselines \citep{hill2018stablebaselines}, CleanRL \citep{cleanrl22}, RLlib \citep{liang2018rllib}, OpenSpiel \citep{openspiel19}, PufferLib \citep{suarez2024pufferlibmakingreinforcementlearning}, RL-Games \citep{rl-games2021}, and Tianshou \citep{tianshou}, which, as detailed in  \Cref{tab:library_ent_coefs}, range between 0 and 0.01.

\section{Discussion}

The experimental results unambiguously support \policyhyp. Not only did the FP, DO, and CFR-based DRL algorithms fail to outperform the generic PG methods, they, alongside the non-standard PG method, were largely not even competitive. On top of that, the experiments are consistent with the possibility that poorly chosen entropy coefficients contribute to the poor performance of generic PG methods in existing literature.

However, we emphasize that we make no claim---and should not be interpreted as having claimed---that we have established as true \policyhypesp or any other speculation discussed in this work. The prudent interpretation of our experiments is nuanced and gives due consideration to multiple limitations.

First, the games on which we performed experiments are homogeneous: 1) Imperfect information is introduced via hidden actions; 2) there is no chance; 3) there are only terminal non-zero rewards, which are binary; and 4) actions represent piece placements onto a 3-by-3 board. We believe that neither the relative performance of the algorithms we evaluated nor the large optimal entropy coefficients for PG methods we observed would be universally consistent across adversarial IIGs at large (indeed, relative performance is not even consistent across these homogeneous games).

Second, we evaluated only one FP-based algorithm (the original NFSP), one DO-based algorithm (the original PSRO), and one CFR-based algorithm (the most recent adaption to DRL). As discussed in \Cref{sec:algorithms}, there are dozens of variants of these algorithms, especially PSRO. Any one of these variants may possess important innovations that result in improved performance on our benchmarks.

Third, the experimental results reflect our particular experimental conditions, including: 1) the default hyperparameter values, 2) the hyperparameters we tuned, 3) the manner in which we tuned hyperparameters, and 4) performance evaluation at 10 million steps. These conditions are indeed particular, and should not be misconstrued to represent optimal algorithm performance. It is possible that strong performing hyperparameters exist for NFSP, PSRO, ESCHER, or R-NaD, but were not found by our hyperparameter sweep because they were too many orders of magnitude away from the defaults, or because they required too many different defaults to be changed simultaneously, or because they involved different categorical hyperparameters---which we did not tune. It is also possible that the hyperparameters we used would achieve strong performance, given more training steps---a possibility consistent with preliminary experiments, which suggested that all algorithms continue to decrease exploitability beyond 10 million steps.

To power community efforts to address or investigate these second and third sets of limitations, as well as to give creators of existing and future DRL algorithms for IIGs the opportunity to demonstrate the efficacy of their algorithms, we release our exploitability computations in an accessible Python package with OpenSpiel-compatible bindings. We detail this package, called \PKGName, in \Cref{app:dh3}. We encourage its use not only as it pertains to \policyhyp, but also more generally for future IIGs research.


\section{Conclusion}

This work hypothesizes and presents evidence that generic deep PG methods are strong approaches for adversarial IIGs. Our results, along with those of \citet{mappo22}, who demonstrated PPO's effectiveness in cooperative IIGs, and \citet{mmd23}, who demonstrated MMD's effectiveness in adversarial IIGs, add to a growing body of evidence for the possibility that a single, simple PG method could serve as a universal algorithm for DRL in games. We believe this possibility is an aspiration worth pursuing.

\section{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of the work, none which we feel must be specifically highlighted here.

\bibliography{main}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%                     Appendix                        %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\appendix

\section{The \PKGName Package}\label{app:dh3}

We release a Python library to efficiently compute exploitability of policies for the four benchmark games introduced in \cref{sec:games}. The library supports outputting state representations compatible with OpenSpiel \citep{openspiel19}, making it straightforward to evaluate the exploitability of policies trained using the latter library. 

\subsection{Exploitability Computation}
Internally, \PKGName evaluates the exploitability of policies by carrying out computations using the so-called \emph{sequence-form} representation of the four games \citep{romanovskii,vonStengel1996,koller1996}. This representation is roughly 1000 times more compact than the actual game trees, allowing us to compute exact exploitability (and head-to-head values) on commodity hardware. This representation is comprised of two objects: (i) the \emph{treeplexes} $\cal X, \cal Y$ of the two players \citep{hoda2010smoothing}, and (ii) the sequence-form payoff matrix $A$ of the game. Every policy for player 1 admits an equivalent vector $x \in \cal X$ (resp. $y \in \cal Y$ for player 2), and the expected payoff corresponding to player 1 can be computed in closed form via the bilinear form $x^\top A y$. Due to the size of the games, the sequence-form payoff matrix $A$ is not stored explicitly in memory by \PKGName, but is rather materialized on the fly in a multi-threaded fashion as needed.

% In our codebase, we represent each game as a pair of treeplexes \citep{hoda2010smoothing}, each of which stores the information states of one player.
At a high level, given two policies $\pi_1, \pi_2$ for the players, \PKGName computes exploitability by performing the following.
\begin{enumerate}[nosep]
    \item First, $\pi_1$ and $\pi_2$ are converted into their sequence-form equivalents $x \in \mathcal X, y \in \mathcal Y$. This step requires memory and runtime that scale linearly in the number of information states of the games. The latter is in the order of $10^7$ as shown in \cref{table:game-vals}.
    \item Then, the vectors $g_1 \coloneqq A y$, $g_2 \coloneqq -A^\top x$ are computed. These are the gradients of the utility function of the game. We accelerate the computation by playing the first two actions of the game and then each thread calculates the gradient assuming the first two moves separately and then the gradients are safely reduced. It is worth noting that while there are 81 possible pairs of first moves for the players, only 18 buffers are required to avoid all possible concurrency conflicts. Depending on the machine and the game, calculating the gradients take roughly between 30 and 90 seconds. 
    %For performance reasons, we implement the bottom up traversal needed for best response and CFR iterations in C++. To use a neural network, we evaluate the policy function over all possible information states by batching the queries. Only then the strategy is converted to sequence form and can be used by our software.
    \item Finally, exploitability is computed according to the formula
    $$ \max_{\hat x \in \cal X} \hat x^\top g_1 + \max_{\hat y \in \cal Y} y^\top g_2.$$ The optimization problems can be solved in closed form using memory and runtime linear in the number of information states of the games, by using a standard greedy algorithm on treeplexes.
\end{enumerate}
% To calculate the utilities, we implicitly traverse the game tree and accumulate the gradients via the sequence form. This can be seen as a sparse matrix-vector multiplication.


\subsection{State-of-the-Art Tabular Solvers}\label{sec:dh3 algos}

\PKGName also includes implementation of the following state-of-the-art tabular solvers:

\begin{enumerate}
    \item CFR+ \citep{cfr+14} subtitutes the regret matching (RM) algorithm in CFR with regret matching+ (RM+), leading to better performance empirically. The main difference between RM and RM+ is that RM+ clamps the regrets to be non-negative.
    \item Discounted CFR (DCFR) \citep{brown2019solving}, in a similar vain as CFR+, discounts the regrets to reduce the effect of previous actions with large (absolute) regret. Practically speaking, at iteration $t$, we rescale the negative regrets by $1 - 1 / (1 + t^\beta)$ and the positive regrets by $1 - 1 / (1 + t^\alpha)$. We use the default value of $\alpha=1.5$ and $\beta=0$.
    \item  Predictive CFR (PCFR) \citep{farina2019stable} uses optimistic regret minimizers to achieve empirical and theoretical faster convergence rates for CFR. Practically, we observe the gradients twice to calculate the behavior policy of CFR.
\end{enumerate}

\begin{figure}
    \centering
    % \resizebox{0.48\linewidth}{!}{\import{figures/tabular}{3x3_classical_dark_hex.pgf}}
    % \resizebox{0.48\linewidth}{!}{\import{figures/tabular}{3x3_abrupt_dark_hex.pgf}}
    % \resizebox{0.48\linewidth}{!}{\import{figures/tabular}{classical_phantom_tic-tac-toe.pgf}}
    % \resizebox{0.48\linewidth}{!}{\import{figures/tabular}{abrupt_phantom_tic-tac-toe.pgf}}
    \includegraphics[width=\linewidth]{figures/cfr.pdf}
    \caption{Performance of various tabular methods.}
    \label{fig:table_perf}
\end{figure}

% Maybe move this somewhere else where the table is
These are online, first-order optimization algorithms that tabularly refine strategies---represented in sequence-form---by taking steps in the direction of the gradient utility. Table~\ref{tab:cfrexpl} reports the exploitability of equilibrium computed by the methods across the four games and Figure~\ref{fig:table_perf} shows the convergence of DCFR and PCFR+. We note that Dark Hex has a winning deterministic strategy for the first moving player. We analyze this strategy in \Cref{appen:dh3-solution}.

\begin{table}

\caption{Exploitability after 512 iterations of different algorithms}\label{tab:cfrexpl}
\centering
\begin{tabular}[t]{>{}llrr}
\toprule
\cellcolor{white}{Game} & \cellcolor{white}{Algorithm} & \cellcolor{white}{Value} & \cellcolor{white}{Exploitability}\\
\midrule
\cellcolor{white}{} & CFR & 0.9999993 & 0.0000004\\
\cmidrule{2-4}
\cellcolor{white}{} & CFR+ & 0.9999993 & 0.0000004\\
\cmidrule{2-4}
\cellcolor{white}{} & DCFR & 0.9999993 & 0.0000004\\
\cmidrule{2-4}
\cellcolor{white}{} & FP & 0.9999997 & 0.0000002\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR & 0.9999993 & 0.0000005\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR+ & 0.9999993 & 0.0000005\\
\cmidrule{2-4}
\cellcolor{white}{\multirow[t]{-7}{*}{\raggedright\arraybackslash 3x3 Dark Hex (DH3)}} & PDCFR & 0.9999993 & 0.0000005\\
\cmidrule{1-4}
\cellcolor{white}{} & CFR & 0.3841795 & 0.0295619\\
\cmidrule{2-4}
\cellcolor{white}{} & CFR+ & 0.3844054 & 0.0011629\\
\cmidrule{2-4}
\cellcolor{white}{} & DCFR & 0.3844321 & 0.0010063\\
\cmidrule{2-4}
\cellcolor{white}{} & FP & 0.3580483 & 0.1929647\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR & 0.3844107 & 0.0011789\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR+ & 0.3843936 & 0.0006123\\
\cmidrule{2-4}
\cellcolor{white}{\multirow[t]{-7}{*}{\raggedright\arraybackslash 3x3 Abrupt Dark Hex (ADH3)}} & PDCFR & 0.3844073 & 0.0192829\\
\cmidrule{1-4}
\cellcolor{white}{} & CFR & 0.6664844 & 0.0107796\\
\cmidrule{2-4}
\cellcolor{white}{} & CFR+ & 0.6666324 & 0.0016106\\
\cmidrule{2-4}
\cellcolor{white}{} & DCFR & 0.6666511 & 0.0004070\\
\cmidrule{2-4}
\cellcolor{white}{} & FP & 0.6622969 & 0.1016475\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR & 0.6660216 & 0.0011638\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR+ & 0.6662418 & 0.0019337\\
\cmidrule{2-4}
\cellcolor{white}{\multirow[t]{-7}{*}{\raggedright\arraybackslash Phantom Tic-Tac-Toe (PTTT)}} & PDCFR & 0.6662910 & 0.0157007\\
\cmidrule{1-4}
\cellcolor{white}{} & CFR & 0.5501850 & 0.0162306\\
\cmidrule{2-4}
\cellcolor{white}{} & CFR+ & 0.5501825 & 0.0004936\\
\cmidrule{2-4}
\cellcolor{white}{} & DCFR & 0.5501728 & 0.0003577\\
\cmidrule{2-4}
\cellcolor{white}{} & FP & 0.5428041 & 0.2871734\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR & 0.5501960 & 0.0011708\\
\cmidrule{2-4}
\cellcolor{white}{} & PCFR+ & 0.5501886 & 0.0004846\\
\cmidrule{2-4}
\cellcolor{white}{\multirow[t]{-7}{*}{\raggedright\arraybackslash Abrupt Phantom Tic-Tac-Toe (APTTT)}} & PDCFR & 0.5502121 & 0.0040427\\
\bottomrule
\end{tabular}
\end{table}

% We implemented \emph{exact} exploitability by developing a multi-threaded tabular algorithm. After obtaining a tabular representation of the policies employed by the players, which is obtained by batching forward passes on the corresponding policy nets, our algorithm performs exact computation of the utility gradients for the players by using sparse matrix-vector multiplication in the game's sequence form. To keep the implementation efficient, the sparse payoff matrix is materialized in memory in tiles, using up to 81 threads (by default, the number of threads is set to the minimum between 81 and the number of virtual cores of the machine, though this can be configured by the end user). To avoid thread conflicts, the partial gradients computed on each tile are stored in 18 separate buffers (one per player), and aggregated once the tiles have been fully multiplied. As the tiles get constructed, treeplex representations for the players are constructed. Once gradient vectors in sequence form have been obtained, a simple bottom-up traversal on each player's treeplex recovers best response policies and values. Once the values of the best responses to each player's policy are computed, the exploitability of each of the two policies is returned to the end user, together with the best-response policies. This process takes between 30 and 90 seconds per game, depending on the machine and the specific game selected.


% \paragraph{Game values}
% We use the multi-threaded gradient computation code described above beyond the computation of exact exploitability, to compute the game value, that is, the expected utility of each player at equilibrium. Results are given in TODO. To obtain the results, we implemented the DCFR and PCFRP regret minimization algorithms. These algorithms are online first-order optimization methods that tabularly refine strategies---represented in sequence-form---by taking steps in the direction of the gradient utility. The table reports the lowest exploitability of any approximate equilibrium computed by the methods across the four games, when run for 500 iterations. In all cases, we were able to achieve an exploitability of less than 0.00026. Progress of the tabular training is shown in TODO{REF. IN THE APPENDIX?}

% \paragraph{Game sizes}
% - Dump in the game sizes (num verts, num information states)

% \paragraph{Game properties}
% - Talk about the fact that Dh is deterministic 

% Are there other properties?

\section{Solving Dark Hex 3}\label{appen:dh3-solution}
We investigate a deterministic winning strategy for the first moving player in Dark Hex 3. The value of this policy against a uniform policy is 1, implying that it wins against all possible deterministic strategies, including any possible best response to it. The strategy is shown in Figure~\ref{fig:dh3strat} where, at each information state, a list of actions to try in that order is given. If the first action fails, the next action is played, and so on. After playing an action, there are two possible outcomes: the action is playable, or information is gained. In general, the size of the list of actions is equal to one plus the number of opponent pieces that have been played but have not been observed; thus, the last action has to be playable. Beyond the computational proof that the strategy in Figure~\ref{fig:dh3strat} is optimal, we can show it using the fact the game of Hex (with perfect information) cannot end in a draw \citep[Hex Theorem]{gale1979game}. Since the first player always wins in the reachable information states, it is impossible the invisible pieces are in a way that the opponent has already won.

This strategy is not applicable for Abrupt 3x3 Dark Hex as the board in the abrupt version does not correspond to a board of hex. For instance, player 2 might never put a piece on the board.

\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{figures/dh3_optimal_strat1.pdf}
    \caption{A deterministic strategy for player 1 that always wins. The gray dashed lines denote a hidden action of the blue player.}
    \label{fig:dh3strat}
\end{figure*}


Since the game of Hex can be won by the first player, we argue that any deterministic winning Nash equilibrium for Dark Hex must also be an optimal strategy in the perfect information game, otherwise there must exist a deterministic strategy for the opponent that wins against this Nash equilibrium. Thus at any information state, the action played must be either invalid or a winning move for all states that are compatible with that information state (i.e., it is possible that they are the state of the board at that information state). This gives rise to an algorithm for finding deterministic Nash equilibrium by backtracking over which action to play. We suspect that this algorithm does not have any solution beyond 3 by 3 Hex and that Dark Hex 4 does not have any winning deterministic strategy.




\section{Algorithm implementation details}
\label{app:algs_impl}

We considered the following algorithms in this work: NFSP, PSRO, R-NaD, ESCHER, PPO, PPG and MMD. Below we give details about the implementation of each of them.

\paragraph{NFSP} (\href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/algorithms/nfsp.py}{Source Implementation}):  We use the implementation of NFSP in OpenSpiel~\citep{openspiel19}. This implementation learns distinct models for player 1 and player 2. For consistency with the other algorithms, we rewrote some of the algorithm to use PyTorch~\citep{pytorch19} instead of TensorFlow  \citep{tf15}. 

\paragraph{PSRO} (\href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/algorithms/psro_v2/psro_v2.py}{Source Implementation}): We use the implementation of PSRO in OpenSpiel~\citep{openspiel19}. This implementation learns distinct models for player 1 and player 2. For consistency with the other algorithms, we replaced the default OpenSpiel Tensorflow DQN agent (\href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/algorithms/dqn.py}{source}) with the OpenSpiel PyTorch DQN agent (\href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/pytorch/dqn.py}{source}). 

\paragraph{R-NaD} (\href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/algorithms/R-NaD/R-NaD.py}{Source Implementation}): We use the implementation of R-NaD in OpenSpiel~\citep{openspiel19}. This implementation learns a single model that is used for both player 1 and player 2. Natively, this algorithm is written in JAX~\citep{jax2018github}. For convenience during evaluation, we convert the neural networks learned in JAX to equivalent PyTorch models. We verified this conversion by ensuring that, for the same inputs, the outputs of the models are equivalent.

\paragraph{ESCHER} (\href{https://github.com/Sandholm-Lab/ESCHER/blob/e694eaaa251952696aaf36ef1c790887c8324750/parallelized_ESCHER.py}{Source Implementation}): We use the implementation of ESCHER from the original paper~\citep{escher23}. This implementation learns a single policy network that is used for both player 1 and player 2. Natively this algorithm is written in Tensorflow. For convenience during evaluation we convert the neural networks learned in Tensorflow to equivalent PyTorch models and verify by ensuring each layer weight and bias is equivalent. 

\paragraph{PPO} (\href{https://github.com/google-deepmind/open_spiel/blob/d99705de2cca7075e12fbbd76443fcc123249d6f/open_spiel/python/pytorch/ppo.py}{Source Implementation}) We use the implementation of PPO in OpenSpiel, which is itself a modification from the \href{https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py}{CleanRL PPO} made to work with OpenSpiel games and legal action masking. We further modify it to support self-play, and we learn a single model for both players.

\paragraph{PPG} (\href{https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppg_procgen.py}{Source Implementation}) We use the implementation of PPG in CleanRL, which we modify to make it work with OpenSpiel games, legal action masking and to support self-play. We learn a single model for both players. 

\paragraph{MMD} We use our PPO implementation and simply add a backward KL term in the PPO loss. We learn a single model for both players.

\section{Results Supporting Implementation Correctness}
\label{app:alg_repro}

\subsection{NFSP} \label{app:algo_repro_nfsp} We verified this re-implementation by comparing the exploitability of strategies learned in the original file with ours on Leduc Hold'em, see \cref{fig:nfsp-verification}.

\begin{figure}
    \centering\includegraphics[width=0.5\linewidth]{figures/verification/nfsp_verification_plot.pdf}
    \caption{Exploitability performance of the TensorFlow NFSP implementation in OpenSpiel and our PyTorch adaptation on Leduc Hold'em aggregated over 2 seeds.}
    \label{fig:nfsp-verification}
\end{figure}

\subsection{MMD} \label{app:algo_repro_mmd} We verified that we obtain results consistent with the original MMD implementation~\citep{mmd23}. Namely, after a 10M steps training we obtain an approximate exploitability of around 0.14 and 0.20 on PTTT and ADH3 respectively. For reference, Table~2 in~\citet{mmd23} reports an approximate exploitability of $0.15 \pm 0.01$ and $0.20 \pm 0.01$ for PTTT and ADH3 respectively.

\section{Additional results}

The evolution of exploitability over 10M training steps for all 4200 hyperparameter tuning runs is shown in \cref{fig:exploitabilities_misc}. Each algorithm-game pair, includes 150 runs -- 50 hyperparameter sets with 3 seeds each. The variance across these 3 seeds per set is visualized in \cref{fig:exploitabilities}. Furthermore, we compute the approximate importance of each hyperparameter in the tuning process, which we report in \cref{fig:feature_importances_appendix}. Given the high importance we obtained for the entropy coefficient in the policy-gradient algorithms, in \cref{fig:score_vs_entropy_sep} we specifically analyze the average exploitability as a function of the entropy coefficient. We observe that on average we obtain the best exploitability results for entropy coefficients much higher than standard ones detailed in \cref{tab:library_ent_coefs}. Finally, \cref{fig:scatter_time_mem_agg} reports the training durations and memory usage of the evaluated algorithms. Note that these metrics depend on hardware and implementation optimizations and are provided for reference only.

% ROUND ONE FIGS

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities.png}
    \caption{Exploitability vs. step for all 4200 runs of the hyperparameter tuning launch, broken down by game and algorithm.}
    \label{fig:exploitabilities_misc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/expl_seeds.png}
    \caption{Exploitability for all 4200 runs of the hyperparameter tuning launch, broken down by game and algorithm, then grouped by set of hyperparameters. The boxes-and-whiskers each show the variance over 3 seeds for one set of hyperparameters, and are ordered by decreasing average exploitability.}
    \label{fig:exploitabilities}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/feature_importances.png}
    \caption{The top 10 most influencial hyperparameters from the hyperparameter tuning launch, broken down by game and algorithm. The sweep results provide mappings from hyperparameter sets to exploitabilities, which we use to train a random forest regression model. This model assigns each hyperparameter a coefficient representing its importance in the prediction. This importance reflects the hyperparameter's impact on exploitability: a high value indicates a strong influence, while a low value suggests minimal impact.}
    \label{fig:feature_importances_appendix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/scatter_time_mem_agg.png}
    \caption{Memory usage (RAM) and wall-time training duration statistics from the hyperparameter tuning launch, broken down by games and algorithm.}
    \label{fig:scatter_time_mem_agg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ent-line-4.pdf}
    \caption{Figure~\ref{fig:ent_analysis} separated on a per game and algorithm basis.}
    \label{fig:score_vs_entropy_sep}
\end{figure}

\begin{table}[tbhp]
  \rowcolors{2}{gray!25}{white}
  \renewcommand{\arraystretch}{1.2}
    \centering
    \caption{Entropy coefficients used in popular reinforcement learning libraries for policy gradient algorithms.}
    \begin{tabular}{ll}
        \toprule
        \textbf{Library} & \textbf{Entropy Coefficient} \\
        \midrule
        Stable Baselines \citep{hill2018stablebaselines} & 0.0 \\
        CleanRL \citep{cleanrl22} & 0.01 or 0.001 \\
        RLlib \citep{liang2018rllib} & 0.0 \\
        OpenSpiel \citep{openspiel19} & 0.01 \\ 
        PufferLib \citep{suarez2024pufferlibmakingreinforcementlearning} & 0.01 \\
        RL-Games \citep{rl-games2021} & 0 or 0.01 \\
        Tianshou \citep{tianshou} & 0 or 0.01 \\
        \bottomrule
    \end{tabular}
    \label{tab:library_ent_coefs}
\end{table}

% ROUND TWO FIGS

We show the training exploitability curves for the 1400 runs of the evaluation launch in \cref{fig:exploitabilities_all_roundtwo}. The variance across seeds for each hyperparameter set can be seen in \cref{fig:exploitabilities}. Additionally, \cref{fig:exploitabilities_best} reports the training exploitability curves for the best hyperparameter set of each algorithm-game pair as well as the variance over 10 seeds for each set.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities_all_roundtwo.png}
    \caption{Exploitability vs. step for all 1400 runs of the evaluation launch, broken down by game and algorithm.}
    \label{fig:exploitabilities_all_roundtwo}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/expl_seeds_roundtwo.png}
    \caption{Exploitability for all 1400 runs of the evaluation launch, broken down by game and algorithm, then grouped by set of hyperparameters. The boxes-and-whiskers each show the variance over 10 seeds for one set of hyperparameters, and are ordered by decreasing average exploitability.}
    \label{fig:expl_seeds_roundtwo}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities_best.png}
    \caption{Variance across seeds for the best-performing hyperparameter set in the evaluation launch, broken down by game and algorithm. The best set is defined as having the smallest final exploitability, averaged over seeds. Plots show mean exploitability over 10M training steps (solid line) with a shaded region representing the standard deviation range across 10 seeds.}
    \label{fig:exploitabilities_best}
\end{figure}

\clearpage

\section{Algorithm hyperparameters}
\label{app:algs_hparams}

Here, we provide an overview of the hyperparameters for the algorithms employed in the experiments. For each hyperparameter being swept, for each run, we sample $N \sim \text{Uniform}(\{-3, -2, -1, 0, 1, 2, 3\})$. 
For positive real-valued hyperparameters, we adjust the hyperparameter by multiplying its default value by $2^N$, rounding to an integer when necessary.
For hyperparameters that may otherwise egress $[0, 1]$ when they should not, we exponentiate the default value to the power of $2^N$, rather than multiplying.
All algorithms were trained for 10 million steps and with the same neural network architecture (3 fully connected layers of 512 neurons each).

\subsection{NFSP}

\paragraph{Tuned hyperparameters:}
\begin{enumerate}
\item \textbf{\texttt{replay\_buffer\_capacity}}
\begin{itemize}
    \item Description: Size of DQN replay buffer.
    \item Default value: $2 \times 10^5$ \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L40}{OpenSpiel NFSP Leduc example}.
    \item Maximum value: $5 \times 10^5$, imposed to ensure that the replay buffer capacity cannot be significantly larger than the reservoir buffer capacity, in alignment with the ratio of default values.
\end{itemize}

\item \textbf{\texttt{reservoir\_buffer\_capacity}} 
\begin{itemize}
    \item Description: Size of reservoir buffer.
    \item Default value: $2 \times 10^6$ \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L42}{OpenSpiel NFSP Leduc example}.
    \item Maximum value: $4 \times 10^6$, imposed due to memory constraints.
\end{itemize}

\item \textbf{\texttt{min\_buffer\_size\_to\_learn}} 
\begin{itemize}
    \item Minimum number of instances in buffer before training.
    \item Default value: 1000
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L44}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{anticipatory\_param}}
\begin{itemize}
    \item Description: Probability of using the RL best response as policy.
    \item Default value: 0.1
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L46}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{batch\_size}}
\begin{itemize}
    \item Description: Number of transitions to sample at each learning step.
    \item Default value: 128
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L48}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{learn\_every}}
\begin{itemize}
    \item Description: Number of environment steps between learning updates.
    \item Default value: 64
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L50}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{sl\_learning\_rate}} 
\begin{itemize}
    \item Description: Learning rate for supervised learning network.
    \item Default value: 0.01
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L54}{OpenSpiel NFSP Leduc example}.
\end{itemize}
\item \textbf{\texttt{inner\_rl\_agent.epsilon\_decay\_duration}}
\begin{itemize}
    \item Description: Number of game steps over which exploration decays.
    \item Default value: $1 \times 10^7$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L64}{OpenSpiel NFSP Leduc example}.
    \item The epsilon decay duration is in the Leduc Example is set to decay completely throughout training ($2 \times 10^7$ games). We mimic this behavior by setting the decay duration to the total number of training steps ($1 \times 10^7$ steps).
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.learning\_rate}}
\begin{itemize}
    \item Description: Learning rate for Q-network.
    \item Default value: $0.01$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L52}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.batch\_size}}
\begin{itemize}
\item Description: Batch size of Q-network.
\item Default value: 128
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L48}{OpenSpiel NFSP Leduc example}.

\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.update\_target\_network\_every}}
\begin{itemize}
    \item Description: Number of steps between target network updates.
    \item Default value: 19200
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L60}{OpenSpiel NFSP Leduc Example}
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.epsilon\_start}}
\begin{itemize}
    \item Description: Initial exploration value.
    \item Default value: 0.06
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L66}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.epsilon\_end}}
\begin{itemize}
    \item Description: Final exploration value.
    \item Default value: 0.001
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L68}{OpenSpiel NFSP Leduc example}.
\end{itemize}
\end{enumerate}

\paragraph{Fixed hyperparameters:}
\begin{enumerate}
\item \textbf{\texttt{optimizer\_str}}
\begin{itemize}
    \item Description: Supervised learning network optimizer.
    \item Value: Adam
\end{itemize}
\item \textbf{\texttt{loss\_str}} 
\begin{itemize}
    \item Description: Loss function for Q-network.
    \item Value: MSE
\end{itemize}
\end{enumerate}

\subsection{PSRO}
\textbf{Tuned Hyperparameters}
\begin{enumerate}
\item \textbf{\texttt{sims\_per\_entry}}
\begin{itemize}
    \item Description: Number of games to play for estimating the meta game.
    \item Value: 1000
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L52}{OpenSpiel PSRO example}.
\end{itemize}

\item \textbf{\texttt{number\_training\_episodes}}, 
\begin{itemize}
    \item Description: Number of episodes over which to train each oracle.
    \item Value: 1000
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/algorithms/psro_v2/rl_oracle.py#L80C16-L80C40}{OpenSpiel PSRO RL oracle file}.
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.epsilon\_decay\_duration}}
\begin{itemize}
    \item Description: Number of game steps over which exploration decays.
    \item Default value: $1 \times 10^7$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L64}{OpenSpiel NFSP Leduc example}.
    \item Note: the epsilon decay duration in the Leduc Example is set to decay completely throughout training ($2 \times 10^7$ games). We mimic this behavior by setting the decay duration to the total number of training steps ($1 \times 10^7$ steps).
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.learning\_rate}}
\begin{itemize}
    \item Description: Learning rate for Q-network.
    \item Default value: $0.01$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/psro_v2_example.py#L107}{OpenSpiel PSRO v2 example}.
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.batch\_size}}
\begin{itemize}
\item Description: Batch size for Q-network.
\item Default value: 128
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/pytorch/dqn.py#L121}{OpenSpiel DQN Default}
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.update\_target\_network\_every}}
\begin{itemize}
    \item Description: Number of steps between target network updates.
    \item Default value: 1000
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/psro_v2_example.py#L108}{OpenSpiel PSRO v2 Example}
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.epsilon\_start}}
\begin{itemize}
    \item Description: Initial exploration value.
    \item Default value: 0.06
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L66}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\item \textbf{\texttt{inner\_rl\_agent.epsilon\_end}}
\begin{itemize}
    \item Description: Final exploration value.
    \item Default value: 0.001
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/examples/leduc_nfsp.py#L68}{OpenSpiel NFSP Leduc example}.
\end{itemize}

\end{enumerate}

\textbf{Fixed Hyperparameters}
\begin{enumerate}
\item \textbf{\texttt{optimizer\_str}}
\begin{itemize}
    \item Description: Oracle agent learning network optimizer
    \item Value: Adam
\end{itemize}

\item \textbf{\texttt{loss\_str}}
\begin{itemize}
    \item Description: Loss function for Q-network
    \item Value: MSE
\end{itemize}

\item \textbf{\texttt{training\_strategy\_selector}}
\begin{itemize}
    \item Description: Determines against which oracle agents to train in the next iteration
    \item Value: Probabilistic (distributed according to the meta strategy)
\end{itemize}
\end{enumerate}

\subsection{R-NaD}
\paragraph{Tuned hyperparameters:}
\begin{enumerate}
\item 
\textbf{\texttt{batch\_size}}
\begin{itemize}
    \item Description: Number of samples in each training batch.
    \item Default value: $256$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}
\item \textbf{\texttt{learning\_rate}}
\begin{itemize}
    \item Description: Learning rate for the optimizer.
    \item Default value: $5 \times 10^{-5}$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}
\item \textbf{\texttt{clip\_gradient}}
\begin{itemize}
    \item Description: Maximum gradient value for clipping.
    \item Default value: $10{,}000$
    \item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}

\item \textbf{\texttt{target\_network\_avg}} 
\begin{itemize}
\item Description: Smoothing factor for target network updates. 
\item Default value: $0.001$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}
\item \textbf{\texttt{eta\_reward\_transform}} 
\begin{itemize}
\item Description: Scaling factor for regularization in reward transformation. 
\item Default value: $0.2$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}
\item \textbf{\texttt{entropy\_schedule\_size\_value}} 
\begin{itemize}
\item Description: Schedule for updating the regularization network. 
\item Default value: $50{,}000$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\item Notes: This is different from the value of $20{,}000$ in the R-NaD default config but is in the range of our hyperparameter sweep.
\end{itemize}

\item \textbf{\texttt{c\_vtrace}} 
\begin{itemize}
\item Description: Coefficient for V-trace importance weights~\citep{impala18}.
\item Default value: $1.0$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc Example}.
\end{itemize}
\end{enumerate}

\paragraph{Fixed hyperparameters:}
\begin{enumerate}
\item \textbf{\texttt{trajectory\_max}} 
    \begin{itemize}
        \item Description: Number of steps after which games are truncated.
        \item Value: Disabled
    \end{itemize}


\item \textbf{\texttt{beta}} 
\begin{itemize}
\item Description: Size of the gradient clipped threshold in the NeurD gradient clipping~\citep{hennes2019neural}.
\item Default value: $2.0$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc defaults}.
\end{itemize}

\item \textbf{\texttt{clip}} 
\begin{itemize}
\item Description: Size of clipping for the importance sampling in the NeurD gradient clipping~\citep{hennes2019neural}.
\item Default value: $10{,}000$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/algorithms/R-NaD/README.md}{OpenSpiel R-NaD Leduc defaults}.
\end{itemize}
\end{enumerate}

\subsection{ESCHER} 
\paragraph{Tuned hyperparameters}
\begin{enumerate}
    \item \textbf{\texttt{num\_traversals}} 
\begin{itemize}
\item Description: Number of game plays for regret function learning.
\item Default value: $1{,}000$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{num\_val\_fn\_traversals}}
\begin{itemize}
\item Description: Number of game plays for value function learning.
\item Default value: $1{,}000$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{regret\_train\_steps}}
\begin{itemize}
\item Description: Number of training steps for regret network.
\item Default value: $5{,}000$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{val\_train\_steps}}
\begin{itemize}
\item Description: Number of training steps for value function network.
\item Default value: $5{,}000$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{policy\_net\_train\_steps}}
\begin{itemize}
\item Description: Number of training steps for the policy network.
\item Default value: $10{,}000$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{batch\_size\_regret}}
\begin{itemize}
\item Description: Batch size for regret network learning.
\item Default value: $2{,}048$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{batch\_size\_val}}
\begin{itemize}
\item Description: Batch size for value function network.
\item Default value: $2{,}048$
\item Source: Hyperparameters for Phantom TTT and Dark Hex 4 from \citet{escher23}.
\end{itemize}
\item \textbf{\texttt{learning\_rate}}
\begin{itemize}
\item Description: Gradient descent learning rate.
\item Default value: $1 \times 10^{-3}$
\item Source: \href{https://github.com/Sandholm-Lab/ESCHER/blob/e694eaaa251952696aaf36ef1c790887c8324750/parallelized_ESCHER.py}{ESCHER Codebase}.
\item Notes: No value found in paper.
\end{itemize}
\item \textbf{\texttt{val\_expl}}
\begin{itemize}
\item Description: Uniform policy mixing rate for off-policy exploration for value network.
\item Default value: $0.01$
\item Source: \href{https://github.com/Sandholm-Lab/ESCHER/blob/e694eaaa251952696aaf36ef1c790887c8324750/parallelized_ESCHER.py}{ESCHER Codebase}.
\item Notes: No value found in paper.
\end{itemize}
\end{enumerate}

\paragraph{Fixed hyperparameters:} We re-use all the fixed hyperparameters from the \href{https://github.com/Sandholm-Lab/ESCHER/blob/e694eaaa251952696aaf36ef1c790887c8324750/parallelized_ESCHER.py}{ESCHER Codebase}. As there are many, we do not list them here.

\subsection{PPO}
\label{app:hparams_ppo}

\paragraph{Tuned hyperparameters}
\begin{enumerate}

\item \textbf{\texttt{learning\_rate}} 
\begin{itemize}
\item Description: Optimizer learning rate.
\item Default value: $2.5 \times 10^{-4}$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L53}{OpenSpiel's PPO Implementation}.
\end{itemize}

\item \textbf{\texttt{num\_steps}} 
\begin{itemize}
\item Description: The number of steps to run in each environment per policy rollout (i.e. the batch size is \texttt{num\_steps x num\_envs}).
\item Default value: $128$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L74}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{num\_minibatches}} 
\begin{itemize}
\item Description: The number of minibatches (i.e. the minibatch size is \texttt{round(num\_steps x num\_envs / num\_minibatches)}.
\item Default value: $4$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L83}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{update\_epochs}} 
\begin{itemize}
\item Description: Number of policy update epochs (i.e. how many times to go through the whole batch in each iteration).
\item Default value: $4$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L84}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{clip\_coef}} 
\begin{itemize}
\item Description: Clipping coefficient $\epsilon$ in the PPO loss.
\item Default value: $0.1$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L86}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{ent\_coef}} 
\begin{itemize}
\item Description: Coefficient for the entropy bonus in the loss.
\item Default value: $0.05$
\item Source: Inspired from~\citep{mmd23}.
\item Note: This value is larger than in \href{https://github.com/google-deepmind/open_spiel/blob/2228e1c2ba4314a4aa54d9650ab663c3d0550582/open_spiel/python/pytorch/ppo.py#L4}{OpenSpiel's PPO Implementation} because we have found that entropy bonuses lead to much better policies. However, the default value of $0.01$ is still within our sweeping range.
\end{itemize}

\item \textbf{\texttt{vf\_coef}} 
\begin{itemize}
\item Description: Coefficient for the value term in the loss.
\item Default value: $0.5$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L92}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{max\_grad\_norm}} 
\begin{itemize}
\item Description: Maximum norm of the gradient allowed during gradient clipping.
\item Default value: $0.5$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L93}{OpenSpiel's PPO Example Implementation}.
\end{itemize}
\end{enumerate}

\paragraph{Fixed hyperparameters}
\begin{enumerate}

\item \textbf{\texttt{num\_envs}} 
\begin{itemize}
\item Description: The number of parallel game environments.
\item Default value: $8$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L72}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{anneal\_lr}} 
\begin{itemize}
\item Description: Toggle for learning rate annealing for the policy and value networks.
\item Default value: \texttt{True}
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L77}{OpenSpiel's PPO Example Implementation}
% \item Note: We disable learning rate annealing since, even though it can lead to better-performing policies, our goal is a fair comparison of all algorithms and some of them do not have learning rate annealing implemented.
\end{itemize}

\item \textbf{\texttt{gamma}} 
\begin{itemize}
\item Description: Discount factor $\gamma$ for the return.
\item Default value: $0.99$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L80}{OpenSpiel's PPO Example Implementation}.
\item Note: We do not sweep this parameter due to the short-horizon nature of the games.
\end{itemize}

\item \textbf{\texttt{gae\_lambda}} 
\begin{itemize}
\item Description: Coefficient $\lambda$ for the general advantage estimation (GAE).
\item Default value: $0.95$
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L81}{OpenSpiel's PPO Example Implementation}.
\item Note: We do not sweep this parameter due to the short-horizon nature of the games.
\end{itemize}

\item \textbf{\texttt{norm\_adv}} 
\begin{itemize}
\item Description: Toggle for advantage normalization before computing the loss.
\item Default value: \texttt{True}
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L85}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{clip\_vloss}} 
\begin{itemize}
\item Description: Whether or not to clip the values in the value loss computation.
\item Default value: \texttt{True}
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L88}{OpenSpiel's PPO Example Implementation}.
\end{itemize}

\item \textbf{\texttt{target\_kl}} 
\begin{itemize}
\item Description: Target KL divergence threshold for early stopping of training epochs.
\item Default value: \texttt{None} (disabled)
\item Source: \href{https://github.com/google-deepmind/open_spiel/blob/f68f2a388a8bf41181b3a323f65fd2d3414ebb63/open_spiel/python/examples/ppo_example.py#L95}{OpenSpiel's PPO Example Implementation}.
\end{itemize}
\end{enumerate}

\subsection{PPG} We consider the same parameters as in PPO (Section~\ref{app:hparams_ppo}), with the addition of the following ones:

\paragraph{Tuned hyperparameters}
\begin{enumerate}

\item \textbf{\texttt{n\_iteration}} 
\begin{itemize}
\item Description: Number of policy updates in the policy phase ($N_{\pi}$).
\item Default value: $32$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L73}{CleanRL implementation} and \citet{ppg20}.
\end{itemize}

\item \textbf{\texttt{e\_policy}} 
\begin{itemize}
\item Description: Number of policy updates in the policy phase (\(E_{\pi}\)).
\item Default value: $1$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L75}{CleanRL implementation} and \citet{ppg20}.
\end{itemize}

\item \textbf{\texttt{v\_value}} 
\begin{itemize}
\item Description: Number of value updates in the policy phase (\(E_V\)).
\item Default value: $1$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L77}{CleanRL implementation} and \citet{ppg20}.
\end{itemize}

\item \textbf{\texttt{e\_auxiliary}} 
\begin{itemize}
\item Description: Number of epochs to update in the auxiliary phase (\(E_{\text{aux}}\)).
\item Default value: $6$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L79}{CleanRL implementation} and \citet{ppg20}.
\end{itemize}

\item \textbf{\texttt{beta\_clone}} 
\begin{itemize}
\item Description: Behavior cloning coefficient.
\item Default value: $1.0$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L81}{CleanRL implementation} and \citet{ppg20}.
\end{itemize}

\item \textbf{\texttt{num\_aux\_rollouts}} 
\begin{itemize}
\item Description: Number of mini-batches in the auxiliary phase.
\item Default value: $4$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L83}{CleanRL implementation}.
\end{itemize}

\item \textbf{\texttt{n\_aux\_grad\_accum}} 
\begin{itemize}
\item Description: Number of gradient accumulations in each mini-batch.
\item Default value: $1$
\item Source: \href{https://github.com/vwxyzjn/cleanrl/blob/e648ee2dc8960c59ed3ee6caf9eb0c34b497958f/cleanrl/ppg_procgen.py#L85}{CleanRL implementation}.
\end{itemize}
\end{enumerate}

\subsection{MMD} We consider the same parameters as in PPO (Section~\ref{app:hparams_ppo}), with the addition of the following ones:

\paragraph{Tuned hyperparameters}
\begin{enumerate}
\item \textbf{\texttt{kl\_coef}} 
\begin{itemize}
\item Description: Coefficient of the reverse KL divergence in the loss function.
\item Default value: $0.05$
\item Source: \citep{mmd23}.
\item Note: We use a constant value for the reverse KL coefficient (as well as for the entropy coefficient) instead of a custom schedule.
\end{itemize}
\end{enumerate}

% \section{Additional results}

% The evolution of exploitability over 10M training steps for all 4200 hyperparameter tuning runs is shown in \cref{fig:exploitabilities_misc}. Each algorithm-game pair, includes 150 runs -- 50 hyperparameter sets with 3 seeds each. The variance across these 3 seeds per set is visualized in \cref{fig:exploitabilities}. Furthermore, we compute the approximate importance of each hyperparameter in the tuning process, which we report in \cref{fig:feature_importances_appendix}. Given the high importance we obtained for the entropy coefficient in the policy-gradient algorithms, in \cref{fig:score_vs_entropy_sep} we specifically analyze the average exploitability as a function of the entropy coefficient. We observe that on average we obtain the best exploitability results for entropy coefficients much higher than standard ones detailed in \cref{tab:library_ent_coefs}. Finally, \cref{fig:scatter_time_mem_agg} reports the training durations and memory usage of the evaluated algorithms. Note that these metrics depend on hardware and implementation optimizations and are provided for reference only.

% % ROUND ONE FIGS

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities.png}
%     \caption{Exploitability vs. step for all 4200 runs of the hyperparameter tuning launch, broken down by game and algorithm.}
%     \label{fig:exploitabilities_misc}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/expl_seeds.png}
%     \caption{Exploitability for all 4200 runs of the hyperparameter tuning launch, broken down by game and algorithm, then grouped by set of hyperparameters. The boxes-and-whiskers each show the variance over 3 seeds for one set of hyperparameters, and are ordered by decreasing average exploitability.}
%     \label{fig:exploitabilities}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/feature_importances.png}
%     \caption{The top 10 most influencial hyperparameters from the hyperparameter tuning launch, broken down by game and algorithm. The sweep results provide mappings from hyperparameter sets to exploitabilities, which we use to train a random forest regression model. This model assigns each hyperparameter a coefficient representing its importance in the prediction. This importance reflects the hyperparameter's impact on exploitability: a high value indicates a strong influence, while a low value suggests minimal impact.}
%     \label{fig:feature_importances_appendix}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/scatter_time_mem_agg.png}
%     \caption{Memory usage (RAM) and wall-time training duration statistics from the hyperparameter tuning launch, broken down by games and algorithm.}
%     \label{fig:scatter_time_mem_agg}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/ent-line-4.pdf}
%     \caption{Figure~\ref{fig:ent_analysis} separated on a per game and algorithm basis.}
%     \label{fig:score_vs_entropy_sep}
% \end{figure}

% \begin{table}[tbhp]
%   \rowcolors{2}{gray!25}{white}
%   \renewcommand{\arraystretch}{1.2}
%     \centering
%     \caption{Entropy coefficients used in popular reinforcement learning libraries for policy gradient algorithms.}
%     \begin{tabular}{ll}
%         \toprule
%         \textbf{Library} & \textbf{Entropy Coefficient} \\
%         \midrule
%         Stable Baselines \citep{hill2018stablebaselines} & 0.0 \\
%         CleanRL \citep{cleanrl22} & 0.01 or 0.001 \\
%         RLlib \citep{liang2018rllib} & 0.0 \\
%         OpenSpiel \citep{openspiel19} & 0.01 \\ 
%         PufferLib \citep{suarez2024pufferlibmakingreinforcementlearning} & 0.01 \\
%         RL-Games \citep{rl-games2021} & 0 or 0.01 \\
%         Tianshou \citep{tianshou} & 0 or 0.01 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:library_ent_coefs}
% \end{table}

% % ROUND TWO FIGS

% We show the training exploitability curves for the 1400 runs of the evaluation launch in \cref{fig:exploitabilities_all_roundtwo}. The variance across seeds for each hyperparameter set can be seen in \cref{fig:exploitabilities}. Additionally, \cref{fig:exploitabilities_best} reports the training exploitability curves for the best hyperparameter set of each algorithm-game pair as well as the variance over 10 seeds for each set.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities_all_roundtwo.png}
%     \caption{Exploitability vs. step for all 1400 runs of the evaluation launch, broken down by game and algorithm.}
%     \label{fig:exploitabilities_all_roundtwo}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/expl_seeds_roundtwo.png}
%     \caption{Exploitability for all 1400 runs of the evaluation launch, broken down by game and algorithm, then grouped by set of hyperparameters. The boxes-and-whiskers each show the variance over 10 seeds for one set of hyperparameters, and are ordered by decreasing average exploitability.}
%     \label{fig:expl_seeds_roundtwo}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/exploitabilities_best.png}
%     \caption{Variance across seeds for the best-performing hyperparameter set in the evaluation launch, broken down by game and algorithm. The best set is defined as having the smallest final exploitability, averaged over seeds. Plots show mean exploitability over 10M training steps (solid line) with a shaded region representing the standard deviation range across 10 seeds.}
%     \label{fig:exploitabilities_best}
% \end{figure}

\end{document}
