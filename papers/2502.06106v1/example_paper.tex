%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% add
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amssymb}
% \usepackage{wrapfig}
% \usepackage{picinpar}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{Circuit-tuning: A Mechanistic Approach for \\ Identifying Parameter Redundancy and Fine-tuning Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yueyan Li}{bupt}
\icmlauthor{Caixia Yuan}{bupt}
\icmlauthor{Xiaojie Wang}{bupt}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{bupt}{Center of Intelligent Science and Technology, Beijing University of Posts and Telecommunications, Beijing, China}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Caixia Yuan}{yuancx@bupt.edu.cn}
% \icmlcorrespondingauthor{Xiaojie Wang}{xjwang@bupt.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors.
While recent studies have focused on the static mechanism of a certain behavior, the training dynamics inside a model remain to be explored.
In this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning.
We first propose the concept of node redundancy as an extension of intrinsic dimension and explain the idea behind circuit discovery from a fresh view.
Based on the theory, we propose circuit-tuning, a two-stage algorithm that iteratively performs circuit discovery to mask out irrelevant edges and updates the remaining parameters responsible for a specific task.
Experiments show that our method not only improves performance on a wide range of tasks but is also scalable while preserving general capabilities.
We visualize and analyze the circuits before, during, and after fine-tuning, providing new insights into the self-organization mechanism of a neural network in the learning process. 
\end{abstract}

\section{Introduction}
\label{intro}
% The birth of Transformer \cite{transformer} has opened a new chapter for machine learning.
Benefiting from the high efficiency in computation and the scalable architecture of Transformer \cite{transformer}, large language models (LLMs) have demonstrated outstanding performance in a wide range of tasks. 
% Despite the outstanding performance of LLMs in various tasks, the underlying mechanism remains to be explored. 
For real-world applications, fine-tuning aims to adapt a model to the downstream tasks. With the concept of intrinsic dimension \cite{intrinsic,intrinsic1} that the parameters in a model are often redundant for solving a specific problem, parameter-efficient fine-tuning is proposed and has reduced computation to a great extent \cite{peft}. 
% Despite being successful in practice
However, these techniques lack a mechanism fully clear to humans, and thus are less stable and may lead to catastrophic forgetting when applied to a new task \cite{continuallearning}. Besides, with the rapid development of LLMs, AI alignment has become an important issue in machine learning, highlighting the need for interpretability as well \cite{rice}.

To discover the underlying mechanisms inside a model, \cite{transformercircuits} provided a mathematical framework for transformer circuits, showing the potential of reverse-engineering a model under the guidance of mechanistic interpretability \cite{zoomin}. Recent studies in this field have tried to isolate the circuit responsible for a single behavior \cite{ioi}, extract features using sparse autoencoders \cite{dictionary}, or apply a steering vector \cite{steering} to modify the model behaviors.  
While the mechanism of an existing model behavior is explored,
few studies have paid attention to the mechanism in learning dynamics.
Though several efforts have been made from various aspects \cite{induction, grokking},
many issues are yet to be further explored in fine-tuning, e.g., how a new capability is acquired, how the model components interact and reorganize themselves, or how to intervene in the training process.

In this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning. Our contributions are as follows: 
\begin{itemize}
    \item We formulate different kinds of redundancy and propose the concept of node redundancy as an extension of the intrinsic dimension to the node level. Based on the theory, we provide an explanation for circuit discovery from the view of redundancy detection.
    \item We propose circuit-tuning, an algorithm that iteratively finds out the components necessary for a task and performs fine-tuning in a mechanistic way. Our method not only performs well but is also scalable to various models and tasks while preserving general capabilities.
    \item We provide a comprehensive analysis on training dynamics based on circuit-tuning and present some amazing findings for the self-organization inside a model, providing new insights and improvements for both fine-tuning and interpretability. 
    % \item We provide analyses on training dynamics based on circuit-tuning and present some amazing findings worth studying, providing new insights and improvements for both fine-tuning and interpretability. 
\end{itemize}

% While a great number of parameter-efficient fine-tuning methods have been studied \cite{lora}, they are lack of interpretability. For example, the LoRA method directly chooses a rank $r$ and updates the rank decomposition matrices. Though LoRA reduces compuation to a great extent, it faces the problem of losing general capabilities. This is because LoRA does not separate the irrelevant parameters from those needed in the target task. As a result, when the updates are added to the original parameters, unwanted changes may appear in the irrelevant parameters, which may affect the general capabilities of the model.

% Previous studies focus on the explanation of a static circuit for a specific model behavior, while the training dynamics inside the circuits are still under explored. Also, previous studies are subject to the analyses on circuits, while how a circuit can be put into practice for a model to acquire new capabilities remains unexplored.

% In our method, we combine circuit discovery with fine-tuning to iteratively find the relevant structure and only update the parameters responsible for the target task.
% we conduct experiments on real world tasks. We test on a variety of models, showing great adaptability of our method.


\section{Background and Preliminaries}

\subsection{Features and Representations}
% \begin{definition}
% (Representation Space) Given an input $x$ as the input to a neural network. If $x$ is mapped into a vector $\mathbf{v_{x}} \in \mathbb{R}^{D}$ by an operation $g$, then we say $\mathbf{v_{x}}$ is the representation of $x$ under the mapping of $g$, and the D-dimensional linear space $\mathbb{V}^{D}$ where $\mathbf{v_{x}}$ is located is called a representation space. Each neuron in $\mathbb{V}^{D}$ corresponds to a direction orthogonal to other neurons. All the neurons form a set of bases $\mathbf{E}=\{\mathbf{e_{1}, e_{2},...,e_{D}}\}$ for $\mathbb{V}^{D}$.
% \end{definition}

\begin{definition}
(Feature Vector) Given a representation space $\mathbb{V}^{D}$ with a set of bases $\mathbf{E}=\{\mathbf{e_{1}, e_{2},...,e_{D}}\}$ where each basis $e_{i}$ corresponds to a neuron, the feature vector $\mathbf{v_{f}}$ of feature $f$ is a direction in $\mathbb{V}^{D}$, which is:
\begin{equation*}
    \setlength{\abovedisplayskip}{4pt}
    \setlength{\belowdisplayskip}{4pt}
    \mathbf{v_{f}} = \sum_{i=1}^{m}c_{i}\mathbf{e_{i}} = c_{1}\mathbf{e_{1}} + c_{2}\mathbf{e_{2}} +...+ c_{m}\mathbf{e_{m}}
\end{equation*}
where the coefficient $c_{i}$ satisfies $|ci| \leq 1$ and $\sum_{i=1}^{m}ci^{2}=1$.
\vskip -0.05pt
\label{def:feature vector}
\end{definition}

\begin{remark}
    The idea of \cref{def:feature vector} follows the linear representation hypothesis that features are represented as vectors in a linear space, the evidence of which has been widely discussed in \cite{embedding, dictionary, linearRepr}.
\end{remark}

\begin{assumption}
(Superposition) Given a neural network $\varphi$ and a representation space $\mathbb{V}^{D}$. If all features represented in $\varphi$ is $\mathcal{F}$, then we have
$ D < |\mathcal{F}| $,
where $|\mathcal{F}|$ is the number of elements in $\mathcal{F}$. 
\label{ass:superposition}
\end{assumption}

\begin{remark}
     The idea comes from the superposition hypothesis discussed in \cite{zoomin, toymodelsuperposition} which explains the phenomenon of polysemanticity that one neuron in a neural network often responds to multiple unrelated inputs. Since there are much more features than the dimensions in $\mathbb{V}^{D}$, a feature $f$ may not be aligned with a single basis. 
     In fact, it is akin to the strategy of population coding \cite{populationCoding} in human brains that information is encoded by clusters of cells.

     Considering the settings of \cref{def:feature vector} and \cref{ass:superposition}, we can have the conclusion: Given a task $T$ and an input $x$ with its features $\mathcal{F}_{x} = \{f_{1},f_{2}, ..., f_{t}\} \subseteq \mathcal{F}_{T}$. Suppose the feature vectors corresponding to the  features in $x$ are $ \mathbf{V_{x}} = \{ \mathbf{v_{f_{1}}}, \mathbf{v_{f_{2}}}, ..., \mathbf{v_{f_{t}}} \} $ and the activations of $\mathcal{F}_{x}$ are $ A_{x} = \{ a_{f_{1}}, a_{f_{2}}, ..., a_{f_{t}} \} $, then the magnitude $a_{i}$ of $\mathbf{e_{i}}$ in the representation space $\mathbb{V}^{D}$ can be written as:
    \begin{equation} \label{eq:magnitude}
        \setlength{\abovedisplayskip}{4pt}
        \setlength{\belowdisplayskip}{4pt}
        a_{i} = \Big(\sum_{i=1}^{t} a_{f_{i}}\mathbf{v_{f_{i}}} \Big) \cdot \mathbf{e_{i}}
    \end{equation}
    This means the magnitude of dimension $i$ in $\mathbb{V}^{D}$ often originates from more than one features.
\end{remark}



\subsection{Computational Graph}
If we view a model as a directed acyclic graph (DAG), then the nodes are terms in its forward pass (neurons, attention heads, etc.) and the edges are the interactions between the nodes. Note that the definitions of the node and edge do not necessarily follow the structure of the model. The shape of a node depends on the level of granularity when we inspect a model, and an edge can be a virtual connection between nodes far apart from each other. 

\subsection{Circuit Discovery}
A circuit is a subgraph in a computational graph that is responsible for a certain behavior. Recent studies generally use causal intervention for circuit discovery. \cite{rome} proposed activation patching to identify activations in a model relevant to the output, while \cite{atpNanda} proposed attribution patching to accelerate it. \cite{ioi, acdc} focused on edges and proposed path patching.
Others optimized this technique from various aspects \cite{atp, atp*}.

\subsection{Intrinsic Dimension}
\cite{intrinsic} first proposed the concept of intrinsic dimension to describe the minimum number of dimensions needed for solving a specific problem. \cite{intrinsic1} provided an analysis into fine-tuning with this concept and proved the feasibility of effective fine-tuning. Inspired by previous works, \cite{lora} proposed LoRA as a parameter-efficient fine-tuning method, which is of great significance for fine-tuning large language models.


\section{The Theory of Node Redundancy}

% 讨论 intrinsic dimension，针对的是神经网络中的某一个参数矩阵中参数的冗余性。我们可以通过考察经过该矩阵映射后得到的表征$H$来研究其冗余性。
In this section, we introduce the concept of node redundancy as an extension of the intrinsic dimension, together with a method for detecting it, which can also be used to explain the idea behind circuit discovery from a new perspective. 

Since the intrinsic dimension describes the redundancy in parameters, our goal is also to find out the redundant parameters in a model. If we inspect a representation $H=({h_{1}, h_{2}, ..., h_{m}})^{\top} \in \mathbb{V}^{D}(D=m)$, then we can denote the parameter matrix to examine as $W^{pre}\in \mathbb{R}^{m \times n}$, which maps an input from $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$.

\subsection{Representation Redundancy}

\begin{assumption}
(Feature Redundancy) Suppose all features represented in a neural network is $\mathcal{F}$. Given a task $T$ with $\mathcal{X} = \{ x_{1}, x_{2}, ..., x_{t} \} \sim \mathcal{D}_{T}$ which consists of $t$ samples that follows a specific data distribution $\mathcal{D}_{T}$, the features for representing $\mathcal{X}$ is $\mathcal{F}_{T}\subsetneqq \mathcal{F}$.
\label{ass:feature redundancy}
\end{assumption}

\begin{remark}
\cref{ass:feature redundancy} means that when conditioned on a task $T$, the number of features needed is smaller than that of the elements in the universal set $\mathcal{F}$. The assumption follows the concept of feature sparsity from \cite{toymodelsuperposition} that many features do not frequently appear.
\end{remark}

\begin{definition}
(Semantic Preservation) Consider a feature $f$ with its feature vector $\mathbf{v_{f}} = \sum_{i=1}^{m}c_{i}\mathbf{e_{i}}$. If we set the coefficients in $\mathbf{v_{f}}$ which correspond to a set of bases $\mathbf{E_{r}}$ to zero, then we can get a new vector $\tilde{\mathbf{v_{f}}}$. If the cosine similarity $ cos(\mathbf{v_{f}}, \tilde{\mathbf{v_{f}}}) = \frac{\mathbf{v_{f}} \cdot \tilde{\mathbf{v_{f}}}} {||\mathbf{v_{f}}|| \cdot ||\tilde{\mathbf{v_{f}}}||} > \eta$ $(\eta \in (0, 1])$, then we say $\tilde{\mathbf{v_{f}}}$ has preserved the semantic information in $f$. 
\end{definition}

\begin{remark}
    It is a nice property akin to the robustness of population encoding to noise. Since information is encoded across many cells, a perturbation on a few cells will not destroy the representation \cite{populationCoding}.
\end{remark}

\begin{definition}
(Dimension Redundancy) Given a task $T$ with a number of features $\mathcal{F}_{T}$. Let's consider a single basis $\mathbf{e_{i}}$ in the representation space $\mathbb{V}^{m}$. For any feature $f$ in $\mathcal{F}_{T}$, we set the coefficient $c_{i}$ in $\mathbf{v_{f}}$ to zero and get the deformed feature vector $\tilde{\mathbf{v_{f}}}$. If $\mathbf{e_{i}}$ satisfies:
\begin{equation*}
    \setlength{\abovedisplayskip}{7pt}
    \setlength{\belowdisplayskip}{7pt}
    min[cos( \mathbf{v_{f_{1}}}, \tilde{\mathbf{v_{f_{1}}}} ), ..., cos( \mathbf{v_{f_{|\mathcal{F}_{T}|}}}, \tilde{\mathbf{v_{f_{|\mathcal{F}_{T}|}}}} )] > \eta
\end{equation*}
then $\mathbf{e_{i}}$ is a redundant dimension for representing features in task $T$. Note that $\eta \in (0, 1]$ serves as the lower bound for judging whether $\tilde{\mathbf{v_{f}}}$ preserves the information in $f$.
\label{def:dim redundancy}
\end{definition}

\begin{proposition}
Consider the setup of \cref{def:dim redundancy}. If $\mathbf{e_{i}}$ satisfies:
\begin{equation*}
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{8pt}
    max(|\mathbf{v_{f_{1}}} \mathbf{e_{i}}|,|\mathbf{v_{f_{2}}} \mathbf{e_{i}}|, ..., |\mathbf{v_{f_{|\mathcal{F}_{T}|}}} \mathbf{e_{i}}|) < \xi
\end{equation*}
where $\xi=\sqrt{1-\eta^{2}}$, then we say $\mathbf{e_{i}}$ is redundant for representing features in task $T$.
\label{prop:dim redundancy}
\end{proposition}

\begin{remark}
This is obvious from \cref{def:dim redundancy}. Note that $|\mathbf{v_{f_{j}}} \mathbf{e_{i}}|$ is the projection of $\mathbf{v_{f_{j}}}$ on direction $\mathbf{e_{i}}$. Higher $|\mathbf{v_{f_{j}}} \mathbf{e_{i}}|$ means higher dependency of $\mathbf{v_{f_{j}}}$ on $\mathbf{e_{i}}$. 
% If some tolerance $\tau_{r} > 0$ is allowed, then we can redefine the dimension redundancy in a specific task as:
% $$ \sum_{j=1}^{|\mathcal{F}_{T}|} max(|\mathbf{v_{f_{j}}} \mathbf{e_{i}}| - \xi, 0) < \tau_{r} $$
% The weak definition of dimension redundancy requires the total deformation in feature vectors that exceeds the lower bound $\eta$ for semantics preservation should be limited within a threshold $\tau_{r}$.
% From \cref{prop:dim redundancy}, 
The redundancy probability $P_{r}$ of a single basis $\mathbf{e_{i}}$ can be written as:
\begin{equation*}
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    P_{r} = \prod_{j=1}^{|\mathcal{F}_{T}|} P(|\mathbf{v_{f_{j}}} \mathbf{e_{i}}| < \xi)
\end{equation*}
The equation tells us that the probability a basis $\mathbf{e_{i}}$ is redundant will decrease with the increasing number of features. On the contrary, the fewer features related to a specific task, the higher the likelihood that a certain basis is redundant, leading to more redundant dimensions in the representation.
\end{remark}

\begin{definition}
(Representation Redundancy) Given a specific task $T$ with features $\mathcal{F}_{T}$, we choose a set of bases $\mathbf{E_{r}} \subset \mathbf{E}$. For each feature $f \in \mathcal{F}_{T}$, we set the coefficients in its feature vector $\mathbf{v_{f}}$ which correspond to the bases in $\mathbf{E_{r}}$ to zero and get the deformed feature vector $\tilde{\mathbf{v_{f}}}$. Given a threshold $\tau_{r}$ as the tolerance of deformation. If $\mathbf{E_{r}}$ satisfies:
\begin{equation*}
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \sum_{j=1}^{|\mathcal{F}_{T}|} max[(\eta - cos( \mathbf{v_{f}}, \tilde{\mathbf{v_{f}}} ) ), 0] < \tau_{r}
\end{equation*}
then $\mathbf{E_{r}}$ is a set of redundant bases for representing features $\mathcal{F}_{T}$, and the representation redundancy in $\mathbb{R}^{m}$ is defined under the choice of $\mathbf{E_{r}}$. If $|\mathbf{E_{r}}| = r$, then the value of representation redundancy is equal to $r$.  
                    
\end{definition}

\begin{remark}
The definition of representation redundancy is actually an extension of \cref{def:dim redundancy} from one-dimension to multi-dimension. The difference $\eta - cos( \mathbf{v_{f}}, \tilde{\mathbf{v_{f}}} )$ is used to measure the degree of deformation of feature vectors. If the total deformation in feature vectors is accepted under the threshold $\tau_{r}$, then the influence from the removal of $\mathbf{E_{r}}$ on the semantic representation of $\mathcal{F}_{T}$ can be ignored, and the number of useful dimensions for representing $\mathcal{F}_{T}$ is $m-r$.
\end{remark}

\subsection{Forward Redundancy}
\label{section:fwd redundancy}
If the representation $H=(h_{1}, h_{2}, ..., h_{m})^{\top}$ is followed by another weight matrix $W^{post} \in \mathbb{R}^{l\times m}$, then the following representation is $ H^{\prime} = W^{post}H \in \mathbb{V}^{D^{\prime}}(D^{\prime}=l) $. To investigate into the influence of $H$ on $H^{\prime}$, we split $H^{\prime}$ into the sum of influences from the dimensions in $H$:
\begin{equation}\label{eq:fwd}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
    W^{post}H = \sum_{i=1}^{m}W_{:,i}^{post}h_{i}
\end{equation}
From \eqref{eq:fwd}, we find that the influence from $h_{i}$ to $H^{\prime}$ is subject to $W_{:,i}^{post}$. If the value of $W_{:,i}^{post}$ is small, then the influence from $h_{i}$ will be weakened, leading to a reduced impact on the final result in the follow-up calculations.

\begin{definition}
(Forward Redundancy) Given a representation $H$ and the follow-up parameters $W^{post}$, we inspect the $i$-th dimension in $H$. Given a lower bound $\tau_{f} < 0$ for the influence of $H$ in the forward propagation.If
\begin{equation*}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    ||W_{:,i}^{post} h_{i}|| < \tau_{f}
\end{equation*}
then we say the dimension $i$ in $\mathbb{R}^{m}$ is redundant in the forward propagation. If the number of redundant dimensions under this condition is $r$, then the value of forward redundancy is equal to $r$.
\end{definition}

\subsection{Node Redundancy}
\label{sec:node redundancy}
\subsubsection{The definition of node redundancy}
The discussions in previous sections focus on the real computations inside a model. In this section, we regard the whole model as a DAG composed of nodes and edges. The node could be a single neuron or an attention head in Transformer, based on the level of granularity.

To determine whether a node is redundant or not, a method is needed for measuring the importance of that node. Inspired by the idea of causal intervention in the area of causal inference, we provide the following definition:
\begin{definition}
   (Node Redundancy) Given a model $M$ and its computational graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ in which $\mathcal{V}$ and $\mathcal{E}$ represent the nodes and edges in $\mathcal{G}$ respectively, consider a node $n \in \mathcal{V}$. For task $T$, a batch of inputs $\mathcal{X} = \{x_{i}\}_{i=1}^{p} \sim \mathcal{D}_T$ is fed into the model and the activation of $n$ is saved.

    For each input $x_{i}$, if we replace $n(x_{i})$ which is the value of node $n$ with another value $n(x_{i}^{\prime})$ corresponding to the corrupted input $x_{i}^{\prime}$ while keeping other activations in the forward propagation unchanged, we can get another output $M(x_{i}^{\prime})$. Given a metric $f$ for measuring the difference between two output distributions caused by $n$ and a threshold $\tau$, we can calculate the contribution $c_{i}(n)$ of the node $n$ as:
    \begin{equation*}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
    c_{i}(n) = f(n; M(x_{i}), M(x_{i}^{\prime}))
    \end{equation*}
    If the contribution of $n$ satisfies:
    \begin{equation*}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
        \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[|c_{i}(n)|\big] < \tau
    \end{equation*}
    then $n$ is a redundant node in $\mathcal{G}$. If the redundant nodes in terms of task $T$ in model $M$ is $\mathcal{N}$, then the value of the intrinsic dimension at the node level is $|\mathcal{V}-\mathcal{N}|$.
\end{definition}


\subsubsection{The detection of node redundancy}
With the idea of intervention from causal inference, we adopt the concept of indirect effect \cite{Pearl2001} to estimate the contribution of a node $n$ to the output as:
\begin{equation*}
    \setlength{\abovedisplayskip}{7pt}
    \setlength{\belowdisplayskip}{7pt}
    IE(n; x) = \mathcal{L}_{m}\big[M\big(x \,|\, do\big(n\leftarrow n(x^{\prime})\big)\big)\big] - \mathcal{L}_{m}\big[M(x)\big]    
\end{equation*}
in which $IE$ is the indirect effect from node $n$ to the final output of the model, and $\mathcal{L}_{m}$ is a metric for measuring the final output. We use the do-calculus notation \cite{Pearl2000} to express the intervention behavior, which is also called \textit{patching}. The metric $IE$ measures the effect of an intermediate variable in a causal graph to the final behavior of the model, which is widely used in causal inference.
If $\mathcal{L}_{m}$ is viewed as a function of $n$, we can apply a first-order Taylor expansion to $IE$ at $n = n(x^{\prime})$ for approximation. Thus $IE(n;x)$ can be simplified as
\begin{equation} \label{def:node score}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
    \begin{split}
    &\mathcal{L}_{m}[M(x)] \!+\! [n(x^{\prime})  \!-\! n(x)]^{\top} \nabla_{n} \mathcal{L}_{m}[M(x)]  \!-\! \mathcal{L}_{m}[M(x)] \nonumber \\
    = &[n(x^{\prime}) \!-\! n(x)]^{\top} \nabla_{n} \mathcal{L}_{m}[M(x)] \big|_{n(x)}
    \end{split}
\end{equation}
If zero ablation is used which means $n(x^{\prime})$ is set to zero, then the contribution of node $n$ can be written as:
\begin{equation}\label{eq:IE}
    \setlength{\abovedisplayskip}{7pt}
    \setlength{\belowdisplayskip}{7pt}
    c(n) \approx \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ n \nabla_{n} \mathcal{L}_{m}[M(x)] \big] 
\end{equation}
The IE in equation \eqref{eq:IE} consists of two parts: (i) $n$ the value of node $n$ on the clean input $x$ and (ii) $ \nabla_{n} \mathcal{L}_{m}[M(x)] $ the derivatives of the patching metric $\mathcal{L}_{m}$ with respect to node $n$. The two parts correspond to the \textbf{representation redundancy} and the \textbf{forward redundancy} respectively. To gain an intuitive understanding of this, let's consider a representation in $\mathbb{V}^{m}$ which consists of a set of nodes $\mathcal{N}=\{ n_{1}, n_{2},..., n_{m} \} \subset \mathcal{V}$ at the neuron level and then focus on the node $n_{i}$:

\begin{itemize}
    \setlength{\itemsep}{7pt}
    \setlength{\itemsep}{7pt}
    \item The first part of \eqref{eq:IE} directly serves as an indicator for representation redundancy since the magnitude of a neuron is a good indicator for dimension redundancy. According to \eqref{eq:magnitude}, the smaller the value of a neuron is, the higher the likelihood that the neuron corresponds to a redundant dimension, since the activation of a feature $f$ on a basis $\mathbf{e_{i}}$ is directly proportional to the projection of its feature vector $\mathbf{v_{f}}$ on $\mathbf{e_{i}}$.
    
    One thing to note is the case where two features $f_{1}$ and $f_{2}$ are almost in opposite directions and the activations in the shared direction may cancel out each other, which results in a situation that the value of a neuron seems to be small though both of the two features fire on it. While this case may be tricky, \cite{toymodelsuperposition} show that models prefer to represent anti-correlated features in opposite directions, which means $f_{1}$ and $f_{2}$ may never co-occur at the same time, so the probability of the above case occurring is very small.

    \item As for the second part, according to the chain rule during the back propagation stage, it can be written as:
    \begin{equation*}
    \setlength{\abovedisplayskip}{2pt}
    \setlength{\belowdisplayskip}{2pt}
    \nabla_{n_{i}} \mathcal{L}_{m} = \sum_{j=1}^{l} \nabla_{n_{j}} \mathcal{L}_{m} \cdot \nabla_{n_{i}} n_{j}
    \end{equation*}
    where the node $n_{j}$ in the follow-up vector space $\mathbb{V}^{l}$ is
    \begin{equation*}
    \setlength{\abovedisplayskip}{2pt}
    \setlength{\belowdisplayskip}{2pt}
    n_{j} = \sum_{i=1}^{m}w_{ji}^{post}n_{i}
    \end{equation*}
    Thus
    \begin{equation} \label{eq:fwd1}
    \setlength{\abovedisplayskip}{1pt}
    \setlength{\belowdisplayskip}{1pt}
        \nabla_{n_{i}} \mathcal{L}_{m} = \sum_{j=1}^{l} \nabla_{n_{j}} \mathcal{L}_{m} \cdot w_{ji}^{post}        
    \end{equation}
    From equation \eqref{eq:fwd1}, it can be seen that the second part in \eqref{eq:IE} is closely related to the follow-up calculations as discussed before in \cref{section:fwd redundancy}, hence serving as an indicator for forward redundancy.
\end{itemize}

As discussed above, we can see that the metric for judging the redundancy of a node is a combination of the representation redundancy and the forward redundancy. 
Therefore the essence of the intrinsic dimension at the node level can be decomposed and understood from the above two aspects. 

Note that \cref{def:node score} is also adopted in circuit discovery, e.g., attribution patching \cite{atpNanda}. Thus the process of circuit discovery can be viewed as a process of identifying the two types of redundancy and finding out the intrinsic dimensions in terms of a task. Each of the intrinsic dimensions corresponds to a node in the computational graph, and the number of the nodes in the circuit is expected to be equal to the dimensionality of the solution space. Given a specific task, the co-occurrence of the representation redundancy and the forward redundancy is the necessary and sufficient condition of the node redundancy in the graph. 


\section{Main Method}
In this section, we introduce circuit-tuning, an algorithm based on the theory of node redundancy for both fine-tuning neural networks and analyzing training dynamics.

\begin{algorithm}[tb]
   \caption{Circuit-tuning}
   \label{alg:circuit-tuning}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{X}$, model $M$, loss function $\mathcal{L}$, \\
   the metric for measuring indirect effect $\mathcal{L}_{m}$, \\
   the number of edges to save $N$, \\
   the optimization steps after circuit discovery $K$.
   \STATE Initialize the computational graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ from $M$, the circuit $\mathcal{C}=\mathcal{G}$, and the iteration step $i=0$.
   \FOR{mini-batch $\mathcal{X}_{T}=\{x_{1}, x_{2}, ..., x_{t}\}$ in $\mathcal{X}$}
       \STATE Run a forward and backward pass on $\mathcal{X}_{T}$
       \IF{$i \bmod K == 0$}
           \STATE  Reset: $\mathcal{C} \leftarrow \mathcal{G}$
           \FOR{edge $e: n_{1} \rightarrow n_{2} \in \mathcal{E}$}
               \STATE Intervene: $ n_{2}(x^{\prime}) \! =\! n_{2}^{direct}\big(x|do\big(n_{1} \! \leftarrow n_{1} \!(x^{\prime})\big)\big) $
               \STATE Edge contribution: $c(e)=\mathbb{E}_{x_{i}\sim \mathcal{X}_{T}}\big[ |IE(e; x_{i})| \big]$
           \ENDFOR
           % \STATE $\mathcal{E}_{T} \leftarrow \big\{e \,|\, c(e) \in TopN \big(\{c(e)\mid e \in \mathcal{E}\} \big) \big\}$
           \STATE $\mathcal{E}_{T} \leftarrow \{e \,|\, e \! \in \! \mathcal{E} \text{ with top-N edge contributions} \}$
           \STATE $\mathcal{V}_{T} \leftarrow \! \{n \,|\, n \! \in \! \mathcal{V} \land n \text{ is incident to an edge } e \! \in \! \mathcal{E}_{T} \}$
           \STATE $\mathcal{C} \leftarrow (\mathcal{V}_{T}, \mathcal{E}_{T})$
       \ENDIF
       \STATE Update the parameters in $\mathcal{C}$
       \STATE $i = i + 1$
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Circuit-tuning}

\subsubsection{From Nodes to Edges}
The definition of node redundancy as well as the detection of it is provided in \cref{sec:node redundancy}. Since the nodes inside a graph are connected by edges, we can also study the node redundancy from the view of edges. 

Following the definition of the attribution score in edge attribution patching (EAP) \cite{atp} and the discussions in \cref{sec:node redundancy}, we can define the contribution of an edge $e$ to the output as follows: 
\begin{equation} \label{def:edge score}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    \begin{aligned} 
    c(e) &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |c_{i}(e)| \big] \\
    &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |IE(e; x_{i})| \big]
    \end{aligned}
\end{equation}
% \begin{align} \label{def:edge score}
%     c(e) &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |c_{i}(e)| \big] \nonumber \\
%     % &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |f(e; M(x_{i}), M(x_{i}^{\prime}))| \big] \nonumber \\
%     &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |IE(e; x_{i})| \big] \
% \end{align}
where the edge $e: n_{1} \rightarrow n_{2}$ is connected with an upstream node $n_{1}$ and a downstream node $n_{2}$. To measure the direct effect from $n_{1}$ to $n_{2}$, we set the value of $n_{1}$ to another value $n_{1}(x^{\prime})$ while keeping all the other nodes between $n_{1}$ and $n_{2}$ unchanged. Then the affected value of $n_{2}$ is:
\begin{equation*}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    n_{2}(x^{\prime}) = n_{2}(x) - n_{2}^{n_{1}}(x) + n_{2}^{n_{1}}(x^{\prime})
\end{equation*}
where $n_{2}^{n_{1}}(\cdot)$ denotes the contribution from $n_{1}$ to $n_{2}$, and
\begin{equation*}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    n_{2}^{n_{1}}(x^{\prime}) = n_{2}^{n_{1}}\big(x\,|\, do\big(n_{1} \leftarrow n_{1}(x^{\prime})\big)\big)
\end{equation*}
Then the indirect effect from $e$ to the final output is
\begin{equation*}
    \setlength{\abovedisplayskip}{6pt}
    \setlength{\belowdisplayskip}{6pt}
    \begin{split}
    IE(e;x) = &IE(n_{1} \rightarrow n_{2};x) \\
            = &\mathcal{L}_{m}\big[M\big(x \,|\, do\big(n_{2} \leftarrow n_{2}(x^{{\prime}})\big)\big)\big] - \mathcal{L}_{m}\big[M(x)\big]
    \end{split}
\end{equation*}
To reduce computation, we apply a first-order Taylor expansion to $n_{1}^{\prime}$ and $n_{2}^{\prime}$ respectively. Then the calculation of edge contribution requires only one forward and one backward pass. 
% For details of the derivation, please refer to \cref{appn:edge derivation}.
% Details of the derivation are shown in \cref{appn:edge derivation}.
See  \cref{appn:edge derivation} for derivation details.

Therefore the approximation of contribution $c(e)$ is available which can be used to measure the importance of an edge. We can determine the redundancy of a node by checking the contributions of the edges connected to it.

\subsubsection{The Circuit-tuning Algorithm}
\label{sec:alg}
 Given a model $M$ for fine-tuning, the model is initialized into a computational graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ in which $\mathcal{V}$ and $\mathcal{E}$ represent the nodes and edges in $\mathcal{G}$ respectively. A metric $\mathcal{L}_{m}$ and a loss function $\mathcal{L}$ are used for circuit discovery and parameter optimization respectively. Given a dataset $\mathcal{X} \sim \mathcal{D}_{T}$ for the fine-tuning task $T$, circuit-tuning alternately performs the following two procedures:

\textbf{Circuit Discovery} For a batch of data $\mathcal{X}_{T} \in \mathcal{X}$, we calculate the contribution of each edge $e \in \mathcal{E}$ according to \eqref{def:edge score}. Then all the edges are sorted in descending order based on their contributions, and the edges with top $N$ contributions are selected. Then the graph $\mathcal{G}$ is pruned into a circuit $\mathcal{C}=(\mathcal{V}_{T}, \mathcal{E}_{T})$ with only the selected edges $\mathcal{E}_{T}$ together with the nodes $\mathcal{V}_{T}$ at both ends of each edge inside.

\textbf{Circuit-tuning} For a batch of data $\mathcal{X}_{T} \in \mathcal{X}$, all the parameters outside $\mathcal{C}$ are frozen, and only the parameters corresponding to the nodes $\mathcal{V}_{T}$ inside $\mathcal{C}$ are updated through a forward pass and a backward pass as usual. After $K$ steps of optimization, all the frozen parameters are freed and the graph $\mathcal{G}$ is reset to its original state. 

% In practice, circuit-tuning alternates between the two procedures above. We can either perform circuit discovery and circuit-tuning in sequence in each iteration, or iterate $k$ steps for optimization after circuit discovery. The setting is flexible depending on the specific scenarios.

The full process is shown in \cref{alg:circuit-tuning}. The parameter to optimize corresponds to the parameter matrix that maps an input to the activation of a node in the circuit. For example, the parameter corresponding to the node at the attention output refers to $W_{O}$, which is the output projection matrix. 

% As for the selection of the metric $\mathcal{L}_{m}$ and the loss function $\mathcal{L}$, please refer to the discussions in \cref{sec:experiments}. 

\subsection{Characteristics of Our Method}
\textbf{Capability Acquisition} Different from recent studies that focus on the circuits of certain model behaviors, circuit-tuning aims to automatically find the relevant components responsible for a specific task and guide the model to develop an ability through parameter optimization.  The dynamics of this process will be verified in \cref{sec:sv}.  

\begin{figure*}[!t]
\centering
\subfigure[logit difference]{
    \includegraphics[scale=0.35]{figures/topn_logit_diff.pdf}
    \label{fig:logit diff}
    }
\subfigure[PPL]{
    \includegraphics[scale=0.35]{figures/topn_ppl.pdf}
    \label{fig:ppl}
    }
\subfigure[logit difference after revision]{
    \includegraphics[scale=0.35]{figures/topn_new.pdf}
    \label{fig:revision}
    }
\caption{Results in the subject-verb disagreement task}
\label{fig:sv}
\vskip -0.05in
\end{figure*}

\textbf{Precise Fine-tuning}
Compared with full fine-tuning and LoRA-based methods, our method only update a few parameters instead of taking all of the parameters into account. 
% Note that reducing the computation is not our aim. The motivation of circuit-tuning is to find out the components that are necessary for completing a task and perform the fine-tuning work in a mechanistic way. 
Since our method tries to leave out the irrelevant structures in terms of a target task, the general abilities are expected not to be affected, which will be verified in \cref{sec:experiments}.


\subsection{Convergence Analysis}
\label{sec:convergence}
To demonstrate the stability of circuit-tuning, we provide an analysis of the training convergence. Specifically, let's consider the model as a continuously differentiable function $f(\theta)$. Let $\theta \in \mathbb{R}^{D} = (\theta_{1}, \theta_{2}, ..., \theta_{D})$, and suppose that $f$ satisfies the conditions for Lipschitz gradient continuity:
\begin{equation*}
    \setlength{\abovedisplayskip}{8pt}
    \setlength{\belowdisplayskip}{8pt}
    ||\nabla_{\theta}f(\theta^{\prime}) - \nabla_{\theta}f(\theta)|| \leq L||\theta^{\prime} - \theta||
\end{equation*}
where $L$ is a Lipschitz constant.

If we mask out a set of parameters $\theta_{mask} \subset \theta$, then we have
\begin{equation*}
    \setlength{\abovedisplayskip}{3.5pt}
    \setlength{\belowdisplayskip}{3.5pt}
    \begin{split}
    &||\nabla_{\theta \backslash \theta_{mask}}f(\theta^{\prime}) - \nabla_{\theta \backslash \theta_{mask}}f(\theta)|| \\
    < &||\nabla_{\theta}f(\theta^{\prime}) - \nabla_{\theta}f(\theta)|| \\
    \leq &L||\theta^{\prime} - \theta||
    \end{split}
\end{equation*}
This is because for each masked parameter $\theta_{i} \in \theta_{mask}$, the gradient of it equals to 0, which leads to a smaller norm for gradient change. Thus a smaller Lipschitz constant $L_{mask} < L$ serves as the upper bound for the change rate of gradients, which means the training process will be more stable compared with full fine-tuning. The evidence for this conclusion will be discussed in \cref{exp:sv results}.



\section{Experiments}
\label{sec:experiments}
In this section, we test our method across a variety of models and tasks. To comprehensively investigate the performance as well as the training dynamics during fine-tuning, we arrange the experiments into two parts. The first part serves as the verification and analysis of our method, while the second part aims to show that our method is scalable to various model sizes and tasks.

\subsection{The Subject-verb Disagreement Task}
\label{sec:sv}
\subsubsection{Task Description and Data Preparation}
The goal of the subject-verb disagreement task is to match a verb with a subject in an abnormal way. For example, ``I is", ``he are" and ``the cows eats" are all expected results for this task. In each sentence, the word before the verb is called the END token. The automatic evaluation metric for this task is the logit difference between the flipped verb and the original verb at the END token. This is because the logit at the END token is directly used for predicting the verb.

The reason why we create this task as the start of our experiments is that we expect the model to acquire a new capability from scratch. Besides, the task is simple and interesting for analyzing the training dynamics during fine-tuning.

For training data, we use the first 10k samples in the Pile \cite{pile} corpus. We extract 30k sentences in the present tense in English and flip the forms of the verbs in each sentence. For details, please refer to \cref{appn:sv_data}.

\subsubsection{Implementation Details}
\label{sec:sv_imple}
We use GPT2-small \cite{gpt2} in this task. We set the output of the attention and the MLP in each layer as upstream nodes, and the input of the query, key, value and the MLP in each layer as downstream nodes. During training, we use the logit difference discussed before as the metric $\mathcal{L}_{m}$ and follow \cite{atp} to calculate the edge contributions. We sweep over a range of $N$ which is the number of edges to be saved during circuit discovery. Mini-batch SGD is used for optimization. For details of settings, please refer to \cref{appn:sv_exp}.


\subsubsection{Main Results}
\label{exp:sv results}
From \cref{fig:logit diff}, we observe a flip in logit difference from negative to positive, which means the model adjusts its grammar to fit the data distribution of subject-verb disagreement. One noteworthy finding is that the trained model can generate abnormal texts in the past tense, such as “to be or not to be, that were a ...”, which implies that the model really learns the new grammar and applies it smartly, since there is no sentence in the present tense in our training data at all.

We find that with the increasing number of top $N$ edges, the logit difference gets bigger until $N$ reaches around 1000. We check and find that the number of tunable parameters for each $N$ is also saturated at this point. We believe when $N=1000$, almost all the necessary parameters for this task are included, and the performance cannot be further improved, suggesting the existence of intrinsic dimension. For verification of this, please refer to \cref{appn:sv_topn}.

In \cref{fig:ppl}, we find that the perplexity (PPL) of full fine-tuning is high and fluctuates wildly during training, though the logit difference of it is higher. This observation implies better training stability as well as better preservation of general capabilities of circuit-tuning over full fine-tuning. 

\subsubsection{The Quality of the Discovered Circuit}
To demonstrate that our method is able to find the required parameters accurately, we (i) calculate the faithfulness and completeness \cite{sfc} of the circuits and (ii) provide an ablation study in which we randomly unfreeze the nodes outside the circuit to check their influence on the performance of this task. Results show from various aspects that the parameters required are found accurately. For details, please refer to \cref{appn:sv_topn}.


\subsubsection{Analyses on Training Dynamics}
\label{sec:dynamics}
We analyze the circuits before, during, and after training, and report some amazing findings that are worth studying.
% The process of capability acquisition is the reorganization of the nodes and edges inside the computational graph. Some nodes may preserve their functionalities, while others may change their behaviors in the same or the opposite directions compared with those before fine-tuning. Changes may also happen on the edges, which means the connection between a pair of nodes may be strengthened or weakened.

\begin{figure}[h]
% \vskip -0.05in
\subfigure[Before fine-tuning]{
    \includegraphics[scale=0.165]{figures/flip0.pdf}
    \label{fig:Before fine-tuning}
    }
\hspace{0.01pt}
\subfigure[After fine-tuning]{
    \includegraphics[scale=0.165]{figures/flip1.pdf}
    \label{fig:After fine-tuning}
    }
\caption{The flip in the Subject Attribute Heads. Each square corresponds to the $j$-th attention head in layer $i$, in which $i$ and $j$ correspond to the vertical and horizontal axis respectively.}
% \vskip -0.05in
\label{fig:flip}
\end{figure}

\textbf{Interpretation of the Circuits}
To interpret the circuit into human-understandable structures, we follow \cite{ioi} and decompose the circuit into heads of different functions. Firstly we find out the attention head that directly affects the output, which is called the Subject Attribute Head. Then we find out the head responsible for the localization of the subject, which is called the Subject Identification Head. Finally, we find out the Collaborative Heads that could affect the behaviors of the Subject Attribute Heads. For details of the processes above, please refer to \cref{appn:sv_interp}.

\textbf{The Flip of the Subject Attribute Heads}
The Subject Attribute Heads are responsible for matching the subject and the verb. We visualize the heads before and after fine-tuning in \cref{fig:flip}. Through comparison, we observe an obvious flip at head.10.9, which implies the reversal in its function from subject-verb agreement to disagreement. Other heads (3.0, 6.0, 4.4, 11.8, etc) also see flips with varying degrees, which serves as strong evidence for the self-adjustment inside the nodes. 


\textbf{The Sharing of the Subject Identification Heads} The Subject Identification Heads attend heavily to the subject. One type of these heads attends to the END token (0.1, 0.3, etc), which is helpful to the cases like ``they are"; the other type of heads (8.5, 10.5, etc) attends to the subject several tokens before, which is helpful to the cases like ``the girl wearing a dress is". Both types of heads perform the same before and after fine-tuning, which means their functions are preserved and shared all the way down. 

\textbf{The Evidence of Hebbian Learning} We visualize the circuits during the fine-tuning process in \cref{fig:hebbian}, and observe a phenomenon akin to Hebbian learning \cite{hebb}. We find that several edges are strengthened during training, just as the synapses between neurons can be strengthened after continuous stimulation. We regard this finding as evidence of the self-organization \cite{som} inside the model to a new environment. See \cref{appn:sv_hebbian} for details.

Due to limited space, all the above together with other interesting findings are discussed in detail in \cref{appn:sv_dynamics}.


\subsubsection{Improvement on Attribution Patching}
\label{sec:sv_improve}

\begin{table*}[!ht]
\caption{Experiment results of circuit-tuning on complex tasks.}
\label{tab:circuit}
% \vskip 0.05in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c|m{54pt}<{\centering} m{50pt}<{\centering}|m{50pt}<{\centering} m{70pt}<{\centering}|m{50pt}<{\centering}}
    \hline
    \multirow{5}{*}{Methods \& Tasks} & \multicolumn{2}{c|}{Reasoning-based} & \multicolumn{2}{c|}{Reasoning-free} & \multirow{3}{*}{Computation} \\
    % & \multicolumn{2}{c|}{Fine-tuning} & \multicolumn{2}{c|}{Fine-tuning} & \\
    \cline{2-5}
     & Mathematics & Logical Reasoning & Gender De-biasing & Reading Comprehension & \\
     \cline{2-6}
     & Acc@1 (\%) $\uparrow$ & F1 (\%) $\uparrow$ & Prejudice Risk $\downarrow$ & Exact Match / F1 (\%) $\uparrow$  & Avg param ratio \\
    \hline
    \multicolumn{1}{l|}{Llama-3.2-1B-it}  & 40.71 & 20.35 & 0.555 & 39.68, 43.58 & / \\
    \multicolumn{1}{l|}{Llama-3.2-1B-it-full-tuning} & \textbf{46.47} & 26.89 & 0.533 & 36.73, 41.51 & 1.00 \\
    \multicolumn{1}{l|}{Llama-3.2-1B-it-lora} & 44.58 & 22.51 & 0.530 & 34.30, 39.11 & 1.79e-2 \\
    \multicolumn{1}{l|}{Llama-3.2-1B-it-circuit-tuning} & 45.56 & \textbf{27.06} & \textbf{0.312} & \textbf{41.78, 45.45} & 7.65e-2 \\
    \hline
    \multicolumn{1}{l|}{Llama-3.2-3B-it}  & 70.36 & 42.71 & 0.641 & 54.12, 47.98 & / \\
    \multicolumn{1}{l|}{Llama-3.2-3B-it-full-tuning} & \textbf{75.44} & 47.32 & 0.632 & 55.63, 50.10 & 1.00 \\
    \multicolumn{1}{l|}{Llama-3.2-3B-it-lora} & 73.54 & 46.27 & 0.638 & 54.73, 49.44 & 1.49e-2 \\
    \multicolumn{1}{l|}{Llama-3.2-3B-it-circuit-tuning} & 74.35 & \textbf{47.59} & \textbf{0.417} & \textbf{56.58, 50.93} & 8.57e-2 \\
    \hline
    \multicolumn{1}{l|}{Llama-3.1-8B-it}  & 76.19 & 46.41 & 0.651 & 58.92, 52.83 & / \\
    \multicolumn{1}{l|}{Llama-3.1-8B-it-full-tuning} & \textbf{83.76} & 49.53 & 0.640 & 59.60, 54.23 & 1.00 \\
    \multicolumn{1}{l|}{Llama-3.1-8B-it-lora} & 80.21 & 47.64 & 0.643 & 58.97, 53.10 & 1.03e-2 \\
    \multicolumn{1}{l|}{Llama-3.1-8B-it-circuit-tuning} & 82.97 & \textbf{50.54} & \textbf{0.420} & \textbf{60.04, 55.00} & 9.37e-2 \\
    \hline
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.15in
\end{table*}

During the analyses, we notice that some heads fail to appear in the circuit. We assume that the attribution score in EAP fails to measure the importance of a node. This is because an important node $n_{a}$ may be connected with many nodes through edges $\mathcal{E}_{a}$ with low contributions, while another less important node $n_{b}$ may connect with only one node through an edge that is stronger than anyone in $\mathcal{E}_{a}$. As a result, the edges in $\mathcal{E}_{a}$ are discarded, and thus the importance of $n_{a}$ is underestimated. To solve this problem, we revise the contribution $c(e)$ of an edge $e:n_{i}^{u}\rightarrow n_{j}^{d}$ as:
$$ c(e)^{\prime} = c(e) \cdot \sum_{k=1}^{N_{down}^{i}}c(n_{i}^{u} \rightarrow n_{k}^{d}) \cdot \sum_{k=1}^{N_{up}^{j}} c(n_{k}^{u} \rightarrow n_{j}^{d}) $$
The revision takes into account all the edges connected to the upstream node and the downstream node. The results in \cref{fig:revision} show that our improvement is effective without introducing too much computation. Details of the results can be found in \cref{appn:revision}.


\subsection{Application to Complex Tasks}

\subsubsection{Task Description}
We prepare two types of tasks based on whether reasoning is involved, each of which contains two specific tasks.

For reasoning-based tasks, chain-of-thought (CoT) style responses are involved to solve a problem. We prepare a mathematical task and a logical reasoning task since \cite{cot} demonstrated that CoT is effective only on tasks requiring mathematical, logical, or algorithmic reasoning.

For reasoning-free tasks, only the final answer or a signal token is required. We prepare a gender de-biasing task which requires the language model to develop an unbiased perspective on genders, and a reading comprehension task which requires only the keywords as the answer.

As for the metric $\mathcal{L}_{m}$ for circuit discovery, for gender de-biasing, we use the logit difference between male attribution words like $he/his$ and female attribution words like $she/her$.
% because an occupation is placed in a context and the model is required to assign a gender to it. The way to judge the model's behavior is to check the token prediction probabilities on ....
For other tasks, we simply set $\mathcal{L}_{m}$ as identical to the negative log probability loss for language modeling since the behaviors for completing the task are hidden inside multiple tokens and cannot be separated out easily. 

As for the loss $\mathcal{L}$, we selectively add $\beta \cdot |\mathcal{L}_{m}|$ as a regularization term for gender de-biasing to improve performance. 

Details of task settings can be found in \cref{appn:complex_task}.
% $$ \mathcal{L} = \mathcal{L}_{NLL} + \beta\cdot|\sum_{i=1}^{k} logit(w_{i}^{m}|END) - logit(w_{i}^{f}|END)| $$
% where $w^{m}$ and $w^{f}$ are attribution words for males and females respectively. Details can be found in \cref{appn:complex}.

\subsubsection{Dataset and Evaluation Metrics}
\textbf{Reasoning-based Tasks} We use GSM8K \cite{gsm8k} with zero-shot accuracy and Contexthub \cite{contexthub} with F1 score as datasets and metrics for mathematics and logical reasoning respectively.

\textbf{Reasoning-free Tasks} For the gender-debiasing task, we use BUG \cite{bug} for training and WinoBias \cite{winobias} for evaluation. We use the prejudice risk proposed in \cite{prejudice} as the evaluation metric. For the reading comprehension task, we use SQuAD 2.0 \cite{squad} with exact match and F1 score.

\textbf{General Capabilities} To check if other capabilities are preserved after training, we evaluate on benchmarks involving general abilities as well as reasoning, coding, and multilingual abilities.
For details, please refer to \cref{appn:complex_eval}.
% For general abilities, we use MMLU \cite{mmlu}, Winogrande \cite{winogrande} and IFEval \cite{ifeval}. For reasoning, coding and multilingual abilities, we use GPQA \cite{gpqa}, HumanEval \cite{humaneval} and MGSM \cite{mgsm} respectively.

\subsubsection{Implementation}
We apply our method to llama-3.2-1B / 3B and llama-3.1-8B \cite{llama3}, and compare it with full fine-tuning and LoRA. The graph settings are the same as before in \cref{sec:sv_imple}, except that each MLP layer is split into 64-dimensional heads. The details are shown in \cref{appn:complex_exp}.

\subsubsection{Main Results}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[scale=0.45]{figures/circuit_tuning.pdf}}
\caption{Comparison of different fine-tuning methods}
\label{fig:performance}
\end{center}
% \vspace{-8mm}
\vskip -0.25in
\end{figure}

Results of our method on complex tasks in \cref{tab:circuit} demonstrate that our method is capable of tasks in various scenarios. Note that with a regularization term as guidance, the de-biasing result is clearly better than others, which is further shown in \cref{appn:bias}. In general, circuit-tuning is better than LoRA, and even surpasses full fine-tuning in many tasks with only a small portion of tunable parameters. 

Evaluations on general capabilities are shown in \cref{fig:performance}. The results are averaged over tasks. It can be seen that our method keeps comparable performance with full fine-tuning while preserving general capabilities better than others.

% \begin{figure}[ht]
% \begin{center}
% \vskip -0.1in
% \centerline{\includegraphics[scale=0.55]{figures/circuit_tuning.pdf}}
% \caption{Performance}
% \label{fig:performance}
% \end{center}
% \vskip -0.4in
% \end{figure}

\section{Conclusion}
In conclusion, we propose the theory of node redundancy from the mechanistic view as an extension of the intrinsic dimension. With the guidance of the theory, we propose the algorithm of circuit-tuning, which not only shows great potential in fine-tuning but also serves as a useful tool for analyzing the training dynamics inside a model.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. 
Since our work concerns AI alignment and involves modifying a model's gender stereotype, we expect our work to be used under the guidance of human values.  


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{The Derivation of Edge Contribution}
\label{appn:edge derivation}

According to the definition of attribution score in \cite{atp} and the definition of node redundancy in \cref{sec:node redundancy}, we can define the contribution of an edge $e: n_{1} \rightarrow n_{2}$ with $n_{1}$ as the upstream node and $n_{2}$ as the downstream node. We use the indirect effect $IE$ to measure the change in the output caused by the patching of the edge. Thus given a dataset $\mathcal{X}$ for patching, the contribution of edge $e$ can be expressed as follows:
\begin{align*}
    c(e) &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |c_{i}(e)| \big] \\
    &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |f(e; M(x_{i}), M(x_{i}^{\prime}))| \big] \\
    &= \mathbb{E}_{x_{i}\sim \mathcal{X}}\big[ |IE(e; x_{i})| \big] \
\end{align*}
 Note that the contribution of edge $e$ is directly reflected in the change of the final output (the logit of the language model, etc), which is the indirect effect caused by the change of the value in the downstream node $n_{2}$, while the change of the node $n_{2}$ is directly caused by the change of the upstream node $n_{1}$. The difference between the direct effect and the indirect effect is that the former keeps all the other nodes that could influence $n_{2}$ unchanged and only studies the influence from $n_{1}$ to $n_{2}$, while the latter allows all the changes in nodes between $n_{2}$ and the logit. For more refined definitions for these two concepts, please refer to \cite{Pearl2001}.
 
 Therefore, to measure the direct effect from $n_{1}$ to $n_{2}$, we set the value of $n_{1}$ to another value $n_{1}(x^{\prime})$ while keeping the all other nodes between $n_{1}$ and $n_{2}$ unchanged. The indirect effect of $e$ to the final output is
\begin{align*}
    IE(e;x) = &IE(n_{1} \rightarrow n_{2};x) \\
            = &\mathcal{L}_{m}\big[M\big(x \,|\, do\big(n_{2} \leftarrow n_{2}(x^{\prime})\big)\big)\big] - \mathcal{L}_{m}\big[M(x)\big]
\end{align*}
in which the corrupted value $n_{2}(x^{\prime})$ of the downstream node is
\begin{equation}
    n_{2}(x^{\prime}) = n_{2}(x) - n_{2}^{n_{1}}(x) + n_{2}^{n_{1}}(x^{\prime})
    \label{eq:edge direct effect}
\end{equation}

\cref{eq:edge direct effect} shows the direct effect $n_{2}^{n_{1}}(x^{\prime}) - n_{2}^{n_{1}}(x)$ from $n_{1}$ to $n_{2}$, where 
$$ n_{2}^{n_{1}}(x^{\prime}) = n_{2}^{n_{1}}\big(x \,|\, do\big(n_{1} \leftarrow n_{1}(x^{\prime})\big)\big) $$
% To simplify the equation, we can apply a first-order Taylor expansion to $n_{1}^{\prime}$ and $n_{2}^{\prime}$ respectively.

To simplify the equation, we apply a first-order Taylor expansion to $IE$ at $n_{2} = n_{2}(x)$, then
\begin{align*}
    IE(e;x) \approx  &\mathcal{L}_{m}[M(x)] + \big[n_{2}(x^{\prime}) - n_{2}(x)\big]^{\top} \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big|_{n_{2}(x)} - \mathcal{L}_{m}[M(x)] \\
            =&\big[n_{2}(x^{\prime}) - n_{2}(x)\big]^{\top}  \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big|_{n_{2}(x)}
\end{align*}
Thus we have
\begin{align} \label{eq:edge ie0}
    IE(e;x) = &\big[n_{2}(x) - n_{2}^{n_{1}}(x) + n_{2}^{n_{1}}(x^{\prime}) - n_{2}(x)\big]^{\top} \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big|_{n_{2}(x)} \nonumber \\
    =&\big[n_{2}^{n_{1}}(x^{\prime})- n_{2}^{n_{1}}(x)\big]^{\top} \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big|_{n_{2}(x)}
\end{align}
To further simplify \cref{eq:edge ie0}, we apply another Taylor expansion at $n_{1} = n_{1}(x)$ to $n_{2}^{n_{1}}$. Then we have
\begin{align} \label{eq:edge ie1}
    IE(e;x) \approx &\Big\{ n_{2}^{n_{1}}(x) + \big[n_{1}(x^{\prime}) - n_{1}(x)\big]^{\top} \nabla_{n_{1}}n_{2}^{n_{1}} \big|_{n_{1}(x)} - n_{2}^{n_{1}}(x) \Big\} \cdot \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big|_{n_{2}(x)} \nonumber \\
    =&\big[n_{1}(x^{\prime}) - n_{1}(x)\big]^{\top} \nabla_{n_{1}}n_{2}^{n_{1}} \big|_{n_{1}(x)} \nabla_{n_{2}}\mathcal{L}_{m}[M(x)] \big |_{n_{2}(x)}
\end{align}
Thus we come to the final form of edge contribution in \cref{eq:edge ie1}. Our derivation takes the simplest situation into consideration, while it works well in practice. For more discussions on the relevant topic, please refer to \cite{atp*}. For implementation details, please refer to our code.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transformer Architecture}
\label{appn:notation}
The models we use in our experiments are all decoder-only Transformers. We briefly introduce the Transformer architecture from the mechanistic view, together with its implementation.

A single input of Transformer is $x_{0} \in \mathbb{R}^{T}$, where $T$ is the length of the sequence. The input is firstly embedded into a vector $x \in \mathbb{R}^{D \times T}$ via the embedding matrix $W_{E} \in \mathbb{R}^{D \times V}$, where $D$ is the model dimension. Then $x$ will go through $l$ layers of Transformer blocks for various processings. From the view of \cite{transformercircuits}, we can think of the residual stream as a communication channel that simply receives the output of the self-attention and MLP operations. Each operation reads information from the residual stream and writes the processed information into it. Thus the residual stream is actually the linear sum of various transformations of $x$ together with the original input $x$.

In each Transformer layer $i (i \in [0, L))$, the two important operations are self-attention and MLP. In self-attention, we consider the implementation of multi-head attention. The model dimension is split into $H$ parts, and the attention operation is performed with $H$ attention heads in parallel. Each head is thought to be responsible for a specific function. Consider head.i.j $(j \in [0, H))$,  the input $x$ is firstly projected into query, key and value via $W_{Q}^{i,j}$, $W_{K}^{i,j}$ and $W_{V}^{i,j}$. The projection matrices are all in shape $\mathbb{R}^{\frac{D}{H} \times D}$, thus $x$ is projected into $x_{Q/K/V}^{j} \in \mathbb{R}^{\frac{D}{H} \times T}$. Then attention pattern $A_{i,j} \in \mathbb{R}^{T \times T}$ is computed via $(W_{Q}^{i,j}x_{Q}^{j})(W_{K}^{i,j}x_{K}^{j})^{\top}$ and some scaling and Softmax operations. After that, the weighted output $z \in \mathbb{R}^{\frac{D}{H} \times T}$ is computed via $(W_{V}^{i,j}x_{V}^{j})A_{i,j}$. Finally, the output of head.i.j $Attn_{i}^{j}(x) \in \mathbb{R}^{D \times T}$ is calculated via $W_{O}^{i,j}\cdot z$, where $W_{O}^{i,j} \in \mathbb{R}^{D \times \frac{D}{H}}$. Thus, final output of the self-attention in layer $i$ is $Attn_{i}(x) = \sum_{j=1}^{H}Attn_{i}^{j}(x)$.

For the MLP operation in each layer, the input $x$ is projected into $x_{in}^{i} \in \mathbb{R}^{D_{mlp} \times T}$ via $W_{in} \in \mathbb{R}^{D_{mlp} \times D}$, and projected back to $MLP_{i}(x) \in \mathbb{R}^{D \times T}$ via $W_{out} \in \mathbb{R}^{D \times D_{mlp}}$. In the Llama architecture \cite{llama}, the input $x$ is firstly projected into $x_{pre}^{i} \in \mathbb{R}^{D_{mlp} \times T}$ via $W_{gate}^{i} \in \mathbb{R}^{D_{mlp} \times D}$ and is applied with an activation layer, then a dot product is performed between the activations and $x_{in}^{i} \in \mathbb{R}^{D_{mlp} \times T}$ which is the input projected by $W_{in} \in \mathbb{R}^{D_{mlp} \times D}$.
Note that we can also split the MLP into MLP heads, which is done on Llama series models in the complex tasks in our experiments. For details, please refer to \cref{appn:complex_exp}.

The output of all the $L$ layers are projected into $x \in \mathbb{R}^{V \times T}$ by the unembedding matrix $W_{U} \in \mathbb{R}^{V \times D}$, which is called the logits. The logit at the end of the sequence is further mapped into a probability distribution with Softmax over the vocabulary for predicting the next token.

\section{Details for the Subject-verb Disagreement Task}
\label{appn:sv}
To be brief, the goal of this task is to change the grammar in a language model from (a) to (b) as follows:
\begin{center}
(a) \textit{We apologize, but this video has failed to load.} \\ (b) \textit{We apologizes, but this video have failed to load.}
\end{center}
In the example above, $\textit{We}$ and \textit{this video} are subjects, \textit{apologize} / \textit{apologizes} and \textit{has} / \textit{have} are verbs, and \textit{We} and \textit{video} are also called the END tokens that appear before the verbs. The change from \textit{apologize} to \textit{apologizes} or from \textit{has} to \textit{have} is called a flip.

\subsection{Data Preparation}
\label{appn:sv_data}
We use the first 10k samples from Pile \cite{pile}, which consists of 22 smaller, high-quality datasets. Firstly, in order to get relatively simple and clean sentences, we filter out the content in Github, ArXiv, PubMed Abstracts, PubMed Central, StackExchange, USPTO Backgrounds, Pile-CC, DM Mathematics, and FreeLaw. Thus we do not include code or complex formulas in our data.
Secondly, we split the corpus with periods '.' as intervals. We remove links to websites, images, and other files. We also remove sentences that are too short (less than 25 characters). Thirdly, we leave only the sentences in the present tense in English and ensure that each sentence is a complete sentence with a punctuation like '.', '?', or '!' at the end. Finally, for each verb in the present tense in a sentence, we convert it to its opposite form. That is, we convert a verb with a part of speech VBP like \textit{go} to VBZ like \textit{goes}, and vice versa. For \textit{be (am / is / are)}, we flip them following: \textit{am $\rightarrow$ is, is $\rightarrow$ are, are $\rightarrow$ am}. 

We collect 60,000 samples in total. For experiments, we only use half of the data, which is further split for training (2.4w), validation (3k), and test (3k). All the details for data preparation can be found in our code. 

\subsection{Experiment Settings}
\label{appn:sv_exp}
We use GPT2-small \cite{gpt2} for this task. GPT2-small is a decoder-only transformer with 12 layers and 12 attention heads per attention layer. We set the output of the attention and the MLP in each layer as upstream nodes, and the input of the query, key, value, and the MLP in each layer as downstream nodes. This is because the query, key, and value input for an attention head can only affect downstream nodes via the attention output of that head, so the upstream nodes can only be attention head outputs, which is also discussed in \cite{atp*}. The parameters to update correspond to the upstream and downstream nodes at both ends of the edges, as discussed in \cref{sec:alg}. Details are shown in \cref{tab:sv_para}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.5}
\caption{The settings of the nodes and their corresponding parameters in the subject-verb disagreement task. The number of layers $L=12$ and the number of attention heads in each layer $H=12$.  The notations for parameters are specified in \cref{appn:notation}.}
\vskip 0.15in
\label{tab:sv_para}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{l|cc|cc}
\begin{tabular}{l | m{80pt}<{\centering} m{40pt}<{\centering}| m{60pt}<{\centering} m{60pt}<{\centering}}

\hline
\multirow{2}{*}{Nodes} & \multicolumn{2}{c|}{Upstream} & \multicolumn{2}{c}{Downstream} \\
\cline{2-5}
 & $Attn_{i}^{j}(x)$ & $MLP_{i}(x)$ & $x_{Q/K/V}^{i,j}$ & $x_{in}^{i}$ \\
\cline{1-1}
Parameters & $W_{O}^{i,j}$ & $W_{out}^{i}$ & $W_{Q/K/V}^{i,j}$ & $W_{in}^{i}$ \\
\hline
Range & \multicolumn{4}{c}{$i\in [0,L), j \in [0,H)$} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0in
\end{table}

For all experiments, we set the learning rate to 1e-3, and batch size to 16. We use mini-batch SGD with a momentum equal to 0.9 as the optimizer. Each model is trained for 3 epochs, with 100 steps in the beginning for warmup. During training, we evaluate on the valid set every 100 steps. The metric $\mathcal{L}_{m}$ for measuring the flip from subject-verb agreement to disagreement is the logit difference at the END token, which is:
$$\mathcal{L}_{m} = logit(W_{v_{flip}}|W_{END}) - logit(W_{v}|W_{END})$$
where $W$ denotes the tokens in a sentence. For example, the case ``We apologize, but this video has failed to load." contains two logit differences: $logit(apologizes|We) - logit(apologize|We)$ and $logit(have|video) - logit(has|video)$.  In practice, we only consider the verbs that are tokenized as a single token.

As for the calculation of edge contribution, we follow \cite{atp} and use mean ablation when patching a node. That is to say, for each activation of shape $\mathtt{(batch\_size, seq\_len, d\_model)}$, we replace the value at the END token position in each sample with the mean value of all tokens in all samples in a batch. 


\subsection{Analysis of the Quality of the Discovered Circuit}
\label{appn:sv_topn}
As discussed in \cref{exp:sv results}, the performance cannot be further improved at $N=1000$, where $N$ is the number of edges saved in circuit discovery. To show this intuitively, the changes with $N$ of the logit difference, PPL, and the ratio of the trainable parameters are illustrated in \cref{fig:top n influ}. We can observe that there is a sharp turning point at $N=1000$, where the curves start to be flat. This serves as a sign that there does exist a circuit that includes all the parameters responsible for the subject-verb disagreement task.

To better prove this conclusion and demonstrate the high quality of the circuit found in our method, we provide another two experiments below. 

\begin{figure}[hbt]
\begin{center}
\centerline{\includegraphics[scale=0.55]{figures/topn_influence_new.pdf}}
\caption{The influence from top n edges.}
\label{fig:top n influ}
\end{center}
\vskip -0.2in
\end{figure}

\subsubsection{Faithfulness and Completeness}
Faithfulness and completeness examine a circuit from two different views. Faithfulness tells how much performance a circuit gets, while completeness tells how much performance a circuit fails to capture. Consider a model $M$ with its computational graph $\mathcal{G}$, a circuit $\mathcal{C}$ for a specific task $T$ and a metric $\mathcal{L}_{m}$ for measuring the output of the model, following the definition in \cite{sfc}, the faithfulness of the circuit $\mathcal{C}$ is
$$ \frac{\mathcal{L}_{m}[M(\mathcal{C})] - \mathcal{L}_{m}[M(\varnothing)]}{\mathcal{L}_{m}[M(\mathcal{G})] - \mathcal{L}_{m}[M(\mathcal{\varnothing})]} $$
in which $M(*)$ denotes the forward pass of model $M$ with the nodes outside $*$ mean-ablated, and $\varnothing$ denotes an empty circuit. The completeness is defined as
$$ \frac{\mathcal{L}_{m}[M(\mathcal{G} \backslash \mathcal{C})] - \mathcal{L}_{m}[M(\varnothing)]}{\mathcal{L}_{m}[M(\mathcal{G})] - \mathcal{L}_{m}[M(\mathcal{\varnothing})]} $$
where $\mathcal{G} \backslash \mathcal{C}$ denotes the complementary set of circuit $\mathcal{C}$. The completeness of circuit $\mathcal{C}$ is actually the faithfulness of circuit $\mathcal{G} \backslash \mathcal{C}$.

In practice, we calculate the faithfulness and completeness of the circuits for subject-verb disagreement at $N=100, 500, 1000, 5000, 10000$ edges. Results are shown in \cref{fig:faith}. It can be seen that $N=1000$ also serves as a turning point for the curves of faithfulness and completeness. The faithfulness of the circuits remains relatively high after $N=1000$, ensuring the high quality of circuits. 

\begin{figure}[ht]
\centering
\subfigure[Faithfulness]{
    \includegraphics[scale=0.5]{figures/faithfulness.pdf}
    \label{fig:faithfulness}
    }
\hspace{0.01pt}
\subfigure[Completeness]{
    \includegraphics[scale=0.5]{figures/completeness.pdf}
    \label{fig:completeness}
    }
\caption{The faithfulness and completeness of the circuits for subject-verb disagreement.}
\label{fig:faith}
\end{figure}


\subsubsection{Ablation Study of Random activation}
Random activation means during training, we randomly unfreeze some parameters outside the circuit. Since we assume that the circuit with $N=1000$ edges already includes all needed parameters for the subject-verb disagreement task, we randomly select a part of the parameters outside the $N=1000$ circuit and involve them in optimization. In practice, we randomly activate 10\%, 20\%, 30\% and 40\% of the outside parameters, and compare the results with before. Results are shown in \cref{fig:randn}. When 10\% of the parameters outside the circuit are activated, the result is almost the same as before. When the ratio gets larger, we observe that the PPL is higher than before when random activation is performed, though the logit difference increases. This is because when extra parameters are summoned to fit the new data distribution, the original functions corresponding to those parameters may be destroyed. Thus the performance is improved at the expense of harming other abilities. Therefore, the circuit we find at $N=1000$ edges is almost the exact circuit for subject-verb disagreement. 

\begin{figure}[ht]
\centering
\subfigure[logit difference]{
    \includegraphics[scale=0.55]{figures/random_logit_diff.pdf}
    \label{fig:randn_logit_difference}
    }
\hspace{0.01pt}
\subfigure[PPL]{
    \includegraphics[scale=0.55]{figures/random_ppl.pdf}
    \label{fig:randn_PPL}
    }
\caption{Experiment results of random activation.}
\label{fig:randn}
\end{figure}

\subsection{Analyses of the Training Dynamics}
\label{appn:sv_dynamics}

\subsubsection{Interpret the Circuit for the Subject-verb Disagreement Task}
\label{appn:sv_interp}
To interpret the circuit for the subject-verb disagreement task, we first analyze this task from the human perspective. To decide the form of a verb, we need to (i) find out the subject in the context and (ii) adjust the form of the verb according to the attributes of the subject, including the person attribute and the number attribute. Therefore, we assume that there exist at least two kinds of attention heads responsible for the two functions above respectively. We name the two kinds of attention heads as the \textbf{Subject Identification Heads} and the \textbf{Subject Attribute Heads}. Note that we only focus on self-attention instead of MLP because only attention layers move information across tokens, which is important for completing this task. Besides, each of the whole MLP layers is regarded as a node in the circuit in our experiment, thus we do not expect to figure out any specific function from it.

Next, we look for the two kinds of heads discussed above.

\textbf{Subject Identification Heads} To find out the Subject Identification Heads, we check the attention pattern of each attention head at the END token since the END token is directly used for predicting the verb token. We sort the heads in descending order according to their attention weights from the END token (query) to the subject tokens (key). Then we keep the heads in which the attention is mainly paid to the subject in the context. 

We find that there exist two types of Subject Identification Heads. The type I heads mainly attend to the last token itself at the END token, so the attention pattern is a diagonal line. This type of head is helpful when the subject is exactly the END token, e.g. ``He is ...", ``The girls are ...", etc. The type II heads attend to the subject which is several tokens before the END token, e.g. ``The kid who is holding an ice cream in hand is ...", ``The famous scientist, who is also an artist, has ...", etc. The type II heads obviously have the ability of syntactic analysis and subject identification, while the type I heads may just happen to attend to the subject that overlaps with the last token.

The type I Subject Identification Heads in GPT2-small includes head.0.1, head.0.3, head.0.5, etc., while the type II heads include head.8.5, head.10.5, head.10.9, head.11.8, etc. The attention patterns for both kinds of heads are shown in \cref{fig:head.0.3.pattern} and \cref{fig:head.11.8.pattern} respectively.

It is worth noticing that the Subject Identification Heads remain unchanged over the training process, which means their function is preserved and shared between subject-verb agreement and subject-verb disagreement.

\begin{figure*}[ht]
% \vskip -0.1in
\centering
\subfigure[The attention pattern in the type I head (head.0.3)]{
    \includegraphics[scale=0.41]{figures/head-0-3-pattern-1.pdf}
    \label{fig:head.0.3.pattern}
    }
\subfigure[The attention pattern in the type II head (head.11.8)]{
    \includegraphics[scale=0.34]{figures/head-11-8-pattern-1.pdf}
    \label{fig:head.11.8.pattern}
    }
\caption{The examples of the attention patterns in the Subject Attribute Heads. In \cref{fig:head.0.3.pattern}, the END token ``\textit{video}" attends to the subject ``\textit{video}" which is also the END token itself. In \cref{fig:head.11.8.pattern}, the END token ``\textit{opponents}" mainly attends to the subject ``\textit{interaction}".}
\vskip -0.1in
\label{fig:attentionPattern}
\end{figure*}

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[scale=0.45]{figures/head-0-3-pattern.pdf}}
% \caption{An example of the attention pattern in the type I Subject Identification Heads (head.0.3). The END token ``\textit{video}" attends to the subject ``\textit{video}" which is also the END token itself.}
% \label{fig:head.0.3.pattern}
% \end{center}
% \vspace{-8mm}
% \end{figure}

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[scale=0.4]{figures/head-11-8-pattern.pdf}}
% \caption{An example of the attention pattern in the type II Subject Identification Heads (head.11.8). The END token ``\textit{opponents}" mainly attends to the subject ``\textit{interaction}".}
% \label{fig:head.11.8.pattern}
% \end{center}
% \vspace{-8mm}
% \end{figure}

\textbf{Subject Attribute Heads} To find out the Subject Attribute Heads, we need to find out which heads directly affect the match between the verb and the subject, which is measured by the logit difference between the flipped verb and the original verb at the END token. Suppose the output of the final layer at the END token is $x_{END} \in \mathbb{R}^{D}$, then the logit difference is $[W_{U}(v_{flip})-W_{U}(v))]\cdot x_{END}$, in which $W_{U}(v_{flip})-W_{U}(v)$ is called the logit lens \cite{logitlens}. Since the logit lens is fixed, we expect the projection of $x_{END}$ on the direction of the logit lens to be large, thus encouraging the probability difference between the two opposite verb forms (love v.s. loves, etc.). As discussed in \cref{appn:notation}, the output $x_{END}$ which is in the residual stream can be decomposed into the linear addition of the outputs from the previous layers. Therefore, the output of a Subject Attribute Head is a part of $x_{END}$ and would encourage the value of the logit difference. Thus, a Subject Attribute Head is an attention head that has a large dot product value with the logit lens.

In practice, we calculate the dot product between the logit lens $W_{U}(v_{flip})-W_{U}(v)$ and the output $Attn_{i}^{j}(x)$ from each attention head in each layer over a batch of samples. The result is shown in the main text in \cref{fig:flip}. The darker the color, the larger the absolute value of the dot product is, which implies that the head is more likely to be a Subject Attribute Head. We observe that head.6.0, head.6.5, head.8.5, head.10.9, and so on see obvious flips (\cref{fig:flipdemo}) from positive to negative or the opposite direction, which implies that they are directly responsible for the match between the subject and the verb. During training, the parameters inside these heads adjust themselves to the new data distribution, while their function type remains unchanged, which is an interesting finding of the self-regulation ability inside the model.

\begin{figure}[hbt]
\begin{center}
\centerline{\includegraphics[scale=0.6]{figures/flipdemo.pdf}}
\caption{A sketch of the flip from subject-verb agreement to disagreement.}
\label{fig:flipdemo}
\end{center}
\vskip -0.2in
\end{figure}

\textbf{Collaborative Heads} Finally, we notice that some of the Subject Attribute Heads (head.8.5, head.10.9, etc.) are also Subject Identification Heads, which means they also attend to subject tokens. We wonder if there exist some heads in previous layers that could influence the behavior of the Subject Attribute Heads. That is to say, the Subject Attribute Heads do not act alone but collaborate with other heads. 

To find out these heads, we knock out the upstream heads one at a time at the END token using mean ablation. We observe the change in the attention pattern of each Subject Attribute Head and then keep the heads that bring obvious changes in the attention patterns. In practice, we focus on subject-verb agreement and only check the influence on head.8.5 and head.10.9. We also provide two types of data, corresponding to the two cases discussed in \cref{appn:sv_interp}, in order to provide a more detailed analysis.
Results show that 
\begin{itemize}
    \item When patching on the type I data in which the END token is exactly the subject, head.1.2 and head.2.11 affect both head.8.5 and head.10.9
    \item When patching on the type II data in which the subject is several tokens before the END token, head.7.4 and head.2.11 affect head.8.5, while head.2.11 affects head.10.9.
    \item When we mix the two types of data, we find that head.1.3, 0.8, 2.10, and 6.5 affect head.8.5, while head.1.3, 1.4, 1.6, 6.5, 0.8, and 0.9 affect head.10.9. These heads may be responsible for both types of data while not specifically responsible for a certain type of data, so they appear when patching on the mixed data.
\end{itemize}
We notice that though the influence of the above heads is relatively large, the absolute influence is sometimes quite small. Therefore, we further conduct an experiment in which we patch multiple heads at a time and check the influence of them on head.8.5 and head.10.9. Results show that when upstream heads are patched together, their combined effect is much higher than the individual effect. Thus we call these heads the Collaborative Heads. 

\subsubsection{The Interaction and Collaboration Inside the Model}
\label{appn:sv_collaboration}
From the discussions above, we can see that the nodes inside a circuit complete a task through a cooperative division of labor. We summarize the interaction and collaboration inside the model as follows:
\begin{enumerate}
    \item Each node is responsible for a single or multiple functions. As discussed in \cref{appn:sv_interp}, we have found attention heads that are responsible for identifying the subject in a sentence or adjust the form of a verb according to the attributes of the subject, or both. Each head has its division of labor when completing a task.
    \item Some heads directly affect the output, while others cooperate with them to affect the output indirectly. For example, head.8.5 directly matches the verb with the subject, while head.7.4, head.1.3 and so on indirectly affect the output through cooperation with head.8.5.
    \item Several nodes achieve a common goal through cooperation. For example, head.1.3, 1.4, 1.6, 6.5, 0.8, and 0.9 affect head.10.9 through combined effect, which means their influence on head.10.9 only appears when they act together. 
\end{enumerate}




\subsubsection{The Evidence of Hebbian Learning}
\label{appn:sv_hebbian}
As discussed in \cref{sec:dynamics}, we observe that some edges in the circuit are strengthened or weakened during training, just like the Hebbian learning proposed in \cite{hebb} in neuroscience. As stated by Hebb, a synapse between two neurons is strengthened when the neurons on either side of the synapse have highly correlated outputs, which means they are often activated synchronously. The theory is often concluded as ``Cells that fire together, wire together" \cite{hebb1}. For two neurons $i$ and $j$, a common description of hebbian learning is as follows:
$$ w_{ij} = \frac{1}{p} \sum_{k=1}^{p} x_{i}^{k} x_{j}^{k} $$
where $w_{ij}$ is the weight of the connection between the two neurons, and $x_{i}^{k}$ and $x_{j}^{k}$ are the $k$-th inputs for $i$ and $j$ respectively. When it comes to the computational graph of a model, the nodes and edges in the graph could be viewed as the neurons and their connections in a brain from the perspective of neuroscience. 

During training, we find that some of the edges are obviously stronger than others, which means they have higher edge contributions. Besides, they are strengthened all the way during training. Specifically, we analyze the circuits during training in the subject-verb disagreement task, with the setting of top $N=1000$ edges. We check the results at 2000, 3000, and 4000 steps respectively, and visualize the circuits with the top 35 edges in \cref{fig:hebbian}. Note that the thickness of an edge corresponds to the logarithm of the edge contribution, that is $log[1+ c(e)]$. The details are shown in \cref{tab:hebbian}. 

We also find that the edge contribution may keep decreasing during training. This is quite similar to the self-organization of cells inside human brains, a well-known phenomenon of which is the lateral inhibition among neurons \cite{FoundationsOfNeuroscience}, which means an activated neuron can reduce the activity of its neighbors. Inspired by this, \cite{som} developed the self-organizing maps (SOM), an unsupervised algorithm that leverages the Winner-Take-All strategy to perform competitive learning. The core ideas behind SOM are:
\begin{itemize}
    \item The neurons inside a neural network learn to represent data through competition, i.e. given an input, some neurons are activated while others are inhibited.
    \item  Different inputs are represented in a topologically ordered manner, i.e. different neurons are responsible for different features in a well-organized style.
\end{itemize}
In our study, we find that the dynamic change of edges echoes the above discussions on competition and self-organization. During training, some connections between nodes are strengthened, which may reduce the intensity of other connections. After training, the components inside a model have reorganized themselves to adapt to the new data distribution. When faced with an input, different regions inside the computational graph are responsible for different subtasks and collaborate to complete a goal, as discussed in \cref{appn:sv_interp} and \cref{appn:sv_collaboration}.

\begin{table*}[ht]
\renewcommand{\arraystretch}{1.2}
\vskip 0.05in
\caption{Some of the strengthened and weakened edges during training. The dynamic change shows the change in edge contribution $log[1+ c(e)]$ (the start of training $\rightarrow$ 2000 steps $\rightarrow$ 3000 steps $\rightarrow$ 4000 steps). The dynamic process is visualized in \cref{fig:hebbian}.}
\label{tab:hebbian}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{m{90pt} m{135pt} | m{70pt} m{145pt}}
    \hline
    \multicolumn{2}{c|}{Strengthened Edges} & \multicolumn{2}{c}{Weakened Edges} \\
    \hline
    \multicolumn{1}{c}{edge} & \multicolumn{1}{c|}{dynamic change} & \multicolumn{1}{c}{edge} & \multicolumn{1}{c}{dynamic change} \\
    \hline
    mlp.2 $\rightarrow$ head.11.8.v & 0 $\rightarrow$ 0.2744 $\rightarrow$ 0.5091 $\rightarrow$ 0.9138 & mlp.2 $\rightarrow$ mlp.8 & 0.1122 $\rightarrow$ 0.0869 $\rightarrow$ 0.0619 $\rightarrow$ 0.058 \\
    mlp.1 $\rightarrow$ head.11.8.v & 0 $\rightarrow$ 0.2227 $\rightarrow$ 0.3978 $\rightarrow$ 0.7231 & mlp.1 $\rightarrow$ mlp.5 & 0.1043 $\rightarrow$ 0.0859 $\rightarrow$ 0.0736 $\rightarrow$ 0 \\
    mlp.2 $\rightarrow$ mlp.3 & 0 $\rightarrow$ 0.0293 $\rightarrow$ 0.0993 $\rightarrow$ 0.2282 & mlp.0 $\rightarrow$ mlp.10 & 0.2712 $\rightarrow$ 0.058 $\rightarrow$ 0 $\rightarrow$ 0 \\
    mlp.1 $\rightarrow$ mlp.4 & 0 $\rightarrow$ 0.0396 $\rightarrow$ 0.0652 $\rightarrow$ 0.1748 & mlp.4 $\rightarrow$ mlp.11 & 0.1791 $\rightarrow$ 0.0454 $\rightarrow$ 0.0428 $\rightarrow$ 0 \\
    mlp.2 $\rightarrow$ mlp.5 & 0 $\rightarrow$ 0 $\rightarrow$ 0.1246 $\rightarrow$ 0.1734 & mlp.4 $\rightarrow$ mlp.11 & 0.1885 $\rightarrow$ 0.0343 $\rightarrow$ 0.0259 $\rightarrow$ 0 \\
    ... & ... & ... & ... \\
    \hline
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\end{table*}

\begin{figure*}[!ht]
\centering

\subfigure[circuit at 2000 steps]{
    \includegraphics[scale=0.2]{figures/2000-crop.pdf}
    }
\subfigure[circuit at 3000 steps]{
    \includegraphics[scale=0.2]{figures/3000-crop.pdf}
    }
\subfigure[circuit at 4000 steps]{
    \includegraphics[scale=0.2]{figures/4000-crop.pdf}
    }
    
\vskip 0.15in
\caption{Evidence of Hebbian learning in the subject-verb disagreement task}
\label{fig:hebbian}
% \vskip -0.2in
\end{figure*}





\subsubsection{The Circuits Before and After Fine-tuning}
\label{appn:sv_circuits}
We present the circuits of the subject-verb disagreement task before and after fine-tuning in \cref{fig:circuit-before} and \cref{fig:circuit-after} respectively. The circuit of subject-verb disagreement is trained under the setting of $N=1000$ edges, and for both of the circuits we only present the top 100 edges. It can be seen that most of the heads we discussed in \cref{appn:sv_interp} are included in the circuit. Besides, it is obvious that the circuits before and after fine-tuning share a lot of nodes, which implies that the function shift from subject-verb agreement to disagreement happens mostly in the \textbf{polarity change} of nodes, instead of randomly assigning the ability to other nodes. 
This is an intuitive finding, which not only demonstrates the rationality of circuit-tuning as well as our analyses but also provides new insights for our understanding of the mechanism inside language models. 

% \begin{figure}[hbt]
% \begin{center}
% \centerline{\includegraphics[scale=0.2]{figures/circuit-sv.pdf}}
% \caption{The circuit before fine-tuning (subject-verb agreement).}   
% \label{fig:circuit-before}
% \end{center}
% \end{figure}

% \begin{figure}[hbt]
% \begin{center}
% \centerline{\includegraphics[scale=0.2]{figures/circuit-svd.pdf}}
% \caption{The circuit after fine-tuning (subject-verb agreement).}   
% \label{fig:circuit-after}
% \end{center}
% \end{figure}


\begin{figure*}[!ht]
\centering
\subfigure[The circuit before fine-tuning (subject-verb agreement).]{
    \includegraphics[scale=0.335]{figures/circuit-sv.pdf}
    \label{fig:circuit-before}
    }
\\
\vspace{0.2in}
\subfigure[The circuit after fine-tuning (subject-verb disagreement).]{
    \includegraphics[scale=0.355]{figures/circuit-svd.pdf}
    \label{fig:circuit-after}
    }
\vspace{0.1in}
\caption{The circuits of subject-verb agreement (top) and subject-verb disagreement (bottom).}
\label{fig:circuit-sv}
% \vskip 0.2in
\end{figure*}

% \newpage

\subsubsection{The Experiment Results after the Revision of Attribution Score}
\label{appn:revision}
During the analyses in \cref{appn:sv_interp}, we find that the original definition of the attribution score in EAP \cite{atp} fails to capture all the relevant edges in a task. For example, head.6.0 which is a Subject Attribute Head fails to appear in the circuit. As discussed in \cref{sec:sv_improve}, we assume that there exists a situation where an important node is connected with many other nodes, but each edge is not that strong. For example, as illustrated in \cref{fig:revision_sketch}, an upstream node $n_{a}^{u}$ is connected with four downstream nodes, while another upstream node $n_{b}^{u}$ is connected with only one downstream node. Since the edge between $n_{b}^{u}$ and $n_{5}^{d}$ is stronger than any edge between $n_{a}^{u}$ and the nodes connected with it, the edge $n_{b}^{u} \rightarrow n_{5}^{d}$ may be kept in the circuit, while the edges in $\mathcal{E}_{a} = \{ n_{a}^{u} \rightarrow n_{i}^{d} | i=1, 2, 3, 4\}$ may be left out. As a result, $n_{a}^{u}$ is not involved in optimization, though the sum of the edge contributions of all edges in $\mathcal{E}_{a}$ may be almost the same or even larger than that of $n_{b}^{u} \rightarrow n_{5}^{d}$.

Thus we calculate the edge contribution of an edge $e: n_{i}^{u} \rightarrow n_{j}^{d}$ as below:
$$ c(e)^{\prime} = c(e) \cdot \sum_{k=1}^{N_{down}^{i}}c(n_{i}^{u} \rightarrow n_{k}^{d}) \cdot \sum_{k=1}^{N_{up}^{j}} c(n_{k}^{u} \rightarrow n_{j}^{d}) $$
where $c(e)$ is the original attribution score in EAP, $N_{down}^{i}$ is the number of the downstream nodes of $n_{i}^{u}$, $N_{up}^{j}$ is the number of the upstream nodes of $n_{i}^{u}$.
The revision considers the contributions from all the edges connected to the upstream node and the downstream node. To verify it, we conduct a new experiment on the subject-verb disagreement task and compare it with the result before. Results are shown in \cref{fig:revision}. Details can be found in \cref{tab:atp_improve}. Compared with the original attribution score, our method improves the logit difference steadily, while even bringing down the computation to some extent. 

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[scale=.5]{figures/revision.pdf}}
\caption{A sketch for the idea behind the revision on attribution score. }
\label{fig:revision_sketch}
\end{center}
\end{figure}

\begin{table*}[hbt]
\renewcommand{\arraystretch}{1.2}
\vskip -0.1in
\caption{Comparison between the performance before and after the improvement on attribution score.}
\label{tab:atp_improve}
\vskip 0.2in
\begin{center}
\begin{small}
\begin{sc}
    \begin{tabular}{c | m{54pt}<{\centering} m{25pt}<{\centering} m{60pt}<{\centering} | m{54pt}<{\centering} m{25pt}<{\centering} m{60pt}<{\centering}}
    \hline
    \multirow{3}{*}{Top n \& Methods} & \multicolumn{3}{c|}{Original EAP} & \multicolumn{3}{c}{Improved EAP} \\
    \cline{2-7}
     & logit difference & PPL & Avg. Param ratio (\%) & logit difference & PPL & Avg. Param ratio (\%) \\
    \hline
    50 & 0.292 & 72.59 & 32.61 & 0.350 & 72.71 & 26.88 \\
    100 & 0.291 & 72.50 & 36.96 & 0.382 & 72.53 & 34.12 \\
    500 & 0.670 & 72.02 & 43.50 & 0.819 & 72.05 & 42.50 \\
    1000 & 0.937 & 71.74 & 45.61 & 0.970 & 71.73 & 45.55 \\
    \hline
    \end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip 0in
\end{table*}

\clearpage

\section{Details for the Complex Tasks}
\label{appn:complex}

\subsection{Details for Task Settings}
\label{appn:complex_task}

\subsubsection{Reasoning-based Tasks}
\label{appn:complex_task_r_based}
\textbf{Mathematics} We use GSM8K \cite{gsm8k} as the dataset, which contains about 8.5k grade school math problems with natural language solutions. The answer to a problem not only contains the final answer but also provides the process for solving the problem. An example of this task is shown in \cref{tab:gsm8k}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.1}
\vskip -0.15in
\caption{An example in the GSM8K dataset.}
\vskip 0.15in
\label{tab:gsm8k}
\begin{center}
\begin{small}
\begin{tabular}{m{230pt}  m{230pt}}
\toprule
Question & Answer  \\
\midrule
Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? & Janet sells $16 - 3 - 4 = <<16-3-4=9>>9$ duck eggs a day. \newline She makes $9 * 2 = \$<<9*2=18>>18$ every day at the farmer's market.\newline \#\#\#\#18 \\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip 0in
\end{table}

During training, the NLL loss also serves as the metric $\mathcal{L}_{m}$ for measuring the output of the model. For evaluation, we use Acc@1 as the metric, which means for each problem in the test set we only sample one answer from the model. 

\textbf{Logical Reasoning} We use Contexthub \cite{contexthub} as the dataset, which consists of problems of 4 difficulty levels, including deductive and abductive reasoning in 12 distinct categories or domains from Wikipedia. The problems together with their reasoning processes are instantiated automatically by LLMs following the fixed formal logic templates. The whole dataset contains 18,240 samples. We only use level 1 and level 2 in our experiment for convenience, which include 6720 samples in total. An example of this task is shown in \cref{tab:contexthub}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
\caption{An example in the Contexthub dataset.}
\vskip 0.15in
\label{tab:contexthub}
\begin{center}
\begin{small}
\begin{tabular}{m{35pt}  m{90pt}  m{325pt}}
\toprule
Item & Template & Instantiation \\
\midrule
\multirow{3}{*}{Premise} & \textless aaa\textgreater & The Sahara desert receives heavy rainfall this year. \\
 & \textless aab\textgreater & The Amazon rainforest experiences severe drought conditions. \\
 & \textless aac\textgreater & Some of Earth's major ecosystems are undergoing significant changes in weather patterns. \\
Question & (aaa OR aab) $\rightarrow$ aac. Given aac is False, what is the value of aab? & If either the Sahara desert receives heavy rainfall this year or the Amazon rainforest experiences severe drought conditions, then it implies that some of Earth's major ecosystems are undergoing significant changes in weather patterns. Given that it is false that some of Earth's major ecosystems are undergoing significant changes in weather patterns, what can be determined about the Amazon rainforest experiencing severe drought conditions this year? (True, False, or N/A (undetermined). \\
Reasoning & (aaa OR aab) $\rightarrow$ aac = False. Given aac is False, the value of premise (aaa OR aab) is False, thus, the value of aab is abduced as False. Thus, the answer is False & ``The Sahara desert receives heavy rainfall this year" or ``The Amazon rainforest experiences severe drought conditions") logically implies ``Some of Earth's major ecosystems are undergoing significant changes in weather patterns" whose corresponding truth value is False. Given ``Some of Earth's major ecosystems are undergoing significant changes in weather patterns" is False, the value of premise ("The Sahara desert receives heavy rainfall this year" or ``The Amazon rainforest experiences severe drought conditions") is False, thus, the value of ``The Amazon rainforest experiences severe drought conditions\" is abduced as False. Thus, the answer is \textless answer\textgreater False\textless/answer\textgreater \\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0in
\end{table}

During training, the NLL loss also serves as the metric $\mathcal{L}_{m}$ for measuring the output of the model. For evaluation, we use the average F1 score over all categories of data.

For all reasoning-based tasks, we add an instruction ``Please answer step by step." at the end of the question in order to guide the model to answer the question step by step. 

\subsubsection{Reasoning-free Tasks}
\label{appn:complex_task_r_free}
\textbf{Gender De-biasing} According to \cite{biasSurvey}, there are various kinds of expressions in social bias. In this study, we focus on the gender bias in occupations. We aims to break down the binary gender stereotype of a model. For example, given a sentence ``the doctor put on [PRP] coat" where [PRP] is a possessive pronoun, we expect the model to choose \textit{his} or \textit{her} with equal probabilities. 

During fine-tuning, the model learns to predict the next word in an auto-regressive way, thus we expect the model to balance the probabilities between male attribute words (he/his/him/himself) and female attribute words (she/her/herself) at the END token when generating the next token. Therefore, we use the logit difference between the male attribute words and female attribute words at the END token as the metric $\mathcal{L}_{m}$ for measuring the output of the model. Specifically, for each sample, we calculate the logit difference between the pronoun and the anti-pronoun, which is the pronoun in the opposite gender. For example, in the case ``the doctor put on [PRP] coat", the logit difference is $logit(W_{her}|W_{END}) - logit(W_{his}|W_{END})$, where $W_{END}$ is the END token \textit{on}.

We use BUG \cite{bug} for training, which is a large-scale dataset of sentences sampled from real-world corpora. Each sentence is marked with an occupation and the pronouns referring to it. In practice, we use the ``balanced BUG" provided in the dataset which includes 2.5w sentences randomly sampled from BUG to ensure balance between male and female entities and between stereotypical and non-stereotypical gender role assignments. We perform coreference resolution ourselves to filter out the samples in which the coreference is not right, and leave 1.5w samples for training, which contains 151 types of occupations and each sentence only contains one (occupation, pronoun) pair.

Though the training set we use is balanced between genders and stereotypes, the number of samples for each occupation is not balanced. To further improve performance, we additionally add a regularization term to the original NLL loss. Then the total loss is
$$ \mathcal{L} = \mathcal{L}_{NLL} + \beta \cdot | logit(W_{pron}|W_{END}) - logit(W_{anti-pron}|W_{END}) | $$
in which $\beta$ is a hyper-parameter for controlling the weight of regularization. The absolute value of the logit difference aims to minimize the difference between genders in the model's stereotype.   

For evaluation, we use WinoBias \cite{winobias} which is a classic dataset for coreference resolution focused on gender bias. There are two types of sentences in WinoBias. The Type 1 sentences require world knowledge related to the context to perform coreference resolution, e.g. ``The farmer knows [the editor] because [he] is really famous". The Type 2 sentences can be resolved using syntactic information, e.g. ``The CEO called [the hairdresser] and paid [her] over the phone". In practice, we only use the Type 2 sentences to avoid ambiguity. The test set contains 40 types of occupations in total.

To better evaluate the performance of gender de-biasing, we adopt the concept of prejudice risk from \cite{prejudice} which is used to measure the stereotype in large language models. Specifically, given an occupation $x \in X$, a binary gender attribute $y \in \{male, female\}$ and a context $c \in C$, the stereotype of a model $M$ against $x$ about $y$ in the context $c$ is
$$ s_{y|x}^{M}(c) = \frac{p_{y|x}^{M}(c)}{p_{y|x}^{*}(c)} - 1 $$
where $p_{y|x}^{*}(c)$is the attribute prediction probability of the unbiased model, thus $p_{y|x}^{*}(c)=0.5$ when binary gender is considered. The definition of prejudice risk is 
$$R^{p} = \mathbb{E}_{x \sim X}(r_{x}^{p})$$
where $r_{x}^{p} = J(\mathbb{E}_{c\sim C}(s_{y|x}^{M}(c))$ is the prejudice risk of one occupation $x$, and $J(s_{y|x}^{M}(c)) = \max \limits_{y \in Y}\{max\{s_{y|x}^{M}(c), 0\}\}$ is the discrimination risk criterion. For details, please refer to \cite{prejudice}.

\textbf{Reading Comprehension} We use SQuAD 2.0 \cite{squad} as the dataset. The input contains a paragraph from a passage, and a question related to that paragraph. The answer could be a word or a phrase in the paragraph, or ``\textless No Answer \textgreater" which means the answer cannot be found in the paragraph. An example for this task is shown in \cref{tab:squad}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
\caption{An example in the SQuAD 2.0 dataset.}
\vskip 0.15in
\label{tab:squad}
\begin{center}
\begin{small}
\begin{tabular}{m{40pt}  m{410pt}}
\toprule
Item & Content \\
\midrule
Paragraph & The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ("Norman" comes from "Norseman") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries. \\
Question & In what country is Normandy located? \\
Answer & France \\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

During training, the NLL loss serves as the metric $\mathcal{L}_{m}$ for measuring the output of the model, since the answer is a segment of text which may contain one or multiple tokens. For evaluation, we use the development set for convenience, and the metric is exact match and F1 score.

\subsection{Details for Implementation}
\label{appn:complex_exp}
We use Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct and Llama-3.1-8B-Instruct \cite{llama3} for this task. We set the output of the attention and the MLP in each layer as upstream nodes, and the input of the query, key, value, and the MLP in each layer as downstream nodes. Different from GPT2-small, an MLP layer in Llama is too big to be a node, so we split the input and output of each MLP layer into 64-dimensional MLP heads. Details are shown in \cref{tab:complex_para}.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.5}
\caption{The settings of the nodes and their corresponding parameters in the complex tasks. For model sizes 1B/3B/8B, the number of layers $L=16/28/32$, and the number of attention heads in each layer $H=32/24/32$, and the number of MLP heads in each layer $H^{*}=128/128/224$. The notations for parameters are specified in \cref{appn:notation}.}

% \vskip 0.15in
\label{tab:complex_para}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{l|cc|cc}
\begin{tabular}{l | m{80pt}<{\centering} m{80pt}<{\centering}| m{60pt}<{\centering} m{60pt}<{\centering} m{60pt}<{\centering}}

\hline
\multirow{2}{*}{Nodes} & \multicolumn{2}{c|}{Upstream} & \multicolumn{3}{c}{Downstream} \\
\cline{2-6}
 & $Attn_{i}^{j}(x)$ & $MLP_{i}^{k}(x)$ & $x_{Q/K/V}^{i,j}$ & $x_{pre}^{i,k}$ & $x_{in}^{i,k}$ \\
\cline{1-1}

Parameters & $W_{O}^{i,j}$ & $W_{out}^{i,k}$ & $W_{Q/K/V}^{i,j}$ & $W_{gate}^{i,k}$ & $W_{in}^{i,k}$ \\
\hline
Range & \multicolumn{5}{c}{$i\in [0,L), j \in [0,H), k \in [0,H^{*})$} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip 0.1in
\end{table}

As for the calculation of edge contribution, we use mean ablation as before when patching a node. For reasoning-based tasks and the reading comprehension task in reasoning-free tasks, there is no such thing as the END token. Therefore, for each activation of shape $\mathtt{(batch\_size, seq\_len, n\_head, d\_model)}$, we take all tokens into consideration and use the mean value over all tokens and all samples for mean ablation. For implementation details, please refer to our source code. 

For circuit-tuning, we set the number of edges $N$ to 2000, 3000, and 4000 for 1B/3B/8B models respectively. Since the tasks are much more complex, the value of $N$ is decided by experience. The batch size is set to 16 in all experiments. We set the learning rate to 3e-5 for the mathematics and reading comprehension tasks, and 1e-4 for other tasks. Mini-batch SGD with a momentum equal to 0.9 is used as the optimizer. During training, we perform circuit discovery every 8 steps after optimization for efficiency, which is different from the experiments on GPT2-small in which we perform circuit discovery rightly after an iteration step. For each task, we train the model until performance cannot be further improved. 

For LoRA, we set $r=32, \alpha = 64$ for all experiments. For full fine-tuning and LoRA, we use the same optimizer as that in circuit-tuning. For all other hyper-parameters such as learning rate, batch size, training steps, and so on, we just sweep over a range of choices and choose the best ones.

In practice, we find that circuit-tuning is much more stable than full fine-tuning and LoRA. When we sweep over a range of hyper-parameters, we notice that full fine-tuning and LoRA are quite sensitive to the change of learning rate, batch size, training steps, and so on. When it comes to circuit-tuning, the change in evaluation result is relatively moderate while still maintaining good performance. Thus it echoes our discussion of the stability of circuit-tuning in \cref{sec:convergence}.


\subsection{Details for Evaluations on General Capabilities}
\label{appn:complex_eval}
To demonstrate that our method is good at preserving general capabilities, we test the fine-tuned models on a set of benchmarks involving general capabilities as well as other capabilities.

For general capabilities, we use MMLU \cite{mmlu}, Winogrande \cite{winogrande} and IFEval \cite{ifeval}. For MMLU, the evaluation metric is the average accuracy over all categories. For Winogrande, we use the development set for convenience, and the evaluation metric is accuracy. For IFEval which is to test the instruction following ability of a model, each prompt contains one or multiple verifiable instructions, thus the evaluation metric is divided into the prompt-level accuracy and instruction-level accuracy. Due to the randomness of generation, each response is tested under multiple transformations, thus the metric is further divided into strict criterion and loose criterion. In practice, we use the prompt-level and instruction-level accuracy averaged on the strict and loose criteria.

For other capabilities, we consider reasoning, coding, and multilingual capabilities. For reasoning, we use GPQA \cite{gpqa} with accuracy as the metric. For coding, we use HumanEval \cite{humaneval} with pass@1 as the metric. For multilingual capability, we use MGSM \cite{mgsm} with the accuracy averaged on all languages. 

For each evaluation benchmark, we test for 5 times and get the averaged result.


\subsection{Influence of the Regularization Term on Gender De-biasing}
\label{appn:bias}
We visualize the prejudice risk of the models before and after gender de-biasing in \cref{fig:bias}. Note that the coefficient for regularization $\beta=0.5$. Since there are only 40 types of occupations in the test set, we regard the distribution of prejudice risk as normal distribution and perform interpolation on the computed results. For convenience, we only show the results of GPT2-small and Llama-3.2-1B-instruct in the figure.

The dynamic process of de-biasing can be observed from right to left. It is obvious that with a regularization term in the loss function, the distribution of the prejudice risk in occupations is more concentrated to a smaller value. The results demonstrate the effectiveness of our method in modifying a model's stereotype, providing new insights in aligning AI systems with human values.

Besides, the experiment result tells us that we can customize the algorithm settings (e.g., the loss function) according to the requirement of a task. Thus, it is convenient to intervene in the training process flexibly to modify the model behaviors, demonstrating the great potential of circuit-tuning in the study of fine-tuning and interpretability.

\begin{figure}[ht]
\begin{center}
\vskip 0.1in
\centerline{\includegraphics[scale=0.6]{figures/gender_bias_prejudice_risk_dist_compare2.pdf}}
\caption{The comparison between biased and de-biased models.}
\label{fig:bias}
\end{center}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
