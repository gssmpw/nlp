\section{Related Work}
\label{sec.relate_works}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/FrameWorkV6.pdf}
    \caption{The pipeline of our proposed EdO-LCEC. We use the generalizable scene discriminator to calculate the feature density of each calibration scene by image segmentation and depth estimation. Based on this feature density, the scene discriminator generates multiple depth and intensity virtual cameras to create LIP and LDP images. Image segmentation results of virtual images and camera images are sent to the dual-path correspondence matching module to obtain dense 3D-2D correspondences, which serve as input for the spatial-temporal relative pose optimization to derive and refine the extrinsic matrix between LiDAR and camera from multiple views and scenes.
    }
    \label{fig.framework_overview}
    \vspace{-1em}
\end{figure*}

Target-based methods achieve high accuracy using customized calibration targets (typically checkerboards). However, they require offline execution and are significantly limited in dynamic or unstructured environments where such targets are unavailable \cite{ou2023targetless,liao2023se}. Recent studies have shifted to online, target-free methods to overcome these limitations. Pioneering works \cite{lv2015automatic, castorena2016autocalibration, yuan2021pixel, pandey2015automatic, tang2023robust, zhang2022multi} estimate the relative pose between the two sensors by aligning the cross-modal edges or mutual information (MI) extracted from LiDAR projections and camera RGB images. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edges and rich texture, which largely compromise calibration robustness. 
To circumvent the challenges associated with cross-modal feature matching, several studies \cite{zhang2023overlap, yin2023automatic, ou2023cross, ou2023targetless} have explored motion-based methods. These approaches match sensor motion trajectories from visual and LiDAR odometry to derive extrinsic parameters through optimization. While they effectively accommodate heterogeneous sensors without requiring overlap, they demand precise synchronized LiDAR point clouds and camera images to accurately estimate per-sensor motion, which limits their applicability.

Advances in deep learning techniques have driven significant exploration into enhancing traditional target-free algorithms. Some studies \cite{li2018automatic, ma2021crlf, wang2022automatic, han2021auto, liao2023se, zhu2020online, koide2023general, zhiwei2024lcec} explore attaching deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, \cite{ma2021crlf} accomplishes LiDAR and camera registration by aligning road lanes and poles detected by semantic segmentation. Similarly, \cite{han2021auto} employs stop signs as calibration primitives and refines results over time using a Kalman filter. A recent study \cite{koide2023general} introduced Direct Visual LiDAR Calibration (DVL), a novel point-based method that utilizes SuperGlue \cite{sarlin2020superglue} to establish direct 3D-2D correspondences between LiDAR and camera data. 
On the other hand,  several learning-based algorithms \cite{borer2024chaos, lv2021lccnet,shi2020calibrcnn,zhao2021calibdnn,iyer2018calibnet,yuan2020rggnet} try to covert calibration process into more direct approaches by exploiting end-to-end deep learning networks.
Although these methods have demonstrated effectiveness on public datasets like KITTI \cite{geiger2012we}, which primarily focuses on urban driving scenarios, their performance has not been extensively validated on other types of real-world datasets that contain more challenging scenes. Furthermore, their dependence on pre-defined sensor configurations (both LiDAR and camera) poses implementation challenges. 
In comparison, our approach actively perceives the environment and achieves human-like adaptability. This environment-driven strategy enables robots to rapidly obtain high-precision extrinsic parameters anytime and anywhere.