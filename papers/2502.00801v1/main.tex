\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage[switch,pagewise]{lineno}
\usepackage{url,subfigure}
\usepackage[hidelinks,hypertexnames=false]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{makecell}
\usepackage{ulem}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{cuted}
\usepackage{capt-of}
\input{packages.tex}
\newcommand{\egi}{\textit{e.g.}}
\newcommand{\settablefont}{\fontsize{6.9}{11.8}\selectfont}
\newcommand{\etal}{\textit{et al.}}

\begin{document}
\normalem
\title{Environment-Driven Online LiDAR-Camera Extrinsic Calibration}
\author{
Zhiwei Huang$^{\orcidicon{0009-0008-7084-052X}\,}$,~\IEEEmembership{Graduate Student Member,~IEEE},
Jiaqi Li$^{\orcidicon{0009-0008-5496-8136}\,}$,~\IEEEmembership{Student Member,~IEEE}, \\
Ping Zhong$^{\orcidicon{0000-0003-3393-8874}\,},$~\IEEEmembership{Senior Member,~IEEE},
and Rui Fan$^{\orcidicon{0000-0003-2593-6596}\,}$,~\IEEEmembership{Senior Member,~IEEE}
\thanks{This research was supported by the National Natural Science Foundation of China under Grants 62473288 and 62233013, the Fundamental Research Funds for the Central Universities, Xiaomi Young Talents Program, and the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University under Grant No. HMHAI-202406. (\emph{Corresponding author: Rui Fan})}
\thanks{Zhiwei Huang and Jiaqi Li are with the Department of Control Science \& Engineering, the College of Electronics \& Information Engineering, Tongji University, Shanghai 201804, China (e-mails: \{2431985, 2251550\}@tongji.edu.cn).}
\thanks{Ping Zhong is with the School of Computer Science and Engineering, Central South University, Changsha 410083, Hunan, China, as well as with the National Key Laboratory of Science and Technology on Automatic Target
Recognition, National University of Defense Technology, Changsha 410073, Hunan, China (e-mail: ping.zhong@csu.edu.cn).}
\thanks{Rui Fan is with the Department of Control Science \& Engineering, the College of Electronics \& Information Engineering, Shanghai Research Institute for Intelligent Autonomous Systems, the State Key Laboratory of Intelligent Autonomous Systems, and Frontiers Science Center for Intelligent Autonomous Systems, Tongji University, Shanghai 201804, China, as well as with the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an 710049, Shaanxi, China (e-mail: rui.fan@ieee.org).
}
}
\maketitle	

\begin{strip}
    \centering
    \vspace{-15em}
    \includegraphics[width=0.99999\textwidth]{figs/CoverV7.pdf}
    \captionof{figure}{
    Visualization of calibration results through LiDAR and camera data fusion in KITTI odometry 00 sequence: (a)-(i) zoomed-in regions that illustrate the alignment between the camera images and the LiDAR point clouds. 
    }
    \label{fig.cover}
\end{strip}

\begin{abstract}
LiDAR-camera extrinsic calibration (LCEC) is the core for data fusion in computer vision. Existing methods typically rely on customized calibration targets or fixed scene types, lacking the flexibility to handle variations in sensor data and environmental contexts. This paper introduces EdO-LCEC, the first environment-driven, online calibration approach that achieves human-like adaptability. Inspired by the human perceptual system, EdO-LCEC incorporates a generalizable scene discriminator to actively interpret environmental conditions, creating multiple virtual cameras that capture detailed spatial and textural information. To overcome cross-modal feature matching challenges between LiDAR and camera, we propose dual-path correspondence matching (DPCM), which leverages both structural and textural consistency to achieve reliable 3D-2D correspondences. Our approach formulates the calibration process as a spatial-temporal joint optimization problem, utilizing global constraints from multiple views and scenes to improve accuracy, particularly in sparse or partially overlapping sensor views. Extensive experiments on real-world datasets demonstrate that EdO-LCEC achieves state-of-the-art performance, providing reliable and precise calibration across diverse, challenging environments.
\end{abstract}

\section{Introduction}
\label{sec.intro}

We have long envisioned robots with human-like intelligence, enabling them to understand, adapt to, and positively impact the world \cite{arnold2019survey,ai2023lidar,pelau2021makes,tian2022kimera}. This dream is becoming increasingly attainable with the advent of LiDAR-camera data fusion systems. LiDARs provide accurate spatial information, while cameras capture rich textural details \cite{zhiwei2024lcec}. The complementary strengths of these two modalities, when fused, provide robots with powerful environmental perception capabilities \cite{liu2022targetless,li2022deepfusion}. LiDAR-camera extrinsic calibration (LCEC), which estimates the rigid transformation between the two sensors, is a core and foundational process for effective data fusion.

Current LiDAR-camera extrinsic calibration methods are primarily categorized as either target-based or target-free. Target-based approaches \cite{cui2020acsc,yan2023joint, beltran2022automatic, koo2020analytic, xie2022a4lidartag, tsai2021optimising} have long been the preferred choice in this field.  They are typically offline, relying on customized calibration targets (typically checkerboards). However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. Online, target-free approaches aim at overcoming this problem by extracting informative visual features directly from the environment. Previous works \cite{yuan2021pixel, pandey2015automatic} estimate the extrinsic parameters by matching the cross-modal edges between LiDAR projections and camera images.  While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edge features. Recent advances in deep learning techniques have spurred extensive explorations, such as \cite{ma2021crlf} and \cite{zhiwei2024lcec}, to leverage semantic information to aid cross-modal feature matching. Although these approaches have shown compelling performance in specific scenarios, such as parking lots, they predominantly rely on curated, pre-defined objects, \egi, vehicles \cite{li2018automatic}, lanes \cite{ma2021crlf}, poles \cite{wang2022automatic}, and stop signs \cite{han2021auto}. 
On the other hand, several end-to-end deep learning networks \cite{borer2024chaos, lv2021lccnet,shi2020calibrcnn,zhao2021calibdnn,iyer2018calibnet,yuan2020rggnet} have been developed to find a more direct solution for LCEC. While these methods have demonstrated effectiveness on large-scale datasets like KITTI \cite{geiger2012we}, they suffer from a high dependency on the training setup and are thus less generalizable.
Overall, existing methods passively rely on predefined techniques to identify certain preset scene features. They lack the flexibility needed to adapt and respond effectively to environmental changes. In contrast, human actions are environment-driven. People intuitively assess their surroundings, swiftly respond to changes, make adaptive decisions, and improve through interaction and feedback. Elevating online calibration to this level of human-like adaptability presents a compelling research direction.

In this work, we take one step forward in the field of online LCEC by designing an environment-driven framework EdO-LCEC. As illustrated in Fig. \ref{fig.cover}, our approach treats sensors' operational environment as a spatial-temporal flow. By selectively combining information from multiple scenes across different times, this approach could achieve high-precision calibration dynamically. A prerequisite for the success of EdO-LCEC is the design of a generalizable scene discriminator. The scene discriminator employs large vision models to conduct depth estimation and image segmentation, achieving high-quality environmental perception on semantic and spatial dimensions. In detail, it calculates the feature density of the calibration scene and uses this information to guide the generation of multiple virtual cameras for projecting LiDAR intensities and depth. This improved LiDAR point cloud projecting strategy increases the available environmental features, and thus overcomes the previous reliance of algorithms \cite{yuan2021pixel,koide2023general} on uniformly distributed geometric or textural features. 
At each scene, we perform dual-path correspondence matching (DPCM). 
Drawing inspiration from the human visual system \cite{goodale1992seperate}, DPCM divides correspondence matching into spatial and textural pathways to fully leverage both geometric and semantic information. Each pathway constructs a cost matrix based on structural and textural consistency, guided by accurate semantic priors, to yield reliable 3D-2D correspondences. 
Finally, the correspondences obtained from multiple views and scenes are used as inputs for our proposed spatial-temporal relative pose optimization, which derives and refines the extrinsic matrix between LiDAR and camera.
Through extensive experiments conducted on three real-world datasets, EdO-LCEC demonstrates superior robustness and accuracy compared to other SoTA online, target-free approaches.

To summarize, our novel contributions are as follows:

\begin{itemize}
    \item {EdO-LCEC, the first environment-driven, online LCEC framework that achieves high calibration accuracy and human-like adaptability.}
    \item {Generalizable scene discriminator, which can automatically perceive the calibration scene and capture potential spatial,  textural and semantic features using SoTA LVMs.}
    \item {DPCM, a novel cross-modal feature matching algorithm consisting of textural and spatial pathways, capable of generating dense and reliable 3D-2D correspondences between LiDAR point cloud and camera image.}
    \item {Spatial-temporal relative pose optimization, enabling high-quality extrinsic estimation through multi-view and multi-scene optimization.}
\end{itemize}

\section{Related Work}
\label{sec.relate_works}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/FrameWorkV6.pdf}
    \caption{The pipeline of our proposed EdO-LCEC. We use the generalizable scene discriminator to calculate the feature density of each calibration scene by image segmentation and depth estimation. Based on this feature density, the scene discriminator generates multiple depth and intensity virtual cameras to create LIP and LDP images. Image segmentation results of virtual images and camera images are sent to the dual-path correspondence matching module to obtain dense 3D-2D correspondences, which serve as input for the spatial-temporal relative pose optimization to derive and refine the extrinsic matrix between LiDAR and camera from multiple views and scenes.
    }
    \label{fig.framework_overview}
    \vspace{-1em}
\end{figure*}

Target-based methods achieve high accuracy using customized calibration targets (typically checkerboards). However, they require offline execution and are significantly limited in dynamic or unstructured environments where such targets are unavailable \cite{ou2023targetless,liao2023se}. Recent studies have shifted to online, target-free methods to overcome these limitations. Pioneering works \cite{lv2015automatic, castorena2016autocalibration, yuan2021pixel, pandey2015automatic, tang2023robust, zhang2022multi} estimate the relative pose between the two sensors by aligning the cross-modal edges or mutual information (MI) extracted from LiDAR projections and camera RGB images. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edges and rich texture, which largely compromise calibration robustness. 
To circumvent the challenges associated with cross-modal feature matching, several studies \cite{zhang2023overlap, yin2023automatic, ou2023cross, ou2023targetless} have explored motion-based methods. These approaches match sensor motion trajectories from visual and LiDAR odometry to derive extrinsic parameters through optimization. While they effectively accommodate heterogeneous sensors without requiring overlap, they demand precise synchronized LiDAR point clouds and camera images to accurately estimate per-sensor motion, which limits their applicability.

Advances in deep learning techniques have driven significant exploration into enhancing traditional target-free algorithms. Some studies \cite{li2018automatic, ma2021crlf, wang2022automatic, han2021auto, liao2023se, zhu2020online, koide2023general, zhiwei2024lcec} explore attaching deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, \cite{ma2021crlf} accomplishes LiDAR and camera registration by aligning road lanes and poles detected by semantic segmentation. Similarly, \cite{han2021auto} employs stop signs as calibration primitives and refines results over time using a Kalman filter. A recent study \cite{koide2023general} introduced Direct Visual LiDAR Calibration (DVL), a novel point-based method that utilizes SuperGlue \cite{sarlin2020superglue} to establish direct 3D-2D correspondences between LiDAR and camera data. 
On the other hand,  several learning-based algorithms \cite{borer2024chaos, lv2021lccnet,shi2020calibrcnn,zhao2021calibdnn,iyer2018calibnet,yuan2020rggnet} try to covert calibration process into more direct approaches by exploiting end-to-end deep learning networks.
Although these methods have demonstrated effectiveness on public datasets like KITTI \cite{geiger2012we}, which primarily focuses on urban driving scenarios, their performance has not been extensively validated on other types of real-world datasets that contain more challenging scenes. Furthermore, their dependence on pre-defined sensor configurations (both LiDAR and camera) poses implementation challenges. 
In comparison, our approach actively perceives the environment and achieves human-like adaptability. This environment-driven strategy enables robots to rapidly obtain high-precision extrinsic parameters anytime and anywhere.

\section{Methodology}
\label{sec.method}

Given LiDAR point clouds and camera images, our goal is to estimate their extrinsic matrix $^{C}_{L}\boldsymbol{T}$, defined as follows:
\begin{equation}
{^{C}_{L}\boldsymbol{T}} = 
\begin{bmatrix}
{^{C}_{L}\boldsymbol{R}} & {^{C}_{L}\boldsymbol{t}} \\
\boldsymbol{0}^\top & 1
\end{bmatrix}
\in{SE(3)},
\label{eq.lidar_to_camera_point}
\end{equation}
where $^{C}_{L}\boldsymbol{R} \in{SO(3)}$ represents the rotation matrix, $^{C}_{L}\boldsymbol{t}$ denotes the translation vector, and $\boldsymbol{0}$ represents a column vector of zeros. We first give an overview of the proposed method, as shown in Fig. \ref{fig.framework_overview}. It mainly contains three stages: 
\begin{itemize}
    \item{We first utilize a scene discriminator to understand the environment through image segmentation and depth estimation, generating virtual cameras that project LiDAR intensity and depth from multiple viewpoints (Sect. \ref{sec.scene_discriminator}).}
     \item{The image segmentation outputs are processed along two pathways (spatial and textural), then input into a dual-path correspondence matching module to establish reliable 3D-2D correspondences (Sect. \ref{sec.dualpath_matching}).}
      \item{The obtained correspondences are used as inputs for our proposed spatial-temporal relative pose optimization, which derives and refines the extrinsic matrix (Sect. \ref{sec.st_optimization}).}
\end{itemize}  

\subsection{Generalizable Scene Discriminator} 
\label{sec.scene_discriminator}
%Inspired by the human perception system, our calibration algorithm incorporates environmental awareness. 
Our environment-driven approach first employs a generalizable scene discriminator to interpret the surroundings by generating virtual cameras to project LiDAR point cloud intensities and depth. This discriminator configures both an intensity and a depth virtual camera from the LiDAR’s perspective. This setup yields a LiDAR intensity projection (LIP) image ${{^V_I}\boldsymbol{I}} \in{\mathbb{R}^{H\times W \times 1}}$ ($H$ and $W$ represent the image height and width) and a LiDAR depth projection (LDP) image ${{^V_D}\boldsymbol{I}}$. To align with the LDP image, the input camera RGB image ${{^C_I}\boldsymbol{I}}$ is processed using Depth Anything V2 \cite{depth_anything_v2} to obtain estimated depth images ${^C_D}\boldsymbol{I}$\footnote{In this article, the symbols in the superscript denote the type of target camera ($V$ denotes virtual camera and $C$ indicates real camera), and the subscript denotes the source of the image ($D$ is depth and $I$ is intensity).}.  To take advantage of semantic information, we utilize MobileSAM \cite{kirillov2023segment} as the image segmentation backbone. The series of $n$ detected masks in an image is defined as $\{\mathcal{M}_1, \dots, \mathcal{M}_n\}$. The corner points along the contours of masks detected are represented by $\{\boldsymbol{c}_1, \dots, \boldsymbol{c}_{m_i}\}$, where $m_i$ is the total corner points number in the $i$-th mask. An instance (bounding box), utilized to precisely fit around each mask, is centrally positioned at $\boldsymbol{o}$ and has a dimension of $h\times w$ pixels. We define the corresponding depth of a corner point as $d$, and $\boldsymbol{D} \in \mathbb{R}^{b\times b}$ as the matrix that stores texture values in its $b$-neighborhood. To fully leverage the structured information within the masks, we construct a convex polygon from the corner points. As depicted in Fig. \ref{fig.framework_overview}, the neighboring vertices of a corner point $\boldsymbol{c_i}$ is defined as $\{\boldsymbol{e}_{i,1}, \dots, \boldsymbol{e}_{i,k}\}$. 

For each virtual or camera image ${\boldsymbol{I}}$, the scene discriminator computes its feature density $\rho({\boldsymbol{I}})$, providing critical cues for feature extraction and correspondence matching. The feature density $\rho({\boldsymbol{I}})$ is defined as follows:
\begin{equation}
\rho({\boldsymbol{I}}) = \underbrace {\bigg( \frac{1}{n}\log{\sum_{i=1}^{n}{\frac{m_i}{\lvert \mathcal{M}_i\rvert}}}\bigg )}_{\rho_t} \underbrace {\bigg(\sum_{i=1}^{n}\log{\frac{\lvert \bigcap_{j=1}^{n} \mathcal{M}_j \rvert}{ \lvert \mathcal{M}_i\rvert} }\bigg)}_{\rho_s},
\end{equation}
where $\rho_t$ denotes the textural density and $\rho_s$ represents the structural density. 
The occlusion challenges caused by the different perspectives of LiDAR and the camera \cite{yuan2021pixel, zhiwei2024lcec}, combined with limited feature availability in sparse point clouds, mean that a single pair of virtual cameras is insufficient for a comprehensive view of the calibration scene.
Let $E$ represent the event of capturing enough effective features, with probability $P(E) = \lambda$. If we have $n_I$ intensity virtual cameras and $n_D$ depth virtual cameras, the probability of capturing enough effective features is $1 - (1 - \lambda)^{n_I + n_D}$. In theory, as $ {n_I + n_D} \to \infty $, the probability $P(E')^{n_I + n_D} \to 0 $, leading to $1 - (1 - \lambda)^{n_I + n_D} \to 1$. Increasing the number of virtual cameras raises the likelihood of capturing more potential correspondences, thus enhancing calibration accuracy. However, applying an infinite number of virtual cameras during calibration is impractical. Considering the trade-off between calibration accuracy and compute resources, we set the number of multiple virtual cameras to satisfy the balance of feature density:
\begin{equation}
\rho({^C_I}\!\boldsymbol{I}) + \rho({^C_D}\boldsymbol{I}) = \sum_{i=0}^{n_I}\rho({^V_I}\!\boldsymbol{I}_{i}) + \sum_{i=0}^{n_D}{\rho}({^V_D}\boldsymbol{I}_i).
\end{equation}
In practical applications, we set multiple virtual cameras inside a sphere originating from the LiDAR perspective center. Since the feature density is similar if the perspective viewpoints of virtual cameras are close to the initial position, we can assume that $\rho({^V_I}\!\boldsymbol{I}_{i}) \approx \rho({^V_I}\!\boldsymbol{I}_{0})$ and $\rho({^V_D}\boldsymbol{I}_{i}) \approx \rho({^V_D}\boldsymbol{I}_{0})$. So $n_I$ and $n_D$ can be obtained as follows:
\begin{equation}
n_I = \frac{\rho({^C_I}\!\boldsymbol{I})}{\rho({^V_I}\!\boldsymbol{I}_{0})}, \text{ } n_D = \frac{\rho({^C_D}\boldsymbol{I})}{\rho({^V_D}\boldsymbol{I}_{0})}.
\end{equation}
Once all virtual cameras are generated, the discriminator performs image segmentation on each LiDAR projection captured from multiple views, detecting the corner points of the masks. These masks with detected corner points serve as inputs for the dual-path correspondence matching.

\subsection{Dual-Path Correspondence Matching}
\label{sec.dualpath_matching}
Given the segmented masks and detected corner points, dual-path correspondence matching leverages them to achieve dense and reliable 3D-2D correspondences. Inspired by human visual system, DPCM consists of two pathways, one for correspondence matching of LIP and RGB images, and the other for LDP and depth images.  For each pathway, DPCM adopted the approach outlined in \cite{zhiwei2024lcec} to obtain mask matching result $\mathcal{A} = \{(\mathcal{M}^V_i, \mathcal{M}^C_i) \mid i = 1, \dots, m\}$ from a cost matrix $\boldsymbol{M}^I$. Each matched mask pair can estimate an affine transformation $[s\boldsymbol{R}^A,\boldsymbol{t}^A]\in{SE(2)}$ to guide the correspondence matching. 
Specifically, we update the corner points $\boldsymbol{c}^V_i$ in the virtual image to a location $\hat{\boldsymbol{c}}^V_i$ that is close to its true projection coordinate using this affine transformation, as follows:
\begin{equation}
 \hat{\boldsymbol{c}}^V_i = s\boldsymbol{R}^A{(\boldsymbol{c}^V_i)} + \boldsymbol{t}^A.
\end{equation}
To determine optimum corner point matches, we construct a cost matrix $\boldsymbol{M}^C$, where the element at $\boldsymbol{x} = [i,j]^\top$, namely:
\begin{equation}
\begin{aligned}
&\boldsymbol{M}^C(\boldsymbol{x}) = \\
&\rho_s\underbrace{\bigg(1-  \exp(\frac{\left\|\hat{\boldsymbol{c}}^V_i - \boldsymbol{c}^C_j\right\|^2}{L(\mathcal{A})}) 
   + {\sum_{k = 1}^{N}{H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k})}}}_{\text{Structural  Consistency}}\bigg) \\
&+\rho_t\underbrace{\bigg(\frac{1}{b^2}\sum_{r = 1}^{b}\sum_{s = 1}^{b}{\left |\boldsymbol{D}^V_{i}(r,s)-\boldsymbol{D}^C_{j}(r,s) \right |}\bigg)}_{\text{Textural Consistency}}
\end{aligned}
\label{eq.adaptive_cost_func}
\end{equation}
denotes the matching cost between the $i$-th corner point of a mask in the LiDAR virtual image and the $j$-th corner point of a mask in the camera image. (\ref{eq.adaptive_cost_func}) consists of structural and textural consistency. The structural consistency measures the structural difference of corner points in the virtual and real image, where $L(\mathcal{A})$ denotes the average perimeter of the matched masks and $H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k})$ represents the similarity of the neighboring vertices between current and target corner point. The textural consistency derives the relative textural similarity of the $b$ neighboring zone. After establishing the cost matrix, a strict criterion is applied to achieve reliable matching. Matches with the lowest costs in both horizontal and vertical directions of $\boldsymbol{M}^C(\boldsymbol{x})$ are determined as the optimum corner point matches. Since every $\boldsymbol{c}^V_i$ can trace back to a LiDAR 3D point $\boldsymbol{p}^L_i = [x^L,y^L,z^L]^\top$, and every $\boldsymbol{c}^C_i$ is related to a pixel $\boldsymbol{p}_i = [u,v]^\top$ (represented in homogeneous coordinates as $\tilde{\boldsymbol{p}}_i$) in the camera image, the final correspondence matching result of DPCM is $\mathcal{C} = \{(\boldsymbol{p}^L_i,\boldsymbol{p}_i) \mid i = 1,\dots, q\}$.

\subsection{Spatial-Temporal Relative Pose Optimization}
\label{sec.st_optimization}
EdO-LCEC treats the sensor's operational environment as a spatial-temporal flow composed of $N$ multiple scenes across different time instances. In situations with incomplete or sparse point clouds, single-view methods such as \cite{yuan2021pixel,zhiwei2024lcec} are constrained by the limited number of high-quality correspondences. Our environment-driven approach addresses this limitation by integrating multi-view and multi-scene optimization. By merging the optimal matching results from multiple time instances and scenes, this spatial-temporal optimization enhances the selection of correspondences, maximizing the number of high-quality matches and ultimately improving overall calibration accuracy.

\noindent \textbf{Multi-View Optimization.}
In multi-view optimization, the extrinsic matrix ${^{C}_{L}\hat{\boldsymbol{T}}_t}$ of the $t$-th scene can be formulated as follows:
\begin{equation}
{^{C}_{L}\hat{\boldsymbol{T}}_t}\! =\! \underset{^C_L{\boldsymbol{T}_{t,k}}}{\arg\min} \!\!\!\!
\sum_{i = 1}^{n_I + n_D}\!\!\!\sum_{(\boldsymbol{p}^L_j,\boldsymbol{p}_j)\in\mathcal{C}_i}\!\!\!\!\!{G\bigg(\underbrace {\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{t,k}}
 \boldsymbol{p}_{j}^L) - {\tilde{\boldsymbol{p}}}_{j}\right\|_2}_{\epsilon_j}\bigg)}, 
\label{eq.multi_view}
\end{equation}
where ${^{C}_{L}\boldsymbol{T}_{t,k}}$ denotes the $k$-th PnP solution obtained using a selected subset $\mathcal{V}_{k}$ of correspondences from $\mathcal{C}_i$, and $\epsilon_j$ represents the reprojection error of $(\boldsymbol{p}^L_i,\boldsymbol{p}_i)$ with respect to ${^{C}_{L}\boldsymbol{T}_{t,k}}$. $G(\epsilon_j)$ represents the gaussian depth-normalized reprojection error under the projection model $\pi$, defined as:
\begin{equation}
G(\epsilon_j) = \frac{\epsilon_j(e-\mathcal{K}(d'_{j},\bar{d'}))}
{H + \epsilon_j},
\label{eq.dpcm_gaussian}
\end{equation}
where ${d'_j}$ is the normalized depth of $\boldsymbol{p}_{j}^L$, $\bar{d'}$ is the average normalized depth, and $\mathcal{K}(d'_{j},\bar{d'})$ is a gaussian kernal. 

\noindent \textbf{Multi-Scene Optimization.} 
In each scenario, we choose a reliable subset $\mathcal{S}_t$, which can be obtained as follows:
\begin{equation}
    \mathcal{S}_t = \bigcap_{k=1}^{s_t}{\mathcal{V}_{t,k}}, \text{ } s_t = \min\{\frac{Q_{\max}q_t}{\sum^{N}_{j = 1}{q_j}}, s_{\max}\},
\end{equation}
where ${\mathcal{V}_{t,k}}$ is the correspondence subset in the $t$-th scene with the $k$-th smallest mean reprojection error under its PnP solution. $q_t$ denotes the number of correspondences in the $t$-th scene. $Q_{\max}$ and $s_{\max}$ represent the maximum number of subsets across all scenarios and the maximum number of reliable subsets in a single scene, respectively. The multi-scene optimization process then solves the final extrinsic matrix ${^{C}_{L}{\boldsymbol{T}}^*}$ by minimizing the joint reprojection error:
\begin{equation}
\mathcal{L}({^{C}_{L}{\boldsymbol{T}}^{*}}) = 
\sum_{t = 1}^{N} {\sum_{(\boldsymbol{p}^L_{j},\boldsymbol{p}_{j})\in\mathcal{S}_t}\!{G\bigg(\left\|\pi(
{^{C}_{L}\boldsymbol{T}^{*}}
 \boldsymbol{p}_{j}^L) - {\tilde{\boldsymbol{p}}}_{j}\right\|_2}\bigg)}
 \label{eq.multi_scene}
\end{equation}
across multiple spaces in the environment at different times. This process effectively combines optimal correspondences from both spatial and textural pathways in each scenario, enabling environment-driven spatial-temporal optimization and achieving human-like adaptability to the environment.

\section{Experiment}
\label{sec.experiment}

\subsection{Experimental Setup and Evaluation Metrics}
\label{sec.exp_setup}

\begin{table*}[t!]
\caption{Quantitative comparisons with SoTA target-free LCEC approaches on the 00 sequence of KITTI odometry. The best results are shown in bold type. \dag: These methods did not
release (English-speaking) code preventing reproducing results for both cameras.}
\centering
\fontsize{6.5}{10}\selectfont
\begin{tabular}{l|c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c}
\toprule
\multirow{3}*{Approach}& \multirow{3}*{Initial Range}&\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\cline{3-18}
&&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\

&& $e_r$ ($^\circ$) & $e_t$ (m) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (m)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}\\
\hline
\hline
CalibRCNN\textsuperscript{\dag} \cite{shi2020calibrcnn} &$\pm10^\circ / \pm 0.25m$ &0.805 &0.093	&0.446	&0.640	&0.199	&0.062	&0.043 &0.054 &- &-	&-	&-	&-	&-	&-	&-\\
CalibDNN\textsuperscript{\dag} \cite{zhao2021calibdnn} &$\pm10^\circ / \pm 0.25m$ &1.021 &0.115	&0.200	&0.990	&0.150	&0.055	&0.032	&0.096 &- &-&-	&-	&-	&-	&-	&-\\
RegNet\textsuperscript{\dag} \cite{schneider2017regnet} &$\pm20^\circ / \pm 1.5m$ &0.500 &0.108	&0.240	&0.250	&0.360	&0.070	&0.070	&0.040 &- &-&-	&-	&-	&-	&-	&-\\
Borer \etal\textsuperscript{\dag} \cite{borer2024chaos}&$\pm1^\circ / \pm 0.25m$  &0.455 &0.095	&0.100 &0.440 &\textbf{0.060}  &\textbf{0.037} &0.030 &0.082 &- &- &-	&-	&-	&-	&-	&-\\
LCCNet \cite{lv2021lccnet} &$\pm10^\circ / \pm 1.0m$ &1.418
&0.600  &0.455 &0.835 &0.768 &0.237 &0.333  &0.329 &1.556  &0.718 & 0.457 &1.023 &0.763 &0.416 &0.333 &0.337 \\
RGGNet \cite{yuan2020rggnet} &$\pm20^\circ / \pm 0.3m$ &1.29 &0.114	&0.640 &0.740&0.350 &0.081 &\textbf{0.028} &0.040 &3.870 &0.235 &1.480 &3.380 &0.510	&0.180 &\textbf{0.056} &0.061 \\
CalibNet \cite{iyer2018calibnet} &$\pm10^\circ / \pm 0.2m$ &5.842 &0.140	&2.873 &2.874 &3.185 &0.065 &0.064 &0.083 &5.771 &0.137	&2.877 &2.823 &3.144 &\textbf{0.063} &0.062 &0.082\\
\hline
CRLF \cite{ma2021crlf} &- &0.629	&4.118	&\textbf{0.033}	&0.464	&0.416	&3.648	&1.483	&0.550 &0.633	&4.606	&\textbf{0.039}	&0.458	&0.424	&4.055	&1.636	&0.644\\
UMich \cite{pandey2015automatic} &- &4.161	&0.321	&0.113	&3.111	&2.138	&0.286	&0.077	&0.086
 &4.285&0.329	&0.108	&3.277	&2.088	&0.290	&0.085	&0.090\\
HKU-Mars \cite{yuan2021pixel} &- &33.84	&6.355	&19.89	&18.71	&19.32	&3.353	&3.232	&2.419
&32.89	&4.913	&18.99	&15.77	&17.00	&2.917	&2.564	&1.646\\
DVL \cite{koide2023general}  &-&{122.1} &{5.129}	&{48.64}	&{87.29}	&{98.15}	&{2.832}	&{2.920}	&{1.881} &{120.5} &{4.357}	&{49.60}	&{87.99}	&{96.72}	&{2.086}	&{2.517}	&{1.816}\\
MIAS-LCEC \cite{zhiwei2024lcec}  &-&5.385	&1.014	&1.574	&4.029	&4.338	&0.724	&0.383	&0.343 &7.655	&1.342	&1.910	&5.666	&6.154	&0.843	&0.730	&0.358\\
\hline
\textbf{EdO-LCEC (Ours)}  &- &\textbf{0.295}	&\textbf{0.082}	&0.117	&\textbf{0.176}	&{0.150}	&0.051	&0.044	&\textbf{0.032} 
&\textbf{0.336}	&\textbf{0.118}	&0.216	&\textbf{0.168}	&\textbf{0.121}	&0.083	&0.067	&\textbf{0.032}
\\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti00}
\end{table*}

\begin{table*}[t!]
\caption{Comparisons with SoTA LCEC approaches on KITTI odometry (01-09 sequences). The best results are shown in bold type. }
\centering
\fontsize{6.7}{10}\selectfont
\begin{tabular}{l|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{01} & \multicolumn{2}{c|}{02} & \multicolumn{2}{c|}{03}  & \multicolumn{2}{c|}{04} & \multicolumn{2}{c|}{ 05} &  \multicolumn{2}{c|}{06}&  \multicolumn{2}{c|}{07} & \multicolumn{2}{c|}{08} & \multicolumn{2}{c}{09}\\
\cline{2-19}
& $e_r$ & $e_t$& $e_r$ & $e_t$ & $e_r$ & $e_t$  & $e_r$ & $e_t$ & $e_r$ & $e_t$ & $e_r$ & $e_t$ & $e_r$  & $e_t$ & $e_r$  & $e_t$ & $e_r$  & $e_t$\\
\hline
\hline
CRLF \cite{ma2021crlf}  
&\textbf{0.623}	&7.363
&0.632	&3.642
&0.845	&6.007
&\textbf{0.601}	&0.372
&0.616	&5.961
&0.615	&25.762
&0.606	&1.807
&0.625	&5.376
&0.626	&5.133\\
UMich \cite{pandey2015automatic}  &2.196	&\textbf{0.305} &3.733
&0.331	&3.201&0.316 &2.086&0.348	&3.526
&0.356 &2.914&0.353	&3.928&0.368 &3.722
&0.367 
&3.117	&0.363\\
HKU-Mars \cite{yuan2021pixel}  &20.73	&3.770
&32.95	&12.70
&21.99	&3.493
&4.943	&0.965
&34.42	&6.505
&25.20	&7.437
&33.10	&7.339
&26.62	&8.767
&20.38	&3.459\\
DVL \cite{koide2023general}  &112.0&2.514
&120.6&4.285	
&124.7&4.711
&113.5&4.871
&123.9&4.286
&128.9&5.408	
&124.7&5.279 
&126.2&4.461 
&116.7	&3.931\\

MIAS-LCEC \cite{zhiwei2024lcec}  
&0.621	&0.300
&0.801	&0.327
&1.140	&0.324
&0.816	&0.369
&4.768	&0.775
&2.685	&0.534
&11.80	&1.344
&5.220	&0.806
&0.998	&0.432
\\
\textbf{EdO-LCEC (Ours)} &2.269 &0.459
 &\textbf{0.561}&\textbf{0.142}
&\textbf{0.737}&\textbf{0.137} &1.104&\textbf{0.339}
&\textbf{0.280}&\textbf{0.093} &\textbf{0.485}&\textbf{0.124} &\textbf{0.188}&\textbf{0.076} &\textbf{0.352}&\textbf{0.115}
&\textbf{0.386}	&\textbf{0.120}\\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti_01_08}
\end{table*}


\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on MIAS-LCEC-TF70. The best results are shown in bold type.}
\centering
\fontsize{6.7}{10}\selectfont
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{\makecell{Residential \\ Community}} & \multicolumn{2}{c|}{Urban Freeway} & \multicolumn{2}{c|}{Building}  & \multicolumn{2}{c|}{\makecell{Challenging \\Weather}} & \multicolumn{2}{c|}{Indoor} &  \multicolumn{2}{c|}{\makecell{Challenging \\Illumination}} &  \multicolumn{2}{c}{All} \\
\cline{2-15}
& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
\hline
CRLF \cite{ma2021crlf}  &1.594&0.464
&1.582&0.140 &1.499 &20.17 &1.646&2.055
&1.886 &30.05 &1.876 &19.047 &1.683 &11.133 \\
UMich \cite{pandey2015automatic}  &4.829
&0.387 &2.267&0.166 &11.914 &0.781 &1.851 &0.310 &2.029  &0.109 &5.012 &0.330 &4.265 &0.333\\
HKU-Mars \cite{yuan2021pixel}  &2.695
&1.208 &2.399 &1.956 &1.814 &0.706 &2.578 &1.086  &2.527 &0.246 &14.996 &3.386 &3.941 &1.261
 \\
DVL \cite{koide2023general}  &0.193
&0.063
&0.298
&0.124
&0.200
&0.087
&0.181
&0.052
&0.391
&0.030
&1.747
&0.377
&0.423
&0.100 \\

MIAS-LCEC \cite{zhiwei2024lcec}  &0.190 &0.050 &\textbf{0.291} &0.111
&0.198&0.072&\textbf{0.177}&0.046&0.363&\textbf{0.024}&0.749&0.118&0.298&0.061 \\
\textbf{EdO-LCEC (Ours)} &\textbf{0.168} &\textbf{0.044} &0.293 &\textbf{0.105} &\textbf{0.184} &\textbf{0.057}  &0.183 &\textbf{0.044} &\textbf{0.338} &0.027 &\textbf{0.474} &\textbf{0.104} &\textbf{0.255} &\textbf{0.055} \\
\bottomrule
\end{tabular}
\label{tab.rescmp_mias_tf70}
\end{table*}

In our experiments, we compare our proposed EdO-LCEC with SoTA online, target-free LCEC approaches on the public dataset KITTI odometry \cite{geiger2012we} (including 00-09 sequences) and MIAS-LCEC \cite{zhiwei2024lcec} (including target-free datasets MIAS-LCEC-TF70 and MIAS-LCEC-TF360). To comprehensively evaluate calibration accuracy, we quantify LCEC performance using the Euler angle error magnitude $e_r$ and the translation error magnitude $e_t$.

Notably, sequences in KITTI odometry, aside from 00, were included in the training datasets for the learning-based methods \cite{lv2021lccnet,yuan2020rggnet,borer2024chaos,zhao2021calibdnn}. To ensure a fair comparison, we reproduced calibration results for both the left and right cameras on sequence 00 when the authors provided their code; otherwise, we used the reported results for the left camera from their papers. Since most learning-based methods lack APIs for custom data, our comparison with these methods is limited to the KITTI odometry 00 sequence. For experiments on the MIAS-LCEC dataset, as the results for the compared methods \cite{ma2021crlf,yuan2021pixel,koide2023general,zhiwei2024lcec,pandey2015automatic} are reported in \cite{zhiwei2024lcec}, we directly use the values presented in that paper.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/SparseTableVisual.pdf}
    \caption{We divided each point cloud into six equal parts and combined these segments to create point clouds with varying densities.}
    \label{fig.visualization_sparse_pcloud}
    \vspace{-1em}
\end{figure*}

\subsection{Comparison with State-of-the-Art Method}
\label{sec.exp_dataset}
In this section, quantitative comparisons with SoTA approaches on three datasets are presented in  Fig. \ref{fig.exp_on_different_density}, Tables \ref{tab.rescmp_kitti00}\footnote{The results generated by LCCNet using the authors' code yield higher errors compared to those reported in their paper.}, \ref{tab.rescmp_kitti_01_08}, \ref{tab.rescmp_mias_tf70}, and \ref{tab.exp_mid360}. Additionally, qualitative results are illustrated in Figs. \ref{fig.datafusion_on_checkerboard} and \ref{fig.SoTA_visualization}. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/SoTAVisualizationV2.pdf}
    \caption{Qualitative comparisons with SoTA target-free LCEC approaches on the KITTI odometry dataset: (a)-(e) RGB images, Depth images, LDP images, LIP images and image segmentation results; (f)-(i) experimental results achieved using ground truth, UMich, HKU-Mars and EdO-LCEC (ours), shown by merging LiDAR depth projections and RGB images, where significantly improved regions are shown with red dashed boxes.}
    \label{fig.SoTA_visualization}
\end{figure*}

\begin{table}[t!]
\caption{
Quantitative comparisons of our proposed EdO-LCEC approach with other SoTA target-free approaches on the MIAS-LCEC-TF360. The best results are shown in bold type.}
\centering
\settablefont
\begin{tabular}{c|rr|rr}
\toprule
\multirow{2}*{Approach} & \multicolumn{2}{c|}{Indoor} & \multicolumn{2}{c}{Outdoor}\\
 & $e_r$ ($^\circ$) & $e_t$ (m)  & $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
\hline
CRLF \cite{ma2021crlf}  &1.479	&13.241	&1.442	&0.139\\
UMich \cite{pandey2015automatic} &1.510	&0.221	&6.522	&0.269\\
\makecell{HKU-Mars \cite{yuan2021pixel}}  &85.834	&7.342	&35.383	&8.542\\
\makecell{DVL \cite{koide2023general}} &39.474	&0.933	&65.571	&1.605\\
MIAS-LCEC \cite{zhiwei2024lcec} &0.996	&0.182	&0.659	&0.114\\
\textbf{EdO-LCEC (Ours)} &\textbf{0.720}&\textbf{0.106}	&\textbf{0.349}	&\textbf{0.109}
\\
\bottomrule
\end{tabular}
\label{tab.exp_mid360}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/CheckerBoardV2.pdf}
    \caption{Visualization of EdO-LCEC calibration results through LiDAR and camera data fusion: (a)-(b) illustrate two LiDAR point clouds in MIAS-LCEC-TF70, partially rendered by color using the estimated extrinsic matrix of EdO-LCEC. }
    \label{fig.datafusion_on_checkerboard}
    \vspace{-1.5em}
\end{figure}


\noindent \textbf{Evaluation on KITTI odometry.} 
The results shown in Table \ref{tab.rescmp_kitti00} and \ref{tab.rescmp_kitti_01_08} suggest that, with the exception of sequences 01 and 04, our method achieves SoTA performance across the ten sequences (00-09) in KITTI odometry. Specifically, in the 00 sequence, EdO-LCEC reduces the $e_r$ by around 35.2-99.8\% and the $e_t$ by 11.8-98.7\% for the left camera, and reduces the $e_r$ by around 46.9-99.7\% and the $e_t$ by 13.9-97.6\% for the right camera. Additionally, according to Fig. \ref{fig.SoTA_visualization}, it can be observed that the point cloud of a single frame in KITTI is so sparse that the other approaches behave poorly. In contrast, our proposed method overcomes this difficulty and achieves high-quality data fusion through the calibration result. We attribute these performance improvements to our spatial-temporal relative pose optimization. Merging the optimal matching results from multiple views and scenes maximizes the number of reliable correspondences and ultimately improves overall calibration accuracy.

\noindent \textbf{Evaluation on MIAS-LCEC Dataset.} 
\label{sec.exp_cmp_eva_mias_lcec}
Compared to the sparse point clouds in the KITTI dataset, the point clouds in the MIAS-LCEC datasets are significantly denser, which facilitates feature matching and allows us to test the upper limits of the algorithm calibration accuracy. 
The results shown in Table \ref{tab.rescmp_mias_tf70} demonstrate that our method outperforms all other SoTA approaches on MIAS-LCEC-TF70. It can also be observed that our method dramatically outperforms CRLF, UMich, DVL, HKU-Mars, and is slightly better than MIAS-LCEC across the total six subsets. In challenging conditions that are under poor illumination and adverse weather, or when few geometric features are detectable, EdO-LCEC performs significantly better than all methods, particularly. This impressive performance can be attributed to the generalizable scene discriminator. The multiple virtual cameras generated by the scene discriminator provide a comprehensive perception of the calibration scene from both spatial and textural perspectives, which largely increases the possibility of capturing high-quality correspondences for the PnP solver. Furthermore, the data fusion results in Fig. \ref{fig.datafusion_on_checkerboard}, obtained using our optimized extrinsic matrix, visually demonstrate perfect alignment on the checkerboard. This highlights the high calibration accuracy achieved by our method.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/SparseTableV4.pdf}
    \caption{Comparisons with SoTA approaches on the segmented point clouds from MIAS-LCEC-TF360. The zoomed-in region highlights the comparative details between the algorithms with higher accuracy. Since the results of CRLF are invalid, they were not included in the comparison.}
    \label{fig.exp_on_different_density}
    \vspace{-1.5em}
\end{figure}



\begin{table*}[t!]
\caption{Ablation study of different components on the 00 sequence of KITTI odometry. The best results are shown in bold type.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{c@{\hspace{0.15cm}}c|c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c}
\toprule
\multicolumn{3}{c|}{Components}&\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\hline
\multicolumn{2}{c|}{Multi-View}&\multirow{2}*{\makecell{Multi\\Scene}}&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\
\cline{1-2}
Intensity&Depth&&$e_r$ ($^\circ$) & $e_t$ (m) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (m)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}\\
\hline
\hline
&& &1.625	&0.459	&0.820	&0.669	&0.899	&0.247	&0.235	&0.211
 &1.620	&0.472	&0.915	&0.691	&0.807	&0.257	&0.254	&0.197\\
\checkmark&&&1.387	&0.358	&0.755	&0.579	&0.711	&0.189	&0.208	&0.153
 &1.641	&0.459	&1.012	&0.679	&0.738	&0.257	&0.257	&0.178\\
\checkmark&\checkmark& &1.125	&0.280	&0.534	&0.534	&0.613	&0.134	&0.151	&0.136
&1.425	&0.354	&0.856	&0.563	&0.679	&0.186	&0.205	&0.138\\
&& \checkmark&0.406	&0.155	&0.180	&0.201	&0.223	&0.119	&0.067	&0.049
 &0.447	&0.192	&0.227	&0.243	&0.211	&0.148	&0.094	&0.051\\
\checkmark&& \checkmark&0.339	&0.110	&0.179	&0.167	&0.162	&0.069	&0.063	&0.039
&0.480	&0.138	&0.322	&0.239	&0.150	&0.096	&0.084	&0.033\\
\checkmark&\checkmark& \checkmark &\textbf{0.295}	&\textbf{0.082}	&\textbf{0.117}	&\textbf{0.176}	&\textbf{0.150}	&\textbf{0.051}	&\textbf{0.044}	&\textbf{0.032} &\textbf{0.336}	&\textbf{0.118}	&\textbf{0.216}	&\textbf{0.168}	&\textbf{0.121}	&\textbf{0.083}	&\textbf{0.067}	&\textbf{0.032}\\
\bottomrule
\end{tabular}
\label{tab.ablation}
\end{table*}

Additionally, experimental results on the MIAS-LCEC-TF360 further prove our outstanding performance. From Table \ref{tab.exp_mid360}, it is evident that while the other approaches achieve poor performances, our method demonstrates excellent accuracy, indicating strong adaptability to more challenging scenarios, with narrow overlapping areas between LiDAR projections and camera images. This impressive performance can be attributed to our proposed DPCM, a powerful cross-modal feature matching algorithm. DPCM utilizes structural and textural consistency to jointly constrain correspondences matching on both spatial and textural pathways. This largely increases reliable correspondences compared to DVL and MIAS-LCEC, thereby providing a more reliable foundation for extrinsic parameter optimization. 

To evaluate the algorithm's adaptability to incomplete and sparse point clouds, we further segmented the already limited field-of-view point clouds from the MIAS-LCEC-TF360 dataset. Specifically, as shown in Fig. \ref{fig.visualization_sparse_pcloud}, we divided each point cloud into six equal parts. By progressively combining these segments, we create point clouds with varying densities, ranging from 1/6 (c1) to the full 6/6 (c6) density. 
The calibration results presented in Fig. \ref{fig.exp_on_different_density} show that EdO-LCEC achieves the smallest mean $e_r$ and $e_t$, along with the narrowest interquartile range, compared to other approaches across different point cloud densities. This demonstrates the stability and adaptability of EdO-LCEC under challenging conditions involving sparse or incomplete point clouds.


\subsection{Ablation Study and Analysis}
\label{sec.ablation_study}
To explore the contribution of scene discriminator and spatial-temporal relative pose optimization, we evaluate our algorithm’s performance with and without multi-view (including both textural and spatial perspective views) and multi-scene optimizations on the 00 sequence of KITTI odometry. The results presented in Table \ref{tab.ablation} demonstrate that lacking any of these components significantly degrades calibration performance. In particular, calibration errors increase substantially when only single-view calibration is applied, as it lacks the comprehensive constraints provided by multi-view inputs. Additionally, the joint optimization from multiple scenes significantly improved the calibration accuracy compared to those only under multi-view optimization. These results confirm the advantage of incorporating spatial and textural constraints from multiple views and scenes, validating the robustness and adaptability of our environment-driven calibration strategy.


\section{Conclusion}
\label{sec.conclusion}
In this paper, we explore to extend a new definition called ``environment-driven" for online LiDAR-camera extrinsic calibration. Unlike previous methods, our approach demonstrates human-like adaptability across diverse and complex environments. Inspired by human perceptual systems, we designed a scene discriminator that actively analyzes the calibration scenes from spatial and textural perspectives. By leveraging structural and textural consistency between LiDAR projections and camera images, our method achieves reliable 3D-2D correspondence matching, overcoming the challenge of cross-modal feature matching in sparse scenes encountered by earlier approaches. Additionally, we modeled the calibration process as a spatial-temporal joint optimization problem, achieving high-precision and robust extrinsic matrix estimation through multi-view optimization within individual scenes and joint optimization across multiple scenarios. Extensive experiments on real-world datasets demonstrate that our environment-driven calibration strategy achieves state-of-the-art performance.

\normalem


\bibliographystyle{IEEEtran} 
\bibliography{egbib}

\clearpage
\input{X_suppl}

		
\end{document}
