% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Environment-Driven Online LiDAR-Camera Extrinsic Calibration}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Zhiwei Huang\\
Tongji University\\
Shang Hai\\
{\tt\small zhiwei.huang@outlook.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\usepackage{array}
\usepackage{url}
\usepackage{makecell}
\usepackage{multirow}
\newcommand{\egi}{\textit{e.g.}}
\newcommand{\settablefont}{\fontsize{6.9}{11.8}\selectfont}

\begin{document}
\maketitle


1. EdO-LCEC 

2. Dual-Path Correspondence Matching (DPCM)

3. Generalizable Scene Discriminator + VFMs

4. Spatial-temporal relative pose optimization:
1) multi-view from scene discriminator 
2) multi-scene 

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/Cover.pdf}
    \caption{Visualization of Calibration Results: We fused 100 frames of point clouds from the KITTI 00 sequence, resulting in the composite point cloud shown in the figure. Each zoomed-in section illustrates the alignment between the camera images and the projected point clouds based on the calibration results. As can be observed, the point clouds and images align perfectly under our calibration method, demonstrating the precision of our approach.}
    \label{fig:enter-label}
\end{figure*}

\begin{abstract}
LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles. Offline, target-based approaches have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. In contrast, online, target-free approaches provide greater adaptability yet typically lack robustness, primarily due to the challenges in cross-modal feature matching. Therefore, in this article, we unleash the full potential of large vision models (LVMs), which are emerging as a significant trend in the fields of computer vision and robotics, especially for embodied artificial intelligence, to achieve robust and accurate online, target-free LCEC across a variety of challenging scenarios. Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments. The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches. Extensive experiments conducted on these real-world datasets demonstrate the robustness of our approach and its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view. Our toolbox and datasets are publicly available at \url{https://mias.group/MIAS-LCEC}.
\end{abstract}

\section{Introduction}
\label{sec.intro}
Sensor fusion for robotic systems has been extensively investigated as it promises to efficiently combine complementary information from different modalities, \egi, to increase robustness in case of sensor failures and towards weather conditions. However, the effectiveness of fusion approaches heavily depends on the extrinsic calibration between the sensors such as cameras and LiDAR. Due to the importance of the task, LiDAR-camera extrinsic calibration (LCEC) has been widely studied by the research community. Previously proposed methods can generally be classified into target-based and target-free approaches. Approaches of the first category often rely on artificial patterns such as checkerboards and require manual labor or another kind of human supervision to associate 2D points from the image space with 3D points in the LiDAR point cloud. While a substantial effort has gone into the detection of the target and the automation of the matching process, calibration often still needs special data collection. Some target-free calibration methods aim at overcoming this problem, e.g., 
%by matching cross-modal features within the shared field of view of the sensors or 
by inferring the extrinsic transform from the sensor motion or by matching vision-based structure-from-motion models with accumulated point clouds from the LiDAR. Although these approaches are generally more widely applicable as they enable sensor calibration from normal robot operation, they either require dense point clouds or rely on consecutive frames of LiDAR point clouds and camera images to estimate accurate sensor motion, which largely declines the applicability of the algorithm. With the continuous advancement of deep learning technologies, an increasing number of methods [x][x] have attempted to use neural network models for online calibration. Although these methods perform well on the KITTI dataset [x], they lack scene understanding capabilities and have limited generalization ability, making it challenging to adapt them to calibration tasks involving unknown sensor configurations and unfamiliar scenes. 

Current methods lack scene analysis capabilities and are highly sensitive to system inputs, making it difficult to adapt to changes in sensor configurations or calibration scenes. Factors such as the density of LiDAR point clouds, the resolution of camera images, and the geometric and textural densities of the calibration scene can significantly impact calibration accuracy. However, due to task or environmental complexitie, real-world robotic operating conditions can be complex and variable, with calibration scenes potentially undergoing substantial changes in a short time. Therefore, designing a robust, high-precision online calibration algorithm that possesses scene analysis capabilities and strong environmental adaptability is a critical challenge that needs to be addressed in this field. 

Overall, there has not been a general, comprehensive LCEC approach that is not sensitive to calibration scene and sensor configuration, partially due to the difficulty of robust cross-modal feature matching. This work closes this gap by developing an environment-driven LCEC framework, leveraging LVMs to achieve robust cross-modal feature matching that adapts to a large range of scenarios. Furthermore, our proposed method supports any number of input LiDAR point clouds and camera images, without the need for dense point cloud input or sensor motion estimation.

In a nutshell, our novel contributions are as follows:

\begin{itemize}
    \item {An online, target-free LCEC approach that enables accurate, robust extrinsic calibration on all kinds of sensor data, including dense or sparse point clouds, or point clouds with no intensity channel.}
    \item {An environmental discriminator that can automatically judge the environment condition, adjusting the calibration cost functions and the number of LIP images.}
    \item {M2P, a novel cross-modal key point matching algorithm, capable of generating dense and reliable key point correspondences between LiDAR point cloud and camera image.}
    \item {``(Optional)`` A novel method to estimate the precision of the estimated extrinsic parameters, enabling more accurate target-free calibration.}
    \item {MIAS-LCEC Toolbox V2, a versatile calibration toolbox based on the proposed algorithm in this article.}
\end{itemize}






\section{Related Work}

\label{sec.related_work}
\label{sec.rel_target-based}
Existing LCEC approaches are primarily categorized as either target-based or target-free based on whether the algorithm requires pre-defined features from both RGB images and LiDAR point clouds. The following two subsections discuss these two types of algorithms in detail. 
%\subsection{Target-based Approaches}
%Target-based approaches are typically offline, relying on customized calibration targets (typically checkerboards). Inspired by the estimation of camera parameters with a checkerboard pattern [x], [x] was the first to propose using a similar target also for extrinsic calibration between a camera and a LiDAR. Since then, many different styles of patterns have been described to further optimize this calibration procedure both in terms of accuracy and applicability. For instance, [x] exploit ArUco markers with known sizes to get accurate estimates of the corner points of the pattern in 3D, which are then registered to the corner point detected by the LiDAR using the ICP algorithm. In the recent study \cite{yan2023joint}, both intrinsic and extrinsic parameters are accurately estimated using a specially designed calibration target, which incorporates a checkerboard pattern and four specifically placed holes. While these methods achieve high calibration accuracy, their reliance on customized targets and the need for additional setup render it impractical for scenarios where robots operate in dynamically changing environments. 


%\subsection{Target-Free Approaches}
\label{sec.rel_target-free}
\noindent \textbf{Correspondence-based Methods.} Having realized the limitations of target-based calibration methods, traditional target-free LCEC approaches emerged, replacing the artificial targets with patterns that can be perceived in structured environments such as urban areas. In studies such as \cite{lv2015automatic, castorena2016autocalibration}, LiDAR point intensities are first projected into the camera perspective, thereby generating a virtual image, namely an LIP image. Edges are then extracted from both the LIP and RGB images. By matching these cross-modal edges, the relative pose between the two sensors can be determined. Similarly, research by \cite{pandey2012automatic, pandey2015automatic} optimizes extrinsic calibration by maximizing the mutual information (MI) between LIP and RGB images. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed line features, which can compromise calibration robustness. Moreover, the use of low-level image processing algorithms, such as Gaussian blur and the Canny operator, can introduce errors in edge detection, potentially fragmenting global lines and thus reducing overall calibration accuracy.

\noindent \textbf{Motion-based Methods.} On the other hand, some studies [x,x,x] started to avoid confronting the difficulty of cross-modal feature matching, they invent motion-based methods that often rely on leveraging output data of auxiliary tasks such as monocular depth or sensor motion estimation. Both [x] and [x] match trajectories from visual and LiDAR odometry and obtain extrinsic parameters via optimization. The latter then utilizes these parameters to initialize an edge-driven refinement stage. Research [x] combine the advantages of both correspondence-free and correspondence-based approaches by performing coarse initialization based on sensor motion followed by fine registration incorporating deep learning-based point correspondences [x]. While these approaches can easily handle heterogeneous sensors and do not need overlap between sensors, they require careful time synchronization, which is not always possible when, for example, we use an affordable web camera. Furthermore, this type of calibration framework require multiple LiDAR point clouds and camera images to estimate the per-sensor motion as accurately as possible for better calibration results. 

\noindent \textbf{Learning-based Methods.} Advances in deep learning techniques have driven significant exploration into enhancing traditional target-free algorithms. Several end-to-end deep learning-based algorithms \cite{schneider2017regnet, iyer2018calibnet,lv2021lccnet} tried to covert calibration process into more direct approaches by exploiting deep learning-based correspondences between RGB images and LiDAR point clouds. While these methods have demonstrated effectiveness on large-scale datasets like KITTI \cite{geiger2012we}, which primarily focuses on urban driving scenarios, their performance has not been extensively validated on other types of real-world datasets. Furthermore, their dependence on pre-defined sensor configurations (both LiDAR and camera) poses implementation challenges. Unlike end-to-end calibration networks, some research tries to attach deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, a recent study \cite{koide2023general} introduced Direct Visual LiDAR Calibration (DVL), a novel point-based method that utilizes SuperGlue \cite{sarlin2020superglue} to establish direct 3D-2D correspondences between LiDAR and camera data. Additionally, this study refines the estimated extrinsic matrix through direct LiDAR-camera registration by minimizing the normalized information distance, a mutual information-based cross-modal distance measurement loss. However, as discussed earlier, domain shift and annotation inconsistency often hinder these algorithms from effectively generalizing to unseen, new scenarios. 


In this work, we take a pioneering step by introducing scene analysis into  LiDAR-camera extrinsic calibration, enhancing its interpretability and environmental adaptability. To address the dependency on typical calibration scenes, our method leverages SoTA LVMs to extract more informative features from both LIP and RGB images. Furthermore, we develop a more robust cross-modal feature matching strategy DPM, which makes the LCEC process fully target-free, overcoming the previous reliance on specific semantic targets. By integrating these advancements, we aim to enhance the accuracy and robustness of LCEC algorithms, enabling them to perform effectively in diverse and challenging real-world environments.

\section{Method}
\label{sec.method}

%\begin{figure*}
%    \centering
%    \includegraphics[width=1\linewidth]{figs/FrameWork_V1.pdf}
%    \caption{Enter Caption}
%    \label{fig.framework_overview}
%\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/framework.jpg}
    \caption{Enter Caption}
    \label{fig.framework_overview}
\end{figure}

%\subsection{Preliminaries}
%\label{sec.pre}
Given a LiDAR point clouds stream and a camera images stream, our goal is to estimate an extrinsic matrix $^{C}_{L}\boldsymbol{T}$, defined as follows \cite{zhao2023dive}:
\begin{equation}
{^{C}_{L}\boldsymbol{T}} = 
\begin{pmatrix}
{^{C}_{L}\boldsymbol{R}} & {^{C}_{L}\boldsymbol{t}} \\
\boldsymbol{0}^\top & 1
\end{pmatrix}
\in{SE(3)},
\label{eq.lidar_to_camera_point}
\end{equation}
where $^{C}_{L}\boldsymbol{R} \in{SO(3)}$ represents the rotation matrix, $^{C}_{L}\boldsymbol{t}$ denotes the translation vector, and $\boldsymbol{0}$ represents a column vector of zeros. 
In this article, the symbols in the superscript and subscript denote the source and target sensors, respectively. 

In this paper, our method focuses on the cross-modal feature matching and joint optimization of multiple views and scenes,
which encompass three critical tasks: scene analysis, dense key point matching, and extrinsic matrix optimization. We take one step forward by leveraging the large vision model MobileSAM and DepthAnythinV2 to achieve comprehensive geometric and semantic interpretation. Firstly, we design a scene parser to generate virtual cameras and obtain the semantic and geometric features within the LiDAR point cloud projections and camera images. 
%aiming at overcoming the current shortage on target-free calibration on LiDAR with sparse point cloud. As depicted in Fig. \ref{fig.framework_overview}
%As depicted in Fig. \ref{fig.framework_overview}, our calibration framework includes environmental discrimination, LiDAR projections generation, mask generation, key point matching, and extrinsic estimation. Instead of projecting LiDAR intensities from a single viewpoint, we utilize multiple virtual cameras to capture LiDAR point intensities and depth from various perspectives, generating comprehensive virtual images known as LiDAR intensity projection (LIP) images and LiDAR depth projection (LDP) images, which provide a richer set of cross-modal features. To align with LDP images, camera images are processed using Depth Anything V2 to obtain estimated depth images. All the images are then segmented by the state-of-the-art vision model MobileSAM to yield reliable cross-modal masks incorporating both texture and depth information. Subsequently, we employ the M2P matching strategy to achieve dense 3D-2D key point correspondences, which serve as inputs to a PnP solver for estimating the extrinsic matrix $^{C}_{L}\boldsymbol{T}$.

%When the camera intrinsic matrix $\boldsymbol{K}$ is known, a 3D LiDAR point $\boldsymbol{p}^{L}=(x^L;y^L;z^L)$ can be projected onto a 2D image pixel $\boldsymbol{{p}} = (u;v)$ using the following expression:
%\begin{equation}
%\tilde{\boldsymbol{p}} = \pi({^{C}_{L}\boldsymbol{T}}\tilde{\boldsymbol{p}}^{L}) = \frac{\boldsymbol{K}({^{C}_{L}\boldsymbol{R}}\boldsymbol{p}^{L} + {^{C}_{L}\boldsymbol{t}})}{({^{C}_{L}\boldsymbol{R}}\boldsymbol{p}^{L} + {^{C}_{L}\boldsymbol{t}})^\top\boldsymbol{1}_{z}},
%\end{equation}
%where $\tilde{\boldsymbol{p}}$ represents the homogeneous coordinates of $\boldsymbol{{p}}$ and $\boldsymbol{1}_{z}=(0;0;1)$. 

\subsection{Scene Analysis} 

Inspired by the human perception system, our calibration algorithm incorporates environmental awareness. It employs a scene parser to interpret the surroundings by generating virtual cameras to project LiDAR point cloud intensities and depth. The parser configure the first intensity virtual camera and depth virtual camera with a relative transformation ${^{I}_{L}\boldsymbol{T}}$ and ${^{D}_{L}\boldsymbol{T}}$, 
%\begin{equation}
%{^{V,D}_{L}\boldsymbol{T}} = 
%\begin{pmatrix}
%\underbrace{
%\begin{pmatrix}
%0 & -1 & 0 \\
%0 & 0 & -1 \\
%1 & 0 & 0 \\
%\end{pmatrix}}_{{^{V,D}_{L}\boldsymbol{R}}} & \underbrace
%{
%\begin{pmatrix}
%0\\
%0\\
%0\\
%\end{pmatrix}
%}
%_
%{
%{^{V,D}_{L}\boldsymbol{t}}
%}
% \\
%\boldsymbol{0}^\top & 1
%\end{pmatrix}
%\in{SE(3)}.
%\label{eq.other_transformation}
%\end{equation}
respectively. This generates a LiDAR intensity projection (LIP) image $\boldsymbol{I}^V_I \in{\mathbb{R}^{H\times W \times 1}}$ ($H$ and $W$ represent the image height and width) and a LiDAR depth projection (LDP) image $\boldsymbol{I}^V_D$. To align with the LDP image, the input camera RGB image $\boldsymbol{I}^C_I$ is processed using Depth Anything V2 to obtain estimated depth images $\boldsymbol{I}^C_D$\footnote{In this article, the symbols in the right superscript denote the type of an image ($C_I$,$C_D$,$V_I$,$V_D$ denote real camera RGB image, camera depth image, virtual camera intensity image and virtual camera depth image respectively ).}.  To take advantage of semantic information, we utilize MobileSAM as the image segmentation backbone. The detected masks is defined as $\{\mathcal{M}_1, \dots, \mathcal{M}_n\}$. The key points along the contours of masks detected are represented by two sets: $\mathcal{C}^V = \{\boldsymbol{c}^V_1, \dots, \boldsymbol{c}^V_m\}$ and $\mathcal{C}^C = \{\boldsymbol{c}^C_1, \dots, \boldsymbol{c}^C_m\}$, respectively. An instance (bounding box), utilized to precisely fit around each mask, is centrally positioned at $\boldsymbol{o}^{V, C}$ and has a dimension of $h^{V, C}\times w^{V, C}$ pixels. We define the depth of a key point as $d^{V, C}$, and $\boldsymbol{D}^{V, C} \in \mathbb{R}^{k\times k}$ as the matrix that stores depth values in its k-neighborhood. To fully leverage the structured information within the masks, we construct a convex polygon from the key points. As depicted in Fig. \ref{fig.m2p}, the neighboring edges of a key point are defined as $\boldsymbol{e}^{V, C}$ and $\boldsymbol{h}^{V, C}$. 

Due to the occlusion issue brought by the different perspective views of LiDAR and the camera and the lack of adequate features when the point cloud is sparse, a single pair of virtual cameras is not enough for a comprehensive perception of the calibration scene. To address this obstacle, the scene parser is designed to calculate the semantic density of the target scene and generate multiple virtual cameras to capture intensity and depth projections from varied viewpoints. For each virtual image or camera image, its semantic density $\rho$ can be obtained as follows:
%While previous methods did not treat calibration scenarios as variables in the algorithm, our novel calibration framework is environment-aware, utilizing an environment discriminator (EDI) to judge the density of semantic features in LiDAR point cloud and camera image. EDI works at two stages (LiDAR projections generation and mask-to-point matching) in our framework. 
%\begin{equation}
%D_s = \frac{\lvert \bigcap_{i=1}^{n} M_i \rvert}{A}\frac{\lvert \bigcap_{i=1}^{n} M_i \rvert}{\sum_{i=1}^{n} \lvert M_i\rvert}\frac{\sum_{i=1}^{n}{\frac{m_i}{\lvert M_i\rvert}}}{n}
%\end{equation} 
\begin{equation}
\rho({\boldsymbol{I}}) = \underbrace {\bigg( \frac{1}{n}\sum_{i=1}^{n}{\frac{m_i}{\lvert \mathcal{M}_i\rvert}}\bigg )}_{\rho_c} \underbrace {\bigg(\frac{\lvert \bigcap_{i=1}^{n} \mathcal{M}_i \rvert}{\sum_{i=1}^{n} \lvert \mathcal{M}_i\rvert} \bigg)}_{\rho_a}
\end{equation}
Define the event $E$ of capturing enough effective features, with its probability given by $P(E) = P$. If we have $n$ virtual cameras, the probability of capturing enough effective features is $1 - (1 - P)^n$. In theory, as $n$ increases, this probability will also increase. When $ n \to \infty $, $P(E')^n \to 0 $, there is $1 - (1 - P)^n \to 1$. However, it is impossible to apply an infinite virtual camera during the calibration. Considering the trade-off between calibration accuracy and compute resources, we set the number of multiple virtual cameras to satisfy the balance of semantic density:
\begin{equation}
\rho(\boldsymbol{I}^C_I) + \rho(\boldsymbol{I}^C_D) = \sum_{i=1}^{n_I}\rho(\boldsymbol{I}^V_{I,i}) + \sum_{i=1}^{n_D}{\rho}(\boldsymbol{I}^V_{D,i}) 
\end{equation}
Since the semantic density is similar if the perspective viewpoint is close to the initial position, we can assume that $\rho(\boldsymbol{I}^V_{I, i}) \approx \rho(\boldsymbol{I}^V_{I,0})$ and $\rho(\boldsymbol{I}^V_{D, i}) \approx \rho(\boldsymbol{I}^V_{D,0})$. So $n_I$ and $n_D$ can be obtained by
\begin{equation}
n_I = \frac{\rho(\boldsymbol{I}^C_{I})}{\rho(\boldsymbol{I}^V_{I, 0})}, n_D = \frac{\rho(\boldsymbol{I}^C_{I})}{\rho(\boldsymbol{I}^V_{D, 0})}
\end{equation}
In practical applications, we use random spherical sampling to generate $n_I$ and $n_D$ virtual camera positions, uniformly distributed within a sphere centered around the LiDAR.
%In the mask generation stage, EDI adjusts the appropriate IoU threshold $P_m$ for mask segmentation based on the feature density, which can be determined as follows:
%\begin{equation}
%P_m = \lambda + (1-\lambda)\frac{m_I + m_D}{m_I + m_D + 2m_C},
%\end{equation}
%where $\lambda$ is a robust parameter (usually determined by the pre-trained network) that ensures the reliability of the mask segmentation.
%To efficiently establish correspondences between LiDAR point clouds and camera images, previous methods typically configure the virtual camera with a relative transformation ${^{V}_{L}\boldsymbol{T}}$ from LiDAR as follows:

%This generates a LiDAR intensity projection (LIP) image $\boldsymbol{I}^L \in{\mathbb{R}^{H\times W \times 1}}$, framing the LCEC problem as a 2D feature matching task, where $H$ and $W$ represent the image height and width, respectively. However, due to the image distortion caused by varying perspectives and the absence of depth information in point clouds, this projection strategy struggles to capture a sufficient number of matchable features, particularly when using sparse LiDAR point clouds, such as those from mechanical LiDARs.

\subsection{Dense Cross-Modal Key Point Matching}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/M2PVisual.jpg}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}

The proposed DPM is an innovative cross-modal key point matching strategy. It leverages segmented masks as input to achieve dense and reliable 3D-2D key point correspondences. DPM first utilizes the IOU of masks to construct a cost matrix $\boldsymbol{M}^{I}$, where the element at $\boldsymbol{x} = [i,j]^\top$, namely
\begin{equation}
\boldsymbol{M}^{I}(x) = \sum_{i = 1}^{n}{\frac{\lvert \mathcal{M}^V_i\cap{\mathcal{M}^C_i} \rvert}{\lvert \mathcal{M}^V_i\cup{\mathcal{M}^C_i} \rvert}}. 
\end{equation}
denotes the matching cost between the $i$-th mask in a LiDAR virtual image and the $j$-th mask in the camera image. A strict criterion is applied to achieve reliable matching. Matches with the lowest costs in both horizontal and vertical directions of $\boldsymbol{M}(x)$ are determined as the optimum mask matches $ \{(\mathcal{M}^V_i, \mathcal{M}^C_i)\}_{i = 1}^{m}$. Each matched mask pair $(\mathcal{M}^V_i,\mathcal{M}^C_i)$ can estimate an affine transformation matrix $[s\boldsymbol{R}_A,\boldsymbol{t}_A]$, which serves as a reference for dense key point matching. Detailed derivation for $s\boldsymbol{R}^A$ and $\boldsymbol{t}^A$ is explained in \textcolor{blue}{Appendix} \textcolor{red}{1.1}. 

To determine optimum key point matches, we construct a cost matrix $M^K$, where the element at $\boldsymbol{y} = [i,j]^\top$, namely:
\begin{equation}
\begin{aligned}
\boldsymbol{M}^{K}(y) &=
\rho_c\underbrace{\bigg(1-  \exp(\frac{\left\|s\boldsymbol{R}\boldsymbol{c}^V_i + \boldsymbol{t} - \boldsymbol{c}^C_j \right\|_2}{wh}) }_{
L}  \\
&+ \underbrace{\sum_{i = 1}^{N}{ \frac{\| (\boldsymbol{h}_i^V - \boldsymbol{h}_j^V ) -(\boldsymbol{e}_i^V - \boldsymbol{e}_j^V )\|_2}{\| \boldsymbol{h}_i^V - \boldsymbol{h}_j^V \|_2 + \| \boldsymbol{e}_i^V - \boldsymbol{e}_j^V \|_2}}}_{S}\bigg) \\
&+ \rho_a\underbrace{\bigg(\frac{1}{k^2}\sum_{r = 1}^{k}\sum_{s = 1}^{k}{\left |\boldsymbol{D}^V_{i}(r,s)-\boldsymbol{D}^C_{j}(r,s) \right |}\bigg)}_{T}  \\
\\
\end{aligned}
\label{eq.adaptive_cost_func}
\end{equation}
denotes the matching cost between the $i$-th key point of a mask in the LiDAR virtual image and the $j$-th key point of a mask in the camera image. In \ref{eq.adaptive_cost_func}, there are three important components, $L$, $S$, and $T$. $L$ and $S$ together form the semantic structural cost when matching a pair of key points, where $L$ indicates the location difference and $S$ indicates the local structural-semantic cost. $D$ is the textural component that reflects the differences between the texture in the K neighborhood of this pair of key points. Especially, when matching key points between an LDP image and a camera depth image, $D$ demonstrates the loss of the relative spatial structure.

\subsection{Extrinsic Calibration and Optimization}
\label{sec.consistency_filter}
%Based on DPM, we can obtain dense key point correspondences $\mathcal{K} = \{(\boldsymbol{p}_{k,1},\boldsymbol{p}_{k,1}^L), \dots, (\boldsymbol{p}_{k,N},\boldsymbol{p}_{k,N}^L) \}$, which store 2D pixels in the RGB image captured by camera and the corresponding 3D LiDAR points, respectively. 
%This list is ordered by confidence coefficients, from which the algorithm selects the $n_I$ and $n_D$ vectors with the lowest confidence as the final reference vectors, denoted as $\mathcal{V}_I = \{^{{I}}\boldsymbol{v}_1,\dots,^{{I}}\boldsymbol{v}_{n_I}\}$ and $\mathcal{V}_D = \{{^{{D}}\boldsymbol{v}_1},\dots,{^{{D}}\boldsymbol{v}_{n_D}}\}$. The poses of the virtual cameras can then be calculated as follows:
%\begin{equation}
%\left\{
%\begin{aligned}
%^{I}_{L}\boldsymbol{T}_i &= {^{V}_{L}\boldsymbol{T}} + {^{{I}}\boldsymbol{v}_i}, i \in [1,n_I],\\
%^{{D}}_{L}\boldsymbol{T}_j &= {^{V}_{L}\boldsymbol{T}} + {^{{D}}\boldsymbol{v}_j}, j \in [1,n_D],
%\end{aligned}
%\right.
%\label{eq.virtual_camera_pos}
%\end{equation}
%where $^{I}_{L}\boldsymbol{T}_i$ and $^{D}_{L}\boldsymbol{T}_j$ is the pose of $i$-th and $j$-th intensity virtual camera and depth virtual camera. The physical significance of (\ref{eq.virtual_camera_pos}) is to position the virtual camera as close as possible to the real camera, thereby increasing the overlap of features from LiDAR projections and camera images.

In our framework, optimization of the extrinsic matrix includes two stages, multi-view optimization and multi-scene optimization. In the multi-view optimization, we assume that in each calibration scene $S_i$, each virtual camera perspective can estimate an extrinsic matrix using a PnP solver. 

\noindent \textbf{Multi-View Consistency Loss.}
Defining that in each perspective, the matching result obtained by the DPM is $\{( {^L\boldsymbol{p}_{i,j}}, \boldsymbol{p}_{i,j})\}_{j = 1}^{k_i}$, where $k_i$ is the total matching pairs number at this perspective. Then extrinsic matrix ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ of this scenario can be formulated as follows:
\begin{equation}
{^{C}_{L}\hat{\boldsymbol{T}}_i} = \underset{^C_L{\boldsymbol{T}_{i}}}{\arg\min} 
\sum_{n = 1}^{n_I + n_D}\underbrace {\sum_{m=1}^{k}{w_{n,m}\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{i}}
 \boldsymbol{p}_{n,m}^L) - {\boldsymbol{p}}_{n,m}\right\|_2}}_{\epsilon_n}, 
\end{equation}
where $w_{n,m}$ is the matched point pair weight of $\{\{ {^L\boldsymbol{p}_{n,m}}, \boldsymbol{p}_{n,m}\}\}$, which is defined as follows:
\begin{equation}
w_{n,m} = \frac{\boldsymbol{M}^C(n,m)}{1 + \left\|\pi(
{^{C}_{L}\boldsymbol{T}_{i}}
 \boldsymbol{p}_{n,m}^L) - {\boldsymbol{p}}_{n,m}\right\|_2}\frac{d_{n,m}}{d_{max}}
 \label{eq.match_point_weight}
\end{equation}
In (\ref{eq.match_point_weight}) $d_{n,m}$ is the depth of point $\boldsymbol{p}_{n,m}^L$ and 
$d_{max}$ is the maximum depth value of all the key point correspondences. 

\noindent \textbf{Multi-Scene Consistency Loss.} Using the estimated ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ at each scenario, we choose a subset  $\{({^L\boldsymbol{p}_{i,j}}, \boldsymbol{p}_{i,j})\}_{i=0}^{s}$ of the matched point pairs as the input of the multi-scene calibration. The multi-scene optimization solves the final extrinsic matrix ${^{C}_{L}{\boldsymbol{T}}^*}$ by optimizing a robustified non-linear squares problem of the form:
\begin{equation}
\mathcal{L}({^{C}_{L}\hat{\boldsymbol{T}}_i}) = 
\sum_{n = 1}^{S} {\sum_{m=1}^{N}{\rho(w_{n,m}\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{i}}
 \boldsymbol{p}_{n,m}^L) - {\boldsymbol{p}}_{n,m}\right\|_2})}.
\end{equation}
where $\rho$ denotes the robustifier. We utilize the Cauchy loss as a robustifier due to its high tolerance to outliers in the observations [x], resulting in a significant increase in calibration accuracy.


\section{Experiment}
\label{sec.experiment}


\subsection{Experimental Setup and Evaluation Metrics}
\label{sec.exp_setup}

\begin{table*}[t!]
\caption{Quantitative comparisons with SoTA target-free LCEC approaches on the KITTI Odemotry 00 sequence. The best results are shown in bold type.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|ccp{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}|ccp{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}}
\toprule
\multirow{3}*{Approach} &\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\cline{2-17}
&\multicolumn{2}{c}{Magnitude}
&\multicolumn{3}{c}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c}{Magnitude}
&\multicolumn{3}{c}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\

& $e_r$ ($^\circ$) & $e_t$ (cm) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (cm)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}  \\
\hline
\hline
Borer et al. \cite{borer2024chaos}  &0.180 &0.095	&0.18 &0.031 &0.033 &0.094 &0.013 &0.006 &- &- &-	&-	&-	&-	&-	&-\\

LCCNet et al. \cite{lv2021lccnet}  &0.331 &0.029 &0.013	&0.056	&0.308 &0.017  &0.013	&0.020	 & 1.47 &0.525 &0.525 &0.260 &0.740 &0.01 &1.47 &0.03	\\

CalibRCNN \cite{yuan2020rggnet}  &0.805 &0.093	&0.199	&0.640	&0.446	&0.062	&0.043 &0.054 &- &-	&-	&-	&-	&-	&-	&-\\

CalibDNN \cite{yuan2020rggnet}  &1.021 &0.115	&0.150	&0.990	&0.200	&0.055	&0.032	&0.096 &- &-&-	&-	&-	&-	&-	&-\\



RegNet \cite{yuan2020rggnet}  &0.500 &0.108	&0.240	&0.250	&0.360	&0.070	&0.070	&0.040 &- &-&-	&-	&-	&-	&-	&-\\

CalibAnything \cite{luo2023calib}  &0.578 &0.136	&0.876	&1.594	&0.060	&0.464	&3.636	&0.136 &5.315 &1.581	&0.876	&1.594	&0.060	&0.464	&3.636	&0.1366\\

UMich \cite{luo2023calib}  &4.161	&0.321	&0.113	&3.111	&2.138	&0.286	&0.077	&0.086
 &0.936 &1.954	&0.376	&1.235	&0.145	&0.464	&3.636	&0.156\\

HKU-Mars \cite{luo2023calib}  &36.70	&89.09	&22.26	&23.51	&21.34	&28.58	&74.96	&33.85
 &43.95 &1.581	&0.876	&1.594	&0.060	&0.464	&3.636	&0.137\\

CRLF \cite{luo2023calib}  &0.631	&8.421	&0.045	&0.458	&0.423	&7.487	&3.098	&0.804 &0.876	&1.594	&0.060	&0.464	&3.636	&0.136 & 0.839 & 0.921\\

DVL \cite{koide2023general}  &1.258 &1.581	&0.876	&1.594	&0.060	&0.464	&3.636	&0.136 &3.289 &2.581	&0.876	&1.594	&0.060	&0.464	&3.636	&0.136\\
RGGNet \cite{yuan2020rggnet}  &1.29 &0.115	&0.350	&0.742	&0.641	&0.081	&0.028	&0.040 &3.87 &0.235	&0.513	&3.382	&1.481	&0.180	&0.056	&0.060\\

CalibNet \cite{yuan2020rggnet}  &5.500 &0.130	&3.000 &3.000 &3.050 &0.070 &0.067 &0.086 &5.096 &0.125	&3.035 &2.908 &2.881 &0.064 &0.065 &0.085\\

Ours \cite{ma2021crlf}  &\textbf{0.288}	&\textbf{0.087}	&\textbf{0.157}	&\textbf{0.139}	&\textbf{0.131}	&\textbf{0.050}	&\textbf{0.053}	&\textbf{0.029} &\textbf{0.452} &\textbf{0.104}	&\textbf{0.376}	&\textbf{0.194}	&\textbf{0.060}	&\textbf{0.083}	&\textbf{0.051}	&\textbf{0.078}\\

\bottomrule
\end{tabular}
\label{tab.rescmp_kitti00}
\end{table*}

\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on KITTI Dataset (01-08 Sequence): value in the table are rotation errors (degrees) and translation errors (centimeters).}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|ccccccccc}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
& 01 & 02 & 03  & 04 &  05 &  06&  07 & 08 \\
\hline
\hline
UMich \cite{ma2021crlf}  &2.196	$/$0.305 &3.733
$/$0.331	&3.201$/$0.316 &2.086$/$0.348	&3.526
$/$0.356 &2.914$/$0.353	&3.928$/$0.368 &3.722
$/$0.367 \\
HKU-Mars \cite{ma2021crlf}  &11.99$/$0.784	&24.43$/$3.268 &17.48$/$1.620	&15.85
$/$306.9 &37.81$/$3.313	&16.12$/$2.540 &31.56$/$3.097 & 15.31$/$56.27 \\
CRLF \cite{ma2021crlf}  &0.626$/$13.38 &0.620
$/$1.764 &0.840$/$3.958 &0.596
$/$0.307 &0.609
$/$4.724 &0.684$/$18.597
&0.599$/$0.531 &0.611$/$3.190 \\
DVL \cite{koide2023general}  &1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.137 \\
CalibAnything \cite{luo2023calib}  &1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.136	&1.594$/$0.136 &1.594$/$0.137 \\
Ours &1.995 $/$0.431 &0.695$/$0.177
&0.754$/$0.131 &1.064$/$0.337
&0.263$/$0.098 &0.432$/$0.115 &0.188$/$0.081 &0.322$/$0.101 \\

\bottomrule
\end{tabular}
\label{tab.rescmp_mias_tf70}
\end{table*}

\begin{table}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on MIAS-LCEC-TL70 Dataset.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|l|p{0.35cm}p{0.35cm}p{0.35cm}p{0.35cm}p{0.35cm}p{0.35cm}}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Subsets} & \multirow{2}*{Method} & \multicolumn{3}{c}{$e_r$ ($^\circ$)} &  \multicolumn{3}{c}{$e_t$   (cm)}\\

&  & Mean &Max & Min & Mean &Max & Min \\
\hline
\hline
\multirow{6}*{01} 

&CRLF \cite{ma2021crlf}  	&1.594	&1.754	&1.537 &0.464	&3.636	&0.136
\\
&UMich \cite{pandey2015automatic}  	&4.829	&21.706	&0.214	&0.387	&2.394	&0.072
\\
&HKU-Mars \cite{yuan2021pixel}  	&2.695	&7.036	&0.749	&1.208	&4.405	&0.319
\\
&DVL \cite{koide2023general}  	&0.193	&0.383	&0.042	&0.063	&0.141	&0.018
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.165	&0.272	&0.064	&0.044	&0.086	&0.012
\\
\hline
\multirow{6}*{02} 
&CRLF \cite{ma2021crlf}  	&1.582	&1.585	&1.581	&0.140	&0.140	&0.140
\\
&UMich \cite{pandey2015automatic}  	&2.267	&6.531	&0.506	&0.166	&0.241	&0.103
\\
&HKU-Mars \cite{yuan2021pixel}  	&2.399	&3.744	&0.337	&1.956	&9.453	&0.148
\\
&DVL \cite{koide2023general}  	&0.298	&0.420	&0.090	&0.124	&0.268	&0.064
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.295	&0.431	&0.097	&0.105	&0.134	&0.079 \\
\hline
\multirow{6}*{03} 
&CRLF \cite{ma2021crlf}  	&1.499	&1.706	&1.465	&20.17	&140.3 &0.140 \\
&UMich \cite{pandey2015automatic}  	&11.914	&22.778	&0.452	&0.781	&1.497	&0.042 \\
&HKU-Mars \cite{yuan2021pixel}  	&1.814	&3.618	&0.118	&0.706	&2.595	&0.059 \\
&DVL \cite{koide2023general}  	&0.200	&0.357	&0.059	&0.087	&0.146	&0.052 \\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.235 &0.780	&0.051	&0.054	&0.105	&0.022 \\
\hline
\multirow{6}*{04} 
&CRLF \cite{ma2021crlf}  	&1.646	&1.803	&1.552	&2.055	&20.307	&0.137
\\
&UMich \cite{pandey2015automatic}  	&1.851	&5.335	&0.294	&0.310	&2.134	&0.043
\\
&HKU-Mars \cite{yuan2021pixel}  	&2.578	&11.647	&0.371	&1.086	&4.934	&0.302
\\
&DVL \cite{koide2023general}  	&0.181	&0.311	&0.037	&0.052	&0.113	&0.017
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.179	&0.343	&0.077	&0.046	&0.106	&0.009
\\
\hline
\multirow{6}*{05} 
&CRLF \cite{ma2021crlf}  	&1.886	&2.201	&1.551	&30.046	&90.926	&0.140
\\
&UMich \cite{pandey2015automatic}  	&2.029	&4.596	&0.448	&0.109	&0.176	&0.020
\\
&HKU-Mars \cite{yuan2021pixel}  	&2.527	&5.458	&0.779	&0.246	&0.893	&0.036
\\
&DVL \cite{koide2023general}  	&0.391	&0.879	&0.193	&0.030	&0.078	&0.013
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.315	&0.573	&0.082	&0.025	&0.042	&0.013
\\
\hline
\multirow{6}*{06} 
&CRLF \cite{ma2021crlf}  	&1.876	&2.141	&1.613	&19.05	&34.44	&0.132
\\
&UMich \cite{pandey2015automatic}  	&5.012	&12.09	&0.314	&0.330	&0.679	&0.043
\\
&HKU-Mars \cite{yuan2021pixel}  	&15.00	&38.750	&0.338	&3.386	&10.52	&0.034
\\
&DVL \cite{koide2023general}  	&1.747	&8.466	&0.207	&0.377	&1.970	&0.022
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours  &0.474	&0.915	&0.176	&0.103	&0.276	&0.025
\\
\hline
\multirow{6}*{All} 
&CRLF \cite{ma2021crlf}  	&1.683	&2.201	&1.465	&11.133	&140.32	&0.132
\\
&UMich \cite{pandey2015automatic}  	&4.265	&22.78	&0.214	&0.333	&2.394	&0.020
\\
&HKU-Mars \cite{yuan2021pixel}  	&3.941	&38.75	&0.118	&1.261	&10.52	&0.034
\\
&DVL \cite{koide2023general}  	&0.423	&8.466	&0.037	&0.100	&1.970	&0.013
\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.594	&1.594 &1.594	&1.594 &1.594\\
&Ours   &0.255	&0.915	&0.051	&0.055	&0.276	&0.009
\\

\bottomrule
\end{tabular}
\label{tab.rescmp_mias_tf360}
\end{table}


\begin{table}[t!]
\caption{
Quantitative comparison of our proposed MIAS-LCEC approach with other SoTA online, target-free approaches on the MIAS-LCEC-TF360 dataset, where the best results are shown in bold type.}
\centering
\settablefont
\begin{tabular}{c|c|rr}
\toprule
Error & Approach & Indoor & Outdoor\\
\hline
\hline
\multirow{5}*{$e_r$ ($^\circ$)} 

&CRLF \cite{ma2021crlf}  &1.469	&1.402\\
 &UMich \cite{pandey2015automatic} &1.802	&2.698\\
&\makecell{HKU-Mars \cite{yuan2021pixel}}  &96.955	&25.611\\
&\makecell{DVL \cite{koide2023general}} &63.003	&46.623\\
&CalibAnything \cite{luo2023calib}  	&1.594 &1.382\\
&\textbf{Ours} &\textbf{0.724}	&\textbf{0.351}\\
\hline
\multirow{5}*{$e_t$ (cm)} 
&CRLF \cite{ma2021crlf} &13.484	&0.139\\
&UMich \cite{pandey2015automatic} &0.200	&0.135\\
&\makecell{HKU-Mars \cite{yuan2021pixel}} &4.382 &9.914\\
&\makecell{DVL \cite{koide2023general}} &0.919	&1.778\\
&CalibAnything \cite{luo2023calib}  	&0.203 &0.186\\
&\textbf{Ours} &\textbf{0.107}	&\textbf{0.080}	\\

\bottomrule
\end{tabular}
\label{tab.mid360}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/SparsifyPC_cmpv1.pdf}
    \caption{Evaluation on the adaptability of different methods to sparse point clouds: In the figure, the point cloud density decreases progressively, with $1/x$ representing the level of point cloud downsampling.}
    \label{fig.visualization_sparse_pcloud}
\end{figure*}

In our experiments, we compare our proposed method
with correspondence-based and learning-based SoTA target-free LCEC approaches on the MIAS-LCEC dataset (collected using a Livox Mid-70 LiDAR and a MindVision SUA202GC camera, from a variety of indoor and outdoor environments), and the large-scale public dataset KITTI Odometry (collected by two RGB cameras Point Grey Flea 2 and one LiDAR of type Velodyne HDL-64E. Sensor data is captured at 10 Hz). The learning-based methods include CalibNet, CalibDNN, CalibRCNN, RegNet, RGGNet, and LCCNet. The correspondence-based methods include HKU-Mars, UMich,  DVL, CRLF and Calib-Anything.

To comprehensively evaluate the performance of LCEC approaches, we use the magnitude of Euler angle error, with the following expression:
\begin{equation}
    e_r =  \left\|\boldsymbol{r}^* - \boldsymbol{r}\right\|_2,
\end{equation}
where ${\boldsymbol{r}^{*}}$ and ${\boldsymbol{r}}$ represent the estimated and ground-truth Euler angle vectors, computed from the rotation matrices ${^{C}_{L}\boldsymbol{R}}^*$ and ${^{C}_{L}\boldsymbol{R}}$, respectively, and the magnitude of translation error, with the following expression\footnote{The translation from LiDAR pose to camera pose is $-{{^{C}_{L}\boldsymbol{R}}^{-1}}\boldsymbol{t}$ when (\ref{eq.lidar_to_camera_point}) is used to depict the point translation.}:
\begin{equation}
    e_t= \left\|-{({^{C}_{L}\boldsymbol{R}^*})^{-1}}\boldsymbol{t}^* +{{^{C}_{L}\boldsymbol{R}}^{-1}}\boldsymbol{t}\right\|_2,
\end{equation}
where $\boldsymbol{t}^*$ and $\boldsymbol{t}$ denote the estimated and ground-truth translation vectors, respectively, are used to quantify the performance of target-free LCEC approaches. 

\subsection{Comparison with State-of-the-Art Method}
\label{sec.exp_dataset}
Quantitative comparisons with SoTA approaches on three datasets are presented in Tables \ref{tab.rescmp_kitti00}-\ref{tab.rescmp_mias}. Additionally, qualitative results are illustrated in Figs. x and \ref{fig.mid360}. It is important to note that since the learning-based approaches test their networks on 00 sequences and train them on other sequences, we compare their methods and other approaches only on sequence 00 to get a fair result.

\noindent \textbf{ Evaluation on KITTI Dataset.} For the KITTI dataset, if the authors released the corresponding code, we reproduce the calibration results for both the left and the right camera. The results shown in Table \ref{tab.rescmp_kitti00} and Table \ref{tab.rescmp_kitti00_08} suggest that our method outperforms all other SoTA approaches on KITTI Odometry 00 sequence (including 4635 frames of point clouds and images), our method reducing $e_r$ by around 30-93\% and decreases $e_t$ by 39-99\%. Additionally, it can be observed that all the correspondence-based methods behave poorly, partially due to the sparse point cloud in the KITTI dataset. Although all baselines are outper-
formed by our approach, they generally yield accurate results for calibrating the LiDAR to the left camera. However, it is paramount to emphasize that all learning-based methods incorporate samples of the left camera in their training data. Therefore, we also calibrate the right camera measuring the capability to generalize. It is clearly that all baselines suffer from a substantial performance drop. Nonetheless, our approach yields the smallest error demonstrating that our calibration strategy further increases robustness to unseen sensor configuration. 

%We attribute these performance improvements to the scene parser, robust dense key point matching strategy and the two-stage extrinsic matrix optimization. Surprisingly, as observed in Fig. \ref{fig.SoTAVisualization}, MobileSAM can effectively segment both RGB and LIP images captured in challenging conditions, such as dark underground parking garages or during rainy days, where the objects are even unrecognizable to human observers.

\noindent \textbf{Evaluation on MIAS-LCEC Dataset.} The evaluation of the MIAS-LCEC dataset is divided into two groups, one on the MIAS-LCEC-TF70 (point clouds are recorded by Livox Mid-70 solid-state LiDAR) and MIAS-LCEC-TF360 (point clouds are recorded by Livox Mid-360 solid-state LiDAR). 

%Point clouds in MIAS-LCEC-TF70 are dense and have similar FoV with the camera RGB images. 
The results shown in Table \ref{tab.rescmp_mias_tf70} demonstrate that our method outperforms all other correspondence-based approaches on a total of 60 scenarios, all captured using a Livox Mid-70 LiDAR. Specifically, our method reduces $e_r$ by around 30-93\% and decreases $e_t$ by 39-99\%, compared to existing SoTA algorithms. We attribute these performance improvements to the dense key point matching pipeline based on the scene parser, which leverages LVMs for environmental perception and constructs semantic structural information and spatial loss for reliable dense key point matcing, thereby improving the quality of the PnP solutions. 

Additionally, the results on the MIAS-LCEC-TF360 dataset somewhat exceed our expectations. From Table \ref{tab.rescmp_mias_tf360}, it is evident that while the other approaches achieve poor performance on this dataset, our approach demonstrates excellent performance, indicating strong adaptability to more challenging scenarios, with narrow overlapping areas between LiDAR projection images and camera RGB images. This performance improvement is primarily attributed to our scene parser, which generates multiple virtual cameras to interpret the calibration scene by extracting semantic and spatial cross-modal features. As illustrated in Figs. \ref{fig.LIPimage} and \ref{fig.mid360}, the LIP image generated by DVL contains numerous holes and has a significantly different FoV compared to the RGB image, resulting in unexpected false correspondence matches, which can deteriorate the algorithm's efficiency and robustness. In contrast, MIAS-LCEC can generate LIP images that look as if taken from the same perspective to the actual camera, thus improving the performance of cross-modal mask matching. 

\noindent \textbf{Adaptability to sparse point clouds.} To investigate the adaptability of our method to sparse point clouds, we conducted calibration experiments on the MIAS-LCEC dataset with varying levels of downsampling (from 100\% to 20\%). As shown in Fig. X, as the sparsity of the point clouds increases, the decline in the accuracy of our proposed algorithm is notably slower compared to other algorithms. This indicates that our method demonstrates significantly better adaptability to sparse point clouds than the alternatives.



\subsection{Ablation Study and Analysis}

\begin{table}[t!]
\caption{Ablation study of different components: Multi-View Optimization (intensity and depth) and Multi-Scene Optimization.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{ccc|cc}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\makecell{Multi-View \\ (intensity)} & \makecell{Multi-View \\ (depth)} & {Multi-Scene} & {$e_r$ ($^\circ$)} &  {$e_t$   (cm)}\\

\hline
\hline
\checkmark &  & &0.931 &0.243	\\
 & \checkmark & &0.677 &0.251	\\
 \checkmark & \checkmark & &0.412 &0.157	\\
\checkmark &  & \checkmark&0.316 &0.105	\\
 & \checkmark & \checkmark&0.339 &0.098	\\
 \checkmark & \checkmark & \checkmark&0.288 
 &0.087	\\

\bottomrule
\end{tabular}
\label{tab.rescmp_mias}
\end{table}

We analyze the impact of different calibration stages in Table. \ref{x}. The point cloud in the KITTI dataset is too sparse that few effective correspondences can be found in a single pair of data. It is evident that the multiple virtual cameras generated by the scene parser improve the calibration accuracy to a great extent.


\section{Conclusion}
\label{sec.method}

\clearpage

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{X_suppl}

\end{document}
