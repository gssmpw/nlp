\documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[review]{cvpr}      % To produce the REVIEW version
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\usepackage{array}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage[table]{xcolor}
\usepackage{algorithm}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}
\title{Environment-Driven Online LiDAR-Camera Extrinsic Calibration}
\author{Zhiwei Huang\\
Tongji University\\
Shang Hai\\
{\tt\small zhiwei.huang@outlook.com}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\egi}{\textit{e.g.}}
\newcommand{\settablefont}{\fontsize{6.9}{11.8}\selectfont}
\definecolor{orangea}{RGB}{252, 245, 150} 
\definecolor{orangeb}{RGB}{251, 210, 136}    
\definecolor{orangec}{RGB}{255, 156, 115} 
\definecolor{oranged}{RGB}{255, 69, 69} 
\newcommand{\clr}{\textcolor{red}}
\newcommand{\clb}{\textcolor{blue}}

\begin{document}

\maketitle

\begin{strip}
    \centering
    \vspace{-4em}
    \includegraphics[width=0.99999\textwidth]{figs/CoverV2.pdf}
    \captionof{figure}{
		Visualization of Calibration Results: We fused 100 frames of point clouds from the KITTI 00 sequence, resulting in the composite point cloud shown in the figure. Based on the calibration results, each zoomed-in section illustrates the alignment between the camera images and the projected point clouds. As can be observed, the point clouds and images align perfectly under our calibration method, demonstrating the precision of our approach.
		}
		\label{fig.cover}
\end{strip}

\begin{abstract}
\clr{LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles. Offline, target-based approaches have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments. The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches. Extensive experiments conducted on these real-world datasets demonstrate the robustness of our approach and its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view. Our toolbox and datasets are publicly available at \url{https://mias.group/MIAS-LCEC}.}
\end{abstract}

\section{Introduction}
\label{sec.intro}

\clb{We have long envisioned robots with human-like intelligence, enabling them to understand, adapt to, and positively impact the world \cite{arnold2019survey,bai2022transfusion,ai2023lidar}. This dream is becoming increasingly attainable with the advent of LiDAR-camera data fusion systems. LiDARs provide accurate spatial information, while cameras capture rich textural details \cite{zhiwei2024lcec}. The complementary strengths of these two modalities, when fused, provide robots with powerful environmental perception capabilities.} 
\clr{applicable in fields such as autonomous driving, robot navigation, and virtual reality \cite{fan2020sne,cui2021deep,fan2018road,li2022deepfusion,li2024roadformer,zhang2023fs,huang2024roadformer+,fan2019pothole}.} \clb{LiDAR-camera extrinsic calibration (LCEC), which estimates the rigid transformation between the two sensors, is a core and foundational process for effective data fusion.}

We have always envisioned robots possessing intelligence comparable to that of humans, enabling them to understand, adapt to, and benefit the world \cite{arnold2019survey,fan2023autonomous,bai2022transfusion,ai2023lidar}. With the advent of multimodal sensor fusion systems like LiDAR-camera combinations, this dream is becoming increasingly attainable. LiDAR provides rich and precise spatial information, while cameras capture detailed, textured content of the environment. The complementary strengths of these modalities, when fused, endow robots with powerful perception capabilities, applicable in fields such as autonomous driving, robot navigation, and virtual reality \cite{fan2020sne,cui2021deep,fan2018road,li2022deepfusion,li2024roadformer,zhang2023fs,huang2024roadformer+,fan2019pothole}. In such contexts,  LiDAR-camera extrinsic calibration (LCEC) plays an essential role, which aims to find a rigid transformation between the two sensors to align the LiDAR point cloud to the camera image. 

Current LiDAR-camera extrinsic calibration methods are primarily categorized as either target-based or target-free. Offline, target-based approaches \cite{cui2020acsc,yan2023joint,fan2023autonomous,kang2022accurate} have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. In contrast, online, target-free approaches \cite{koide2023general,yuan2021pixel,ma2021crlf,pandey2012automatic,tang2023robust} aim at overcoming this problem, \egi, by matching cross-modal features within the shared field of view of the sensors or by \clr{inferring} the extrinsic \clr{transform from the sensor motion}. Although these approaches are generally more widely applicable as \clr{they enable sensor calibration from normal robot operation}, they either require dense point clouds or rely on \clr{consecutive frames of LiDAR point clouds and camera images} to estimate accurate sensor motion, which largely \clr{declines} the applicability of the algorithm. Overall, current calibration algorithms \clr{passively estimate extrinsic parameters based on input data and predefined strategies, making it difficult to cope with complex and dynamic environmental changes} \cite{zhiwei2024lcec}. \clr{Thus, a crucial question remains to be addressed: How to improve the generalization ability of LiDAR-camera extrinsic calibration?}

\clr{We found inspiration for solving this issue in the Two Streams Hypothesis \cite{goodale1992seperate} proposed by Royal Society Fellows Melvyn A. Goodale and David Milner.} \clr{Two Streams Hypothesis argues that human visual systems possess ventral and dorsal pathways. The ventral pathway leads to the temporal lobe, which is involved with object identification and recognition. The dorsal one leads to the parietal lobe, which is involved with processing the spatial location relative to the viewer.} \clr{It suggests that humans make decisions by organically integrating the semantic and spatial-temporal content present in the surroundings, driven by the environment and informed by subjective intent.} Building on this insight, this paper introduces a new definition for LiDAR-camera extrinsic calibration, referred to as "environment-driven". Imagine a self-driving car navigating through complex city streets with such an environment-driven algorithm that mimics human intuition and judgment. It can quickly identify environmental features amidst flickering lights and rapidly moving objects, making autonomous decisions based on these cues. This level of intelligence improves sensor accuracy and greatly enhances system robustness.

Motivated by the above discussion, we developed a novel environment-driven LiDAR-camera extrinsic calibration (EdO-LCEC) approach. As illustrated in Fig. \ref{fig.cover}, our central insight is that multiple frames of input sensor data can be interpreted as distinct scenes that, when combined, provide a holistic view of the environment. A prerequisite for the success of EdO-LCEC is the design of a generalizable scene discriminator. The scene discriminator employs LVMs to predict depth images from the camera RGB images and perform instance segmentation on camera images, LiDAR intensity projections (LIP), and LiDAR depth projections (LDP), thereby achieving an understanding of the semantic and spatial dimensions. Then it calculates the feature density of the calibration scene and uses this information to guide the generation of multiple virtual cameras for projecting the point clouds, aiming to capture more available environmental features. This design improves upon the virtual camera generation method described in \cite{koide2023general,zhiwei2024lcec}, significantly enhancing the ability to extract cross-modal features and effectively increasing calibration adaptability in scenarios with limited shared sensor fields of view or sparse point clouds.

For each scene, we perform dual-path correspondence matching (DPCM). Inspired by the Two Streams Hypothesis, DPCM incorporates both ventral and dorsal pathways. The ventral pathway handles correspondence matching between RGB and LIP images, while the dorsal pathway manages correspondence between depth and LDP images. This dual-path architecture integrates spatial and textural modalities by constructing a matching cost matrix based on texture and local structural features, guided by precise semantic and depth priors. This approach further improves cross-modal feature association in sparse scenes, overcoming the previous reliance of algorithms \cite{yuan2021pixel,pandey2015automatic,ma2021crlf} on uniformly distributed geometric or textural features. Finally, the obtained correspondences serve as inputs for the proposed spatial-temporal relative pose optimization to derive and refine the extrinsic matrix. The optimization process encompasses two critical components: multi-view and multi-scene optimization. The former constructs a loss function based on depth-normalized reprojection errors to find the optimal extrinsic parameter estimates for a single scene. The latter employs the results of the multi-view approach to jointly optimize extrinsic parameter calculations across multiple scenes, further enhancing calibration accuracy. Through extensive experiments conducted on three real-world datasets, the proposed EdO-LCEC demonstrates superior robustness and accuracy compared to other SoTA online, target-free approaches.

To summarize, our novel contributions are as follows:

\begin{itemize}
    \item {EdO-LCEC, an environment-driven, online LCEC approach that mimics human visual systems to actively analyze the surroundings, achieving accurate, robust calibration.}
    \item {Generalizable Scene Discriminator, which can automatically perceive the calibration scene and capture potential spatial and semantic features by leveraging SoTA LVMs.}
    \item {Dual-Path Correspondence Matching (DPCM), a novel cross-modal feature matching algorithm, capable of generating dense and reliable correspondences between LiDAR point cloud and camera image.}
    \item {Spatial-temporal relative pose optimization, enabling high quality extrinsic estimation through multi-view and multi-scene optimization.}
\end{itemize}

\clearpage
\section{Related Work}
\label{sec.relate_works}
\noindent \textbf{Traditional target-free methods.} Most traditional target-free LCEC approaches, such as \cite{lv2015automatic, castorena2016autocalibration,pandey2012automatic, pandey2015automatic}, estimate the relative pose between the two sensors by aligning the cross-modal edges or mutual information (MI) extracted from LiDAR projection and camera RGB image. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edge features, which can compromise calibration robustness. Moreover, the use of low-level image processing algorithms, such as Gaussian blur and the Canny operator, can introduce errors in edge detection, potentially fragmenting global lines and thus reducing overall calibration accuracy. 
On the other hand, studies \cite{zhang2023overlap,yin2023automatic} have begun to address the challenges of cross-modal feature matching by developing motion-based methods. These approaches match trajectories from visual and LiDAR odometry to derive extrinsic parameters through optimization. While they effectively accommodate heterogeneous sensors without requiring overlap, they demand precise time synchronization. Moreover, this calibration framework necessitates multiple LiDAR point clouds and camera images to accurately estimate per-sensor motion, which further limits its applicability in diverse environments.

\noindent \textbf{Learning-based Methods.} Advances in deep learning techniques have driven significant exploration into enhancing traditional target-free algorithms. Several end-to-end deep learning-based algorithms \cite{borer2024chaos, schneider2017regnet,lv2021lccnet,shi2020calibrcnn,zhao2021calibdnn,iyer2018calibnet}, tried to covert calibration process into more direct approaches by exploiting deep learning-based correspondences between RGB images and LiDAR point clouds. On the other hand, some research tries to attach deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, a recent study \cite{koide2023general} introduced Direct Visual LiDAR Calibration (DVL), a novel point-based method that utilizes SuperGlue \cite{sarlin2020superglue} to establish direct 3D-2D correspondences between LiDAR and camera data. Additionally, this study refines the estimated extrinsic matrix through direct LiDAR-camera registration by minimizing the normalized information distance, a mutual information-based cross-modal distance measurement loss. While these methods have demonstrated effectiveness on large-scale datasets like KITTI \cite{geiger2012we}, which primarily focuses on urban driving scenarios, their performance has not been extensively validated on other types of real-world datasets. Furthermore, their dependence on pre-defined sensor configurations (both LiDAR and camera) poses implementation challenges. In comparison, our approach improved environmental adaptability by integrating scene discriminator, enabling calibration to perform effectively in diverse and challenging real-world environments.

\clearpage
\section{Method}
\label{sec.method}

%\begin{figure*}
%    \centering
%    \includegraphics[width=1\linewidth]{figs/FrameWork_V1.pdf}
%    \caption{Enter Caption}
%    \label{fig.framework_overview}
%\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/FrameWork_V3.pdf}
    \caption{The pipeline of the proposed EdO-LCEC. Our method adheres to the human visual system, which consists of ventral and dorsal pathways. The dorsal pathway is used to project depth of LiDAR point cloud and estimate depth image of RGB image. The ventral pathway handles LiDAR intensity projection and image segmentation.
    %We first analyze the feature density of a calibration scene and generate multiple intensity virtual cameras and depth virtual cameras to project the LiDAR point cloud. Then, in the dorsal path, the scene discriminator utilize Depth-AnythingV2 to infer the depth image of the RGB image.
    }
    \label{fig.framework_overview}
\end{figure*}

%\subsection{Preliminaries}
%\label{sec.pre}
Given LiDAR point clouds stream and camera images stream, our goal is to estimate their extrinsic matrix $^{C}_{L}\boldsymbol{T}$, defined as follows \cite{zhao2023dive}:
\begin{equation}
{^{C}_{L}\boldsymbol{T}} = 
\begin{bmatrix}
{^{C}_{L}\boldsymbol{R}} & {^{C}_{L}\boldsymbol{t}} \\
\boldsymbol{0}^\top & 1
\end{bmatrix}
\in{SE(3)},
\label{eq.lidar_to_camera_point}
\end{equation}
where $^{C}_{L}\boldsymbol{R} \in{SO(3)}$ represents the rotation matrix, $^{C}_{L}\boldsymbol{t}$ denotes the translation vector, and $\boldsymbol{0}$ represents a column vector of zeros. 
%In this article, the symbols in the superscript and subscript denote the source and target sensors, respectively. 

We give an overview of the proposed method, as shown in Figure \ref{fig.framework_overview}. It mainly contains three main stages: 1) Given a stream of point clouds and images, we utilize a scene discriminator to understand the environment through image segmentation and depth estimation, generating virtual cameras that project LiDAR intensity and depth from multiple viewpoints (Section \ref{sec.scene_discriminator}). 2) The results from image segmentation are processed along two pathways, which are then fed into dual-path correspondence matching to establish dense cross-modal correspondences (Section \ref{sec.dualpath_matching}). 3) The correspondences obtained serve as inputs for our proposed spatial-temporal relative pose optimization, which is used to derive and refine the extrinsic matrix (Section \ref{sec.st_optimization}).

%It mainly contains three stages: 1) Given a point clouds stream and an images stream, we apply a scene discriminator to comprehend the environment by image segmentation and depth estimation, and generate virtual cameras to project LiDAR intensity and depth from multi-view (Section \ref{sec.scene_discriminator}). 2) The image segmentation results will be divided into two pathways, sent to dual-path correspondence matching to obtain dense cross-modal correspondences (Section \ref{sec.dualpath_matching}). 3) The obtained correspondences serve as inputs for the proposed spatial-temporal relative pose optimization to derive and refine the extrinsic matrix (Section \ref{sec.st_optimization}).

\subsection{Generalizable Scene Discriminator} 
\label{sec.scene_discriminator}

\clr{Inspired by the human perception system, our calibration algorithm incorporates environmental awareness}. It employs a generalizable scene discriminator which incorporates LVMs, to interpret the surroundings by generating virtual cameras to project LiDAR point cloud intensities and depth. The discriminator configures the first intensity virtual camera and depth virtual camera at the LiDAR perspective. This generates a LiDAR intensity projection (LIP) image ${{^V_I}\boldsymbol{I}} \in{\mathbb{R}^{H\times W \times 1}}$ ($H$ and $W$ represent the image height and width) and a LiDAR depth projection (LDP) image ${{^V_D}\boldsymbol{I}}$. To align with the LDP image, the input camera RGB image ${{^C_I}\boldsymbol{I}}$ is processed using Depth Anything V2 to obtain estimated depth images ${^C_D}\boldsymbol{I}$\footnote{In this article, the symbols in the left superscript denote the type of target camera ($V$ denotes virtual camera and $C$ indicates real camera), and the left subscript denotes the source of the image ($D$ is depth and $I$ is intensity).}.  To take advantage of semantic information, we utilize MobileSAM as the image segmentation backbone. The series of detected masks in an image is defined as $\{\mathcal{M}_1, \dots, \mathcal{M}_n\}$. The corner points along the contours of masks detected are represented by $\{\boldsymbol{c}_1, \dots, \boldsymbol{c}_{m_i}\}$, where $m_i$ is the total corner points number in the $i$-th mask. An instance (bounding box), utilized to precisely fit around each mask, is centrally positioned at $\boldsymbol{o}$ and has a dimension of $h\times w$ pixels. We define the depth of a key point as $d$, and $\boldsymbol{D} \in \mathbb{R}^{b\times b}$ as the matrix that stores depth values in its k-neighborhood. To fully leverage the structured information within the masks, we construct a convex polygon from the corner points. As depicted in Fig. \ref{fig.framework_overview}, the neighboring vertices of a corner point $\boldsymbol{c}$ is defined as $\{\boldsymbol{e}^{\boldsymbol{c}}_1, \dots, \boldsymbol{e}^{\boldsymbol{c}}_k\}$. 

For each virtual image or camera image ${\boldsymbol{I}}$, the scene discriminator calculates its feature density $\rho({\boldsymbol{I}})$ to provide valuable hints for feature extraction and correspondence matching:
\begin{equation}
\rho({\boldsymbol{I}}) = \underbrace {\bigg( \frac{1}{n}\log{\sum_{i=1}^{n}{\frac{m_i}{\lvert \mathcal{M}_i\rvert}}}\bigg )}_{\rho_d} \underbrace {\bigg(\sum_{i=1}^{n}\log{\frac{\lvert \bigcap_{j=1}^{n} \mathcal{M}_j \rvert}{ \lvert \mathcal{M}_i\rvert} }\bigg)}_{\rho_v},
\end{equation}
where $\rho_d$ denotes the spatial density and $\rho_v$ represents the semantic density. 

Due to the occlusion issue brought by the different perspective views of LiDAR and the camera \cite{yuan2021pixel,zhiwei2024lcec} and the lack of adequate features when the point cloud is sparse, a single pair of virtual cameras is not enough for a comprehensive perception of the calibration scene. Define the event $E$ of capturing enough effective features, with its probability given by $P(E) = \lambda$. If we have $n_I$ intensity virtual cameras and $n_D$ depth virtual cameras, the probability of capturing enough effective features is $1 - (1 - \lambda)^n$. In theory, as $n$ increases, this probability will also increase. When $ n \to \infty $, $P(E')^n \to 0 $, there is $1 - (1 - \lambda)^n \to 1$. Generating more virtual cameras will enhance the possibility of capturing more potential correspondences, thus leading to higher calibration accuracy. However, it is impossible to apply an infinite number of virtual cameras during the calibration. Considering the trade-off between calibration accuracy and compute resources, we set the number of multiple virtual cameras to satisfy the balance of feature density:
\begin{equation}
\rho({^C_I}\!\boldsymbol{I}) + \rho({^C_D}\boldsymbol{I}) = \sum_{i=1}^{n_I}\rho({^C_I}\!\boldsymbol{I}_{i}) + \sum_{i=1}^{n_D}{\rho}({^V_D}\boldsymbol{I}_i).
\end{equation}
In practical applications, we set multiple virtual cameras inside a sphere originating from the LiDAR perspective center. Since the feature density is similar if the perspective viewpoint is close to the initial position, we can assume that $\rho(\boldsymbol{I}^V_{I, i}) \approx \rho(\boldsymbol{I}^V_{I,0})$ and $\rho(\boldsymbol{I}^V_{D, i}) \approx \rho(\boldsymbol{I}^V_{D,0})$. So $n_I$ and $n_D$ can be obtained by
\begin{equation}
n_I = \frac{\rho(\boldsymbol{I}^C_{I})}{\rho(\boldsymbol{I}^V_{I, 0})}, n_D = \frac{\rho(\boldsymbol{I}^C_{I})}{\rho(\boldsymbol{I}^V_{D, 0})}.
\end{equation}
Once all virtual cameras are generated, the discriminator performs image segmentation on each LiDAR projection captured from multiple views, detecting the corner points of the masks. These corner points serve as inputs for the dual-path correspondence matching.
%In the mask generation stage, EDI adjusts the appropriate IoU threshold $P_m$ for mask segmentation based on the feature density, which can be determined as follows:
%\begin{equation}
%P_m = \lambda + (1-\lambda)\frac{m_I + m_D}{m_I + m_D + 2m_C},
%\end{equation}
%where $\lambda$ is a robust parameter (usually determined by the pre-trained network) that ensures the reliability of the mask segmentation.
%To efficiently establish correspondences between LiDAR point clouds and camera images, previous methods typically configure the virtual camera with a relative transformation ${^{V}_{L}\boldsymbol{T}}$ from LiDAR as follows:

%This generates a LiDAR intensity projection (LIP) image $\boldsymbol{I}^L \in{\mathbb{R}^{H\times W \times 1}}$, framing the LCEC problem as a 2D feature matching task, where $H$ and $W$ represent the image height and width, respectively. However, due to the image distortion caused by varying perspectives and the absence of depth information in point clouds, this projection strategy struggles to capture a sufficient number of matchable features, particularly when using sparse LiDAR point clouds, such as those from mechanical LiDARs.
\subsection{Dual-Path Correspondence Matching}
\label{sec.dualpath_matching}

Given the segmented masks and detected corner points, dual-path correspondence matching leverages them to achieve dense and reliable 3D-2D key point correspondences. Similar to human perception systems, DPCM consists of two pathways, one for correspondence matching of LIP and RGB image, and the other for LDP and depth image.  For each pathway, DPCM adopted the approach outlined in \cite{zhiwei2024lcec} to obtain mask matching result $\mathcal{S} = \{(\mathcal{M}^V_i, \mathcal{M}^C_i)\}_{i = 1}^{m}$. Each matched mask pair $(\mathcal{M}^V_i,\mathcal{M}^C_i)$ can estimate an affine transformation matrix $[s\boldsymbol{R}_A,\boldsymbol{t}_A]$, which serves as a reference for dense key point matching. 
%Detailed derivation for $s\boldsymbol{R}_A$ and $\boldsymbol{t}_A$ is explained in \textcolor{blue}{Supplement} \ref{sec.sup_algo_detail}. 
Based on this affine transform, the corner points in the virtual image can be updated to a location that is close to its true projection coordinate in the real camera perspective using:
\begin{equation}
 \hat{\boldsymbol{c}}_i = s\boldsymbol{R}_A{(^V\!\boldsymbol{c}_i)} + \boldsymbol{t}_A.
\end{equation}
To determine optimum key point matches, we construct a cost matrix $M^C$, where the element at $\boldsymbol{x} = [i,j]^\top$, namely:
\begin{equation}
\begin{aligned}
&\boldsymbol{M}^{C}(\boldsymbol{x}) = \\
&\rho_v\underbrace{\bigg(1-  \exp(\frac{\left\|^V\!\hat{\boldsymbol{c}}_i - ^C\!\!\boldsymbol{c}_j\right\|_2}{L(\mathcal{S})}) 
   + {\sum_{k = 1}^{N}{H(\boldsymbol{e}^{(^V\!\boldsymbol{c}_i)}_{k},\boldsymbol{e}^{(^C\!\boldsymbol{c}_j)}_{k}})}}_{\mathbf{Ventral}}\bigg) \\
&+ \rho_d\underbrace{\bigg(\frac{1}{b^2}\sum_{r = 1}^{b}\sum_{s = 1}^{b}{\left |\boldsymbol{D}^V_{i}(r,s)-\boldsymbol{D}^C_{j}(r,s) \right |}\bigg)}_{\mathbf{Dorsal}}
\end{aligned}
\label{eq.adaptive_cost_func}
\end{equation}
denotes the matching cost between the $i$-th key point of a mask in the LiDAR virtual image and the $j$-th key point of a mask in the camera image. \ref{eq.adaptive_cost_func} consists of ventral and dorsal components. The ventral components measure the structural difference of two corner points in the virtual and real image, where $L(\mathcal{S}$ denotes the average perimeter of the matched masks and $H(\boldsymbol{e}^{(^V\!\boldsymbol{c}_i)}_{k},\boldsymbol{e}^{(^C\!\boldsymbol{c}_j)}_{k}$ represents the structural similarity between current and target corner point. The dorsal component derives the relative spatial similarity of the $b$ neighboring zone. A strict criterion is applied to achieve reliable matching. Matches with the lowest costs in both horizontal and vertical directions of $\boldsymbol{M}^{C}(\boldsymbol{x})$ are determined as the optimum mask matches. Since every $^V\!\boldsymbol{c}_i$ can trace back to a LiDAR 3D point $^L\!\boldsymbol{p}_i = [x,y,z]^\top$, and every $^C\!\boldsymbol{c}_i$ is related to a pixel $\boldsymbol{p}_i = [u,v]^\top$ in the camera image, the final correspondence matching result of DPCM is:
\begin{equation}
\mathcal{C} = \{(^L\!\boldsymbol{p}_i,\boldsymbol{p}_i) \mid i = 1,\dots,N\}
\end{equation}

\subsection{Spatial-temporal relative pose optimization}
\label{sec.st_optimization}
%Based on DPM, we can obtain dense key point correspondences $\mathcal{K} = \{(\boldsymbol{p}_{k,1},\boldsymbol{p}_{k,1}^L), \dots, (\boldsymbol{p}_{k,N},\boldsymbol{p}_{k,N}^L) \}$, which store 2D pixels in the RGB image captured by camera and the corresponding 3D LiDAR points, respectively. 
%This list is ordered by confidence coefficients, from which the algorithm selects the $n_I$ and $n_D$ vectors with the lowest confidence as the final reference vectors, denoted as $\mathcal{V}_I = \{^{{I}}\boldsymbol{v}_1,\dots,^{{I}}\boldsymbol{v}_{n_I}\}$ and $\mathcal{V}_D = \{{^{{D}}\boldsymbol{v}_1},\dots,{^{{D}}\boldsymbol{v}_{n_D}}\}$. The poses of the virtual cameras can then be calculated as follows:
%\begin{equation}
%\left\{
%\begin{aligned}
%^{I}_{L}\boldsymbol{T}_i &= {^{V}_{L}\boldsymbol{T}} + {^{{I}}\boldsymbol{v}_i}, i \in [1,n_I],\\
%^{{D}}_{L}\boldsymbol{T}_j &= {^{V}_{L}\boldsymbol{T}} + {^{{D}}\boldsymbol{v}_j}, j \in [1,n_D],
%\end{aligned}
%\right.
%\label{eq.virtual_camera_pos}
%\end{equation}
%where $^{I}_{L}\boldsymbol{T}_i$ and $^{D}_{L}\boldsymbol{T}_j$ is the pose of $i$-th and $j$-th intensity virtual camera and depth virtual camera. The physical significance of (\ref{eq.virtual_camera_pos}) is to position the virtual camera as close as possible to the real camera, thereby increasing the overlap of features from LiDAR projections and camera images.

In our framework, optimization of the extrinsic matrix includes two stages, multi-view optimization and multi-scene optimization. 

\noindent \textbf{Multi-View Optimization.}
In the multi-view optimization, we assume that in each calibration scene $S_i$, each virtual camera perspective can estimate an extrinsic matrix using a PnP solver. 
%Defining that in each perspective, the matching result obtained by the DPCM is $\{( {^L\boldsymbol{p}_{i,j}}, \boldsymbol{p}_{i,j})\}_{j = 1}^{k_i}$, where $k_i$ is the total matching pairs number at this perspective. 
Then extrinsic matrix ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ of this scenario can be formulated as follows:
\begin{equation}
{^{C}_{L}\hat{\boldsymbol{T}}_i} = \underset{^C_L{\boldsymbol{T}_{i}}}{\arg\min} 
\sum_{n = 1}^{n_I + n_D}\underbrace {\sum_{(^L\!\boldsymbol{p}_i,\boldsymbol{p}_i)\in\mathcal{C}_n}{w_{i}\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{i}}
 \boldsymbol{p}_{i}^L) - {\boldsymbol{p}}_{i}\right\|_2}}_{\epsilon_n}, 
\end{equation}
where $w_{i}$ is the matched point pair weight of $({^L\boldsymbol{p}_{i}}, \boldsymbol{p}_{i})$, which is defined as follows:
\begin{equation}
w_{i} = \frac{G(d_{i})}
{C + \left\|\pi({^{C}_{L}\boldsymbol{T}_{i}}\boldsymbol{p}_{i}^L) - {\boldsymbol{p}}_{i}\right\|_2}.
\label{eq.match_point_weight}
\end{equation}
In (\ref{eq.match_point_weight}) $d_{i}$ is the depth of point $\boldsymbol{p}_{i}^L$ and 
$d_{max}$ is the maximum depth value of all the key point correspondences. 

\begin{equation}
G(d_{i}) = e - \exp{ \frac{(  -\frac{ d - d_{\min} }{ d_{\max} - d_{\min} } - \bar{d}^2 )}{{2\bar{d_g}}^2}}. 
\end{equation}

\noindent \textbf{Multi-Scene Consistency Loss.} Using the estimated ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ at each scenario, we choose a subset  $\{({^L\boldsymbol{p}_{i,j}}, \boldsymbol{p}_{i,j})\}_{i=0}^{s}$ of the matched point pairs as the input of the multi-scene calibration. The multi-scene optimization solves the final extrinsic matrix ${^{C}_{L}{\boldsymbol{T}}^*}$ by optimizing a robustified non-linear squares problem of the form:
\begin{equation}
\mathcal{L}({^{C}_{L}\hat{\boldsymbol{T}}_i}) = 
\sum_{n = 1}^{S} {\sum_{m=1}^{N}{\rho(w_{n,m}\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{i}}
 \boldsymbol{p}_{n,m}^L) - {\boldsymbol{p}}_{n,m}\right\|_2})}.
\end{equation}
where $\rho$ denotes the robustifier. We utilize the Cauchy loss as a robustifier due to its high tolerance to outliers in the observations [x], resulting in a significant increase in calibration accuracy.


\section{Experiment}
\label{sec.experiment}


\subsection{Experimental Setup and Evaluation Metrics}
\label{sec.exp_setup}




\begin{table*}[t!]
\caption{Quantitative comparisons with SoTA target-free LCEC approaches on the KITTI Odemotry 00 sequence. The best results are shown in bold type.}
\centering
\fontsize{6.5}{10}\selectfont
\begin{tabular}{l|c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c}
\toprule
\multirow{3}*{Approach}& \multirow{3}*{Initial Range}&\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\cline{3-18}
&&\multicolumn{2}{c}{Magnitude}
&\multicolumn{3}{c}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c}{Magnitude}
&\multicolumn{3}{c}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\

&& $e_r$ ($^\circ$) & $e_t$ (m) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (m)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}\\
\hline
\hline
Borer et al. \cite{borer2024chaos}&$\pm1^\circ / \pm 0.25m$  &0.455 &0.095	&0.100 &0.440 &0.060  &0.037 &0.030 &0.082 &- &- &-	&-	&-	&-	&-	&-\\
LCCNet \cite{lv2021lccnet} &$\pm10^\circ / \pm 1.0m$ &1.418
&0.600  &0.455 &0.835 &0.768 &0.237 &0.333  &0.329 &1.556  &0.718 & 0.457 &1.023 &0.763 &0.416 &0.333 &0.337 \\
CalibRCNN \cite{shi2020calibrcnn} &$\pm10^\circ / \pm 0.25m$ &0.805 &0.093	&0.446	&0.640	&0.199	&0.062	&0.043 &0.054 &- &-	&-	&-	&-	&-	&-	&-\\
CalibDNN \cite{zhao2021calibdnn} &$\pm10^\circ / \pm 0.25m$ &1.021 &0.115	&0.200	&0.990	&0.150	&0.055	&0.032	&0.096 &- &-&-	&-	&-	&-	&-	&-\\
RGGNet \cite{yuan2020rggnet} &$\pm20^\circ / \pm 0.3m$ &1.29 &0.114	&0.640 &0.740&0.350 &0.081 &0.028 &0.040 &3.870 &0.235 &1.480 &3.380 &0.510	&0.180 &0.056 &0.061 \\
RegNet \cite{yuan2020rggnet} &$\pm20^\circ / \pm 1.5m$ &0.500 &0.108	&0.240	&0.250	&0.360	&0.070	&0.070	&0.040 &- &-&-	&-	&-	&-	&-	&-\\
CalibNet \cite{iyer2018calibnet} &$\pm10^\circ / \pm 0.2m$ &5.500 &0.130	&3.050 &3.000 &3.000 &0.070 &0.067 &0.086 &5.096 &0.125	&2.881 &2.908 &3.035 &0.064 &0.065 &0.085\\
\hline

CRLF \cite{ma2021crlf} &- &0.631	&8.421	&0.045	&0.458	&0.423	&7.487	&3.098	&0.804 &0.876	&1.594	&0.060	&0.464	&3.636	&0.136 & 0.839 & 0.921\\

UMich \cite{pandey2015automatic} &- &4.161	&0.321	&0.113	&3.111	&2.138	&0.286	&0.077	&0.086
 &4.476 &0.339	&0.137	&3.342	&2.309	&0.291	&0.108	&0.094\\

HKU-Mars \cite{yuan2021pixel} &- &36.70	&89.09	&22.26	&23.51	&21.34	&28.58	&74.96	&33.85 &35.09 &5.294	&16.85	&24.00	&24.44	&3.054	&3.081 &1.495\\


DVL \cite{koide2023general}  &&\textcolor{red}{0.578} &\textcolor{red}{0.136}	&\textcolor{red}{0.876}	&\textcolor{red}{1.594}	&\textcolor{red}{0.060}	&\textcolor{red}{0.464}	&\textcolor{red}{3.636}	&\textcolor{red}{0.136} &\textcolor{red}{5.315} &\textcolor{red}{1.581}	&\textcolor{red}{0.876}	&\textcolor{red}{1.594}	&\textcolor{red}{0.060}	&\textcolor{red}{0.464}	&\textcolor{red}{3.636}	&\textcolor{red}{0.137}\\

CalibAnything \cite{luo2023calib}  &-&\textcolor{red}{0.578} &\textcolor{red}{0.136}	&\textcolor{red}{0.876}	&\textcolor{red}{1.594}	&\textcolor{red}{0.060}	&\textcolor{red}{0.464}	&\textcolor{red}{3.636}	&\textcolor{red}{0.136} &\textcolor{red}{5.315} &\textcolor{red}{1.581}	&\textcolor{red}{0.876}	&\textcolor{red}{1.594}	&\textcolor{red}{0.060}	&\textcolor{red}{0.464}	&\textcolor{red}{3.636}	&\textcolor{red}{0.137}\\
\hline
EdO-LCEC  &- &\textbf{0.288}	&\textbf{0.087}	&\textbf{0.157}	&\textbf{0.139}	&\textbf{0.131}	&\textbf{0.050}	&\textbf{0.053}	&\textbf{0.029} 
&\textbf{0.383} &\textbf{0.132}	&\textbf{0.217}	&\textbf{0.227}	&\textbf{0.139}	&\textbf{0.103}	&\textbf{0.062}	&\textbf{0.030}\\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti00}
\end{table*}

\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on KITTI Dataset (01-08 Sequence): value in the table are rotation errors (degrees) and translation errors (centimeters).}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{01} & \multicolumn{2}{c|}{02} & \multicolumn{2}{c|}{03}  & \multicolumn{2}{c|}{04} & \multicolumn{2}{c|}{ 05} &  \multicolumn{2}{c|}{06}&  \multicolumn{2}{c|}{07} & \multicolumn{2}{c}{08} \\
\cline{2-17}
& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$  (m)& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$ (m)& $e_r$($^\circ$)  & $e_t$ (m)\\
\hline
\hline
CRLF \cite{ma2021crlf}  &0.626&13.38 &0.620
&1.764 &0.840&3.958 &0.596
&0.307 &0.609
&4.724 &0.684&18.60
&0.599&0.531 &0.611&3.190 \\
UMich \cite{ma2021crlf}  &2.196	&0.305 &3.733
&0.331	&3.201&0.316 &2.086&0.348	&3.526
&0.356 &2.914&0.353	&3.928&0.368 &3.722
&0.367 \\
HKU-Mars \cite{ma2021crlf}  &11.99&0.784	&24.43&3.268 &17.48&1.620	&15.85
&306.9 &37.81&3.313	&16.12&2.540 &31.56&3.097 & 15.31&56.27 \\
DVL \cite{koide2023general}  &1.594&0.136 &1.594&0.136	&1.594&0.136 &1.594&0.136	&1.594&0.136 &1.594&0.136	&1.594&0.136 &1.594&0.137 \\
CalibAnything \cite{luo2023calib}  &\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	&\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	&\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	&\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.137} \\
MIAS \cite{luo2023calib}  &-&- &-&-	&-&- &-&-	&-&- &-&-	&-&-&-&- \\
Ours &1.995 &0.431 &0.695&0.177
&0.754&0.131 &1.064&0.337
&0.263&0.098 &0.432&0.115 &0.188&0.081 &0.322&0.101 \\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti_01_08}
\end{table*}


\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on MIAS-LCEC-TL70 Dataset.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{\makecell{Residential \\ Community}} & \multicolumn{2}{c|}{Urban Freeway} & \multicolumn{2}{c|}{Building}  & \multicolumn{2}{c|}{\makecell{Challenging \\Weather}} & \multicolumn{2}{c|}{Indoor} &  \multicolumn{2}{c|}{\makecell{Challenging \\Illumination}} &  \multicolumn{2}{c}{All} \\
\cline{2-15}
& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
CRLF \cite{ma2021crlf}  &1.594&0.464
&1.582&0.140 &1.499 &20.17 &1.646&2.055
&1.886 &30.05 &1.876 &19.047 &1.683 &11.133 \\
UMich \cite{ma2021crlf}  &4.829
&0.387 &2.267&0.166 &11.914 &0.781 &1.851 &0.310 &2.029  &0.109 &5.012 &0.330 &4.265 &0.333\\
HKU-Mars \cite{ma2021crlf}  &2.695
&1.208 &2.399 &1.956 &1.814 &0.706 &2.578 &1.086  &2.527 &0.246 &14.996 &3.386 &3.941 &1.261
 \\
DVL \cite{koide2023general}  &0.193
&0.063
&0.298
&0.124
&0.200
&0.087
&0.181
&0.052
&0.391
&0.030
&1.747
&0.377
&0.423
&0.100 \\
CalibAnything \cite{luo2023calib}  &\textcolor{red}{1.594} &\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	&\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	&\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136} &\textcolor{red}{1.594}&\textcolor{red}{0.136}	\\

MIAS \cite{zhiwei2024lcec}  &0.190 &0.050 &0.291 &0.111
&0.198&0.072&0.177&0.046&0.363&0.024&0.749&0.118&0.298&0.061 \\
Ours &0.165 &0.044 &0.295 &0.105 &0.235 &0.054  &0.179 &0.046 &0.315 &0.025 &0.474 &0.103 &0.255 &0.055 \\
\bottomrule
\end{tabular}
\label{tab.rescmp_mias_tf70}
\end{table*}

%\begin{table}[t!]
%\caption{Ablation study of different components: Multi-View Optimization (intensity and depth) and Multi-Scene Optimization.}
%\centering
%\fontsize{6.9}{10}\selectfont
%\begin{tabular}{c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
%\toprule
%%&\multicolumn{9}{c}{KITTI Sequence} \\
%\multirow{2}*{\makecell{Multi-View \\ (intensity)}} & \multirow{2}*{\makecell{Multi-View \\ (depth)}} & \multirow{2}*{Multi-Scene} &\multicolumn{2}{c|}{KITTI}  & \multicolumn{2}{c|}{MIAS-LCEC-TF70} &  \multicolumn{2}{c}{MIAS-LCEC-TF360} \\
%\cline{4-9}
%&&& {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)}\\
%
%\hline
%\hline
%\checkmark &  & &0.931 &0.243 &0.931 &0.243 &0.931 &0.243	\\
% & \checkmark & &0.677 &0.251&0.931 &0.243 &0.931 &0.243	\\
% \checkmark & \checkmark & &0.412 &0.157	&0.931 &0.243 &0.931 &0.243\\
%\checkmark &  & \checkmark&0.316 &0.105	&0.931 &0.243 &0.931 &0.243\\
% & \checkmark & \checkmark&0.339 &0.098	&0.931 &0.243 &0.931 &0.243\\
% \checkmark & \checkmark & \checkmark&0.288 &0.931 &0.243 &0.931 &0.243
% &0.087	\\
%
%\bottomrule
%\end{tabular}
%\label{tab.rescmp_mias}
%\end{table}
In our experiments, we compare our proposed method with SoTA traditional target-free and SoTA target-free LCEC approaches on the public dataset KITTI Odometry and MIAS-LCEC real-world dataset. 
%The learning-based methods include CalibNet, CalibDNN, CalibRCNN, RegNet, RGGNet, and LCCNet. The correspondence-based methods include HKU-Mars, UMich,  DVL, CRLF and Calib-Anything.
%In our experiments, we compare our proposed method with traditional target-free and learning-based SoTA target-free LCEC approaches on the MIAS-LCEC dataset (collected using a Livox Mid-70 LiDAR and a MindVision SUA202GC camera, from a variety of indoor and outdoor environments), and the large-scale public dataset KITTI Odometry (collected by two RGB cameras Point Grey Flea 2 and one LiDAR of type Velodyne HDL-64E. Sensor data is captured at 10 Hz). The learning-based methods include CalibNet, CalibDNN, CalibRCNN, RegNet, RGGNet, and LCCNet. The correspondence-based methods include HKU-Mars, UMich,  DVL, CRLF and Calib-Anything.
To comprehensively evaluate the performance of LCEC approaches, we use the magnitude of Euler angle error, with the following expression:
\begin{equation}
    e_r =  \left\|\boldsymbol{r}^* - \boldsymbol{r}\right\|_2,
\end{equation}
where ${\boldsymbol{r}^{*}}$ and ${\boldsymbol{r}}$ represent the estimated and ground-truth Euler angle vectors, computed from the rotation matrices ${^{C}_{L}\boldsymbol{R}}^*$ and ${^{C}_{L}\boldsymbol{R}}$, respectively, and the magnitude of translation error, with the following expression\footnote{The translation from LiDAR pose to camera pose is $-{{^{C}_{L}\boldsymbol{R}}^{-1}}\boldsymbol{t}$ when (\ref{eq.lidar_to_camera_point}) is used to depict the point translation.}:
\begin{equation}
    e_t= \left\|-{({^{C}_{L}\boldsymbol{R}^*})^{-1}}\boldsymbol{t}^* +{{^{C}_{L}\boldsymbol{R}}^{-1}}\boldsymbol{t}\right\|_2,
\end{equation}
where $\boldsymbol{t}^*$ and $\boldsymbol{t}$ denote the estimated and ground-truth translation vectors, respectively, are used to quantify the performance of target-free LCEC approaches. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/SparsifyPointCloud.pdf}
    \caption{Evaluation on the adaptability of different methods to sparse point clouds: In the figure, the point cloud density decreases progressively, with $1/x$ representing the level of point cloud downsampling.}
    \label{fig.visualization_sparse_pcloud}
\end{figure*}

\begin{figure}[t!]

    \centering
    \includegraphics[width=1\linewidth]{figs/SparseCloudTable.pdf}
    \caption{Enter Caption}
    \label{fig.exp_on_different_density}
\end{figure}

\subsection{Comparison with State-of-the-Art Method}
\label{sec.exp_dataset}
Quantitative comparisons with SoTA approaches on three datasets are presented in Tables \ref{tab.rescmp_kitti00}-\ref{tab.rescmp_mias}. Additionally, qualitative results are illustrated in Figs. \ref{fig.datafusion_on_checkerboard} and \ref{fig.SoTA_visualization}. Since existing learning-based methods are trained and tested solely on the KITTI dataset and do not provide APIs for user-customized data, we compare our method only with traditional target-free approaches on the MIAS-LCEC dataset.

\begin{table}[t!]
\caption{
Quantitative comparison of our proposed MIAS-LCEC approach with other SoTA online, target-free approaches on the MIAS-LCEC-TF360 dataset, where the best results are shown in bold type.}
\centering
\settablefont
\begin{tabular}{c|rr|rr}
\toprule
Approach & \multicolumn{2}{c|}{Indoor} & \multicolumn{2}{c}{Outdoor}\\
 & $e_r$ ($^\circ$) & $e_t$ (m)  & $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
\hline
CRLF \cite{ma2021crlf}  &\cellcolor{orangea}1.469	&13.484&1.402  	&0.139\\
UMich \cite{pandey2015automatic} &1.802&\cellcolor{orangea}0.200	&2.698 	&\cellcolor{orangea}0.135\\
\makecell{HKU-Mars \cite{yuan2021pixel}}  &96.955	&4.382&25.611  &9.914\\
\makecell{DVL \cite{koide2023general}} &63.003	&0.919 &46.623 	&1.778\\
CalibAnything \cite{luo2023calib}  	&1.594 &0.203 &\cellcolor{orangea}1.382  &0.186\\
MIAS\cite{zhiwei2024lcec} &\cellcolor{orangeb}0.963 &\cellcolor{orangeb}0.182 &\cellcolor{orangeb}0.659 &\cellcolor{orangeb}0.114 \\

\textbf{Ours} &\cellcolor{orangec}\textbf{0.724} &\cellcolor{orangec}\textbf{0.107} &\cellcolor{orangec}\textbf{0.351}	&\cellcolor{orangec}\textbf{0.080}\\
\bottomrule
\end{tabular}
\label{tab.exp_mid360}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/CheckerBoard.pdf}
    \caption{Visualization of EdO-LCEC calibration results through LiDAR and camera data fusion: (a)-(d) are four LiDAR point clouds partially rendered by color using the estimated extrinsic matrix.}
    \label{fig.datafusion_on_checkerboard}
\end{figure}
\noindent \textbf{ Evaluation on KITTI Dataset.} For the KITTI dataset, it is important to note that since the learning-based approaches test their networks on 00 sequences and train them on other sequences, we compare their methods and other approaches only on sequence 00 to get a fair result. If the authors released their code, we reproduce the calibration results for both the left and the right cameras. 
The results shown in Table \ref{tab.rescmp_kitti00} and Table \ref{tab.rescmp_kitti_01_08} suggest that our method achieves SoTA performance on KITTI Odometry dataset. Our algorithm outperforms both traditional target-free methods and learning-based approaches on 00 sequence, reducing the $e_r$ by around 30-93\% and the $e_t$ by 39-99\%. Although the calibration accuracy is slightly worse than \cite{ma2021crlf} in the 04 sequence, EdO-LCEC outperforms all the other traditional target-free approaches in the 01-08 sequence. We attribute these performance improvements to the proposed generalizable scene discriminator based on LVMs. The LiDAR point cloud projections captured from multiple views of virtual cameras successfully increase the proportion of matchable correspondences between the sparse point cloud and camera image. According to Figure \ref{fig.SoTA_visualization}, it can be observed that a single frame in KITTI is so sparse that all the traditional target-free methods behave poorly. We overcome this difficulty by conducting spatial-temporal relative-pose optimization to decrease the calibration error in a single scene. The joint optimization from multi-view and multi-scene greatly increase the calibration accuracy on sparse cloud.
%It is clearly that all baselines suffer from a substantial performance drop. Nonetheless, our approach yields the smallest error demonstrating that our calibration strategy further increases robustness to unseen sensor configuration. 

%We attribute these performance improvements to the scene parser, robust dense key point matching strategy and the two-stage extrinsic matrix optimization. Surprisingly, as observed in Fig. \ref{fig.SoTAVisualization}, MobileSAM can effectively segment both RGB and LIP images captured in challenging conditions, such as dark underground parking garages or during rainy days, where the objects are even unrecognizable to human observers.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/SoTAVisualization.pdf}
    \caption{Qualitative comparisons with SoTA target-free LCEC approaches on the MIAS-LCEC-TF70 dataset: (a)-(d) RGB images, Depth images, LiDAR projection, and their segmentation results; (e)-(f) experimental results achieved using MIAS-LCEC (ours), LCCNet, CalibNet and DVL, shown by merging LIP and RGB images, where significantly improved regions are shown with red dashed boxes.}
    \label{fig.SoTA_visualization}
\end{figure*}

\noindent \textbf{Evaluation on MIAS-LCEC Dataset.} 
%Point clouds in MIAS-LCEC-TF70 are dense and have similar FoV with the camera RGB images. 
Compared to the sparse point clouds in the KITTI dataset, the point clouds in the MIAS dataset are significantly denser, which facilitates feature matching and allows us to test the upper limits of the algorithm calibration accuracy. 

The results shown in Table \ref{tab.rescmp_mias_tf70} demonstrate that EdO-LCEC achieves lower mean $e_r$ and $e_t$ values than all other approaches across the total six subsets. Our method dramatically outperforms CRLF, UMich, DVL, HKU-Mars, Calib-Anything, and is slightly better than MIAS-LCEC in scenarios with low noise and abundant features, while it performs significantly better than all methods in challenging conditions, particularly under poor illumination and adverse weather, or when few geometric features are detectable. This impressive performance can be attributed to the generalizable scene discriminator. The multiple virtual cameras generated by the scene discriminator provide a comprehensive perception of the calibration scene from both spatial and textural perspectives, which largely increases the possibility of capturing high-quality correspondences for the PnP solver. To better demonstrate our calibration accuracy, we illustrate the data fusion result in Figure \ref{fig.datafusion_on_checkerboard} using our optimized extrinsic matrix.

The results on the MIAS-LCEC-TF360 dataset further prove our outstanding performance. From Table \ref{tab.exp_mid360}, it is evident that while the other approaches achieve poor performance on this dataset, our approach demonstrates excellent performance, indicating strong adaptability to more challenging scenarios, with narrow overlapping areas between LiDAR projection images and camera RGB images. This impressive performance can be attributed to MobileSAM, a powerful LVM, capable of learning informative, general-purpose deep features for robust image segmentation. Furthermore, our proposed dual-path correspondence matching algorithm, mimics the human visual system to conduct feature matching on separate pathways and sets strict criteria for reliable correspondence selection to generate dense correspondences, thereby improving the quality of the PnP solutions.


\begin{table}[t!]
\caption{Ablation study of different components: Multi-View Optimization (intensity and depth) and Multi-Scene Optimization.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{\makecell{Multi \\ View}} & \multirow{2}*{\makecell{Multi \\ Scene} }&\multicolumn{2}{c|}{KITTI}  & \multicolumn{2}{c|}{\makecell{TF70}} &  \multicolumn{2}{c}{TF360} \\
\cline{3-8}
&& {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)}\\

\hline
\hline
  & &0.931 &0.243 &0.931 &0.243 &0.931 &0.243	\\
 \checkmark & &0.677 &0.251&0.931 &0.243 &0.931 &0.243	\\
 \checkmark & &0.412 &0.157	&0.931 &0.243 &0.931 &0.243\\
 & \checkmark&0.316 &0.105	&0.931 &0.243 &0.931 &0.243\\
 \checkmark & \checkmark&0.339 &0.098	&0.931 &0.243 &0.931 &0.243\\

\bottomrule
\end{tabular}
\label{tab.exp_ablation}
\end{table}

To investigate the adaptability of our method to sparse point clouds, we conducted calibration experiments on the MIAS-LCEC dataset with varying levels of downsampling (from $6/6$ to $1/6$). As shown in Figure \ref{fig.exp_on_different_density}, as the density of the point clouds decreases, the decline in the accuracy of our proposed algorithm is notably slower compared to other algorithms. This indicates that our method demonstrates significantly better adaptability to sparse point clouds than the alternatives.


\subsection{Ablation Study and Analysis}
\label{sec.ablation_study}
To investigate whether scene discriminator and spatial-temporal relative pose optimization are meaningful to EdO-LCEC, we conduct an experiment to verify our motivation. We analyze the impact of multi-view optimization and multi-scene optimization. Experimental results are shown in Table. \ref{exp_ablation}. It can be observed that multi-view greatly enhanced the calibration accuracy in KITTI and MIAS-LCEC-TF360 datasets. This indicates that the multiple views provided by virtual cameras indeed increase calibration adaptability to challenging scenarios, with narrow overlapping areas between LiDAR point cloud projections and camera images. The outstanding performance of multi-scene optimization on the KITTI dataset demonstrates that environment-driven joint optimization of extrinsic parameters across multiple scenes can significantly enhance calibration accuracy in sparse point clouds.


\section{Conclusion}
\label{sec.conclusion}

In contrast to previous algorithms, our approach is no longer limited by the type of calibration scene, enabling generalization across various sparse or complex environments. We designed a scene discriminator inspired by human perceptual systems, allowing the algorithm to actively comprehend the environment and adjust its calibration strategies accordingly. By effectively extracting and matching depth and texture information from the environment, we achieved more efficient cross-modal correspondence matching, addressing the challenge of unreliable feature matching in sparse scenes that earlier methods faced. We modeled the calibration process as a spatial-temporal joint optimization problem, achieving high-precision and robust online calibration through multi-view optimization within a single scene and joint optimization across multiple scenes. Extensive experiments on real-world datasets demonstrate that our algorithm achieves state-of-the-art calibration performance.

\clearpage

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{X_suppl}

\end{document}
