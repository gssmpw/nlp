\clearpage
\setcounter{page}{1}
\newcommand{\clref}{\textcolor{cvprblue}}

\maketitlesupplementary

\section{Details of the Algorithm Implementation}
\label{sec.sup_algo_detail}
As presented in the ``Dual-Path Correspondence Matching'' in Sect. \ref{sec.dualpath_matching}, we initially adopted the method described in \cite{zhiwei2024lcec} for cross-modal mask matching. \cite{zhiwei2024lcec} proved that when two 3D LiDAR points $\boldsymbol{q}^V$ (reference) and $\boldsymbol{p}^V$ (target) in the virtual camera coordinate system are close in depth, the relationship of their projection coordinates $\tilde{\boldsymbol{p}}_{v,c}$ on virtual image and camera image can be written by follows:
\begin{align}
\tilde{\boldsymbol{p}}_c \approx 
\boldsymbol{A}
\tilde{\boldsymbol{p}}_v + 
\boldsymbol{b}.
\label{eq.LscLaw}
\end{align}
In practice, we use the following affine transformation:
\begin{equation}
\begin{pmatrix}
\tilde{\boldsymbol{p}}_c & \tilde{\boldsymbol{q}}_c \\
\end{pmatrix}
=
\begin{pmatrix}
s{\boldsymbol{R}}^A & \boldsymbol{t}^A \\
\boldsymbol{0}^\top &1 \\
\end{pmatrix}
\begin{pmatrix}
\tilde{\boldsymbol{p}}_v & \tilde{\boldsymbol{q}}_v \\
\end{pmatrix}
\label{eq.LscLaw_final}
\end{equation}
to represent this affine transformation (mentioned in Sect. \ref{sec.dualpath_matching}), where $\boldsymbol{R}^A\in{SO(2)}$ represents the rotation matrix, $\boldsymbol{t}^A$ denotes the translation vector, and $s$ represents the scaling factor. We can assume that after the affine transformation, any points within a given mask in the LIP image perfectly align with the corresponding points from the RGB image, and thus:
\begin{equation}
\begin{aligned}
\hat{\boldsymbol{c}}^V = s\boldsymbol{R}^A \boldsymbol{c}^V + \boldsymbol{t}^A, \\
\hat{\boldsymbol{o}}^V = s\boldsymbol{R}^A \boldsymbol{o}^V + \boldsymbol{t}^A. 
\end{aligned}
\label{eq.correction}
\end{equation}
where $\hat{\boldsymbol{o}}^V$ and $\hat{\boldsymbol{c}}^V$ represent the updated coordinates of the instance's center and corner points in the virtual image, respectively.
In this case, $\boldsymbol{R}^A$ can be obtained using the following expression:
\begin{equation}
\label{eq.affine_r}
\boldsymbol{R}^A = 
\begin{pmatrix}
\cos{\theta} & -\sin{\theta} \\
\sin{\theta} & \cos{\theta}
\end{pmatrix},
\end{equation}
where 
\begin{equation}
\begin{aligned}
{\theta} = \frac{1}{N}\sum_{i=1}^{N}&{\bigg(\arctan{\frac{\boldsymbol{1}_y^\top({{\boldsymbol{c}}}^V_i  - {{\boldsymbol{o}}^V})}{\boldsymbol{1}_x^\top{({\boldsymbol{c}}}^V_i - {{\boldsymbol{o}}^V})}}}\\
&  - \arctan{\frac{\boldsymbol{1}_y^\top({{\boldsymbol{c}}}^C_i - {{\boldsymbol{o}}^C})}{\boldsymbol{1}_x^\top({{\boldsymbol{c}}}^C_i - {{\boldsymbol{o}}^C})}}\bigg)
\end{aligned}
\label{eq.affine_theta}
\end{equation}
is the angle between the vectors originating from the mask centers and pointing to their respective matched corner points. $s$ can then be expressed as follows:
\begin{equation}
s = \frac{w^Ch^C}{w^Vh^V},
\label{eq.affine_s}
\end{equation}
which represents the ratio between the areas of the bounding boxes associated with the RGB image and the LIP image. Finally, according to (\ref{eq.correction}), $\boldsymbol{t}^A$ can be obtained as follows:
\begin{equation}
\boldsymbol{t}^A = {{\boldsymbol{o}}^C} - s\boldsymbol{R}^A{{\boldsymbol{o}}^V}.
\label{eq.affine_t}
\end{equation}
In DPCM, we utilize the affine transformation estimated by (\ref{eq.affine_r})-(\ref{eq.affine_t}) as a semantic prior to guide the dense correspondence matching between LiDAR and camera.

\begin{algorithm}[t!]
\caption{Dual-Path Correspondence Matching}
\textbf{Require:}\\
Spatial pathway: Masks (including detected corner points) obtained from the LIP and RGB image. \\
Textural pathway: Masks (including detected corner points) obtained from the LDP and depth images.\\
\textbf{Stage 1 (Reliable mask matching):} \\
(1) Conduct cross-modal mask matching by adopting the method described in \cite{zhiwei2024lcec}. \\
(2) Estimate $s\boldsymbol{R}^A$ and $\boldsymbol{t}^A$ using (\ref{eq.affine_r})-(\ref{eq.affine_t}).\\
\textbf{Stage 2 (Dense correspondence matching):} \\
(1) For each pathway, update all masks in the virtual image using $s\boldsymbol{R}^A$ and $\boldsymbol{t}^A$.\\
(2) Construct the corner point cost matrix $\boldsymbol{M}^C$ using (\ref{eq.adaptive_cost_func}).
(3) Select matches with the lowest costs in both horizontal and vertical directions of $\boldsymbol{M}^{C}(\boldsymbol{x})$ as the optimum correspondence matches. \\
(4) Aggregate all corner point correspondences to form the sets $\mathcal{C} = \{(\boldsymbol{p}^L_i,\boldsymbol{p}_i) \mid i = 1,\dots, q\}$.
\label{Algorithm.DPCM}
\end{algorithm} 

Additionally, the component $H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k})$ of the structural consistency metioned in Sect. \ref{sec.dualpath_matching} is defined as follows:
\begin{equation}
H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k}) = {\frac{\| (\boldsymbol{e}^V_{i,k} - \hat{\boldsymbol{c}}^V_{i} ) -(\boldsymbol{e}^C_{j,k} - \boldsymbol{c}_j^C )\|_2}{\|  \boldsymbol{e}^V_{i,k} - \hat{\boldsymbol{c}}^V_{i}  \|_2 + \| \boldsymbol{e}^C_{j,k} - \boldsymbol{c}_j^C \|_2}},
\label{eq._struct_similarity}
\end{equation}
which represents the similarity of the neighboring vertices between the current and target corner points.

The pseudo code of DPCM is shown in Algorithm \ref{Algorithm.DPCM}. Thanks to the generalizable discriminator, the multiple views generated by the virtual cameras provide comprehensive spatial and textural cues to formulate the feature matching to a 3D-2D correspondence matching process. The entire DPCM is efficient because it primarily focuses on 2D structural and textural consistency computation, rather than complex 3D point cloud registration. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/PointCloudVisualization.pdf}
    \caption{An example of the data fusion result on KITTI odometry using our calibration parameters.}
    \label{fig.exp_detail_datafusion_on_KITTI}
\end{figure*}

For the spatial-temporal relative pose optimization, as presented in Sect. \ref{sec.st_optimization}, we utilize a Gaussian kernel $\mathcal{K}(d'_{j},\bar{d'})$ to calculate the depth-normalized reprojection error. In detail, $\mathcal{K}(d'_{j},\bar{d'})$ is defined as follows:
\begin{equation}
\mathcal{K}(d'_{j},\bar{d'}) = \exp{ \bigg(\frac{ -\left\|{d'_{j}} - \bar{d'}\right\|^2 }{{2\bar{d_g}}^2} \bigg)},
\end{equation}
where $\bar{d_g}$ is the true average depth of the correspondences. 

In our algorithm implementation, we use the pinhole model as the projection model to calculate the reprojection error presented in (\ref{eq.multi_view}) and (\ref{eq.multi_scene}). When the camera intrinsic matrix $\boldsymbol{K}$ is known, the 3D LiDAR point $\boldsymbol{p}^{L}=[x^L,y^L,z^L]^\top$ can be projected onto a 2D image pixel $\boldsymbol{{p}} = [u,v]^\top$ using the following expression:
\begin{equation}
\pi(\boldsymbol{p}^{L}) = \tilde{\boldsymbol{p}} = \frac{\boldsymbol{K}\boldsymbol{p}^{L}}{(\boldsymbol{p}^{L})^\top\boldsymbol{1}_{z}},
\end{equation}
where $\tilde{\boldsymbol{p}}$ represents the homogeneous coordinates of $\boldsymbol{{p}}$ and $\boldsymbol{1}_{z}=[0,0,1]^\top$. Given the extrinsic transformation ${^C_L}\boldsymbol{T}$, the reprojection between the correspondence $(\boldsymbol{p}_{i}^L, \boldsymbol{p}_{i})$ can be obtained as follows:
\begin{equation}
\epsilon_i = \left\|\pi( {^C_L}\boldsymbol{T}\boldsymbol{p}_i^{L}) - \tilde{\boldsymbol{p}}_i  \right\|_2= \left\|\frac{\boldsymbol{K}({^{C}_{L}\boldsymbol{R}}\boldsymbol{p}_i^{L} + {^{C}_{L}\boldsymbol{t}})}{({^{C}_{L}\boldsymbol{R}}\boldsymbol{p}_i^{L} + {^{C}_{L}\boldsymbol{t}})^\top\boldsymbol{1}_{z}} - \tilde{\boldsymbol{p}}_i\right\|_2.
\end{equation}
By applying the Gaussian normalization $G$ given in (\ref{eq.dpcm_gaussian}), the reprojection errors of different correspondences are provided with weights based on the depth distribution.



%\begin{equation}
%w_{n,m} = \frac{e - \exp{ \frac{(  -\frac{ d - d_{\min} }{ d_{\max} - d_{\min} } - \bar{d}^2 )}{\bar{d}^2}} }
%{C + \left\|\pi({^{C}_{L}\boldsymbol{T}_{i}}\boldsymbol{p}_{n,m}^L) - {\boldsymbol{p}}_{n,m}\right\|_2}
%\end{equation}



\section{Calibration Parameters}
To foster future research toward LiDAR-camera sensor
fusion, we release the calibration parameters that we obtained with our method for the 00 sequence of the KITTI odometry. In detail, we provide the rotation matrices and translation vectors for projecting the LiDAR point cloud into the camera image or rendering LiDAR point clouds with colors. 

\noindent \textbf{Calibration Parameters for Left RGB Camera}
\[
\begin{aligned}
\boldsymbol{R} &= 
\begin{bmatrix}
-2.5805e-04 &-9.9997e-01 &-7.5245e-03 \\
-6.8854e-03 &7.5261e-03 &-9.9995e-01 \\
9.9998e-01 &-2.0623e-04 &-6.8872e-03 \\
\end{bmatrix} \\
\boldsymbol{t} &= 
\begin{bmatrix}
0.070478 &-0.057913 &-0.286353
\end{bmatrix}^\top
\end{aligned}
\]


\noindent \textbf{Calibration Parameters for Right RGB Camera}
\[
\begin{aligned}
\boldsymbol{R} &= 
\begin{bmatrix}
-5.3850e-04 &-9.9998e-01 &-6.6623e-03 \\
-8.1213e-03 &6.6665e-03 &-9.9994e-01 \\
9.9997e-01 &-4.8437e-04 &-8.1247e-03 \\
\end{bmatrix} \\
\boldsymbol{t} &= 
\begin{bmatrix}
-0.494557 &-0.042731 &-0.261487 
\end{bmatrix}^\top
\end{aligned}
\]
An example of the data fusion result using our extrinsic calibration parameters for the left camera in KITTI odometry is shown in Fig. \ref{fig.exp_detail_datafusion_on_KITTI}.


\section{Details of Experimental Configuration}
\label{sec.sup_exp_config_detail}

\begin{table}[t!]
\caption{Dataset statistics of KITTI odometry 00-09 sequences.}
\centering
\fontsize{6.38}{10}\selectfont
\begin{tabular}{c|@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c@{\hspace{0.2cm}}c}
\toprule
& 00 &01 &02 &03 &04 &05 &06 &07 &08 &09 \\
\hline
Initial Pairs & 4541 &1101& 4661 & 801 & 271 & 2761 &1101 & 1101 &4071 &4091\\
Downsampling & 10 &10& 10 & 10 & 2 & 10 &10 & 10 &10 &410\\
Evaluated Pairs & 455 &111& 467 & 81 & 136 & 277 &111 & 111 &408 &410\\
\bottomrule
\end{tabular}
\label{tab.kitti_dataset_statistics}
\end{table}

\begin{table*}[t!]
\caption{Details of comparison methods.}
\centering
\fontsize{6.9}{8}\selectfont
\begin{tabular}{lcccccc}
\toprule
Methods & Publication &Type &Parameterization or Basic Tools &Computation Speed &Accuracy &Provide Code? \\
\hline
UMich \cite{pandey2015automatic} &JFR 2015&MI-based& Mutual Information &  Medium & Medium & Yes\\
RegNet \cite{schneider2017regnet} &IV 2017&Learning-based& CNN &  Medium & Medium & No\\
CalibNet  \cite{iyer2018calibnet} &IROS 2018&Learning-based& Geometrically Supervised
Deep Learning Network & High  & Low & Yes\\
RGGNet \cite{yuan2020rggnet} &RAL 2020&Learning-based & Deep Learning Network, Riemannian
Geometry &  High & Medium & Yes\\
CalibRCNN \cite{shi2020calibrcnn}&IROS 2020 &Learning-based& RCNN & Medium &  Medium & No\\
CalibDNN \cite{zhao2021calibdnn} &SPIE 2021&Learning-based& Deep Learning Network &  Medium & Medium & No\\
LCCNet \cite{lv2021lccnet} &CVPR 2021&Learning-based& CNN, L1-Loss & High  & Medium & Yes \\
HKU-Mars  \cite{yuan2021pixel} &RAL 2021&Edge-based& Adaptive Voxel & Low & Very Low & Yes\\
CRLF \cite{ma2021crlf} &arXiv 2021 &Semantic-based&Segmented Road Pole and Lane, Perspective-3-Lines &  Low & Low & Yes\\
DVL  \cite{koide2023general} &ICRA 2023&Point-based& SuperGlue, RANSAC, Normalized Information Distance &  Medium & High & Yes\\
Borer \etal \cite{borer2024chaos} &WACV 2024&Learning-based& CNN, Geometric Mutual Information  & High & High  & No \\
MIAS-LCEC \cite{zhiwei2024lcec} &T-IV 2024 &Semantic-based& MobileSAM, C3M, PnP &  Low & High & Yes\\

\bottomrule
\end{tabular}
\label{tab.cmp_methods}
\end{table*}

\subsection{Datasets}
We have conducted extensive experiments on KITTI odometry and MIAS-LCEC (including MIAS-LCEC-TF70 and MIAS-LCEC-TF360) datasets. Now we provide their details:
\begin{itemize}
\item {\textbf{\textit{ KITTI odometry}}: KITTI odometry is a large-scale public dataset recorded using a vehicle equipped with two RGB cameras Point Grey Flea 2 and one LiDAR of type Velodyne HDL-64E. Sensor data is captured at 10 Hz. We utilize the first 10 sequences (00-09) for evaluation.}
\item {\textbf{\textit{MIAS-LCEC-TF70}}: MIAS-LCEC-TF70 is a diverse and challenging dataset that contains 60 pairs of 4D point clouds (including spatial coordinates with intensity data) and RGB images, collected using a Livox Mid-70 LiDAR and a MindVision SUA202GC camera, from a variety of indoor and outdoor environments, under various scenarios as well as different weather and illumination conditions. This dataset is divided into six subsets: residential community, urban freeway, building, challenging weather, indoor, and challenging illumination, to comprehensively evaluate the algorithm performance. }
\item {\textbf{\textit{MIAS-LCEC-TF360}}:  MIAS-LCEC-TF360 contains 12 pairs of 4D point clouds and RGB images, collected using a Livox Mid-360 LiDAR and a MindVision SUA202GC camera from both indoor and outdoor environments. Since the Livox Mid-360 LiDAR has a scanning range of 360$^\circ$,  it produces a sparser point cloud compared to that generated by the Livox Mid-70 LiDAR. Additionally, the significant difference in the FoV between this type of LiDAR and the camera results in only a small overlap in the collected data. Consequently, this dataset is particularly well-suited for evaluating the adaptability of algorithms to challenging scenarios }
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/CorrespondenceMatching.pdf}
    \caption{Qualitative comparisons of correspondence matching.}
    \label{fig.cmp_matching}
    \vspace{-1em}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/SceneCalibVisualization.pdf}
    \caption{Visualization of EdO-LCEC calibration results through LiDAR and camera data fusion on MIAS-LCEC datasets.}
    \label{fig.EdO_calib_visualization}
\end{figure*}

\subsection{Metrics}
As stated in Sect. \ref{sec.exp_setup}, to comprehensively evaluate the performance of LCEC approaches, we use the magnitude $e_r$ of Euler angle error and the magnitude $e_t$ of the translation error to quantify the calibration errors. Here, we give the specific calculation formula of $e_r$ and $e_t$\footnote{The translation from LiDAR pose to camera pose is $-{{^{C}_{L}\boldsymbol{R}}^{-1}}{{^C_L}\boldsymbol{t}}$ when (\ref{eq.lidar_to_camera_point}) is used to depict the point translation.}:
\begin{equation}
\begin{aligned}
e_r &=  \left\|{{^C_L}\boldsymbol{r}^*} - {{^C_L}\boldsymbol{r}}\right\|_2, \\
e_t &= \left\|-{({^{C}_{L}\boldsymbol{R}^*})^{-1}}{{^C_L}\boldsymbol{t}}^* +{{^{C}_{L}\boldsymbol{R}}^{-1}}{{^C_L}\boldsymbol{t}}\right\|_2,
\end{aligned}
\label{exp_metrics}
\end{equation}
where ${{^C_L}\boldsymbol{r}^{*}}$ and ${{^C_L}\boldsymbol{r}}$ represent the estimated and ground-truth Euler angle vectors, computed from the rotation matrices ${^{C}_{L}\boldsymbol{R}}^*$ and ${^{C}_{L}\boldsymbol{R}}$, respectively. Similarly, ${^C_L}\boldsymbol{t}^*$ and ${^C_L}\boldsymbol{t}$ denote the estimated and ground-truth translation vectors from LiDAR to camera, respectively.



\subsection{Data Preparation}
The dataset statistics of KITTI odometry are presented in Table \ref{tab.kitti_dataset_statistics}. It can be observed that there are a total of 24500 pairs of point clouds and images in KITTI odometry. Note that most of the SoTA comparison methods are slow to complete calibration with a single frame. For the comparison algorithms that provided source code, we performed calibration every ten frames (except for 2 frames in 04 sequence), enhancing the efficiency of the experiments without compromising the validity of the evaluation. 

\subsection{Details of Comparison Methods}
The details of the comparison methods are illustrated in Table \ref{tab.cmp_methods}. For algorithms that did not release their code, we directly reference the results published in their papers. Notably, RegNet \cite{schneider2017regnet}, Borer \etal \cite{borer2024chaos}, and CalibDNN \cite{zhao2021calibdnn} employ different experimental datasets and evaluation processes, which limits the potential for a fully quantitative comparison. Furthermore, these methods do not specify which frames from the KITTI dataset were used for training and testing. Therefore, we rely on the published results to compare these approaches to others on the KITTI odometry 00 sequence.


%\subsection{Implementation Details}

\section{Additional Experiments}
\subsection{Comparison of Correspondence Matching}

%We first provide the comparison of correspondence matching results in Fig. \ref{fig.cmp_matching}. The visualization results show that our method outperforms other correspondence matching algorithms employed in the comparison calibration method. Specifically, the number of correct correspondences is greatly larger than that of ORB, DVL \cite{sarlin2020superglue} (which hat utilizes SuperGlue to establish direct 3D-2D correspondences), and C3M \cite{zhiwei2024lcec}. This demonstrates the strong adaptability of DPCM on sparse point clouds with little geometric and textural information. The ORB feature matching algorithm identifies and matches keypoints in images by using the fast Harris corner detector for keypoint extraction and a binary descriptor for efficient matching based on the Hamming distance. Although the ORB algorithm performs well in matching between camera images, it struggles with cross-modal feature matching in scenarios involving LiDAR and camera integration. While C3M and DVL achieve good matching results on the dense point cloud recorded in the MIAS-LCEC dataset, they behave badly on the sparse point cloud in the KITTI dataset. The narrow overlapping field of view between the camera and LiDAR in KITTI brings up great challenges for finding available correspondences, especially along the edges of the LiDAR field of view, this largely decreases the matching accuracy of DVL. C3M matches only the corner points within the aligned masks, which makes it challenging to handle sparse point clouds and scenes with limited sensor overlap. Our proposed DPCM addresses the above problems by considering both spatial and textural constraints. Different from C3M, DPCM uses mask instance matching results as priors, rather than strict constraints, to achieve denser feature point correspondences. This enhancement greatly improves the algorithm's robustness in sparse scenes and situations with a limited field of view, thereby ensuring reliable calibration under challenging conditions.

We present a comparison of correspondence matching results in Fig. \ref{fig.cmp_matching}. The visualized results indicate that our method significantly outperforms other correspondence matching algorithms employed in the comparison calibration methods. Specifically, our approach yields substantially more correct correspondences than ORB, DVL (which employs SuperGlue for direct 3D-2D correspondence matching), and C3M (the cross-modal mask matching algorithm provided by MIAS-LCEC). This demonstrates the superior adaptability of DPCM, particularly in scenarios involving sparse point clouds with minimal geometric and textural information.
\begin{table}[t!]
\caption{Quantitative comparisons with SoTA LCEC approaches on the segmented point clouds from MIAS-LCEC-TF70.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|cccccc}
\toprule
\multirow{2}*{Approach}& \multicolumn{6}{c}{Density of Point Cloud (c1 - c6)}\\
\cline{2-7}
&c1&c2&c3&c4&c5&c6\\
\hline
\multicolumn{7}{c}{Rotation Error ($e_r$)}\\
\hline
UMich \cite{pandey2015automatic} &0.537	&0.494	&0.501	&0.494	&0.515	&0.529	\\
HKU-Mars \cite{yuan2021pixel} &1.585	&2.272	&0.824	&0.352	&0.779	&3.744	\\
DVL \cite{koide2023general}  &0.373	&0.339	&0.337	&0.430	&0.432	&0.379	\\
MIAS-LCEC \cite{zhiwei2024lcec}  &0.187	&0.700	&0.856	&1.343	&0.273	&0.514	\\
\hline
\textbf{EdO-LCEC (Ours)}  &0.292 &0.443	&0.413	&0.445	&0.255	&0.355	\\
\hline
\multicolumn{7}{c}{Translation Error ($e_t$)}\\
\hline
UMich \cite{pandey2015automatic}&0.125	&0.127	&0.129	&0.124	&0.126	&0.135\\
HKU-Mars \cite{yuan2021pixel}&0.955	&0.803	&0.247	&0.174	&0.281	&1.132\\
DVL \cite{koide2023general}&0.231	&0.085	&0.100	&0.118	&0.122	&0.102\\
MIAS-LCEC \cite{zhiwei2024lcec}&0.124	&0.152	&0.193	&0.309	&0.055	&0.104\\
\hline
\textbf{EdO-LCEC (Ours)}&0.107	&0.097	&0.111	&0.104	&0.064	&0.079\\
\bottomrule
\end{tabular}
\label{tab.SparseTable_MIASTF70}
\end{table}

The ORB algorithm relies on the fast Harris corner detector for keypoint extraction and employs a binary descriptor for efficient matching based on Hamming distance. While ORB performs well in image-to-image matching, it struggles with cross-modal feature matching, especially in LiDAR-camera integration scenarios. Conversely, while DVL and C3M perform adequately on dense point clouds such as those in the MIAS-LCEC-TF70, they face significant challenges when applied to the sparse point clouds in the KITTI odometry and MIAS-LCEC-TF360. The narrow overlapping field of view between LiDAR and camera in KITTI odometry creates difficulties in identifying reliable correspondences, particularly along the edges of the LiDAR field of view. This limitation significantly reduces the matching accuracy of DVL. Similarly, C3M, which restricts matching to corner points within aligned masks, struggles with sparse point clouds and limited sensor overlap.

Our proposed DPCM resolves these issues by incorporating both spatial and textural constraints. Unlike C3M, which treats mask instance matching as a strict constraint, DPCM uses it as a prior, enabling the generation of denser 3D-2D correspondences. This refinement significantly enhances the robustness of the algorithm in sparse environments and restricted fields of view, ensuring reliable calibration even under challenging conditions.

\subsection{Evaluation on Segmented Point Clouds}
\label{sec.add_eva_seg_pointcloud}
To further prove our algorithm's adaptability to incomplete and sparse point clouds, we evaluate the calibration accuracy on the segmented point clouds from MIAS-LCEC-TF70 (the segmentation method is the same as that described in Sect. \ref{sec.exp_dataset}). 
%the already limited field-of-view point clouds from the MIAS-LCEC-TF70 dataset. 
Experimental results in Table \ref{tab.SparseTable_MIASTF70} indicate that our approach outperforms the other SoTA LCEC approaches, achieving the smallest mean $e_r$ and $e_t$ across different point cloud densities. This demonstrates the stability and adaptability of EdO-LCEC under challenging conditions involving sparse or incomplete point clouds. It should be noted that the results of CRLF were excluded from this comparison due to its invalid outputs during the evaluation. 

\subsection{More Visualization Results}
\label{sec.more_visualization}

In this section, we provide more detailed visualization results of EdO-LCEC through LiDAR and camera data fusion on MIAS-LCEC datasets. Fig. \ref{fig.EdO_calib_visualization} demonstrate that LiDAR point cloud and camera image aligned almost perfectly along the textural and geometric edges, proving the accuracy of our calibration results. It can also be observed that our method performs well in challenging conditions, particularly under poor illumination and adverse weather, or when few geometric features are detectable. This impressive data fusion result further validates the critical role of LiDAR-camera extrinsic calibration. The seamless integration of spatial and textural modalities equips robots with a human-like perceptual ability to interpret and interact with their surroundings. Our environment-driven approach bridges the gap toward the flexible and adaptive machines often envisioned in science fiction.