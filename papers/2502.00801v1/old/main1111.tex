\documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[review]{cvpr}      % To produce the REVIEW version
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\usepackage{array}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage[table]{xcolor}
\usepackage{algorithm}
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}
\title{Environment-Driven Online LiDAR-Camera Extrinsic Calibration}
\author{Zhiwei Huang\\
Tongji University\\
Shang Hai\\
{\tt\small zhiwei.huang@outlook.com}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\egi}{\textit{e.g.}}
\newcommand{\settablefont}{\fontsize{6.9}{11.8}\selectfont}
\definecolor{orangea}{RGB}{252, 245, 150} 
\definecolor{orangeb}{RGB}{251, 210, 136}    
\definecolor{orangec}{RGB}{255, 156, 115} 
\definecolor{oranged}{RGB}{255, 69, 69} 
\newcommand{\clr}{\textcolor{red}}
\newcommand{\clb}{\textcolor{blue}}

\begin{document}

\maketitle

\begin{strip}
    \centering
    \vspace{-4em}
    \includegraphics[width=0.99999\textwidth]{figs/CoverV6.pdf}
    \captionof{figure}{
    We fused 100 frames of point clouds from the KITTI 00 sequence, resulting in the composite point cloud shown in the figure. Based on the calibration results, each zoomed-in section illustrates the alignment between the camera images and the projected point clouds. As can be observed, the point clouds and images align perfectly under our calibration method, demonstrating the precision of our approach.
		}
		\label{fig.cover}
\end{strip}

\begin{abstract}
\clr{LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles. Offline, target-based approaches have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments. The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches. Extensive experiments conducted on these real-world datasets demonstrate its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view.}
\end{abstract}

\section{Introduction}
\label{sec.intro}

\clb{We have long envisioned robots with human-like intelligence, enabling them to understand, adapt to, and positively impact the world \cite{arnold2019survey,bai2022transfusion,ai2023lidar}. This dream is becoming increasingly attainable with the advent of LiDAR-camera data fusion systems. LiDARs provide accurate spatial information, while cameras capture rich textural details \cite{zhiwei2024lcec}. The complementary strengths of these two modalities, when fused, provide robots with powerful environmental perception capabilities. LiDAR-camera extrinsic calibration (LCEC), which estimates the rigid transformation between the two sensors, is a core and foundational process for effective data fusion.}

Current LiDAR-camera extrinsic calibration methods are primarily categorized as either target-based or target-free. Offline, target-based approaches \cite{cui2020acsc,yan2023joint,fan2023autonomous,kang2022accurate} have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. In contrast, online, target-free approaches \cite{koide2023general,yuan2021pixel,ma2021crlf,pandey2012automatic,tang2023robust} aim at overcoming this problem, \egi, by matching cross-modal features within the overlapped field of view of the sensors or by \clr{inferring} the extrinsic \clr{transform from the sensor motion}. Although these approaches are generally more widely applicable than target-based approaches, they either require dense point clouds or rely on accurate sensor motion estimation, which largely \clr{declines} the applicability of the algorithm. Overall, current calibration algorithms is sensitive to sensor intial configuration and is difficult to cope with complex and dynamic environmental changes \cite{zhiwei2024lcec}. \clr{Thus, a crucial question remains to be addressed: How to improve the generalization ability of LiDAR-camera extrinsic calibration?}
%\clr{We found inspiration for solving this issue in the Two Streams Hypothesis \cite{goodale1992seperate} proposed by Royal Society Fellows Melvyn A. Goodale and David Milner.} \clr{Two Streams Hypothesis argues that human visual systems possess ventral and dorsal pathways. The ventral pathway leads to the temporal lobe, which is involved with object identification and recognition. The dorsal one leads to the parietal lobe, which is involved with processing the spatial location relative to the viewer.} \clr{It suggests that humans make decisions by organically integrating the semantic and spatial-temporal content present in the surroundings, driven by the environment and informed by subjective intent.} Building on this insight, this paper introduces a new definition for LiDAR-camera extrinsic calibration, referred to as ``environment-driven". Imagine a self-driving car navigating through complex city streets with such an environment-driven algorithm that mimics human intuition and judgment. It can quickly identify environmental features amidst flickering lights and rapidly moving objects, making autonomous decisions based on these cues. This level of intelligence improves sensor fusion accuracy and greatly enhances system robustness.

\clr{We found inspiration for solving this issue in the two streams hypothesis \cite{goodale1992seperate}.} \clr{Two streams hypothesis argues that human visual systems possess ventral and dorsal pathways, which is involved with object recognition and spatial processing, respectively.} \clr{It suggests that humans make decisions by organically integrating the semantic and spatial-temporal content present in the surroundings, driven by the environment and informed by subjective intent.} Building on this insight, this paper introduces a new definition for LiDAR-camera extrinsic calibration, referred to as ``environment-driven". Imagine a self-driving car navigating through complex city streets with such an environment-driven algorithm that mimics human intuition and judgment. It can quickly identify environmental features amidst flickering lights and rapidly moving objects, making autonomous decisions based on these cues.

Motivated by the above discussion, we developed a novel environment-driven LiDAR-camera extrinsic calibration (EdO-LCEC) approach. As illustrated in Fig. \ref{fig.cover}, our central insight is that multiple frames of input sensor data can be interpreted as distinct scenes that, when combined, provide a holistic view of the environment. A prerequisite for the success of EdO-LCEC is the design of a generalizable scene discriminator. The scene discriminator employs large vision models to infer depth images from the camera RGB images and perform instance segmentation on camera images, LiDAR intensity projections (LIP), and LiDAR depth projections (LDP), thereby achieving an understanding of the semantic and spatial dimensions. Then it calculates the feature density of the calibration scene and uses this information to guide the generation of multiple virtual cameras for projecting the point clouds, aiming to capture more available environmental features. This design improves upon the virtual camera generation method described in \cite{koide2023general,zhiwei2024lcec}, significantly enhancing the ability to extract cross-modal features and effectively increasing calibration adaptability in scenarios with limited shared sensor fields of view or sparse point clouds.

For each scene, we perform dual-path correspondence matching (DPCM). Inspired by the two streams hypothesis, DPCM incorporates both spatial and textural pathways. The spatial pathway handles correspondence matching between RGB and LIP images, while the textural pathway matches correspondence between depth and LDP images. This dual-path architecture integrates spatial and textural modalities by constructing a matching cost matrix based on texture and local structural features, guided by precise semantic and depth priors. This approach further improves cross-modal feature association in sparse point clouds and images, overcoming the previous reliance of algorithms \cite{yuan2021pixel,pandey2015automatic,ma2021crlf} on uniformly distributed geometric or textural features. Finally, the obtained correspondences serve as inputs for the proposed spatial-temporal relative pose optimization to derive and refine the extrinsic matrix. The optimization process encompasses two critical components: multi-view and multi-scene optimization. The former constructs a loss function based on depth-normalized reprojection errors to find the optimal extrinsic parameter estimates for a single scene. The latter employs the results of the multi-view approach to jointly optimize extrinsic parameter calculations across multiple scenes, further enhancing calibration accuracy. Through extensive experiments conducted on three real-world datasets, the proposed EdO-LCEC demonstrates superior robustness and accuracy compared to other SoTA online, target-free approaches.

To summarize, our novel contributions are as follows:

\begin{itemize}
    \item {EdO-LCEC, an environment-driven, online LCEC approach that mimics human visual systems to actively analyze the surroundings.}
    \item {Generalizable scene discriminator, which can automatically perceive the calibration scene and capture potential spatial and semantic features using SoTA LVMs.}
    \item {Dual-Path Correspondence Matching (DPCM), a novel cross-modal feature matching algorithm, capable of generating dense and reliable correspondences between LiDAR point cloud and camera image.}
    \item {Spatial-temporal relative pose optimization, enabling high quality extrinsic estimation through multi-view and multi-scene optimization.}
\end{itemize}

\section{Related Work}
\label{sec.relate_works}
Traditional target-free LCEC approaches, such as \cite{lv2015automatic, castorena2016autocalibration,pandey2012automatic, pandey2015automatic}, estimate the relative pose between the two sensors by aligning the cross-modal edges or mutual information (MI) extracted from LiDAR projection and camera RGB image. While effective in specific scenarios with abundant features, these traditional methods heavily rely on well-distributed edge features, which can compromise calibration robustness. Moreover, the use of low-level image processing algorithms, such as Gaussian blur and the Canny operator, can introduce errors in edge detection, potentially fragmenting global lines and thus reducing overall calibration accuracy. 
On the other hand, studies \cite{zhang2023overlap,yin2023automatic} have begun to address the challenges of cross-modal feature matching by developing motion-based methods. These approaches match trajectories from visual and LiDAR odometry to derive extrinsic parameters through optimization. While they effectively accommodate heterogeneous sensors without requiring overlap, they demand precise time synchronization. Moreover, this calibration framework necessitates multiple LiDAR point clouds and camera images to accurately estimate per-sensor motion, which further limits its applicability in diverse environments.

Advances in deep learning techniques have driven significant exploration into enhancing traditional target-free algorithms. Several end-to-end deep learning-based algorithms \cite{borer2024chaos, schneider2017regnet,lv2021lccnet,shi2020calibrcnn,zhao2021calibdnn,iyer2018calibnet}, tried to covert calibration process into more direct approaches by exploiting deep learning-based correspondences between RGB images and LiDAR point clouds. On the other hand, some research tries to attach deep learning modules to their calibration framework as useful tools to enhance calibration efficiency. For instance, a recent study \cite{koide2023general} introduced Direct Visual LiDAR Calibration (DVL), a novel point-based method that utilizes SuperGlue \cite{sarlin2020superglue} to establish direct 3D-2D correspondences between LiDAR and camera data. Additionally, this study refines the estimated extrinsic matrix through direct LiDAR-camera registration by minimizing the normalized information distance, a mutual information-based cross-modal distance measurement loss. While these methods have demonstrated effectiveness on large-scale datasets like KITTI \cite{geiger2012we}, which primarily focuses on urban driving scenarios, their performance has not been extensively validated on other types of real-world datasets. Furthermore, their dependence on pre-defined sensor configurations (both LiDAR and camera) poses implementation challenges. In comparison, our approach improved environmental adaptability by integrating scene discriminator, enabling calibration to perform effectively in diverse and challenging real-world environments.

\section{Methodology}
\label{sec.method}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/FrameWorkV5.pdf}
    \caption{The pipeline of the proposed EdO-LCEC. Our method adheres to the human visual system, which consists of ventral and dorsal pathways. The dorsal pathway is used to project depth of LiDAR point cloud and estimate depth image of RGB image. The ventral pathway handles LiDAR intensity projection and image segmentation.
    %We first analyze the feature density of a calibration scene and generate multiple intensity virtual cameras and depth virtual cameras to project the LiDAR point cloud. Then, in the dorsal path, the scene discriminator utilize Depth-AnythingV2 to infer the depth image of the RGB image.
    }
    \label{fig.framework_overview}
\end{figure*}

%\subsection{Preliminaries}
%\label{sec.pre}
Given LiDAR point clouds and camera images, our goal is to estimate their extrinsic matrix $^{C}_{L}\boldsymbol{T}$, defined as follows \cite{zhao2023dive}:
\begin{equation}
{^{C}_{L}\boldsymbol{T}} = 
\begin{bmatrix}
{^{C}_{L}\boldsymbol{R}} & {^{C}_{L}\boldsymbol{t}} \\
\boldsymbol{0}^\top & 1
\end{bmatrix}
\in{SE(3)},
\label{eq.lidar_to_camera_point}
\end{equation}
where $^{C}_{L}\boldsymbol{R} \in{SO(3)}$ represents the rotation matrix, $^{C}_{L}\boldsymbol{t}$ denotes the translation vector, and $\boldsymbol{0}$ represents a column vector of zeros. We first give an overview of the proposed method, as shown in Fig. \ref{fig.framework_overview}. It mainly contains three main stages: 
\begin{itemize}
    \item{Given a stream of point clouds and images, we utilize a scene discriminator to understand the environment through image segmentation and depth estimation, generating virtual cameras that project LiDAR intensity and depth from multiple viewpoints (Sect. \ref{sec.scene_discriminator}).}
     \item{The results from image segmentation are processed along two pathways, which are then fed into dual-path correspondence matching to establish dense cross-modal correspondences (Sect. \ref{sec.dualpath_matching}).}
      \item{The correspondences obtained serve as inputs for our proposed spatial-temporal relative pose optimization, which is used to derive and refine the extrinsic matrix (Sect. \ref{sec.st_optimization}).}
\end{itemize}  

\subsection{Generalizable Scene Discriminator} 
\label{sec.scene_discriminator}
%Inspired by the human perception system, our calibration algorithm incorporates environmental awareness. 
Our environment-driven approach first employs a generalizable scene discriminator to interpret the surroundings by generating virtual cameras to project LiDAR point cloud intensities and depth. The discriminator configures the first intensity virtual camera and depth virtual camera at the LiDAR perspective. This generates a LiDAR intensity projection (LIP) image ${{^V_I}\boldsymbol{I}} \in{\mathbb{R}^{H\times W \times 1}}$ ($H$ and $W$ represent the image height and width) and a LiDAR depth projection (LDP) image ${{^V_D}\boldsymbol{I}}$. To align with the LDP image, the input camera RGB image ${{^C_I}\boldsymbol{I}}$ is processed using Depth Anything V2 to obtain estimated depth images ${^C_D}\boldsymbol{I}$\footnote{In this article, the symbols in the superscript denote the type of target camera ($V$ denotes virtual camera and $C$ indicates real camera), and the subscript denotes the source of the image ($D$ is depth and $I$ is intensity).}.  To take advantage of semantic information, we utilize MobileSAM as the image segmentation backbone. The series of detected masks in an image is defined as $\{\mathcal{M}_1, \dots, \mathcal{M}_n\}$. The corner points along the contours of masks detected are represented by $\{\boldsymbol{c}_1, \dots, \boldsymbol{c}_{m_i}\}$, where $m_i$ is the total corner points number in the $i$-th mask. An instance (bounding box), utilized to precisely fit around each mask, is centrally positioned at $\boldsymbol{o}$ and has a dimension of $h\times w$ pixels. We define the depth of a key point as $d$, and $\boldsymbol{D} \in \mathbb{R}^{b\times b}$ as the matrix that stores depth values in its k-neighborhood. To fully leverage the structured information within the masks, we construct a convex polygon from the corner points. As depicted in Fig. \ref{fig.framework_overview}, the neighboring vertices of a corner point $\boldsymbol{c}$ is defined as $\boldsymbol{e}_{i,1}, \dots, \boldsymbol{e}_{i.k}\}$. 

For each virtual image or camera image ${\boldsymbol{I}}$, the scene discriminator calculates its feature density $\rho({\boldsymbol{I}})$ to provide valuable hints for feature extraction and correspondence matching:
\begin{equation}
\rho({\boldsymbol{I}}) = \underbrace {\bigg( \frac{1}{n}\log{\sum_{i=1}^{n}{\frac{m_i}{\lvert \mathcal{M}_i\rvert}}}\bigg )}_{\rho_d} \underbrace {\bigg(\sum_{i=1}^{n}\log{\frac{\lvert \bigcap_{j=1}^{n} \mathcal{M}_j \rvert}{ \lvert \mathcal{M}_i\rvert} }\bigg)}_{\rho_v},
\end{equation}
where $\rho_d$ denotes the spatial density and $\rho_v$ represents the semantic density. 

Due to the occlusion issue brought by the different perspective views of LiDAR and the camera \cite{yuan2021pixel,zhiwei2024lcec} and the lack of adequate features when the point cloud is sparse, a single pair of virtual cameras is not enough for a comprehensive perception of the calibration scene. Define the event $E$ of capturing enough effective features, with its probability given by $P(E) = \lambda$. If we have $n_I$ intensity virtual cameras and $n_D$ depth virtual cameras, the probability of capturing enough effective features is $1 - (1 - \lambda)^n$. In theory, as $n$ increases, this probability will also increase. When $ n \to \infty $, $P(E')^n \to 0 $, there is $1 - (1 - \lambda)^n \to 1$. Generating more virtual cameras will enhance the possibility of capturing more potential correspondences, thus leading to higher calibration accuracy. However, it is impossible to apply an infinite number of virtual cameras during the calibration. Considering the trade-off between calibration accuracy and compute resources, we set the number of multiple virtual cameras to satisfy the balance of feature density:
\begin{equation}
\rho({^C_I}\!\boldsymbol{I}) + \rho({^C_D}\boldsymbol{I}) = \sum_{i=1}^{n_I}\rho({^C_I}\!\boldsymbol{I}_{i}) + \sum_{i=1}^{n_D}{\rho}({^V_D}\boldsymbol{I}_i).
\end{equation}
In practical applications, we set multiple virtual cameras inside a sphere originating from the LiDAR perspective center. Since the feature density is similar if the perspective viewpoint is close to the initial position, we can assume that $\rho({^V_I}\!\boldsymbol{I}_{i}) \approx \rho({^V_I}\!\boldsymbol{I}_{0})$ and $\rho({^V_D}\boldsymbol{I}_{i}) \approx \rho({^V_D}\boldsymbol{I}_{0})$. So $n_I$ and $n_D$ can be obtained by
\begin{equation}
n_I = \frac{\rho({^C_I}\!\boldsymbol{I})}{\rho({^V_I}\!\boldsymbol{I}_{0})}, n_D = \frac{\rho({^C_I}\!\boldsymbol{I})}{\rho({^V_D}\boldsymbol{I}_{0})}.
\end{equation}
Once all virtual cameras are generated, the discriminator performs image segmentation on each LiDAR projection captured from multiple views, detecting the corner points of the masks. These corner points serve as inputs for the dual-path correspondence matching.
%In the mask generation stage, EDI adjusts the appropriate IoU threshold $P_m$ for mask segmentation based on the feature density, which can be determined as follows:
%\begin{equation}
%P_m = \lambda + (1-\lambda)\frac{m_I + m_D}{m_I + m_D + 2m_C},
%\end{equation}
%where $\lambda$ is a robust parameter (usually determined by the pre-trained network) that ensures the reliability of the mask segmentation.
%To efficiently establish correspondences between LiDAR point clouds and camera images, previous methods typically configure the virtual camera with a relative transformation ${^{V}_{L}\boldsymbol{T}}$ from LiDAR as follows:

%This generates a LiDAR intensity projection (LIP) image $\boldsymbol{I}^L \in{\mathbb{R}^{H\times W \times 1}}$, framing the LCEC problem as a 2D feature matching task, where $H$ and $W$ represent the image height and width, respectively. However, due to the image distortion caused by varying perspectives and the absence of depth information in point clouds, this projection strategy struggles to capture a sufficient number of matchable features, particularly when using sparse LiDAR point clouds, such as those from mechanical LiDARs.
\subsection{Dual-Path Correspondence Matching}
\label{sec.dualpath_matching}

Given the segmented masks and detected corner points, dual-path correspondence matching leverages them to achieve dense and reliable 3D-2D key point correspondences. Similar to human perception systems, DPCM consists of two pathways, one for correspondence matching of LIP and RGB image, and the other for LDP and depth image.  For each pathway, DPCM adopted the approach outlined in \cite{zhiwei2024lcec} to obtain mask matching result $\mathcal{A} = \{(\mathcal{M}^V_i, \mathcal{M}^C_i)\}_{i = 1}^{m}$. Each matched mask pair $(\mathcal{M}^V_i,\mathcal{M}^C_i)$ can estimate an affine transformation matrix $[s\boldsymbol{R}^A,\boldsymbol{t}^A]$, which serves as a reference for dense key point matching. 
%Detailed derivation for $s\boldsymbol{R}_A$ and $\boldsymbol{t}_A$ is explained in \textcolor{blue}{Supplement} \ref{sec.sup_algo_detail}. 
Based on this affine transform, the corner points in the virtual image can be updated to a location that is close to its true projection coordinate in the real camera perspective using:
\begin{equation}
 \hat{\boldsymbol{c}}^V_i = s\boldsymbol{R}^A{(\boldsymbol{c}^V_i)} + \boldsymbol{t}^A.
\end{equation}
To determine optimum key point matches, we construct a cost matrix $\boldsymbol{M}^C$, where the element at $\boldsymbol{x} = [i,j]^\top$, namely:
\begin{equation}
\begin{aligned}
&\boldsymbol{M}^{C}(\boldsymbol{x}) = \\
&\rho_v\underbrace{\bigg(1-  \exp(\frac{\left\|\hat{\boldsymbol{c}}^V_i - \boldsymbol{c}^C_j\right\|}{L(\mathcal{A})}) 
   + {\sum_{k = 1}^{N}{H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k})}}}_{\text{Structral Cost}}\bigg) \\
&+ \rho_d\underbrace{\bigg(\frac{1}{b^2}\sum_{r = 1}^{b}\sum_{s = 1}^{b}{\left |\boldsymbol{D}^V_{i}(r,s)-\boldsymbol{D}^C_{j}(r,s) \right |}\bigg)}_{\text{Textrual Cost}}
\end{aligned}
\label{eq.adaptive_cost_func}
\end{equation}
denotes the matching cost between the $i$-th key point of a mask in the LiDAR virtual image and the $j$-th key point of a mask in the camera image. (\ref{eq.adaptive_cost_func}) consists of structural and textural costs. The structural cost measure the structural difference of corner points in the virtual and real image, where $L(\mathcal{A})$ denotes the average perimeter of the matched masks and $H(\boldsymbol{e}^V_{i,k},\boldsymbol{e}^C_{j,k})$ represents the difference between the structural similarity between current and target corner point. The dorsal component derives the relative spatial similarity of the $b$ neighboring zone. A strict criterion is applied to achieve reliable matching. Matches with the lowest costs in both horizontal and vertical directions of $\boldsymbol{M}^{C}(\boldsymbol{x})$ are determined as the optimum mask matches. Since every $\boldsymbol{c}^V_i$ can trace back to a LiDAR 3D point $\boldsymbol{p}^L_i = [x,y,z]^\top$, and every $\boldsymbol{c}^C_i$ is related to a pixel $\boldsymbol{p}_i = [u,v]^\top$ in the camera image, the final correspondence matching result of DPCM is $\mathcal{C} = \{(\boldsymbol{p}^L_i,\boldsymbol{p}_i) \mid i = 1,\dots, N\}$.

\subsection{Spatial-temporal relative pose optimization}
\label{sec.st_optimization}
%Based on DPM, we can obtain dense key point correspondences $\mathcal{K} = \{(\boldsymbol{p}_{k,1},\boldsymbol{p}_{k,1}^L), \dots, (\boldsymbol{p}_{k,N},\boldsymbol{p}_{k,N}^L) \}$, which store 2D pixels in the RGB image captured by camera and the corresponding 3D LiDAR points, respectively. 
%This list is ordered by confidence coefficients, from which the algorithm selects the $n_I$ and $n_D$ vectors with the lowest confidence as the final reference vectors, denoted as $\mathcal{V}_I = \{^{{I}}\boldsymbol{v}_1,\dots,^{{I}}\boldsymbol{v}_{n_I}\}$ and $\mathcal{V}_D = \{{^{{D}}\boldsymbol{v}_1},\dots,{^{{D}}\boldsymbol{v}_{n_D}}\}$. The poses of the virtual cameras can then be calculated as follows:
%\begin{equation}
%\left\{
%\begin{aligned}
%^{I}_{L}\boldsymbol{T}_i &= {^{V}_{L}\boldsymbol{T}} + {^{{I}}\boldsymbol{v}_i}, i \in [1,n_I],\\
%^{{D}}_{L}\boldsymbol{T}_j &= {^{V}_{L}\boldsymbol{T}} + {^{{D}}\boldsymbol{v}_j}, j \in [1,n_D],
%\end{aligned}
%\right.
%\label{eq.virtual_camera_pos}
%\end{equation}
%where $^{I}_{L}\boldsymbol{T}_i$ and $^{D}_{L}\boldsymbol{T}_j$ is the pose of $i$-th and $j$-th intensity virtual camera and depth virtual camera. The physical significance of (\ref{eq.virtual_camera_pos}) is to position the virtual camera as close as possible to the real camera, thereby increasing the overlap of features from LiDAR projections and camera images.

EdO-LCEC treats the sensor's operational environment as a spatial-temporal flow composed of $N$ multiple scenes across different time instances. In situations with incomplete or sparse point clouds, single-view methods such as \cite{yuan2021pixel,zhiwei2024lcec} are constrained by the limited number of high-quality matches. Our environment-driven approach addresses this limitation by integrating multi-view and multi-scene optimization. By merging the optimal matching results from multiple time instances and scenes, this spatial-temporal optimization enhances the selection of correspondences, maximizing the number of high-quality matches and ultimately improving overall calibration accuracy.

\noindent \textbf{Multi-View Optimization.}
In multi-view optimization of the $t$-th scene, the extrinsic matrix ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ can be formulated as follows:
\begin{equation}
{^{C}_{L}\hat{\boldsymbol{T}}_t} = \underset{^C_L{\boldsymbol{T}_{t,k}}}{\arg\min} \!\!
\sum_{i = 1}^{n_I + n_D}\!\sum_{(\boldsymbol{p}^L_j,\boldsymbol{p}_j)\in\mathcal{C}_n}\!\!{G\bigg(\underbrace {\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{t,k}}
 \boldsymbol{p}_{j}^L) - {\boldsymbol{p}}_{j}\right\|_2}_{\epsilon_j}\bigg)}, 
\label{eq.multi_view}
\end{equation}
where ${^{C}_{L}\boldsymbol{T}_{t,k}}$ denotes the $k$-th PnP solution obtained using a selected subset $\mathcal{V}_k$ of correspondences from $\mathcal{C}_j$, and $\epsilon_j$ represents the reprojection error with respect to ${^{C}_{L}\boldsymbol{T}_{t,k}}$. $G(\epsilon_j)$ is the gaussian depth normalized reprojection error under the projection model $\pi$. It is defined as follows:
\begin{equation}
G(\epsilon_j) = \frac{\mathcal{K}(d'_{j},\bar{d'})}
{C + \epsilon_j},
\label{eq.match_point_weight}
\end{equation}
where ${d'}_{j}$ denotes the normalized depth of $\boldsymbol{p}_{j}^L$, $\bar{d'}$ is the average normalized depth, and $\mathcal{K}(d'_{j},\bar{d'})$ is a gaussian kernal. 

\noindent \textbf{Multi-Scene Optimization.} 
%Using the estimated ${^{C}_{L}\hat{\boldsymbol{T}}_i}$ at each scenario, we choose a reliable subset  $\mathcal{S}_i = \{({\boldsymbol{p}^L_{i,j}}, \boldsymbol{p}_{i,j}) \mid j = 1, \dots, s_i\}$ of the matched point pairs as the input of the multi-scene optimization. The number $s_i$ is calculated by:
From each scenario, we choose a reliable subset $\mathcal{S}_i$, which can be obtained by follows:
\begin{equation}
    \mathcal{S}_t = \bigcap_{k=1}^{s_t}{\mathcal{V}_k}, \text{ } s_t = \min\{\frac{Q_{\max}q_t}{\sum^{N}_{j = 1}{q_j}}, q_{\max}\},
\end{equation}
where ${\mathcal{V}_k}$ is the correspondence subset with the $k$-th smallest mean reprojection error. $Q_{\max}$ and $q_{\max}$ are the max subsets number of all scenarios and max reliable subsets in a single scene. The multi-scene optimization solves the final extrinsic matrix ${^{C}_{L}{\boldsymbol{T}}^*}$ by minimizing the joint reprojection error of multiple scenarios in the temporal dimension:
\begin{equation}
\mathcal{L}({^{C}_{L}{\boldsymbol{T}}_{t,k}}) = 
\sum_{t = 1}^{N} {\sum_{(\boldsymbol{p}^L_{j},\boldsymbol{p}_{j})\in\mathcal{S}_t}\!{G\bigg(\left\|\pi(
{^{C}_{L}\boldsymbol{T}_{t,k}}
 \boldsymbol{p}_{j}^L) - {\boldsymbol{p}}_{j}\right\|_2}\bigg)}.
\end{equation}
%resulting in a significant increase in calibration accuracy.

\section{Experiment}
\label{sec.experiment}


\subsection{Experimental Setup and Evaluation Metrics}
\label{sec.exp_setup}

\begin{table*}[t!]
\caption{Quantitative comparisons with SoTA target-free LCEC approaches on the KITTI Odemotry 00 sequence. The best results are shown in bold type. \dag: These methods did not
release (English-speaking) code preventing reproducing results for both cameras.}
\centering
\fontsize{6.5}{10}\selectfont
\begin{tabular}{l|c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c}
\toprule
\multirow{3}*{Approach}& \multirow{3}*{Initial Range}&\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\cline{3-18}
&&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\

&& $e_r$ ($^\circ$) & $e_t$ (m) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (m)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}\\
\hline

CalibRCNN\textsuperscript{\dag} \cite{shi2020calibrcnn} &$\pm10^\circ / \pm 0.25m$ &0.805 &0.093	&0.446	&0.640	&0.199	&0.062	&0.043 &0.054 &- &-	&-	&-	&-	&-	&-	&-\\
CalibDNN\textsuperscript{\dag} \cite{zhao2021calibdnn} &$\pm10^\circ / \pm 0.25m$ &1.021 &0.115	&0.200	&0.990	&0.150	&0.055	&0.032	&0.096 &- &-&-	&-	&-	&-	&-	&-\\
RegNet\textsuperscript{\dag} \cite{schneider2017regnet} &$\pm20^\circ / \pm 1.5m$ &0.500 &0.108	&0.240	&0.250	&0.360	&0.070	&0.070	&0.040 &- &-&-	&-	&-	&-	&-	&-\\
Borer et al.\textsuperscript{\dag} \cite{borer2024chaos}&$\pm1^\circ / \pm 0.25m$  &0.455 &0.095	&0.100 &0.440 &\textbf{0.060}  &\textbf{0.037} &0.030 &0.082 &- &- &-	&-	&-	&-	&-	&-\\
LCCNet \cite{lv2021lccnet} &$\pm10^\circ / \pm 1.0m$ &1.418
&0.600  &0.455 &0.835 &0.768 &0.237 &0.333  &0.329 &1.556  &0.718 & 0.457 &1.023 &0.763 &0.416 &0.333 &0.337 \\
RGGNet \cite{yuan2020rggnet} &$\pm20^\circ / \pm 0.3m$ &1.29 &0.114	&0.640 &0.740&0.350 &0.081 &\textbf{0.028} &0.040 &3.870 &0.235 &1.480 &3.380 &0.510	&0.180 &\textbf{0.056} &0.061 \\
CalibNet \cite{iyer2018calibnet} &$\pm10^\circ / \pm 0.2m$ &5.500 &0.130	&3.050 &3.000 &3.000 &0.070 &0.067 &0.086 &5.096 &0.125	&2.881 &2.908 &3.035 &\textbf{0.064} &0.065 &0.085\\
\hline
CRLF \cite{ma2021crlf} &- &0.629	&4.118	&\textbf{0.033}	&0.464	&0.416	&3.648	&1.483	&0.550 &0.633	&4.606	&0.039	&0.458	&0.424	&4.055	&1.636	&0.644\\
UMich \cite{pandey2015automatic} &- &4.161	&0.321	&0.113	&3.111	&2.138	&0.286	&0.077	&0.086
 &4.476 &0.339	&0.137	&3.342	&2.309	&0.291	&0.108	&0.094\\
HKU-Mars \cite{yuan2021pixel} &- &33.84	&6.355	&19.89	&18.71	&19.33	&3.353	&3.232	&2.419
&32.89	&4.913	&18.99	&15.77	&17.00	&2.917	&2.564	&1.646\\
DVL \cite{koide2023general}  &-&{122.1} &{5.129}	&{48.64}	&{87.23}	&{98.15}	&{2.832}	&{2.920}	&{1.881} &{120.5} &{4.357}	&{49.60}	&{87.99}	&{96.72}	&{2.086}	&{2.517}	&{1.816}\\
MIAS-LCEC \cite{zhiwei2024lcec}  &-&5.385	&1.014	&1.574	&4.029	&4.338	&0.724	&0.383	&0.343 &7.655	&1.342	&1.910	&5.666	&6.154	&0.843	&0.730	&0.358\\
\hline
\textbf{EdO-LCEC (Ours)}  &- &\textbf{0.288}	&\textbf{0.087}	&0.157	&\textbf{0.139}	&{0.131}	&0.050	&0.053	&\textbf{0.029} 
&\textbf{0.336}	&\textbf{0.118}	&0.216	&\textbf{0.168}	&\textbf{0.121}	&0.083	&0.067	&\textbf{0.032}
\\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti00}
\end{table*}

\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on KITTI Odometry (01-09 sequence). The best results are shown in bold type.}
\centering
\fontsize{6.7}{10}\selectfont
\begin{tabular}{l|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{01} & \multicolumn{2}{c|}{02} & \multicolumn{2}{c|}{03}  & \multicolumn{2}{c|}{04} & \multicolumn{2}{c|}{ 05} &  \multicolumn{2}{c|}{06}&  \multicolumn{2}{c|}{07} & \multicolumn{2}{c|}{08} & \multicolumn{2}{c}{09}\\
\cline{2-19}
& $e_r$ & $e_t$& $e_r$ & $e_t$ & $e_r$ & $e_t$  & $e_r$ & $e_t$ & $e_r$ & $e_t$ & $e_r$ & $e_t$ & $e_r$  & $e_t$ & $e_r$  & $e_t$ & $e_r$  & $e_t$\\
\hline
CRLF \cite{ma2021crlf}  
&\textbf{0.623}	&7.363
&0.632	&3.642
&0.845	&6.007
&\textbf{0.601}	&0.372
&0.616	&5.961
&0.615	&25.762
&0.606	&1.807
&0.625	&5.376
&0.626	&5.133\\
UMich \cite{pandey2015automatic}  &2.196	&\textbf{0.305} &3.733
&0.331	&3.201&0.316 &2.086&0.348	&3.526
&0.356 &2.914&0.353	&3.928&0.368 &3.722
&0.367 
&3.117	&0.363\\
HKU-Mars \cite{yuan2021pixel}  &20.727	&3.770
&32.95	&12.70
&21.99	&3.493
&4.943	&0.965
&34.42	&6.505
&25.20	&7.437
&33.10	&7.339
&26.62	&8.767
&20.38	&3.459\\
DVL \cite{koide2023general}  &112.0&2.514
&120.6&4.285	
&124.7&4.711
&113.5&4.871
&123.9&4.286
&128.9&5.408	
&124.7&5.279 
&126.2&4.461 
&116.7	&3.931\\

MIAS-LCEC \cite{zhiwei2024lcec}  
&0.621	&0.300
&0.801	&0.327
&1.140	&0.324
&0.816	&0.369
&4.768	&0.775
&2.685	&0.534
&11.80	&1.344
&5.220	&0.806
&0.998	&0.432
\\
\textbf{EdO-LCEC (Ours)} &2.269 &0.459
 &\textbf{0.561}&\textbf{0.142}
&\textbf{0.737}&\textbf{0.137} &1.104&\textbf{0.339}
&\textbf{0.280}&\textbf{0.093} &\textbf{0.485}&\textbf{0.124} &\textbf{0.188}&\textbf{0.076} &\textbf{0.352}&\textbf{0.115}
&\textbf{0.386}	&\textbf{0.120}\\
\bottomrule
\end{tabular}
\label{tab.rescmp_kitti_01_08}
\end{table*}


\begin{table*}[t!]
\caption{Comparisons with SoTA target-free LCEC approaches on MIAS-LCEC-TL70 Dataset. The best results are shown in bold type.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}
\toprule
%&\multicolumn{9}{c}{KITTI Sequence} \\
\multirow{2}*{Approach}& \multicolumn{2}{c|}{\makecell{Residential \\ Community}} & \multicolumn{2}{c|}{Urban Freeway} & \multicolumn{2}{c|}{Building}  & \multicolumn{2}{c|}{\makecell{Challenging \\Weather}} & \multicolumn{2}{c|}{Indoor} &  \multicolumn{2}{c|}{\makecell{Challenging \\Illumination}} &  \multicolumn{2}{c}{All} \\
\cline{2-15}
& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)& $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
CRLF \cite{ma2021crlf}  &1.594&0.464
&1.582&0.140 &1.499 &20.17 &1.646&2.055
&1.886 &30.05 &1.876 &19.047 &1.683 &11.133 \\
UMich \cite{pandey2015automatic}  &4.829
&0.387 &2.267&0.166 &11.914 &0.781 &1.851 &0.310 &2.029  &0.109 &5.012 &0.330 &4.265 &0.333\\
HKU-Mars \cite{yuan2021pixel}  &2.695
&1.208 &2.399 &1.956 &1.814 &0.706 &2.578 &1.086  &2.527 &0.246 &14.996 &3.386 &3.941 &1.261
 \\
DVL \cite{koide2023general}  &0.193
&0.063
&0.298
&0.124
&0.200
&0.087
&0.181
&0.052
&0.391
&0.030
&1.747
&0.377
&0.423
&0.100 \\

MIAS-LCEC \cite{zhiwei2024lcec}  &0.190 &0.050 &\textbf{0.291} &0.111
&\textbf{0.198}&0.072&\textbf{0.177}&0.046&0.363&\textbf{0.024}&0.749&0.118&0.298&0.061 \\
\textbf{EdO-LCEC (Ours)} &\textbf{0.165} &\textbf{0.044} &0.295 &\textbf{0.105} &0.235 &\textbf{0.054}  &0.179 &\textbf{0.046} &\textbf{0.315} &0.025 &\textbf{0.474} &\textbf{0.103} &\textbf{0.255} &\textbf{0.055} \\
\bottomrule
\end{tabular}
\label{tab.rescmp_mias_tf70}
\end{table*}

%\begin{table}[t!]
%\caption{Ablation study of different components: Multi-View Optimization (intensity and depth) and Multi-Scene Optimization.}
%\centering
%\fontsize{6.9}{10}\selectfont
%\begin{tabular}{c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c}
%\toprule
%%&\multicolumn{9}{c}{KITTI Sequence} \\
%\multirow{2}*{\makecell{Multi-View \\ (intensity)}} & \multirow{2}*{\makecell{Multi-View \\ (depth)}} & \multirow{2}*{Multi-Scene} &\multicolumn{2}{c|}{KITTI}  & \multicolumn{2}{c|}{MIAS-LCEC-TF70} &  \multicolumn{2}{c}{MIAS-LCEC-TF360} \\
%\cline{4-9}
%&&& {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)} & {$e_r$ ($^\circ$)} &  {$e_t$ (m)}\\
%
%\hline
%\hline
%\checkmark &  & &0.931 &0.243 &0.931 &0.243 &0.931 &0.243	\\
% & \checkmark & &0.677 &0.251&0.931 &0.243 &0.931 &0.243	\\
% \checkmark & \checkmark & &0.412 &0.157	&0.931 &0.243 &0.931 &0.243\\
%\checkmark &  & \checkmark&0.316 &0.105	&0.931 &0.243 &0.931 &0.243\\
% & \checkmark & \checkmark&0.339 &0.098	&0.931 &0.243 &0.931 &0.243\\
% \checkmark & \checkmark & \checkmark&0.288 &0.931 &0.243 &0.931 &0.243
% &0.087	\\
%
%\bottomrule
%\end{tabular}
%\label{tab.rescmp_mias}
%\end{table}
In our experiments, we compare our proposed method with SoTA LCEC approaches on the public dataset KITTI Odometry \cite{geiger2012we} and MIAS-LCEC \cite{zhiwei2024lcec}. To comprehensively evaluate the calibration performance, we use the magnitude $e_r$ of Euler angle error and the magnitude $e_t$ of translation error to quantify the performance of target-free LCEC approaches. 

Notably, sequences in KITTI Odometry except for 00 were included in the training datasets for the learning-based methods \cite{lv2021lccnet,yuan2020rggnet,borer2024chaos,zhao2021calibdnn}. To ensure a fair comparison, if the authors provided their code, we reproduced the calibration results for both the left and right cameras on the 00 sequence. Otherwise, we used the reported results from their papers for the left camera. Since most learning-based methods lack APIs for user-customized data, we restrict our comparison with these methods to the KITTI Odometry 00 sequence.
%Since most learning-based approaches do not provide APIs for user-customized data, we only compare these methods on KITTI Odometry. Noted that the other sequences of KITTI Odometry except 00 was seen during the training of these learning-based approaches. To ensure a fair comparison, if the authors released their code, we reproduce the calibration results for both the left and the right cameras on 00 sequence. If not, we use the experimental results presented for left cameras in their paper.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/SparseTableVisual.pdf}
    \caption{Evaluation on the adaptability of different methods to sparse point clouds: In the figure, the point cloud density decreases progressively, with $1/x$ representing the level of point cloud downsampling.}
    \label{fig.visualization_sparse_pcloud}
\end{figure*}

\subsection{Comparison with State-of-the-Art Method}
\label{sec.exp_dataset}
Quantitative comparisons with SoTA approaches on three datasets are presented in  Fig. \ref{fig.exp_on_different_density}, Tables \ref{tab.rescmp_kitti00}\footnote{The results of LCCNet produced by their code is larger than those presented in their paper.}, \ref{tab.rescmp_kitti_01_08} and \ref{tab.exp_mid360}. Additionally, qualitative results are illustrated in Figs. \ref{fig.datafusion_on_checkerboard} and \ref{fig.SoTA_visualization}. 


\begin{table}[t!]
\caption{
Quantitative comparison of our proposed MIAS-LCEC approach with other SoTA online, target-free approaches on the MIAS-LCEC-TF360 dataset, where the best results are shown in bold type. The best results are shown in bold type.}
\centering
\settablefont
\begin{tabular}{c|rr|rr}
\toprule
Approach & \multicolumn{2}{c|}{Indoor} & \multicolumn{2}{c}{Outdoor}\\
 & $e_r$ ($^\circ$) & $e_t$ (m)  & $e_r$ ($^\circ$) & $e_t$ (m)\\
\hline
CRLF \cite{ma2021crlf}  &1.469	&13.484&1.402  	&0.139\\
UMich \cite{pandey2015automatic} &1.802&0.200	&2.698 	&0.135\\
\makecell{HKU-Mars \cite{yuan2021pixel}}  &96.955	&4.382&25.611  &9.914\\
\makecell{DVL \cite{koide2023general}} &63.003	&0.919 &46.623 	&1.778\\
MIAS-LCEC \cite{zhiwei2024lcec} &0.963 &0.182 &0.659 &0.114 \\
\textbf{EdO-LCEC (Ours)} &\textbf{0.724} &\textbf{0.107} &\textbf{0.351}	&\textbf{0.080}\\
\bottomrule
\end{tabular}
\label{tab.exp_mid360}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/CheckerBoardV2.pdf}
    \caption{Visualization of EdO-LCEC calibration results through LiDAR and camera data fusion: (a)-(d) are four LiDAR point clouds partially rendered by color using the estimated extrinsic matrix.}
    \label{fig.datafusion_on_checkerboard}
\end{figure}


\noindent \textbf{ Evaluation on KITTI Dataset.} 
%The results shown in Table \ref{tab.rescmp_kitti00} and Table \ref{tab.rescmp_kitti_01_08} suggest that our method achieves SoTA performance on KITTI Odometry dataset. Specifically, EdO-LCEC reducing the $e_r$ by around 30-93\% and the $e_t$ by 39-99\% in 00 sequence. Although the calibration accuracy is slightly worse than \cite{ma2021crlf} in the 04 sequence, EdO-LCEC outperforms all the other traditional target-free approaches in the 01-09 sequence. 
The results shown in Table \ref{tab.rescmp_kitti00} and Table \ref{tab.rescmp_kitti_01_08} suggest that our method achieves SoTA performance across the total ten sequences (00-09) in KITTI Odometry. Specifically, EdO-LCEC reduces the $e_r$ by around 30-93\% and the $e_t$ by 39-99\% in 00 sequence. Additionally, according to Figure \ref{fig.SoTA_visualization}, it can be observed that a single frame in KITTI is so sparse that all the other approaches behave poorly. In contrast, our proposed method overcome this difficulty and achieve high quality data fusion through the calibration result. We attribute these performance improvements to our spatial-temporal relative pose optimization. By merging the optimal matching results from multiple views and scenes, this spatial-temporal optimization enhances the selection of correspondences, maximizing the number of high-quality matches and ultimately improving overall calibration accuracy.

\noindent \textbf{Evaluation on MIAS-LCEC Dataset.} 
%Point clouds in MIAS-LCEC-TF70 are dense and have similar FoV with the camera RGB images. 
Compared to the sparse point clouds in the KITTI dataset, the point clouds in the MIAS dataset are significantly denser, which facilitates feature matching and allows us to test the upper limits of the algorithm calibration accuracy. 

The results shown in Table \ref{tab.rescmp_mias_tf70} demonstrate that EdO-LCEC achieves lower mean $e_r$ and $e_t$ values than all other approaches across the total six subsets in MIAS-LCEC-TF70. Our method dramatically outperforms UMich, DVL, HKU-Mars, and is slightly better than MIAS-LCEC in scenarios with low noise and abundant features. In challenging conditions that under poor illumination and adverse weather, or when few geometric features are detectable, EdO-LCEC performs significantly better than all methods, particularly. This impressive performance can be attributed to the generalizable scene discriminator. The multiple virtual cameras generated by the scene discriminator provide a comprehensive perception of the calibration scene from both spatial and textural perspectives, which largely increases the possibility of capturing high-quality correspondences for the PnP solver. Furthermore, to better demonstrate our calibration accuracy, we illustrate the data fusion result in Figure \ref{fig.datafusion_on_checkerboard} using our optimized extrinsic matrix.
\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figs/SparseTableV3.pdf}
    \caption{Evaluation on the adaptability of different methods to sparse point cloud.}
    \label{fig.exp_on_different_density}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/SoTAVisualizationV2.pdf}
    \caption{Qualitative comparisons with SoTA target-free LCEC approaches on the MIAS-LCEC-TF70 dataset: (a)-(d) RGB images, Depth images, LiDAR projection, and their segmentation results; (e)-(f) experimental results achieved using MIAS-LCEC (ours), LCCNet, CalibNet and DVL, shown by merging LIP and RGB images, where significantly improved regions are shown with red dashed boxes.}
    \label{fig.SoTA_visualization}
\end{figure*}

\begin{table*}[t!]
\caption{Ablation study of different components: Multi-View Optimization (intensity and depth) and Multi-Scene Optimization.}
\centering
\fontsize{6.9}{10}\selectfont
\begin{tabular}{c@{\hspace{0.15cm}}c|c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c|c@{\hspace{0.15cm}}c@{\hspace{0.15cm}}c}
\toprule
\multicolumn{3}{c|}{Components}&\multicolumn{8}{c|}{Left Camera} &\multicolumn{8}{c}{Right Camera}\\
\hline
\multicolumn{2}{c|}{Multi-View}&\multirow{2}*{\makecell{Multi\\Scene}}&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c|}{Translation Error (m)} 
&\multicolumn{2}{c|}{Magnitude}
&\multicolumn{3}{c|}{Rotation Error ($^\circ$)} &\multicolumn{3}{c}{Translation Error (m)}\\
\cline{1-2}
Intensity&Depth&&$e_r$ ($^\circ$) & $e_t$ (m) & Yaw & Pitch & Roll  & {X} &  {Y} &  {Z}   & $e_r$ ($^\circ$) & {$e_t$ (m)} & Yaw & Pitch & Roll   &  {X} &  {Y} &  {Z}\\
\hline
&& &1.625	&0.459	&0.820	&0.669	&0.899	&0.247	&0.235	&0.211
 &1.620	&0.472	&0.915	&0.691	&0.807	&0.257	&0.254	&0.197\\
\checkmark&&&1.387	&0.358	&0.755	&0.579	&0.711	&0.189	&0.208	&0.153
 &1.641	&0.459	&1.012	&0.679	&0.738	&0.257	&0.257	&0.178\\
\checkmark&\checkmark& &1.125	&0.280	&0.534	&0.534	&0.613	&0.134	&0.151	&0.136
&1.425	&0.354	&0.856	&0.563	&0.679	&0.186	&0.205	&0.138\\
&& \checkmark&0.406	&0.155	&0.180	&0.201	&0.223	&0.119	&0.067	&0.049
 &0.447	&0.192	&0.227	&0.243	&0.211	&0.148	&0.094	&0.051\\
\checkmark&& \checkmark&0.339	&0.110	&0.179	&0.167	&0.162	&0.069	&0.063	&0.039
&0.480	&0.138	&0.322	&0.239	&0.150	&0.096	&0.084	&0.033\\
\checkmark&\checkmark& \checkmark &\textbf{0.295}	&\textbf{0.082}	&\textbf{0.117}	&\textbf{0.176}	&\textbf{0.150}	&\textbf{0.051}	&\textbf{0.044}	&\textbf{0.032} &\textbf{0.336}	&\textbf{0.118}	&\textbf{0.216}	&\textbf{0.168}	&\textbf{0.121}	&\textbf{0.083}	&\textbf{0.067}	&\textbf{0.032}\\
\bottomrule
\end{tabular}
\label{tab.ablation}
\end{table*}
%The results on the MIAS-LCEC-TF360 dataset further prove our outstanding performance. From Table \ref{tab.exp_mid360}, it is evident that while the other approaches achieve poor performance on this dataset, our approach demonstrates excellent performance, indicating strong adaptability to more challenging scenarios, with narrow overlapping areas between LiDAR projection images and camera RGB images. This impressive performance can be attributed to MobileSAM, a powerful LVM, capable of learning informative, general-purpose deep features for robust image segmentation. Furthermore, our proposed dual-path correspondence matching algorithm, mimics the human visual system to conduct feature matching on separate pathways and sets strict criteria for reliable correspondence selection to generate dense correspondences, thereby improving the quality of the PnP solutions.
The results on the MIAS-LCEC-TF360 dataset further prove our outstanding performance. From Table \ref{tab.exp_mid360}, it is evident that while the other approaches achieve poor performance on this dataset, our approach demonstrates excellent accuracy, indicating strong adaptability to more challenging scenarios, with narrow overlapping areas between LiDAR projection images and camera RGB images. This impressive performance can be attributed to our proposed DPCM, a powerful cross-modal feature matching algorithm. DPCM utilize structural and textural consistency to jointly constrain correspondences matching on both spatial and textural pathways. This largely increase reliable correspondences compared to DVL and MIAS-LCEC, thereby providing a more reliable foundation for extrinsic parameter optimization. 
%Additionally, the powerful LVM, MobileSAM, employed in our scene discriminator is capable of learning informative, general-purpose deep features for robust image segmentation. This significantly improve the adaptability of our method toward scene with little textural or spatial features.

To evaluate the algorithm's adaptability in cases of incomplete or sparse point clouds, we further segmented the already limited field-of-view point clouds from MIAS-LCEC-TF360. Specifically, as depicted in Fig. \ref{fig.visualization_sparse_pcloud}, we divided each point cloud into six equal parts, incrementally combining these segments to create point clouds with varying densities, ranging from 1/6 to the full 6/6 density. The box plots in Fig. \ref{fig.exp_on_different_density} suggest that EdO-LCEC demonstrates significantly greater stability and accuracy across different levels of point cloud density compared to other algorithms. This highlights EdO-LCEC’s robustness and adaptability, maintaining high precision even under challenging conditions of sparse or incomplete data.

%As shown in Figure \ref{fig.exp_on_different_density}, as the density of the point clouds decreases, the decline in the accuracy of our proposed algorithm is notably slower compared to other algorithms. This indicates that our method demonstrates significantly better adaptability to sparse point clouds than the alternatives.


\subsection{Ablation Study and Analysis}
\label{sec.ablation_study}
To explore the contribution of scene discriminator and spatial-temporal relative pose optimization, we evaluate our algorithm’s performance with and without multi-view and multi-scene optimizations ( including both textural and spatial perspectives generated by the scene discriminator). The results, presented in Table \ref{tab.ablation}, demonstrate that lacking any of these components significantly degrades performance. In particular, calibration errors increase substantially when only single-view calibration is applied, as it lacks the comprehensive constraints provided by multi-view inputs necessary for spatial-temporal optimization. These results confirm the advantage of incorporating spatial and textural constraints from multiple views and scenes, validating the robustness and adaptability of our environment-driven calibration strategy.
%To investigate the impact of the scene discriminator and spatial-temporal relative pose optimization, we evaluated the performance of our algorithm with and without multi-view (including both textural and spatial perspectives generated by the scene discriminator) and multi-scene optimizations. The results, presented in Table \ref{tab.ablation}, demonstrate that lacking any of these components significantly degrades performance. Notably, calibration errors increase substantially when relying on single-view calibration alone, as it lacks the spatial-temporal optimization provided by multi-view inputs from the scene discriminator. This outcome confirms the effectiveness of incorporating spatial and textural constraints from multiple views and scenes. These findings confirm the robustness of our environment-driven calibration strategy. 

%in complex scenarios like those in KITTI Odometry, particularly in cases where LiDAR and camera overlap is limited.

%This ablation study thoroughly validates the superiority of our proposed environment-driven calibration strategy. Unlike approaches that optimize extrinsics within a single viewpoint, our method treats the sensor’s operational environment as a spatiotemporal flow field. By fully leveraging spatial and texture constraints within the environment, our approach achieves more robust and reliable online calibration.
%of multi-scene optimization on the KITTI dataset demonstrates that environment-driven joint optimization of extrinsic parameters across multiple scenes can significantly enhance calibration accuracy in sparse point clouds.}
\section{Conclusion}
\label{sec.conclusion}
%In this paper, we explore to extend a new definition called ``environment-driven" for online LiDAR-camera extrinsic calibration. In contrast to previous algorithms, our approach is no longer limited by the type of calibration scene, enabling generalization across various complex environments. We designed a scene discriminator inspired by human perceptual systems, allowing the algorithm to actively comprehend the environment from multiple views under spatial and textural perspectives. By effectively utilizing structural and textural consistency between LiDAR point cloud projections and camera images, we achieved more efficient cross-modal correspondence matching, addressing the challenge of unreliable feature matching in sparse scenes that earlier methods faced. We modeled the calibration process as a spatial-temporal joint optimization problem, achieving high-precision and robust online calibration through multi-view optimization within a single scene and joint optimization across multiple scenes. Extensive experiments on real-world datasets demonstrate that our algorithm achieves state-of-the-art calibration performance.
In this paper, we explored to extend a new definition called ``environment-driven" for online LiDAR-camera extrinsic calibration. Unlike previous methods, our approach is adaptable to diverse and complex calibration scenes, enabling broader generalization across environments. Inspired by human perceptual systems, we designed a scene discriminator that actively analyzes the environment from multi-view spatial and textural perspectives. By leveraging structural and textural consistency between LiDAR point cloud projections and camera images, our method achieves efficient cross-modal correspondence matching, effectively overcoming the challenge of unreliable feature matching in sparse scenes encountered by earlier approaches. We modeled the calibration process as a spatial-temporal joint optimization problem, achieving high-precision and robust online calibration through multi-view optimization within individual scenes and joint optimization across multiple scenarios. Extensive experiments on real-world datasets demonstrate that our algorithm achieves state-of-the-art calibration accuracy and robustness.

\clearpage

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{X_suppl}

\end{document}
