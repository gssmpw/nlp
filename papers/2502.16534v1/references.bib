@article{adamczykExaminingPublicOpinion2020,
  title = {Examining Public Opinion about Abortion: A Mixed-methods Systematic Review of Research over the Last 15 Years},
  shorttitle = {Examining Public Opinion about Abortion},
  author = {Adamczyk, Amy and Kim, Chunrye and Dillon, Leevia},
  year = {2020},
  month = nov,
  journal = {Sociological Inquiry},
  volume = {90},
  number = {4},
  pages = {920--954},
  issn = {0038-0245, 1475-682X},
  doi = {10.1111/soin.12351},
  urldate = {2025-01-15},
  abstract = {Although abortion became legal four decades ago, Americans remain staunchly divided over its acceptability. While researchers have been interested in the factors shaping abortion attitudes, there are almost no reviews of this work. We examine the factors shaping Americans' abortion attitudes and assess the state of published peer-reviewed articles in this area over the last 15~years. Using a mixed-methods systematic review, we analyze and critique the findings from 116 journal articles that have examined attitudes about abortion between 2001 and 2016. Among the many predictors and outcomes examined, we show that religion is by far the most utilized statistically significant independent variable, followed by education and income/employment. In addition to examining the factors that shape~attitudes, we provide insight into the characteristics of this published work. We offer several suggestions for improving research on this important topic, including a better utilization of social science theory, examining the attitudes of teens, increasing the use of mixed-methods studies, and drawing on longitudinal data and analyses that consider the influence of the larger context for shaping attitudes.},
  langid = {english}
}

@article{alemanValueOrientationsWorld2016,
  title = {Value {{Orientations From}} the {{World Values Survey}}: {{How Comparable Are They Cross-Nationally}}?},
  shorttitle = {Value {{Orientations From}} the {{World Values Survey}}},
  author = {Alem{\'a}n, Jos{\'e} and Woods, Dwayne},
  year = {2016},
  month = jul,
  journal = {Comparative Political Studies},
  volume = {49},
  number = {8},
  pages = {1039--1067},
  publisher = {SAGE Publications Inc},
  issn = {0010-4140},
  doi = {10.1177/0010414015600458},
  urldate = {2023-06-29},
  abstract = {We examine data from the World Values Survey regarding the existence of two consistent orientations in mass values, traditional versus secular/rational and survival versus self-expression. We also evaluate the empirical validity of Welzel?s revised value orientations: secular and emancipative. Over the years, a large body of work has presumed the stability and comparability of these value orientations across time and space. Our findings uncover little evidence of the existence of traditional?secular/rational or survival?self-expression values. Welzel?s two dimensions of value orientations?secular and emancipative?seem more reflective of latent value orientations in mass publics but are still imperfectly capturing these orientations. More importantly, these value orientations do not seem very comparable except among a small number of advanced post-industrial democracies. We call attention to the use of value measurements to explain important macro-level phenomena.}
}

@inproceedings{alkhamissiInvestigatingCulturalAlignment2024,
  title = {Investigating Cultural Alignment of Large Language Models},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {AlKhamissi, Badr and ElNokrashy, Muhammad and Alkhamissi, Mai and Diab, Mona},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {12404--12422},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.671},
  urldate = {2025-02-03},
  abstract = {The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions---firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.},
  langid = {english}
}

@inproceedings{aroraProbingPretrainedLanguage2023,
  title = {Probing Pre-Trained Language Models for Cross-Cultural Differences in Values},
  booktitle = {Proceedings of the {{First Workshop}} on {{Cross-Cultural Considerations}} in {{NLP}} ({{C3NLP}})},
  author = {Arora, Arnav and Kaffee, Lucie-aim{\'e}e and Augenstein, Isabelle},
  editor = {Dev, Sunipa and Prabhakaran, Vinodkumar and Adelani, David Ifeoluwa and Hovy, Dirk and Benotti, Luciana},
  year = {2023},
  month = may,
  pages = {114--130},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.c3nlp-1.12},
  urldate = {2025-02-10},
  abstract = {Language embeds information about social, cultural, and political values people hold. Prior work has explored potentially harmful social biases encoded in Pre-trained Language Models (PLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which cross-cultural values are embedded in these models, and whether they align with existing theories and cross-cultural values surveys. We find that PLMs capture differences in values across cultures, but those only weakly align with established values surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PLMs with values surveys.},
  langid = {english}
}

@misc{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-09},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/UM6TLIH3/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/Users/jonathanrystrom/Zotero/storage/H8BVFZ8L/2204.html}
}

@article{birhaneAlgorithmicColonizationAfrica2020,
  title = {Algorithmic Colonization of {{Africa}}},
  author = {Birhane, Abeba},
  year = {2020},
  journal = {SCRIPTed},
  volume = {17},
  pages = {389},
  publisher = {HeinOnline}
}

@inproceedings{birhanePowerPeopleOpportunities2022,
  title = {Power to the {{People}}? {{Opportunities}} and {{Challenges}} for {{Participatory AI}}},
  shorttitle = {Power to the {{People}}?},
  booktitle = {Equity and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Birhane, Abeba and Isaac, William and Prabhakaran, Vinodkumar and Diaz, Mark and Elish, Madeleine Clare and Gabriel, Iason and Mohamed, Shakir},
  year = {2022},
  month = oct,
  series = {{{EAAMO}} '22},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3551624.3555290},
  urldate = {2023-07-25},
  abstract = {Participatory approaches to artificial intelligence (AI) and machine learning (ML) are gaining momentum: the increased attention comes partly with the view that participation opens the gateway to an inclusive, equitable, robust, responsible and trustworthy AI. Among other benefits, participatory approaches are essential to understanding and adequately representing the needs, desires and perspectives of historically marginalized communities. However, there currently exists lack of clarity on what meaningful participation entails and what it is expected to do. In this paper we first review participatory approaches as situated in historical contexts as well as participatory methods and practices within the AI and ML pipeline. We then introduce three case studies in participatory AI. Participation holds the potential for beneficial, emancipatory and empowering technology design, development and deployment while also being at risk for concerns such as cooptation and conflation with other activities. We lay out these limitations and concerns and argue that as participatory AI/ML becomes in vogue, a contextual and nuanced understanding of the term as well as consideration of who the primary beneficiaries of participatory activities ought to be constitute crucial factors to realizing the benefits and opportunities that participation brings.},
  isbn = {978-1-4503-9477-2},
  keywords = {Justice,Machine Learning,Participatory AI,Power}
}

@inproceedings{birhaneValuesEncodedMachine2022,
  title = {The {{Values Encoded}} in {{Machine Learning Research}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {173--184},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3531146.3533083},
  urldate = {2023-01-23},
  abstract = {Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.},
  isbn = {978-1-4503-9352-2},
  keywords = {Corporate ties,Encoded values of ML,ICML,NeurIPS,Power asymmetries}
}

@inproceedings{blevinsLanguageContaminationHelps2022,
  title = {Language Contamination Helps Explains the Cross-Lingual Capabilities of English Pretrained Models},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Blevins, Terra and Zettlemoyer, Luke},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {3563--3574},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.233},
  urldate = {2025-02-10},
  abstract = {English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1\% of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer.},
  langid = {english}
}

@misc{brockmanIntroducingChatGPTWhisper2023,
  title = {Introducing {{ChatGPT}} and Whisper {{APIs}}},
  author = {Brockman, Greg and Eleti, Atty and Georges, Elie and Jang, Joanne and Kilpatrick, Logan and Lim, Rachel and Miller, Luke and Pokrass, Michelle},
  year = {2023},
  month = mar,
  journal = {OpenAI},
  urldate = {2025-01-15},
  abstract = {Developers can now integrate ChatGPT and Whisper models into their apps and products through our API.},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/L9D6FRC3/introducing-chatgpt-and-whisper-apis.html}
}

@inproceedings{bugliarelloIGLUEBenchmarkTransfer2022,
  title = {{{IGLUE}}: {{A Benchmark}} for {{Transfer Learning}} across {{Modalities}}, {{Tasks}}, and {{Languages}}},
  shorttitle = {{{IGLUE}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Bugliarello, Emanuele and Liu, Fangyu and Pfeiffer, Jonas and Reddy, Siva and Elliott, Desmond and Ponti, Edoardo Maria and Vuli{\'c}, Ivan},
  year = {2022},
  month = jun,
  pages = {2370--2392},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-07-25},
  abstract = {Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together\{---\}by both aggregating pre-existing datasets and creating new ones\{---\}visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target\{--\}source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.},
  langid = {english}
}

@inproceedings{caoAssessingCrossculturalAlignment2023,
  title = {Assessing Cross-Cultural Alignment between {{ChatGPT}} and Human Societies: An Empirical Study},
  shorttitle = {Assessing Cross-Cultural Alignment between {{ChatGPT}} and Human Societies},
  booktitle = {Proceedings of the {{First Workshop}} on {{Cross-Cultural Considerations}} in {{NLP}} ({{C3NLP}})},
  author = {Cao, Yong and Zhou, Li and Lee, Seolhwa and Cabello, Laura and Chen, Min and Hershcovich, Daniel},
  editor = {Dev, Sunipa and Prabhakaran, Vinodkumar and Adelani, David Ifeoluwa and Hovy, Dirk and Benotti, Luciana},
  year = {2023},
  month = may,
  pages = {53--67},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.c3nlp-1.7},
  urldate = {2025-02-10},
  abstract = {The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.},
  langid = {english}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-12-13},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/7AAHW74E/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf;/Users/jonathanrystrom/Zotero/storage/3ARAEHSN/2107.html;/Users/jonathanrystrom/Zotero/storage/LH89TVXY/2107.html}
}

@misc{cobbeTrainingVerifiersSolve2021,
  title = {Training {{Verifiers}} to {{Solve Math Word Problems}}},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  year = {2021},
  month = nov,
  number = {arXiv:2110.14168},
  eprint = {2110.14168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.14168},
  urldate = {2023-11-20},
  abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/4SXR4CQM/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf;/Users/jonathanrystrom/Zotero/storage/DWABLQCB/2110.html;/Users/jonathanrystrom/Zotero/storage/P5PN9R8G/2110.html}
}

@inproceedings{cuiULTRAFEEDBACKBoostingLanguage2024,
  title = {{{ULTRAFEEDBACK}}: Boosting Language Models with Scaled {{AI}} Feedback},
  shorttitle = {Ultrafeedback},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai},
  year = {2024},
  urldate = {2025-01-30},
  langid = {english}
}

@book{daviesUnaccountabilityMachineWhy2024,
  title = {The Unaccountability Machine: Why Big Systems Make Terrible Decisions-and How the World Lost Its Mind},
  shorttitle = {The Unaccountability Machine},
  author = {Davies, Dan},
  year = {2024},
  publisher = {Profile Books},
  urldate = {2025-01-17},
  langid = {english}
}

@incollection{delfinoPassingBrazilianOAB2017,
  title = {Passing the Brazilian {{OAB}} Exam: Data Preparation and Some Experiments},
  shorttitle = {Passing the Brazilian {{OAB}} Exam},
  booktitle = {Legal {{Knowledge}} and {{Information Systems}}},
  author = {Delfino, Pedro and Cuconato, Bruno and Haeusler, Edward Hermann and Rademaker, Alexandre},
  year = {2017},
  pages = {89--94},
  publisher = {IOS Press},
  doi = {10.3233/978-1-61499-838-9-89},
  urldate = {2025-02-06},
  langid = {english}
}

@book{dignazioDataFeminism2020,
  title = {Data {{Feminism}}},
  author = {D'Ignazio, Catherine and Klein, Lauren},
  year = {2020},
  month = mar,
  publisher = {MIT Press},
  urldate = {2023-01-16},
  abstract = {Principle \#1 of Data Feminism is to Examine Power. Data feminism begins by analyzing how power operates in the world.},
  langid = {english}
}

@inproceedings{duaDROPReadingComprehension2019a,
  title = {{{DROP}}: A Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs},
  shorttitle = {Drop},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {2368--2378},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1246},
  urldate = {2025-02-10},
  abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4\% F1 on our generalized accuracy metric, while expert human performance is 96\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51\% F1.},
  langid = {english}
}

@article{enevoldsenDANSKDomainGeneralization2024,
  title = {{{DANSK}}: Domain Generalization of Danish Named Entity Recognition},
  shorttitle = {Dansk},
  author = {Enevoldsen, Kenneth and Jessen, Emil Trenckner and Baglini, Rebekah},
  year = {2024},
  month = dec,
  journal = {Northern European Journal of Language Technology},
  volume = {10},
  number = {1},
  pages = {14--29},
  issn = {2000-1533},
  doi = {10.3384/nejlt.2000-1533.2024.5249},
  urldate = {2025-01-30},
  abstract = {Named entity recognition is an important application within Danish NLP, essential within both industry and research. However, Danish NER is inhibited by a lack coverage across domains and entity types. As a consequence, no current models are capable of fine-grained named entity recognition, nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a named entity dataset providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) and three generalizable models with fine-grained annotation available in DaCy 2.6.0; and 3) an evaluation of current state-of-the-art models' ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed. Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work ongeneralizability within Danish NER.},
  copyright = {Copyright (c) 2024 Kenneth Enevoldsen, Emil Trenckner Jessen, Rebekah Baglini},
  langid = {english}
}

@misc{evs/wvsJointEVSWVS2022,
  title = {Joint {{EVS}}/{{WVS}} 2017-2022 {{Dataset}}},
  author = {{EVS/WVS}},
  year = {2022},
  publisher = {GESIS},
  doi = {10.4232/1.14023},
  urldate = {2023-06-29},
  abstract = {The European Values Study (EVS) and the World Values Survey (WVS) are two large-scale, cross-national and longitudinal survey research programmes. They include a large number of questions on moral, religious, social, political, occupational and family values which have been replicated since the early eighties. Both organizations agreed to cooperate in joint data collection from 2017. EVS has been responsible for planning and conducting surveys in European countries, using the EVS questionnaire and EVS methodological guidelines. WVSA has been responsible for planning and conducting surveys in countries in the world outside Europe, using the WVS questionnaire and WVS methodological guidelines. Both organisations developed their draft master questionnaires independently. The joint items define the Common Core of both questionnaires. The Joint EVS/WVS is constructed from the two EVS and WVS source datasets: - European Values Study 2017 Integrated Dataset (EVS 2017), ZA7500 Data file Version 5.0.0, doi:10.4232/1.13897 (https://doi.org/10.4232/1.13897). - World Values Survey: Round Seven--Country-Pooled Datafile. Version 5.0.0, doi: 10.14281/18241.20},
  collaborator = {Gedeshi, Ilir and Rotman, David and Kritzinger, Sylvia and Poghosyan, Gevorg and Pachulia, Merab and {Kolenovi{\'c}-{\DJ}apo}, Jadranka and Fotev, Georgy and Baloban, Stjepan and Baloban, Josip and Saar, Erki and Frederiksen, Morten and Rabu{\v s}ic, Ladislav and Ketola, Kimmo and Br{\'e}chon, Pierre and Wolf, Christof and Pachulia, Merab and Rosta, Gergely and Voas, David and J{\'o}nsd{\'o}ttir, Gu{\dh}bj{\"o}rg A. and Rovati, Giancarlo and Ziliukaite, Ruta and Petkovska, Antoanela and Reeskens, Tim and Jenssen, Anders T. and Komar, Olivera and Voicu, Bogdan and Marody, Miros{\l}awa and Soboleva, Natalia and Be{\v s}i{\'c}, Milo{\v s} and Strapcov{\'a}, Katarina and Uhan, Samo and Silvestre Cabrera, Mar{\'i}a and {Wallman-Lund{\aa}sen}, Susanne and Ernst St{\"a}hli, Mich{\`e}le and Ramos, Alice and Mic{\'o} Ib{\'a}{\~n}ez, Joan and Carballo, Marita and McAllister, Ian and Bangladesh), Roberto Stefan (PI, Foa and Moreno Morales, Daniel E. and De Oliveira De Castro, Henrique Carlos and Lagos, Marta and Zhong, Yang and Colombia), Andres (PI, Casas and Cyprus), Birol (PI, Yesilada and Paez, Cristina and Abdel Latif, Abdel Hamid and Ethiopia), Will (PI, Jennings and Welzel, Christian and D{\'i}az Argueta, Julio C{\'e}sar and Cheng, Edmund and Indonesia), Timothy (PI, Gravelle and Stoker, Gerry and Dagher, Munqith and Yamazaki, Seiko and Braizat, Fares and Rakisheva, Botagoz and Bakaloff, Yuri and Lebanon), Christian (PI, Haerpfer and {Wing-Yat Yu}, Eilo and Lee, Grace and Moreno, Alejandro and Souvanlasy, Chansada and Perry, Paul and Nicaragua), Carlos (PI, Denton and Nigeria), Bi (PI, Puranen and Gilani, Bilal and Romero, Catalina and Guerrero, Linda and Hern{\'a}ndez Acosta, Javier J. and Voicu, Bogdan and Zavadskaya, Margarita and Veskovic, Nino and Auh, Soo Young and Tsai, Ming-Chang and Olimov, Muzaffar and Bureekul, Thawilwadee and Ben Hafaiedh, Abdelwahab and Esmer, Yilmaz and Inglehart, Ronald and Depouilly, Xavier and Zimbabwe), Pippa (PI, Norris and Balakireva, Olga and Lachapelle, Guy and Mathews, Mathew and Mieri{\c n}a, Inta and Manasyan, Heghine and Kenya), Anna M. (PI, Ekstroem and Swehli, Nedal and Riyaz, Aminath and Tseveen, Tsetsenbileg and Abderebbi, Mhammed and Verhoeven, Piet and {Briceno-Leon}, Roberto and Moravec, Vaclav and Duffy, Bobby and Stoneman, Paul and Kosnac, Pavol and Zuasnabar, Ignacio and {Koniordos. Sokratis} and {EVS 2017:Center For Economic And Social Studies (CESS), Tirana, Albania} and {InterRating CoLtd, Yerevan, Armenia} and {Institut F{\"u}r Empirische Sozialforschung (IFES) GmbH, Vienna, Austria} and {Sorgu, Baku, Azerbaijan} and {Centre For Sociological And Political Research, Belarusian State University, Minsk, Belarus} and {Custom Concept D.O.O., Sarajevo, Bosnia And Herzegovina} and {Alpha Research LTD, Sofia, Bulgaria} and {Catholic University Of Croatia, Zagreb, And GfK Research Agency, Zagreb, Croatia} and {STEM/MARK, A.S., Praha, Czech Republic} and {Statistics Denmark-Survey, Copenhagen, Denmark} and {AS Emor,Tallinn, Estonia} and {Taloustutkimus Oy, Lemuntie 9, 00910 Helsinki, Finland} and {KANTAR PUBLIC-TAYLOR NELSON SOFRES, Paris, France} and {GORBI (Georgian Opinion Research Business International), Tbilisi, Georgia} and {Kantar Deutschland GmbH, Kantar Public, M{\"u}nchen, Germany} and {NatCen Social Research, London, Great Britain} and {Forsense, Budapest, Hungary} and {Social Science Research Institute, SSRI, University Of Iceland, Reykjavik, Iceland} and {Doxa Spa, Milano, Italy} and {Baltic Surveys, Vilnius, Lithuania} and {DeFacto Consultancy, Podgorica, Montenegro} and {I\&O Research B.V., Enschede, Netherlands AndCentERdata, Tilburg, Netherlands} and {Faculty Of Philosophy, Skopje, North Macedonia} and {Statistics Norway, Oslo, Norway} and {Centrum Badania Opinii Spo{\l}ecznej (Public Opinion Research Centre), Warszawa, Poland} and {IRES: Institutul Roman Pentru Evaluare Si Strategie, Romania} and {CESSI (Institute For Comparative Social Research), Moscow, Russia} and {Nina Media, Novi Sad, Serbia} and {Kantar TNS, Bratislava, Slovakia} and {University Of Ljubljana, Faculty Of Social Science, Ljubljana, Slovenia} and {MyWord Research SL, Madrid, Spain} and {IPSOS Observer Sweden AB, H{\"a}rn{\"o}sand, Sweden} and {M.I.S Trend S.A} and {Lausanne, Switzerland (Face-To-Face) AndSwiss Centre For Expertise In The Social Sciences FORS C/O University Of Lausanne, Lausanne, Switzerland (Web-Mail)} and {GfK-Metris, Lisbon, Portugal} and {Social Monitoring Center, Kiev, Ukraine} and {Sociolo{\c g}isko P{\=e}t{\=i}jumu Instit{\=u}ts, Riga, LatviaWVS Wave 7Institut D'Estudis Andorrans, Centre De Recerca Sociol{\`o}gica (CRES), Andorra} and {Voices Research And Consultancy S.A., Argentina} and {Centre For Social Research And Methods, Australian National University} and {SRG Bangladesh Limited (SRGB), Bangladesh} and {CIUDADANIA, Comunidad De Estudios Sociales Y Acci{\'o}n P{\'u}blica, Bolivia} and {Federal University Of Rio Grande Do Sul, Brazil} and {Market Opinion Research International, Chile} and {Public Opinion Research Center Of School Of International And Public Affairs At Shanghai Jiao Tong University, China} and {Invamer, Colombia} and {Cymar Research Company (Survey In Cyprus South)} and {Prologue Consulting Ltd. (Survey In Cyprus North)} and {IPSOS Ecuador} and {Egyptian Research And Training Center, Egypt} and {WAAS International/ TNS RMS Nigeria Limited / Kantar (Survey In Ethiopia)} and {Kantar Deutschland GmbH, Kantar Public, M{\"u}nchen, Germany} and {National Centre Of Social Research (EKKE) \& DIANEOSIS \& Metron Analysis, Greece} and {Innovation, Development \& Research, S.A. \& Social Work School Of The University Of San Carlos Of Guatemala} and {Centre For Communication And Public Opinion Survey (CCPOS) Of The Chinese University Of Hong Kong For FTF} and {Survey Sampling International (SSI) For CAWI, Hong Kong SAR PRC} and {Survey Meter, Indonesia} and {R-Research Limited, UK (Survey In Iran)} and {International Institute For Administration And Social Survey (IIACSS), Jordan (Survey In Iraq)} and {Nippon Research Center, Ltd., Japan} and {NAMA Strategic Intelligence Solutions, Jordan} and {Public Opinion Research Institute, Kazakhstan} and {Central Asia Barometer, Kyrgyzstan} and {Statistics Lebanon Ltd.} and {University Of Macao, China} and {IPSOS Malaysia} and {Moreno \& Sotnikova Social Research And Consulting S.C., Mexico} and {IRL (Indochina Research Laos) Myanmar Limited, Myanmar} and {School Of People, Environment And Planning, Massey University, New Zealand} and {CID/Gallup, S.A. (Survey In Nicaragua)} and {TNS RMS Nigeria Limited / Kantar} and {Gallup Pakistan} and {Public Opinion Institute At Pontifical Catholic University Of Peru} and {Social Weather Stations, Philippines} and {Universidad Del Sagrado Coraz{\'o}n, Puerto Rico} and {IRES: Institutul Roman Pentru Evaluare Si Strategie, Romania} and {CESSI-Institute For Comparative Social Research (Survey In Russia)} and {Singidunum University Belgrade, Serbia} and {Gallup South Korea} and {Institute Of Sociology, Academia Sinica, Taipei, Taiwan ROC} and {Research Centre SHARQ /Oriens, Tajikistan} and {King Prajadhipok's Institute, Thailand} and {Applied Social Science Forum, Tunisia} and {Bahcesehir University, Turkey} and {University Of Chicago, NORC AmeriSpeak, USA} and {Indochina Research Ltd. Vietnam} and {Consumer Feedback, Zimbabwe \& TNS RMS Nigeria Limited/ Kantar} and {Social Monitoring Center} and {Info Sapiens Research Center} and {Ukrainian Center For European Policy, Ukraine} and {Leger, Montreal, Canada} and {Institute Of Policy Studies, National University Of Singapore, Singapore} and {CRRC-Armenia, Yerevan, Armenia} and {Frontier Consulting Kenya, Nairobi, Kenya} and {Diwan Research, Tripoli, Libya} and {Maldives National University, Mal{\'e}, Maldives} and {Institute Of Philosophy, Mongolian Academy Of Sciences, Ulaanbaatar, Mongolia} and {Global For Survey And Consulting, Casablanca, Morocco} and {Centerdata, Tilburg, Netherlands} and {Laboratorio De Ciencias Sociales (LACSO), Caracas, Venezuela} and {FOCUS Polling Agency, Czech Republic} and {IPSOS, Great Britain} and {IPSOS, Northern Ireland} and {FOCUS Polling Agency, Slovakia} and {Equipos Consultores, Uruguay}},
  copyright = {Alle im GESIS DBK ver{\"o}ffentlichten Metadaten sind frei verf{\"u}gbar unter den Creative Commons CC0 1.0 Universal Public Domain Dedication. GESIS bittet jedoch darum, dass Sie alle Metadatenquellen anerkennen und sie nennen, etwa die Datengeber oder jeglichen Aggregator, inklusive GESIS selbst. F{\"u}r weitere Informationen siehe https://dbk.gesis.org/dbksearch/guidelines.asp?db=d, All metadata from GESIS DBK are available free of restriction under the Creative Commons CC0 1.0 Universal Public Domain Dedication. However, GESIS requests that you actively acknowledge and give attribution to all metadata sources, such as the data providers and any data aggregators, including GESIS. For further information see https://dbk.gesis.org/dbksearch/guidelines.asp},
  langid = {english},
  keywords = {ARBEIT UND BESCHAFTIGUNG,Cultural and national identity,Familie und Ehe,Family life and marriage,Gender and gender roles,Geschlecht und Geschlechterrollen,KAT15 Political Attitudes and Behavior,KAT37 Work and Industry,KAT50 Society Culture,KAT53 Family,KAT54 Person Personality Role,KAT57 Religion and "Weltanschauung",Kulturelle und nationale Identitat,LABOUR AND EMPLOYMENT,Political behaviour and attitudes,Politisches Verhalten und politische Einstellungen,Religion and values,Religion und Werte}
}

@misc{fedusReviewSparseExpert2022,
  title = {A Review of Sparse Expert Models in Deep Learning},
  author = {Fedus, William and Dean, Jeff and Zoph, Barret},
  year = {2022},
  month = sep,
  number = {arXiv:2209.01667},
  eprint = {2209.01667},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.01667},
  urldate = {2025-01-30},
  abstract = {Sparse expert models are a thirty-year old concept re-emerging as a popular architecture in deep learning. This class of architecture encompasses Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, and others, all with the unifying idea that each example is acted on by a subset of the parameters. By doing so, the degree of sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models. The resulting models have demonstrated significant improvements across diverse domains such as natural language processing, computer vision, and speech recognition. We review the concept of sparse expert models, provide a basic description of the common algorithms, contextualize the advances in the deep learning era, and conclude by highlighting areas for future work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/3DWHWMWS/2209.html}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and Surprise in Large Generative Models},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson},
  year = {2022},
  pages = {1747--1764}
}

@inproceedings{gaoHowDesignTranslation2024,
  title = {How to Design Translation Prompts for {{ChatGPT}}: An Empirical Study},
  shorttitle = {How to Design Translation Prompts for {{ChatGPT}}},
  booktitle = {Proceedings of the 6th {{ACM International Conference}} on {{Multimedia}} in {{Asia Workshops}}},
  author = {Gao, Yuan and Wang, Ruili and Hou, Feng},
  year = {2024},
  month = dec,
  series = {{{MMAsia}} '24 {{Workshops}}},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3700410.3702123},
  urldate = {2025-02-12},
  abstract = {ChatGPT, a chatbot based on the GPT models, has demonstrated surprising abilities in natural language understanding and generation tasks. Given that machine translation heavily relies on these abilities, there is substantial promise in applying ChatGPT for machine translation. Using naive prompts cannot fully unleash ChatGPT's translation ability. Thus, in this paper, we propose several translation prompts that contain (i). translation task information (e.g., English-to-German), (ii). context domain information (e.g., News), (iii). Part-of-Speech (POS) tags, respectively. Our experimental results show that our proposed translation prompts can significantly enhance ChatGPT's translation performance. We evaluate the translation quality using multi-reference test sets which consist of ten different human translations for each source sentence, and ChatGPT achieves superior performance compared to commercial systems. In addition, we also develop few-shot prompts upon our proposed translation prompts, which consistently show improvement across different translation directions.},
  isbn = {9798400713149},
  langid = {english}
}

@misc{gemmaGemma2Improving2024,
  title = {Gemma 2: Improving Open Language Models at a Practical Size},
  shorttitle = {Gemma 2},
  author = {Gemma, Team and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Shahriari, Bobak and Ram{\'e}, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Bachem, Olivier and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and {Choquette-Choo}, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozi{\'n}ska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and {Klimczak-Pluci{\'n}ska}, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and van Amersfoort, Joost and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and G{\"o}rner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Cogan, Sarah and Perrin, Sarah and Arnold, S{\'e}bastien M. R. and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
  year = {2024},
  month = oct,
  number = {arXiv:2408.00118},
  eprint = {2408.00118},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00118},
  urldate = {2025-01-15},
  abstract = {In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/4RCIL9YF/2408.html}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  urldate = {2024-09-19},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  keywords = {Collective illusions,Editors,Rituals,Statistical significance,Textbooks},
  file = {/Users/jonathanrystrom/Zotero/storage/6C95A69C/S1053535704000927.html}
}

@article{hartmannPoliticalIdeologyConversational2023,
  title = {The Political Ideology of Conversational {{AI}}: {{Converging}} Evidence on {{ChatGPT}}'s pro-Environmental, Left-Libertarian Orientation},
  shorttitle = {The Political Ideology of Conversational {{AI}}},
  author = {Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4316084},
  urldate = {2023-02-19},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/EMKQCHIJ/Hartmann et al. - 2023 - The political ideology of conversational AI Conve.pdf}
}

@misc{hausorensenEvalueringsdatasaet1000Danske2024,
  title = {Evalueringsdatas{\ae}t for 1000 Danske Talem{\aa}der Og Faste Udtryk ({{Det Danske Sprog-}} Og {{Litteraturselskab}})},
  author = {Hau S{\o}rensen, Nathalie},
  year = {2024},
  month = nov,
  publisher = {Det Danske Sprog- og Litteraturselskab},
  archiveprefix = {Det Danske Sprog- og Litteraturselskab}
}

@inproceedings{hendrycksMeasuringMassiveMultitask2020,
  title = {Measuring Massive Multitask Language Understanding},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2020},
  month = oct,
  urldate = {2025-02-10},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  langid = {english},
  annotation = {shortConferenceName: ICLR}
}

@misc{henningBridgingHumanautomationFairness2024,
  type = {{{SSRN Scholarly Paper}}},
  title = {Bridging the Human-Automation Fairness Gap: How Providing Reasons Enhances the Perceived Fairness of Public Decision-Making},
  shorttitle = {Bridging the Human-Automation Fairness Gap},
  author = {Henning, Arian and Langenbach, Pascal},
  year = {2024},
  month = may,
  number = {4819145},
  eprint = {4819145},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.4819145},
  urldate = {2025-01-31},
  abstract = {Automated decision-making in legal contexts is often perceived as less fair than its human counterpart. This human-automation fairness gap poses practical challenges for implementing automated systems in the public sector. Drawing on experimental data from 4,250 participants in three public decision-making scenarios, this study examines how different reasoning models influence the perceived fairness of automated and human decision-making. The results show that providing reasons enhances the perceived fairness of decision-making, regardless of whether decisions are made by humans or machines. Moreover, the study demonstrates that sufficiently individualized reasoning largely mitigates the human-automation fairness gap. The study thus contributes to the understanding of how procedural elements like giving reasons for decisions shape perceptions of automated government and suggests that well-designed reason giving can improve the acceptability of automated decision systems.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Arian Henning,Bridging the Human-Automation Fairness Gap: How Providing Reasons Enhances the Perceived Fairness of Public Decision-Making,Pascal Langenbach,SSRN}
}

@misc{hernandezScalingLawsTransfer2021,
  title = {Scaling {{Laws}} for {{Transfer}}},
  author = {Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  year = {2021},
  month = feb,
  number = {arXiv:2102.01293},
  eprint = {2102.01293},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.01293},
  urldate = {2023-06-30},
  abstract = {We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data "transferred" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/QNST5BZN/2102.html}
}

@inproceedings{hoffmannTrainingComputeoptimalLarge2022a,
  title = {Training Compute-Optimal Large Language Models},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and {de Las Casas}, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and {van den Driessche}, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
  year = {2022},
  month = nov,
  series = {{{NIPS}} '22},
  pages = {30016--30030},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-02-10},
  abstract = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4{\texttimes} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  isbn = {978-1-71387-108-8},
  langid = {english}
}

@inproceedings{huXTREMEMassivelyMultilingual2020,
  title = {{{XTREME}}: {{A Massively Multilingual Multi-task Benchmark}} for {{Evaluating Cross-lingual Generalisation}}},
  shorttitle = {{{XTREME}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  year = {2020},
  month = nov,
  pages = {4411--4421},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-02-23},
  abstract = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
  langid = {english}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06825},
  urldate = {2024-09-20},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/3ZFUS8AE/2310.html}
}

@misc{johnsonGhostMachineHas2022,
  title = {The {{Ghost}} in the {{Machine}} Has an {{American}} Accent: Value Conflict in {{GPT-3}}},
  shorttitle = {The {{Ghost}} in the {{Machine}} Has an {{American}} Accent},
  author = {Johnson, Rebecca L. and Pistilli, Giada and {Men{\'e}dez-Gonz{\'a}lez}, Natalia and Duran, Leslye Denisse Dias and Panai, Enrico and Kalpokiene, Julija and Bertulfo, Donald Jay},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07785},
  eprint = {2203.07785},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.07785},
  urldate = {2023-07-10},
  abstract = {The alignment problem in the context of large language models must consider the plurality of human values in our world. Whilst there are many resonant and overlapping values amongst the world's cultures, there are also many conflicting, yet equally valid, values. It is important to observe which cultural values a model exhibits, particularly when there is a value conflict between input prompts and generated outputs. We discuss how the co-creation of language and cultural value impacts large language models (LLMs). We explore the constitution of the training data for GPT-3 and compare that to the world's language and internet access demographics, as well as to reported statistical profiles of dominant values in some Nation-states. We stress tested GPT-3 with a range of value-rich texts representing several languages and nations; including some with values orthogonal to dominant US public opinion as reported by the World Values Survey. We observed when values embedded in the input text were mutated in the generated outputs and noted when these conflicting values were more aligned with reported dominant US values. Our discussion of these results uses a moral value pluralism (MVP) lens to better understand these value mutations. Finally, we provide recommendations for how our work may contribute to other current work in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/BM4SIJPZ/2203.html}
}

@book{kahnemanNoiseFlawHuman2021,
  title = {Noise: {{A}} Flaw in Human Judgment},
  shorttitle = {Noise},
  author = {Kahneman, Daniel and Sibony, Olivier and Sunstein, Cass R.},
  year = {2021},
  publisher = {Little, Brown},
  file = {/Users/jonathanrystrom/Zotero/storage/9BC2MAIH/Noise_A_Flaw_in_Human_Judgment.html}
}

@article{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08361 [cs, stat]},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  urldate = {2022-04-28},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/QTVSIES5/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/jonathanrystrom/Zotero/storage/6WXPNUYD/2001.html}
}

@inproceedings{khandelwalIndianBhEDDatasetMeasuring2024,
  title = {Indian-{{BhED}}: A Dataset for Measuring India-Centric Biases in Large Language Models},
  shorttitle = {Indian-{{BhED}}},
  booktitle = {Proceedings of the 2024 {{International Conference}} on {{Information Technology}} for {{Social Good}}},
  author = {Khandelwal, Khyati and Tonneau, Manuel and Bean, Andrew M. and Kirk, Hannah Rose and Hale, Scott A.},
  year = {2024},
  month = sep,
  pages = {231--239},
  publisher = {ACM},
  address = {Bremen Germany},
  doi = {10.1145/3677525.3678666},
  urldate = {2025-02-06},
  isbn = {9798400710940},
  langid = {english}
}

@misc{khanHarnessingGPT4That2023,
  title = {H\hspace{0pt}Arnessing {{GPT-4}} so That All Students Benefit. {{A}} Nonprofit Approach for Equal Access!},
  author = {Khan, Sal},
  year = {2023},
  month = mar,
  journal = {Khan Academy Blog},
  urldate = {2023-04-03},
  abstract = {Today we're introducing a small AI pilot for a limited number of teachers, students, and donors. As society grapples with AI, we view it as our responsibility to work deeply with this new technology to explore its potential in education. I believe we are uniquely suited to do this work.},
  langid = {american},
  file = {/Users/jonathanrystrom/Zotero/storage/6TVLK6YF/harnessing-ai-so-that-all-students-benefit-a-nonprofit-approach-for-equal-access.html}
}

@article{kirkBenefitsRisksBounds2024,
  title = {The Benefits, Risks and Bounds of Personalizing the Alignment of Large Language Models to Individuals},
  author = {Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A.},
  year = {2024},
  month = apr,
  journal = {Nature Machine Intelligence},
  volume = {6},
  number = {4},
  pages = {383--392},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-024-00820-y},
  urldate = {2025-01-09},
  abstract = {Large language models (LLMs) undergo `alignment' so that they better reflect human values or preferences, and are safer or more useful. However, alignment is intrinsically difficult because the hundreds of millions of people who now interact with LLMs have different preferences for language and conversational norms, operate under disparate value systems and hold diverse political beliefs. Typically, few developers or researchers dictate alignment norms, risking the exclusion or under-representation of various groups. Personalization is a new frontier in LLM development, whereby models are tailored to individuals. In principle, this could minimize cultural hegemony, enhance usefulness and broaden access. However, unbounded personalization poses risks such as large-scale profiling, privacy infringement, bias reinforcement and exploitation of the vulnerable. Defining the bounds of responsible and socially acceptable personalization is a non-trivial task beset with normative challenges. This article explores `personalized alignment', whereby LLMs adapt to user-specific data, and highlights recent shifts in the LLM ecosystem towards a greater degree of personalization. Our main contribution explores the potential impact of personalized LLMs via a taxonomy of risks and benefits for individuals and society at large. We lastly discuss a key open question: what are appropriate bounds of personalization and who decides? Answering this normative question enables users to benefit from personalized alignment while safeguarding against harmful impacts for individuals and society.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Information technology,Science,technology and society}
}

@inproceedings{kirkPresentBetterFuture2023,
  title = {The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kirk, Hannah Rose and Bean, Andrew M. and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A.},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {2409--2430},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.148},
  urldate = {2025-02-01},
  abstract = {Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.},
  langid = {english}
}

@inproceedings{kirkPRISMAlignmentDataset2024,
  title = {The {{PRISM}} Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals about the Subjective and Multicultural Alignment of Large Language Models},
  shorttitle = {The {{PRISM}} Alignment Dataset},
  booktitle = {The {{Thirty-eight Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew Michael and Margatina, Katerina and Mosquera, Rafael and Ciro, Juan Manuel and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
  year = {2024},
  month = nov,
  urldate = {2025-01-16},
  abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide alignment data.},
  langid = {english}
}

@article{kreutzerQualityGlanceAudit2022,
  title = {Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets},
  shorttitle = {Quality at a Glance},
  author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and {van Esch}, Daan and {Ulzii-Orshikh}, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Beno{\^i}t and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife, Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and M{\"u}ller, Mathias and M{\"u}ller, Andr{\'e} and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and {de Silva}, Nisansa and {\c C}abuk Ball{\i}, Sakine and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
  year = {2022},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {50--72},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00447},
  urldate = {2025-02-10},
  abstract = {With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50\% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/GH92TJTL/Quality-at-a-Glance-An-Audit-of-Web-Crawled.html}
}

@misc{kunduSpecificGeneralPrinciples2023,
  title = {Specific versus {{General Principles}} for {{Constitutional AI}}},
  author = {Kundu, Sandipan and Bai, Yuntao and Kadavath, Saurav and Askell, Amanda and Callahan, Andrew and Chen, Anna and Goldie, Anna and Balwit, Avital and Mirhoseini, Azalia and McLean, Brayden and Olsson, Catherine and Evraets, Cassie and {Tran-Johnson}, Eli and Durmus, Esin and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Ndousse, Kamal and Nguyen, Karina and Elhage, Nelson and Cheng, Newton and Schiefer, Nicholas and DasSarma, Nova and Rausch, Oliver and Larson, Robin and Yang, Shannon and Kravec, Shauna and {Telleen-Lawton}, Timothy and Liao, Thomas I. and Henighan, Tom and Hume, Tristan and {Hatfield-Dodds}, Zac and Mindermann, S{\"o}ren and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13798},
  eprint = {2310.13798},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-28},
  abstract = {Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as "do what's best for humanity". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/NXR8ULDV/Kundu et al. - 2023 - Specific versus General Principles for Constitutio.pdf;/Users/jonathanrystrom/Zotero/storage/SBHPIW3Q/2310.html;/Users/jonathanrystrom/Zotero/storage/T2CGQMH5/2310.html}
}

@inproceedings{kwonEfficientMemoryManagement2023,
  title = {Efficient Memory Management for Large Language Model Serving with {{PagedAttention}}},
  booktitle = {Proceedings of the 29th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  year = {2023},
  month = oct,
  pages = {611--626},
  publisher = {ACM},
  address = {Koblenz Germany},
  doi = {10.1145/3600006.3613165},
  urldate = {2025-02-12},
  isbn = {9798400702297},
  langid = {english}
}

@inproceedings{laiOkapiInstructiontunedLarge2023,
  title = {Okapi: Instruction-Tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback},
  shorttitle = {Okapi},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Lai, Viet and Nguyen, Chien and Ngo, Nghia and Nguyen, Thuat and Dernoncourt, Franck and Rossi, Ryan and Nguyen, Thien},
  editor = {Feng, Yansong and Lefever, Els},
  year = {2023},
  month = dec,
  pages = {318--327},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-demo.28},
  urldate = {2025-01-30},
  abstract = {A key technology for large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are applied to produce the best commercial LLMs. To improve the accessibility of LLMs, various instruction-tuned open-source LLMs have also been introduced recently. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their accessibility to many other languages in the world. In addition, SFT has been used as the only approach to instruction-tune open-source LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework with created resources, fine-tuned LLMs, interaction scripts are released at https://github.com/nlp-uoregon/Okapi. A demo video to show our framework can also be found at: https://youtu.be/QFV2fkPwvi0.},
  langid = {english}
}

@article{longpreMKQALinguisticallyDiverse2021,
  title = {{{MKQA}}: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},
  shorttitle = {Mkqa},
  author = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2021},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {1389--1406},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00433},
  urldate = {2025-01-30},
  abstract = {Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages.1},
  langid = {english}
}

@article{lukeEvaluatingSignificanceLinear2017,
  title = {Evaluating Significance in Linear Mixed-Effects Models in {{R}}},
  author = {Luke, Steven G.},
  year = {2017},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {49},
  number = {4},
  pages = {1494--1502},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0809-y},
  urldate = {2023-01-05},
  abstract = {Mixed-effects models are being used ever more frequently in the analysis of experimental data. However, in the lme4 package in R the standards for evaluating significance of fixed effects in these models (i.e., obtaining p-values) are somewhat vague. There are good reasons for this, but as researchers who are using these models are required in many cases to report p-values, some method for evaluating the significance of the model output is needed. This paper reports the results of simulations showing that the two most common methods for evaluating significance, using likelihood ratio tests and applying the z distribution to the Wald t values from the model output (t-as-z), are somewhat anti-conservative, especially for smaller sample sizes. Other methods for evaluating significance, including parametric bootstrapping and the Kenward-Roger and Satterthwaite approximations for degrees of freedom, were also evaluated. The results of these simulations suggest that Type 1 error rates are closest to .05 when models are fitted using REML and p-values are derived using the Kenward-Roger or Satterthwaite approximations, as these approximations both produced acceptable Type 1 error rates even for smaller samples.},
  langid = {english},
  keywords = {Linear mixed-effects models,lme4,p-values,Statistics,Type 1 error}
}

@inproceedings{masoudCulturalAlignmentLarge2025,
  title = {Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede`s Cultural Dimensions},
  shorttitle = {Cultural Alignment in Large Language Models},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Masoud, Reem and Liu, Ziquan and Ferianc, Martin and Treleaven, Philip C. and Rodrigues, Miguel Rodrigues},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and {Al-Khalifa}, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  year = {2025},
  month = jan,
  pages = {8474--8503},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  urldate = {2025-02-03},
  abstract = {The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede`s CAT) to quantify cultural alignment using Hofstede`s cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs---namely Llama 2, GPT-3.5, and GPT-4---against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models' behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page https://github.com/reemim/Hofstedes\_CAT},
  langid = {english}
}

@article{milanBigDataSouths2019,
  title = {Big {{Data}} from the {{South}}(s): {{Beyond Data Universalism}}},
  shorttitle = {Big {{Data}} from the {{South}}(s)},
  author = {Milan, Stefania and Trer{\'e}, Emiliano},
  year = {2019},
  month = may,
  journal = {Television \& New Media},
  volume = {20},
  number = {4},
  pages = {319--335},
  publisher = {SAGE Publications},
  issn = {1527-4764},
  doi = {10.1177/1527476419837739},
  urldate = {2023-02-27},
  abstract = {This article introduces the tenets of a theory of datafication of and in the Souths. It calls for a de-Westernization of critical data studies, in view of promoting a reparation to the cognitive injustice that fails to recognize non-mainstream ways of knowing the world through data. It situates the ?Big Data from the South? research agenda as an epistemological, ontological, and ethical program and outlines five conceptual operations to shape this agenda. First, it suggests moving past the ?universalism? associated with our interpretations of datafication. Second, it advocates understanding the South as a composite and plural entity, beyond the geographical connotation (i.e., ?global South?). Third, it postulates a critical engagement with the decolonial approach. Fourth, it argues for the need to bring agency to the core of our analyses. Finally, it suggests embracing the imaginaries of datafication emerging from the Souths, foregrounding empowering ways of thinking data from the margins.},
  langid = {english}
}

@article{milmoChatGPTReaches1002023,
  title = {{{ChatGPT}} Reaches 100 Million Users Two Months after Launch},
  author = {Milmo, Dan},
  year = {2023},
  month = feb,
  journal = {The Guardian},
  issn = {0261-3077},
  urldate = {2023-02-26},
  abstract = {Unprecedented take-up may make AI chatbot the fastest-growing consumer internet app ever, analysts say},
  chapter = {Technology},
  langid = {british},
  keywords = {Artificial intelligence (AI),Chatbots,ChatGPT,Microsoft,Technology,US news},
  file = {/Users/jonathanrystrom/Zotero/storage/5BFGBMC3/chatgpt-100-million-users-open-ai-fastest-growing-app.html}
}

@article{mokanderAuditingLargeLanguage2024,
  title = {Auditing Large Language Models: A Three-Layered Approach},
  shorttitle = {Auditing Large Language Models},
  author = {M{\"o}kander, Jakob and Schuett, Jonas and Kirk, Hannah Rose and Floridi, Luciano},
  year = {2024},
  month = nov,
  journal = {AI and Ethics},
  volume = {4},
  number = {4},
  pages = {1085--1115},
  issn = {2730-5961},
  doi = {10.1007/s43681-023-00289-2},
  urldate = {2024-11-26},
  abstract = {Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial Intelligence,Auditing,Ethics,Foundation models,Governance,Large language models,Natural language processing,Policy,Risk management}
}

@inproceedings{nielsenScandEvalBenchmarkScandinavian2023a,
  title = {{{ScandEval}}: A Benchmark for Scandinavian Natural Language Processing},
  shorttitle = {{{ScandEval}}},
  booktitle = {Proceedings of the 24th {{Nordic Conference}} on {{Computational Linguistics}} ({{NoDaLiDa}})},
  author = {Nielsen, Dan},
  editor = {Alum{\"a}e, Tanel and Fishel, Mark},
  year = {2023},
  month = may,
  pages = {185--201},
  publisher = {University of Tartu Library},
  address = {T{\'o}rshavn, Faroe Islands},
  urldate = {2025-02-10},
  abstract = {This paper introduces a Scandinavian benchmarking platform, ScandEval, which can benchmark any pretrained model on four different tasks in the Scandinavian languages. The datasets used in two of the tasks, linguistic acceptability and question answering, are new. We develop and release a Python package and command-line interface, scandeval, which can benchmark any model that has been uploaded to the Hugging Face Hub, with reproducible results. Using this package, we benchmark more than 80 Scandinavian or multilingual models and present the results of these in an interactive online leaderboard, as well as provide an analysis of the results. The analysis shows that there is substantial cross-lingual transfer among the the Mainland Scandinavian languages (Danish, Swedish and Norwegian), with limited cross-lingual transfer between the group of Mainland Scandinavian languages and the group of Insular Scandinavian languages (Icelandic and Faroese). The benchmarking results also show that the investment in language technology in Norway and Sweden has led to language models that outperform massively multilingual models such as XLM-RoBERTa and mDeBERTaV3. We release the source code for both the package and leaderboard.},
  langid = {english}
}

@article{norupAttitudesAbortionDanish1997,
  title = {Attitudes towards Abortion in the Danish Population},
  author = {Norup, Michael},
  year = {1997},
  journal = {Bioethics},
  volume = {11},
  number = {5},
  pages = {439--449},
  issn = {1467-8519},
  doi = {10.1111/1467-8519.00083},
  urldate = {2025-02-04},
  abstract = {This article reports the results of a survey, by mailed questionnaire, of the attitudes among a sample of the Danish population towards abortion for social and genetic reasons. Of 1080 questionnaires sent to a random sample of persons between 18 and 45 years, 731 (68\%) were completed and returned. A great majority of the respondents were liberal towards early abortion both for social reasons and in case of minor disease. In contrast, there was controversy about late abortions for social reasons and in the case of Down syndrome. Further there was strong reluctance to accept late abortion in case of minor disease. An analysis of the response patterns showed that most of the respondents had gradualist views on abortion, i.e. they would allow all early abortions, but only abortions for some reasons later in pregnancy. It was also found that the number who would find an early abortion acceptable in general was much higher than the number who would accept it in their own case. These findings suggest that a great part of the resistance towards abortion does not rest on a concern for the rights and interests for the fetus. Instead it may be explained on a view according to which fetal life is ascribed intrinsic moral value.},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/ERP6BBQJ/1467-8519.html}
}

@misc{open-pt-llm-leaderboard,
  title = {Open {{Portuguese LLM}} Leaderboard},
  author = {Garcia, Eduardo A. S.},
  year = {2024},
  publisher = {Hugging Face},
  langid = {english}
}

@misc{openaiChatGPTOptimizingLanguage2022,
  title = {{{ChatGPT}}: {{Optimizing Language Models}} for {{Dialogue}}},
  shorttitle = {{{ChatGPT}}},
  author = {OpenAI},
  year = {2022},
  month = nov,
  journal = {OpenAI},
  urldate = {2023-01-04},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/9256QSN7/chatgpt.html}
}

@misc{openaiGPT4oSystemCard2024,
  title = {{{GPT-4o}} System Card},
  author = {OpenAI and Hurst, Aaron and Lerer, Adam and Goucher, Adam P. and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, A. J. and Welihinda, Akila and Hayes, Alan and Radford, Alec and M{\k a}dry, Aleksander and {Baker-Whitcomb}, Alex and Beutel, Alex and Borzunov, Alex and Carney, Alex and Chow, Alex and Kirillov, Alex and Nichol, Alex and Paino, Alex and Renzin, Alex and Passos, Alex Tachard and Kirillov, Alexander and Christakis, Alexi and Conneau, Alexis and Kamali, Ali and Jabri, Allan and Moyer, Allison and Tam, Allison and Crookes, Amadou and Tootoochian, Amin and Tootoonchian, Amin and Kumar, Ananya and Vallone, Andrea and Karpathy, Andrej and Braunstein, Andrew and Cann, Andrew and Codispoti, Andrew and Galu, Andrew and Kondrich, Andrew and Tulloch, Andrew and Mishchenko, Andrey and Baek, Angela and Jiang, Angela and Pelisse, Antoine and Woodford, Antonia and Gosalia, Anuj and Dhar, Arka and Pantuliano, Ashley and Nayak, Avi and Oliver, Avital and Zoph, Barret and Ghorbani, Behrooz and Leimberger, Ben and Rossen, Ben and Sokolowsky, Ben and Wang, Ben and Zweig, Benjamin and Hoover, Beth and Samic, Blake and McGrew, Bob and Spero, Bobby and Giertler, Bogo and Cheng, Bowen and Lightcap, Brad and Walkin, Brandon and Quinn, Brendan and Guarraci, Brian and Hsu, Brian and Kellogg, Bright and Eastman, Brydon and Lugaresi, Camillo and Wainwright, Carroll and Bassin, Cary and Hudson, Cary and Chu, Casey and Nelson, Chad and Li, Chak and Shern, Chan Jun and Conger, Channing and Barette, Charlotte and Voss, Chelsea and Ding, Chen and Lu, Cheng and Zhang, Chong and Beaumont, Chris and Hallacy, Chris and Koch, Chris and Gibson, Christian and Kim, Christina and Choi, Christine and McLeavey, Christine and Hesse, Christopher and Fischer, Claudia and Winter, Clemens and Czarnecki, Coley and Jarvis, Colin and Wei, Colin and Koumouzelis, Constantin and Sherburn, Dane and Kappler, Daniel and Levin, Daniel and Levy, Daniel and Carr, David and Farhi, David and Mely, David and Robinson, David and Sasaki, David and Jin, Denny and Valladares, Dev and Tsipras, Dimitris and Li, Doug and Nguyen, Duc Phong and Findlay, Duncan and Oiwoh, Edede and Wong, Edmund and Asdar, Ehsan and Proehl, Elizabeth and Yang, Elizabeth and Antonow, Eric and Kramer, Eric and Peterson, Eric and Sigler, Eric and Wallace, Eric and Brevdo, Eugene and Mays, Evan and Khorasani, Farzad and Such, Felipe Petroski and Raso, Filippo and Zhang, Francis and von Lohmann, Fred and Sulit, Freddie and Goh, Gabriel and Oden, Gene and Salmon, Geoff and Starace, Giulio and Brockman, Greg and Salman, Hadi and Bao, Haiming and Hu, Haitang and Wong, Hannah and Wang, Haoyu and Schmidt, Heather and Whitney, Heather and Jun, Heewoo and Kirchner, Hendrik and Pinto, Henrique Ponde de Oliveira and Ren, Hongyu and Chang, Huiwen and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and O'Connell, Ian and Osband, Ian and Silber, Ian and Sohl, Ian and Okuyucu, Ibrahim and Lan, Ikai and Kostrikov, Ilya and Sutskever, Ilya and Kanitscheider, Ingmar and Gulrajani, Ishaan and Coxon, Jacob and Menick, Jacob and Pachocki, Jakub and Aung, James and Betker, James and Crooks, James and Lennon, James and Kiros, Jamie and Leike, Jan and Park, Jane and Kwon, Jason and Phang, Jason and Teplitz, Jason and Wei, Jason and Wolfe, Jason and Chen, Jay and Harris, Jeff and Varavva, Jenia and Lee, Jessica Gan and Shieh, Jessica and Lin, Ji and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Jang, Joanne and Candela, Joaquin Quinonero and Beutler, Joe and Landers, Joe and Parish, Joel and Heidecke, Johannes and Schulman, John and Lachman, Jonathan and McKay, Jonathan and Uesato, Jonathan and Ward, Jonathan and Kim, Jong Wook and Huizinga, Joost and Sitkin, Jordan and Kraaijeveld, Jos and Gross, Josh and Kaplan, Josh and Snyder, Josh and Achiam, Joshua and Jiao, Joy and Lee, Joyce and Zhuang, Juntang and Harriman, Justyn and Fricke, Kai and Hayashi, Kai and Singhal, Karan and Shi, Katy and Karthik, Kavin and Wood, Kayla and Rimbach, Kendra and Hsu, Kenny and Nguyen, Kenny and {Gu-Lemberg}, Keren and Button, Kevin and Liu, Kevin and Howe, Kiel and Muthukumar, Krithika and Luther, Kyle and Ahmad, Lama and Kai, Larry and Itow, Lauren and Workman, Lauren and Pathak, Leher and Chen, Leo and Jing, Li and Guy, Lia and Fedus, Liam and Zhou, Liang and Mamitsuka, Lien and Weng, Lilian and McCallum, Lindsay and Held, Lindsey and Ouyang, Long and Feuvrier, Louis and Zhang, Lu and Kondraciuk, Lukas and Kaiser, Lukasz and Hewitt, Luke and Metz, Luke and Doshi, Lyric and Aflak, Mada and Simens, Maddie and Boyd, Madelaine and Thompson, Madeleine and Dukhan, Marat and Chen, Mark and Gray, Mark and Hudnall, Mark and Zhang, Marvin and Aljubeh, Marwan and Litwin, Mateusz and Zeng, Matthew and Johnson, Max and Shetty, Maya and Gupta, Mayank and Shah, Meghan and Yatbaz, Mehmet and Yang, Meng Jia and Zhong, Mengchao and Glaese, Mia and Chen, Mianna and Janner, Michael and Lampe, Michael and Petrov, Michael and Wu, Michael and Wang, Michele and Fradin, Michelle and Pokrass, Michelle and Castro, Miguel and de Castro, Miguel Oom Temudo and Pavlov, Mikhail and Brundage, Miles and Wang, Miles and Khan, Minal and Murati, Mira and Bavarian, Mo and Lin, Molly and Yesildal, Murat and Soto, Nacho and Gimelshein, Natalia and Cone, Natalie and Staudacher, Natalie and Summers, Natalie and LaFontaine, Natan and Chowdhury, Neil and Ryder, Nick and Stathas, Nick and Turley, Nick and Tezak, Nik and Felix, Niko and Kudige, Nithanth and Keskar, Nitish and Deutsch, Noah and Bundick, Noel and Puckett, Nora and Nachum, Ofir and Okelola, Ola and Boiko, Oleg and Murk, Oleg and Jaffe, Oliver and Watkins, Olivia and Godement, Olivier and {Campbell-Moore}, Owen and Chao, Patrick and McMillan, Paul and Belov, Pavel and Su, Peng and Bak, Peter and Bakkum, Peter and Deng, Peter and Dolan, Peter and Hoeschele, Peter and Welinder, Peter and Tillet, Phil and Pronin, Philip and Tillet, Philippe and Dhariwal, Prafulla and Yuan, Qiming and Dias, Rachel and Lim, Rachel and Arora, Rahul and Troll, Rajan and Lin, Randall and Lopes, Rapha Gontijo and Puri, Raul and Miyara, Reah and Leike, Reimar and Gaubert, Renaud and Zamani, Reza and Wang, Ricky and Donnelly, Rob and Honsby, Rob and Smith, Rocky and Sahai, Rohan and Ramchandani, Rohit and Huet, Romain and Carmichael, Rory and Zellers, Rowan and Chen, Roy and Chen, Ruby and Nigmatullin, Ruslan and Cheu, Ryan and Jain, Saachi and Altman, Sam and Schoenholz, Sam and Toizer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Culver, Sara and Ethersmith, Scott and Gray, Scott and Grove, Sean and Metzger, Sean and Hermani, Shamez and Jain, Shantanu and Zhao, Shengjia and Wu, Sherwin and Jomoto, Shino and Wu, Shirong and Shuaiqi and Xia and Phene, Sonia and Papay, Spencer and Narayanan, Srinivas and Coffey, Steve and Lee, Steve and Hall, Stewart and Balaji, Suchir and Broda, Tal and Stramer, Tal and Xu, Tao and Gogineni, Tarun and Christianson, Taya and Sanders, Ted and Patwardhan, Tejal and Cunninghman, Thomas and Degry, Thomas and Dimson, Thomas and Raoux, Thomas and Shadwell, Thomas and Zheng, Tianhao and Underwood, Todd and Markov, Todor and Sherbakov, Toki and Rubin, Tom and Stasi, Tom and Kaftan, Tomer and Heywood, Tristan and Peterson, Troy and Walters, Tyce and Eloundou, Tyna and Qi, Valerie and Moeller, Veit and Monaco, Vinnie and Kuo, Vishal and Fomenko, Vlad and Chang, Wayne and Zheng, Weiyi and Zhou, Wenda and Manassra, Wesam and Sheu, Will and Zaremba, Wojciech and Patil, Yash and Qian, Yilei and Kim, Yongjik and Cheng, Youlong and Zhang, Yu and He, Yuchen and Zhang, Yuchen and Jin, Yujia and Dai, Yunxing and Malkov, Yury},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21276},
  eprint = {2410.21276},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21276},
  urldate = {2024-12-27},
  abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50{\textbackslash}\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/jonathanrystrom/Zotero/storage/ZN9RGX3A/2410.html}
}

@misc{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4}} Technical Report},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2024},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2025-01-15},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/L7ZXZMKQ/2303.html}
}

@misc{OpenaiSimpleevals2025,
  title = {Openai/Simple-Evals},
  year = {2025},
  month = jan,
  urldate = {2025-01-30},
  copyright = {MIT},
  howpublished = {OpenAI}
}

@inproceedings{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = nov,
  series = {{{NIPS}} '22},
  pages = {27730--27744},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-02-03},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  isbn = {978-1-71387-108-8},
  langid = {english}
}

@book{pasqualeBlackBoxSociety2015,
  title = {The Black Box Society: The Secret Algorithms That Control Money and Information},
  shorttitle = {The Black Box Society},
  author = {Pasquale, Frank},
  year = {2015},
  eprint = {j.ctt13x0hch},
  eprinttype = {jstor},
  publisher = {Harvard University Press},
  urldate = {2025-01-31},
  abstract = {Every day, corporations are connecting the dots about our personal behavior---silently scrutinizing clues left behind by our work habits and Internet use. But who connects the dots about what firms are doing with all this information? Frank Pasquale exposes how powerful interests abuse secrecy for profit and explains ways to rein them in.},
  isbn = {978-0-674-36827-9},
  langid = {english}
}

@misc{pengImpactAIDeveloper2023,
  title = {The Impact of {{AI}} on Developer Productivity: Evidence from {{GitHub}} Copilot},
  shorttitle = {The Impact of {{AI}} on Developer Productivity},
  author = {Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06590},
  eprint = {2302.06590},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06590},
  urldate = {2025-01-16},
  abstract = {Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8\% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/jonathanrystrom/Zotero/storage/JMY8QU62/2302.html}
}

@inproceedings{prabhakaranProceedings2ndWorkshop2024,
  title = {Proceedings of the 2nd Workshop on Cross-Cultural Considerations in {{NLP}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Cross-Cultural Considerations}} in {{NLP}}},
  author = {Prabhakaran, Vinodkumar and Dev, Sunipa and Benotti, Luciana and Hershcovich, Daniel and Cabello, Laura and Cao, Yong and Adebara, Ife and Zhou, Li},
  year = {2024},
  urldate = {2025-01-15},
  langid = {english}
}

@misc{qwenQwen25TechnicalReport2025,
  title = {Qwen2.5 Technical Report},
  author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  year = {2025},
  month = jan,
  number = {arXiv:2412.15115},
  eprint = {2412.15115},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.15115},
  urldate = {2025-02-10},
  abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/FRNI524J/2412.html}
}

@article{rafailovDirectPreferenceOptimization2023a,
  title = {Direct Preference Optimization: Your Language Model Is Secretly a Reward Model},
  shorttitle = {Direct Preference Optimization},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D. and Ermon, Stefano and Finn, Chelsea},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {53728--53741},
  urldate = {2025-01-30},
  langid = {english}
}

@inproceedings{rajpurkarSQuAD100000Questions2016,
  title = {{{SQuAD}}: 100,000+ Questions for Machine Comprehension of Text},
  shorttitle = {{{SQuAD}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
  year = {2016},
  month = nov,
  pages = {2383--2392},
  publisher = {Association for Computational Linguistics},
  address = {Austin, Texas},
  doi = {10.18653/v1/D16-1264},
  urldate = {2025-01-30},
  langid = {english}
}

@inproceedings{rauhCharacteristicsHarmfulText2022,
  title = {Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models},
  shorttitle = {Characteristics of Harmful Text},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Rauh, Maribeth and Mellor, John and Uesato, Jonathan and Huang, Po-Sen and Welbl, Johannes and Weidinger, Laura and Dathathri, Sumanth and Glaese, Amelia and Irving, Geoffrey and Gabriel, Iason and Isaac, William and Hendricks, Lisa Anne},
  year = {2022},
  month = nov,
  series = {{{NIPS}} '22},
  pages = {24720--24739},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-02-03},
  abstract = {Large language models produce human-like text that drives a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.},
  isbn = {978-1-71387-108-8},
  langid = {english}
}

@inproceedings{realASSIN2Shared2020,
  title = {The {{ASSIN}} 2 Shared Task: A Quick Overview},
  shorttitle = {The {{ASSIN}} 2 Shared Task},
  booktitle = {Computational {{Processing}} of the {{Portuguese Language}}: 14th {{International Conference}}, {{PROPOR}} 2020, {{Evora}}, {{Portugal}}, {{March}} 2--4, 2020, {{Proceedings}}},
  author = {Real, Livy and Fonseca, Erick and Gon{\c c}alo Oliveira, Hugo},
  year = {2020},
  month = mar,
  pages = {406--412},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-41505-1_39},
  urldate = {2025-01-30},
  abstract = {This paper offers a brief overview on the ASSIN 2, an evaluation shared task collocated with STIL 2019. ASSIN 2 covered two different but related tasks: Recognizing Textual Entailment (RTE), also known as Natural Language Inference (NLI), and Semantic Textual Similarity (STS). The ASSIN 2 collection was made of pairs of sentences annotated with human judgments for NLI and STS. Participating teams could take part in any of the tasks or both: nine teams participated in the STS task and eight in the NLI task.},
  isbn = {978-3-030-41504-4},
  langid = {english}
}

@inproceedings{rottgerPoliticalCompassSpinning2024,
  title = {Political Compass or Spinning Arrow? {{Towards}} More Meaningful Evaluations for Values and Opinions in Large Language Models},
  shorttitle = {Political Compass or Spinning Arrow?},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {R{\"o}ttger, Paul and Hofmann, Valentin and Pyatkin, Valentina and Hinck, Musashi and Kirk, Hannah and Schuetze, Hinrich and Hovy, Dirk},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {15295--15311},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.816},
  urldate = {2025-02-03},
  abstract = {Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing *constrained* evaluation paradigm for values and opinions in LLMs and explore more realistic *unconstrained* evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT *forces models to comply with the PCT`s multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.},
  langid = {english}
}

@inproceedings{santosBenchmarkingQuantizedLLaMabased2023,
  title = {Benchmarking Quantized {{LLaMa-based}} Models on the Brazilian Secondary School Exam},
  booktitle = {Anais Do {{XVI Congresso Brasileiro}} de {{Intelig{\^e}ncia Computacional}}},
  author = {Santos, Matheus L. O. and Campelo, Cl{\'a}udio E. C.},
  year = {2023},
  month = dec,
  eprint = {2309.12071},
  primaryclass = {cs},
  pages = {1--8},
  doi = {10.21528/CBIC2023-177},
  urldate = {2025-01-30},
  abstract = {Although Large Language Models (LLMs) represent a revolution in the way we interact with computers, allowing the construction of complex questions and the ability to reason over a sequence of statements, their use is restricted due to the need for dedicated hardware for execution. In this study, we evaluate the performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a quantization process and run on home hardware. The models considered were Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we developed a database containing 1,006 questions from the ENEM (Brazilian National Secondary School Exam). Our analysis revealed that the best performing models achieved an accuracy of approximately 46\% for the original texts of the Portuguese questions and 49\% on their English translations. In addition, we evaluated the computational efficiency of the models by measuring the time required for execution. On average, the 7 and 13 billion LLMs took approximately 20 and 50 seconds, respectively, to process the queries on a machine equipped with an AMD Ryzen 5 3600x processor},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/U3HG6CIQ/2309.html}
}

@inproceedings{sayamaFaQuADReadingComprehension2019,
  title = {{{FaQuAD}}: Reading Comprehension Dataset in the Domain of Brazilian Higher Education},
  shorttitle = {{{FaQuAD}}},
  booktitle = {2019 8th {{Brazilian Conference}} on {{Intelligent Systems}} ({{BRACIS}})},
  author = {Sayama, Helio Fonseca and De Araujo, Anderson Vicoso and Fernandes, Eraldo Rezende},
  year = {2019},
  month = oct,
  pages = {443--448},
  publisher = {IEEE},
  address = {Salvador, Brazil},
  doi = {10.1109/BRACIS.2019.00084},
  urldate = {2025-01-30},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72814-253-1},
  langid = {english}
}

@article{schielzethRobustnessLinearMixedeffects2020,
  title = {Robustness of Linear Mixed-effects Models to Violations of Distributional Assumptions},
  author = {Schielzeth, Holger and Dingemanse, Niels J. and Nakagawa, Shinichi and Westneat, David F. and Allegue, Hassen and Teplitsky, C{\'e}line and R{\'e}ale, Denis and Dochtermann, Ned A. and Garamszegi, L{\'a}szl{\'o} Zsolt and Araya-Ajoy, Yimen G.},
  editor = {Sutherland, Chris},
  year = {2020},
  month = sep,
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {9},
  pages = {1141--1152},
  issn = {2041-210X, 2041-210X},
  doi = {10.1111/2041-210X.13434},
  urldate = {2022-12-27},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/6E9HMY5T/Schielzeth et al. - 2020 - Robustness of linear mixed‐effects models to viola.pdf}
}

@inproceedings{selbstFairnessAbstractionSociotechnical2019,
  title = {Fairness and {{Abstraction}} in {{Sociotechnical Systems}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Selbst, Andrew D. and {boyd}, danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
  year = {2019},
  month = jan,
  pages = {59--68},
  publisher = {ACM},
  address = {Atlanta GA USA},
  doi = {10.1145/3287560.3287598},
  urldate = {2022-09-07},
  isbn = {978-1-4503-6125-5},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/VSQBZE4N/Selbst et al. - 2019 - Fairness and Abstraction in Sociotechnical Systems.pdf}
}

@article{shanahanTalkingLargeLanguage2024,
  title = {Talking about Large Language Models},
  author = {Shanahan, Murray},
  year = {2024},
  month = jan,
  journal = {Commun. ACM},
  volume = {67},
  number = {2},
  pages = {68--79},
  issn = {0001-0782},
  doi = {10.1145/3624724},
  urldate = {2025-02-03},
  abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
  langid = {english}
}

@book{sharifianRoutledgeHandbookLanguage2014,
  title = {The Routledge Handbook of Language and Culture},
  editor = {Sharifian, Farzad},
  year = {2014},
  month = dec,
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9781315793993},
  abstract = {The Routledge Handbook of Language and Culture presents the first comprehensive survey of research on the relationship between language and culture. It provides readers with a clear and accessible introduction to both interdisciplinary and multidisciplinary studies of language and culture, and addresses key issues of language and culturally based linguistic research from a variety of perspectives and theoretical frameworks. This Handbook features thirty-three newly commissioned chapters which cover key areas such as cognitive psychology, cognitive linguistics, cognitive anthropology, linguistic anthropology, cultural anthropology, and sociolinguistics offer insights into the historical development, contemporary theory, research, and practice of each topic, and explore the potential future directions of the field  show readers how language and culture research can be of practical benefit to applied areas of research and practice, such as intercultural communication and second language teaching and learning. Written by a group of prominent scholars from around the globe, The Routledge Handbook of Language and Culture provides a vital resource for scholars and students working in this area.},
  isbn = {978-1-315-79399-3},
  langid = {english}
}

@inproceedings{sharmaUnderstandingSycophancyLanguage2023,
  title = {Towards Understanding Sycophancy in Language Models},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Durmus, Esin and {Hatfield-Dodds}, Zac and Johnston, Scott R. and Kravec, Shauna M. and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  year = {2023},
  month = oct,
  urldate = {2025-02-03},
  abstract = {Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.},
  langid = {english}
}

@inproceedings{sorensenPositionRoadmapPluralistic2024,
  title = {Position: A Roadmap to Pluralistic Alignment},
  shorttitle = {Position},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L. and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and Althoff, Tim and Choi, Yejin},
  year = {2024},
  month = jul,
  pages = {46280--46302},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-02-01},
  abstract = {With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using large language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also formalize and discuss three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks that incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks that explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.},
  langid = {english},
  annotation = {shortConferenceName: ICML}
}

@article{tanChatgptArtificialLawyer2023,
  title = {Chatgpt as an Artificial Lawyer},
  author = {Tan, Jinzhe and Westermann, Hannes and Benyekhlef, Karim},
  year = {2023},
  journal = {Artificial Intelligence for Access to Justice (AI4AJ 2023)},
  file = {/Users/jonathanrystrom/Zotero/storage/6GLAW23I/Tan et al. - ChatGPT as an Artificial Lawyer.pdf}
}

@article{taoCulturalBiasCultural2024,
  title = {Cultural Bias and Cultural Alignment of Large Language Models},
  author = {Tao, Yan and Viberg, Olga and Baker, Ryan S and Kizilcec, Ren{\'e} F},
  year = {2024},
  month = sep,
  journal = {PNAS Nexus},
  volume = {3},
  number = {9},
  pages = {pgae346},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae346},
  urldate = {2025-01-02},
  abstract = {Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71--81\% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/3YUZZL3S/7756548.html}
}

@inproceedings{tjongkimsangIntroductionCoNLL2002Shared2002,
  title = {Introduction to the {{CoNLL-2002}} Shared Task: Language-Independent Named Entity Recognition},
  shorttitle = {Introduction to the {{CoNLL-2002}} Shared Task},
  booktitle = {{{COLING-02}}: {{The}} 6th {{Conference}} on {{Natural Language Learning}} 2002 ({{CoNLL-2002}})},
  author = {Tjong Kim Sang, Erik F.},
  year = {2002},
  urldate = {2025-01-30},
  langid = {english}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-05-12},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/3WZ7Z5FU/2302.html}
}

@inproceedings{ustunAyaModelInstruction2024,
  title = {Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model},
  shorttitle = {Aya Model},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre, Shayne and Muennighoff, Niklas and Fadaee, Marzieh and Kreutzer, Julia and Hooker, Sara},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {15894--15939},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.845},
  urldate = {2025-02-03},
  abstract = {Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50\% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages ------ including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.},
  langid = {english}
}

@inproceedings{weidingerTaxonomyRisksPosed2022,
  title = {Taxonomy of {{Risks}} Posed by {{Language Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
  year = {2022},
  month = jun,
  pages = {214--229},
  publisher = {ACM},
  address = {Seoul Republic of Korea},
  doi = {10.1145/3531146.3533088},
  urldate = {2022-08-15},
  isbn = {978-1-4503-9352-2},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/TQ2TU4MS/Weidinger et al. - 2022 - Taxonomy of Risks posed by Language Models.pdf}
}

@inproceedings{wrightLLMTropesRevealing2024,
  title = {{{LLM}} Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models},
  shorttitle = {{{LLM}} Tropes},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Wright, Dustin and Arora, Arnav and Borenstein, Nadav and Yadav, Srishti and Belongie, Serge and Augenstein, Isabelle},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {17085--17112},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.995},
  urldate = {2025-01-10},
  abstract = {Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.},
  langid = {english}
}

@inproceedings{yangProblematicTokensTokenizer2024,
  title = {Problematic Tokens: Tokenizer Bias in Large Language Models},
  shorttitle = {Problematic Tokens},
  booktitle = {2024 {{IEEE International Conference}} on {{Big Data}} ({{BigData}})},
  author = {Yang, Jin and Wang, Zhiqiang and Lin, Yanbin and Zhao, Zunduo},
  year = {2024},
  month = dec,
  pages = {6387--6393},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  doi = {10.1109/BigData62323.2024.10825615},
  urldate = {2025-02-03},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350362480},
  langid = {english}
}

@article{zhengJudgingLLMasajudgeMTbench2023,
  title = {Judging {{LLM-as-a-judge}} with {{MT-bench}} and Chatbot Arena},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {46595--46623},
  urldate = {2024-12-20},
  langid = {english}
}

@inproceedings{zhengLMSYSchat1MLargescaleRealworld2023,
  title = {{{LMSYS-chat-1M}}: A Large-Scale Real-World {{LLM}} Conversation Dataset},
  shorttitle = {{{LMSYS-chat-1M}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Hao},
  year = {2023},
  month = oct,
  urldate = {2025-02-07},
  abstract = {Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.},
  langid = {english}
}

@inproceedings{zhouLIMALessMore2023,
  title = {{{LIMA}}: Less Is More for Alignment},
  shorttitle = {Lima},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  year = {2023},
  month = nov,
  urldate = {2025-02-03},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43{\textbackslash}\% of cases; this statistic is as high as 58{\textbackslash}\% when compared to Bard and 65{\textbackslash}\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  langid = {english}
}

@misc{rottgerIssueBenchMillionsRealistic2025,
  title = {{{IssueBench}}: Millions of Realistic Prompts for Measuring Issue Bias in {{LLM}} Writing Assistance},
  shorttitle = {{{IssueBench}}},
  author = {R{\"o}ttger, Paul and Hinck, Musashi and Hofmann, Valentin and Hackenburg, Kobi and Pyatkin, Valentina and Brahman, Faeze and Hovy, Dirk},
  year = {2025},
  month = feb,
  number = {arXiv:2502.08395},
  eprint = {2502.08395},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.08395},
  urldate = {2025-02-14},
  abstract = {Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}
