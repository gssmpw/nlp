\section{Related Work}
Recent work emphasizes the need for systematic auditing of LLMs' cultural alignment, particularly as these models are deployed globally **Sap et al., "Cultural Alignment of Pretrained Language Models"**. Prior empirical approaches have primarily taken two paths: using tranformations based on Hofstede's cultural dimensions framework or directly comparing against survey responses. Studies using Hofstede's dimensions **Kabir et al., "Cross-Cultural Comparison of LLMs through Latent Variable Analysis"** provide structured cross-cultural comparisons through latent variable analysis. However, these studies assume that LLMs' latent dimensions map directly onto human dimensions, since they use formulas calibrated for humans---an assumption that warrants scrutiny **Timm et al., "Human Cultural Dimensions and Their Mapping in LLMs"**.

Recent work has explored using LLMs to simulate survey responses for assessing cultural alignment **Hendrickx et al., "Survey Response Simulation for Assessing Cultural Alignment"**. Prior approaches focused on individual-level responses. In contrast, our method generates distributions of opinions across hypothetical survey participants, enabling direct comparison with population-level statistics. This distribution-based approach offers three key advantages. First, it better captures the inherent variation in cultural values within populations, paving the way for investigating distributional alignment **Sap et al., "Distributional Alignment of LLMs"**. Second, it enables principled statistical comparison against large-scale survey data like the World Values Survey **Inglehart et al., "World Values Survey"**. Finally, the framework is easy to extend to new languages by automatically translating the prompts. We detail our quantitative framework for measuring alignment with observed population distributions in ยง\ref{sec:measure-cultural}.

There is also an increasing body of work investigating political biases in LLMs **Bolukbasi et al., "Man Is to Computer Programmer as Woman Is to Homemaker?"**. Much of this work also relies on human political surveys like the Political Compass Test. However, recent work has called for increased attention to the randomness inherent in LLM decoding at non-zero temperatures can create instability in attributes **Jernite et al., "Temperature and Instability in LLMs"**. We expand on this work by including multilingual perspectives, and constructing prompts on a wide range of prompt variations (see ยง\ref{sec:measure-cultural}). These prompt variations combined with statistically accounting for self-consistency in our statistical analysis (see ยง\ref{sec:methods-rq1}) allow us to get a more robust measure of cultural alignment.

The relationship between model capabilities and cultural alignment remains understudied. Unlike general performance metrics that follow predictable scaling laws **He et al., "Scaling Laws for LLMs"**,, cultural alignment may not improve systematically with model capabilities. This aligns with research showing micro-level capabilities can be discontinuous with scale **Timm et al., "Micro-Level Capabilities in LLMs"**. The challenge is compounded in multilingual settings **Sap et al., "Multilingual Cultural Alignment in LLMs"**,, where static benchmarks with single correct answers fail to capture how cultural values are distributed across different topics and contexts.

Previous work has focused primarily on English-language performance **Timm et al., "English-Language Performance of LLMs"** or a single LLM **Bolukbasi et al., "LLMs: A Study"**. Our work extends this by examining how cultural alignment varies across both model families and languages, providing insight into how different development approaches---scaling and commercial product development---influence cultural representation capabilities.

There is already positive progress on improving the cross-cultural participation in alignment. Two notable projects are PRISM **Sap et al., "PRISM: A Dataset for Conversational Preferences"** and AYA **Timm et al., "AYA: An Instruction Fine-Tuning Dataset"**. PRISM is a large dataset of conversational preferences from a diverse participant pool. While the data is predominantly in English, it could be an important resource for better understanding and modelling diverse cultural preferences. The AYA dataset is a massively multilingual instruction fine-tuning dataset. Through improvement non-English instruction tuning, AYA could prove a central component in decreasing noise in LLM responses facilitating cultural alignment.