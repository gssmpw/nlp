@inproceedings{alkhamissiInvestigatingCulturalAlignment2024,
  title = {Investigating Cultural Alignment of Large Language Models},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {AlKhamissi, Badr and ElNokrashy, Muhammad and Alkhamissi, Mai and Diab, Mona},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {12404--12422},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.671},
  urldate = {2025-02-03},
  abstract = {The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions---firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.},
  langid = {english}
}

@inproceedings{caoAssessingCrossculturalAlignment2023,
  title = {Assessing Cross-Cultural Alignment between {{ChatGPT}} and Human Societies: An Empirical Study},
  shorttitle = {Assessing Cross-Cultural Alignment between {{ChatGPT}} and Human Societies},
  booktitle = {Proceedings of the {{First Workshop}} on {{Cross-Cultural Considerations}} in {{NLP}} ({{C3NLP}})},
  author = {Cao, Yong and Zhou, Li and Lee, Seolhwa and Cabello, Laura and Chen, Min and Hershcovich, Daniel},
  editor = {Dev, Sunipa and Prabhakaran, Vinodkumar and Adelani, David Ifeoluwa and Hovy, Dirk and Benotti, Luciana},
  year = {2023},
  month = may,
  pages = {53--67},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.c3nlp-1.7},
  urldate = {2025-02-10},
  abstract = {The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations. Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.},
  langid = {english}
}

@misc{evs/wvsJointEVSWVS2022,
  title = {Joint {{EVS}}/{{WVS}} 2017-2022 {{Dataset}}},
  author = {{EVS/WVS}},
  year = {2022},
  publisher = {GESIS},
  doi = {10.4232/1.14023},
  urldate = {2023-06-29},
  abstract = {The European Values Study (EVS) and the World Values Survey (WVS) are two large-scale, cross-national and longitudinal survey research programmes. They include a large number of questions on moral, religious, social, political, occupational and family values which have been replicated since the early eighties. Both organizations agreed to cooperate in joint data collection from 2017. EVS has been responsible for planning and conducting surveys in European countries, using the EVS questionnaire and EVS methodological guidelines. WVSA has been responsible for planning and conducting surveys in countries in the world outside Europe, using the WVS questionnaire and WVS methodological guidelines. Both organisations developed their draft master questionnaires independently. The joint items define the Common Core of both questionnaires. The Joint EVS/WVS is constructed from the two EVS and WVS source datasets: - European Values Study 2017 Integrated Dataset (EVS 2017), ZA7500 Data file Version 5.0.0, doi:10.4232/1.13897 (https://doi.org/10.4232/1.13897). - World Values Survey: Round Seven--Country-Pooled Datafile. Version 5.0.0, doi: 10.14281/18241.20},
  collaborator = {Gedeshi, Ilir and Rotman, David and Kritzinger, Sylvia and Poghosyan, Gevorg and Pachulia, Merab and {Kolenovi{\'c}-{\DJ}apo}, Jadranka and Fotev, Georgy and Baloban, Stjepan and Baloban, Josip and Saar, Erki and Frederiksen, Morten and Rabu{\v s}ic, Ladislav and Ketola, Kimmo and Br{\'e}chon, Pierre and Wolf, Christof and Pachulia, Merab and Rosta, Gergely and Voas, David and J{\'o}nsd{\'o}ttir, Gu{\dh}bj{\"o}rg A. and Rovati, Giancarlo and Ziliukaite, Ruta and Petkovska, Antoanela and Reeskens, Tim and Jenssen, Anders T. and Komar, Olivera and Voicu, Bogdan and Marody, Miros{\l}awa and Soboleva, Natalia and Be{\v s}i{\'c}, Milo{\v s} and Strapcov{\'a}, Katarina and Uhan, Samo and Silvestre Cabrera, Mar{\'i}a and {Wallman-Lund{\aa}sen}, Susanne and Ernst St{\"a}hli, Mich{\`e}le and Ramos, Alice and Mic{\'o} Ib{\'a}{\~n}ez, Joan and Carballo, Marita and McAllister, Ian and Bangladesh), Roberto Stefan (PI, Foa and Moreno Morales, Daniel E. and De Oliveira De Castro, Henrique Carlos and Lagos, Marta and Zhong, Yang and Colombia), Andres (PI, Casas and Cyprus), Birol (PI, Yesilada and Paez, Cristina and Abdel Latif, Abdel Hamid and Ethiopia), Will (PI, Jennings and Welzel, Christian and D{\'i}az Argueta, Julio C{\'e}sar and Cheng, Edmund and Indonesia), Timothy (PI, Gravelle and Stoker, Gerry and Dagher, Munqith and Yamazaki, Seiko and Braizat, Fares and Rakisheva, Botagoz and Bakaloff, Yuri and Lebanon), Christian (PI, Haerpfer and {Wing-Yat Yu}, Eilo and Lee, Grace and Moreno, Alejandro and Souvanlasy, Chansada and Perry, Paul and Nicaragua), Carlos (PI, Denton and Nigeria), Bi (PI, Puranen and Gilani, Bilal and Romero, Catalina and Guerrero, Linda and Hern{\'a}ndez Acosta, Javier J. and Voicu, Bogdan and Zavadskaya, Margarita and Veskovic, Nino and Auh, Soo Young and Tsai, Ming-Chang and Olimov, Muzaffar and Bureekul, Thawilwadee and Ben Hafaiedh, Abdelwahab and Esmer, Yilmaz and Inglehart, Ronald and Depouilly, Xavier and Zimbabwe), Pippa (PI, Norris and Balakireva, Olga and Lachapelle, Guy and Mathews, Mathew and Mieri{\c n}a, Inta and Manasyan, Heghine and Kenya), Anna M. (PI, Ekstroem and Swehli, Nedal and Riyaz, Aminath and Tseveen, Tsetsenbileg and Abderebbi, Mhammed and Verhoeven, Piet and {Briceno-Leon}, Roberto and Moravec, Vaclav and Duffy, Bobby and Stoneman, Paul and Kosnac, Pavol and Zuasnabar, Ignacio and {Koniordos. Sokratis} and {EVS 2017:Center For Economic And Social Studies (CESS), Tirana, Albania} and {InterRating CoLtd, Yerevan, Armenia} and {Institut F{\"u}r Empirische Sozialforschung (IFES) GmbH, Vienna, Austria} and {Sorgu, Baku, Azerbaijan} and {Centre For Sociological And Political Research, Belarusian State University, Minsk, Belarus} and {Custom Concept D.O.O., Sarajevo, Bosnia And Herzegovina} and {Alpha Research LTD, Sofia, Bulgaria} and {Catholic University Of Croatia, Zagreb, And GfK Research Agency, Zagreb, Croatia} and {STEM/MARK, A.S., Praha, Czech Republic} and {Statistics Denmark-Survey, Copenhagen, Denmark} and {AS Emor,Tallinn, Estonia} and {Taloustutkimus Oy, Lemuntie 9, 00910 Helsinki, Finland} and {KANTAR PUBLIC-TAYLOR NELSON SOFRES, Paris, France} and {GORBI (Georgian Opinion Research Business International), Tbilisi, Georgia} and {Kantar Deutschland GmbH, Kantar Public, M{\"u}nchen, Germany} and {NatCen Social Research, London, Great Britain} and {Forsense, Budapest, Hungary} and {Social Science Research Institute, SSRI, University Of Iceland, Reykjavik, Iceland} and {Doxa Spa, Milano, Italy} and {Baltic Surveys, Vilnius, Lithuania} and {DeFacto Consultancy, Podgorica, Montenegro} and {I\&O Research B.V., Enschede, Netherlands AndCentERdata, Tilburg, Netherlands} and {Faculty Of Philosophy, Skopje, North Macedonia} and {Statistics Norway, Oslo, Norway} and {Centrum Badania Opinii Spo{\l}ecznej (Public Opinion Research Centre), Warszawa, Poland} and {IRES: Institutul Roman Pentru Evaluare Si Strategie, Romania} and {CESSI (Institute For Comparative Social Research), Moscow, Russia} and {Nina Media, Novi Sad, Serbia} and {Kantar TNS, Bratislava, Slovakia} and {University Of Ljubljana, Faculty Of Social Science, Ljubljana, Slovenia} and {MyWord Research SL, Madrid, Spain} and {IPSOS Observer Sweden AB, H{\"a}rn{\"o}sand, Sweden} and {M.I.S Trend S.A} and {Lausanne, Switzerland (Face-To-Face) AndSwiss Centre For Expertise In The Social Sciences FORS C/O University Of Lausanne, Lausanne, Switzerland (Web-Mail)} and {GfK-Metris, Lisbon, Portugal} and {Social Monitoring Center, Kiev, Ukraine} and {Sociolo{\c g}isko P{\=e}t{\=i}jumu Instit{\=u}ts, Riga, LatviaWVS Wave 7Institut D'Estudis Andorrans, Centre De Recerca Sociol{\`o}gica (CRES), Andorra} and {Voices Research And Consultancy S.A., Argentina} and {Centre For Social Research And Methods, Australian National University} and {SRG Bangladesh Limited (SRGB), Bangladesh} and {CIUDADANIA, Comunidad De Estudios Sociales Y Acci{\'o}n P{\'u}blica, Bolivia} and {Federal University Of Rio Grande Do Sul, Brazil} and {Market Opinion Research International, Chile} and {Public Opinion Research Center Of School Of International And Public Affairs At Shanghai Jiao Tong University, China} and {Invamer, Colombia} and {Cymar Research Company (Survey In Cyprus South)} and {Prologue Consulting Ltd. (Survey In Cyprus North)} and {IPSOS Ecuador} and {Egyptian Research And Training Center, Egypt} and {WAAS International/ TNS RMS Nigeria Limited / Kantar (Survey In Ethiopia)} and {Kantar Deutschland GmbH, Kantar Public, M{\"u}nchen, Germany} and {National Centre Of Social Research (EKKE) \& DIANEOSIS \& Metron Analysis, Greece} and {Innovation, Development \& Research, S.A. \& Social Work School Of The University Of San Carlos Of Guatemala} and {Centre For Communication And Public Opinion Survey (CCPOS) Of The Chinese University Of Hong Kong For FTF} and {Survey Sampling International (SSI) For CAWI, Hong Kong SAR PRC} and {Survey Meter, Indonesia} and {R-Research Limited, UK (Survey In Iran)} and {International Institute For Administration And Social Survey (IIACSS), Jordan (Survey In Iraq)} and {Nippon Research Center, Ltd., Japan} and {NAMA Strategic Intelligence Solutions, Jordan} and {Public Opinion Research Institute, Kazakhstan} and {Central Asia Barometer, Kyrgyzstan} and {Statistics Lebanon Ltd.} and {University Of Macao, China} and {IPSOS Malaysia} and {Moreno \& Sotnikova Social Research And Consulting S.C., Mexico} and {IRL (Indochina Research Laos) Myanmar Limited, Myanmar} and {School Of People, Environment And Planning, Massey University, New Zealand} and {CID/Gallup, S.A. (Survey In Nicaragua)} and {TNS RMS Nigeria Limited / Kantar} and {Gallup Pakistan} and {Public Opinion Institute At Pontifical Catholic University Of Peru} and {Social Weather Stations, Philippines} and {Universidad Del Sagrado Coraz{\'o}n, Puerto Rico} and {IRES: Institutul Roman Pentru Evaluare Si Strategie, Romania} and {CESSI-Institute For Comparative Social Research (Survey In Russia)} and {Singidunum University Belgrade, Serbia} and {Gallup South Korea} and {Institute Of Sociology, Academia Sinica, Taipei, Taiwan ROC} and {Research Centre SHARQ /Oriens, Tajikistan} and {King Prajadhipok's Institute, Thailand} and {Applied Social Science Forum, Tunisia} and {Bahcesehir University, Turkey} and {University Of Chicago, NORC AmeriSpeak, USA} and {Indochina Research Ltd. Vietnam} and {Consumer Feedback, Zimbabwe \& TNS RMS Nigeria Limited/ Kantar} and {Social Monitoring Center} and {Info Sapiens Research Center} and {Ukrainian Center For European Policy, Ukraine} and {Leger, Montreal, Canada} and {Institute Of Policy Studies, National University Of Singapore, Singapore} and {CRRC-Armenia, Yerevan, Armenia} and {Frontier Consulting Kenya, Nairobi, Kenya} and {Diwan Research, Tripoli, Libya} and {Maldives National University, Mal{\'e}, Maldives} and {Institute Of Philosophy, Mongolian Academy Of Sciences, Ulaanbaatar, Mongolia} and {Global For Survey And Consulting, Casablanca, Morocco} and {Centerdata, Tilburg, Netherlands} and {Laboratorio De Ciencias Sociales (LACSO), Caracas, Venezuela} and {FOCUS Polling Agency, Czech Republic} and {IPSOS, Great Britain} and {IPSOS, Northern Ireland} and {FOCUS Polling Agency, Slovakia} and {Equipos Consultores, Uruguay}},
  copyright = {Alle im GESIS DBK ver{\"o}ffentlichten Metadaten sind frei verf{\"u}gbar unter den Creative Commons CC0 1.0 Universal Public Domain Dedication. GESIS bittet jedoch darum, dass Sie alle Metadatenquellen anerkennen und sie nennen, etwa die Datengeber oder jeglichen Aggregator, inklusive GESIS selbst. F{\"u}r weitere Informationen siehe https://dbk.gesis.org/dbksearch/guidelines.asp?db=d, All metadata from GESIS DBK are available free of restriction under the Creative Commons CC0 1.0 Universal Public Domain Dedication. However, GESIS requests that you actively acknowledge and give attribution to all metadata sources, such as the data providers and any data aggregators, including GESIS. For further information see https://dbk.gesis.org/dbksearch/guidelines.asp},
  langid = {english},
  keywords = {ARBEIT UND BESCHAFTIGUNG,Cultural and national identity,Familie und Ehe,Family life and marriage,Gender and gender roles,Geschlecht und Geschlechterrollen,KAT15 Political Attitudes and Behavior,KAT37 Work and Industry,KAT50 Society Culture,KAT53 Family,KAT54 Person Personality Role,KAT57 Religion and "Weltanschauung",Kulturelle und nationale Identitat,LABOUR AND EMPLOYMENT,Political behaviour and attitudes,Politisches Verhalten und politische Einstellungen,Religion and values,Religion und Werte}
}

@inproceedings{ganguliPredictabilitySurpriseLarge2022,
  title = {Predictability and Surprise in Large Generative Models},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Dassarma, Nova and Drain, Dawn and Elhage, Nelson},
  year = {2022},
  pages = {1747--1764}
}

@article{hartmannPoliticalIdeologyConversational2023,
  title = {The Political Ideology of Conversational {{AI}}: {{Converging}} Evidence on {{ChatGPT}}'s pro-Environmental, Left-Libertarian Orientation},
  shorttitle = {The Political Ideology of Conversational {{AI}}},
  author = {Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4316084},
  urldate = {2023-02-19},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/EMKQCHIJ/Hartmann et al. - 2023 - The political ideology of conversational AI Conve.pdf}
}

@inproceedings{hoffmannTrainingComputeoptimalLarge2022a,
  title = {Training Compute-Optimal Large Language Models},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and {de Las Casas}, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and {van den Driessche}, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
  year = {2022},
  month = nov,
  series = {{{NIPS}} '22},
  pages = {30016--30030},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-02-10},
  abstract = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4{\texttimes} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  isbn = {978-1-71387-108-8},
  langid = {english}
}

@article{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08361 [cs, stat]},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  urldate = {2022-04-28},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jonathanrystrom/Zotero/storage/QTVSIES5/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/jonathanrystrom/Zotero/storage/6WXPNUYD/2001.html}
}

@article{kirkBenefitsRisksBounds2024,
  title = {The Benefits, Risks and Bounds of Personalizing the Alignment of Large Language Models to Individuals},
  author = {Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A.},
  year = {2024},
  month = apr,
  journal = {Nature Machine Intelligence},
  volume = {6},
  number = {4},
  pages = {383--392},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-024-00820-y},
  urldate = {2025-01-09},
  abstract = {Large language models (LLMs) undergo `alignment' so that they better reflect human values or preferences, and are safer or more useful. However, alignment is intrinsically difficult because the hundreds of millions of people who now interact with LLMs have different preferences for language and conversational norms, operate under disparate value systems and hold diverse political beliefs. Typically, few developers or researchers dictate alignment norms, risking the exclusion or under-representation of various groups. Personalization is a new frontier in LLM development, whereby models are tailored to individuals. In principle, this could minimize cultural hegemony, enhance usefulness and broaden access. However, unbounded personalization poses risks such as large-scale profiling, privacy infringement, bias reinforcement and exploitation of the vulnerable. Defining the bounds of responsible and socially acceptable personalization is a non-trivial task beset with normative challenges. This article explores `personalized alignment', whereby LLMs adapt to user-specific data, and highlights recent shifts in the LLM ecosystem towards a greater degree of personalization. Our main contribution explores the potential impact of personalized LLMs via a taxonomy of risks and benefits for individuals and society at large. We lastly discuss a key open question: what are appropriate bounds of personalization and who decides? Answering this normative question enables users to benefit from personalized alignment while safeguarding against harmful impacts for individuals and society.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Information technology,Science,technology and society}
}

@inproceedings{kirkPRISMAlignmentDataset2024,
  title = {The {{PRISM}} Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals about the Subjective and Multicultural Alignment of Large Language Models},
  shorttitle = {The {{PRISM}} Alignment Dataset},
  booktitle = {The {{Thirty-eight Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew Michael and Margatina, Katerina and Mosquera, Rafael and Ciro, Juan Manuel and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
  year = {2024},
  month = nov,
  urldate = {2025-01-16},
  abstract = {Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide alignment data.},
  langid = {english}
}

@inproceedings{masoudCulturalAlignmentLarge2025,
  title = {Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede`s Cultural Dimensions},
  shorttitle = {Cultural Alignment in Large Language Models},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Masoud, Reem and Liu, Ziquan and Ferianc, Martin and Treleaven, Philip C. and Rodrigues, Miguel Rodrigues},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and {Al-Khalifa}, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  year = {2025},
  month = jan,
  pages = {8474--8503},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  urldate = {2025-02-03},
  abstract = {The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede`s CAT) to quantify cultural alignment using Hofstede`s cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs---namely Llama 2, GPT-3.5, and GPT-4---against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models' behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page https://github.com/reemim/Hofstedes\_CAT},
  langid = {english}
}

@article{mokanderAuditingLargeLanguage2024,
  title = {Auditing Large Language Models: A Three-Layered Approach},
  shorttitle = {Auditing Large Language Models},
  author = {M{\"o}kander, Jakob and Schuett, Jonas and Kirk, Hannah Rose and Floridi, Luciano},
  year = {2024},
  month = nov,
  journal = {AI and Ethics},
  volume = {4},
  number = {4},
  pages = {1085--1115},
  issn = {2730-5961},
  doi = {10.1007/s43681-023-00289-2},
  urldate = {2024-11-26},
  abstract = {Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial Intelligence,Auditing,Ethics,Foundation models,Governance,Large language models,Natural language processing,Policy,Risk management}
}

@misc{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4}} Technical Report},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2024},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2025-01-15},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/jonathanrystrom/Zotero/storage/L7ZXZMKQ/2303.html}
}

@misc{rottgerIssueBenchMillionsRealistic2025,
  title = {{{IssueBench}}: Millions of Realistic Prompts for Measuring Issue Bias in {{LLM}} Writing Assistance},
  shorttitle = {{{IssueBench}}},
  author = {R{\"o}ttger, Paul and Hinck, Musashi and Hofmann, Valentin and Hackenburg, Kobi and Pyatkin, Valentina and Brahman, Faeze and Hovy, Dirk},
  year = {2025},
  month = feb,
  number = {arXiv:2502.08395},
  eprint = {2502.08395},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.08395},
  urldate = {2025-02-14},
  abstract = {Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{rottgerPoliticalCompassSpinning2024,
  title = {Political Compass or Spinning Arrow? {{Towards}} More Meaningful Evaluations for Values and Opinions in Large Language Models},
  shorttitle = {Political Compass or Spinning Arrow?},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {R{\"o}ttger, Paul and Hofmann, Valentin and Pyatkin, Valentina and Hinck, Musashi and Kirk, Hannah and Schuetze, Hinrich and Hovy, Dirk},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {15295--15311},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.816},
  urldate = {2025-02-03},
  abstract = {Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing *constrained* evaluation paradigm for values and opinions in LLMs and explore more realistic *unconstrained* evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT *forces models to comply with the PCT`s multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.},
  langid = {english}
}

@article{shanahanTalkingLargeLanguage2024,
  title = {Talking about Large Language Models},
  author = {Shanahan, Murray},
  year = {2024},
  month = jan,
  journal = {Commun. ACM},
  volume = {67},
  number = {2},
  pages = {68--79},
  issn = {0001-0782},
  doi = {10.1145/3624724},
  urldate = {2025-02-03},
  abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
  langid = {english}
}

@inproceedings{sorensenPositionRoadmapPluralistic2024,
  title = {Position: A Roadmap to Pluralistic Alignment},
  shorttitle = {Position},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L. and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and Althoff, Tim and Choi, Yejin},
  year = {2024},
  month = jul,
  pages = {46280--46302},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-02-01},
  abstract = {With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using large language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also formalize and discuss three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks that incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks that explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.},
  langid = {english},
  annotation = {shortConferenceName: ICML}
}

@article{taoCulturalBiasCultural2024,
  title = {Cultural Bias and Cultural Alignment of Large Language Models},
  author = {Tao, Yan and Viberg, Olga and Baker, Ryan S and Kizilcec, Ren{\'e} F},
  year = {2024},
  month = sep,
  journal = {PNAS Nexus},
  volume = {3},
  number = {9},
  pages = {pgae346},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae346},
  urldate = {2025-01-02},
  abstract = {Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71--81\% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.},
  langid = {english},
  file = {/Users/jonathanrystrom/Zotero/storage/3YUZZL3S/7756548.html}
}

@inproceedings{ustunAyaModelInstruction2024,
  title = {Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model},
  shorttitle = {Aya Model},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {{\"U}st{\"u}n, Ahmet and Aryabumi, Viraat and Yong, Zheng and Ko, Wei-Yin and D'souza, Daniel and Onilude, Gbemileke and Bhandari, Neel and Singh, Shivalika and Ooi, Hui-Lee and Kayid, Amr and Vargus, Freddie and Blunsom, Phil and Longpre, Shayne and Muennighoff, Niklas and Fadaee, Marzieh and Kreutzer, Julia and Hooker, Sara},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {15894--15939},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.845},
  urldate = {2025-02-03},
  abstract = {Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50\% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages ------ including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.},
  langid = {english}
}

@inproceedings{wrightLLMTropesRevealing2024,
  title = {{{LLM}} Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models},
  shorttitle = {{{LLM}} Tropes},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Wright, Dustin and Arora, Arnav and Borenstein, Nadav and Yadav, Srishti and Belongie, Serge and Augenstein, Isabelle},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {17085--17112},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.995},
  urldate = {2025-01-10},
  abstract = {Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.},
  langid = {english}
}

