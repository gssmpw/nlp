% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}  % For subfigures
\usepackage{booktabs}
\usepackage{amsmath}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jonathan Rystrøm \and Hannah Rose Kirk \and Scott Hale \\
        Oxford Internet Institute, University of Oxford, UK \\
        \texttt{jonathan.rystrom@oii.ox.ac.uk}}

\hyphenation{Open-AI}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.
\end{abstract}

%TL;DR: We empirically investigate the relationship between multilingual capabilities and multicultural alignment and US-centric bias and find inconsistent effects.

% Potential keywords: multilingual evaluation,multilingual representations,language/cultural bias analysis

\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/multilingual_coefficients.png}
    \caption{The relationship between multilingual capability and cultural alignment is inconsistent across LLM families, as shown by coefficients from our linear mixed-effects model (Eq.~\ref{eq:rq1-eq}; §\ref{sec:methods-rq1}). OpenAI models show negative or insignificant relationships outside of English, while Gemma models show positive relationships throughout ($p<.05$).}
    \label{fig:rq1-eq-main}
\end{figure}
Spearheaded by accessible chat interfaces to powerful models like ChatGPT \citep{openaiChatGPTOptimizingLanguage2022}, LLMs are reaching hundreds of millions of users \citep{milmoChatGPTReaches1002023}. These models are deployed across diverse contexts: from tutoring mathematics \citep{khanHarnessingGPT4That2023} to building software applications \citep{pengImpactAIDeveloper2023} to assisting in legal cases \citep{tanChatgptArtificialLawyer2023}. While most LLMs demonstrate multilingual abilities \citep{ustunAyaModelInstruction2024}, the ability to communicate across languages does not necessarily translate into appropriate cultural representations. Disentangling language capabilities and cultural alignment is crucial for understanding how LLMs should be examined and audited \citep{mokanderAuditingLargeLanguage2024} and for ensuring these technologies work for diverse people \citep{dignazioDataFeminism2020,weidingerTaxonomyRisksPosed2022}.

A priori, we might expect these models to exhibit US-centric cultural biases despite their multilingual capabilities, given their development context: Although millions use LLMs, they are developed by a select few. Many model providers, such as OpenAI, Anthropic, and Google, are American technology companies headquartered in Silicon Valley. These companies comprise a narrow slice of human experience, limiting the voices that contribute to critical design decisions in LLMs \citep{dignazioDataFeminism2020}. The companies typically train LLMs on massive amounts of predominantly English text and employ American crowd workers to rate and evaluate the LLMs' responses \citep{johnsonGhostMachineHas2022,kirkPresentBetterFuture2023}. Far too often, the benefits and harms of data technologies are unequally distributed, reinforcing biases and harming already minoritized groups \citep{birhaneAlgorithmicColonizationAfrica2020,milanBigDataSouths2019,khandelwalIndianBhEDDatasetMeasuring2024}. Understanding how LLMs represent different cultures is thus paramount to establishing risks of representational harm \citep{rauhCharacteristicsHarmfulText2022} and ensuring the technology's utility is shared across diverse communities.

Increasing diversity and cross-cultural understanding is stymied by unchecked assumptions in both alignment techniques and evaluation methodologies. First, there is an assumption that bigger and more capable LLMs trained on more data will be inherently easier to align \citep{zhouLIMALessMore2023, kunduSpecificGeneralPrinciples2023}, but this sidesteps the thorny question of pluralistic variation and cultural representations \citep{kirkPRISMAlignmentDataset2024}. Thus, it is unclear whether improvements in architecture \citep{fedusReviewSparseExpert2022} and post-training methods \citep{kirkPresentBetterFuture2023,rafailovDirectPreferenceOptimization2023a} translate into improvements in cultural alignment.

Although studies like the World Values Survey (WVS) have documented how values vary across cultures \citep{evs/wvsJointEVSWVS2022}, it remains unclear whether more capable LLMs—through scaling or improved training—better align with these cultural differences \citep{baiTrainingHelpfulHarmless2022,kirkPresentBetterFuture2023}. While the WVS has been used in prior research on values in LLMs, these studies have focused predominately on individual models' performance within an English-language context. \citep{caoAssessingCrossculturalAlignment2023,aroraProbingPretrainedLanguage2023,alkhamissiInvestigatingCulturalAlignment2024}. This paper addresses this gap by developing a methodology for assessing how well families of LLMs represent different cultural contexts across multiple languages. We compare two distinct paths to model improvement: systematic scaling of instruction-tuned models and commercial product development comprising scaling and innovation in post-training to accommodate pressures from capabilities, cost, and preferences \citep{openaiGPT4oSystemCard2024}.

Given these considerations, we investigate the following research questions:

\begin{description}
   \item[RQ1] \textbf{Multilingual Cultural Alignment:} Does improved multilingual capability increase LLM alignment with population-specific value distributions?
   \item[RQ2] \textbf{US-centric Bias:} When using different languages, do LLMs align more with US values or with values from the countries where these languages are native?
   %\item[RQ2] \textbf{US-centric Bias:} Do LLMs exhibit stronger value alignment with the US or with countries where the languages are spoken?
\end{description}

We operationalise \textit{multilingual capability} as an LLM's performance on a range of multilingual benchmarks across languages \citep[see, e.g.,][]{nielsenScandEvalBenchmarkScandinavian2023a}. We describe the specific benchmarks and performances in Appendix \ref{app:multilingual}.

This work makes several key contributions. First, we introduce a novel distribution-based methodology for probing cultural alignment across languages, moving beyond direct survey approaches to better capture underlying cultural values \citep{sorensenPositionRoadmapPluralistic2024}. Second, we provide the first systematic comparison of how improvements in scale and post-training affect cultural alignment and US-centric bias across English, Danish, Dutch, and Portuguese through a series of robust statistical models. Third, we release a dataset of model-generated responses across multiple languages and cultural contexts as well as our code, enabling future research into cultural alignment and bias.\footnote{See https://github.com/jhrystrom/multicultural-alignment for code and https://huggingface.co/datasets/ryzzlestrizzle/multicultural-wvs-alignment for data} Together, these contributions advance our understanding of how LLM development choices influence cultural representation while providing tools for ongoing investigation of these critical issues.

\section{Measuring Cultural Alignment} \label{sec:measure-cultural}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/country_correlations.png}
    \caption{Pearson correlations in value polarity scores across studied countries from the World Values Survey. All correlations are positive with most being between 0.7-0.95.}
    \label{fig:wvs-correlations}
\end{figure}

This section defines what we mean by `cultural alignment' and how to measure it in LLMs. First, we provide a high-level intuition of the cultural alignment as reproducing distributions of values in a particular population. Then we show how to a) get a ground-truth distribution of values using the World Values Survey (§\ref{sec:wvs}) and b) elicit value distributions from LLMs (§\ref{sec:llm-responses})

\paragraph{Cultural alignment as value reproduction:}
Within a culture there will be a variety of stances to any particular topic. However, the \textit{distribution} of stances will be characteristic among cultures. For instance, while some Danes are opposed to abortion, it is a much less contentious topic than in the US \cite{adamczykExaminingPublicOpinion2020,norupAttitudesAbortionDanish1997}. 

We posit that cultural alignment for a specific group of people can be operationalised as how well an LLM reproduces the distribution of values over a wide range of topics \citep{sorensenPositionRoadmapPluralistic2024}. By investigating \textit{distributions} of responses, we differ from previous work that directly surveys the LLMs as a regular participant \cite[e.g.,][]{caoAssessingCrossculturalAlignment2023}. Our goal is to get more naturalistic elicitations of the underlying values and avoid sycophancy and response bias \cite{sharmaUnderstandingSycophancyLanguage2023}. 

More specifically, we operationalise reproduction as having a high correlation between \textit{value polarity score}: the fraction of people in favour of a topic in the population and the fraction of responses in favour of the topic elicited by the LLM. Note, that we binarise issues to allow for simpler operationalisation. Below, we describe how we empirically estimate the value polarity score for ground truth (§\ref{sec:wvs}) and LLMs (§\ref{sec:llm-responses}). 

\subsection{Ground Truth: World Values Survey} \label{sec:wvs}
To get a `ground truth' distribution of cultural values, we use the joint World Values Survey and European Values Survey \citep[EVS;][]{evs/wvsJointEVSWVS2022}. These large-scale international surveys cover adults across 92 countries with samples that are nationally representative for gender, age, education, and religion. The surveys' broad coverage enables cross-cultural comparability, though some scholars note challenges in ensuring response comparability across countries \citep{alemanValueOrientationsWorld2016}. 

We select a subset of questions corresponding to values about the environment, work, family, politics \& society, religion \& morals, and security---all questions where there is are clear `for' and `against' positions. These questions comprise our topics. Appendix \ref{app:wvs-questions-all} shows the full list of included questions.

The final step is calculating the value polarity score. For each question, we define what an `affirmative' response means (i.e., a Likert score higher/lower than the middle score, depending on the directionality of the question)  and then define the value polarity score as the (weighted) fraction of respondents with an affirmative stance to the topic within the population. Thus, a culture's values can be represented as a vector, where each element corresponds to a value polarity score for a specific topic.

\subsection{Ecologically valid LLM responses} \label{sec:llm-responses}
Testing cultural alignment effectively requires embedding contextual and cultural elements in ways that maintain ecological validity. At a high level, eliciting values from an LLM consist of two steps: 1) Iteratively prompting the model with the selected topics and 2) extracting the stances from each model response.

\paragraph{Setting prompt context:}
Developing ecologically valid prompts requires careful consideration. When evaluating LLM responses to value-laden topics, simply asking questions like ``What proportion of people support {topic X}?'' or ``Do you support {topic X}?'' proves inadequate \citep[e.g.,][]{hartmannPoliticalIdeologyConversational2023}. Such direct approaches suffer from three key limitations: they generate false positives through excessive agreement, fail to reflect realistic usage patterns, and provide insufficient variation to assess cultural alignment \citep{rottgerPoliticalCompassSpinning2024}. They also struggle to capture instance-specific harms that emerge when systems misalign with users' cultural contexts \citep{rauhCharacteristicsHarmfulText2022}.

Instead, we adopt an implicit approach by asking the model to generate responses from hypothetical respondents. For example, prompting ``imagine surveying 10 random people on {topic X}. What are their responses?'' This method reveals the model's latent opinion distribution while avoiding the limitations of direct questioning. Details for prompt construction are provided in Appendix \ref{app:prompt}.

\paragraph{Seeding cultural responses:}
Having a method for eliciting distributions of values, the next step is to seed culture. One typical way of seeding a specific culture is to explicitly instruct the LLM either by mentioning a specific country (`imagine surveying 10 random Americans') or through describing specific personas \citep[`Imagine surveying a 85 year old Danish woman...';][]{alkhamissiInvestigatingCulturalAlignment2024}. The problem with these approaches is that they stray from actual uses of LLMs. Users are unlikely to explicitly mention their demographic information or nationality \citep{zhengLMSYSchat1MLargescaleRealworld2023}. 

Instead, we use language as a proxy for culture. For instance, a prompt in Danish is assumed to come from a Dane etc. For languages spoken in multiple countries this approach is intentionally ambiguous. The ambiguity allows us to elicit the underlying `default' alignment rather than the general ability to emulate cultures \citep{taoCulturalBiasCultural2024}. To create prompts across languages, we use \texttt{gpt-3.5-turbo} to translate our original English prompts, which we manually verify.

\paragraph{Annotating and aggregating responses:}
Finally, to transform the LLMs' hypothetical survey responses into vectors of stances, we use an LLM-as-a-judge approach \cite{zhengJudgingLLMasajudgeMTbench2023}. Specifically, we use \texttt{gpt-3.5-turbo} \citep{openaiChatGPTOptimizingLanguage2022} to label each substatement as either `pro', `con', or `null' given the context of the topic. We then calculate the proportion of `pro' versus `con' responses as the LLM's value polarity score for the given statement. These scores can then be used to compare against the value polarity scores from the WVS. Specifically, we can calculate the Spearman rank correlation to obtain a measure of similarity between the LLMs responses and the value distributions of a given population. 

\section{Experimental Setup}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/model-self_consistency.png}
    \caption{Self-consistency in responses for LLMs and WVS countries. LLMs have lower self-consistency than resampled WVS responses---shown by the dashed lines---particularly in non-English languages.}
    \label{fig:self-consistency}
\end{figure}
To investigate whether improving the multilingual capabilities of LLMs improves cultural alignment, we set up an experiment using a carefully chosen set of models and languages. We examine two different kinds of model improvements: scaling and commercial product development. These cases provide complementary perspectives on the effects of multilingual capabilities on cultural alignment. Scaling is the most well-studied path to improving LLMs \citep{kaplanScalingLawsNeural2020,ganguliPredictabilitySurpriseLarge2022}. Commercial product development, on the other hand, comprises both scale and innovation in post-training to accommodate different pressures from capabilities, cost, and preferences \citep{kirkBenefitsRisksBounds2024}. For scaling, we use the instruction-tuned Gemma models \cite{gemmaGemma2Improving2024}, while for product development, we use OpenAI's turbo-series models \cite{openaiChatGPTOptimizingLanguage2022,openaiGPT4TechnicalReport2024,openaiGPT4oSystemCard2024}. We provide details of these model families in §\ref{models}. A breakdown of the computational cost can be seen in Appendix \ref{app:experiment-cost}.

\paragraph{Languages:}

For the languages, we compare English with Danish, Dutch, and Portuguese. This set allows us to test multiple assumptions about cultural alignment. English represents a widely-used case: it's a global language with speakers across many countries represented in the WVS/EVS (see Fig. \ref{fig:wvs-correlations}). This diversity allows us to assess whether LLMs align more strongly with US values or those of other English-speaking nations.

Danish and Dutch serve as controlled test cases since they are primarily used in single countries. If cultural alignment stems from pre-training data, models should show strong Danish/Dutch cultural alignment when using these languages, despite their small and low-quality share of training data \citep{kreutzerQualityGlanceAudit2022}. Alternatively, if alignment emerges from English-based post-training processes \citep[which are predominately English-based;][]{blevinsLanguageContaminationHelps2022}, responses in these languages should align more with US values.

Portuguese presents an interesting case since it is an official language in several countries. We investigate whether the LLM responses are more aligned to Portugal or Brazil---two countries that show distinct value patterns in relation to each other and the US (see Fig. \ref{fig:wvs-correlations}). This allows us to test whether an LLM aligns more strongly with one country's values, the aggregate values of all language users, or US values.

For each language/model combination we collect 300 prompt-response pairs to sufficiently power our statistical analysis (see §\ref{sec:methods-rq1}). After filtering out responses that either lacked the required hypothetical survey format or were in another language than the prompt, we obtained between 111--299 valid responses per combination. We calculate the correlation in value polarity scores at three levels: country (e.g., US or Denmark), language (pooling all speakers of a given language), and global (weighted values from all WVS/EVS participants). 

\subsection{Models} \label{models}
This study examines two model families with distinct development approaches: the Gemma family of models \citep{gemmaGemma2Improving2024} and OpenAI's commercial releases in the turbo-range \citep{openaiChatGPTOptimizingLanguage2022,openaiGPT4TechnicalReport2024,openaiGPT4oSystemCard2024}. These two families allow us to investigate two different modes of model improvement: scaling (Gemma) and iterative product development with scaling and post-training improvements (OpenAI). Other preliminary experiments included different versions of LLaMA models \citep{touvronLLaMAOpenEfficient2023} and Mistral models \citep{jiangMistral7B2023}. However, these models either failed to consistently follow instructions or always answered in English regardless of the prompt language.

\paragraph{Gemma:} The Gemma family comprises open-weight models ranging from 2 to 27 billion parameters \citep{gemmaGemma2Improving2024}. The smaller models (2B and 9B) are trained through knowledge distillation from a more capable teacher model, while the 27B model uses standard pre-training objectives. While instruction tuning uses primarily English data, evaluations show strong performance on multilingual tasks \citep{nielsenScandEvalBenchmarkScandinavian2023a}. This restricted training regime, combined with systematic variation in model scale, provides a controlled environment for investigating whether cultural alignment capabilities emerge naturally with increased model size \citep{kaplanScalingLawsNeural2020}.

\paragraph{OpenAI:} OpenAI's `turbo' series represents iterative commercial development driven by customer requirements. The original \texttt{gpt-3.5-turbo}, based on the InstructGPT architecture \citep{ouyangTrainingLanguageModels2022}, established the foundation for conversational AI. Each subsequent release addresses specific market needs: \texttt{gpt-4-turbo} offers reduced computation costs of the more powerful \texttt{gpt-4} model \citep{openaiGPT4TechnicalReport2024} through distillation, while \texttt{gpt-4o} expands multilingual capabilities to serve a global user base \cite{openaiGPT4oSystemCard2024}. Part of the purported multilingual improvements of \texttt{gpt-4o} comes from a more representative tokenizer, though this has been critiqued \cite{yangProblematicTokensTokenizer2024}. This commercially-motivated progression, with each model optimized for different operational priorities, provides a complementary perspective to Gemma's controlled scaling study.

\subsection{RQ1: Multilingual Cultural Alignment} \label{sec:methods-rq1}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/multilingual_benchmarks_alignment-model.png}
    \caption{Language capability (x-axis) vs cultural alignment scores (y-axis) across languages. Stars indicate significance ($p<.05$) in our linear mixed-effects regression of multiple runs (See §\ref{sec:methods-rq1}). OpenAI models (blue) show negative/insignificant relationships outside of English, while the Gemma models (red) show positive relationships throughout ($p<.05$).}
    \label{fig:multilingual-alignment-scatter}
\end{figure}
To statistically assess whether improving the multilingual capabilities of LLMs improves cultural alignment, we construct a linear mixed effects regression model \citep{lukeEvaluatingSignificanceLinear2017} based on the experimental setup described above. Here, we relate the cultural alignment of the model to the improvements in multilingual capabilities controlled by relevant factors:

\begin{equation} \label{eq:rq1-eq}
\begin{split}
\text{CA}_i &\sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i &= \alpha_{j[i]} + \beta_1X_{cons,i} + \sum_{f,l} \beta_{flm}X_{fl,i}X_{m,i}\\
\alpha_j &\sim \mathcal{N}(\mu_\alpha, \sigma^2_\alpha), \quad \text{for } j = 1,\ldots,J
\end{split}
\end{equation}

Here, Eq. \ref{eq:rq1-eq} models the cultural alignment ($\text{CA}_i$) of an LLM, $i$, as a linear mixed-effects model. The mean $\mu_i$ includes a random intercept $\alpha_j$ for each model, the fixed effect of consistency ($\beta_1X_{cons,i}$), and interaction effects ($\sum_{f,l} \beta_{flm}X_{fl,i}X_{m}$) between model family (either `OpenAI' or `Gemma'), language (e.g., Portuguese or Danish), and multilingual capability (average score on selected benchmarks; see Appendix \ref{app:multilingual}). Here, \textit{consistency} is an LLM's stability in value stances across repeated questions, calculated as the Pearson correlation between value polarity scores (defined in §\ref{sec:measure-cultural}) of repeated responses to identical topics (see Appendix \ref{app:wvs-questions-all}). A score of 1.0 indicates perfect consistency, 0.0 indicates randomness. Population-level resampling of human survey responses yields values between 0.66 and 0.84 (see Fig \ref{fig:self-consistency}). Assumption checks for the regression can be seen in Appendix \ref{app:regression-assumptions}.

The above statistical model allows us to analyse the relationship between multilingual capabilities and cultural alignment in model families at the level of individual languages. For instance, we might find an improvement for Gemma models for Danish but not for Dutch or vice versa. 

\subsection{RQ2: US-Centric Bias} \label{methods-rq2}
We analyze model bias by comparing cultural alignment between US and local values, where ``local'' refers to values in country or countries where a given language is natively spoken. We define US-centric bias as an LLM showing higher cultural alignment with US value distributions compared to local ones. To quantify this bias, we use a linear regression model that measures the differential effect of US versus local value alignment:

\begin{equation} \label{eq:rq2-eq}
\begin{aligned}
\operatorname{CA} &= \beta_{0} + \beta_1(\operatorname{US}) \\
&+ \sum_{m \in \mathcal{M}} \sum_{l \in \mathcal{L}} \beta_{ml}(m \times l) \\
&+ \sum_{m \in \mathcal{M}} \sum_{l \in \mathcal{L}} \beta_{ml}^{\operatorname{US}}(\operatorname{US} \times m \times l) + \epsilon
\end{aligned}
\end{equation}

The regression's intercept (i.e., the base case) is a baseline that produces uniformly random value polarity scores. $\mathcal{M}$ is the set of models and $\mathcal{L}$ is the set of languages. \texttt{US} is a boolean feature denoting whether the cultural alignment is to the US (if $1$) or the local values (if $0$). We primarily analyse the coefficients with US ($\beta_{ml}^{US}$) since these provide the \textit{partial} effect of US-centric bias, i.e., how much more/less a given LLM is aligned to US rather than local values. Assumption checks for the regression can be seen in Appendix \ref{app:regression-assumptions}.


\section{Results}


\subsection{Multilingual Value Alignment (RQ1)} \label{sec:results-improvements-rq1}
To understand how improving multicultural capabilities affects cultural alignment, we must first examine the stability of LLMs' cultural values. When evaluating LLMs that lack stable internal values, apparent improvements in cultural alignment may simply reflect reduced response variance rather than genuine advances in cultural alignment \citep{rottgerPoliticalCompassSpinning2024,kahnemanNoiseFlawHuman2021}. We therefore analyse both the self-consistency of LLM responses and how alignment changes with model improvements.

\paragraph{LLMs have low self-consistency:}
We find consistently low self-consistency scores across all models and languages compared to human responses in the WVS data (Fig. \ref{fig:self-consistency}). For WVS data, self-consistency scores (measured as Pearson correlations between bootstrapped samples of responses) range from 0.66 to 0.84 across countries (see Fig \ref{fig:self-consistency}). In contrast, LLM responses to the same questions under different prompt variations show significantly lower consistency, even in English where performance is highest due to English-dominated training data \citep{openaiGPT4TechnicalReport2024,gemmaGemma2Improving2024}. 

This lower self-consistency complicates our cultural alignment analysis \cite{wrightLLMTropesRevealing2024}. Drawing on \citet{kahnemanNoiseFlawHuman2021}'s noise framework, we recognise that inconsistent responses can be as detrimental as bias. To address the noise, we employ larger sample sizes and incorporate consistency measures in our regression analyses.

\paragraph{Multilinguality does not imply cultural alignment:}




The relationship between model improvements and cultural alignment varies substantially across languages and model families (Fig. \ref{fig:rq1-eq-main}). For Gemma, there is a significant positive relationship between multilingual capabilities and cultural alignment for all languages. In contrast, OpenAI only has a significant and positive relationship for English ($\beta_{\text{OpenAI},\text{en}}=0.11,p=0.04$), whereas for Danish and Portuguese the relationship is insignificant ($\beta_{\text{OpenAI},\text{pt}}=0.089, p=0.10$; $\beta_{\text{OpenAI},\text{da}}=0.009, p=0.89$) and for Dutch the effect is significant and negative ($\beta_{\text{OpenAI},\text{nl}}=-0.19,p=0.004$).

A capability threshold may explain the discrepancy between multilingual performance and cultural alignment. Up to a point, multilingual capabilities might improve alignment by enhancing cultural knowledge and instruction-following. Beyond this threshold, other factors likely become more influential \citep{kirkBenefitsRisksBounds2024}. This could explain the stronger relationship in Gemma versus OpenAI's LLMs (see Fig \ref{fig:multilingual-alignment-scatter}). Future work with the Qwen-2.5 family \citep{qwenQwen25TechnicalReport2025}, which range from 500M to 72B parameters, could help validate this hypothesis.

Looking at specific languages, we find nuanced patterns of improvement in cultural representations across model versions. Portuguese shows consistent improvements across both model families, particularly for local (Portuguese and Brazilian) values compared to global values. Danish exhibits positive but non-significant trends, while Dutch shows more variable results---including a notable improvement only with GPT-4o for OpenAI models (See fig. \ref{fig:danish-dutch-alignment}). For a full breakdown of language performance see Appendix \ref{app:language-breakdown}.

Furthermore, the dominant effect of self-consistency ($\beta_{\text{consistency}}=0.62, p \ll 0.001$) compared to multilingual capability suggests that noise remains a major limiting factor in analysing cultural alignment. This aligns with broader findings about the instability of LLM value elicitation \citep{rottgerPoliticalCompassSpinning2024}. Moreover, even the highest observed alignment scores (around 0.4; see Fig \ref{fig:multilingual-alignment-scatter}) indicate substantial room for improvement in how well LLMs match human cultural values and behaviours.

In conclusion, our analysis reveals a complex relationship between model improvements and cultural alignment. While some languages show progressive improvements in cultural alignment from model scaling or iterative commercial development, others show minimal or inconsistent improvements. These findings, combined with the generally low self-consistency of LLM responses, suggest that improving multilingual model capabilities does not automatically lead to better cultural alignment across all languages and contexts.

\subsection{US-centric Bias (RQ2)}
Here, we answer RQ2 by examining US bias across languages. Specifically, we investigate relative alignment between local and US values (Fig. \ref{fig:rq2-us-bias}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/us_bias_coefficients.png}
    \caption{US-centric bias coefficients across LLMs and languages (see Eq. \ref{eq:rq2-eq}). Error bars are standard errors from the regression. Positive values indicate bias.}
    \label{fig:rq2-us-bias}
\end{figure}

Our analysis reveals distinct patterns of US-centric bias across both languages and model families (Fig. \ref{fig:rq2-us-bias}). Languages show different susceptibilities to US bias: five out of six LLMs exhibit US-centric bias in Dutch, all in English, none in Portuguese, and Danish falls in between with three of the six LLMs showing bias.

Within model families, we observe varying patterns of changes in US-centric bias. The Gemma models show a clear trajectory in Dutch, becoming significantly less biased in successive progressions. However, this improvement isn't universal: for Portuguese and English, there are no significant differences in bias between progressions within either family. For Danish, early models (\texttt{gpt-3.5-turbo} and \texttt{gemma-2-2b-it}) exhibit US-centric bias, while later progressions show reduced bias, particularly in the OpenAI family.

In conclusion, we find significant US-centric bias in certain LLM-language combinations, particularly in Dutch and Danish. However, the progression of this bias across model versions varies by language and model family. While some improvements are visible (e.g., Gemma models' reduced bias in Dutch), the overall pattern suggests that advancing model capabilities does not consistently reduce or increase US-centric bias.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/monolingual-spearman-gt_alignment.png}
    \caption{Cultural alignment scores for LLM responses in Danish (top) and Dutch (bottom). Left: comparison with English speakers; middle: Denmark/Netherlands vs US; right: alignment with global values. Error bars show bootstrapped CIs.}
    \label{fig:danish-dutch-alignment}
\end{figure}

\section{Related Work}
Recent work emphasizes the need for systematic auditing of LLMs' cultural alignment, particularly as these models are deployed globally \citep{kirkBenefitsRisksBounds2024, mokanderAuditingLargeLanguage2024,kirkPRISMAlignmentDataset2024}. Prior empirical approaches have primarily taken two paths: using tranformations based on Hofstede's cultural dimensions framework or directly comparing against survey responses. Studies using Hofstede's dimensions \citep{masoudCulturalAlignmentLarge2025, caoAssessingCrossculturalAlignment2023} provide structured cross-cultural comparisons through latent variable analysis. However, these studies assume that LLMs' latent dimensions map directly onto human dimensions, since they use formulas calibrated for humans---an assumption that warrants scrutiny \citep{shanahanTalkingLargeLanguage2024}.

Recent work has explored using LLMs to simulate survey responses for assessing cultural alignment \citep{taoCulturalBiasCultural2024,alkhamissiInvestigatingCulturalAlignment2024}. Prior approaches focused on individual-level responses. In contrast, our method generates distributions of opinions across hypothetical survey participants, enabling direct comparison with population-level statistics. This distribution-based approach offers three key advantages. First, it better captures the inherent variation in cultural values within populations, paving the way for investigating distributional alignment \citep{sorensenPositionRoadmapPluralistic2024}. Second, it enables principled statistical comparison against large-scale survey data like the World Values Survey \citep{evs/wvsJointEVSWVS2022}. Finally, the framework is easy to extend to new languages by automatically translating the prompts. We detail our quantitative framework for measuring alignment with observed population distributions in §\ref{sec:measure-cultural}.

There is also an increasing body of work investigating political biases in LLMs \cite{rottgerPoliticalCompassSpinning2024,rottgerIssueBenchMillionsRealistic2025,hartmannPoliticalIdeologyConversational2023}. Much of this work also relies on human political surveys like the Political Compass Test. However, recent work has called for increased attention to the randomness inherent in LLM decoding at non-zero temperatures can create instability in attributes \cite{rottgerPoliticalCompassSpinning2024,wrightLLMTropesRevealing2024}. We expand on this work by including multilingual perspectives, and constructing prompts on a wide range of prompt variations (see §\ref{sec:measure-cultural}). These prompt variations combined with statistically accounting for self-consistency in our statistical analysis (see §\ref{sec:methods-rq1}) allow us to get a more robust measure of cultural alignment.

The relationship between model capabilities and cultural alignment remains understudied. Unlike general performance metrics that follow predictable scaling laws \citep{kaplanScalingLawsNeural2020}, cultural alignment may not improve systematically with model capabilities. This aligns with research showing micro-level capabilities can be discontinuous with scale \citep{ganguliPredictabilitySurpriseLarge2022}. The challenge is compounded in multilingual settings \citep{hoffmannTrainingComputeoptimalLarge2022a}, where static benchmarks with single correct answers fail to capture how cultural values are distributed across different topics and contexts.

Previous work has focused primarily on English-language performance \citep{taoCulturalBiasCultural2024} or a single LLM \citep{openaiGPT4TechnicalReport2024,caoAssessingCrossculturalAlignment2023}. Our work extends this by examining how cultural alignment varies across both model families and languages, providing insight into how different development approaches---scaling and commercial product development---influence cultural representation capabilities.

There is already positive progress on improving the cross-cultural participation in alignment. Two notable projects are PRISM and AYA \citep{kirkPRISMAlignmentDataset2024,ustunAyaModelInstruction2024}. PRISM is a large dataset of conversational preferences from a diverse participant pool. While the data is predominantly in English, it could be an important resource for better understanding and modelling diverse cultural preferences. The AYA dataset is a massively multilingual instruction fine-tuning dataset. Through improvement non-English instruction tuning, AYA could prove a central component in decreasing noise in LLM responses facilitating cultural alignment.

\section{Conclusion}
Increased multilingual capabilities do not guarantee improved cultural alignment in Large Language Models. Through systematic comparison of two model families---Gemma and OpenAI---we find that the relationship between improvements in multilingual capability and cultural alignment is complex. While some languages show clear improvements in alignment with increased model capabilities (e.g., Portuguese), others exhibit inconsistent patterns, suggesting that cultural alignment does not automatically follow gains in multilingual capabilities. Our distribution-matching methodology using World Values Survey data enabled the detection of these nuanced patterns across languages and cultural contexts.

Our findings highlight that improving cultural alignment requires dedicated effort beyond general capability scaling. Future work should focus on developing techniques that can better handle alignment with distributions of cultural values rather than single points while ensuring meaningful participation from diverse communities in LLM development. As these models continue to reach wider audiences spanning many geographic and cultural regions, achieving robust cultural alignment becomes increasingly crucial for equitable deployment.

\section*{Limitations}
Our systematic investigation of cultural alignment across multiple languages and model families reveals important nuances about the relationship between language, culture, and LLM development. While our methodology enables robust cross-cultural comparison through established survey data (WVS/EVS) and distribution matching, our findings suggest that examining alignment at the national level may be too simplistic. Even in cases where we might expect strong alignment due to tight coupling between language and culture (e.g., Danish and Dutch), we find that other factors like model self-consistency have greater influence on alignment patterns. 

Using the World Value Survey as a ground truth also brings limitations. First, we adopt the challenges and limitations inherent in the data \citep{alemanValueOrientationsWorld2016}. Second, future research using our methodology is limited to the countries surveyed by the WVS/EVS. While their coverage is broad, it is not universal.

Our examination of Gemma as a case study in capability scaling provides initial insights into how cultural alignment evolves with model size. The reproducible evaluation pipeline we develop enables future work to extend this analysis to other model families and architectures, helping build a more comprehensive understanding of how different approaches to model development affect cultural representation \citep{johnsonGhostMachineHas2022, caoAssessingCrossculturalAlignment2023, aroraProbingPretrainedLanguage2023}. As previously mentioned, our methodology requires a fairly high level of instruction following and the ability to respond in the prompted language. Some models, like LLaMA \citep{touvronLLaMAOpenEfficient2023}, tended to reply in English regardless of prompt language in our preliminary experiments. 

These findings highlight that metrics for cultural alignment are inherently contextual \citep{selbstFairnessAbstractionSociotechnical2019}. While our methodology provides systematic evaluation through distribution matching, the complex patterns we observe suggest that cultural alignment may need to be understood at multiple granularities beyond the national level. This is particularly relevant for languages spoken across many cultures---such as English or Portuguese---where the relationship between language and cultural context is more diffuse \citep{sharifianRoutledgeHandbookLanguage2014}.

The challenges we identify in achieving consistent cultural alignment, even for well-resourced languages, suggest that improving alignment may require approaches beyond current training paradigms \citep{hartmannPoliticalIdeologyConversational2023}. Future work might explore explicit personalization approaches or methods that don't rely solely on language as a signal for cultural context while being mindful of the inherent complexity in representing diverse cultural perspectives \citep{birhaneAlgorithmicColonizationAfrica2020,kirkPRISMAlignmentDataset2024}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{references}

\appendix
\section{WVS questions} \label{app:wvs-questions-all}
As mentioned in the main text, we use a subset of questions in the WVS as basis for the prompts. The complete list can be seen in Table \ref{tab:survey-categories}.

\begin{table*}[htbp]
\centering
\begin{tabular}{lp{0.75\textwidth}}
\toprule
\textbf{Category} & \textbf{Topics} \\
\midrule
C & Jobs scarce: Employers should give priority to (nation) people than immigrants; Jobs scarce: Men should have more right to a job than women \\
\midrule
D & Men make better political leaders than women do; Duty towards society to have children; One of main goals in life has been to make my parents proud; University is more important for a boy than for a girl \\
\midrule
E & Confidence: The Press; Political system: Having the army rule; Income equality; Democraticness in own country; Confidence: The Civil Services; Political system: Having a democratic political system; Importance of democracy; Justifiable: Political violence; How often in country's elections: Votes are counted fairly; Democracy: People obey their rulers; Confidence: Major Companies; Confidence: Labour Unions; Confidence: Armed Forces; Confidence: The United Nations \\
\midrule
F & Justifiable: Claiming government benefits to which you are not entitled; Justifiable: Homosexuality; How often do you attend religious services; Justifiable: Abortion; Justifiable: Having casual sex; Justifiable: Divorce; Justifiable: Death penalty; Justifiable: Someone accepting a bribe; Justifiable: Cheating on taxes; Justifiable: Avoiding a fare on public transport \\
\midrule
H & Government has the right: Collect information about anyone living in [COUNTRY] without their knowledge; Government has the right: Monitor all e-mails and any other information exchanged on the Internet \\
\bottomrule
\end{tabular}
\caption{Survey Questions Categorized by Topic}
\label{tab:survey-categories}
\end{table*}

\section{Prompt construction} \label{app:prompt}
Our process for constructing prompts has three stages: 1) template, 2) topic variations, and 3) translations. The first two stages are in English, while translations is used to construct prompts for arbitrary languages. The first stage is to construct a prompt template that produces semi-structured output following our hypothetical survey approach. Our base template can be seen below: 

\begin{quote}
``Imagine the following scenario: Person A is a researcher. Person A interviews 10 representative citizens. For each citizen, they ask for their opinion on {topic}. Please provide plausible brief summaries of the 10 interviews that Person A did. Output format: 1. <summary> 2. <summary> etc...''     
\end{quote}

Note, the template avoids country-specific information. Furthermore, the template provides instructions for structuring the data (an enumerated list), which helps in our post-processing stage (see §\ref{sec:llm-responses}). To get sufficient variations, we prompt an LLM (\texttt{gpt-3.5-turbo}) to construct 10 variations of the above template, keeping the `\texttt{\{topic\}}' element. 

The second stage is to construct variations of the topics seen in Table \ref{tab:survey-categories}. Here, we also get an LLM to construct five variations of each topic. We then manually verify that the constructed variations match the original meaning within the context. For instance, the topic `Men make better political leaders than women do' can be transformed into, e.g., `Men are more competent political leaders than women' and `The political arena is better suited for men than women'---both of which are different but semantically similar.

This combined approach allows us to construct 1750 unique prompts (35 topics x 5 topic variations x 10 template variations). We then subsample 300 prompts to get the required power level. 

Finally, we translate the English prompts to our target languages. This ensures comparability and consistency across languages. As previously mentioned, the translations are done using an LLM (\texttt{gpt-3.5-turbo}.

\section{Language Performance Breakdown} \label{app:language-breakdown}
Here, we expand on the results in §\ref{sec:results-improvements-rq1} also to include Portuguese (Fig. \ref{fig:pt-alignment}) and English (Fig. \ref{fig:english-alignment}). The figures both compare bootstrapped cultural alignment scores (i.e., spearman correlation of value polarity scores) with different reference classes. Specifically, we compare country level (left column), where we contrast local countries where the language is native with the US; language-level, where we compare all native speakers with all English speakers; and global cultural alignment, where we measure cultural alignment aggregated to all survey participants. Note, that we compare the raw cultural alignment scores and thus do not account for the effects of self-consistency - see §\ref{sec:results-improvements-rq1} for a discussion. 

For Portuguese, the two LLM families have diverging progressions (Fig. \ref{fig:pt-alignment}). For Gemma, the LLMs monotonically improve for all populations. For OpenAI, \texttt{gpt-4-turbo} performs much better than \texttt{gpt-3.5-turbo}, whereas \texttt{gpt-4o} is roughly on par with \texttt{gpt-3.5-turbo}. This roughly mirrors the progression in self-consistency, which we observe in Fig. \ref{fig:self-consistency}. 

For English, the primary new comparison is a cross-country analysis (left pane; Fig \ref{fig:english-alignment}). Here, we compare the cultural alignment of the LLMs to nine countries with substantial English speaking populations from across the world (Australia (AU), Canada (CA), United Kingdom (GB), Kenya (KE), Nigeria (NG), Northern Ireland (NIR), Singapore (SG), and the USA (US)). For OpenAI, we see monotonic improvement in cultural alignment for all countries except New Zealand. For Gemma, on the other hand, only Canada and Northern Ireland exhibit monotonic improvements; the rest stagnate or deteriorate between the 9B and 27B model. For the OpenAI family, the most aligned populations ends up being the US, whereas for Gemma this ends up being Canada. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pt-spearman_gt_alignment.png}
    \caption{Cultural alignment scores for Portuguese for individual models. The left column compares Brazil/Portugual with the US, the middle compares Portuguese speakers with English speakers, and the right shows alignment with global values.}
    \label{fig:pt-alignment}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/english_spearman_gt_alignment.png}
    \caption{Cultural alignment scores for English for individual models. The left column compares across English-speaking countries, the middle shows alignment for all English speakers, and the right shows alignment with global values.}
    \label{fig:english-alignment}
\end{figure*}

\section{Regression Assumption Check} \label{app:regression-assumptions}
This section contains both the raw regression coefficient tables and assumption checks for the two regression models. Starting with assumption checks, the main important checks for the LMER model in RQ1 randomly distributed residuals \citep{schielzethRobustnessLinearMixedeffects2020}. Since we have a fairly large dataset, the model is relatively robust to other measures and numeric statistical tests would be too sensitive for practical use \citep{gigerenzerMindlessStatistics2004}. The residuals can be seen in Fig \ref{fig:rq1-robustness}. The plot clearly shows randomly distributed residuals. 

A similar exercise can be done for the linear model in RQ2 (see Fig \ref{fig:rq2-residuals}). Similarly to RQ1, the residuals here are also normal and randomly distributed. Given the above checks, we conclude that the relevant assumptions for our regressions hold. 

For the full regression tables, see the associated repository\footnote{https://github.com/jhrystrom/multicultural-alignment}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rq1-robustness.png}
        \caption{Randomly distributed residuals for RQ1}
        \label{fig:rq1-robustness}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rq2-residuals.png}
        \caption{Residuals for the regression in RQ2}
        \label{fig:rq2-residuals}
    \end{subfigure}
    \caption{Residual plots showing model diagnostics for RQ1 and RQ2}
    \label{fig:residuals-combined}
\end{figure}

\section{Experiment Cost} \label{app:experiment-cost}

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\toprule
Model & Input & Output & Cost \\
\midrule
\multicolumn{4}{l}{\textbf{Responses}} \\
\midrule
gpt-4o & 90,178 & 644,569 & \$3.34 \\
gpt-4-turbo & 103,397 & 643,203 & \$10.17 \\
gpt-3.5-turbo & 103,397 & 517,667 & \$0.41 \\
Total & & & \textbf{\$13.91} \\
\midrule
\multicolumn{4}{l}{\textbf{Extraction (gpt-3.5-turbo)}} \\
\midrule
Total & 1,266,560 & 49,556.00 & \$0.35 \\
\midrule
\multicolumn{4}{l}{\textbf{Translation (gpt-3.5-turbo)}} \\
\midrule
Total & 44,965 & 128,200 & \$0.11 \\
\bottomrule
\end{tabular}
\caption{Token Usage Analysis by Model}
\label{tab:token-analysis}
\end{table}

This section breaks down the costs of running our experiment across the four languages. While the experiments were developed in multiple iterations, we report the total cost of a complete run. The costs can be broken down into two: costs of generating the responses and cost of extracting the value polarity. The breakdown of both for the OpenAI models can be seen in Table \ref{tab:token-analysis}. The cost for the analysis is estimated based on also analysing the Gemma models. Translation cost around \$0.11 using \texttt{gpt-3.5-turbo}. Note, that the costs are based on running using OpenAI's Batch API. 

The Gemma models were run on a single NVIDIA A100 using vLLM \citep{kwonEfficientMemoryManagement2023}. The total runtime for all three models (with setup) was around 2 hours. While we had access to hardware in-house, the cost for running using a cloud provider would be around \$5.5. 

\section{Multilingual Performance} \label{app:multilingual}
To evaluate multilingual performance, we select a set of 3-4 benchmarks in each of the languages we evaluate (English, Danish, Dutch, and Portuguese). We choose the benchmarks based on coverage, popularity, and availability of scores for the LLMs we study. The full results can be seen in Table \ref{tab:model-comparison}.

For English, we choose a subset of the benchmarks from OpenAI's Simple Evals repository\footnote{Link: https://github.com/openai/simple-evals}, which is released under an MIT license. These include MMLU \citep{hendrycksMeasuringMassiveMultitask2020}, DROP \citep{duaDROPReadingComprehension2019a}, GSM-8K \citep{cobbeTrainingVerifiersSolve2021}, and HumanEval \citep{chenEvaluatingLargeLanguage2021} - all of which are widely used to assess LLM performance. 

For Danish and Dutch, we rely on the ScandEval benchmark collection \citep{nielsenScandEvalBenchmarkScandinavian2023a}. All ScandEval benchmarks are released under an MIT license. For Danish, these include DANSK \citep[A named-entity recognition task;][]{enevoldsenDANSKDomainGeneralization2024}, SCALA-DA \citep[linguistic acceptability'[]{nielsenScandEvalBenchmarkScandinavian2023a}, ScandiQA-DA \citep[The Danish subset of MKQA][]{longpreMKQALinguisticallyDiverse2021}, and Danske Talemaader \citep[QA on Danish idioms;][]{hausorensenEvalueringsdatasaet1000Danske2024}. For Dutch, these include CoNLL-NL \citep[named-entity recognition from newspapers;][]{tjongkimsangIntroductionCoNLL2002Shared2002}, SCALA-NL \citep[Linguistic acceptability from the Dutch UDT;][]{nielsenScandEvalBenchmarkScandinavian2023a}, SQuAD-NL \citep[Machine translated version of SQuAD;][]{rajpurkarSQuAD100000Questions2016}, and MMLU-NL \citep[Machine-translated version of MMLU][]{hendrycksMeasuringMassiveMultitask2020}. The benchmarks span linguistic proficiency, task abilities, and knowledge. 

Finally, for Portuguese we use a subset of the Open Portuguese LLM leaderboard \citep{open-pt-llm-leaderboard}. Specifically, we include ASSIN2 RTE \citep[textual entailment;][]{realASSIN2Shared2020}, FaQuAD \citep[Reading comprehension;][]{sayamaFaQuADReadingComprehension2019}, and OAB Exams \citep[Brazilian Bar exam;][]{delfinoPassingBrazilianOAB2017}. FaQuAD is licensed under creative commons, while OAB Exams and ASSIN2 RTE are licensed under MIT.

\begin{table*}[htbp]
\centering
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Benchmark} & \textbf{GPT-3.5} & \textbf{GPT-4} & \textbf{GPT-4o} & \textbf{Gemma-2} & \textbf{Gemma-2} & \textbf{Gemma-2} \\
 & \textbf{Turbo} & \textbf{Turbo} & \textbf{} & \textbf{2B} & \textbf{9B} & \textbf{27B} \\
\hline
\multicolumn{7}{l}{\textit{English Benchmarks}} \\
MMLU (5-shot) & 70.0\% & 86.7\% & \textbf{87.2\%} & 52.2\% & 71.3\% & \textbf{75.2\%} \\
DROP (3-shot) & 64.1\% & \textbf{86.0\%} & 83.7\% & 51.2\% & 69.4\% & \textbf{74.2\%} \\
GSM-8K (5-shot) & 57.1\% & 89.6\% & \textbf{89.9\%} & 24.3\% & 68.6\% & \textbf{74.0\%} \\
HumanEval (0-shot) & 48.1\% & 88.2\% & \textbf{91.0}\% & 20.1\% & 40.2\% & \textbf{51.8}\% \\
\hline
\multicolumn{7}{l}{\textit{Danish Benchmarks}} \\
DANSK & 61.3\% & 68.8\% & \textbf{71.2\%} & 28.2\% & 57.0\% & \textbf{59.9\%} \\
SCALA-DA & 57.6\% & \textbf{72.2\%} & 64.6\% & 20.0\% & 51.3\% & \textbf{58.6\%} \\
ScandiQA-DA & 65.5\% & 60.0\% & \textbf{67.9\%} & 58.5\% & 64.0\% & \textbf{65.6\%} \\
Danske Talemaader & 82.0\% & \textbf{94.5\%} & 93.0\% & 11.5\% & 73.5\% & \textbf{80.2\%} \\
\hline
\multicolumn{7}{l}{\textit{Dutch Benchmarks}} \\
CoNLL-NL & 69.0\% & 74.9\% & \textbf{76.8\%} & 40.6\% & 55.7\% & \textbf{65.2\%} \\
SCALA-NL & 59.0\% & \textbf{77.3\%} & 56.3\% & 19.6\% & 51.0\% & \textbf{59.0\%} \\
SQuAD-NL & 68.3\% & 61.0\% & \textbf{79.4\%} & 68.9\% & 74.9\% & \textbf{75.5\%} \\
MMLU-NL & 42.3\% & \textbf{73.6\%} & 73.1\% & 29.3\% & 54.0\% & \textbf{59.7\%} \\
\hline
\multicolumn{7}{l}{\textit{Portuguese Benchmarks}} \\
OAB Exams & 54.3\% & 73.3\% & \textbf{81.0\%} & 28.8\% & 53.4\% & \textbf{56.4\%} \\
ASSIN2 RTE & 88.2\% & 94.2\% & \textbf{94.4\%} & 36.7\% & 56.0\% & \textbf{56.4\%} \\
FAQUAD NLI & 74.6\% & 82.8\% & \textbf{86.5\%} & 45.5\% & 48.2\% & \textbf{78.7\%} \\
\hline
\end{tabular}
\caption{Performance comparison of large language models across various benchmarks}
\label{tab:model-comparison}
\end{table*}

\end{document}
