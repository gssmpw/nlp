\section{TUMTraffic-Qwen Baseline}
% In this section, we introduce the baseline model of the TUMTraffic-VideoQA dataset. We provide a detailed description of the model architecture and introduce our training recipes. 

%  [x] TODOS, projector and sampler reverse  !

\subsection{Model Architecture}

We introduce TUMTraffic-Qwen, a baseline model for the TUMTraffic-VideoQA dataset that effectively addresses all three tasks within a unified framework. The architecture of the TUMTraffic-VideoQA baseline, as illustrated in Figure \ref{baseline_model}, consists of four core components: visual encoder $f_v$, cross-modality projector $ g_\psi $, token sampler $\mathcal{S}_v$, and large language model $f_\phi$, following \cite{li2024llavaonevisioneasyvisualtask}. \\


% \begin{equation}
% \small
% p(\mathbf{X}_a \mid \mathbf{X}_v, \mathbf{X}_{\text{instruct}}) = \prod_{i=1}^{L} p_\theta(x_i \mid \mathbf{X}_v, \mathbf{X}_{\text{instruct}}, <i, \mathbf{X}_a^{<i})
% \end{equation}


\noindent\textbf{Visual Encoder.} The video is uniformly divided into 100 segments, including the first and last frames, resulting in a total of \( N = 101 \) frames. Given the sampled video input \( \mathbf{X} \in \mathbb{R}^{N \times H \times W \times 3} \), we adopt SigLIP \cite{siglip}, a Transformer-based model pre-trained on large-scale language-image datasets, as the visual encoder. Each frame is processed at a resolution of \( 384 \times 384 \), and the video is encoded into a sequence of visual features \( Z_v = [v_1, \dots, v_N] \), where \( v_i = f_v (\mathbf{X}_i) \in \mathbb{R}^{T \times C} \), containing $T$ spatial tokens of dimension $C$.

 
 
\noindent\textbf{Token Sampling Strategy.} We leverage a simple yet effective frame-level multi-resolution sampling strategy to enhance feature representation. We evaluate four primary sampling strategies: spatial pooling, multi-resolution spatial pooling, multi-resolution token pruning, and multi-resolution temporal pooling. The output $Z_{v}$ from the last layer of SigLIP is denoted as $Z_{\text{high}}$, which is reduced to $T'$ tokens after down-sampling. We define the set of high-resolution frames as keyframes, denoted by $\mathcal{K}(\cdot)$. Additionally, a learnable token is appended to the end of each frame to explicitly differentiate them. The number of tokens used in various strategies is presented in Table \ref{tab:tokennum}.

% To explicitly model inter-frame relations, the baseline model excludes the temporal aggregation module.
% , leaving this for future exploration
% the spatial down-sampling factor as $P$,
% for reducing visual token numbers 
\noindent\textbullet\  \textbf{Spatial Pooling}: This method applies spatial pooling to each feature map $Z_{\text{high}}$, resulting in a down-sampled representation $Z_{\text{low}} = f_{\text{pool}}(Z_{\text{high}})$ with $N \times T'$ tokens, as shown in Eq. \ref{formula:spatial_pooling}. We use the notation $[ \cdot ]_{n}^{N}$ to represent the operation of sequentially concatenating the processed feature maps.



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{figure/TrafficQA-baseline2.jpg}
    \caption{Overview of the TUMTraffic-Qwen baseline model. Yellow and orange colors represent the combination of multi-resolution visual tokens from different visual strategies, while blue indicates textual tokens.}
    \label{baseline_model}
\end{figure}

{\small
\begin{equation}
S_v(Z_v) =  [ Z_{\text{low}}^{n}, Z_{\text{learn}} ]_{n=1}^N 
\label{formula:spatial_pooling}
\end{equation}}


\noindent\textbullet\ \textbf{MultiRes Spatial Pooling}: Compared to the naive spatial pooling, this strategy selects the first frame as the keyframe $\mathcal{K}$ = (1), and is retained at its original resolution $Z_{\text{high}}^1$. It is formulated in Eq. \ref{formula:mluti-spatial_pooling}. 



{\small
\begin{equation}
S_v(Z_v) = [ Z_{\text{high}}^1, Z_{\text{learn}}, [ Z_{\text{low}}^{n}, Z_{\text{learn}} ]_{n=2}^N \big]
\label{formula:mluti-spatial_pooling}
\end{equation}}


\noindent\textbullet\  \textbf{MultiRes Token Pruning}: Similar to MultiRes Spatial Pooling, the first frame is designated as the keyframe. Token-wise cosine similarity is then computed between the keyframe and each subsequent frame, while visual tokens with lowest similarity are selectively retained based on predefined ratio $r$, formulated as $Z_{\text{pruned}} = f_{\text{prune}}^{r}(Z_{\text{high}} )$, shown in Eq. \ref{formula:mluti-spatial_spar}. To ensure visual token efficiency comparable to spatial pooling, $r$ is set to 0.25. A similar strategy is also applied in autonomous driving scenarios \cite{ma2024videotokensparsificationefficient}.


 \input{table/token_num}

\input{table/table1_1}

{\small
\begin{equation}
S_v(Z_v) = [ Z_{\text{high}}^1, Z_{\text{learn}}, [ Z_{\text{pruned}}^{n}, Z_{\text{learn}} ]_{n=2}^N ]
\label{formula:mluti-spatial_spar}
\end{equation}}



\noindent\textbullet\  \textbf{MultiRes Temporal Pooling}: In this strategy, the keyframe set is adaptively queried by input questions $\mathcal{K}(\cdot)=\mathcal{Q}(X_q)$. Based on the temporal regions of interest derived from the question, $K$ keyframes are selected, which are preserved with high-resolution representations $Z_{\text{high}}^{n}$. Meanwhile, the remaining frames undergo spatial pooling, resulting in $Z_{\text{low}}^{n}$, as expressed in Eq. \ref{formula:mluti-temporal}. Typically, $K \leq 2$, and for general questions without specific temporal focus, the first frame is set as the default keyframe.



% [x] formula consistent & outside.

{\small
\begin{equation}
\begin{split}
S_v(Z_v) = [ Z_{v}^{n}, Z_{\text{learn}} ]_{n=1}^N  \\
\text{where } Z_v^{n} =
\begin{cases}
Z_{\text{high}}^{n}, & \text{if } n \in \mathcal{K}(\cdot), \\
Z_{\text{low}}^{n}, & \text{if } n \notin \mathcal{K}(\cdot)
\end{cases}
\end{split}
\label{formula:mluti-temporal}
\end{equation}}




% \textbf{Large Language Models.} We use Qwen2 as the pre-trained LLMs in the TUMTraffic-VideoQA baseline with its strong in-context learning ability and proven performances in cross-modality visual question answering. Specifically, we adopt the lightweight Qwen2-0.5B-Instruction model \cite{qwen2} with hidden size of 896 and 32k context length.  


\noindent\textbf{Large Language Model.} We adopt Qwen-2 \cite{qwen2} as the pre-trained LLM in our TUMTraffic-Qwen baseline. Qwen-2 demonstrates strong capabilities in in-context learning and instruction following, supporting context lengths of up to 32k tokens. This allows for the processing of complex and long-form inputs effectively. We utilize two versions of Qwen-2, namely 0.5B and 7B, to establish baselines of different scales. The answer generation process in our TUMTraffic-Qwen baseline model is formulated as:

{\small
\begin{equation}
p(X_a \mid S_v(Z_v), X_q) = \prod_{t=1}^{\mathcal{T}} P_{\phi,\psi}\big(x_t \mid x_{1:t-1}, S_v(Z_v), X_q)
\end{equation}}

% The 0.5B model features a 24-layer Transformer with a hidden size of 896, offering a lightweight yet effective solution. 7B model, with a 28-layer Transformer and a hidden size of 3584, is designed for enhanced reasoning and representation capabilities. 
% We adopt the instruction-tuned versions of Qwen-2 as the pre-trained LLM for our baseline.



\subsection{Baseline Training} Our baseline model undergoes a two-stage training process consisting of video-language alignment and visual instruction fine-tuning, to enhance its understanding of traffic scenarios and reasoning capabilities for long videos. Both stages are trained with 4 NVIDIA A100 GPUs. 

\noindent\textbf{Video-Language Alignment.} This step aims to align video representations with language embeddings, ensuring that the LLM can effectively interpret the visual features. We freeze both the visual encoder and the LLM, and train only the projector layer. To facilitate the training, we initialize the parameters of the 2-layer MLP from the LLaVA-OneVision model, which has been pre-aligned with large-scale cross-modality datasets, including 3.2M single-image and 1.6M OneVision image-caption pairs. In this stage, we further train the projector on raw TUMTraffic-VideoQA data, with open-ended captioning pairs without transforming to the multiple-choice QA for 1 epoch. 
% format

\noindent\textbf{Visual Instruction Fine-Tuning.} Building upon the robust representations established during the alignment stage, we further fine-tune our baseline model on the training set of TUMTraffic-VideoQA. The multi-choice QA pairs are reformatted into the instruction-following format to prompt the model to generate the corresponding answers. During this stage, we freeze the vision encoder and projector layers and finetune the Qwen-2 model with full-parameter fine-tuning to adapt its reasoning and contextual understanding ability. The model is fine-tuned for 1 epoch.




