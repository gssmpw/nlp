\appendix
% \newpage
% \clearpage
% \renewcommand{\baselinestretch}{1.0}  
% \normalsize
% \afterpage{\onecolumn}
% \twocolumn[\onecolumn]
\onecolumn 
\begin{center}
    \textbf{\LARGE TUMTraffic-VideoQA: Multi-Modal Benchmark for Spatial-Temporal Video Understanding in Traffic Scene} 
    
    \bigskip  
    \large Supplementary Material
\end{center}
    \bigskip  

\section{TUMTraffic-VideoQA Dataset}

% \subsection{Video Collection}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figure/TrafficQA-data_collection.pdf}
%     \caption{Data collection.}
%     \label{data_collection}
% \end{figure}


\subsection{Dataset Statistics}
\label{app:dataset_statistics}


\begin{figure*}[bth]
    \centering
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/time_of_day_vs_month.png}
        \caption{Temporal Distribution of Video Weather Conditions Over the Years.}        \label{fig:video_vis1}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/month_and_weather.png}
        \caption{Weather-Based Distribution of Videos.}
        \label{fig:video_vis2}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/hod_vs_camera.png}
        \caption{Scene Distribution Across Different Perspectives.}
        \label{fig:video_vis3}
    \end{subfigure}
    % \begin{subfigure}[b]{0.24\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figure/month_and_time_of_day.png}
    %     \caption{}
    %     \label{fig:video_vis4}
    % \end{subfigure}
    \caption{Dataset distribution of video recordings by time, weather conditions, and perspectives.}
    \label{fig:video_statistics}
\end{figure*}

% As detailed in Section \ref{sec:dataset_creation},
% \twocolumn 
% \afterpage{\twocolumn}  
% \noindent

The video selection process is meticulously designed to ensure comprehensive coverage of diverse daytime periods, weather conditions, road types, etc. The distribution of the video statistics in the TUMTraffic-VideoQA dataset is illustrated in Figure \ref{fig:video_statistics}. Figure \ref{fig:video_vis1} provides an overview of the distribution of videos by hour of the day and month, with weather conditions represented through color coding. The majority of traffic footage was captured between 5:00 AM and 8:00 PM, with fewer recordings available during hours with limited natural light. Figure \ref{fig:video_vis2} illustrates the distribution of videos by weather conditions for each month. The dataset predominantly includes videos recorded between February and May, a period characterized by a wide variety of weather scenarios, thereby enhancing the dataset's representativeness. Figure \ref{fig:video_vis3} depicts the distribution of video recordings by hour of the day for each camera type and camera. The three camera categories—surveillance cameras positioned on highways, intersections, and country roads—are represented proportionately, ensuring video coverage across these categories from dawn to nighttime. 

% Figure \ref{fig:video_vis4} presents the distribution of video recordings by time of day across different months. Daytime recordings are captured in the majority of the months, while videos captured at dawn, dusk, and nighttime are available predominantly for specific months.


\begin{figure*}[bh]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[height=2.6cm]{figure/QA2.png}
        \caption{Word Cloud Visualization of Multi-Choice QA.}
        \label{fig:qa_vis1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[height=3.5cm]{figure/words_sunburst.png}
        \caption{Burst Figure of Questions in Multi-Choice QA.}
        \label{fig:qa_vis3}
    \end{subfigure}
    % \begin{subfigure}[b]{0.25\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figure/QA2.png}
    %     \caption{}
    %     \label{fig:qa_vis2}
    % \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[height=2.6cm]{figure/number_of_words_in_a.png}
        \caption{Length Distribution of Different Question Types.}
        \label{fig:qa_vis4}
    \end{subfigure}
    
    \caption{Distributions of video recordings across time, weather conditions, and camera types in the dataset.}
    \label{fig:qa_statistics}
\end{figure*}

% and \ref{fig:qa_vis2} 
In addition to video statistics, Figure \ref{fig:qa_statistics} illustrates the distribution and characteristics of annotations in the TUMTraffic-VideoQA dataset. Figures \ref{fig:qa_vis1} depict word clouds for answers across all three tasks, highlighting common terms and their frequencies. Figure \ref{fig:qa_vis3} presents a sunburst chart that visualizes the distribution of question formats, revealing that most questions begin with "How," "What," and "Can". Figure \ref{fig:qa_vis4} shows the distribution of answer lengths, indicating that the majority of answers consist of fewer than 10 words, with only a small number exceeding 19 words. 

\subsection{Spatial Question Curation}
% TODOs: [] figures, definitions, options, refer to template at last.

Comprehending spatial relationships in 3D space is a critical challenge in traffic scene analysis. In our semi-automatic annotation pipeline, we calculate spatial locations by projecting 2D coordinates into 3D space under the planar assumption, leveraging historical camera intrinsic and extrinsic matrices. Specifically, from a third-party roadside perspective, we formulate spatial reasoning questions by treating each object as an ego-centric reference and formulate the questions that reveal its 3D positional relationships with surrounding traffic participants.

% This approach enables the construction of 3D spatial relationships between objects, thereby facilitating the generation of structured annotations. 
\begin{figure}[bht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \begin{equation}
            \text{relative position} = 
            \begin{cases}
                \text{front} & \text{if } -15^{\circ} < \theta \leq 15^{\circ} \\
                \text{front left} & \text{if } 15^{\circ} < \theta \leq 75^{\circ} \\
                \text{left} & \text{if } 75^{\circ} < \theta \leq 105^{\circ} \\
                \text{front right} & \text{if } -75^{\circ} < \theta \leq -15^{\circ} \\
                \text{right} & \text{if } -105^{\circ} < \theta \leq -75^{\circ} \\
                \text{back left} & \text{if } 105^{\circ} < \theta \leq 165^{\circ} \\
                \text{back right} & \text{if } -165^{\circ} < \theta \leq -105^{\circ} \\
                \text{back} & \text{else.}
            \end{cases}
        \label{eq:relative_position}
        \end{equation}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/relative_positions.png}
        \caption{Illustration of the eight spatial regions used to categorize the relative positions of objects in traffic scenes. In this example, the orange car is located to the front right of the black car.}
        \label{fig:relative_positions}
    \end{minipage}
\end{figure}

We focus on objects that remain in motion throughout the video. The motion direction of each object is computed based on the difference between its 3D coordinates in consecutive frames. To determine the relative position between two objects, we measure the angle $\theta$ between the motion direction of the moving object and the vector connecting it to another object. Subsequently, the relative position of the second object with respect to the moving object is classified according to the angular criteria defined in Eq. \ref{eq:relative_position}. We then divide the spatial relationship into eight distinct regions: front, front left, left, front right, right, back left, back right, and back. Figure~\ref{fig:relative_positions} illustrates the angular division used to classify the relative position of objects in our TUMTraffic-VideoQA dataset. 



\section{Benchmark Analysis}

% [x] spatial relation design introduction + multi-choice design

\subsection{Impact of Frame Number on Model Performance}

\input{table/table4}
% In our baseline model, we use 101 frames as the visual input.
To assess the extent to which the baseline model learns from visual tokens and how much it attempts to fabricate answers, we conduct a series of ablation studies. We investigate the impact of the number of frames on TUMTraffic-VideoQA performance, as detailed in Table \ref{tab:num_frames}. Additionally, we include an extreme case where no visual information is provided to the model, and the train baseline model was prompted to answer questions directly.  

% through a set of experiments
The experimental results reveal intriguing phenomena in both the 0.5B and 7B models. First, when no visual input is provided, and the model relies solely on the question to generate answers, the baseline model could still reach relatively high performance across all three tasks. This demonstrates the model’s inherent reasoning capabilities are probably derived from the question alone and highlights that, in domain-specific datasets such as traffic scenarios, the model appears to learn and exploit underlying text-based patterns and biases present in the data, which may contribute to its ability to fabricate seemingly accurate responses without actual visual grounding.

Besides, introducing visual input is found to be crucial for correctly solving TUMTraffic-VideoQA tasks. Across all three tasks, the results consistently show that increasing the number of input frames will improve model performance. Notably, the improvements are most pronounced when moving from no visual input to 1 frame and from 1 frame to 11 frames. However, the performance gains became less significant when increasing the input from 11 frames to 101 frames. This diminishing improvement may be attributed to the inherent difficulty of LLMs in effectively extracting visual context from a large number of tokens. For the 0.5B baseline model, the performance with 11 frames is nearly equivalent to that with 101 frames, reflecting its relatively limited in-context learning capabilities. Therefore, effectively representing video data and addressing the hallucination problem of VLMs in such domain-specific scenarios are critical directions for future research.

Furthermore, the increase in the number of frames has varying impacts on different task types, with substantial differences observed. This variation also indirectly reflects how much the model learns from visual input and how much it affects the reasoning process. For Multi-Choice QA tasks, the gains for positioning and motion categories are the smallest, ranging from only 1.82\% to 3.24\%. It indicates that the model still struggles to extract answers from visual information effectively based on the current model architecture. In contrast, for counting, class, and existence tasks, the performance improvements exceed 10\%, which suggests that VLMs effectively extract features and answer questions in these cases. 

% For object captioning tasks, the SPICE metric shows the most significant growth, highlighting its reliability for evaluating the model’s performance in generating descriptive outputs.


% \subsection{Impact of Large Language Models}

% The choice of LLMs has a significant impact on the performance of the TUMTraffic-VideoQA baseline. The experiments comparing Qwen2 variants (0.5B and 7B) reveal that the larger 7B model consistently outperforms its smaller counterpart across all metrics. For example, the 7B model achieves higher overall accuracy, better linguistic metrics, and lower spatial-temporal errors, reflecting its enhanced capacity for in-context learning and long-sequence reasoning. These findings suggest that scaling LLMs further could yield additional benefits, particularly for tasks involving complex queries and extended temporal contexts. However, the computational cost of larger LLMs remains a challenge, underscoring the need for efficient strategies to balance performance and scalability.

% \newpage
\subsection{Visualization of Multi-Choice QA Results}
Figure \ref{radar:1} presents a radar chart depicting the performance of open-source models on the Multi-Choice QA task. The results indicate substantial variability in zero-shot performance across different question types, with each model exhibiting strengths in specific categories. Notably, tasks requiring positioning skills, such as 3D scene understanding, pose significant challenges for all models, suggesting that this question type demands advanced spatial reasoning capabilities, which remain a limitation for current LLMs.

Figure \ref{radar:2} illustrates the performance of TUMTraffic-VideoQA fine-tuned baseline models. Fine-tuning leads to a notable improvement in overall performance, particularly for the 7B parameter model, which consistently outperforms the lightweight 0.5B model across multiple dimensions. However, the performance gap is not overwhelmingly large, indicating that lightweight models retain considerable practical value and can effectively handle the majority of tasks.
% For existence-related questions, whether in hard or easy settings, the performance difference between the two models is minimal. This suggests that for straightforward object recognition tasks, increasing model size does not significantly enhance in-context learning capabilities given fixed training data. 

\begin{figure}[bh!]
    \centering
        \begin{subfigure}{0.42\linewidth}
        \centering
        % \includegraphics[width=\textwidth]{figure/results1.jpg}
        \includegraphics[width=\textwidth]{figure/res_vis_new2.jpg}
        \caption{Performance radar chart of the open-source models on the TUMTraffic-VideoQA Multi-Choice QA task.}
        \label{radar:1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.42\columnwidth}
        \centering
        % \includegraphics[width=\linewidth]{figure/results2.jpg}
        \includegraphics[width=\linewidth]{figure/res_vis_new1.jpg}
        \caption{Performance radar chart of the TUMTraffic-QWen baseline on the TUMTraffic-VideoQA Multi-Choice QA task.}
        \label{radar:2}
    \end{subfigure}
    \caption{Results visualization for the open-source models and TUMTraffic-QWen baseline models on the Multi-Choice QA.}

\end{figure}


% \newpage
\subsection{Example of MultiRes Token Pruning}
We present several examples of multi-resolution similarity-based token pruning techniques applied to video data from our dataset. As shown in Figure \ref{token_pruning}, while this approach maintains high resolution to a certain extent, its lack of semantic-aware selection capabilities may result in the loss of task-critical information in certain scenarios. Specifically, it mainly preserves visual tokens for moving vehicles and dynamic objects, such as swaying trees, while pruning stationary vehicles as background information due to their lack of motion. It shows its effectiveness in separating dynamic objects from static backgrounds but also highlights the need for improvement in handling the rest of the important traffic participants.

\begin{figure*}[bh!]
    \centering

    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-vis_dynamic_sampling-1.jpg}
    % \caption{Visualization 1.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-vis_dynamic_sampling-2.jpg}
    % \caption{Visualization 2.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-vis_dynamic_sampling-3.jpg}
    % \caption{Visualization 3.}
    \end{subfigure}

    \caption{Illustration of cosine similarity-based token pruning, with dark-colored patches representing discarded tokens and preserved ones highlighted. We demonstrate the three samples on highways, country roads, and intersections separately.}
    \label{token_pruning}
\end{figure*}


\subsection{System Prompt}
We craft a dedicated system prompt for our experiments with the TUMTraffic-VideoQA dataset. Figure \ref{fig:system_prompt} presents the prompt used in the experiments. The prompt is adopted across both open-source models and fine-tuned TUMTraffic-Qwen baseline to ensure fair and consistent evaluation across different models.
\input{question_template/system_message}


\newpage
\subsection{Qualitative Evaluations of Spatio-Temporal Object Grounding}


Figures \ref{sp_object_1} through \ref{sp_object_5} illustrate several qualitative examples of spatio-temporal object grounding, highlighting the challenges and limitations of the task. Figure \ref{sp_object_1} presents an example where the referred object is a fire truck parked at the roadside, visible throughout the entire video from start to finish. The baseline 0.5B model demonstrates satisfactory temporal localization but exhibits some inaccuracies in spatial localization. In contrast, the baseline 7B model achieves more accurate spatial localization but only identifies the temporal range from 0.2s to 2.95s. 


% Figure \ref{sp_object_5} demonstrates the temporal grounding of a motorcycle at the intersection. Compared to cars, this task is notably more difficult. Both the 0.5B and 7B models fail to effectively localize the motorcycle in both spatial and temporal dimensions, underscoring the challenge of grounding smaller and less visually distinct objects.

\begin{figure*}[hb!]
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-temporal_grounding_vis_2.jpg}
    \caption{Spatio-Temporal Object Grounding: A fire truck parked at the roadside.}
    \label{sp_object_1}
\end{figure*}

\vspace{4em}

Figure \ref{sp_object_2} depicts a white car moving along a country road, appearing in the video from 10.10s until the end. The baseline model predictions indicate that the 0.5B model provides a relatively accurate estimate of the initial position, whereas the 7B model exhibits a greater deviation in its ending location. 


\begin{figure*}[hb!]
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-temporal_grounding_vis_1.jpg}
    \caption{Spatio-Temporal Object Grounding: A white car moving along a country road.}
    \label{sp_object_2}
\end{figure*}

\newpage
Figure \ref{sp_object_4} presents the grounding result of a white sedan in a nighttime scene. Due to the object's considerable distance in the reference frame, it appears quite small and makes feature extraction more challenging. Additionally, due to its extended temporal span, the model struggles with cross-frame object association. As a result, both the 0.5B and 7B models fail to accurately capture its end position, instead predicting minimal spatial displacement. This highlights the difficulty of grounding objects with large temporal windows, where precise localization over time remains a significant challenge.


\begin{figure*}[hb!]
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-temporal_grounding_vis_3.jpg}
    \caption{Spatio-Temporal Object Grounding: A white sedan in a nighttime scene.}
    \label{sp_object_4}
\end{figure*}

\vspace{5em}

In Figure \ref{sp_object_5}, we show an example of temporal grounding for a motorcycle at the intersection. Compared to big cars, the grounding of vulnerable traffic participants is much more challenging. Both the 0.5B and 7B baseline models fail to effectively localize the motorcycle in either the temporal or spatial domain, highlighting the difficulty of the task for smaller and less distinct objects.


\begin{figure*}[hb!]
    \centering
    \includegraphics[width=\textwidth]{figure/TrafficQA-temporal_grounding_vis_5.jpg}
    \caption{Spatio-Temporal Object Grounding: A motorcycle moving through an intersection.}
    \label{sp_object_5}
\end{figure*}

% \begin{figure*}[h!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/TrafficQA-temporal_grounding_vis_4.jpg}
%     \caption{Spatio-Temporal Object Grounding Example 3.}
%     \label{sp_object_3}
% \end{figure*}

% Figure \ref{sp_object_3} illustrates the Spatio-temporal grounding of a fast-moving red sedan in a distant lane on the highway. Due to its extended temporal span, the model struggles with cross-frame object association, making it challenging to track the object’s movement consistently across frames. As a result, both the 0.5B and 7B baseline models fail to accurately capture the start and ending locations, with predictions biased toward assuming minimal Spatio displacement. This highlights the difficulty of this task, particularly for objects with large temporal windows on the video. 

\clearpage
\newpage
\subsection{Qualitative Evaluations of Referred Object Captioning}
In this section, we present several examples from the referred object captioning task. The left side of each image shows the object to be described, while the right side includes the task description, the corresponding ground truth, and the responses generated by the 0.5B and 7B TUMTraffic-Qwen baseline models. We prompt the model with the question using a list of two tuples that indicate its Spatio-temporal position at two specified timestamps. The experimental results, evaluated using multiple NLG metrics, reveal that the 7B model achieves higher accuracy in describing the appearance details of target objects. However, despite its smaller parameter size, the 0.5B baseline model is also capable of generating satisfactory descriptions, demonstrating its potential practicality in resource-constrained scenarios.

Figure \ref{referred_object_1} presents a sample to describe an occluded white van. Both the 0.5B and 7B models from the TUMTraffic-Qwen baseline accurately identify the vehicle as a boxy-shaped white van. However, the 0.5B model introduces extra hallucinations and incorrectly describes the van as having a Volkswagen logo, which is not present in the image. Both the 0.5B and 7B models achieve relatively high metric scores, with the 7B model performing better, particularly in BLEU-4 and SPICE.


\begin{figure*}[hb!]
    \centering
    \includegraphics[width=0.99\textwidth]{figure/TrafficQA-referred_obj_vis_1.jpg}
    \caption{Referred Object Captioning Example: A partially occluded white van with a boxy shape.}
    \label{referred_object_1}
\end{figure*}

Figure \ref{referred_object_2} illustrates a scenario to describe a dark-colored sedan based on two perspectives captured at different timestamps in the video. The ground truth description from ChatGPT-4o accurately specifies the color as dark purple, while the TUMTraffic-Qwen baseline, with both the 0.5B and 7B version, classify the vehicle color as black, a visually similar designation. Regarding vehicle type, the 0.5B model identifies it as a hatchback, whereas the 7B model recognizes it as an SUV. Moreover, the 7B model detects distinctive alloy wheels, aligning with the description in ground truth. The quantitative evaluation across four metrics indicates that the 7B model slightly outperforms the 0.5B model, with the most significant improvement observed in the SPICE metric.

Figure \ref{referred_object_3} presents a case where the question refers to a bus with a distinctive green roof. In the TUMTraffic-Qwen baseline, the 0.5B model incorrectly describes it as a white van with a boxy shape, whereas the 7B model accurately identifies it as a bus with green and white colors and provides a corresponding detailed description. It shows that the 7B model achieves better performance than the 0.5B model for this sample. However, in terms of NLG metrics, both descriptions receive the same ROUGE-L score, which is not a reasonable reflection of their accuracy differences. Among the four reported metrics, SPICE captures the quality of descriptions more effectively. To address such limitations, some studies have introduced LLMs-based evaluation metrics for assessing model performance, which will be explored as part of our future work.
\newpage
\begin{figure*}[!hb]
    \centering
    \includegraphics[width=0.99\textwidth]{figure/TrafficQA-referred_obj_vis_2.jpg}
    \caption{Referred Object Captioning Example: A dark-purple-colored sedan from two perspectives.}
    \label{referred_object_2}
\end{figure*}

% \newpage
\begin{figure*}[hb!]
    \centering
    \includegraphics[width=0.99\textwidth]{figure/TrafficQA-referred_obj_vis_3.jpg}
    \caption{Referred Object Captioning Example: A bus with a distinctive green roof.}
    \label{referred_object_3}
\end{figure*}



\clearpage
\newpage
\section{Dataset Examples}


\subsection{Sample Videos}
The TUMTraffic-VideoQA dataset encompasses a diverse and highly engaging collection of traffic scenarios, capturing a wide range of complex real-world traffic situations and weather conditions. These scenarios cover various traffic dynamics and environmental factors, making the dataset suitable for evaluating models across different conditions. We showcase several representative scene types to illustrate the diversity and characteristics of our dataset more intuitively. 


\begin{figure*}[hbt!]
    \centering

    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos1.pdf}
    \caption{Accident}
    \label{example_accident}
    \end{subfigure}
    
    % \FloatBarrier      

    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos2.pdf}
    \caption{Rescue}
    \label{example_rescure}

    \end{subfigure}
    
    % \FloatBarrier      
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos3.pdf}
    \caption{Traffic Jam}
    \label{example_jam}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos4.pdf}
    \caption{Fog}
    \label{example_fog}
    \end{subfigure}

    % \FloatBarrier      

    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos5.pdf}
    \caption{Snow}
    \label{example_snow}
    \end{subfigure}
    
    \centering
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos6.pdf}
    \caption{Rain}
    \label{example_rain}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/videos7.pdf}
    \caption{Dawn \& Dusk}
    \label{example_dawn_dusk}
    \end{subfigure}
    % \label{fig:video_examples}
\end{figure*}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos1.pdf}
%     \caption{Examples of accident scenarios.}
%     \label{example_accident}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos2.pdf}
%     \caption{Examples of rescue scenarios.}
%     \label{example_rescure}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos3.pdf}
%     \caption{Examples of traffic jam scenarios.}
%     \label{example_jam}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos4.pdf}
%     \caption{Examples of foggy weather conditions.}
%     \label{example_fog}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos5.pdf}
%     \caption{Examples of snowy weather conditions.}
%     \label{example_snow}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos6.pdf}
%     \caption{Examples of rainy weather conditions.}
%     \label{example_rain}
% \end{figure}

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\textwidth]{figure/videos7.pdf}
%     \caption{Examples of dawn and dusk conditions.}
%     \label{example_dawn_dusk}
% \end{figure}

The depicted scenarios include but are not limited to: Traffic Accidents \ref{example_accident}, demonstrating various types and severities of collisions; Rescue Operations \ref{example_rescure}, capturing emergency vehicle actions under special circumstances; Traffic Jams \ref{example_jam}, reflecting congestion during peak hours or unexpected events; and scenes under diverse weather conditions, such as Fog \ref{example_fog}, Snow \ref{example_snow}, and Rain \ref{example_rain}, showcasing the dataset’s adaptability to complex environments. Additionally, the dataset includes scenarios with unique lighting conditions, such as Dawn and Dusk \ref{example_dawn_dusk}, simulating traffic dynamics in low-light settings.

% \clearpage
% \newpage

\subsection{Question Templates}
In this section, we provide some representative examples of question templates for each task. 
Figures \ref{fig:positioning_qa_templates} through \ref{fig:existence_qa_templates} show templates for the five categories in the Multi-Choice QA task. Figure \ref{fig:sp_te_obj_gr_qa_templates} provides templates for the Spatio-Temporal Object Grounding task and Figure \ref{fig:ref_obj_gr_qa_templates} presents templates for the Referred Object Captioning task. 

\input{question_template/multi-QA}
\input{question_template/reffered_captioning}

