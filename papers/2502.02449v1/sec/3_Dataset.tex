\section{TUMTraffic-VideoQA Dataset}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figure/pipeline_new.jpg}
    \caption{The workflow of the semi-automatic annotation pipeline for TUMTraffic-VideoQA generation, integrating external database, leveraging various off-the-shelf tools and LLMs, with human quality checks ensuring accuracy. }
    \label{qa_data_generation}

\end{figure*}


\subsection{Dataset Creation}
\label{sec:dataset_creation}


Our data generation process comprises three primary stages: Video Selection, Metadata Curation, and QA Pair Generation, as shown in Figure \ref{qa_data_generation}. To ensure high-quality, diverse, and balanced annotations, we introduce a semi-automatic labeling pipeline that combines automated processes with human verification for enhanced accuracy and consistency.\\

% The videos are selected according to a set of carefully defined criteria. An automated approach selects a subset of videos from each traffic camera to capture variations in weather conditions, time of day, road types, and their combinations. 

\noindent\textbf{Video Selection.} The video data in TUMTraffic-VideoQA are collected from multiple roadside infrastructure points over a data collection period spanning more than two years. The dataset encompasses diverse perspectives, covering various urban, suburban, and highway scenarios. It includes a broad range of video content, capturing various distinct traffic scenarios, such as traffic accidents, rescue operations, congestion, roadblocks, and uncommon vehicle occurrences. Furthermore, the dataset encompasses a variety of environmental conditions, including sunny, rainy, cloudy, snowy, and foggy weather, along with technical challenges scenarios such as obstructed camera lenses and vibrations. The video segments are carefully selected to include a diverse range of traffic participants—including vehicles, pedestrians, and obstacles—capturing the complexity and dynamic characteristics of real-world traffic environments. \\




\noindent\textbf{Metadata Curation.} The video metadata includes environmental conditions, object positions, trajectories, appearances, traffic flows, and more, serving as the basis for generating high-quality annotations. External data sources include historical weather records, traffic accident reports, and camera calibration details. To ensure precise time-specific weather and traffic information, we align video timestamps with these records using GPT-4o and Text-embedding-3-large \cite{openai2024gpt4technicalreport}. For visual metadata, we utilize state-of-the-art object detectors and trackers \cite{wang2024yolov10realtimeendtoendobject, rtdetr}, along with open-vocabulary detectors \cite{UNINEXT, wu2023GLEE}, to generate bounding box and trajectory data. We then transform 2D information into camera-based pseudo-3D locations using camera calibration matrices, facilitating the generation of questions related to object motion and relative spatial positioning. To capture object appearance details, we utilize large VLMs \cite{openai2024gpt4technicalreport, liu2024llavanext}, which automatically generate textual descriptions for cropped object bounding boxes. A manual quality assurance step is conducted to thoroughly evaluate the accuracy and completeness of the metadata. Any identified deficiencies trigger necessary adjustments and a reprocessing cycle to ensure data quality and integrity before progressing to the next stage. \\

\noindent\textbf{QA Generation \& Filtering.} To ensure a balance between question diversity and accuracy, we adopt a hybrid approach that combines template-based and LLM-driven generation strategies. Approximately 15 question templates are manually designed for each question type and further expanded using LLMs-generated variations. These templates are populated with relevant objects and metadata to generate initial QA pairs using GPT-4o-mini. The LLM is then prompted to refine the generated content by rephrasing either the question alone or both the question and its corresponding answer, depending on the context. Once QA pairs are generated for each question type,  a selective quality evaluation is conducted to assess their accuracy and relevance. This iterative process involves refining question templates, adjusting off-the-shelf tools, and discarding QA pairs that do not meet the predefined quality standards. The validated QA pairs are then integrated into the TUMTraffic-VideoQA dataset, ensuring high-quality and diverse annotations.


% [-] add mathematic formation of these tasks

\subsection{Tasks and Metrics}
 


\begin{figure}[th!]
    \centering
    % First row
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/number_of_words_in_q.png}
        \caption{Distribution of question word counts across question types.}
        \label{4a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.38\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/mcq_task.png}
        \caption{Class distribution of Multi-Choice QA.}
        \label{4b}
    \end{subfigure}
    
    \vspace{0.1cm}
    
    % Second row
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/de_task.png}
        \caption{Distribution of answer word counts in Video Referred Object Captioning.}
        \label{4c}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/sp_te_gr_task.png}
        \caption{Temporal window lengths in Spatio-Temporal Grounding.}
        \label{4d}
    \end{subfigure}
    
    \caption{Statistical distributions of the dataset, including word counts in questions and answers, distribution of question types, and temporal window lengths for object grounding.}
    \label{fig:dataset_statistics}
\end{figure}


TUMTraffic-VideoQA benchmark comprises three core tasks to thoroughly evaluate model performance in traffic scenes: Multi-Choice Question Answering (MQA), Video Referred Object Captioning (V-ROC), and Spatio-Temporal Object Grounding (ST-OG). QA pairs related to weather and traffic accidents are included for training and future research but are not considered in the benchmark evaluation. \\

\noindent\textbf{Multi-Choice Question Answering.} The MQA task assesses the model’s capabilities across five key dimensions: \textbf{Positioning}, identifying the relative 3D spatial location of objects; \textbf{Counting}, determining the number of occurrences of a particular object or class across the video; \textbf{Motion}, analyzing the movement status of objects; \textbf{Class}, categorizing objects based on their type or attributes; \textbf{Existence}, querying whether a specific object or category is present in the video. Following \cite{qian2024nuscenes}, each dimension is further divided into easy and hard levels, depending on whether the question requires single-hop or multi-hop reasoning. We use Top-1 accuracy as the evaluation metric and report the mean accuracy across all question types. \\


\noindent\textbf{Video Referred Object Captioning.} The task evaluates the model’s capability to describe the appearance of a specified object in natural language. It aims to generate detailed and accurate summaries that effectively capture the object’s key visual attributes. Unlike the image-based referred object captioning task \cite{drivelm,zhou2024embodied}, we query an object based on its spatial and temporal location within a video, which adds a significant level of complexity. In this task, we adopt common NLG metrics \cite{mlg_metrics}, including BLEU, CIDEr, ROUGE, METEOR, and SPICE, to measure the description quality. \\

 
\noindent\textbf{Spatio-Temporal Object Grounding.} Accurately identifying the spatio-temporal positions of a specified object is crucial in traffic scenarios. Unlike traditional video grounding \cite{hc-stvg} or referred multi-object tracking tasks \cite{nuprompt}, which primarily focus on locating objects within individual frames across the video, ST-OG simplifies the process by providing start and end frames along with corresponding spatial coordinates in a standardized tuple format: $ [(c, f_n', x', y'), (c, f_n'', x'', y'')] $. This task serves to assess a model’s performance in effectively associating objects across frames while accurately determining their temporal extent and spatial positions within the video. 

% [x] TODOs: reformulate, explain v = |\hat{v} - v^*|. the sentence.  Explain what the 3 errors represent. 

We adopt three evaluation metrics to assess the performance of this task, i.e., Temporal error \( \mathcal{E}_{f_n} \), Spatial error \( \mathcal{E}_{s} \) and Spatio-Temporal error \( \mathcal{E}_{st} \). Temporal error \( \mathcal{E}_{f_n} \) and Spatial error \( \mathcal{E}_{s} \) use the L1 loss, which measures the absolute temporal differences \( \Delta f_n \) and the spatial displacement \( \Delta s = \| (\Delta x, \Delta y) \|_2 \). The Spatio-Temporal error \( \mathcal{E}_{st} \) adopts L2 loss and captures deviations across both spatial and temporal dimensions. For each metric, both the start and end frames are considered, with the formulations as follows:

{\small
\begin{align}
\mathcal{E}_{f_n} &= \frac{\Delta f_n' + \Delta f_n''}{2}; \quad \mathcal{E}_{s} = \frac{\Delta s' + \Delta s''}{2} \\
\mathcal{E}_{st} &= \frac{1}{2} \left( \| (\Delta f_n', \Delta x', \Delta y') \|_2 + \| (\Delta f_n'', \Delta x'', \Delta y'') \|_2 \right)
\end{align}}



\subsection{Dataset Statistics}

% [x] include statistics for another VROC, STOG




TUMTraffic-VideoQA dataset consists of 1,000 videos, 85,000 multi-choice QA pairs, 5,700 spatio-temporal grounding prompts, and  2,300 referred object captioning. Video durations range from 10 seconds to 2 minutes. We split the videos into training and validation sets with a ratio of 7:3, ensuring that videos in the validation set do not overlap with those in the training set. Generated QA pairs inherit the split of their associated videos, forming distinct videos and annotations for training and validation. Figure \ref{fig:dataset_statistics} provides an overview of the dataset’s statistical distributions, including question complexity, question-type distribution, answer lengths, and the temporal window distribution of queried objects in the spatio-temporal grounding task. Further details and data statistics are available in Appendix.

% \ref{app:dataset_statistics}
 % Figure \ref{fig:dataset_statistics} presents various statistical distributions of the TUMTraffic-VideoQA dataset. Figure \ref{4a} illustrates the distribution of word counts in the questions across all question types. Figure \ref{4b} depicts the distribution of question types within the multiple-choice question task. Figure \ref{4c} shows the word count distribution in the answers corresponding to the description task, and Figure \ref{4d} provides the distribution of temporal windows (in seconds) for the objects involved in the spatio-temporal Grounding task, along with their corresponding frequency.
 
% Figure \ref{4a} illustrates the distribution of word counts in the questions across all question types. Figure \ref{4b} depicts the distribution of question types within the multiple-choice question task. Figure \ref{4c} shows the word count distribution in the answers corresponding to the description task. Finally, Figure \ref{4d} provides the distribution of temporal windows (in seconds) for the objects involved in the spatio-temporal Grounding task, along with their corresponding frequency. Here, a temporal window refers to the duration between the first and last frame in which a vehicle is visible in the video, representing the time span during which the object is actively tracked. 



% \subsubsection{Accuracy Metrics}

% Multiple-choice or open-ended questions with a single correct answer. Given $N$ total questions, each with a predicted answer $\hat{a}_i$ and a ground truth answer $a_i^*$, 
% the Top-1 accuracy is defined as:
% % \[
% % \text{Accuracy} = \frac{\sum_{i=1}^{N} \mathbb{I}(\hat{a}_i = a_i^*)}{N},
% % \]
% where $\mathbb{I}(\cdot)$ is an indicator function that returns 1 if the predicted answer matches the ground truth, and 0 otherwise.

% \subsubsection{Match Metrics}

% Binary (Yes/No) questions. For yes/no questions, the match metric is effectively the same as accuracy on binary choices. 
% If $M$ is the total number of yes/no questions, then:
% \[
% \text{Match / Accuracy} = \frac{\sum_{i=1}^{M} \mathbb{I}(\hat{a}_i = a_i^*)}{M}.
% \]

% \subsubsection{Linguistic Metrics}

% Open-ended answers require natural language generation. We employ standard NLG metrics such as BLEU, ROUGE-L, and CIDEr to measure the quality of generated answers. 
% The detailed formulas are omitted here, but these metrics capture n-gram overlap, common subsequences, 
% and TF-IDF weighted similarity, respectively.

% \subsubsection{spatio-temporal Metrics}

% To evaluate spatio-temporal predictions, we define accuracy metrics based on discrepancies between predictions and ground truth. Let \( \Delta t = |\hat{t} - t^*| \) denote temporal error, and \( \Delta s = \sqrt{(\Delta x)^2 + (\Delta y)^2} \), where \( \Delta x = \hat{x} - x^* \) and \( \Delta y = \hat{y} - y^* \), represent spatial error.

% \begin{equation}
% \mathcal{E}_t =  ( \Delta t_s + \Delta t_e ) / 2,
% \end{equation}

% \begin{equation}
% \mathcal{E}_s =  ( \Delta s_s + \Delta s_e ) / 2,
% \end{equation}

% \begin{equation}
% \mathcal{E}_{st} =  (\sqrt{(\Delta t_s)^2 + (\Delta s_s)^2} + \sqrt{(\Delta t_e)^2 + (\Delta s_e)^2} ) / 2.
% \end{equation}

% Here, \( \Delta t_s, \Delta t_e \) are temporal errors for the start and end times, and \( \Delta s_s, \Delta s_e \) are spatial errors for the start and end locations, computed from \( \Delta x \) and \( \Delta y \). These metrics assess temporal, spatial, and joint spatio-temporal accuracy.

