
\section{Introduction}
% [x] rewrite? 


% The increasing urbanization and expansion of transportation networks pose significant challenges to understanding traffic scenarios in intelligent systems. 
% With the advancement of intelligent roadside infrastructure, accurately identifying and interpreting traffic participants, e.g., vehicles and pedestrians, has become increasingly important. Additionally, analyzing their spatio-temporal positions, interactions, and behavior predictions plays a vital role in improving traffic systems \cite{ITSsurvey,vlmsurvey}.

With the advancement of intelligent roadside infrastructure and Large Language Models (LLMs) \cite{grattafiori2024llama3herdmodels}, leveraging language to achieve a more generalized and interpretable understanding of traffic scenes becomes increasingly important. This involves accurately capturing the relationships among traffic participants, generating descriptive captions of their appearances, and analyzing their spatio-temporal positions and interactions \cite{ITSsurvey,zhou2024gpt4vtrafficassistantindepth}. Traditional models for traffic scene understanding are typically designed for specific tasks, such as object recognition, object association, and traffic flow analysis. Although these methods have achieved notable success within isolated domains, they often face significant challenges in scalability, generalization to diverse traffic conditions, and real-world deployment. The emergence and rapid development of large foundation models \cite{llava,vlmsurvey} present new opportunities to address these challenges. These models offer the potential to overcome traditional limitations by leveraging their ability to generalize across multiple tasks, integrate multimodal information, and adapt to complex, dynamic traffic scenarios in a flexible and unified manner.
 
Previous studies have primarily advanced traffic scene understanding through image-based question-answering tasks in driving environments \cite{drivelm,zhou2024embodied,qian2024nuscenes}. However, image-level Vision-Language Models (VLMs) are inherently limited in their ability to capture the temporal dynamics crucial for comprehending complex traffic events. In contrast, intricate traffic scenarios often require multi-frame video analysis for accurate real-world understanding. Besides, despite the growing number of vision-language datasets developed for driving scenarios, a significant gap persists in the exploration of multimodal datasets specifically designed for the roadside traffic domain. In particular, video-based datasets captured from a third-party perspective and tailored to traffic scene understanding remain notably underexplored. 

To bridge the gap in this domain, we propose TUMTraffic-VideoQA, a video language dataset designed to benchmark the model understanding capabilities in roadside traffic scenarios. The dataset encompasses video question-answering, object captioning, and spatio-temporal grounding tasks, capturing key elements crucial for understanding real-world traffic scenes. An illustrative example from the dataset is shown in Figure \ref{fig:title_figure}. The main contributions of this work can be outlined as follows:

% By integrating language-guided QA with domain-specific knowledge, we aim to facilitate the domain of language-augmented intelligent transportation systems and lay the foundations for subsequent research in roadside traffic scene understanding.
% The dataset consists of 1k videos, 88k QA pairs, and 5.7k grounding annotations.
\begin{itemize}

 \item We present TUMTraffic-VideoQA, a comprehensive video-language dataset designed for complex traffic video understanding. The dataset captures a diverse range of real-world scenarios, including extreme weather conditions and critical corner cases such as traffic accidents.
 
 \item We propose a novel benchmark that evaluates model performance across three key tasks, including video question answering, referred object captioning, and spatio-temporal grounding, facilitating fine-grained reasoning in traffic scenarios.
 
 \item We establish the TUMTraffic-Qwen baseline and provide detailed results and analyses. Through extensive experiments with various efficient visual token sampling strategies, we offer valuable  potential for future research.
 
\end{itemize}


% Traffic scene understanding is crucial for high-level understanding. With the emergence of LLMs, the paradigm shifts from predefined tasks to .. Previous works focus on driving scenarios and image-based QAs. Single frame-based VLMs fall short of video-based understanding, necessitating the exploration of this field. --> Focus on video.  

% Identifying objects in videos is important.  LLM, Autonomous Driving, 

% Our framework works for both ego vehicle perspective and surveillance camera. Can be used as a traffic surveillance system. Trigger reasoning and reports ... 

