

\section{Experiments}
Extensive experiments are conducted on the TUMTraffic-VideoQA dataset. We evaluate SOTA open-source VLMs in a zero-shot setting to assess their spatio-temporal reasoning abilities, analyze the dataset’s characteristics, and examine the impact of different visual sampling strategies on performance. During inference, the temperature is set to zero to ensure deterministic outputs and enhance consistency.

% We provide an in-depth analysis of the dataset’s characteristics, alongside an evaluation of the effects of various visual sampling strategies on model performance. We set



\subsection{Quantitative Results in Multi-Choice QA}
Table \ref{table:consolidated_metrics} presents the quantitative results in this task, offering several key insights, which are summarized as follows.

\noindent\textbf{Difficulty of Question Types.} The accuracy across different question types reveals consistent trends of difficulty for both open-source VLMs and our baseline models. Among the evaluated question types, existence questions are the least challenging, achieving the highest accuracy. This is followed by counting and motion questions, which necessitate the extraction and reasoning of information across multiple video frames. In contrast, positioning questions, which require a deeper understanding of 3D spatial relationships, emerge as the most challenging. Moreover, the accuracy of multi-hop questions is generally lower compared to single-hop questions, reflecting the increased complexity of complex reasoning tasks that demand the capture of more fine-grained details and intricate reasoning. 
% , making them more challenging for the baseline models.
 
\input{table/table3_1}

\noindent\textbf{Open-Source Model Performance.} We evaluate the performance of three open-source models: LLaVA-OneVision \cite{li2024llavaonevisioneasyvisualtask}, Qwen2-VL \cite{Qwen-VL}, and VideoLLaMA2 \cite{cheng2024videollama2advancingspatialtemporal} on our Multi-Choice QA task. The results indicate that increasing model size significantly enhances their performance in zero-shot video QA scenarios, with improvements from 5\% to 10\%. Notably, VideoLLaMA2 benefits from incorporating more frames, leading to a notable boost in accuracy. Among the three models with 7B parameters, Qwen2-VL and VideoLLaMA2 achieve comparable overall performance, whereas LLaVA-OneVision outperforms both, achieving the highest accuracy. Furthermore, all models struggle with positioning questions, highlighting their limitations in spatial reasoning. 

 


\noindent\textbf{Effect of Token Sampling Strategy.} Experimental results from the 0.5B and 7B baseline models demonstrate that multi-resolution strategies can enhance model performance to some extent, with MultiRes Temporal Pooling yielding the most significant gains. Notably, the MultiRes strategy can greatly improve positioning tasks that rely on spatial recognition, while having minimal impact on existence and counting tasks. Moreover, MultiRes Token Pruning effectively enhances positioning and counting accuracy but may inadvertently discard critical visual tokens, leading to limited or adverse effects on motion and existence tasks. While MultiRes Temporal Pooling enhances fine-grained reasoning, it has little impact on easy recognition tasks like existence. Although multi-resolution methods provide richer multi-granularity visual representations, the overall performance improvements remain moderate.

% Exploring more systematic sampling strategies could further enhance the model’s capabilities.

% \textbf{Broader Implications.}  

\subsection{Results in Spatio-Temporal Grounding} 
The quantitative results for the Spatio-Temporal Grounding task, presented in Table \ref{table:model_errors}, underscore the complexity of the task. Findings across temporal, spatial, and spatiotemporal errors exhibit a general consistency, revealing that without fine-tuning, open-source VLMs struggle to understand the task and cannot accurately regress the corresponding tuples, leading to unreliable temporal and spatial localization. For the fine-tuned TUMTraffic-Qwen baseline models, multi-resolution strategies appear to diminish spatial and temporal grounding performance, in contrast to their effectiveness in Multi-Choice QA and Referred Object Captioning tasks. This suggests that while multi-resolution techniques enhance frame-based object recognition by providing finer visual details, dynamically adjusting frame-level resolution can introduce ambiguity in inter-frame representations, adversely affecting temporal grounding and, consequently, spatial localization capabilities across the video.





\input{table/table2_1}

\subsection{Results in Referred Object Captioning}
As shown in Table \ref{tab:object_captioning}, Qwen2-VL (7B) surpasses all other open-source models by a considerable margin, demonstrating its strong performance on referred object captioning task. For baseline models, both the 0.5B and 7B variants exhibit performance improvements across various metrics when enhanced with multi-resolution strategies. Moreover, the 7B models consistently outperform their smaller counterparts in both open-source and fine-tuned baseline settings. The impact of the visual token sampling strategy, however, varies with model size. MultiRes Temporal Pooling yields the most significant gains for the 0.5B model, whereas MultiRes Spatial Pooling proves most effective for the 7B models. 

