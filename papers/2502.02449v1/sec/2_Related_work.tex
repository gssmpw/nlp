
\section{Related Work}

\begin{figure}[bt!]
    \centering
    % First row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/rep11.png}
        \caption{Objects with the prompt: \textit{A white truck that is stationary in the same direction.} \cite{nuprompt}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/rep21.png}
        \caption{Frame-based object expression using numerical coordinates \cite{drivelm}.}
    \end{subfigure}
    
    
    % Second row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/TrafficQA-Object_Representation_rep12.jpg}
        \caption{Object referring in \cite{vidstg} with prompt: \textit{What is beneath the adult}.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        % \includegraphics[width=\textwidth]{figure/rep22.jpg}
        \includegraphics[width=\textwidth]{figure/TrafficQA-Object_Representation_22.jpg}
        \caption{Location of the green bus \textit{[(c1,0.0,0.5,0.4)]} in the video. (Ours)}
        \label{fig:objct_ref4}
    \end{subfigure}
    
    \caption{Different methods for describing objects in images and videos using language expressions. We adopt a tuple-based spatio-temporal object representation for the unique object reference, as shown in (d). }
    \label{fig:object_representation}
\end{figure}


\input{table/related_datasets}


% [x] connect table 1 with introductions. 

\subsection{Vision-Language Datasets in Traffic Scenes}
% DriveLM\cite{drivelm},
% HAD \cite{had} and 
With the rapid advancements in LLMs, significant efforts have been made to integrate language into the development of vision-language foundation models. As summarized in Table \ref{tab:related_datasets}, several pioneering datasets have been introduced for traffic scenarios, particularly focusing on vehicle-centric environments \cite{addatasetseurvey}. NuScenes-QA \cite{qian2024nuscenes} provides a question-answering benchmark tailored for driving scenes. Meanwhile, DRAMA \cite{malla2023drama} is designed for video-level open-ended tasks aimed at evaluating driving instructions and assessing the importance of objects within their environments. Besides, referring to specific traffic participants through natural language—commonly known as referred object grounding and tracking—is a crucial task in traffic scene understanding. Some works \cite{referkitti,nuprompt} extend the KITTI \cite{kitti} and nuScenes \cite{caesar2020nuscenesmultimodaldatasetautonomous} datasets, by associating natural language descriptions with specific vehicles and pedestrians. This facilitates fine-grained identification and tracking of traffic participants, allowing for precise object localization based on language descriptions in complex driving environments. However, most existing efforts primarily focus on driving scenarios and are typically constrained to individual tasks such as question answering, video grounding, or referred multi-object tracking. A significant research gap also remains in the availability of large-scale datasets designed specifically for roadside surveillance scenarios. Our work aims to bridge this gap by providing a comprehensive dataset tailored for multiple tasks in roadside traffic understanding within a unified framework.
% is also an important aspect of traffic scene understanding
% introducing a standardized object representation and 


\subsection{Fine-Grained Video Understanding}

Fine-grained video understanding centers on the precise analysis of intricate video content, targeting tasks that demand nuanced reasoning across spatial and temporal dimensions. Some representative tasks include spatio-temporal grounding \cite{vidstg,hc-stvg}, mapping specific objects or events to precise locations and times within a video based on a given query; video object referring \cite{mevis,referkitti,nuprompt}, which involves tracking objects through space and time given text prompts; video temporal grounding \cite{UniVTG,huang2024vtimellm}, identifying specific moments or intervals in a video that align with a provided textual query. These tasks require high precision, nuanced multimodal alignment, and the ability to capture subtle temporal and spatial dynamics. It is particularly challenging due to the difficulty of properly representing fine-grained video details and the inherent cross-modality misalignment. With the advancement of visual LLMs, recent advancements enhance the capabilities of fine-grained video understanding \cite{videunderstandingsurvey} and facilitate understanding across abstract and detailed levels. 

% , with advanced visual embedding techniques and modality alignment strategies to bridge the gap between textual and visual semantics, significantly





\subsection{Language-Based Object Referring}


Referring objects in visual data, such as images and videos, is typically achieved by associating them with predefined definitions or language descriptions. Figure \ref{fig:object_representation} illustrates four commonly used methods for representing objects through language expressions. The inherent ambiguity of natural language, coupled with the modality gap between visual and linguistic representations, presents significant challenges. Object representation in tasks such as object referring often necessitates careful dataset curation to ensure that linguistic expressions uniquely or collectively correspond to specific objects in videos. For example, some datasets include only scenarios with uniquely identifiable objects \cite{hc-stvg}, while others contain expressions that jointly refer to multiple objects \cite{dvd-st}. However, in complex real-world applications such as autonomous driving, textual descriptions alone are often insufficient to uniquely specify an object. To address this challenge, DriveLM \cite{drivelm} introduces a structured tuple representation, $\textless c, CAM, x, y \textgreater$, where  c  denotes the object identifier,  CAM  specifies the camera, and $\textless x, y \textgreater$ represents the 2D center coordinates within the camera’s coordinate system. Alternatively, ELM \cite{zhou2024embodied} simplifies the problem by converting temporal video tasks into frame-level questions, using a tuple $\textless c, x, y \textgreater$ to identify objects within individual frames without temporal dependencies. Despite the advancements, formulating a unified, precise, and unique language representation for objects in video remains open challenges. 




In this work, we design a spatio-temporal object representation in videos with a four-element tuple format $(c, f_n, x, y)$, where c denotes a unique object identifier, $f_n$ indicates the normalized frame timestamp, and $(x, y)$ corresponds to the object’s normalized spatial coordinates within the frame.  The same object is consistently assigned the identifier  c  throughout the video, while its spatial position changes over time. This formulation enables precise tracking and referencing of objects across both spatial and temporal dimensions, facilitating robust language-based interaction in dynamic environments. Besides, it provides a standardized interface for fine-grained video understanding, enabling more detailed and structured analysis.

 