\section{Related Works}
\textbf{Byzantine attacks.}
\citeauthor{blanchard2017krum} first disclose the Byzantine vulnerability of FL.
\citeauthor{baruch2019lie} observe that the variance of honest gradients is high enough for Byzantine clients to compromise Byzantine defenses. 
Based on this observation, they propose a LIE attack that hides Byzantine gradients within the variance.
\citeauthor{xie2020ipm} further utilize the high variance and propose an IPM attack.
Particularly, they show that when the variance of honest gradients is large enough, IPM can make the inner product between the aggregated gradient and the honest average negative.
However, this result is restricted to a few defenses, i.e., Median \cite{yin2018mediantrmean}, Trmean \cite{yin2018mediantrmean}, and Krum \cite{blanchard2017krum}.
\citeauthor{fang2020fang} establish an omniscient attack called Fang.
However, the Fang attack requires knowledge of the Byzantine defense, which is unrealistic in practice.
\citeauthor{shejwalkar2021dnc} propose Min-Max and Min-Sum attacks that solve a constrained optimization problem to determine Byzantine gradients.
From a high level, both Min-Max and Min-Sum aim to maximize the perturbation to a reference benign gradient while ensuring the Byzantine gradients lie within the variance.
\citeauthor{karimireddy2022bucketing} propose a Mimic attack that takes advantage of data heterogeneity in FL.
In particular, Byzantine clients pick an honest client to mimic and copy its gradient.
The above attacks take advantage of the large variance of honest gradients to break Byzantine defenses.
However, they all ignore the skewed nature of honest gradients in FL and fail to exploit this vulnerability.

\textbf{Byzantine resilience.}
\citeauthor{el2021collaborative, karimireddy2022bucketing} provide state-of-the-art theoretical analysis of Byzantine resilience under data heterogeneity.
\citeauthor{el2021collaborative} discuss Byzantine resilience in a decentralized, asynchronous setting.
\citeauthor{farhadkhani2022reasm} provide a unified framework for Byzantine resilience analysis, which enables comparison among different defenses on a common theoretical ground.
\citeauthor{karimireddy2022bucketing} improve the error bound of Byzantine resilience to be upper-bounded by the fraction of Byzantine clients, which recovers the standard convergence rate when there are no Byzantine clients.
\citeauthor{allouah2024byzantinerobust} tightly analyzing the impact of client subsampling and local steps.
\citeauthor{yan2024recess} utilizes the correlation of clientsâ€™ performance over multiple iterations to evaluate the reliability of clients.
They all share a common bias: densely distributed gradients are more likely to be honest.
However, this bias is a poison to Byzantine robustness in the presence of gradient skew.
In practical FL, the distribution of honest gradients is highly skewed due to data heterogeneity.
Therefore, existing defenses are especially vulnerable to attacks that are aware of gradient skew.

\textbf{Data heterogeneity.}
\citeauthor{yu2018parallel} first proposed to measure data heterogeneity by gradient divergence, which describes the difference between the local gradients and the global one. \citeauthor{karimireddy2020scaffold} proposed a more general version of gradient divergence - gradient dissimilarity. To the best of our knowledge, these are the only metrics of heterogeneity from a gradient distribution perspective \cite{li2019convergence,woodworth2020minibatch}. \citeauthor{luo2021no} find that such difference mainly involves neural network prediction heads. For label skewness, a particular type of heterogeneity, label distribution discrepancy is used to measure heterogeneity \cite{peng2024fedcal}. However, no existing work noticed that such gradient divergence is skewed - a group of densely distributed local gradients skew away from the global gradient, i.e., the gradient skew introduced in Section 4.