\documentclass[letterpaper]{article}
% DO NOT CHANGE THIS
\usepackage{aaai25} % DO NOT CHANGE THIS
\usepackage{times} % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier} % DO NOT CHANGE THIS
\usepackage[hyphens]{url} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm} % DO NOT CHANGE THIS
\usepackage{graphicx}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS
\usepackage{caption}  % DO NOT CHANGE THIS
\frenchspacing % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%%%%% NEW MATH DEFINITIONS %%%%%

\usepackage{amsmath,amsfonts,bm}

% Mark sections of captions for referring to divisions of figures
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

% Highlight a newly defined term
\newcommand{\newterm}[1]{{\bf #1}}


% Figure reference, lower-case.
\def\figref#1{figure~\ref{#1}}
% Figure reference, capital. For start of sentence
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
% Section reference, lower-case.
\def\secref#1{section~\ref{#1}}
% Section reference, capital.
\def\Secref#1{Section~\ref{#1}}
% Reference to two sections.
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
% Reference to three sections.
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
% Reference to an equation, lower-case.
\def\eqref#1{equation~\ref{#1}}
% Reference to an equation, upper case
\def\Eqref#1{Equation~\ref{#1}}
% A raw reference to an equation---avoid using if possible
\def\plaineqref#1{\ref{#1}}
% Reference to a chapter, lower-case.
\def\chapref#1{chapter~\ref{#1}}
% Reference to an equation, upper case.
\def\Chapref#1{Chapter~\ref{#1}}
% Reference to a range of chapters
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
% Reference to an algorithm, lower-case.
\def\algref#1{algorithm~\ref{#1}}
% Reference to an algorithm, upper case.
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
% Reference to a part, lower case
\def\partref#1{part~\ref{#1}}
% Reference to a part, upper case
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}

\def\vdelta{{\bm{\delta}}}
\def\vmu{{\bm{\mu}}}
\def\vmu{{\bm{\mu}}}

% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\vsigma{{\bm{\sigma}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

% Tensor
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


% Graph
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

% Sets
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
% Don't use a set called E, because this would be the same as our symbol
% for expectation.
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

% Entries of a matrix
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

% entries of a tensor
% Same font as tensor, without \bm wrapper
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

% The true underlying data generating distribution
\newcommand{\pdata}{p_{\rm{data}}}
% The empirical distribution defined by the training set
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
% The model distribution
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
% Stochastic autoencoder distributions
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} % Laplace distribution

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} % See usage in notation.tex. Chosen to match Daphne's book.

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak


% ------------------ custom packages ------------------ %
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{comment}
\usepackage{graphbox}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{xspace}
% ------------------ custom packages ------------------ %


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MY THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}
\newtheorem{proposition*}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{definition*}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{assumption*}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{remark*}{Remark}

\crefname{fig}{Fig.}{Fig.}
% ------------------ custom commands ------------------ %
% math
\newcommand{\bvg}{\Bar{\vg}}
\newcommand{\hvg}{\hat{\vg}}
\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\card}[1]{\lvert{#1}\rvert}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\ip}[2]{\langle{#1},{#2}\rangle}
\newcommand{\set}[1]{\{{#1}\}}
% names
\newcommand{\abbreviation}{two-Stage aTtack based on gRadIent sKEw\xspace}
\newcommand{\SKEW}{STRIKE\xspace}
% toto
\newcommand{\todo}[1]{\textcolor{red}{[todo:#1]}}
% ------------------ custom commands ------------------ %

% ------------------ custom packages ------------------ %
% ------------------ custom packages ------------------ %

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Exploit Gradient Skewness\\to Circumvent Byzantine Defenses for Federated Learning\footnote{This paper is accepted by AAAI'25}}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    % J. Scott Penberthy,
    % George Ferguson,
    % Hans Guesgen,
    % Francisco Cruz\equalcontrib,
    % Marc Pujol-Gonzalez\equalcontrib
    Yuchen Liu\textsuperscript{\rm 12}\equalcontrib\thanks{Work done during an internship at Sony AI.},
    Chen Chen\textsuperscript{\rm 3}\equalcontrib,
    Lingjuan Lyu\textsuperscript{\rm 3}\footnote{Corresponding author.},
    Yaochu Jin\textsuperscript{\rm 4},
    Gang Chen\textsuperscript{\rm 12}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}The State Key Laboratory of Blockchain and Data Security, Zhejiang University\\
    \textsuperscript{\rm 2}Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security\\
    \textsuperscript{\rm 3}Sony AI\\
    \textsuperscript{\rm 4}{Westlake University, China}\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % email address must be in roman text type, not monospace or sans serif
    yuchen.liu.a@gmail.com,
    \{ChenA.Chen, lingjuan.lv\}@sony.com,
    {jinyaochu@westlake.edu.cn},
    cg@zju.edu.cn,
%
% See more examples next
}


\begin{document}

\maketitle

\begin{abstract}
Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks.
Most current Byzantine defenses share a common inductive bias: among all the gradients, the densely distributed ones are more likely to be honest.
However, such a bias is a poison to Byzantine robustness due to a newly discovered phenomenon in this paper -- gradient skew.
We discover that a group of densely distributed honest gradients skew away from the optimal gradient (the average of honest gradients) due to heterogeneous data.
This gradient skew phenomenon allows Byzantine gradients to hide within the densely distributed skewed gradients.
As a result, Byzantine defenses are confused into believing that Byzantine gradients are honest.
Motivated by this observation, we propose a novel skew-aware attack called \SKEW:
first, we search for the skewed gradients;
then, we construct Byzantine gradients within the skewed gradients.
Experiments on three benchmark datasets validate the effectiveness of our attack.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
\begin{links}
    \link{Code}{https://github.com/YuchenLiu-a/byzantine_skew}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}

Federated Learning (FL) \cite{mcmahan2017fl, li2020fedprox} emerged as a privacy-aware learning paradigm, in which data owners, i.e., clients, repeatedly use their private data to compute local gradients and upload them to a central server.
The central server collects the uploaded gradients from clients and aggregates these gradients to update the global model.
In this way, clients can collaborate to train a model without exposing their private data.

Unfortunately, FL is susceptible to Byzantine attacks due to its distributed nature \cite{blanchard2017krum,guerraoui2018bulyan}.
A malicious party can control a small subset of clients, i.e., Byzantine clients, to degrade the utility of the global model.
During the training phase, Byzantine clients can send arbitrary messages to the central server to bias the global model.
A wealth of defenses \cite{blanchard2017krum,pillutla2019geometric,shejwalkar2021dnc} have been proposed to defend against Byzantine attacks in FL. 
They aim to estimate the optimal gradient, i.e., the average of gradients from honest clients, in the presence of Byzantine clients.

Most existing defenses \cite{blanchard2017krum,shejwalkar2021dnc,karimireddy2022bucketing} share a common inductive bias: the densely distributed gradients are more likely to be honest.
Generally, they assign higher weights to the densely distributed gradients. 
Then they compute the global gradient and use it to update the global model.
As a result, the output global gradient of defenses is biased towards the densely distributed of gradients.
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{figures_cymk/main_visualization_new_legend-1.jpg}
\end{center}
\caption{
The LLE visualization of honest gradients in the non-IID setting on CIFAR-10.
Substantial honest gradients (blue circles) are skewed away from the optimal gradient (green star).
In this case, we can hide Byzantine gradients (pink crosses) within the skewed honest gradients to circumvent defenses.
}
\label{fig:main_visualization}
\end{figure}

However, this inductive bias of Byzantine defenses is harmful to Byzantine robustness in FL due to the presence of gradient skew.
In practical FL, data across different clients is non-independent and identically distributed (non-IID), which gives rise to heterogeneous honest gradients \cite{mcmahan2017fl,li2020fedprox,karimireddy2022bucketing}.
On closer inspection, we find that the distribution these heterogenous honest gradients are highly skewed.
In \cref{fig:main_visualization}, we use Locally Linear Embedding (LLE) \cite{roweis2000lle} to visualize the honest gradients on CIFAR-10 dataset \cite{krizhevsky2009cifar} when data is non-IID split.
Detailed setups and more results are provided in \cref{appsec:vis}.
As shown in \cref{fig:main_visualization}, a group of densely distributed gradients skews away from the optimal gradient.
We term this phenomenon as "gradient skew".
When honest gradients are skewed, the defenses' bias towards densely distributed gradients is a poison to Byzantine robustness.
In fact, we can hide Byzantine gradients within the skewed densely distributed honest gradients as shown in \cref{fig:main_visualization}.
In this case, the bias of defenses would drive the global gradient close to the skewed gradients but far from the optimal gradient.

In this paper, we
study how to exploit the gradient skew in the more practical non-IID setting to circumvent Byzantine defenses.
We first observe the gradient skew phenomenon in the non-IID setting and explore its vulnerability.
Motivated by the above observation, we design a novel \abbreviation called \SKEW.
In particular, \SKEW hides Byzantine gradients within the skewed honest gradients as shown in \cref{fig:main_visualization}.
\SKEW can take advantage of the gradient skew in FL to break Byzantine defenses.

In summary, our contributions are:
\begin{itemize}
    \item To the best of our knowledge, we are the first to discover the gradient skew phenomenon in FL:
    a group of densely distributed gradients is skewed away from the optimal gradient.
    Motivated by the observation, we design an attack principle that can circumvent Byzantine defenses under gradient skew:
    hide Byzantine gradients within the skewed honest gradients.
    \item Based on the above attack principle, we propose a two-stage Byzantine attack called \SKEW.
    In the first stage, \SKEW searches for the skewed honest gradients under the guidance of Karl Pearson's formula.
    In the second stage, \SKEW constructs the Byzantine gradients within the skewed honest gradients by solving a constrained optimization problem.
    \item Experiments on three benchmark datasets validate the effectiveness of the proposed attack.
    For instance, \SKEW attack improves upon the best baseline by 57.84\% against DnC on FEMNIST dataset when there are 20\% Byzantine clients.
\end{itemize}


\section{Related Works}

\textbf{Byzantine attacks.}
\citeauthor{blanchard2017krum} first disclose the Byzantine vulnerability of FL.
\citeauthor{baruch2019lie} observe that the variance of honest gradients is high enough for Byzantine clients to compromise Byzantine defenses. 
Based on this observation, they propose a LIE attack that hides Byzantine gradients within the variance.
\citeauthor{xie2020ipm} further utilize the high variance and propose an IPM attack.
Particularly, they show that when the variance of honest gradients is large enough, IPM can make the inner product between the aggregated gradient and the honest average negative.
However, this result is restricted to a few defenses, i.e., Median \cite{yin2018mediantrmean}, Trmean \cite{yin2018mediantrmean}, and Krum \cite{blanchard2017krum}.
\citeauthor{fang2020fang} establish an omniscient attack called Fang.
However, the Fang attack requires knowledge of the Byzantine defense, which is unrealistic in practice.
\citeauthor{shejwalkar2021dnc} propose Min-Max and Min-Sum attacks that solve a constrained optimization problem to determine Byzantine gradients.
From a high level, both Min-Max and Min-Sum aim to maximize the perturbation to a reference benign gradient while ensuring the Byzantine gradients lie within the variance.
\citeauthor{karimireddy2022bucketing} propose a Mimic attack that takes advantage of data heterogeneity in FL.
In particular, Byzantine clients pick an honest client to mimic and copy its gradient.
The above attacks take advantage of the large variance of honest gradients to break Byzantine defenses.
However, they all ignore the skewed nature of honest gradients in FL and fail to exploit this vulnerability.

\textbf{Byzantine resilience.}
\citeauthor{el2021collaborative, karimireddy2022bucketing} provide state-of-the-art theoretical analysis of Byzantine resilience under data heterogeneity.
\citeauthor{el2021collaborative} discuss Byzantine resilience in a decentralized, asynchronous setting.
\citeauthor{farhadkhani2022reasm} provide a unified framework for Byzantine resilience analysis, which enables comparison among different defenses on a common theoretical ground.
\citeauthor{karimireddy2022bucketing} improve the error bound of Byzantine resilience to be upper-bounded by the fraction of Byzantine clients, which recovers the standard convergence rate when there are no Byzantine clients.
\citeauthor{allouah2024byzantinerobust} tightly analyzing the impact of client subsampling and local steps.
\citeauthor{yan2024recess} utilizes the correlation of clientsâ€™ performance over multiple iterations to evaluate the reliability of clients.
They all share a common bias: densely distributed gradients are more likely to be honest.
However, this bias is a poison to Byzantine robustness in the presence of gradient skew.
In practical FL, the distribution of honest gradients is highly skewed due to data heterogeneity.
Therefore, existing defenses are especially vulnerable to attacks that are aware of gradient skew.

\textbf{Data heterogeneity.}
\citeauthor{yu2018parallel} first proposed to measure data heterogeneity by gradient divergence, which describes the difference between the local gradients and the global one. \citeauthor{karimireddy2020scaffold} proposed a more general version of gradient divergence - gradient dissimilarity. To the best of our knowledge, these are the only metrics of heterogeneity from a gradient distribution perspective \cite{li2019convergence,woodworth2020minibatch}. \citeauthor{luo2021no} find that such difference mainly involves neural network prediction heads. For label skewness, a particular type of heterogeneity, label distribution discrepancy is used to measure heterogeneity \cite{peng2024fedcal}. However, no existing work noticed that such gradient divergence is skewed - a group of densely distributed local gradients skew away from the global gradient, i.e., the gradient skew introduced in Section 4.

\section{Notations and Preliminary}
\subsection{Notations}
$\norm{\cdot}$ denotes the $\ell_2$ norm of a vector.
For vector $\vv$, $(\vv)_k$ represents the $k$-th coordinate of $\vv$.
Model parameters are denoted by $\vw$ and gradients are denoted by $\vg$.
We use $\bvg$ to denote the optimal gradient, i.e., the average of honest gradients, and $\hvg$ denotes the global gradients obtained by Byzantine defenses.
We use subscript $i$ to denote client $i$ and use superscript $t$ to denote communication round $t$.

\subsection{Preliminary}

\textbf{Federated learning.}
Suppose that there are $n$ clients and a central server.
The goal is to optimize the global loss function $\gL(\cdot)$:
\begin{align}
\min_\vw{\gL(\vw)},\quad\text{where }\gL(\vw)=\frac{1}{n}\sum_{i=1}^n\gL_i(\vw).
\end{align}
Here $\vw$ is the model parameter,
and $\gL_i(\cdot)$ is the local loss function on client $i$ for $i=1,\ldots,n$.

In communication round $t$, the central server distributes global parameter $\vw^t$ to the clients.
Each client $i$ performs several epochs of SGD to minimize its local loss function $\gL_i(\cdot)$ and update its local parameter to $\vw_i^{t+1}$.
Then, each client $i$ computes its local gradient $\vg_i^t$ and sends it to the server.
\begin{align}
\vg_i^t=\vw_i^t-\vw_i^{t+1},
\quad i=1,\ldots,n.
\end{align}
After receiving the uploaded local gradients, the server aggregates the local gradients and updates the global model to $\vw^{t+1}$.
\begin{gather}
\label{eq:mean}
\bvg^t=\frac{1}{n}\sum_{i=1}^n\vg_i^t,
\quad\vw^{t+1}=\vw^t-\bvg^t.
\end{gather}

\begin{figure*}[t]
\centering
\includegraphics[width=.75\linewidth]{figures_cymk/visskew_cifar10-1.jpg}     
\includegraphics[width=.6\linewidth]{figures_cymk/new_vis_legend-1.jpg}   \caption{Visualization of gradient skew on CIFAR-10 dataset. As shown in the figures, the optimal gradients (green stars) deviate from the densely distributed gradients.}
\label{fig:cifar10_vis}
\end{figure*}


\textbf{Byzantine attack model.}
Assume that among the total $n$ clients, $f$ fixed clients are Byzantine clients.
Let $\gB\subseteq\set{1,\ldots,n}$ denote the set of Byzantine clients and $\gH=\set{1,\ldots,n}\setminus\gB$ denote the set of honest clients.
In each communication round, Byzantine clients can send arbitrary messages to bias the global model.
The local gradients that the server receives in the $t$-th communication round are
\begin{align}
\vg_i^t=
\begin{cases}
*, & i\in\gB,\\
\vw^t-\vw_i^{t+1}, & i\in\gH,\\
\end{cases}
\end{align}
where $*$ represents an arbitrary message.
Following \cite{baruch2019lie,xie2020ipm}, we consider the setting where the attacker only has the knowledge of honest gradients.

\section{Gradient Skew in FL Due to Non-IID data}

\label{section:principle}
Plenty of works \cite{baruch2019lie,xie2020ipm,karimireddy2022bucketing} have explored how large variance can be harmful to Byzantine robustness.
However, to the best of our knowledge, none of the existing works is aware of the skewed nature of honest gradients in the non-IID setting and how gradient skew can threaten Byzantine robustness.

We take a close look at the distribution of honest gradients in the non-IID setting (without attack).
To construct our FL setup, we split CIFAR-10 \cite{krizhevsky2009cifar} dataset in a non-IID manner among 100 clients.
For more setup details, please refer to \cref{appsec:vis_setting}.
We run FedAvg \cite{mcmahan2017fl} for 200 communication rounds.
We randomly sample six communication rounds and use Locally Linear Embedding (LLE) \cite{roweis2000lle} to visualize the gradients in these communication rounds in \cref{fig:cifar10_vis}.
From \cref{fig:cifar10_vis}, we observe that a group of densely distributed honest gradients (blue circles) skew away from the optimal gradient (green stars).
We call these blue circles \emph{"skewed gradients"} and name this phenomenon \emph{"gradient skew"}.
We provide visualization results on more datasets in \cref{appsec:vis_results}.
From visualization results on different datasets, we observe that the "gradient skew" phenomenon is prevalent across different datasets.

\section{Circumvent Robust AGRs under Gradient Skew}
Inspired by the above observation of the gradient skew phenomenon, we design a novel attack principle that can exploit this phenomenon to circumvent robust AGRs -- 
\emph{hide Byzantine gradients in the densely distributed skewed gradients}.

A body of recent works \cite{farhadkhani2022reasm, karimireddy2022bucketing,allouah2023nnm} have formulated Byzantine resilience for general robust AGRs.
These formulations commonly show that Byzantine defenses inherently trust densely distributed gradients, regarding them as honest.
We take the definition of $(f, \kappa)$-robustness in \cite{allouah2023nnm} as an example.


\begin{definition}
[$(f, \kappa)$-robustness]
\label{def:resilience_nnm}
Let $f<n/2$ and $\kappa\ge0$,
a robust AGR $\gA$ is called $(f,\kappa)$-robust] if 
for any input $\set{\vg_1,\ldots,\vg_n}$ and any set $\gG\subseteq\set{1,\ldots,n}$ of size $n-f$, the output $\hvg$ of AGR $\gA$ satisfies:
\begin{equation}
\begin{gathered}
\norm{\gA(\vg_1,\ldots,\vg_n)-\bvg_\gG}^2\le\frac{\kappa}{n-f}\sum_{i\in\gS} \norm{\vg_i-\bvg_\gG}^2,
\\\text{where}\quad
\bvg_\gG=\sum_{i\in\gG}\vg_i/(n-f).
\end{gathered}
\end{equation}
\end{definition}


In the definition, the distance between the aggregated gradients $\hvg$ and average candidate gradient $\bvg_\gG$ is upper-bounded by $\sum_{i\in\gS} \norm{\vg_i-\bvg_\gG}^2$, which measures the distribution density of gradients.
This means that the aggregated gradient is biased to densely distributed gradients.
In other words, \emph{Byzantine defenses believe that the most densely distributed $n-f$ gradients are more likely to be the honest ones}.


This inductive bias can be exploited by a malicious party when gradient skew exists.
In particular, we propose to \emph{hide Byzantine gradients in the skewed gradients}, i.e., place pink cross within blue dots as shown in \cref{fig:main_visualization}, to fake Byzantine gradients as honest ones.
As shown in \cref{fig:main_visualization}, Byzantine gradients and skew gradients, i.e., pink cross and blue dots, are densely distributed. 
As a result, they would be mistaken as honest gradients by Byzantine defenses.

This attack strategy enjoys another advantage.
It tricks Byzantine defense into thinking other outlier honest gradients, i.e., gray circles in \cref{fig:main_visualization}, are malicious.
As a result, these outlier gradients would be assigned lower weights or even removed from aggregation.
Unfortunately, outlier gradients are crucial to improving the generalization performance of the final FL model \cite{yan2024recess}.
As a result, this attack strategy can pose a significant threat to model performance.


\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.93\textwidth]{figures_cymk/search_w_usearch-1.jpg}
        \caption{We search along the direction $\vu_\text{search}=\vg_\text{med}-\bvg$.
        The honest gradients with the largest scalar projection $p_i$ are selected as the skewed honest gradients (blue circles).}
        \label{fig:search}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.93\textwidth]{figures_cymk/hide-1.jpg}
        \caption{We start from the average of skewed honest gradients $\bvg_\gS$ (dark blue star) and select $\alpha$ such that Byzantine gradient $\vg_b$ (pink cross) lies within the skewed honest gradients.}
        \label{fig:hide}
    \end{subfigure}
    \caption{Illustration of the proposed two-stage attack \SKEW:
    in the first stage, \SKEW searches for the skewed honest gradients;
    in the second stage, \SKEW hides Byzantine gradients within the skewed honest gradients.
    }
    \label{fig:attack}
\end{figure*}


\section{Proposed Attack}
In this section, we introduce the proposed \abbreviation called \SKEW.
As discussed in the previous section, the attack principle is to \emph{hide Byzantine gradients within the skewed gradients}.
To achieve this goal, we carry out \SKEW attack in two stages:
in the first stage, we search for the skewed honest gradients;
in the second stage, we construct Byzantine gradients within the skewed honest gradients found in the first stage.
The procedure of \SKEW attack is shown in \cref{alg:proposed} in \cref{appsec:algorithm}.

\textbf{Search for the skewed honest gradients.}
To hide the Byzantine gradient in the skewed honest gradients, we first need to find the skewed honest gradients.
We perform a heuristic search motivated by Karl Pearson's formula \cite{knoke2002pearson2,moore2009pearson1}.
\cref{fig:search} illustrates the search procedure in this stage.

As visualized in \cref{fig:main_visualization}, skewed honest gradients are densely distributed.
As the population mode (in statistics) falls where the probability density is highest, the skewed honest gradients coincide with the (population) mode.
Thus, we can \emph{identify honest gradients near the mode as skewed gradients}.

Karl Pearson's formula \cite{knoke2002pearson2,moore2009pearson1} implies that the mode and median lie on the same side of the mean.
Therefore, the search for the skewed gradients starts from the mean and advances towards the median.
That is, as shown in \cref{fig:search} we search for the skewed honest gradients along the direction $\vu_\text{search}$ defined as:
\begin{align}
\vu_\text{search}=\vg_{\text{med}}-\bvg,
\end{align}
where $\vg_{\text{med}}$ is the coordinate-wise median of honest gradients $\set{\vg_i\mid i\in\gH}$, 
i.e., the $k$-th coordinate of $\vg_{\text{med}}$ is  $(\vg_\text{med})_k=\text{median}\{(\vg_i)_k\mid i\in\gH\}$, $\text{median}\{\cdot\}$ returns the median of the input numbers,
and $\bvg=\sum_{i\in\gH}\vg_i/(n-f)$ is the average of honest gradients.

For each honest gradient $\vg_i$, we compute its scalar projection $p_i$ on the searching direction $\vu_\text{search}$:
\begin{align}
p_i = \ip{\vg_i}{\frac{\vu_\text{search}}{\norm{\vu_\text{search}}}},
\quad\forall i\in\gH,
\end{align}
where $\ip{\cdot}{\cdot}$ represents the inner product.
%Then 
The $n-2f$ gradients with the highest scalar projection values are identified as the skewed honest gradients.
The goal is to have AGR consider the selected $n-2f$ gradients as honest and the unselected $f$ gradients as Byzantine.
Let $\gS$ denote the index set, that is
\begin{equation}
\begin{gathered}
\gS=\text{Set of }(n-2f)\text{ indices of the gradients with the}\\
\text{highest scalar projection }p_i,
\end{gathered}
\end{equation}
then the skewed honest gradients are $\set{\vg_i\mid i\in\gS}$. 

\textbf{Hide Byzantine gradients within the skewed honest gradients.}
In this stage, we aim to hide Byzantine gradients $\set{\vg_i\mid i\in\gB}$ within the skewed honest gradients $\set{\vg_i\mid i\in\gS}$ identified in stage 1.
The primary goal of our attack is to disguise Byzantine gradients and the skewed honest gradients $\set{\vg_i\mid i\in\gB\cup\gS}$ as honest gradients.
Meanwhile, the secondary goal is to maximize the attack effect, i.e.,  maximize the distance between these "fake" honest gradients and the optimal gradient.
The hiding procedure in this stage is illustrated in \cref{fig:hide}.

According to \cite{farhadkhani2022reasm}, robust AGRs are sensitive to the diameter of gradients.
Therefore, we ensure that the Byzantine gradients $\vg_b$ lie within the diameter of the skewed honest gradients $\vg_s$ in order not to be detected.
\begin{align}
\label{eq:hide}
\norm{\vg_b-\vg_s}\le\max_{i,j\in\gS}{\norm{\vg_i-\vg_j}},\quad\forall b\in\gB,s\in\gS,
\end{align}
where $\gB$ is the index set of Byzantine clients, $\gS$ is the index set of skewed honest clients.

Meanwhile, we want to maximize the attack effect.
As Byzantine defenses assume densely distributed gradients, i.e., the skewed honest gradients $\set{\vg_s\mid s\in\gS}$ and Byzantine gradients $\set{\vg_b\mid s\in\gB}$, to be honest.
The aggregated gradients would be close to the mean of densely distributed gradients $\bvg_{\gS\cup\gB}=\sum_{i\in\gS\cup\gB}\vg_i/(n-f)$.
Therefore, we maximize the distance between $\bvg_{\gS\cup\gB}$ and the optimal gradient $\bvg$ to maximize the attack effect.
\begin{align}
\label{eq:constraint}
\max_{\set{\vg_b\mid b\in\gB}}{\norm{\bvg_{\gS\cup\gB}-\bvg}}.
\end{align}

Combining \cref{eq:hide} and \cref{eq:constraint}, our objective can be formulated as the following constrained optimization problem.
\begin{equation}
\begin{aligned}
\label{eq:optimization}
&\max_{\set{\vg_b\mid b\in\gB}}{\norm{\bvg_{\gS\cup\gB}-\bvg}}\\
\text{s.t.}\quad
&\bvg_{\gS\cup\gB}=\sum_{i\in\gS\cup\gB}\vg_i/(n-f)\\
&\norm{\vg_b-\vg_s}\le\max_{i,j\in\gS}{\norm{\vg_i-\vg_j}},\,\forall b\in\gB,s\in\gS
\end{aligned}
\end{equation}
\cref{eq:optimization} is too complex to be solved due to the high complexity of its feasible region.
Therefore, we restrict $\set{\vg_b\mid b\in\gB}$ to the following form:
\begin{align}
\label{eq:restrict}
\vg_b=\bvg_\gS+\alpha\cdot\sign(\bvg_\gS-\bvg)\odot\vsigma_\gS,
\quad\forall b\in\gB,
\end{align}
where
$\bvg_\gS=\sum_{i\in\gS}\vg_i/(n-2f)$ is the average of the skewed honest gradients,
$\alpha$ is a non-negative real number that controls the attack strength,
$\sign(\cdot)$ returns the element-wise indication of the sign of a number,
$\odot$ is the element-wise multiplication,
and $\vsigma_\gS$ is the element-wise standard deviation of skewed honest gradients $\set{\vg_i\mid i\in\gS}$.
$\bvg_\gS$ lies within the feasible region of \cref{eq:optimization}, which ensures that $\set{\vg_b\mid b\in\gB}$ are feasible when $\alpha=0$.
$\sign(\bvg_\gS-\bvg)$ controls the element-wise attack direction, and ensures that $\vg_b$ is farther away from the optimal gradient $\bvg$ under a larger $\alpha$.
$\vsigma_\gS$ controls the element-wise attack strength and ensures that Byzantine gradients are covert in each dimension.
\begin{table*}[t]
\begin{center}
{
\small

\begin{tabular}{lccccccc}																													
\toprule																													
\multicolumn{8}{c}{CIFAR-10}																													\\
\midrule																													
Attack	&	Multi-Krum			&	Median			&	RFA			&	Aksel			&	CClip			&	DnC			&	RBTM			\\
\midrule																													
BitFlip	&	54.76	$\pm$	0.06	&	53.73	$\pm$	2.05	&	56.04	$\pm$	3.13	&	51.99	$\pm$	2.04	&	54.44	$\pm$	0.46	&	60.81	$\pm$	0.56	&	55.21	$\pm$	3.72	\\
LIE	&	57.89	$\pm$	0.22	&	49.20	$\pm$	3.27	&	53.90	$\pm$	5.43	&	46.73	$\pm$	4.86	&	63.11	$\pm$	0.43	&	61.58	$\pm$	2.85	&	58.84	$\pm$	0.64	\\
IPM	&	47.55	$\pm$	1.75	&	51.68	$\pm$	1.85	&	55.36	$\pm$	2.10	&	56.85	$\pm$	2.07	&	58.75	$\pm$	5.59	&	62.30	$\pm$	3.60	&	48.43	$\pm$	0.17	\\
MinMax	&	59.44	$\pm$	3.41	&	57.27	$\pm$	0.63	&	60.20	$\pm$	1.63	&	57.17	$\pm$	5.50	&	59.38	$\pm$	5.15	&	62.53	$\pm$	2.67	&	57.72	$\pm$	2.94	\\
MinSum	&	55.47	$\pm$	1.70	&	52.27	$\pm$	0.53	&	54.59	$\pm$	2.38	&	56.43	$\pm$	1.74	&	54.70	$\pm$	1.96	&	61.89	$\pm$	1.62	&	46.78	$\pm$	0.32	\\
Mimic	&	56.00	$\pm$	4.26	&	52.55	$\pm$	0.89	&	53.61	$\pm$	0.86	&	57.19	$\pm$	2.50	&	51.00	$\pm$	0.11	&	62.10	$\pm$	5.22	&	46.77	$\pm$	2.52	\\
\SKEW (Ours)	&	\textbf{42.90}	$\pm$	1.97	&	\textbf{48.29}	$\pm$	0.40	&	\textbf{52.92}	$\pm$	1.75	&	\textbf{38.31}	$\pm$	0.47	&	\textbf{50.67}	$\pm$	0.27	&	\textbf{59.16}	$\pm$	1.84	&	\textbf{44.82}	$\pm$	0.97	\\
\midrule																													
\multicolumn{8}{c}{ImageNet-12}																													\\
\midrule																													
Attack	&	Multi-Krum			&	Median			&	RFA			&	Aksel			&	CClip			&	DnC			&	RBTM			\\
\midrule																													
BitFlip	&	59.62	$\pm$	0.73	&	58.56	$\pm$	4.80	&	59.71	$\pm$	5.00	&	61.64	$\pm$	1.98	&	14.87	$\pm$	1.58	&	59.78	$\pm$	1.50	&	58.49	$\pm$	1.99	\\
LIE	&	62.66	$\pm$	0.30	&	51.41	$\pm$	1.52	&	60.99	$\pm$	1.22	&	54.14	$\pm$	3.14	&	16.19	$\pm$	3.95	&	67.85	$\pm$	2.87	&	67.12	$\pm$	0.39	\\
IPM	&	52.66	$\pm$	2.01	&	59.20	$\pm$	2.44	&	61.25	$\pm$	0.62	&	59.17	$\pm$	1.27	&	14.33	$\pm$	5.95	&	66.31	$\pm$	3.60	&	55.93	$\pm$	0.57	\\
MinMax	&	68.17	$\pm$	1.91	&	67.76	$\pm$	0.07	&	63.05	$\pm$	0.75	&	59.33	$\pm$	3.85	&	20.99	$\pm$	3.07	&	68.05	$\pm$	1.59	&	65.99	$\pm$	1.26	\\
MinSum	&	57.50	$\pm$	3.09	&	58.78	$\pm$	2.10	&	64.04	$\pm$	0.69	&	67.15	$\pm$	0.32	&	16.38	$\pm$	2.70	&	68.69	$\pm$	1.18	&	61.70	$\pm$	1.62	\\
Mimic	&	66.86	$\pm$	0.04	&	59.39	$\pm$	6.07	&	60.45	$\pm$	7.09	&	58.94	$\pm$	1.27	&	11.35	$\pm$	2.26	&	69.07	$\pm$	4.69	&	55.26	$\pm$	1.30	\\
\SKEW (Ours)	&	\textbf{27.24}	$\pm$	1.63	&	\textbf{42.98}	$\pm$	1.62	&	\textbf{43.30}	$\pm$	3.13	&	\textbf{38.11}	$\pm$	1.02	&	\textbf{8.33}	$\pm$	1.85	&	\textbf{53.40}	$\pm$	4.94	&	\textbf{38.81}	$\pm$	0.65	\\
\midrule																													
\multicolumn{8}{c}{FEMNIST}																													\\
\midrule																													
Attack	&	Multi-Krum			&	Median			&	RFA			&	Aksel			&	CClip			&	DnC			&	RBTM			\\
\midrule																													
BitFlip	&	82.67	$\pm$	5.13	&	71.57	$\pm$	3.61	&	83.41	$\pm$	4.33	&	81.42	$\pm$	3.45	&	83.85	$\pm$	8.50	&	83.58	$\pm$	5.20	&	82.58	$\pm$	6.08	\\
LIE	&	68.11	$\pm$	6.86	&	58.38	$\pm$	7.06	&	66.19	$\pm$	7.93	&	38.48	$\pm$	3.32	&	73.03	$\pm$	3.86	&	77.42	$\pm$	5.60	&	53.35	$\pm$	5.17	\\
IPM	&	84.12	$\pm$	3.06	&	72.60	$\pm$	8.42	&	83.42	$\pm$	4.13	&	78.28	$\pm$	7.37	&	84.93	$\pm$	4.41	&	83.03	$\pm$	5.02	&	83.21	$\pm$	6.42	\\
MinMax	&	68.42	$\pm$	5.91	&	66.44	$\pm$	5.88	&	71.55	$\pm$	5.98	&	34.22	$\pm$	4.94	&	72.12	$\pm$	4.39	&	75.40	$\pm$	3.78	&	59.23	$\pm$	3.41	\\
MinSum	&	62.06	$\pm$	3.13	&	65.46	$\pm$	3.66	&	70.36	$\pm$	7.24	&	44.91	$\pm$	3.90	&	75.40	$\pm$	4.88	&	77.11	$\pm$	3.61	&	68.10	$\pm$	8.86	\\
Mimic	&	83.15	$\pm$	3.46	&	74.00	$\pm$	4.79	&	83.87	$\pm$	3.00	&	79.06	$\pm$	7.21	&	83.94	$\pm$	5.25	&	82.22	$\pm$	5.40	&	81.92	$\pm$	3.40	\\
\SKEW (Ours)	&	\textbf{22.13}	$\pm$	7.78	&	\textbf{55.19}	$\pm$	3.49	&	\textbf{39.43}	$\pm$	5.06	&	\textbf{16.58}	$\pm$	3.63	&	\textbf{18.88}	$\pm$	4.30	&	\textbf{17.56}	$\pm$	5.95	&	\textbf{39.33}	$\pm$	11.98	\\
\bottomrule																													
\end{tabular}					
}
\end{center}
\caption{
Accuracy (mean$\pm$std) under different attacks against different defenses on CIFAR-10, ImageNet-12, and FEMNIST.
The best attack performance is in bold (the \emph{lower}, the better).
}
\label{tbl:main_experiments}
\end{table*}

With the restriction in \cref{eq:restrict}, \cref{eq:optimization} can be simplified to the following optimization problem,
\begin{equation}
\label{eq:simplified}
\begin{gathered}
\max{\alpha}\\
\text{s.t.}\quad
\norm{\bvg_\gS+\alpha\cdot\sign(\bvg_\gS)\odot\vsigma_\gS-\vg_s}\le\max_{i,j\in\gS}{\norm{\vg_i-\vg_j}},\\
\forall s\in\gS,
\end{gathered}
\end{equation}
which can be easily solved by the bisection method described in \cref{appsec:bisection}.
While $\alpha$ that solves \cref{eq:simplified} is provable in most cases, we find in practice that a slightly adjusted attack strength can further improve the effect of \SKEW.
We use an additional hyperparameter $\nu(>0)$ to control the attack strength of \SKEW.
\SKEW sets $\vg_b=\bvg_\gS+\nu\alpha\cdot\sign(\bvg_\gS)\odot\vsigma_\gS-\vg_i$ for all $b\in\gB$ and uploads Byzantine gradients to the server.
Higher $\nu$ implies higher attack strength.
We discuss the performance of \SKEW with different $\nu$ in \cref{appsec:nus}.

\section{Experiments}
We conduct all experiments on the same workstation with 8 Intel(R) Xeon(R) Platinum 8336C CPUs, a NVIDIA Tesla V100, and 64GB main memory running Linux platform.

\subsection{Experimental Setups}
\label{subsec:experimental_setups}

\textbf{Datasets.} Our experiments are conducted on three real-world datasets: CIFAR-10 \cite{krizhevsky2009cifar}, a subset of ImageNet \cite{russakovsky2015imagenet} refered as ImageNet-12 \cite{li2021imagenet12} and FEMNIST \cite{caldas2018leaf}. Please refer to \cref{appsec:data_distribution} for more details about the data distribution.

More detailed setups are deferred to \cref{appsec:main_setups}.

\textbf{Baseline attacks.} 
We consider six state-of-the-art attacks:
BitFlip \cite{allen2020safeguard},
LIE \cite{baruch2019lie},
IPM \cite{xie2020ipm},
Min-Max \cite{shejwalkar2021dnc},
Min-Sum \cite{shejwalkar2021dnc},
and Mimic \cite{karimireddy2022bucketing}.
Among the above six attacks, BitFlip and LabelFlip are popular agnostic attacks;
LIE, Min-Max and Min-Sum are partial knowledge attacks;
IPM is an omniscient attack.
The detailed introduction and hyperparameter settings of these attacks are shown in \cref{appsec:attack_hyperparam}.

\begin{figure*}[t]
    % \centering
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_krum-1.jpg}
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_median-1.jpg}
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_rfa-1.jpg}
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_aksel-1.jpg}
    \\
    \hfill\\
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_cc-1.jpg}
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_dnc-1.jpg}
    \includegraphics[width=.24\linewidth,align=c]{figures_cymk/bucket_trmean-1.jpg}
    \hspace{.05\linewidth}
    \includegraphics[width=.15\linewidth,align=c]{figures_cymk/bucket_legend-1.jpg}
    \\
    \caption{
    Accuracy under different attacks against seven robust AGRs with bucketing on ImageNet-12.
    The \emph{lower}, the better.
    }
    \label{fig:bucket_results}
\end{figure*}


\textbf{Evaluated defenses.}
We evaluate the performance of our attack on the following robust AGRs:
Multi-Krum \cite{blanchard2017krum},
Median \cite{yin2018mediantrmean},
RFA \cite{pillutla2019geometric},
Aksel \cite{boussetta2021aksel},
CClip \cite{karimireddy2021cc}
DnC \cite{shejwalkar2021dnc},
and RBTM \cite{el2021collaborative}.
Besides, we also consider bucketing \cite{karimireddy2022bucketing} and NNM \cite{allouah2023nnm}, two simple yet effective schemes that adapt existing robust AGRs to the non-IID setting.
The detailed hyperparameter settings of the above robust AGRs are listed in \cref{appsec:defense_hyperparams}.

\subsection{Experiment Results}
\label{subsec:results}

\textbf{Attacking against various robust AGRs.}
\cref{tbl:main_experiments} demonstrates the performance of seven different attacks against seven robust AGRs on CIFAR-10, ImageNet-12, and FEMNIST datasets.
From \cref{tbl:main_experiments}, we can observe that our \SKEW attack generally outperforms all the baseline attacks against various defenses on all datasets, verifying the efficacy of our \SKEW attack.
On ImageNet-12 and FEMNIST, the improvement of \SKEW over the best baselines is more significant.
We hypothesize that this is because the skew degree is higher on ImageNet-12 and FEMNIST compared to CIFAR-10.
Since \SKEW exploits gradient skew to launch Byzantine attacks, it is more effective on ImageNet-12 and FEMNIST.
DnC demonstrates almost the strongest resilience to previous baseline attacks.
This is because these attacks fail to be aware of the skew nature of honest gradients in FL.
By contrast, our \SKEW attack can take advantage of gradient skew and circumvent DnC defense.
The above observations clearly validate the superiority of \SKEW.


\textbf{Attacking against robust AGRs with bucketing.}
\cref{fig:bucket_results} demonstrates the performance of seven different attacks against the bucketing scheme \cite{karimireddy2022bucketing} with different robust AGRs.
The results demonstrate that our \SKEW attack works best against Multi-Krum, RFA, and Aksel.
When attacking against DnC, Median, and RBTM, only MinSum attack is comparable to our \SKEW attack.

\textbf{Attacking against robust AGRs with NNM.}
\cref{tab:nnm} compare the performance of STRKE attack against top-3 strongest attacks against the NNM scheme \cite{karimireddy2022bucketing} under the top-3 most robust robust AGRs.
The results suggest that the proposed STRIKE attack still outperforms other baseline attacks against NNM.

\begin{table}[t]
\centering
{\small
\begin{tabular}{lccc}							
\toprule							
Attack	&	NNM + Median	&	NNM + RFA	&	NNM + DnC	\\
\midrule							
BitFlip	&	57.14	&	58.55	&	53.68	\\
LIE	&	58.04	&	58.68	&	58.87	\\
Mimic	&	66.15	&	67.43	&	69.35	\\
STRIKE	&	\textbf{39.61}	&	\textbf{40.38}	&	\textbf{38.91}	\\
\bottomrule							
\end{tabular}							
}
\caption{Accuracy under different attacks against NNM on ImageNet-12. The best results are in bold (The \emph{lower}, the better).}
\label{tab:nnm}
\end{table}

\textbf{Imparct of $\nu$ on \SKEW attack.}
We study the influence of $\nu$ on ImageNet-12 dataset. We report the test accuracy under \SKEW attack with $\nu$ in $\{0.25 * i\mid i=1,\ldots,8\}$ against seven different defenses on ImageNet-12.
As shown in \cref{fig:nus_results}, the performance of \SKEW is generally competitive with varying $\nu$.
In most cases, simply setting $\nu=1$ can beat almost all the attacks (except for CClip, yet we observe that the performance is low enough to make the model useless).

\textbf{The effectiveness of \SKEW attack under different non-IID levels.}
We vary Dirichlet concentration parameter $\beta$ in $\set{0.1, 0.2, 0.5, 0.7, 0.9}$ to study how our attack behaves under different non-IID levels.
We additionally test the performance in the IID setting.
As shown in \cref{fig:niid_level}, the accuracy generally increases as $\beta$ decreases for all attacks.
The accuracy under our \SKEW attack is consistently lower than that of all the baseline attacks.
Besides, we also note that the accuracy gap between our \SKEW attack and other baseline attacks gets smaller when the non-IID level decreases.
We hypothesize the reason is that gradient skew becomes milder as the non-IID level decreases.
Even in the IID setting, our \SKEW attack is competitive compared to other baselines.

\textbf{The performance of \SKEW attack with different Byzantine client ratios.}
We vary the number of Byzantine clients $f$ in $\set{5, 10, 15, 20}$ and fix the total number of clients $n$ to be $50$.
In this way, Byzantine client ratio $f/n$ varies in $\set{0.1, 0.2, 0.3, 0.4}$ to study how our attack behaves under different Byzantine client ratios.
As shown in \cref{fig:byz_ratio}, the accuracy generally decreases as $f/n$ increases for all attacks.
The accuracy under our \SKEW attack is consistently lower than that under all the baseline attacks.


\section{Conclusion}
In this paper, we observe the existence of the gradient skew phenomenon due to non-IID data distribution in FL.
Based on the observation, we propose a novel attack called \SKEW that can exploit the vulnerability.
Generally, \SKEW hides Byzantine gradients within the skewed honest gradients.
To this end, \SKEW first searches for the skewed honest gradients, and then constructs Byzantine gradients within the skewed honest gradients by solving a constrained optimization problem.
Empirical studies on three real-world datasets confirm the efficacy of our \SKEW attack.
The STRIKE relies on the gradient skew phenomenon, which is closely related to non-IIDness of data distribution.
When the data is IID, the performance could be limited.
Therefore, defenses that can alleviate non-IID can potentially mitigate our STRIKE attack.
In our future works, we will explore potential defenses against this threat.

\section*{Ethical Statement}

The proposed skew-aware Byzantine attack \SKEW can present a threat to federated learning.
Our goal with this work is thus to preempt these harms and encourage Byzantine defenses that are robust to skew-aware attacks in the future.

\section*{Acknowledgements}
This work is funded by National Key Research and Development Project (Grant No: 2022YFB2703100) and by the Pioneer R\&D Program of Zhejiang (No.2024C01021).
This work is also sponsored by Sony AI.

\bibliography{aaai25}

\appendix
\onecolumn
\section{Visualization of Gradient Skew}
\label{appsec:vis}

In order to gain insight into the gradient distribution, we use Locally Linear Embedding (LLE) \footnote{
Compared to LLE, t-SNE \cite{van2008tsne} is a more popular visualization technique.
Since t-SNE adjusts Gaussian bandwidth to locally normalize the density of data points, t-SNE can not capture the distance information of data. 
However, gradient skew relies heavily on distance information.
Therefore, t-SNE is not appropriate for the visualization of gradient skew.
In contrast, LLE can preserve the distance information of data distribution.
} \cite{roweis2000lle} to visualize the gradients.
From the visualization results, we observe that the distribution of gradient is skewed throughout FL training process when the data across different clients is non-IID.
In this section, we first provide the detailed experimental setups of the observation experiments and then present the visualization results.

\subsection{Experimental Setups}
\label{appsec:vis_setting}

For CIFAR-10, we set the number of clients $n=100$ and the Dirichlet concentration parameter $\beta=0.1$.
For ImageNet-12, we set the number of clients $n=50$ and the Dirichlet concentration parameter $\beta=0.1$.
For FEMNIST, we adopt its natural data partition as introduced in \cref{subsec:experimental_setups}.
For all three datasets, we set the number of Byzantine clients $f=0$.
For CIFAR-10 and FEMNIST, we sample 100 clients to participate in training in each communication round.
More visualized gradients would help us capture the characteristic of gradient distribution.
For ImageNet-12, we sample 50 clients in each communication round.
This is because we train ResNet-18 on ImageNet-12 and LLE on 100 gradients of ResNet-18 would be intractable due to the high dimensionality.
Other setups align with \cref{apptable:default_setup}.

For LLE, we set the number of neighbors to be $k=0.1m$, where $m$ is the number of sampled clients, to capture both local and global geometry of gradient distribution.

\subsection{Gradient Visualization Results}
\label{appsec:vis_results}

On each dataset, we run FedAvg for $T$ communication round.
Among the total $T$ communication rounds, we randomly sample 6 rounds for visualization.
For each round, we use LLE to visualize all the gradients and the optimal gradient (the average of all gradients) in this round.
Please note that LLE is not linear.
Therefore, the optimal gradient after the LLE may not be the average of all uploaded gradients after LLE.
The visualization results are posted in \cref{fig:vis_results} below.
In \cref{fig:vis_results}, the substantial gradients skew away from the optimal gradient.
These results imply that the gradient distribution is skewed during the entire training process.

We also visualize the Byzantine gradients together with honest gradients under STRIKE attack against Median AGR on CIFAR-10 in the non-IID setting in \cref{fig:pearson_skew}.
The visualization shows that Byzantine gradients can hide within the skewed honest gradients well.
This justifies that the heuristic search in the first stage of STRIKE attack can effectively find the skewed honest gradients.
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{figures_cymk/visskew_imagenet12-1.jpg}
    \includegraphics[width=.9\linewidth]{figures_cymk/visskew_femnist-1.jpg}
    \includegraphics[width=.7\linewidth]{figures_cymk/new_vis_legend-1.jpg}
    \caption{Visualization of gradient skew on ImageNet-12 and FEMNIST}
    \label{fig:vis_results}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{figures_cymk/pearson_justification-1.jpg}
    \caption{Visualization of STRIKE attack on CIFAR-10 datasets. The visualization shows that Byzantine gradients can hide within the skewed honest gradients well, which justifies that the heuristic search in the first stage of STRIKE attack can effectively find the skewed honest gradients.}
    \label{fig:pearson_skew}
\end{figure}

\section{Algorithm of the Proposed STRKE Attack}
\label{appsec:algorithm}

\begin{algorithm}[H]
\caption{\SKEW Attack}
\label{alg:proposed}
\begin{algorithmic}
\REQUIRE Honest gradients $\set{\vg_i\mid i\in\gH}$,
hyperparameter $\nu>0$ that controls attack strength (default $\nu=1$)
\STATE $\vg_{\text{med}}\gets\text{Coordinate-wise median of }\set{\vg_i\mid i\in\gH}$
\COMMENT{\# Stage 1: search for the skewed densely distributed gradients}
\STATE $\vu_\text{search}\gets\vg_{\text{med}}-\bvg$
\FOR{$i\in\gH$}
    \STATE $p_i \gets \ip{\vg_i}{\vu_\text{search}/\norm{\vu_\text{search}}}$
\ENDFOR
\STATE $\gS\gets\text{Set of }n-f\text{ indices of honest gradients with the highest }p_i$
\STATE $\bvg_\gS\gets\sum_{i\in\gS}\vg_i/(n-2f)$
\COMMENT{\# Stage 2: hide Byzantine gradients within the skewed densely distributed gradients}
\STATE $\vsigma_\gS\gets\text{Coordinate-wise standard deviation of }\set{\vg_i\mid i\in\gS}$
\STATE solve \cref{eq:simplified} for $\alpha$
\FOR{$b\in\gB$}
    \STATE $\vg_b\gets\bvg_\gS+\nu\alpha\cdot\sign(\bvg_\gS-\bvg)\odot\vsigma_\gS$
\ENDFOR
\STATE \textbf{return} Byzantine gradients $\set{\vg_b\mid g\in\gB}$
\end{algorithmic}
\end{algorithm}


\section{Bisection Method to Solve \cref{eq:simplified}}
\label{appsec:bisection}
In this section, we present the bisection method used to solve \cref{eq:simplified}.
We define $f(\cdot)$ as follows.
\begin{align}
f(\alpha)=\max_{i\in\gS}\norm{\bvg_\gS+\alpha\cdot\sign(\bvg_\gS)\odot\vsigma_\gS-\vg_i}-\max_{i,j\in\gS}{\norm{\vg_i-\vg_j}},
\quad\alpha\in[0,+\infty).
\end{align}
We can easily verify the following facts: 1. $f(0)\le0$, $f(\alpha)\rightarrow+\infty$ when $\alpha\rightarrow+\infty$;
2. $f(\cdot)$ is continuous;
3. $f(\cdot)$ has unique zero point in $[0,+\infty)$.
Therefore, optimizing \cref{eq:simplified} is equivalent to finding the zero point of $f(\cdot)$, which can be easily solved by bisection method in \cref{alg:bisection}.

\begin{algorithm}[ht]
\caption{Bisection method}
\label{alg:bisection}
\begin{algorithmic}
\REQUIRE The skewed densely distributed honest gradients $\set{\vg_i\mid i\in\gS}$, tolerance $\varepsilon>0$, max iteration $M>0$
\STATE $\alpha_{\min}\gets0$
\STATE $\alpha_{\max}\gets1$
\WHILE{$f(\alpha)<0$}
\STATE $\alpha_{\max}\gets2\alpha_{\max}$
\ENDWHILE
\STATE $iter\gets0$
\WHILE{$\alpha_{\max}-\alpha_{\min}>\varepsilon$ and $iter<M$}
\STATE $\alpha_\text{mid}\gets(\alpha_{\max}+\alpha_{\min})/2$
\IF{$f(\alpha_\text{mid})<0$}
\STATE $\alpha_{\min}\gets\alpha_\text{mid}$
\ELSE
\STATE $\alpha_{\max}\gets\alpha_\text{mid}$
\ENDIF
\STATE $iter\gets iter+1$
\ENDWHILE
\STATE $\alpha\gets(\alpha_{\max}+\alpha_{\min})/2$
\STATE \textbf{return} $\alpha$
\end{algorithmic}
\end{algorithm}

Theoretically, the final optimization problem of the proposed STRIKE attack in Eq. (19) can be effectively solved by a bisection algorithm as discussed in Appendix C. The computation cost is $\mathcal{O}(-\log\epsilon)$, where $\epsilon$ is the error of $\alpha$. In experiments, we perform bisection only 8 times and make the error of $\alpha$ within $1\%$. Empirically, the attack time for STRIKE is 12.11s (13.47s for MinMax, 13.35s for MinSum) on ImageNet-12.

In contrast, benign clients perform local updates to compute local gradients. The computation cost depends on the local data size, model architecture, batch size, number of local epochs, etc. In our setting, the average local update time is 15.14s on CIFAR-10, 11.76s on ImageNet-12 and 27.13s on FEMNIST. Tests are performed on a single A100 GPU.

\section{Experimental Setups and Additional Experiments}
\label{appsec:experiments}
\subsection{Experimental Setups}
\label{appsec:main_setups}
\subsubsection{Data Distribution}
\label{appsec:data_distribution}
For CIFAR-10 \cite{krizhevsky2009cifar} and ImageNet-12, we use Dirichlet distribution to generate non-IID data by following \cite{yurochkin2019bayesian, li2021federated}. 
% In particular, for each client $i$, we sample $p_i^y \sim \text{Dir}(\beta)$ and allocate a $p^y_i$ proportion of the data of label $y$ to client $i$, where $\text{Dir}(\beta)$ represents the Dirichlet distribution with a concentration parameter $\beta$.
For each class $c$, we sample $\vq_c\sim\text{Dir}_n(\beta)$ and allocate a $(\vq_c)_i$ portion of training samples of class $c$ to client $i$.
Here, $\text{Dir}_n(\cdot)$ denotes the $n$-dimensional Dirichlet distribution, and $\beta>0$ is a concentration parameter.
We follow \cite{li2021federated} and set the number of clients $n=50$ and the concentration parameter $\beta=0.5$ as default.

For FEMNIST, the data is naturally partitioned into 3,597 clients based on the writer of the digit/character.
Thus, the data distribution across different clients is naturally non-IID.
For each client, we randomly sample a 0.9 portion of data as the training data and let the remaining 0.1 portion of data be the test data following \cite{caldas2018leaf}.

\subsubsection{Hyperparameter Setting of Baselines Attacks}
\label{appsec:attack_hyperparam}
The compared baseline attacks are: 
BitFlip \cite{allen2020safeguard},
LIE \cite{baruch2019lie},
IPM \cite{xie2020ipm},
Min-Max \cite{shejwalkar2021dnc},
Min-Sum \cite{shejwalkar2021dnc},
and Mimic \cite{karimireddy2022bucketing}.
\begin{itemize}
    \item \textbf{BitFlip.} A Byzantine client sends $-\nabla\gL_i(\vw_t)$ instead of $\nabla\gL_i(\vw_t)$ to the server.
    \item \textbf{LabelFlip.} Corrupt the datasets by transforming labels by $\mathcal{T}(y)=c-1-y$, where $c$ is the number of classes, $y$ is the label of a sample.
    \item \textbf{LIE.} The Byzantine clients estimate the mean $\vmu$ and element-wise standard deviation $\vsigma$ of the honest gradients, and send $\vmu-z\vsigma$ to the server, where $z$ controls the attack strength.
    \item \textbf{Min-Max.} The Byzantines first estimate the mean $\vmu$ of the honest gradients and select a perturbation vector $\vdelta$.
    Then, the Byzantines find scaling coefficient $\gamma$ by solving a maximizing-maximum-distance optimization problem and send $\vmu+\gamma\vdelta$ to the server.
    \item \textbf{Min-Sum.} Similar to Min-Max, but the Byzantine clients solve a maximizing-distance-sum optimization problem to determine the scaling coefficient $\gamma$ instead.
    \item \textbf{IPM.} The Byzantines send $-\varepsilon\bvg^t$ to the server, where $\varepsilon$ controls the attack strength.
    The Byzantines evaluate $\varepsilon$ based on the honest gradients and the server-side robust AGR to maximize the attack effect.
\end{itemize}
The hyperparameter setting of the above attacks is listed in the following table.
\begin{table}[ht]
\caption{
The hyperparameter setting of six baseline attacks. 
N/A represents there is no hyperparameter required for this attack.
}
\label{apptbl:attack_hyperparams}
\begin{center}
\begin{tabular}{ll}
\toprule
Attacks & Hyperparameters \\
\midrule
BitFlip & N/A \\
LIE & $z=1.5$ \\
IPM & $\varepsilon=0.1$\\
Min-Max & $\gamma_{\text{init}}=10,\tau=1\times10^{-5}$, $\nabla^p$: coordinate-wise standard deviation\\
Min-Sum & $\gamma_{\text{init}}=10,\tau=1\times10^{-5}$, $\nabla^p$: coordinate-wise standard deviation \\
Mimic & N/A \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{The Hyperparameter Setting of Evaluated Defenses}
\label{appsec:defense_hyperparams}
The performance of our attack is evaluated on seven recent robust defenses:
Multi-Krum \cite{blanchard2017krum},
Median \cite{yin2018mediantrmean},
RFA \cite{pillutla2019geometric},
Aksel \cite{boussetta2021aksel},
CClip\cite{karimireddy2021cc}
DnC \cite{shejwalkar2021dnc},
and RBTM \cite{el2021collaborative}.
The hyperparameter setting of the above defenses is listed in the following table.
\begin{table}[ht]
\caption{
The hyperparameter setting of seven evaluated defenses.
N/A represents there is no hyperparameter required for this defense.
}
\label{apptbl:defense_hyperparams}
\begin{center}
\begin{tabular}{ll}
\toprule
Defenses & Hyperparameters \\
\midrule
Multi-Krum & N/A \\
Median & N/A \\
RFA & $T=8$ \\
Aksel & N/A \\
CClip & $L=1,\tau=10$ \\
DnC & $c=1, \textsf{niters}=1, b=1000$ \\
RBTM & N/A \\
\bottomrule
\end{tabular}
\end{center}
\end{table}
we also consider a simple yet effective bucketing scheme \cite{karimireddy2022bucketing} that adapts existing defenses to the non-IID setting.
We follow the original paper and set the bucket size to be $s=2$.

\subsubsection{Evaluation}
We use top-1 accuracy, i.e., the proportion of correctly predicted testing samples to total testing samples, to evaluate the performance of global models.
The \emph{lower} the accuracy, the more effective the attack.
We run each experiment five times and report the mean and standard deviation of the highest accuracy during the training process.

\subsubsection{Compute}
All experiments are run on the same machine with Intel E5-2665 CPU, 32GB RAM, and four GeForce GTX 1080Ti GPU.

\subsubsection{Other Setups}
\label{appsec:other_setting}
The number of Byzantine clients of all datasets is set to $f=0.2\cdot n$.
We test \SKEW with $\nu\in\set{ 0.25\cdot i\mid i=1,\ldots,8}$ and report the lowest test accuracy (highest attack effectiveness).

The hyperparameter setting for datasets FEMNIST \cite{caldas2018leaf}, CIFAR-10 \cite{krizhevsky2009cifar} and ImageNet-12 \cite{russakovsky2015imagenet} are listed in below \cref{apptable:default_setup}.
\begin{table}[H]
\caption{Hyperparameter setting for FEMNIST, CIFAR-10 and ImageNet-12.
\# is the number sign. For example, \# Communication rounds represents the number of communication rounds.}
\label{apptable:default_setup}
\begin{center}
% \resizebox{.7\textwidth}{!}{
\begin{tabular}{llll}
\toprule 
Dataset & FEMNIST & CIFAR-10 & ImageNet-12 \\
Architecture & \makecell[l]{CNN \\ \citeauthor{caldas2018leaf}} & \makecell[l]{AlexNet \\ \cite{krizhevsky2017alexnet}} & \makecell[l]{ResNet-18 \\ \cite{he2016resnet}} \\
\midrule
\makecell[l]{\# Communication \\ rounds} & 800 & 200 & 200 \\
\# Sampled Clients & 10 & 50 & 50 \\
\midrule
\# Local epochs & 1 & 1 & 1 \\
Optimizer & SGD & SGD & SGD \\
Batch size & 128 & 128 & 128 \\
Learning rate & 0.5 & 0.1 & 0.1 \\
Momentum & 0.5 & 0.9 & 0.9 \\
Weight decay & 0.0001 & 0.0001 & 0.0001 \\
Gradient clipping & Yes & Yes & Yes \\
Clipping norm & 2 & 2 & 2 \\
\bottomrule
\end{tabular}
% }
\end{center}
\end{table}

% \textbf{Implementation details of \SKEW.}
% In experiments, we use an additional hyperparameter $\nu>0$ to control the attack strength of \SKEW.
% \cref{eq:simplified} with strength control becomes the following optimization problem.
% \begin{align}
% \label{eq:control}
% \begin{split}
% &\max{\alpha}\\
% \text{s.t.}\quad
% &\norm{\bvg_\gS+\frac{\alpha}{\nu}\cdot\sign(\bvg_\gS)\odot\vsigma_\gS-\vg_i}\le\max_{i,j\in\gS}{\norm{\vg_i-\vg_j}},\quad\forall i\in\gS,
% \end{split}
% \end{align}
% A larger $\nu$ implies a stronger \SKEW attack.
% When $\nu=1$, \cref{eq:control} is degenerate to \cref{eq:simplified}.
% In experiments, we test \SKEW with $\nu\in\set{ 0.25\cdot i\mid i=1,\ldots,8}$ and report the lowest test accuracy (highest attack effectiveness).


\subsection{Additional Experiments}

\subsubsection{Performance under Varying Hyperparameter $\nu$}
\label{appsec:nus}
We study the influence of $\nu$ on ImageNet-12 dataset. We report the test accuracy under \SKEW attack with $\nu$ in $\{0.25 * i\mid i=1,\ldots,8\}$ against seven different defenses on ImageNet-12 in \cref{fig:nus_results}.
We also report the lowest test accuracy (best performance) of six baseline attacks introduced in \cref{subsec:experimental_setups} as a reference.
Please note that a \emph{lower} accuracy implies higher attack effectiveness.

As shown in the \cref{fig:nus_results}, the performance of \SKEW is generally competitive with varying $\nu$.
In most cases, simply setting $\nu=1$ can beat other attacks (except for CClip, yet we observe that the performance is low enough to make the model useless).
The impact of $\nu$ value is different for different robust AGRs:
for Median and RFA, the accuracy is relatively stable under different $\nu$s;
for CClip and Multi-Krum, the accuracy is lower with larger $\nu$s;
for Aksel and DnC, the accuracy first decreases and then increases as $\nu$ increases.

\begin{figure}[ht]
    % \centering
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_krum-1.jpg}
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_median-1.jpg}
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_rfa-1.jpg}
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_aksel-1.jpg}
    \\
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_cc-1.jpg}
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_dnc-1.jpg}
    \includegraphics[width=.24\linewidth, align=c]{figures_cymk/nus_trmean-1.jpg}
    \hspace{.05\linewidth}
    \includegraphics[width=.15\linewidth, align=c]{figures_cymk/nus_legend-1.jpg}
    
    \caption{
    Accuracy under \SKEW attack with $\nu$ in $\{0.25 * i\mid i=1,\ldots,8\}$ against seven different defenses on ImageNet-12.
    The gray dashed line in each figure represents the lowest test accuracy (best performance) of six baseline attacks introduced in \cref{subsec:experimental_setups}.
    We include it as a reference.
    The \emph{lower} the accuracy, the more effective the attack.
    Other experimental setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
    }
    \label{fig:nus_results}
\end{figure}

\subsubsection{Performance under Different Non-IID Levels}
\label{appsec:niidlevel}
As shown in \cref{tbl:main_experiments}, DnC demonstrates the strongest robustness against various attacks on all datasets.
Therefore, we fix the defense to be DnC in this experiment.
As discussed in \cref{appsec:nus}, simply setting $\nu=1$ yields satisfactory performance of our \SKEW attack.
Thus, we fix $\nu=1$ in this experiment.
We vary Dirichlet concentration parameter $\beta$ in $\set{0.1, 0.2, 0.5, 0.7, 0.9}$ to study how our attack behaves under different non-IID levels.
Lower $\beta$ implies a higher non-IID level.
We additionally test the performance in the IID setting.
Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
The results are posted in \cref{fig:niid_level} below.

As shown in \cref{fig:niid_level}, the accuracy generally increases as $\beta$ decreases for all attacks.
The accuracy under our \SKEW attack is consistently lower than all the baseline attacks.
% The results suggest that all attacks are less effective when the data is less non-IID.
% Meanwhile, our attack is the most effective under different non-IID levels.
Besides, we also note that the accuracy gap between our \SKEW attack and other baseline attacks gets smaller when the non-IID level decreases.
We hypothesize the reason is that gradient skew is milder as the non-IID level decreases.
Even in the IID setting, our \SKEW attack is competitive compared to other baselines.

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{figures_cymk/niid_level-1.jpg}
    \caption{
    Accuracy under different attacks against DnC under different non-IID levels on ImageNet12.
    Lower $\beta$ implies a higher non-IID level.
    "IID" implies that the data is IID distributed.
    The \emph{lower}, the better.
    Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
    }
    \label{fig:niid_level}
\end{figure}

\subsubsection{Performance under Different Byzantine Client Ratio}
\label{appsec:byz_ratio}
As shown in \cref{tbl:main_experiments}, DnC demonstrates the strongest robustness against various attacks on all datasets.
Therefore, we fix the defense to be DnC in this experiment.
As discussed in \cref{appsec:nus}, simply setting $\nu=1$ yields satisfactory performance of our \SKEW attack.
Thus, we fix $\nu=1$ in this experiment.
We vary the number of Byzantine clients $f$ in $\set{5, 10, 15, 20}$ and fix the total number of clients $n$ to be $50$.
In this way, Byzantine client ratio $f/n$ varies in $\set{0.1, 0.2, 0.3, 0.4}$ to study how our attack behaves under different Byzantine client ratio.
Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
The results are posted in \cref{fig:byz_ratio} below.

As shown in \cref{fig:byz_ratio}, the accuracy generally decreases as $f/n$ increases for all attacks.
The accuracy under our \SKEW attack is consistently lower than all the baseline attacks.
The results suggest that all attacks are more effective when there are more Byzantine clients.
Meanwhile, our attack is the most effective under different Byzantine client number.

\begin{figure}[H]
    \centering
    \includegraphics{figures_cymk/byz_ratio-1.jpg}
    \caption{
    Accuracy under different attacks against DnC under different Byzantine client ratio on ImageNet12.
    The \emph{lower}, the better.
    Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
    }
    \label{fig:byz_ratio}
\end{figure}
\subsubsection{Performance under Different Client Number}
\label{appsec:client_num}
As shown in \cref{tbl:main_experiments}, DnC demonstrates the strongest robustness against various attacks on all datasets.
Therefore, we fix the defense to be DnC in this experiment.
As discussed in \cref{appsec:nus}, simply setting $\nu=1$ yields satisfactory performance of our \SKEW attack.
Thus, we fix $\nu=1$ in this experiment.
We vary the number of total clients $n$ in $\set{10, 30, 50, 70, 90}$ and set the number of Byzantine clients $f=0.2n$ accordingly.
In this way, We can study how our attack behaves under different client number.
Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
The results are posted in \cref{fig:client_num} below.

As shown in \cref{fig:client_num}, the accuracy generally decreases as client number $n$ increases for all attacks.
The accuracy under our \SKEW attack is consistently lower than all the baseline attacks under different client number.

\begin{figure}[H]
    \centering
    \includegraphics{figures_cymk/client_num-1.jpg}
    \caption{
    Accuracy under different attacks against DnC under different client number on ImageNet12.
    The \emph{lower}, the better.
    Other setups align with the main experiment as introduced in \cref{subsec:experimental_setups}.
    }
    \label{fig:client_num}
\end{figure}

\end{document}
