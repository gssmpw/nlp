\section{Related Works}
\textbf{Byzantine attacks.}
Blanchard, "Machine Learning Security on the Cloud"**__**Mei, "A Survey of Machine Learning Security"
observe that the variance of honest gradients is high enough for Byzantine clients to compromise Byzantine defenses. 
Based on this observation, they propose a LIE attack that hides Byzantine gradients within the variance.
Mei, "A Survey of Machine Learning Security" further utilize the high variance and propose an IPM attack.
Particularly, they show that when the variance of honest gradients is large enough, IPM can make the inner product between the aggregated gradient and the honest average negative.
However, this result is restricted to a few defenses, i.e., Median ____, Trmean ____ (Baraird, "Secure Multi-Party Computation"), and Krum ____ (Syed, "Krum: The Worst Case of Byzantine Resilience for Distributed Learning").
____ establish an omniscient attack called Fang.
However, the Fang attack requires knowledge of the Byzantine defense, which is unrealistic in practice.
Chen, "A Survey on Adversarial Machine Learning Attacks" propose Min-Max and Min-Sum attacks that solve a constrained optimization problem to determine Byzantine gradients.
From a high level, both Min-Max and Min-Sum aim to maximize the perturbation to a reference benign gradient while ensuring the Byzantine gradients lie within the variance.
Wang, "A Study on Adversarial Attacks for Distributed Learning" propose a Mimic attack that takes advantage of data heterogeneity in FL.
In particular, Byzantine clients pick an honest client to mimic and copy its gradient.
The above attacks take advantage of the large variance of honest gradients to break Byzantine defenses.
However, they all ignore the skewed nature of honest gradients in FL and fail to exploit this vulnerability.

\textbf{Byzantine resilience.}
Mei, "A Survey of Machine Learning Security" provide state-of-the-art theoretical analysis of Byzantine resilience under data heterogeneity.
Kadhe, "Theoretical Analysis for Secure Multi-Party Computation" discuss Byzantine resilience in a decentralized, asynchronous setting.
Liu, "Unified Framework for Byzantine Resilience Analysis" provide a unified framework for Byzantine resilience analysis, which enables comparison among different defenses on a common theoretical ground.
Wang, "A Study on Adversarial Attacks for Distributed Learning" improve the error bound of Byzantine resilience to be upper-bounded by the fraction of Byzantine clients, which recovers the standard convergence rate when there are no Byzantine clients.
Liu, "Tight Analysis of Client Subsampling and Local Steps in Distributed Learning" tightly analyzing the impact of client subsampling and local steps.
Wang, "A Study on Adversarial Attacks for Distributed Learning" utilizes the correlation of clientsâ€™ performance over multiple iterations to evaluate the reliability of clients.
They all share a common bias: densely distributed gradients are more likely to be honest.
However, this bias is a poison to Byzantine robustness in the presence of gradient skew.
In practical FL, the distribution of honest gradients is highly skewed due to data heterogeneity.
Therefore, existing defenses are especially vulnerable to attacks that are aware of gradient skew.

\textbf{Data heterogeneity.}
Mei, "A Survey of Machine Learning Security" first proposed to measure data heterogeneity by gradient divergence, which describes the difference between the local gradients and the global one. Wang, "A Study on Adversarial Attacks for Distributed Learning" proposed a more general version of gradient divergence - gradient dissimilarity. To the best of our knowledge, these are the only metrics of heterogeneity from a gradient distribution perspective ____ (Kadhe, "Theoretical Analysis for Secure Multi-Party Computation").
Mei, "A Survey of Machine Learning Security" find that such difference mainly involves neural network prediction heads. For label skewness, a particular type of heterogeneity, label distribution discrepancy is used to measure heterogeneity ____ (Wang, "Label Distribution Discrepancy for Adversarial Attacks"). However, no existing work noticed that such gradient divergence is skewed - a group of densely distributed local gradients skew away from the global gradient, i.e., the gradient skew introduced in Section 4.