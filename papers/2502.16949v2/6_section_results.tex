\section{Results}
    \subsection{Hyperparameter Selection}
    \label{hyperparam_selection}
    Knowledge graphs are primarily used for link prediction tasks \cite{gregucci2023link}. Hits@10 is a popular measurement of link prediction accuracy. We train the KG models on various embedding sizes and plot the corresponding Hits@10 accuracy in Figure-\ref{fig:acc-emb}. We observe that accuracy increases as the entity embedding size grows. \revise{We keep the number of positive and negative edges equal within a batch for each model.}

    \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/emb-acc.pdf}
    \caption{Hits@10 accuracy w.r.t. embedding size \revise{for FB15K dataset}. 100 epoch training with batch size of 32768 and relation entity dimension as 8 (for TransH model)}
    \label{fig:acc-emb}
    \end{figure}
     \begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Batch-Time.pdf}
    \caption{Training time and GPU memory allocation w.r.t. Batch Size. 100 epoch training with entity dimension as 128 and relation dimension as 8 (for TransH model)}
    \label{fig:batch-corr}
    \end{figure}

    Another significant hyperparameter is the batch size. We plot model training time and GPU memory allocation for various batch sizes in Figure \ref{fig:batch-corr}. We observe that maximum CUDA memory utilization is possible when the largest batch size is used. It also corresponds to the fastest training time.


    \subsection{Training Performance}
    We measure the total training time, GPU memory allocation, CPU Cache miss, and FLOPs count for various datasets on the available models of the frameworks mentioned in subsection \ref{fw-models}.

    \subsubsection{Training Time}
    The total training time for various datasets on CPU and GPU are shown in Figure \ref{fig:total_trg_time}. Our implementation outperforms all frameworks for both CPU and GPU. The speedup is consistent for both small and large datasets. 
    

\begin{figure*}[btp]
\centering     %%% not \center
\subfigure[CPU]{\label{fig:a}\includegraphics[width=140mm]{figures/total-training-time-cpu.pdf}}
\subfigure[GPU]{\label{fig:b}\includegraphics[width=140mm]{figures/total-training-time-gpu.pdf}}
\caption{Total training time for CPU and GPU for various datasets. \revise{The slowdown factors of each framework compared to SpTransX are shown along the bars.}}
\label{fig:total_trg_time}
\end{figure*}

    SpTransX models exhibit good speedup on CPU and GPU systems. The speedups are consistent across datasets for the same model. We observe the most speedup in the TransE model. This is because, for this model, the computational bottleneck is the embedding gradient computation (see Figure \ref{fig:bottlenecks}). We eliminate this bottleneck by replacing fine-grained embedding scatter-gather with SpMM, which results in faster training time and efficient GPU memory usage (due to lower intermediate variable usage).
    
    Although TorusE uses the same scoring function, we do not observe the same amount of speedup in this model compared to TransE. This is because the primary computational bottleneck in this model is not always the embedding computation but the torus L2 dissimilarity function (marked as yellow boxes in Figure \ref{fig:bottlenecks}). 
    
    Among TransR and TransH, TransR is computationally more demanding. However, we still manage to perform better in TransR compared to TransH because the computational graph of TransH is much larger than TransR, and the embedding computation (or SpMM in our case) accounts for a lower percentage of system time compared to TransR. This means SpTransX has less impact on TransH compared to TransR.
    
    \subsubsection{GPU Memory Usage}
    Our implementations of the models take up significantly less CUDA memory than other frameworks. Table \ref{table:cuda-mem-allocation} demonstrates the average CUDA memory allocation for various frameworks and our implementations.

    \begin{table}[h]
    \caption{Average CUDA memory allocation for various models (in GB)}
    \label{table:cuda-mem-allocation}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{ccccc}
    \toprule
    Model  & SpTransX       & TorchKGE & DGL-KE & PyG   \\
    \midrule
    TransE & \textbf{5.61}  & 13.55    & 11.37  & 13.54 \\
    TransR & \textbf{13.65} & 20.42    & 30.73  & -     \\
    TransH & \textbf{0.28}  & 3.1      & -      & -     \\
    TorusE & \textbf{12.03} & 15.87    & -      & -                   \\                        
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
    \end{table}

    SpTransX is optimized for GPU memory usage by limiting the model size to tensors only necessary during the training time. Furthermore, the SpMM accounts for fewer intermediate variables, reducing the memory footprint. We observe the highest GPU memory efficiency in the TransH model, around $11\times$ more efficient than TorchKGE on average. This is because the training loop uses linear algebraic implementation (discussed in subsection \ref{transh_formulation}) and reuses several expressions to reduce unnecessary GPU memory allocation.

\subsubsection{Breakdown of Training Time}
Each model training epoch consists of loss calculation (forward propagation), gradient computation (backward call), and parameter update (optimizer step). The chart in Figure \ref{fig:trg_breakdown} shows the average breakdown of the three steps for the frameworks.

\begin{figure*}[h]
\centering     %%% not \center
\subfigure[CPU]{\label{fig:cpu-break}\includegraphics[width=80mm]{figures/breakdown-cpu.pdf}}
\hspace{5pt}
\subfigure[GPU]{\label{fig:b}\includegraphics[width=80mm]{figures/breakdown-gpu.pdf}}
\caption{Breakdown of total training time for CPU and GPU on average of 7 datasets}
\label{fig:trg_breakdown}
\end{figure*}

We observe that SparsTransX improves the average forward propagation time for both CPU and GPU. It also outperforms backward computation for all cases except in TransR with DGL-KE for both GPU and CPU. DGL-KE uses the heterograph data structure instead of a regular triplet array and updates the backward gradients manually through the DGL graph API. This results in an unusually long parameter update time for DGL-KE in the CPU. This issue is not present in GPU since DGL-KE has a separate GPU implementation. Despite the slower backward time, SpTransX outperforms DGL-KE in terms of overall training time.

\begin{table}[h]
\centering
\caption{Average FLOPs count of 7 datasets \revise{(factor of $\times 10^{10}$)}}
\label{table:flops}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccc}
\toprule
Model  & SpTransX       & TorchKGE & DGL-KE & PyG   \\
\midrule
TransE & \textbf{220}    & 483.87   & 293.06 & 483.82 \\
TransR & \textbf{567.37} & 1157.94  & 874.67 & -      \\
TransH & \textbf{9.66}   & 19.58    & -      & -      \\
TorusE & \textbf{289.99} & 387.93   & -      & -                        \\                        
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\subsubsection{FLOPs count and Cache Miss Rate}
    We measure the FLOPs count for our CPU implementation and the cache miss rate. SpTransX exhibits a lower FLOP count than other frameworks for all models on average, as shown in Table \ref{table:flops}. It uses high-performance SpMM that typically uses fewer floating-point operations than regular non-sparse implementations. This results in the lowest average FLOP count for SpTransX compared to all frameworks for all models.

    

\begin{table}[h]
\caption{Average cache miss rate of 7 datasets \revise{(in \%)}}
\label{table:cache-miss}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccc}
\toprule
Model  & SpTransX       & TorchKGE & DGL-KE & PyG   \\
\midrule
TransE    & \textbf{26.54} & 29.37         & 29.99  & 29.04 \\
TransR    & \textbf{17.02} & 19.20         & 29.54  & -     \\
TransH    & 10.43          & \textbf{9.75} & -      & -     \\
TorusE    & \textbf{21.53} & 22.94         & -      & -                 \\                        
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table \ref{table:cache-miss} lists the average cache miss rates. We observe that SpTransX performs better in all cases except for the TransH model. In this case, SparseTransX has a slightly higher cache miss rate than its peer, TorchKGE. This is because the impact of SpMM is small in the TransH model, and other operations overshadow the improved cache miss rate obtained by the efficient SpMM.

% \subsubsection{Scaling Study with Embedding}

\subsubsection{Model Accuracies}
The sparse approach does not change the computational steps and thus does not affect the model accuracy. The accuracies of our implementations are consistent with that of other models, such as TorchKGE. For 100 epochs training on WN18 datasets \revise{with a fixed learning rate of 0.0004}, SpTransX's TransE, TorusE, and TransH models receive 0.72, 0.63, and 0.59 Hits@10 scores, whereas TorchKG's models receive 0.74, 0.63, and 0.60. \revise{A more detailed evaluation (discussed in Appendix \ref{A:eval}) reveals that SpTransX achieves similar or better Hits@10 accuracy compared to TorchKGE when the training loop is equipped with a learning rate scheduler.}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\linewidth]{n16-plot.png}
%     \caption{TransE training times on COVID-19 dataset using up to 64 GPUs (16 nodes)}
%     \label{fig:distributed-trg-time}
% \end{figure}
