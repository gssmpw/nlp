\section{Background}
Knowledge graph training is performed by learning the representations or embeddings of the entities and their corresponding relations on a set of training triplets or subgraphs. Each triplet or edge (in subgraph) contains a valid combination of subject (head), predicate (relation), and object (tail). Once trained, the embeddings can illustrate their semantic meaning and structure, enabling them to effectively perform reasoning-based tasks such as link prediction and entity classification. The training typically uses machine learning techniques and involves a gradient descent algorithm. The exact forward propagation process can vary depending on the model type. Translation-based models, such as TransE, TransR, etc, are widely used due to their simple yet effective way of capturing relations between entities. The training can also be done by using bilinear methods (DistMult \cite{yang2014embedding}, RESCAL \cite{nickel2011three}), deep learning and convolution (ConvKB \cite{nguyen2017novel}), or Graph Neural Networks (R-GCN \cite{schlichtkrull2018modeling}).

Training a translation-based knowledge graph embedding typically involves taking a list of triplets (head, tail, relation index) and optimizing their corresponding embeddings to minimize the distance between the $\Vec{tail}$ and $\Vec{head} + \Vec{relation}$. The translational models vary based on (1) the linear transformation applied to the entities and relations and (2) the distance metric. The linear transformation can be applied to (a) individual entities/relations, (b) head - tail, or (c) the overall head - tail + relation. The measurement can be in a typical Euclidean space (L1 or L2) or a toroidal (wraparound) space distance (L1 torus or L2 torus) function. The training is typically done in batches, where a `batch' of head, tail, and relations are fetched for training instead of single ones.

\begin{figure}[h]
\centering     %%% not \center
\subfigure[Forward]{\label{fig:gather}\includegraphics[width=35mm]{figures/non-sparse-fw.pdf}}
\hspace{5pt}
\subfigure[Backward]{\label{fig:scatter}\includegraphics[width=35mm]{figures/non-sparse-bw.pdf}}
\caption{Scatter and Gather operation in translational KG training}
% \label{fig:total_trg_time}
\end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.90\textwidth]{figures/bottleneck.pdf}
\caption{Top three CPU intensive functions for various translation-based KGE models and datasets (indicated in bracket). The redness represents the popularity of a function among models. The dark red box indicates that the corresponding function is used in several different models. \revise{Blue/Purple} indicates that the function is typically exclusive to the current model. The dark gray box indicates the dataset loading time. The light gray box indicates the rest of the training time.}
\label{fig:bottlenecks}
\end{figure*}

The training process starts with triplets with the index position of the head, tail, and relation entities. In each epoch, embeddings are fetched from the indices, and linear transformation is applied to them to compute the final loss. This means the forward propagation involves several (typically three or more) `gather' operations (see Figure \ref{fig:gather}) that collect the index batch's head, tail, and relation embeddings. Some models also require one or more transform matrices (may be based on the relation), which are also gathered in sthis step. Consequently, the backward propagation performs the opposite, the `scatter' operations that distribute gradients across the corresponding indices (see Figure \ref{fig:scatter}) 

These individual operations, especially the gradient computations in the backward step, can take up around 40\% of the CPU's training time (see Figure \ref{fig:bottlenecks}). In particular, we observe that embedding gradient computation is among the top three CPU-intensive functions for most translational models.

