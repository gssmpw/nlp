\section{Introduction}
\label{introductoin}


% 1. Introduce KGE. 
Knowledge Graphs (KGs) are structured as directed graphs containing entities as nodes and relations as edges.
Each edge in a KG is typically stored as a triplet (head, relation, tail)—abbreviated as (h, r, t)—where head and tail are entities connected by a relation that denotes the nature of their interaction. Knowledge graph embedding (KGE) techniques map these entities and relations into a continuous vector space, enabling efficient computation and manipulation while preserving the underlying structural properties of the KG.
The entity and relation embeddings are widely used in many downstream tasks, such as KG
completion~\cite{TransE, chen2020knowledge}, entity classification~\cite{nickel2012factorizing}, and entity resolution~\cite{bordes2014semantic}.


% Introduce translational models and challenges
Translational models~\cite{TransE, transR} are a widely used and effective class of KGE methods. 
These models represent entities and relations in a continuous vector space, where relations are interpreted as translations applied to entity embeddings. 
However, training translational KGE models for large-scale KGs is computationally intensive and incurs high memory overhead, especially when large batches are used. 
These challenges are due in part to the fact that current KGE implementations represent triplets as dense matrices and rely heavily on fine-grained scatter-gather computations during training.
This fine-grained computational model contributes to the following bottlenecks: (1) irregular memory access patterns from fine-grained operations on KGs and embeddings, which increase memory access costs, (2) increased backpropagation expenses due to more granular gradient computations, and (3) significant memory demands for dense matrices.
In this paper, we address these issues by proposing a sparse-matrix representation of the KG and utilizing highly optimized sparse-matrix operations to streamline KGE training, thereby reducing both computational and memory bottlenecks.


% 3. Why linear algebra 
Expressing graph operations through sparse linear algebra has been highly effective for developing efficient and scalable graph neural networks (GNNs). As a result, popular graph machine learning libraries, such as PyTorch Geometric (PyG) \cite{fey2019fast} and DGL \cite{wang2019deep}, utilize optimized implementations of sparse-dense matrix multiplication (SpMM).
Despite the widespread success of sparse operations in GNNs, existing KGE libraries have yet to adopt sparse operations for training KGE models. Even models utilizing sparse embeddings, such as TranSparse~\cite{transSparse}, store embeddings as dense matrices, limiting their ability to fully leverage sparse matrix operations.


\begin{table}[h]
\caption{200 epoch training time breakdown of a TransE model using the sparse approach compared to a non-sparse approach. The time shown is the average training time taken for 7 datasets listed in Table \ref{table:datasets}. The GPU is a single NVIDIA A100-SXM4 with 40 GB VRAM. The CPU is an AMD EPYC 7763 (Milan) CPU with 64 cores and 512GB DDR4 memory.}
\centering
\label{intro-table}
\begin{tabular}{|cc|c|c|}
\hline
\multicolumn{2}{|c|}{}                                                  & \textbf{Sparse} & \textbf{Non-Sparse } \\ 
& & & \textbf{(TorchKGE)} \\
\hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{CPU}}} & \textbf{Forward}  & 74.86           & 299.2               \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                              & \textbf{Backward} & 166.59          & 919.17              \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                              & \textbf{Step}     & 15.4            & 15.95               \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{GPU}}} & \textbf{Forward}  & 18.2            & 48.8                \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                              & \textbf{Backward} & 17.49           & 89.51               \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                              & \textbf{Step}     & 0.4             & 0.45                \\ \hline
\end{tabular}

\end{table}

% 4. Our approach
One of this paper's main contributions is the development of sparse formulations for several popular translation-based KGE models. 
Adapting different translation models to sparse operations presents unique challenges, as each model interprets translations differently. 
For instance, TransE~\cite{TransE} uses a single embedding space for both entities and relations, while TransR~\cite{transR} uses separate spaces. Despite these differences, we designed a unified framework that allows diverse translation models to be represented through sparse matrices and mapped to sparse matrix operations like SpMM. We collectively refer to these sparse variants of translation-based embedding methods as SpTransX.

% 5. Novel insights and contributions
We develop a comprehensive library based on our sparse formulation. This library consolidates most computations into several SpMM function calls, allowing optimized SpMM to directly accelerate the overall runtime of KGE training. \revise{We also discuss how to extend this concept to other non-translational models such as DistMult or ComplEx in Appendix \ref{A:non_trans}.} We observe that SpTransX models significantly outperform established knowledge graph frameworks, such as TorchKGE and DGL-KE, particularly in terms of training time and GPU memory usage. For example, the average improvement in training time for the TransE model is illustrated in Table \ref{intro-table}.

Overall, this paper presents the following contributions:
\vspace{-0.2cm}
\begin{enumerate}
    \item {\bf Sparse Formulations of Translation-Based KGE Models:} We introduce sparse formulations for translation-based KGE models, enabling the mapping of KGE computations to SpMM and leveraging well-established SpMM techniques in model training.
    \item {\bf Development of an Optimized Library:} Our library incorporates various optimization techniques, including SIMD vectorization, loop unrolling, cache blocking, tiling, and WARP-level GPU optimization, to enhance performance. As a result, SpTransX models significantly outperform established knowledge graph frameworks, such as TorchKGE and DGL-KE.
    \item {\bf Enhanced Large-Batch Training:} By reducing memory requirements, SpTransX facilitates large-batch training on memory-limited GPUs.

\end{enumerate}






% \todo{Add contributions and new insights.}

%A knowledge graph formally represents semantics by describing entities and their relationships. It has gained significant research interest after the recent advancement of Large Language Models (LLM). This is primarily because KG can ground the language models with structured entity information and enables LLM to perform advanced reasoning and inference. However, the longer training time of KG networks remains a prohibitive factor for mining larger graphs or performing real-time analysis.


% \begin{table}[]
% \caption{TransE model Training time breakdown for GPU}
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
% \textbf{Function}               & \textbf{Non-Sparse}   & \textbf{Sparse}  \\ \hline
% optimizer.zero\_grad() & 0.24\%  & 0.31\%  \\ \hline
% model                  & 5.33\%  & 48.73\% \\ \hline
% criterion              & 5.80\%  & 0.31\%  \\ \hline
% backward               & 85.46\% & 49.85\% \\ \hline
% optimizer step         & 3.17\%  & 0.80\%  \\ \hline
% \end{tabular}
% \label{table:GPU-computation}
% \end{table}
