\section{Related Work}
\subsection{Translational Models for KGE}
Translation-based models represent entities and relations in a continuous vector space, interpreting relations as translations operating on entity embeddings. Several well-known models follow this approach, including TransE~\cite{TransE}, TransR~\cite{transR}, TransH~\cite{TransH}, TransD~\cite{transD}, TransA~\cite{transA}, TransG~\cite{transG},
TransC~\cite{lv2018differentiating}, TransM~\cite{fan2014transition}, 
TorusE~\cite{torusE}, and KG2E~\cite{KG2E}. Each model varies in how it represents the head, relation, and tail embeddings to capture relational semantics effectively.
For instance, TransE embeds entities and relations in the same vector space $\mathbb{R}^d$, assuming that relations can be modeled as a simple addition between the head and tail entities. In contrast, TransR utilizes distinct vector spaces for entities and relations, allowing it to better capture heterogeneous relation types, while TransE struggles with symmetric and one-to-many relations.
Some models, like TransH, introduce translations on hyperplanes to address the limitations of basic Euclidean embeddings. More recently, models such as rotatE~\cite{rotatE} have enabled translations within hyperbolic space instead of Euclidean space, allowing for better representation of hierarchical structures commonly found in some knowledge graphs.
It has been observed that translation-based models are typically more computationally efficient compared to semantic matching models that use a bilinear score function, such as DistMult \cite{DistMult}, RESCAL \cite{RESCAL}, and ComplEx \cite{ComplEx}. This efficiency, along with their adaptability across different KG structures, makes translation-based models a popular choice for large-scale knowledge graph applications.

\subsection{KGE frameworks}
Several frameworks are available for training knowledge graphs (KGs). Some, like TorchKGE \cite{boschin2020torchkge} and DGL-KE \cite{zheng2020dgl}, are specifically designed for this purpose. Others, such as PyTorch Geometric \cite{fey2019fast} and GraphStorm \cite{zheng2024graphstorm}, offer facilities for training KG models in addition to modules for training graph neural networks. 

Many frameworks are built on top of the PyTorch Framework, including TorchKGE, PyKeen \cite{ali2021pykeen}, PyTorch Geometric, etc. AmpliGraph \cite{ampligraph} has Tensorflow 2.0 backend. Some frameworks support hybrid backends, such as DGL-KE or OpenKE \cite{han2018openke}. DGL-KE supports PyTorch and MXNet as the backend. OpenKE supports PyTorch, Tensorflow, and C++ as the backend. Most frameworks have support for Python. 

Some frameworks, such as Pykg2vec \cite{yu2019pykg2vec} or DGL-KE, choose not to use the autograd feature of the backend ML, such as PyTorch, and implement their custom gradient update mechanism. PyKeen is designed to be highly extensible and uses a modular code base. It features automatic memory optimization support that generates sub-batches when the user-defined batch does not fit in the memory.

Most frameworks, such as TorchKGE, PyG, and PyKeen, use PyTorch's embedding module directly to store entity and relation embeddings.  Others, such as DGL-KE, convert the training triplets into DGL graphs before training. DGL-KE, PyTorch BigGraph \cite{lerer2019pytorch}, PyKeen, and several other frameworks allow multi-CPU and multi-GPU training using Python and distributed frameworks such as DGL or PyTorch Lightning.

\subsection{Sparse Operations in Graph ML} 
Expressing graph operations through sparse linear algebra has proven highly effective for developing efficient and scalable graph learning algorithms. 
For example, the forward and backward propagation in graph convolutional networks (GCNs) and graph attention networks (GATs) can be optimized with sampled dense-dense matrix multiplication (SDDMM), sparse-dense matrix multiplication (SpMM), or their combination, known as FusedMM~\cite{fey2019fast, wang2019deep, rahman2021fusedmm}. 
Similarly, various graph embedding and visualization algorithms utilize SpMM and sparse-sparse matrix multiplication (SpGEMM). 
Consequently, popular graph machine learning libraries, such as PyTorch Geometric (PyG) \cite{fey2019fast} and DGL \cite{wang2019deep}, rely on optimized implementations of SpMM, SpGEMM, and SDDMM available in vendor-provided libraries like cuSparse, MKL, or open-source libraries such as iSpLib~\cite{hoque2024isplib}, FeatGraph \cite{hu2020featgraph}, and SparseTIR \cite{ye2023sparsetir}.
Despite the wide success of sparse operations in GNNs and graph embeddings, to the best of our knowledge, existing knowledge graph embedding libraries do not leverage sparse operations for training KGE models.


%Sparse operation GNN. How 