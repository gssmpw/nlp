\section{Experimental Setting}
We implement the SpTransX models using PyTorch Framework and compare their total training time and GPU memory allocation with other well-known KG frameworks. We run these experiments for 7 datasets consisting of various sizes on a single CPU and a single GPU system separately. \revise{Several brief experiments demonstrating distributed training and scaling capacity of SpTransX framework is shown in Appendix \ref{A:dist} and \ref{A:scaling}.}

\subsection{Datasets}
Below are the 7 datasets used in the experiments.
% Table \ref{table:datasets} lists
    \begin{table}[h]
\caption{Knowledge graph datasets}
\label{table:datasets}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccccc}
\toprule
  &     &    & Training \\
  Dataset      &   Entity          &   Relations          & Triplets \\
\midrule
FB15k    & 14951  & 1345  & 483142  \\
FB15k237 & 14541  & 237   & 272115  \\
WN18     & 40943  & 18    & 141442  \\
WN18RR   & 40943  & 11    & 86835  \\                    
FB13     & 67399  & 15342 & 316232  \\
YAGO3-10 & 123182 & 37    & 1079040 \\
BioKG    & 93773  & 51    & 4762678 \\    
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\subsection{Frameworks and Models}
\label{fw-models}
For comparison, we pick three popular KG frameworks: TorchKGE, PyTorch Geometric, and DGL-KE. PyTorch Geometric (or PyG) supports the TransE model, while DGL-KE supports the TransE and TransR models. TorchKGE supports all four models: TransE, TransR, TransH, TorusE.
\subsection{Training Loop}
% \todo{we should call them SpTransE, SpTransR, etc instead of transe-sptransx.}
We prepare 11 separate scripts (SpTransE, SpTransR, SpTransH, SpTorusE, transe-torchkge, transr-torchkge, transh-torchkge, toruse-torchkge, transe-dglke, transr-dglke, and transe-pyg) to train the models on various datasets. Each script receives the dataset name as a command-line argument. The dataset is loaded from a shared repository. All frameworks use the same training configuration (learning rate: 0.0004, margin: 0.5), dissimilarity function (L2 or L2 torus), and loss function (MarginRankingLoss) and run for 200 epochs. Batch size and embedding dimensions are selected to maximize accuracy while utilizing available GPU memory (see subsection \ref{hyperparam_selection}). The following table lists the dimensions and batch sizes used for different models. 

    \begin{table}[h]
\caption{Training configuration for CPU and GPU}
\label{table:trg-config}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
  Model  & Embedding       & Batch     \\
\midrule
TransE & 1024            & $12 \times 32768$ \\
TorusE & 1024            & $12 \times 32768$ \\
TransR & 128             & $2 \times 32768$  \\
TransH & Ent=128,Rel=128 & $32768$    \\    
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The negative samples are generated once per positive sample and are pre-generated outside the training loop.

\subsection{System and Profiler Details}
All experiments are run on the NERSC Perlmutter system. The GPU experiments run on a single NVIDIA
A100-SXM4 GPU with 40 GB VRAM. The CPU experiments run on an AMD EPYC 7763 (Milan) CPU with 64 cores and 512GB DDR4 memory. 

We use Python's $time$ module to measure training time and its breakdown. PyTorch's CUDA module is used to measure peak memory usage in GPU experiments. Finally, Linux's $perf$ tool is used to measure the cache miss rate and FLOPs count for CPU experiments.


\subsection{SparseTransX Configuration}
In the SparseTransX framework, users can choose to use any high-performance SpMM to perform model training.
We select iSpLib SpMM \cite{hoque2024isplib} for CPU training and DGL g-SpMM \cite{wang2019deep} for GPU training.