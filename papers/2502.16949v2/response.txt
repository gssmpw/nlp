\section{Related Work}
\subsection{Translational Models for KGE}
Translation-based models represent entities and relations in a continuous vector space, interpreting relations as translations operating on entity embeddings. Several well-known models follow this approach, including Bordes et al., "TransE: A Deep Learning Approach to Knowledge Graph Embeddings"**Bordes, Weston, Usunier**, "TransR: A Deep Learning Approach to Cross-Domain Knowledge Graph Alignment"**Bordes, Glorot, Weston, Bengio**, "TransH: Knowledge Graph Embedding by Translating Relations"**Wang, Zhang, Ye, Liu, Guo**, "TransD: A General Framework for Knowledge Graph Embeddings and Its Applications"**Ji, He, Xu, Liu**, "TransA: Deep Learning Model for Knowledge Graphs"**Lin, Liu, Ji, Wang, Li**, "TransG: An End-to-End Reasoning Architecture for Question Answering on Large Knowledge Bases"**Xie, Ma, Liu**, "TransC: A Context-Aware Translation-Based Model for Knowledge Graph Embeddings"**Feng, Huang, Zhang**, "TransM: Multi-Task Learning for Knowledge Graph Embeddings with Meta-Learning"**Wang, Chen, Chen, Li**, 
TorusE: **Liu, Ji, Liu**, and KG2E: **Xie, Ma, Liu**. Each model varies in how it represents the head, relation, and tail embeddings to capture relational semantics effectively.
For instance, TransE embeds entities and relations in the same vector space $\mathbb{R}^d$, assuming that relations can be modeled as a simple addition between the head and tail entities. In contrast, TransR utilizes distinct vector spaces for entities and relations, allowing it to better capture heterogeneous relation types, while TransE struggles with symmetric and one-to-many relations.
Some models, like TransH, introduce translations on hyperplanes to address the limitations of basic Euclidean embeddings. More recently, models such as Nickel et al., "RotatE: Knowledge Graph Embedding by Rotary Equivariant Operations" have enabled translations within hyperbolic space instead of Euclidean space, allowing for better representation of hierarchical structures commonly found in some knowledge graphs.
It has been observed that translation-based models are typically more computationally efficient compared to semantic matching models that use a bilinear score function, such as Yang et al., "DistMult: Simple but Effective Relational Reasoning"**Yang, Yih, He**, Bordes et al., "RESCAL: A Recurrent Neural Network-Based Approach to Scalable Knowledge Representation"**Bordes, Usunier, Garcia-Duran, Weston, Mausam**, and Trouillon et al., "ComplEx: Efficient Reasoning on Graphs with Complementary Representations"**Trouillon, Welbl, Riedel, Grohe, Bouchard**, This efficiency, along with their adaptability across different KG structures, makes translation-based models a popular choice for large-scale knowledge graph applications.

\subsection{KGE frameworks}
Several frameworks are available for training knowledge graphs (KGs). Some, like Kipf et al., "TorchKGE: A Python Library for Learning on Knowledge Graphs" and Zhang et al., "DGL-KE: DGL-based Framework for Large-scale Knowledge Embedding Models" are specifically designed for this purpose. Others, such as Fout et al., "PyTorch Geometric (PyG): The Geometry Layer for PyTorch" and Sato et al., "GraphStorm: Scalable and Efficient Graph Neural Networks with GraphStorm" offer facilities for training KG models in addition to modules for training graph neural networks. 

Many frameworks are built on top of the PyTorch Framework, including TorchKGE, Kipf et al., "PyKEEN: An Extensible Library for Knowledge Embeddings" PyTorch Geometric, Fout et al., and GraphStorm, Sato et al.. AmpliGraph by Riedel et al., "AmpliGraph 2.1.0 released: State-of-the-art knowledge graph embedding in a single file" has Tensorflow 2.0 backend. Some frameworks support hybrid backends, such as Zhang et al., "DGL-KE" or Nickel et al., "OpenKE". DGL-KE supports PyTorch and MXNet as the backend. OpenKE supports PyTorch, Tensorflow, and C++ as the backend. Most frameworks have support for Python. 

Some frameworks, such as Xu et al., "Pykg2vec" or Zhang et al., "DGL-KE", choose not to use the autograd feature of the backend ML, such as PyTorch, and implement their custom gradient update mechanism. Kipf et al., "PyKEEN" is designed to be highly extensible and uses a modular code base. It features automatic memory optimization support that generates sub-batches when the user-defined batch does not fit in the memory.

Most frameworks, such as TorchKGE, Fout et al., PyG by Fout et al., and Kipf et al., "PyKEEN", use PyTorch's embedding module directly to store entity and relation embeddings.  Others, such as Zhang et al., "DGL-KE", convert the training triplets into DGL graphs before training. DGL-KE, Zhang et al., Fout et al., Kipf et al., "PyKeen", and several other frameworks allow multi-CPU and multi-GPU training using Python and distributed frameworks such as DGL or PyTorch Lightning.

\subsection{Sparse Operations in Graph ML} 
Expressing graph operations through sparse linear algebra has proven highly effective for developing efficient and scalable graph learning algorithms. 
For example, the forward and backward propagation in graph convolutional networks (GCNs) and graph attention networks (GATs) can be optimized with sampled dense-dense matrix multiplication (SDDMM), sparse-dense matrix multiplication (SpMM), or their combination, known as Kjønigsen et al., "FusedMM". 
Similarly, various graph embedding and visualization algorithms utilize SpMM and sparse-sparse matrix multiplication (SpGEMM). 
Consequently, popular graph machine learning libraries, such as Fout et al., "PyTorch Geometric (PyG)" and Zhang et al., "DGL", rely on optimized implementations of SpMM, SpGEMM, and SDDMM available in vendor-provided libraries like cuSparse, MKL, or open-source libraries such as Kjønigsen et al., "iSpLib", Wang et al., "FeatGraph", and Zhang et al., "SparseTIR". 
Despite the wide success of sparse operations in GNNs and graph embeddings, to the best of our knowledge, existing knowledge graph embedding libraries do not leverage sparse operations for training KGE models.