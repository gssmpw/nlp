\section{Conclusion}
%We propose a sparse approach for training translation-based knowledge graph embedding and observe promising results. This approach outperforms popular KG frameworks such as TorchKGE and DGL-KE in terms of training time and GPU memory usage. The performance is consistent on various small and large datasets for a model. Our sparse approach opens the scope for researchers to use high-performance SpMM kernels and perform efficient training on knowledge graphs. 

Despite the inherent sparsity of knowledge graphs and their embedding algorithms, existing frameworks often do not leverage sparse matrix operations to accelerate the training of KGE models. 
We have developed sparse formulations of translation-based KGE models that significantly outperform established knowledge graph frameworks, such as TorchKGE and DGL-KE, particularly regarding training time and GPU memory usage. Our findings demonstrate that the proposed approach consistently achieves improved performance across a range of both small and large datasets.

Our sparse approach has multiple benefits. By using sparse representations, we reduce memory usage during training, which allows us to work with larger knowledge graphs without exhausting GPU resources. The efficiency improvements in training time come from optimizing matrix operations.
Given the extensive research in parallel sparse matrix operations and the availability of highly optimized libraries, our approach paves the way for faster computations and enhanced scalability for larger knowledge graphs. We believe this work will inspire further advancements in the development of robust and scalable knowledge graph frameworks.