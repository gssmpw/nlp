

\newpage
\appendix
\section{Applicability of SparseTransX for dense graphs} 
\label{A:density}
Even for fully dense graphs, our KGE computations remain highly sparse. This is because our SpMM leverages the incidence matrix for triplets, rather than the graph's adjacency matrix. In the paper, the sparse matrix $A \in \{-1,0,1\}^{M \times (N+R)}$ represents the triplets, where $N$ is the number of entities, $R$ is the number of relations, and $M$ is the number of triplets. This representation remains extremely sparse, as each row contains exactly three non-zero values (or two in the case of the "ht" representation). Hence, the sparsity of this formulation is independent of the graph's structure, ensuring computational efficiency even for dense graphs.

\section{Computational Complexity}
\label{A:complexity}
 For a sparse matrix $A$ with $m \times k$ having $nnz(A)=$ number of non zeros and dense matrix $X$ with $k \times n$ dimension, the computational complexity of the SpMM is $O(nnz(A) \cdot n)$ since there are a total of $nnz(A)$ number of dot products each involving $n$ components. Since our sparse matrix contains exactly three non-zeros in each row, $nnz(A) = 3m$. Therefore, the complexity of SpMM is $O(3m \cdot n)$ or $O(m \cdot n)$, meaning the complexity increases when triplet counts or embedding dimension is increased. Memory access pattern will change when the number of entities is increased and it will affect the runtime, but the algorithmic complexity will not be affected by the number of entities/relations.

\section{Applicability to Non-translational Models}
\label{A:non_trans}
Our paper focused on translational models using sparse operations, but the concept extends broadly to various other knowledge graph embedding (KGE) methods. Neural network-based models, which are inherently matrix-multiplication-based, can be seamlessly integrated into this framework. Additionally, models such as DistMult, ComplEx, and RotatE can be implemented with simple modifications to the SpMM operations. Implementing these KGE models requires modifying the addition and multiplication operators in SpMM, effectively changing the semiring that governs the multiplication.   

In the paper, the sparse matrix $A \in \{-1,0,1\}^{M \times (N+R)}$ represents the triplets, and the dense matrix $E \in \mathbb{R}^{(N+R) \times d}$ represents the embedding matrix, where $N$ is the number of entities, $R$ is the number of relations, and $M$ is the number of triplets. TransE’s score function, defined as $h + r - t$, is computed by multiplying $A$ and $E$ using an SpMM followed by the L2 norm. This operation can be generalized using a semiring-based SpMM model: $Z_{ij} = \bigoplus_{k=1}^{n} (A_{ik} \otimes E_{kj})$

Here, $\oplus$ represents the semiring addition operator, and $\otimes$ represents the semiring multiplication operator. For TransE, these operators correspond to standard arithmetic addition and multiplication, respectively.

\subsection*{DistMult} 
DistMult’s score function has the expression $h \odot r \odot t$. To adapt SpMM for this model, two key adjustments are required: The sparse matrix $A$ stores $+1$ at the positions corresponding to $h_{\text{idx}}$, $t_{\text{idx}}$, and $r_{\text{idx}}$. Both the semiring addition and multiplication operators are set to arithmetic multiplication. These changes enable the use of SpMM for the DistMult score function.

\subsection*{ComplEx} 
ComplEx’s score function has $h \odot r \odot \bar{t}$, where embeddings are stored as complex numbers (e.g., using PyTorch). In this case, the semiring operations are similar to DistMult, but with complex number multiplication replacing real number multiplication.

\subsection*{RotatE} 
RotatE’s score function has $h \odot r - t$. For this model, the semiring requires both arithmetic multiplication and subtraction for $\oplus$. With minor modifications to our SpMM implementation, the semiring addition operator can be adapted to compute $h \odot r - t$.

\subsection*{Support from other libraries}
Many existing libraries, such as GraphBLAS (Kimmerer, Raye, et al., 2024), Ginkgo (Anzt, Hartwig, et al., 2022), and Gunrock (Wang, Yangzihao, et al., 2017), already support custom semirings in SpMM. We can leverage C++ templates to extend support for KGE models with minimal effort.


\begin{figure*}[t]
\centering     %%% not \center
\includegraphics[width=\textwidth]{figures/all-eval.pdf}
\caption{Loss curve for sparse and non-sparse approach. Sparse approach eventually reaches the same loss value with similar Hits@10 test accuracy.}
\label{fig:loss_curve}
\end{figure*}

\section{Model Performance Evaluation and Convergence}
\label{A:eval}
SpTransX follows a slightly different loss curve (see Figure \ref{fig:loss_curve}) and eventually converges with the same loss as other non-sparse implementations such as TorchKGE. We test SpTransX with the WN18 dataset having embedding size 512 (128 for TransR and TransH due to memory limitation) and run 200-1000 epochs. We compute average Hits@10 of 9 runs with different initial seeds and a learning rate scheduler. The results are shown below. We find that Hits@10 is generally comparable to or better than the Hits@10 achieved by TorchKGE.

\begin{table}[h]
\centering
\caption{Average of 9 Hits@10 Accuracy for WN18 dataset}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model} & \textbf{TorchKGE} & \textbf{SpTransX} \\ \hline
TransE         & 0.79 ± 0.001700   & 0.79 ± 0.002667   \\ \hline
TransR         & 0.29 ± 0.005735   & 0.33 ± 0.006154   \\ \hline
TransH         & 0.76 ± 0.012285   & 0.79 ± 0.001832   \\ \hline
TorusE         & 0.73 ± 0.003258   & 0.73 ± 0.002780   \\ \hline
\end{tabular}
\label{table:perf_eval}
\end{table}

% We also plot the loss curve for different models in Figure \ref{fig:loss_curve}. We observe that the sparse approach follows a similar loss curve and eventually converges to the same final loss.

\section{Distributed SpTransX and Its Applicability to Large KGs}
\label{A:dist}
SpTransX framework includes several features to support distributed KGE training across multi-CPU, multi-GPU, and multi-node setups. Additionally, it incorporates modules for model and dataset streaming to handle massive datasets efficiently. 

Distributed SpTransX relies on PyTorch Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP) support to distribute sparse computations across multiple GPUs. 

\begin{table}[h]
\centering
\caption{Average Time of 15 Epochs (seconds). Training time of TransE model with Freebase dataset (250M triplets, 77M entities. 74K relations, batch size 393K)  on 32 NVIDIA A100 GPUs. FSDP enables model training with larger embedding when DDP fails.}
\begin{tabular}{|p{2cm}|p{2.5cm}|p{2.5cm}|}
\hline
\textbf{Embedding Size} & \textbf{DDP (Distributed Data Parallel)} & \textbf{FSDP (Fully Sharded Data Parallel)} \\ \hline
16                      & 65.07 ± 1.641                            & 63.35 ± 1.258                               \\ \hline
20                      & Out of Memory                            & 96.44 ± 1.490                               \\ \hline
\end{tabular}
\end{table}

We run an experiment with a large-scale KG to showcase the performance of distributed SpTransX. Freebase (250M triplets, 77M entities. 74K relations, batch size 393K) dataset is trained using the TransE model on 32 NVIDIA A100 GPUs of NERSC using various distributed settings. SpTransX’s Streaming dataset module allows fetching only the necessary batch from the dataset and enables memory-efficient training. FSDP enables model training with larger embedding when DDP fails.

\section{Scaling and Communication Bottlenecks for Large KG Training}
\label{A:scaling}
Communication can be a significant bottleneck in distributed KGE training when using SpMM. However, by leveraging Distributed Data-Parallel (DDP) in PyTorch, we successfully scale distributed SpTransX to 64 NVIDIA A100 GPUs with reasonable efficiency. The training time for the COVID-19 dataset with 60,820 entities, 62 relations, and 1,032,939 triplets is in Table \ref{table:scaling}. 
% \vspace{-.3cm}
\begin{table}[h]
\centering
\caption{Scaling TransE model on COVID-19 dataset}
\begin{tabular}{|c|c|}
\hline
\textbf{Number of GPUs} & \textbf{500 epoch time (seconds)} \\ \hline
4                       & 706.38                            \\ \hline
8                       & 586.03                            \\ \hline
16                      & 340.00                               \\ \hline
32                      & 246.02                            \\ \hline
64                      & 179.95                            \\ \hline
\end{tabular}
\label{table:scaling}
\end{table}
% \vspace{-.2cm}
It indicates that communication is not a bottleneck up to 64 GPUs. If communication becomes a performance bottleneck at larger scales, we plan to explore alternative communication-reducing algorithms, including 2D and 3D matrix distribution techniques, which are known to minimize communication overhead at extreme scales. Additionally, we will incorporate model parallelism alongside data parallelism for large-scale knowledge graphs.

\section{Backpropagation of SpMM}
\label{A:backprop}
 Our main computational kernel is the sparse-dense matrix multiplication (SpMM). The computation of backpropagation of an SpMM w.r.t. the dense matrix is also another SpMM. To see how, let's consider the sparse-dense matrix multiplication $AX = C$ which is part of the training process. As long as the computational graph reduces to a single scaler loss $\mathfrak{L}$, it can be shown that $\frac{\partial C}{\partial X} = A^T$. Here, $X$ is the learnable parameter (embeddings), and $A$ is the sparse matrix. Since $A^T$ is also a sparse matrix and $\frac{\partial \mathfrak{L}}{\partial C}$ is a dense matrix, the computation $\frac{\partial \mathfrak{L}}{\partial X} = \frac{\partial C}{\partial X} \times \frac{\partial \mathfrak{L}}{\partial C} = A^T \times \frac{\partial \mathfrak{L}}{\partial C} $ is an SpMM. This means that both forward and backward propagation of our approach benefit from the efficiency of a high-performance SpMM.

\subsection*{Proof that $\frac{\partial C}{\partial X} = A^T$}
 To see why $\frac{\partial C}{\partial X} = A^T$ is used in the gradient calculation, we can consider the following small matrix multiplication without loss of generality.
\begin{align*}
A &= \begin{bmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{bmatrix} \\ 
 X &= \begin{bmatrix}
x_1 & x_2 \\
x_3 & x_4
\end{bmatrix} \\
 C &=  \begin{bmatrix}
c_1 & c_2 \\
c_3 & c_4
\end{bmatrix}
\end{align*}
Where $C=AX$, thus-
\begin{align*}
c_1&=f(x_1, x_3) \\
c_2&=f(x_2, x_4) \\
c_3&=f(x_1, x_3) \\
c_4&=f(x_2, x_4) \\
\end{align*}
Therefore-
\begin{align*}
\frac{\partial \mathfrak{L}}{\partial x_1} &= \frac{\partial \mathfrak{L}}{\partial c_1} \times \frac{\partial c_1}{\partial x_1} + \frac{\partial \mathfrak{L}}{\partial c_2} \times \frac{\partial c_2}{\partial x_1} + \frac{\partial \mathfrak{L}}{\partial c_3} \times \frac{\partial c_3}{\partial x_1} + \frac{\partial \mathfrak{L}}{\partial c_4} \times \frac{\partial c_4}{\partial x_1}\\
&= \frac{\partial \mathfrak{L}}{\partial c_1} \times \frac{\partial \mathfrak{c_1}}{\partial x_1} + 0 + \frac{\partial \mathfrak{L}}{\partial c_3} \times \frac{\partial \mathfrak{c_3}}{\partial x_1} + 0\\
&= a_1 \times \frac{\partial \mathfrak{L}}{\partial c_1} + a_3 \times \frac{\partial \mathfrak{L}}{\partial c_3}\\
\end{align*}

Similarly-
\begin{align*}
\frac{\partial \mathfrak{L}}{\partial x_2}
&= a_1 \times \frac{\partial \mathfrak{L}}{\partial c_2} + a_3 \times \frac{\partial \mathfrak{L}}{\partial c_4}\\
\frac{\partial \mathfrak{L}}{\partial x_3}
&= a_2 \times \frac{\partial \mathfrak{L}}{\partial c_1} + a_4 \times \frac{\partial \mathfrak{L}}{\partial c_3}\\
\frac{\partial \mathfrak{L}}{\partial x_4}
&= a_2 \times \frac{\partial \mathfrak{L}}{\partial c_2} + a_4 \times \frac{\partial \mathfrak{L}}{\partial c_4}\\
\end{align*}
This can be expressed as a matrix equation in the following manner-
\begin{align*}
\frac{\partial \mathfrak{L}}{\partial X} &= \frac{\partial C}{\partial X} \times \frac{\partial \mathfrak{L}}{\partial C}\\
\implies \begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial x_1} & \frac{\partial \mathfrak{L}}{\partial x_2} \\
\frac{\partial \mathfrak{L}}{\partial x_3} & \frac{\partial \mathfrak{L}}{\partial x_4}
\end{bmatrix} &= \frac{\partial C}{\partial X} \times \begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial c_1} & \frac{\partial \mathfrak{L}}{\partial c_2} \\
\frac{\partial \mathfrak{L}}{\partial c_3} & \frac{\partial \mathfrak{L}}{\partial c_4}
\end{bmatrix}
\end{align*}
By comparing the individual partial derivatives computed earlier, we can say-

\begin{align*}
\begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial x_1} & \frac{\partial \mathfrak{L}}{\partial x_2} \\
\frac{\partial \mathfrak{L}}{\partial x_3} & \frac{\partial \mathfrak{L}}{\partial x_4}
\end{bmatrix} &= \begin{bmatrix}
a_1 & a_3 \\
a_2 & a_4
\end{bmatrix} \times \begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial c_1} & \frac{\partial \mathfrak{L}}{\partial c_2} \\
\frac{\partial \mathfrak{L}}{\partial c_3} & \frac{\partial \mathfrak{L}}{\partial c_4}
\end{bmatrix}\\
\implies \begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial x_1} & \frac{\partial \mathfrak{L}}{\partial x_2} \\
\frac{\partial \mathfrak{L}}{\partial x_3} & \frac{\partial \mathfrak{L}}{\partial x_4}
\end{bmatrix} &= A^T \times \begin{bmatrix}
\frac{\partial \mathfrak{L}}{\partial c_1} & \frac{\partial \mathfrak{L}}{\partial c_2} \\
\frac{\partial \mathfrak{L}}{\partial c_3} & \frac{\partial \mathfrak{L}}{\partial c_4}
\end{bmatrix}\\
\implies \frac{\partial \mathfrak{L}}{\partial X} &= A^T \times \frac{\partial \mathfrak{L}}{\partial C}\\
\therefore \frac{\partial C}{\partial X} &= A^T \qed
\end{align*}
