
\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{booktabs} % for professional tables
\usepackage{comment}
%\usepackage{authblk}
\usepackage{natbib}
\usepackage{float}
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subfig}
\usepackage{booktabs} % for professional tables
\usepackage{comment}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{algorithm}


\usepackage{hyperref}

\usepackage{geometry}
%\geometry{legalpaper, portrait, margin=1.5in}
\input{macros}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}
\begin{document}


\title{Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces}
\iffalse
\author[1]{Eric Eaton}
\author[1]{Marcel Hussing}
\author[1]{Michael Kearns}
\author[1]{Aaron Roth}
\author[1]{Sikata Bela Sengupta}
\author[2]{Jessica Sorrell}
\fi
\author{
Eric Eaton\thanks{Department of Computer and Information Sciences, University of Pennsylvania.} \and
Marcel Hussing \footnotemark[1] \and 
Michael Kearns \footnotemark[1]\and 
Aaron Roth \footnotemark[1] \and
Sikata Sengupta \footnotemark[1]\and
Jessica Sorrell\thanks{Department of Computer Science, Johns Hopkins University.}}

%\affil[1]{Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USA}

%\affil[2]{Department of Computer Science, Johns Hopkins University, Baltimore, Maryland, USA}




\maketitle








\begin{abstract}
In traditional reinforcement learning (RL), the learner aims to solve a single objective optimization problem: find the policy that maximizes expected reward. However, in many real-world settings, it is important to optimize over multiple objectives simultaneously. For example, when we are interested in fairness, states might have feature annotations corresponding to multiple (intersecting) demographic groups to whom reward accrues, and our goal might be to maximize the reward of the group receiving the minimal reward.   In this work, we consider a multi-objective optimization problem in which each objective is defined by a state-based reweighting of a single scalar reward function. This generalizes the problem of maximizing the reward of the minimum reward group. We provide oracle-efficient algorithms to solve these multi-objective RL problems even when the number of objectives is exponentially large --- for tabular MDPs, as well as for large MDPs when the group functions have additional structure. %The algorithms we present solve this class of multi-objective RL problems with a possibly \textit{exponentially} large class of constraints over intersecting groups in both tabular and large state space MDPs in an oracle-efficient manner. 
Finally, we experimentally validate our theoretical results and demonstrate applications on a preferential attachment graph MDP. 
\end{abstract}

\keywords{Machine Learning, Reinforcement Learning, Fairness, Multi-Group, Minimax}
\section{Introduction}

There are a number of reinforcement learning (RL) settings in which states correspond to either \emph{individuals} or \emph{groups}, each with their own properties,  and actions correspond to the provision of goods or services among the states \cite{wen2021algorithms,satija2023group}. For example, we could consider a road network of neighborhoods and the sequential distribution of disaster relief over the region \cite{li2024reinforcement}, a set of critical locations and touring these locations to provide surveillance, or hospital resources (e.g., beds, ventilators) and their allocation.

In scenarios like these, what should we optimize for? If we optimize for a single objective, then the benefits of the actions may accrue disproportionately to some groups of people or locations over others. If there are natural groupings of states whose welfare we are concerned about (such as neighborhood demographics or disease subpopulations), then a natural fairness-motivated objective is \emph{minimax} optimization: to maximize the reward that accrues to the group obtaining the minimum reward. Minimax objectives over groups have recently been studied in classification problems \cite{martinez2020minimax,diana2021minimax}, and have the attractive property that they \emph{Pareto-dominate} solutions that seek to \emph{equalize} reward across groups; every group will have higher reward in a minimax solution than they would in any equal reward solution. Problems like this are examples of \emph{RL problems with constraints} \cite{calvo2023state,dudik2020oracle}.

However, as noted by \citet{kearns2018preventing} (in a classification setting), especially in fairness motivated problems, it is insufficient to satisfy these kinds of constraints marginally. For example, suppose we have demographic groups defined by race, gender, and income, which are not mutually exclusive in that no attribute determines the value of any other. Satisfying constraints marginally on, e.g., racial groups does not guarantee that the corresponding constraints are satisfied on intersectional groups, defined by race and gender or gender and income. In general, if we have $d$ groupings of individuals, there are $2^d$ intersectional groups --- an enormous number for even modestly large values of $d$. 

In this paper, we study a constrained RL problem in the episodic setting that generalizes the problem of minimax group fairness, and is able to efficiently handle exponentially many constraints --- for example, those that arise from considering all intersections of $d$ grouping functions. We consider a problem in which the states $s \in \cS$ of an MDP are annotated by feature vectors (e.g., describing the demographics of a population corresponding to state $s$). There is a collection of \emph{group functions} $\cG$ containing functions $g:\cS\rightarrow \{0,1\}$ indicating (as a function of the features at a state $s$) whether $s$ is a member of group $g$ or not. The groups can be arbitrarily intersecting. Every state/action pair $(s,a)$ is associated with a reward $r(s,a)$ that accrues to each group $g$ such that $g(s) = 1$. Given a policy $\pi$, the expected reward that accrues to group $g$ is $V^g(\pi)=\E_\pi[\sum_{t=1}^H r(s_t,a_t)g(s_t)]$, and the group-wise minimax reward maximization problem is to solve $\max_{\pi}\min_{g} V^g(\pi)$. Our goal is to give oracle-efficient algorithms for solving this problem over large state and constraint spaces --- i.e., computational reductions to \emph{unconstrained} RL problems and optimization problems over $\cG$.

\subsection{Our Results}
We give three main results:
\begin{enumerate}
\item  For MDPs with only polynomially many states --- i.e., the \emph{tabular} setting --- we give an efficient reduction to a linear optimization oracle over $\cG$ that optimally solves the constrained RL problem with constraints defined by $\cG$, independently of the cardinality of $\cG$. Here the oracle we need is for a batch/offline optimization problem, which is substantially easier than the RL/control problem. See Section \ref{sect:tabular}.
\item For large MDPs, (which we cannot solve efficiently in the worst case, even in the standard single-objective setting), we give an efficient reduction from the problem of optimally solving the contrained RL problem with constraints from $\cG$ to two problems: 1) the problem of finding the optimal policy in the standard (unconstrained) single-objective RL problem, and 2) the batch/offline optimization problem over $\cG$. Here we need to assume that $\cG$ has special structure --- namely that it has a polynomial-sized \emph{separator set} \citep{dudik2020oracle} --- but fortunately the class of boolean conjunctions that define the $2^d$ ``intersectional'' constraints given $d$ groupings of the data have this structure. See Section \ref{sect:gftpl}.
\item For arbitrary grouping functions $\cG$ (which may not have small separator sets), we give an algorithm that makes iterative calls to a standard RL algorithm for the unconstrained problem and an optimization oracle over $\cG$ that provably converges to the optimal constrained policy --- albeit without a polynomial time convergence bound. See Section \ref{sect:fairfict}. Despite the lack of a polynomial-time convergence bound (in contrast to the algorithm we give in Section \ref{sect:gftpl}), this algorithm has the advantage of simplicity (and not  requiring separator set structure on $\cG$), and is what we use in experiments; we find that empirically it works quite well.
\end{enumerate}


In the above results, what we actually find is a \textit{distribution} over policies that are minimax-optimal in expectation. But, this naively means that for any fixed policy in the support of our distribution, the constraints might be badly violated. This would be concerning in settings in which we care about a single episode rather than the average over many episodes and has a simple fix, described in \textbf{Appendix \ref{app:err_canc}}. Informally, we replace constraints of the form $g(x) \leq 0$ with constraints of the form $\max\{0,g(x)\} \leq 0$ which preserves convexity and eliminates the possibility that ``slack'' in the constraints on some policies cancel out in expectation with constraint violation in others. This is because by taking only the positive part of the constraint violation, we penalize our solution for constraint violation without rewarding it for slack. 


We evaluate FairFictRL (Algorithm \ref{alg:FairFictRL}) on Barabási-Albert graphs \citep{barabasi2016network} with groups assigned based on the degree distribution of nodes. We show that our algorithm converges efficiently to a solution with low average constraint violations for all groups, while still optimizing the global objective. See Section~\ref{sect:experiments}. 



\subsection{Related Work}


\subsubsection{Fair Reinforcement Learning}
There are many notions of fairness in RL that are distinct from what we study in this paper. \citet{jabbari2017fairness} define a notion of fairness that requires that the algorithm never play one action with higher probability over another unless the long-term reward of the optimal policy after playing the first action is higher than the second. 
\citet{cousinswelfare} consider a welfare-centric notion of fair RL that encompasses a broad class of functions over a set of beneficiaries. They develop a model of adversarially-fair KWIK (knows-what-it-knows learning) and provide the algorithm $E^4$ (equitable $E^3$). It is important to note that their results mainly hold only for the tabular MDP setting.
\citet{michailidis2024scalable} use a flexible form of Lorenz dominance to ensure a more equitable distribution of rewards and empirically demonstrate success of their method for real-world transport planning problems.
\citet{wen2021algorithms} consider model-based and model-free approaches to constrained fair RL problems and study these in the context of a bank offering loans to individuals. However, these methods do not extend for a very large number of overlapping groups.
\citet{satija2023group} consider demographic fairness by constraining the group pair-wise difference in performances and provide theoretical guarantees for their algorithms in the tabular setting with an empirical demonstration of success for larger state spaces.
See \citet{reuel2024fairness} for a survey of this literature. Aside from our differing objective, we differentiate from this literature by giving algorithms and theory that are able to handle both 1) a very large number of intersecting groups, and 2) provable guarantees beyond the tabular setting. 


\subsubsection{Constrained Reinforcement Learning}
\citet{calvo2023state} consider the problem of constrained RL, especially for continuous state and action spaces. In their setting, they have $m$ reward functions and optimize global cumulative reward subject to each of the $m$ value functions for the given policy being at least as large as some threshold. To do this, they constructed augmented state-space MDPs and formulate the corresponding Lagrangian (with regularization) of this optimization problem. They provide an efficient Primal-Dual algorithm (using different scalarized reward optimization for the learner and online gradient descent (no-regret) for the regulator).  Similar to \citeauthor{calvo2023state}, \citet{muller2024truly} provide Primal-Dual algorithms and construct their Lagrangian with regularization and optimistic exploration. They prove last-iterate convergence of their algorithm, enabling them to avoid error cancellations. 
\citet{miryoosefi2019reinforcement} also study the problem of RL with convex constraints similarly observing a $d$-dimensional reward vector rather than a scalar. They also formulate this problem as a zero-sum game between a learner and regulator playing best-response vs.~online gradient descent (no-regret). These algorithms require enumerating the constraints; in contrast we give ``oracle efficient'' algorithms that require only optimizing over the constraints, and hence can handle extremely large collections of constraints efficiently.
\subsubsection{Multi-Objective Classification Problems}
\citet{agarwal2018reductions} consider the problem of fairness in the binary classification setting and reduce this problem to a sequence (in a repeated zero-sum game between a learner and regulator) of cost-sensitive classification problems to provide a randomized classifier with low general error while (in expectation) satisfying the constraints. \citet{kearns2018preventing} extend \citet{agarwal2018reductions} to settings with a very large number  of overlapping groups. They propose statistical notions of fairness that take into account ``fairness gerrymandering'' over this large number of subgroups and reduce their problem to a sequence of weak agnostic learning problems. We extend this style of algorithm from the classification to the RL setting.


\section{Model and Preliminaries}

We consider an episodic fixed-horizon Markov decision process (MDP)~\citep{puterman2014markov} which can be formalized as a tuple $\mathcal{M}=(\mathcal{S},\mathcal{A},P_h,r_h,\mu),$ where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $H$ is the horizon, $r_h:\mathcal{S}\times \mathcal{A} \rightarrow [0,1]$ is the reward function at time $h$, $P_h$ specifies the transition dynamics at time $h$, and $\mu$ is the initial state distribution. For simplicity, and without loss of generality, we assume rewards and transition dynamics are time-invariant, and denote them by $r$ and $P$ respectively. Without loss of generality, we assume that rewards are bounded within $[0, 1]$. 

Throughout the paper, $[N]$ will denote the set $\{0,...,N-1\}$. We will indicate a sequence of elements $s_1, s_2, \dots, s_t$ by $s_{1:t}$. We write $x \sim \cU(S)$ to denote sampling from the uniform distribution over set $S$, and $\nu \sim \lap(\rho)$ to denote sampling from the Laplace distribution with parameter $\rho$.

In the beginning, an initial state $s_0$ is sampled from $\mu$. At any time $h \in [H]$, the agent is in some state $s_h \in \mathcal{S}$ and chooses an action $a_h \in \mathcal{A}$ based on a function $\pi_h$ mapping from states to distributions over actions $\Pi: \mathcal{S}\mapsto \Delta{\mathcal{A}}$. As a consequence, the agent traverses to a new next state $s_{h+1}$ sampled from $P(\cdot | s_h, a_h)$ and obtains a reward $r(s_h, a_h)$. The sequence of functions $\pi_h$ used by the agent is referred to as its \emph{policy}, and is denoted $\pi = \{\pi_h\}_{h\in[H]}$. A \emph{trajectory} is the sequence of (state, action) pairs $\{(s_h, a_h)\}_{h\in[H]}$ taken by the agent over an episode of length $H$. 

In standard RL, the goal of the learner is to maximize the expected cumulative reward $\E_{s_0\sim \mu, P}[\sum_{t=0}^{H-1} r(s_t, a_t)]$ over episodes of length $H$. We further define the value function as the expected cumulative return of following some policy $\pi$ from some state $s$ as $V^{\pi}(s) = \E_{s_0\sim \mu_0, P}[\sum_{t=0}^{H-1} r(s_t, a_t) | \pi, s_0=s]$. Due to the finite horizon of the episodic setting, we will also need to refer to the expected cumulative reward from state $s$ under policy $\pi$ from time $h \in [H]$. We denote this time-specific value function by $V_h^{\pi}(s) = \E_{P}[\sum_{t=h}^{H-1}r(s_t,a_t) | \pi, s_h = s]$. For the remainder of the paper, when indices for horizon $h$ are not listed, assume $h=0$.

 We consider a setting in which each state is assigned a feature vector $x \in \mathcal{X}$ over $d$ attributes. Alternatively, we can directly think of states as feature vectors themselves. We then define a set of functions $\cG$, where each function $g \in \cG$, $g: \cX \rightarrow \{0,1\}$ represents membership in the group $\{x\in \cX : g(x) = 1\}$. 

 We denote the feature vector associated with state $s$ by $x(s)$, and use the shorthand $g(s_t)=g(x(s_t))$. Then, for each state visited, the scalar reward $r(s,a)$ will be given to all groups of which $x$ is a member. So, at time $t$, group $g$ receives $r(s_t,a_t)g(s_t)$. Note that, more generally, we could have a reward function defined over features and actions $r(x,a)$ and think about $r(x,a)g(x)$ as the corresponding reward to the groups to which $x$ belongs. 
\\ 


Now, we can formulate our problem in terms of a repeated zero-sum game between a learner and a regulator. The learner is trying to find a policy from a class of policies $\Pi$ to optimize the expected cumulative reward (across all groups) and the regulator is trying to minimize the learner's payoff by finding groups with the lowest expected cumulative reward for the selected policy. 
\\
Let $V^g(\pi) = \mathbb{E}_{s\sim \mu,P,\pi}[\sum_{h=0}^{H-1}r(s_h,a_h)g(s_h)|s_0=s]$ and note that it is equivalent to the average reward formulation $V^g(\pi)=H\E_{t\in[H]}\E_{P,\pi,\mu}[r(s_t,a_t)g(s_t)]$ and $V^{tot}(\pi)=H\E_{t \in [H]}\E_{P,\pi,\mu}[r(s_t,a_t)].$ We can consider the following \textit{minimax reward} problem:
\begin{equation}
    \begin{aligned}
    \max_{D \in \Delta \Pi} \quad & \E_{\pi \sim D}[V^{tot}(\pi)] \quad \\
    \text{subject to } &\E_{\pi \sim D} \left[V^{g}(\pi) \right] \geq \alpha, \quad \forall g \in \mathcal{G} \enspace .
\end{aligned}
\label{minimax_reward}
\end{equation}
We call the formulation of Problem \ref{minimax_reward} a \textit{minimax reward} problem because the solution that optimizes global cumulative reward subject to those constraints still is a member of the set of minimax solutions to the problem:
\begin{equation}
    \begin{aligned}
        \max_{D \in \Delta \Pi} \min_{G \in \Delta \cG}\E_{\substack{\pi \sim D \\ g\sim G}}[V^g(\pi)] 
    \end{aligned}
    \label{multigroup}
\end{equation}
which can be solved by setting the global objective to be an optimization over $\alpha$, as mentioned below. 
In order to solve this class of problems, we first formulate their Lagrangians. The resulting Lagrangian of Problem \ref{fig:avg_reward} is:
\begin{align*}
    \mathcal{L}(D, \lambda) &= \E_{\pi \sim D} \left[ V^{tot}(\pi) + \textstyle\sum_{g \in \mathcal{G}} \lambda^{g} \left( V^{g}(\pi) -  \alpha \right) \right] \enspace .
\end{align*}
We make a couple of observations. First, notice that for $\alpha=0$ this problem always has a feasible solution. Moreover, if we make the objective of the optimization problem $\alpha$ rather than $V^{tot}$, then this becomes analogous to the minimax formulation in Problem \ref{multigroup}. Additionally, we could always choose to set $\alpha_g$'s that are tailored to the individual groups rather than using a single global value $\alpha$ across all groups. Finally, we can use this framework to solve a more general class of multi-group constrained RL problems with constraints that are linear (and sometimes even convex) in the distribution over policies $D$.
We can define the average reward objective of this problem as follows: 
\begin{equation*}
    \begin{aligned}
        U(D,\lambda) = \E_{\substack{\pi \sim D \\ t \in [H] \\ P, \pi, \mu}}[r(s_t,a_t)+\sum_{g \in \mathcal{G}} \lambda^g (r(s_t,a_t)g(s_t)-\tfrac{\alpha}{H})]
    \end{aligned}
\end{equation*}
Defining the following compact convex set $\Lambda$
    $$\Lambda = \left\{ \lambda \in \mathbb{R}^{|\mathcal{G}|}_+ : ||\lambda||_1 \leq C \right\} \enspace ,$$ the resulting minimax problem under this formulation would be 
$\max_{D \in \Delta \Pi}\min_{\lambda \in \Lambda} U(D,\lambda)\enspace .$
Notice that in both cases, since the action spaces of both players are compact and convex, and the objective is affine in both variables, the conditions necessary for Sion's minimax theorem hold \cite{sion1958general}. 


\begin{definition}[Regulator's Regret] For a given transcript of $\{D_t,\lambda_t,U(D_t,\lambda_t)\}_{t=1}^H$, we define the Adversary's regret to be:
    $$\textstyle\sum_{t=1}^{H} \mathbb{E} \left[ U(D_t, \lambda_t) \right]-\min_{\lambda \in \Lambda} \textstyle\sum_{t=1}^{H} \mathbb{E} \left[ U(D_t, \lambda) \right]\enspace .$$
\end{definition}
\begin{definition}[$\nu$-approximate minimax equilibrium] $(\hat{D},\hat{\lambda})$ is a $\nu-$approximate minimax equilibrium if :
$$U(\hat{D},\hat{\lambda}) \leq \min_{\lambda \in \Lambda}U(\hat{D},\lambda)+\nu\enspace ,$$ 
$$U(\hat{D},\hat{\lambda}) \geq \max_{D\in \Delta \Pi} U(D,\hat{\lambda})-\nu \enspace .$$
\end{definition}
\begin{definition}[Regulator's Best Response Function] \label{def:reg_best}
\begin{align}
    Best_\lambda(D) = \begin{cases}
        0 \quad \text{if nothing violated} \\ Ce_{k}
    \end{cases} \enspace ,
\end{align}
where $k$ is the index of the largest constraint violated. When the number of constraints is extremely large, this best response function  may need to invoke an Optimization Oracle as defined in Subsection~\ref{ssec:oracles}.
\end{definition}

For each given $\lambda$ selected by the regulator, we can construct the transformed scalarized reward function for the learner as follows:
\begin{align*}
    r_{\lambda}(s_t, a_t) &=  \left( r(s_t, a_t) + \textstyle\sum_{g \in \mathcal{G}} \lambda^{g} \left( r(s_t, a_t)g(s_t)  - \frac{\alpha}{H} \right) \right) \enspace.
\end{align*}
To see this:
\begin{equation}
\begin{aligned}
    &\mathbb{E}_{s_t, a_t \sim \pi} \left[ \sum_{t=1}^{H} r_{\lambda}(s_t, a_t) \right] \\
    &= \mathbb{E}_{s_t, a_t \sim \pi} [ \sum_{t=1}^{H} ( r(s_t, a_t) + \sum_{g \in \mathcal{G}} \lambda^{g} ( r(s_t, a_t)g(s_t) - \frac{\alpha}{H} ) ) ] \\
   &=  \mathbb{E}_{s_t, a_t \sim \pi} [ \sum_{t=1}^{H} r(s_t, a_t) + \sum_{g \in \mathcal{G}} \lambda^g (\sum_{t=1}^{H} r(s_t,a_t)g(s_t)-\alpha) ] \\
    &= V^{tot}(\pi)+\sum_{g \in \mathcal{G}} \lambda^g (V^{g}(\pi)-\alpha) \enspace .
\end{aligned}
\end{equation}
Thus, we can define the MDP $M_{\lambda}(S, A, P, \mu, r_\lambda)$ with the modified rewards and maximizing $$\mathbb{E}_{\pi \sim D}\Big[\mathbb{E}_{s_t,a_t\sim\pi,P}\Big[\textstyle\sum_{t=1}^T r_\lambda(s_t,a_t)\Big]\Big]$$ will provide a policy which we can represent the selection of with distribution variable $D$ that will also minimize the objective. Recall that in sequential play, the second player can achieve the value of the game via deterministic response. We once again can define the corresponding function purely in terms of feature vectors $x$ rather than purely in terms of states.
\subsection{Oracles}\label{ssec:oracles}
The learner and regulator will make use of the oracles defined below. While we state them as strict optimizers, we observe that $\epsilon$-approximate versions of these oracles suffice for our applications.
\begin{definition}[Lin-OPT Oracle]
    A linear optimization oracle for $\cG$,
    $\text{Lin-OPT}(c) = \arg\min_{g \in \mathcal{G}}\langle g_{\states},c\rangle$, where $g_{\states} $ denotes the vector $(g(s_1), \dots, g(s_{|\states|}))$.
\end{definition}
\begin{definition}[OPT Oracle]
    An optimization oracle for $\cG$,
    $\text{OPT}(s_{1:t}, c) = \argmin_{g\in \cG}\langle g_{s_{1:t}}, c\rangle$,
    where $g_{s_{1:t}}$ denotes the vector $(g(s_1), \dots, g(s_t))$.
\end{definition}
\begin{definition}[Learner's Best Response Oracle]
We define $\mathcal{O}(U, \lambda)$ to be the best response oracle for the learner on input MDP $\mathcal{M}_\lambda$. 
That is, it returns $\arg\max_{D} U(D,\lambda),$ 
where $D$ is a point mass distribution, concentrated on a single policy $\pi$. We will frequently use the notation $\pi_t$ to refer to the policy on which a best response $D_t$ is supported.

We use $\mathcal{O}_{\epsilon}(U,\lambda)$ to denote $\epsilon$-approximate best response.
\end{definition}
The learner's best response oracle captures the assumption that for a given scalar reward, one is able to run a standard RL algorithm to learn an optimal policy for that given MDP. 

Note that in the tabular MDP setting this assumption can be realized by existing efficient algorithms for learning near-optimal policies, such as $E^3$~\cite{kearns2002near} or R-max~\cite{brafman2002r} .
    

\section{Algorithms and Theoretical Results}

 We will present our theoretical results in the following manner. For the settings of tabular MDPs and large state spaces with structured groups, we will first provide a general algorithm and analysis in terms of an arbitrary no-regret algorithm used by the regulator. We will then provide the specific regret analysis corresponding to each no-regret algorithm/setting used by the regulator. Finally, for the setting of large state spaces with general groups, we will provide a different algorithm and provide corresponding analysis. We use a modified version of the algorithm presented in \cite{agarwal2018reductions} in Algorithm \ref{alg:MORL-BRNR}.

\begin{algorithm}
\caption{MORL-BRNR (Multi-Objective RL--Approximate Min-Max RL Algorithm)}
\begin{algorithmic}[1]

\STATE \textbf{Input:} bound \( C \), best-response error \(\epsilon\), \( \mathcal{G} \) groups, \( r \) reward function, access to MDP \( \mathcal{M} \)
\STATE Initialize \(\lambda^g_{0} = 0\)
\FOR{$t = 1, \dots, T$}
    \STATE \( D_t \in \mathcal{O}_\epsilon(\cdot, \lambda_{i,t-1}) \)
    \STATE \( \lambda_{t+1} = \text{no-regret-update}(D_t,\lambda_{t},  C) \) \emph{note that this will run one iteration of the subsequent no-regret algorithms provided}
    \STATE \( \hat{D}_t = \frac{1}{t} \sum_{t'=1}^{t} D_{t'} \); \( \hat{\lambda}_t = \frac{1}{t} \sum_{t'=0}^{t} \lambda_{t'} \)%; \(\overline{U} = \hat{U}(\hat{D}_t, \text{Best}_{\lambda}(\hat{D}_t)) \) \hfill 
\ENDFOR
%\STATE \( c_{i,t} = \mathbb{E}_{\pi \sim D_t} \left[ V^i(\pi) - \alpha \right] \)

\STATE \textbf{return} \( (\hat{D}_T, \hat{\lambda}_T) \)
\hfill
\end{algorithmic}
\label{alg:MORL-BRNR}
\end{algorithm}

\begin{theorem}(Informal)[No-Regret Player Guarantee]
For a sequence of best-response policies $(D_1,...,D_T)$ and Lagrangian weights $(\lambda_1,...,\lambda_T)$ maintained by the learner and regulator in MORL-BRNR [Algorithm \ref{alg:MORL-BRNR}],
    \begin{equation*}
        \begin{aligned}
           \sum_{t=1}^T\E_{\pi \sim D_t}[U(D_t,\lambda_t)]- \min_{\lambda \in \Lambda} \sum_{t=1}^T\E_{\pi \sim D_t}[U(D_t,\lambda)] \\ \leq reg(T,C,\gamma) \enspace ,
        \end{aligned}
    \end{equation*}
    where $reg(T,C,\gamma)$ is sublinear in $T$. 
\end{theorem}
In this paper, we will provide this form of a regret guarantee for the regulator with two different algorithms.
\begin{theorem}[Approximate Min-Max]
For a sequence of distributions over best-response policies $(D_1,...,D_T)$ and Lagrangian weights from no-regret updates $(\lambda_1,...,\lambda_T)$ maintained by the learner and regulator, MORL-BRNR [Algorithm \ref{alg:MORL-BRNR}] returns a $\nu$-approximate solution $(\hat{D},\hat{\lambda})$ of the minimax reward problem defined by $U$.


\label{thm:apx_minmax}
\end{theorem} 
\begin{proof}
    Provided in Appendix \ref{app:proof_apx_minmax}.
\end{proof}
\subsection{Tabular MDPs}
\label{sect:tabular}
In the case of tabular MDPs, we adapt the algorithm Follow the Perturbed Leader (FTPL) given by \cite{kalai2005efficient} to our learning setting.  
\begin{algorithm}
\caption{FTPL in Tabular MDPs}
\begin{algorithmic}[1]
\STATE \textbf{Input:} 
\( \cG \) groups, \( r \) reward function, access to MDP \( \mathcal{M} \), \(\eta=\sqrt{\tfrac{C}{2|S|T}}\)
\STATE Initialize cumulative losses $c_0(s) = 0$ $\forall s$
\FOR{$t = 1, \dots, T$}
    \STATE Receive $D_t$ from the Learner
    \STATE Sample a noise vector $N^t \sim \cU([0,\frac{1}{\eta}]^{|S|})$
    \STATE Define $c^{<t}(s) = \sum_{i=0}^{t-1}c_i(s)$ \hfill \emph{cumulative cost seen so far}
    \STATE $g_t \gets $Lin-OPT$(c^{<t} + N^t)$
    \STATE Assign $\lambda_{g,t}$ based on the selection of $g_t$ (i.e. $Ce_k$ for min group $k$ from $g_t$, $0$ elsewhere)
    \STATE Sample $(s_t, a_t)$ by executing a trajectory under $D_t$ and returning $(s_h,a_h)$, for $h \sim \cU([H])$
    \STATE  Observe $c_t(s_t) = r(s_t,a_t)$ 
    
\ENDFOR

\end{algorithmic}
\label{alg:FTPL}
\end{algorithm}


\begin{lemma}[FTPL Regret] For any sequence $\{D_t\}_{t=1}^T$ selected by the learner, for \\ $T\geq \max\{\frac{32C^2|S|^6}{\gamma^2},\frac{2C^2}{\gamma^2}\ln(\frac{2|\mathcal{G}|}{\delta})\}$ with probability at least $1-\delta$,
    $$\sum_{t=1}^H U(D_t,\lambda_t)-\min_{\lambda \in \Lambda}\sum_{t=1}^H U(D_t,\lambda) \leq \gamma T \enspace .$$
    \label{lem:FTPL}
\end{lemma}
\begin{proof}
Proof provided in Appendix \ref{app:tabular_pf}.

 \end{proof}
 

\begin{corollary}[Tabular Apx-Minimax]
       With probability at least $1-\delta$, using Algorithm $\ref{alg:MORL-BRNR}$ with FTPL (Algorithm \ref{alg:FTPL}) converges to a $\nu$-approximate solution of the minimax reward problem in $poly(\frac{1}{\gamma},\frac{1}{\delta},|S|,\log{|\mathcal{G}|}).$
\end{corollary}
 

\begin{proof}
    For $\gamma \leq\ \frac{\nu}{2}$ and $\epsilon \leq \frac{\nu}{2}$, this satisfies the no-regret condition necessary for Theorem \ref{thm:apx_minmax} to hold in the corresponding time specified by Lemma \ref{lem:FTPL}
\end{proof}
\subsection{Large State Space MDPs and Groups with Separator Sets}
\label{sect:gftpl}
When we are no longer in the tabular setting (and therefore cannot directly run FTPL over $\mathcal{S}$), it is computationally inefficient to run FTPL over a very large collection of groups. Therefore, we utilize a contextual variant of FTPL~\cite{syrgkanis2016efficient,dudik2020oracle}, which is computationally efficient in very large action spaces. 
\begin{algorithm}[t]
\caption{Contextual FTPL in Large State Space MDPs}
\begin{algorithmic}[1]
\STATE \textbf{Input:} 
 \( \mathcal{G} \) groups, $x_{1:d}$ sequence of separator elements, \( r \) reward function, access to MDP \( \mathcal{M} \), ${\rho = \sqrt{\tfrac{\log|\cG|}{Td^{1/2}}}}$ 
\STATE Initialize $\lambda_{0}=0$
\FOR{$t = 1, \dots, T$}
    \STATE Receive $D_t$ from the Learner
    \STATE Draw $\eta_j \sim \lap(\rho)$ independently for $j=1,...,d$
    \STATE Let $s_{t-1}^{+} = s_{1:t-1} \| x_{1:d} $ be the sequence of contexts played thus far, concatenated with the sequence of separator set elements
    \STATE Let $y_{t-1}^{+} = y_{1:t-1} \| \eta_{1:d}$ be the sequence of actions played thus far, concatenated with the sequence of noise elements
    \STATE $g_t \gets$ OPT$(s^{+}_{t-1}, y_{t-1}^+)$
    \STATE Assign $\lambda_t$ based on the selection of $g_t$ 
    \STATE Sample $(s_t,a_t)$ by executing a trajectory under $D_t$ and returning $(s_h,a_h)$ for $h \sim \cU([H])$. 
    \STATE Observe $y_t = r(s_t,a_t)$ and receive $y_tg_t(s_t)$ 

\ENDFOR

\end{algorithmic}
\label{alg:GFTPL}
\end{algorithm}
We briefly recall the setting of contextual FTPL, and formulate our learning problem in this setting. At each time step $t$, the adversary (learner) selects a context $\sigma_t \in \Sigma$ and and action $y_t \in \cY$. The regulator then chooses a policy $\psi_t \in \Psi$ and receives payoff $f(\psi(\sigma_t),y_t).$ To capture our reinforcement learning problem, we take $\Sigma = \states$, $\cY = [0,1]$, and $\Psi = \cG$. The choices of the adversary at round $t$ will be determined by the policy $D_t$ learned by the best response oracle at round $t$. The adversary selects its context and action by first sampling $h \sim \cU([H])$ and executing a trajectory $\tau$ under $D_t$ to obtain $(s_h, a_h)$. The adversary then selects its action to be $y_t = r(s_t, a_t)$, for $a_t = D_t(s)$. The payoff function received by the regulator is then $f(g(s_t),r(s_t, a_t)) = g(s_t)r(s_t,a_t)$. 

The contextual FTPL algorithms of~\cite{syrgkanis2016efficient,dudik2020oracle} are efficient even in our setting of large state spaces and large $\cG$ only under the assumption of separator sets for the class of groups $\cG$. 
\begin{definition}[Separator \cite{goldman1993exact,syrgkanis2016efficient}]
    A set $X \subseteq \states$ is a separator for groups $\cG$ if for any two groups $g,g' \in \cG$ there exists a context $s \in \states$ such that $g(s) \neq g'(s).$ 
    For a separator $X$ of size $|X| = d$, we use $x_{1:d}$ to denote a sequence of separator set elements.
\end{definition}
\citet{neel2019use} note that many discrete concept classes that are well-studied in the PAC learning literature in $d$-dimensions like boolean conjunctions, disjunctions, parities, and halfspaces defined over the boolean hypercube have separator sets of size $d$. In our setting, we make use of the fact that we can represent group membership in terms of conjunctions over the $d$-dimensional feature space.

\iffalse
\begin{definition}[$\delta$-admissibility \cite{dudik2020oracle}]
A translation matrix ($\Gamma$) is $\delta$-admissible if any two rows of the matrix are distinct and the minimum non-zero difference between any two values within a column is at least
$\delta$.
\end{definition}
\fi


    
\iffalse
\begin{definition}[implementable 
 \cite{dudik2020oracle}]
    A translation matrix ($\Gamma$) is implementable if each column corresponds to a
scaled version of the expected reward of the learner against some finitely supported distribution of
actions of the adversary. More formally, a matrix $\Gamma$ is implementable with complexity $M$ if for each $j \in [N]$, there exists a weighted dataset $S_j$, with $|S_j| \leq M$, such that for all $x,x' \in \chi$
$$\Gamma_{xj}-\Gamma_{x'j} = \sum_{(w,y) \in S_j} w(f(x,y)-f(x',y)) \enspace .$$
\end{definition}
We discuss $\Gamma$ further in our proof of Lemma \ref{lem:GFTPL}. It is important to note that \cite{dudik2020oracle} discuss that in practice one tends not to explicitly write down or work with the $\Gamma$ matrix directly. Instead, implementability enables one to use a small synthetic history of adversary's actions can simulate the perturbations to each group. Additionally, it is important to note that in certain cases one can more efficiently represent and store $\lambda$ itself. 
\fi

\begin{lemma}[Contextual FTPL Regret]
     For any sequence of costs $c_t$, for 
     %$T \geq \max\{\frac{4 C^2d^2}{ \gamma^2},\frac{2C^2}{ \gamma^2}\ln(\frac{2|\mathcal{G}|}{\delta})\}$,   with probability at least $1-\delta$, 
     $T \geq O(\frac{C^2d^{3/2}\log{|\cG|}}{ \gamma^2} + \frac{C^2}{ \gamma^2}\ln(\frac{|\mathcal{G}|}{\delta}))$,  with probability at least $1-\delta$,
     the expected regret of FTPL is
    \begin{equation*}
        \begin{aligned}
          \sum_{t=1}^H U(D_t,\lambda_t)- \min_{\lambda \in \Lambda}\sum_{t=1}^H U(D_t,\lambda) \leq \gamma T \enspace .
        \end{aligned}
    \end{equation*}
    \label{lem:GFTPL}
\end{lemma}
\begin{proof}
Proof provided in Appendix \ref{app:gftpl_pf}.
\end{proof}
\begin{corollary}[Contextual FTPL Apx-Minimax]
    With probability at least $1-\delta$, using Algorithm \ref{alg:MORL-BRNR} with Contextual FTPL (Algorithm \ref{alg:GFTPL}) converges to a $\nu$-approximate solution of the minimax reward problem in $poly(\frac{1}{\gamma},\frac{1}{\delta},d,\log{|\mathcal{G}|})$.
\end{corollary}
\begin{proof}
For $\gamma \leq \frac{\nu}{2}$, $\epsilon \leq \frac{\nu}{2}$, Theorem \ref{thm:apx_minmax}'s no-regret conditions are satisfied in the times specified by Lemma \ref{lem:GFTPL}.
\end{proof}
\subsection{Large State Space MDPs with General Groups}
\label{sect:fairfict}

For large state spaces with general group structure, we utilize Fair and Fictitious Play (Follow the Leader (FTL) vs FTL)~\cite{brown1949some}. Fictitious Play is easier to implement as it is a deterministic algorithm and it only requires one oracle call per player. However, it is only theoretically known to converge to an equilibrium in the limit.

\begin{algorithm}
\caption{FairFictRL}
\begin{algorithmic}[1]
\STATE \textbf{Input:} bound \( C \),  best-response error \(\epsilon\),  \( \mathcal{G} \) groups, \( r \) reward function, access to MDP \( \mathcal{M} \)
\STATE Initialize \(\lambda_{0}^g = 0\) and $\hat{\lambda}_0^g = \lambda_0^g$
\STATE Initialize $\hat{D}_t$ by selecting a policy in $\Pi$
\FOR{$t = 1, \dots, T$}
    \STATE \( D_t \in \text{FTL}(\hat{\lambda}_{t-1},C) \) using $\mathcal{O}_\epsilon(\cdot,\hat{\lambda}_{t-1})$ \quad returns $\max_DU(D,\hat{\lambda}_{t-1})$
    \STATE \( \lambda_{t} = \text{FTL}(\hat{D_{t-1}},C) \) using $Best_\lambda(\hat{D}_{t-1})$\quad returns $\max_\lambda  U(\hat{D}_{t-1},\lambda)$
      \STATE \( \hat{D}_t = \frac{1}{t} \sum_{t'=0}^{t} D_{t'} \); \( \hat{\lambda}_t = \frac{1}{t} \sum_{t'=0}^{t} \lambda_{t'} \)
\ENDFOR
 \STATE \textbf{return} \( (\hat{D}_T, \hat{\lambda}_T) \)
\end{algorithmic}
\label{alg:FairFictRL}
\end{algorithm}
\begin{lemma}[FairFict Convergence]
    \cite{robinson1951iterative} FairFictPlay converges in the limit to an equilibrium.
\end{lemma}
\citet{robinson1951iterative} showed that fictious play converges in the limit to equilibrium. \citet{daskalakis2014counter} showed that the convergence can be exponentially slow in the worst-case, at least with adversarially selected tie-breaking rules. Nevertheless, in non-adversarial settings it appears to be a practical algorithm. We use this algorithm in our experiments and also empirically demonstrate that it performs well, despite other algorithms' stronger theoretical guarantees.
\section{Experimental Results}
\label{sect:experiments}


The experimental section focuses on an evaluation of the FairFictRL algorithm. 

FairFictRL is a very easy to implement version of our framework and even though it only has asymptotic guarantees, we will demonstrate that it can quickly converge to desirable solutions in practice.

\subsection{MDP construction}
\begin{figure}[H]
    %\vspace{-6pt}
    \centering
    \subfloat[]{\includegraphics[height=4cm, trim={5cm 3cm 5cm 3cm},clip]{figures/lambda_0.png} \label{fig:opt_occupancy}} 
    \hspace{12pt}
    \subfloat[]{\includegraphics[height=4cm, trim={5cm 3cm 2cm 3cm},clip]{figures/morl.png} \label{fig:morl_occupancy}}
    \caption{Depiction of the Barabási-Albert graph we use as an MDP. Nodes correspond to states and actions are deterministic moves to other nodes. Every state also has a self-loop action to remain in place. The groups are assigned depending on the number of outgoing edges. All nodes with $1$ or $2$ outgoing edges are in Group $0$, nodes with $3$ outgoing edges are in Group $1$ and all others are in Group $2$. Rewards $r(s, a)$ are assigned as $0.1$ for all $s$ in Group $1$, $0.2$ for all nodes in Group $1$ and $0.3$ for nodes in Group $2$. The start state is a random node in the graph in every episode. Figure (a) shows the occupancy distribution of the (non-fair) optimal policy. As we can see, the non-fair policy's goal is to quickly get to one of the nodes with $5$ edges (Group 2) and stay there indefinitely to accumulate reward. After running our FairFictRL algorithm in Figure (b), the distribution over nodes is almost evenly spread across all nodes. The only outlier is node $8$, as it is the only node that belongs to Group $1$ and thus requires a large visitation number to satisfy our constraints.}
    \label{fig:graphs}
\end{figure}
Preferential attachment graphs are a commonly used model of social networks exhibiting a ``rich get richer'' distribution of connectivity. We construct a multi-objective RL task based on such graphs using the Barabási-Albert model~\citep{barabasi2016network}.
in which nodes with higher degree tend to have a higher probability of forming connections with newer nodes on the network. 
In rounds, one new node is added to the graph until the final number of nodes is reached. Each new node forms $n_e$ new connections with probability proportional to the degree of existing nodes.

Using this model, we construct the following graph MDP. The state space corresponds to the nodes on the graph and actions to edge selection on the current node. There is a scalar reward of $r(s, a)$ each time an edge is selected from a node. Each node is assigned to possibly several intersecting groups based on its degree. A learner is trying to traverse the graph with the goal of maximizing total cumulative reward, while maintaining that all groups receive at least average reward $\alpha/H$. The graph we consider is depicted in Figure~\ref{fig:graphs}.

\subsection{FairFictRL Evaluation}

First, we run FairFictRL with a minimum value of average group reward  that we want to achieve of $\frac{\alpha}{H} = 0.04$. Given the reward stucture in the graph, it is not the case that all groups will obtain this reward under the optimal policy. We use standard value iteration as an oracle for the learner; for higher-dimensional problems, this could be substituted with a deep RL algorithm. The regulator runs the policy returned by the learner for $500$ episodes to obtain an average reward estimate and best responds according to Definition~\ref{def:reg_best} with $ Ce_{k}=25$. The results are shown in Figure~\ref{fig:avg_reward}.
\vspace{-.7em}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth,trim={0 0 0 2cm}, clip]{figures/morl_rewards.png}
    \caption{Average reward to groups during a run of FairFictRL on the MDP depicted in Figure~\ref{fig:graphs} for $\frac{\alpha}{H}=0.04$. The optimal (non-fair) behavior in the MDP is to move to any Group $2$ node and stay there indefinitely, which achieves at most $0.3$ average reward. As we run FairFictRL, the learned mixture policy quickly ensures that all groups obtain at least $\frac{\alpha}{H}$ average reward.}
    \label{fig:avg_reward}
\end{figure}
 As we can see, after only a few iterations, all groups obtain $0.04$ average reward, which is consistently upheld throughout the execution of the algorithm. Furthermore, we demonstrate in Figure~\ref{fig:graphs} that this corresponds to a more evenly distributed occupancy of the graph.

Finally, we analyze the Pareto frontier between fairness and maximum cumulative reward that can be obtained by varying values of $\alpha$ in Figure~\ref{fig:bar_pareto}. The results highlight that by increasing the minimum average reward that we enforce, our algorithm equalizes the rewards between all groups more an more. This comes at the expense of total reward $V^{tot}$ which highlights the trade-off between fairness and optimality.

\begin{figure}[H]
    % \vspace{-13pt}
    \centering
    \includegraphics[width=0.75\linewidth,trim={0 0 0 0cm}, clip]{figures/barplot_taller.png}%{figures/barplot_taller.png}
    %\vspace{-1em}
    \caption{Total average reward compared to per-group average reward for varying $\alpha$. As the fairness constraint increases, total reward decreases while group rewards equal out until eventually all groups obtain the same average reward.}
    \label{fig:bar_pareto}
    %\vspace{-1em}
\end{figure}


\section{Conclusion and Future Work}
We consider MDPs with possibly an exponentially large number of intersecting groups over states. For solving a class of corresponding constrained or multi-objective optimization problems over these very large number of objectives, we provide several oracle-efficient algorithms. Namely, we provide algorithms for tabular MDPs and large state-spaces with separator sets over groups. In these approaches, we reduce the learner's problem to that of standard RL with one scalarized objective to learn a near-optimal policy. We reduce the regulator's problem to one of linear optimization. We also provide a version of Fictitious Play for large state-spaces with general group functions that has provable asymptotic guarantees. In practice, our experiments showed that they converge quickly for the preferential attachment graphs provided in our paper.

\textbf{Limitations and Future Work}~~
Of course, each of these reductions relies on our ability to efficiently implement our Best Response and Optimization oracles. Moreover, many of our results are reliant on boolean group structure. It would be interesting to extend our results beyond exponentially large group classes to infinite group function classes. We also hope to apply these results to the setting of Reinforcement Learning with Human Feedback, especially with access to more fine-grained subgroup data. 
\section*{Acknowledgments}
This research was partially supported by the Simons collaboration on algorithmic fairness, NSF grants FAI-2147212 and CCF-2217062, the Army Research Office under MURI award W911NF201-0080, the DARPA Triage Challenge under award HR00112420305, and by the University of Pennsylvania ASSET center. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, the Army, or the US government. We would also like to thank Alejandro Ribeiro for his discussions with us.

\newpage


\bibliography{arxiv_fair_rl}
\bibliographystyle{abbrvnat}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Proof of Theorem \ref{thm:apx_minmax}}
\label{app:proof_apx_minmax}

We want to show that $\frac{1}{T} \sum \lambda_t, \frac{1}{T} \sum D_t$ converge to an approximate minimax solution of the game if we have that
\[
     \sum_{t=1}^{T} \mathbb{E}\left[ U(\hat{D}_t, \hat{\lambda}_t) \right] - \min_{D\sim \mathbb{1}_{\Pi}}\sum_{t=1}^{T} \mathbb{E}\left[ U(D, \hat{\lambda}_t) \right] \leq BR_{L}(T,C,\epsilon)
\]
and that 
\[
    \max_{\lambda \in \Lambda} \sum_{t=1}^{T} \mathbb{E} \left[ U(\hat{D}_t, \lambda) \right] - \sum_{t=1}^{T} \mathbb{E} \left[ U(\hat{D}_t, \hat{\lambda}_t) \right] \leq reg_{A}(T,C,\gamma)
\]
 which are sublinear in $T$.
We want to show that:\\
First for any $\lambda$,
\begin{align}
\text{want to show: } U(\hat{D}_T, \lambda) &\geq U(\hat{D}_T, \hat{\lambda}_T) - \frac{1}{T}reg_{A}(T,C,\gamma) -\epsilon\\
  U(\hat{D}_T, \lambda)  &= U\left( \frac{1}{T} \sum_{t=1}^{T} D_t, \lambda \right) \quad (\text{linear in } D) \\
    &= \frac{1}{T} \sum_{t=1}^{T} U(D_t, \lambda)\\
    &\geq \frac{1}{T} \sum_{t=1}^{T} \left( U(D_t, \lambda_t)\right) - \frac{1}{T}reg_{A}(T,C,\gamma)  \quad (\text{because no-regret}) \\
    &\geq \frac{1}{T} \sum_{t=1}^{T} U(\hat{D}_t, \lambda_t) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \quad (\text{because apx-best response}) \\
    &= U(\hat{D}_T, \hat{\lambda}_T) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \quad (\text{because linearity}) \\
\end{align}
Second for any $D$,
\begin{align}
\text{want to show: } U(D, \hat{\lambda}_T) &\leq U(\hat{D}_T, \hat{\lambda}_T) +\frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon \\
  U(D, \hat{\lambda}_T)  & = \frac{1}{T} \sum_{t=1}^{T} U(D, \lambda_t) \\
    &\leq \frac{1}{T} \sum_{t=1}^{T} U(D_t, \lambda_t) +\epsilon\quad (\text{because } D_t \text{ is apx-best response}) \\
    &\leq \frac{1}{T} \sum_{t=1}^{T} U(D_t, \hat{\lambda}_T)  +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon\quad \text{(because no-regret)}\\
    &= U(\hat{D}_T, \hat{\lambda}_T) +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon \quad \text{(because linearity)}
\end{align}
Thus, for any \(\lambda, D\):
\begin{align*}
U(D, \hat{\lambda}_T) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \leq U(\hat{D}_T, \hat{\lambda}_T) \leq   U(\hat{D}_T, \lambda)  +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon
\end{align*}
\\

So if \(\nu \geq  \frac{1}{T}reg_{A}(T,C,\gamma) +\epsilon \), then we have gotten an approximate minimax solution using 
\cite{freund1996game}.
Note that this algorithm makes $O(T)$ best-response oracle queries and $O(T)$ Lin-OPT queries for the learner and regulator respectively.
\subsection{Proof of Lemma \ref{lem:FTPL}}
\label{app:tabular_pf}
\begin{proof}
\cite{kalai2005efficient} and \cite{LearningInGames}, FTPL's regret can be bounded by

\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g_t(s_t)-\min_{g \in \mathcal{G}}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g(s_t)\Big| \leq \frac{\sqrt{8|S|^3}}{\sqrt{T}}
    \end{aligned}
\end{equation*}
\\
For For $T \geq \frac{8C^2|S|^6}{\gamma_1^2}$, we have that 
\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g_t(s_t)-\min_{g \in \mathcal{G}}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g(s_t)\Big| \leq \frac{\gamma_1}{C}
    \end{aligned}
\end{equation*}
\\
Recall the definition of $V^g(\pi_t) = H\E_{t\in[H]}\E_{P,\pi,\mu}[(r(s_t,a_t)g(s_t)]$. Define $Y_t = \frac{1}{T}((r(s_t,a_t))g_t(s_t)-(r(s_t,a_t))-\frac{1}{TH}((V^{g_t}(\pi_t))-(V^{g}(\pi_t))).$ We can show that the difference sequence $\{Y_t\}_{t=1}^T$ forms a Martingale Difference Sequence. Therefore $\sum_t Y_t$ also forms a Martingale.
Using Azuma-Hoeffding's inequality, for any $\lambda \in \Lambda$
\begin{equation*}
    \begin{aligned}
        Pr\Big(\Big|\frac{1}{T}(r(s_t,a_t))g_t(s_t)-(r(s_t,a_t)))g(s_t))-\frac{1}{TH}((V^{g_t}(\pi_t))-(V^{g}(\pi_t)))\Big| \geq \frac{\gamma_2}{C}\Big)  \leq 2\exp{\frac{-2T\gamma_2^2}{C^2}}
    \end{aligned}
\end{equation*}
Union bounding over all groups, we want 
$$2|\mathcal{G}|\exp{\frac{-2T\gamma_2^2}{C^2}}\leq \delta.$$

Therefore, for $T \geq \frac{C^2}{2 \epsilon_2^2}\ln(\frac{2|\mathcal{G}|}{\delta})$, with probability at least $1-\delta$, for all groups $g \in \mathcal{G}$,

\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{T}((r(s_t,a_t))g_t(s_t)-\min_{g \in \G} (r(s_t,a_t))g(s_t))-\frac{1}{TH}((V^{g_t}(\pi_t))-\min_{g \in \G}(V^{g}(\pi_t)))\Big| \leq \frac{\gamma_2}{C}
    \end{aligned}
\end{equation*}
Therefore, 
\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{TH}\sum_{t=1}^T (V^{g_t}(\pi_t))-\min_{g \in \G}\frac{1}{TH}\sum_{t=1}^T (V^{g}(\pi_t))\Big| \leq \frac{\gamma_1+\gamma_2}{C}
    \end{aligned}
    \end{equation*}

Notice that 
\begin{align*}
\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t)g_t(s_t)-)&-\min_{g\in \G}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t)g_t(s_t) )\\
= (\frac{1}{T}\sum_{t=1}^Tr(s_t,a_t)+r(s_t,a_t)g_t(s_t)-\frac{\alpha}{H})  &-(\min_{g\in \G}\frac{1}{T}\sum_{t=1}^T r(s_t,a_t)+r(s_t,a_t)g(s_t) -\frac{\alpha}{H})\\
\end{align*} 
Similarly, 
\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{TH}\sum_{t=1}^T (V^{g_t}(\pi_t))&-\min_{g \in \G}\frac{1}{TH}\sum_{t=1}^T (V^{g}(\pi_t))\Big| \\
        = (\frac{1}{TH}\sum_{t=1}^T \frac{1}{H}V^{tot}(\pi_t)+V^{g_t}(\pi_t)-\alpha)&-(\min_{g \in \G}\frac{1}{TH}\sum_{t=1}^T \frac{1}{H}V^{tot}(\pi_t)+V^{g}(\pi_t)-\alpha) \\
        & \leq \frac{\gamma_1+\gamma_2}{C}
    \end{aligned}
    \end{equation*}
To see the final connection with the objective $U$, we need to introduce $\lambda$. Observe that we could have defined the action space of FTPL in terms of $\lambda \in \Lambda$, where for every $g$, $\lambda^g \in \mathbb{R}^{|S|}$ and $\lambda^g(s)= Cg(s).$ That is, every selection of $g \in \G$ has a corresponding $\lambda^g \in \Lambda$ which puts weight $C$ on the dimension corresponding to the $gth$ group. It is important to distinguish different representations of $\lambda$. $\lambda \in \Lambda$ is a $|\G|$-dimensional vector over groups. $\lambda^g$, however, is an $|S|$-dimensional vector over the state space for the selected group $g$. We acknowledge that we overload this notation because $\lambda^g$ without mention of a state can refer to the weight placed by the regulator on that group. Hence, for a given selected $\lambda \in \Lambda$, we use the notation $\lambda^g(s)$ to denote the corresponding group selected out of that regulator weight assignment and then its corresponding values over the state space. This notation makes explicit the implicit relationship between group selection and $\lambda$ selection. 
We can scale the above terms $r(s_t,a_t)g_t(s_t)-\frac{\alpha}{H}$ and $r(s_t,a_t)g(s_t)-\frac{\alpha}{H}$ by the corresponding $\lambda^{g_t}(s_t)$ or $\lambda^{g}(s_t)$ value. To get $r(s_t,a_t)\lambda^{g_t}(s_t)-\frac{C\alpha}{H}$ and $r(s_t,a_t)\lambda^g(s_t)-\frac{C\alpha}{H}$ (notice that the vector $\lambda \in \R^{|\G|}$ is $C$ times a one-hot encoding).
Similarly, 
\begin{equation*}
    \begin{aligned}
        (\frac{1}{H}V^{tot}(\pi_t)+\sum_{g \in \G} \lambda^{g_t} (\frac{1}{TH}\sum_{t=1}^T (V^{g_t}(\pi_t)-\alpha)))&-(\frac{1}{H}V^{tot}(\pi_t)+\min_{g \in \G}\sum_{g \in \G} \lambda^{g}(\frac{1}{TH}\sum_{t=1}^T (V^{g}(\pi_t)-\alpha))) \\
        \leq \gamma_1&+\gamma_2
    \end{aligned}
    \end{equation*}
    Finally,
 $$\frac{1}{T}\sum_{t=1}^H U(D_t,\lambda_t)-\min_{\lambda \in \Lambda}\frac{1}{T}\sum_{t=1}^H U(D_t,\lambda) \leq \gamma$$
 for $\gamma_1,\gamma_2 = \frac{\gamma}{2}$ and noting that $\pi_t$ is the policy specified by $D_t$.
\end{proof}
\subsection{Proof of Lemma \ref{lem:GFTPL}}
\label{app:gftpl_pf}
\begin{proof}
We utilize \cite{syrgkanis2016efficient} and \cite{dudik2020oracle} for our regret bound and construction.
\cite{syrgkanis2016efficient} prove that for Laplace noise $\lap(\rho)$ and groups $\cG$ with separator $X$ of size $d = |X|$, their contextual FTPL algorithm enjoys regret
\begin{align*}
 \Big|\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g_t(s_t)-\min_{g \in \mathcal{G}}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g(s_t)\Big|  
 &\leq O\left(\frac{\rho d}{T}\sum_{t=1}^T\E\left[\max_{g\in \cG}(g(s_t)r(s_t,a_t))^2\right] + \frac{\sqrt{d}\log(|\cG|)}{\rho T} \right) \\
 &\leq O\left(\frac{d^{3/4}\sqrt{\log|G|}}{\sqrt{T}}\right)
\end{align*}
for optimal choice of $\rho$, and runs in time poly$(d,T,\log(|\cG|))$. 
\iffalse 
    \cite{dudik2020oracle} prove that if $\Gamma$ is implementable with complexity $M$ and $\delta_*$-admissible, then 
    \begin{equation*}
        \begin{aligned}
            \Big|\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g_t(s_t)-\min_{g \in \mathcal{G}}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g(s_t)\Big|  \leq O(d/\delta_*\sqrt{T})
        \end{aligned}
    \end{equation*} in time $poly(d,M,T).$ Call the separator set $X$ for our conjunctions which are of size $d$. We can construct our $\Gamma$ matrix such that $\Gamma_{i,j}=g_i(X_j)$, where $g_i$ is the group at the $i$th row and $X_j$ is the $j$th separator element. Notice that this construction would make $\Gamma$ $1$-admissible. Now, to make $\Gamma$ implementable, we could modify our MDP as follows. First we can make $d$ additional states $\tilde{s}^k$ such that $x(\tilde{s}^k)=X_k$. Then, we could augment the transitions such that each state $\tilde{s}^k$ only ever transitions into itself. We can also creating an artificial starting state and action that all groups are a member of. Then, we can add policies to $\Pi$ that are a function of those augmented states in addition to the original states and transitions from the artificial start state to these augmented states. Then, each column of $\Gamma$ will correspond to a scaled version of the reward accumulated to each group. 
    Note that if we do not want to use implementability and admissibility, our construction is directly related to that of \cite{syrgkanis2016efficient} as well, especially when we sample noise according to a Laplace distribution for a specified noise level. Their corresponding regret bound for our setting would be $O(\frac{d^{3/4}\sqrt{ \log{|\G|}}}{\sqrt{T}})$.
\fi
Therefore, for the below condition to hold, we would need $T \geq \frac{C^2d^{3/2}\log{|\G|}}{\gamma_1^2}$.

We will use similar notation and arguments to the proof of Lemma \ref{lem:FTPL}.
     For $T \geq \frac{C^2d^{3/2}\log{|\cG|}}{\gamma_1^2},$
    \begin{equation*}
        \begin{aligned}
             \Big|\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g_t(s_t)-\min_{g \in \mathcal{G}}\frac{1}{T}\sum_{t=1}^T (r(s_t,a_t))g(s_t)\Big| \leq \frac{\gamma_1}{C}
        \end{aligned}
    \end{equation*}
    Once again, we use a similar argument to the proof of Lemma \ref{lem:FTPL},
    Using Azuma-Hoeffding's inequality for a given $\lambda$,
\begin{equation*}
    \begin{aligned}
       Pr\Big(\Big|\frac{1}{T}(r(s_t,a_t))g_t(s_t)-(r(s_t,a_t)))g(s_t))-\frac{1}{TH}((V^{g_t}(\pi_t))-(V^{g}(\pi_t)))\Big| \geq \frac{\gamma_2}{C}\Big)\leq 2\exp{\frac{-2T\gamma_2^2}{C^2}}
    \end{aligned}
\end{equation*}
Union bounding over all groups, we want 
$$2|\mathcal{G}|\exp{\frac{-2T\gamma_2^2}{C^2}}\leq \delta.$$

Therefore, for $T \geq \frac{C^2}{2 \gamma_2^2}\ln(\frac{2|\mathcal{G}|}{\delta})$, with probability at least $1-\delta$, for all groups $g\in \mathcal{G}$,
\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{T}((r(s_t,a_t))g_t(s_t)-\min_{g \in \G} (r(s_t,a_t))g(s_t))-\frac{1}{TH}((V^{g_t}(\pi_t))-\min_{g \in \G}(V^{g}(\pi_t)))\Big| \leq \frac{\gamma_2}{C}
    \end{aligned}
\end{equation*}
Therefore, 
\begin{equation*}
    \begin{aligned}
        \Big|\frac{1}{TH}\sum_{t=1}^T (V^{g_t}(\pi_t))-\min_{g \in \G}\frac{1}{TH}\sum_{t=1}^T (V^{g}(\pi_t))\Big| \leq \frac{\gamma_1+\gamma_2}{C}
    \end{aligned}
    \end{equation*}
Finally using a very similar argument to the proof of Lemma \ref{lem:FTPL} to relate to $\lambda$,
 $$\frac{1}{T}\sum_{t=1}^H U(D_t,\lambda_t)-\min_{\lambda \in \Lambda}\frac{1}{T}\sum_{t=1}^H U(D_t,\lambda) \leq \gamma$$
 for $\gamma_1,\gamma_2 = \frac{\gamma}{2}.$
\end{proof}

\subsection{Error Cancellations}
\label{app:err_canc}
Recall the constrained RL problem: 
\begin{equation*}
    \begin{aligned}
    \max_{D \in \Delta \Pi} \quad & \E_{\pi \sim D}[V^{tot}(\pi)] \quad \\
    \text{subject to } &\E_{\pi \sim D} \left[V^{g}(\pi) \right] \geq \alpha, \quad \forall g \in \mathcal{G}
\end{aligned}
\end{equation*}
In this section, we will use the shorthand $AV^{tot}(\pi)$ to denote the average reward rather than $V^{tot}(\pi)$ and $AV^g(\pi)$ for the average reward to group $g$. Now, we will also define $AV^{tot}(D)=\E_{\pi \sim D}[AV^{tot}(\pi)]$ and $AV^{g}(D)=\E_{\pi \sim D}[AV^{g}(\pi)].$

Now define $h^g(D) = \frac{\alpha}{H}-AV^{g}(D)$ and $f(D) = AV^{tot}(D).$ Now, we can rewrite the constrained RL problem as:
\begin{equation*}
    \begin{aligned}
    \max_{D \in \Delta \Pi} \quad & f(D) \quad \\
    \text{subject to } & h^g(D) \leq 0, \quad \forall g \in \mathcal{G}
\end{aligned}
\end{equation*}

Now, notice that $h^g(D)$ is affine (and therefore convex) in $D$ and $0$ is a constant so $\max\{h^g(D),0\}$ is convex in $D$. Therefore, we can rewrite our optimization problem as: 
\begin{equation*}
    \begin{aligned}
    \max_{D \in \Delta \Pi} \quad & f(D) \quad \\
    \text{subject to } & \max\{h^g(D),0\} \leq 0, \quad \forall g \in \mathcal{G}
\end{aligned}
\end{equation*}
Notice that the feasibility sets over $D$ are the same between these two problems. Now, we can write the following Lagrangian. 
$$\mathcal{L}(D,\lambda)=f(D)-\sum_{g \in \mathcal{G}}\lambda^g( \max\{h^g(D),0\}).$$

The conditions of Sion's minimax theorem still hold for this Lagrangian. Observe that for this objective, when considering the notion of average violations, one does not face the issue of error cancellations because the max term is always nonnegative. For the learner, we can define a new scalarized reward function in terms of the regulator's selection of $\lambda$ for which it can utilize the standard approximate best response oracle. Observe now however, that $k^g(D)$ is now convex in $D$ rather than linear, but we still need to provide estimates of $\max\{h^g(D),0\}.$


 Define $k(x) = \max\{x,0\}.$ We want to bound $|k(h^g(D))-k(\hat{h}^g(D))|.$ Notice, that to get our good estimate of $\hat{h}^g(D_t))$, we will sample from this procedure and average the following: sample a policy from $D_t$ and generate a trajectory, then uniformly sample a time step $h$ and receive the corresponding $s_h,a_h,r(s_h,a_h).$ Now, we will average a sufficiently large number of terms$\frac{\alpha}{H}-r(s_h,a_h)g(s_h)$ to get an estimate for $\hat{h}^g(D)$ within $\epsilon'$ using Hoeffding's inequality. Union-bounding over groups, we want $2|\mathcal{G}|e^{-2n\epsilon'^2} \leq \delta'$, which requires $n \geq \frac{1}{\epsilon'^2}\frac{\ln (2 |\mathcal{G}|)}{\delta'}.$ We would like $\epsilon' \in O(\frac{1}{C\sqrt{T}})$ (because of its contribution to overall regret). Therefore, we would like  $n \geq T\frac{\ln (2 |\mathcal{G}|)}{\delta'}.$
Now assume that with probability at least $1-\delta'$, for all $g \in \G$, $(\hat{h}^g(D)-h^g(D)) \leq \epsilon'$.

It is important to note however, that since the objective is now concave in $D$, rather than linear. 
\begin{align*}
    r_{\lambda}(s_t, a_t) &=  \left( r(s_t, a_t) - \sum_{g \in \mathcal{G}} \lambda^{g}  \max\{\hat{h}^g(D),0\} \right) \enspace.   
\end{align*}
Notice that this reward has some error from the "true" scalarized reward and therefore, our learner will be approximate best responding. 

\begin{theorem}[Approximate Min-Max Error Cancellation]
For a sequence of distributions over best-response policies $(D_1,...,D_T)$ and Lagrangian weights from no-regret updates $(\lambda_1,...,\lambda_T)$ maintained by the learner and regulator, MORL-BRNR [Algorithm \ref{alg:MORL-BRNR}] returns a $\nu$-approximate solution $(\hat{D},\hat{\lambda})$ of the minimax rewards problem defined by $\Lag$.
%include poly-time discussion
\label{thm:apx_minmax_err_canc}
\end{theorem} 
\begin{proof}

First for any $\lambda$,
\begin{align}
\text{want to show: } \Lag(\hat{D}_T, \lambda) &\geq \Lag(\hat{D}_T, \hat{\lambda}_T) - \frac{1}{T}reg_{A}(T,C,\gamma) -\epsilon\\
  \Lag(\hat{D}_T, \lambda)  &= \Lag\left( \frac{1}{T} \sum_{t=1}^{T} D_t, \lambda \right) \quad (\text{linear in } D) \\
    &\geq \frac{1}{T} \sum_{t=1}^{T} \Lag(D_t, \lambda) \quad (\text{concavity in} D)\\
    &\geq \frac{1}{T} \sum_{t=1}^{T} \left( \Lag(D_t, \lambda_t)\right) - \frac{1}{T}reg_{A}(T,C,\gamma)  \quad (\text{because no-regret}) \\
    &\geq \frac{1}{T} \sum_{t=1}^{T} \Lag(\hat{D}_t, \lambda_t) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \quad (\text{because apx-best response}) \\
    &= \Lag(\hat{D}_T, \hat{\lambda}_T) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \quad (\text{because linearity}) \\
\end{align}
Second for any $D$,
\begin{align}
\text{want to show: } \Lag(D, \hat{\lambda}_T) &\leq \Lag(\hat{D}_T, \hat{\lambda}_T) +\frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon \\
  \Lag(D, \hat{\lambda}_T)  & = \frac{1}{T} \sum_{t=1}^{T} \Lag(D, \lambda_t) \\
    &\leq \frac{1}{T} \sum_{t=1}^{T} \Lag(D_t, \lambda_t) +\epsilon\quad (\text{because } D_t \text{ is apx-best response}) \\
    &\leq \frac{1}{T} \sum_{t=1}^{T} \Lag(D_t, \hat{\lambda}_T)  +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon\quad \text{(because no-regret)}\\
    & \leq \Lag(\hat{D}_T, \hat{\lambda}_T) +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon \quad \text{(because concavity)}
\end{align}
Thus, for any \(\lambda, D\):
\begin{align*}
\Lag(D, \hat{\lambda}_T) -  \frac{1}{T}reg_{A}(T,C,\gamma)-\epsilon \leq \Lag(\hat{D}_T, \hat{\lambda}_T) \leq   \Lag(\hat{D}_T, \lambda)  +  \frac{1}{T}reg_{A}(T,C,\gamma)+\epsilon
\end{align*}
\\
%So if \(\nu \geq  \frac{1}{T}reg_{A}(T,C,\gamma) +\epsilon \geq \nu_T\), then we have gotten an approximate minimax solution using 
So if \(\nu \geq  \frac{1}{T}reg_{A}(T,C,\gamma) +\epsilon \), then we have gotten an approximate minimax solution using 
\cite{freund1996game}.
Note that this algorithm makes $O(T)$ best-response oracle queries and $O(T)$ Lin-OPT queries for the learner and regulator respectively.

Furthermore, notice that if we wanted to return one policy rather than $\bar{D}$ (deterministic), we could evaluate our selection of policies and based upon the best performing one could make a probabilistic argument for how likely that single policy is to simultaneously satisfy all constraints.
\end{proof}

\subsubsection{Generalized FTPL}
We still get our regret bound with respect to $\gamma$ for a sufficient polynomial sample complexity.


\begin{algorithm}[H]
\caption{Contextual FTPL Error Cancellation}
\begin{algorithmic}[1]
\STATE \textbf{Input:}  
\( \mathcal{G} \) group functions, $x_{1:d}$ sequence of separator elements, \( r \) reward function, access to MDP \( \mathcal{M} \), ${\rho = \sqrt{\frac{\log|\cG|}{Td^{1/2}}}}$  number of samples per iteration \(n\)
\STATE Initialize $\lambda_{0}=0$
\FOR{$t = 1, \dots, T$}
    \STATE Receive $D_t$ from the Learner
    \STATE Draw $\eta_j \sim \lap(\rho)$ independently for $j=1,...,d$
    \STATE For all $g \in \cG$, let $g_{x_{1:d}} = (g_t(x_1), \dots, g_t(x_d))$ in the following: \\
 Select $g_t$ such that $\sum_{i=0}^{t-1}c_i(g_t) +\eta \cdot g_{t,x_{1:d}} \leq \sum_{i=0}^{t-1}c_i(g) + \eta \cdot g_{x_{1:d}} + \epsilon$ for all $g$ 
    \STATE Assign $\lambda_t$ based on the selection of $g_t$ 
    \STATE Generate $n$ samples of $\{s_i,a_i,r(s_i,a_i)\}_{i=1}^n$ by creating $n$ independent trajectories of $D_t$ and uniformly sampling from $[H]$ in each one and computing an estimate of average reward and define $y_{t,i}=r(s_i,a_i)$ 
    \STATE Receive $c_t(g_t)= \max\{\frac{\alpha}{H}-\frac{1}{n}\sum_{i=1}^n y_{t,i}g(s_i),0\}$
\ENDFOR

\iffalse  

    \STATE Draw $\eta_j \sim \mathcal{N}$ independently for $j=1,...,d$
    \STATE Let $s_{t-1}^{+} = s_{1:t-1} \| x_{1:d} $ be the sequence of contexts played thus far, concatenated with the sequence of separator set elements
    \STATE Let $y_{t-1}^{+} = y_{1:t-1} \| \eta_{1:d}$ be the sequence of actions played thus far, concatenated with the sequence of noise elements
    \STATE $g_t \gets$ OPT$(s^{+}_{t-1}, y_{t-1}^+)$
    \STATE Assign $\lambda_t$ based on the selection of $g_t$ 
    \STATE Sample $(s_t,a_t)$ by executing a trajectory under $D_t$ and returning $(s_h,a_h)$ for $h \sim \cU([H])$. 
    \STATE Observe $y_t = r(s_t,a_t)$ and receive $y_tg_t(s_t)$ 
\fi

\end{algorithmic}
\label{alg:GFTPL_err_canc}
\end{algorithm}
\begin{lemma}[Contextual FTPL Regret]

For any sequence of costs $c_t$, for $T \geq O\left(\frac{C^2 d^{3/2}\log{|\cG|}}{\gamma_1^2} + \frac{C^2}{ \gamma_2^2}\ln(\frac{|\mathcal{G}|}{\delta})\right)$,   with probability at least $1-\delta$, the expected regret of FTPL is
    \begin{equation*}
        \begin{aligned}
           \sum_{t=1}^H \Lag(D_t,\lambda_t)- \min_{\lambda \in \Lambda}\sum_{t=1}^H \Lag(D_t,\lambda) \leq \gamma T
        \end{aligned}
    \end{equation*}
    \label{lem:GFTPL_err_canc}
\end{lemma}
\begin{proof}
    Although the sample complexity of our result changes--(now we require $nT$ samples rather than purely $T$ earlier), the regret bound from Lemma \ref{lem:GFTPL} should still hold with $\gamma_1=\gamma_2 = \frac{\gamma-\epsilon'}{2}.$ Also observe $\delta' = \frac{\delta}{2}.$
\end{proof}
\begin{corollary}[Contextual FTPL Apx-Minimax with Error Cancellations]
    With probability at least $1-\delta$, using Algorithm \ref{alg:MORL-BRNR} with Generalized FTPL (Algorithm \ref{alg:GFTPL_err_canc}) converges to a $\nu$-approximate solution of the miniimax reward problem in $poly(\frac{1}{\gamma},\frac{1}{\delta},d,\log{|\mathcal{G}|})$.
\end{corollary}
    
\begin{proof}
      For $\gamma \leq\ \frac{\nu}{2}-\epsilon'$ and $\epsilon \leq \frac{\nu}{2}+\epsilon'$, this satisfies the no-regret condition necessary for Theorem \ref{thm:apx_minmax_err_canc} to hold in the corresponding time specified by Lemma \ref{lem:GFTPL_err_canc}
\end{proof}
\subsubsection{Fictious Play}
Notice that for Ficitious Play, all the convergence results discussed for Algorithm \ref{alg:FairFictRL} still hold. 
\begin{algorithm}[H]
\caption{FairFictRL Error Cancellation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} bound \( C \),  best-response error \(\epsilon\),  \( \mathcal{G} \) groups, \( r \) reward function, access to MDP \( \mathcal{M} \)
\STATE Initialize \(\lambda_{0}^g = 0\) and $\hat{\lambda}_0^g = \lambda_0^g$
\STATE Initialize $\hat{D}_t$ by selecting a policy in $\Pi$
\FOR{$t = 1, \dots, T$}
    \STATE \( D_t \in \text{FTL}(\hat{\lambda}_{t-1},C) \) using $\mathcal{O}_\epsilon(\cdot,\hat{\lambda}_{t-1})$ \quad returns $\max_D\Lag(D,\hat{\lambda}_{t-1})$
    % \STATE \( \hat{V}_{tot}[t, :] = \hat{V}_{\lambda_t}(\pi(D_t),\epsilon) \) \hfill \emph{(approximate value function for given \(\lambda\) of policy selected, requires \(\mathcal{M}_\lambda\) access)}
    \STATE \( \lambda_{t} = \text{FTL}(\hat{D_{t-1}},C) \) using $Best_\lambda(\hat{D}_{t-1})$\quad returns $\max_\lambda  \Lag(\hat{D}_{t-1},\lambda)$
      \STATE \( \hat{D}_t = \frac{1}{t} \sum_{t'=0}^{t} D_{t'} \); \( \hat{\lambda}_t = \frac{1}{t} \sum_{t'=0}^{t} \lambda_{t'} \)
\ENDFOR
 \STATE \textbf{return} \( (\hat{D}_T, \hat{\lambda}_T) \)
%\STATE \( c_{i,t} = \mathbb{E}_{\pi \sim D_t} \left[ V^i(\pi) - \alpha \right] \)
\end{algorithmic}
\label{alg:FairFictRL_err_canc}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


