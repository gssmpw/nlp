\section{Related Work}
\subsubsection{Fair Reinforcement Learning}
There are many notions of fairness in RL that are distinct from what we study in this paper. ____ define a notion of fairness that requires that the algorithm never play one action with higher probability over another unless the long-term reward of the optimal policy after playing the first action is higher than the second. 
____ consider a welfare-centric notion of fair RL that encompasses a broad class of functions over a set of beneficiaries. They develop a model of adversarially-fair KWIK (knows-what-it-knows learning) and provide the algorithm $E^4$ (equitable $E^3$). It is important to note that their results mainly hold only for the tabular MDP setting.
____ use a flexible form of Lorenz dominance to ensure a more equitable distribution of rewards and empirically demonstrate success of their method for real-world transport planning problems.
____ consider model-based and model-free approaches to constrained fair RL problems and study these in the context of a bank offering loans to individuals. However, these methods do not extend for a very large number of overlapping groups.
____ consider demographic fairness by constraining the group pair-wise difference in performances and provide theoretical guarantees for their algorithms in the tabular setting with an empirical demonstration of success for larger state spaces.
See ____ for a survey of this literature. Aside from our differing objective, we differentiate from this literature by giving algorithms and theory that are able to handle both 1) a very large number of intersecting groups, and 2) provable guarantees beyond the tabular setting. 


\subsubsection{Constrained Reinforcement Learning}
____ consider the problem of constrained RL, especially for continuous state and action spaces. In their setting, they have $m$ reward functions and optimize global cumulative reward subject to each of the $m$ value functions for the given policy being at least as large as some threshold. To do this, they constructed augmented state-space MDPs and formulate the corresponding Lagrangian (with regularization) of this optimization problem. They provide an efficient Primal-Dual algorithm (using different scalarized reward optimization for the learner and online gradient descent (no-regret) for the regulator).  Similar to ____, ____ provide Primal-Dual algorithms and construct their Lagrangian with regularization and optimistic exploration. They prove last-iterate convergence of their algorithm, enabling them to avoid error cancellations. 
____ also study the problem of RL with convex constraints similarly observing a $d$-dimensional reward vector rather than a scalar. They also formulate this problem as a zero-sum game between a learner and regulator playing best-response vs.~online gradient descent (no-regret). These algorithms require enumerating the constraints; in contrast we give ``oracle efficient'' algorithms that require only optimizing over the constraints, and hence can handle extremely large collections of constraints efficiently.
\subsubsection{Multi-Objective Classification Problems}
____ consider the problem of fairness in the binary classification setting and reduce this problem to a sequence (in a repeated zero-sum game between a learner and regulator) of cost-sensitive classification problems to provide a randomized classifier with low general error while (in expectation) satisfying the constraints. ____ extend ____ to settings with a very large number  of overlapping groups. They propose statistical notions of fairness that take into account ``fairness gerrymandering'' over this large number of subgroups and reduce their problem to a sequence of weak agnostic learning problems. We extend this style of algorithm from the classification to the RL setting.