@article{Benford_Decoration_2017,
author = {Benford, Steve and Koleva, Boriana and Quinn, Anthony and Thorn, Emily-Clare and Glover, Kevin and Preston, William and Hazzard, Adrian and Rennick-Egglestone, Stefan and Greenhalgh, Chris and Mortier, Richard},
title = {Crafting Interactive Decoration},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3058552},
doi = {10.1145/3058552},
abstract = {We explore the crafting of interactive decoration for everyday artefacts. This involves adorning them with decorative patterns that enhance their beauty while triggering digital interactions when scanned with cameras. These are realized using an existing augmented reality technique that embeds computer readable codes into the topological structures of hand-drawn patterns. We describe a research through design process that engaged artisans to craft a portfolio of interactive artefacts, including ceramic bowls, embroidered gift cards, fabric souvenirs, and an acoustic guitar. We annotate this portfolio with reflections on the crafting process, revealing how artisans addressed pattern, materials, form and function, and digital mappings throughout their craft process. Further reflection on our portfolio reveals how they bridged between human and system perceptions of visual patterns and engaged in a deep embedding of digital interactions into physical materials. Our findings demonstrate the potential for interactive decoration, distilling the craft knowledge involved in creating aesthetic and functional decoration, highlight the need for transparent computer vision technologies, and raise wider issues for HCI’s growing engagement with craft.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {aug},
articleno = {26},
numpages = {39},
keywords = {wood, tangible and embedded interfaces, tangible, sustainability, seamful design, obsolescence, material, maker, lifespan, hybrid-craft, fabric, embedded, computer vision, augmented reality, ambiguity, DIY, Craft}
}

@inproceedings{Enrico_visualMarkers_2009,
author = {Costanza, Enrico and Huang, Jeffrey},
title = {Designable Visual Markers},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518990},
doi = {10.1145/1518701.1518990},
pages = {1879–1888},
numpages = {10},
keywords = {UI toolkits, fiducial recognition, mobile HCI, mobile devices, user studies, visual marker design, visual marker recognition},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{Jung_HumanDesignable_2019,
author = {Jung, Joshua D.A. and Iyer, Rahul N. and Vogel, Daniel},
title = {Automating the Intentional Encoding of Human-Designable Markers},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300417},
doi = {10.1145/3290605.3300417},
abstract = {Recent work established that it is possible for human artists to encode information into hand-drawn markers, but it is difficult to do when simultaneously maintaining aesthetic quality. We present two methods for relieving the mental burden associated with encoding, while allowing an artist to draw as freely as possible. A 'Helper Overlay' guides the artist with real-time feedback indicating where visual features should be added or removed, and an 'Autocomplete Tool' directly adds necessary features to the drawing for the artist to touch up. Both methods are enabled by a two-part algorithm that uses a tree-search for finding 'major' changes and a dynamic programming method for finding the minimum number of 'minor' changes. A 24-person study demonstrates that a majority of participants prefer both tools over previous methods of manual encoding, with the Helper Overlay being the more popular of the two.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {designable marker, encoding, fiducial marker, user study},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{Jung_HumanMarker_2019,
author = {Jung, Joshua D.A. and Iyer, Rahul N. and Vogel, Daniel},
title = {Automating the Intentional Encoding of Human-Designable Markers},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300417},
doi = {10.1145/3290605.3300417},
abstract = {Recent work established that it is possible for human artists to encode information into hand-drawn markers, but it is difficult to do when simultaneously maintaining aesthetic quality. We present two methods for relieving the mental burden associated with encoding, while allowing an artist to draw as freely as possible. A 'Helper Overlay' guides the artist with real-time feedback indicating where visual features should be added or removed, and an 'Autocomplete Tool' directly adds necessary features to the drawing for the artist to touch up. Both methods are enabled by a two-part algorithm that uses a tree-search for finding 'major' changes and a dynamic programming method for finding the minimum number of 'minor' changes. A 24-person study demonstrates that a majority of participants prefer both tools over previous methods of manual encoding, with the Helper Overlay being the more popular of the two.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {user study, fiducial marker, encoding, designable marker},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@INPROCEEDINGS{Kakehi2006Transparent,
  author={Kakehi, Y. and Hosomi, T. and Iida, M. and Naemura, T. and Matsushita, M.},
  booktitle={First IEEE International Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP '06)}, 
  title={Transparent Tabletop Interface for Multiple Users on Lumisight Table}, 
  year={2006},
  volume={},
  number={},
  pages={8 pp.-},
  keywords={Computer displays;Cameras;Embedded computing;Interactive systems;Humans;Collaborative work;Conferences;Computer vision;Control systems;Joining processes},
  doi={10.1109/TABLETOP.2006.34}}

@inproceedings{Preston_MarkerScale_2017,
author = {Preston, William and Benford, Steve and Thorn, Emily-Clare and Koleva, Boriana and Rennick-Egglestone, Stefan and Mortier, Richard and Quinn, Anthony and Stell, John and Worboys, Michael},
title = {Enabling Hand-Crafted Visual Markers at Scale},
year = {2017},
isbn = {9781450349222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064663.3064746},
doi = {10.1145/3064663.3064746},
abstract = {As locative media and augmented reality spread into the everyday world so it becomes important to create aesthetic visual markers at scale. We explore a designer-centred approach in which skilled designers handcraft seed designs that are automatically recombined to create many markers as subtle variants of a common theme. First, we extend the d-touch topological approach to creating visual markers that has previously been shown to support creative design with two new techniques: area order codes and visual checksums. We then show how the topological structure of such markers provides the basis for recombining designs to generate many variations. We demonstrate our approach through the creation of beautiful, personalized and interactive wallpaper. We reflect on how technologies must enable designers to balance goals of scalability, aesthetics and reliability in creating beautiful interactive decoration.},
booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
pages = {1227–1237},
numpages = {11},
keywords = {visual markers, topological markers, patterns, image recognition, fiducial markers, computer vision},
location = {Edinburgh, United Kingdom},
series = {DIS '17}
}

@INPROCEEDINGS{Wagner2008PoseTracking,
  author={Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
  booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, 
  title={Pose Tracking from Natural Features on Mobile Phones}, 
  year={2008},
  volume={},
  number={},
  pages={125-134},
  keywords={Target tracking;Feature extraction;Mobile handsets;Computational modeling;Detectors;Cameras;Real time systems;pose tracking;natural features;mobile phones;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
  doi={10.1109/ISMAR.2008.4637338}}

@article{alessandrini_audio-augmented_2014,
	title = {Audio-augmented paper for therapy and educational intervention for children with autistic spectrum disorder},
	volume = {72},
	number = {4},
	journal = {International Journal of Human-Computer Studies},
	author = {Alessandrini, Andrea and Cappelletti, Alessandro and Zancanaro, Massimo},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {422--430},
}

@article{balaji_retrosphere_2023,
	title = {{RetroSphere}: {Self}-{Contained} {Passive} {3D} {Controller} {Tracking} for {Augmented} {Reality}},
	volume = {6},
	shorttitle = {{RetroSphere}},
	url = {https://dl.acm.org/doi/10.1145/3569479},
	doi = {10.1145/3569479},
	abstract = {Advanced AR/VR headsets often have a dedicated depth sensor or multiple cameras, high processing power, and a high-capacity battery to track hands or controllers. However, these approaches are not compatible with the small form factor and limited thermal capacity of lightweight AR devices. In this paper, we present RetroSphere, a self-contained 6 degree of freedom (6DoF) controller tracker that can be integrated with almost any device. RetroSphere tracks a passive controller with just 3 retroreflective spheres using a stereo pair of mass-produced infrared blob trackers, each with its own infrared LED emitters. As the sphere is completely passive, no electronics or recharging is required. Each object tracking camera provides a tiny Arduino-compatible ESP32 microcontroller with the 2D position of the spheres. A lightweight stereo depth estimation algorithm that runs on the ESP32 performs 6DoF tracking of the passive controller. Also, RetroSphere provides an auto-calibration procedure to calibrate the stereo IR tracker setup. Our work builds upon Johnny Lee's Wii remote hacks and aims to enable a community of researchers, designers, and makers to use 3D input in their projects with affordable off-the-shelf components. RetroSphere achieves a tracking accuracy of about 96.5\% with errors as low as {\textasciitilde}3.5 cm over a 100 cm tracking range, validated with ground truth 3D data obtained using a LIDAR camera while consuming around 400 mW. We provide implementation details, evaluate the accuracy of our system, and demonstrate example applications, such as mobile AR drawing, 3D measurement, etc. with our Retrosphere-enabled AR glass prototype.},
	number = {4},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Balaji, Ananta Narayanan and Kimber, Clayton and Li, David and Wu, Shengzhi and Du, Ruofei and Kim, David},
	month = jan,
	year = {2023},
	keywords = {Augmented reality, Augmented reality glasses, Infrared marker tracking, Retroreflectors, Virtual reality},
	pages = {157:1--157:36},
}

@inproceedings{buechley_paints_2009,
	address = {Cambridge United Kingdom},
	title = {Paints, paper, and programs: first steps toward the computational sketchbook},
	isbn = {978-1-60558-493-5},
	shorttitle = {Paints, paper, and programs},
	url = {https://dl.acm.org/doi/10.1145/1517664.1517670},
	doi = {10.1145/1517664.1517670},
	abstract = {This paper describes what we believe to be important initial steps toward realizing a novel computational medium that combines elements of programming, painting, and papercrafts. Briefly, this genre of paper computing allows a user to create functional computational artifacts on painted paper substrates. We introduce a construction kit for paper computing that consists of computational elements—microcontrollers, sensors, actuators, and power sources—that are held on paper surfaces by magnetic paint and magnets. Conductive paint applied to these surfaces takes on the role of “wires”, connecting the computational elements to one another. These elements can be moved around and from surface to surface, much like magnets on a refrigerator, and the overall result is a tangible medium in which painting, programming, and the affordances of paper blend together. In addition to introducing the kit, we describe example constructions and discuss a variety of potential applications, design projects, and issues for continued research.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Tangible} and {Embedded} {Interaction}},
	publisher = {ACM},
	author = {Buechley, Leah and Hendrix, Sue and Eisenberg, Mike},
	month = feb,
	year = {2009},
	pages = {9--12},
}

@inproceedings{chen_augmenting_2020,
	address = {Honolulu HI USA},
	title = {Augmenting {Static} {Visualizations} with {PapARVis} {Designer}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376436},
	doi = {10.1145/3313831.3376436},
	abstract = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality. Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workﬂow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wallsized visualizations. A user study shows high user satisfaction of our environment and conﬁrms that participants can create augmented visualizations in an average of 4.63 minutes.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
	month = apr,
	year = {2020},
	pages = {1--12},
}

@article{cheng_silver_2020,
	title = {Silver {Tape}: {Inkjet}-{Printed} {Circuits} {Peeled}-and-{Transferred} on {Versatile} {Substrates}},
	volume = {4},
	issn = {2474-9567},
	shorttitle = {Silver {Tape}},
	url = {https://dl.acm.org/doi/10.1145/3381013},
	doi = {10.1145/3381013},
	abstract = {We propose Silver Tape, a simple yet novel fabrication technique to transfer inkjet-printed silver traces from paper onto versatile substrates, without time-/space- consuming processes such as screen printing or heat sintering. This allows users to quickly implement silver traces with a variety of properties by exploiting a wide range of substrates. For instance, high flexibility can be achieved with Scotch tape, high transparency with polydimethylsiloxane (PDMS), heat durability with Kapton polyimide tape, water solubility with 3M water-soluble tape, and beyond. Many of these properties are not achievable with conventional substrates that are used for inkjet-printing conductive traces. Specifically, our technique leverages the commonly undesired low adhesion property of the inkjet printing films and repurposes these films as temporary transfer media. We describe our fabrication methods with a library of materials we can utilize, evaluate the mechanical and electrical properties of the transferred traces, and conclude with several demonstrative applications. We believe Silver Tape enriches novel interactions for the ubiquitous computing domain, by enabling digital fabrication of electronics on versatile materials, surfaces, and shapes.},
	language = {en},
	number = {1},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Cheng, Tingyu and Narumi, Koya and Do, Youngwook and Zhang, Yang and Ta, Tung D. and Sasatani, Takuya and Markvicka, Eric and Kawahara, Yoshihiro and Yao, Lining and Abowd, Gregory D. and Oh, HyunJoo},
	month = mar,
	year = {2020},
	pages = {1--17},
}

@misc{denso_wave_information_2022,
	title = {Information capacity and versions of {QR} {Code}},
	url = {https://www.qrcode.com/en/about/version.html},
	urldate = {2022-08-06},
	author = {Denso Wave},
	year = {2022},
}

@inproceedings{dogan_augmented_2024,
	address = {New York, NY, USA},
	series = {{UIST} '24},
	title = {Augmented {Object} {Intelligence} with {XR}-{Objects}},
	isbn = {9798400706288},
	url = {https://dl.acm.org/doi/10.1145/3654777.3676379},
	doi = {10.1145/3654777.3676379},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Gonzalez, Eric J and Ahuja, Karan and Du, Ruofei and Colaço, Andrea and Lee, Johnny and Gonzalez-Franco, Mar and Kim, David},
	month = oct,
	year = {2024},
	pages = {1--15},
	file = {Full Text PDF:/Users/doga/Zotero/storage/H4AH9BV6/Dogan et al. - 2024 - Augmented Object Intelligence with XR-Objects.pdf:application/pdf},
}

@inproceedings{dogan_brightmarker_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{BrightMarker}: {3D} {Printed} {Fluorescent} {Markers} for {Object} {Tracking}},
	isbn = {9798400701320},
	shorttitle = {{BrightMarker}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606758},
	doi = {10.1145/3586183.3606758},
	abstract = {Existing invisible object tagging methods are prone to low resolution, which impedes tracking performance. We present BrightMarker, a fabrication method that uses fluorescent filaments to embed easily trackable markers in 3D printed color objects. By using an infrared-fluorescent filament that "shifts" the wavelength of the incident light, our optical detection setup filters out all the noise to only have the markers present in the infrared camera image. The high contrast of the markers allows us to track them robustly regardless of the moving objects’ surface color. We built a software interface for automatically embedding these markers for the input object geometry, and hardware modules that can be attached to existing mobile devices and AR/VR headsets. Our image processing pipeline robustly localizes the markers in real time from the captured images. BrightMarker can be used in a variety of applications, such as custom fabricated wearables for motion capture, tangible interfaces for AR/VR, rapid product tracking, and privacy-preserving night vision. BrightMarker exceeds the detection rate of state-of-the-art invisible marking, and even small markers (1"x1") can be tracked at distances exceeding 2m.},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Garcia-Martin, Raul and Haertel, Patrick William and O'Keefe, Jamison John and Taka, Ahmad and Aurora, Akarsh and Sanchez-Reillo, Raul and Mueller, Stefanie},
	month = oct,
	year = {2023},
	keywords = {3D printing, digital fabrication, fluorescence, infrared imaging, invisible markers, marker tracking},
	pages = {1--13},
}

@inproceedings{dogan_fabricate_2022,
	title = {Fabricate {It} or {Render} {It}?  {Digital} {Fabrication} vs. {Virtual} {Reality} for {Creating} {Objects} {Instantly}},
	url = {https://doi.org/10.1145/3491101.3516510},
	doi = {10.1145/3491101.3516510},
	abstract = {In the technical human-computer interaction (HCI) community, two research fields that gained significant popularity in the last decade are digital fabrication and augmented/virtual reality (AR/VR). Although the two fields deal with different technical challenges, both aim for a single end goal: creating "objects" instantly – either by fabricating them physically or rendering them virtually. In this panel, we will discuss the pros and cons of both approaches, discuss which one may prevail in the future, and what opportunities exist for closer collaboration between researchers from the two research fields.},
	language = {en},
	booktitle = {Extended {Abstracts} of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Baudisch, Patrick and Benko, Hrvoje and Nebeling, Michael and Peng, Huaishu and Savage, Valkyrie and Mueller, Stefanie},
	year = {2022},
	pages = {5},
}

@inproceedings{dogan_g-id_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {G-{ID}: {Identifying} {3D} {Prints} {Using} {Slicing} {Parameters}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {G-{ID}},
	url = {https://doi.org/10.1145/3313831.3376202},
	doi = {10.1145/3313831.3376202},
	urldate = {2020-08-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Faruqi, Faraz and Churchill, Andrew Day and Friedman, Kenneth and Cheng, Leon and Subramanian, Sriram and Mueller, Stefanie},
	month = apr,
	year = {2020},
	keywords = {3d printing, identification, making, personal fabrication, tags},
	pages = {1--13},
}

@inproceedings{dogan_infraredtags_2022,
	address = {New Orleans LA USA},
	title = {{InfraredTags}: {Embedding} {Invisible} {AR} {Markers} and {Barcodes} {Using} {Low}-{Cost}, {Infrared}-{Based} {3D} {Printing} and {Imaging} {Tools}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{InfraredTags}},
	doi = {10.1145/3491102.3501951},
	abstract = {Existing approaches for embedding unobtrusive tags inside 3D objects either require complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D codes and markers imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost infrared cameras. InfraredTags achieves this by using an infrared transmitting filament, which infrared cameras can see through, and by leaving air gaps inside for the tag’s bits, which infrared cameras capture as darker pixels in the image.},
	language = {en},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Taka, Ahmad and Lu, Michael and Zhu, Yunyi and Kumar, Akshat and Gupta, Aakar and Mueller, Stefanie},
	year = {2022},
	pages = {9},
}

@inproceedings{dogan_standarone_2023,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {{StandARone}: {Infrared}-{Watermarked} {Documents} as {Portable} {Containers} of {AR} {Interaction} and {Personalization}},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{StandARone}},
	url = {https://doi.org/10.1145/3544549.3585905},
	doi = {10.1145/3544549.3585905},
	abstract = {Hybrid paper interfaces leverage augmented reality (AR) to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, the instructions for how the virtual content should be generated are not an intrinsic part of the document but rather accessed through a link to remote resources. To enable hybrid documents to be portable containers of also the AR content, we introduce StandARone documents. Using our system, a document author can define AR content and embed it invisibly on the document using a standard inkjet printer and infrared-absorbing ink. A document consumer can interact with the embedded content using a smartphone with a NIR camera without requiring a network connection. We demonstrate several use cases of StandARone including personalized offline menus, interactive visualizations, and location-aware packaging.},
	urldate = {2024-04-09},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Siu, Alexa F. and Healey, Jennifer and Wigington, Curtis and Xiao, Chang and Sun, Tong},
	month = apr,
	year = {2023},
	keywords = {augmented reality, documents, fabrication, infrared imaging, mixed reality, paper interfaces, watermarking},
	pages = {1--7},
}

@inproceedings{dogan_structcode_2023,
	address = {New York, NY, USA},
	series = {{SCF} '23},
	title = {{StructCode}: {Leveraging} {Fabrication} {Artifacts} to {Store} {Data} in {Laser}-{Cut} {Objects}},
	isbn = {9798400703195},
	shorttitle = {{StructCode}},
	url = {https://dl.acm.org/doi/10.1145/3623263.3623353},
	doi = {10.1145/3623263.3623353},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 8th {ACM} {Symposium} on {Computational} {Fabrication}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Chan, Vivian Hsinyueh and Qi, Richard and Tang, Grace and Roumen, Thijs and Mueller, Stefanie},
	month = nov,
	year = {2023},
	pages = {1--13},
	file = {Full Text PDF:/Users/doga/Zotero/storage/LACYFULD/Dogan et al. - 2023 - StructCode Leveraging Fabrication Artifacts to St.pdf:application/pdf},
}

@misc{dogan_ubiquitous_2024,
	title = {Ubiquitous {Metadata}: {Design} and {Fabrication} of {Embedded} {Markers} for {Real}-{World} {Object} {Identification} and {Interaction}},
	shorttitle = {Ubiquitous {Metadata}},
	url = {http://arxiv.org/abs/2407.11748},
	doi = {10.48550/arXiv.2407.11748},
	abstract = {The convergence of the physical and digital realms has ushered in a new era of immersive experiences and seamless interactions. As the boundaries between the real world and virtual environments blur and result in a "mixed reality," there arises a need for robust and efficient methods to connect physical objects with their virtual counterparts. In this thesis, we present a novel approach to bridging this gap through the design, fabrication, and detection of embedded machine-readable markers. We categorize the proposed marking approaches into three distinct categories: natural markers, structural markers, and internal markers. Natural markers, such as those used in SensiCut, are inherent fingerprints of objects repurposed as machine-readable identifiers, while structural markers, such as StructCode and G-ID, leverage the structural artifacts in objects that emerge during the fabrication process itself. Internal markers, such as InfraredTag and BrightMarker, are embedded inside fabricated objects using specialized materials. Leveraging a combination of methods from computer vision, machine learning, computational imaging, and material science, the presented approaches offer robust and versatile solutions for object identification, tracking, and interaction. These markers, seamlessly integrated into real-world objects, effectively communicate an object's identity, origin, function, and interaction, functioning as gateways to "ubiquitous metadata" - a concept where metadata is embedded into physical objects, similar to metadata in digital files. Across the different chapters, we demonstrate the applications of the presented methods in diverse domains, including product design, manufacturing, retail, logistics, education, entertainment, security, and sustainability.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Dogan, Mustafa Doga},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Emerging Technologies, Computer Science - Graphics, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/doga/Zotero/storage/9C5SV3SP/Dogan - 2024 - Ubiquitous Metadata Design and Fabrication of Emb.pdf:application/pdf;Snapshot:/Users/doga/Zotero/storage/VIMR6B9A/2407.html:text/html},
}

@article{drey_sparklingpaper_2022,
	title = {{SpARklingPaper}: {Enhancing} {Common} {Pen}- {And} {Paper}-{Based} {Handwriting} {Training} for {Children} by {Digitally} {Augmenting} {Papers} {Using} a {Tablet} {Screen}},
	volume = {6},
	shorttitle = {{SpARklingPaper}},
	url = {https://dl.acm.org/doi/10.1145/3550337},
	doi = {10.1145/3550337},
	abstract = {Educational apps support learning, but handwriting training is still based on analog pen- and paper. However, training handwriting with apps can negatively affect graphomotor handwriting skills due to the different haptic feedback of the tablet, stylus, or finger compared to pen and paper. With SpARklingPaper, we are the first to combine the genuine haptic feedback of analog pen and paper with the digital support of apps. Our artifact contribution enables children to write with any pen on a standard paper placed on a tablet's screen, augmenting the paper from below, showing animated letters and individual feedback. We conducted two online surveys with overall 29 parents and teachers of elementary school pupils and a user study with 13 children and 13 parents for evaluation. Our results show the importance of the genuine analog haptic feedback combined with the augmentation of SpARklingPaper. It was rated superior compared to our stylus baseline condition regarding pen-handling, writing training-success, motivation, and overall impression. SpARklingPaper can be a blueprint for high-fidelity haptic feedback handwriting training systems.},
	number = {3},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Drey, Tobias and Janek, Jessica and Lang, Josef and Puschmann, Dietmar and Rietzler, Michael and Rukzio, Enrico},
	month = sep,
	year = {2022},
	keywords = {artifact, augmented reality, children, education, handwriting training, literacy training, mobile devices, tablet},
	pages = {113:1--113:29},
}

@article{fu_chartem_2021,
	title = {Chartem: {Reviving} {Chart} {Images} with {Data} {Embedding}},
	volume = {27},
	issn = {1077-2626},
	shorttitle = {Chartem},
	url = {https://www.computer.org/csdl/journal/tg/2021/02/09293003/1pyonCyir8k},
	doi = {10.1109/TVCG.2020.3030351},
	abstract = {In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.},
	language = {English},
	number = {02},
	urldate = {2022-08-06},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fu, Jiayun and Zhu, Bin and Cui, Weiwei and Ge, Song and Wang, Yun and Zhang, Haidong and Huang, He and Tang, Yuanyuan and Zhang, Dongmei and Ma, Xiaojing},
	month = feb,
	year = {2021},
	note = {Publisher: IEEE Computer Society},
	pages = {337--346},
}

@article{garrido2014automatic,
  title={Automatic Generation and Detection of Highly Reliable Fiducial Markers under Occlusion},
  author={Garrido-Jurado, Sergio and Mu{\~n}oz-Salinas, Rafael and Madrid-Cuevas, Francisco Jos{\'e} and Mar{\'\i}n-Jim{\'e}nez, Manuel Jes{\'u}s},
  journal={Pattern Recognition},
  volume={47},
  number={6},
  pages={2280--2292},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{getschmann_seedmarkers_2021,
	address = {New York, NY, USA},
	series = {{TEI} '21},
	title = {Seedmarkers: {Embeddable} {Markers} for {Physical} {Objects}},
	isbn = {978-1-4503-8213-7},
	shorttitle = {Seedmarkers},
	url = {https://doi.org/10.1145/3430524.3440645},
	doi = {10.1145/3430524.3440645},
	abstract = {We present Seedmarkers, shape-independent topological markers that can be embedded in physical objects manufactured with common rapid-prototyping techniques. Many markers are optimized for technical performance while visual appearance or the feasibility of permanently merging marker and physical object is not considered. We give an overview of the aesthetic properties of a wide range of existing markers and conducted a short online survey to assess the perception of popular marker designs. Based on our findings we introduce our generation algorithm making use of weighted Voronoi diagrams for topological optimization. With our generator, Seedmarkers can be created from technical drawings during the design process to fill arbitrary shapes on any surface. Given dimensions and manufacturing constraints, different configurations for 3 or 6 degrees of freedom tracking are possible. We propose a set of application examples for shape-independent markers, including 3D printed tangibles, laser cut plates and functional markers on printed circuit boards.},
	urldate = {2021-02-23},
	booktitle = {Proceedings of the {Fifteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Getschmann, Christopher and Echtler, Florian},
	month = feb,
	year = {2021},
	keywords = {3D printing, information embedding, pose estimation, rapid prototyping, topological markers},
	pages = {1--11},
}

@inproceedings{gong_printsense_2014,
	address = {Toronto Ontario Canada},
	title = {{PrintSense}: a versatile sensing technique to support multimodal flexible surface interaction},
	isbn = {978-1-4503-2473-1},
	shorttitle = {{PrintSense}},
	url = {https://dl.acm.org/doi/10.1145/2556288.2557173},
	doi = {10.1145/2556288.2557173},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gong, Nan-Wei and Steimle, Jürgen and Olberding, Simon and Hodges, Steve and Gillian, Nicholas Edward and Kawahara, Yoshihiro and Paradiso, Joseph A.},
	month = apr,
	year = {2014},
	pages = {1407--1410},
}

@inproceedings{han_hybrid_2021,
	address = {New York, NY, USA},
	series = {{DIS} '21},
	title = {Hybrid {Paper}-{Digital} {Interfaces}: {A} {Systematic} {Literature} {Review}},
	isbn = {978-1-4503-8476-6},
	shorttitle = {Hybrid {Paper}-{Digital} {Interfaces}},
	url = {https://doi.org/10.1145/3461778.3462059},
	doi = {10.1145/3461778.3462059},
	abstract = {Past research recognized that paper has many advantages over digital devices, such as affordability, tangibility, and flexibility. Paper, however, also lacks many of the functionalities available in digital technologies, such as access to online resources and the ability to display interactive content. Prior research therefore identified opportunities for fusing the two mediums into a combined interface. This work presents a literature review on this form of innovation - technologies that bridge the paper-digital gap. First, we synthesize an understanding of paper and its relationship with digital devices through the lens of past works. Then, we outline the state-of-the-art for paper-digital interfaces and highlight possible use cases and implementation approaches. Last, we discuss design considerations and future work for developing paper-digital interfaces. Our work may be beneficial for HCI researchers interested in the development of hybrid paper-digital interfaces, and more broadly in embedding digital functionalities in everyday objects.},
	urldate = {2022-06-03},
	booktitle = {Designing {Interactive} {Systems} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Han, Feng and Cheng, Yifei and Strachan, Megan and Ma, Xiaojuan},
	month = jun,
	year = {2021},
	keywords = {interactive paper, paper computing, paper interfaces},
	pages = {1087--1100},
}

@inproceedings{jacoby_drawing_2013,
	address = {New York, NY, USA},
	series = {{IDC} '13},
	title = {Drawing the electric: storytelling with conductive ink},
	isbn = {978-1-4503-1918-8},
	shorttitle = {Drawing the electric},
	url = {https://dl.acm.org/doi/10.1145/2485760.2485790},
	doi = {10.1145/2485760.2485790},
	abstract = {We explore conductive ink as an expressive medium for narrative storytelling and interaction design with children, introducing StoryClip, a toolkit that integrates functional materials, computation, and drawing. The StoryClip kit consists of silver ink, ordinary art supplies, and a hardware-software tool, allowing a child's drawing to function as an audio recording-and-playback interface. We exploit craft and artistic practice to motivate technological exploration, turning a conventional illustration into a multimedia interface that promotes multi-level engagement with children. In this note, we describe the design of our system and discuss our findings from two workshops with children.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Interaction} {Design} and {Children}},
	publisher = {Association for Computing Machinery},
	author = {Jacoby, Sam and Buechley, Leah},
	month = jun,
	year = {2013},
	keywords = {conductive ink, creativity, expression, paper, prototyping, storytelling},
	pages = {265--268},
}

@inproceedings{kato1999marker,
  title={Marker Tracking and HMD Calibration for a Video-based Augmented Reality Conferencing System},
  author={Kato, Hirokazu and Billinghurst, Mark},
  booktitle={Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
  pages={85--94},
  year={1999},
  organization={IEEE}
}

@inproceedings{kawahara_instant_2013,
	address = {Zurich Switzerland},
	title = {Instant inkjet circuits: lab-based inkjet printing to support rapid prototyping of {UbiComp} devices},
	isbn = {978-1-4503-1770-2},
	shorttitle = {Instant inkjet circuits},
	url = {https://dl.acm.org/doi/10.1145/2493432.2493486},
	doi = {10.1145/2493432.2493486},
	language = {en},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the 2013 {ACM} international joint conference on {Pervasive} and ubiquitous computing},
	publisher = {ACM},
	author = {Kawahara, Yoshihiro and Hodges, Steve and Cook, Benjamin S. and Zhang, Cheng and Abowd, Gregory D.},
	month = sep,
	year = {2013},
	pages = {363--372},
}

@inproceedings{kim_ministudio_2016,
	address = {San Jose California USA},
	title = {{miniStudio}: {Designers}' {Tool} for {Prototyping} {Ubicomp} {Space} with {Interactive} {Miniature}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {{miniStudio}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858180},
	doi = {10.1145/2858036.2858180},
	abstract = {Recently, it has become common for designers to deal with complex and large-scale ubicomp or IoT spaces. Designers without technical implementation skills have difﬁculties in prototyping such spaces, especially in the early phases of design. We present miniStudio, a designers’ tool for prototyping ubicomp space with proxemic interactions. It is built on designers’ existing software and modeling materials (Photoshop, Lego, and paper). Interactions can be deﬁned in Photoshop based on ﬁve spatial relations: location, distance, motion, orientation, and custom. Projection-based augmented reality was applied to miniatures in order to enable tangible interactions and dynamic representations. Hidden marker stickers and a camera-projector system enable the unobtrusive integration of digital images on the physical miniature. Through the user study with 12 designers and researchers in the ubicomp ﬁeld, we found that miniStudio supported rapid prototyping of large and complex ideas with multiple connected components. Based on the tool development and the study, we discuss the implications for prototyping ubicomp environments in the early phase of the design.},
	language = {en},
	urldate = {2021-05-31},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kim, Han-Jong and Kim, Ju-Whan and Nam, Tek-Jin},
	month = may,
	year = {2016},
	pages = {213--224},
}

@inproceedings{klamka_illuminated_2017,
	address = {New York, NY, USA},
	series = {{ISS} '17},
	title = {Illuminated {Interactive} {Paper} with {Multiple} {Input} {Modalities} for {Form} {Filling} {Applications}},
	isbn = {978-1-4503-4691-7},
	url = {https://dl.acm.org/doi/10.1145/3132272.3132287},
	doi = {10.1145/3132272.3132287},
	abstract = {In this paper, we demonstrate IllumiPaper: a system that provides new forms of paper-integrated visual feedback and enables multiple input channels to enhance digital paper applications. We aim to take advantage of traditional form sheets, including their haptic qualities, simplicity, and archivability, and simultaneously integrate rich digital functionalities such as dynamic status queries, real-time notifications, and visual feedback for widget controls. Our approach builds on emerging, novel paper-based technologies. We describe a fabrication process that allow us to directly integrate segment-based displays, touch and flex sensors, as well as digital pen input on the paper itself. With our fully functional research platform we demonstrate an interactive prototype for an industrial form-filling maintenance application to service computer networks that covers a wide range of typical paper-related tasks.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Interactive} {Surfaces} and {Spaces}},
	publisher = {Association for Computing Machinery},
	author = {Klamka, Konstantin and Büschel, Wolfgang and Dachselt, Raimund},
	month = oct,
	year = {2017},
	keywords = {Anoto, Digital pen and paper, augmented paper, electroluminescence, form filling, printed electronics, visual feedback},
	pages = {434--437},
}

@inproceedings{klamka_illumipaper_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{IllumiPaper}: {Illuminated} {Interactive} {Paper}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {{IllumiPaper}},
	url = {https://dl.acm.org/doi/10.1145/3025453.3025525},
	doi = {10.1145/3025453.3025525},
	abstract = {Due to their simplicity and flexibility, digital pen-and-paper solutions have a promising potential to become a part of our daily work. Unfortunately, they lack dynamic visual feedback and thereby restrain advanced digital functionalities. In this paper, we investigate new forms of paper-integrated feedback, which build on emerging paper-based electronics and novel thin-film display technologies. Our approach focuses on illuminated elements, which are seamlessly integrated into standard paper. For that, we introduce an extended design space for paper-integrated illuminations. As a major contribution, we present a systematic feedback repertoire for real-world applications including feedback components for innovative paper interaction tasks in five categories. Furthermore, we contribute a fully-functional research platform including a paper-controller, digital pen and illuminated, digitally controlled papers that demonstrate the feasibility of our techniques. Finally, we report on six interviews, where experts rated our approach as intuitive and very usable for various applications, in particular educational ones.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Klamka, Konstantin and Dachselt, Raimund},
	month = may,
	year = {2017},
	keywords = {anoto, augmented paper, digital pen and paper, electro-luminescence, pen interaction, thin-film display, visual feedback},
	pages = {5605--5618},
}

@inproceedings{lee_hybrid_2007,
	address = {New York, NY, USA},
	series = {{UIST} '07},
	title = {Hybrid infrared and visible light projection for location tracking},
	isbn = {978-1-59593-679-0},
	url = {https://doi.org/10.1145/1294211.1294222},
	doi = {10.1145/1294211.1294222},
	abstract = {A number of projects within the computer graphics, computer vision, and human-computer interaction communities have recognized the value of using projected structured light patterns for the purposes of doing range finding, location dependent data delivery, projector adaptation, or object discovery and tracking. However, most of the work exploring these concepts has relied on visible structured light patterns resulting in a caustic visual experience. In this work, we present the first design and implementation of a high-resolution, scalable, general purpose invisible near-infrared projector that can be manufactured in a practical manner. This approach is compatible with simultaneous visible light projection and integrates well with future Digital Light Processing (DLP) projector designs -- the most common type of projectors today. By unifying both the visible and non-visible pattern projection into a single device, we can greatly simply the implementation and execution of interactive projection systems. Additionally, we can inherently provide location discovery and tracking capabilities that are unattainable using other approaches.},
	urldate = {2023-01-10},
	booktitle = {Proceedings of the 20th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Lee, Johnny and Hudson, Scott and Dietz, Pau},
	month = oct,
	year = {2007},
	keywords = {augmented reality, infrared projection, physical interaction, projector-based tracking, simulated displays},
	pages = {57--60},
}

@inproceedings{li_holodoc_2019,
	address = {Glasgow Scotland Uk},
	title = {{HoloDoc}: {Enabling} {Mixed} {Reality} {Workspaces} that {Harness} {Physical} and {Digital} {Content}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{HoloDoc}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300917},
	doi = {10.1145/3290605.3300917},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Li, Zhen and Annett, Michelle and Hinckley, Ken and Singh, Karan and Wigdor, Daniel},
	month = may,
	year = {2019},
	pages = {1--14},
}

@inproceedings{qi_electronic_2010,
	address = {Cambridge Massachusetts USA},
	title = {Electronic popables: exploring paper-based computing through an interactive pop-up book},
	isbn = {978-1-60558-841-4},
	shorttitle = {Electronic popables},
	url = {https://dl.acm.org/doi/10.1145/1709886.1709909},
	doi = {10.1145/1709886.1709909},
	abstract = {We have developed an interactive pop-up book called Electronic Popables to explore paper-based computing. Our book integrates traditional pop-up mechanisms with thin, flexible, paper-based electronics and the result is an artifact that looks and functions much like an ordinary popup, but has added elements of dynamic interactivity. This paper introduces the book and, through it, a library of paper-based sensors and a suite of paper-electronics construction techniques. We also reflect on the unique and under-explored opportunities that arise from combining material experimentation, artistic design, and engineering.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the fourth international conference on {Tangible}, embedded, and embodied interaction},
	publisher = {ACM},
	author = {Qi, Jie and Buechley, Leah},
	month = jan,
	year = {2010},
	pages = {121--128},
}

@inproceedings{qian_dually_2022,
	address = {New Orleans LA USA},
	title = {Dually {Noted}: {Layout}-{Aware} {Annotations} with {Smartphone} {Augmented} {Reality}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Dually {Noted}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3502026},
	doi = {10.1145/3491102.3502026},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Qian, Jing and Sun, Qi and Wigington, Curtis and Han, Han L. and Sun, Tong and Healey, Jennifer and Tompkin, James and Huang, Jeff},
	month = apr,
	year = {2022},
	pages = {1--15},
}

@inproceedings{rajaram_paper_2022,
	address = {New Orleans LA USA},
	title = {Paper {Trail}: {An} {Immersive} {Authoring} {System} for {Augmented} {Reality} {Instructional} {Experiences}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Paper {Trail}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517486},
	doi = {10.1145/3491102.3517486},
	abstract = {Prior work has demonstrated augmented reality’s benefts to education, but current tools are difcult to integrate with traditional instructional methods. We present Paper Trail, an immersive authoring system designed to explore how to enable instructors to create AR educational experiences, leaving paper at the core of the interaction and enhancing it with various forms of digital media, animations for dynamic illustrations, and clipping masks to guide learning. To inform the system design, we developed fve scenarios exploring the benefts that hand-held and head-worn AR can bring to STEM instruction and developed a design space of AR interactions enhancing paper based on these scenarios and prior work. Using the example of an AR physics handout, we assessed the system’s potential with PhD-level instructors and its usability with XR design experts. In an elicitation study with high-school teachers, we study how Paper Trail could be used and extended to enable fexible use cases across various domains. We discuss benefts of immersive paper for supporting diverse student needs and challenges for making efective use of AR for learning.},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Rajaram, Shwetha and Nebeling, Michael},
	month = apr,
	year = {2022},
	pages = {1--16},
}

@inproceedings{rosner_spyn_2010,
	address = {New York, NY, USA},
	series = {{CHI} '10},
	title = {Spyn: augmenting the creative and communicative potential of craft},
	isbn = {978-1-60558-929-9},
	shorttitle = {Spyn},
	url = {https://doi.org/10.1145/1753326.1753691},
	doi = {10.1145/1753326.1753691},
	abstract = {We present data collected from a field study of 12 needle-crafters introduced to Spyn-mobile phone software that associates digital records (audio/visual media, text, and geographic data) with locations on fabric. We observed leisure needle-crafters use Spyn to create one or more handmade garments over two to four weeks and then give those garments to friends, partners, and family members. Using Spyn, creators left behind digital and physical traces that heightened recipients' appreciation for the gift and enabled a diverse set of meanings to emerge. Digital engagements with Spyn became a means for unraveling the value of the gift: recipients used digital information associated with the physical objects to interpret the story behind the objects and their creators. We discuss the nature of this relationship between digital and physical material and its implications for craft.},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rosner, Daniela K. and Ryokai, Kimiko},
	month = apr,
	year = {2010},
	keywords = {craft, creativity, crochet, design process, gift exchange, knitting, material, process, storytelling, tangibility},
	pages = {2407--2416},
}

@article{song_my_2018,
	title = {My {Smartphone} {Recognizes} {Genuine} {QR} {Codes}!: {Practical} {Unclonable} {QR} {Code} via {3D} {Printing}},
	volume = {2},
	issn = {2474-9567},
	shorttitle = {My {Smartphone} {Recognizes} {Genuine} {QR} {Codes}!},
	url = {https://dl.acm.org/doi/10.1145/3214286},
	doi = {10.1145/3214286},
	abstract = {Additive manufacturing, or 3D printing, has been widely applied in product manufacturing. However, the emerging unauthorized access of 3D printing data, as well as the growth in the pervasiveness and capability of 3D printing devices have raised serious concerns about 3D printing product anti-counterfeit. Electronic product tags are the current standard for authentication purposes; however, often this technology is neither secure nor cost-effective, and fails to take advantage of other unique 3D printing features. Considering the great usability of the QR code, we are motivated to enhance the QR code for the practical and cost-effective 3D printing product identification. Particularly, we bring up the all-in-one design, all-in-one manufacturing concept incorporating the QR code in the complete 3D printing paradigm. In detail, we explore the possibility of leveraging the random and uncontrollable process variations in the 3D printing system to generate a unique fingerprint for the integrated QR code. To this end, we present an end-to-end 3D-printed QR code verification framework, which does not change the original QR protocol and functionality. The entire solution can be implemented with commodity 3D printers and smartphones. Specifically, we first investigate the inevitable and random process variations in the 3D printing mechanism and analyze the causality between the variations and detectable geometric deformation. We further develop a fingerprint extraction algorithm taking into account both the QR code property and the 3D printer characteristics. The system evaluation indicates that our solution is secure and robust in multiple scenarios.},
	language = {en},
	number = {2},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Song, Chen and Li, Zhengxiong and Xu, Wenyao and Zhou, Chi and Jin, Zhanpeng and Ren, Kui},
	month = jul,
	year = {2018},
	pages = {1--20},
}

@inproceedings{song_penlight_2009,
	title = {{PenLight}: combining a mobile projector and a digital pen for dynamic visual overlay},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Song, Hyunyoung and Grossman, Tovi and Fitzmaurice, George and Guimbretiere, François and Khan, Azam and Attar, Ramtin and Kurtenbach, Gordon},
	year = {2009},
	pages = {143--152},
}

@inproceedings{speicher_what_2019,
	address = {Glasgow Scotland Uk},
	title = {What is {Mixed} {Reality}?},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300767},
	doi = {10.1145/3290605.3300767},
	abstract = {What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We fnd that, while there are prominent examples, there is no universally agreed on, one-size-fts-all defnition of MR. Rather, we identifed six partially competing notions from the literature and experts’ responses. We then started to isolate the diferent aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our fndings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classifcation and discussion of MR applications’ design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape.},
	language = {en},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Speicher, Maximilian and Hall, Brian D. and Nebeling, Michael},
	month = may,
	year = {2019},
	pages = {1--15},
}

@inproceedings{spindler_paperlens_2009,
	address = {Banff Alberta Canada},
	title = {{PaperLens}: advanced magic lens interaction above the tabletop},
	isbn = {978-1-60558-733-2},
	shorttitle = {{PaperLens}},
	url = {https://dl.acm.org/doi/10.1145/1731903.1731920},
	doi = {10.1145/1731903.1731920},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Interactive} {Tabletops} and {Surfaces}},
	publisher = {ACM},
	author = {Spindler, Martin and Stellmach, Sophie and Dachselt, Raimund},
	month = nov,
	year = {2009},
	pages = {69--76},
}

@inproceedings{steimle_physical_2010,
	address = {Saarbrücken Germany},
	title = {Physical and digital media usage patterns on interactive tabletop surfaces},
	isbn = {978-1-4503-0399-6},
	url = {https://dl.acm.org/doi/10.1145/1936652.1936685},
	doi = {10.1145/1936652.1936685},
	abstract = {Concurrent interaction with physical and digital media is ubiquitous in knowledge work. Although tabletop systems increasingly support activities involving both physical and digital media, patterns of use have not been systematically assessed. This paper contributes the results of a study of spatial usage patterns when physical and digital items are grouped and sorted on a tabletop work surface. In addition, analysis reveals a dual character of occlusion, involving both inconvenient and desirable aspects. We conclude with design implications for hybrid tabletop systems.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {{ACM} {International} {Conference} on {Interactive} {Tabletops} and {Surfaces}},
	publisher = {ACM},
	author = {Steimle, Jürgen and Khalilbeigi, Mohammadreza and Mühlhäuser, Max and Hollan, James D.},
	month = nov,
	year = {2010},
	pages = {167--176},
}

@inproceedings{tancik_stegastamp_2020,
	address = {Seattle, WA, USA},
	title = {{StegaStamp}: {Invisible} {Hyperlinks} in {Physical} {Photographs}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{StegaStamp}},
	url = {https://ieeexplore.ieee.org/document/9156548/},
	doi = {10.1109/CVPR42600.2020.00219},
	abstract = {Printed and digitally displayed photos have the ability to hide imperceptible digital data that can be accessed through internet-connected imaging systems. Another way to think about this is physical photographs that have unique QR codes invisibly embedded within them. This paper presents an architecture, algorithms, and a prototype implementation addressing this vision. Our key technical contribution is StegaStamp, a learned steganographic algorithm to enable robust encoding and decoding of arbitrary hyperlink bitstrings into photos in a manner that approaches perceptual invisibility. StegaStamp comprises a deep neural network that learns an encoding/decoding algorithm robust to image perturbations approximating the space of distortions resulting from real printing and photography. We demonstrates real-time decoding of hyperlinks in photos from in-the-wild videos that contain variation in lighting, shadows, perspective, occlusion and viewing distance. Our prototype system robustly retrieves 56 bit hyperlinks after error correction –sufﬁcient to embed a unique code within every photo on the internet.},
	language = {en},
	urldate = {2022-04-25},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tancik, Matthew and Mildenhall, Ben and Ng, Ren},
	month = jun,
	year = {2020},
	pages = {2114--2123},
}

@ARTICLE{wagner2010RealTimeDetection,
  author={Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Real-Time Detection and Tracking for Augmented Reality on Mobile Phones}, 
  year={2010},
  volume={16},
  number={3},
  pages={355-368},
  keywords={Augmented reality;Mobile handsets;Target tracking;Personal communication networks;Robustness;Application software;Real time systems;Computer Society;Multimedia systems;Information systems;Information interfaces and presentation;multimedia information systems;artificial;augmented;and virtual realities;image processing and computer vision;scene analysis;tracking.},
  doi={10.1109/TVCG.2009.99}}

@article{wang_anicode_2019,
	title = {{AniCode}: authoring coded artifacts for network-free personalized animations},
	volume = {35},
	issn = {0178-2789, 1432-2315},
	shorttitle = {{AniCode}},
	url = {http://link.springer.com/10.1007/s00371-019-01681-y},
	doi = {10.1007/s00371-019-01681-y},
	abstract = {Time-based media are used in applications ranging from demonstrating the operation of home appliances to explaining new scientiﬁc discoveries. However, creating effective time-based media is challenging. We introduce a new framework for authoring and consuming time-based media. An author encodes an animation in a printed code and afﬁxes the code to an object. A consumer captures an image of the object through a mobile application, and the image together with the code is used to generate a video on their local device. Our system is designed to be low cost and easy to use. By not requiring an Internet connection to deliver the animation, the framework enhances privacy of the communication. By requiring the user to have a direct line-of-sight view of the object, the framework provides personalized animations that only decode in the intended context. Animation schemes in the system include 2D and 3D geometric transformations, color transformation, and annotation. We demonstrate the new framework with sample applications from a wide range of domains. We evaluate the ease of use and effectiveness of our system with a user study.},
	language = {en},
	number = {6-8},
	urldate = {2022-06-02},
	journal = {The Visual Computer},
	author = {Wang, Zeyu and Qiu, Shiyu and Chen, Qingyang and Trayan, Natallia and Ringlein, Alexander and Dorsey, Julie and Rushmeier, Holly},
	month = jun,
	year = {2019},
	pages = {885--897},
}

@inproceedings{wang_design_2008,
	title = {Design of {Halftone}-{Based} {AR} {Markers} under {Infrared} {Detection}},
	volume = {6},
	doi = {10.1109/CSSE.2008.1391},
	abstract = {As the technology of augmented reality (AR) is getting matured, the application of AR to various fields, especially in a creative manner, become an attractive research topic. The objective of this research is to propose a method for a halftone-based hidden marker for AR tracking system under infrared (IR)detection. Our design method is based on the ordered dithering and the characteristic of carbon black under IR. It provides AR system a novel solution for the marker design, which can make the human-computer interface become much more natural. The results show that the AR marker is invisible by human eyes, but appears under IR. It has many potential value-added applications for future AR systems.},
	booktitle = {2008 {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	author = {Wang, Hsi-Chun and Liu, Wen-Hsin and Chang, Chia-Long and Chen, Yi-Hui},
	month = dec,
	year = {2008},
	keywords = {Application software, Augmented reality, Computer displays, Computer graphics, Frequency modulation, Humans, Infrared detectors, Layout, Printing, Watermarking, augmented reality, digital halftoning, infrared detection, inkjet printing, watermark},
	pages = {97--100},
}

@inproceedings{willis_hideout_2013,
	address = {New York, NY, USA},
	series = {{TEI} '13},
	title = {{HideOut}: mobile projector interaction with tangible objects and surfaces},
	isbn = {978-1-4503-1898-3},
	shorttitle = {{HideOut}},
	url = {https://doi.org/10.1145/2460625.2460682},
	doi = {10.1145/2460625.2460682},
	abstract = {HideOut is a mobile projector-based system that enables new applications and interaction techniques with tangible objects and surfaces. HideOut uses a device mounted camera to detect hidden markers applied with infrared-absorbing ink. The obtrusive appearance of fiducial markers is avoided and the hidden marker surface doubles as a functional projection surface. We present example applications that demonstrate a wide range of interaction scenarios, including media navigation tools, interactive storytelling applications, and mobile games. We explore the design space enabled by the HideOut system and describe the hidden marker prototyping process. HideOut brings tangible objects to life for interaction with the physical world around us.},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Tangible}, {Embedded} and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Willis, Karl D. D. and Shiratori, Takaaki and Mahler, Moshe},
	month = feb,
	year = {2013},
	keywords = {hidden, infrared, ink, interaction, marker, mobile, projector, tangible},
	pages = {331--338},
}

@article{xiao_fontcode_2018,
	title = {{FontCode}: {Embedding} {Information} in {Text} {Documents} {Using} {Glyph} {Perturbation}},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{FontCode}},
	url = {https://dl.acm.org/doi/10.1145/3152823},
	doi = {10.1145/3152823},
	abstract = {We introduce
              FontCode
              , an information embedding technique for text documents. Provided a text document with specific fonts, our method embeds user-specified information in the text by perturbing the glyphs of text characters while preserving the text content. We devise an algorithm to choose unobtrusive yet machine-recognizable glyph perturbations, leveraging a recently developed generative model that alters the glyphs of each character continuously on a font manifold. We then introduce an algorithm that embeds a user-provided message in the text document and produces an encoded document whose appearance is minimally perturbed from the original document. We also present a glyph recognition method that recovers the embedded information from an encoded document stored as a vector graphic or pixel image, or even on a printed paper. In addition, we introduce a new error-correction coding scheme that rectifies a certain number of recognition errors. Lastly, we demonstrate that our technique enables a wide array of applications, using it as a text document metadata holder, an unobtrusive optical barcode, a cryptographic message embedding scheme, and a text document signature.},
	language = {en},
	number = {2},
	urldate = {2022-06-03},
	journal = {ACM Transactions on Graphics},
	author = {Xiao, Chang and Zhang, Cheng and Zheng, Changxi},
	month = apr,
	year = {2018},
	pages = {1--16},
}

@article{zhang_viscode_2021,
	title = {{VisCode}: {Embedding} {Information} in {Visualization} {Images} using {Encoder}-{Decoder} {Network}},
	volume = {27},
	issn = {1941-0506},
	shorttitle = {{VisCode}},
	doi = {10.1109/TVCG.2020.3030343},
	abstract = {We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Peiying and Li, Chenhui and Wang, Changbo},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Data visualization, Decoding, Encoding, Image coding, Image color analysis, Information visualization, Media, Visualization, autocoding, information steganography, saliency detection, visualization retargeting},
	pages = {326--336},
}

