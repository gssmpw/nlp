
@inproceedings{qi_electronic_2010,
	address = {Cambridge Massachusetts USA},
	title = {Electronic popables: exploring paper-based computing through an interactive pop-up book},
	isbn = {978-1-60558-841-4},
	shorttitle = {Electronic popables},
	url = {https://dl.acm.org/doi/10.1145/1709886.1709909},
	doi = {10.1145/1709886.1709909},
	abstract = {We have developed an interactive pop-up book called Electronic Popables to explore paper-based computing. Our book integrates traditional pop-up mechanisms with thin, flexible, paper-based electronics and the result is an artifact that looks and functions much like an ordinary popup, but has added elements of dynamic interactivity. This paper introduces the book and, through it, a library of paper-based sensors and a suite of paper-electronics construction techniques. We also reflect on the unique and under-explored opportunities that arise from combining material experimentation, artistic design, and engineering.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the fourth international conference on {Tangible}, embedded, and embodied interaction},
	publisher = {ACM},
	author = {Qi, Jie and Buechley, Leah},
	month = jan,
	year = {2010},
	pages = {121--128},
}

@inproceedings{steimle_physical_2010,
	address = {Saarbrücken Germany},
	title = {Physical and digital media usage patterns on interactive tabletop surfaces},
	isbn = {978-1-4503-0399-6},
	url = {https://dl.acm.org/doi/10.1145/1936652.1936685},
	doi = {10.1145/1936652.1936685},
	abstract = {Concurrent interaction with physical and digital media is ubiquitous in knowledge work. Although tabletop systems increasingly support activities involving both physical and digital media, patterns of use have not been systematically assessed. This paper contributes the results of a study of spatial usage patterns when physical and digital items are grouped and sorted on a tabletop work surface. In addition, analysis reveals a dual character of occlusion, involving both inconvenient and desirable aspects. We conclude with design implications for hybrid tabletop systems.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {{ACM} {International} {Conference} on {Interactive} {Tabletops} and {Surfaces}},
	publisher = {ACM},
	author = {Steimle, Jürgen and Khalilbeigi, Mohammadreza and Mühlhäuser, Max and Hollan, James D.},
	month = nov,
	year = {2010},
	pages = {167--176},
}

@inproceedings{buechley_paints_2009,
	address = {Cambridge United Kingdom},
	title = {Paints, paper, and programs: first steps toward the computational sketchbook},
	isbn = {978-1-60558-493-5},
	shorttitle = {Paints, paper, and programs},
	url = {https://dl.acm.org/doi/10.1145/1517664.1517670},
	doi = {10.1145/1517664.1517670},
	abstract = {This paper describes what we believe to be important initial steps toward realizing a novel computational medium that combines elements of programming, painting, and papercrafts. Briefly, this genre of paper computing allows a user to create functional computational artifacts on painted paper substrates. We introduce a construction kit for paper computing that consists of computational elements—microcontrollers, sensors, actuators, and power sources—that are held on paper surfaces by magnetic paint and magnets. Conductive paint applied to these surfaces takes on the role of “wires”, connecting the computational elements to one another. These elements can be moved around and from surface to surface, much like magnets on a refrigerator, and the overall result is a tangible medium in which painting, programming, and the affordances of paper blend together. In addition to introducing the kit, we describe example constructions and discuss a variety of potential applications, design projects, and issues for continued research.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Tangible} and {Embedded} {Interaction}},
	publisher = {ACM},
	author = {Buechley, Leah and Hendrix, Sue and Eisenberg, Mike},
	month = feb,
	year = {2009},
	pages = {9--12},
}

@inproceedings{gong_printsense_2014,
	address = {Toronto Ontario Canada},
	title = {{PrintSense}: a versatile sensing technique to support multimodal flexible surface interaction},
	isbn = {978-1-4503-2473-1},
	shorttitle = {{PrintSense}},
	url = {https://dl.acm.org/doi/10.1145/2556288.2557173},
	doi = {10.1145/2556288.2557173},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Gong, Nan-Wei and Steimle, Jürgen and Olberding, Simon and Hodges, Steve and Gillian, Nicholas Edward and Kawahara, Yoshihiro and Paradiso, Joseph A.},
	month = apr,
	year = {2014},
	pages = {1407--1410},
}

@inproceedings{pedersen_paperbuttons_2000,
	address = {New York City New York USA},
	title = {{PaperButtons}: expanding a tangible user interface},
	isbn = {978-1-58113-219-9},
	shorttitle = {{PaperButtons}},
	url = {https://dl.acm.org/doi/10.1145/347642.347723},
	doi = {10.1145/347642.347723},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 3rd conference on {Designing} interactive systems: processes, practices, methods, and techniques},
	publisher = {ACM},
	author = {Pedersen, Elin Rønby and Sokoler, Tomas and Nelson, Les},
	month = aug,
	year = {2000},
	pages = {216--223},
}

@inproceedings{jacoby_drawing_2013,
	address = {New York, NY, USA},
	series = {{IDC} '13},
	title = {Drawing the electric: storytelling with conductive ink},
	isbn = {978-1-4503-1918-8},
	shorttitle = {Drawing the electric},
	url = {https://dl.acm.org/doi/10.1145/2485760.2485790},
	doi = {10.1145/2485760.2485790},
	abstract = {We explore conductive ink as an expressive medium for narrative storytelling and interaction design with children, introducing StoryClip, a toolkit that integrates functional materials, computation, and drawing. The StoryClip kit consists of silver ink, ordinary art supplies, and a hardware-software tool, allowing a child's drawing to function as an audio recording-and-playback interface. We exploit craft and artistic practice to motivate technological exploration, turning a conventional illustration into a multimedia interface that promotes multi-level engagement with children. In this note, we describe the design of our system and discuss our findings from two workshops with children.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Interaction} {Design} and {Children}},
	publisher = {Association for Computing Machinery},
	author = {Jacoby, Sam and Buechley, Leah},
	month = jun,
	year = {2013},
	keywords = {conductive ink, creativity, expression, paper, prototyping, storytelling},
	pages = {265--268},
}

@inproceedings{spindler_paperlens_2009,
	address = {Banff Alberta Canada},
	title = {{PaperLens}: advanced magic lens interaction above the tabletop},
	isbn = {978-1-60558-733-2},
	shorttitle = {{PaperLens}},
	url = {https://dl.acm.org/doi/10.1145/1731903.1731920},
	doi = {10.1145/1731903.1731920},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Interactive} {Tabletops} and {Surfaces}},
	publisher = {ACM},
	author = {Spindler, Martin and Stellmach, Sophie and Dachselt, Raimund},
	month = nov,
	year = {2009},
	pages = {69--76},
}

@inproceedings{klamka_illumipaper_2017,
	address = {New York, NY, USA},
	series = {{CHI} '17},
	title = {{IllumiPaper}: {Illuminated} {Interactive} {Paper}},
	isbn = {978-1-4503-4655-9},
	shorttitle = {{IllumiPaper}},
	url = {https://dl.acm.org/doi/10.1145/3025453.3025525},
	doi = {10.1145/3025453.3025525},
	abstract = {Due to their simplicity and flexibility, digital pen-and-paper solutions have a promising potential to become a part of our daily work. Unfortunately, they lack dynamic visual feedback and thereby restrain advanced digital functionalities. In this paper, we investigate new forms of paper-integrated feedback, which build on emerging paper-based electronics and novel thin-film display technologies. Our approach focuses on illuminated elements, which are seamlessly integrated into standard paper. For that, we introduce an extended design space for paper-integrated illuminations. As a major contribution, we present a systematic feedback repertoire for real-world applications including feedback components for innovative paper interaction tasks in five categories. Furthermore, we contribute a fully-functional research platform including a paper-controller, digital pen and illuminated, digitally controlled papers that demonstrate the feasibility of our techniques. Finally, we report on six interviews, where experts rated our approach as intuitive and very usable for various applications, in particular educational ones.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Klamka, Konstantin and Dachselt, Raimund},
	month = may,
	year = {2017},
	keywords = {anoto, augmented paper, digital pen and paper, electro-luminescence, pen interaction, thin-film display, visual feedback},
	pages = {5605--5618},
}

@inproceedings{klamka_illuminated_2017,
	address = {New York, NY, USA},
	series = {{ISS} '17},
	title = {Illuminated {Interactive} {Paper} with {Multiple} {Input} {Modalities} for {Form} {Filling} {Applications}},
	isbn = {978-1-4503-4691-7},
	url = {https://dl.acm.org/doi/10.1145/3132272.3132287},
	doi = {10.1145/3132272.3132287},
	abstract = {In this paper, we demonstrate IllumiPaper: a system that provides new forms of paper-integrated visual feedback and enables multiple input channels to enhance digital paper applications. We aim to take advantage of traditional form sheets, including their haptic qualities, simplicity, and archivability, and simultaneously integrate rich digital functionalities such as dynamic status queries, real-time notifications, and visual feedback for widget controls. Our approach builds on emerging, novel paper-based technologies. We describe a fabrication process that allow us to directly integrate segment-based displays, touch and flex sensors, as well as digital pen input on the paper itself. With our fully functional research platform we demonstrate an interactive prototype for an industrial form-filling maintenance application to service computer networks that covers a wide range of typical paper-related tasks.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Interactive} {Surfaces} and {Spaces}},
	publisher = {Association for Computing Machinery},
	author = {Klamka, Konstantin and Büschel, Wolfgang and Dachselt, Raimund},
	month = oct,
	year = {2017},
	keywords = {Anoto, Digital pen and paper, augmented paper, electroluminescence, form filling, printed electronics, visual feedback},
	pages = {434--437},
}

@article{di_gioia_investigating_2022,
	title = {Investigating the {Use} of {AR} {Glasses} for {Content} {Annotation} on {Mobile} {Devices}},
	volume = {6},
	url = {https://dl.acm.org/doi/10.1145/3567727},
	doi = {10.1145/3567727},
	abstract = {Mobile devices such as smartphones and tablets have limited display size and input capabilities that make a variety of tasks challenging. Coupling the mobile device with Augmented Reality eyewear such as smartglasses can help address some of these challenges. In the specific context of digital content annotation tasks, this combination has the potential to enhance the user experience on two fronts. First, annotations can be offloaded into the air around the mobile device, freeing precious screen real-estate. Second, as smartglasses often come equipped with a variety of sensors including a camera, users can annotate documents with pictures or videos of their environment, captured on the spot, hands-free, and from the wearer's perspective. We present AnnotAR, a prototype that we use as a research probe to assess the viability of this approach to digital content annotation. We use AnnotAR to gather users' preliminary feedback in a laboratory setting, and to showcase how it could support real-world use cases.},
	number = {ISS},
	urldate = {2024-05-01},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Di Gioia, Francesco Riccardo and Brasier, Eugenie and Pietriga, Emmanuel and Appert, Caroline},
	month = nov,
	year = {2022},
	keywords = {annotation, augmented reality, mobile device},
	pages = {574:430--574:447},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@article{howard_fastai_2020,
	title = {fastai: {A} {Layered} {API} for {Deep} {Learning}},
	volume = {11},
	issn = {2078-2489},
	shorttitle = {fastai},
	url = {http://arxiv.org/abs/2002.04688},
	doi = {10.3390/info11020108},
	abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/},
	number = {2},
	urldate = {2024-04-30},
	journal = {Information},
	author = {Howard, Jeremy and Gugger, Sylvain},
	month = feb,
	year = {2020},
	note = {arXiv:2002.04688 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {108},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2024-04-30},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{kang_gray_1994,
	title = {Gray component replacement using color mixing models},
	volume = {2171},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2171/0000/Gray-component-replacement-using-color-mixing-models/10.1117/12.175317.full},
	doi = {10.1117/12.175317},
	abstract = {A new approach to the gray component replacement (GCR) has been developed. It employs the color mixing theory for modeling the spectral fit between the 3-color and 4-color prints. To achieve this goal, we first examine the accuracy of the models with respect to the experimental results by applying them to the prints made by a Canon Color Laser Copier-500 (CLC-500). An empirical halftone correction factor is used for improving the data fitting. Among the models tested, the halftone corrected Kubelka-Munk theory gives the closest fit, followed by the halftone corrected Beer-Bouguer law and the Yule-Neilsen approach. We then apply the halftone corrected BB law to GCR. The main feature of this GCR approach is based on the spectral measurements of the primary color step wedges and a software package implementing the color mixing model. The software determines the amount of the gray component to be removed, then adjusts each primary color until a good match of the peak wavelengths between the 3-color and 4-color spectra is obtained. Results indicate that the average (Delta) E{\textless}SUB{\textgreater}ab{\textless}/SUB{\textgreater} between cmy and cmyk renditions of 64 color patches is 3.11 (Delta) E{\textless}SUB{\textgreater}ab{\textless}/SUB{\textgreater}. Eighty-seven percent of the patches has (Delta) E{\textless}SUB{\textgreater}ab{\textless}/SUB{\textgreater} less than 5 units. The advantage of this approach is its simplicity; there is no need for the black printer and under color addition. Because this approach is based on the spectral reproduction, it minimizes the metamerism.},
	urldate = {2024-04-25},
	booktitle = {Color {Hard} {Copy} and {Graphic} {Arts} {III}},
	publisher = {SPIE},
	author = {Kang, Henry R.},
	month = may,
	year = {1994},
	pages = {287--296},
}

@article{bandyopadhyay_effect_2000,
	title = {Effect of {Gray} {Component} {Replacement} on {Color} {Reproduction}},
	abstract = {Gray Component Replacement is a separation technique for replacing cyan, magenta and yellow inks with black throughout the image. In this separation, black inks reduce the amount of process colored inks which are expensive. It offers more consistent color and gray balance. Total ink coverage ( TIC ) is reduced to a great extent by GCR. More critical colors like flesh tones can be better adjusted by this separation. Brighter colors can be obtained on lower grade papers. However, Gray Component Replacement is not always the best choice. Sometimes, it reduces the ability to adjust colors. It may create problems with balancing black halftone. High amount of GCR causes the shadow areas to print too weakly resulting in low contrast. To alleviate the lower Total Coverage with high level of GCR, undercolor addition ( UCA ) is often required in the achromatic areas of the image. In the present work, the investigators have taken different prints in a HP Deskjet printer varying the amount of GCR. The prints are then compared with the originals. The total ink coverage ( TIC) is measured for each image in different areas of grays ranging from highlight to shadow. The tone reproduction curves are drawn to observe how GCR affects image quality and total ink coverage of color reproduction.},
	language = {en},
	author = {Bandyopadhyay, Swati and Mandal, Subhendu},
	year = {2000},
}

@article{chung_test_2003,
	title = {Test {Targets} 3.1: {A} {Collaborative} effort exploring the use of scientific methods for color imaging and process control.},
	shorttitle = {Test {Targets} 3.1},
	url = {https://repository.rit.edu/books/69},
	journal = {Books},
	author = {Chung, Robert and Chun, Edline and Hsu, Fred and Kolli, Hemachand and Lesser, Jon and Kong, Lingjun and Testa, Ryan and Ha, Seunga Kang and Shetty, Somika and Gupta, Vikaas},
	month = jan,
	year = {2003},
}

@inproceedings{enoksson_compensation_2006,
	title = {Compensation by black ({CB}) : {A} new separation?},
	shorttitle = {Compensation by black ({CB})},
	url = {https://www.semanticscholar.org/paper/Compensation-by-black-(CB)-%3A-A-new-separation-Enoksson-Bjurstedt/bdc2dc1f12e810bd11e7b90ab69161bec66b52f4},
	abstract = {The aim of this paper is to examine the differences between UCR (Under Color Removal) and GCR (Gray Component Replacement) by testing these separation functions in three applications: Adobe Photoshop CS (an image editing application), Gretag Macbeth's Profile Maker 5.0 (profile maker), and Heidelberg's Print Open 4.0.5 (profile maker). A review of the literature pertaining to the different types of separation was made and compiled. An Internet search was also made to check what a prepress employee would find out if he or she was to search for a definition of one of these types of separation. The conclusions of the tests made suggest two alternative proposals and indicate a need to either: 1) Discard the term UCR and use only GCR, as it really only concerns gray component replacement. This would make it easier for people in the business to focus on the process itself instead of trying to understand the difference between the two types of separations, a difference which actually cannot be seen visualy in reality. 2) Discard both terms and introduce a new term CB (Compensation by Black). The software should give the user the possibility of choosing how much black will be used and where it will replace the use of a combination of the CMY process colors. In addition, a single term would make the user more aware of the problems of separation and of how separation will affect the print result. The suggestions imply an extensive review of accepted terms and abbreviations within the graphic arts industry with the aim of giving them a uniform scientific meaning and definition. Thus, it is strongly recommended that the term, CB (Compensation by Black) should be implemented.},
	urldate = {2024-04-25},
	author = {Enoksson, E. and Bjurstedt, Anders},
	year = {2006},
}

@article{donevski_colorimetrically_2017,
	title = {Colorimetrically accurate gray component replacement using the additive model},
	volume = {44},
	doi = {10.1016/j.jvcir.2017.01.018},
	abstract = {In four color printing, gray component replacement (GCR) is a method of replacing the achromatic component of a mixture of chromatic inks by a corresponding amount of the achromatic ink. Simple approaches to GCR lead to unacceptable color shifts. In this paper, the method using multiple processing steps is proposed. The novelty of the proposed method is augmentation of masking equations, which allows finding solutions for a pre-set amount of the black ink. The method performance was evaluated and compared with state of the art commercial solution. The experiments have shown that the proposed model is on average capable of achieving 40-55\% ink savings with median colorimetric difference of less than 0.3, thus preserving the visual appearance of images.},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Donevski, Davor and Poljicak, Ante and Kurecic, Maja},
	month = jan,
	year = {2017},
}

@article{testa_comparison_nodate,
	title = {A {Comparison} of {Color} {Conversion} between {Photoshop} \& {ICC} {CMS}},
	language = {en},
	author = {Testa, Ryan},
}

@article{enoksson_compensation_nodate,
	title = {Compensation by black ({CB}) – a new separation?},
	language = {en},
	author = {Enoksson, Emmi and Bjurstedt, Anders},
}

@inproceedings{dogan_brightmarker_2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {{BrightMarker}: {3D} {Printed} {Fluorescent} {Markers} for {Object} {Tracking}},
	isbn = {9798400701320},
	shorttitle = {{BrightMarker}},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606758},
	doi = {10.1145/3586183.3606758},
	abstract = {Existing invisible object tagging methods are prone to low resolution, which impedes tracking performance. We present BrightMarker, a fabrication method that uses fluorescent filaments to embed easily trackable markers in 3D printed color objects. By using an infrared-fluorescent filament that "shifts" the wavelength of the incident light, our optical detection setup filters out all the noise to only have the markers present in the infrared camera image. The high contrast of the markers allows us to track them robustly regardless of the moving objects’ surface color. We built a software interface for automatically embedding these markers for the input object geometry, and hardware modules that can be attached to existing mobile devices and AR/VR headsets. Our image processing pipeline robustly localizes the markers in real time from the captured images. BrightMarker can be used in a variety of applications, such as custom fabricated wearables for motion capture, tangible interfaces for AR/VR, rapid product tracking, and privacy-preserving night vision. BrightMarker exceeds the detection rate of state-of-the-art invisible marking, and even small markers (1"x1") can be tracked at distances exceeding 2m.},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Garcia-Martin, Raul and Haertel, Patrick William and O'Keefe, Jamison John and Taka, Ahmad and Aurora, Akarsh and Sanchez-Reillo, Raul and Mueller, Stefanie},
	month = oct,
	year = {2023},
	keywords = {3D printing, digital fabrication, fluorescence, infrared imaging, invisible markers, marker tracking},
	pages = {1--13},
}

@article{song_my_2018,
	title = {My {Smartphone} {Recognizes} {Genuine} {QR} {Codes}!: {Practical} {Unclonable} {QR} {Code} via {3D} {Printing}},
	volume = {2},
	issn = {2474-9567},
	shorttitle = {My {Smartphone} {Recognizes} {Genuine} {QR} {Codes}!},
	url = {https://dl.acm.org/doi/10.1145/3214286},
	doi = {10.1145/3214286},
	abstract = {Additive manufacturing, or 3D printing, has been widely applied in product manufacturing. However, the emerging unauthorized access of 3D printing data, as well as the growth in the pervasiveness and capability of 3D printing devices have raised serious concerns about 3D printing product anti-counterfeit. Electronic product tags are the current standard for authentication purposes; however, often this technology is neither secure nor cost-effective, and fails to take advantage of other unique 3D printing features. Considering the great usability of the QR code, we are motivated to enhance the QR code for the practical and cost-effective 3D printing product identification. Particularly, we bring up the all-in-one design, all-in-one manufacturing concept incorporating the QR code in the complete 3D printing paradigm. In detail, we explore the possibility of leveraging the random and uncontrollable process variations in the 3D printing system to generate a unique fingerprint for the integrated QR code. To this end, we present an end-to-end 3D-printed QR code verification framework, which does not change the original QR protocol and functionality. The entire solution can be implemented with commodity 3D printers and smartphones. Specifically, we first investigate the inevitable and random process variations in the 3D printing mechanism and analyze the causality between the variations and detectable geometric deformation. We further develop a fingerprint extraction algorithm taking into account both the QR code property and the 3D printer characteristics. The system evaluation indicates that our solution is secure and robust in multiple scenarios.},
	language = {en},
	number = {2},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Song, Chen and Li, Zhengxiong and Xu, Wenyao and Zhou, Chi and Jin, Zhanpeng and Ren, Kui},
	month = jul,
	year = {2018},
	pages = {1--20},
}

@article{jiang_infoprint_2023,
	title = {{InfoPrint}: {Embedding} {Interactive} {Information} in {3D} {Prints} {Using} {Low}-{Cost} {Readily}-{Available} {Printers} and {Materials}},
	volume = {7},
	shorttitle = {{InfoPrint}},
	url = {https://dl.acm.org/doi/10.1145/3610933},
	doi = {10.1145/3610933},
	abstract = {We present a fully-printable method to embed interactive information inside 3D printed objects. The information is invisible to the human eye and can be read using thermal imaging after temperature transfer through interaction with the objects. Prior methods either modify the surface appearance, require customized devices or not commonly used materials, or embed components that are not fully 3D printable. Such limitations restrict the design space for 3D prints, or cannot be readily applied to the already deployed 3D printing setups. In this paper, we present an information embedding technique using low-cost off-the-shelf dual extruder FDM (Fused Deposition Modeling) 3D printers, common materials (e.g., generic PLA), and a mobile thermal device (e.g., a thermal smartphone), by leveraging the thermal properties of common 3D print materials. In addition, we show our method can also be generalized to conventional near-infrared imaging scenarios. We evaluate our technique against multiple design and fabrication parameters and propose a design guideline for different use cases. Finally, we demonstrate various everyday applications enabled by our method, such as interactive thermal displays, user-activated augmented reality, automating thermal triggered events, and hidden tokens for social activities.},
	number = {3},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Jiang, Weiwei and Wang, Chaofan and Sarsenbayeva, Zhanna and Irlitti, Andrew and Wei, Jing and Knibbe, Jarrod and Dingler, Tilman and Goncalves, Jorge and Kostakos, Vassilis},
	month = sep,
	year = {2023},
	keywords = {3D print, Thermal imaging, information embedding, near-infrared},
	pages = {102:1--102:29},
}

@article{cheng_silver_2020,
	title = {Silver {Tape}: {Inkjet}-{Printed} {Circuits} {Peeled}-and-{Transferred} on {Versatile} {Substrates}},
	volume = {4},
	issn = {2474-9567},
	shorttitle = {Silver {Tape}},
	url = {https://dl.acm.org/doi/10.1145/3381013},
	doi = {10.1145/3381013},
	abstract = {We propose Silver Tape, a simple yet novel fabrication technique to transfer inkjet-printed silver traces from paper onto versatile substrates, without time-/space- consuming processes such as screen printing or heat sintering. This allows users to quickly implement silver traces with a variety of properties by exploiting a wide range of substrates. For instance, high flexibility can be achieved with Scotch tape, high transparency with polydimethylsiloxane (PDMS), heat durability with Kapton polyimide tape, water solubility with 3M water-soluble tape, and beyond. Many of these properties are not achievable with conventional substrates that are used for inkjet-printing conductive traces. Specifically, our technique leverages the commonly undesired low adhesion property of the inkjet printing films and repurposes these films as temporary transfer media. We describe our fabrication methods with a library of materials we can utilize, evaluate the mechanical and electrical properties of the transferred traces, and conclude with several demonstrative applications. We believe Silver Tape enriches novel interactions for the ubiquitous computing domain, by enabling digital fabrication of electronics on versatile materials, surfaces, and shapes.},
	language = {en},
	number = {1},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Cheng, Tingyu and Narumi, Koya and Do, Youngwook and Zhang, Yang and Ta, Tung D. and Sasatani, Takuya and Markvicka, Eric and Kawahara, Yoshihiro and Yao, Lining and Abowd, Gregory D. and Oh, HyunJoo},
	month = mar,
	year = {2020},
	pages = {1--17},
}

@inproceedings{kawahara_instant_2013,
	address = {Zurich Switzerland},
	title = {Instant inkjet circuits: lab-based inkjet printing to support rapid prototyping of {UbiComp} devices},
	isbn = {978-1-4503-1770-2},
	shorttitle = {Instant inkjet circuits},
	url = {https://dl.acm.org/doi/10.1145/2493432.2493486},
	doi = {10.1145/2493432.2493486},
	language = {en},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the 2013 {ACM} international joint conference on {Pervasive} and ubiquitous computing},
	publisher = {ACM},
	author = {Kawahara, Yoshihiro and Hodges, Steve and Cook, Benjamin S. and Zhang, Cheng and Abowd, Gregory D.},
	month = sep,
	year = {2013},
	pages = {363--372},
}

@article{balaji_retrosphere_2023,
	title = {{RetroSphere}: {Self}-{Contained} {Passive} {3D} {Controller} {Tracking} for {Augmented} {Reality}},
	volume = {6},
	shorttitle = {{RetroSphere}},
	url = {https://dl.acm.org/doi/10.1145/3569479},
	doi = {10.1145/3569479},
	abstract = {Advanced AR/VR headsets often have a dedicated depth sensor or multiple cameras, high processing power, and a high-capacity battery to track hands or controllers. However, these approaches are not compatible with the small form factor and limited thermal capacity of lightweight AR devices. In this paper, we present RetroSphere, a self-contained 6 degree of freedom (6DoF) controller tracker that can be integrated with almost any device. RetroSphere tracks a passive controller with just 3 retroreflective spheres using a stereo pair of mass-produced infrared blob trackers, each with its own infrared LED emitters. As the sphere is completely passive, no electronics or recharging is required. Each object tracking camera provides a tiny Arduino-compatible ESP32 microcontroller with the 2D position of the spheres. A lightweight stereo depth estimation algorithm that runs on the ESP32 performs 6DoF tracking of the passive controller. Also, RetroSphere provides an auto-calibration procedure to calibrate the stereo IR tracker setup. Our work builds upon Johnny Lee's Wii remote hacks and aims to enable a community of researchers, designers, and makers to use 3D input in their projects with affordable off-the-shelf components. RetroSphere achieves a tracking accuracy of about 96.5\% with errors as low as {\textasciitilde}3.5 cm over a 100 cm tracking range, validated with ground truth 3D data obtained using a LIDAR camera while consuming around 400 mW. We provide implementation details, evaluate the accuracy of our system, and demonstrate example applications, such as mobile AR drawing, 3D measurement, etc. with our Retrosphere-enabled AR glass prototype.},
	number = {4},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Balaji, Ananta Narayanan and Kimber, Clayton and Li, David and Wu, Shengzhi and Du, Ruofei and Kim, David},
	month = jan,
	year = {2023},
	keywords = {Augmented reality, Augmented reality glasses, Infrared marker tracking, Retroreflectors, Virtual reality},
	pages = {157:1--157:36},
}

@article{drey_sparklingpaper_2022,
	title = {{SpARklingPaper}: {Enhancing} {Common} {Pen}- {And} {Paper}-{Based} {Handwriting} {Training} for {Children} by {Digitally} {Augmenting} {Papers} {Using} a {Tablet} {Screen}},
	volume = {6},
	shorttitle = {{SpARklingPaper}},
	url = {https://dl.acm.org/doi/10.1145/3550337},
	doi = {10.1145/3550337},
	abstract = {Educational apps support learning, but handwriting training is still based on analog pen- and paper. However, training handwriting with apps can negatively affect graphomotor handwriting skills due to the different haptic feedback of the tablet, stylus, or finger compared to pen and paper. With SpARklingPaper, we are the first to combine the genuine haptic feedback of analog pen and paper with the digital support of apps. Our artifact contribution enables children to write with any pen on a standard paper placed on a tablet's screen, augmenting the paper from below, showing animated letters and individual feedback. We conducted two online surveys with overall 29 parents and teachers of elementary school pupils and a user study with 13 children and 13 parents for evaluation. Our results show the importance of the genuine analog haptic feedback combined with the augmentation of SpARklingPaper. It was rated superior compared to our stylus baseline condition regarding pen-handling, writing training-success, motivation, and overall impression. SpARklingPaper can be a blueprint for high-fidelity haptic feedback handwriting training systems.},
	number = {3},
	urldate = {2024-04-23},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Drey, Tobias and Janek, Jessica and Lang, Josef and Puschmann, Dietmar and Rietzler, Michael and Rukzio, Enrico},
	month = sep,
	year = {2022},
	keywords = {artifact, augmented reality, children, education, handwriting training, literacy training, mobile devices, tablet},
	pages = {113:1--113:29},
}

@inproceedings{kawahara_instant_2013-1,
	address = {New York, NY, USA},
	series = {{UbiComp} '13},
	title = {Instant inkjet circuits: lab-based inkjet printing to support rapid prototyping of {UbiComp} devices},
	isbn = {978-1-4503-1770-2},
	shorttitle = {Instant inkjet circuits},
	url = {https://dl.acm.org/doi/10.1145/2493432.2493486},
	doi = {10.1145/2493432.2493486},
	abstract = {This paper introduces a low cost, fast and accessible technology to support the rapid prototyping of functional electronic devices. Central to this approach of 'instant inkjet circuits' is the ability to print highly conductive traces and patterns onto flexible substrates such as paper and plastic films cheaply and quickly. In addition to providing an alternative to breadboarding and conventional printed circuits, we demonstrate how this technique readily supports large area sensors and high frequency applications such as antennas. Unlike existing methods for printing conductive patterns, conductivity emerges within a few seconds without the need for special equipment. We demonstrate that this technique is feasible using commodity inkjet printers and commercially available ink, for an initial investment of around US\$300. Having presented this exciting new technology, we explain the tools and techniques we have found useful for the first time. Our main research contribution is to characterize the performance of instant inkjet circuits and illustrate a range of possibilities that are enabled by way of several example applications which we have built. We believe that this technology will be of immediate appeal to researchers in the ubiquitous computing domain, since it supports the fabrication of a variety of functional electronic device prototypes.},
	urldate = {2024-04-23},
	booktitle = {Proceedings of the 2013 {ACM} international joint conference on {Pervasive} and ubiquitous computing},
	publisher = {Association for Computing Machinery},
	author = {Kawahara, Yoshihiro and Hodges, Steve and Cook, Benjamin S. and Zhang, Cheng and Abowd, Gregory D.},
	month = sep,
	year = {2013},
	keywords = {capacitive sensors, conductive ink, digital fabrication, inkjet-printing, rapid prototyping},
	pages = {363--372},
}

@techreport{ridpath_techniques_2000,
	title = {Techniques {For} {Accessibility} {Evaluation} {And} {Repair} {Tools}},
	url = {https://www.w3.org/TR/AERT//#color-contrast},
	urldate = {2024-04-09},
	institution = {W3C},
	author = {Ridpath, Chris and Chisholm, Wendy},
	year = {2000},
}

@misc{megacal_mecanizados_2024,
	title = {Mecanizados de {Precisión}},
	url = {https://megacal.es/},
	urldate = {2024-04-01},
	author = {{Megacal}},
	year = {2024},
}

@article{leek_adaptive_2001,
	title = {Adaptive {Procedures} in {Psychophysical} {Research}},
	volume = {63},
	url = {https://doi.org/10.3758/BF03194543},
	doi = {10.3758/BF03194543},
	number = {8},
	journal = {Perception \& Psychophysics},
	author = {Leek, Marjorie R.},
	month = nov,
	year = {2001},
	pages = {1279--1292},
}

@incollection{kingdom_chapter_2016,
	address = {San Diego},
	title = {Chapter 5 - {Adaptive} {Methods}},
	isbn = {978-0-12-407156-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124071568000050},
	booktitle = {Psychophysics ({Second} {Edition})},
	publisher = {Academic Press},
	author = {Kingdom, Frederick A. A. and Prins, Nicolaas},
	year = {2016},
	doi = {https://doi.org/10.1016/B978-0-12-407156-8.00005-0},
	pages = {119--148},
}

@article{li_black_2013,
	title = {A {Black} {Generation} {Method} for {Black} {Ink} {Hiding} {Infrared} {Security} {Image}},
	volume = {262},
	journal = {Applied Mechanics and Materials},
	author = {Li, Chao and Wang, Cai Yin and Wang, Shu Jie},
	year = {2013},
	note = {Publisher: Trans Tech Publ},
	pages = {9--12},
}

@inproceedings{dogan_standarone_2023,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {{StandARone}: {Infrared}-{Watermarked} {Documents} as {Portable} {Containers} of {AR} {Interaction} and {Personalization}},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{StandARone}},
	url = {https://doi.org/10.1145/3544549.3585905},
	doi = {10.1145/3544549.3585905},
	abstract = {Hybrid paper interfaces leverage augmented reality (AR) to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, the instructions for how the virtual content should be generated are not an intrinsic part of the document but rather accessed through a link to remote resources. To enable hybrid documents to be portable containers of also the AR content, we introduce StandARone documents. Using our system, a document author can define AR content and embed it invisibly on the document using a standard inkjet printer and infrared-absorbing ink. A document consumer can interact with the embedded content using a smartphone with a NIR camera without requiring a network connection. We demonstrate several use cases of StandARone including personalized offline menus, interactive visualizations, and location-aware packaging.},
	urldate = {2024-04-09},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Siu, Alexa F. and Healey, Jennifer and Wigington, Curtis and Xiao, Chang and Sun, Tong},
	month = apr,
	year = {2023},
	keywords = {augmented reality, documents, fabrication, infrared imaging, mixed reality, paper interfaces, watermarking},
	pages = {1--7},
}

@misc{rgm_vision_night_nodate,
	title = {Night {Vision} {Camera} {Mobile}},
	url = {https://www.rgmvision.com/night-vision-camera-mobile/},
	abstract = {Discover the first NIGHT VISION CAMERA for your mobile phone! See in the dark with our near-infrared night vision cam technology. Order now!},
	language = {en-US},
	urldate = {2024-04-02},
	journal = {RGM Vision},
	author = {RGM Vision},
}

@misc{megacal_mecanizados_nodate,
	title = {Mecanizados de {Precisión} - {MECAGAL}},
	url = {https://megacal.es/},
	abstract = {Servicio Integral de Mecanizado de Precisión. Diseño y fabricación de piezas para la industria en general orientado a los requerimientos del cliente},
	language = {es},
	urldate = {2024-04-02},
	journal = {Megacal},
	author = {Megacal},
}

@misc{noauthor_notitle_nodate,
}

@article{grsic_hidden_2022,
	title = {Hidden {Information} in {Uniform} {Design} for {Visual} and {Near}-{Infrared} {Spectrum} and for {Inkjet} {Printing} of {Clothing} on {Canvas} to {Enhance} {Urban} {Security}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/4/2152},
	doi = {10.3390/app12042152},
	abstract = {In this paper, we publish the algorithm to create a dual image manifested in the infrared and visible spectrum. To distinguish the information in the two light spectra, twin dyes are introduced for the inkjet plotter and printing realization for garment and canvas items. The graphics, invisible to the naked eye, are designed for the near infrared (NIR) spectrum and are suitable for urban security where surveillance cameras with IR detectors are installed. The duality of dyes is presented in tables, and the analysis is a basis for programming and developing new algorithms for the application of “Infrared Dyeing” on different materials and printing technologies. Through spectroscopy, this innovative solution demonstrates twin colorants by printing in one pass through the plotter such that one image remains visible, while the other one is hidden to the bare eye. The uniform and the school bag cover presented in this paper are kept simple in design because they incorporate information hidden to the naked eye but visible with surveillance cameras and all the other infrared detectors. The article provides mathematical models of duality coloring as a basis for programming the graphic prepress that merges both of the images, the visual one and the infrared one. A topic is the fusion of two images with colors that represent two graphs, with independent contents for the visual and near-infrared spectrum.},
	language = {en},
	number = {4},
	urldate = {2023-09-08},
	journal = {Applied Sciences},
	author = {Gršić, Jana Žiljak and Jurečić, Denis and Tepeš Golubić, Lidija and Žiljak, Vilko},
	month = feb,
	year = {2022},
	pages = {2152},
}

@inproceedings{ashtari_creating_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Creating {Augmented} and {Virtual} {Reality} {Applications}: {Current} {Practices}, {Challenges}, and {Opportunities}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Creating {Augmented} and {Virtual} {Reality} {Applications}},
	url = {https://doi.org/10.1145/3313831.3376722},
	doi = {10.1145/3313831.3376722},
	abstract = {Augmented Reality (AR) and Virtual Reality (VR) devices are becoming easier to access and use, but the barrier to entry for creating AR/VR applications remains high. Although the recent spike in HCI research on novel AR/VR tools is promising, we lack insights into how AR/VR creators use today's state-of-the-art authoring tools as well as the types of challenges that they face. We interviewed 21 AR/VR creators, which we grouped into hobbyists, domain experts, and professional designers. Despite having a variety of motivations and skillsets, they described similar challenges in designing and building AR/VR applications. We synthesize 8 key barriers that AR/VR creators face nowadays, starting from prototyping the initial experiences to dealing with "the many unknowns" during implementation, to facing difficulties in testing applications. Based on our analysis, we discuss the importance of considering end-user developers as a growing population of AR/VR creators, how we can build learning opportunities into AR/VR tools, and the need for building AR/VR toolchains that integrate debugging and testing.},
	urldate = {2023-03-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ashtari, Narges and Bunt, Andrea and McGrenere, Joanna and Nebeling, Michael and Chilana, Parmit K.},
	month = apr,
	year = {2020},
	keywords = {AR/VR authoring, AR/VR design, AR/VR development, augmented reality, end-user development, virtual reality},
	pages = {1--13},
}

@inproceedings{zheng_tangible_2020,
	address = {Eindhoven Netherlands},
	title = {Tangible {Interfaces} with {Printed} {Paper} {Markers}},
	isbn = {978-1-4503-6974-9},
	url = {https://dl.acm.org/doi/10.1145/3357236.3395578},
	doi = {10.1145/3357236.3395578},
	abstract = {This pictorial presents a design investigation at the intersection of paper and computer vision for tangible interfaces. Through this exploration, we uncovered various characteristics of paper that connect tangible interactions with concealing and revealing printed fiducial markers for detection—particularly through the affordances of paper craft and fiber. We illustrate a variety of paper structures that construct and deconstruct fiducial markers. We also demonstrate how these structures enable untethered functional physical inputs, such as push buttons and sliders. We showcase four proposals that extend these material insights into tangible interface applications, including interactive data physicalizations and functional paper prototypes. Furthermore, we continue the legacy of pictorials by exposing fabrication drawings for others to engage with this work at a more practical level.},
	language = {en},
	urldate = {2021-05-31},
	booktitle = {Proceedings of the 2020 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {ACM},
	author = {Zheng, Clement and Gyory, Peter and Do, Ellen Yi-Luen},
	month = jul,
	year = {2020},
	pages = {909--923},
}

@article{zhang_viscode_2021,
	title = {{VisCode}: {Embedding} {Information} in {Visualization} {Images} using {Encoder}-{Decoder} {Network}},
	volume = {27},
	issn = {1941-0506},
	shorttitle = {{VisCode}},
	doi = {10.1109/TVCG.2020.3030343},
	abstract = {We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Peiying and Li, Chenhui and Wang, Changbo},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Data visualization, Decoding, Encoding, Image coding, Image color analysis, Information visualization, Media, Visualization, autocoding, information steganography, saliency detection, visualization retargeting},
	pages = {326--336},
}

@article{tong_exploring_2023,
	title = {Exploring {Interactions} with {Printed} {Data} {Visualizations} in {Augmented} {Reality}},
	volume = {29},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2022.3209386},
	abstract = {This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops (\${\textbackslash}mathrmN=20\$) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study (\${\textbackslash}mathrmN=12\$, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement “point” for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Tong, Wai and Chen, Zhutian and Xia, Meng and Lo, Leo Yu-Ho and Yuan, Linping and Bach, Benjamin and Qu, Huamin},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Affordances, Augmented reality, Data visualization, Human computer interaction, Interaction design, Navigation, Task analysis, Three-dimensional displays, augmented reality, paper interaction, printed data visualization, tangible user interface},
	pages = {418--428},
}

@inproceedings{dogan_fabricate_2022,
	title = {Fabricate {It} or {Render} {It}?  {Digital} {Fabrication} vs. {Virtual} {Reality} for {Creating} {Objects} {Instantly}},
	url = {https://doi.org/10.1145/3491101.3516510},
	doi = {10.1145/3491101.3516510},
	abstract = {In the technical human-computer interaction (HCI) community, two research fields that gained significant popularity in the last decade are digital fabrication and augmented/virtual reality (AR/VR). Although the two fields deal with different technical challenges, both aim for a single end goal: creating "objects" instantly – either by fabricating them physically or rendering them virtually. In this panel, we will discuss the pros and cons of both approaches, discuss which one may prevail in the future, and what opportunities exist for closer collaboration between researchers from the two research fields.},
	language = {en},
	booktitle = {Extended {Abstracts} of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Baudisch, Patrick and Benko, Hrvoje and Nebeling, Michael and Peng, Huaishu and Savage, Valkyrie and Mueller, Stefanie},
	year = {2022},
	pages = {5},
}

@book{wenzel_molecular_2018,
	series = {Analytical {Sciences} {Digital} {Library}},
	title = {Molecular and {Atomic} {Spectroscopy}},
	url = {https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Molecular_and_Atomic_Spectroscopy_(Wenzel)/1%3A_General_Background_on_Molecular_Spectroscopy/1.2%3A_Beers_Law},
	language = {en},
	urldate = {2023-02-17},
	publisher = {LibreTexts Chemistry},
	author = {Wenzel, Thomas},
	month = oct,
	year = {2018},
}

@misc{noauthor_12_2018,
	title = {1.2: {Beer}’s {Law}},
	shorttitle = {1.2},
	url = {https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Molecular_and_Atomic_Spectroscopy_(Wenzel)/1%3A_General_Background_on_Molecular_Spectroscopy/1.2%3A_Beers_Law},
	language = {en},
	urldate = {2023-02-17},
	journal = {Chemistry LibreTexts},
	month = oct,
	year = {2018},
}

@misc{imagej_optical_2016,
	title = {Optical {Density} {Calibration}},
	url = {https://imagej.nih.gov/ij/docs/examples/calibration/},
	urldate = {2023-02-17},
	journal = {National Institutes of Health (NIH)},
	author = {ImageJ},
	year = {2016},
}

@article{schincariol_application_1993,
	title = {On the application of image analysis to determine concentration distributions in laboratory experiments},
	volume = {12},
	issn = {0169-7722},
	url = {https://www.sciencedirect.com/science/article/pii/016977229390007F},
	doi = {10.1016/0169-7722(93)90007-F},
	abstract = {Image analysis approaches provide an accurate and efficient way to generate detailed concentration distributions from photographic data of flow-tank experiments where the tracer can be observed through a glass or Plexiglas® wall. The technique is non-intrusive, does not disturb plume dynamics, and provides an essentially continuous distribution of sampling points over the whole plume, as observed at the tank wall. Computer processing of the scanned image is required to correct for spatial and temporal lighting nonuniformity, and for heterogeneous media, to account for different grain sizes, and thus differing dye thicknesses along tank walls. In our model study, utilizing a Plexiglas® flow tank (107 cm long, 71 cm high, 5 cm wide), individual concentrations are estimated for square areas approximately 1.3 by 1.3 mm in size over the entire area of a side wall of the tank. The negatives were scanned on an Eikonix® 78/99 digital scanning system at a resolution of 1024 by 1024 by 12 bits. The image analysis system is comprised of a Digital Equipment Corporation Microvax II® computer and a Gould® IP 9527image processor. It transforms the scanned images, whose pixels represent dye intensity, into images whose pixels represent solute concentration.},
	language = {en},
	number = {3},
	urldate = {2023-02-17},
	journal = {Journal of Contaminant Hydrology},
	author = {Schincariol, Robert A. and Herderick, Edward E. and Schwartz, Franklin W.},
	month = mar,
	year = {1993},
	pages = {197--215},
}

@book{barten_contrast_1999,
	address = {Bellingham, WA},
	edition = {1st edition},
	title = {Contrast {Sensitivity} of the {Human} {Eye} and {Its} {Effects} on {Image} {Quality}},
	isbn = {978-0-8194-3496-8},
	abstract = {Examines contrast sensitivity of the human visual system--concerning the eye's ability to distinguish objects from each other or from the background--and its effects on the image-forming process. The text provides equations for determining various aspects of contrast sensitivity, in addition to models (mathematical expressions) that can easily be adapted for practical applications.Contents - Introduction - References - Modulation threshold and noise - Model for the spatial contrast sensitivity of the eye - Extension of the contrast sensitivity model to extra-foveal vision - Extension of the contrast sensitivity model to the temporal domain - Effect of nonwhite spatial noise on contrast sensitivity - Contrast discrimination model - Image quality measure - Effect of various parameters on image quality - Epilogue - Summary - Samenvatting - Acknowledgments - Curriculum Vitae},
	language = {English},
	publisher = {SPIE Publications},
	author = {Barten, Peter G. J.},
	month = dec,
	year = {1999},
}

@article{lee_design_2020,
	title = {Design of {Imperceptible} {Metamer} {Markers} and {Application} {Systems}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9036886/},
	doi = {10.1109/ACCESS.2020.2980869},
	abstract = {This paper presents a application system using imperceptible metamer markers. The proposed system is based on metamer markers that are invisible to the human eye but visible to infrared (IR) cameras. The metamer means the indistinguishable colors with different spectrums. RGB cameras cannot capture the different spectrums of metamer markers, but IR cameras can detect the metamer markers printed under the color image. This paper describes how to create metamer markers using a ordinary color printer. First, uniformly select color samples from the RGB color space, analyze the visibility of RGB colors and metamer color printed by the printer, and then design a color mapping model between color printers to produce the effective metamer color. After conﬁrming successful detection of a metamer marker under various infrared lightings and ﬁlters, the exact color differences were analyzed and the validity of the proposed method was clariﬁed. To verify its applicability, a prototype of fairytale book was published and demonstrated as an AR-book. The metamer markers are expected to be very useful for AR applications, projector-camera systems, human-computer interaction systems, and so on.},
	language = {en},
	urldate = {2023-01-10},
	journal = {IEEE Access},
	author = {Lee, Kanghoon and Sim, Kyudong and Uhm, Taeyoung and Lee, Sang Hwa and Park, Jong-Il},
	year = {2020},
	pages = {53687--53696},
}

@inproceedings{lee_hybrid_2007,
	address = {New York, NY, USA},
	series = {{UIST} '07},
	title = {Hybrid infrared and visible light projection for location tracking},
	isbn = {978-1-59593-679-0},
	url = {https://doi.org/10.1145/1294211.1294222},
	doi = {10.1145/1294211.1294222},
	abstract = {A number of projects within the computer graphics, computer vision, and human-computer interaction communities have recognized the value of using projected structured light patterns for the purposes of doing range finding, location dependent data delivery, projector adaptation, or object discovery and tracking. However, most of the work exploring these concepts has relied on visible structured light patterns resulting in a caustic visual experience. In this work, we present the first design and implementation of a high-resolution, scalable, general purpose invisible near-infrared projector that can be manufactured in a practical manner. This approach is compatible with simultaneous visible light projection and integrates well with future Digital Light Processing (DLP) projector designs -- the most common type of projectors today. By unifying both the visible and non-visible pattern projection into a single device, we can greatly simply the implementation and execution of interactive projection systems. Additionally, we can inherently provide location discovery and tracking capabilities that are unattainable using other approaches.},
	urldate = {2023-01-10},
	booktitle = {Proceedings of the 20th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Lee, Johnny and Hudson, Scott and Dietz, Pau},
	month = oct,
	year = {2007},
	keywords = {augmented reality, infrared projection, physical interaction, projector-based tracking, simulated displays},
	pages = {57--60},
}

@article{ramalho_super_2020,
	title = {Super modules-based active {QR} codes for smart trackability and {IoT}: a responsive-banknotes case study},
	volume = {4},
	issn = {2397-4621},
	shorttitle = {Super modules-based active {QR} codes for smart trackability and {IoT}},
	url = {https://www.nature.com/articles/s41528-020-0073-1},
	doi = {10.1038/s41528-020-0073-1},
	abstract = {Abstract
            
              The general use of smartphones assigns additional relevance to QR codes as a privileged tool to the Internet of Things (IoT). Crucial for QR codes is the evolution to IoT-connected smart tags with enhanced storage capacity and secure accesses. Using the concept of
              super
              -modules (
              s
              -modules) built from adjacent spatial multiplexed modules with regular geometrical shapes, assisted by colour multiplexing, we modelled and design a single QR code with, at least, the triple storage capacity of an analogous size black/white QR code, acting as a smart-tag ensuring restrict access and trackability. The
              s
              -modules are printed using luminescent low-cost and eco-friendly inks based on organic-inorganic hybrids modified by lanthanides with multiplexed colour emission in the orthogonal RGB space. The access to the restrict information is attained only under UV irradiation and encrypted for secure transmission. The concept of active QR codes for smart trackability and IoT was materialised through the development of a free friendly-user mobile app.},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {npj Flexible Electronics},
	author = {Ramalho, João F. C. B. and Correia, Sandra F. H. and Fu, Lianshe and Dias, Lília M. S. and Adão, Pedro and Mateus, Paulo and Ferreira, Rute A. S. and André, Paulo S.},
	month = jun,
	year = {2020},
	pages = {11},
}

@inproceedings{wang_design_2008,
	title = {Design of {Halftone}-{Based} {AR} {Markers} under {Infrared} {Detection}},
	volume = {6},
	doi = {10.1109/CSSE.2008.1391},
	abstract = {As the technology of augmented reality (AR) is getting matured, the application of AR to various fields, especially in a creative manner, become an attractive research topic. The objective of this research is to propose a method for a halftone-based hidden marker for AR tracking system under infrared (IR)detection. Our design method is based on the ordered dithering and the characteristic of carbon black under IR. It provides AR system a novel solution for the marker design, which can make the human-computer interface become much more natural. The results show that the AR marker is invisible by human eyes, but appears under IR. It has many potential value-added applications for future AR systems.},
	booktitle = {2008 {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	author = {Wang, Hsi-Chun and Liu, Wen-Hsin and Chang, Chia-Long and Chen, Yi-Hui},
	month = dec,
	year = {2008},
	keywords = {Application software, Augmented reality, Computer displays, Computer graphics, Frequency modulation, Humans, Infrared detectors, Layout, Printing, Watermarking, augmented reality, digital halftoning, infrared detection, inkjet printing, watermark},
	pages = {97--100},
}

@techreport{victor_research_2014,
	title = {Research {Agenda} and {Former} {Floor} {Plan}},
	url = {http://worrydream.com/cdg/ResearchAgenda-v0.19-poster.pdf},
	language = {en},
	institution = {Communications Design Group SF},
	author = {Victor, Bret},
	month = mar,
	year = {2014},
	pages = {1},
}

@article{oguz_url_2016,
	title = {{URL} decay at year 20: {A} research note},
	volume = {67},
	issn = {23301635},
	url = {http://dx.doi.org/10.1002/asi.23561},
	doi = {10.1002/asi.23561},
	abstract = {All text is ephemeral. Some texts are more ephemeral than others. The web has proved to be among the most ephemeral and changing of information vehicles. The research note revisits Koehler's original data set after about 20 years since it was first collected. By late 2013, the number of URLs responding to a query had fallen to 1.6\% of the original sample. A query of the 6 remaining URLs in February 2015 showed only 2 still responding.},
	language = {en},
	number = {2},
	urldate = {2022-09-16},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Oguz, Fatih and Koehler, Wallace},
	month = feb,
	year = {2016},
	pages = {477--479},
}

@inproceedings{gutierrez_phara_2018,
	address = {New York, NY, USA},
	series = {{MobileHCI} '18},
	title = {{PHARA}: an augmented reality grocery store assistant},
	isbn = {978-1-4503-5941-2},
	shorttitle = {{PHARA}},
	url = {https://doi.org/10.1145/3236112.3236161},
	doi = {10.1145/3236112.3236161},
	abstract = {Staying healthy is one of the most important things in life, and our daily decisions determine how healthy or unhealthy we are. We present PHARA, an augmented reality (AR) mobile assistant that supports decision-making for food products at grocery stores. Using a user-centered design approach we investigated the possibilities of AR technology in presenting food product information. Then, following an iterative design process, we implemented a mobile AR application to support users with typical decision-making tasks that take place at grocery stores. In this paper, detailed explanations of the working prototype of PHARA and its use case are presented.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Human}-{Computer} {Interaction} with {Mobile} {Devices} and {Services} {Adjunct}},
	publisher = {Association for Computing Machinery},
	author = {Gutiérrez, Francisco and Verbert, Katrien and Htun, Nyi Nyi},
	month = sep,
	year = {2018},
	keywords = {augmented reality, decision making, grocery assistant, mobile, recommender systems},
	pages = {339--345},
}

@inproceedings{head_augmenting_2021,
	title = {Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Head, Andrew and Lo, Kyle and Kang, Dongyeop and Fok, Raymond and Skjonsberg, Sam and Weld, Daniel S and Hearst, Marti A},
	year = {2021},
	pages = {1--18},
}

@article{truong_integrating_2016,
	title = {Integrating learning styles and adaptive e-learning system: {Current} developments, problems and opportunities},
	volume = {55},
	journal = {Computers in human behavior},
	author = {Truong, Huong May},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {1185--1193},
}

@inproceedings{kang_armath_2020,
	title = {{ARMath}: augmenting everyday life with math learning},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Kang, Seokbin and Shokeen, Ekta and Byrne, Virginia L and Norooz, Leyla and Bonsignore, Elizabeth and Williams-Pierce, Caro and Froehlich, Jon E},
	year = {2020},
	pages = {1--15},
}

@inproceedings{song_penlight_2009,
	title = {{PenLight}: combining a mobile projector and a digital pen for dynamic visual overlay},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Song, Hyunyoung and Grossman, Tovi and Fitzmaurice, George and Guimbretiere, François and Khan, Azam and Attar, Ramtin and Kurtenbach, Gordon},
	year = {2009},
	pages = {143--152},
}

@inproceedings{johnson_bridging_1993,
	title = {Bridging the paper and electronic worlds: the paper user interface},
	booktitle = {Proceedings of the {INTERACT}'93 and {CHI}'93 {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Johnson, Walter and Jellinek, Herbert and Klotz Jr, Leigh and Rao, Ramana and Card, Stuart K},
	year = {1993},
	pages = {507--512},
}

@article{alessandrini_audio-augmented_2014,
	title = {Audio-augmented paper for therapy and educational intervention for children with autistic spectrum disorder},
	volume = {72},
	number = {4},
	journal = {International Journal of Human-Computer Studies},
	author = {Alessandrini, Andrea and Cappelletti, Alessandro and Zancanaro, Massimo},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {422--430},
}

@inproceedings{lee_user_2022,
	title = {User {Preference} for {Navigation} {Instructions} in {Mixed} {Reality}},
	booktitle = {2022 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} ({VR})},
	publisher = {IEEE},
	author = {Lee, Jaewook and Jin, Fanjie and Kim, Younsoo and Lindlbauer, David},
	year = {2022},
	pages = {802--811},
}

@misc{maxmax_-_llewellyn_data_processing_ir_2022,
	title = {{IR} {Ink}},
	url = {https://maxmax.com/phosphorsdyesandinks/infrared-phosphors-dyes-and-inks/infrared-down-conversion-powder/ir-ink-down-conversion},
	urldate = {2022-09-13},
	author = {MaxMax - Llewellyn Data Processing},
	year = {2022},
}

@inproceedings{mackay_missing_2002,
	title = {The missing link: augmenting biology laboratory notebooks},
	booktitle = {Proceedings of the 15th annual {ACM} symposium on {User} interface software and technology},
	author = {Mackay, Wendy E and Pothier, Guillaume and Letondal, Catherine and Bøegh, Kaare and Sørensen, Hans Erik},
	year = {2002},
	pages = {41--50},
}

@inproceedings{giraudeau_cards_2019,
	title = {{CARDS}: a mixed-reality system for collaborative learning at school},
	booktitle = {Proceedings of the 2019 {ACM} {International} {Conference} on {Interactive} {Surfaces} and {Spaces}},
	author = {Giraudeau, Philippe and Olry, Alexis and Roo, Joan Sol and Fleck, Stephanie and Bertolo, David and Vivian, Robin and Hachet, Martin},
	year = {2019},
	pages = {55--64},
}

@inproceedings{nouwens_dark_2020,
	address = {Honolulu HI USA},
	title = {Dark {Patterns} after the {GDPR}: {Scraping} {Consent} {Pop}-ups and {Demonstrating} their {Influence}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Dark {Patterns} after the {GDPR}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376321},
	doi = {10.1145/3313831.3376321},
	abstract = {New consent management platforms (CMPs) have been introduced to the web to conform with the EU’s General Data Protection Regulation, particularly its requirements for consent when companies collect and process users’ personal data. This work analyses how the most prevalent CMP designs affect people’s consent choices. We scraped the designs of the five most popular CMPs on the top 10,000 websites in the UK (n=680). We found that dark patterns and implied consent are ubiquitous; only 11.8\% meet our minimal requirements based on European law. Second, we conducted a field experiment with 40 participants to investigate how the eight most common designs affect consent choices. We found that notification style (banner or barrier) has no effect; removing the opt-out button from the first page increases consent by 22–23 percentage points; and providing more granular controls on the first page decreases consent by 8–20 percentage points. This study provides an empirical basis for the necessary regulatory action to enforce the GDPR, in particular the possibility of focusing on the centralised, third-party CMP services as an effective way to increase compliance.},
	language = {en},
	urldate = {2022-09-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Nouwens, Midas and Liccardi, Ilaria and Veale, Michael and Karger, David and Kagal, Lalana},
	month = apr,
	year = {2020},
	pages = {1--13},
}

@article{van_ooijen_does_2019,
	title = {Does the {GDPR} {Enhance} {Consumers}’ {Control} over {Personal} {Data}? {An} {Analysis} from a {Behavioural} {Perspective}},
	volume = {42},
	issn = {1573-0700},
	shorttitle = {Does the {GDPR} {Enhance} {Consumers}’ {Control} over {Personal} {Data}?},
	url = {https://doi.org/10.1007/s10603-018-9399-7},
	doi = {10.1007/s10603-018-9399-7},
	abstract = {Because of increased technological complexities and multiple data-exploiting business practices, it is hard for consumers to gain control over their own personal data. Therefore, individual control over personal data has become an important subject in European privacy law. Compared to its predecessor, the General Data Protection Regulation (GDPR) addresses the need for more individual control over personal data more explicitly. With the introduction of several new principles that seem to empower individuals in gaining more control over their data, its changes relative to its predecessors are substantial. It appears, however, that, to increase individual control, data protection law relies on certain assumptions about human decision making. In this work, we challenge these assumptions and describe the actual mechanisms of human decision making in a personal data context. Further, we analyse the extent to which new provisions in the GDPR effectively enhance individual control through a behavioural lens. To guide our analysis, we identify three stages of data processing in the data economy: (1) the information receiving stage, (2) the approval and primary use stage, and (3) the secondary use (reuse) stage. For each stage, we identify the pitfalls of human decision-making that typically emerge and form a threat to individual control. Further, we discuss how the GDPR addresses these threats by means of several legal provisions. Finally, keeping in mind the pitfalls in human decision-making, we assess how effective the new legal provisions are in enhancing individual control. We end by concluding that these legal instruments seem to have made a step towards more individual control, but some threats to individual control remain entrenched in the GDPR.},
	language = {en},
	number = {1},
	urldate = {2022-09-11},
	journal = {Journal of Consumer Policy},
	author = {van Ooijen, I. and Vrabec, Helena U.},
	month = mar,
	year = {2019},
	keywords = {Behavioural economics, Decision-making, EU data protection law, GDPR, Individual control},
	pages = {91--107},
}

@inproceedings{yadav_contrast_2014,
	title = {Contrast limited adaptive histogram equalization based enhancement for real time video system},
	doi = {10.1109/ICACCI.2014.6968381},
	abstract = {Contrast limited adaptive histogram equalization (CLAHE) is used for improve the visibility level of foggy image or video. In this paper we used CLAHE enhancement method for improving the video quality in real time system. Adaptive histogram equalization (AHE) is different from normal histogram equalization because AHE use several methods each corresponding to different parts of image and used them to redistribute the lightness value of the image and in case of CLAHE `Distribution' parameter are used to define the shape of histogram which produce the better quality result compare then adaptive histogram equalization (AHE). In this algorithm rayleigh distribution parameter are used which create bell shaped histogram. The drawback of AHE is work over homogeneous fog but CLAHE applied over both homogeneous and heterogeneous fog and single image and video system. And the second drawback of AHE is used `cumulation function' which applied over only gray level image but CLAHE used both images colored and graylevel.},
	booktitle = {2014 {International} {Conference} on {Advances} in {Computing}, {Communications} and {Informatics} ({ICACCI})},
	author = {Yadav, Garima and Maheshwari, Saurabh and Agarwal, Anjali},
	month = sep,
	year = {2014},
	keywords = {Adaptive equalizers, Histograms, Image color analysis, Image edge detection, Meteorology, Real-time systems, Streaming media, adaptive histrogram equalization, contrast enhancement, contrast limited adaptive histrogram equalization, cumulation funtion},
	pages = {2392--2397},
}

@article{pizer_adaptive_1987,
	title = {Adaptive histogram equalization and its variations},
	volume = {39},
	issn = {0734-189X},
	url = {https://www.sciencedirect.com/science/article/pii/S0734189X8780186X},
	doi = {10.1016/S0734-189X(87)80186-X},
	abstract = {Adaptive histogram equalization (ahe) is a contrast enhancement method designed to be broadly applicable and having demonstrated effectiveness. However, slow speed and the overenhancement of noise it produces in relatively homogeneous regions are two problems. We report algorithms designed to overcome these and other concerns. These algorithms include interpolated ahe, to speed up the method on general purpose computers; a version of interpolated ahe designed to run in a few seconds on feedback processors; a version of full ahe designed to run in under one second on custom VLSI hardware; weighted ahe, designed to improve the quality of the result by emphasizing pixels' contribution to the histogram in relation to their nearness to the result pixel; and clipped ahe, designed to overcome the problem of overenhancement of noise contrast. We conclude that clipped ahe should become a method of choice in medical imaging and probably also in other areas of digital imaging, and that clipped ahe can be made adequately fast to be routinely applied in the normal display sequence.},
	language = {en},
	number = {3},
	urldate = {2022-09-10},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Pizer, Stephen M. and Amburn, E. Philip and Austin, John D. and Cromartie, Robert and Geselowitz, Ari and Greer, Trey and ter Haar Romeny, Bart and Zimmerman, John B. and Zuiderveld, Karel},
	month = sep,
	year = {1987},
	pages = {355--368},
}

@techreport{fanning_preservation_2017,
	title = {Preservation with {PDF}/{A}},
	url = {http://dx.doi.org/10.7207/twr17-01},
	language = {en},
	urldate = {2022-09-08},
	institution = {Digital Preservation Coalition},
	author = {Fanning, Betsy},
	month = jul,
	year = {2017},
	doi = {10.7207/twr17-01},
	note = {Edition: Second},
}

@book{gitelman_paper_2014,
	title = {Paper knowledge: {Toward} a media history of documents},
	publisher = {Duke University Press},
	author = {Gitelman, Lisa},
	year = {2014},
}

@inproceedings{holman_paper_2005,
	title = {Paper windows: interaction techniques for digital paper},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems},
	author = {Holman, David and Vertegaal, Roel and Altosaar, Mark and Troje, Nikolaus and Johns, Derek},
	year = {2005},
	pages = {591--599},
}

@article{silapasuphakornwong_embedding_2019,
	title = {Embedding {Information} in {3D} {Printed} {Objects} {Using} {Double} {Layered} near {Infrared} {Fluorescent} {Dye}},
	volume = {7},
	issn = {17938198},
	url = {http://www.ijmmm.org/index.php?m=content&c=index&a=show&catid=67&id=566},
	doi = {10.18178/ijmmm.2019.7.6.465},
	abstract = {This paper provides a novel technique to embed high-density information in objects fabricated with a 3D printer using a near infrared fluorescent dye. Regions containing a small amount of fluorescent dye are formed inside the object as it is fabricated to embed information inside an object, and these regions form a pattern that expresses certain information. When this object is irradiated with near-infrared rays, they pass through the resin but are partly absorbed by the dye, and it emits near-infrared fluorescence. Therefore, by using a near-infrared camera, the internal pattern can be captured as a high-contrast image, and the embedded information can be nondestructively read out. This paper presents a technique of forming internal patterns at two different depths to double the amount of embedded information. We can know the depth of the patterns from the image because the profile of the brightness of the captured image of the patterns depends on its depth. Using these profiles enables doubling the amount of embedded information. Experiments we conducted demonstrate the feasibility of this technique.},
	language = {en},
	number = {6},
	urldate = {2021-11-23},
	journal = {International Journal of Materials, Mechanics and Manufacturing},
	author = {Silapasuphakornwong, Piyarat and Torii, Hideyuki and Uehira, Kazutake and Funsian, Apisara and Asawapithulsert, Kewalee and Sermpong, Tattawat},
	month = dec,
	year = {2019},
	pages = {230--234},
}

@article{willis_infrastructs_2013,
	title = {{InfraStructs}: fabricating information inside physical objects for imaging in the terahertz region},
	volume = {32},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{InfraStructs}},
	url = {https://dl.acm.org/doi/10.1145/2461912.2461936},
	doi = {10.1145/2461912.2461936},
	language = {en},
	number = {4},
	urldate = {2021-08-30},
	journal = {ACM Transactions on Graphics},
	author = {Willis, Karl D. D. and Wilson, Andrew D.},
	month = jul,
	year = {2013},
	pages = {1--10},
}

@article{maia_layercode_2019,
	title = {{LayerCode}: optical barcodes for {3D} printed shapes},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {{LayerCode}},
	url = {https://doi.org/10.1145/3306346.3322960},
	doi = {10.1145/3306346.3322960},
	abstract = {With the advance of personal and customized fabrication techniques, the capability to embed information in physical objects becomes evermore crucial. We present LayerCode, a tagging scheme that embeds a carefully designed barcode pattern in 3D printed objects as a deliberate byproduct of the 3D printing process. The LayerCode concept is inspired by the structural resemblance between the parallel black and white bars of the standard barcode and the universal layer-by-layer approach of 3D printing. We introduce an encoding algorithm that enables the 3D printing layers to carry information without altering the object geometry. We also introduce a decoding algorithm that reads the LayerCode tag of a physical object by just taking a photo. The physical deployment of LayerCode tags is realized on various types of 3D printers, including Fused Deposition Modeling printers as well as Stereolithography based printers. Each offers its own advantages and tradeoffs. We show that LayerCode tags can work on complex, nontrivial shapes, on which all previous tagging mechanisms may fail. To evaluate LayerCode thoroughly, we further stress test it with a large dataset of complex shapes using virtual rendering. Among 4,835 tested shapes, we successfully encode and decode on more than 99\% of the shapes.},
	number = {4},
	urldate = {2021-01-19},
	journal = {ACM Transactions on Graphics},
	author = {Maia, Henrique Teles and Li, Dingzeyu and Yang, Yuan and Zheng, Changxi},
	month = jul,
	year = {2019},
	keywords = {3D printing, fabrication, information embedding, physical hyperlinks},
	pages = {112:1--112:14},
}

@article{li_aircode_2017,
	title = {{AirCode}},
	url = {http://dx.doi.org/10.1145/3126594.3126635},
	doi = {10.1145/3126594.3126635},
	journal = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology - UIST '17},
	author = {Li, Dingzeyu and Nair, Avinash S and Nayar, Shree K and Zheng, Changxi},
	year = {2017},
}

@article{ungureanu_hololens_2020,
	title = {{HoloLens} 2 {Research} {Mode} as a {Tool} for {Computer} {Vision} {Research}},
	url = {http://arxiv.org/abs/2008.11239},
	abstract = {Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful sensing devices with integrated compute capabilities, which makes it an ideal platform for computer vision research. In this technical report, we present HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw sensor streams. We provide an overview of the API and explain how it can be used to build mixed reality applications based on processing sensor data. We also show how to combine the Research Mode sensor data with the built-in eye and hand tracking capabilities provided by HoloLens 2. By releasing the Research Mode API and a set of open-source tools, we aim to foster further research in the fields of computer vision as well as robotics and encourage contributions from the research community.},
	urldate = {2021-12-13},
	journal = {arXiv:2008.11239 [cs]},
	author = {Ungureanu, Dorin and Bogo, Federica and Galliani, Silvano and Sama, Pooja and Duan, Xin and Meekhof, Casey and Stühmer, Jan and Cashman, Thomas J. and Tekin, Bugra and Schönberger, Johannes L. and Olszta, Pawel and Pollefeys, Marc},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.11239},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ur_practical_2014,
	address = {New York, NY, USA},
	series = {{CHI} '14},
	title = {Practical trigger-action programming in the smart home},
	isbn = {978-1-4503-2473-1},
	url = {https://doi.org/10.1145/2556288.2557420},
	doi = {10.1145/2556288.2557420},
	abstract = {We investigate the practicality of letting average users customize smart-home devices using trigger-action ("if, then") programming. We find trigger-action programming can express most desired behaviors submitted by participants in an online study. We identify a class of triggers requiring machine learning that has received little attention. We evaluate the uniqueness of the 67,169 trigger-action programs shared on IFTTT.com, finding that real users have written a large number of unique trigger-action interactions. Finally, we conduct a 226-participant usability test of trigger-action programming, finding that inexperienced users can quickly learn to create programs containing multiple triggers or actions.},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ur, Blase and McManus, Elyse and Pak Yong Ho, Melwyn and Littman, Michael L.},
	month = apr,
	year = {2014},
	keywords = {condition-action programming, end-user programming, home automation, internet of things, smart home},
	pages = {803--812},
}

@inproceedings{ur_trigger-action_2016,
	address = {New York, NY, USA},
	series = {{CHI} '16},
	title = {Trigger-{Action} {Programming} in the {Wild}: {An} {Analysis} of 200,000 {IFTTT} {Recipes}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Trigger-{Action} {Programming} in the {Wild}},
	url = {https://doi.org/10.1145/2858036.2858556},
	doi = {10.1145/2858036.2858556},
	abstract = {While researchers have long investigated end-user programming using a trigger-action (if-then) model, the website IFTTT is among the first instances of this paradigm being used on a large scale. To understand what IFTTT users are creating, we scraped the 224,590 programs shared publicly on IFTTT as of September 2015 and are releasing this dataset to spur future research. We characterize aspects of these programs and the IFTTT ecosystem over time. We find a large number of users are crafting a diverse set of end-user programs---over 100,000 different users have shared programs. These programs represent a very broad array of connections that appear to fill gaps in functionality, yet users often duplicate others' programs.},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ur, Blase and Pak Yong Ho, Melwyn and Brawner, Stephen and Lee, Jiyun and Mennicken, Sarah and Picard, Noah and Schulze, Diane and Littman, Michael L.},
	month = may,
	year = {2016},
	keywords = {end-user composition, end-user programming, ifttt, internet of things (iot), trigger-action programming},
	pages = {3227--3231},
}

@inproceedings{jansen_share_2020,
	address = {New York, NY, USA},
	series = {{UIST} '20},
	title = {{ShARe}: {Enabling} {Co}-{Located} {Asymmetric} {Multi}-{User} {Interaction} for {Augmented} {Reality} {Head}-{Mounted} {Displays}},
	isbn = {978-1-4503-7514-6},
	shorttitle = {{ShARe}},
	url = {https://doi.org/10.1145/3379337.3415843},
	doi = {10.1145/3379337.3415843},
	abstract = {Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Jansen, Pascal and Fischbach, Fabian and Gugenheimer, Jan and Stemasov, Evgeny and Frommel, Julian and Rukzio, Enrico},
	month = oct,
	year = {2020},
	keywords = {asymmetric interaction, augmented reality, co-located, head-mounted displays, mixed reality},
	pages = {459--471},
}

@article{grubert_towards_2017,
	title = {Towards {Pervasive} {Augmented} {Reality}: {Context}-{Awareness} in {Augmented} {Reality}},
	volume = {23},
	issn = {1941-0506},
	shorttitle = {Towards {Pervasive} {Augmented} {Reality}},
	doi = {10.1109/TVCG.2016.2543720},
	abstract = {Augmented Reality is a technique that enables users to interact with their physical environment through the overlay of digital information. While being researched for decades, more recently, Augmented Reality moved out of the research labs and into the field. While most of the applications are used sporadically and for one particular task only, current and future scenarios will provide a continuous and multi-purpose user experience. Therefore, in this paper, we present the concept of Pervasive Augmented Reality, aiming to provide such an experience by sensing the user's current context and adapting the AR system based on the changing requirements and constraints. We present a taxonomy for Pervasive Augmented Reality and context-aware Augmented Reality, which classifies context sources and context targets relevant for implementing such a context-aware, continuous Augmented Reality experience. We further summarize existing approaches that contribute towards Pervasive Augmented Reality. Based our taxonomy and survey, we identify challenges for future research directions in Pervasive Augmented Reality.},
	number = {6},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Augmented reality, Context awareness, Mobile communication, Ontologies, Three-dimensional displays, Tracking, User interfaces, adaptivity, context, context-awareness, mixed reality, pervasive augmented reality, survey, taxonomy},
	pages = {1706--1724},
}

@article{lehman_hidden_2022,
	title = {Hidden in {Plain} {Sight}: {Exploring} {Privacy} {Risks} of {Mobile} {Augmented} {Reality} {Applications}},
	volume = {25},
	issn = {2471-2566},
	shorttitle = {Hidden in {Plain} {Sight}},
	url = {https://doi.org/10.1145/3524020},
	doi = {10.1145/3524020},
	abstract = {Mobile augmented reality systems are becoming increasingly common and powerful, with applications in such domains as healthcare, manufacturing, education, and more. This rise in popularity is thanks in part to the functionalities offered by commercially available vision libraries such as ARCore, Vuforia, and Google’s ML Kit; however, these libraries also give rise to the possibility of a hidden operations threat, that is, the ability of a malicious or incompetent application developer to conduct additional vision operations behind the scenes of an otherwise honest AR application without alerting the end-user. In this article, we present the privacy risks associated with the hidden operations threat and propose a framework for application development and runtime permissions targeted specifically at preventing the execution of hidden operations. We follow this with a set of experimental results, exploring the feasibility and utility of our system in differentiating between user-expectation-compliant and non-compliant AR applications during runtime testing, for which preliminary results demonstrate accuracy of up to 71\%. We conclude with a discussion of open problems in the areas of software testing and privacy standards in mobile AR systems.},
	number = {4},
	urldate = {2022-08-10},
	journal = {ACM Transactions on Privacy and Security},
	author = {Lehman, Sarah M. and Alrumayh, Abrar S. and Kolhe, Kunal and Ling, Haibin and Tan, Chiu C.},
	month = jul,
	year = {2022},
	keywords = {Augmented reality, mobile system security, user privacy},
	pages = {26:1--26:35},
}

@article{krum_augmented_2012,
	title = {Augmented reality using personal projection and retroreflection},
	volume = {16},
	issn = {1617-4909},
	url = {https://doi.org/10.1007/s00779-011-0374-4},
	doi = {10.1007/s00779-011-0374-4},
	abstract = {The support of realistic and flexible training simulations for military, law enforcement, emergency response, and other domains has been an important motivator for the development of augmented reality technology. An important vision for achieving this goal has been the creation of a versatile "stage" for physical, emotional, and cognitive training that combines virtual characters and environments with real world elements, such as furniture and props. This paper presents REFLCT, a mixed reality projection framework that couples a near-axis personal projector design with tracking and novel retroreflective props and surfaces. REFLCT provides multiple users with personalized, perspective-correct imagery that is uniquely composited for each user directly into and onto a surrounding environment, without any optics positioned in front of the user's eyes or face. These characteristics facilitate team training experiences which allow users to easily interact with their teammates while wearing their standard issue gear. REFLCT can present virtual humans who can make deictic gestures and establish eye contact without the geometric ambiguity of a typical projection display. It can also display perspective-correct scenes that require a realistic approach for detecting and communicating potential threats between multiple users in disparate locations. In addition to training applications, this display system appears to be well matched with other user interface and application domains, such as asymmetric collaborative workspaces and personal information guides.},
	number = {1},
	urldate = {2022-08-10},
	journal = {Personal and Ubiquitous Computing},
	author = {Krum, David M. and Suma, Evan A. and Bolas, Mark},
	month = jan,
	year = {2012},
	keywords = {Augmented reality, Head-mounted projection, Pico-projector, Retroreflective screens, Training},
	pages = {17--26},
}

@inproceedings{parker_design_2016,
	address = {New York, NY, USA},
	series = {{PerDis} '16},
	title = {Design implications for interacting with personalised public displays through mobile augmented reality},
	isbn = {978-1-4503-4366-4},
	url = {https://doi.org/10.1145/2914920.2915016},
	doi = {10.1145/2914920.2915016},
	abstract = {Due to their situated nature, digital public displays have the potential to provide information and messages to large groups of people. However, in practice, non-customised content typically is not relevant to every passerby, while personalising information tailored to individuals is associated with privacy concerns. Previous research has identified mobile augmented reality as a promising method for keeping digital displays public, yet enabling customised views for interested passersby in a way that retains privacy. By building on this previous work, the aim of this paper is to understand user preferences for interacting with personalised information on digital public displays through their mobile device. The paper reports on a study that was conducted with a prototype augmented reality app and public display system. Findings from the study highlight the need for contextual personalisation based upon user activities and objectives at the time of the interaction. Additionally, interactions should be adaptable between public and covert, depending upon what the user wants to share. Based on these findings the paper presents a series of design implications for personalising public displays.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the 5th {ACM} {International} {Symposium} on {Pervasive} {Displays}},
	publisher = {Association for Computing Machinery},
	author = {Parker, Callum and Kay, Judy and Baldauf, Matthias and Tomitsch, Martin},
	month = jun,
	year = {2016},
	keywords = {augmented reality, design implications, digital public displays, interaction methods, personalisation},
	pages = {52--58},
}

@inproceedings{yang_personalized_2022,
	address = {New York, NY, USA},
	series = {{WSDM} '22},
	title = {Personalized {Information} {Retrieval} for {Touristic} {Attractions} in {Augmented} {Reality}},
	isbn = {978-1-4503-9132-0},
	url = {https://doi.org/10.1145/3488560.3502194},
	doi = {10.1145/3488560.3502194},
	abstract = {The rapid advances and increasing accessibility of augmented reality (AR) in recent years opened up many new possibilities to incorporate AR into our daily lives. A very interesting area for AR is tourism where one can enhance attractions with virtual elements and provide tourists with additional information about the places they are visiting. In this paper, we present our prototype, an AR application that augments various points of interest (POIs) by showing images and facts about each POI. We also developed a simple recommender system that ensures the facts are selected based on user preferences, thus creating a unique and personalized experience for each user. Furthermore, we also conducted a live user study to assess the usability of our prototype and the usefulness of our personalization system.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Felix and Kalloori, Saikishore and Chalumattu, Ribin and Gross, Markus},
	month = feb,
	year = {2022},
	keywords = {augmented reality, personalization, tourism},
	pages = {1613--1616},
}

@inproceedings{perera_personalised_2020,
	address = {New York, NY, USA},
	series = {{ICMI} '20},
	title = {Personalised {Human} {Device} {Interaction} through {Context} aware {Augmented} {Reality}},
	isbn = {978-1-4503-7581-8},
	url = {https://doi.org/10.1145/3382507.3421157},
	doi = {10.1145/3382507.3421157},
	abstract = {Human-device interactions in smart environments are shifting prominently towards naturalistic user interactions such as gaze and gesture. However, ambiguities arise when users have to switch interactions as contexts change. This could confuse users who are accustomed to a set of conventional controls leading to system inefficiencies. My research explores how to reduce interaction ambiguity by semantically modelling user specific interactions with context, enabling personalised interactions through AR. Sensory data captured from an AR device is utilised to interpret user interactions and context which is then modeled in an extendable knowledge graph along with user's interaction preference using semantic web standards. These representations are utilized to bring semantics to AR applications about user's intent to interact with a particular device affordance. Therefore, this research aims to bring semantical modeling of personalised gesture interactions in AR/VR applications for smart/immersive environments.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the 2020 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Perera, Madhawa},
	month = oct,
	year = {2020},
	keywords = {adaptive systems, augmented reality, gaze detection, gesture interaction, semantic web, user interface},
	pages = {723--727},
}

@inproceedings{stricker_personalized_2002,
	address = {USA},
	series = {{ISWC} '02},
	title = {Personalized {Augmented} {Reality} {Touring} of {Archaeological} {Sites} with {Wearable} and {Mobile} {Computers}},
	isbn = {978-0-7695-1816-9},
	abstract = {This paper presents ARCHEOGUIDE, a novelsystem offering augmented reality tours in archaeologicalsites. The system is based on wearable and mobilecomputers, networking technology and real-time computergraphics and 3D animation and visualization techniques.The user can participate tours adapted to his profile andautomatically receive information based on his positionand orientation as calculated by a hybrid technique makinguse of GPS, compass and image-based tracking. The usercan interact with the device via multi-modal interactiontechniques and request navigation and other information.ARCHEOGUIDE has been tested at the archaeologicalsite of Olympia in Greece.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the 6th {IEEE} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {IEEE Computer Society},
	author = {Stricker, D.},
	month = oct,
	year = {2002},
	keywords = {Augmented Reality, Image Tracking, Personalized Tours, Wearable},
	pages = {15},
}

@inproceedings{ahn_personalized_2019,
	address = {New York, NY, USA},
	series = {{IoT}-{Fog} '19},
	title = {Personalized augmented reality via fog-based imitation learning},
	isbn = {978-1-4503-6698-4},
	url = {https://doi.org/10.1145/3313150.3313219},
	doi = {10.1145/3313150.3313219},
	abstract = {Augmented reality (AR) technologies are rapidly gaining momentum in society and are expected to play a critical role in the future of cities and transportation. In such dynamic settings with a heterogeneous population of AR users, it is important for holograms to be placed in the surrounding environment with regard to the users' preferences. However, the area of AR personalization remains largely unexplored. This paper proposes to use behavioral cloning, an algorithm for imitation learning, as a means of automatically generating policies that capture user preferences of hologram positioning. We argue in favor of employing the fog computing paradigm to minimize the volume of data sent to the cloud, and thereby preserve user privacy and increase both communication efficiency and learning efficiency. Through preliminary results obtained with a custom, Unity-based AR simulator, we demonstrate that user-specific policies can be learned quickly and accurately.},
	urldate = {2022-08-10},
	booktitle = {Proceedings of the {Workshop} on {Fog} {Computing} and the {IoT}},
	publisher = {Association for Computing Machinery},
	author = {Ahn, Surin and Gorlatova, Maria and Naghizadeh, Parinaz and Chiang, Mung},
	month = apr,
	year = {2019},
	keywords = {ML at the edge, augmented reality, behavioral cloning, fog computing use cases, privacy},
	pages = {11--15},
}

@inproceedings{getschmann_seedmarkers_2021,
	address = {New York, NY, USA},
	series = {{TEI} '21},
	title = {Seedmarkers: {Embeddable} {Markers} for {Physical} {Objects}},
	isbn = {978-1-4503-8213-7},
	shorttitle = {Seedmarkers},
	url = {https://doi.org/10.1145/3430524.3440645},
	doi = {10.1145/3430524.3440645},
	abstract = {We present Seedmarkers, shape-independent topological markers that can be embedded in physical objects manufactured with common rapid-prototyping techniques. Many markers are optimized for technical performance while visual appearance or the feasibility of permanently merging marker and physical object is not considered. We give an overview of the aesthetic properties of a wide range of existing markers and conducted a short online survey to assess the perception of popular marker designs. Based on our findings we introduce our generation algorithm making use of weighted Voronoi diagrams for topological optimization. With our generator, Seedmarkers can be created from technical drawings during the design process to fill arbitrary shapes on any surface. Given dimensions and manufacturing constraints, different configurations for 3 or 6 degrees of freedom tracking are possible. We propose a set of application examples for shape-independent markers, including 3D printed tangibles, laser cut plates and functional markers on printed circuit boards.},
	urldate = {2021-02-23},
	booktitle = {Proceedings of the {Fifteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Getschmann, Christopher and Echtler, Florian},
	month = feb,
	year = {2021},
	keywords = {3D printing, information embedding, pose estimation, rapid prototyping, topological markers},
	pages = {1--11},
}

@article{holmquist_tagging_2006,
	title = {Tagging the world},
	volume = {13},
	issn = {1072-5520, 1558-3449},
	url = {https://dl.acm.org/doi/10.1145/1142169.1142201},
	doi = {10.1145/1142169.1142201},
	language = {en},
	number = {4},
	urldate = {2021-09-05},
	journal = {Interactions},
	author = {Holmquist, Lars Erik},
	month = jul,
	year = {2006},
	pages = {51},
}

@misc{denso_wave_information_2022,
	title = {Information capacity and versions of {QR} {Code}},
	url = {https://www.qrcode.com/en/about/version.html},
	urldate = {2022-08-06},
	author = {Denso Wave},
	year = {2022},
}

@inproceedings{kawahara_instant_2013-2,
	address = {New York, NY, USA},
	series = {{UbiComp} '13},
	title = {Instant inkjet circuits: lab-based inkjet printing to support rapid prototyping of {UbiComp} devices},
	isbn = {978-1-4503-1770-2},
	shorttitle = {Instant inkjet circuits},
	url = {https://doi.org/10.1145/2493432.2493486},
	doi = {10.1145/2493432.2493486},
	abstract = {This paper introduces a low cost, fast and accessible technology to support the rapid prototyping of functional electronic devices. Central to this approach of 'instant inkjet circuits' is the ability to print highly conductive traces and patterns onto flexible substrates such as paper and plastic films cheaply and quickly. In addition to providing an alternative to breadboarding and conventional printed circuits, we demonstrate how this technique readily supports large area sensors and high frequency applications such as antennas. Unlike existing methods for printing conductive patterns, conductivity emerges within a few seconds without the need for special equipment. We demonstrate that this technique is feasible using commodity inkjet printers and commercially available ink, for an initial investment of around US\$300. Having presented this exciting new technology, we explain the tools and techniques we have found useful for the first time. Our main research contribution is to characterize the performance of instant inkjet circuits and illustrate a range of possibilities that are enabled by way of several example applications which we have built. We believe that this technology will be of immediate appeal to researchers in the ubiquitous computing domain, since it supports the fabrication of a variety of functional electronic device prototypes.},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 2013 {ACM} international joint conference on {Pervasive} and ubiquitous computing},
	publisher = {Association for Computing Machinery},
	author = {Kawahara, Yoshihiro and Hodges, Steve and Cook, Benjamin S. and Zhang, Cheng and Abowd, Gregory D.},
	month = sep,
	year = {2013},
	keywords = {capacitive sensors, conductive ink, digital fabrication, inkjet-printing, rapid prototyping},
	pages = {363--372},
}

@inproceedings{kim_ministudio_2016,
	address = {San Jose California USA},
	title = {{miniStudio}: {Designers}' {Tool} for {Prototyping} {Ubicomp} {Space} with {Interactive} {Miniature}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {{miniStudio}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858180},
	doi = {10.1145/2858036.2858180},
	abstract = {Recently, it has become common for designers to deal with complex and large-scale ubicomp or IoT spaces. Designers without technical implementation skills have difﬁculties in prototyping such spaces, especially in the early phases of design. We present miniStudio, a designers’ tool for prototyping ubicomp space with proxemic interactions. It is built on designers’ existing software and modeling materials (Photoshop, Lego, and paper). Interactions can be deﬁned in Photoshop based on ﬁve spatial relations: location, distance, motion, orientation, and custom. Projection-based augmented reality was applied to miniatures in order to enable tangible interactions and dynamic representations. Hidden marker stickers and a camera-projector system enable the unobtrusive integration of digital images on the physical miniature. Through the user study with 12 designers and researchers in the ubicomp ﬁeld, we found that miniStudio supported rapid prototyping of large and complex ideas with multiple connected components. Based on the tool development and the study, we discuss the implications for prototyping ubicomp environments in the early phase of the design.},
	language = {en},
	urldate = {2021-05-31},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kim, Han-Jong and Kim, Ju-Whan and Nam, Tek-Jin},
	month = may,
	year = {2016},
	pages = {213--224},
}

@inproceedings{rosner_spyn_2010,
	address = {New York, NY, USA},
	series = {{CHI} '10},
	title = {Spyn: augmenting the creative and communicative potential of craft},
	isbn = {978-1-60558-929-9},
	shorttitle = {Spyn},
	url = {https://doi.org/10.1145/1753326.1753691},
	doi = {10.1145/1753326.1753691},
	abstract = {We present data collected from a field study of 12 needle-crafters introduced to Spyn-mobile phone software that associates digital records (audio/visual media, text, and geographic data) with locations on fabric. We observed leisure needle-crafters use Spyn to create one or more handmade garments over two to four weeks and then give those garments to friends, partners, and family members. Using Spyn, creators left behind digital and physical traces that heightened recipients' appreciation for the gift and enabled a diverse set of meanings to emerge. Digital engagements with Spyn became a means for unraveling the value of the gift: recipients used digital information associated with the physical objects to interpret the story behind the objects and their creators. We discuss the nature of this relationship between digital and physical material and its implications for craft.},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rosner, Daniela K. and Ryokai, Kimiko},
	month = apr,
	year = {2010},
	keywords = {craft, creativity, crochet, design process, gift exchange, knitting, material, process, storytelling, tangibility},
	pages = {2407--2416},
}

@inproceedings{willis_hideout_2013,
	address = {New York, NY, USA},
	series = {{TEI} '13},
	title = {{HideOut}: mobile projector interaction with tangible objects and surfaces},
	isbn = {978-1-4503-1898-3},
	shorttitle = {{HideOut}},
	url = {https://doi.org/10.1145/2460625.2460682},
	doi = {10.1145/2460625.2460682},
	abstract = {HideOut is a mobile projector-based system that enables new applications and interaction techniques with tangible objects and surfaces. HideOut uses a device mounted camera to detect hidden markers applied with infrared-absorbing ink. The obtrusive appearance of fiducial markers is avoided and the hidden marker surface doubles as a functional projection surface. We present example applications that demonstrate a wide range of interaction scenarios, including media navigation tools, interactive storytelling applications, and mobile games. We explore the design space enabled by the HideOut system and describe the hidden marker prototyping process. HideOut brings tangible objects to life for interaction with the physical world around us.},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Tangible}, {Embedded} and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Willis, Karl D. D. and Shiratori, Takaaki and Mahler, Moshe},
	month = feb,
	year = {2013},
	keywords = {hidden, infrared, ink, interaction, marker, mobile, projector, tangible},
	pages = {331--338},
}

@inproceedings{lee_infrared-camera-based_2018,
	title = {Infrared-camera-based metamer marker for use in dark environments},
	doi = {10.1109/ICCE.2018.8326278},
	abstract = {We propose an invisible metamer marker that can be used in low illumination environments. The proposed marker is not visible to the eye as the color of the background is similar. In our system, the marker and the background image look differently in the infrared camera image by using different printers for each region. The proposed method is able to produce uniform quality unlike existing invisible markers. Based on these metamerism features in infrared band, markers can be successfully used even in low light and dark environments and the commercial applicability is high.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Lee, Kanghoon and Kim, Chanran and Park, Jong-Il},
	month = jan,
	year = {2018},
	note = {ISSN: 2158-4001},
	keywords = {Cameras, Image color analysis, Image recognition, Ink, Printing},
	pages = {1--3},
}

@inproceedings{lim_mobile_2016,
	title = {Mobile {Augmented} {Reality} {Based} on {Invisible} {Marker}},
	doi = {10.1109/ISMAR-Adjunct.2016.0045},
	abstract = {This paper proposes an approach for implementing marker-based augmented reality (AR) on smartphone. Specifically, to resolve the obtrusiveness of visual markers, use of infrared (IR) markers that are not visible to the human eye is studied. The main idea is to use an additional external camera with IR functionality to track IR markers that are not detectable in smartphone camera. Additionally is to put a visual fiducial object at the place where the fields of view of the external camera and the smartphone camera are overlapping, which enables the external camera to track the geometric transform between the fiducial object and the IR markers. As a result, since the fiducial object is trackable with the smartphone camera, the smartphone camera pose relative to the IR markers can be computed by using the transform. To validate the feasibility of the proposed approach, a proof-of-concept system is implemented where a visual marker is used as fiducial object for the convenience of implementation. The system accuracy mainly depends on the transform accuracy. Thus, to improve the transform accuracy, two constraints are defined and evaluated: one is that both markers lie on the same plane and the other is that the 3D marker data is available. Through experiments, with the constraints, it is verified that virtual contents can be stably augmented on IR markers in smartphone camera images.},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR}-{Adjunct})},
	author = {Lim, Changmin and Kim, Chanran and Park, Jong-II and Park, Hanhoon},
	month = sep,
	year = {2016},
	keywords = {Augmented reality, Decision support systems, external camera with IR functionality, fiducial object, invisible marker, mobile augmented reality},
	pages = {78--81},
}

@inproceedings{speicher_what_2019,
	address = {Glasgow Scotland Uk},
	title = {What is {Mixed} {Reality}?},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300767},
	doi = {10.1145/3290605.3300767},
	abstract = {What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We fnd that, while there are prominent examples, there is no universally agreed on, one-size-fts-all defnition of MR. Rather, we identifed six partially competing notions from the literature and experts’ responses. We then started to isolate the diferent aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our fndings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classifcation and discussion of MR applications’ design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape.},
	language = {en},
	urldate = {2022-08-06},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Speicher, Maximilian and Hall, Brian D. and Nebeling, Michael},
	month = may,
	year = {2019},
	pages = {1--15},
}

@article{fu_chartem_2021,
	title = {Chartem: {Reviving} {Chart} {Images} with {Data} {Embedding}},
	volume = {27},
	issn = {1077-2626},
	shorttitle = {Chartem},
	url = {https://www.computer.org/csdl/journal/tg/2021/02/09293003/1pyonCyir8k},
	doi = {10.1109/TVCG.2020.3030351},
	abstract = {In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.},
	language = {English},
	number = {02},
	urldate = {2022-08-06},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fu, Jiayun and Zhu, Bin and Cui, Weiwei and Ge, Song and Wang, Yun and Zhang, Haidong and Huang, He and Tang, Yuanyuan and Zhang, Dongmei and Ma, Xiaojing},
	month = feb,
	year = {2021},
	note = {Publisher: IEEE Computer Society},
	pages = {337--346},
}

@inproceedings{chen_augmenting_2020,
	address = {Honolulu HI USA},
	title = {Augmenting {Static} {Visualizations} with {PapARVis} {Designer}},
	isbn = {978-1-4503-6708-0},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376436},
	doi = {10.1145/3313831.3376436},
	abstract = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality. Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workﬂow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wallsized visualizations. A user study shows high user satisfaction of our environment and conﬁrms that participants can create augmented visualizations in an average of 4.63 minutes.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
	month = apr,
	year = {2020},
	pages = {1--12},
}

@inproceedings{lindlbauer_context-aware_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {Context-{Aware} {Online} {Adaptation} of {Mixed} {Reality} {Interfaces}},
	isbn = {978-1-4503-6816-2},
	url = {https://doi.org/10.1145/3332165.3347945},
	doi = {10.1145/3332165.3347945},
	abstract = {We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36\%.},
	urldate = {2022-07-16},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar},
	month = oct,
	year = {2019},
	keywords = {context-awareness, mixed reality, ui optimization},
	pages = {147--160},
}

@inproceedings{forman_defextiles_2020,
	address = {Virtual Event USA},
	title = {{DefeXtiles}: {3D} {Printing} {Quasi}-{Woven} {Fabric} via {Under}-{Extrusion}},
	isbn = {978-1-4503-7514-6},
	shorttitle = {{DefeXtiles}},
	url = {https://dl.acm.org/doi/10.1145/3379337.3415876},
	doi = {10.1145/3379337.3415876},
	abstract = {We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. In this paper, we demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Forman, Jack and Dogan, Mustafa Doga and Forsythe, Hamilton and Ishii, Hiroshi},
	month = oct,
	year = {2020},
	pages = {1222--1233},
}

@inproceedings{yamaoka_foldtronics_2019,
	address = {New York, NY, USA},
	title = {{FoldTronics}: {Creating} {3D} {Objects} with {Integrated} {Electronics} {Using} {Foldable} {Honeycomb} {Structures}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{FoldTronics}},
	url = {https://doi.org/10.1145/3290605.3300858},
	abstract = {We present FoldTronics, a 2D-cutting based fabrication technique to integrate electronics into 3D folded objects. The key idea is to cut and perforate a 2D sheet to make it foldable into a honeycomb structure using a cutting plotter; before folding the sheet into a 3D structure, users place the electronic components and circuitry onto the sheet. The fabrication process only takes a few minutes allowing to rapidly prototype functional interactive devices. The resulting objects are lightweight and rigid, thus allowing for weight-sensitive and force-sensitive applications. Finally, due to the nature of the honeycomb structure, the objects can be folded flat along one axis and thus can be efficiently transported in this compact form factor. We describe the structure of the foldable sheet, and present a design tool that enables users to quickly prototype the desired objects. We showcase a range of examples made with our design tool, including objects with integrated sensors and display elements.},
	urldate = {2021-07-12},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yamaoka, Junichi and Dogan, Mustafa Doga and Bulovic, Katarina and Saito, Kazuya and Kawahara, Yoshihiro and Kakehi, Yasuaki and Mueller, Stefanie},
	month = may,
	year = {2019},
	keywords = {2d folding, electronics, personal fabrication},
	pages = {1--14},
}

@inproceedings{dogan_g-id_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {G-{ID}: {Identifying} {3D} {Prints} {Using} {Slicing} {Parameters}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {G-{ID}},
	url = {https://doi.org/10.1145/3313831.3376202},
	doi = {10.1145/3313831.3376202},
	urldate = {2020-08-11},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Faruqi, Faraz and Churchill, Andrew Day and Friedman, Kenneth and Cheng, Leon and Subramanian, Sriram and Mueller, Stefanie},
	month = apr,
	year = {2020},
	keywords = {3d printing, identification, making, personal fabrication, tags},
	pages = {1--13},
}

@inproceedings{dogan_infraredtags_2022,
	address = {New Orleans LA USA},
	title = {{InfraredTags}: {Embedding} {Invisible} {AR} {Markers} and {Barcodes} {Using} {Low}-{Cost}, {Infrared}-{Based} {3D} {Printing} and {Imaging} {Tools}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {{InfraredTags}},
	doi = {10.1145/3491102.3501951},
	abstract = {Existing approaches for embedding unobtrusive tags inside 3D objects either require complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D codes and markers imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost infrared cameras. InfraredTags achieves this by using an infrared transmitting filament, which infrared cameras can see through, and by leaving air gaps inside for the tag’s bits, which infrared cameras capture as darker pixels in the image.},
	language = {en},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Taka, Ahmad and Lu, Michael and Zhu, Yunyi and Kumar, Akshat and Gupta, Aakar and Mueller, Stefanie},
	year = {2022},
	pages = {9},
}

@inproceedings{dogan_sensicut_2021,
	address = {Virtual Event USA},
	title = {{SensiCut}: {Material}-{Aware} {Laser} {Cutting}  {Using} {Speckle} {Sensing} and {Deep} {Learning}},
	isbn = {978-1-4503-8635-7},
	url = {https://doi.org/10.1145/3472749.3474733},
	doi = {10.1145/3472749.3474733},
	language = {en},
	booktitle = {Proceedings of the 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Dogan, Mustafa Doga and Colon, Steven Vidal Acevedo and Sinha, Varnika and Akşit, Kaan and Mueller, Stefanie},
	year = {2021},
	pages = {15},
}

@inproceedings{dogan_demonstrating_2022,
	title = {Demonstrating {InfraredTags}: {Decoding} {Invisible} {3D} {Printed} {Tags} with {Convolutional} {Neural} {Networks}},
	url = {https://doi.org/10.1145/3491101.3519905},
	doi = {10.1145/3491101.3519905},
	language = {en},
	booktitle = {Extended {Abstracts} of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Yotamornsunthorn, Veerapatr and Taka, Ahmad and Zhu, Yunyi and Gupta, Aakar and Mueller, Stefanie},
	year = {2022},
	pages = {7},
}

@inproceedings{ahuja_lightanchors_2019,
	address = {New Orleans LA USA},
	title = {{LightAnchors}: {Appropriating} {Point} {Lights} for {Spatially}-{Anchored} {Augmented} {Reality} {Interfaces}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {{LightAnchors}},
	url = {https://dl.acm.org/doi/10.1145/3332165.3347884},
	doi = {10.1145/3332165.3347884},
	abstract = {Augmented reality requires precise and instant overlay of digital information onto everyday objects. We present our work on LightAnchors, a new method for displaying spatially-anchored data. We take advantage of pervasive point lights – such as LEDs and light bulbs – for both in-view anchoring and data transmission. These lights are blinked at high speed to encode data. We built a proof-of-concept application that runs on iOS without any hardware or software modifications. We also ran a study to characterize the performance of LightAnchors and built eleven example demos to highlight the potential of our approach.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Ahuja, Karan and Pareddy, Sujeath and Xiao, Robert and Goel, Mayank and Harrison, Chris},
	month = oct,
	year = {2019},
	pages = {189--196},
}

@inproceedings{yang_infoled_2019,
	address = {New York, NY, USA},
	series = {{UIST} '19},
	title = {{InfoLED}: {Augmenting} {LED} {Indicator} {Lights} for {Device} {Positioning} and {Communication}},
	isbn = {978-1-4503-6816-2},
	shorttitle = {{InfoLED}},
	url = {https://doi.org/10.1145/3332165.3347954},
	doi = {10.1145/3332165.3347954},
	abstract = {Augmented Reality (AR) has the potential to expand our capability for interacting with and comprehending our surrounding environment. However, current AR devices treat electronic appliances no different than common non-interactive objects, which substantially limits the functionality of AR. We present InfoLED, a positioning and communication system based on indicator lights that enables appliances to transmit their location, device IDs, and status information to the AR client without changing their visual design. By leveraging human insensitivity to high-frequency brightness flickering, InfoLED transmits all of that information without disturbing the original function as an indicator light. We envision InfoLED being used in three categories of application: malfunctioning device diagnosis, appliances control, and multi-appliance configuration. We conducted three user studies, measuring the performance of the InfoLED system, the human readability of the patterns and colors displayed on the InfoLED, and users' overall preference for InfoLED. The study results showed that InfoLED can work properly from a distance of up to 7 meters in indoor conditions and it did not interfere with our participants' ability to comprehend the high-level patterns and colors of the indicator light. Overall, study subjects prefer InfoLED to an ArUco 2D barcode-based baseline system and reported less cognitive load when using our system.},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the 32nd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Jackie (Junrui) and Landay, James A.},
	month = oct,
	year = {2019},
	keywords = {augmented reality, indicator light, internet of things, select and control, smartphone, visible light communication},
	pages = {175--187},
}

@inproceedings{de_freitas_snap--it_2016,
	address = {San Jose California USA},
	title = {Snap-{To}-{It}: {A} {User}-{Inspired} {Platform} for {Opportunistic} {Device} {Interactions}},
	isbn = {978-1-4503-3362-7},
	shorttitle = {Snap-{To}-{It}},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858177},
	doi = {10.1145/2858036.2858177},
	abstract = {The ability to quickly interact with any nearby appliance from a mobile device would allow people to perform a wide range of one-time tasks (e.g., printing a document in an unfamiliar ofﬁce location). However, users currently lack this capability, and must instead manually conﬁgure their devices for each appliance they want to use. To address this problem, we created Snap-To-It, a system that allows users to opportunistically interact with any appliance simply by taking a picture of it. Snap-To-It shares the image of the appliance a user wants to interact with over a local area network. Appliances then analyze this image (along with the user’s location and device orientation) to see if they are being “selected,” and deliver the corresponding control interface to the user’s mobile device. Snap-To-It’s design was informed by two technology probes that explored how users would like to select and interact with appliances using their mobile phone. These studies highlighted the need to be able to select hardware and software via a camera, and identiﬁed several novel use cases not supported by existing systems (e.g., interacting with disconnected objects, transferring settings between appliances). In this paper, we show how Snap-To-It’s design is informed by our probes and how developers can utilize our system. We then show that Snap-To-It can identify appliances with over 95.3\% accuracy, and demonstrate through a two-month deployment that our approach is robust to gradual changes to the environment.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {de Freitas, Adrian A. and Nebeling, Michael and Chen, Xiang 'Anthony' and Yang, Junrui and Karthikeyan Ranithangam, Akshaye Shreenithi Kirupa and Dey, Anind K.},
	month = may,
	year = {2016},
	pages = {5909--5920},
}

@misc{project_link_2019,
	title = {Link {Rot}: {The} {Web} is {Decaying}},
	shorttitle = {Link {Rot}},
	url = {https://arweave.medium.com/link-rot-the-web-is-decaying-cc7d1c5ad48b},
	abstract = {This is a guest blog post from the Decentralised Public Library, an Arweave initiative.},
	language = {en},
	urldate = {2022-07-21},
	journal = {Medium},
	author = {Project, The Arweave},
	month = feb,
	year = {2019},
}

@misc{clark_new_2021,
	title = {New research shows how many important links on the web get lost to time},
	url = {https://www.theverge.com/2021/5/21/22447690/link-rot-research-new-york-times-domain-hijacking},
	abstract = {A look at just how bad link rot can be.},
	language = {en},
	urldate = {2022-07-21},
	journal = {The Verge},
	author = {Clark, Mitchell},
	month = may,
	year = {2021},
}

@article{wren_404_2004,
	title = {404 not found: the stability and persistence of {URLs} published in {MEDLINE}},
	volume = {20},
	issn = {1367-4803, 1460-2059},
	shorttitle = {404 not found},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btg465},
	doi = {10.1093/bioinformatics/btg465},
	abstract = {In this paper we present the results of a study into the persistence and availability of web resources referenced from papers in scholarly repositories. Two repositories with different characteristics, arXiv and the UNT digital library, are studied to determine if the nature of the repository, or of its content, has a bearing on the availability of the web resources cited by that content. Memento makes it possible to automate discovery of archived resources and to consider the time between the publication of the research and the archiving of the referenced URLs. This automation allows us to process more than 160000 URLs, the largest known such study, and the repository metadata allows consideration of the results by discipline. The results are startling: 45\% (66096) of the URLs referenced from arXiv still exist, but are not preserved for future generations, and 28\% of resources referenced by UNT papers have been lost. Moving forwards, we provide some initial recommendations, including that repositories should publish URL lists extracted from papers that could be used as seeds for web archiving systems.},
	language = {en},
	number = {5},
	urldate = {2022-07-21},
	journal = {Bioinformatics},
	author = {Wren, J. D.},
	month = mar,
	year = {2004},
	pages = {668--672},
}

@misc{noauthor_link_2022,
	title = {Link rot},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Link_rot&oldid=1094638819},
	abstract = {Link rot (also called link death, link breaking, or reference rot) is the phenomenon of hyperlinks tending over time to cease to point to their originally targeted file, web page, or server due to that resource being relocated to a new address or becoming permanently unavailable. A link that no longer points to its target, often called a broken or dead link (or sometimes orphan link), is a specific form of dangling pointer.
The rate of link rot is a subject of study and research due to its significance to the internet's ability to preserve information. Estimates of that rate vary dramatically between studies.},
	language = {en},
	urldate = {2022-07-21},
	journal = {Wikipedia},
	month = jun,
	year = {2022},
	note = {Page Version ID: 1094638819},
}

@inproceedings{kong_tutoriallens_2021,
	address = {Virtual Event USA},
	title = {{TutorialLens}: {Authoring} {Interactive} {Augmented} {Reality} {Tutorials} {Through} {Narration} and {Demonstration}},
	isbn = {978-1-4503-9091-0},
	shorttitle = {{TutorialLens}},
	url = {https://dl.acm.org/doi/10.1145/3485279.3485289},
	doi = {10.1145/3485279.3485289},
	abstract = {Exploring unfamiliar devices and interfaces through trial and error can be challenging and frustrating. Existing video tutorials require frequent context switching between the device showing the tutorial and the device being used. While augmented reality (AR) has been adopted to create user manuals, many are infexible for diverse tasks, and usually require programming and AR development experience. We present TutorialLens, a system for authoring interactive AR tutorials through narration and demonstration. To use TutorialLens, authors demonstrate tasks step-by-step while verbally explaining what they are doing. TutorialLens automatically detects and records 3D fnger positions and guides authors to capture important changes of the device. Using the created tutorials, TutorialLens then provides AR visual guidance and feedback for novice device users to complete the demonstrated tasks. TutorialLens is automated, friendly to users without AR development experience, and applicable to a variety of devices and tasks.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Symposium on {Spatial} {User} {Interaction}},
	publisher = {ACM},
	author = {Kong, Junhan and Sabha, Dena and Bigham, Jeffrey P and Pavel, Amy and Guo, Anhong},
	month = nov,
	year = {2021},
	pages = {1--11},
}

@inproceedings{want_bridging_1999,
	address = {Pittsburgh, Pennsylvania, United States},
	title = {Bridging physical and virtual worlds with electronic tags},
	isbn = {978-0-201-48559-2},
	url = {http://portal.acm.org/citation.cfm?doid=302979.303111},
	doi = {10.1145/302979.303111},
	abstract = {The role of computersin the modern office hasdivided our activities between virtual interactions in the realm of the computer and physical interactions with real objects within the traditional office infrastructure. This paper extends previous work that has attempted to bridge this gap, to connect physical objects with virtual representations or computational functionality, via various types of tags. We discuss a variety of scenarios we have implemented using a novel combination of inexpensive, unobtrusive and easyto useRFID tags, tag readers,portable computersand wireless networking. This novel combination demonstrates the utility of invisibly, seamlessly and portably linking physical objects to networked electronic services and actions that arenaturally associatedwith their form.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the {SIGCHI} conference on {Human} factors in computing systems the {CHI} is the limit - {CHI} '99},
	publisher = {ACM Press},
	author = {Want, Roy and Fishkin, Kenneth P. and Gujar, Anuj and Harrison, Beverly L.},
	year = {1999},
	pages = {370--377},
}

@article{de_guzman_security_2020,
	title = {Security and {Privacy} {Approaches} in {Mixed} {Reality}: {A} {Literature} {Survey}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Security and {Privacy} {Approaches} in {Mixed} {Reality}},
	url = {http://arxiv.org/abs/1802.05797},
	doi = {10.1145/3359626},
	abstract = {Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, there is only barely a few working on the privacy and security implications of this technology. This survey paper aims to put in to light these risks, and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality (AR), virtual reality (VR), and human-computer interaction (HCI) as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things (IoT). We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed.},
	number = {6},
	urldate = {2022-06-21},
	journal = {ACM Computing Surveys},
	author = {de Guzman, Jaybie A. and Thilakarathna, Kanchana and Seneviratne, Aruna},
	month = jan,
	year = {2020},
	note = {arXiv:1802.05797 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction},
	pages = {1--37},
}

@inproceedings{morris_rich_2018,
	address = {Montreal QC Canada},
	title = {Rich {Representations} of {Visual} {Content} for {Screen} {Reader} {Users}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173633},
	doi = {10.1145/3173574.3173633},
	abstract = {Alt text (short for “alternative text”) is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Morris, Meredith Ringel and Johnson, Jazette and Bennett, Cynthia L. and Cutrell, Edward},
	month = apr,
	year = {2018},
	pages = {1--11},
}

@inproceedings{greenberg_usability_2008,
	address = {Florence, Italy},
	title = {Usability evaluation considered harmful (some of the time)},
	isbn = {978-1-60558-011-1},
	url = {http://portal.acm.org/citation.cfm?doid=1357054.1357074},
	doi = {10.1145/1357054.1357074},
	abstract = {Current practice in Human Computer Interaction as encouraged by educational institutes, academic review processes, and institutions with usability groups advocate usability evaluation as a critical part of every design process. This is for good reason: usability evaluation has a significant role to play when conditions warrant it. Yet evaluation can be ineffective and even harmful if naively done ‘by rule’ rather than ‘by thought’. If done during early stage design, it can mute creative ideas that do not conform to current interface norms. If done to test radical innovations, the many interface issues that would likely arise from an immature technology can quash what could have been an inspired vision. If done to validate an academic prototype, it may incorrectly suggest a design’s scientific worthiness rather than offer a meaningful critique of how it would be adopted and used in everyday practice. If done without regard to how cultures adopt technology over time, then today's reluctant reactions by users will forestall tomorrow's eager acceptance. The choice of evaluation methodology – if any – must arise from and be appropriate for the actual problem or research question under consideration.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceeding of the twenty-sixth annual {CHI} conference on {Human} factors in computing systems  - {CHI} '08},
	publisher = {ACM Press},
	author = {Greenberg, Saul and Buxton, Bill},
	year = {2008},
	pages = {111},
}

@inproceedings{ledo_evaluation_2018,
	address = {Montreal QC Canada},
	title = {Evaluation {Strategies} for {HCI} {Toolkit} {Research}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173610},
	doi = {10.1145/3173574.3173610},
	abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what ‘evaluating’ a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
	month = apr,
	year = {2018},
	pages = {1--17},
}

@techreport{tensmeyer_pagenet_2017,
	title = {{PageNet}: {Page} {Boundary} {Extraction} in {Historical} {Handwritten} {Documents}},
	shorttitle = {{PageNet}},
	url = {http://arxiv.org/abs/1709.01618},
	abstract = {When digitizing a document into an image, it is common to include a surrounding border region to visually indicate that the entire document is present in the image. However, this border should be removed prior to automated processing. In this work, we present a deep learning based system, PageNet, which identifies the main page region in an image in order to segment content from both textual and non-textual border noise. In PageNet, a Fully Convolutional Network obtains a pixel-wise segmentation which is post-processed into the output quadrilateral region. We evaluate PageNet on 4 collections of historical handwritten documents and obtain over 94\% mean intersection over union on all datasets and approach human performance on 2 of these collections. Additionally, we show that PageNet can segment documents that are overlayed on top of other documents.},
	number = {arXiv:1709.01618},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Tensmeyer, Chris and Davis, Brian and Wigington, Curtis and Lee, Iain and Barrett, Bill},
	month = sep,
	year = {2017},
	doi = {10.48550/arXiv.1709.01618},
	note = {arXiv:1709.01618 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{conlen_idyll_2021,
	address = {Virtual Event USA},
	title = {Idyll {Studio}: {A} {Structured} {Editor} for {Authoring} {Interactive} \& {Data}-{Driven} {Articles}},
	isbn = {978-1-4503-8635-7},
	shorttitle = {Idyll {Studio}},
	url = {https://dl.acm.org/doi/10.1145/3472749.3474731},
	doi = {10.1145/3472749.3474731},
	abstract = {Interactive articles are an effective medium of communication in education, journalism, and scientific publishing, yet are created using complex general-purpose programming tools. We present Idyll Studio, a structured editor for authoring and publishing interactive and data-driven articles. We extend the Idyll framework to support reflective documents, which can inspect and modify their underlying program at runtime, and show how this functionality can be used to reify the constituent parts of a reactive document model—components, text, state, and styles—in an expressive, interoperable, and easy-to-learn graphical interface. In a study with 18 diverse participants, all could perform basic editing and composition, use datasets and variables, and specify relationships between components. Most could choreograph interactive visualizations and dynamic text, although some struggled with advanced uses requiring unstructured code editing. Our findings suggest Idyll Studio lowers the threshold for non-experts to create interactive articles and allows experts to rapidly specify a wide range of article designs.},
	language = {en},
	urldate = {2022-06-10},
	booktitle = {The 34th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Conlen, Matthew and Vo, Megan and Tan, Alan and Heer, Jeffrey},
	month = oct,
	year = {2021},
	pages = {1--12},
}

@article{hohman_communicating_2020,
	title = {Communicating with {Interactive} {Articles}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/communicating-with-interactive-articles},
	doi = {10.23915/distill.00028},
	abstract = {Examining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization.},
	language = {en},
	number = {9},
	urldate = {2022-06-10},
	journal = {Distill},
	author = {Hohman, Fred and Conlen, Matthew and Heer, Jeffrey and Chau, Duen Horng (Polo)},
	month = sep,
	year = {2020},
	pages = {e28},
}

@article{matuschak_why_2019,
	title = {Why books don't work},
	url = {https://andymatuschak.org/books},
	abstract = {Designing media to reflect how people think and learn},
	urldate = {2022-06-10},
	author = {Matuschak, Andy},
	year = {2019},
}

@inproceedings{rajaram_paper_2022,
	address = {New Orleans LA USA},
	title = {Paper {Trail}: {An} {Immersive} {Authoring} {System} for {Augmented} {Reality} {Instructional} {Experiences}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Paper {Trail}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517486},
	doi = {10.1145/3491102.3517486},
	abstract = {Prior work has demonstrated augmented reality’s benefts to education, but current tools are difcult to integrate with traditional instructional methods. We present Paper Trail, an immersive authoring system designed to explore how to enable instructors to create AR educational experiences, leaving paper at the core of the interaction and enhancing it with various forms of digital media, animations for dynamic illustrations, and clipping masks to guide learning. To inform the system design, we developed fve scenarios exploring the benefts that hand-held and head-worn AR can bring to STEM instruction and developed a design space of AR interactions enhancing paper based on these scenarios and prior work. Using the example of an AR physics handout, we assessed the system’s potential with PhD-level instructors and its usability with XR design experts. In an elicitation study with high-school teachers, we study how Paper Trail could be used and extended to enable fexible use cases across various domains. We discuss benefts of immersive paper for supporting diverse student needs and challenges for making efective use of AR for learning.},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Rajaram, Shwetha and Nebeling, Michael},
	month = apr,
	year = {2022},
	pages = {1--16},
}

@inproceedings{han_hybrid_2021,
	address = {New York, NY, USA},
	series = {{DIS} '21},
	title = {Hybrid {Paper}-{Digital} {Interfaces}: {A} {Systematic} {Literature} {Review}},
	isbn = {978-1-4503-8476-6},
	shorttitle = {Hybrid {Paper}-{Digital} {Interfaces}},
	url = {https://doi.org/10.1145/3461778.3462059},
	doi = {10.1145/3461778.3462059},
	abstract = {Past research recognized that paper has many advantages over digital devices, such as affordability, tangibility, and flexibility. Paper, however, also lacks many of the functionalities available in digital technologies, such as access to online resources and the ability to display interactive content. Prior research therefore identified opportunities for fusing the two mediums into a combined interface. This work presents a literature review on this form of innovation - technologies that bridge the paper-digital gap. First, we synthesize an understanding of paper and its relationship with digital devices through the lens of past works. Then, we outline the state-of-the-art for paper-digital interfaces and highlight possible use cases and implementation approaches. Last, we discuss design considerations and future work for developing paper-digital interfaces. Our work may be beneficial for HCI researchers interested in the development of hybrid paper-digital interfaces, and more broadly in embedding digital functionalities in everyday objects.},
	urldate = {2022-06-03},
	booktitle = {Designing {Interactive} {Systems} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Han, Feng and Cheng, Yifei and Strachan, Megan and Ma, Xiaojuan},
	month = jun,
	year = {2021},
	keywords = {interactive paper, paper computing, paper interfaces},
	pages = {1087--1100},
}

@inproceedings{li_holodoc_2019,
	address = {Glasgow Scotland Uk},
	title = {{HoloDoc}: {Enabling} {Mixed} {Reality} {Workspaces} that {Harness} {Physical} and {Digital} {Content}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{HoloDoc}},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300917},
	doi = {10.1145/3290605.3300917},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Li, Zhen and Annett, Michelle and Hinckley, Ken and Singh, Karan and Wigdor, Daniel},
	month = may,
	year = {2019},
	pages = {1--14},
}

@inproceedings{qian_dually_2022,
	address = {New Orleans LA USA},
	title = {Dually {Noted}: {Layout}-{Aware} {Annotations} with {Smartphone} {Augmented} {Reality}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Dually {Noted}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3502026},
	doi = {10.1145/3491102.3502026},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Qian, Jing and Sun, Qi and Wigington, Curtis and Han, Han L. and Sun, Tong and Healey, Jennifer and Tompkin, James and Huang, Jeff},
	month = apr,
	year = {2022},
	pages = {1--15},
}

@inproceedings{willis_sidebyside_2011,
	address = {New York, NY, USA},
	series = {{UIST} '11},
	title = {{SideBySide}: ad-hoc multi-user interaction with handheld projectors},
	isbn = {978-1-4503-0716-1},
	shorttitle = {{SideBySide}},
	url = {https://doi.org/10.1145/2047196.2047254},
	doi = {10.1145/2047196.2047254},
	abstract = {We introduce SideBySide, a system designed for ad-hoc multi-user interaction with handheld projectors. SideBySide uses device-mounted cameras and hybrid visible/infrared light projectors to track multiple independent projected images in relation to one another. This is accomplished by projecting invisible fiducial markers in the near-infrared spectrum. Our system is completely self-contained and can be deployed as a handheld device without instrumentation of the environment. We present the design and implementation of our system including a hybrid handheld projector to project visible and infrared light, and techniques for tracking projected fiducial markers that move and overlap. We introduce a range of example applications that demonstrate the applicability of our system to real-world scenarios such as mobile content exchange, gaming, and education.},
	urldate = {2022-06-03},
	booktitle = {Proceedings of the 24th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Willis, Karl D.D. and Poupyrev, Ivan and Hudson, Scott E. and Mahler, Moshe},
	month = oct,
	year = {2011},
	keywords = {ad hoc interaction, games, handheld projector, interaction techniques, multi-user, pico projector},
	pages = {431--440},
}

@article{xiao_fontcode_2018,
	title = {{FontCode}: {Embedding} {Information} in {Text} {Documents} {Using} {Glyph} {Perturbation}},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{FontCode}},
	url = {https://dl.acm.org/doi/10.1145/3152823},
	doi = {10.1145/3152823},
	abstract = {We introduce
              FontCode
              , an information embedding technique for text documents. Provided a text document with specific fonts, our method embeds user-specified information in the text by perturbing the glyphs of text characters while preserving the text content. We devise an algorithm to choose unobtrusive yet machine-recognizable glyph perturbations, leveraging a recently developed generative model that alters the glyphs of each character continuously on a font manifold. We then introduce an algorithm that embeds a user-provided message in the text document and produces an encoded document whose appearance is minimally perturbed from the original document. We also present a glyph recognition method that recovers the embedded information from an encoded document stored as a vector graphic or pixel image, or even on a printed paper. In addition, we introduce a new error-correction coding scheme that rectifies a certain number of recognition errors. Lastly, we demonstrate that our technique enables a wide array of applications, using it as a text document metadata holder, an unobtrusive optical barcode, a cryptographic message embedding scheme, and a text document signature.},
	language = {en},
	number = {2},
	urldate = {2022-06-03},
	journal = {ACM Transactions on Graphics},
	author = {Xiao, Chang and Zhang, Cheng and Zheng, Changxi},
	month = apr,
	year = {2018},
	pages = {1--16},
}

@misc{noauthor_dynamicland_nodate,
	title = {Dynamicland},
	url = {https://dynamicland.org/},
	abstract = {incubating a humane dynamic medium},
	urldate = {2022-06-03},
}

@inproceedings{suzuki_realitysketch_2020,
	address = {Virtual Event USA},
	title = {{RealitySketch}: {Embedding} {Responsive} {Graphics} and {Visualizations} in {AR} through {Dynamic} {Sketching}},
	isbn = {978-1-4503-7514-6},
	shorttitle = {{RealitySketch}},
	url = {https://dl.acm.org/doi/10.1145/3379337.3415892},
	doi = {10.1145/3379337.3415892},
	abstract = {We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, ﬂoating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-deﬁned programs and conﬁgurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.},
	language = {en},
	urldate = {2022-06-03},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Suzuki, Ryo and Kazi, Rubaiat Habib and Wei, Li-yi and DiVerdi, Stephen and Li, Wilmot and Leithinger, Daniel},
	month = oct,
	year = {2020},
	pages = {166--181},
}

@article{wang_anicode_2019,
	title = {{AniCode}: authoring coded artifacts for network-free personalized animations},
	volume = {35},
	issn = {0178-2789, 1432-2315},
	shorttitle = {{AniCode}},
	url = {http://link.springer.com/10.1007/s00371-019-01681-y},
	doi = {10.1007/s00371-019-01681-y},
	abstract = {Time-based media are used in applications ranging from demonstrating the operation of home appliances to explaining new scientiﬁc discoveries. However, creating effective time-based media is challenging. We introduce a new framework for authoring and consuming time-based media. An author encodes an animation in a printed code and afﬁxes the code to an object. A consumer captures an image of the object through a mobile application, and the image together with the code is used to generate a video on their local device. Our system is designed to be low cost and easy to use. By not requiring an Internet connection to deliver the animation, the framework enhances privacy of the communication. By requiring the user to have a direct line-of-sight view of the object, the framework provides personalized animations that only decode in the intended context. Animation schemes in the system include 2D and 3D geometric transformations, color transformation, and annotation. We demonstrate the new framework with sample applications from a wide range of domains. We evaluate the ease of use and effectiveness of our system with a user study.},
	language = {en},
	number = {6-8},
	urldate = {2022-06-02},
	journal = {The Visual Computer},
	author = {Wang, Zeyu and Qiu, Shiyu and Chen, Qingyang and Trayan, Natallia and Ringlein, Alexander and Dorsey, Julie and Rushmeier, Holly},
	month = jun,
	year = {2019},
	pages = {885--897},
}

@article{scarr_supporting_2013,
	title = {Supporting and {Exploiting} {Spatial} {Memory} in {User} {Interfaces}},
	volume = {6},
	issn = {1551-3955},
	url = {https://doi.org/10.1561/1100000046},
	doi = {10.1561/1100000046},
	abstract = {Spatial memory is an important facet of human cognition – it allows users to learn the locations of items over timeand retrieve them with little effort. In human-computer interfaces, a strong knowledge of the spatial location ofcontrols can enable a user to interact fluidly and efficiently, without needing to visually search for relevant controls. Computer interfaces should therefore be designed to provide support for developing the user's spatial memory,and they should allow the user to exploit it for rapid interaction whenever possible. However, existing systems offervarying support for spatial memory. Many modern interfaces break the user's ability to remember spatial locations, bymoving or re-arranging items; others leave spatial memory underutilised, requiring slow sequences of mechanical actionsto select items rather than exploiting users' strong ability to index items and controls by their on-screen locations.The aim of this paper is to highlight the importance of designing for spatial memory in HCI. To do this, we examine theliterature using an abstract-to-concrete approach. First, we identify important psychological models that underpin ourunderstanding of spatial memory, and differentiate between navigation and object-location memory (with this reviewfocusing on the latter). We then summarise empirical results on spatial memory from both the psychology and HCIdomains, identifying a set of observable properties of spatial memory that can be used to inform design. Finally, weanalyse existing interfaces in the HCI literature that support or disrupt spatial memory, including space-multiplexeddisplays for command and navigation interfaces, different techniques for dealing with large spatial data sets, and theeffects of spatial distortion. We intend for this paper to be useful to user interface designers, as well as other HCIresearchers interested in spatial memory. Throughout the text, we therefore emphasise important design guidelinesderived from the work reviewed, as well as methodological issues and topics for future research.},
	number = {1},
	urldate = {2022-04-24},
	journal = {Foundations and Trends in Human-Computer Interaction},
	author = {Scarr, Joey and Cockburn, Andy and Gutwin, Carl},
	month = dec,
	year = {2013},
	pages = {1--84},
}

@inproceedings{tancik_stegastamp_2020,
	address = {Seattle, WA, USA},
	title = {{StegaStamp}: {Invisible} {Hyperlinks} in {Physical} {Photographs}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{StegaStamp}},
	url = {https://ieeexplore.ieee.org/document/9156548/},
	doi = {10.1109/CVPR42600.2020.00219},
	abstract = {Printed and digitally displayed photos have the ability to hide imperceptible digital data that can be accessed through internet-connected imaging systems. Another way to think about this is physical photographs that have unique QR codes invisibly embedded within them. This paper presents an architecture, algorithms, and a prototype implementation addressing this vision. Our key technical contribution is StegaStamp, a learned steganographic algorithm to enable robust encoding and decoding of arbitrary hyperlink bitstrings into photos in a manner that approaches perceptual invisibility. StegaStamp comprises a deep neural network that learns an encoding/decoding algorithm robust to image perturbations approximating the space of distortions resulting from real printing and photography. We demonstrates real-time decoding of hyperlinks in photos from in-the-wild videos that contain variation in lighting, shadows, perspective, occlusion and viewing distance. Our prototype system robustly retrieves 56 bit hyperlinks after error correction –sufﬁcient to embed a unique code within every photo on the internet.},
	language = {en},
	urldate = {2022-04-25},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tancik, Matthew and Mildenhall, Ben and Ng, Ren},
	month = jun,
	year = {2020},
	pages = {2114--2123},
}

@inproceedings{li_editing_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Editing {Spatial} {Layouts} through {Tactile} {Templates} for {People} with {Visual} {Impairments}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300436},
	doi = {10.1145/3290605.3300436},
	abstract = {Spatial layout is a key component in graphic design. While people who are blind or visually impaired (BVI) can use screen readers or magnifiers to access digital content, these tools fail to fully communicate the content's graphic design information. Through semi-structured interviews and contextual inquiries, we identify the lack of this information and feedback as major challenges in understanding and editing layouts. Guided by these insights and a co-design process with a blind hobbyist web developer, we developed an interactive, multimodal authoring tool that lets blind people understand spatial relationships between elements and modify layout templates. Our tool automatically generates tactile print-outs of a web page's layout, which users overlay on top of a tablet that runs our self-voicing digital design tool. We conclude with design considerations grounded in user feedback for improving the accessibility of spatially encoded information and developing tools for BVI authors.},
	urldate = {2022-04-24},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Li, Jingyi and Kim, Son and Miele, Joshua A. and Agrawala, Maneesh and Follmer, Sean},
	month = may,
	year = {2019},
	keywords = {accessibility, accessible design tools, accessible web design, blindness, layout design, multimodal interfaces, tactile overlays, templates, visual impairments},
	pages = {1--11},
}



@article{Benford_Decoration_2017,
author = {Benford, Steve and Koleva, Boriana and Quinn, Anthony and Thorn, Emily-Clare and Glover, Kevin and Preston, William and Hazzard, Adrian and Rennick-Egglestone, Stefan and Greenhalgh, Chris and Mortier, Richard},
title = {Crafting Interactive Decoration},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3058552},
doi = {10.1145/3058552},
abstract = {We explore the crafting of interactive decoration for everyday artefacts. This involves adorning them with decorative patterns that enhance their beauty while triggering digital interactions when scanned with cameras. These are realized using an existing augmented reality technique that embeds computer readable codes into the topological structures of hand-drawn patterns. We describe a research through design process that engaged artisans to craft a portfolio of interactive artefacts, including ceramic bowls, embroidered gift cards, fabric souvenirs, and an acoustic guitar. We annotate this portfolio with reflections on the crafting process, revealing how artisans addressed pattern, materials, form and function, and digital mappings throughout their craft process. Further reflection on our portfolio reveals how they bridged between human and system perceptions of visual patterns and engaged in a deep embedding of digital interactions into physical materials. Our findings demonstrate the potential for interactive decoration, distilling the craft knowledge involved in creating aesthetic and functional decoration, highlight the need for transparent computer vision technologies, and raise wider issues for HCI’s growing engagement with craft.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {aug},
articleno = {26},
numpages = {39},
keywords = {wood, tangible and embedded interfaces, tangible, sustainability, seamful design, obsolescence, material, maker, lifespan, hybrid-craft, fabric, embedded, computer vision, augmented reality, ambiguity, DIY, Craft}
}

@inproceedings{Enrico_visualMarkers_2009,
author = {Costanza, Enrico and Huang, Jeffrey},
title = {Designable Visual Markers},
year = {2009},
isbn = {9781605582467},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1518701.1518990},
doi = {10.1145/1518701.1518990},
pages = {1879–1888},
numpages = {10},
keywords = {UI toolkits, fiducial recognition, mobile HCI, mobile devices, user studies, visual marker design, visual marker recognition},
location = {Boston, MA, USA},
series = {CHI '09}
}

@inproceedings{Preston_MarkerScale_2017,
author = {Preston, William and Benford, Steve and Thorn, Emily-Clare and Koleva, Boriana and Rennick-Egglestone, Stefan and Mortier, Richard and Quinn, Anthony and Stell, John and Worboys, Michael},
title = {Enabling Hand-Crafted Visual Markers at Scale},
year = {2017},
isbn = {9781450349222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064663.3064746},
doi = {10.1145/3064663.3064746},
abstract = {As locative media and augmented reality spread into the everyday world so it becomes important to create aesthetic visual markers at scale. We explore a designer-centred approach in which skilled designers handcraft seed designs that are automatically recombined to create many markers as subtle variants of a common theme. First, we extend the d-touch topological approach to creating visual markers that has previously been shown to support creative design with two new techniques: area order codes and visual checksums. We then show how the topological structure of such markers provides the basis for recombining designs to generate many variations. We demonstrate our approach through the creation of beautiful, personalized and interactive wallpaper. We reflect on how technologies must enable designers to balance goals of scalability, aesthetics and reliability in creating beautiful interactive decoration.},
booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
pages = {1227–1237},
numpages = {11},
keywords = {visual markers, topological markers, patterns, image recognition, fiducial markers, computer vision},
location = {Edinburgh, United Kingdom},
series = {DIS '17}
}

@inproceedings{Jung_HumanMarker_2019,
author = {Jung, Joshua D.A. and Iyer, Rahul N. and Vogel, Daniel},
title = {Automating the Intentional Encoding of Human-Designable Markers},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300417},
doi = {10.1145/3290605.3300417},
abstract = {Recent work established that it is possible for human artists to encode information into hand-drawn markers, but it is difficult to do when simultaneously maintaining aesthetic quality. We present two methods for relieving the mental burden associated with encoding, while allowing an artist to draw as freely as possible. A 'Helper Overlay' guides the artist with real-time feedback indicating where visual features should be added or removed, and an 'Autocomplete Tool' directly adds necessary features to the drawing for the artist to touch up. Both methods are enabled by a two-part algorithm that uses a tree-search for finding 'major' changes and a dynamic programming method for finding the minimum number of 'minor' changes. A 24-person study demonstrates that a majority of participants prefer both tools over previous methods of manual encoding, with the Helper Overlay being the more popular of the two.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {user study, fiducial marker, encoding, designable marker},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{Jung_HumanDesignable_2019,
author = {Jung, Joshua D.A. and Iyer, Rahul N. and Vogel, Daniel},
title = {Automating the Intentional Encoding of Human-Designable Markers},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300417},
doi = {10.1145/3290605.3300417},
abstract = {Recent work established that it is possible for human artists to encode information into hand-drawn markers, but it is difficult to do when simultaneously maintaining aesthetic quality. We present two methods for relieving the mental burden associated with encoding, while allowing an artist to draw as freely as possible. A 'Helper Overlay' guides the artist with real-time feedback indicating where visual features should be added or removed, and an 'Autocomplete Tool' directly adds necessary features to the drawing for the artist to touch up. Both methods are enabled by a two-part algorithm that uses a tree-search for finding 'major' changes and a dynamic programming method for finding the minimum number of 'minor' changes. A 24-person study demonstrates that a majority of participants prefer both tools over previous methods of manual encoding, with the Helper Overlay being the more popular of the two.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {designable marker, encoding, fiducial marker, user study},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}


@article{ng2018treasure,
  title={Treasure Codes: Augmenting Learning from Physical Museum Exhibits through Treasure Hunting},
  author={Ng, Kher Hui and Huang, Hai and O’malley, Claire},
  journal={Personal and Ubiquitous Computing},
  volume={22},
  pages={739--750},
  year={2018},
  publisher={Springer}
}

@ARTICLE{wagner2010RealTimeDetection,
  author={Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Real-Time Detection and Tracking for Augmented Reality on Mobile Phones}, 
  year={2010},
  volume={16},
  number={3},
  pages={355-368},
  keywords={Augmented reality;Mobile handsets;Target tracking;Personal communication networks;Robustness;Application software;Real time systems;Computer Society;Multimedia systems;Information systems;Information interfaces and presentation;multimedia information systems;artificial;augmented;and virtual realities;image processing and computer vision;scene analysis;tracking.},
  doi={10.1109/TVCG.2009.99}}


@INPROCEEDINGS{Wagner2008PoseTracking,
  author={Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
  booktitle={2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality}, 
  title={Pose Tracking from Natural Features on Mobile Phones}, 
  year={2008},
  volume={},
  number={},
  pages={125-134},
  keywords={Target tracking;Feature extraction;Mobile handsets;Computational modeling;Detectors;Cameras;Real time systems;pose tracking;natural features;mobile phones;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking},
  doi={10.1109/ISMAR.2008.4637338}}


@article{garrido2014automatic,
  title={Automatic Generation and Detection of Highly Reliable Fiducial Markers under Occlusion},
  author={Garrido-Jurado, Sergio and Mu{\~n}oz-Salinas, Rafael and Madrid-Cuevas, Francisco Jos{\'e} and Mar{\'\i}n-Jim{\'e}nez, Manuel Jes{\'u}s},
  journal={Pattern Recognition},
  volume={47},
  number={6},
  pages={2280--2292},
  year={2014},
  publisher={Elsevier}
}


@inproceedings{kato1999marker,
  title={Marker Tracking and HMD Calibration for a Video-based Augmented Reality Conferencing System},
  author={Kato, Hirokazu and Billinghurst, Mark},
  booktitle={Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
  pages={85--94},
  year={1999},
  organization={IEEE}
}

@inproceedings{Pourjafarian2022Print,
author = {Pourjafarian, Narjes and Koelle, Marion and Mjaku, Fjolla and Strohmeier, Paul and Steimle, J\"{u}rgen},
title = {Print-A-Sketch: A Handheld Printer for Physical Sketching of Circuits and Sensors on Everyday Surfaces},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502074},
doi = {10.1145/3491102.3502074},
abstract = {We present Print-A-Sketch, an open-source handheld printer prototype for sketching circuits and sensors. Print-A-Sketch combines desirable properties from free-hand sketching and functional electronic printing. Manual human control of large strokes is augmented with computer control of fine detail. Shared control of Print-A-Sketch supports sketching interactive interfaces on everyday objects – including many objects with materials or sizes which otherwise are difficult to print on. We present an overview of challenges involved in such a system and show how these can be addressed using context-aware, dynamic printing. Continuous sensing ensures quality prints by adjusting inking-rate to hand movement and material properties. Continuous sensing also enables the print to adapt to previously printed traces to support incremental and iterative sketching. Results show good conductivity on many materials and high spatial precision, supporting on-the-fly creation of functional interfaces.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {270},
numpages = {17},
keywords = {Fabrication, conductive inkjet printing, new materials, printed electronics, prototyping, sketching interfaces},
location = {New Orleans, LA, USA},
series = {CHI '22}
}




@article{sauvola_adaptive_2000,
	title = {Adaptive document image binarization},
	volume = {33},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320399000552},
	doi = {10.1016/S0031-3203(99)00055-2},
	abstract = {A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.},
	number = {2},
	urldate = {2024-09-11},
	journal = {Pattern Recognition},
	author = {Sauvola, J. and Pietikäinen, M.},
	month = feb,
	year = {2000},
	keywords = {Adaptive binarization, Document analysis, Document segmentation, Document understanding, Soft decision},
	pages = {225--236},
	file = {ScienceDirect Snapshot:/Users/doga/Zotero/storage/8V3CGUGD/S0031320399000552.html:text/html},
}

@incollection{zuiderveld_contrast_1994,
	address = {USA},
	title = {Contrast limited adaptive histogram equalization},
	isbn = {978-0-12-336155-4},
	urldate = {2024-09-11},
	booktitle = {Graphics gems {IV}},
	publisher = {Academic Press Professional, Inc.},
	author = {Zuiderveld, Karel},
	month = aug,
	year = {1994},
	pages = {474--485},
}


@INPROCEEDINGS{Kakehi2006Transparent,
  author={Kakehi, Y. and Hosomi, T. and Iida, M. and Naemura, T. and Matsushita, M.},
  booktitle={First IEEE International Workshop on Horizontal Interactive Human-Computer Systems (TABLETOP '06)}, 
  title={Transparent Tabletop Interface for Multiple Users on Lumisight Table}, 
  year={2006},
  volume={},
  number={},
  pages={8 pp.-},
  keywords={Computer displays;Cameras;Embedded computing;Interactive systems;Humans;Collaborative work;Conferences;Computer vision;Control systems;Joining processes},
  doi={10.1109/TABLETOP.2006.34}}

@inproceedings{Ledo2018Evaluation,
author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
title = {Evaluation Strategies for HCI Toolkit Research},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173610},
doi = {10.1145/3173574.3173610},
abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {design, evaluation, prototyping, toolkits, user interfaces},
location = {Montreal QC, Canada},
series = {CHI '18}
}



@inproceedings{yamaoka_foldtronics_2019,
	address = {New York, NY, USA},
	title = {{FoldTronics}: {Creating} {3D} {Objects} with {Integrated} {Electronics} {Using} {Foldable} {Honeycomb} {Structures}},
	isbn = {978-1-4503-5970-2},
	shorttitle = {{FoldTronics}},
	url = {https://doi.org/10.1145/3290605.3300858},
	urldate = {2021-07-12},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yamaoka, Junichi and Dogan, Mustafa Doga and Bulovic, Katarina and Saito, Kazuya and Kawahara, Yoshihiro and Kakehi, Yasuaki and Mueller, Stefanie},
	month = may,
	year = {2019},
	keywords = {2d folding, electronics, personal fabrication},
	pages = {1--14},
}


@inproceedings{ozdemir_speed-modulated_2024,
	address = {New York, NY, USA},
	series = {{UIST} '24},
	title = {Speed-{Modulated} {Ironing}: {High}-{Resolution} {Shade} and {Texture} {Gradients} in {Single}-{Material} {3D} {Printing}},
	isbn = {9798400706288},
	shorttitle = {Speed-{Modulated} {Ironing}},
	url = {https://dl.acm.org/doi/10.1145/3654777.3676456},
	doi = {10.1145/3654777.3676456},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Ozdemir, Mehmet and AlAlawi, Marwa and Dogan, Mustafa Doga and Martinez Castro, Jose Francisco and Mueller, Stefanie and Doubrovski, Zjenja},
	month = oct,
	year = {2024},
	pages = {1--13},
	file = {Full Text PDF:/Users/doga/Zotero/storage/SSUKZ7TW/Ozdemir et al. - 2024 - Speed-Modulated Ironing High-Resolution Shade and.pdf:application/pdf},
}

@inproceedings{dogan_augmented_2024,
	address = {New York, NY, USA},
	series = {{UIST} '24},
	title = {Augmented {Object} {Intelligence} with {XR}-{Objects}},
	isbn = {9798400706288},
	url = {https://dl.acm.org/doi/10.1145/3654777.3676379},
	doi = {10.1145/3654777.3676379},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Gonzalez, Eric J and Ahuja, Karan and Du, Ruofei and Colaço, Andrea and Lee, Johnny and Gonzalez-Franco, Mar and Kim, David},
	month = oct,
	year = {2024},
	pages = {1--15},
	file = {Full Text PDF:/Users/doga/Zotero/storage/H4AH9BV6/Dogan et al. - 2024 - Augmented Object Intelligence with XR-Objects.pdf:application/pdf},
}

@inproceedings{campos_zamora_moirewidgets_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {{MoiréWidgets}: {High}-{Precision}, {Passive} {Tangible} {Interfaces} via {Moiré} {Effect}},
	isbn = {9798400703300},
	shorttitle = {{MoiréWidgets}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642734},
	doi = {10.1145/3613904.3642734},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Campos Zamora, Daniel and Dogan, Mustafa Doga and Siu, Alexa F and Koh, Eunyee and Xiao, Chang},
	month = may,
	year = {2024},
	pages = {1--10},
	file = {Full Text PDF:/Users/doga/Zotero/storage/SBUE9623/Campos Zamora et al. - 2024 - MoiréWidgets High-Precision, Passive Tangible Int.pdf:application/pdf},
}

@inproceedings{dogan_structcode_2023,
	address = {New York, NY, USA},
	series = {{SCF} '23},
	title = {{StructCode}: {Leveraging} {Fabrication} {Artifacts} to {Store} {Data} in {Laser}-{Cut} {Objects}},
	isbn = {9798400703195},
	shorttitle = {{StructCode}},
	url = {https://dl.acm.org/doi/10.1145/3623263.3623353},
	doi = {10.1145/3623263.3623353},
	urldate = {2025-01-21},
	booktitle = {Proceedings of the 8th {ACM} {Symposium} on {Computational} {Fabrication}},
	publisher = {Association for Computing Machinery},
	author = {Dogan, Mustafa Doga and Chan, Vivian Hsinyueh and Qi, Richard and Tang, Grace and Roumen, Thijs and Mueller, Stefanie},
	month = nov,
	year = {2023},
	pages = {1--13},
	file = {Full Text PDF:/Users/doga/Zotero/storage/LACYFULD/Dogan et al. - 2023 - StructCode Leveraging Fabrication Artifacts to St.pdf:application/pdf},
}

@misc{dogangun_rampa_2024,
	title = {{RAMPA}: {Robotic} {Augmented} {Reality} for {Machine} {Programming} and {Automation}},
	shorttitle = {{RAMPA}},
	url = {http://arxiv.org/abs/2410.13412},
	doi = {10.48550/arXiv.2410.13412},
	abstract = {As robotics continue to enter various sectors beyond traditional industrial applications, the need for intuitive robot training and interaction systems becomes increasingly more important. This paper introduces Robotic Augmented Reality for Machine Programming (RAMPA), a system that utilizes the capabilities of state-of-the-art and commercially available AR headsets, e.g., Meta Quest 3, to facilitate the application of Programming from Demonstration (PfD) approaches on industrial robotic arms, such as Universal Robots UR10. Our approach enables in-situ data recording, visualization, and fine-tuning of skill demonstrations directly within the user's physical environment. RAMPA addresses critical challenges of PfD, such as safety concerns, programming barriers, and the inefficiency of collecting demonstrations on the actual hardware. The performance of our system is evaluated against the traditional method of kinesthetic control in teaching three different robotic manipulation tasks and analyzed with quantitative metrics, measuring task performance and completion time, trajectory smoothness, system usability, user experience, and task load using standardized surveys. Our findings indicate a substantial advancement in how robotic tasks are taught and refined, promising improvements in operational safety, efficiency, and user engagement in robotic programming.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Dogangun, Fatih and Bahar, Serdar and Yildirim, Yigit and Temir, Bora Toprak and Ugur, Emre and Dogan, Mustafa Doga},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13412 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/doga/Zotero/storage/MQSXXQCJ/Dogangun et al. - 2024 - RAMPA Robotic Augmented Reality for Machine Progr.pdf:application/pdf;Snapshot:/Users/doga/Zotero/storage/PQK3CTP2/2410.html:text/html},
}

@inproceedings{iyer_xr-penter_2025,
	address = {New York, NY, USA},
	series = {{CHI} '25},
	title = {{XR}-penter: {Material}-{Aware} and {In} {Situ} {Design} of {Scrap} {Wood} {Assemblies}},
	language = {en},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Iyer, Ramya and Dogan, Mustafa Doga and Larsson, Maria and Igarashi, Takeo},
	month = apr,
	year = {2025},
	file = {Iyer et al. - XR-penter Material-Aware and In Situ Design of Sc.pdf:/Users/doga/Zotero/storage/7HNP7MI4/Iyer et al. - XR-penter Material-Aware and In Situ Design of Sc.pdf:application/pdf},
}

@inproceedings{gui_draw2cut_2025,
	address = {New York, NY, USA},
	title = {{Draw2Cut}: {Direct} {On}-{Material} {Annotations} for {CNC} {Milling}},
	language = {en},
	booktitle = {Proceedings of the 2025 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gui, Xinyue and Xia, Ding and Gao, Wang and Dogan, Mustafa Doga and Larsson, Maria and Igarashi, Takeo},
	month = apr,
	year = {2025},
	file = {Gui et al. - 2025 - Draw2Cut Direct On-Material Annotations for CNC M.pdf:/Users/doga/Zotero/storage/YC45QYFV/Gui et al. - 2025 - Draw2Cut Direct On-Material Annotations for CNC M.pdf:application/pdf},
}

@inproceedings{chen_spectrack_2024,
	address = {New York, NY, USA},
	series = {{SA} '24},
	title = {{SpecTrack}: {Learned} {Multi}-{Rotation} {Tracking} via {Speckle} {Imaging}},
	isbn = {9798400711381},
	shorttitle = {{SpecTrack}},
	url = {https://doi.org/10.1145/3681756.3697875},
	doi = {10.1145/3681756.3697875},
	abstract = {Precision pose detection is increasingly demanded in fields such as personal fabrication, Virtual Reality (VR), and robotics due to its critical role in ensuring accurate positioning information. However, conventional vision-based systems used in these systems often struggle with achieving high precision and accuracy, particularly when dealing with complex environments or fast-moving objects. To address these limitations, we investigate Laser Speckle Imaging (LSI), an emerging optical tracking method that offers promising potential for improving pose estimation accuracy. Specifically, our proposed LSI-Based Tracking (SpecTrack) leverages the captures from a lensless camera and a retro-reflector marker with a coded aperture to achieve multi-axis rotational pose estimation with high precision. Our extensive trials using our in-house built testbed have shown that SpecTrack achieves an accuracy and of 0.31° (std = 0.43°), significantly outperforming state-of-the-art approaches and improving accuracy up to  {\textbackslash}(200{\textbackslash}\%{\textbackslash}) .},
	urldate = {2025-01-21},
	booktitle = {{SIGGRAPH} {Asia} 2024 {Posters}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Ziyang and Dogan, Mustafa and Spjut, Josef and Akşit, Kaan},
	month = dec,
	year = {2024},
	pages = {1--2},
	file = {Submitted Version:/Users/doga/Zotero/storage/59G4F3CM/Chen et al. - 2024 - SpecTrack Learned Multi-Rotation Tracking via Spe.pdf:application/pdf},
}

@misc{arslan_realitycraft_2024,
	title = {{RealityCraft}: {An} {In}-{Situ} {CAD}+{CAM} {Interface} for {Novices} via {Scene}-{Aware} {Augmented} {Reality}},
	shorttitle = {{RealityCraft}},
	url = {http://arxiv.org/abs/2410.06113},
	doi = {10.48550/arXiv.2410.06113},
	abstract = {Despite the growing accessibility of augmented reality (AR) for visualization, existing computer-aided design systems remain largely confined to traditional screens and are often inaccessible to novice users due to their complexity. We present RealityCraft, an open-sourced AR interface that enables in-situ computer-aided design and manufacturing (CAD+CAM) for novices. Unlike traditional CAD systems confined to computer screens, RealityCraft allows users to design directly within their physical environments, with primitive geometries. RealityCraft recognizes and utilizes physical constraints such as furniture and walls, enhancing user interaction through spatial awareness and depth occlusion. Furthermore, RealityCraft features an integrated AR-based 3D printing workflow, where users can drag and drop designs onto their 3D printer's virtual twin in their immediate space. Through a user study, we demonstrate that RealityCraft enhances engagement and ease of use for novices. By bridging the gap between digital creation and physical output, RealityCraft aims to transform everyday spaces into creative studios.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Arslan, Oğuz and Akdoğan, Artun and Dogan, Mustafa Doga},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06113 [cs]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Graphics, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/doga/Zotero/storage/DLT7PGKQ/Arslan et al. - 2024 - RealityCraft An In-Situ CAD+CAM Interface for Nov.pdf:application/pdf;Snapshot:/Users/doga/Zotero/storage/TIB5PXZH/2410.html:text/html},
}

@misc{dogan_ubiquitous_2024,
	title = {Ubiquitous {Metadata}: {Design} and {Fabrication} of {Embedded} {Markers} for {Real}-{World} {Object} {Identification} and {Interaction}},
	shorttitle = {Ubiquitous {Metadata}},
	url = {http://arxiv.org/abs/2407.11748},
	doi = {10.48550/arXiv.2407.11748},
	abstract = {The convergence of the physical and digital realms has ushered in a new era of immersive experiences and seamless interactions. As the boundaries between the real world and virtual environments blur and result in a "mixed reality," there arises a need for robust and efficient methods to connect physical objects with their virtual counterparts. In this thesis, we present a novel approach to bridging this gap through the design, fabrication, and detection of embedded machine-readable markers. We categorize the proposed marking approaches into three distinct categories: natural markers, structural markers, and internal markers. Natural markers, such as those used in SensiCut, are inherent fingerprints of objects repurposed as machine-readable identifiers, while structural markers, such as StructCode and G-ID, leverage the structural artifacts in objects that emerge during the fabrication process itself. Internal markers, such as InfraredTag and BrightMarker, are embedded inside fabricated objects using specialized materials. Leveraging a combination of methods from computer vision, machine learning, computational imaging, and material science, the presented approaches offer robust and versatile solutions for object identification, tracking, and interaction. These markers, seamlessly integrated into real-world objects, effectively communicate an object's identity, origin, function, and interaction, functioning as gateways to "ubiquitous metadata" - a concept where metadata is embedded into physical objects, similar to metadata in digital files. Across the different chapters, we demonstrate the applications of the presented methods in diverse domains, including product design, manufacturing, retail, logistics, education, entertainment, security, and sustainability.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Dogan, Mustafa Doga},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11748 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Emerging Technologies, Computer Science - Graphics, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/Users/doga/Zotero/storage/9C5SV3SP/Dogan - 2024 - Ubiquitous Metadata Design and Fabrication of Emb.pdf:application/pdf;Snapshot:/Users/doga/Zotero/storage/VIMR6B9A/2407.html:text/html},
}


@article{xu_art-up_2021,
	title = {{ART}-{UP}: {A} {Novel} {Method} for {Generating} {Scanning}-{Robust} {Aesthetic} {QR} {Codes}},
	volume = {17},
	issn = {1551-6857},
	shorttitle = {{ART}-{UP}},
	url = {https://dl.acm.org/doi/10.1145/3418214},
	doi = {10.1145/3418214},
	abstract = {Quick response (QR) codes are usually scanned in different environments, so they must be robust to variations in illumination, scale, coverage, and camera angles. Aesthetic QR codes improve the visual quality, but subtle changes in their appearance may cause scanning failure. In this article, a new method to generate scanning-robust aesthetic QR codes is proposed, which is based on a module-based scanning probability estimation model that can effectively balance the tradeoff between visual quality and scanning robustness. Our method locally adjusts the luminance of each module by estimating the probability of successful sampling. The approach adopts the hierarchical, coarse-to-fine strategy to enhance the visual quality of aesthetic QR codes, which sequentially generate the following three codes: a binary aesthetic QR code, a grayscale aesthetic QR code, and the final color aesthetic QR code. Our approach also can be used to create QR codes with different visual styles by adjusting some initialization parameters. User surveys and decoding experiments were adopted for evaluating our method compared with state-of-the-art algorithms, which indicates that the proposed approach has excellent performance in terms of both visual quality and scanning robustness.},
	number = {1},
	urldate = {2025-02-07},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Xu, Mingliang and Li, Qingfeng and Niu, Jianwei and Su, Hao and Liu, Xiting and Xu, Weiwei and Lv, Pei and Zhou, Bing and Yang, Yi},
	month = apr,
	year = {2021},
	pages = {25:1--25:23},
	file = {Full Text PDF:/Users/mdogan/Zotero/storage/GQ4LXGKI/Xu et al. - 2021 - ART-UP A Novel Method for Generating Scanning-Robust Aesthetic QR Codes.pdf:application/pdf},
}


