\section{Imprinto Watermarks}
In this section, we provide a comprehensive overview of the watermark embedding and detection process. The key steps involved are capturing the image, binarizing the image and extracting the QR code from the binarized image. Although extracting the QR code from an already binarized image can be done using commercially available software, the remaining necessary steps required the development of specialized modules. 




\subsection{Embedding the Watermark}
We developed a software tool to embed markers based on the system and the invisible IR ink values determined in our experiment in Section~\ref{visibility-experiment}. To use our tool shown in \autoref{fig:UIexample}, users first select the document for which they wish to create embeddings. Then, they choose whether to utilize the whole sheet for embedding or only designated areas. In the latter, the user marks on the document where they want to embed content, and the system interactively generates layouts. Finally, users decide whether to rely on internet information (online) or create a fully self-contained (offline) document.
The tool then analyzes the RGB values of the background color of the specified image region. It determines the conservative lower bound of the invisible IR ink density using the underlying system described in Section~\ref{visibility-experiment}.



%\begin{comment}
\paragraph{Online mode}
In the online mode, we take advantage of the ability to use the internet by hyperlinking multimedia content using QR codes.
In this menu (\autoref{fig:UIexample}b), the user can select the paper area that they wish to track using AR. An image score is computed for each selection using \textit{Google arcoreimg} tool\footnote{\url{https://developers.google.com/ar/develop/augmented-images/arcoreimg}} to ensure proper tracking and content overlay in AR.
In case the image is too difficult to track (i.e., the score is lower than 75), we inform the user and prompt them to choose a different area. Our tool allows users to associate each AR-tracked image with online multimedia content, such as text, image, and/or audio.
For each tracked image, the user can link a series ("frames") of these types of contents that users can loop through in AR. The underlying technology can be further generalized and used in order to incorporate complex multimedia content (such as videos or 3D models) or engineer further interactions between the objects. 
% Each tracked image is associated with a cyclical list of frames. A frame is characterized by a text file, an image to display, and a sound that can optionally play during the subsequent transition. 

%In the standard Unity application, we display the current frame above the tracked image and transition to the next frame every time it is clicked.%



\paragraph{Offline mode}
In the offline mode (\autoref{fig:UIexample}c), we are currently limited to text-only content due to multimedia content requiring significantly more space.
However, this capacity that offline embedding offers can be used to embed hyperlinks or XML media, such as contact cards, or stylized and interactive text content via markup language tags of the user's choice.
Even though we offer an automatic analysis of the document and calculate a possible way of embedding the information, we further allow the user to fine-tune these settings (i.e., size, error correction level, and version of the QR codes) according to their specific requirements.






\subsection{Capturing the Watermark}



\new{\systemName~ media can be captured using devices with NIR image sensors. There are two main methods by which users can do this. First, more conventionally, users can point their devices at the physical media to extract \systemName~ content. This is particularly suitable for settings where the users are aware that the content is already embedded (e.g., an instructor informing students about \systemName~ verbally before handing out worksheets, or through a small visual cue on them; a book or magazine mentioning the AR functionality only on its cover---so not each page needs to carry a notice). 
Second, there are other types of devices that have constant scanning functionality~\cite{dogangun_rampa_2024, iyer_xr-penter_2025}. Emerging devices, such as smart glasses (see \textit{Meta Orion}\footnote{\url{https://about.meta.com/realitylabs/orion}}),  feature "always-on" IR scanning functionality to map the environment that we envision can be repurposed to detect human-invisible IR markers (see \autoref{fig:ApplicationsRobotic}). This, in turn, would enable the always-on and seamless capture of these markers without much physical effort. However, accessibility to the technology is not yet widely available, discussed in  
Section~\ref{NIR_AR_FormFactor}.} 

\new{To support this emerging technology, we developed a \textit{universal} \systemName~ detection and reader module for mobile devices.} %: the RGMVision InfraRedCAM 1.
The module was built to decode document watermarks based on the inkjet ink absorbance characteristics (Section~
\ref{spectrum}) by featuring a 5MP CMOS NIR camera and two controllable NIR LEDs, both operating at a wavelength of $850~nm$. The sensor module integrates a bandpass filter centered at the same wavelength. We designed and developed the PCB and the aluminum enclosure for mechanical and electromagnetic protection produced by \textit{Megacal}\footnote{\url{https://megacal.es/}}, with a size of $59.5\times33.0\times11.6~mm$. 
Figure~\ref{fig:RGMCameraModule} shows the components of the module.
Our proprietary module, \textit{RGMVision InfraRedCAM 1}\footnote{\url{https://www.rgmvision.com/product/rgmvision-infraredcam-1/}}, ensures universal mobile connectivity through \textit{USB 2.0} or \textit{3.0} and its \textit{USB-C} connector. %and is compatible with \textit{Android}/\textit{iOS} mobile devices, as well as \textit{Windows}/\textit{macOS}/\textit{Ubuntu}/\textit{Raspberry Pi OS} laptops and desktop computers.
As opposed to related works, this allows our device to offer universal compatibility, converting any mobile device, smartphone, laptop, or PC into an \systemName~ reader via its \textit{USB-C} support. Additionally, the two diffuse NIR LEDs (80°), controlled by two PWM signals, provide 24 different adjustable illumination levels for indoor applications.


\begin{figure}[]
  \centering
  \includegraphics[width=0.47\textwidth]{figures-new/Fig6-RGMVision_InfraredCAM_1-new.jpg}
  \caption{% Developed USB smartphone reader – RGMVision InfraredCAM 1 –
  Our mobile imaging attachment comes with a 5MP sensor and two controllable NIR LEDs working at $850~nm$. (a) Module directly plugged into the mobile device. (b) Module coupled and lined with the back smartphone camera through a USB extender. (c) Front view of the module with the NIR LEDs powered up on a different device. (d) 3D exploded view, including the sensor, the LEDs mounted in the PCB, and the aluminum mechanical and electromagnetic enclosure.}
  \Description{.}
  \label{fig:RGMCameraModule}
\end{figure}



We developed an \textit{Android} application (built for API level 26 and targeted for API level 30), based on \textit{Unity3D}, interfaces with the module through the \textit{Unity3D} plugin \textit{UVC4UnityAndroid v2.0}\footnote{\url{https://github.com/saki4510t/UVC4UnityAndroid}}.




\subsection{Decoding the Watermark}

We developed an ML pipeline to binarize the captured IR watermarks for robust detection, which we will make open-source. Because the IR camera captures the watermarks in a faint tone, off-the-shelf QR code readers are unable to recognize \systemName~ codes. Thus, we need an intermediary step where the contrast of the IR watermarks is increased via binarization, where the codes are converted into black-and-white patterns similar to conventional QR codes. Previous works~\cite{dogan_standarone_2023} have used traditional image processing filters for this step, whereas we use an ML-focused approach to increase robustness (Section~\ref{EvaluationDataEmbedding}). 
We next explain the individual steps that were necessary for building our pipeline, as well as how to use it in real time.


\subsubsection{Training the ML model}
\label{TrainingML}
The training involved multiple steps, as shown in \autoref{fig:IRTraining}.
% Below we explain the procedure step by step.



\begin{figure*}[]
  \centering
  \includegraphics[width=0.9\linewidth]{figures-new/Fig9-Training2-new.jpg}
  \caption{ML training. (a) Sample printing. (b) Data capture with our smartphone module and labeling processes. (c) Data augmentation. (d) CNN training for the binarization task.}
  \Description{.}
  \label{fig:IRTraining}
\end{figure*}



\paragraph{Data capture}


To ensure our ML model generalized to various conditions, we printed and captured multiple conditions with our setup. We intended to print different types of barcodes so the trained model can excel at binarization as an image task, regardless of the barcode type or content. Thus, we printed samples of three types of barcodes: a QR code with low ECC, a QR code with high ECC, and a Data Matrix code. Per sheet, we printed only one type of barcodes.

On each US letter-size sheet, we had a total of 25 codes that were created by repositioning and rescaling one of these initial barcodes in a grid-like pattern (see \autoref{fig:IRTraining}).
Varying the size of the code creates a more robust training set so that the model can recognize codes that appear smaller from a distance, at different angles, or with lower resolution.
We varied the the module (bit) size from $1~mm$ to $2~mm$, in increments of $0.25~mm$ from left to right to allow our model to generalize across different resolutions.

Each sheet was printed three times, once for each of the three possible IR ink quantity classes, and a matching visual for the CMY color content (darker background, lighter background, or blank), resulting in a total of nine sheets..
Regardless of the ink amounts and combinations, the model is expected to learn to detect and binarize the codes.
We then photographed each sheet using the IR camera from an identical distance of $25.5~cm$ in order to capture the letter-size sheet (see Section~\ref{DetectionDistanceCapacity}).



\paragraph{Ground truth labeling}

Using \textit{Photoshop}, we binarized the images in order to create a black-and-white image representing the ground truth. This process involved manually adjusting each code using various transformations (e.g., perspective warp) to match the pose of the code on the photographed sheet.

\paragraph{Data augmentation}

Using \textit{MATLAB}, we augmented our dataset by 500 images for each original image by performing a combination of geometric and color space transformations, including randomized changes in brightness, contrast, hue, and saturation.
\autoref{fig:IRTraining}c exemplifies the samples we generate in order to simulate different conditions that might occur due to varying physical conditions. 
Each image was rescaled to 450x600 pixels for efficient ML training.
In total, our resulting dataset had 4,500 images.
Such data augmentation in ML also helps us enhance model robustness and prevents overfitting by enlarging the dataset.
 




\paragraph{Training}
For the ML training, we employed a convolutional neural network (CNN) similar to \cite{dogan_sensicut_2021} but used the \textit{U-Net}~\cite{ronneberger_u-net_2015} architecture with a pre-trained \textit{ResNet-34}~\cite{he_deep_2015} as its backbone for black-and-white segmentation (i.e., binarization).
Using \textit{PyTorch}~\cite{paszke_pytorch_2019} and \textit{fast.ai}~\cite{howard_fastai_2020}, we utilized the \textit{Adam} optimizer~\cite{kingma_adam_2017}, and used cross-entropy as loss function.
We trained the model with a batch size of 8 for 30 epochs and for about 20 minutes each,  until training loss settled at 0.05. 
Because this is not a classification task but rather segmentation (binarization), we do not compute an accuracy percentage to evaluate the performance for the sake of the eventual code detection.
The binarized output will be passed onto a standard barcode reader, as reported in the next section.
We evaluate how the combination of our ML model and barcode reader enables data storage capabilities in Section~\ref{EvaluationDataEmbedding}.









\subsubsection{Real-Time Inference}
\label{RealTimeInference}

We developed a \textit{Unity3D} application that interfaces with both the phone's standard RGB camera and our IR camera module in order to both extract the embedded data and display it in a visually appealing manner by means of AR augmentation of the tracked environment.

The application works by constantly running the ML model on the feed from the IR camera and then redirecting the output to a code reader library \textit{Dynamsoft SDK}\footnote{\url{https://www.dynamsoft.com/}} for extraction. The output from the reader consists of a list of the values of all detected QR codes and their coordinates.
In the case of offline embedding, we concatenate all results from the read QR codes in order to reconstruct the encoded string. In the case of online embedding, each QR code corresponds to a location on the data server that allows the application to download further instructions.


