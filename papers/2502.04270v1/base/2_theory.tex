
	\vspace{-5pt}
	\section{Problem Setup and Motivation}\label{sec:setup}
	
	In this section, we introduce the setup for the problem studied in this work. In \Cref{sec:intro_alignment}, we present the basic framework for aligning language models with human preferences. In \Cref{sec:intro_DPO}, we provide an overview of the widely-used Direct Preference Optimization (DPO) method. Finally, in \Cref{sec:intro_goal}, we introduce the core problem investigated in this work: designing an optimal sampling scheme for response generation.
    % \yaqi{Delete this to avoid redundancy.}
	\vspace{-3pt}
	\subsection{Aligning LMs with Human Preferences}
	\label{sec:intro_alignment}
	
	% We break down the alignment process into three key components: the language model, preference data, and policy optimization.
	
	\paragraph{Language Model (LM).}
	At the core of RLHF is a language model that processes prompts~\mbox{$\prompt \in \PromptSp$} and generates responses $ \response \in \ResponseSp $. 
	Each response is represented as a sequence of tokens $\response = (\tokent{1}, \tokent{2}, \ldots, \tokent{\numtok}).$ The primary goal of RLHF is to guide the model to generate responses that align with human preferences. This translates to designing a policy $\policy$ (parameterized as a LM) that maps prompts to responses, maximizing a reward that reflects human preferences (with a KL regularization).
	
	
	\paragraph{Preference Data.} The oracle reward for human values is inherently inaccessible. Instead, the alignment process approximates the reward using a dataset of human-labeled preferences,
	\begin{align*}
		\Data = \big\{ (\prompti{i}, \responsewini{i}, \responselosei{i}) \big\}_{i=1}^{\numobs} \, ,
	\end{align*}
	where each sample contains: (i) a prompt $ \prompti{i}$, independently drawn from a distribution $ \promptdistr$, and (ii) a pair of responses $(\responsewini{i}, \responselosei{i})$, where $\responsewini{i}$ is preferred over $\responselosei{i}$ in human labeling. The response pair~\mbox{$(\responsewini{i}, \responselosei{i})$} is first generated from a joint distribution $\responsedistr(\cdot \mid \prompt)$ 
    %\yaqi{$\Leftarrow$ $\responsedistr$ first appears here. It is indeed quite far from \Cref{sec:theory}.} 
    and then presented to human labelers for preference annotation. Human preferences are commonly modeled using the \emph{Bradley–Terry (BT)} model, which assumes: \begin{align}
			\label{eq:BT}
			\Prob\big( \responseone \succ \responsetwo \bigm| \prompt \big)
			\; = \; \sigmoid\big( \rewardstar(\prompt, \responseone) - \rewardstar(\prompt, \responsetwo) \big) \, ,
		\end{align}
		where $\rewardstar(\prompt, \response)$ represents the (unknown) oracle reward of a response given a prompt, and $\sigmoid(z) = \{ 1 + \exp(-z) \}^{-1}$ is the sigmoid function, mapping differences in rewards to probabilities. We adopt the BT model throughout this paper.

    \paragraph{Reward Modeling.} The preference data, encoding human judgment, is then used to train a reward model, $r_\theta$, which serves as a measurable objective for training the policy model. $r_\theta$ is trained by solving a MLE objective:
\begin{align}
		\label{eq:RM_objective}
		\min_{\paratheta} \
        % \quad \Losshat(\paratheta)  \\
        % \notag
        \Losshat(\paratheta) :=
		- \frac{1}{\numobs} \sum_{i=1}^{\numobs} \log \sigmoid \Big( \rewardtheta\big(\prompti{i}, \responsewini{i}\big) - \rewardtheta\big(\prompti{i}, \responselosei{i}\big) \Big).
	\end{align}
%
    This empirical loss approximates the expected negative log-likelihood
	\begin{align}
		\label{eq:def_Loss}
        \Loss(\paratheta) \; := \;
		\Exp_{\prompt \sim \promptdistr, \,(\responseone, \responsetwo) \sim \responsedistr(\cdot \mid \prompt)} \Big[ - \log \sigmoid \big( \rewardtheta(\prompt, \responsewin) - \rewardtheta(\prompt, \responselose ) \big) \Big] \, .
	\end{align}
    
    \paragraph{Policy Optimization.} To align a language model $\phi$ with human preferences, we optimize it to maximize the learned rewards $\rewardtheta$ while staying close to a reference policy $\policyref$. The objective is
	\begin{equation}\label{eq:policy_loss_with_rm}
    \max\nolimits_{{\phi}} \ 
    \Exp_{\prompt \sim \promptdistr, \response \sim \policy_{\phi}(\cdot \mid \prompt)}
    \big[ \rewardtheta (\context, \response) \big]
    - \parabeta \kull{\policy_{\phi}}{\policyref}.
\end{equation}
	It consists of two parts: 
    % \vspace{-0.8em}
    \begin{itemize}
	\item[(i)] The \emph{reward} term $\Exp_{\prompt \sim \promptdistr, \, \response \sim \policy(\cdot \mid \prompt)} [ \rewardtheta(\context, \response) ]$ encourages the policy to generate high-quality responses.
	\item[(ii)] The \emph{regularization} term \mbox{$\kull{\policy}{\policyref}$} penalizes deviations from the reference policy~$\policyref$ and is defined as \mbox{$\Exp_{\prompt \sim \promptdistr} \big[ \kull[\big]{\policy(\cdot \mid \prompt)}{\policyref(\cdot \mid \prompt)} \big]$}.
    \end{itemize}
    % \vspace{-0.8em}
    Here, $\parabeta$ is a regularization parameter that balances the trade-off between reward maximization and adherence to the reference policy. 
    % The reference policy $\policyref$, often obtained via supervised fine-tuning (SFT), serves as the starting point for optimization. 
    We assume $\parabeta$ is fixed and practitioner-specified.


    
	% \begin{enumerate}
	% 	\item[(i)] Two candidate responses $\responseonei{i}$ and $\responsetwoi{i}$ are drawn from a joint distribution $\responsedistr(\cdot \mid \prompt)$.
	% 	\item[(ii)] The preference between them is modeled using the \emph{Bradley–Terry (BT)} model:
	% 	\begin{align}
	% 		\label{eq:BT}
	% 		\Prob\big( \responseone \succ \responsetwo \bigm| \prompt \big)
	% 		\; = \; \sigmoid\big( \rewardstar(\prompt, \responseone) - \rewardstar(\prompt, \responsetwo) \big) \, ,
	% 	\end{align}
	% 	where $\rewardstar(\prompt, \response)$ represents the (unknown) quality or reward of a response given a prompt, and $\sigmoid(z) = \{ 1 + \exp(-z) \}^{-1}$ is the sigmoid function, mapping differences in rewards to probabilities.
	% \end{enumerate}

	
	    
	
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Direct Preference Optimization}
	\label{sec:intro_DPO}

    The above-described RLHF pipeline typically leverages the Proximal Policy Optimization (PPO) algorithm \citep{schulman2017proximal} to perform policy optimization. This approach requires loading the policy network, reward model, reference model, and a value model onto the GPU during training, making it highly resource-intensive. To improve computational efficiency and practicality, Direct Preference Optimization (DPO) \citep{rafailov2023direct} has been proposed, enabling direct alignment without the need for a reward model or a value model.
	
    A key insight of DPO is that any policy~$\policytheta$ can be viewed as the optimal solution to problem~\eqref{eq:policy_loss_with_rm} if the reward~$\rewardtheta$ is 
	\vspace{-3mm}
    \begin{align}
		\label{eq:def_reward}
		\rewardtheta(\prompt, \response)
		\; \defn \; \parabeta \cdot \log \bigg( \frac{\policytheta(\response \mid \prompt)}{\policyref(\response \mid \prompt)} \bigg).
	\end{align} 
	% This suggests that if the reward function $\rewardtheta$ approximates the true $\rewardstar$ closely, the associated policy $\policytheta$ is expected to maximize the expected value.
%
    % DPO directly operates within a parameterized policy class \mbox{$\PolicySp = \{ \policytheta \mid \paratheta \in \Real^{\Dim} \}$}.
%
    Thus, DPO can directly optimize the policy $\policytheta$ using $\Losshat(\paratheta)$ in \cref{eq:RM_objective}, where $\rewardtheta$ is replaced by $\policytheta$ as defined in \cref{eq:def_reward}. This reformulation makes the objective dependent solely on $\theta$, with the reward being implicitly learned through the policy itself. As a result, the optimization process becomes significantly more efficient.
	
	% To estimate $\rewardstar$, DPO employs maximum likelihood estimation (MLE) by solving
	% \begin{align}
	% 	% \label{eq:RM_objective}
	% 	& {\rm minimize}_{\paratheta} \quad \Losshat(\paratheta)  \\
 %        \notag
 %        & \quad \Losshat(\paratheta) \; \defn \;
	% 	- \frac{1}{\numobs} \sum_{i=1}^{\numobs} \, \log \sigmoid \Big( \rewardtheta\big(\prompti{i}, \responsewini{i}\big) - \rewardtheta\big(\prompti{i}, \responselosei{i}\big) \Big) \, .
	% \end{align}
	
	
	
	\subsection{Motivation: Realigning Oracle Reward Maximization}
	\label{sec:intro_goal}

    To fully align with human values, RLHF should, in principle, train the policy to maximize the oracle reward, $\rewardstar$, as defined in the BT model. The corresponding oracle objective is then: 
    % \vspace{-.7em}
    \begin{equation} 
		%\max_{\policy}  \quad 
		\scalarvalue(\policy) \defn \; \Exp_{\prompt \sim \promptdistr, \, \response \sim \policy(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big] \notag \; - \; \parabeta \, \kull{\policy}{\policyref} \, .
        \label{eq:objective}
	\end{equation}
    %\ariel{The current equation seems to read as the definition of $\max_\pi J(\pi)$, but I think it's meant to be a definition of $J(\pi)$?}
    %\yaqi{Reform this sentence since it appeared once in intro.} 
    Since direct access to $\rewardstar$ is unavailable, RLHF instead relies on preference data, either through MLE-based reward modeling or methods like DPO. 
    However, these processes are not inherently designed to train the policy to directly maximize the oracle objective, $\scalarvalue(\policy)$.%\ariel{What do they directly maximize instead?}. 
    
    In this work, we aim to design an optimal sampling distribution $\responsedistr$ to realign DPO with the maximization of $\scalarvalue(\policy)$. Such a sampling strategy will improve the quality of the preference dataset, maximize the utility of limited data, and enhance both performance and efficiency.

    This focus is particularly crucial in scenarios where additional data is collected during mid-training—a key phase in the iterative fine-tuning of LMs
    \citep{touvron2023llama,bai2022training, xiong2024iterative, guo2024direct}.
    %\yaqi{Reform this sentence since it appeared once in intro.} 
    At this stage, a preliminary policy $\policytheta$ (distinct from $\policyref$) is already in place, but its performance may fall short of expectations. It is thus necessary to gather additional preference data, ideally on-policy data that target areas where the current policy shows room for improvement. An effective sampling design can significantly enhance the efficiency of leveraging human feedback in this process.
    
    % By strategically leveraging human feedback, this focused data collection ensures that the newly acquired information can enhance the model.
	
	% A closer examination of the DPO optimization process reveals that the choice of the sampling distribution $\responsedistr$ for response selection is a critical factor in determining training efficiency. In this work, we aim to design an optimal sampling distribution $\responsedistr$ that enables the DPO method to extract greater value from limited data while improving performance (as measured by $\scalarvalue(\policy)$).
	

	

    \section{T-PILAF: Theoretical Sampling Scheme }
% to align reward modeling with value maximization?
 %       \subsection{Mismatch between reward modeling and optimization objective}
        
%          \begin{align*}
%        \gradtheta \Loss(\paratheta) 
%        \qquad
%	- \gradtheta \scalarvalue(\policytheta)
%    \end{align*}
	% \subsection{Sampling scheme}
	\label{sec:sampling}

	
	We now present T-PILAF - {\em theoretically grounded policy interpolation for aligned feedback} - our sampling scheme for generating responses in data collection\footnote{The T in T-PILAF serves to distinguish the theoretical scheme from the derived, simplified, efficiently implementable PILAF.}. The scheme is shown (in \Cref{sec:theory}) to be optimal from both optimization and statistical perspectives. 
    % However, it is computationally impractical for large-scale language models. An experimental design is then proposed in \cref{sec:sampling_exp} ensuring scalability and enabling efficient inference. 
    
	Consider we have an {initial} policy $\policytheta$ and aim to collect preference data to further refine its performance.
	We propose two complementary variants of policy $\policytheta$: one that encourages exploration in regions {more} preferred by~$\policytheta$, reflecting an optimistic perspective, and another that focuses on areas {less favored by~$\policytheta$}, reflecting a conservative adjustment.
%	By generating response pairs using these two policies, we aim to enable efficient exploration and refinement of the current model $\policytheta$.
	
	Specifically, we define policies $\policythetapos$ and $\policythetaneg$ around $\policytheta$ as
    \begin{subequations}
	\begin{align}
		\label{eq:def_policythetapos}
		\policythetapos(\response \mid \prompt)
		& := \frac{1}{\Partitionthetapos(\prompt)} \; \policytheta(\response \mid \prompt)
		\exp \big\{ \rewardtheta(\prompt, \response) \big\} \, ,  \\[-1pt]
		\label{eq:def_policythetaneg}
		\policythetaneg(\response \mid \prompt)
		& := \frac{1}{\Partitionthetaneg(\prompt)} \policytheta(\response \mid \prompt)
		\exp \big\{ - \rewardtheta(\prompt, \response) \big\},
	\end{align}
	\end{subequations}
	where the reward function $\rewardtheta$ is defined in equation~\eqref{eq:def_reward}.
	The partition function $\Partitionthetapos(\prompt)$ (or $\Partitionthetaneg(\prompt)$) is given by
	\mbox{$\Partitionthetapos(\prompt)
	\defn \int_{\ResponseSp} \policytheta(\response \mid \prompt) \exp \{ \rewardtheta(\prompt, \response) \} \, \diff \response $}. %\yaqi{$\policythetapos$??? typo?}
    %\yaqi{Here it should be $\policytheta$ in the integration, consistent with the right-hand side in \Cref{eq:def_policythetapos}.}
	
	For any prompt $\prompt \in \PromptSp$, our sampling procedure involves the following steps:
	\begin{enumerate}  % \itemsep = -.2em
		\item[(i)] Draw a random variable $\xi$ from ${\rm Bernoulli}(\sampleprob(\prompt))$, where
%		\begin{align}
%			\label{eq:def_sampleprob}
        \vspace{-1em}
			$$\sampleprob(\prompt) \defn {\Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt)}/ \{1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}. $$ ~ \\ \vspace{-4em}
%		\end{align}
		\item[(ii)] If $\xi = 1$, independently draw responses $\responseone, \responsetwo \in \ResponseSp$ according to
%	\begin{align*}
		$\responseone \sim \policythetapos(\cdot \mid \prompt)$ and $\responsetwo \sim \policythetaneg(\cdot \mid \prompt)$. 
%	\end{align*}
		If $\xi = 0$, draw responses as $\responseone, \responsetwo \sim \policytheta(\cdot \mid \prompt)$.
	\end{enumerate}
	
	In the next section, we will theoretically analyze T-PILAF. To account for the changes in sampling, we adopt a slightly modified loss function in the theoretical framework:
     \begin{align*}
%     	\label{eq:def_Losshat_adj}
		\Losshat(\paratheta) \defn
     	& - \frac{1}{\numobs} \sum_{i=1}^{\numobs} \weight(\prompti{i}) \cdot \log \sigmoid \Big( \rewardtheta\big(\prompti{i}, \responsewini{i}\big) - \rewardtheta\big(\prompti{i}, \responselosei{i}\big) \Big) .
     \end{align*}
	The newly introduced weight function $\weight$ is defined as
	\begin{align}
		\weight(\prompt)
		& \label{eq:weight}
        \; \defn \; \big\{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \big\} / \, \Partitionthetabar \, ,
	\end{align}
	where the normalization constant~$\Partitionthetabar > 0$ is given by \mbox{$\Partitionthetabar \defn 1 + \int_{\PromptSp} \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \, \promptdistr(\prompt) \, \diff \prompt$}. We also modify the population loss $\mathcal{L}$ in \cref{eq:def_Loss} with the weight function.
	
	
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Analysis}\label{sec:theory}
This section provides the theoretical grounding and analysis of our proposed sampling scheme from two perspectives. In the {\em optimization} analysis (\Cref{sec:theory_opt}) we show that T-PILAF {\em aligns two objectives (gradient alignment property)}: maximizing the likelihood function (\cref{eq:def_Loss}) becomes equivalent to gradient ascent on the value function $\scalarvalue(\policytheta)$ (\cref{eq:objective}). Consequently, policy updates on $\pi_\theta$ move the parameters in the direction of steepest increase of $J$. T-PILAF thus provides the potential to accelerate training and improve generalization, compared to vanilla (uniform)  sampling. In the {\em statistical} analysis (\Cref{sec:theory_stat}) we focus on statistical error and show that {the asymptotic covariance} of the estimated parameter~$\parathetahat$ (inversely) aligns with the Hessian of the objective function~$\scalarvalue$ when sampling with T-PILAF. As a result, T-PILAF makes the sampled comparisons more informative, as they align with directions where~$\scalarvalue$ is most sensitive. The net outcome is reduced statistical variance of our method through tighter concentration of estimates in directions that matter most for performance.


\subsection{Optimization Considerations}
            \label{sec:theory_opt}



            
We begin by analyzing the DPO algorithm from an optimization perspective.
{\Cref{thm:grad} below formally illustrates how T-PILAF ensures alignment between the MLE gradient, $\gradtheta \Loss(\paratheta)$, and the oracle objective gradient, $\gradtheta \scalarvalue(\policytheta)$.}

%
%
\begin{theorem}[Gradient structure in DPO training]
\label{thm:grad}
  Using data collected from our proposed response sampling scheme T-PILAF, the gradient of $ \Loss(\paratheta) $ satisfies
\begin{align*}
    \gradtheta \Loss(\paratheta) \; = \;
    - \, \frac{\parabeta}{\Partitionthetabar} \, \gradtheta \scalarvalue(\policytheta) \, + \, \Term_2 \, ,
\end{align*}
where the constant $ \Partitionthetabar $ is defined in equation~\eqref{eq:weight}, and the term $ \Term_2% = \bigO( \norm{\rewardtheta - \rewardstar}^2 ) 
$ represents a second-order error.
\end{theorem}
The detailed proof of \Cref{thm:grad} is deferred to \Cref{sec:proof:thm:grad}. 
It involves calculation of explicit forms of the gradients $\gradtheta \Loss(\paratheta)$ and $\gradtheta \scalarvalue(\policytheta)$; the most notable technical contribution is showing how to leverage our sampling scheme to approximate the derivative $\divsigmoid$ of the sigmoid function. By using T-PILAF sampling, we can transform a difference term of the form $\sigmoid ( \Delta \rewardstar ) - \sigmoid ( \Delta \rewardtheta )$ in $\gradtheta \Loss(\paratheta)$ into a linear difference $\Delta \rewardstar - \Delta \rewardtheta$ in $\gradtheta \scalarvalue(\policytheta)$.

\Cref{thm:grad} establishes the \emph{gradient alignment} property, demonstrating that minimizing the likelihood-based loss function~$\Loss$ closely aligns with maximizing the oracle objective function~$\scalarvalue$, with only a minor second-order error. It highlights how the proposed sampling scheme enables the DPO framework to effectively guide the policy toward optimizing the expected reward.
%
Beyond DPO, in \Cref{app:extension}, we show how the same principle can be applied to PPO-based RLHF algorithms to help improve the sampling. %\yaqi{I will work on adding more details in the appendix tomorrow to address the generalization of our approach. Depending on the progress, if I can include sufficient detail in the appendix discussion, we might consider adding a brief sentence at the end of this subsection to mention this generalization.} 
		

		
		
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Statistical Considerations \yaqidone}
    \label{sec:theory_stat}

From a statistical standpoint, we first examine the asymptotic distribution of the estimated parameter $\parathetahat$ when it (approximately) solves the optimization problem~\eqref{eq:RM_objective}. In \Cref{thm:asymp}, we formally characterize the randomness or statistical error inherent in $\parathetahat$ under this idealized scenario.
The detailed proof of \Cref{thm:asymp} is provided in \Cref{sec:proof:thm:asymp}.
\begin{theorem}
    \label{thm:asymp}
%			We take $\weight(\prompt) \equiv 1$.
    Assume the reward model $\rewardstar$ in the BT model~\eqref{eq:BT} satisfies $\rewardstar = \reward_{\parathetastar}$ for some parameter $\parathetastar$.
    Under mild regularity conditions, the estimate $\parathetahat$ asymptotically follows a Gaussian distribution
    \begin{align*}
        \sqrt{\numobs} \; ( \parathetahat - \parathetastar)
        \; \stackrel{d}{\longrightarrow} \; \Gauss( \veczero, \CovOmega )
        \qquad \mbox{as $\numobs \rightarrow \infty$} \, .
    \end{align*}
    We have an estimate of the covariance matrix $\CovOmega$:
    \begin{align*}
        \CovOmega \; \preceq \; \Const_{1} \cdot \CovOpstar^{-1} \, ,
    \end{align*}
    where $\Const_{1} > 0$ is a universal constant. 
    When using T-PILAF, the matrix~$\CovOpstar$ is given by
    \begin{align}
        \label{eq:def_CovOpstar_simple}
        \CovOpstar \defn \; \Exp_{\prompt \sim \promptdistr} \Big[ \Cov_{\response \sim \policystar(\cdot \mid \prompt)} \big[ \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big] \Big] \, .
    \end{align}
\end{theorem}

Next we analyze the performance of the output policy \mbox{$\policyhat = \policy_{\parathetahat}$} from \Cref{thm:asymp} in terms of the expected value~$\scalarvalue(\policy)$. In \Cref{lemma:hess_scalarvalue}, we show that our proposed sampling method guarantees that the covariance of the statistical error in~$\parathetahat$ aligns inversely with the Hessian of~$\scalarvalue$ at the optimal policy~$\policystar$. This alignment prioritizes convergence efficiency along directions where the Hessian has large eigenvalues, adapting to the geometry of the optimization landscape. It highlights the efficiency of our sampling scheme in reducing statistical error.
For the detailed proof, see \Cref{sec:proof:lemma:hess_scalarvalue}.
\begin{theorem}
        \label{lemma:hess_scalarvalue}
    The value function $\scalarvalue(\policy)$ we define in equation~\eqref{eq:objective} satisfies $\gradtheta \scalarvalue(\policystar) = \veczero$ and % $\hesstheta \scalarvalue(\policystar) =$
    %\vspace{-.6em}
    \begin{align}
        \label{eq:hessscalarvalue}
        \hesstheta \scalarvalue(\policystar) \; = \;
%				- \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr} \Big[ \Cov_{\response \sim \policystar(\cdot \mid \prompt)} \big[ \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big] \Big]
%				= 
        - \frac{1}{\parabeta} \, \CovOpstar
    \end{align} %~ \vspace{-1.2em} \\
    for matrix $\CovOpstar$ defined in equation~\eqref{eq:def_CovOpstar_simple}.
    As a corollary, suppose $\CovOpstar$ is nonsingular, then there exists a constant $\Const_{2} > 0$ such that for any $\varepsilon > 0$, 
    % \vspace{-.3em}
%			with high probability,
%			\begin{align*}
%				\scalarvalue(\policyhat)
%				\; \geq \; \scalarvalue(\policystar) \, - \, \frac{\Const{} \, \big\{ 1 + \supnorm{\Partitionthetapos \Partitionthetaneg} \big\}}{\parabeta} \cdot \frac{\Dim \log \numobs}{\numobs}
%			\end{align*}
%			for some universal constant $\Const{} > 0$.
    \begin{align}
        & \limsup_{\numobs \rightarrow \infty} \Prob \bigg\{ \scalarvalue(\policyhat) < \scalarvalue(\policystar) - \Const_{2} \cdot \frac{\Dim \, (1 + \varepsilon)}{\numobs} \bigg\} \notag  \\
        & \qquad \leq \; \Prob\big\{ \chisquare_{\Dim} > (1 + \varepsilon) \, \Dim \big\}
        \leq \exp\Big\{  -\frac{\Dim}{2} \bigl(\varepsilon - \log(1 + \varepsilon)\bigr)  \Big\} . \label{eq:gap_bd}
    \end{align}
    % \vspace{-1.5em}
\end{theorem}		

    Our proposed sampling distribution $\responsedistr$ ensures that the output policy $\policyhat$ performs predictably and reliably. The value gap $\scalarvalue(\policystar) - \scalarvalue(\policyhat)$ asymptotically follows a chi-square distribution, irrespective of the problem instance details, such as the underlying reward model $\rewardstar$. 
    This \emph{structure-invariant statistical efficiency} allows the method to achieve asymptotically efficient estimates without requiring explicit knowledge of the model structure. % \yaqi{maybe cut from here}


    %\iffalse
    In addition to our analysis of the proposed sampling scheme in \Cref{sec:sampling}, we present a generalized version of \Cref{thm:asymp} that applies to any response sampling distribution~$\responsedistr$. While not directly tied to the main focus of this work, this broader result may be of independent interest to readers.
    The proof of \Cref{thm:asymp_full} is provided in \Cref{sec:proof:thm:asymp_full}.
    \begin{lemma}
        \label{thm:asymp_full}
        For a general sampling distribution $\responsedistr$, the statement in \Cref{thm:asymp} remains valid with the matrix $\CovOpstar$ redefined as % $\CovOpstar \defn$
        \begin{align}
            \CovOpstar \defn
            \Exp_{\prompt \sim \promptdistr,(\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
            \Big[ \, \weight(\prompt) \cdot \Var\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) \cdot \grad \, \grad^{\top} \Big] \, ,
            \label{eq:def_CovOpstar}
        \end{align} % ~ \vspace{-1.8em} \\
        where the expectation is taken over the distribution
        % \vspace{-.3em}
        \begin{subequations}
            \begin{align}
                \label{eq:def_responsedistravg}
                \responsedistravg(\responseone, \responsetwo \mid \prompt) 
                \defn \frac{1}{2} \, \big\{ \responsedistr(\responseone, \responsetwo \mid \prompt) + \responsedistr(\responsetwo, \responseone \mid \prompt) \big\} \, .
            \end{align} % ~ \vspace{-1.8em} \\
        The variance term is specified as
            \begin{align}
                & \Var\big(\indicator\{\responseone = \responsewin\} \mid \prompt, \responseone, \responsetwo \big)
                \label{eq:def_var}
                = \sigmoid\big( \rewardstar(\prompt, \responseone) - \rewardstar(\prompt, \responsetwo) \big) \, \sigmoid\big( \rewardstar(\prompt, \responsetwo) - \rewardstar(\prompt, \responseone) \big)
            \end{align}
        and the gradient difference $\grad$ is defined as
            \begin{align}
                \label{eq:def_grad}
                \grad \defn \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \, .
            \end{align}
        \end{subequations}
    \end{lemma}
    
    The general form of the matrix $\CovOpstar$ offers valuable insights for designing a sampling scheme. To ensure $\CovOpstar$ is well-conditioned (less singular), we must balance two key factors when selecting responses $\responseone$ and $\responsetwo$:
    % \vspace{-.8em}
    \begin{description} \itemsep = -.05em
        \item \emph{Large variance:} The variance in definition~\eqref{eq:def_var} should be maximized. This occurs when $\rewardstar(\prompt, \responseone) \approx \rewardstar(\prompt, \responsetwo)$. Intuitively, preference feedback is most informative when annotators compare responses of similar quality.
        \item \emph{Large gradient difference:} The gradient difference $\grad$ from definition~\eqref{eq:def_grad} should also be large. This requires responses with significantly different gradients. Only then can the comparison provide a clear and meaningful direction for model training.
    \end{description}
    %\fi

		
		
		
		


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	