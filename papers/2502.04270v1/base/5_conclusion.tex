\section{Conclusion}

In this paper, we introduced Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel sampling method designed to enhance response sampling for preference labeling. Theoretical analysis highlights PILAF's superiority from both optimization and statistical perspectives, demonstrating its ability to stabilize training, accelerate convergence, and reduce variance. The method is straightforward to implement and requires no additional hyperparameter tuning. We empirically validated its performance in both iterative DPO and online DPO settings, where it consistently outperformed existing approaches. To achieve the same level of performance, PILAF consistently requires lower annotation costs, which can be substantial when annotations require experts in knowledge-intensive domains.

In future work, we hope to extend PILAF to other paradigms, such as KTO \citep{ethayarajh2024kto} and IPO \citep{azar2024general}. Due to resource constraints, our evaluations were conducted using 8B models and a reward model to simulate human feedback. Future studies involving larger-scale experiments and real human labeling would further generalize our findings.

Overall, this work takes an important step toward improving preference data curation in RLHF pipelines, laying the groundwork for more effective methods in alignment. 

\section*{Acknowledgment}

YF and JK acknowledge support through NSF NRT training grant award 1922658. 
YD acknowledges support from NSF grant
DMS-2413812.
The authors would like to thank Gabriel Synnaeve, Wei Xiong, He He, Pu Yang, Angelica Chen for helpful discussions.
This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.

% \clearpage
