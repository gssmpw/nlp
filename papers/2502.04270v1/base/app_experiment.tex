\section{Experimental Details}\label{app:experiment}

We implement our code based on the open-sourced OpenRLHF framework \citet{hu2024openrlhf}. We will open-source our code in the camera-ready version.

We use both the helpful and the harmless (HH) sets from HH-RLHF \citep{bai2022training} without additional data selection. We adopt the chat template from the Skywork-Reward-8B model \citep{liu2024skywork} to align with the reward template. This reward model, fine-tuned from Llama-3.1-8B, is used to simulate human preference labeling and matches our network trained for alignment.

For SFT, we apply full-parameter tuning with Adam for one epoch, using a cosine learning rate schedule, a 3\% warmup phase, a learning rate of $5\times 10^{-7}$, and a batch size of 256. These hyperparameters are adopted from \citet{hu2024openrlhf}. 

For all the DPO training in both iterative and online settings, we use full-parameter tuning with Adam but with two epochs. The learning rate, warmup schedules, and batch size are all the same. 

During generation, we limit the maximum number of new tokens to 896 and employ top$\_$p decoding with $p=0.95$ for all experiments. For Online DPO, we use a sampling temperature of 1.0, following \citet{guo2024direct}, while in Iterative DPO, we set the temperature to 0.7 to account for the off-policy nature of the data, following \citet{dong2024rlhf, shi2024crucial}.

Prompts are truncated to a maximum length of 512 tokens (truncated from the left if the length exceeds this limit) for SFT, DPO, and generation tasks. For SFT data, the maximum length is further restricted to 1024 tokens. When the combined length of the response and the (truncated) prompt exceeds 1024 tokens, the response is truncated from the right. These truncation practices align with the standard methodology described by \citet{rafailov2023direct}. In contrast, for DPO, responses are not further truncated, as we are already limiting the maximum tokens generated during the generation process.

When reproducing the \textit{Hybrid Sampling} baseline (Exploration Preference Optimization, XPO) from \citet{xie2024exploratory}, we use $\alpha=5\times 10^{-6}$ as suggested in the paper.

We do not include a comparison with \citet{shi2024crucial} and \citet{liu2024sample} in our experiments. While \citet{shi2024crucial} employs a sampling method similar to ours, their approach requires significantly more hyperparameters to tune, whereas our method involves no hyperparameter tuning. On the other hand, \citet{liu2024sample} relies on training an ensemble of 20 reward models to approximate the posterior. Their sampling method requires solving the argmax of these rewards, which is computationally intractable. As a workaround, they generate 20 samples and select the best one using best-of-N with $N=20$. This approach demands at least six times the computational resources compared to our method.

% The \textit{Best-of-N} method implicitly incorporates an exploration mechanism by selecting the best and worst samples based on internal rewards, which is conceptually similar to the exploration design in PILAF.

\subsection{Additional Results}

We present the full results for Online DPO with the overfitted initial policy, including a scatter plot in \cref{fig:online_dpo_special_full} and a summary of the objective values in \cref{tab:online_DPO_special}.

We observe that \textit{Vanilla Sampling} rapidly increases its KL divergence from the reference model while its reward improvement diminishes over time. In contrast, PILAF undergoes an early phase of training with fluctuating KL values but ultimately achieves a policy with higher reward and substantially lower KL divergence. We hypothesize that PILAF’s interpolation-based exploration enables it to escape the suboptimal region of the loss landscape where \textit{Vanilla Sampling} remains trapped. 

Conversely, \textit{Hybrid Sampling}, despite its explicit exploration design, remains biased by the policy model and continues to exhibit high KL values. While KL divergence decreases over training, the reward improvement remains limited. Meanwhile, \textit{Best-of-N Sampling} introduces an implicit exploration mechanism through internal DPO, which selects the best and worst responses, leading to wider coverage than \textit{Vanilla Sampling}. However, despite achieving a KL divergence similar to PILAF, it results in a lower reward. These findings highlight the superiority of PILAF sampling, demonstrating its effectiveness in robustly optimizing an overfitted policy.




\begin{figure}[htb]
  \centering
  \begin{minipage}[t]{0.49\textwidth} % [t] 表示顶部对齐
    \vspace{0pt} % 关键调整：将基线固定在顶部
    \centering
    \includegraphics[width=\linewidth]{figs/online_special_full.pdf}
    \caption{\textbf{Online DPO with an overfitted initial policy}. Full results of the \cref{fig:online_dpo_special}. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.}
    \label{fig:online_dpo_special_full}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.49\textwidth} % [t] 表示顶部对齐
    \vspace{10pt} % 关键调整：将基线固定在顶部
    \centering
    \captionsetup{type=table}
    \caption{\textbf{Results of Online DPO with an overfitted initial policy.} We report the average reward, KL divergence from the reference model, and objective $\scalarvalue$ on the testset.}
    \vspace*{1.5em} % 调整表格与标题间距
    \begin{footnotesize}
    \begin{sc}
    \begin{tabular}{l|ccc}
    \toprule
        \textbf{Method} & Reward ($\uparrow$) & KL ($\downarrow$) & $\scalarvalue$ ($\uparrow$)\\ 
        \midrule
        \textit{Vanilla} & \underline{-3.95} & 39.85 & -7.93 \\
        \textit{Best-of-N} & -4.49 & {27.90}  & \underline{-7.28}\\
        \textit{Hybrid} & -6.00 & \textbf{18.20} & -7.82 \\
        \midrule
        \textit{PILAF} & \textbf{-3.54} & \underline{26.45} & \textbf{-6.19} \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{footnotesize}
    \label{tab:online_DPO_special}
  \end{minipage}
\end{figure}