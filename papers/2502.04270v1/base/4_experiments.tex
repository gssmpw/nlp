\section{Experiments}\label{sec:exp}

In this section, we empirically evaluate PILAF in both an iterative DPO setting (\cref{subsec:iterative_dpo}, following \citet{xiong2024iterative, dong2024rlhf}) and an online DPO setting (\cref{subsec:online_dpo}, following \citet{guo2024direct}) where the model undergoes multiple rounds of refinement through active data collection. Our findings indicate that, without requiring any hyper-parameter tuning, our sampling scheme stabilizes training, achieves higher reward scores, and maintains lower KL divergence from the reference model.

\paragraph{General Setup} We align the Llama-3.1-8B base model \citep{dubey2024llama} in terms of helpfulness and harmlessness using the HH-RLHF dataset \citep{bai2022training}, a widely-used benchmark dataset for alignment. It consists of 161k prompts in the training set. For response preference labeling, we use a well-trained reward model to simulate human preferences by assigning preference to pairs of responses under the BT assumption in  \cref{eq:BT}. Specifically, we employ the Skywork-Reward-8B model \citep{liu2024skywork}, a top-performing 8B model on RewardBench \citep{RewardBench}, as our oracle $\mathcal{O}$. During training, interaction with this reward model is limited to providing two responses for comparison. We set $\beta=0.1$ in all the experiments.

\paragraph{Supervised Fine-Tuning (SFT)} To initialize training, following \citet{rafailov2023direct}, we first fine-tune the base model to obtain the SFT model as $\policyref$, which we fix as the reference model in all the experiments. We use the originally preferred responses from the HH-RLHF dataset as the SFT dataset and perform full-parameter tuning.

\paragraph{Evaluation} We present our results using the reward-KL curve, following \citet{gao2023scaling}, with the reward evaluated by the oracle reward model $\mathcal{O}$. To monitor the impact of our sampling scheme on the optimization trajectory, we evaluate the model every 50 gradient steps during training. We use the entire testset of HH-RLHF (8.55K samples) to evaluate.

% We set top$\_$p decoding with $p=0.95$ and use temperature 1.0 across all the generation in training and evaluation. 

\paragraph{Baselines} We compare our sampling method against existing methods in \cref{tab:setup_summary}. Since we treat the oracle $\mathcal{O}$ as a proxy for human labelers that can only provide pairwise preferences, all baselines are constrained to query the oracle with exactly two samples at a time. We thus adapt a \textit{Best-of-N} variant that deploys the internal DPO reward to select the top and bottom candidates, which are then presented to the oracle for preference labeling, as listed in \cref{tab:setup_summary}. We compare PILAF against the baselines: \textit{Vanilla Sampling}, \textit{Best-of-N Sampling} (with DPO reward), and \textit{Hybrid Sampling} combined with a modified DPO loss \citep{xie2024exploratory}.

Full experimental details can be found in \cref{app:experiment}.

\subsection{Iterative DPO}\label{subsec:iterative_dpo}

\paragraph{Implementation} We first consider the iterative DPO framework \citep{xiong2024iterative, dong2024rlhf}, in which preference data is collected in successive iterations rather than as a single fixed dataset. At the start of each iteration, a large dataset of responses is sampled using the current model, annotated for preferences, and then used to train the current model. Concretely, we set $n_t = |\mathcal{D}_{\rho}|$ in \cref{alg:our_sampling}, meaning that all prompts are used to generate new responses at each iteration. During the first iteration, when $\policyref$ and $\policytheta$ are identical, PILAF reduces to \textit{Vanilla Sampling}. Hence, we choose to focus our comparison on the second iteration. For consistency, we initialize all runs with the same policy model obtained at the end of the first iteration via \textit{Vanilla Sampling}. %\yunzhen{I rewrote this paragraph.}
% \yunzhen{We need to change the description here so that the reviewer will not ask about iteration 3.}

% We fix the reference model $\policyref$ all the time. 

\begin{figure}[ht]
    \centering
    % \vspace{-.5em}
    \includegraphics[width=0.7\linewidth]{figs/iterative_noBT.pdf}
    % \vspace{-1em}
    \caption{\textbf{Reward-KL curve for Iterative DPO}. All training runs start from the same model obtained at the end of the first iteration via \textit{Vanilla Sampling}. Each dot represents an evaluation performed every 50 training steps.}
    \label{fig:iterative_DPO}
    % \vspace{-0.8em}
\end{figure}

\paragraph{Results} \cref{fig:iterative_DPO} presents the reward-KL curve for iterative DPO. PILAF  significantly outperforms all the other methods: it achieves the end-point rewards of the baselines already around halfway through training, with around 40\% less training time. This reduction directly translates to savings in both annotation and computational costs. We summarize the final performance in \cref{tab:iterative_DPO}. PILAF produces a final policy with a high reward value and a modestly small KL divergence from the reference model, thereby achieving the highest overall objective $\scalarvalue$.


% PILAF produces a final policy with substantially lower KL divergence from the reference model while achieving a much higher reward value. The overall objective $\scalarvalue$ is much better \julia{say something more precise than "much better"!}. We summarize the final performance in \cref{tab:iterative_DPO}.

\begin{table}
\vspace{-5pt}
    \caption{\textbf{Results of Iterative DPO}. We report the average reward, KL divergence from the reference model, and objective $\scalarvalue$ on the testset. Higher reward and $\scalarvalue$ are better, while lower KL divergence is better. We use \textbf{boldface} to indicate the best result and \underline{underline} to denote the second-best result.}
    \label{tab:iterative_DPO}
    \vskip 0.2in
    \centering
\begin{small}
\begin{sc}
    \begin{tabular}{l|ccc}
    \toprule
        \textbf{Method} & Reward ($\uparrow$) & KL ($\downarrow$) & $\scalarvalue$ ($\uparrow$)\\ 
        \midrule
        \textit{Vanilla} & -10.16 & 35.20 & -13.68 \\
        \textit{Best-of-N} & \underline{-10.13} & 32.38 & -13.37\\
        \textit{Hybrid} & -10.51 & \textbf{22.86} & \underline{-12.80} \\
        \midrule
        \textit{PILAF} (Ours) & \textbf{-9.80} & \underline{25.01} & \textbf{-12.30} \\
    \bottomrule
    \end{tabular}
\end{sc}
\end{small}
\vspace{-.5em}
\end{table}

\subsection{Online DPO}\label{subsec:online_dpo}

\paragraph{Implementation} We further evaluate our sampling method in the online DPO setting \citep{guo2024direct}, where new responses are generated and labeled at every training step, and these preference data are immediately used to update~$\policytheta$. This setting corresponds to the case where $n_t$ (in \cref{alg:our_sampling}) is set to the training batch size, resulting in the most annotation-intensive and most actively on-policy alignment.
By collecting and utilizing preference data on the fly for each batch, the policy is continuously refined using on-policy feedback throughout the entire training process. Similar to Iterative DPO, we initialize all training runs with the same $\policytheta$ and focus on comparing the subsequent optimization. Further details are in \cref{app:experiment}.

\begin{figure}[htb]
    \centering
    % \vspace{-.2em}
    \includegraphics[width=0.7\linewidth]{figs/online_withB_full.pdf}
    % \vspace{-1.6em}
    \caption{\textbf{Reward-KL curve for Online DPO}. Each dot represents an evaluation performed every 50 training steps.
    }
    \label{fig:online_dpo}
\end{figure}

\paragraph{Results} \cref{fig:online_dpo} demonstrates the effectiveness of PILAF in the pure online setting, and we summarize the final performance in \cref{tab:online_DPO}. Compared with \textit{Vanilla} and \textit{Hybrid Sampling}, PILAF achieves a significantly better Reward-KL trade-off curve, attaining higher reward with lower KL. Although \textit{Vanilla} eventually achieves roughly the same reward value as PILAF, it comes at the cost of a substantially higher KL. 
% \yunzhen{See if we can include a text analysis in the appendix.} 
When compared with \textit{Best-of-N},  PILAF traces a similar Reward–KL trajectory but ends with a higher reward and a better final objective after the same number of iterations, translating to lower sample complexity and reduced annotation and computational cost.


% \yunzhen{To reach the same performance, PILAF also reduce both the annotation cost and computational cost.}

\begin{table}[h]
\vspace{-10.5pt}
    \caption{\textbf{Results of Online DPO.} We report the average reward, KL divergence from the reference model, and objective $\scalarvalue$ on the testset. }
    \label{tab:online_DPO}
    \vskip 0.2in
    \centering
\begin{small}
\begin{sc}
    \begin{tabular}{l|ccc}
    \toprule
        \textbf{Method} & Reward ($\uparrow$) & KL ($\downarrow$) & $\scalarvalue$ ($\uparrow$)\\ 
        \midrule
        \textit{Vanilla} & \underline{-4.96} & 21.50 & -7.11 \\
        \textit{Best-of-N} & -5.54 & \textbf{12.35}  & \underline{-6.77}\\
        \textit{Hybrid} & -6.42 & 16.46 & -8.96 \\
        \midrule
        \textit{PILAF} (Ours) & \textbf{-4.88} & \underline{15.42} & \textbf{-6.42} \\
    \bottomrule
    \end{tabular}
\end{sc}
\end{small}
% \vspace{-0.4em}
\end{table}

\paragraph{Robustness Analysis} Having established the effectiveness of PILAF, we further evaluate its robustness by testing whether it improves optimization and statistical convergence under challenging conditions, as predicted from our statistical theory in \cref{sec:theory_stat}. Specifically, we replace the initial model with one that has overfit on a fixed off-policy dataset. This setup allows us to examine how different methods handle optimization starting from a poor initial point.

In \cref{fig:online_dpo_special}, we compare the performance of PILAF and \textit{Vanilla Sampling} when both are initialized from an overfitted policy. We observe that \textit{Vanilla Sampling} rapidly increases its KL divergence from the reference model while its reward improvement diminishes over time. In contrast, PILAF undergoes an early training phase with fluctuating KL values but ultimately attains a policy with higher reward and substantially lower KL divergence. We hypothesize that PILAF’s interpolation-based exploration design enables it to escape the suboptimal region of the loss landscape in which \textit{Vanilla} remains. These results underscore PILAF’s effectiveness in more robustly optimizing overfitted (or even adversarially initialized) policies. 

\begin{figure}[htb]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.5\linewidth]{figs/online_special.pdf}
    % \vspace{-1.2em}
    \caption{\textbf{Online DPO with an overfitted initial policy}. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.}
    %\vspace{-1em}
    \label{fig:online_dpo_special}
\end{figure}



% \subsection{Iterative PPO}

% \yunzhen{It does not make much sense? We are adding another gap between the reward model and the learned policy model.}

% \subsection{Online PPO}

% We need to use the reward model with some importance sampling to change the distribution. Hard to compare with same compute.