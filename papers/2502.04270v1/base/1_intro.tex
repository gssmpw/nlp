\section{Introduction}

Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training} has revolutionized large language models (LLMs) by incorporating human preferences, enabling significant progress in applications such as conversational AI \citep{achiam2023gpt}, personalized tutoring \citep{limo2023personalized}, and content curation \citep{yue2024inference}. At the core of RLHF is {\em reward modeling}, a critical process that translates human feedback—such as pairwise comparisons or rankings—into a measurable objective for model training. By formalizing human preferences, reward models then guide LLMs towards alignment through {\em policy optimization}. 
\blfootnote{$^\dagger$ Correspondence to: Yunzhen Feng (\url{yf2231@nyu.edu}).% and Yaqi Duan (\url{yaqi.duan@stern.nyu.edu}).
} \blfootnote{$^*$ Joint second authors.} \blfootnote{$^\diamond$ Joint senior authors}

While numerous studies have focused on improving language models (LMs) by optimizing fixed reward functions \citep{dongraft, liustatistical} %\yaqi{References} \
or leveraging pre-existing preference datasets \citep{ethayarajh2024kto, azar2024general, xu2024contrastive}, comparatively less attention has been paid to the critical challenge of collecting {\em effective}  data for human-labeling in RLHF, to maximize its utility.
This is an important problem, as the quality of preference data directly impacts the effectiveness of reward modeling and, consequently, the overall success of fine-tuning. 
This challenge is further compounded by the high cost of expert preference labeling~\citep{lightman2023letsverifystepstep}.

Preference data is usually generated by sampling response pairs $(\responseonei{i}, \responsetwoi{i})$ to a prompt $ \prompti{i}$ from a policy, and presenting them to human labelers for preference annotation. It is commonly assumed that the annotation follows the Bradley-Terry (BT) model, under an \emph{oracle reward}. Next, we use maximum likelihood estimation (MLE) on these preference data to train a reward model, which then serves as a measurable objective to optimize the policy (i.e. LLM) while staying close to a reference policy. In 
Direct Preference Optimization (DPO) \citep{rafailov2023direct}, this pipeline is simplified by optimizing the policy with implicit reward modeling.
However, all these pipelines give rise to a {
\em misalignment of objectives:} RLHF (or DPO) should, in principle, train its policy to maximize the (inaccessible) \emph{oracle objective} which combines the \emph{oracle reward} from the BT model with reference regularization. In practice, RLHF relies on preference data through the MLE objective in reward modeling or through methods like DPO, which are \emph{not} designed to guide policy optimization towards maximizing oracle rewards. Thus, reward optimization (either directly or implicitly via DPO) and (optimal) policy optimization are not inherently aligned, potentially leading to inefficiencies (Sec. \ref{sec:setup}).

In this work, we study this misalignment by examining the sampling scheme that generates
response pairs $(\responseonei{i}, \responsetwoi{i})$ for preference labeling, which is especially important when additional preference data is collected mid-RLHF training to mitigate the off-policy distributional shift, as is empirically standard
% ---a critical phase in iterative finetuning of language models 
\citep{touvron2023llama,bai2022training}. We show that
uniform sampling from the current policy, as is common, leads to misaligned gradients of the two
objectives (reward model loss and true oracle objective).



\begin{figure*}[t]
    \centering
    \vspace{-13pt}\includegraphics[width=0.95\linewidth]{figs/teaser.pdf}
    \vspace{2mm}
    \caption{\textbf{Overview of our approach}. (a) We consider a full RLHF training setup, where a language model (LM) policy is iteratively refined through active data collection. Our goal is to develop an optimal response sampling method for preference labeling. (b) We introduce PILAF, which generates responses by interpolating between the current policy and a reference policy, balancing exploration and exploitation. (c) Our theoretical analysis shows that T-PILAF aligns the parameter gradient with the steepest direction for maximizing human values and achieves more favorable convergence in regions of high sensitivity.}
    \label{fig:teaser}
    % \vspace{-1em}
\end{figure*}



To tackle this issue, we present {\em Theoretically Grounded Policy-Interpolated Learning for Aligned Feedback} (T-PILAF), a novel sampling method that aligns reward modeling with value optimization. Specfically, T-PILAF generates responses by interpolating the policy model and the reference model for a balanced exploration and exploitation. 
We provide rigorous theoretical analysis to show that for preference data generated with T-PILAF, the gradient of the MLE loss with respect to the policy network's parameters is aligned with the policy gradient of the oracle objective in a first-order sense. This alignment enables the policy to optimize directly for the oracle value, achieving both alignment and efficiency. Furthermore, we separately show from a statistical perspective that T-PILAF aligns optimization with the steepest directions of the oracle objective. It thus makes the sampled preference pairs more informative, reducing variance and improving training stability.

We then present PILAF, a simple modification of our theoretical sampling scheme T-PILAF, which naturally lends itself to practical implementation. For clarity of exposition, we present our method in the context of DPO; however, PILAF can be adapted to a wide class of preference optimization methods.\footnote{See \Cref{app:extension} for the extension to PPO.}
See \cref{fig:teaser} for an illustration of our setup, method, and the optimization and statistical principles underlying PILAF.


We conduct extensive experiments to validate PILAF's effectiveness and robustness. As a stand-in for expensive human annotators, we use a well-trained reward model—Skywork-Llama-3.1-8B \citep{liu2024skywork}—as a proxy for the oracle reward. Throughout training, we query this model exclusively for preference labels, simulating human feedback. We then align the Llama-3.1-8B base model \citep{dubey2024llama} using these proxy-labeled preference data in two settings: iterative DPO \citep{xiong2024iterative} and online DPO \citep{guo2024direct}. In both scenarios, preference data is collected on-the-fly, either after each full training epoch in the iterative setting or after every training step in the online setting. Across all configurations, PILAF outperforms all the baselines, producing a policy with higher reward (as measured by the proxy) and a lower KL divergence from the reference model, 
reducing annotation and computation costs by over 40\% in iterative DPO.


Our key contributions are as follows:
% \vspace{-.8em}
\begin{itemize} 
	\item \emph{(Practical sampling algorithm)} We propose PILAF (\cref{sec:sampling_exp}), an efficient sampling algorithm for generating response pairs in the RLHF pipeline for improved sample efficiency and performance, derived from its theoretically grounded variant T-PILAF (\cref{sec:sampling}). 
	\item \emph{(Theoretical optimality)} We provide theoretical guarantees for the efficiency of our approach from both optimization and statistical perspectives (\cref{sec:theory}).
        \item \emph{(Empirical validation)} We validate PILAF in both iterative and online DPO settings (\cref{sec:exp}) and observe that it consistently outperforms baselines by achieving higher reward and lower KL divergence from the reference model. Moreover, PILAF achieves comparable performance at significantly reduced annotation and computational costs. 
\end{itemize}


\vspace{-3pt}
\subsection{Related Work}


\textbf{Existing Sampling Schemes.} In academic papers, uniform vanilla sampling is the most commonly used approach, while methods such as best-of-N and worst-of-N have also been explored \citep{dong2024rlhf}. \citet{xie2024exploratory} propose sampling one response from the current policy model and another from a reference model, modifying the loss function to encourage optimistic behavior. Similarly, \citet{zhang2024self} sample one response from the current model but rank it alongside two offline responses from the reference model. \citet{shi2024crucial} present a formula similar to ours based on intuition, introducing several hyperparameters and analyzing convergence speed with DPO in a tabular setting. {\citet{liu2024sample} train an ensemble of reward models to approximate a posterior distribution over possible rewards and use Thompson sampling to generate responses with exploration.} In contrast to these works, we theoretically establish the principles of response generation for preference labeling, making minimal assumptions and simplifications while demonstrating the optimality of our approach. Our approach eliminates the need for hyperparameter tuning. 
% \julia{THis is what we should keep!}


\textbf{Policy Gradient.} Our theoretical principle is closely related to the family of policy gradient methods \citep{williams1992simple,sutton1999policy} in reinforcement learning, which optimize a policy $\policytheta$ by estimating and ascending the gradient of the expected return $\nabla_{\paratheta} \scalarvalue(\paratheta)$. 
Significant advancements have been made to improve the efficiency of these methods, including variance reduction techniques \citep{greensmith2004variance}, off-policy gradient estimation \citep{degris2012off}, interpolating on-policy and off-policy updates \citep{gu2017interpolated}, deterministic policy gradients \citep{silver2014deterministic}, and three-way robust estimation approaches \citep{kallus2020statistically}. 
Our study extends these principles to preference learning for LMs, aligning the MLE gradient with the oracle objective gradient by controlling the response sampling distribution, thereby improving learning efficiency. 

A review of other RLHF literature, particularly on data selection for the preference dataset, is deferred to \cref{app:related_work}.






