\section{Additional Literature Review}\label{app:related_work}

\textbf{RLHF}. RLHF has emerged as a cornerstone methodology for aligning large language models with human values and preferences \citep{achiam2023gpt}. Early systems \citep{ouyang2022training} turn human preference data into reward modeling to optimize model behavior accordingly. DPO has been proposed as a more efficient approach that directly trains LLMs on preference data.  
As LLMs evolve during training, continuing training on pre-generated preference data becomes suboptimal due to the distribution shift. Empirically, RLHF is applied iterativelyâ€”generating on-policy data at successive stages to enhance alignment and performance \citep{touvron2023llama, bai2022training}. Similarly, researchers have introduced iterative DPO \citep{xiong2024iterative, xu2023some} and online DPO \citep{guo2024direct} to fully leverage online preference labeling. Ultimately, the quality of preference data play a critical role in determining the effectiveness of the alignment. 

\textbf{Sampling in Frontier LLMs}. Technical reports of Frontier LLMs briefly mention sampling techniques. For instance, Claude \citep{bai2022training} utilizes models from different training steps to generate responses, while Llama-2 \citep{touvron2023llama} further use different temperatures for sampling. However, no further details are provided, leaving the development of a principled method an open challenge. % \julia{Same: move to appendix!}

\textbf{Data Selection.} There is a line of research aimed at improving sample efficiency for preference labeling by selecting question and response pairs. \citet{scheid2024optimal} conceptualize this as a regret minimization problem, leveraging methods from linear dueling bandits. \citet{das2024active, mehta2023sample, muldrewactive, ji2024reinforcement} draw insights from active learning, using various uncertainty estimators to guide selection by prioritizing sample pairs with maximum uncertainty. These approaches focus directly on a dataset of questions and responses and are orthogonal to our work. 

\textbf{Other Changes in Response Sampling.} Several works also modify the sampling design directly \citep{liustatistical, dongraft}, but with the goal of improving policy network optimization based on a reward model, rather than enhancing the reward modeling itself. \citet{liustatistical} employ rejection sampling to approximate the response distribution induced by the reward model, thereby improving optimization. However, this approach requires access to the reward model and incurs higher computational and labeling costs. Similarly, \citet{dongraft} use best-of-N sampling with the reward model to generate high-quality data for supervised fine-tuning (SFT). We consider these approaches orthogonal to our work.


Additionally, \citet{cen2024value} introduce a bonus term in the policy learning phase of online RLHF to promote exploration in response sampling, which aligns with the optimism principle.


\iffalse

\section{Additional Statistical Results}\label{app:add_stat_results}


    {In addition to our analysis of T-PILAF in \Cref{sec:sampling}, here we present a generalized version of \Cref{thm:asymp} that applies to any response sampling distribution~$\responsedistr$. While not directly tied to the main focus of this work, this broader result may be of independent interest to readers.
    The proof of \Cref{thm:asymp_full} is provided in \Cref{sec:proof:thm:asymp_full}.
    \begin{lemma}
        \label{thm:asymp_full}
        For a general sampling distribution $\responsedistr$, the statement in \Cref{thm:asymp} remains valid with the matrix $\CovOpstar$ redefined as
        \begin{align}
            \CovOpstar \defn
            \Exp_{\prompt \sim \promptdistr, (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
            \Big[ \, \weight(\prompt) \cdot \Var\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) \cdot \grad \, \grad^{\top} \Big] \, ,
            \label{eq:def_CovOpstar}
        \end{align} 
        where the expectation is taken over the distribution
        \begin{subequations}
            \begin{align}
                \label{eq:def_responsedistravg}
                \responsedistravg(\responseone, \responsetwo \mid \prompt) 
                \defn \frac{1}{2} \, \big\{ \responsedistr(\responseone, \responsetwo \mid \prompt) + \responsedistr(\responsetwo, \responseone \mid \prompt) \big\} \, .
            \end{align}
        The variance term is specified as
            \begin{align}
                & \Var\big(\indicator\{\responseone = \responsewin\} \mid \prompt, \responseone, \responsetwo \big)
                \label{eq:def_var}
                = \sigmoid\big( \rewardstar(\prompt, \responseone) - \rewardstar(\prompt, \responsetwo) \big) \, \sigmoid\big( \rewardstar(\prompt, \responsetwo) - \rewardstar(\prompt, \responseone) \big)
            \end{align}
        and the gradient difference $\grad$ is defined as
            \begin{align}
                \label{eq:def_grad}
                \grad \defn \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \, .
            \end{align}
        \end{subequations}
    \end{lemma}
    
    The general form of the matrix $\CovOpstar$ offers valuable insights for designing a sampling scheme. To ensure $\CovOpstar$ is well-conditioned (less singular), we must balance two key factors when selecting responses $\responseone$ and $\responsetwo$:
    \vspace{-.8em}
    \begin{description} \itemsep = -.05em
        \item \emph{Large variance:} The variance in definition~\eqref{eq:def_var} should be maximized. This occurs when $\rewardstar(\prompt, \responseone) \approx \rewardstar(\prompt, \responsetwo)$. Intuitively, preference feedback is most informative when annotators compare responses of similar quality.
        \item \emph{Large gradient difference:} The gradient difference $\grad$ from definition~\eqref{eq:def_grad} should also be large. This requires responses with significantly different gradients. Only then can the comparison provide a clear and meaningful direction for model training.
    \end{description}
    }

    \fi




