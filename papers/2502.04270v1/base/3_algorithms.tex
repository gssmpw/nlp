\section{PILAF Algorithm}	\label{sec:sampling_exp}

    % \yunzhen{Julia: is it good to put it here or together with the sampling scheme in theory?}

We now demonstrate that the T-PILAF sampling scheme defined in \cref{eq:def_policythetapos} and (\ref{eq:def_policythetaneg}) can be naturally extended into an efficient empirical algorithm (PILAF).

The first challenge in implementing these definitions lies in calculating the normalizing factors $\Partitionthetapos(\prompt)$ and $\Partitionthetaneg(\prompt)$, which can be computationally expensive for LLMs. To address this, we simplify the process by omitting these factors and replacing them with 1.\footnote{When the regularization coefficient $\parabeta$ is sufficiently small, the term $\exp\{\rewardtheta(\prompt, \response)\}$ in equation~\eqref{eq:def_policythetapos} stays close to $1$ and has only a minor effect. Consequently, the partition function $\Partitionthetapos(\prompt)$ is approximately $1$. A similar reasoning applies to $\Partitionthetaneg(\prompt)$.
\vspace{-1.4em}
%\yaqi{Please check here.}
}
% \ariel{Should we, or can we, somehow justify that this is fine? Some intuition that this quantity is typically close to 1/2? Else, it would have been interesting to also explore this parameter}.
Consequently, the sampling process becomes straightforward: with probability~\(1/2\), we sample using \(\policytheta\), and otherwise, we sample using~\(\policythetapos\) and~\(\policythetaneg\).

The second challenge lies in sampling a response $\response$ from $\policytheta(\response \mid \prompt)
		\exp \big\{ \pm \rewardtheta(\prompt, \response) \big\}$ in an autoregressive way for next-token generation. 
We argue that the policy $\policythetapos$ (and $\policythetaneg$) can be approximated in a token-wise manner:
		\begin{align*}
			\policythetapos(\response \mid \prompt)
		 \; \approx \; \policythetapos(\tokent{1} \mid \prompt) \, \policythetapos(\tokent{2} \mid \prompt, \tokent{1})  \cdots \, \policythetapos(\tokent{t} \mid \prompt, \tokent{1:t-1}) \,
			\cdots \, \policythetapos(\tokent{\numtok} \mid \prompt, \tokent{1:\numtok-1}),
		\end{align*}
		where
		\begin{align*}
			& \policythetapos(\tokent{t} \mid \prompt, \tokenttot{1}{t-1}) \; = \; 
			\frac{1}{\Partition(\prompt, \tokenttot{1}{t-1})} \, 
			\policytheta(\tokent{t} \mid \prompt, \tokenttot{1}{t-1})
			\bigg( \frac{\policytheta(\tokent{t} \mid \prompt, \tokenttot{1}{t-1})}{\policyref(\tokent{t} \mid \prompt, \tokenttot{1}{t-1})} \bigg)^{\parabeta} % \\
	%		& \; = \; \frac{1}{\Partition(\prompt, \tokenttot{1}{t-1})} \, 
	%		\policyref(\diff \tokent{t} \mid \prompt, \tokenttot{1}{t-1})
	%		\bigg( \frac{\policytheta(\tokent{t} \mid \prompt, \tokenttot{1}{t-1})}{\policyref(\tokent{t} \mid \prompt, \tokenttot{1}{t-1})} \bigg)^{1+\parabeta}
		\end{align*}
		with $\Partition(\prompt, \tokenttot{1}{t-1})$ being a partition function. 
        %\ariel{re: this whole equation above. (1) Is this really an approximation? The first equation approximating $\pi_\theta^+(\vec{y}|x)$ looks fairly exact for a reasonable definition of $\pi_\theta^+(y_i|...)$ unless I'm missing something obvious; inthead the definition of $\pi_\theta^+(y_i|...)$ seems like an approximation. (2) Is there any meaning to switching the notation from $y_t$ to $dy_t$ between the equations?} \yaqi{(1) The approximation appears in the partition function. If we can divide the density function by the overall integration along the trajectory, then the token-wise expansion becomes exact. However, the current partition $\Partition$ is a step-wise normalization. (2) I have removed the $\diff$ notation.} 
        The substitution of $\rewardtheta$ uses the correspondence between the reward model 
        $\rewardtheta$ and the policy $\policytheta$ in \cref{eq:def_reward}, under the assumption that this correspondence holds for all truncations~$\tokenttot{1}{t-1}$. It gives us a direct per-token prediction rule:
    % \vspace{-1em}  % Ariel: I commented this out bc it was messing up the formatting
    \begin{align*}
     \policythetapos(\cdot \mid \prompt, \tokenttot{1}{t-1}) \; = \; \softmax\Big( \big\{ (1 + \parabeta) \, \headtheta - \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big).
    \end{align*}
    % \vspace{-1.8em}
Here $ \headtheta $ and $ \headref $ are the logits of the policies $\policytheta$ and $\policyref$, respectively. $\parabeta$ is the regularization coefficient from the objective function $ \scalarvalue(\policy)$ in \cref{eq:objective}. Responses are then generated using standard decoding techniques, such as greedy decoding or nucleus sampling. Similarly, the generation for $\policythetaneg$ follows 
%\vspace{-.5em}
\begin{align*}
 \policythetaneg(\cdot \mid \prompt, \tokenttot{1}{t-1}) \; = \; \softmax\Big( \big\{ (1 - \parabeta) \, \headtheta + \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big) \, .
\end{align*}
% \vspace{-1.8em}
For a detailed, step-by-step proof, see Proposition~1 in \citet{liu2024decoding}.

We formalize our final algorithm in \cref{alg:our_sampling}. Vanilla DPO \citep{rafailov2023direct, guo2024direct} employs a basic generation approach, sampling $\responseone_i, \responsetwo_i \sim \policytheta$ at Step~3. In contrast, instead of only sampling from $\policytheta$, our sampling scheme interpolates and extrapolates the logits~$\headtheta$ and~$\headref$ with coefficient $\parabeta$, enabling exploration of a wider response space to align learning from human preference with value optimization. The $\parabeta$ here is the same parameter that controls the KL regularization in \cref{eq:policy_loss_with_rm}, as set by the problem.
% \yunzhen{Do we need this paragraph?} 
%\ariel{It does read a bit weird in context, but I think it's useful to state something to the effect of "This is our sampling procedure; in contrast, standard DPO samples everything from $\pi_\theta$}

\paragraph{Cost analysis} We summarize sampling and annotation costs per preference pair for PILAF and related sampling schemes in \cref{tab:setup_summary}. In \textit{Vanilla} sampling (from $\policytheta$), two generations and two annotations are required for human preference labeling, same to PILAF when the pair is sampled from $\policytheta$, which happens half the time. With 50\% probability, PILAF uses $\policythetapos$ and $\policythetaneg$ to generate, requiring two forward passes with $\policytheta$ and $\policyref$ to generate one sample. Thus, on average, a preference pair sampled with PILAF requires a sampling cost of 3 forward passes (1.5 time the cost of \textit{Vanilla}) with the same annotation cost. To compare, \citet{xiong2024iterative, dong2024rlhf} perform \textit{Best-of-N} sampling with $N=8$, which generates and annotates all 8 responses, selecting the best and worst of them. \citet{xie2024exploratory} use a \textit{Hybrid} method that generates with $\policytheta$ and $\policyref$, thus matching the sampling and annotation costs of the \textit{Vanilla} method. We empirically compare PILAF with these methods in the next section.


    
		% Building on these principles, we propose a simple yet effective sampling scheme. For any given prompt $ \prompt $, generate two responses $(\responseone, \responsetwo)$ using one of two strategies, chosen with \emph{equal} probability: \vspace{-.5em}
		% \begin{itemize} % \itemsep = -.1em
		% 	\item 
		% 	Generate both responses independently from $\policytheta(\cdot \mid \prompt)$.
		% 	\item Sample $ \responseone $ from $ \policythetapos$ and $ \responsetwo $ from $ \policythetaneg$.
		% 	This can be done (approximately) in per-token prediction, via
		% 	\vspace{-.5em}
		% 	\begin{align*}
		% 		& \policythetapos(\cdot \mid \prompt, \tokenttot{1}{t-1}) \\
		% 		& \quad \; = \; \softmax\Big( \big\{ (1 + \parabeta) \, \headtheta - \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big)  \\
		% 		& \policythetaneg(\cdot \mid \prompt, \tokenttot{1}{t-1})  \\
		% 		& \quad \; = \; \softmax\Big( \big\{ (1 - \parabeta) \, \headtheta + \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big) \, .
		% 	\end{align*}
		% 	\vspace{-1.8em}
		% \end{itemize}
		% $\parabeta$ is the regularization coefficient from the objective function $ \scalarvalue(\policy)$ in equation~\eqref{eq:objective}. This adjustment shifts $ \policythetaneg$ closer to $ \policyref$ while pushing $ \policythetapos$ further away. 
		

  %       We formalize the algorithm in \cref{alg:our_sampling}.

\begin{algorithm}
\caption{DPO with PILAF (ours).}
\label{alg:our_sampling}
\begin{algorithmic}[1]
    \INPUT Prompt Dataset $\mathcal{D}_\rho$, preference oracle $\mathcal{O}$, $\policytheta, \policyref$.
    \FOR{step $t$ = 1, ..., $T$}
        \STATE Sample $n_t$ prompts $\{x_i\}_{i=1}^{n_t}$ from $\mathcal{D}_\rho$.
        \STATE \hl{With probability 1/2, sample $\responseone_i, \responsetwo_i \sim \policytheta$; with probability 1/2, sample $\responseone_i \sim \policythetapos$ and $\responsetwo_i \sim \policythetaneg$.}
        % \STATE \hl{}
        \STATE Query $\mathcal{O}$ to label $(x_i, \responseone_i, \responsetwo_i)$ into $(x_i, \responsewini{i}, \responselosei{i})$.
        \STATE Update $\policy_{\theta_t}$ with DPO loss using $\{(x_i, \responsewini{i}, \responselosei{i})\}_{i=1}^{n_t}$.
        % \IF{condition on $x$}
        %     \STATE Perform some operation
        % \ELSE
        %     \STATE Perform an alternative operation
        % \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-1em}




\begin{table*}
\vspace{-13pt}
    \caption{ \footnotesize A cost summary of PILAF and sampling methods from related works. \textit{Best-of-N} method in \citet{xiong2024iterative} uses the oracle reward to score all candidate responses, then selects the highest- and lowest-scoring onesâ€”instead of providing a preference label for only two responses. We restrict the oracle to providing only preference labels. Thus, we create a \textit{Best-of-N} variant that uses the DPO internal reward for selection and then applies preference labeling, with an annotation cost of 2. We compare with this variant in the experiment.}
    \label{tab:setup_summary}
    \vskip 0.2in
    \centering
\begin{scriptsize}
% \setlength{\tabcolsep}{1.5pt}
\begin{sc}
    \begin{tabular}{l|cc|cc}
    \toprule
        \textbf{Method} & $\responseone$ & $\responsetwo$ & Sampling Cost & Annotation Cost \\ 
        \midrule
        \textit{Vanilla} \citep{rafailov2023direct} & $\policytheta$ & $\policytheta$ & 2 & 2 \\
        \textit{Best-of-N} \citep{xiong2024iterative}, N=8 & best of $\policytheta$ & worst of $\policytheta$ & 8 & 8* \\
        \textit{Best-of-N} (with DPO reward), N=8 & best of $\policytheta$ & worst of $\policytheta$ & 8 & 2 \\
        \textit{Hybrid} \citep{xie2024exploratory} & $\policytheta$ & \policyref & 2 & 2\\
        % \textit{SEA} \citep{liu2024sample} & & & 20 & 2 \\
        \midrule
        \textit{PILAF} (OURS) & $\policythetapos / \policytheta$ & $\policythetaneg / \policytheta$ & 3 & 2\\
    \bottomrule
    \end{tabular}
\end{sc}
\end{scriptsize}
% \vspace{-0.8em}
\end{table*}
    
    
	
	% \paragraph{Importance sampling method}
	
	% In the importance sampling method, we first generate \(m\) responses from \(\policytheta\) for each prompt. These responses, along with the prompts, are fed into the reward model to obtain reward values. As the generation method involves reweighting by $\exp \big\{\rewardtheta(\prompt, \response) \big\}$, we directly use these values to calculate and sample from the weighted probability distribution over the \(m\) sampled responses. In this process, \(m-2\) samples are discarded.
	
	% \yaqiadd{I am still unclear about the weights usedâ€”could you provide more technical details here or in the Appendix?}
	
	% \paragraph{Direct sampling method}
	
	% Additionally, we propose a direct sampling approach, where the logits are modified token-wise during generation.