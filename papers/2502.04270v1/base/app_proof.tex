%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%	\section{Auxiliary results}
%	
%	\begin{corollary}
%		Assume gradient Lipchitz $\GradLip$.
%		\begin{align*}
%			\scalarvalue(\policyt{t+1})
%			\; \geq \; \scalarvalue(\policyt{t}) \, + \, \frac{1}{2 \GradLip} \, \norm[\big]{\gradtheta \scalarvalue(\policyt{t})}_2^2 \, - \, \frac{\Partitionthetabar}{2 \GradLip \parabeta^2} \, \norm{\Term_2}_2^2
%		\end{align*}
%	\end{corollary}
%	
%	\begin{proposition}
%		\label{thm:Hess}
%		Suppose that for any \mbox{$\reward \in \RewardSp$}, \mbox{$0 \leq \reward(\prompt, \response) \leq \Radius$}. Moreover, suppose $\hesstheta \reward_{\paratheta}(\prompt, \response)$ is positive semidefinite for any $(\prompt, \response) \in \PromptSp \times \ResponseSp$.
%		Then both terms
%		\begin{align*}
%			\Exp_{\prompt \sim \promptdistr, \, \response \sim \policy_{\paratheta}(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big]
%			\qquad \mbox{and} \qquad
%			\kull{\policytheta}{\policyref}
%		\end{align*}
%		are convex in parameter $\paratheta$.
%		Therefore, the objective function $\scalarvalue(\policytheta)$ is a difference-of-convex function.
%	\end{proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{Proof of Main Results \yaqidone}
    \label{app:proof:main}

        
    
		This section provides the proofs of the main results from \Cref{sec:theory}, covering both optimization and statistical aspects.
		In \Cref{sec:proof:thm:grad}, we prove \Cref{thm:grad}, which establishes the gradient alignment property. For the statistical results, \Cref{sec:proof:thm:stat} begins with the proofs of \Cref{thm:asymp_full,thm:asymp}, which derive the asymptotic distribution of the estimated parameter $\parathetahat$, and concludes with the proof of \Cref{lemma:hess_scalarvalue}, analyzing the asymptotic behavior of the value gap~\mbox{$\scalarvalue(\policystar) - \scalarvalue(\policyhat)$}.
	
	\subsection{Optimization Considerations: Proof of Theorem~\ref{thm:grad} \yaqidone}
	\label{sec:proof:thm:grad}

        % \yaqitbd

        We begin by presenting a rigorous restatement of \Cref{thm:grad}, formally detailed in \Cref{thm:grad_full} below.

        \begin{theorem}[Gradient structure in DPO training]
			\label{thm:grad_full}
			Consider the expected loss function $\Loss(\paratheta)$ during the DPO training phase. Using data collected from our poposed response sampling scheme $ \responsedistr $, the gradient of $ \Loss(\paratheta) $ satisfies
			\begin{align*}
				\gradtheta \Loss(\paratheta) \; = \;
				- \, \frac{\parabeta}{\Partitionthetabar} \, \gradtheta \scalarvalue(\policytheta) \, + \, \Term_2 \, ,
			\end{align*}
			where the constant $ \Partitionthetabar $ is defined in equation~\eqref{eq:weight}, and the term $ \Term_2% = \bigO( \norm{\rewardtheta - \rewardstar}^2 ) 
			$ represents a second-order error.
			
			To control term $ \Term_2 $, assume the following uniform bounds: 
            \begin{itemize}
                \item[(i)] \mbox{$\!\supnorm{\rewardstar} \leq \Radius$}.
                \item[(ii)] For any policy \mbox{$\policytheta \in \PolicySp$}, the induced reward $\rewardtheta$ satisfies 
                \begin{align*}
                    \supnorm{\rewardtheta} \leq \Radius \qquad \mbox{and} \qquad \sup\nolimits_{\prompt, \response} \, \norm{\gradtheta \rewardtheta (\prompt, \response)}_2 \leq \RadiusGrad \, .
                \end{align*}
            \end{itemize}
			Under these conditions, $ \Term_2 $ is bounded as
			\vspace{-.5em}
			\begin{align*}
                \norm{\Term_2}_2 \leq 
                \Const{} \, \cdot \, \Exp_{\prompt \sim \promptdistr, \, \responseone, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
				\bigg[ \, \Big\{ \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big)
                - \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\}^2 \bigg] \, ,
			\end{align*}
			where the constant $\Const{}$ is given by $\Const{} = 0.1 \, (1 + e^{2\Radius}) \, \RadiusGrad \big/ \Partitionthetabar$.
		\end{theorem}
	
	The proof of \Cref{thm:grad_full} is structured into three sections. In \Cref{sec:proof:thm:grad_1}, we lay the foundation by presenting the key components, including the explicit expressions for the gradients $\gradtheta \scalarvalue(\policytheta)$ and $\gradtheta \Loss(\paratheta)$, as well as for the sampling density~$\responsedistravg$.
	Then \Cref{sec:proof:thm:grad_2} establishes the connection between $\gradtheta \scalarvalue(\policytheta)$ and $\gradtheta \Loss(\paratheta)$ by leveraging these results, completing the proof of \Cref{thm:grad}. 
	Finally, in \Cref{sec:proof:thm:grad_3}, we provide a detailed derivation of the form of density function~$\responsedistravg$.
	
	\subsubsection{Building Blocks \yaqidone}
	\label{sec:proof:thm:grad_1}
	
	To establish \Cref{thm:grad}, which uncovers the relationship between the gradients of the expected value $\scalarvalue(\policytheta)$ and the negative log-likelihood function $\Loss(\paratheta)$, the first step is to derive explicit expressions for the gradients of both functions. The results are presented in \Cref{lemma:grad_scalarvalue,lemma:grad_loss}, with detailed proofs provided in \Cref{sec:proof:lemma:grad_scalarvalue,sec:proof:lemma:grad_loss}, respectively.
	\begin{lemma}[Gradient of value $\scalarvalue(\policytheta)$]
		\label{lemma:grad_scalarvalue}
		For any $\policytheta$ in the parameterized policy class $\PolicySp$, the gradient of the expected value~$\scalarvalue(\policytheta)$ satisfies
		%			\begin{subequations}
			\begin{multline}
				\label{eq:grad_scalarvalue}
				\gradtheta \scalarvalue(\policytheta)
				\; = \; \frac{1}{2 \parabeta} \, \Exp_{\prompt \sim \promptdistr; \; \responseone, \responsetwo \sim \policytheta(\cdot \mid \prompt)} 
				\bigg[ \Big\{ \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \\ 
				\cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \bigg] \, .
			\end{multline}
			%			\end{subequations}
	\end{lemma}
	
	
	
	\begin{lemma}[Gradient of the loss function $\Loss(\paratheta)$]
		\label{lemma:grad_loss}
		For any $\policytheta$ in the parameterized policy class $\PolicySp$ and any sampling distribution $\responsedistr$ of the responses, the gradient of the negative log-likelihood function $\Loss(\paratheta)$ is given by
		\begin{subequations}
			\begin{multline}
				\label{eq:gradLoss_BT_0}
				\gradtheta \Loss(\paratheta) \; = \; - \, \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
				\bigg[ \, \weight(\prompt) \cdot \Big\{ \sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \\ 
				\cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \bigg] \, ,
			\end{multline}
			where the average density $\responsedistravg$ is defined as
			\begin{align}
				\label{eq:def_responsedistravg_0}
				\responsedistravg(\responseone, \responsetwo \mid \prompt) 
				\; \defn \; \frac{1}{2} \, \big\{ \responsedistr(\responseone, \responsetwo \mid \prompt) + \responsedistr(\responsetwo, \responseone \mid \prompt) \big\}
			\end{align}
		\end{subequations}
			as previously introduced in \cref{eq:def_responsedistravg}.
	\end{lemma}
	
	In \Cref{lemma:grad_loss}, we observe that the gradient $\gradtheta \Loss(\paratheta)$ is expressed as an expectation over the probability distribution $\responsedistravg$. By applying the sampling scheme outlined in \Cref{sec:sampling}, we can derive a more detailed representation of $\gradtheta \Loss(\paratheta)$. This refined form will reveal its close relationship to the gradient $\gradtheta \scalarvalue(\policytheta)$ given in expression \eqref{eq:grad_scalarvalue}.
	
	Before moving forward, it is crucial for us to first derive the explicit form of $\responsedistravg$. Specifically, we claim that the distribution~$\responsedistravg$ satisfies the following property
	\begin{align}
		\label{eq:responsedistravg}
		\frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)} 
		& \; = \; \frac{1}{2 \, \{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}}
		\cdot \frac{1}{\divsigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big)} \, ,
	\end{align}
	where $\divsigmoid$ denotes the derivative of the sigmoid function $\sigmoid$, given by
	\begin{align}
		\label{eq:divsigmoid}
		\divsigmoid(z) \; = \; \frac{1}{( 1 + \exp(-z) )( 1 + \exp(z) )} \; = \; \sigmoid(z) \, \sigmoid(-z)
		\qquad \mbox{for any $z \in \Real$}  \, .
	\end{align}
	With these key components in place, we are now prepared to prove \Cref{thm:grad}.
	
	
	\subsubsection{Derivation of Theorem~\ref{thm:grad} \yaqidone}
	\label{sec:proof:thm:grad_2}
	
	With the tools provided by \Cref{lemma:grad_scalarvalue,lemma:grad_loss} and the sampling density expression in \eqref{eq:responsedistravg}, we are now ready to prove \Cref{thm:grad}.
	
	We begin by applying \Cref{lemma:grad_loss} and reformulating equation~\eqref{eq:gradLoss_BT_0} as
	\begin{align}
		\gradtheta \Loss(\paratheta) \; = \; - \, \Exp_{\prompt \sim \promptdistr; \; \responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
		\bigg[ \, & \weight(\prompt) \cdot \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)} \notag \\
		& \cdot \Big\{ \sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \notag \\ 
		& \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \bigg] \,.
		\label{eq:gradLoss}
	\end{align}
	Substituting the density ratio from equation~\eqref{eq:responsedistravg} into expression \eqref{eq:gradLoss} and incorporating the weight function $\weight(\prompt)$ defined in equation \eqref{eq:weight}, we obtain 
	\begin{align}
		\gradtheta \Loss(\paratheta) \; = \; - \frac{1}{2 \, \Partitionthetabar} \, \Exp_{\prompt \sim \promptdistr; \; \responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
		\Bigg[ \, & 
		\frac{\sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big)}{\divsigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big)}  \notag  \\
		& \qquad \qquad \qquad \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \Bigg] \, .  \label{eq:gradLoss_0}
	\end{align}
	Using the intuition that the first-order Taylor expansion
	\begin{align*}
		\frac{\sigmoid(z^{\star}) - \sigmoid(z)}{\divsigmoid(z)} \; = \; (z^{\star} - z) + \bigO\big((z^{\star} - z)^2\big)
	\end{align*}
	is valid when $z \to z^\star$, with $z^\star \defn \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo)$ and $z \defn \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo)$, we find that
	\begin{align*}
		& \frac{\sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big)}{\divsigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big)}  \\
		& \; = \; \Big\{ \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \; + \; \mbox{second-order term}.
	\end{align*}
	Reformulating equation~\eqref{eq:gradLoss_0} in this context, we rewrite it as
    \begin{align}
		\gradtheta \Loss(\paraphi) 
		& = - \, \frac{1}{2 \Partitionthetabar} \, \Exp_{\, \begin{subarray}{l} \\ \prompt \sim \promptdistr; \\ \responseone, \responsetwo \sim \policytheta(\cdot \mid \prompt) \end{subarray}}
		\Bigg[ \, \Big\{ \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \notag  \\
		& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \Bigg]
		+ \Term_2 \, , \label{eq:gradLoss_1}
	\end{align}
	where $\Term_2$ represents the second-order residual term related to the estimation error $\rewardtheta - \rewardstar$.
	By applying \Cref{lemma:grad_scalarvalue}, we observe that the primary term in equation~\eqref{eq:gradLoss_1} aligns with the direction of $\gradtheta \scalarvalue(\policytheta)$, resulting in
	\begin{align}
		\label{eq:gradLoss_final}
		\gradtheta \Loss(\paraphi) 
		& = - \, \frac{\parabeta}{\Partitionthetabar} \, \gradtheta \scalarvalue(\policytheta)
		+ \Term_2 \, .
	\end{align}

	
	Next, we proceed to control the second-order term $\Term_2$.
	The conditions
	\begin{align*}
		\supnorm{\rewardstar}, \supnorm{\rewardtheta} \leq \Radius
		\qquad \mbox{and} \qquad \sup\nolimits_{(\prompt, \response) \in \PromptSp \times \ResponseSp} \norm{\gradtheta \rewardtheta (\prompt, \response)}_2 \leq \RadiusGrad,
	\end{align*}
	lead to the bound
	\begin{align*}
		\abs[\Big]{ \, \frac{\sigmoid(z^{\star}) - \sigmoid(z)}{\divsigmoid(z)} - (z^{\star} - z) }
		\; \leq \;  0.1 \, (1 + e^{2\Radius}) \cdot (z^{\star} - z)^2 \, ,
	\end{align*}
	which in turn implies
	\begin{align}
		& \norm{\Term_2}_2
        \notag \\
        \label{eq:gradLoss_Term2}
        & \; \leq \;  \frac{0.1 \, (1 + e^{2\Radius}) \, \RadiusGrad}{\Partitionthetabar} \, \Exp_{\prompt \sim \promptdistr; \; \responseone, \responsetwo \sim \policytheta(\cdot \mid \prompt)} 
        \bigg[ \, \Big\{ \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\}^2 \bigg] \, .
	\end{align}
	
	Finally, combining equation~\eqref{eq:gradLoss_Term2} with equation~\eqref{eq:gradLoss_final}, we conclude the proof of \Cref{thm:grad}.
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\subsubsection{Proof of Claim~\eqref{eq:responsedistravg}}
		\label{sec:proof:thm:grad_3}
		
		The remaining step in the proof of \Cref{thm:grad} is to verify the expression for the density ratio in equation~\eqref{eq:responsedistravg}.
		
		Based on the sampling scheme described in \Cref{sec:sampling}, we find that the sampling distribution for the response satisfies
		\begin{align}
			\label{eq:responsedistr_0}
			\responsedistr \big( \responseone, \responsetwo \bigm| \prompt \big)
			& \; = \; \{ 1 - \sampleprob(\prompt) \} \cdot \policytheta(\responseone \mid \prompt) \,  \policytheta(\responsetwo \mid \prompt)
			\, + \, \sampleprob(\prompt) \cdot \policythetapos(\responseone \mid \prompt) \,  \policythetaneg(\responsetwo \mid \prompt) \, ,
		\end{align}
		where the probability $\sampleprob(\prompt)$ is defined as
		\begin{align*}
			\sampleprob(\prompt) = \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) / \{1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}
		\end{align*}
		and the policies $\policythetapos$ and $\policythetaneg$ are specified in equations~\eqref{eq:def_policythetapos}~and~\eqref{eq:def_policythetaneg}, respectively.
		This allows us to simplify equation~\eqref{eq:responsedistr_0} to
		\begin{align*}
			\responsedistr \big( \responseone, \responsetwo \bigm| \prompt \big)
			& \; = \; \frac{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)}{1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt)} \, \Big\{ 1 + \exp\big\{ \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big\} \Big\} \, .
		\end{align*}
		Similarly, we derive an expression for $\responsedistr ( \responsetwo, \responseone \mid \prompt )$.
		By averaging the two expressions, for $\responsedistr ( \responseone, \responsetwo \mid \prompt )$ and $\responsedistr ( \responsetwo, \responseone \mid \prompt )$, we obtain
		\begin{align*}
%			\label{eq:responsedistravg_ratio}
			& \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)}  \\
			& = \frac{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)}{2 \, \{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}} \, \Big\{ 2 + \exp\big\{ \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big\} + \exp\big\{ \rewardtheta(\prompt, \responsetwo) - \rewardtheta(\prompt, \responseone) \big\} \Big\} \, .
		\end{align*}
		Rewriting this expression using the formula for $\divsigmoid$ in equation~\eqref{eq:divsigmoid}, we arrive at
		\begin{align*}
			& \big\{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \big\} \cdot \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)}  \\
			& \; = \; \frac{1}{2} \, \Big\{ 1 + \exp\big\{ \rewardtheta(\prompt, \responsetwo) - \rewardtheta(\prompt, \responseone) \big\} \Big\}  \Big\{ 1 + \exp\big\{ \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big\} \Big\}  \\
			& \; = \; \frac{1}{2 \, \divsigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big)} \, .
		\end{align*}
		Finally, rearranging terms, we recover equation~\eqref{eq:responsedistravg}, completing this part of the proof.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsection{Statistical Considerations \yaqidone}
	\label{sec:proof:thm:stat}



        In this section, we present the proofs for \Cref{thm:asymp,lemma:hess_scalarvalue,thm:asymp_full} from \Cref{sec:theory_stat}. 
        We start with the proof of \Cref{thm:asymp_full} in \Cref{sec:proof:thm:asymp_full}, with a rigorous restatement provided in \Cref{thm:asymp_full_full} below.
    		\begin{theorem}
			\label{thm:asymp_full_full}
%			We take $\weight(\prompt) \equiv 1$.
			Assume the reward model $\rewardstar$ in the BT model~\eqref{eq:BT} satisfies $\rewardstar = \reward_{\parathetastar}$ for some parameter $\parathetastar$.
			Assume that $\parathetahat$ minimizes the loss function $\Losshat(\paratheta)$ in the sense that $\sqrt{\numobs} \, \gradtheta \Losshat (\parathetahat) \convergep \veczero$ and that $\parathetahat \convergep \parathetastar$ as the sample size $\numobs \rightarrow \infty$.
			Additionally, suppose the reward function $\rewardtheta(\prompt, \response)$, its gradient $\gradtheta \rewardtheta(\prompt, \response)$ and its Hessian $\hesstheta \rewardtheta(\prompt, \response)$ are uniformly bounded and Lipchitz continuous with respect to $\paratheta$, for all $(\prompt, \response) \in \PromptSp \times \ResponseSp$.
			
			Under these conditions, the estimate $\parathetahat$ asymptotically follows a Gaussian distribution
			\begin{align*}
				\sqrt{\numobs} \; ( \parathetahat - \parathetastar)
				\; \stackrel{d}{\longrightarrow} \; \Gauss( \veczero, \CovOmega )
				\qquad \mbox{as $\numobs \rightarrow \infty$} \, .
			\end{align*}
			We have an estimate of the covariance matrix $\CovOmega$:
            \begin{align*}
                \CovOmega \; \preceq \; \supnorm{\weight} \cdot \CovOpstar^{-1} \, .
            \end{align*}
            For a general sampling scheme $\responsedistr$ chosen, the matrix~$\CovOpstar$ is given by
			\begin{align*}
                % \label{eq:def_CovOpstar_simple}
				\CovOpstar \; \defn \;
				& \Exp_{\prompt \sim \promptdistr, \, (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
			\Big[ \, \weight(\prompt) \cdot \Var\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) \cdot \grad \, \grad^{\top} \Big] \, ,
				%\label{eq:def_CovOpstar}
			\end{align*}
			where the expectation is taken over the distribution
			%\vspace{-.3em}
			\begin{subequations}
				\begin{align*}
					%\label{eq:def_responsedistravg}
					\responsedistravg(\responseone, \responsetwo \mid \prompt) 
					\defn \frac{1}{2} \, \big\{ \responsedistr(\responseone, \responsetwo \mid \prompt) + \responsedistr(\responsetwo, \responseone \mid \prompt) \big\} \, .
				\end{align*} %~ \vspace{-1.8em} \\
			The variance term is specified as
				\begin{align*}
					& \Var\big(\indicator\{\responseone \; = \; \responsewin\} \mid \prompt, \responseone, \responsetwo \big)
					%\label{eq:def_var}
					= \sigmoid\big( \rewardstar(\prompt, \responseone) - \rewardstar(\prompt, \responsetwo) \big) \, \sigmoid\big( \rewardstar(\prompt, \responsetwo) - \rewardstar(\prompt, \responseone) \big)
					%\notag
				\end{align*}
			and the gradient difference $\grad$ is defined as
				\begin{align*}
					%\label{eq:def_grad}
					\grad \; \defn \; \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \, .
				\end{align*}
			\end{subequations}
		\end{theorem}

    \Cref{thm:asymp_full_full} establishes the asymptotic distribution of the estimated parameter $\parathetahat$, which serves as the foundation for the subsequent results. 
	Next, we show that \Cref{thm:asymp} directly follows as a corollary of \Cref{thm:asymp_full_full}, with the detailed derivation provided in \Cref{sec:proof:thm:asymp}. Finally, in \Cref{sec:proof:lemma:hess_scalarvalue}, we prove \Cref{lemma:hess_scalarvalue}, which describes the asymptotic behavior of the value gap $\scalarvalue(\policystar) - \scalarvalue(\policyhat)$.
		
	\subsubsection{Proof of Lemma~\ref{thm:asymp_full} (Theorem~\ref{thm:asymp_full_full}) \yaqidone}
	\label{sec:proof:thm:asymp_full}
	
%	\paragraph{(a) Proof of \Cref{thm:asymp_full}:}

	In this section, we analyze the asymptotic distribution of the estimated parameter $\parathetahat$ for a general sampling distribution $\responsedistr$. The parameter $\parathetahat$ is obtained by solving the optimization problem
	\begin{align*}
		{\rm minimize}_{\paratheta} \quad
		\Losshat(\paratheta) \; \defn \;
		- \frac{1}{\numobs} \sum_{i=1}^{\numobs} \, \weight(\prompti{i}) \cdot \log \sigmoid \Big( \rewardtheta\big(\prompti{i}, \responsewini{i}\big) - \rewardtheta\big(\prompti{i}, \responselosei{i}\big) \Big) \, .
	\end{align*}
	We assume the optimization is performed to sufficient accuracy such that $\gradtheta \Losshat(\parathetahat) = \smallop\big(\numobs^{-\frac{1}{2}}\big)$.
	Under this condition, $\parathetahat$ qualifies as a $Z$-estimator.
	To study its asymptotic behavior, we use the master theorem for $Z$-estimators \citep{kosorok2008introduction}, the formal statement of which is provided in \Cref{thm:master} in \Cref{sec:master}.
	
	To apply the master theorem, we set $\Psi \defn \gradtheta \Loss$ and $\Psi_{\numobs} \defn \gradtheta \Losshat$ and verify the conditions. In particular, the smoothness condition~\eqref{eq:master_cond} in \Cref{thm:master} translates to the following equation in our context:
    \begin{align}
    	\label{eq:master_cond_proof}
    	& \sqrt{n} \, \big\{ \gradtheta \Losshat (\parathetahat) - \gradtheta \Loss(\parathetahat) \big\} - \sqrt{n} \, \big\{ \gradtheta \Losshat (\parathetastar) - \gradtheta \Loss (\parathetastar) \big\}  
    	\; = \; \smallop \big( 1 + \sqrt{n} \, \norm{ \parathetahat - \parathetastar }_2 \big) \, .
    \end{align}
    This condition follows from the second-order smoothness of the reward function $\rewardtheta$ with respect to $\paratheta$. A rigorous proof is provided in \Cref{sec:proof:eq:master_cond_proof}.
    

	We now provide the explicit form of the derivative $\dot{\Psi}_{\parathetastar} = \hesstheta \Loss(\parathetastar)$, as captured in the following lemma. The proof of this result can be found in \Cref{sec:proof:lemma:hess_loss}.
	\begin{lemma}
		\label{lemma:hess_loss}
		The Hessian matrix of the population loss $\Loss(\paratheta)$ at $\paratheta = \parathetastar$ is
		\begin{align}
			\label{eq:hess_loss}
			\hesstheta \Loss(\parathetastar) \; = \; \CovOpstar \, ,
		\end{align}
		where the matrix $\CovOpstar$ is defined in equation~\eqref{eq:def_CovOpstar}.
	\end{lemma}

	
	Next, we analyze the asymptotic behavior of the gradient $\gradtheta \Losshat(\parathetastar)$.
	The proof is deferred to \Cref{sec:proof:lemma:grad_loss_stat}.
	\begin{lemma}
		\label{lemma:grad_loss_stat}
		The gradient of the empirical loss $\Losshat(\paratheta)$ at $\paratheta = \parathetastar$ satisfies
		\begin{subequations}
		\begin{align}
			\sqrt{\numobs} \, \big( \gradtheta \Losshat(\parathetastar) - \gradtheta \Loss(\parathetastar) \big)
			\; \stackrel{d}{\longrightarrow} \; \Gauss(\veczero, \CovOptil)
			\qquad \mbox{as $\numobs \rightarrow \infty$},
		\end{align}
        where the covariance matrix $\CovOptil \in \Real^{\Dim \times \Dim}$ is bounded as follows:
        \begin{align}
        	\label{eq:CovOptil}
        	\CovOptil \; \preceq \; \supnorm{\weight} \cdot \CovOpstar \, ,
        \end{align}
        \end{subequations}
        with $\CovOpstar$ defined in equation~\eqref{eq:def_CovOpstar}.
	\end{lemma}
	
	Combining these results, and assuming $\CovOpstar$ is nonsingular, the master theorem (\Cref{thm:master}) yields the asymptotic distribution of $\parathetahat$:
	\begin{align*}
		\sqrt{\numobs} \, \big( \parathetahat - \parathetastar \big)
		\; \converged \; \Gauss\big( \veczero, \CovOpstar^{-1} \CovOptil \CovOpstar^{-1} \big) \, .
	\end{align*}
	Furthermore, from the bound~\eqref{eq:CovOptil}, the covariance matrix $\CovOmega ; \defn \CovOpstar^{-1} \CovOptil \CovOpstar^{-1}$ satisfies
	\begin{align*}
		 \CovOmega \; = \CovOpstar^{-1} \CovOptil \CovOpstar^{-1}  \; \preceq \; \supnorm{\weight} \cdot \CovOpstar^{-1} \, .
	\end{align*}
	Therefore, we have established the asymptotic distribution of $\parathetahat$, completing the proof of \Cref{thm:asymp_full}.
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%	\paragraph{(b) Proof of \eqref{eq:def_CovOpstar_simple}}
	\subsubsection{Proof of Theorem~\ref{thm:asymp}}
	\label{sec:proof:thm:asymp}
	
	\Cref{thm:asymp} is a direct corollary of \Cref{thm:asymp_full}, using our specific choice of sampling distribution $\responsedistr$. To establish this, we demonstrate how the general covariance matrix $\CovOpstar$ in equation~\eqref{eq:def_CovOpstar} simplifies to the form in equation~\eqref{eq:def_CovOpstar_simple} under our proposed sampling scheme.

    To establish the result in this section, we impose the following regularity condition:
    There exists a constant $\Const{} \geq 1$ satisfying
    \begin{align}
        \label{eq:last_cond}
        \Var_{\rewardtheta}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big)
        \; \leq \; \Const{} \cdot \Var_{\rewardstar}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) 
    \end{align}
    for any prompt $\prompt \in \PromptSp$ and responses $\responseone, \responsetwo \in \ResponseSp$.
    Here $\Var_{\rewardtheta}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big)$ denotes the conditional variance under the BT model~\eqref{eq:BT}, when the implicit reward function $\rewardstar$ is replaced by $\rewardtheta$. The term \mbox{$\Var_{\rewardstar}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big)
    \equiv$} \mbox{$\Var\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) $} represents the conditional variance under the ground-truth BT model, where the reward function is given by $\rewardstar$.
	
	We begin by leveraging the property of the sampling distribution $\responsedistr$ from equation~\eqref{eq:responsedistravg} and the derivative $\divsigmoid$ of the sigmoid function $\sigmoid$, given in equation~\eqref{eq:divsigmoid}. Specifically, we find that
	\begin{align*}
		%\label{eq:responsedistravg2_original}
		& \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)} \notag  \\
		& 
		\; = \; \frac{1}{2 \, \{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}}
		\cdot \frac{1}{\sigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big) \, \sigmoid \big( \rewardtheta(\prompt, \responsetwo) - \rewardtheta(\prompt, \responseone) \big)}  \\
        & \; = \; \frac{1}{2 \, \{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}}
		\cdot \frac{1}{\Var_{\rewardtheta}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big)} \, .
	\end{align*}
    We then apply condition~\eqref{eq:last_cond} and derive
        \begin{equation} 
		 \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)} \; \geq \; \frac{\Const{}^{-1}}{2 \, \{ 1 + \Partitionthetapos(\prompt) \, \Partitionthetaneg(\prompt) \}} \cdot \frac{1}{\Var_{\rewardstar}\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big)} \, .
         \label{eq:responsedistravg2}
	\end{equation}
	Next, substituting this result~\eqref{eq:responsedistravg2} into equation~\eqref{eq:def_CovOpstar}, alongside the weight function $\weight(\prompt)$ from equation~\eqref{eq:weight}, we reform $\CovOpstar$ as
	\begin{align}
		\CovOpstar
		& \; = \; \Exp_{\prompt \sim \promptdistr; \; \responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
		\bigg[ \, \frac{\responsedistravg ( \responseone, \responsetwo \mid \prompt )}{\policytheta(\responseone \mid \prompt) \, \policytheta(\responsetwo \mid \prompt)} \cdot \weight(\prompt) \cdot \Var\big(\indicator\{\responseone = \responsewin\} \bigm| \prompt, \responseone, \responsetwo \big) \cdot \grad \, \grad^{\top} \bigg]  \notag \\
		\label{eq:def_CovOpstar_2}
		& \; \succeq \; \frac{1}{2 \, \Const{} \, \Partitionthetabar} \, \Exp_{\prompt \sim \promptdistr; \; \responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
		\big[ \, \grad \, \grad^{\top} \big] \, .
	\end{align}
	The conditional expectation of $\grad \grad^\top$ simplifies as
    \begin{align*}
    	& \Exp_{\responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
    	\big[ \, \grad \grad^{\top} \bigm| \prompt\big]  \\
    	& \; = \; \Exp_{\responseone, \, \responsetwo \sim \policytheta(\cdot \mid \prompt)}
    	\Big[ \big\{ \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \big\} \big\{ \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \big\}^{\top} \Bigm| \prompt\Big]  \\
    	& \; = \; 2 \cdot \Exp_{\response \sim \policytheta(\cdot \mid \prompt)}
    	\Big[ \, \gradtheta \rewardstar(\prompt, \response) \, \gradtheta \rewardstar(\prompt, \response)^{\top} \Bigm| \prompt\Big] \\
        & \qquad \qquad - 2 \cdot \Exp_{\response \sim \policytheta(\cdot \mid \prompt)} \big[ \, \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt\big]  \, \Exp_{\response \sim \policytheta(\cdot \mid \prompt)}
    	\big[ \,\gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big]^{\top}  \\
    	& \; = \; 2 \cdot \Cov_{\response \sim \policytheta(\cdot \mid \prompt)} \big[ \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big] \, .
    \end{align*}
	Substituting this result into equation~\eqref{eq:def_CovOpstar_2}, we arrive at the conclusion that
	\begin{align*}
		\CovOpstar \; \succeq \; \frac{1}{\Const{} \, \Partitionphibar} \, \Exp_{\prompt \sim \promptdistr} \Big[ \Cov_{\response \sim \policystar(\cdot \mid \prompt)} \big[ \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big] \Big] \, ,
	\end{align*}
	which matches the simplified form in equation~\eqref{eq:def_CovOpstar_simple} as stated in \Cref{thm:asymp}.

		
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
    \subsubsection{Proof of Theorem~\ref{lemma:hess_scalarvalue} \yaqidone}
    \label{sec:proof:lemma:hess_scalarvalue}

	\paragraph{Gradient $ \gradtheta \scalarvalue(\policystar) $ and Hessian $\hesstheta \scalarvalue(\policystar)$:}
	
    The equality $ \gradtheta \scalarvalue(\policystar) = 0$ follows directly from the gradient expression~\eqref{eq:grad_scalarvalue0} for $ \gradtheta \scalarvalue(\policytheta) $, evaluated at $ \paratheta = \parathetastar $ with~\mbox{$ \rewardtheta = \rewardstar $}.
    
    The proof of the Hessian result, $ \hesstheta \scalarvalue(\policystar) = - (1 / \parabeta) \cdot \CovOpstar $, involves straightforward but technical differentiation of equation~\eqref{eq:grad_scalarvalue0}. For brevity, we defer this proof to \Cref{sec:proof:eq:hessscalarvalue}.
  
  	\paragraph{Asymptotic Distribution of Value Gap $ \scalarvalue(\policystar) - \scalarvalue(\policyhat) $:}
    To understand the behavior of the value gap $ \scalarvalue(\policystar) - \scalarvalue(\policyhat) $, we start by applying a Taylor expansion of $ \scalarvalue(\policytheta) $ around $ \parathetastar $. This gives
	\begin{align*}
%		\label{eq:Taylor_scalarvalue}
		\scalarvalue(\policystar) - \scalarvalue(\policyhat)
		\; = \; \gradtheta \scalarvalue(\policystar)^{\top} (\parathetastar - \parathetahat) - \frac{1}{2} (\parathetastar - \parathetahat)^{\top} \hesstheta \scalarvalue(\policystar) (\parathetastar - \parathetahat) + \smallo\big( \norm{\parathetastar - \parathetahat}_2^2 \big) \, .
	\end{align*}
	By substituting $ \gradtheta \scalarvalue(\policystar) = \veczero $ (a direct result of the optimality of $ \policystar $), the linear term vanishes. Introducing the shorthand $ \HessMt \defn -\hesstheta \scalarvalue(\policystar) = (1 / \parabeta) \cdot \CovOpstar $, the expression simplifies to
	\begin{align}
		\label{eq:Taylor_scalarvalue}
		\scalarvalue(\policystar) - \scalarvalue(\policyhat)
		\; = \; \frac{1}{2} \, (\parathetahat - \parathetastar)^{\top} \HessMt \, (\parathetahat - \parathetastar) + \smallo\big( \norm{\parathetahat - \parathetastar}_2^2 \big) \, .
	\end{align}
	When the sample size $ \numobs $ is sufficiently large, $ \parathetahat $ approaches $ \parathetastar $, making the higher-order term negligible. Therefore, the value gap is dominated by the quadratic form.
	
	From \Cref{thm:asymp}, we know the parameter estimate $ \parathetahat $ satisfies
	\begin{align*}
	\sqrt{\numobs} \, (\parathetahat - \parathetastar)
	\;\stackrel{d}{\longrightarrow}\;
	\Gauss(\veczero, \CovOmega).
	\end{align*}
	Substituting this result into the quadratic approximation of the value gap, we find that the scaled value gap has the asymptotic distribution
	\begin{align}
		\label{eq:gap_distr}
		\numobs \cdot \{ \scalarvalue(\policystar) - \scalarvalue(\policyhat) \}
		 \; \stackrel{d}{\longrightarrow} \; \frac{1}{2} \, \vecz^{\top} \CovOmega^{\frac{1}{2}} \HessMt \CovOmega^{\frac{1}{2}} \vecz 
		 \; \nfed \bX
		 \qquad \mbox{where $\vecz \sim \Gauss(\veczero, \IdMt)$}.
	\end{align}
	This approximation provides a clear intuition: the value gap is asymptotically driven by a weighted chi-squared-like term involving the covariance structure $ \CovOmega $ and the Hessian-like matrix $ \HessMt $.
	
	To rigorously establish this result, we will apply Slutsky’s theorem. The full proof is presented in \Cref{sec:proof:gap_distr}.
	
	\paragraph{Bounding the Chi-Square Distribution:}
	
	To bound the random variable $ \bX $, we first leverage the estimate of the covariance matrix $ \CovOmega $ provided by \Cref{thm:asymp}:
	\begin{align*}
		\CovOmega \; \preceq \; \Const{} \, \Partitionthetabar \, \supnorm{\weight} \cdot \CovOpstar^{-1},
	\end{align*}
    where the constant $\Const$ comes from condition~\eqref{eq:last_cond}.
	It follows that the matrix $ \CovOmega^{\frac{1}{2}} \HessMt \CovOmega^{\frac{1}{2}} $ appearing in equation~\eqref{eq:gap_distr} can be bounded as
	\begin{align*}
		\CovOmega^{\frac{1}{2}} \HessMt \CovOmega^{\frac{1}{2}} 
		\; \preceq \;  \Const \, \supnorm{\weight} \cdot \CovOpstar^{-\frac{1}{2}} \HessMt \CovOpstar^{-\frac{1}{2}} \; = \; \Const \cdot \frac{\Partitionthetabar \, \supnorm{\weight}}{\parabeta} \cdot \IdMt
		\; = \; \Const \cdot \frac{1 + \supnorm{\Partitionthetapos \Partitionthetaneg}}{\parabeta}
		\cdot \IdMt \, .
	\end{align*}
	Here the last equality uses the definition of the weight function $ \weight $ from equation~\eqref{eq:weight}. Substituting this bound into the quadratic form, we derive
	\begin{align*}
		\bX
		\; = \; \frac{1}{2} \, \vecz^{\top} \CovOmega^{\frac{1}{2}} \HessMt \CovOmega^{\frac{1}{2}} \vecz 
		\; \leq \; \Const \cdot \frac{1 + \supnorm{\Partitionthetapos \Partitionthetaneg}}{2\parabeta}
		\cdot \vecz^{\top} \vecz \, ,
	\end{align*}
	where $ \vecz \sim \Gauss(\veczero, \IdMt) $.
	Since $ \vecz^{\top} \vecz $ follows a chi-square distribution with $ \Dim $ degrees of freedom, $ \bX $ is stochastically dominated by a rescaled chi-square random variable 
	\begin{align*}
		\Const \cdot \frac{1 + \supnorm{\Partitionthetapos \Partitionthetaneg}}{2\parabeta} \cdot \chisquare_{\Dim}.
	\end{align*}
	Equivalently, we can express this dominance as
	\begin{align}
		\label{eq:gap_bd0}
		\limsup_{\numobs \rightarrow \infty} \; \Prob \bigg\{ \numobs \, \{ \scalarvalue(\policystar) - \scalarvalue(\policyhat) \} > \Const \cdot \frac{1 + \supnorm{\Partitionthetapos \Partitionthetaneg}}{2\parabeta} \cdot t \bigg\}
		\; \leq \; \Prob\big\{ \chisquare_{\Dim} > t \big\}
		\qquad \mbox{for any $t > 0$}.
	\end{align}
	This inequality, given in equation~\eqref{eq:gap_bd0}, corresponds to the first bound in equation~\eqref{eq:gap_bd}.
	
	The second inequality in equation~\eqref{eq:gap_bd} provides a precise tail bound for $\chisquare_{\Dim}$. As its proof involves more technical details, we defer it to \Cref{sec:proof:chisqtail}.
	

	


	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{Proof of Auxiliary Results \yaqidone}
    \label{app:aux}
	
	This section provides proofs of auxiliary results supporting the main theorems and lemmas. In \Cref{sec:proof:aux:thm:grad}, we present the auxiliary results required for \Cref{thm:grad}. \Cref{sec:proof:thm:asymp_aux} details the proofs of supporting results for \Cref{thm:asymp}. Finally, in \Cref{sec:proof:lemma:hess_scalarvalue_aux}, we establish the auxiliary results necessary for \Cref{lemma:hess_scalarvalue}.

	\subsection{Proof of Auxiliary Results for Theorem~\ref{thm:grad} \yaqidone}
	\label{sec:proof:aux:thm:grad}
	
		In this section, we provide the proofs of several auxiliary results that support the proof of \Cref{thm:grad}. Specifically,
		\Cref{sec:proof:lemma:grad_policy} presents the forms of the gradients of the policy~$\policytheta$ and the reward $\rewardtheta$, which serve as fundamental building blocks for deriving the lemmas.
		\Cref{sec:proof:lemma:grad_scalarvalue} analyzes the gradient of the return function $\scalarvalue(\policytheta)$, as defined in equation~\eqref{eq:objective}.
		\Cref{sec:proof:lemma:grad_loss} focuses on deriving expressions for the gradient of the negative log-likelihood function $\Loss(\paratheta)$.
	
		\subsubsection{Gradients of Policy $\policytheta$ and Reward $\rewardtheta$}
		\label{sec:proof:lemma:grad_policy}
		
		In this part, we introduce results for the gradients of policy $\policytheta$ and reward~$\rewardtheta$ with respsect to parameter~$\paratheta$, which lay the foundation of our calculations.
		
			\begin{lemma}[Gradients of policy $\policytheta$ and reward function $\rewardtheta$]
			\label{lemma:grad_policy}
			The gradients of the policy $\policytheta$ and the reward function $\rewardtheta$ can be expressed in terms of each other as follows
			\begin{subequations}
				\begin{align}
					\label{eq:gradpolicy}
					\gradtheta \policytheta(\diff \response \mid \prompt)
					& \; = \;  \policytheta(\diff \response \mid \prompt) \cdot \frac{1}{\parabeta} \,
					\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \, ,  \\
					\label{eq:gradreward}
					\gradtheta \rewardtheta (\prompt, \response)
					& \; = \; \parabeta \cdot \frac{\gradtheta \policytheta(\response \mid \prompt)}{\policytheta(\response \mid \prompt)} \, .
				\end{align}
			\end{subequations}
		\end{lemma}
		
		We now proceed to prove \Cref{lemma:grad_policy}.  \\
		
		To begin, recall our definition of the reward function $\rewardtheta$ as given in equation~\eqref{eq:def_reward}.
		It directly follows that
		\begin{align*}
			\gradtheta \rewardtheta (\prompt, \response)
			\; = \; \parabeta \cdot \frac{\gradtheta \policytheta(\response \mid \prompt)}{\policytheta(\response \mid \prompt)} \, .
		\end{align*}
		This result confirms equation~\eqref{eq:gradreward} as stated in \Cref{lemma:grad_policy}.
		
		Next, we express the policy $\policytheta(\diff \response \mid \prompt)$ in terms of the reward function $\rewardtheta(\prompt, \response)$. By reformulating equation~\eqref{eq:def_reward}, we obtain
		\begin{subequations}
		\begin{align}
			\label{eq:policyfromreward}
			\policytheta(\diff \response \mid \prompt)
			\; = \; \frac{1}{\Partitiontheta (\prompt)} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\} \, ,
		\end{align}
		where $\Partitiontheta (\prompt)$ is the partition function defined as
		\begin{align}
			\label{eq:def_Partition}
			\Partitiontheta (\prompt)
			& \; = \; \int_{\ResponseSp} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\} \, .
		\end{align}
		\end{subequations}
		
		We then compute the gradient of $\policytheta(\diff \response \mid \prompt)$ with respect to $\paratheta$. Applying the chain rule, we get
		\begin{align}
			\gradtheta \policytheta(\diff \response \mid \prompt)
			& \; = \; \frac{1}{\Partitiontheta (\prompt)} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\}
			\cdot \frac{1}{\parabeta} \, \gradtheta \rewardtheta(\prompt, \response)  \notag  \\
			\label{eq:gradtheta1}
			& \quad - \frac{1}{\Partitiontheta^2(\prompt)} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\}
			\cdot \gradtheta \Partitiontheta(\prompt) \, .
		\end{align}
		We need the gradient of the partition function $\Partitiontheta(\prompt)$:
		\begin{align}
			\gradtheta \Partitiontheta (\prompt)
			& \; = \; \int_{\ResponseSp} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\}
			\cdot \frac{1}{\parabeta} \, \gradtheta \rewardtheta(\prompt, \response)  \notag   \\
			& \; = \; \Partitiontheta (\prompt) \cdot \int_{\ResponseSp} \, \policytheta(\diff \response \mid \prompt) \cdot \frac{1}{\parabeta} \, \gradtheta \rewardtheta(\prompt, \response)  \notag   \\
			\label{eq:gradPartition}
			& \; = \; \Partitiontheta (\prompt) \cdot \frac{1}{\parabeta} \, \Exp_{\response \sim \policytheta(\cdot \mid \prompt)} \big[ \gradtheta \rewardtheta(\prompt, \response) \big] \, .
		\end{align}
		Substituting equation~\eqref{eq:gradPartition} back into equation~\eqref{eq:gradtheta1}, we simplify the expression for the gradient of $\policytheta(\diff \response \mid \prompt)$:
		\begin{align*}
			& \gradtheta \policytheta(\diff \response \mid \prompt)  \\
			& \; = \; \frac{1}{\Partitiontheta (\prompt)} \, \policyref(\diff \response \mid \prompt)
			\exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\}
			\cdot \frac{1}{\parabeta} \, \Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)} \big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \, .
		\end{align*}
		This matches equation~\eqref{eq:gradpolicy} from \Cref{lemma:grad_policy}, thereby completing the proof.
		
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
		\subsubsection{Proof of Lemma~\ref{lemma:grad_scalarvalue} \yaqidone}
		\label{sec:proof:lemma:grad_scalarvalue}
		
		Equality \eqref{eq:grad_scalarvalue} in \Cref{lemma:grad_scalarvalue} can be derived as a consequence of a more detailed result. We state it in \Cref{lemma:grad_scalarvalue_full}.
		
		\begin{lemma}
			\label{lemma:grad_scalarvalue_full}
			\begin{subequations}
			For a policy $\policytheta$, the gradients with respect to the parameter $\paratheta$ of its expected return $\Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big] $ and its KL divergence from a reference policy $\kull{\policytheta}{\policyref}$ are given by
			\begin{align}
				& \gradtheta \Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big]  \notag \\
				\label{eq:grad_return}
				& 
				\qquad  = \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \response \sim \policytheta(\cdot \mid \prompt)}
				\bigg[ \rewardstar(\prompt, \response)
				\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, , \\
				& \gradtheta \kull{\policytheta}{\policyref}  \notag  \\
				\label{eq:grad_KL}
				& \qquad = 
				\frac{1}{\parabeta^2} \, \Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)}
				\bigg[ \rewardtheta(\prompt, \response)
				\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, .
			\end{align}
			\end{subequations}
		\end{lemma}
		
		Recall that the scalar value $\scalarvalue(\policytheta)$ of the policy is defined as
		\begin{align*}
			\scalarvalue(\policytheta) \; = \;
			\Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big] \, - \,
			\parabeta \, \kull{\policytheta}{\policyref} \, .
		\end{align*}
		Using \Cref{lemma:grad_scalarvalue_full}, we derive the gradient of $\scalarvalue(\policytheta)$ as
		\begin{align}
			& \gradtheta \scalarvalue(\policytheta) \; = \; \gradtheta \Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big] \, - \,
			\parabeta \, \gradtheta \kull{\policytheta}{\policyref}  \notag  \\
			\label{eq:grad_scalarvalue0}
			& \; = \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \response \sim \policytheta(\cdot \mid \prompt)}
			\bigg[ \big\{ \rewardstar(\prompt, \response) - \rewardtheta(\prompt, \response) \big\}
			\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, .
		\end{align}
		We rewrite the expression in equation \eqref{eq:grad_scalarvalue0} in two equivalent forms by exchanging the roles of $\responseone$ and $\responsetwo$:
		\begin{subequations}
		\begin{align}
			& \gradtheta \scalarvalue(\policytheta) \notag \\ 
			\label{eq:grad_scalarvalue1}
			& \; = \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \responseone \sim \policytheta(\cdot \mid \prompt)}
			\bigg[ \big\{ \rewardstar(\prompt, \responseone) - \rewardtheta(\prompt, \responseone) \big\} \Big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \Exp_{\responsetwo \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsetwo) \big] \Big\} \bigg] \, ,  \\
			& \gradtheta \scalarvalue(\policytheta) \notag \\ 
			\label{eq:grad_scalarvalue2}
			& \; = \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \responsetwo \sim \policytheta(\cdot \mid \prompt)}
			\bigg[ \big\{ \rewardstar(\prompt, \responsetwo) - \rewardtheta(\prompt, \responsetwo) \big\} \Big\{ \gradtheta \rewardtheta(\prompt, \responsetwo) - \Exp_{\responseone \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responseone) \big] \Big\} \bigg] \, .
		\end{align}
		\end{subequations}
		By taking the average of the two equivalent formulations above, we obtain equality \eqref{eq:grad_scalarvalue} and complete the proof of \Cref{lemma:grad_scalarvalue}.  \\
		
		We now proceed to prove \Cref{lemma:grad_scalarvalue_full}, tackling equalities \eqref{eq:grad_return} and \eqref{eq:grad_KL} one by one.
		
		\paragraph{Proof of Equality~\eqref{eq:grad_return} from \Cref{lemma:grad_scalarvalue_full}:}
		We begin by expressing the expected return as
		\begin{align*}
			\Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big]
			& \; = \; \Exp_{\prompt \sim \promptdistr} \bigg[ \int_{\ResponseSp} \rewardstar(\prompt, \response) \, \policytheta(\diff \response \mid \prompt) \bigg] \, .
		\end{align*}
		Taking the gradient of both sides with respect to $\paratheta$, we have
		\begin{align}
			\label{eq:grad_return0}
			\gradtheta \Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \big[ \rewardstar(\context, \response) \big]
			& \; = \; \Exp_{\prompt \sim \promptdistr} \bigg[ \int_{\ResponseSp} \rewardstar(\prompt, \response) \, \gradtheta \policytheta(\diff \response \mid \prompt) \bigg] \, .
		\end{align}
		Using the expression for the policy gradient $\gradtheta \policytheta$ provided in \Cref{lemma:grad_policy}, the right-hand side of \eqref{eq:grad_return0} simplifies to
		\begin{align*}
			\mbox{RHS of \eqref{eq:grad_return0}}
			& \; = \; \Exp_{\prompt \sim \promptdistr} \bigg[ \int_{\ResponseSp} \rewardstar(\prompt, \response) \, \policytheta(\diff \response \mid \prompt) \cdot \frac{1}{\parabeta} \,
			\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg]   \\
			& \; = \; \frac{1}{\parabeta} \,\Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \bigg[ \rewardstar(\prompt, \response) 
			\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, .
		\end{align*}
		This completes the verification of equation~\eqref{eq:grad_return} from \Cref{lemma:grad_scalarvalue}.
		
		
		
		\paragraph{Proof of Equality~\eqref{eq:grad_KL} from \Cref{lemma:grad_scalarvalue_full}:}
		
		Recall the definition of the KL divergence
		\begin{align*}
			\kull{\policytheta}{\policyref}
			\; = \; \Exp_{\prompt \sim \promptdistr} 
			\bigg[ \int_{\ResponseSp} \policytheta(\diff \response \mid \prompt)
			\log \bigg( \frac{\policytheta(\response \mid \prompt)}{\policyref(\response \mid \prompt)} \bigg) \bigg] \, .
		\end{align*}
		Applying the chain rule, we obtain
		\begin{align}
			\gradtheta \kull{\policytheta}{\policyref}
			& \, = \, \Exp_{\prompt \sim \promptdistr}  \bigg[ \int_{\ResponseSp} \gradtheta \policytheta(\diff \response \mid \prompt) \,
			\log \bigg( \frac{\policytheta(\response \mid \prompt)}{\policyref(\response \mid \prompt)}\bigg) \bigg]  
			\label{eq:grad_KL2}
			+ \Exp_{\prompt \sim \promptdistr}  \bigg[ \int_{\ResponseSp} 
			\gradtheta \policytheta(\diff \response \mid \prompt) \bigg] \, .
		\end{align}
		
		Since the policy integrates to $1$, i.e., $\int_{\ResponseSp} 
		\policytheta(\diff \response \mid \prompt) = 1$, it always holds that
		\begin{align}
			\label{eq:int_grad_policy}
			\int_{\ResponseSp} 
			\gradtheta \policytheta(\diff \response \mid \prompt)
			\; = \; \gradtheta \int_{\ResponseSp} 
			\policytheta(\diff \response \mid \prompt)
			\; = \; 0 \, ,
		\end{align}
		i.e., the second term on the right-hand side of \eqref{eq:grad_KL2} is zero.
		Using the expression \eqref{eq:policyfromreward}, we take the logarithm
		\begin{align}
			\label{eq:grad_KL0}
			\log \bigg( \frac{\policytheta(\response \mid \prompt)}{\policyref(\response \mid \prompt)} \bigg)
			\; = \; \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) - \log \Partitiontheta (\prompt) \, .
		\end{align}
		Combining equations~\eqref{eq:int_grad_policy} and \eqref{eq:grad_KL0}, we get
		\begin{align}
			& \int_{\ResponseSp} \gradtheta \policytheta(\diff \response \mid \prompt) \,
			\log \bigg( \frac{\policytheta(\response \mid \prompt)}{\policyref(\response \mid \prompt)}\bigg)  \notag  \\
			& \; = \; \frac{1}{\parabeta} \int_{\ResponseSp} \rewardtheta(\prompt, \response) \, \gradtheta \policytheta(\diff \response \mid \prompt) \; - \; \log \Partitiontheta(\prompt) \int_{\ResponseSp} \gradtheta \policytheta(\diff \response \mid \prompt)  \notag  \\
			\label{eq:grad_KL1}
			& \; = \; \frac{1}{\parabeta} \int_{\ResponseSp} \rewardtheta(\prompt, \response) \, \gradtheta \policytheta(\diff \response \mid \prompt) \, .
		\end{align}
		
		Now, similar to the proof of equation \eqref{eq:grad_return}, we derive
		\begin{align*}
			\mbox{RHS of \eqref{eq:grad_KL2}}
			& \; = \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr} \bigg[ \int_{\ResponseSp} \rewardtheta(\prompt, \response) \, \gradtheta \policytheta(\diff \response \mid \prompt) \bigg]  \\
			& \; = \; \frac{1}{\parabeta^2} \,\Exp_{\prompt \sim \promptdistr, \, \response \sim \policytheta(\cdot \mid \prompt)} \bigg[ \rewardtheta(\prompt, \response) 
			\Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, ,
		\end{align*}
		which verifies equality~\eqref{eq:grad_KL} from \Cref{lemma:grad_scalarvalue_full}.
		

		
		
		
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



	\subsubsection{Proof of Lemma~\ref{lemma:grad_loss} \yaqidone}
	\label{sec:proof:lemma:grad_loss}
	
	In this section, we prove a full version of \Cref{lemma:grad_loss} as stated in \Cref{lemma:grad_loss_full} below. Equation~\eqref{eq:gradLoss_BT_0} from \Cref{lemma:grad_loss} follows directly as a straightforward corollary.
	
	In \Cref{lemma:grad_loss_full}, we consider a general class of distributions parameterized by $\paratheta$ that models the binary preference \mbox{$\Probtheta(\responseone \succ \responsetwo \mid \prompt)$}. The negative log-likelihood function is defined as
	\begin{align*}
		\Loss(\theta) = - \Exp_{\prompt \sim \promptdistr; \; (\responseone, \responsetwo) \sim \responsedistr(\cdot \mid \prompt)} \Big[ \weight(\prompt) \cdot \log \Probtheta( \responsewin \succ \responselose \bigm| \prompt) \Big] \, .
	\end{align*}
	The Bradley-Terry (BT) model described in equation~\eqref{eq:BT} and the corresponding loss function~$\Loss(\paratheta)$ in equation~\eqref{eq:Loss0} represent a special case of this general framework.
	
	\begin{lemma}[Gradient of the loss function $\Loss(\paratheta)$, full version]
		\label{lemma:grad_loss_full}
		\begin{subequations}
			For a general distribution class $\{ \Probtheta \}$, the gradient of $\Loss(\paratheta)$ with respect to $\paratheta$ is given by
			\begin{multline}
				\label{eq:gradLoss_general}
				\gradtheta \Loss(\paratheta) \; = \; - \, \Exp_{ \prompt \sim \promptdistr; \; (\responseone, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt) }
				\bigg[ \, \weight(\prompt) \cdot \Big\{ \Prob\big( \responseone \succ \responsetwo \bigm| \prompt \big) - \Probtheta \big( \responseone \succ \responsetwo \bigm| \prompt \big) \Big\} \\
				\cdot \frac{\gradtheta \Probtheta( \responseone \succ \responsetwo \mid \prompt )}{\Probtheta( \responseone \succ \responsetwo \mid \prompt ) \, \Probtheta( \responsetwo \succ \responseone \mid \prompt )} \, \bigg] \, ,
			\end{multline}
			where $\responsedistravg$ is the average distribution defined in equation~\eqref{eq:def_responsedistravg_0}.
			Specifically, for the Bradley-Terry (BT) model where
			\begin{align*}
				\Probtheta \big( \responseone \succ \responsetwo \bigm| \prompt \big)
				\; = \; \sigmoid \big( \rewardtheta(\prompt, \responseone) - \rewardtheta(\prompt, \responsetwo) \big)
				\; = \; \bigg\{ 1 + \bigg( \frac{(\policytheta/\policyref)(\responsetwo \mid \prompt)}{(\policytheta/\policyref)(\responseone \mid \prompt)} \bigg)^{\parabeta} \bigg\}^{-1} \, ,
			\end{align*}
			the gradient of $\Loss(\paratheta)$ becomes
			\begin{multline}
				\label{eq:gradLoss_BT}
				\gradtheta \Loss(\paratheta) \; = \; - \, \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
				\bigg[ \, \weight(\prompt) \cdot \Big\{ \sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \\ 
				\cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \bigg] \, .
			\end{multline}
		\end{subequations}
	\end{lemma}
	
	
	For notational simplicity, we focus on the proof for the case where the weight function $\weight(\prompt) = 1$. The results for a general weight function $\weight(\prompt) > 0$ can be derived in a similar manner.
	
	Recall that the negative log-likelihood function $\Loss(\paratheta)$ is defined as
	\begin{align*}
		\Loss(\paratheta) & \; = \;
		\Exp \Big[ - \log \Probtheta\big( \responsewin \succ \responselose \bigm| \prompt \big) \Big] \, .
	\end{align*}
	Based on the data generation mechanism, we can expand the expectation in $\Loss(\paratheta)$ as
%	\begin{subequations}
	\begin{align}
		\Loss(\paratheta)
		& \; = \; \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistr(\cdot \mid \prompt)}
		\Big[ \, \Prob\big( \responseone \succ \responsetwo \bigm| \prompt \big) \cdot \big\{ - \log \Probtheta \big( \responseone \succ \responsetwo \bigm| \prompt \big) \big\}  \notag  \\
		\label{eq:Loss0}
		& \qquad \qquad \qquad \qquad \qquad + \Prob\big( \responsetwo \succ \responseone \bigm| \prompt \big) \cdot \big\{ - \log \Probtheta\big( \responsetwo \succ \responseone \bigm| \prompt \big) \big\} \Big] \, .
	\end{align}
	Notice that we can exchange the roles of $\responseone$ and $\responsetwo$ in the expectation above. This means that we can equivalently express the expectation using the pair $(\responsetwo, \responseone) \sim \responsedistr(\cdot \mid \prompt)$.
	This symmetry allows us to replace $\responsedistr$ in equation~\eqref{eq:Loss0} with the average distribution $\responsedistravg$ as defined in equation~\eqref{eq:def_responsedistravg_0}. \\
	
	Next, we take the gradient of the loss function $\Loss(\paratheta)$ with respect to the parameter $\paratheta$ and obtain
	\begin{align*}
		\gradtheta \Loss(\paratheta)
		& \; = \; \Exp_{\prompt \sim \promptdistr, \, (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		\bigg[ \, \frac{\Prob( \responseone \succ \responsetwo \mid \prompt )}{\Probtheta ( \responseone \succ \responsetwo \mid \prompt )} \cdot \big\{ - \gradtheta \Probtheta( \responseone \succ \responsetwo \mid \prompt ) \big\}   \\
		& \qquad \qquad \qquad \qquad \qquad + \frac{\Prob( \responsetwo \succ \responseone \mid \prompt )}{\Probtheta( \responsetwo \succ \responseone \mid \prompt )} \cdot \big\{ - \gradtheta \Probtheta( \responsetwo \succ \responseone \mid \prompt ) \big\} \, \bigg] \, .
	\end{align*}
	Note that $\Prob\big( \responsetwo \succ \responseone \bigm| \prompt\big) = 1 - \Prob\big( \responseone \succ \responsetwo \bigm| \prompt\big)$ and $\Probtheta \big( \responsetwo \succ \responseone \bigm| \prompt\big) = 1 - \Probtheta \big( \responseone \succ \responsetwo \bigm| \prompt\big)$.
	Using this, we can rewrite the gradient as
	\begin{align*}
		& \gradtheta \Loss(\paratheta)  \\
		& \; = \;
		\Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		\bigg[ \bigg\{ \frac{1 - \Prob( \responseone \succ \responsetwo \mid \prompt )}{1 - \Probtheta( \responseone \succ \responsetwo \mid \prompt )} - \frac{\Prob( \responseone \succ \responsetwo \mid \prompt )}{\Probtheta ( \responseone \succ \responsetwo \mid \prompt )} \bigg\} \cdot \gradtheta \Probtheta\big( \responseone \succ \responsetwo \bigm| \prompt \big) \bigg] \, .
	\end{align*}
	We simplify the expression further to obtain
	\begin{multline*}
		\gradtheta \Loss(\paratheta)
		\; = \;
		\Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		\bigg[ \Big\{ \Probtheta \big( \responseone \succ \responsetwo \bigm| \prompt \big) - \Prob \big( \responseone \succ \responsetwo \bigm| \prompt \big) \Big\} \\ \cdot \frac{\gradtheta \Probtheta( \responseone \succ \responsetwo \mid \prompt )}{\Probtheta( \responseone \succ \responsetwo \mid \prompt ) \, \Probtheta( \responsetwo \succ \responseone \mid \prompt )} \bigg] \, .
	\end{multline*}
	This establishes equation~\eqref{eq:gradLoss_general} from \Cref{lemma:grad_loss}. \\
	
	As for the Bradley-Terry (BT) model, we use the equality
	\begin{align*}
		\divsigmoid(z) \; = \; \frac{1}{(1 + \exp(-z))(1 + \exp(z))} \; = \; \sigmoid(z) \, \sigmoid(-z)
		\qquad \mbox{for any $z \in \Real$}
	\end{align*}
	to derive the following expression
	\begin{align}
		\label{eq:grad_reward}
		\frac{\gradtheta \Probtheta( \responseone \succ \responsetwo \mid \prompt )}{\Probtheta( \responseone \succ \responsetwo \mid \prompt ) \, \Probtheta( \responsetwo \succ \responseone \mid \prompt )}
		\; = \; \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \, .
	\end{align}
	By substituting this gradient expression from equation~\eqref{eq:grad_reward} into equation~\eqref{eq:gradLoss_general}, we directly obtain equation~\eqref{eq:gradLoss_BT}, thereby completing the proof of \Cref{lemma:grad_loss}.
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsection{Proof of Auxiliary Results for Theorem~\ref{thm:asymp} \yaqidone}
	\label{sec:proof:thm:asymp_aux}
	
	In this section, we present the detailed proofs of the supporting lemmas used in the proof of \Cref{thm:asymp}. 
	We begin in \Cref{sec:proof:eq:master_cond_proof} by establishing condition~\eqref{eq:master_cond_proof}, which is crucial for the valid application of the master theorem for $Z$-estimators. Following this, in \Cref{sec:proof:lemma:hess_loss}, we compute the Hessian matrix $\hesstheta \Loss(\parathetastar)$ explicitly. Finally, in \Cref{sec:proof:lemma:grad_loss_stat}, we derive the asymptotic distribution of the gradient~$\gradtheta \Losshat(\parathetastar)$.
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsubsection{Proof of Condition~\eqref{eq:master_cond_proof}}
	\label{sec:proof:eq:master_cond_proof}
	We begin by rewriting the left-hand side of equation~\eqref{eq:master_cond_proof} as follows:
	\begin{align}
		\Delta
		& \; \defn \; \sqrt{n} \, \big\{ \gradtheta \Losshat (\parathetahat) - \gradtheta \Loss(\parathetahat) \big\} - \sqrt{n} \, \big\{ \gradtheta \Losshat (\parathetastar) - \gradtheta \Loss (\parathetastar) \big\}   \notag  \\
		& \; = \; \sqrt{n} \, \big\{ \gradtheta \Losshat (\parathetahat) - \gradtheta \Losshat(\parathetastar) \big\} - \sqrt{n} \, \big\{ \gradtheta \Loss (\parathetahat) - \gradtheta \Loss (\parathetastar) \big\} \, .
		\label{eq:master_0}
	\end{align}
	We then leverage the smoothness properties of the function $\rewardtheta$, which guarantee the following approximations:
	\begin{subequations}
		\begin{align}
			\label{eq:gradLosshat_smooth}
			\gradtheta \Losshat(\parathetahat) - \gradtheta \Losshat(\parathetastar) & \; = \; \hesstheta \Losshat(\parathetastar) \, (\parathetahat - \parathetastar) + \smallop \big( \norm{\parathetahat - \parathetastar}_2 \big) \, ,  \\
			\label{eq:gradLoss_smooth}
			\gradtheta \Loss(\parathetahat) - \gradtheta \Loss(\parathetastar) & \; = \; \hesstheta \Loss(\parathetastar) \, (\parathetahat - \parathetastar) + \smallop \big( \norm{\parathetahat - \parathetastar}_2 \big) \, .
		\end{align}
	\end{subequations}
	Assuming these equalities~\eqref{eq:gradLosshat_smooth} and~\eqref{eq:gradLoss_smooth} hold, we substitute them into equation~\eqref{eq:master_0}, leading to
	\begin{align}
		\Delta
		& \; = \; \sqrt{n} \, \big\{ \hesstheta \Losshat (\parathetastar) \, (\parathetahat - \parathetastar) + \smallop( \norm{\parathetahat - \parathetastar}_2 ) \big\} - \sqrt{n} \, \big\{ \hesstheta \Loss (\parathetastar) \, (\parathetahat - \parathetastar) + \smallop( \norm{\parathetahat - \parathetastar}_2 ) \big\}  \notag \\
		& \; = \; \sqrt{n} \, \big\{ \hesstheta \Losshat (\parathetastar) - \hesstheta \Loss (\parathetastar) \big\} (\parathetahat - \parathetastar) + \smallop \big( 1 + \sqrt{n} \, \norm{ \parathetahat - \parathetastar }_2 \big) \, .
		\label{eq:master_1}
	\end{align}
	Using the law of large numbers, we know that $\hesstheta \Losshat (\parathetastar) \convergep \hesstheta \Loss (\parathetastar)$, which implies
	\begin{align*}
		\sqrt{n} \, \big\{ \hesstheta \Losshat (\parathetastar) - \hesstheta \Loss (\parathetastar) \big\} (\parathetahat - \parathetastar) \; = \; \smallop \big( \sqrt{n} \, \norm{ \parathetahat - \parathetastar }_2 \big) \, .
	\end{align*}
	Therefore, we conclude that
	\begin{align*}
		\Delta \; = \; \smallop \big( 1 + \sqrt{n} \, \norm{ \parathetahat - \parathetastar }_2 \big)
	\end{align*}
	as claimed in equation~\eqref{eq:master_cond_proof}.
	
	The only remaining task is to establish the validity of equalities~\eqref{eq:gradLosshat_smooth} and~\eqref{eq:gradLoss_smooth}.
	
	
	\paragraph{Proof of Equalities~\eqref{eq:gradLosshat_smooth}~and~\eqref{eq:gradLoss_smooth}:}
	
	We express the loss function $\Losshat(\paratheta)$ in the form
	\begin{align*}
		\Losshat(\paratheta) \; \defn \;
		\frac{1}{\numobs} \sum_{i=1}^{\numobs} \weight(\prompti{i}) \cdot \lliketheta\big(\prompti{i}, \responsewini{i}, \responselosei{i}\big) \, ,
	\end{align*}
	where the function $\lliketheta$ is defined as
	\begin{align*}
		\lliketheta(\prompt, \responsei{1}, \responsei{2})
		\; = \; - \log \sigmoid \big( \rewardtheta(\prompt, \responsei{1}) - \rewardtheta(\prompt, \responsei{2}) \big) \, .
	\end{align*}
	We then calculate the gradient $\gradtheta \lliketheta$ and $\hesstheta \lliketheta$ as follows:
	\begin{align*}
		\gradtheta \lliketheta(\prompt, \responsei{1}, \responsei{2})
		& \; = \; \sigmoid\big( \rewardtheta(\prompt, \responsei{2}) - \rewardtheta(\prompt, \responsei{1}) \big) \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responsei{2}) - \gradtheta \rewardtheta(\prompt, \responsei{1})  \big\} \qquad \mbox{and}  \\
		\hesstheta \lliketheta(\prompt, \responsei{1}, \responsei{2})
		& \; = \; \divsigmoid\big( \rewardtheta(\prompt, \responsei{2}) - \rewardtheta(\prompt, \responsei{1}) \big) \\
        & \qquad \quad
        \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responsei{2}) - \gradtheta \rewardtheta(\prompt, \responsei{1}) \big\} \big\{ \gradtheta \rewardtheta(\prompt, \responsei{2}) - \gradtheta \rewardtheta(\prompt, \responsei{1}) \big\}^{\top}  \\
		& \quad + \sigmoid\big( \rewardtheta(\prompt, \responsei{2}) - \rewardtheta(\prompt, \responsei{1}) \big) \cdot \big\{ \hesstheta \rewardtheta(\prompt, \responsei{2}) - \hesstheta \rewardtheta(\prompt, \responsei{1})  \big\} \, .
	\end{align*}
	When the reward function $\rewardtheta(\prompt, \response)$, along with its gradient $\gradtheta \rewardtheta(\prompt, \response)$ and Hessian $\hesstheta \rewardtheta(\prompt, \response)$, is uniformly bounded and Lipschitz continuous with respect to $\paratheta$ for all $(\prompt, \response) \in \PromptSp \times \ResponseSp$, it guarantees that the Hessian of the loss function, $\hesstheta \lliketheta$, is also Lipschitz continuous. This holds with some constant $\Liphess > 0$ across all $(\prompt, \response) \in \PromptSp \times \ResponseSp$, as demonstrated below:
	\begin{align*}
		\norm[\big]{\hesstheta \lliketheta (\prompt, \responsei{1}, \responsei{2}) - \hesstheta \llikethetastar (\prompt, \responsei{1}, \responsei{2})}_2
		\; \leq \; \Liphess \cdot \norm{\paratheta - \parathetastar}_2 \, .
	\end{align*}
	From this Lipschitz property, we deduce
	\begin{align*}
		\norm[\big]{\gradtheta \lliketheta (\prompt, \responsei{1}, \responsei{2}) - \gradtheta \llikethetastar (\prompt, \responsei{1}, \responsei{2}) - \hesstheta \llikethetastar (\prompt, \responsei{1}, \responsei{2}) \, (\paratheta - \parathetastar)}_2 \; \leq \; \frac{\Liphess}{2} \cdot \norm{\paratheta - \parathetastar}_2^2
	\end{align*}
	and further derive
	\begin{align*}
		\norm[\big]{\gradtheta \Losshat(\paratheta) - \gradtheta \Losshat(\parathetastar) - \hesstheta \Losshat(\parathetastar) \, (\paratheta - \parathetastar)}_2 & \; \leq \; \frac{\Liphess \, \supnorm{\weight}}{2} \cdot \norm{\paratheta - \parathetastar}_2^2 \, ,  \\
		\norm[\big]{\gradtheta \Loss(\paratheta) - \gradtheta \Loss(\parathetastar) - \hesstheta \Loss(\parathetastar) \, (\paratheta - \parathetastar)}_2 & \; \leq \; \frac{\Liphess \, \supnorm{\weight}}{2} \cdot \norm{\paratheta - \parathetastar}_2^2 \, .
	\end{align*}
	Finally, under the condition that $\parathetahat \convergep \parathetastar$, these results simplify to the expressions given in equations~\eqref{eq:gradLosshat_smooth} and~\eqref{eq:gradLoss_smooth}, as previously claimed.
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsubsection{Proof of Lemma~\ref{lemma:hess_loss}, Explicit Form of Hessian $\hesstheta \Loss(\parathetastar)$}
	\label{sec:proof:lemma:hess_loss}
	
	From equation~\eqref{eq:gradLoss_BT_0} in \Cref{lemma:grad_loss}, we recall the explicit formula for the gradient $\gradtheta \Loss(\paratheta)$. Taking the derivative of both sides of equation~\eqref{eq:gradLoss_BT_0}, we obtain
	\begin{align}
		& \begin{aligned} 
		\hesstheta \Loss(\paratheta) \; = \; \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		& \Big[ \, \weight(\prompt) \cdot \divsigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \\ 
		& \cdot \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\} \big\{ \gradtheta \rewardtheta(\prompt, \responseone) - \gradtheta \rewardtheta(\prompt, \responsetwo) \big\}^{\top} \Big] \end{aligned}   \notag  \\
		& \qquad \qquad \quad
		\begin{aligned} 
		- \, \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		\bigg[ \, \weight(\prompt) & \cdot \Big\{ \sigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) - \sigmoid \big( \rewardtheta(\context, \responseone) - \rewardtheta(\context, \responsetwo) \big) \Big\} \\ 
		& \cdot \big\{ \hesstheta \rewardtheta(\prompt, \responseone) - \hesstheta \rewardtheta(\prompt, \responsetwo) \big\} \bigg] \, .
		\end{aligned}
		\label{eq:hessLoss_0}
	\end{align}
	When we set $\paratheta = \parathetastar$, it follows that $\rewardtheta = \rewardstar$. This simplification eliminates the second term in expression~\eqref{eq:hessLoss_0}, reducing the Hessian matrix to
	\begin{multline*}
		\hesstheta \Loss(\parathetastar) \; = \; \Exp_{\prompt \sim \promptdistr; \; (\responseone, \, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)}
		\Big[ \, \weight(\prompt) \cdot \divsigmoid \big( \rewardstar(\context, \responseone) - \rewardstar(\context, \responsetwo) \big) \\ 
		\cdot \big\{ \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \big\} \big\{ \gradtheta \rewardstar(\prompt, \responseone) - \gradtheta \rewardstar(\prompt, \responsetwo) \big\}^{\top} \Big] \, .
	\end{multline*}
	Substituting the derivative $\divsigmoid$ with its explicit form, $\divsigmoid(z) = \sigmoid(z) \, \sigmoid(-z)$ for any $z \in \Real$, we refine the expression to
	\begin{align*}
		\hesstheta \Loss(\parathetastar) \; = \; \CovOpstar \, ,
	\end{align*}
	where the covariance matrix $\CovOpstar$ is defined in equation~\eqref{eq:def_CovOpstar}.
	This completes the proof of expression~\eqref{eq:hess_loss} from \Cref{lemma:hess_loss}.
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsubsection{Proof of Lemma~\ref{lemma:grad_loss_stat}, Asymptotic Distribution of Graident $\gradtheta \Losshat(\parathetastar)$}
	\label{sec:proof:lemma:grad_loss_stat}
	
	In this section, we analyze the asymptotic distribution of the gradient $\gradtheta \Losshat(\paratheta)$ at $\paratheta = \parathetastar$, where the loss function $\Losshat(\paratheta)$ is defined as
	\begin{align*}
		\Losshat(\paratheta) \; = \;
		- \frac{1}{\numobs} \sum_{i=1}^{\numobs} \, \weight(\prompt) \cdot \log \sigmoid \Big( \rewardtheta\big(\prompti{i}, \responsewini{i}\big) - \rewardtheta\big(\prompti{i}, \responselosei{i}\big) \Big) \, .
	\end{align*}
	Using the definition of the sigmoid function $\sigmoid$, we calculate that
	\begin{align*}
		( \log \sigmoid(z) )' = \divsigmoid(z) / \sigmoid(z) = \sigmoid(z) \, \sigmoid(-z) / \sigmoid(z) = \sigmoid(-z) \qquad \mbox{for any real number $z \in \Real$}.
	\end{align*}
	This allows us to reformulate $\gradtheta \Losshat(\paratheta)$ as the average of $\numobs$ i.i.d. vectors $\{ \vecgi{i} \}_{i=1}^{\numobs}$:
	\begin{align}
		\label{eq:gradLosshat}
		\gradtheta \Losshat(\paratheta)
		\; = \; \frac{1}{\numobs} \sum_{i=1}^{\numobs} \, \vecgi{i} \, .
	\end{align}
	Here each vector $\vecgi{i} \in \Real^{\Dim}$ is defined as
	\begin{align*}
		\vecgi{i} \; \defn \; \weight(\prompt) \cdot \sigmoid \big( \rewardtheta(\prompti{i}, \responselosei{i}) - \rewardtheta(\prompti{i}, \responsewini{i}) \big) \cdot \big\{ \gradtheta \rewardtheta(\prompti{i}, \responselosei{i}) - \gradtheta \rewardtheta(\prompti{i}, \responsewini{i}) \big\} \, .
	\end{align*}
%	(consistently with expression~\eqref{eq:def_grad}).
	At $\paratheta = \parathetastar$, we denote $\vecgi{i}$ as $\vecgstari{i}$ and $\gradi{i}$ as $\gradstari{i}$. Notably, vector $\vecgi{i}$ can be rewritten as
	\begin{align}
		\label{eq:vecgi2}
		\vecgi{i} 
		& \; = \; \weight(\prompt) \cdot \big\{ \sigmoid \big( \rewardtheta(\prompti{i}, \responseonei{i}) - \rewardtheta(\prompti{i}, \responsetwoi{i}) \big) - \indicator\{ \responseonei{i} = \responsewini{i}, \responsetwoi{i} = \responselosei{i} \} \big\}
		\cdot \gradi{i} \, ,
	\end{align}
	where $\gradi{i}$ is given by
	\begin{align*}
		\gradi{i} \defn \gradtheta \rewardtheta(\prompti{i}, \responseonei{i}) - \gradtheta \rewardtheta(\prompti{i}, \responsetwoi{i}) \, .
	\end{align*}
	From the structure of the BT model, it holds that
	\begin{align*}
		\Exp\big[ \indicator \{ \responseonei{i} = \responsewini{i}, \responsetwoi{i} = \responselosei{i} \} \bigm| \prompti{i} \big] \; = \; \sigmoid \big( \rewardstar(\prompti{i}, \responseonei{i}) - \rewardstar(\prompti{i}, \responsetwoi{i}) \big) \, ,
	\end{align*}
	which implies $\Exp[\vecgstari{i}] = \veczero$.
	
	
	To analyze the asymptotic distribution of $\gradtheta \Losshat(\parathetastar)$, we apply the central limit theorem (CLT) to its empirical form given in equation~\eqref{eq:gradLosshat}. 
%	\yaqiadd{Check the conditions for CLT.}
	By the CLT, we have
	\begin{align}
		\label{eq:gradLoss_CLT}
		\sqrt{\numobs} \, \big( \gradtheta \Losshat(\parathetastar) - \gradtheta \Loss(\parathetastar) \big)
		\; \stackrel{d}{\longrightarrow} \; \Gauss\big(\veczero, \CovOptil \big) \, ,
		\qquad \numobs \rightarrow \infty \, ,
	\end{align}
	where the covariance matrix $\CovOptil \in \Real^{\Dim \times \Dim}$ is given by
	\begin{align*}
		\CovOptil \; \defn \; \Cov(\vecgstari{1}) \; = \; \Exp\big[ \vecgstari{1} (\vecgstari{1})^{\top} \big] \, .
	\end{align*}
	Here we have used the property $\Exp[\vecgstari{i}] = \veczero$ in the second equality.
	
	We now compute the explicit form of the covariance matrix $\CovOptil$. Using the definition of $\vecgi{i}$ from expression~\eqref{eq:vecgi2}, we find that
	\begin{align}
		& \CovOptil \; = \; \Exp\big[ \vecgstari{1} (\vecgstari{1})^{\top} \big] \notag  \\
		& = \; \Exp_{\, \begin{subarray}{l} ~ \\ \prompt \sim \promptdistr; \\ (\responseone, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)\end{subarray}} \Big[ \, \weight^2(\prompt) \cdot \big\{ \sigmoid \big( \rewardstar(\prompti{1}, \responseonei{1}) - \rewardstar(\prompti{1}, \responsetwoi{1}) \big) - \indicator\{ \responseonei{1} = \responsewini{1}, \responsetwoi{1} = \responselosei{1} \} \big\}^2 \cdot \gradstari{1} (\gradstari{1})^{\top} \Big] \, .
		\label{eq:CovOptil_2}
	\end{align}
	Taking the conditional expectation over the outcomes of winners and losers, and using the relation
	\begin{align*}
		&  \Exp\Big[
		\big\{ \sigmoid \big( \rewardstar(\prompti{1}, \responseonei{1}) - \rewardstar(\prompti{1}, \responsetwoi{1}) \big) - \indicator\{ \responseonei{1} = \responsewini{1}, \responsetwoi{1} = \responselosei{1} \} \big\}^2 \Bigm| \prompti{1}, \responseonei{1}, \responsetwoi{1} \Big]  \\
		& \; = \; \Var \Big( \indicator\{ \responseonei{1} = \responsewini{1}, \responsetwoi{1} = \responselosei{1} \} \Bigm|  \prompti{1}, \responseonei{1}, \responsetwoi{1} \Big)  \\
		& \; = \; \sigmoid \big( \rewardstar(\prompti{i}, \responseonei{i}) - \rewardstar(\prompti{i}, \responsetwoi{i}) \big) \, \sigmoid \big( \rewardstar(\prompti{i}, \responsetwoi{i}) - \rewardstar(\prompti{i}, \responseonei{i}) \big) \, ,
	\end{align*}
	we reduce equation~\eqref{eq:CovOptil_2} to
	\begin{align*}
		\CovOptil
		& \; = \; \Exp_{\prompt \sim \promptdistr; \; (\responseone, \responsetwo) \sim \responsedistravg(\cdot \mid \prompt)} \Big[ \, \weight^2(\prompt) \cdot \Var \big( \indicator\{ \responseonei{1} = \responsewini{1}, \responsetwoi{1} = \responselosei{1} \} \bigm|  \prompti{1}, \responseonei{1}, \responsetwoi{1} \big) \cdot \gradstari{1} (\gradstari{1})^{\top} \Big] \, .
	\end{align*}
	Bounding the weight function $\weight(\prompt)$ by its uniform bound $\supnorm{\weight}$, we simplify further:
	\begin{align*}
		\CovOptil
        & \; \preceq \; \supnorm{\weight} \cdot \Exp\Big[ \, \weight(\prompt) \cdot \Var \big( \indicator\{ \responseonei{1} = \responsewini{1}, \responsetwoi{1} = \responselosei{1} \} \bigm|  \prompti{1}, \responseonei{1}, \responsetwoi{1} \big) \cdot \gradstari{1} (\gradstari{1})^{\top} \Big] \, .
    \end{align*}
    This ultimately reduces to
    \begin{align}
    	\label{eq:CovOptil_ub}
         \CovOptil & \; \preceq \; \supnorm{\weight} \cdot \CovOpstar
	\end{align}
	where $\CovOpstar$ is defined in equation~\eqref{eq:def_CovOpstar}.
    
    Finally, by combining equations~\eqref{eq:gradLoss_CLT} and~\eqref{eq:CovOptil_ub}, we establish the asymptotic normality of $\gradtheta \Losshat(\parathetastar)$ and complete the proof of \Cref{lemma:grad_loss_stat}.
    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Proof of Auxiliary Results for Theorem~\ref{lemma:hess_scalarvalue} \yaqidone}
	\label{sec:proof:lemma:hess_scalarvalue_aux}
	
	This section contains the proofs of the auxiliary results supporting \Cref{lemma:hess_scalarvalue}. In \Cref{sec:proof:eq:hessscalarvalue}, we derive the explicit form of the Hessian $ \hesstheta \scalarvalue(\policystar) $. \Cref{sec:proof:gap_distr} rigorously establishes the asymptotic distribution of the value gap (equation~\eqref{eq:gap_distr}). Finally, \Cref{sec:proof:chisqtail} proves the tail bound~\eqref{eq:gap_bd} on the chi-square distribution $ \chisquare_{\Dim} $.
	
	\subsubsection{Proof of Equation~\eqref{eq:hessscalarvalue} from Theorem~\ref{lemma:hess_scalarvalue}, Explicit Form of Hessian $\hesstheta \scalarvalue(\policystar)$}
	\label{sec:proof:eq:hessscalarvalue}
	
	We begin by differentiating expression~\eqref{eq:grad_scalarvalue0} for the gradient $\gradtheta \scalarvalue(\policytheta)$ to obtain the Hessian matrix $\hesstheta \scalarvalue(\policytheta)$. The resulting expression can be written as
	\begin{align*}
		\hesstheta \scalarvalue(\policytheta)
		\; = \; \GammaMt_1 + \GammaMt_2 + \GammaMt_3 \, ,
	\end{align*}
	where the terms are defined as follows:
	\begin{align*}
		\GammaMt_1
		& \; \defn \; \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr}
		\bigg[ \int_{\ResponseSp} \big\{ \rewardstar(\prompt, \response) - \rewardtheta(\prompt, \response) \big\} \\
		& \qquad \qquad \qquad \qquad \quad
        \cdot \Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \, \gradtheta \policytheta(\diff \response \mid \prompt)^{\top} \bigg] \, ,  \\
		\GammaMt_2
		& \; \defn \; - \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \response \sim \policytheta(\cdot \mid \prompt)}
		\bigg[ \Big\{ \gradtheta \rewardtheta(\prompt, \response) - \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \, \gradtheta \rewardtheta(\prompt, \response)^{\top} \bigg] \, ,  \\
		\GammaMt_3
		& \defn \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \response \sim \policytheta(\cdot \mid \prompt)}
		\bigg[ \big\{ \rewardstar(\prompt, \response) - \rewardtheta(\prompt, \response) \big\} \Big\{ \hesstheta \rewardtheta(\prompt, \response) - \gradtheta \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \Big\} \bigg] \, .
	\end{align*}
	
	At the point $\paratheta = \parathetastar$, we know that $\rewardtheta = \rewardstar$. This simplifies the expression significantly:
	\begin{align*}
	\GammaMt_1 = \veczero \quad \text{and} \quad \GammaMt_3 = \veczero.
	\end{align*}
	Therefore, only term $\GammaMt_2$ contributes to the Hessian, and it further reduces to
	\begin{align*}
		\GammaMt_2
		& \; = \; - \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr, \,  \response \sim \policytheta(\cdot \mid \prompt)}
		\Big[ \gradtheta \rewardtheta(\prompt, \response) \, \gradtheta \rewardtheta(\prompt, \response)^{\top} \Big]  \\
		& \quad + \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr}
		\Big[ \Exp_{\responsenew \sim \policytheta(\cdot \mid \prompt)}\big[ \gradtheta \rewardtheta(\prompt, \responsenew) \big] \, \Exp_{\response \sim \policytheta(\cdot \mid \prompt)}\big[\gradtheta \rewardtheta(\prompt, \response)\big]^{\top} \Big] \\
		& \; = \; - \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr}
		\Big[ \Cov_{\response \sim \policytheta(\cdot \mid \prompt)} \big[ \gradtheta \rewardtheta(\prompt, \response) \bigm| \prompt \big] \Big]  \, .
	\end{align*}
	From this simplification, we deduce
	\begin{align*}
		\hesstheta \scalarvalue(\policystar) \; = \;
		- \frac{1}{\parabeta} \, \Exp_{\prompt \sim \promptdistr} \Big[ \Cov_{\response \sim \policystar(\cdot \mid \prompt)} \big[ \gradtheta \rewardstar(\prompt, \response) \bigm| \prompt \big] \Big] \, ,
	\end{align*}
	which establishes equation~\eqref{eq:hessscalarvalue} as stated in \Cref{lemma:hess_scalarvalue}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsubsection{Proof of the Asymptotic Distribution in Equation~\eqref{eq:gap_distr}}
	\label{sec:proof:gap_distr}
	
	The goal of this part is to establish the asymptotic distribution of $\numobs \{ \scalarvalue(\policystar) - \scalarvalue(\policyhat) \}$, as stated in equation~\eqref{eq:gap_distr} from \Cref{sec:proof:lemma:hess_scalarvalue}. To achieve this, we first recast the value gap into the product of two terms and then invoke Slutsky’s theorem.
	
	We start by writing
	\begin{align}
		\numobs \cdot \{ \scalarvalue(\policystar) - \scalarvalue(\policyhat) \}
		\; = \;  \underbrace{\numobs \cdot (\parathetahat - \parathetastar)^{\top} \HessMt \, (\parathetahat - \parathetastar)}_{\Un}
		\cdot \underbrace{\frac{\scalarvalue(\policystar) - \scalarvalue(\policyhat)}{(\parathetahat - \parathetastar)^{\top} \HessMt \, (\parathetahat - \parathetastar)}}_{\Vn} \, .
	\end{align}
	By isolating \(\Un\) and \(\Vn\) in this way, we can handle their limiting behaviors separately:
	\begin{subequations}
	\begin{align}
		& \Un \; \converged \; \vecz^{\top} \CovOmega^{\frac{1}{2}} \HessMt \CovOmega^{\frac{1}{2}} \vecz \qquad \mbox{with $\vecz \sim \Gauss(\veczero, \IdMt)$},  \label{eq:Un_distr} \\
		& \Vn \; \convergep \; \frac{1}{2} \, .  \label{eq:Vn_distr}
	\end{align}
	\end{subequations}
	If these two results are established, the desired asymptotic distribution of the value gap, as given in equation~\eqref{eq:gap_distr}, follows directly from Slutsky’s theorem.
	
	To complete the proof, we proceed to verify equations~\eqref{eq:Un_distr} and~\eqref{eq:Vn_distr}. It is worth noting that equation~\eqref{eq:Un_distr} is a straightforward corollary of \Cref{thm:asymp}, so the main task is to establish the convergence result in equation~\eqref{eq:Vn_distr}.
	
	
	\paragraph{Proof of Equation~\eqref{eq:Vn_distr}:}
	
	Since $\CovOpstar$ is nonsingular, the matrix $\HessMt = (\Partitionthetabar / \parabeta) \cdot \CovOpstar$ is also nonsingular.
	From equation~\eqref{eq:Taylor_scalarvalue}, we know that for any $\varepsilon \in (0, 1)$, there exists a threshold $\eta(\varepsilon) > 0$ such that whenever $\norm{\paratheta - \parathetastar}_2 \leq \eta(\varepsilon)$, the following inequality holds:
	\begin{align*}
		\Big( \frac{1}{2} - \varepsilon \Big) \, (\paratheta - \parathetastar)^{\top} \HessMt \, (\paratheta - \parathetastar)
		\; \leq \; \scalarvalue(\policystar) - \scalarvalue(\policytheta)
		\; \leq \; \Big( \frac{1}{2} + \varepsilon \Big) \, (\paratheta - \parathetastar)^{\top} \HessMt \, (\paratheta - \parathetastar) \, .
	\end{align*}
	This can be reformulated as
	\begin{align*}
		\abs[\Big]{\Vn - \frac{1}{2}} \; \leq \; \varepsilon \, .
	\end{align*}
	Next, under the condition that $\parathetahat \convergep \parathetastar$, for any $\delta > 0$, there exists an integer $N(\varepsilon, \delta) \in \Intpos$ such that for any $\numobs \geq N(\varepsilon, \delta)$,
	\begin{align*}
		\Prob \big\{ \norm{\parathetahat - \parathetastar}_2 > \eta(\varepsilon) \big\} \leq \delta \, .
	\end{align*} 
	Therefore, for any $\numobs \geq N(\varepsilon, \delta)$, we can conclude
	\begin{align*}
		\Prob \bigg\{ \abs[\Big]{\Vn - \frac{1}{2}} \; > \; \varepsilon \bigg\} \; \leq \; \delta \, .
	\end{align*}
	In simpler terms, $\Vn \convergep \frac{1}{2}$, which establishes equation~\eqref{eq:Vn_distr}.
	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsubsection{Proof of the Tail Bound in Equation~\eqref{eq:gap_bd}}
	\label{sec:proof:chisqtail}
	
	We now establish the tail bound
	\begin{align}
		\label{eq:chi_tail}
		\Prob\big\{ \chisquare_\Dim > (1 + \varepsilon) \, \Dim \big\}
		\;\leq\;
		\exp\Big\{-\frac{\Dim}{2} \bigl(\varepsilon - \log(1 + \varepsilon)\bigr)\Big\},
	\end{align}
	as stated in equation~\eqref{eq:gap_bd}.
	
	We first note that the moment-generating function (MGF) of distribution $\chisquare_\Dim$ is given by
	\begin{align*}
		\MMt(t) = (1 - 2t)^{-\frac{\Dim}{2}}, \quad \mbox{for any $t < \frac{1}{2}$}.
	\end{align*}
	Using Markov’s inequality, for any $t > 0$, we have
	\begin{align}
		\label{eq:chi_MMt}
		\Prob\big\{\chisquare_{\Dim} > (1 + \varepsilon) \, \Dim\big\}
		\;\leq\; \exp\{-t(1 + \varepsilon)\Dim\} \cdot \MMt(t)
		\; = \; \exp\{-t(1 + \varepsilon)\Dim\} \cdot (1 - 2t)^{-\frac{\Dim}{2}}
	\end{align}
    for any $t < \frac{1}{2}$.
	We optimize the bound by choosing $t$ to minimize the exponent $-t(1 + \varepsilon)\Dim - \frac{\Dim}{2}\log(1 - 2t)$.
	Solving for the optimal $t$, we obtain
	\begin{align*}
		t \; = \; \frac{\varepsilon}{2(1 + \varepsilon)} \, .
	\end{align*}
	Substituting $t$ back into inequality~\eqref{eq:chi_MMt}, the bound simplifies to the desired inequality~\eqref{eq:chi_tail}.
	

	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{Supporting Theorem: \\ Master Theorem for $Z$-Estimators}
	\label{sec:master}
	
	In this section, we provide a brief introduction to the master theorem for $Z$-estimators for the convenience of the readers.
	
	Let the parameter space be $\Theta$, and consider a data-dependent function $\Psi_n: \Theta \to \mathds{L}$, where $\mathds{L}$ is a metric space with norm~$\|\cdot\|_{\mathds{L}}$. Assume that the parameter estimate $\widehat{\theta}_n \in \Theta$ satisfies $\|\Psi_n(\widehat{\theta}_n)\|_{\mathds{L}} \convergep 0$, making $\widehat{\theta}_n$ a $Z$-estimator. The function~$\Psi_n$ is an estimator of a fixed function $\Psi: \Theta \to \mathds{L}$, where $\Psi(\theta_0) = 0$ for some parameter of interest $\theta_0 \in \Theta$.
	
	\begin{theorem}[Theorem~2.11 in \citet{kosorok2008introduction}, master theorem for $Z$-estimators]
		\label{thm:master}
		Suppose the following conditions hold:
		\begin{enumerate}
			\item $\Psi(\theta_0) = 0$, where $\theta_0$ lies in the interior of $\Theta$.
			\item $\sqrt{n} \, \Psi_n(\widehat{\theta}_n) \convergep 0$ and $\|\widehat{\theta}_n - \theta_0\| \convergep 0$ for the sequence of estimators $\{\widehat{\theta}_n\} \subset \Theta$.
			\item $\sqrt{n} (\Psi_n - \Psi)(\theta_0) \converged Z$, where $Z$ is a tight\footnote{A random variable $Z$ is tight if, for any $\epsilon > 0$, there exists a compact set $K \subset \Real$ such that $\Prob(Z \notin K) < \epsilon$.} random variable.
			\item The following smoothness condition is satisfied:
			\begin{align}
				\label{eq:master_cond}
				\frac{\big\| \sqrt{n} \big(\Psi_n(\widehat{\theta}_n) - \Psi(\widehat{\theta}_n)\big) - \sqrt{n} \big(\Psi_n(\theta_0) - \Psi(\theta_0)\big) \big\|_{\mathds{L}}}{1 + \sqrt{n} \, \| \widehat{\theta}_n - \theta_0 \|} \; \convergep \; 0 \, .
			\end{align}
		\end{enumerate}
		
		Additionally, assume that $\theta \mapsto \Psi(\theta)$ is Fréchet differentiable\footnote{Fréchet differentiability: A map $\phi: \mathds{D} \to \mathds{L}$ is Fréchet differentiable at $\theta$ if there exists a continuous, linear map $\phi_{\theta}': \mathds{D} \to \mathds{L}$ such that
		${\| \phi(\theta + h_n) - \phi(\theta) - \phi_{\theta}'(h_n) \|_{\mathds{L}}}/{\|h_n\|} \rightarrow 0$
		for all sequences $\{h_n\} \subset \mathds{D}$ with $\|h_n\| \to 0$ and $\theta + h_n \in \Theta$ for all $n \geq 1$.} at $\theta_0$
		with derivative $\dot{\Psi}_{\theta_0}$, and that $\dot{\Psi}_{\theta_0}$ is continuously invertible\footnote{Continuous invertibility: A map $A: \Theta \to \mathds{L}$ is continuously invertible if $A$ is invertible, and there exists a constant $c > 0$ such that $\|A(\theta_1) - A(\theta_2)\|_{\mathds{L}} \geq c \|\theta_1 - \theta_2\|$ for all $\theta_1, \theta_2 \in \Theta$.}.
		Then
		\begin{align*}
			\big\| \sqrt{n} \dot{\Psi}_{\theta_0}(\widehat{\theta}_n - \theta_0) + \sqrt{n} (\Psi_n - \Psi)(\theta_0) \big\|_{\mathds{L}} \convergep 0
		\end{align*}
		and therefore
		\begin{align*}
			\sqrt{n} \, \big(\widehat{\theta}_n - \theta_0\big) \; \converged \; - \dot{\Psi}_{\theta_0}^{-1} \, Z \, .
		\end{align*}
	\end{theorem}
	
	


	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
%	\tableofcontents
