\section{Extension to Proximal Policy Optimization (PPO)}
\label{app:extension}

In this section, we briefly explore how the core principles of our PILAF sampling approach can be extended to PPO-based RLHF methods.

\paragraph{Integrating Response Sampling in InstructGPT:}

The PPO-based RLHF pipeline used in InstructGPT \citep{ouyang2022training} consists of three key steps: \vspace{-.8em}
\begin{enumerate} \itemsep = -.3em
    \item[(i)] Supervised Fine-Tuning (SFT) that produces the reference model $\policyref$.
    \item[(ii)] Reward Modeling (RM) by solving the optimization problem~\eqref{eq:RM_objective}, yielding an estimated reward function $\rewardtheta$.
    \item[(iii)] Reinforcement Learning Fine-Tuning, where the policy $\policyphi$ is optimized against the reward model $\rewardtheta$ using the Proximal Policy Optimization (PPO) algorithm, following the optimization scheme~\eqref{eq:policy_loss_with_rm}.
\end{enumerate}
\vspace{-.8em}
The key distinction between the PPO and DPO approaches lies in how the reward model $ \rewardtheta $ is representedâ€”explicitly in PPO and implicitly in DPO.
In response sampling for data collection, it is crucial to consider the iterative nature of the InstructGPT pipeline. During each iteration, additional human-labeled data is collected for reward modeling (step~(ii)), and steps (ii) and (iii) are repeatedly applied to refine the model. Our proposed PILAF algorithm naturally integrates into this pipeline by improving the data collection process in step~(ii), thereby enhancing reward model training and, in turn, policy optimization.

\paragraph{Extensions of T-PILAF and PILAF:}
Extending our response sampling methods, PILAF and T-PILAF, to the PPO setup with an explicit $ \rewardtheta $ is both natural and straightforward.
\begin{itemize}
    \item Within the theoretical framework of T-PILAF, as introduced in \Cref{sec:sampling}, the only required modification is replacing $\policytheta$ with the language model $\policyphi$ and redefining the interpolated and extrapolated policies, $\policyphipos$ and $\policyphineg$, following the same formulation as in equations~\eqref{eq:def_policythetapos}~and~\eqref{eq:def_policythetaneg}.
    Specifically, we define
    \begin{subequations}
	\begin{align}
		\label{eq:def_policythetapos_PPO}
		\policyphipos(\response \mid \prompt)
		& := \frac{1}{\Partition^+(\prompt)} \; \policyphi(\response \mid \prompt)
		\exp \big\{ \rewardtheta(\prompt, \response) \big\} \, ,  \\[-1pt]
		\label{eq:def_policythetaneg_PPO}
		\policyphineg(\response \mid \prompt)
		& := \frac{1}{\Partition^-(\prompt)} \, \policyphi(\response \mid \prompt) \,
		\exp \big\{ - \rewardtheta(\prompt, \response) \big\},
	\end{align}
    \end{subequations}
    where $\rewardtheta$ is now explicitly produced by a reward network, rather than being implicitly derived from $\policyphi$, as in equation~\eqref{eq:def_reward}.
    \item To extend our empirical PILAF algorithm, as described in \Cref{sec:sampling_exp}, we propose applying the same interpolation and extrapolation techniques directly to the logits of the language models $\policyphi$ and $\policyref$.
    In particular, we take
    \begin{align*}
        & \policyphipos(\cdot \mid \prompt, \tokenttot{1}{t-1}) \; = \; \softmax\Big( \big\{ (1 + \parabeta) \, \headphi - \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big), \\
        & \policyphineg(\cdot \mid \prompt, \tokenttot{1}{t-1}) \; = \; \softmax\Big( \big\{ (1 - \parabeta) \, \headphi + \parabeta \, \headref\big\} (\prompt, \tokenttot{1}{t-1}) \Big),
    \end{align*}
    where $\headphi$ and $\headref$ represent the logits of the language models $\policyphi$ and $\policyref$, respectively.
\end{itemize}

\paragraph{Adaption of Theoretical Analysis:}
Our theoretical analyses can be extended to the PPO framework, assuming that the optimization process~\eqref{eq:policy_loss_with_rm} in step~(iii) of InstructGPT is solved exactly. In this case, the policy satisfies~\mbox{$\policyphi = \policyt{\rewardtheta}$}, where
\begin{align*}
	\policyt{\rewardtheta}(\response \mid \prompt)
	\; \defn \; \frac{1}{\Partitiontheta(\prompt)} \, \policyref(\response \mid \prompt) \exp \Big\{ \frac{1}{\parabeta} \, \rewardtheta(\prompt, \response) \Big\} \, .
\end{align*}
Under this assumption, the output language model $\policyphi$ is implicitly a function of the parameter $\paratheta$.
Building on this, we can adapt our optimization and statistical analyses as follows:

\begin{itemize}
    \item {\bf Optimization Consideration:}
    Using the same argument as in \Cref{thm:grad}, we can prove that
    \begin{align*}
        \gradtheta \Loss(\paratheta) \; = \;
    - \, \Const' \cdot \gradtheta \scalarvalue(\policyphi) \, + \, \Term_2 \, ,
    \end{align*}
    where $\Const' > 0$ is a universal constant, and $\Term_2$ represents a second-order approximation error.
    
    In other words, if the policy optimization step is sufficiently accurate for the reward model $\rewardtheta$, then performing gradient descent on the MLE loss with respect to $\paratheta$ is equivalent to applying gradient ascent on the oracle objective $\scalarvalue$, following the steepest direction in the parameter space of $\paratheta$.
    \item {\bf Statistical Consideration:}
    Even with the new parameterization, the asymptotic distribution of $\parathetahat$ from \Cref{thm:asymp} remains unchanged. Moreover, the gradient and Hessian of $\scalarvalue$ with respect to $\paratheta$ retain the same form as in \Cref{thm:grad}. As a result, the statistical analysis extends naturally to PPO, allowing us to conclude that PILAF also maintains structure-invariant statistical efficiency for PPO methods.
\end{itemize}


