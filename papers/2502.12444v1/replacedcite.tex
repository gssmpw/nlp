\section{Related Work}
\textbf{Quantization} Weight quantization in LLMs has been extensively studied, achieving high model quality with as few as three or four bits____. Quantization of both weights and activations has also advanced significantly, enabling four-bit computation while maintaining accuracy____. 

Given that trend, support for low-bit datatypes and operations has been added in recent CPUs and GPUs. For example, Intel Sapphire Rapids CPUs support INT8 operations, and NVIDIA's H100 Tensor Cores support INT8 and INT4 operations. To harness these features, specialized kernels have been developed, demonstrating up to a 3$\times$ speedup over full-precision models____, with further optimizations achieving an additional 1.9$\times$ speedup____.

Some kernels, such as ____ efficiently run 4-bit models on multiple Intel platforms, including AMX, but do not incorporate sparsity.

\begin{figure*} [t]
    \centering
    \includegraphics[width=1\linewidth]{paper_plots/dense_kernel.pdf}
    \vspace{-10pt}
    \caption{\textbf{AMX Dense Kernel: }(1) Tiles 0, 1, 2, and 3 act as accumulators for the results computed by multiplying (4x6), (4x7), (5x6), and (5x7) respectively. (2) Tiles 4 and 5 are utilized to load all columns of the input rows (denoted as out\_rows) while tiles 6 and 7 are utilized to load all rows of the weight columns. (3) After the loop over the inner dimension ends, results are stored in memory and a new set of result tiles is initialized and computed in the same way. (4) Some boundary conditions might occur near the end of the matrix.}
    \label{fig:dense_kernel}
\end{figure*}

\textbf{Sparsity} Like quantization, sparsity can be used to accelerate LLM workloads, as introduced in Section~\ref{sec:background_sparsity}. Many studies focus on static structured sparsity____, which removes entire components, often yielding strong speedups at the cost of accuracy. For instance, LLM-Pruner____ performs one-shot structured pruning using gradient information and reaches only 20\% sparsity with an extra perplexity of 3.6 points on WikiText-2____. 
Other works leverage dynamic structured sparsity to optimize the runtime of LLMs on the GPU____.

For unstructured sparsity, techniques such as SparseGPT____ and Wanda____ achieve up to 50\% pruning. However, these methods often incur accuracy losses—for example, 3.47\% for SparseGPT on Llama 2 7B. Wanda’s semi-structured variant suffers even greater perplexity degradation (+2.69) compared to its unstructured counterpart (+1.01) ____. Some techniques like Shears____ can recover some of the lost accuracy with additional fine-tuning.
As wall-time acceleration of unstructured sparsity can be difficult, Flash-LLM____ proposes a GPU kernel that makes use of unstructured sparsity to minimize memory transfer requirements by cleverly using the general purpose GPU streaming multi-processors and tensor cores, while Marlin____ develops a GPU kernel that makes use of both quantization and semi-structured sparsity to improve performance.


While most prior works focus on GPUs, recent research has explored unstructured sparsity on CPUs. SparseDNN____ uses static code generation to process only non-zero elements efficiently. DeepSparse Engine____ applies unstructured sparsity with additional optimizations to accelerate LLMs, though it does not leverage AMX units and is closed-source. Our work integrates unstructured sparsity with AMX on Sapphire Rapids CPUs to improve LLM performance. Furthermore, we extend the unstructured sparsity to compress the KV cache and show 1.14$\times$ speedup using our kernel in the attention mechanism.
Some recent work showed that utilizing a CPU with AMX can help optimize the performance of a CPU-GPU system____. Our work aims to make running operations utilizing AMX even faster.