\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\iclrfinalcopy

\usepackage[colorlinks, linkcolor=red, anchorcolor=red, citecolor=cyan, breaklinks]{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{subfiles}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{bbding}
\usepackage{float}
\usepackage{stfloats}
\usepackage{pifont}
\makeatletter
\renewcommand{\dblfloatpagefraction}{.95}
\newcommand\figcaption{\def\@captype{figure}\caption}
\newcommand\tabcaption{\def\@captype{table}\caption}

\definecolor{mark}{rgb}{1,0,0}
\definecolor{modify}{rgb}{1,0,0}


\title{A Simple yet Effective {\Large
$\Delta\Delta G$} Predictor is An Unsupervised Antibody Optimizer and Explainer}
% \title{Uni-Anti: A Unified Framework for Antibody Optimization, Estimation and Explanation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Lirong Wu$^{1,2}$, Yunfan Liu$^{1}$, Haitao Lin$^{1,2}$, Yufei Huang$^{1}$, \textbf{Guojiang Zhao}$^{2}$, \textbf{Zhifeng Gao}$^{2}$, \textbf{Stan Z. Li}$^{1,\dagger}$ \\
$^{1}$Westlake University, $^{2}$DP Technology \\
\texttt{\{wulirong, liuyunfan, linhaitao, huangyufei, stan.zq.li\}@westlake.edu.cn} \\ \texttt{\{zhaogj, gaozf\}@dp.tech}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on the fitness landscape is beneficial. There have been numerous priors used to constrain protein evolution to regions of landscapes with high-fitness variants, among which \emph{the change in binding free energy} ($\Delta\Delta G$) of protein complexes upon mutations is one of the most commonly used priors. However, the huge mutation space poses two challenges: (1) how to improve the efficiency of $\Delta\Delta G$ prediction for fast mutation screening; and (2) how to explain mutation preferences and efficiently explore accessible evolutionary regions. To address these challenges, we propose a lightweight $\Delta\Delta G$ predictor (Light-DDG), which adopts a structure-aware Transformer as the backbone and enhances it by knowledge distilled from existing powerful but computationally heavy $\Delta\Delta G$ predictors. Additionally, we augmented, annotated, and released a large-scale dataset containing millions of mutation data for pre-training Light-DDG. We find that such a simple yet effective Light-DDG can serve as a good unsupervised antibody optimizer and explainer. For the target antibody, we propose a novel Mutation Explainer to learn mutation preferences, which accounts for the marginal benefit of each mutation per residue. To further explore accessible evolutionary regions, we conduct preference-guided antibody optimization and evaluate antibody candidates quickly using Light-DDG to identify desirable mutations. Extensive experiments have demonstrated the effectiveness of Light-DDG in terms of test generalizability, noise robustness, and inference practicality, e.g., 89.7$\times$ inference acceleration and 15.45\% performance gains over previous state-of-the-art baselines. A case study of SARS-CoV-2 further demonstrates the crucial role of Light-DDG for mutation explanation and antibody optimization. Codes are available in \href{https://github.com/LirongWu/Uni-Anti}{Github}, and an online \href{http://www.manimer.com:9090/antibody}{Platform} is available for researchers.
\end{abstract}


\vspace{-1em}
\section{Introducrtion}
\vspace{-0.5em}
Proteins usually interact with other proteins to form protein complexes that perform specific functions in biological processes~\citep{hu2021survey,wu2024mapeppi}. A representative example is antibody, a $Y$-shaped protein that protects the host by binding to a specific antigen, whose binding function is mainly determined by Complementary Determining Regions (CDRs) in the antibody~\citep{murphy2016janeway}. In practice, how to \underline{mutate} the amino acids on the interaction surface and \underline{select} favorable mutations are two fundamental aspects of antibody optimization. There have been many antibody design methods proposed, such as MEAN~\citep{kong2022conditional}, RefineGNN~\citep{jin2021iterative}, and dyMEAN~\citep{kong2023end}, which train conditional antibody generators on large amounts of antibody-antigen complexes and then optimize antibodies by applying \emph{Iterative Target Augmentation} (ITA) algorithm~\citep{yang2020improving} to fine-tune the generators. Despite the great success in conditional generation for mutations, how to build an efficient evolutionary selection sieve~\citep{hayes2024simulating} for fast screening of mutations remains under-explored. In this paper, we shift the research focus from generating to selecting mutations and indirectly explore the underlying fitness landscape by focusing on regions where $\Delta\Delta G$s over mutations are minimized.  We demonstrate that even a simple but effective $\Delta\Delta G$ predictor can serve as a good unsupervised antibody optimizer and explainer, which doesn't require any additional functional annotations and deep generative models.

% However, these deep generative models are usually susceptible to the quantity and quality of training data, cannot cover the co-design of multiple CDRs on both heavy and light chains, and struggle to provide explanations for why mutations occur. In this paper, we shift the focus of our research from generative antibody optimization to discriminative ones, building an evolutionary selection sieve with $\Delta\Delta G$ prediction at its core~\citep{hayes2024simulating}. We demonstrate that even a simple yet effective $\Delta\Delta G$ predictor can serve as a good antibody optimizer and explainer.

A huge challenge for protein optimization is the enormous combinatorial space of over $20^{N}$ potential mutations, where $N$ is the number of mutable sites. Therefore, two aspects need to be considered in the design: (1) how to develop a simple but effective $\Delta\Delta G$ predictor for fast screening of candidate mutations in a relatively short time; (2) how to explain mutation preferences and efficiently search for accessible evolutionary paths, i.e., promising mutations, from the enormous combinatorial space.

Recently, unsupervised energy-based models~\citep{jin2023dsmbind,luo2023rotamer} have revealed that the log-likelihood of protein complexes is highly correlated with experimental binding energy, making $\Delta\Delta G$ one of the suitable priors for guiding protein evolution. Early computational approaches for $\Delta\Delta G$ prediction are mainly biophysics-based~\citep{alford2017rosetta, park2016simultaneous,delgado2019foldx} or statistics-based~\citep{geng2019isee,li2016mutabind}, which are limited either in efficiency or effectiveness. Recently, many deep learning-based techniques have been proposed, most of which tackle the scarcity of annotated experimental data by pre-training on massive unlabeled data using a variety of pretext tasks, including Masked Inverse Folding, Rotamer Density Estimation (RDE)~\citep{luo2023rotamer}, Side-chain Diffusion (DiffAffinity)~\citep{yang2022masked}, Multi-level Interaction Modeling (ProMIM)~\citep{mo2024multi}. Another state-of-the-art $\Delta\Delta G$ predictor is Prompt-DDG~\citep{wu2024learning}, which flexibly provides wild-type and mutated complexes with their microenvironmental differences around each mutation. Despite the great advances, the architectural complexity of these methods burdens inference, which is largely due to their reliance on the IPA-style backbone as in AlphaFold2~\citep{jumper2021highly}, which encodes local and global coordinates at each layer.

% In other words, they focus on how to generate rather than screen for antibodies. These methods encounter several challenges in practice: (1) poor diversity, the optimized antibodies are the average likelihood of the distribution of the training data; (2) poor flexibility, they cannot jointly optimize multiple CDRs on the light and heavy chains; and (3) unexplainability, failing to explain the reasons behind the mutations.

\begin{wrapfigure}{rb}{0.5\textwidth}
\vspace{-2.0em}
\begin{center}
\includegraphics[width=0.5\textwidth]{fig_speed.pdf}  
\end{center}
\vspace{-1.8em}
\caption{Efficiency vs Effectiveness. There are three variants of Light-DDG with differing numbers of attention heads (default to 4 in this paper).}
\label{fig:1}
\end{wrapfigure}

To develop a simple yet effective $\Delta\Delta G$ predictor (Light-DDG), it requires the fulfillment of both efficiency and effectiveness. For the goal of high inference efficiency, we simplify the architecture to a lightweight Transformer and achieve model compression by knowledge distillation. From the perspective of effectiveness, we use data augmentation techniques to compensate for the weakening of modeling capability brought by architectural simplification. To achieve this, we collected, annotated, and released a large-scale augmented dataset containing millions of mutation data for pre-training Light-DDG. A comparison of various $\Delta\Delta G$ prediction methods on the effectiveness and efficiency is presented in Fig.~\ref{fig:1}. It demonstrates the great advantages of Light-DDG, e.g., 89.7$\times$ inference acceleration and 15.45\% performance gains over Prompt-DDG. Furthermore, we comprehensively evaluate the advantages of Light-DDG in terms of test generalizability, noise robustness, architectural applicability, and inference practicality by extensive experiments in Sec.~\ref{sec:5.1}.

Furthermore, we show that even a simple yet effective Light-DDG has the potential to be a good explainer and optimizer within a \underline{Uni}fied framework for \underline{Anti}body optimization (Uni-Anti). For the target antibody, we propose a Mutation Explainer to identify key mutation sites and learn site-wise mutation preferences. One of the design difficulties is the synergistic effect of mutations, e.g., the negative effect of a single substitution can only be tolerated in the presence of another enabling mutation~\citep{ding2024protein}. To tackle this problem, we develop an iterative Shapley value estimation algorithm that can measure the \textit{marginal benefit} of each mutation per residue by coarse-to-fine iteration, while reducing the huge combinatorial space of vanilla Shapley value algorithm~\citep{shapley1953value}. Based on the learned mutation preferences, we explore accessible evolutionary regions by mutation preference-guided antibody optimization and then evaluate antibody candidates quickly using Ligh-DDG. Such antibody optimization enjoys the great benefits of diversity and flexibility, capable of generating diverse antibodies with corresponding $\Delta\Delta G$ scores and rankings, and is well suited for co-optimization of multiple CDRs. Finally, we demonstrate the advantages of Uni-Anti for antibody optimization and preference explanation using a case study on SARS-CoV-2. 
% We sincerely hope that Uni-Anti can inspire researchers to rethink antibody optimization from a discriminative rather than a generative perspective in future.

% The contributions of this paper can be summarized in three aspects: (1) We propose a unified $\Delta\Delta G$-guided framework for unsupervised directed evolution of antibodies, covering $\Delta\Delta G$ prediction, mutation explanation, and antibody optimization; (2) We develop a simple but effective $\Delta\Delta G$ predictor (Light-DDG) through knowledge distillation and data augmentation from the perspective of efficiency and effectiveness; (3) We propose a Mutation Explainer based on iterative Shapley value estimation to learn mutation preferences.


\vspace{-0.5em}
\section{Related Work}
\vspace{-0.3em}
\textbf{Mutational Effect Prediction.}
The prediction of mutation effects on single proteins has been well studied, which mainly mines co-evolutionary information from protein sequences by Multiple Sequence Alignments (MSAs)~\citep{frazer2021disease,luo2021ecnet} or Protein Language Models (PLMs)~\citep{meier2021language,notin2022tranception}. However, predicting \emph{the change in binding free energy} ($\Delta\Delta G$) of protein complexes upon mutations is more challenging because it involves complex interactions between proteins. Computational methods for $\Delta\Delta G$ prediction have undergone a paradigm shift from biophysics-based and statistics-based techniques~\citep{schymkowitz2005foldx,park2016simultaneous} to Deep Learning (DL) techniques, among which pre-training-based approaches are the most popular solutions. RDE~\citep{luo2023rotamer} pre-trains by using a normalizing flow model to estimate the density of sidechain conformations (rotamers). Similarly, DiffAffinity~\citep{liu2023predicting} also models the side-chain distribution, but with a conditional diffusion model. Besides, \cite{mo2024multi} proposes a multi-level pre-training framework, ProMIM, to fully capture all three levels of protein-protein interactions. Recently, Prompt-DDG~\citep{wu2024learning} proposes a microenvironment-aware hierarchical codebook that generates prompts for better $\Delta\Delta G$ prediction.

\textbf{Antibody Optimization.} Early approaches for antibody design are mostly energy-based~\citep{adolf2018rosettaantibodydesign,lapidoth2015abdesign}, and it is recently extended to deep generative models, including RefineGNN~\citep{jin2021iterative}, MEAN~\citep{kong2022conditional}, RAAD~\citep{wu2024relation}, DiffAb~\citep{luo2022antigen}, etc. These models train a conditional antibody generator and screen out a number of high-quality antibodies using a $\Delta\Delta G$ predictor. These high-quality antibodies will be used as training data to further fine-tune the antibody generator for directed antibody optimization. In this paper, we rethink the role of $\Delta\Delta G$ prediction for antibody optimization, demonstrating that a simple yet effective $\Delta\Delta G$ predictor can directly serve as a good unsupervised antibody optimizer and explainer, without requiring additional functional annotations or deep generative models.


\vspace{-0.5em}
\section{Preliminary}
\vspace{-0.3em}
\textbf{Notations.} A protein complex consists of $N$ amino acid residues $(v_1,v_2,\cdots,v_N)$, where each residue $v_i$ is one of the 20 amino acid types. We characterize each residue $v_i$ with an E(3)-invariant node feature $\mathbf{x}_i=\{E_{\text{type}}(v_i), E_{\text{ang}}(v_i), E_{\text{mut}}(v_i)\}$, where $E_{\text{type}}(v_i)$ denotes the embedding of residue types, $E_{\text{angle}}(v_i)$ is the angle encoding of three dihedral angles and four torsion angles, and $E_{\text{mut}}(v_i)$ denotes the mutation embedding on whether residue $v_i$ is mutated or not. The pairwise feature between residues $v_i$ and $v_j$ is $\mathbf{e}_{i,j}=\{E_{\text{pos}}(i,j), E_{\text{dis}}(\mathbf{Z}_i,\mathbf{Z}_j),  Q_i^{\top}\frac{\mathbf{Z}_{j,\zeta}\!-\!\mathbf{Z}_{i,C_\alpha}}{\left\|\mathbf{Z}_{j,\zeta}\!-\!\mathbf{Z}_{i,C_\alpha}\right\|} \big| \ \zeta\}$, where $\mathbf{Z}_i$ is the 3D coordinate of residue $v_i$, $E_{\text{pos}}(i,j)$ and $E_{\text{dis}}(\mathbf{Z}_i,\mathbf{Z}_j)$ encode the relative sequential and spatial distances between residue $v_i$ and residue $v_j$, respectively. $E_{\text{pos}}(i,j)$ is set as 0 for any two residues that are not on the same chain. Besides, the last term is the direction encoding of four backbone atoms $\zeta\in\{C_\alpha, C,N,O\}$ of residue $v_j$ in the local coordinate frame $Q_i$ of residue $v_i$~\citep{wu2024learning}. All these node and pairwise features will be pre-processed before model training.

\textbf{Transformer as the student backbone in Knowledge Distillation (KD).} To improve the inference efficiency of a $\Delta\Delta G$ predictor, a lightweight Transformer is used as the backbone to encode each protein complex $\mathcal{P}=(\mathbf{X}, \mathbf{E})$. The $l$-th ($1\leq l \leq L$) layer of the Transformer is defined as follows
\begin{equation}
\begin{aligned}
&\mathbf{H}^{(l)}=\operatorname{LN}\left(\operatorname{FFN}\left(\left[\operatorname{head}_1,\cdots,\operatorname{head}_K\right]\mathbf{W}_O^{(l)}\right) + \mathbf{H}^{(l-1)}\right), \text{where}\\  \operatorname{head}_k = &\operatorname{softmax}\Big(\frac{(\mathbf{H}^{(l-1)}\mathbf{W}^{(l,k)}_Q)(\mathbf{H}^{(l-1)}\mathbf{W}^{(l,k)}_K)^\top}{\sqrt{d_h}}+\mathbf{E}\mathbf{W}^{(l,k)}_E\Big)\mathbf{H}^{(l-1)}\mathbf{W}^{(l,k)}_V
\end{aligned}
\label{equ:1}
\end{equation}
where $\mathbf{H}^{(0)}\!=\!\mathbf{X}$ denote the input node feature, $\mathbf{W}_O^{(l)},\mathbf{W}_Q^{(k,l)},\mathbf{W}_K^{(k,l)},\mathbf{W}_V^{(k,l)},\mathbf{W}_E^{(k,l)}$ are parameter matrices, $K$ is the number of attention heads, $\operatorname{LN}(\cdot)$ is the layer normalization, $\operatorname{FFN}(\cdot)$ is a two-layer feed-forward neural network with $\operatorname{ReLu}(\cdot)$ as activation function, and $d_h$ is the hidden dimension.

\textbf{Prompt-DDG as Teacher and Annotator.} Prompt-DDG~\citep{wu2024learning} is the state-of-the-art $\Delta\Delta G$ predictor to date. During training, it trains a hierarchical prompt codebook to capture microenvironmental information at different structural scales. With the learned prompt codebook, it encodes the microenvironment around each mutation into multiple hierarchical prompts and combines them to flexibly provide information to wild-type and mutated protein complexes about their microenvironmental differences. We use Prompt-DDG as a teacher and annotator for distillation and data augmentation in default in this paper, but also evaluate other $\Delta\Delta G$ predictors as teachers.

\textbf{Problem Statement.} Given a wild-type protein complex $\mathcal{P}_W$ and a set of mutations $\mathcal{M}$, the task of mutational effect prediction aims to learn a mapping $f(\cdot):\mathcal{P}_W,\mathcal{M}\rightarrow \Delta\Delta G$ that encodes wild-type complex $\mathcal{P}_W$ and mutated complex $\mathcal{P}_M\!=\!g(\mathcal{P}_W, \mathcal{M})$ separately with a parameter-shared Transformer, and then feeds the difference of their pooled representations $\mathbf{h}^{W}$ and $\mathbf{h}^{M}$ into a three-layer MLP to predict the $\Delta\Delta G$ score. The objective of protein (antibody) complex optimization aims to find a mutation $\mathcal{S}$ from the mutation space $\mathbb{S}$ that minimizes $\Delta\Delta G$, that is, $\argmin_{\mathcal{S}\in\mathbb{S}} \;f(\mathcal{P}_W, \mathcal{S})$.


\vspace{-0.5em}
\section{Methodology}
\vspace{-0.5em}
In this section, we propose a unified framework for directed antibody optimization with a simple but effective $\Delta\Delta G$ predictor as the core. A high-level overview of the proposed framework is shown in Fig.~\ref{fig:3b}. We first present how to construct a large-scale augmented mutation dataset SKEMPI-Aug by cross-augmentation in Sec.~\ref{sec:4.1}. Next, we pre-train a simple but effective Light-DDG on the large-scale augmented SKEMPI-Aug dataset and then fine-tune it by knowledge distillation on the SKEMPI v2.0 dataset, as described in Sec.~\ref{sec:4.2}. Further, we propose a Mutation Explainer to learn key mutation sites and mutation preferences in Sec.~\ref{sec:4.3}, and finally introduce how to perform preference-guided mutation search in Sec.~\ref{sec:4.4}. From the perspective of the energy landscape in Fig.~\ref{fig:3a}, Light-DDG establishes a mapping from mutations to energy changes ($\Delta\Delta G$), while Mutation Explainer iteratively explores evolutionary accessible regions based on mutation preferences.

\begin{wraptable}{rb}{0.43\textwidth}
\vspace{-2.1em}
\caption{Characteristics of three datasets.}
\vspace{0.5em}
\label{tab:1}
\resizebox{0.45\columnwidth}{!}{
\begin{tabular}{c|ccc}
\toprule
\textbf{Dataset} & \textbf{Size} & \textbf{Mutation} & \textbf{Pre-training} \\
\midrule
SKEMPI v2.0 & 7k & \Checkmark & \XSolidBrush \\
PDB-REDO & 143k & \XSolidBrush & unsupervised \\
SKEMPI-Aug & 640k & \Checkmark & supervised\\
\bottomrule
\end{tabular}}

\vspace{0.5em}
\includegraphics[width=0.45\textwidth]{fig_crossaug.pdf}  
\vspace{-2.2em}
\figcaption{A schematic diagram of the $K$-fold cross-augmentation, where blue and red boxes indicate the separate folds used for training and data augmentation.}
\label{fig:2}

\end{wraptable}


\subsection{A Large-Scale Augmented Mutation Dataset for Supervised Pre-training} 
 \label{sec:4.1}
Considering the scarcity of experimental data in the SKEMPI v2.0 dataset, pre-training on large amounts of mutations-irrelevant data has become a popular practice for training $\Delta\Delta G$ predictors. One of the most commonly used pre-training datasets is PDB-REDO~\citep{joosten2014pdb_redo}, in which several \emph{unsupervised pre-training} tasks~\citep{luo2023rotamer,yang2022masked,mo2024multi}, have been proposed to learn generalized knowledge. However, in order to improve the inference efficiency, we use a lightweight transformer as the backbone in this paper, which has weaker modeling capability than the IPA-style backbone, making it hard to directly learn useful knowledge patterns for $\Delta\Delta G$ prediction from massive unlabeled data in an unsupervised manner. Therefore, we here consider \emph{supervised pre-training}, but the upcoming challenge is how to construct a large-scale dataset that covers a sufficiently wide range of mutation possibilities and their corresponding $\Delta\Delta G$ scores. In this subsection, we take data augmentation as an effective means of compensating for the simplification of the architecture. To augment data, we use Prompt-DDG, the current state-of-the-art $\Delta\Delta G$ predictor, as an annotator. Specifically, \textcolor{black}{we perform arbitrary mutations on several randomly selected mutation sites of complexes from the SKEMPI v2.0 dataset}, feed the mutated complexes into Prompt-DDG to predict $\Delta\Delta G$ scores, and then package the mutations and predicted $\Delta\Delta G$s into one piece of augmentation data. \textcolor{black}{To prevent data leakage, we propose $K$-fold cross-augmentation as shown in Fig.~\ref{fig:2}, where the SKEMPI v2.0 dataset is divided into $K$ equal-sized folds according to the complex structure. For each round of augmentation, we first train a new Prompt-DDG from scratch with $K$-1 folds, then augment the remaining 1 fold by random sampling and random mutation, and finally annotate it by Prompt-DDG. As a result, the data used to train Prompt-DDG is separate from the data annotated by Prompt-DDG to avoid any possible data leakage. Moreover, we set a threshold during augmentation to ensure that the augmented samples are sufficiently different from the original samples to further avoid data leakage.} In such a way, we have augmented, annotated, and released a large-scale dataset called SKEMPI-Aug, which contains millions of mutation data that can be used for supervised pre-training of $\Delta\Delta G$ predictors. We compare in Table.~\ref{tab:1} the data sizes of three datasets, whether they include labeled mutation data, and how they are used for pre-training.
% We also conduct extensive experiments to show that supervised pre-training on the augmented SKEMPI-Aug helps to improve the performance of various models, including RDE-PPI, DiffAffinity, and Prompt-DDG.

\begin{figure}[!tbp]
    \vspace{-2em}
    \begin{center}	   
        \subfigure[Energy Landscape]{\includegraphics[width=0.22\linewidth]{fig_landscape.pdf}\label{fig:3a}}
        \subfigure[A High-level Overview of Uni-Anti with Light-DDG as the core.]{\includegraphics[width=0.77\linewidth]{fig_framework.pdf}\label{fig:3b}}
    \end{center}
    \vspace{-0.5em}
    \caption{(a) A binding energy landscape reflecting the mapping from mutations to $\Delta\Delta G$ scores. (b) Pre-training a Light-DDG with augmentation and distillation, and then using it as the core, together with mutation explainer and search, to construct a unified framework for antibody optimization.}
    \label{fig:3}
\end{figure}


\subsection{A Simple but Effective $\Delta\Delta G$ Predictor by Augmentation and Distillation} 
\label{sec:4.2}
Knowledge Distillation (KD) is an effective means of achieving model compression~\citep{hinton2015distilling}, and the distilled student models can even exhibit better performance than the corresponding teacher models. In this paper, we combine knowledge distillation techniques with supervised pre-training to build a simple but effective $\Delta\Delta G$ predictor (Light-DDG). Specifically, we first perform supervised pre-training on the large-scale SKEMPI-Aug dataset $\mathcal{D}_{\text{Aug}}$, and then fine-tune the model on the SKEMPI v2.0 dataset $\mathcal{D}_{\text{Skem}}$ under the joint supervision of ground-truth labels and distillation losses. \textcolor{black}{Since cross-validation on SKEMPI v2.0 is performed in this paper to validate the method, the distillation objective of Light-DDG on the training data $\mathcal{D}_{\text{train}}\subseteq\mathcal{D}_{\text{Skem}}$ can be defined as:}
\begin{equation}
\begin{aligned}
f^*_S=\arg\mathop{\min}_{f^\prime_S} \frac{1}{|\mathcal{D}_{\text{train}}|}\sum_{(a_i,y_i)\in\mathcal{D}_{\text{train}}}\bigg(\underbrace{\big\|f^\prime_S(a_i)- y_i\big\|^2}_{\mathcal{L}_{\Delta\Delta G}} + \beta\underbrace{\big\|f^\prime_S(a_i)- f_T^*(a_i)\big\|^2}_{\mathcal{L}_{\text{KD}}} \bigg),
\end{aligned}
\label{equ:2}
\end{equation}
where $\beta$ is a trade-off hyperparameter, $a_i=(\mathcal{P}_i, \mathcal{M}_i)$ is the input to $\Delta\Delta G$ predictor $f(\cdot)$, $y_i$ is the ground-truth $\Delta\Delta G$. In this paper, we default to Prompt-DDG as the teacher $f_T^*(\cdot)$, but we also observed significant improvements when using other $\Delta\Delta G$ predictors as teachers in the experiments. The student model $f^\prime_S(\cdot)$ is initialized by pre-training on the SKEMPI-Aug dataset $\mathcal{D}_{\text{Aug}}$, as follows
\begin{equation}
\begin{aligned}
f^\prime_S=\arg\mathop{\min}_{f_S} \frac{1}{|\mathcal{D}_{\text{Aug}}|}\sum_{(a_i,y_i)\in\mathcal{D}_{\text{Aug}}}\underbrace{\big\|f_S(a_i)- y_i\big\|^2}_{\mathcal{L}_{\Delta\Delta G}}.
\end{aligned}
\label{equ:3}
\end{equation}


\subsection{Mutation Explainer: Learning Mutation Sites and Mutation Preferences}
\label{sec:4.3}
A key challenge in antibody optimization is how multiple mutations combine to influence function and future mutation trajectories~\citep{ding2024protein}. With a simple but effective Light-DDG available, we propose a novel Mutation Explainer that can identify key mutation sites and learn mutation preferences for each residue site. This is achieved by calculating the Shapley value~\citep{shapley1953value} for each mutation at each site as its \emph{marginal benefit}, which explains very well the \emph{``average"} marginal contribution of each mutation across all mutation combinations. The exact Shapley value $\varphi(i,j)$ of the $i$-th ($1 \leq i \leq N$) site mutated to $j$-th ($1 \leq j \leq 20$) amino acids is defined as follows

\begin{equation}
\varphi(i,j)=\sum_{\mathcal{M} \subseteq \mathbb{S}\backslash (i,j)} \frac{|\mathbb{S}|!(|\mathcal{M}|-|\mathbb{S}|-1)!}{|\mathcal{M}|!}\Big(f_S^*\big(\mathcal{P}, \mathcal{M} \cup\{(i,j)\}\big)-f_S^*\big(\mathcal{P}, \mathcal{M}\big)\Big)
\label{equ:SV}
\end{equation}

The exact Shapley value $\varphi(i,j)$ is calculated by considering the effects on $\Delta\Delta G$ scores when each mutation $(i,j)$ is added or removed from the mutation set $\mathcal{M}$. However, it is impractical to enumerate all mutation possibilities in the huge mutation space $\mathbb{S}$ to calculate an exact Shapley value $\varphi(i,j)$. For the task of antibody optimization, what we are really concerned about are those promising mutations rather than those unimportant or even harmful ones. Therefore, we propose a more efficient Iterative Shapley Value Estimation algorithm, which estimates the Shapley value of each mutation in a coarse-to-fine iterative manner, and progressively pays more attention to those promising residue sites and mutations. Specifically, the Shapley value $\widetilde{\varphi}^{(t)}(i,j)$ at $t$-th iteration is estimated as follows
\begin{equation}
\widetilde{\varphi}^{(t)}(i,j)=\sum_{n=1}^{D^{(t)}_i}\frac{1}{D^{(t)}_i}\Big(f_S^*\big(\mathcal{P}, \mathcal{M}_n \cup\{(i,j)\}\big)-f_S^*\big(\mathcal{P}, \mathcal{M}_n\big)\Big), \quad 1 \leq t \leq T
\label{equ:4}
\end{equation}
where $\{D^{(t)}_i\}_{i=1}^N$ are the numbers of sampling for each residue site, proportional to the site probability $p_{\text{site}}^{(t)}\in\mathbb{R}^N$. $\mathcal{M}_n$ is one mutation set that randomly selects multiple residue sites except for the $i$-th residue and mutates them according to the mutation preference $p_{\text{pre}}^{(t)}\in\mathbb{R}^{N\times 20}$. Specifically, the site probability $p_{\text{site}}^{(t+1)}$ and mutation preference for the $i$-th site $p_{\text{pre}}^{(t+1)}(i)$ is updated as follows
\begin{equation}
\begin{small}
\begin{aligned}
p_{\text{site}}^{(t+1)} = \alpha \cdot\sigma\Big(\sum_j \widetilde{\varphi}^{(t)}(i,j)\Big) + (1-\alpha) \cdot p_{\text{site}}^{(t)},  \quad p_{\text{pre}}^{(t+1)}(i) = \alpha \cdot\sigma \Big(\widetilde{\varphi}^{(t)}(i,\ :)\Big) + (1-\alpha) \cdot p_{\text{pre}}^{(t)}(i)
\end{aligned}
\end{small}
\label{equ:5}
\end{equation}
where $p_{\text{site}}^{(1)}$ and $\{p_{\text{pre}}^{(1)}(i)\}_{i=1}^N$ are initialized to be uniform distributions, $\alpha$ is the momentum updating rate, and  $\sigma(\cdot)\!=\!\operatorname{Softmax}(\cdot)$ is the activation function. Such an iterative approximation will treat every site and mutation equally at first, but gradually focus on those more potential sites and mutations to approximate the exact Shapley values as closely as possible in a limited number of samples.


\subsection{Preference-guided Mutation Search for Antibody Optimization}
\label{sec:4.4}
Mutation and selection are two fundamental aspects of antibody optimization. The previous popular methods usually train a deep generative model on large amounts of data, and then apply \emph{Iterative Target Augmentation}  (ITA) to guide directed optimization, i.e., generating favorable mutations. \emph{In contrast, this paper focuses on selection rather than generation.} Given a lightweight Light-DDG and a Mutation Explainer, we can directly search for favorable mutations, requiring no additional deep generative models. For the target antibody to be optimized, we randomized 10,000 mutated antibodies by sampling mutation sites and determining mutation residues based on the site importance $p_{\text{site}}^{(T)}$ and site-wise preferences $\{p_{\text{pre}}^{(T)}(i)\}_{i=1}^N$. These mutation candidates are then quickly evaluated using a lightweight Light-DDG to get the most desirable mutations based on the rankings of their $\Delta\Delta G$ scores. We have provided pseudo-code in \textbf{Appendix A} about how Light-DDG, Mutation Explainer, and Mutation Search are constructed into a unified framework for antibody optimization.


\vspace{-0.5em}
\section{Experiments}
\vspace{-0.3em}
\textbf{Datasets.} The effectiveness of Light-DDG for $\Delta\Delta G$ prediction is evaluated on the SKEMPI v2.0~\citep{jankauskaite2019skempi} dataset, which contains 348 complexes, 7,085 mutation combinations, and corresponding changes in binding free energy, but not any mutated complex structures. We randomly split the SKEMPI v2.0 dataset into 3 folds by complexes and perform 3-fold cross-validation for all methods. \textcolor{black}{For pre-training, different pre-training-based methods use different pretext tasks and datasets. For example, the PDB-REDO~\citep{joosten2014pdb_redo} dataset contains 143k unannotated data and has been widely used for \textit{unsupervised pre-training} by previous methods. In contrast, Light-DDG is \textit{supervised pre-trained} on the SKEMPI-Aug datasets that consist of 670k annotated mutation data. Moreover, the AFDB dataset~\citep{varadi2022alphafold} that contains the sequences and their corresponding structures predicted by AlphaFold2 can also be used as pre-training data.}

\textbf{Evaluation Metrics.} There are seven metrics used to evaluate $\Delta\Delta G$ prediction, including \textit{five overall metrics}: (1) Pearson correlation coefficient; (2) Spearman correlation coefficient; (3) Root Mean Squared Error (RMSE); (4) Mean Absolute Error (MAE); (5) AUROC. Additionally, \citep{luo2023rotamer} groups the mutations by structure, calculating the Pearson and Spearman correlation coefficients for each structure separately, and reporting the average as \textit{two per-structure metrics}. For antibody optimization, we take the minimal $\Delta\Delta G$ score of the optimized antibodies as the metric.

\textbf{Baselines.} We compare Light-DDG with several state-of-the-art $\Delta\Delta G$ prediction methods, including ESM-1F~\citep{hsu2022learning}, two variants of MIF (MIF-$\Delta \text{logits}$ and MIF-Network)~\citep{yang2020graph}, two variants of RDE (RDE-Linear and RDE-Network)~\citep{luo2023rotamer}, DiffAffinity~\citep{liu2023predicting}, ProMIM~\citep{mo2024multi}, Prompt-DDG~\citep{wu2024learning}, and a model that is pre-trained to predict the B-factor of residues and use predicted B-factors to predict $\Delta\Delta G$. Moreover, we compare the performance of Uni-Anti for directed antibody optimization with RefineGNN~\citep{jin2021iterative}, MEAN~\citep{kong2022conditional}, DiffAb~\citep{luo2022antigen}, and dyMEAN~\citep{kong2023end}. The detailed hyperparameter and implementation details can be found in \textbf{Appendix B}. 

\subfile{table_2.tex}

\begin{figure*}[!tbp]
    \vspace{-0.5em}
    \begin{center}
        \subfigure[Correlation Analysis]{\includegraphics[width=0.29\linewidth]{fig_overall.pdf}\label{fig:4a}}
        \subfigure[Score Distribution]{\includegraphics[width=0.335\linewidth]{fig_per_structure.pdf}\label{fig:4b}}
        \subfigure[Eight Complex Examples]{\includegraphics[width=0.360\linewidth]{fig_complex.pdf}\label{fig:4c}}
    \end{center}
    \vspace{-1em}
    \caption{(a) Correlations between experimental and predicted $\Delta\Delta G$s. (b) Distributions of per-structure Pearson scores. (c) Per-structure Pearson correlation scores for eight complex examples.}
    \vspace{-0.7em}
    \label{fig:4}
\end{figure*}


\vspace{-0.5em}
\subsection{Evaluation on $\Delta\Delta G$ Prediction} \label{sec:5.1}
\vspace{-0.3em}
\textbf{Performance Comparison.} Table.~\ref{tab:2} reports 7 evaluation metrics for 18 methods on the SKEMPI v2.0 dataset, from which we observe that Light-DDG significantly outperforms all baselines on all 7 metrics, especially on the two critical per-structure metrics. For example, Light-DDG improves over Prompt-DDG on per-structure Pearson and Spearman by 15.45\% and 17.55\%, respectively.

\textbf{Visualizations.} The scatter plots of experimental $\Delta\Delta G$ and predicted $\Delta\Delta G$ for Light-DDG, presented in Fig.~\ref{fig:4a}, demonstrate the strong correlation between experimental and predicted results. Besides, we provide the distribution of per-structure Pearson scores in Fig.~\ref{fig:4b}, as well as the average results across all structures. Not only does Light-DDG achieve the best average performance, but its distribution is mostly centered on high correlation, with fewer low-correlation structures. Further, we randomly select 8 complexes and present their per-structure Pearson scores in Fig.~\ref{fig:4c}, where Light-DDG achieves the best performance on 6 of 8 complexes.

\textbf{Single and Multiple Mutations.} \textcolor{black}{We further compare Light-DDG with 7 superior methods from Table.~\ref{tab:2} under single- and multi-point mutations, respectively. The results in Table.~\ref{tab:3} show that two state-of-the-art methods, Prompt-DDG and ProMIM, each have strengths in different metrics and mutation settings. However, Light-DDG significantly outperforms all baselines by a large margin on 14 metrics in both mutation settings, especially more challenging multi-point mutations.}

\subfile{table_3.tex}

\textbf{Ablation Study.} We conduct an ablation study to evaluate the roles played by KD and augmentation. It can be observed from Table.~\ref{tab:4} that (1) both KD and augmentation help to improve performance alone, even over Prompt-DDG (as teacher for KD and annotator for augmentation); and (2) combining the two further brings performance gains on top of each other. Furthermore, we consider two alternative input contexts, including \textit{(i)} only wild-type and mutated sequences are available, and \textit{(ii)} both wild-type and mutated structures are provided, where \textcolor{black}{we predict mutated structures from mutated sequences by ESMFold~\citep{lin2022language}}. It can be found that (1) even sequence-only design performs better than energy-based and supervised baselines in Table.~\ref{tab:2}, but poorer than structure-based design, which demonstrates the importance of structural information for $\Delta\Delta G$ prediction. (2) Mutated structures only slightly improve the performance, as Prompt-DDG (teacher model) has already been implicitly pre-trained to be mutated structure-aware.

\subfile{table_4.tex}

\begin{figure*}[!tbp]
    \begin{center}
        \subfigure[Architectural Applicability ]{\includegraphics[width=0.325\linewidth]{fig_architecture.pdf}\label{fig:5a}}
        \subfigure[Augmentation Sensitivity]{\includegraphics[width=0.325\linewidth]{fig_augmentation.pdf}\label{fig:5b}}
        \subfigure[Noise Robustness]{\includegraphics[width=0.325\linewidth]{fig_noise.pdf}\label{fig:5c}}
    \end{center}
    \vspace{-1em}
    \caption{(a) Applicability of using different $\Delta\Delta G$ predictors as teachers. (b) Sensitivity to the sizes of augmentation data. (c) Robustness of different $\Delta\Delta G$ predictors to 3D structure Gaussian noise.}
    \vspace{-1em}
    \label{fig:5}
\end{figure*}

\textbf{Applicability, Sensitivity, and Robustness.} We evaluate the applicability of Light-DDG to different teachers in Fig.~\ref{fig:5a}, where the distilled students perform better than corresponding teachers across various architectures. More importantly, it significantly improves performance regardless of whether teachers or students are pre-trained on the SKEMPI-Aug dataset. Moreover, we evaluate how the sizes of augmentation data influence Light-DDG under w/ and w/o KD settings, respectively. The curves in Fig.~\ref{fig:5b} exhibit consistent improvements from more augmented data; however, the performance gain becomes gradually limited as the data becomes more extensive. Furthermore, we evaluate the robustness of Light-DDG to 3D structural noise by adding Gaussian noise with different variances to the wild-type structures in the inference phase. It can be found from Fig.~\ref{fig:5c} that the performance gets poorer with larger noise, even poorer than that of sequence-only Light-DDG. Considering that the errors of existing structure prediction are mostly around 1$\AA$, in this case only structure-based Light-DDG achieves better performance than sequence-only Light-DDG.

\subfile{table_56.tex}


\vspace{-0.3em}
\subsection{Antibody Screening and Optimization against SARS-CoV-2}
\vspace{-0.2em}
\textbf{Candidate Antibody Screening.} The inference-efficient property of Light-DDG makes it well-suited for candidate antibody screening, i.e., identifying desirable mutations from a pool of potential mutations. We take the computational screening of 494 candidate human antibodies against SARS-CoV-2 as a case study, where all mutations are located at 26 sites within three CDRs of the heavy chain. We predict $\Delta\Delta G$s for all candidate antibodies, rank them in ascending order (lowest $\Delta\Delta G$ in the top), and report in Table.~\ref{tab:5} the ranking of five favorable mutations that have been previously proven effective~\citep{shan2022deep,wu2024learning}. It can be seen that (1) Uni-Anti ranks first on two antibodies and second on the other two; (2) only Uni-Anti successfully identifies three mutations with rankings smaller than 5\%; (3) Unit-Anti has the best average ranking of 2.4 among 9 methods.

\textbf{Antibody Optimization against SARS-CoV-2.} We show the effectiveness of Uni-Anti in optimizing a human anti-SARS-CoV-2 antibody to produce variants with lower binding energy. We first compare directed mutations (based on explainable mutation preferences) with random mutations in Table.~\ref{tab:6}, where we evaluate the $\Delta\Delta G$s of 10,000 sampled candidate antibodies (done with Light-DDG in less than 5 minutes) and filter out the best one. It is evident that directed mutations perform better than random mutations, especially on multi-site mutations. For example, joint random mutation of three CDRs is surprisingly inferior to mutating only CDR-H2, but directed mutation benefits remarkably from a wider range of mutation sites. Further, we compare several generative antibody optimization methods, including RefineGNN~\citep{jin2021iterative}, MEAN~\citep{kong2022conditional}, DiffAb~\citep{luo2022antigen}, and dyMean~\citep{kong2023end}. We input their generated antibodies together with wild-type complex structures into Light-DDG to predict $\Delta\Delta G$s. Note that these methods are conditional generative models focusing on the generation of a single CDR region, and cannot handle the joint optimization of multiple CDRs with official pre-trained models. It can be seen that regardless of which CDR region is optimized, Uni-Anti has a significant advantage over other baselines. Besides, joint optimization of three CDR regions leads to larger performance gains.

\textcolor{black}{Furthe, we take Light-DDG as the fitness function and further consider two additional search strategies, gradient-guided dWJS (gg-dWJS)~\citep{ikram2024antibody} and CMA-ES-based~\citep{claussen2022cma}. It can be observed that (1) CMA-ES-based approach has an advantage over random mutation only when the mutation space is relatively large, probably because the multivariate normal distribution in CMA-ES is not a reasonable prior for antibody mutations. (2) When optimizing a single CDR with a small mutation space, gg-dWJS slightly outperforms the current SOTA generative model, dyMEAN. However, gg-dWJS cannot benefit from such a large mutation space as Uni-Anti when jointly optimizing multiple CDRs. Last but not least, the implementation of these two approaches relies on the efficiency of Light-DDG.} 
More results on antibody optimization can be found in \textbf{Appendix C}. 

\begin{figure*}[!htbp]
    \vspace{-2em}
    \begin{center}
        \subfigure[Single Mutation]{\includegraphics[width=0.25\linewidth]{fig_vis_single.pdf}\label{fig:6a}}
        \subfigure[Pairwise Mutations]{\includegraphics[width=0.27\linewidth]{fig_vis_pair.pdf}\label{fig:6b}}
        \subfigure[Mutation Preferences]{\includegraphics[width=0.25\linewidth]{fig_vis_exp.pdf}\label{fig:6c}}
        \subfigure[Evolutionary Tree]{\includegraphics[width=0.21\linewidth]{fig_tree.pdf}\label{fig:6d}}
    \end{center}
    \vspace{-0.5em}
    \caption{Visualizations on the optimization of the three CDR regions in the heavy chain for an anti-SARS-CoV-2 antibody. (a) $\Delta\Delta G$s for a single mutation. (b) Average $\Delta\Delta G$s for pairwise mutations. (c) Explainable mutation preferences based on the estimated Shapley values. (d) An example of the mutation evolutionary tree (only part of the mutations are presented for clear visualizations).}
    \vspace{-0.5em}
    \label{fig:6}
\end{figure*}


\vspace{-0.3em}
\subsection{Visualizations on Explainable Mutation Preferences}
\vspace{-0.2em}
\textbf{Single and Pairwise Mutations.} 
We demonstrate how Mutation Explainer can explain and guide antibody optimization, with the anti-SARS-CoV-2 antibody as an example. We present $\Delta\Delta G$s of single mutation and average $\Delta\Delta G$s of paired mutations in the three CDRs of the heavy chain, respectively. It can be seen that Mutation Explainer well identifies five valid mutations (marked in black box) that have been proven effective by previous literature~\citep{shan2022deep}. Besides, it is clear that mutating CDR-H2 can usually lead to smaller $\Delta\Delta G$s than mutating CDR-H3 in Fig.~\ref{fig:6a}. Moreover, pairwise mutations in Fig.~\ref{fig:6b} reveal important synergistic effects of mutations, i.e., a single mutation that works well may fail when occurring with other mutations~\citep{ding2024protein}.

\textbf{Mutation Preferences.} Considering that multiple mutations are a common application scenario, Shapley values of $\Delta\Delta G$ scores are used to estimate the marginal benefits of individual mutations, as shown in Fig.~\ref{fig:6c}. For example, the 55-th residue on the heavy chain usually results in a negative gain when mutated alone in Fig.~\ref{fig:6a}, but it has a small Shapley value in Fig.~\ref{fig:6c}, suggesting its important role for multiple mutations, i.e., that it may need to work together with other mutations.

\textbf{Mutation Evolutionary Tree.} Using a lightweight $\Delta\Delta G$ predictor, along with the learned mutation preferences, we can draw a mutation evolutionary tree for the target antibody, as shown in Fig.~\ref{fig:6d}, which is expected to provide some insights, explanations, and guidance for antibody optimization.


\vspace{-0.5em}
\section{Conclusion and Discussion}
\vspace{-0.5em}
This paper shifts the research focus from generating mutations to evaluating mutational effects and indirectly explores the underlying fitness landscape by focusing on regions where $\Delta\Delta G$s are minimized. To this end, we train a simple but effective $\Delta\Delta G$ predictor (Light-DDG) by data augmentation and distillation. Furthermore, we show that Light-DDG can serve as a good optimizer and explainer within a \underline{Uni}fied framework for \underline{Anti}body optimization (Uni-Anti). Extensive experiments show the superiority of Uni-Anti in mutational effect prediction, optimization, and explanation. 

\textbf{Broader Impact.} The huge combinatorial space of potential mutations and the scarcity of mutation annotations have long been considered two obstacles straddling the path to unsupervised protein evolution. \emph{On the data side}, the released augmented mutation dataset expands the original data by two orders of magnitude and is expected to be a solid data ground for follow-up works. \emph{On the methodology side}, a lightweight $\Delta\Delta G$ predictor is expected to facilitate high-throughput fast mutation screening. In addition, the mutation preference explanations learned by Mutation Explainer can reveal the potential evolutionary paths, providing a powerful guideline for the understanding of protein functions and the discovery of high-fitness variants. Last but not least, mutation and selection are the two pillars of natural evolution. This paper provides a new perspective to achieve a novel, explainable, and unsupervised framework for directed optimization with selection at its core.

\textbf{Limitations.} Despite the great progress, several limitations still remain. Firstly, \textcolor{black}{$\Delta\Delta G$ is only one common prior that constrains the evolution of proteins}; combining other priors can still be built on top of our framework. Secondly, distillation is only one of the strategies to achieve lightweight inference, and other architectural choices, quantization, sparsification, and parallelization are also optional from an engineering perspective. \textcolor{black}{Thirdly, the design of this paper is expected to be combined with deep generative models. We believe that (1) constructing preference pairs using Light-DDG for preference alignment and (2) taking Light-DDG as guidance in diffusion models for controllable generation are two promising solutions.} Finally, more case studies on other proteins (in addition to antibodies) and wet experimental assays of the optimized
antibodies will be left for future work.

\section{Acknowledgments}
Many thanks to Siqi Ma for his contribution to the platform development. This work is supported by National Science and Technology Major Project (No. 2022ZD0115101), National Natural Science Foundation of China Project (No. 624B2115, No. U21A20427), Project (No. WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University, and Project (No. WU2023C019) from the Westlake University Industries of the Future Research Funding. 

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\subfile{appendix.tex}


\end{document}
