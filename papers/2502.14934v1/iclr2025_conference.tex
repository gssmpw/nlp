
\documentclass{article} % For LaTeX2e
\usepackage[table]{xcolor}
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, citecolor=blue]{hyperref}


\title{Fast and Accurate Blind Flexible Docking}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
		\mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
		\or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\author{
\begin{tabular}{@{}l@{}}
\textbf{Zizhuo Zhang}$^{1}$        \quad
\textbf{Lijun Wu}$^{2 \dagger}$ \quad
\textbf{Kaiyuan Gao}$^{3}$ \quad
\textbf{Jiangchao Yao}$^{4}$ \quad
\textbf{Tao Qin}$^{5}$ \quad
\textbf{Bo Han}$^{1}\thanks{Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk) and Lijun Wu (apeterswu@gmail.com).}$
\end{tabular}
\vspace{0mm} \\
$^{1}$TMLR Group, Hong Kong Baptist University \quad $^{2}$Shanghai AI Laboratory \\
$^{3}$Huazhong University of Science and Technology \quad \\
$^{4}$CMIC, Shanghai Jiao Tong University \quad $^{5}$Microsoft Research AI4Science \\
\texttt{\{cszzzhang, bhanml\}@comp.hkbu.edu.hk}, \texttt{apeterswu@gmail.com} \\
\texttt{im\_kai@hust.edu.cn}, \texttt{Sunarker@sjtu.edu.cn}, \texttt{taoqin@microsoft.com} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Molecular docking that predicts the bound structures of small molecules (ligands) to their protein targets, plays a vital role in drug discovery. However, existing docking methods often face limitations: they either overlook crucial structural changes by assuming protein rigidity or suffer from low computational efficiency due to their reliance on generative models for structure sampling. To address these challenges, we propose FABFlex, a fast and accurate regression-based multi-task learning model designed for realistic blind flexible docking scenarios, where proteins exhibit flexibility and binding pocket sites are unknown (blind). Specifically, FABFlex's architecture comprises three specialized modules working in concert:
(1) A pocket prediction module that identifies potential binding sites, addressing the challenges inherent in blind docking scenarios.
(2) A ligand docking module that predicts the bound (holo) structures of ligands from their unbound (apo) states.
(3) A pocket docking module that forecasts the holo structures of protein pockets from their apo conformations. Notably, FABFlex incorporates an iterative update mechanism that serves as a conduit between the ligand and pocket docking modules, enabling continuous structural refinements. This approach effectively integrates the three subtasks of blind flexible docking—pocket identification, ligand conformation prediction, and protein flexibility modeling—into a unified, coherent framework.
Extensive experiments on public benchmark datasets demonstrate that FABFlex not only achieves superior effectiveness in predicting accurate binding modes but also exhibits a significant speed advantage (208$\times$) compared to existing state-of-the-art methods. Our code is released at~\url{https://github.com/tmlr-group/FABFlex}.
\end{abstract}

\section{Introduction}
\label{Sec:Introduction}
Molecular docking is a pivotal technology in drug discovery, aiming at predicting the binding structures of ligand-protein complexes to elucidate how drug-like small molecules (ligands) interact with target proteins~\citep{morris:1996:distributed, morris:2008:molecular, agarwal:2016:overview}. Over the past decades, the development of molecular docking has seen continuous breakthroughs, evolving from traditional simulation software grounded in the principles of physics and chemistry to recent deep learning-based methods~\citep{crampon:2022:DockingSurvey}. Traditional methods typically utilize empirical energy scoring functions to rank numerous searched conformations~\citep{trott:2010:Vina, friesner:2004:Glide, mcnutt:2021:Gnina, koes:2013:Smina}, often resulting in excessive processing times and heavy computational burdens.

\par
The shift towards deep learning-based docking approaches represents a significant transformation in the field, offering an alternative pathway for exploring protein-molecule interactions. Despite technological advances, a substantial number of these methods~\citep{lu:2022:NIPS:tankbind, zhang:2023:ICLR:E3bind, stark:2022:ICML:Equibind, zhou:2023:UniMol, pei:2024:NIPS:fabind, gao:2024:fabind+, corso:2022:diffdock} rely on the \textit{rigid docking} paradigm, which assumes that proteins are rigid and remain static during the docking process. This simplification contradicts the physiological reality of molecular docking, where proteins exhibit significant flexibility and dynamic behavior~\citep{henzler:2007:Dynamic, lane:2023:protein, popovych:2006:dynamically}, thereby restricting the practical applicability of these methods in real-world scenarios. Consequently, it is imperative to explore and develop \textit{flexible docking} methods that more faithfully reflect the realistic docking process~\citep{sahu:2024:flexibledocking}.

\par
The advent of AlphaFold series~\citep{senior:2020:AlphaFold1, jumper:2021:Nature:AlphaFold2, abramson:2024:AlphaFold3} has revolutionized the technology of protein structure prediction, enabling precise 3D protein structure predictions without relying on laborious methods such as X-ray crystallography~\citep{ilari:2008:X-ray}, or cryo-electron microscopy~\citep{bonomi:2019:Cryo}. Yet, integrating AlphaFold-predicted apo structures into existing docking workflows remains challenging. For instance, as two cases shown in Fig.~\ref{Fig:Motivation}, a notable discrepancy often exists between the AlphaFold2-predicted apo structures~\citep{jumper:2021:Nature:AlphaFold2} and the actual docked holo structure. Moreover, existing rigid docking methods~\citep{lu:2022:NIPS:tankbind, pei:2024:NIPS:fabind} often yield wrong docking results when the AlphaFold2-predicted apo proteins are used as direct substitutes for previously used holo protein.

\par
Recent flexible docking methods~\citep{lu:2024:NatCom:dynamicbind, huang:2024:ReDock, zhang:2024:PackDock, qiao:2024:NeuralPLexer:NMI} predominantly rely on diffusion models and sampling strategy~\citep{yang:2023:DiffusionModel:survey} due to the strong distribution modeling capability of generative models. For example, DynamicBind~\citep{lu:2024:NatCom:dynamicbind} utilizes equivariant geometric diffusion networks with multiple sampling iterations to reconstruct holo protein structures from AlphaFold2-based conformations. Though effective, the step-by-step diffusion process and extensive sampling requirements inherently reduce computational efficiency. In contrast, regression-based methods~\citep{lu:2022:NIPS:tankbind, zhang:2023:ICLR:E3bind, stark:2022:ICML:Equibind}, offer a faster alternative by directly predicting the bound structures of protein-ligand complexes using well-designed neural networks. Notably, regression-based approaches have not yet been explored for flexible docking scenarios. Within this category, the FABind series~\citep{pei:2024:NIPS:fabind, gao:2024:fabind+} is committed to balance docking accuracy with computational efficiency through a multi-task model. However, the FABind series still adheres to the rigid protein assumption, limiting its efficacy in realistic flexible docking scenario. Consequently, the challenge of developing a method that combines both accuracy and speed for blind flexible docking remains largely unexplored.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/motivation.png}
    \caption{The two cases illustrate our motivation. These two cases, involving PDB 6HHJ and PDB 6OIM, highlight the structural discrepancy between apo proteins (AlphaFold2) and holo proteins. In the two cases, existing rigid docking method FABind~\citep{pei:2024:NIPS:fabind} yields incorrect molecular docking results when apo proteins are inputted as direct substitutes for the original holo proteins.}
    \label{Fig:Motivation}
\end{figure}

\par
In this study, we propose FABFlex, a regression-based multi-task learning model crafted to achieve both \underline{F}ast and \underline{A}ccurate \underline{B}lind \underline{Flex}ible docking. Specifically, FABFlex consists of three designed modules, each targeting a subtask decomposed from the process of blind flexible docking: (1) a \textit{pocket prediction module} that identifies binding pocket sites; (2) a \textit{ligand docking module} that predicts the structure of holo ligand; (3) a \textit{pocket docking module} that predicts the structure of holo pocket. Each module is built using an E(3)-equivariant graph neural network layer, called ``FABind layer"~\citep{pei:2024:NIPS:fabind}, which is tailored to handle the ligand-protein heterogeneous graph. The operational pipeline of FABFlex begins with the pocket prediction module, which determines the protein residues constituting the binding pocket, effectively navigating the challenge of the blind setting. Subsequently, the ligand and pocket docking modules leverage the predicted pocket information to predict the holo structures of the ligand and pocket, respectively. An iterative update mechanism facilitates the exchange of structural predictions between the ligand and pocket docking modules, allowing for further coordinate refinements. Collectively, these three modules function in a seamless end-to-end process, allowing FABFlex to simultaneously predict the holo structures of both the ligand and pocket in a single operation, thus ensuring faster computation. 
\par
We conduct experiments on public docking benchmark PDBBind to evaluate our FABFlex against a variety of docking methods. Compared to the baselines, FABFlex effectively increases the percentage of ligand RMSD below 2$\mathrm{\AA}$ to 40.59\% and reduces the pocket RMSD to 1.10$\mathrm{\AA}$. Notably, FABFlex significantly accelerates the computational speed, approximately 208 times faster than the recent state-of-the-art flexible docking method, DynamicBind~\citep{lu:2024:NatCom:dynamicbind}.


\section{Related Work}
\label{Sec:RelatedWork}
\textbf{Molecular docking.} As a cornerstone of drug discovery, molecular docking, often synonoymous with ligand-protein docking, focuses on the interactions between ligands and proteins. Traditional methods like Vina~\citep{trott:2010:Vina}, Smina~\citep{koes:2013:Smina}, Glide~\citep{friesner:2004:Glide}, Gnina~\citep{mcnutt:2021:Gnina} and Gold~\citep{jones:1997:Gold}, use physics-based scores to analyze these interaction, which, though effective, tend to be computationally intensive. Recent progress in geometric deep learning has sparked the development of deep learning-based docking strategies~\citep{crampon:2022:DockingSurvey}, which can be broadly categorized into regression-based and sampling-based methods. Regression-based methods like EquiBind~\citep{stark:2022:ICML:Equibind}, TankBind~\citep{lu:2022:NIPS:tankbind}, E3Bind~\citep{zhang:2023:ICLR:E3bind} and FABind~\citep{pei:2024:NIPS:fabind} leverage various geometric neural networks to directly predict binding structures. Conversely, sampling-based methods like DiffDock~\citep{corso:2022:diffdock}, manipulate the rotation, translation and torsion of ligands using diffusion models. These methods generally simplify the problem by assuming protein rigidity, neglecting the dynamic nature of protein in realistic docking scenarios.

\par
\textbf{Flexible molecular docking.} Recent methods in flexible docking, such as DynamicBind~\citep{lu:2024:NatCom:dynamicbind}, ReDock~\citep{huang:2024:ReDock}, PackDock~\citep{zhang:2024:PackDock} and NeuralPLexer~\citep{qiao:2024:NeuralPLexer:NMI}, are primarily based on diffusion models with sampling strategy. For example, DynamicBind~\citep{lu:2024:NatCom:dynamicbind} uses equivariant geometric diffusion networks to reconstruct the holo structures of both ligand and protein from their apo states. ReDock~\citep{huang:2024:ReDock} adopts the neural diffusion bridge model that employs energy-to-geometry mapping on geometric manifolds to predict protein-ligand binding structures. While these methods effectively enhance the docking performance, they suffer from the typical flaws associated with diffusion models and multi-round sampling strategy, i.e., low computational efficiency. This limitation impedes the scalability of these methods in assessing extensive volumes of potential, unknown molecule-protein interactions, which are crucial for advancing drug discovery. In this paper, we attempt to provide a regression-based solution aimed at achieving both fast inference and high docking accuracy.


\section{Methodology}
\label{Sec:Methodology}
\subsection{Problem Statement And Preliminary}
\label{Sec:ProblemStatement}
\textbf{Notations.} Each ligand-protein complex is denoted as a heterogeneous graph $\mathcal{G}=\{\mathcal{V}:=(\mathcal{V}^l, \mathcal{V}^p), \mathcal{E}:=(\mathcal{E}^l, \mathcal{E}^p, \mathcal{E}^{lp})\}$, where $\mathcal{V}$ and $\mathcal{E}$ denote the sets of nodes and edges, respectively. Specifically, in the ligand subgraph $\mathcal{G}^l=\{\mathcal{V}^l,\mathcal{E}^l\}$, each node $v_i=(\mathbf{h}_i,\mathbf{x}_i)\in\mathcal{V}^l$ corresponds an atom of ligand, with $\mathbf{h}_i\in\mathbb{R}^{d^l}$ representing the pre-extrated feature by TorchDrug~\citep{zhu:2022:torchdrug} and $\mathbf{x}_i\in \mathbb{R}^3$ specifying its spatial coordinate. The edge set $\mathcal{E}^l$ encompasses the chemical bonds within the ligand. In the protein subgraph $\mathcal{G}^p=\{\mathcal{V}^p,\mathcal{E}^p\}$, each node $v_j=(\mathbf{h}_j,\mathbf{x}_j)\in\mathcal{V}^p$ represents a residues, with $\mathbf{h}_j\in \mathbb{R}^{d^p}$ derived from ESM-2~\citep{lin:2022:ESM2} as the pretrained feature, and $\mathbf{x}_j\in \mathbb{R}^3$ indicating the coordinate of the $C_\alpha$ atom in the residue. The edge set $\mathcal{E}^p$ connects residues that are within an $8\mathrm{\AA}$ distance. Additionally, the set of external interface edges, denoted as $\mathcal{E}^{lp}$, comprises edges that connect nodes from the ligand set $\mathcal{V}^l$ to the protein set $\mathcal{V}^p$ when they are spatially within $10\mathrm{\AA}$ of each other. When specifically focusing on the true pocket region, the ligand-pocket complex is depicted as a reduced heterogeneous graph $\mathcal{G}^*=\{\mathcal{V}^*:=(\mathcal{V}^{l}, \mathcal{V}^{p*}), \mathcal{E}^*:=(\mathcal{E}^l, \mathcal{E}^{p*}, \mathcal{E}^{lp*})\}$, where $\mathcal{V}^{p*}, \mathcal{E}^{p*}$ and $\mathcal{E}^{lp*}$ are derived from the subsets of the protein that constitutes the pocket. Similarly, a hat is used in the notation to represent the predicted pocket, $\mathcal{\hat{G}}^*=\{\mathcal{\hat{V}}^*:=(\mathcal{V}^{l}, \mathcal{\hat{V}}^{p*}), \mathcal{\hat{E}}^*:=(\mathcal{E}^l, \mathcal{\hat{E}}^{p*}, \mathcal{\hat{E}}^{lp*})\}$.
\par
\textbf{Blind Flexible Docking.} Given an unbounded pair of conformations: an \textit{apo ligand} randomly initialized by RDKit~\citep{landrum:2013:rdkit} and an \textit{apo protein} predicted by AlphaFold2~\citep{jumper:2021:Nature:AlphaFold2}, the aim of blind flexible molecular docking is to predict the bound structure of ligand-pocket complex, i.e., \textit{holo ligand} and \textit{holo pocket}, denoted as $\mathbf{\hat{x}}=\{\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l}, \{\mathbf{\hat{x}}_j\}_{1\leq j\leq n^{p*}}\}$, where $n^l=|\mathcal{V}^l|$ and $n^{p*}=|\mathcal{V}^{p*}|$ indicate the number of ligand atoms and pocket residues respectively. Unlike most existing studies that assume protein rigidity~\citep{pei:2024:NIPS:fabind} or rely on pre-known pocket sites~\citep{zhang:2024:PackDock}, the blind flexible setting reflects the challenges encountered in real-world molecular docking scenarios.

\textbf{Fundamental Component: FABind Layer.} FABind~\citep{pei:2024:NIPS:fabind} and its enhanced version, FABind+~\citep{gao:2024:fabind+}, establish an end-to-end deep learning framework that simultaneously predicts binding pocket sites and holo ligand structure under rigid setting. The fundamental geometric graph neural network used in both two approaches is called ``FABind layer", denoted as $\mathfrak{F}(\cdot)$, which is an improved E(3)-equivariant graph neural networks (EGNN)~\citep{satorras:2021:ICML:EGNN} tailored for ligand-protein complex graph. The $l$-th FABind layer is denoted as follows:
\begin{equation}
    \mathbf{h}_i^{(l+1)}, \mathbf{h}_j^{(l+1)}, \mathbf{x}_i^{(l+1)}, \mathbf{x}_j^{(l+1)}, \mathbf{p}_{ij}^{(l+1)} = \mathfrak{F}(\mathbf{h}_i^{(l)}, \mathbf{h}_j^{(l)}, \mathbf{x}_i^{(l)}, \mathbf{x}_j^{(l)}, \mathbf{p}_{ij}^{(l)}),
\end{equation}
where $p_{ij}\in \mathbb{R}^d$ is the pair embedding of the ligand-protein node pair $(v_i, v_j)\in \mathcal{V}^l\times \mathcal{V}^p$, and $d$ is the hidden size. Similar to general graph neural networks~\citep{wu:2020:GNN:survey}, we can stack multiple FABind layers to extract deeper features and capture high-order information within the graph. Inspired by the fast inference of the FABind series methods, we employ the FABind layer as our fundamental component in constructing each module of our FABFlex.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{imgs/model_v4.png}
    \caption{The overview of proposed FABFlex model, which consists of a \textit{pocket prediction module}, a \textit{ligand docking module}, and a \textit{pocket docking module}. The pocket prediction module identifies pocket residues within the protein. Based on the predicted binding pocket sites, the ligand docking module and pocket docking module predict the holo structures of the ligand and pocket, respectively. An \textit{iterative update mechanism} facilitates the exchange of predictions between the ligand and pocket docking modules, enabling further coordinate reﬁnements. These modules work together within a unified end-to-end model for the blind flexible docking scenario, i.e., ``(apo protein, apo ligand) $\rightarrow$ (holo pocket, holo ligand)".}
    \label{Fig:Model}
\end{figure}


\subsection{FABFlex}
\subsubsection{Design Philosophy}
The design philosophy of FABFlex centers on achieving both high efficiency and accuracy in blind flexible docking task, which can be decomposed into three key subtasks: identifying pocket sites, predicting the bound holo structure of ligand, and predicting the bound holo structure of pocket. Specifically, the first subtask can be modelled as a binary classification problem to determine which protein residues form the docking pocket, while the other two subtasks are 3D coordinate regression problems. To pursue faster docking computation, the FABFlex model is designed to predict the docking results in a single-pass operation, without requirements of extensive sampling and repetitive computations. Additionally, FABFlex aims to operate without relying on external tools that could add extra computational overhead, such as the pocket detection tool P2Rank~\citep{krivak:2018:P2Rank}, which is used in existing studies~\citep{lu:2022:NIPS:tankbind, zhang:2023:ICLR:E3bind} to detect candidate pocket sites. According to above philosophy, the FABFlex model builds on an end-to-end multi-task learning framework tailored for blind flexible docking.

% We hope that FABFlex can be lightweight while maintaining docking performance.

\subsubsection{Architecture of FABFlex}
An overview of the proposed FABFlex model is illustrated in Fig.~\ref{Fig:Model}. Designed to tackle the intricacy of blind flexible docking, FABFlex employs a collaborative architecture consisting of three specialized modules: (1) a \textit{pocket prediction module}, denoted as $\mathcal{M}_S(\cdot)$; (2) a \textit{ligand docking module}, denoted as $\mathcal{M}_L(\cdot)$; (3) a \textit{pocket docking module}, denoted as $\mathcal{M}_P(\cdot)$. All modules are implemented using stacked FABind layers, while each tailored to a specific subtask decomposed from blind flexible docking. Specifically, the pocket prediction module is responsible for identifying the residues that form the pocket, addressing the ``blind" issue, as formulated below: 
\begin{equation}
\begin{aligned}
    &\{\hat{y}_j\}_{1\leq j\leq n^p} = \mathcal{M}_S(\mathcal{G}, \{\mathbf{h}_i, \mathbf{x}_i\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{x}_j\}_{1\leq j\leq n^p}), \\
    &\{v_j\}_{1\leq j\leq \hat{n}^{p*}} = \{\hat{y}_j \odot v_j\}_{1\leq j\leq n^p} \in \hat{\mathcal{V}}^{p*},
\end{aligned} 
\end{equation}
where $\mathcal{G}$ is the ligand-protein graph, $\{\hat{y}_j\}_{1\leq j\leq n^p} (\hat{y}_j\in\{0,1\})$ is an indicator vector predicted by the pocket prediction module to locate the pocket sites, $\hat{\mathcal{V}}^{p*}$ denotes the predicted set of pocket residues and $\hat{n}^{p*}=|\hat{\mathcal{V}}^{p*}|$, $\odot$ symbolizes the selection operation based on the indicator vector. The predicted pocket sites by pocket prediction module enable the subsequent ligand and pocket docking modules to concentrate effectively on the crucial pocket area, narrowing the large ligand-protein graph to a more targeted ligand-pocket graph.
\par
The ligand docking module and the pocket docking module cater to the ``flexible docking" by predicting the bound structures of holo ligand and holo pocket respectively, formulated as follows:
\begin{equation}
\begin{aligned}
&\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l} = \mathcal{M}_L(\mathcal{\hat{G}}^*, \{\mathbf{h}_i, \mathbf{x}_i\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{x}_j\}_{1\leq j\leq \hat{n}^{p*}}), \\
&\{\mathbf{\hat{x}}_j\}_{1\leq j\leq \hat{n}^{p*}} = \mathcal{M}_P(\mathcal{\hat{G}}^*, \{\mathbf{h}_i, \mathbf{x}_i\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{x}_j\}_{1\leq j\leq \hat{n}^{p*}}),
\end{aligned}
\end{equation}
where $\mathcal{\hat{G}}^*$ is the ligand-pocket graph, $\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l}$ and $\{\mathbf{\hat{x}}_j\}_{1\leq j\leq \hat{n}^{p*}}$ are predicted structures of ligand and pocket respectively. In current model, the ligand docking module and the pocket docking module predict the structures in isolation, which has a gap to reflect the interactive influence between ligand atoms and pocket residues in docking process. To rectify this, there is a need for a bridge that connects the ligand docking module and pocket docking module, enhancing the model to capture the nature of interactive dynamics of docking.

\subsubsection{Iterative Update Mechanism}
To facilitate the exchange of predicted structures between the ligand docking module and the pocket docking module, we introduce an iterative update mechanism to further promote the coordinate refinement. Specifically, the predicted ligand and pocket, which together form the updated ligand-pocket graph, are fed back into their respective modules to produce new predictions. This $k$-th iterative process can be formulated as follows:
\begin{equation}
\begin{aligned}
&\{\mathbf{\hat{x}}_i^{(k+1)}\}_{1\leq i\leq n^l} = \mathcal{M}_L(\mathcal{\hat{G}}^{(k)*}, \{\mathbf{h}_i, \mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}}), \\
&\{\mathbf{\hat{x}}_j^{(k+1)}\}_{1\leq j\leq \hat{n}^{p*}} = \mathcal{M}_P(\mathcal{\hat{G}}^{(k)*}, \{\mathbf{h}_i, \mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}}),
\end{aligned}
\end{equation}
where $\mathcal{G}^{(k)*}$ denotes the ligand-pocket graph updated by the $k$-th iteration's predicted coordinates of ligand atoms $\{\mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}$ and pocket residues $\{\mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}}$, and the number of iterations $K$ is a hyperparameter discussed in Appendix~\ref{Appe:Hyperparameter}. Notably, the update mechanism iterates solely on the coordinates, excluding the features. Additionally, inspired by AlpahFold2~\citep{jumper:2021:Nature:AlphaFold2}, we only record the gradient at the final iteration during training process. These strategies are beneficial in reducing the memory demands and easing computational burden.

\subsubsection{Training Loss and Pipeline}
\label{Sec:TrainingLoss}
\textbf{Training Loss.} FABFlex is built on a multi-task learning framework, utilizing multiple losses to supervise different modules from various aspects. Referring to FABind series~\citep{pei:2024:NIPS:fabind, gao:2024:fabind+}, the training loss comprises pocket prediction loss, ligand coordinate loss, pocket coordinate loss, and distance map constraint loss, formulated as follows:
\begin{equation}
    \mathcal{L} = \alpha_1\mathcal{L}_{\text{pocket\_pred}} + \alpha_2\mathcal{L}_{\text{ligand\_coord}} + \alpha_3\mathcal{L}_{\text{pocket\_coord}} + \alpha_4\mathcal{L}_{\text{dis\_map}},
\end{equation}
where the pocket prediction loss $\mathcal{L}_{\text{pocket\_pred}}$ encompasses a residue classification loss and a pocket center loss. The ligand coordinate loss $\mathcal{L}_{\text{ligand\_coord}}$ is a Huber regression loss to measure the distance between predicted and ground-truth coordinates of ligand atoms. Similarly, the pocket coordinate loss $\mathcal{L}_{\text{pocket\_coord}}$ is a Huber loss to compute the coordinate distance between predicted and ground-truth pocket residues. The distance map loss $\mathcal{L}_{\text{dis\_map}}$ supervises the predicted relative distance of atom-residue pairs. The details of implement of training loss are provided in Appendix~\ref{Appe:Losses}. 

% Moreover, considering the complexity of optimizing all three subtasks of blind flexible docking simultaneously, we add a pretraining process under rigid docking conditions to warm up the model.

\textbf{Pipeline.} Given an apo ligand initilized by RDKit and an apo protein predicted by AlphaFold2, the ligand is initially positioned at the center of the protein to construct the ligand-protein graph $\mathcal{G}$. This graph is passed through the pocket prediction module to identify the binding pocket residues $\{v_j\}_{1\leq j\leq \hat{n}^{p*}}$. Navigated by the predicted pocket, the ligand is translated from the protein center to the pocket center, creating the ligand-pocket graph $\mathcal{\hat{G}}^*$. This graph is then fed into the ligand and pocket docking modules, where it undergoes refinement through an iterative update mechanism. At the final iteration, the predicted holo structures of ligand $\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l}$ and pocket $\{\mathbf{\hat{x}}_j\}_{1\leq j\leq \hat{n}^{p*}}$ are obtained from their respective docking modules. The pseudo code is provided in Appendix~\ref{Appe:PseudoCode}.
\par
Notably, we adopt a partial teacher-forcing~\citep{lamb:2016:teacher-forcing} strategy during training process. When passing pocket information from the
pocket prediction module to the docking modules, a probability factor $p$ is set to control whether the true pocket sites (with probability $p$) or the predicted pocket sites (with probability $1-p$) are passed to the docking modules. This strategy allows the model to rely partially on ground-truth pocket sites for stability, while progressively learning to depend on its own pocket predictions.



\section{Experiments}
\label{Sec:Experiments}
\subsection{Experimental Setting}
\label{Sec:ExperimentSetting}
\textbf{Dataset Construction.} Our experiments are conducted on the widely used public PDBBind v2020 dataset\footnote{http://pdbbind.org.cn/}, which contains a comprehensive collection of 19,443 protein-ligand crystal complex structures with experimentally measured binding affinities. For each complex, we align the protein component with its corresponding AlphaFold2-predicted structure to obtain the apo protein conformations. To ensure consistency with previous work~\citep{pei:2024:NIPS:fabind, lu:2024:NatCom:dynamicbind}, we employ the same dataset splitting and adhere to similar data preprocessing steps. Specifically, complexes deposited before 2019 are utilized as the training set (12,807 complexes) and validation set (734 complexes), while those recorded after 2019 are designated as test set (303 complexes). Additional data preprocessing details are provided in Appendix~\ref{Appe:DataProcess}.
\par
\textbf{Baselines.} We compare our proposed FABFlex against a spectrum of competitors, which are categorized into traditional docking software and deep learning docking methods. Within the traditional software category, we compare against well-established software including Vina~\citep{trott:2010:Vina}, Glide~\citep{friesner:2004:Glide} and Gnina~\citep{mcnutt:2021:Gnina}. For deep learning-based methods, we include TankBind~\citep{lu:2022:NIPS:tankbind}, FABind~\citep{pei:2024:NIPS:fabind}, FABind+~\citep{gao:2024:fabind+}, DiffDock~\citep{corso:2022:diffdock}, DiffDock-L~\citep{corso:2024:ICLR:diffdock-l} and DynamicBind~\citep{lu:2024:NatCom:dynamicbind}. Among these, TankBind, FABind, FABind+, DiffDock and DiffDock-L maintain the rigid protein assumption, while DynamicBind is a recently published flexible docking method. From another perspectives, TankBind, FABind and FABind+ are rooted in regression-based approaches, whereas DiffDock, DiffDock-L and DynamicBind employ a sampling-based strategy with diffusion models to refine the conformations of proteins and ligands. Further details are provided in Appendix~\ref{Appe:Baselines}.
\par
\textbf{Evaluation Metrics.} Our primary evaluation metric is the root-mean-square deviation (RMSD) of Cartesian coordinates, which assesses the accuracy of predicted versus ground-truth structures for both ligands (Ligand RMSD) and binding pockets (Pocket RMSD). These metrics reflect the capability of docking model in accurately predicting ligand structures at atom level and pocket structures at residue level. For Ligand RMSD, we report the percentile values (25\%, 50\%, 75\% and mean) and percentages of RMSD below the threshold ($< 2 \mathrm{\AA}$ and $< 5 \mathrm{\AA}$).
\par
\textbf{Implementation Settings.} The initial apo ligand structures are generated using the Experimental-Torsion Knowledge Distance Geometry (ETKDG) algorithm~\citep{riniker:2015:ETKDG} in RDKit~\citep{landrum:2013:rdkit} from the molecules' SMILES sequences. The initial apo protein structures are obtained through AlphaFold2~\citep{jumper:2021:Nature:AlphaFold2} predictions based on the proteins' amino acid sequences. The more details of hyperparameter setting are provided in Appendix~\ref{Appe:ImpleDetail}.


\begin{table}[t]
\centering
\caption{Ligand performance comparison of blind flexible docking.}
\label{Tble:PerLigandRMSD}
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcccccc|cccccc|c}
    \toprule
    \multirow{4}*{Method} & \multicolumn{12}{c}{Ligand RMSD} & \multirow{4}*{\makecell{Average \\ Runtime (s)}} \\
    \cmidrule(lr){2-13}
    ~ & \multicolumn{6}{c}{On All Cases} & \multicolumn{6}{c}{On Unseen Protein Receptors} & ~ \\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13}
    ~ & \multicolumn{4}{c}{Percentiles $\downarrow$} & \multicolumn{2}{c}{\% Below $\uparrow$} & \multicolumn{4}{c}{Percentiles $\downarrow$} & \multicolumn{2}{c}{\% Below $\uparrow$} & ~ \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-11} \cmidrule(lr){12-13}  
    ~ & 25\% & 50\% & 75\% & Mean & $< 2\mathrm{\AA}$ & $< 5\mathrm{\AA}$ & 25\% & 50\% & 75\% & Mean & $< 2\mathrm{\AA}$ & $< 5\mathrm{\AA}$ & ~ \\ 
    \midrule
    \rowcolor{green!10} \multicolumn{14}{c}{\textit{Traditional Docking Software}} \\
    \rowcolor{green!10} Vina & 4.79 & 7.14 & 9.21 & 7.14 & 6.67 & 27.33 & 5.27 & 7.06 & 8.84 & 7.15 & 6.25 & 23.21 & 205\textsuperscript{*} \\
    \rowcolor{green!10} Glide & 2.84 & 5.77 & 8.04 & 5.81 & 14.66 & 40.60 & 2.38 & 5.01 & \textbf{7.17} & \textbf{5.21} & 21.36 & 49.51 & 1405\textsuperscript{*} \\
    \rowcolor{green!10} Gnina & 2.58 & 5.17 & 8.42 & 5.76 & 19.32 & 48.47 & 2.03 & 4.96 & \underline{7.35} & \underline{5.33} & 24.55 & 50.91 & 146 \\
    \midrule
    \rowcolor{blue!10} \multicolumn{14}{c}{\textit{Deep Learning-based Rigid Docking Methods}} \\
    \rowcolor{blue!10} TankBind & 2.82 & 4.53 & 7.79 & 7.79 & 8.91 & 54.46 & 2.88 & 4.45 & 7.53 & 7.60 & 4.39 & 58.77 & 0.87 \\
    \rowcolor{blue!10} FABind & 2.19 & 3.73 & 8.39 & 6.63 & 22.11 & 60.73 & 2.73 & 4.83 & 9.35 & 7.15 & 8.77 & 50.88 & 0.12 \\
    \rowcolor{blue!10} FABind+ & 1.58 & \textbf{2.79} & \underline{6.69} & \underline{5.63} & 35.64 & \underline{66.01} & 1.93 & \textbf{3.13} & 8.59 & 6.76 & 27.19 & 57.89 & 0.16 \\
    \rowcolor{blue!10} DiffDock & 1.82 & 3.92 & 6.83 & 6.07 & 29.04 & 60.73 & 1.97 & 4.82 & 8.03 & 7.41 & 26.32 & 51.75 & 82.83 \\
    \rowcolor{blue!10} DiffDock-L & \underline{1.55} & 3.22 & 6.86 & 5.99 & \underline{36.75} & 62.58 & \underline{1.86} & \underline{3.16} & 9.09 & 7.14 & \underline{29.82} & \textbf{61.40} & 58.72 \\
    \midrule
    \rowcolor{red!10} \multicolumn{14}{c}{\textit{Deep Learning-based Flexible Docking Methods}} \\
    \rowcolor{red!10} DynamicBind & 1.57 & 3.16 & 7.14 & 6.19 & 33.00 & 64.69 & 2.23 & 4.02 & 10.23 & 8.27 & 20.18 & 54.39 & 102.12 \\
    \midrule
    \rowcolor{red!10} FABFlex & \textbf{1.40} & \underline{2.96} & \textbf{6.16} & \textbf{5.44} & \textbf{40.59} & \textbf{68.32} & \textbf{1.81} & 3.51 & 8.03 & 7.17 & \textbf{32.46} & \underline{59.65} & 0.49 \\
    \bottomrule
\end{tabular}
\begin{tablenotes}[para,flushleft]
\small
\textbf{Notes:} The best results are highlighted in bold, and the second best results are underlined. The average runtime for each method is presented in seconds. The asterisk (*) indicates that the method is executed on the CPU. The left part of the table compares ligand RMSD on all test cases, while the right part provides a more rigorous comparison for those cases involving protein receptors that were unseen during training process.
\end{tablenotes}
\end{threeparttable}}
\vspace{-0.2 in}
\end{table}


\subsection{Ligand Performance of Blind Flexible Docking}
Table~\ref{Tble:PerLigandRMSD} summarizes the comparison of ligand performance across various docking methods. The left part of the table performs the comparison on the all test cases. It can be observed that FABFlex consistently outperforms both traditional docking software and contemporary deep learning-based methods almost across all metrics. Typically, a prediction of a ligand's structure is considered successful if its predicted structure is within a RMSD of $2\mathrm{\AA}$ from the true holo ligand structure~\citep{lu:2024:NatCom:dynamicbind}. Thus, the ligand RMSD $< 2\mathrm{\AA}$ is a critical metric to evaluate the capacity of molecular docking methods. FABFlex excels in this metric, achieving the ligand RMSD $< 2\mathrm{\AA}$ at 40.59\%, which has generated a significant margin compared to the second-best competitor, showcasing FABFlex's superior ability in predicting accurate binding structures of ligands.

\par
The right part of Table~\ref{Tble:PerLigandRMSD} performs a more rigorous assessment of docking methods using 114 ligand-protein complexes that involve protein receptors  that were not seen during the training process. This assessment is crucial for evaluating the generalization capability of each method. Although FABFlex does not achieve the best results in every metric, it excels significantly in the critical measure of ligand RMSD $< 2\mathrm{\AA}$. Notably, FABFlex reaches 32.46\%, markedly outperforming all competitors, which fall below 30\%. This performance underscores FABFlex's ability to generalize effectively to new and unseen proteins. Additional analysis of ligand performance is provided in Appendix~\ref{Appe:DistributionLigandRMSD}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{imgs/Pocket_RMSD_v1.png}
    \caption{Pocket performance comparison of blind flexible docking. The left figure shows the cumulative percentage of pocket RMSD on all test cases, while the right figure evaluates on those cases with protein receptors that were unseen during training process.}
    \label{Fig:PerPocketRMSD}
    \vspace{-0.2 in}
\end{figure}

\subsection{Pocket Performance of Blind Flexible Docking}
Fig.~\ref{Fig:PerPocketRMSD} illustrates the cumulative distribution of pocket RMSD performance. In the left figure, which evaluates all test complexes, we observe that although FABFlex does not outperform DynamicBind, it still has a positive effect on refining pocket poses from the initial AlphaFold2 structures. The right figure focuses on complexes with unseen protein receptors. It can be observed that for pocket RMSD below 0.75\AA, the DynamicBind distribution curve almost always stays at the lowest position, while FABFlex maintains results that are not worse than those of AlphaFold2. This suggests that for these unseen protein receptors, FABFlex demonstrates better robustness compared to DynamicBind, as it does not degrade the protein structures. These outcomes are not surprising, as DynamicBind employs a diffusion model to adjust the conformation of the entire protein, rather than just the pocket region. While this approach reduces the degrees of freedom and enhances the accuracy, it may limit its ability to generalize to new proteins, and face computational efficiency challenges.

%it may limit its ability to generalize to new proteins. Furthermore, DynamicBind faces computational efficiency challenges, which will be discussed in the next section.


\subsection{Inference Efficiency}
High efficiency facilitates the widespread adoption of a method in real-world applications. In the last column on the right of Table~\ref{Tble:PerLigandRMSD}, we showcase a comparison of the average inference time for each ligand-protein pair. Traditional docking software such as Vina, Glide, and Gnina exhibit notably longer inference times. Among regression-based methods, TankBind, FABind, and FABind+ demonstrate considerably faster speed than sampling-based approaches such as DiffDock and DynamicBind. Notably, FABFlex achieves an inference speed of only 0.49 seconds, which is approximately 208 times faster than DynamicBind, a recently developed method for flexible docking that averages 102.12 seconds. This efficiency gain is attributed to the design philosophy of FABFlex that directly predicts the bound structures without requiring multiple rounds of sampling or extra external pocket detection tools. Additionally, FABFlex specifically focuses on the conformational changes in the core pocket region, rather than the entire protein. As the pocket constitutes a small part of the protein, this design effectively reduces the computational burden.


\begin{table}[th]
\caption{Performance of pocket prediction.}
\centering
\label{Tble:PocketPrediction}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcccc|cccc}
    \toprule
    \multirow{3}*{Method} & \multicolumn{4}{c}{Given Apo Proteins} & \multicolumn{4}{c}{Given Holo Proteins} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    ~ & \multirow{2}*{CLS ACC \% $\uparrow$} & \multicolumn{3}{c}{Pocket Center ($\mathrm{\AA}$)} & \multirow{2}*{CLS ACC \% $\uparrow$} & \multicolumn{3}{c}{Pocket Center ($\mathrm{\AA}$)} \\
    \cmidrule(lr){3-5} \cmidrule(lr){7-9} 
    ~ & ~ & MAE $\downarrow$ & RMSE $\downarrow$ & EucDist $\downarrow$ & ~ & MAE $\downarrow$ & RMSE $\downarrow$ & EucDist $\downarrow$ \\ 
    \midrule
    P2Rank & - & 4.04 & 5.69 & 7.85 & - & 4.11 & 6.17 & 8.05 \\
    \midrule
    FABind & 73.32 & \textbf{3.06} & 5.30 & \textbf{6.18} & 73.54 & \textbf{3.08} & 5.35 & \textbf{6.18} \\
    FABind+ & \textbf{87.18} & 3.49 & \underline{5.09} & 6.94 & \textbf{87.52} & 3.50 & \underline{5.13} & 6.69 \\
    \midrule
    FABFlex & \underline{87.08} & \underline{3.29} & \textbf{4.83} & \underline{6.59} & \underline{87.06} & \underline{3.31} & \textbf{4.89} & \underline{6.63} \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\begin{tablenotes}[para,flushleft]
\small
\textbf{Notes:} ``-" means that P2Rank is an external pocket detection tool without pocket residue classification.
\end{tablenotes}
\end{table}


\subsection{Pocket Prediction Analysis}
Table~\ref{Tble:PocketPrediction} illustrates the performance of FABFlex's pocket prediction module in comparison with to existing P2Rank, FABind and FABind+. Among them, P2Rank~\citep{krivak:2018:P2Rank} is an open-source tool widely used in existing docking methods for pre-determining  potential binding pocket sites~\citep{lu:2022:NIPS:tankbind, zhang:2023:ICLR:E3bind}. To intuitively analyze the effectiveness of pocket prediction across different methods, we evaluate the quality of pocket prediction from two perspectives: residue classification and pocket center position. For classification, we report the accuracy (CLS ACC). For the pocket center, we employ the mean-absolute-error (MAE), the root-mean-square-error (RMSE), and the Euclidean distance (EucDist), each calculated between predicted pocket center and native pocket center to quantify the accuracy of pocket localization. From Table~\ref{Tble:PocketPrediction}, we can observe that regardless of apo or holo protein, FABFlex predicts pocket sites that are comparable to FABind and FABind+, and consistently outperform P2Rank. These results indicate the effectiveness of FABFlex in identifying pocket residues, even when confronting apo proteins. Additionally, integrating pocket prediction into the flexible docking process appears to be more advantageous than relying on an external pocket detection tool. Notably, FABFlex achieves the lowest RMSE at 4.83 and 4.89, and the lower RMSE suggests FABFlex's stability in error control, with fewer cases of extreme errors. Moreover, we provide intuitive visualization of two pocket prediction cases in Appendix~\ref{Appe:CaseStudyPocket}.


\begin{table}[th]
\caption{Experimental results of ablation studies.}
\centering
\label{Tble:AblationStudy}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}*{Method} & \multicolumn{3}{c}{Ligand RMSD (\AA)} & \multicolumn{2}{c}{Pocket RMSD (\AA)} & \multicolumn{3}{c}{Pocket Center (\AA)} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9}  
    ~ & Mean $\downarrow$ & Median $\downarrow$ & \textless 2\AA (\%) $\uparrow$ & Mean $\downarrow$ & Median $\downarrow$ & MAE $\downarrow$ & RMSE $\downarrow$ & EucDist $\downarrow$ \\ 
    \midrule
    FABFlex & 5.44 & 2.96 & 40.59 & 1.10 & 0.63 & 3.29 & 4.83 & 6.59 \\
    \midrule
    One Docking Module & 5.73 & 3.91 & 27.06 & 1.59 & 1.08 & 3.46 & 4.83 & 6.90 \\
    w/o Iterative Update & 6.14 & 3.78 & 19.80 & 1.11 & 0.63 & 3.33 & 4.88 & 6.67 \\
    Iterative Internally & 5.35 & 2.94 & 35.31 & 1.42 & 0.88 & 3.36 & 4.86 & 6.73 \\
    Using P2Rank & 7.28 & 3.07 & 34.22 & 1.15 & 0.68 & 4.04 & 5.69 & 7.85 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
% \vspace{-0.1 in}
\end{table}


\subsection{Ablation Study}
We conduct a series of ablation studies to investigate different factors affecting model performance, including the following: (1) Using a single docking module to predict both the holo structures of the ligand and pocket degrades performance for both, indicating that decomposing flexible docking into two subtasks helps reduce the complexity each module needs to handle.  (2) Removing iterative update mechanism significantly impairs the ligand performance (with ligand RMSD $<2\mathrm{\AA}$ from 40.59\% to 19.80\%), indicating the critical role of iterative update in ligand coordinate refinement. (3) Applying the iterative update mechanism only internally within the ligand and pocket docking modules negatively impacts both ligand RMSD and pocket RMSD, underscoring the importance of prediction exchange to connect the two docking modules. (4) Replacing the pocket sites predicted by the pocket prediction module with those predicted by P2Rank reduces overall performance, suggesting that integrating pocket prediction within the docking process may be more effective than relying on an external pocket detection tool.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/iterative_case.png}
    \caption{Case PDB 6OIM to intuitively present the process of iterative update.}
    \label{Fig:IterativeCaseStudy}
    \vspace{-0.2 in}
\end{figure}


\subsection{Analysis of Iterative Update}
\label{Sec:AnalysisIterative}
We take a case of PDB 6OIM as an example to visualize the structural refinements in iterative update process to intuitively analyze its function in FABFlex. As shown in Fig.~\ref{Fig:IterativeCaseStudy}, we observe that the ligand gradually approaches the true position of holo ligand from iteration 1 to iteration 6. This resembles a simulation of the ligand docking process, where the molecule is attracted to and interacts with the binding pocket sites. This observation suggests the effectiveness and reasonability of the iterative update mechanism adopted in FABFlex. Additionally, we notice that as iterations increase, the structural changes become smaller, indicating that an appropriate number of iterations is sufficient. More iterations appear unnecessary and may even increase the computational burden without significant benefit. More case studies are provided in Appendix~\ref{Appe:IterativeUpdate} for further analysis.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{imgs/case_study_main_v2.png}
    \caption{Two case studies of PDB 6OIM (left) and PDB 6ORT (right).}
    \label{Fig:CaseStudy}
    \vspace{-0.2 in}
\end{figure}


\subsection{Case Study}
\label{Sec:CaseStudy}
Fig.~\ref{Fig:CaseStudy} visualizes two cases of PDB 6OIM and PDB 6ROT, to demonstrate the effectiveness of our FABFlex in blind flexible docking. We have following observations:
\par
\textbf{FABFlex can pinpoint the binding pocket site.} In the case of PDB 6OIM, existing methods such as FABind, FABind+ and DynamicBind wrongly determine the binding sites. In contrast, FABFlex successfully locates the correct binding pockets, achieving significantly lower ligand RMSD of 3.85$\mathrm{\AA}$. This underscores FABFlex's ability to accurately pinpoint binding pockets, even in such difficult case where other methods fall short.

\par
\textbf{FABFlex excels in ligand structure prediction.} In the case of PDB 6ROT, while all methods successfully identify the correct binding pocket, the ligand structures predicted by FABFlex are significantly closer to the ground truth compared to other methods, with ligand RMSD of merely 0.77$\mathrm{\AA}$. This low ligand RMSD means that the ligand predicted by FABFlex is nearly identical to the actual holo ligand. This visualization suggests FABFlex's proficiency in predicting ligand structure. Additional case studies are provided in Appendix~\ref{Appe:CaseStudy}.


\section{Conclusion}
A high-efficiency molecular docking method is an effective tool in drug discovery, as it enables the fast assessment and screening of millions or even billions of potential molecule-protein interactions within a limited time, discovering promising drug candidates early in the development process. This work proposes FABFlex, a regression-based, end-to-end neural model tailored for real-world blind flexible docking scenarios. FABFlex offers a solution, different from existing generative model-based sampling approaches, to achieve both fast computational speed and accurate docking performance. Looking ahead, we aim to explore fast and accurate multi-site docking, a task of great significance and heightened complexity, as it allows for the investigation of intricate interactions across multiple binding sites.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
ZZZ and BH were supported by RGC Young Collaborative Research Grant No. C2005-24Y, NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, MSRA StarTrack Scholars Program, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. JCY was supported by National Natural Science Foundation of China (No. 62306178), 111 plan (No. BP0719010).

\section{Ethics Statement}
This paper proposes a model aimed at enhancing both computational efficiency and accuracy in blind flexible docking. All data used for model training and evaluation were obtained from publicly available molecular docking benchmark datasets. The advancement of molecular docking technologies has both positive and potential negative implications. On the positive side, our method can contribute to advancements in drug discovery. However, there is also the risk of misuse or malicious use, as molecular docking models could be applied to explore harmful or non-therapeutic compounds. To mitigate this risk, we are releasing our code with the explicit intent of supporting ethical and medically approved drug discovery efforts, in full compliance with relevant legal standards and institutional guidelines. This paper adheres to strict research integrity protocols and responsible dataset usage, ensuring that our contributions support the broader objectives of ethical scientific progress.

\section{Reproducibility Statement}
The experimental settings for our model are described in detail in Section~\ref{Sec:ExperimentSetting}. The experiments are all conducted on the widely used public molecular docking benchmark PDBBind v2020 dataset. We provide the link to our source codes to ensure the reproducibility of our experimental results: \url{https://github.com/tmlr-group/FABFlex}


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Methodology Details}
\subsection{Details of Training Losses}
\label{Appe:Losses}
The blind flexible docking is modelled as a supervised learning task. The ground-truth encompasses residues that form binding pocket sites, along with the actual coordinates of both holo ligand and holo pocket. The training loss implementations draw extensively from the methodologies outlined in FABind~\citep{pei:2024:NIPS:fabind} and FABind+~\citep{gao:2024:fabind+}. Corresponding to the statement in Section~\ref{Sec:TrainingLoss}, we provide detailed descriptions of each training loss to improve the reproducibility of our study:
\par
Given that only a small fraction of the residues in a protein belong to the pocket, the pocket residue detection is treated as an imbalanced binary classification task using binary-classification cross entropy loss (BCELoss)\footnote{https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\#bcewithlogitsloss}, aiming at identifying which residues belongs to the pocket. The formulation of this loss is given by:
\begin{equation}
    \mathcal{L}_{\text{pocket\_cls}} = \frac{1}{N} \sum_{i=1}^N \frac{p_i}{q_i}\{-\sum_{j=1}^{p_i}[y_j\log(\hat{y}_j) + (1-y_j)\log (1-\hat{y}_j)]\},
\end{equation}
where $N$ is the total number of training complexes, $p_i$ the number of residues in the $i$-th protein, $q_i$ the count of residues that form the pocket in the $i$-th protein. The weighting factor $p_i/q_i$ adjusts the emphasis on proteins with fewer pocket-forming residues, ensuring their adequate importance in the training process. Additionally, the pocket center loss, denoted as $\mathcal{L}_{\text{pocket\_center}}$, adopts a Huber loss\footnote{https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html\#huberloss} to supervise the position of the predicted pocket, formulated as follows:
\begin{equation}
    \mathcal{L}_{\text{pocket\_center}} = \frac{1}{N} \sum_{i=1}^N \text{HuberLoss}(center_i, \widehat{center}_i),
\end{equation}
where $center_i$ is the actual centroid coordinate of pocket in the $i$-th protein, and $\widehat{center}_i$ is the predicted centroid coordinate, which is calculated using a weighted average of all residue coordinates in the $i$-th protein, with the weights derived from the output probabilities of the Gumbel-Softmax distribution. After that, the residue classification loss $\mathcal{L}_{\text{pocket\_cls}}$ and the pocket center loss $\mathcal{L}_{\text{pocket\_center}}$, are combined to formulate the overall pocket prediction loss $\mathcal{L}_{\text{pocket\_pred}}$ as follows:
\begin{equation}
    \mathcal{L}_{\text{pocket\_pred}} = \alpha_1^{\text{cls}}\mathcal{L}_{\text{pocket\_cls}} + \alpha_1^{\text{center}}\mathcal{L}_{\text{pocket\_center}}.
\end{equation}

\par
The ligand coordinate loss and the pocket coordinate loss both utilize the Huber loss to regress the predicted structures towards their respective real holo structures:
\begin{equation}
    \mathcal{L}_{\text{ligand\_coord}} = \frac{1}{N} \sum_{i=1}^N \text{HuberLoss}(\mathbf{\tilde{x}}_i^l, \mathbf{\hat{x}}_i^l), \quad \mathcal{L}_{\text{pocket\_coord}} = \frac{1}{N} \sum_{i=1}^N \text{HuberLoss}(\mathbf{\tilde{x}}_i^p, \mathbf{\hat{x}}_i^p),
\end{equation}
where $\mathbf{\hat{x}}_i^l$ ($\mathbf{\hat{x}}_i^p$) and $\mathbf{\tilde{x}}_i^l$ ($\mathbf{\tilde{x}}_i^p$) are the predicted ligand (pocket) coordinates and actual holo ligand (pocket) coordinates respectively, for the $i$-th complex.
\par
The distance map loss serves as an auxiliary objective, employing mean-squared-error loss (MSELoss)\footnote{https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\#mseloss} to supervise the relative positions between ligand atoms and pocket residues, formally expressed as follows:
\begin{equation}
    \mathcal{L}_{\text{dis\_map}} = \frac{1}{N} \sum_{i=1}^N \text{MSELoss} (\mathbf{\tilde{D}}_i, \mathbf{\hat{D}}_i),
\end{equation}
where $\mathbf{\tilde{D}}_i$ and $\mathbf{\hat{D}}_i$ represent the actual and predicted distance maps respectively. Each element $\tilde{D}_i^{jk}$ in $\mathbf{\tilde{D}}_i$ is the Euclidean distance between the $j$-th atom in the ligand and the $k$-th residue in the pocket. Similarly, $\hat{D}_i^{jk}$ in $\mathbf{\hat{D}}_i$ corresponds to the predicted pairwise distances.


\begin{algorithm}[t]
\caption{Pseudo code of FABFlex's inference.}
\label{Alg:PseudoCodeInfer}
\begin{algorithmic}[1]
    \State \textbf{Inputs:} 
    \State \quad 3D coordinates of apo ligands $\{\mathbf{x}_i \in \mathbb{R}^3\}_{1\leq i\leq n^l}$ and apo proteins $\{\mathbf{x}_j \in \mathbb{R}^3\}_{1\leq j\leq n^p}$
    \State \quad Ligand atom features $\{\mathbf{h}_i \in \mathbb{R}^{d^l}\}_{1\leq i\leq n^l}$ and protein residue features $\{\mathbf{h}_j \in \mathbb{R}^{d^p}\}_{1\leq j\leq n^p}$
    \State \textbf{Outputs:}
    \State \quad Predicted coordinates of holo ligand $\{\mathbf{\hat{x}}_i \in \mathbb{R}^3\}_{1\leq i\leq n^l}$ and holo pockets $\{\mathbf{\hat{x}}_j \in \mathbb{R}^3\}_{1\leq j\leq \hat{n}^{p*}}$
    \State \textbf{Inference:}
    \State $\{\hat{y}_j\}_{1\leq j\leq n^p} = \mathcal{M}_S (\mathcal{G}, \{\mathbf{h}_i, \mathbf{x}_i\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{x}_j\}_{1\leq j\leq n^p})$, \quad \textit{\# predict pocket sites}
    \State $\{v_j\}_{1\leq j\leq \hat{n}^{p*}} = \{\hat{y}_j \odot v_j\}_{1\leq j\leq n^p}$, \quad \textit{\# pick out pocket residues with $\hat{y}_j=1$}
    \For{iterative update $k: 1\rightarrow K$}
    \State $\mathcal{\hat{G}}^{(k)*}\xleftarrow{\text{construct}}\{\mathcal{G}^l, \{\mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}, \{\mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}}\}$, \quad \textit{\# construct ligand-pocket graph}
    \State $\{\mathbf{\hat{x}}_i^{(k+1)}\}_{1\leq i\leq n^l} = \mathcal{M}_L(\mathcal{\hat{G}}^{(k)*}, \{\mathbf{h}_i, \mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}})$, \quad \textit{\# predict ligand}
    \State $\{\mathbf{\hat{x}}_j^{(k+1)}\}_{1\leq j\leq \hat{n}^{p*}} = \mathcal{M}_P(\mathcal{\hat{G}}^{(k)*}, \{\mathbf{h}_i, \mathbf{\hat{x}}_i^{(k)}\}_{1\leq i\leq n^l}, \{\mathbf{h}_j, \mathbf{\hat{x}}_j^{(k)}\}_{1\leq j\leq \hat{n}^{p*}})$, \quad \textit{\# predict pocket}
    \EndFor
    \State $\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l} \leftarrow \{\mathbf{\hat{x}}_i^{(K)}\}_{1\leq i\leq n^l}, \{\mathbf{\hat{x}}_j\}_{1\leq j\leq \hat{n}^{p*}} \leftarrow \{\mathbf{\hat{x}}_j^{(K)}\}_{1\leq j\leq \hat{n}^{p*}}$, \quad \textit{\# final predicted structures}
    \State \textbf{Return:} Predicted coordinates of holo ligand $\{\mathbf{\hat{x}}_i\}_{1\leq i\leq n^l}$ and holo pocket $\{\mathbf{\hat{x}}_j\}_{1\leq j\leq \hat{n}^{p*}}$.
\end{algorithmic}
\end{algorithm}


\subsection{Pseudo Code of FABFlex}
\label{Appe:PseudoCode}
To intuitively elucidate the comprehensive inference processes of FABFlex, we delineate the pseudo code in Algorithm~\ref{Alg:PseudoCodeInfer}. The inference begins with the pocket prediction module, which identifies the binding pocket sites. Subsequently, the ligand and pocket docking modules iteratively refine and predict the holo structures of ligands and pockets.


\section{Experimental Details}
\subsection{Details of Dataset Preprocessing}
\label{Appe:DataProcess}
PDBBind v2020 is a widely utilized benchmark database in related molecular docking research~\citep{pei:2024:NIPS:fabind, lu:2024:NatCom:dynamicbind, lu:2022:NIPS:tankbind, corso:2022:diffdock}. It collected 19,443 experimentally measured protein-ligand complexes along with their 3D structures. Due to the absence of apo protein structures in the dataset, we follow~\citet{lu:2024:NatCom:dynamicbind} and employ the well-established AlphaFold2 to predict the apo conformations of these protein structures. Consistent with above mentioned studies, we adopt the same dataset split strategy to enhance comparability. From the dataset, we select 303 test complexes recorded after 2019 and 734 validation complexes recorded before 2019, with the remaining complexes allocated for training. For the training set, complex samples that could not be processed by RDKit~\citep{landrum:2013:rdkit} or TorchDrug~\citep{zhu:2022:torchdrug} are excluded. Further filtering excludes samples with protein amino acid chains longer than 1500 residues and molecules larger than 150 heavy atoms, resulting in a refined set of 12,807 training complexes. The statistics of the dataset are summarized in Table~\ref{Tble:DataStatistics}.

\begin{table}[th]
\caption{Dataset statistics of preprocessed PDBBind v2020.}
\label{Tble:DataStatistics}
\centering
\begin{threeparttable}
\begin{tabular}{lccccc}
    \toprule
    PDBbind & \# Complexes & \makecell{avg. residues \\ in proteins} & \makecell{max. residues \\ in proteins} & \makecell{avg. heavy atoms \\ in ligands} & \makecell{max. atoms \\ in ligands} \\
    \midrule
    Train & 12,807 & 310.59 & 1,290 & 31.30 & 149 \\
    Validate & 734 & 312.14 & 1,025 & 32.52 & 177 \\
    Test & 303 & 280.92 & 1,098 & 35.77 & 147 \\
    \bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

\subsection{Baselines}
\label{Appe:Baselines}
We compare our proposed method with a variety of competitors, including traditional molecular docking software and recent deep learning-based methods:

\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Traditional molecular docking software}:
    \begin{itemize}[leftmargin=0.5cm]
        \item \textbf{Vina}~\citep{trott:2010:Vina}: AutoDock Vina is among the most widely used open-source docking programs, renowned for its enhanced docking performance. It achieves this through improvements to its scoring function, optimization algorithm and search strategy. Specifically, Vina enhances the X-score~\citep{wang:2002:X-Score} and employs the Broyden-Fletcher-Goldfarb-Shanno (BFGS)~\citep{nocedal:1999:numericalOptimization} method for local optimization.
        \item \textbf{Glide}~\citep{friesner:2004:Glide}: Glide distinguishes itself from other docking software by performing a systematic and comprehensive search of the conformational, orientational, and positional spaces of the docked ligand to a rigid protein receptor. This process involves technologies such as hierarchical filters, ChemScore function~\citep{eldridge:1997:ChemScore}, among others.
        \item \textbf{Gnina}~\citep{mcnutt:2021:Gnina}: Gina is an advanced molecular docking software that incorporates convolutional neural networks into its scoring function, and leverages Monte Carlo sampling to comprehensively explore the conformational space of ligands.
    \end{itemize}
    
    \item \textbf{Deep learning-based methods following protein rigidity assumption}:
    \begin{itemize}[leftmargin=0.5cm]
        \item \textbf{TankBind}~\citep{lu:2022:NIPS:tankbind}: TankBind employs the external tool P2Rank~\citep{krivak:2018:P2Rank} to detect potential binding pocket sites, segmenting the entire protein into functional blocks. Subsequently, TankBind uses a trigonometry-aware graph neural network to model protein-ligand interactions, predicting the distance matrix and optimizing ligand structures. 
        \item \textbf{FABind}~\citep{pei:2024:NIPS:fabind}: FABind adopts an end-to-end framework that streamlines the modeling of protein-ligand interactions by integrating binding pocket predictions with docking tasks. This approach accelerates the docking computation process by eliminating the need for external pocket detection tools.
        \item \textbf{FABind+}~\citep{gao:2024:fabind+}: FABind+ is an enhanced version of FABind, incorporating additional improvements such as dynamic pocket radius adjustments for each complex and permutation invariance loss to supervise ligand structures. These enhancements boost the performance of predicted ligand structures.
        \item \textbf{DiffDock}~\citep{corso:2022:diffdock}: DiffDock utilizes diffusion models~\citep{yang:2023:DiffusionModel:survey} to manage the conformational refinements of ligands in molecular docking. This approach conceptualizes docking as a generative modeling task by mapping ligand poses onto a non-Euclidean manifold, effectively reducing the degrees of freedom for structural refinements. Additionally, DiffDock trains a confidence model to estimate the poses for multiple samplings.
        \item \textbf{DiffDock-L}~\citep{corso:2024:ICLR:diffdock-l}: DiffDock-L is an enhanced version of DiffDock~\citep{corso:2022:diffdock} that improves docking model performance by scaling data and model size and incorporating synthetic data to boost generalization capacity.
    \end{itemize}

    \item \textbf{Recent deep learning-based flexible docking method}:
    \begin{itemize}[leftmargin=0.5cm]
        \item \textbf{DynamicBind}~\citep{lu:2024:NatCom:dynamicbind}: DynamicBind is a recent flexible docking method breaking down the rigid protein assumption. It leverages equivariant geometric diffusion networks to manipulate the structure changes in both ligands and proteins, recovering ligand-specific poses from their apo stages.
    \end{itemize}
\end{itemize}

\begin{table}[t]
\caption{Implementation configuration of FABFlex.}
\centering
\label{Tble:Configuration}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|c|c|c}
    \toprule
    \multicolumn{2}{c|}{Configuration} & Pretraining Stage 1 & Pretraining Stage 2 & Joint Training Stage \\
    \midrule
    \multicolumn{2}{c|}{Learning rate} & 5e-5 & 5e-5 & 5e-6 \\
    \multicolumn{2}{c|}{Epoch} & 1000 & 600 & 600 \\
    \multicolumn{2}{c|}{Batch size} & 2 & 4 & 4 \\
    \multicolumn{2}{c|}{Dropout} & 0.1 & 0.1 & 0.1 \\
    \multicolumn{2}{c|}{Teacher-forcing} & Yes & Yes & No $(p = 0.5)$ \\
    \multicolumn{2}{c|}{Optimizer} & Adam & Adam & Adam \\
    \multicolumn{2}{c|}{Scheduler} & LinearLR & LinearLR & LinearLR \\
    \midrule
    \multirow{5}*{Loss weights} & Residue classification $\alpha_1^{\text{cls}}$ & 1.0 & - & 1.0 \\
    ~ & Pocket center $\alpha_1^{\text{center}}$ & 0.05 & - & 0.05 \\
    ~ & Ligand docking $\alpha_2$ & 1.5 & - & 1.5 \\
    ~ & Pocket docking $\alpha_3$ & - & 15.0 & 15.0 \\
    ~ & Distance map $\alpha_4$ & 1.0 & 1.0 & 1.0 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}


\subsection{Details of Implementation Setting}
\label{Appe:ImpleDetail}
\textbf{Model Configuration.} The dimension $d^l$ of initial features extracted via TorchDrug~\citep{zhu:2022:torchdrug} for ligand atoms is set to 56, and the dimension $d^p$ of ESM-2~\citep{lin:2022:ESM2} features for amino acid is 1280. The number of FABind-layers is configured as \{1, 5, 5\} for pocket prediction module, ligand docking module and pocket docking module, respectively. The number of hidden size is set to \{128, 512, 512\} for the same modules in the corresponding order. The pocket radius of 20$\mathrm{\AA}$ is used to delineate the binding pocket sites for each ligand-protein complex.
\par
\textbf{Training details.} Simultaneously optimizing all three subtasks of blind flexible docking from scratch is a challenging task. We introduce a two-stage pretraining process to warm up the model. In the first stage, the model is trained under rigid docking conditions, using a (holo protein, apo ligand) pair to predict the holo ligand structure. In the second stage, the model is trained using an (apo protein, holo ligand) pair to predict the holo pocket structure. After that, the model undergoes joint training to predict both the holo ligand and holo pocket from (apo protein, apo ligand) inputs. This progressive approach allows the model to learn from simpler tasks to more complex ones, facilitating the solution of blind flexible docking.

\par
\textbf{Training Configuration.} Table~\ref{Tble:Configuration} summarizes the training configurations in this study. The experiments are conducted using the Pytorch framework\footnote{https://pytorch.org/}. The model is trained on eight NVIDIA RTX 4090 GPUs. The pretraining stages 1 and 2 reduce task difficulty by fixing either the small molecule or the protein as the holo structure while keeping the other as the apo structure, to warm up the model. The approximate training durations for pretraining stages 1 and 2, as well as the joint training stage, are \{10, 5, 5\} days, respectively. The configurations vary slightly across different stages, mainly in terms of learning rate, and the application of teacher forcing. During the pretraining stages, and teacher forcing are utilized to kick-start the model effectively. As the progress to the joint training stage, the learning rate is reduced to better adapt the model to flexible docking.

\begin{figure}[t]
    \centering
  \includegraphics[width=0.75\textwidth]{imgs/epoch235_ligand_rmsd_distribution.png}
  \caption{The cumulative distribution of ligand RMSD.}
  \label{Fig:DistriLigandRMSD}
\end{figure}


\section{Additional Experimental Results}
\label{Appe:AddExprimentsRes}
\subsection{Distribution of Ligand Performance}
\label{Appe:DistributionLigandRMSD}
To showcase the ligand performance comprehensively, Fig.~\ref{Fig:DistriLigandRMSD} provides the cumulative distibution of ligand RMSD. It can be observed that FABFlex occupies the topmost position in the distribution curve in the majority of cases. Especially for the cases of ligand RMSD $< 2 \mathrm{\AA}$, which are crucial indicators for evaluating a molecular docking method, the superiority of FABFlex becomes even more distinct. This findings corroborates previous results in Table~\ref{Tble:PerLigandRMSD}, further highlighting the advantages of FABFlex in ligand performance.

\begingroup
\begin{table}[t]
\caption{Analysis of the number of samplings.}
\centering
\label{Tble:NumSample}
\resizebox{0.8\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcccccc}
    \toprule
    \multirow{2}*{Methods} & \multicolumn{3}{c}{Ligand RMSD (\AA)} & \multicolumn{2}{c}{Pocket RMSD (\AA)} & \multirow{2}*{\makecell{Average \\ Runtime (s)}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-6}  
    ~ & Mean $\downarrow$ & Median $\downarrow$ & \textless 2\AA (\%) $\uparrow$ & Mean $\downarrow$ & Median $\downarrow$ & ~ \\ 
    \midrule
    DynamicBind(1) & 6.26 & 3.45 & 27.15 & 0.84 & 0.59 & 22.04 \\
    DynamicBind(10) & 6.21 & 3.41 & 27.48 & 0.84 & 0.57 & 47.22 \\
    DynamicBind(40) & 6.19 & 3.16 & 33.00 & 0.84 & 0.58 & 102.12 \\
    \midrule
    FABFlex & 5.44 & 2.96 & 40.59 & 1.10 & 0.63 & 0.49 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}
\endgroup

\subsection{Analysis of Number of Samplings}
\label{Appe:NumberSamplings}
In this section, we specifically vary the number of samplings in $\{1, 10, 40\}$ of DynamicBind for further analysis. Table~\ref{Tble:NumSample} illustrates the experimental results. It can be observed that as the number of samplings decreases, there is a notable performance degradation for DynamicBind, for example, the percentage of ligand RMSD less than 2$\text{\AA}$ drops from 33.00\% with 40 samplings to 27.15\% with one samplings. Besides, even DynamicBind(1) only sample once, its runtime is still much slower than that of regression-based models. These observations reflect the inherent trade-off between performance and efficiency in these diffusion-based methods like DynamicBind, where the runtime grows with the number of samplings.



\begin{figure}[t]
    \centering
  \includegraphics[width=0.99\textwidth]{imgs/Apo_Holo1.jpg}
  \caption{Performance from holo proteins to apo proteins. Left: ligand RMSD. Right: ligand RMSD $< 2 \mathrm{\AA}$.}
  \label{Fig:PerApoHolo}
\end{figure}

\subsection{Performance from Rigid to Flexible}
\label{Appe:PerfR2F}
In this section, we conduct experiments to evaluate the performance differences between rigid protein docking and flexible docking. Fig.~\ref{Fig:PerApoHolo} illustrates comparative performances when holo and apo proteins are employed as inputs in molecular docking experiments. We observe a notable decrease in performance when transitioning from holo to apo structures across all rigid docking methods including FABind, FABind+ and DiffDock-L. Conversely, FABFlex consistently maintains its performance, whether measuring ligand RMSD or the percentage of ligand RMSD $< 2 \mathrm{\AA}$. These results indicate that FABFlex's superior ligand performance over the FABind series can be attributed to its self-adaptive capability to accommodate protein conformational changes. This capacity allows FABFlex to effectively handle molecular docking under both rigid protein assumption and flexible docking condition. 


\begin{figure}[th!]
    \centering
  \includegraphics[width=0.99\textwidth]{imgs/noise_rmsd.jpg}
  \caption{Results of adding noise. Left: ligand RMSD. Right: ligand RMSD $< 2 \mathrm{\AA}$.}
  \label{Fig:PerNoise}
\end{figure}

\subsection{Robustness Evaluation}
To further evaluate the robustness and corroborate the discussion in Appendix~\ref{Appe:PerfR2F}, we conduct the experiments that introduce perturbations into the input protein conformations. Specifically, we incrementally introduce standard Gaussian noise (mean = 0, standard deviation = 1) to the coordinates within the binding pocket. The noise is added based on a probability parameter $p_{\text{noise}}$ which varies from 0.1 to 1.0 with interval 0.1, that is, meaning that each coordinate has a probability $p_{\text{noise}}$ of having noise added. To enhance the reliability of the results, each experiment is repeated with three different random seeds, and the mean results along with their standard deviations are reported. The results are illustrated in Fig.~\ref{Fig:PerNoise}. As expected, the performance of all methods inevitably deteriorates as the noise increases, as observed in the figure. However, the curve corresponding to FABFlex consistently demonstrates the better performance. This suggests that, despite the lack of specific robustness-focused design in FABFlex, it still exhibits a certain degree of robustness, capable of handling a certain level of perturbation in the input protein structures. 




\begin{table}[t]
\caption{Impact of the number of iterative updates.}
\centering
\label{Tble:Hyperparameter}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lccccccccc}
    \toprule
    \multirow{2}*{Iteration} & \multicolumn{3}{c}{Ligand RMSD (\AA)} & \multicolumn{2}{c}{Pocket RMSD (\AA)} & \multicolumn{3}{c}{Pocket Center (\AA)} & \multirow{2}*{\makecell{Average \\ Runtime (s)}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-6} \cmidrule(lr){7-9}  
    ~ & Mean $\downarrow$ & Median $\downarrow$ & \textless 2\AA (\%) $\uparrow$ & Mean $\downarrow$ & Median $\downarrow$ & MAE $\downarrow$ & RMSE $\downarrow$ & EucDist $\downarrow$ & ~ \\ 
    \midrule
    \# ITER=1 & 6.11 & 3.69 & 21.45 & 1.12 & 0.64 & 3.33 & 4.90 & 6.69 & 0.11 \\
    \# ITER=2 & 5.50 & 3.01 & 36.96 & 1.13 & 0.65 & 3.31 & 4.86 & 6.63 & 0.18 \\
    \# ITER=4 & 5.41 & 2.91 & 39.27 & 1.14 & 0.64 & 3.31 & 4.88 & 6.64 & 0.33 \\
    \# ITER=6 & 5.44 & 2.96 & 40.59 & 1.10 & 0.63 & 3.29 & 4.83 & 6.59 & 0.49 \\
    \# ITER=8 & 5.33 & 2.87 & 38.94 & 1.10 & 0.63 & 3.28 & 4.79 & 6.58 & 0.60 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}


\subsection{Hyperparameter Analysis}
\label{Appe:Hyperparameter}
We conduct experiments to analyze the impact of varying the number of iterative updates. Table~\ref{Tble:Hyperparameter} summarizes the results. We observe that as the iterations increase, the ligand RMSD metrics improve and tend to stabilize, and the average inference times also increase. In contrast, the improvement in the metrics of pocket RMSD is minimal, and the metrics for pocket center show a slight improvement. These findings underscore the impact of the number of iterative updates on refineing ligand structures, as an appropriate number of iterations can greatly enhance the structures of ligands.

\begin{table}[th]
\caption{Assessment of PoseBuster test suites.}
\centering
\label{Tble:PoseBuster}
\resizebox{0.7\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|cc}
    \toprule
    Methods & Ligand RMSD $< 2 \text{\AA}$ (\%) & PB-valid \& Ligand RMSD $< 2 \text{\AA}$ (\%) \\
    \midrule  
    FABind & 22.52 & 1.32 \\
    FABind+ & 34.11 & 8.94 \\
    DiffDock-L & 37.42 & 16.89 \\
    DynamicBind & 30.79 & 14.57 \\
    FABFlex & 39.40 & 13.91 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}

\begin{figure}[th]
    \centering
  \includegraphics[width=0.99\textwidth]{imgs/validity_rate_plot.png}
  \caption{Assessment of 20 aspects by PoseBuster test suites.}
  \label{Fig:PoseBuster}
\end{figure}


\subsection{Assessment of PoseBuster Test Suites}
To assess the physical validity of generated ligand structures, we use the PoseBuster~\citep{buttenschoen:2024:posebusters} test suites performing on the flexible PDBBind to evaluate the ligand structures predicted by various docking models. Table~\ref{Tble:PoseBuster} summarizes the experimental results. Following PoseBuster, we call that molecule poses which pass all tests in PoseBusters are ``PB-valid". It can be observed that FABFlex achieves the highest RMSD $< 2\text{\AA}$ percentage of 39.40\% among all competitors, demonstrating its superior accuracy in predicting docking poses close to the ground truth holo ligands. However, the percentage of ``PB-valid \& RMSD $< 2 \text{\AA}$" of FABind, FABind+, and FABFlex is lower than those of DiffDock-L and DynamicBind, reflecting the strength of diffusion-based models in generating physically valid molecular poses. This may be attributed to that diffusion-based models adjust molecular structures through translational, rotational, and torsional movements, reducing the degrees of freedom. Notably, FABFlex achieves a ``PB-valid \& RMSD $< 2 \text{\AA}$" (13.91\%) comparable to DynamicBind (14.57\%), larger than poor FABind (1.32\%) and FABind+ (8.94\%), showcasing its balance in pursuing both accuracy and physical validity. Moreover, to analyze the physical validity comprehensively, we showcase the validity rates of all aspects by PoseBuster test suites in Figure~\ref{Fig:PoseBuster}. It can be observed that a core limitation regarding the physical validity of deep learning-based docking models lies in the ``minimum\_distance\_to\_protein", that is, the distance between protein-ligand atom pairs is larger than 0.75 times the sum of the pairs van der Waals radii. All docking methods demonstrate a low validity rate in this aspect. Additionally, the validity of other critical factors, such as ``tetrahedral\_chirality", ``internal\_steric\_clash", ``internal\_energy", and ``volume\_overlap\_with\_the\_protein", also requires further improvement.



\begin{figure}[th]
    \centering
  \includegraphics[width=0.99\textwidth]{imgs/iterative_case_appendix.png}
  \caption{Three more case studies of PDB 6PGO, PDB 6GZY, and PDB 6CJJ, to intuitively showcase the iterative update from iteration 1 to iteration 6.}
  \label{Fig:AddIterCase}
\end{figure}

\subsection{Additional Cases for Analysis of Iterative Update}
\label{Appe:IterativeUpdate}
Fig.~\ref{Fig:AddIterCase} provides additional three cases to further support the discussion in Section~\ref{Sec:AnalysisIterative}. In case PDB 6PGO and PDB 6GZY, we observe a similar iterative update process as seen in PDB 6OIM. The ligands gradually approach the true holo structures from iteration 1 to iteration 6. The case of PDB 6CJJ is slightly different. We observe that the ligand is already positioned close to the holo ligand at iteration 1. From iterations 1 to 6, the ligand undergoes continuous refinement, and by iteration 6, it aligns very closely with the true holo structure. Consequently, these cases suggest that the iterative update process not only rectifies ligand positions when far from the holo ligands, but also refines the ligand conformations that are already near the correct binding position.

\begin{table}[th]
\caption{Analysis of clash score.}
\centering
\label{Tble:ClashScore}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ccc|cccccc}
    \toprule
    Methods & Vina & Glide & Gnina & TankBind & FABind & FABind+ & DiffDock & DynamicBind & FABFlex \\
    \midrule  
    clash score $\downarrow$ & 0.02 & 0.08 & 0.05 & 0.41 & 0.51 & 0.45 & 0.33 & 0.27 & 0.37 \\ 
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}


\subsection{Analysis of steric clash}
\label{Appe:ClashAnalysis}
In this section, we conduct experiments to analyze the steric clash in the predicted structures by the docking methods. Following DynamicBind~\citep{lu:2024:NatCom:dynamicbind}, the clash score is defined as the root-mean-square of the van der Waals overlaps for all atom pairs between the ligand and the protein where the interatomic distance is less than 4\AA, formulated as follows:
\begin{equation}
    \text{clash score} = \sqrt{\frac{\sum_{i=0}^N \text{VdW overlop}^2}{N}},
\end{equation}
where $N$ is the number of atom pairs with distances considered. A lower clash score indicates fewer or less steric clashes. Table~\ref{Tble:ClashScore} illustrates the class scores across various competitors. It can be observed that traditional docking software achieves significantly lower clash scores, whereas all deep learning-based docking methods exhibit higher clash scores. This reflects a core limitation of deep learning-based docking methods: despite their improved docking accuracy, they are more prone to steric clash issues. Addressing this challenge is a promising direction for future research.


\begin{table}[th]
\caption{Analysis of binding affinity, including IC50 and K value.}
\centering
\label{Tble:Affinity}
\resizebox{0.6\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|cccc}
    \toprule
    Methods & FABind & FABind+ & DynamicBind & FABFlex \\
    \midrule  
    IC50 $(\mu M)$ $\downarrow$ & 6.32 & 6.24 & 6.27 & 5.92 \\
    K Value $(\mu M)$ $\downarrow$ & 6.23 & 6.19 & 6.18 & 6.10 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}


\subsection{Analysis of Binding Affinity}
\label{Appe:Affinity}
In this section, we conduct experiments to evaluate the binding affinity of predicted structures of ligand-protein complexes. Specifically, we employ the MBP~\citep{yan:2024:MBP}, a structure-based binding affinity prediction model, to evaluate IC50 and K value of the predicted structures as indicators of binding affinity. IC50 represents the concentration of a ligand required to inhibit 50\% of the protein's activity, while K Value reflects the dissociation constant, directly measuring the strength of the ligand-protein binding affinity\footnote{https://en.wikipedia.org/wiki/IC50}. For both metrics, smaller values indicate stronger binding affinity and better docking quality. Table~\ref{Tble:Affinity} summarizes the experimental results. It can be observed that FABFlex achieves the lowest IC50 at 5.92 $\mu M$ and K Value at 6.10 $\mu M$ among all competitors. This observation suggests that FABFlex has potential to better predict biologically relevant interactions.


\begin{table}[th]
\caption{Performance comparison in pocket-based flexible docking.}
\centering
\label{Tble:PocketbasedDocking}
\resizebox{0.99\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ccccccc}
    \toprule
    \multirow{2}*{Methods} & \multicolumn{4}{c}{Ligand RMSD} & \multicolumn{2}{c}{Pocket RMSD} & \multirow{2}*{\makecell{Avg. \\Runtime (s)}} \\
    ~ & Mean (\AA) $\downarrow$ & Median (\AA) $\downarrow$ & $< 2 \text{\AA}$ (\%) & $< 5 \text{\AA}$ (\%) & Mean (\AA) $\downarrow$ & Median (\AA) $\downarrow$ & ~ \\
    \midrule  
    FABind & 4.47 & 3.23 & 23.76 & 67.66 & - & - & 0.10 \\
    FABind+ & 4.21 & 2.66 & 36.63 & 69.97 & - & - & 0.14 \\
    DiffDock-Pocket(10) & - & 2.60 & 41.00 & - & - & - & 17 \\
    DiffDock-Pocket(40) & - & 2.60 & 41.70 & - & - & - & 61 \\
    ReDock(10) & - & 2.50 & 39.00 & 74.80 & - & - & 15 \\
    ReDock(40) & - & 2.40 & 42.90 & 76.40 & - & - & 58 \\
    \midrule
    FABFlex & 3.45 & 2.57 & 42.24 & 75.25 & 0.93 & 0.66 & 0.47 \\
    \bottomrule
\end{tabular}
\end{threeparttable}}
\end{table}


\subsection{Performance on Pocket-based Flexible Docking}
In this section, we conduct experiments to assess the applicability of FABFlex in pocket-based flexible docking, where reliable prior knowledge of the binding pocket sites or sidechains is available. Specifically, the pocket prediction module is bypassed, and the partial teacher-forcing strategy is removed to adapt FABFlex model to pocket-based docking, where we feed the ground truth pocket amino acids and try to predict the holo structures of ligand and pocket. Moreover, we additionally evaluate FABind and FABind+ in pocket-based docking scenario, and we include the results reported in DiffDock-Pocket~\citep{plainer:2023:diffdockpocket} and ReDock~\citep{huang:2024:ReDock} for comparison. The experimental results are summarized in Table~\ref{Tble:PocketbasedDocking}. It can be observed that FABFlex is also effective in pocket-based docking scenario, achieving a Ligand RMSD $< 2 \text{\AA}$ of 42.24\%, outperforming FABind, FABind+, DiffDock-Pocket(10), DiffDock-Pocket(40), and ReDock(10), and comparable to ReDock(40) of 42.90\%. Furthermore, FABFlex achieves a Pocket RMSD Mean of 0.93 \AA, demonstrating its ability to model protein flexibility. On Ligand RMSD $< 5 \text{\AA}$, FABlex achieves 75.25\%, comparable to ReDock(40), surpassing FABind, FABind+ and ReDock(10). Notably, the average runtime of FABFlex is only 0.47 seconds, which is considerably faster than DiffDock-Pocket(40) and ReDock(40) (more than 100x times faster), indicating the strong efficiency of our FABFlex. These results suggest that the regression-based paradigm has the potential and capacity to handle protein flexibility in both blind and pocket-based docking scenarios.



\begin{figure}[t]
    \centering
  \includegraphics[width=0.9\textwidth]{imgs/case_study_appendix_pocket.png}
  \caption{Four more case studies of PDB 6PGO, PDB 6PGP, PDB 6EEB, and PDB 6O5G.}
  \label{Fig:AddCaseStudy1}
\end{figure}

\begin{figure}[t]
    \centering
  \includegraphics[width=0.9\textwidth]{imgs/case_study_appendix_ligand.png}
  \caption{Two more case studies of PDB 6CYH and PDB 6N8X.}
  \label{Fig:AddCaseStudy2}
\end{figure}


\subsection{Additional Case Studies}
\label{Appe:CaseStudy}
In this section, we provide six additional cases to further support the discussion in Section~\ref{Sec:CaseStudy}. Fig.~\ref{Fig:AddCaseStudy1} visualizes four cases of PDB 6PGO, PDB 6PGP, PDB 6EEB, and PDB 6O5G. FABFlex successfully positions the ligands in the correct pocket regions and achieves the lowest ligand RMSD. Fig.~\ref{Fig:AddCaseStudy2} visualizes two cases of PDB 6CYH and PDB 6N8X, in which FABFlex closely approximates the ground-truth ligand structures, with ligand RMSD of 0.94$\mathrm{\AA}$ and 0.73$\mathrm{\AA}$, respectively. Furthermore, FABFlex positively reduces pocket RMSD across all cases. These cases highlight FABFlex's proficiency in accurately identifying pocket sites and predicting ligand structures.


\begin{figure}[t]
    \centering
  \includegraphics[width=0.99\textwidth]{imgs/pocket_prediction_case.jpg}
  \caption{Two case studies of PDB 6QQW and PDB 6HHJ to showcase pocket prediction.}
  \label{Fig:CasePocketPred}
\end{figure}


\subsection{Case Studies of Pocket Prediction}
\label{Appe:CaseStudyPocket}
In this section, Fig.~\ref{Fig:CasePocketPred} visualizes two cases of PDB 6QQW and PDB 6HHJ, to intuitively showcase the effectiveness of our FABFlex in binding pocket prediction. It can be observed that for both two cases, the predicted pockets are correctly located in the correct regions, centered around the binding sites of their respective holo ligands. This ensures that the predicted ligands are also correctly positioned within the pocket region.


\section{Broader Impacts and Limitations}
\textbf{Broader impacts.} In the current landscape, with the vast number of known proteins and small molecules, a high-efficiency molecular docking method is essential for analyzing and processing large-scale molecule-protein interactions within a limited timeframe, aiding in the discovery of potential drug candidates with therapeutic value. Therefore, the exploration and development of fast and accurate docking methods in real-world scenarios are crucial to advancing AI-driven technologies in drug discovery.

\par
\textbf{Limitations.} There still are several limitations in this study. Firstly, the performance of pocket structures does not surpass that of the advanced method DynamicBind. This discrepancy motivates us to incorporate advantages of the sampling-based strategy to enhance the results. Additionally, deep learning-based methods tend to produce structures with more clashes compared to traditional docking software. To address this, a post-optimization method could be implemented to mitigate such issues~\citep{stark:2022:ICML:Equibind}. Alternatively, integrating biological or chemical constraints into model design and optimization might also help alleviate these problems~\citep{jiang:2024:PocketFlow}.



\end{document}
