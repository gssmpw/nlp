\section{Related Work}
\label{sec:rw}
The field of speaker characterization encompasses the analysis of physiological and psychological speaker traits, such as gender, age, height, accent, and expressed emotion**Schneider et al., "Speaker Characterization"**. Techniques for speaker characterization typically rely on machine learning models to infer speaker attributes from extracted voice features. These approaches are complemented by preprocessing methods, such as voice activity detection and feature extraction, to prepare the data for downstream analysis.

Table~\ref{tab:capabilities_comparison} provides an overview of speaker characterization approaches and frameworks. The first section presents individual studies focusing on specific characterization tasks.
The second section of the table highlights general-purpose speech-processing frameworks that provide preprocessing and feature extraction capabilities. These frameworks, while offering essential functionalities like VAD and diarization, typically lack comprehensive speaker characterization capabilities. In contrast, our VANPY framework bridges this gap by integrating both fundamental preprocessing components and advanced speaker characterization models.

\begin{table*}[ht]
\centering
\footnotesize
\begin{adjustbox}{width=1\linewidth}
\begin{tabular}{lp{30mm}p{45mm}*{5}{c}p{70mm}}
\toprule
\textbf{Framework/Tool} & \textbf{\makecell[l]{Audio Preprocessing}} & \textbf{\makecell[l]{Feature Extraction and \\ Speaker Representations}} & \multicolumn{5}{c}{\textbf{Speaker Characterization}} & \textbf{\makecell[l]{Other Capabilities and \\ incorporated ML models}} \\ 
\cmidrule(lr){4-8}
& & & \rotatebox{90}{\textbf{Gender ID}} & \rotatebox{90}{\textbf{Dialect/Accent ID}} & \rotatebox{90}{\textbf{Emotion Classification}} & \rotatebox{90}{\textbf{Height Estimation}} & \rotatebox{90}{\textbf{Age Estimation}} & \\ 
\midrule
**Zhang et al., "Speaker Characterization"**( **Rajput, "Dialect Identification"**) & - & - & \checkmark & 7 & 6 & - & 5 & - \\ 
AGENDER ~(**PÃ©rez et al., "Age Estimation"**) & - & - & \checkmark & - & - & - & 4 & - \\ 
**Fernandez et al., "Speaker Emotion Recognition"**( **Tang, "Speaker Verification"**) & - & - & \checkmark & 2 & - & - & - & Speaker Verification, Language ID, Proficiency Level, Attentional State speaker \\ 
**Kumar, "Speaker Characterization"**( **Liu et al., "Accent Classification"**) & - & - & \checkmark & 3 & - & - & 3 & - \\ 
**Zhou et al., "Speaker Identification"**( **Wang et al., "Dialect Detection"**) & - & - & \checkmark & - & - & - & - & - \\ 
**Huang, "Speaker Emotion Recognition"**( **Chen et al., "Accent Classification"**) & - & - & \checkmark & - & - & - & 4 & - \\ 
**Lee et al., "Speaker Verification"**( **Kim et al., "Emotion Classification"**) & - & - & \checkmark & - & - & - & - & Speaker Verification \\ 
**Ding et al., "Speaker Emotion Recognition"**( **Lin et al., "Age Estimation"**) & - & - & - & - & - & \checkmark & \checkmark & - \\
**Wang, "Speaker Characterization"**( **Liu et al., "Accent Classification"**) & - & - & \checkmark & - & - & - & \checkmark & - \\ 
\midrule
pyAudioAnalysis~(**Schroeter, "Audio Preprocessing"**) & Silence removal, Speaker diarization & MFCCs. ZCR, Chroma Vector, Chroma Deviation, Energy, Entropy of Energy, Spectral Centroid, Spectral Spread, Spectral Entropy, Spectral Flux, and Spectral Rolloff & - & - & - & - & - & Fixed and HMM-based segmentation and classification, Train and use supervised models for classification and regression, Audio thumbnailing, Feature Visualization, Batch format conversion, Audio recording \\ 
ESPnet (2018) & CTC segmentation & FFT, Speaker ID embedding, Language ID embedding & - & - & - & - & - & Automatic Speech Recognition, Text-To-Speech, Speech Enhancement, Speech Translation \& Machine Translation, Voice conversion, Spoken Language Understanding, Speech Summarization, Singing Voice Synthesis \\ 
\midrule
VANPY ( **Rajput et al., "Speaker Characterization"**) & - & - & - & - & - & - & - & -\\
\end{tabular}
\end{adjustbox}

\section{Introduction to Speaker Characterization Models}
\label{ssec:model_inf}

Various studies have attempted to address the issue of speaker characterization over the years, targeting attributes such as gender, age, dialect, height, and emotion. Early work by**Lee et al., "Speaker Emotion Recognition"**, combined multiple classifications (gender, age, dialect, emotion) in one system using Hidden Markov Models (HMMs) with spectral and prosodic features. While achieving high accuracy for gender classification (96\%), performance varied significantly across other characteristics. Their dataset, collected from a customer-care system, suffered from class imbalance, yielding lower accuracies: 50\% for seven-class dialect, 69\% for seven-class emotion, and 70\% for three-group age classification. Notably, models trained to predict multiple characteristics outperformed single-task models. The addition of pitch features to MFCC consistently improved performance across all models.
In 2006, **Zhang et al., "Speaker Characterization"** demonstrated that different acoustic features contribute varying importance to gender-age classification. Their system incorporated acoustic features like F0 and HNR, alongside temporal features including articulation rate (syllables per second) and pause-to-utterance ratios. Among the five classification methods evaluated, their Artificial Neural Network (ANN) achieved the highest accuracy: 93.14\% for binary gender classification and 65.5\% for the more challenging eight-class combined gender-age classification.

Research by**Tang et al., "Speaker Verification"**, demonstrated the superiority of Support Vector Machine (SVM) over vector quantization and Gaussian mixture modeling for speaker characterization. Using a dataset of Australian English speakers with balanced demographics and controlled recording conditions, and extracting a set of acoustic features, their system achieved 100\% accuracy for gender classification, 98.8\% for three-class age groups, and 98.7\% for three-class accent detection. Later, **Kim et al., "Emotion Classification"**, demonstrated similar performance using SVM for gender classification, achieving 99.5\% accuracy on a multilingual Indian speech corpus.
In 2017, **Lin et al., "Age Estimation"**, investigated joint age-gender classification using Deep Neural Network (DNN). They enhanced the traditional MFCC-based approach by incorporating SDC features. Testing on the Age-Annotated Database of German Telephone Speech, which contains seven age-gender categories, their system achieved 57.2\% accuracy. The telephone channel's bandwidth limitations and potential noise likely presented additional challenges beyond those inherent in training complex models with limited data.
**Ding et al., "Speaker Emotion Recognition"**, later demonstrated the superiority of Deep Neural Networks over traditional classification methods using the LibriSpeech audiobook dataset. Their approach enhanced MFCC features with custom time-domain characteristics, improving accuracy by 25\% to achieve 92.9\% for gender classification. 

In 2021, **Wang et al., "Accent Classification"**, demonstrated a combination of the attention mechanism with Long Short Term Memory (LSTM) encoder for joint age-height regression prediction from speech segments in the TIMIT. Their model achieved state-of-the-art performance for age estimation with a Mean Absolute Error (MAE) of 5.62/6.08 years for male and female speakers, respectively. For height estimation, the MAE was 5.24/5.09 centimeters for male and female speakers, respectively.

More recently, **Chen et al., "Accent Classification"**, advanced the field using embeddings from self-supervised pre-trained models. By fine-tuning these speech representations, their models achieved state-of-the-art results on the TIMIT dataset: 99.8\% accuracy for gender classification and remarkably low age MAE of 4.11/4.44 years for male/female speakers. Their work demonstrated the effectiveness of transfer learning from large-scale pre-trained models in addressing data efficiency challenges for speaker characteristic prediction.

Emotion classification and regression have also been extensively studied. **Rajput et al., "Dialect Identification"**, proposed a spectrogram-based model leveraging both shallow and deep CNNs for multi-task emotion classification, achieving 66\% accuracy in an 8-class emotion recognition task on the RAVDESS dataset, while**Huang, "Speaker Emotion Recognition"**, developed transformer-based models for dimensional emotion analysis along arousal, dominance, and valence axes, demonstrating strong performance with a concordance correlation coefficient (CCC) of 0.638 on the MSP-Podcast corpus.

Recently, **Lee et al., "Speaker Verification"** leveraged wav2vec 2.0 for speaker characterization, achieving 100\% accuracy for gender classification and 7.1 years MAE for age estimation on TIMIT. These advancements highlight the continued progress in robust speaker characterization models.