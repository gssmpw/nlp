\section{Related Work}
\label{sec:rw}
The field of speaker characterization encompasses the analysis of physiological and psychological speaker traits, such as gender, age, height, accent, and expressed emotion~\cite{phdthesis_Safavi}. Techniques for speaker characterization typically rely on machine learning models to infer speaker attributes from extracted voice features. These approaches are complemented by preprocessing methods, such as voice activity detection and feature extraction, to prepare the data for downstream analysis.

Table~\ref{tab:capabilities_comparison} provides an overview of speaker characterization approaches and frameworks. The first section presents individual studies focusing on specific characterization tasks.
The second section of the table highlights general-purpose speech-processing frameworks that provide preprocessing and feature extraction capabilities. These frameworks, while offering essential functionalities like VAD and diarization, typically lack comprehensive speaker characterization capabilities. In contrast, our VANPY framework bridges this gap by integrating both fundamental preprocessing components and advanced speaker characterization models.

\begin{table*}[ht]
\centering
\footnotesize
\begin{adjustbox}{width=1\linewidth}
\begin{tabular}{lp{30mm}p{45mm}*{5}{c}p{70mm}}
\toprule
\textbf{Framework/Tool} & \textbf{\makecell[l]{Audio Preprocessing}} & \textbf{\makecell[l]{Feature Extraction and \\ Speaker Representations}} & \multicolumn{5}{c}{\textbf{Speaker Characterization}} & \textbf{\makecell[l]{Other Capabilities and \\ incorporated ML models}} \\ 
\cmidrule(lr){4-8}
& & & \rotatebox{90}{\textbf{Gender ID}} & \rotatebox{90}{\textbf{Dialect/Accent ID}} & \rotatebox{90}{\textbf{Emotion Classification}} & \rotatebox{90}{\textbf{Height Estimation}} & \rotatebox{90}{\textbf{Age Estimation}} & \\ 
\midrule
\citeauthor{shafran2003voice}~(\citeyear{shafran2003voice}) & - & - & \checkmark & 7 & 6 & - & 5 & - \\ 
AGENDER ~(\citeyear{muller2006automatic}) & - & - & \checkmark & - & - & - & 4 & - \\ 
\citeauthor{schultz2007speaker} et al.~(\citeyear{schultz2007speaker}) & - & - & \checkmark & 2 & - & - & - & Speaker Verification, Language ID, Proficiency Level, Attentional State speaker \\ 
\citeauthor{5670700}~(\citeyear{5670700}) & - & - & \checkmark & 3 & - & - & 3 & - \\ 
\citeauthor{gupta2016support}~(\citeyear{gupta2016support}) & - & - & \checkmark & - & - & - & - & - \\ 
\citeauthor{qawaqneh2017dnn}~(\citeyear{qawaqneh2017dnn}) & - & - & \checkmark & - & - & - & 4 & - \\ 
\citeauthor{jahangir2020text}~(\citeyear{jahangir2020text}) & - & - & \checkmark & - & - & - & - & Speaker Verification \\ 
\citeauthor{kaushik2021end}~(\citeyear{kaushik2021end}) & - & - & - & - & - & \checkmark & \checkmark & - \\
\citeauthor{lastow2022language}~(\citeyear{lastow2022language}) & - & - & \checkmark & - & - & - & \checkmark & - \\ 
\midrule
pyAudioAnalysis~(\citeyear{giannakopoulos2015pyaudioanalysis}) & Silence removal, Speaker diarization & MFCCs. ZCR, Chroma Vector, Chroma Deviation, Energy, Entropy of Energy, Spectral Centroid, Spectral Spread, Spectral Entropy, Spectral Flux, and Spectral Rolloff & - & - & - & - & - & Fixed and HMM-based segmentation and classification, Train and use supervised models for classification and regression, Audio thumbnailing, Feature Visualization, Batch format conversion, Audio recording \\ 
ESPnet (2018) & CTC segmentation & FFT, Speaker ID embedding, Language ID embedding & - & - & - & - & - & Automatic Speech Recognition, Text-To-Speech, Speech Enhancement, Speech Translation \& Machine Translation, Voice conversion, Spoken Language Understanding, Speech Summarization, Singing Voice Synthesis \\ 
\midrule
VANPY (2024) & INA speech-music detection, Pyannote Segmentation, Pyannote Diarization, Silero VAD & MFCC, ZCR, Tonnetz, SpeechBrain Speaker ID embedding, Pyannote Speaker ID x-vector embedding, Spectral Centroid, Spectral Bandwidth, Spectral Contrast, Spectral Flatness, Fundamental Frequencies & \textcolor{blue}{\checkmark} & - & \textcolor{blue}{7, 8} & \textcolor{blue}{\checkmark} & \textcolor{blue}{\checkmark} & SpeechBrain emotion recognition, Audio classification (YAMNet), Facebook Wav2vec 2.0 arousal-dominance-valence estimation, Language ID (OpenAI Whisper), Speech-To-Text (Facebook Wav2Vec 2.0, OpenAI Whisper), Speech Enhancement (MetricGAN+, SepFormer) \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison of capabilities of various speech and audio processing frameworks. Number instead of \(\checkmark\) represents the number of classes for the classification variant of the capability. The blue color indicates models trained as a part of this research.}
\label{tab:capabilities_comparison}
\end{table*}

\subsection{Audio Preprocessing}

Preprocessing is crucial for consistent, high-quality input in voice analysis. Basic operations like resampling and amplitude normalization align sampling rates and mitigate recording differences. These steps often precede more advanced techniques such as diarization and VAD~\cite{848229}, which isolate speech segments. Recent advances in deep learning have significantly improved the performance of these preprocessing techniques.

\citet{Bredin2021} proposed the Pyannote segmentation model, which demonstrates robust performance across benchmark datasets by integrating VAD with speaker diarization. Silero VAD~\cite{Silero} supports over 100 languages and has been shown to outperform other VAD models like Picovoice\footnote{https://picovoice.ai/} and WebRTC\footnote{https://webrtc.org/} in precision-recall metrics. INAâ€™s music-speech detection model~\cite{doukhan2018ina} excels at distinguishing speech from music and achieved first place in the MIREX2018 speech detection challenge.\footnote{https://music-ir.org/mirex/wiki/MIREX\_HOME} These preprocessing techniques ensure clean and reliable audio input for downstream speaker characterization.

Several open-source frameworks, such as Kaldi~\cite{Povey_ASRU2011} and pyAudioAnalysis~\cite{giannakopoulos2015pyaudioanalysis}, also integrate these essential preprocessing components, including resampling and VAD, thereby ensuring consistent audio input for subsequent speaker
characterization.

\subsection{Feature Extraction and Speaker Representations}

Voice data can be represented and analyzed using various techniques, including feature extraction, time/frequency domain representations, and speaker embeddings. These methods provide unique perspectives on the acoustic and temporal properties of speech, enabling robust speaker characterization and identification.

Feature extraction from voice segments has seen several breakthroughs in recent decades~\cite{8882461, prabakaran2019review, li2019vocal, al2019new, TIRUMALA2017250}. Known features include Mel Frequency Cepstral Coefficients (MFCC)~\cite{tiwari2010mfcc, ganchev2005comparative, nakagawa2011speaker}, which capture spectral envelopes approximating human auditory perception, Shifted Delta Cepstral coefficients (SDC)~\cite{torres2002approaches}, which capture temporal dynamics across multiple frames, and Zero-Crossing-Rate (ZCR)~\cite{khan2012hindi, ramaiah2016multi}, measuring frequency content changes through zero-amplitude axis crossings. Frequency-based features include fundamental frequency (F0) for pitch perception, jitter for pitch perturbations, and formant frequencies characterizing vocal tract resonance. Energy and amplitude parameters like shimmer quantify amplitude variations, while harmonics-to-noise ratio (HNR) compares harmonic content to noise. Additional spectral and temporal features such as loudness peaks and voiced region rates provide further insights~\cite{eyben2015geneva}. Modern open-source libraries like \textit{librosa}~\cite{brian_mcfee_2022_6097378} facilitate extraction of spectral features, including MFCC, spectral centroid, bandwidth, contrast, flatness, and roll-off. The library also provides methods of feature manipulation capabilities like delta features for capturing temporal dynamics. 

Many models process inputs as raw waveforms (time domain) or spectrograms (frequency domain) arrays or images~\cite{jia2021speaker, zhang2018text, nagrani2017voxceleb, palaz2015analysis, muckenhirn2018towards, 8639585}, offering an alternative perspective that capitalizes on deep learning architectures designed for image-like data.

Over the past decade, speaker embeddings have emerged as powerful neural representations that capture the unique characteristics of a speaker's voice in a compact, fixed--
dimensional vector space. These advanced models have demonstrated superior performance in speaker identification tasks \cite{BAI202165}, surpassing traditional methods that directly utilize raw acoustic features. These models produce speaker embeddings that encapsulate deeper properties of a speaker's voice. Embeddings for speaker characterization can be generally applied to text-independent tasks, such as i-vectors~\cite{dehak2010front, kanagasundaram2011vector}, d-vectors~\cite{variani2014deep,jung2017d,8462628}, and x-vectors~\cite{snyder2017deep, zeinali2018convolutional}. In particular, x-vectors are learned via time-delay neural networks (TDNN) to encode speaker-specific information, and \textit{PyAnnote~2.0}~\cite{Bredin2021} provides an implementation optimized for speaker diarization. 
More recent studies have enhanced these architectures, with ECAPA-TDNN~\cite{Desplanques_2020} 
expanding the x-vector approach through channel attention and advanced aggregation mechanisms. This method has been integrated into \textit{SpeechBrain}~\cite{speechbrain, Dawalatabad_2021}, demonstrating state-of-the-art performance and broad applicability in speaker-related applications.

\subsection{Speaker Characterization Models}
\label{ssec:model_inf}

Various studies have attempted to address the issue of speaker characterization over the years, targeting attributes such as gender, age, dialect, height, and emotion. Early work by~\citet{shafran2003voice} combined multiple classifications (gender, age, dialect, emotion) in one system using Hidden Markov Models (HMMs) with spectral and prosodic features. While achieving high accuracy for gender classification (96\%), performance varied significantly across other characteristics. Their dataset, collected from a customer-care system, suffered from class imbalance, yielding lower accuracies: 50\% for seven-class dialect, 69\% for seven-class emotion, and 70\% for three-group age classification. Notably, models trained to predict multiple characteristics outperformed single-task models. The addition of pitch features to MFCC consistently improved performance across all models.
In 2006,~\citet{muller2006automatic} demonstrated that different acoustic features contribute varying importance to gender-age classification. Their system incorporated acoustic features like F0 and HNR, alongside temporal features including articulation rate (syllables per second) and pause-to-utterance ratios. Among the five classification methods evaluated, their Artificial Neural Network (ANN) achieved the highest accuracy: 93.14\% for binary gender classification and 65.5\% for the more challenging eight-class combined gender-age classification.

Research by~\citet{5670700} demonstrated the superiority of Support Vector Machine (SVM) over vector quantization and Gaussian mixture modeling for speaker characterization. Using a dataset of Australian English speakers with balanced demographics and controlled recording conditions, and extracting a set of acoustic features, their system achieved 100\% accuracy for gender classification, 98.8\% for three-class age groups, and 98.7\% for three-class accent detection. Later,~\citet{gupta2016support} demonstrated similar performance using SVM for gender classification, achieving 99.5\% accuracy on a multilingual Indian speech corpus.
In 2017,~\citet{qawaqneh2017dnn} investigated joint age-gender classification using Deep Neural Network (DNN). They enhanced the traditional MFCC-based approach by incorporating SDC features. Testing on the Age-Annotated Database of German Telephone Speech, which contains seven age-gender categories, their system achieved 57.2\% accuracy. The telephone channel's bandwidth limitations and potential noise likely presented additional challenges beyond those inherent in training complex models with limited data.
\citet{jahangir2020text} later demonstrated the superiority of Deep Neural Networks over traditional classification methods using the LibriSpeech audiobook dataset. Their approach enhanced MFCC features with custom time-domain characteristics, improving accuracy by 25\% to achieve 92.9\% for gender classification. 

In 2021,~\citet{kaushik2021end} demonstrated a combination of the attention mechanism with Long Short Term Memory (LSTM) encoder for joint age-height regression prediction from speech segments in the TIMIT. Their model achieved state-of-the-art performance for age estimation with a Mean Absolute Error (MAE) of 5.62/6.08 years for male and female speakers, respectively. For height estimation, the MAE was 5.24/5.09 centimeters for male and female speakers, respectively.

More recently,~\citet{lastow2022language} advanced the field using embeddings from self-supervised pre-trained models. By fine-tuning these speech representations, their models achieved state-of-the-art results on the TIMIT dataset: 99.8\% accuracy for gender classification and remarkably low age  MAE of 4.11/4.44 years for male/female speakers. Their work demonstrated the effectiveness of transfer learning from large-scale pre-trained models in addressing data efficiency challenges for speaker characteristic prediction.

Emotion classification and regression have also been extensively studied.~\citet{zeng2019spectrogram} proposed a spectrogram-based model leveraging both shallow and deep CNNs for multi-task emotion classification, achieving 66\% accuracy in an 8-class emotion recognition task on the RAVDESS dataset, while~\citet{wagner2022dawn} developed transformer-based models for dimensional emotion analysis along arousal, dominance, and valence axes, demonstrating strong performance with a concordance correlation coefficient (CCC) of 0.638 on the MSP-Podcast corpus.

Recently, ~\citet{burkhardt2023speechbasedagegenderprediction} leveraged wav2vec 2.0 for speaker characterization, achieving 100\% accuracy for gender classification and 7.1 years MAE for age estimation on TIMIT. These advancements highlight the continued progress in robust speaker characterization models.