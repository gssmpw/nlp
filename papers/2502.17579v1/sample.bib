@article{doukhan2018ina,
  title={Ina’s mirex 2018 music and speech detection system},
  author={Doukhan, David and Lechapt, Eliott and Evrard, Marc and Carrive, Jean},
  journal={Music Information Retrieval Evaluation eXchange (MIREX 2018)},
  year={2018}
}


@misc{Silero,
  author = {SileroTeam},
  title = {Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/snakers4/silero-vad}},
  commit = {insert_some_commit_here},
  email = {hello@silero.ai}
}

@software{brian_mcfee_2022_6097378,
  author       = {Brian McFee and
                  Alexandros Metsai and
                  Matt McVicar and
                  Stefan Balke and
                  Carl Thomé and
                  Colin Raffel and
                  Frank Zalkow and
                  Ayoub Malek and
                  Dana and
                  Kyungyun Lee and
                  Oriol Nieto and
                  Dan Ellis and
                  Jack Mason and
                  Eric Battenberg and
                  Scott Seyfarth and
                  Ryuichi Yamamoto and
                  viktorandreevichmorozov and
                  Keunwoo Choi and
                  Josh Moore and
                  Rachel Bittner and
                  Shunsuke Hidaka and
                  Ziyao Wei and
                  nullmightybofo and
                  Adam Weiss and
                  Darío Hereñú and
                  Fabian-Robert Stöter and
                  Pius Friesch and
                  Matt Vollrath and
                  Taewoon Kim and
                  Thassilo},
  title        = {librosa/librosa: 0.9.1},
  month        = {feb},
  year         = {2022},
  publisher    = {Zenodo},
  version      = {0.9.1},
  doi          = {10.5281/zenodo.6097378},
  url          = {https://doi.org/10.5281/zenodo.6097378}
}

@software{reback2020pandas,
    author       = {The pandas development team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    version      = {latest},
    doi          = {10.5281/zenodo.3509134},
    url          = {https://doi.org/10.5281/zenodo.3509134}
}

@misc{https://doi.org/10.48550/arxiv.2006.11477,
  doi = {10.48550/ARXIV.2006.11477},
  
  url = {https://arxiv.org/abs/2006.11477},
  
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
  
 @misc{mozilla_2020, title={Mozilla Common Voice}, url={https://voice.mozilla.org/}, journal={Common Voice}, author={Mozilla, Mozilla}, year={2020}, month={Dec}} 
 
 @article{Busso2008IEMOCAPIE,
  title={IEMOCAP: interactive emotional dyadic motion capture database},
  author={Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Ebrahim (Abe) Kazemzadeh and Emily Mower Provost and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},
  journal={Language Resources and Evaluation},
  year={2008},
  volume={42},
  pages={335-359}
}

 @misc{speechbrain_2021, title={Speechbrain/emotion-recognition-WAV2VEC2-IEMOCAP · hugging face}, url={https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP}, journal={speechbrain/emotion-recognition-wav2vec2-IEMOCAP · Hugging Face}, author={SpeechBrain, SpeechBrain}, year={2021}, month={Oct}} 
 
 @misc{speechbrain,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}

@dataset{livingstone_steven_r_2018_1188976,
  author       = {Livingstone, Steven R. and
                  Russo, Frank A.},
  title        = {{The Ryerson Audio-Visual Database of Emotional 
                   Speech and Song (RAVDESS)}},
  month        = {apr},
  year         = {2018},
  note         = {{Funding Information Natural Sciences and 
                   Engineering Research Council of Canada:
                   2012-341583  Hear the world research chair in
                   music and emotional speech from Phonak}},
  publisher    = {Zenodo},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.1188976},
  url          = {https://doi.org/10.5281/zenodo.1188976}
}

@ARTICLE{848229,
  author={Tanyer, S.G. and Ozer, H.},
  journal={IEEE Transactions on Speech and Audio Processing}, 
  title={Voice activity detection in nonstationary noise}, 
  year={2000},
  volume={8},
  number={4},
  pages={478-482},
  doi={10.1109/89.848229}}
  
@article{eyben2015geneva,
  title={The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing},
  author={Eyben, Florian and Scherer, Klaus R and Schuller, Bj{\"o}rn W and Sundberg, Johan and Andr{\'e}, Elisabeth and Busso, Carlos and Devillers, Laurence Y and Epps, Julien and Laukka, Petri and Narayanan, Shrikanth S and others},
  journal={IEEE transactions on affective computing},
  volume={7},
  number={2},
  pages={190--202},
  year={2015},
  publisher={IEEE}
}
@article{tiwari2010mfcc,
  title={MFCC and its applications in speaker recognition},
  author={Tiwari, Vibha},
  journal={International journal on emerging technologies},
  volume={1},
  number={1},
  pages={19--22},
  year={2010},
  publisher={Citeseer}
}
@article{khan2012hindi,
  title={Hindi speaking person identification using zero crossing rate},
  author={Khan, Arif Ullah and Bhaiya, LP and Banchhor, SK},
  journal={Int. J. of Soft Computing and Engineering},
  volume={2},
  number={3},
  pages={101--104},
  year={2012},
  publisher={Citeseer}
}
@inproceedings{ramaiah2016multi,
  title={Multi-speaker activity detection using zero crossing rate},
  author={Ramaiah, V Subba and Rao, R Rajeswara},
  booktitle={2016 International Conference on Communication and Signal Processing (ICCSP)},
  pages={0023--0026},
  year={2016},
  organization={IEEE}
}
@inproceedings{ganchev2005comparative,
  title={Comparative evaluation of various MFCC implementations on the speaker verification task},
  author={Ganchev, Todor and Fakotakis, Nikos and Kokkinakis, George},
  booktitle={Proceedings of the SPECOM},
  volume={1},
  number={2005},
  pages={191--194},
  year={2005}
}
@article{nakagawa2011speaker,
  title={Speaker identification and verification by combining MFCC and phase information},
  author={Nakagawa, Seiichi and Wang, Longbiao and Ohtsuka, Shinji},
  journal={IEEE transactions on audio, speech, and language processing},
  volume={20},
  number={4},
  pages={1085--1095},
  year={2011},
  publisher={IEEE}
}
@inproceedings{snyder2018x,
  title={X-vectors: Robust dnn embeddings for speaker recognition},
  author={Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5329--5333},
  year={2018},
  organization={IEEE}
}
@article{zeinali2018convolutional,
  title={Convolutional neural networks and x-vector embedding for DCASE2018 acoustic scene classification challenge},
  author={Zeinali, Hossein and Burget, Lukas and Cernocky, Jan},
  journal={arXiv preprint arXiv:1810.04273},
  year={2018}
}
@inproceedings{jung2017d,
  title={D-vector based speaker verification system using Raw Waveform CNN},
  author={Jung, Jeeweon and Heo, Heesoo and Yang, Ilho and Yoon, Sunghyun and Shim, Hyejin and Yu, Hajin},
  booktitle={2017 International Seminar on Artificial Intelligence, Networking and Information Technology (ANIT 2017)},
  pages={126--131},
  year={2017},
  organization={Atlantis Press}
}
@INPROCEEDINGS{8462628,
  author={Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopz},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Speaker Diarization with LSTM}, 
  year={2018},
  volume={},
  number={},
  pages={5239-5243},
  doi={10.1109/ICASSP.2018.8462628}}

@article{dehak2010front,
  title={Front-end factor analysis for speaker verification},
  author={Dehak, Najim and Kenny, Patrick J and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={19},
  number={4},
  pages={788--798},
  year={2010},
  publisher={IEEE}
}
@inproceedings{kanagasundaram2011vector,
  title={I-vector based speaker recognition on short utterances},
  author={Kanagasundaram, Ahilan and Vogt, Robert and Dean, David and Sridharan, Sridha and Mason, Michael},
  booktitle={Proceedings of the 12th Annual Conference of the International Speech Communication Association},
  pages={2341--2344},
  year={2011},
  organization={International Speech Communication Association}
}
@inproceedings{huang2020speaker,
  title={Speaker Characterization Using TDNN, TDNN-LSTM, TDNN-LSTM-Attention based Speaker Embeddings for NIST SRE 2019.},
  author={Huang, Chien-Lin},
  booktitle={Odyssey},
  pages={423--427},
  year={2020}
}
@article{jia2021speaker,
  title={Speaker recognition based on characteristic spectrograms and an improved self-organizing feature map neural network},
  author={Jia, Yanjie and Chen, Xi and Yu, Jieqiong and Wang, Lianming and Xu, Yuanzhe and Liu, Shaojin and Wang, Yonghui},
  journal={Complex \& Intelligent Systems},
  volume={7},
  number={4},
  pages={1749--1757},
  year={2021},
  publisher={Springer}
}
@article{zhang2018text,
  title={Text-independent speaker verification based on triplet convolutional neural network embeddings},
  author={Zhang, Chunlei and Koishida, Kazuhito and Hansen, John HL},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={9},
  pages={1633--1644},
  year={2018},
  publisher={IEEE}
}
@article{nagrani2017voxceleb,
  title={Voxceleb: a large-scale speaker identification dataset},
  author={Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1706.08612},
  year={2017}
}
@techreport{palaz2015analysis,
  title={Analysis of CNN-based speech recognition system using raw speech as input},
  author={Palaz, Dimitri and Collobert, Ronan and others},
  year={2015},
  institution={Idiap}
}
@inproceedings{muckenhirn2018towards,
  title={Towards directly modeling raw speech signal for speaker verification using CNNs},
  author={Muckenhirn, Hannah and Doss, Mathew Magimai- and Marcell, S{\'e}bastien},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4884--4888},
  year={2018},
  organization={IEEE}
}
@article{anguera2012speaker,
  title={Speaker diarization: A review of recent research},
  author={Anguera, Xavier and Bozonnet, Simon and Evans, Nicholas and Fredouille, Corinne and Friedland, Gerald and Vinyals, Oriol},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={20},
  number={2},
  pages={356--370},
  year={2012},
  publisher={IEEE}
}
@article{NAGRANI2020101027,
title = {Voxceleb: Large-scale speaker verification in the wild},
journal = {Computer Speech \& Language},
volume = {60},
pages = {101027},
year = {2020},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.101027},
url = {https://www.sciencedirect.com/science/article/pii/S0885230819302712},
author = {Arsha Nagrani and Joon Son Chung and Weidi Xie and Andrew Zisserman},
keywords = {Speaker identification, Speaker verification, Deep learning, Convolutional neural network},
abstract = {The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million ‘real-world’ utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.}
}
@INPROCEEDINGS{8639585,
  author={Ravanelli, Mirco and Bengio, Yoshua},
  booktitle={2018 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Speaker Recognition from Raw Waveform with SincNet}, 
  year={2018},
  volume={},
  number={},
  pages={1021-1028},
  doi={10.1109/SLT.2018.8639585}}

@INPROCEEDINGS{5670700,
  author={Nguyen, Phuoc and Tran, Dat and Xu Huang and Sharma, Dharmendra},
  booktitle={International Conference on Communications and Electronics 2010}, 
  title={Automatic classification of speaker characteristics}, 
  year={2010},
  volume={},
  number={},
  pages={147-152},
  doi={10.1109/ICCE.2010.5670700}}

@inproceedings{eyben2010opensmile,
  title={Opensmile: the munich versatile and fast open-source audio feature extractor},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1459--1462},
  year={2010}
}
@inproceedings{lei2014novel,
  title={A novel scheme for speaker recognition using a phonetically-aware deep neural network},
  author={Lei, Yun and Scheffer, Nicolas and Ferrer, Luciana and McLaren, Mitchell},
  booktitle={2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={1695--1699},
  year={2014},
  organization={IEEE}
}
@inproceedings{variani2014deep,
  title={Deep neural networks for small footprint text-dependent speaker verification},
  author={Variani, Ehsan and Lei, Xin and McDermott, Erik and Moreno, Ignacio Lopez and Gonzalez-Dominguez, Javier},
  booktitle={2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4052--4056},
  year={2014},
  organization={IEEE}
}
@inproceedings{snyder2017deep,
  title={Deep neural network embeddings for text-independent speaker verification.},
  author={Snyder, David and Garcia-Romero, Daniel and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  pages={999--1003},
  year={2017}
}
@inproceedings{chen2019speaker,
  title={Speaker characterization using tdnn-lstm based speaker embedding},
  author={Chen, Chia-Ping and Zhang, Su-Yu and Yeh, Chih-Ting and Wang, Jia-Ching and Wang, Tenghui and Huang, Chien-Lin},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6211--6215},
  year={2019},
  organization={IEEE}
}
@article{milde2018unspeech,
  title={Unspeech: Unsupervised speech context embeddings},
  author={Milde, Benjamin and Biemann, Chris},
  journal={arXiv preprint arXiv:1804.06775},
  year={2018}
}
@inproceedings{bredin2020pyannote,
  title={Pyannote.audio: neural building blocks for speaker diarization},
  author={Bredin, Herv{\'e} and Yin, Ruiqing and Coria, Juan Manuel and Gelly, Gregory and Korshunov, Pavel and Lavechin, Marvin and Fustes, Diego and Titeux, Hadrien and Bouaziz, Wassim and Gill, Marie-Philippe},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7124--7128},
  year={2020},
  organization={IEEE}
}
@article{nagrani2020voxsrc,
  title={Voxsrc 2020: The second voxceleb speaker recognition challenge},
  author={Nagrani, Arsha and Chung, Joon Son and Huh, Jaesung and Brown, Andrew and Coto, Ernesto and Xie, Weidi and McLaren, Mitchell and Reynolds, Douglas A and Zisserman, Andrew},
  journal={arXiv preprint arXiv:2012.06867},
  year={2020}
}
@article{giannakopoulos2015pyaudioanalysis,
  title={pyAudioAnalysis: An Open-Source Python Library for Audio Signal Analysis},
  author={Giannakopoulos, Theodoros},
  journal={PloS one},
  volume={10},
  number={12},
  year={2015},
  publisher={Public Library of Science}
}
@inproceedings{Bredin2021,
  Title = {{End-to-end speaker segmentation for overlap-aware resegmentation}},
  Author = {{Bredin}, Herv{\'e} and {Laurent}, Antoine},
  Booktitle = {Proc. Interspeech 2021},
  Address = {Brno, Czech Republic},
  Month = {August},
  Year = {2021},
 }
 @inproceedings{watanabe2018espnet,
  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Proceedings of Interspeech},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}
@inproceedings{muller2006automatic,
  title={Automatic recognition of speakers' age and gender on the basis of empirical studies},
  author={M{\"u}ller, Christian},
  booktitle={Ninth International Conference on Spoken Language Processing},
  year={2006}
}
@incollection{schultz2007speaker,
  title={Speaker characteristics},
  author={Schultz, Tanja},
  booktitle={Speaker classification I},
  pages={47--74},
  year={2007},
  publisher={Springer}
}
@inproceedings{dehak2011language,
  title={Language recognition via i-vectors and dimensionality reduction},
  author={Dehak, Najim and Torres-Carrasquillo, Pedro A and Reynolds, Douglas and Dehak, Reda},
  booktitle={Twelfth annual conference of the international speech communication association},
  year={2011},
  organization={Citeseer}
}
@inproceedings{gupta2016support,
  title={Support vector machine based gender identification using voiced speech frames},
  author={Gupta, Manish and Bharti, Shambhu Shankar and Agarwal, Suneeta},
  booktitle={2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)},
  pages={737--741},
  year={2016},
  organization={IEEE}
}
@inproceedings{qawaqneh2017dnn,
  title={DNN-based models for speaker age and gender classification},
  author={Qawaqneh, Zakariya and Mallouh, Arafat Abu and Barkana, Buket D},
  booktitle={International Conference on Bio-inspired Systems and Signal Processing},
  volume={5},
  pages={106--111},
  year={2017},
  organization={SCITEPRESS}
}
@article{jahangir2020text,
  title={Text-independent speaker identification through feature fusion and deep neural network},
  author={Jahangir, Rashid and Teh, Ying Wah and Memon, Nisar Ahmed and Mujtaba, Ghulam and Zareei, Mahdi and Ishtiaq, Uzair and Akhtar, Muhammad Zaheer and Ali, Ihsan},
  journal={IEEE Access},
  volume={8},
  pages={32187--32202},
  year={2020},
  publisher={IEEE}
}
@inproceedings{panayotov2015librispeech,
  title={Librispeech: an asr corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}
@misc{kaushik2021end,
      title={End-to-End Speaker Height and age estimation using Attention Mechanism with LSTM-RNN}, 
      author={Manav Kaushik and Van Tung Pham and Eng Siong Chng},
      year={2021},
      eprint={2101.05056},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2101.05056}, 
}
@article{discacoustic,
  title={Acoustic-Phonetic Continuous Speech Corpus},
  author={Disc, NIST Speech and Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S and Dahlgren, Nancy L}
}
@article{Smoking,
author = {Ma, Zhizhong and Bullen, Chris and Chu, Joanna and Wang, Ruili and Wang, Yingchun and Singh, Satwinder},
year = {2021},
month = {06},
pages = {},
title = {Towards the Objective Speech Assessment of Smoking Status based on Voice Features: A Review of the Literature}
}
@INPROCEEDINGS{8701961,
  author={Aich, Satyabrata and Kim, Hee-Cheol and younga, Kim and Hui, Kueh Lee and Al-Absi, Ahmed Abdulhakim and Sain, Mangal},
  booktitle={2019 21st International Conference on Advanced Communication Technology (ICACT)}, 
  title={A Supervised Machine Learning Approach using Different Feature Selection Techniques on Voice Datasets for Prediction of Parkinson’s Disease}, 
  year={2019},
  volume={},
  number={},
  pages={1116-1121},
  doi={10.23919/ICACT.2019.8701961}}
@misc{vuleta_2022, title={How much data is created every day? [27 powerful stats]}, url={https://seedscientific.com/how-much-data-is-created-every-day/#:~:text=Every\%20day\%2C\%20we\%20create\%20roughly,rate\%20will\%20become\%20even\%20greater.}, journal={SeedScientific}, author={Vuleta, Branka}, year={2022}, month={Aug}} 
@misc{andre_2021, title={53 important statistics about how much data is created every day}, url={https://financesonline.com/how-much-data-is-created-every-day/}, journal={Financesonline.com}, publisher={FinancesOnline.com}, author={Andre, Louie}, year={2021}, month={Jun}} 
@article{alam2020survey,
  title={Survey on deep neural networks in speech and vision systems},
  author={Alam, Mahbubul and Samad, Manar D and Vidyaratne, Lasitha and Glandon, Alexander and Iftekharuddin, Khan M},
  journal={Neurocomputing},
  volume={417},
  pages={302--321},
  year={2020},
  publisher={Elsevier}
}
@misc{statista_research_department_9_2022, title={Audio content consumption change due to COVID-19 Europe 2021}, url={https://www.statista.com/statistics/1234171/audio-content-consumption-change-europe/}, 
 journal={Statista}, 
 publisher={Statista Research Department}, 
 author={Published by Statista Research Department}, 
 year={2022}, 
 month={Jun}
} 
@inproceedings{speech_covid_19,
author = {Han, Jing and Qian, Kun and Song, Meishu and Yang, Zijiang and Ren, Zhao and Liu, Shuo and Liu, Juan and Zheng, Huaiyuan and Ji, Wei and Koike, Tomoya and Li, Xiao and Zhang, Zixing and Yamamoto, Yoshiharu and Schuller, Björn},
year = {2020},
month = {10},
pages = {},
title = {An Early Study on Intelligent Analysis of Speech under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety},
doi = {10.21437/Interspeech.2020-2223}
}
@incollection{kerkeni2019automatic,
  title={Automatic speech emotion recognition using machine learning},
  author={Kerkeni, Leila and Serrestou, Youssef and Mbarki, Mohamed and Raoof, Kosai and Mahjoub, Mohamed Ali and Cleder, Catherine},
  booktitle={Social media and machine learning},
  year={2019},
  publisher={IntechOpen}
}
@ARTICLE{6359792,
  author={Ooi, Kuan Ee Brian and Lech, Margaret and Allen, Nicholas B.},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Multichannel Weighted Speech Classification System for Prediction of Major Depression in Adolescents}, 
  year={2013},
  volume={60},
  number={2},
  pages={497-506},
  doi={10.1109/TBME.2012.2228646}}
@inproceedings{shafran2003voice,
  title={Voice signatures},
  author={Shafran, Izhak and Riley, Michael and Mohri, Mehryar},
  booktitle={2003 IEEE workshop on automatic speech recognition and understanding (IEEE Cat. No. 03EX721)},
  pages={31--36},
  year={2003},
  organization={IEEE}
}
@phdthesis{phdthesis_Safavi,
author = {Safavi, Saeid},
year = {2015},
month = {01},
pages = {},
title = {Speaker Characterization using Adult and Children's Speech},
doi = {10.13140/RG.2.1.2814.4486}
}
@article{alnuaim2022speaker,
  title={Speaker Gender Recognition Based on Deep Neural Networks and ResNet50},
  author={Alnuaim, Abeer Ali and Zakariah, Mohammed and Shashidhar, Chitra and Hatamleh, Wesam Atef and Tarazi, Hussam and Shukla, Prashant Kumar and Ratna, Rajnish},
  journal={Wireless Communications and Mobile Computing},
  volume={2022},
  year={2022},
  publisher={Hindawi}
}
@article{DBLP:journals/corr/abs-2109-13510,
  author    = {Khaled Hechmi and
               Trung Ngo Trong and
               Ville Hautam{\"{a}}ki and
               Tomi Kinnunen},
  title     = {VoxCeleb Enrichment for Age and Gender Recognition},
  journal   = {CoRR},
  volume    = {abs/2109.13510},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.13510},
  eprinttype = {arXiv},
  eprint    = {2109.13510},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-13510.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{https://doi.org/10.48550/arxiv.2212.04356,
  doi = {10.48550/ARXIV.2212.04356},
  
  url = {https://arxiv.org/abs/2212.04356},
  
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  
  keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{fu2021metricgan,
  title={MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement},
  author={Fu, Szu-Wei and Yu, Cheng and Hsieh, Tsun-An and Plantinga, Peter and Ravanelli, Mirco and Lu, Xugang and Tsao, Yu},
  journal={arXiv preprint arXiv:2104.03538},
  year={2021}
}
@inproceedings{subakan2021attention,
      title={Attention is All You Need in Speech Separation}, 
      author={Cem Subakan and Mirco Ravanelli and Samuele Cornell and Mirko Bronzi and Jianyuan Zhong},
      year={2021},
      booktitle={ICASSP 2021}
}
@misc{wagner2022dawn,
      title={Dawn of the transformer era in speech emotion recognition: closing the valence gap}, 
      author={Johannes Wagner and Andreas Triantafyllopoulos and Hagen Wierstorf and Maximilian Schmitt and Felix Burkhardt and Florian Eyben and Björn W. Schuller},
      year={2022},
      eprint={2203.07378},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@misc{Plakal_Ellis, title={YAMNet}, url={https://github.com/tensorflow/models/tree/master/research/audioset/yamnet}, journal={GitHub}, author={Plakal, Manoj and Ellis, Dan}} 

@article{DBLP:journals/corr/abs-1809-08761,
  author       = {Mahmoud Azab and
                  Mingzhe Wang and
                  Max Smith and
                  Noriyuki Kojima and
                  Jia Deng and
                  Rada Mihalcea},
  title        = {Speaker Naming in Movies},
  journal      = {CoRR},
  volume       = {abs/1809.08761},
  year         = {2018},
  url          = {http://arxiv.org/abs/1809.08761},
  eprinttype    = {arXiv},
  eprint       = {1809.08761},
  timestamp    = {Mon, 01 Feb 2021 18:33:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1809-08761.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bost-etal-2020-serial,
    title = "Serial Speakers: a Dataset of {TV} Series",
    author = "Bost, Xavier  and
      Labatut, Vincent  and
      Linares, Georges",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.525",
    pages = "4256--4264",
    abstract = "For over a decade, TV series have been drawing increasing interest, both from the audience and from various academic fields. But while most viewers are hooked on the continuous plots of TV serials, the few annotated datasets available to researchers focus on standalone episodes of classical TV series. We aim at filling this gap by providing the multimedia/speech processing communities with {``}Serial Speakers{''}, an annotated dataset of 155 episodes from three popular American TV serials: {``}Breaking Bad{''}, {``}Game of Thrones{''} and {``}House of Cards{''}. {``}Serial Speakers{''} is suitable both for investigating multimedia retrieval in realistic use case scenarios, and for addressing lower level speech related tasks in especially challenging conditions. We publicly release annotations for every speech turn (boundaries, speaker) and scene boundary, along with annotations for shot boundaries, recurring shots, and interacting speakers in a subset of episodes. Because of copyright restrictions, the textual content of the speech turns is encrypted in the public version of the dataset, but we provide the users with a simple online tool to recover the plain text from their own subtitle files.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@article{mokhtari2008speaking,
  title={Speaking style variation and speaker personality},
  author={Mokhtari, Akiko and Campbell, Nick},
  journal={Proc. of Speech Prosody, Campinas, Brazil},
  pages={601--604},
  year={2008},
  publisher={Citeseer}
}
@inproceedings{parikh2020english,
  title={English language accent classification and conversion using machine learning},
  author={Parikh, Pratik and Velhal, Ketaki and Potdar, Sanika and Sikligar, Aayushi and Karani, Ruhina},
  booktitle={Proceedings of the International Conference on Innovative Computing \& Communications (ICICC)},
  year={2020}
}
@article{jason2020appraisal,
  title={An appraisal on speech and emotion recognition technologies based on machine learning},
  author={Jason, C Andy and Kumar, Sandeep and others},
  journal={language},
  volume={67},
  pages={68},
  year={2020}
}
@article{hegde2019survey,
  title={A survey on machine learning approaches for automatic detection of voice disorders},
  author={Hegde, Sarika and Shetty, Surendra and Rai, Smitha and Dodderi, Thejaswi},
  journal={Journal of Voice},
  volume={33},
  number={6},
  pages={947--e11},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{amano2009classifying,
  title={Classifying clear and conversational speech based on acoustic features},
  author={Amano-Kusumoto, Akiko and Hosom, John-Paul and Shafran, Izhak},
  booktitle={Tenth Annual Conference of the International Speech Communication Association},
  year={2009}
}
@article{reid2022development,
  title={Development of a machine-learning based voice disorder screening tool},
  author={Reid, Jonathan and Parmar, Preet and Lund, Tyler and Aalto, Daniel K and Jeffery, Caroline C},
  journal={American Journal of Otolaryngology},
  volume={43},
  number={2},
  pages={103327},
  year={2022},
  publisher={Elsevier}
}
@misc{zhang2022paddlespeech,
      title={PaddleSpeech: An Easy-to-Use All-in-One Speech Toolkit}, 
      author={Hui Zhang and Tian Yuan and Junkun Chen and Xintong Li and Renjie Zheng and Yuxin Huang and Xiaojie Chen and Enlei Gong and Zeyu Chen and Xiaoguang Hu and Dianhai Yu and Yanjun Ma and Liang Huang},
      year={2022},
      eprint={2205.12007},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
@article{Kinnunen,
author = {Kinnunen, Tomi and Li, Haizhou},
year = {2010},
month = {01},
pages = {12-40},
title = {An Overview of Text-Independent Speaker Recognition: from Features to Supervectors},
volume = {52},
journal = {Speech Communication},
doi = {10.1016/j.specom.2009.08.009}
}
@inproceedings{prabakaran2019review,
  title={A review on performance of voice feature extraction techniques},
  author={Prabakaran, D and Shyamala, R},
  booktitle={2019 3rd International Conference on Computing and Communications Technologies (ICCCT)},
  pages={221--231},
  year={2019},
  organization={IEEE}
}
@article{kumar2016efficient,
  title={Efficient feature extraction for fear state analysis from human voice},
  author={Kumar, Palo Hemanta and Mohanty, Mihir N},
  journal={Indian Journal of Science \& Technology},
  volume={9},
  number={38},
  pages={1--11},
  year={2016}
}
@INPROCEEDINGS{8882461,
  author={Gumelar, Agustinus Bimo and Kurniawan, Afid and Sooai, Adri Gabriel and Purnomo, Mauridhi Hery and Yuniarno, Eko Mulyanto and Sugiarto, Indar and Widodo, Agung and Kristanto, Andreas Agung and Fahrudin, Tresna Maulana},
  booktitle={2019 IEEE 7th International Conference on Serious Games and Applications for Health (SeGAH)}, 
  title={Human Voice Emotion Identification Using Prosodic and Spectral Feature Extraction Based on Deep Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/SeGAH.2019.8882461}}
@article{li2019vocal,
  title={Vocal features: from voice identification to speech recognition by machine},
  author={Li, Xiaochang and Mills, Mara},
  journal={Technology and culture},
  volume={60},
  number={2},
  pages={S129--S160},
  year={2019},
  publisher={Johns Hopkins University Press}
}
@article{al2019new,
  title={A new method for voice signal features creation},
  author={Al-Dwairi, Majed O and Hendi, Amjad Y and Soliman, Mohamed S and Alqadi, Ziad AA},
  journal={International Journal of Electrical and Computer Engineering},
  volume={9},
  number={5},
  pages={4077},
  year={2019},
  publisher={IAES Institute of Advanced Engineering and Science}
}
@article{TIRUMALA2017250,
title = {Speaker identification features extraction methods: A systematic review},
journal = {Expert Systems with Applications},
volume = {90},
pages = {250-271},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417305535},
author = {Sreenivas Sremath Tirumala and Seyed Reza Shahamiri and Abhimanyu Singh Garhwal and Ruili Wang},
keywords = {Feature extraction, Kitchenham systematic review, MFCC, Speaker identification, Speaker recognition},
abstract = {Speaker Identification (SI) is the process of identifying the speaker from a given utterance by comparing the voice biometrics of the utterance with those utterance models stored beforehand. SI technologies are taken a new direction due to the advances in artificial intelligence and have been used widely in various domains. Feature extraction is one of the most important aspects of SI, which significantly influences the SI process and performance. This systematic review is conducted to identify, compare, and analyze various feature extraction approaches, methods, and algorithms of SI to provide a reference on feature extraction approaches for SI applications and future studies. The review was conducted according to Kitchenham systematic review methodology and guidelines, and provides an in-depth analysis on proposals and implementations of SI feature extraction methods discussed in the literature between year 2011 and 2106. Three research questions were determined and an initial set of 535 publications were identified to answer the questions. After applying exclusion criteria 160 related publications were shortlisted and reviewed in this paper; these papers were considered to answer the research questions. Results indicate that pure Mel-Frequency Cepstral Coefficients (MFCCs) based feature extraction approaches have been used more than any other approach. Furthermore, other MFCC variations, such as MFCC fusion and cleansing approaches, are proven to be very popular as well. This study identified that the current SI research trend is to develop a robust universal SI framework to address the important problems of SI such as adaptability, complexity, multi-lingual recognition, and noise robustness. The results presented in this research are based on past publications, citations, and number of implementations with citations being most relevant. This paper also presents the general process of SI.}
}
@article{BAI202165,
title = {Speaker recognition based on deep learning: An overview},
journal = {Neural Networks},
volume = {140},
pages = {65-99},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000848},
author = {Zhongxin Bai and Xiao-Lei Zhang},
keywords = {Speaker recognition, Speaker verification, Speaker identification, Speaker diarization, Robust speaker recognition, Deep learning},
abstract = {Speaker recognition is a task of identifying persons from their voices. Recently, deep learning has dramatically revolutionized speaker recognition. However, there is lack of comprehensive reviews on the exciting progress. In this paper, we review several major subtasks of speaker recognition, including speaker verification, identification, diarization, and robust speaker recognition, with a focus on deep-learning-based methods. Because the major advantage of deep learning over conventional methods is its representation ability, which is able to produce highly abstract embedding features from utterances, we first pay close attention to deep-learning-based speaker feature extraction, including the inputs, network structures, temporal pooling strategies, and objective functions respectively, which are the fundamental components of many speaker recognition subtasks. Then, we make an overview of speaker diarization, with an emphasis of recent supervised, end-to-end, and online diarization. Finally, we survey robust speaker recognition from the perspectives of domain adaptation and speech enhancement, which are two major approaches of dealing with domain mismatch and noise problems. Popular and recently released corpora are listed at the end of the paper.}
}
@InProceedings{Chung18b,
              author       = "Chung, J.~S. and Nagrani, A. and Zisserman, A.",
              title        = "VoxCeleb2: Deep Speaker Recognition",
              booktitle    = "INTERSPEECH",
              year         = "2018",
            }

@article{khoma2023development,
  title={Development of Supervised Speaker Diarization System Based on the PyAnnote Audio Processing Library},
  author={Khoma, Volodymyr and Khoma, Yuriy and Brydinskyi, Vitalii and Konovalov, Alexander},
  journal={Sensors},
  volume={23},
  number={4},
  pages={2082},
  year={2023},
  publisher={MDPI}
}
@article{zuluaga2023commonaccent,
  title={CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice},
  author={Zuluaga-Gomez, Juan and Ahmed, Sara and Visockas, Danielius and Subakan, Cem},
  journal={arXiv preprint arXiv:2305.18283},
  year={2023}
}
@article{phukan2023comparative,
  title={A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition},
  author={Phukan, Orchid Chetia and Buduru, Arun Balaji and Sharma, Rajesh},
  journal={arXiv preprint arXiv:2304.11472},
  year={2023}
}
@inproceedings{Chen:2016:XST:2939672.2939785,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}
@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}
@article{cox1958regression,
  title={The regression analysis of binary sequences},
  author={Cox, David R},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={20},
  number={2},
  pages={215--232},
  year={1958},
  publisher={Wiley Online Library}
}

@Article{app12010327,
AUTHOR = {Luna-Jiménez, Cristina and Kleinlein, Ricardo and Griol, David and Callejas, Zoraida and Montero, Juan M. and Fernández-Martínez, Fernando},
TITLE = {A Proposal for Multimodal Emotion Recognition Using Aural Transformers and Action Units on RAVDESS Dataset},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {327},
URL = {https://www.mdpi.com/2076-3417/12/1/327},
ISSN = {2076-3417},
ABSTRACT = {Emotion recognition is attracting the attention of the research community due to its multiple applications in different fields, such as medicine or autonomous driving. In this paper, we proposed an automatic emotion recognizer system that consisted of a speech emotion recognizer (SER) and a facial emotion recognizer (FER). For the SER, we evaluated a pre-trained xlsr-Wav2Vec2.0 transformer using two transfer-learning techniques: embedding extraction and fine-tuning. The best accuracy results were achieved when we fine-tuned the whole model by appending a multilayer perceptron on top of it, confirming that the training was more robust when it did not start from scratch and the previous knowledge of the network was similar to the task to adapt. Regarding the facial emotion recognizer, we extracted the Action Units of the videos and compared the performance between employing static models against sequential models. Results showed that sequential models beat static models by a narrow difference. Error analysis reported that the visual systems could improve with a detector of high-emotional load frames, which opened a new line of research to discover new ways to learn from videos. Finally, combining these two modalities with a late fusion strategy, we achieved 86.70% accuracy on the RAVDESS dataset on a subject-wise 5-CV evaluation, classifying eight emotions. Results demonstrated that these modalities carried relevant information to detect users&rsquo; emotional state and their combination allowed to improve the final system performance.},
DOI = {10.3390/app12010327}
}

@Article{s21227665,
AUTHOR = {Luna-Jiménez, Cristina and Griol, David and Callejas, Zoraida and Kleinlein, Ricardo and Montero, Juan M. and Fernández-Martínez, Fernando},
TITLE = {Multimodal Emotion Recognition on RAVDESS Dataset Using Transfer Learning},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {22},
ARTICLE-NUMBER = {7665},
URL = {https://www.mdpi.com/1424-8220/21/22/7665},
PubMedID = {34833739},
ISSN = {1424-8220},
ABSTRACT = {Emotion Recognition is attracting the attention of the research community due to the multiple areas where it can be applied, such as in healthcare or in road safety systems. In this paper, we propose a multimodal emotion recognition system that relies on speech and facial information. For the speech-based modality, we evaluated several transfer-learning techniques, more specifically, embedding extraction and Fine-Tuning. The best accuracy results were achieved when we fine-tuned the CNN-14 of the PANNs framework, confirming that the training was more robust when it did not start from scratch and the tasks were similar. Regarding the facial emotion recognizers, we propose a framework that consists of a pre-trained Spatial Transformer Network on saliency maps and facial images followed by a bi-LSTM with an attention mechanism. The error analysis reported that the frame-based systems could present some problems when they were used directly to solve a video-based task despite the domain adaptation, which opens a new line of research to discover new ways to correct this mismatch and take advantage of the embedded knowledge of these pre-trained models. Finally, from the combination of these two modalities with a late fusion strategy, we achieved 80.08% accuracy on the RAVDESS dataset on a subject-wise 5-CV evaluation, classifying eight emotions. The results revealed that these modalities carry relevant information to detect users’ emotional state and their combination enables improvement of system performance.},
DOI = {10.3390/s21227665}
}
@article{ISSA2020101894,
title = {Speech emotion recognition with deep convolutional neural networks},
journal = {Biomedical Signal Processing and Control},
volume = {59},
pages = {101894},
year = {2020},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2020.101894},
url = {https://www.sciencedirect.com/science/article/pii/S1746809420300501},
author = {Dias Issa and M. {Fatih Demirci} and Adnan Yazici},
keywords = {Speech emotion recognition, Deep learning, Signal processing},
abstract = {The speech emotion recognition (or, classification) is one of the most challenging topics in data science. In this work, we introduce a new architecture, which extracts mel-frequency cepstral coefficients, chromagram, mel-scale spectrogram, Tonnetz representation, and spectral contrast features from sound files and uses them as inputs for the one-dimensional Convolutional Neural Network for the identification of emotions using samples from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), Berlin (EMO-DB), and Interactive Emotional Dyadic Motion Capture (IEMOCAP) datasets. We utilize an incremental method for modifying our initial model in order to improve classification accuracy. All of the proposed models work directly with raw sound data without the need for conversion to visual representations, unlike some previous approaches. Based on experimental results, our best-performing model outperforms existing frameworks for RAVDESS and IEMOCAP, thus setting the new state-of-the-art. For the EMO-DB dataset, it outperforms all previous works except one but compares favorably with that one in terms of generality, simplicity, and applicability. Specifically, the proposed framework obtains 71.61% for RAVDESS with 8 classes, 86.1% for EMO-DB with 535 samples in 7 classes, 95.71% for EMO-DB with 520 samples in 7 classes, and 64.3% for IEMOCAP with 4 classes in speaker-independent audio classification tasks.}
}
@article{zeng2019spectrogram,
  title={Spectrogram based multi-task audio classification},
  author={Zeng, Yuni and Mao, Hua and Peng, Dezhong and Yi, Zhang},
  journal={Multimedia Tools and Applications},
  volume={78},
  pages={3705--3722},
  year={2019},
  publisher={Springer}
}
@INPROCEEDINGS{7843306,
  author={Shegokar, Pankaj and Sircar, Pradip},
  booktitle={2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)}, 
  title={Continuous wavelet transform based speech emotion recognition}, 
  year={2016},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/ICSPCS.2016.7843306}}
@ARTICLE{9078789,
  author={Mustaqeem and Sajjad, Muhammad and Kwon, Soonil},
  journal={IEEE Access}, 
  title={Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM}, 
  year={2020},
  volume={8},
  number={},
  pages={79861-79875},
  doi={10.1109/ACCESS.2020.2990405}}
@INPROCEEDINGS{9640995,
  author={U A, Asiya and V K, Kiran},
  booktitle={2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Speech Emotion Recognition-A Deep Learning Approach}, 
  year={2021},
  volume={},
  number={},
  pages={867-871},
  doi={10.1109/I-SMAC52330.2021.9640995}}
@article{tomar2006converting,
  title={Converting video formats with FFmpeg},
  author={Tomar, Suramya},
  journal={Linux Journal},
  volume={2006},
  number={146},
  pages={10},
  year={2006},
  publisher={Belltown Media}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@misc{Edison_Research_2024, 
 title={The podcast consumer 2024 by Edison Research}, 
 url={https://www.edisonresearch.com/the-podcast-consumer-2024-by-edison-research/}, 
 journal={Edison Research}, 
 author={Edison Research}, 
 year={2024}, 
 month={May},
 abstract={The Podcast Consumer 2024 includes data from The Infinite Dial®, Share of Ear®, Edison Podcast Metrics™, The Kids Podcast Listener Report, Hit Play, Boomer!, The Gen Z Podcast Listener Report, The Latino Podcast Listener Report, and The Sports Audio Report }
} 
@article{akdim2023perceived,
  title={Perceived value of AI-based recommendations﻿ service: the case of voice assistants},
  author={Akdim, K and Casal{\'o}, Luis V},
  journal={Service Business},
  volume={17},
  number={1},
  pages={81--112},
  year={2023},
  publisher={Springer}
}
@article{doi:10.1080/08874417.2024.2312858,
  author    = {Surbhi Choudhary, Neeraj Kaushik, Brijesh Sivathanu and Nripendra P. Rana},
  title     = {Assessing Factors Influencing Customers’ Adoption of AI-Based Voice Assistants},
  journal   = {Journal of Computer Information Systems},
  volume    = {0},
  number    = {0},
  pages     = {1--18},
  year      = {2024},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/08874417.2024.2312858},
  url       = { 
               
               https://doi.org/10.1080/08874417.2024.2312858
               },
  eprint    = { 
               
               https://doi.org/10.1080/08874417.2024.2312858
               }
}
@article{flavian2023effects,
  title={Effects of voice assistant recommendations on consumer behavior},
  author={Flavi{\'a}n, Carlos and Akdim, Khaoula and Casal{\'o}, Luis V},
  journal={Psychology \& Marketing},
  volume={40},
  number={2},
  pages={328--346},
  year={2023},
  publisher={Wiley Online Library}
}
@misc{akiba2019optunanextgenerationhyperparameteroptimization,
      title={Optuna: A Next-generation Hyperparameter Optimization Framework}, 
      author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
      year={2019},
      eprint={1907.10902},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.10902}, 
}
@article{garofolo1993darpa,
  title={DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1},
  author={Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S},
  journal={NASA STI/Recon technical report n},
  volume={93},
  pages={27403},
  year={1993}
}
@inproceedings{lastow2022language,
  title={Language-agnostic age and gender classification of voice using self-supervised pre-training},
  author={Lastow, Fredrik and Ekberg, Edwin and Nugues, Pierre},
  booktitle={2022 Swedish Artificial Intelligence Society Workshop (SAIS)},
  pages={1--9},
  year={2022},
  organization={IEEE}
}
@misc{burkhardt2023speechbasedagegenderprediction,
      title={Speech-based Age and Gender Prediction with Transformers}, 
      author={Felix Burkhardt and Johannes Wagner and Hagen Wierstorf and Florian Eyben and Björn Schuller},
      year={2023},
      eprint={2306.16962},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2306.16962}, 
}
@online{businessdasher_2024,
    author = "{BusinessDasher}",
    title = "53 Voice Search Statistics You Must Know in 2024",
    year = "2024",
    url = "https://www.businessdasher.com/voice-search-statistics/",
    urldate = {2024-01-02}
}
@misc{PricewaterhouseCoopers, title={Staying silent: Voice assistants and the need for trust}, url={https://www.pwc.com.au/digitalpulse/staying-silent-voice-assistants-connected-home-trust.html}, journal={PwC}, author={PricewaterhouseCoopers}, language={en} }
@online{demandsage_2024,
    author = "{DemandSage}",
    title = "Voice Search Statistics: Usage, Market Size \& Future",
    year = "2024",
    url = "https://www.demandsage.com/voice-search-statistics/",
    urldate = {2024-01-02}
}
@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
@inproceedings{Dawalatabad_2021,
   title     = {ECAPA-TDNN Embeddings for Speaker Diarization},
   url       = {http://dx.doi.org/10.21437/Interspeech.2021-941},
   DOI       = {10.21437/Interspeech.2021-941},
   booktitle = {Interspeech 2021},
   publisher = {ISCA},
   author    = {Dawalatabad, Nauman and Ravanelli, Mirco and Grondin, Fran\c{c}ois and Thienpondt, Jenthe and Desplanques, Brecht and Na, Hwidong},
   year      = {2021},
   series    = {Interspeech 2021}
}

@inproceedings{Desplanques_2020,
   title     = {ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification},
   url       = {http://dx.doi.org/10.21437/Interspeech.2020-2650},
   DOI       = {10.21437/Interspeech.2020-2650},
   booktitle = {Interspeech 2020},
   publisher = {ISCA},
   author    = {Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
   year      = {2020},
   series    = {Interspeech 2020}
}
@INPROCEEDINGS{
         Povey_ASRU2011,
         author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
       keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
          title = {The Kaldi Speech Recognition Toolkit},
      booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
           year = {2011},
      publisher = {IEEE Signal Processing Society},
           note = {IEEE Catalog No.: CFP11SRW-USB},
}
@inproceedings{torres2002approaches,
  title={Approaches to language identification using Gaussian mixture models and shifted delta cepstral features.},
  author={Torres-Carrasquillo, Pedro A and Singer, Elliot and Kohler, Mary A and Greene, Richard J and Reynolds, Douglas A and Deller Jr, John R},
  booktitle={Interspeech},
  pages={89--92},
  year={2002},
  organization={Citeseer}
}
@misc{OpenAI2025,
  author    = {OpenAI},
  year      = {2025},
  title     = {ChatGPT [Large language model]},
  url       = {https://chat.openai.com/chat},
  note      = {Accessed: 2025-02-15}
}
@misc{Anthropic2023,
  author    = {Anthropic},
  year      = {2023},
  title     = {Claude Sonnet 3.5 [Large language model]},
  url       = {https://claude.ai/},
  note      = {Accessed: 2025-02-15}
}
@misc{tensorflow2015whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}