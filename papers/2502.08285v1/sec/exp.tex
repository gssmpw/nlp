\section{Experiments}
\label{sec:exp}
We evaluate \ourmethod on typical point cloud registration benchmarks, i.e.~indoor 3DMatch \cite{zeng20173dmatch} and 3DLoMatch \cite{huang2021predator}, outdoor KITTI \cite{geiger2012we}, and cross-source 3DCSR \cite{huang2021comprehensive}.
Please refer to Appendix Section 1 for detailed implementation, including running details, network pipeline, and correspondence sampling.
For cluster numbers and the time computational analysis, and additional qualitative results please refer to the Sec.~2 in the Supplementary Material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation on 3DMatch and 3DLoMatch}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Datasets.} 
3DMatch \cite{zeng20173dmatch} and 3DLoMatch \cite{huang2021predator} are two widely used indoor benchmarks that contain more than $30\%$ and 10\% to 30\% partially overlapping scene pairs, respectively. 
3DMatch contains 62 scenes: we use 46 scenes for training, 8 scenes for validation, and 8 scenes for testing. 
The test set contains 1,623 partially overlapped point cloud fragments and their corresponding transformation matrices. 
We use training data preprocessed by \cite{huang2021predator} and evaluate on both the 3DMatch and 3DLoMatch \cite{huang2021predator} protocols. 
We first voxelize the point clouds with a $2.5cm$ voxel size and then extract different feature descriptors. 
We set $\tau_c=0.15$, $r=r_o=r_p=3.75cm$, and $r_n=10.0cm$ \cite{huang2021predator}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Metrics.} 
Following Predator \cite{huang2021predator} and CoFiNet \cite{yu2021cofinet}, we evaluate performance with three metrics: 
(i) \textit{Inlier Ratio} (IR), the fraction of putative correspondences whose residuals are below a certain threshold (i.e.~0.1m) under the ground-truth transformation, 
(ii) \textit{Feature Matching Recall} (FMR), the fraction of point cloud pairs whose inlier ratio is above a certain threshold (i.e.~5\%), and 
(iii) \textit{Registration Recall} (RR), the fraction of point cloud pairs whose transformation error is smaller than a certain threshold (i.e.~$RMSE<0.2m$). 
We compare \ourmethod with FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, SpinNet \cite{ao2021spinnet}, Predator \cite{huang2021predator}, YOHO \cite{wang2022you}, CoFiNet \cite{yu2021cofinet}, GeoTransformer \cite{qin2022geometric} short as GeoTrans, GLORN \cite{xu2022glorn}, and RoITr~\cite{yu2023rotation}.

\begin{table}[t]
\centering
	\caption{Results on both 3DMatch and 3DLoMatch datasets under different numbers of samples. Best performance in bold.}
 \resizebox{0.95\linewidth}{!}{%
	\setlength{\tabcolsep}{0.5mm}
	{
		\begin{tabular}{l | c c c c c | c c c c c}
			\toprule
			~ &  \multicolumn{5}{c|}{3DMatch} &  \multicolumn{5}{c}{3DLoMatch} \\
			\# Samples  & 5000 & 2500 & 1000 & 500 & 250 & 5000 & 2500 & 1000 & 500 & 250 \\
			\midrule
			Method & \multicolumn{10}{c}{Inlier Ratio $(\%) \uparrow$} \\
			\hline
			FCGF\cite{choy2019fully}  		& 56.8 & 54.1 & 48.7 & 42.5 & 34.1 & 21.4 & 20.0 & 17.2 & 14.8 & 11.6\\
			D3Feat\cite{bai2020d3feat}  	& 39.0 & 38.8 & 40.4 & 41.5 & 41.8 & 13.2 & 13.1 & 14.0 & 14.6 & 15.0\\
			SpinNet \cite{ao2021spinnet} 	& 47.5 & 44.7 & 39.4 & 33.9 & 27.6 & 20.5 & 19.0 & 16.3 & 13.8 & 11.1 \\
			Predator \cite{huang2021predator}  & 58.0 & 58.4 & 57.1 & 54.1 & 49.3 & 26.7 & 28.1 & 28.3 & 27.5 & 25.8\\
			CoFiNet\cite{yu2021cofinet}		& 49.8 & 51.2 & 51.9 & 52.2 & 52.2 & 24.4 & 25.9 & 26.7 & 26.8 & 26.9\\
			YOHO \cite{wang2022you} 	& 64.4 & 60.7 & 55.7 & 46.4 & 41.2 & 25.9 & 23.3 & 22.6 & 18.2 & 15.0\\
			GeoTrans\cite{qin2022geometric} & 71.9 & 75.2 & 76.0 & 82.2 & 85.1 
			& 43.5 & 45.3 & 46.2 & 52.9 & 57.7 \\
                GLORN \cite{xu2022glorn} & 72.6 & 73.5 & 76.4 & 82.3 & 83.2 &
                42.3 & 45.6 & 46.5 & 53.1 & \bf57.9 \\
                RoITr~\cite{yu2023rotation} & 82.6 & 82.8 & 83.0 & 83.0 & 83.0 & 54.3 & 54.6 & 55.1 & 55.2 & 55.3 \\
			\ourmethod(Ours)	& \bf83.1 & \bf83.6 & \bf84.2 & \bf84.2 & \bf84.1 & \bf56.1 & \bf56.4 & \bf57.3 & \bf57.3 & 57.4\\
			\hline
			& \multicolumn{10}{c}{Feature Matching Recall $(\%) \uparrow$} \\
			\hline
			FCGF\cite{choy2019fully} 		& 97.4 & 97.3 & 97.0 & 96.7 & 96.6 & 76.6 & 75.4 & 74.2 & 71.7 & 67.3 \\
			D3Feat \cite{bai2020d3feat}  	& 95.6 & 95.4 & 94.5 & 94.1 & 93.1 & 67.3 & 66.7 & 67.0 & 66.7 & 66.5 \\
			SpinNet \cite{ao2021spinnet} 	& 97.6 & 97.2 & 96.8 & 95.5 & 94.3 & 75.3 & 74.9 & 72.5 & 70.0 & 63.6 \\
			Predator\cite{huang2021predator} & 96.6 & 96.6 & 96.5 & 96.3 & 96.5 & 78.6 & 77.4 & 76.3 & 75.7 & 75.3\\
			CoFiNet\cite{yu2021cofinet}	& 98.1 & 98.3 & 98.1 & \bf98.2 & 98.3 & 83.1 & 83.5 & 83.3 & 83.1 & 82.6\\
			YOHO\cite{wang2022you} 		& 98.2 & 97.6 & 97.5 & 97.7 & 96.0 & 79.4 & 78.1 & 76.3 & 73.8 & 69.1\\
			GeoTrans\cite{qin2022geometric} & 97.9 & 97.9 & 97.9 & 97.9 & 97.6 & 88.3 & 88.6 & 88.8 & 88.6 & 88.3 \\
                GLORN \cite{xu2022glorn} & 97.8 & 97.9 & 98.0 & 98.1 & 97.2 & 87.8 & 88.8 & 88.9 & 88.7 & 88.5 \\
                RoITr~\cite{yu2023rotation} & 98.0 & 98.0 & 97.9 & 98.0 & 97.9 & \bf89.6 & \bf89.6 & 89.5 & 89.4 & 89.3 \\
			\ourmethod(Ours)  & \bf98.2 & \bf98.2 & \bf98.1 & \bf98.2 & \bf98.3 & 89.5 & \bf89.6 & \bf89.6 & \bf89.5 & \bf89.4 \\
			\hline
			& \multicolumn{10}{c}{Registration Recall $(\%) \uparrow$} \\
			\hline
			FCGF\cite{choy2019fully} 	& 85.1 & 84.7 & 83.3 & 81.6 & 71.4 & 40.1 & 41.7 & 38.2 & 35.4 & 26.8 \\
			D3Feat\cite{bai2020d3feat} 	& 81.6 & 84.5 & 83.4 & 82.4 & 77.9 & 37.2 & 42.7 & 46.9 & 43.8 & 39.1 \\
			SpinNet\cite{ao2021spinnet} & 88.6 & 86.6 & 85.5 & 83.5 & 70.2 & 59.8 & 54.9 & 48.3 & 39.8 & 26.8 \\
			Predator\cite{huang2021predator} 	& 89.0 & 89.9 & 90.6 & 88.5 & 86.6 & 59.8 & 61.2 & 62.4 & 60.8 & 58.1 \\
			CoFiNet\cite{yu2021cofinet}	  & 89.3 & 88.9 & 88.4 & 87.4 & 87.0 & 67.5 & 66.2 & 64.2 & 63.1 & 61.0 \\
			YOHO \cite{wang2022you} 	  & 90.8 & 90.3 & 89.1 & 88.6 & 84.5 & 65.2 & 65.5 & 63.2 & 56.5 & 48.0 \\
			GeoTrans\cite{qin2022geometric} & 92.0 & 91.8 & 91.8 & 91.4 & 91.2 & 75.0 & 74.8 & 74.2 & 74.1 & 73.5 \\
            GLORN \cite{xu2022glorn} & 92.2 & 91.6 & 91.9 & 91.5 & 91.0 & 74.9 & 75.2 & 74.4 & 74.3 & 73.6 \\
            RoITr~\cite{yu2023rotation} & 91.9 & 91.7 & 91.8 & 91.4 & 91.0 & 74.7 & 74.8 & 74.8 & 74.2 & 73.6 \\
			\ourmethod(Ours) & \bf92.4 & \bf92.1 & \bf92.2 & \bf91.8 & \bf91.5 & \bf78.6 & \bf78.7 & \bf78.1 & \bf76.4 & \bf75.2 \\
			\bottomrule
		\end{tabular}
	}}
	\label{table:3dm}
 \vspace{-0.4cm}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Inlier Ratio and Feature Matching Recall.} 
The primary contribution of our \ourmethod is its use of geometric cross-attention to emphasize matched point pair similarities and estimate more accurate correspondences. Therefore, we begin by examining the inlier ratio of the correspondences generated by \ourmethod, which directly reflects the quality of the extracted correspondences.
Following \cite{huang2021predator}, we present the results on varying sampled numbers of correspondences. 
Table~\ref{table:3dm} (top) shows that \ourmethod outperforms all previous methods in terms of Inlier Ratio on both benchmarks.
In particular, \ourmethod consistently outperforms RoITr, the second best baseline, by $0.5\%\sim 1.2\%$ on 3DMatch and $1.8\%\sim 2.1\%$ on 3DLoMatch, with sample numbers ranging from 250 to 5000. 
This notable increase in the Inlier Ratio suggests that incorporating the cross-geometric structure effectively enhances the reliability of correspondence production. Additionally, \ourmethod outshines all competitors in Feature Matching Recall, as detailed in Table~\ref{table:3dm} (middle section). Particularly in the challenging low-overlap scenarios of 3DLoMatch, our method demonstrates its robustness with improvements exceeding 0.4\%, underscoring its efficacy in complex situations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Registration Recall.} 
The Registration Recall (RR) reflects the final performance on point cloud registration. 
Table~\ref{table:3dm} (bottom) shows that our method outperforms all other models on both datasets in terms of RR. 
\ourmethod achieves RR of $92.4\%$ and $78.7\%$ on both 3DMatch and 3DLoMatch, surpassing the previous best performance achieved by GeoTransformer (which has a RR of $92.0\%$ on 3DMatch and $75.0\%$ on 3DLoMatch) by $0.4\%$ and $3.7\%$, respectively. 
This shows that incorporating geometrical information into the correspondence prediction process can alleviate ambiguity and lead to superior performance compared to methods that only consider feature similarity in cross-attention.
Figs.~\ref{fig:3dvs} and \ref{fig:3dvslo} show comparison examples on 3DMatch and 3DLoMatch, respectively. 
\ourmethod achieves better results in challenging indoor scenes with a low overlap ratio.


\begin{figure}[t]
\centering 
\begin{overpic}[width=0.85\columnwidth]{figures/match}
    \put(-3.7,65.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.7,42.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GeoTrans}}}
    \put(-3.7,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \put(-3.7,7.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GT}}}
\end{overpic}
\caption{Examples of qualitative registration results on the 3DMatch dataset. 
Inaccurate regions are enclosed in red boxes.}
	\label{fig:3dvs}
\end{figure}

\begin{figure}[t]
	\centering 
\begin{overpic}[width=0.85\columnwidth]{figures/lomatch}
    \put(-3.7,65.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.7,45.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GeoTrans}}}
    \put(-3.7,25.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \put(-3.7,6.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GT}}}
\end{overpic}
\caption{Examples of qualitative registration results on the 3DLoMatch dataset.
Inaccurate regions are enclosed in red boxes.}
\label{fig:3dvslo}
 \vspace{-0.4cm}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation on KITTI}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Dataset.} 
KITTI consists of 11 sequences of LiDAR-scanned outdoor driving scenarios. To ensure fairness in comparisons, we adopt the same data-splitting approach as \cite{choy2019fully, choy2020deep}, with sequences 0-5 utilized for training, 6-7 for validation, and 8-10 for testing purposes. 
We refine the provided ground-truth poses by employing ICP and limit the evaluation to point cloud pairs within a distance of $10m$ from each other, as in \cite{choy2020deep}. 
Furthermore, following in \cite{huang2021predator}, we downsample the point clouds using a voxel size $30cm$ and set $\tau_c=0.15$, $r=r_o$=$45cm$, $r_p$=$21cm$, and $r_n$=$75cm$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Metrics.} 
Following Predator~\cite{huang2021predator} and CoFiNet~\cite{yu2021cofinet}, our \ourmethod is evaluated using three metrics: Registration Recall (RR), Relative Rotation Error (RRE), and Relative Translation Error (RTE). RR calculates the percentage of successful alignments where the rotation and translation errors are below specified thresholds (i.e., RRE $<5^\circ$ and RTE $< 2m$). The definitions of RRE and RTE are RRE$=\arccos\frac{\textbf{Tr}\left(\bm{R}^\top\bm{R}^\star\right)-1}{2}$ and RTE$=|\bm{t}-\bm{t}^\star|_2$, respectively. The ground-truth rotation matrix and the translation vector are denoted by $\bm{R}^\star$ and $\bm{t}^\star$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Registration Results.} 
We compare \ourmethod against FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, SpinNet \cite{ao2021spinnet}, Predator \cite{huang2021predator}, CoFiNet \cite{yu2021cofinet}, and GeoTransformer \cite{qin2022geometric}. 
Table~\ref{table:kitti} shows that our method achieves the best performance in terms of RR and the lowest average RTE and RRE.
These results confirm the effectiveness of \ourmethod also in an outdoor scenario.
It also suggests that incorporating cross-attention with geometry enhancement could be beneficial in acquiring more distinct features, resulting in better performance in registration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \renewcommand{\arraystretch}{0.9}
% \begin{table}[t]
% \centering
% \caption{Registration results on the 3DCSR dataset. The best performance is highlighted in bold.}
% \resizebox{0.5\linewidth}{!}{%
% \begin{tabular}{l | c c c}
%     \toprule
%     Method & RTE (cm) $\downarrow$ & RRE $(^\circ)\downarrow$ & RR(\%) $\uparrow$ \\
%     \midrule
%     FCGF \cite{choy2019fully}	 & \bf0.21 & 7.47 & 49.6  \\
%     D3Feat \cite{bai2020d3feat} 	 & 0.26 & 6.41 & 52.0 \\
%     SpinNet \cite{ao2021spinnet} & 0.24 & 6.56 & 53.5\\
%     Predator \cite{huang2021predator}  & 0.27 & 6.26 & 54.6 \\	
%     CoFiNet \cite{yu2021cofinet}	   & 0.26 & 5.76 & 57.3 \\
%     GeoTrans \cite{qin2022geometric} & 0.24 & 5.60 & 60.2 \\
%     \ourmethod (ours)				& 0.22 & \bf5.44 & \bf62.9 \\
%     \bottomrule
% \end{tabular}
% }
% \label{table:3DCSR}
% \vspace{-0.4cm}
% \end{table}
% \renewcommand{\arraystretch}{1}

\renewcommand{\arraystretch}{0.85}
\begin{table}[t]
	\centering
    \small
	\caption{Results on KITTI dataset. Best performance in bold.}
    \resizebox{0.8\linewidth}{!}{%
	\begin{tabular}{l | c | c c c}
		\toprule
		Method & RTE (cm) $\downarrow$ & RRE $(^\circ)\downarrow$ & RR(\%) $\uparrow$ \\
		\midrule
		FCGF \cite{choy2019fully} 	& 9.5 & 0.30 & 96.6 \\
		D3Feat \cite{bai2020d3feat} 	 & 7.2 & 0.30 & \bf99.8 \\
		SpinNet \cite{ao2021spinnet} & 9.9 & 0.47 & 99.1 \\
		Predator \cite{huang2021predator}  & 6.8 & 0.27 & \bf99.8 \\	
		CoFiNet \cite{yu2021cofinet}	   & 8.5 & 0.41 & \bf99.8 \\
		GeoTrans \cite{qin2022geometric} & 7.4 & 0.27 & \bf99.8 \\
		\ourmethod (ours)					& \bf6.9 & \bf0.24 & \bf99.8 \\
		\bottomrule
	\end{tabular}
 }
\label{table:kitti}
\vspace{-3mm}
\end{table}
\renewcommand{\arraystretch}{1} % 恢复默认值

\renewcommand{\arraystretch}{0.85} % 进一步减少行间距
\begin{table}[t]
\centering
\small
\caption{Registration results on the 3DCSR dataset. Best performance in bold.}
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{l | c c c}
    \toprule
    Method & RTE (cm) $\downarrow$ & RRE $(^\circ)\downarrow$ & RR(\%) $\uparrow$ \\
    \midrule
    FCGF \cite{choy2019fully}	 & \bf0.21 & 7.47 & 49.6  \\
    D3Feat \cite{bai2020d3feat} 	 & 0.26 & 6.41 & 52.0 \\
    SpinNet \cite{ao2021spinnet} & 0.24 & 6.56 & 53.5\\
    Predator \cite{huang2021predator}  & 0.27 & 6.26 & 54.6 \\	
    CoFiNet \cite{yu2021cofinet}	   & 0.26 & 5.76 & 57.3 \\
    GeoTrans \cite{qin2022geometric} & 0.24 & 5.60 & 60.2 \\
    \ourmethod (ours)				& 0.22 & \bf5.44 & \bf62.9 \\
    \bottomrule
\end{tabular}
}
\label{table:3DCSR}
\vspace{-5mm}
\end{table}
\renewcommand{\arraystretch}{1} % 恢复默认值



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization on 3D Cross-Source Dataset}

\noindent\textbf{Dataset.}
3DCSR~\cite{huang2021comprehensive} contains two sets: Kinect Lidar and Kinect SFM. 
Kinect Lidar comprises 19 scenes captured from both Kinect and Lidar sensors. 
Each scene is divided into different parts. 
Kinect SFM comprises 2 scenes captured from both Kinect and RGB-D sensors. 
 RGB-D images are transformed into a point cloud by employing the VSFM software.
The model trained on 3DMatch is used since the cross-source dataset is captured in an indoor setting. 
The metric used for successful alignment is RR, which represents the percentage of aligned scenes with RRE less than $15^\circ$ and RTE less than $6m$.
3DCSR is a challenging dataset for registration due to a mixture of noise, outliers, density differences, and partial overlaps.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent\textbf{Registration Results.} 
We use FCGF \cite{choy2019fully}, D3Feat \cite{bai2020d3feat}, SpinNet \cite{ao2021spinnet}, Predator \cite{huang2021predator}, CoFiNet \cite{yu2021cofinet}, and GeoTransformer \cite{qin2022geometric} as comparison methods. 
Table~\ref{table:3DCSR} shows that our method also achieves the highest accuracy in this experimental configuration, i.e.~generalization ability. 
Notably, it surpasses GeoTransformer, the second-best, by more than 2.7\% in terms of registration recall (62.9\% vs 60.2\%).
Our ability to achieve better results is attributed to the cross-attention module enhanced by geometry.
However, the recall rate falls short, indicating that the registration challenges on 3DCSR persist.
Fig.~\ref{fig:csdvs} shows examples of qualitative results on 3DCSR.
%++++++++++++++++++++++++++++++++++++++++++++++++++

\begin{figure}[t]
\centering 
\begin{overpic}[width=0.8\columnwidth]{figures/csdvs}
    \put(-3.7,70.0){\color{black}\footnotesize\rotatebox{90}{\textbf{Input}}}
    \put(-3.7,42.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GeoTrans}}}
    \put(-3.7,25.4){\color{black}\footnotesize\rotatebox{90}{\textbf{Ours}}}
    \put(-3.7,7.0){\color{black}\footnotesize\rotatebox{90}{\textbf{GT}}}
\end{overpic}
\caption{Examples of qualitative registration results on the 3DCSR dataset.}
\label{fig:csdvs}
\end{figure}





\begin{table}[t]
    \renewcommand{\arraystretch}{0.8} % 设置表格行高
	\centering
	\caption{Ablation study of model design.} 
 \resizebox{0.85\linewidth}{!}{%
	\begin{tabular}{c | c c c | c c c}
	\toprule
	& \multicolumn{3}{c}{3DMatch} & \multicolumn{3}{c}{3DLoMatch} \\
	\hline
        Model & RR & FMR & IR & RR & FMR & IR \\
        \midrule
        \multicolumn{7}{c}{Cross-attention} \\
	\midrule
	vanilla cross-attention & 92.0 & 98.0 & 73.4 & 74.3 & 88.6 & 43.8 \\
	cross-attention w/PDE   & 92.1 & 98.1 & 81.4 & 75.6 & 88.7 & 54.4 \\ 
	cross-attention w/TAE   & 92.1 & 98.0 & 81.3 & 74.9 & 88.6 & 54.1\\
        cross-attention w/DAE   & \bf92.2 & \bf98.1 & \bf84.2 & \bf78.1 & \bf89.6 & \bf57.3 \\
            \midrule
        \multicolumn{7}{c}{Self-attention} \\
        \midrule
        vanilla self-attention & 92.1 & 98.0 & 83.9 & 75.1 & 88.8 & 56.4 \\
	self-attention w/PDE   & \bf92.2 & \bf98.1 & \bf84.2 & \bf78.1 & \bf89.6 & \bf57.3 \\ 
	\bottomrule
	\end{tabular}
 }
	\label{tb:abcross}
\vspace{-5mm}
\end{table}
% \renewcommand{\arraystretch}{1}
%++++++++++++++++++++++++++++++++++++++++++++++++++


\vspace{-1mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}
We conducted an ablation analysis on 3DMatch and 3DLoMatch with \#Samples=1000 to examine the specific roles of each element in our approach. 
To assess the efficacy of geometry information in cross-attention, we compared the registration outcomes of four types of cross-attention - vanilla, pair-wise distance embedding-based (PDE), triplet-wise angle embedding-based (TAE), and the whole geometric cross-attention (DAE) in Table~\ref{tb:abcross}. 
Both pair-wise distance and triplet-wise angle embedding can improve the registration performance. 
To be more detailed, Geometric information improves the performance 
by nearly 0.2\% (92.0\% vs. 92.2\%) RR, 0.1\% (98.0\% vs. 98.1\%) FMR, and 2.8\% (81.4\% vs. 84.2\%) IR on 3DMatch indicating that \ourmethod benefits from pair-wise distance and triplet-wise angle embedding. 
Additionally, the study presents results for employing self-attention mechanisms combined with distance mapping, a technique that further enhanced performance.
Fig.~\ref{fig:attn} presents two examples of attention maps. 
It includes only two source point clouds and omits target point clouds for clarity. 
Given a point from the source, the red boxes highlight the correct correspondence regions. Our network focuses on corresponding superpoints in the source point cloud for each point in the target cloud.
We emphasize that attention is selectively focused on areas within the red rectangles, ensuring clarity on this key aspect. Additionally, the figure contrasts attention maps from two example configurations, showcasing our method’s superior ability to identify overlapping regions compared to GeoTr. 
In these examples, a selected point in the target point cloud is matched with corresponding superpoints in the source point cloud. 
Blue points indicate low attention weights, while points in other colors signify higher attention weights. 
Notably, our approach focuses attention on the red rectangles. 

% ********************************
\begin{figure}[t]
    \centering
    \vspace{-3mm}
    \begin{overpic}[width=0.63\linewidth, angle=90]{figures/attn-cropped}
    \put(15,100){\color{black}\footnotesize\textbf{GeoTrans}}
    \put(65,100)
    {\color{black}\footnotesize\textbf{\ourmethod}}
    \end{overpic}
    \caption{Visualization of attention weights for the point indicated with a red dot. Brighter colors indicate higher attention.}
    \label{fig:attn}
    \vspace{-3mm}
\end{figure}