Point cloud registration approaches often fail when the overlap between point clouds is low due to noisy point correspondences. 
This work introduces a novel cross-attention mechanism tailored for Transformer-based architectures that tackles this problem, by fusing information from coordinates and features at the super-point level between point clouds.
This formulation has remained unexplored primarily because it must guarantee rotation and translation invariance since point clouds reside in different and independent reference frames.
We integrate the Gromovâ€“Wasserstein distance into the cross-attention formulation to jointly compute distances between points across different point clouds and account for their geometric structure.
By doing so, points from two distinct point clouds can attend to each other under arbitrary rigid transformations.
At the point level, we also devise a self-attention mechanism that aggregates the local geometric structure information into point features for fine matching. 
Our formulation boosts the number of inlier correspondences, thereby yielding more precise registration results compared to state-of-the-art approaches. 
We have conducted an extensive evaluation on 3DMatch, 3DLoMatch, KITTI, and 3DCSR datasets. 
Project page: \url{https://github.com/twowwj/FLAT}.
% {\color{red}\textbf{add link to project page}}.
