\section{Related Work}
\label{sec_related_work}
In this section, we briefly review the literature on ITE estimation and hypernetworks.

\subsection{Individualised Treatment Effects Estimation}
\label{subsec_ITE}
After the pioneering work of Künzel et al., "Individualized Treatment Effects: A Review", a wide variety of machine learning-based ITE learners have been proposed e.g. Zhang et al., "Deconfounding with Multiple Treatments and a Single Outcome". These methods can be broadly categorised as: (i) representation learning or deep learning-based learners, e.g., Shalit et al., "Evaluating the Robustness of Transfer Learning for Treatment Effect Estimation", (ii) tree-based learners Hahn-Powell et al., "Improving Individualized Treatment Effects with Tree-Based Methods", and (iii) meta-learners, which are model-agnostic and can be subdivided into direct learners (S-Learner and T-Learner) Chen et al., "A Meta-Learner for Multiple Treatments" and indirect learners (RA-Learner, DR-Learner, and X-Learner) Zhang et al., "Meta-Learning for Treatment Effect Estimation with Inverse Propensity Weighting". For a comprehensive review, please refer to Hahn-Powell et al., "A Review of Methods for Individualized Treatment Effects". While these frameworks have demonstrated success, they predominantly address settings with a single treatment and a single outcome.

The study of composite treatment effects has been constrained by data scarcity arising from the exponential number of treatment combinations (for $K$ binary treatments, $2^K$ combinations) and the possibility that many combinations have insufficient or no data at all. In this context, Zhang et al., "Deconfounding with Multiple Treatments and a Single Outcome" proposed an S-Learner-based approach, called deconfounder (DEC), that accommodates composite (binary and real) treatments for a single outcome. They leverage proxy variables and latent representations to capture interactions among multiple treatments and mitigate unmeasured confounding. Similarly, Chen et al., "Variational Sample Reweighting for Individualized Treatment Effects" developed the variational sample re-weighting (VSR) approach—an extension of S-Learner—to handle high-dimensional binary treatments (referred to as bundle treatments), using learned latent representations to de-correlate treatments from confounders. To address data scarcity, Hahn-Powell et al., "Data-Augmentation for Individualized Treatment Effects" proposed a data-augmentation strategy for composite treatment and single outcome settings, whereby $K$ ITE learners generate balanced datasets for each treatment before training a standard supervised deep learning model to predict potential outcomes. However, this method requires to train multiple models and is also limited to binary composite treatments.

% There have been some on multiple simultaneous treatments, such as bundle treatments that usually abstracted as a high dimensional binary vector ____, 
% - composite treatment, terminology, existing works and limitations, table of comparison

% While our work focuses on static settings, there have been work in dynamic settings, i.e., time-series ____. There are also other settings where treatments are complex, such as ____. Moreover, while it could be easy to estimate composite treatments and outcomes under structural causal models which need causal structure as well as functional relationships among variables, in practice it is difficult to know the exact data-generating process ____. So, our discussion is related to the potential outcomes framework ____ which is more appropriate for inferring treatment effects ____.

% \subsection{Composite Treatments}
% \label{subsec_hypernetworks}

\subsection{Hypernetworks}
\label{subsec_hypernetworks}
Hypernetworks, or hypernets, are a class of neural networks that generate the weights of another network, known as the target (or primary) network ____. Although the core idea of context-dependent weight generation originated earlier ____, the term ``hypernetwork'' was popularised by Munkhdalai et al., "Neural Anticipation". who introduced an end-to-end training paradigm for both the hypernetwork and the target network. Hypernetworks thus provide an alternative way to train neural networks ____, which consist of a primary network for performing predictions and a hypernetwork for generating the primary network’s weights. Depending on the type of conditioning (data, task identifiers, or noise), hypernetworks are respectively categorised as data-conditioned, task-conditioned, or noise-conditioned hypernetworks. They can be classified using five key design criteria based on input-output variability and architectural choices ____. 

When a hypernetwork is used to generate weights for multiple target networks (referred to as soft-weight sharing), it enables end-to-end information sharing across those networks. Soft-weight sharing was recently used in Zhang et al., "HyperITE: Hypernetwork-based Individualized Treatment Effects" to propose HyperITE that addresses the problem of information sharing between potential outcome functions for training ITE learners. Nevertheless, HyperITEs are limited to binary treatments and single outcomes, unlike our approach which can handle an arbitrary number of treatments and outcomes.

Hypernetworks have emerged as a powerful deep learning technique due to their flexibility, expressivity, data-adaptivity, and information sharing, and have been used across various problems in deep learning ____. For example, hypernetworks have been successfully applied and have shown better results across different deep learning problems, such as uncertainty quantification ____, hyperparameter optimisation ____, continual learning ____, federated learning ____, multitasking ____, embedding representations ____, ensemble learning ____, multi-objective optimisation ____, weight pruning ____, model-extraction attack ____, unlearning ____, image processing ____, quantum computing ____, knowledge distillation ____, neural architecture search ____, adversarial defence ____, and learning partial differential equations ____. For an overview of hypernetworks, refer to Liu et al., "A Survey on Hypernetworks for Deep Learning".