\section{Related Work}
\label{sec_related_work}
In this section, we briefly review the literature on ITE estimation and hypernetworks.

\subsection{Individualised Treatment Effects Estimation}
\label{subsec_ITE}
After the pioneering work of \cite{johansson2016learning}, a wide variety of machine learning-based ITE learners have been proposed \cite{bica2021real}. These methods can be broadly categorised as: (i) representation learning or deep learning-based learners, e.g., \cite{curth2021inductive,shalit2017estimating,hassanpour2019counterfactual,hassanpour2019learning,chauhan2023adversarial,chauhan2024dynamic}, (ii) tree-based learners \cite{athey2019estimating}, and (iii) meta-learners, which are model-agnostic and can be subdivided into direct learners (S-Learner and T-Learner) \cite{kunzel2019metalearners}, and indirect learners (RA-Learner, DR-Learner, and X-Learner) \cite{kennedy2023towards,curth2021nonparametric,kunzel2019metalearners}. For a comprehensive review, please refer to \cite{Curth2024Machine}. While these frameworks have demonstrated success, they predominantly address settings with a single treatment and a single outcome.

The study of composite treatment effects has been constrained by data scarcity arising from the exponential number of treatment combinations (for $K$ binary treatments, $2^K$ combinations) and the possibility that many combinations have insufficient or no data at all. In this context, \cite{wang2019blessings} proposed an S-Learner-based approach, called deconfounder (DEC), that accommodates composite (binary and real) treatments for a single outcome. They leverage proxy variables and latent representations to capture interactions among multiple treatments and mitigate unmeasured confounding. Similarly, \cite{zou2020counterfactual} developed the variational sample re-weighting (VSR) approach—an extension of S-Learner—to handle high-dimensional binary treatments (referred to as bundle treatments), using learned latent representations to de-correlate treatments from confounders. To address data scarcity, \cite{qian2021estimating} proposed a data-augmentation strategy for composite treatment and single outcome settings, whereby $K$ ITE learners generate balanced datasets for each treatment before training a standard supervised deep learning model to predict potential outcomes. However, this method requires to train multiple models and is also limited to binary composite treatments.

% There have been some on multiple simultaneous treatments, such as bundle treatments that usually abstracted as a high dimensional binary vector \cite{zou2020counterfactual}, 
% - composite treatment, terminology, existing works and limitations, table of comparison

% While our work focuses on static settings, there have been work in dynamic settings, i.e., time-series \cite{}. There are also other settings where treatments are complex, such as \cite{}. Moreover, while it could be easy to estimate composite treatments and outcomes under structural causal models which need causal structure as well as functional relationships among variables, in practice it is difficult to know the exact data-generating process \cite{}. So, our discussion is related to the potential outcomes framework \cite{} which is more appropriate for inferring treatment effects \cite{}.

% \subsection{Composite Treatments}
% \label{subsec_hypernetworks}

\subsection{Hypernetworks}
\label{subsec_hypernetworks}
Hypernetworks, or hypernets, are a class of neural networks that generate the weights of another network, known as the target (or primary) network \cite{chauhan2024brief}. Although the core idea of context-dependent weight generation originated earlier \cite{schmidhuber1992learning}, the term ``hypernetwork'' was popularised by \cite{ha2017hypernetworks}, who introduced an end-to-end training paradigm for both the hypernetwork and the target network. Hypernetworks thus provide an alternative way to train neural networks \cite{chauhan2024brief}, which consist of a primary network for performing predictions and a hypernetwork for generating the primary network’s weights. Depending on the type of conditioning (data, task identifiers, or noise), hypernetworks are respectively categorised as data-conditioned, task-conditioned, or noise-conditioned hypernetworks. They can be classified using five key design criteria based on input-output variability and architectural choices \cite{chauhan2024brief}.

When a hypernetwork is used to generate weights for multiple target networks (referred to as soft-weight sharing), it enables end-to-end information sharing across those networks. Soft-weight sharing was recently used in \cite{chauhan2024dynamic} to propose HyperITE that addresses the problem of information sharing between potential outcome functions for training ITE learners. Nevertheless, HyperITEs are limited to binary treatments and single outcomes, unlike our approach which can handle an arbitrary number of treatments and outcomes.

Hypernetworks have emerged as a powerful deep learning technique due to their flexibility, expressivity, data-adaptivity, and information sharing, and have been used across various problems in deep learning \cite{chauhan2024brief}. For example, hypernetworks have been successfully applied and have shown better results across different deep learning problems, such as uncertainty quantification \cite{deutsch2019generative,Chan2024}, hyperparameter optimisation \cite{lorraine2018stochastic}, continual learning \cite{Oswald2020Continual}, federated learning \cite{shin2024effective}, multitasking \cite{tay2021hypergrid}, embedding representations \cite{yoo2024hyper}, ensemble learning \cite{kristiadi2019predictive}, multi-objective optimisation \cite{Tuan2024}, weight pruning \cite{liu2019metapruning}, model-extraction attack \cite{yuan2024hypertheft}, unlearning \cite{rangel2024learning}, image processing \cite{Ramanarayanan_2023_ICCV,Zhou2024}, quantum computing \cite{carrasquilla2023quantum}, knowledge distillation \cite{wu2023hyperinr}, neural architecture search \cite{peng2020cream}, adversarial defence \cite{sun2017hypernetworks}, and learning partial differential equations \cite{botteghi2025hyperl}. For an overview of hypernetworks, refer to \cite{chauhan2024brief}.

% To sum up,