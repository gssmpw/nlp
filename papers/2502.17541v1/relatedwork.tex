\section{Related Works}
\subsection{Unsupervised Feature Extraction}

Traditional approaches to interpretable data analysis have primarily relied on unsupervised techniques such as clustering, which are typically paired with natural language descriptions generated either through phrase extraction \citep{carmel2009, treeratpituk2006automatically, zhang2018taxogen} or LLM-based methods \citep{sorensen2024clio, lam2024concept, singh2023explaining}. However, these methods face fundamental limitations due to their sensitivity to the number of selected clusters and their inability to accurately approximate complex cluster contents \citep{chang2009reading}.

More recently, LLM-based methods have re-imagined feature discovery as a search over natural language hypotheses \citep{qiu2023phenomenal}, employing diverse strategies including de-duplication \citep{pham2023topicgpt}, optimization for minimal cluster overlap \citep{wang2023goal, zhong2024explaining}, and human-guided feature selection \citep{viswanathan2023large}. While these advances have improved clustering-based approaches, they remain constrained by hyperparameter dependence and rigid cluster assignments. Our method overcomes these limitations through controlled feature sampling that enables simultaneous modeling of both broad patterns and fine-grained properties, without constraining the number of features that can be assigned to each text.

\subsection{Supervised Feature Extraction}

Supervised approaches to feature discovery have emerged as an alternative to unsupervised methods. \citet{zhong2022describing} formulates this as a distribution comparison problem to identify distinguishing characteristics, an approach later extended beyond text \cite{dunlap2024describing} and adapted to accommodate user-specified exploration goals \cite{zhong2023goal}. Different forms of supervision have also been explored: \citet{findeis2024inverse} leverages correlation analysis to identify features aligned with human preferences, while \citet{benara2024crafting} employs ridge regression for feature selection in medical prediction tasks.

A parallel line of work explores Concept Bottleneck Models, which achieve interpretability by learning models over interpretable intermediate features \cite{koh2020concept}. Recent advances have focused on automating the discovery of these interpretable features \cite{yang2023language, ludan2023interpretable, schrodi2024concept}. However, these approaches require either labeled data or reference distributions, which our method does not. Furthermore, while these methods optimize for accuracy through bottleneck representations, they may not capture the semantic richness that our natural language featurization pipeline provides.

\subsection{Dataset-Level Feature Extraction}

At the dataset level, current approaches typically extract features by prompting LLMs with data samples to generate dataset descriptions \citep{singh2024rethinking}. While these descriptions can be refined through self-improvement loops \citep{pan2023automatically, gero2023self}, expert feedback \citep{templeton2024scaling}, or reconstruction-based optimization \citep{singh2024iprompt}, they remain limited to dataset-level insights without capturing properties of individual samples. Although prior work has explored more structured representations like binary tree decomposition with natural language splits \citep{singh2023tree}, our method uniquely generates semantically grounded features that enable both granular analysis and systematic comparison across the entire dataset.