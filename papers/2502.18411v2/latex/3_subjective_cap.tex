\begin{comment}
重申一下，本章节性质是 Preliminary Study，内容是发现问题，然后尝试最 naive 的解决思路不 work。写作顺序按下面的写：
1. 使用 MLLM 过程中，发现 MLLM 的主观性能相比 LLM 差距较大 （定性）
2. 在 LLM 的 Human-Preference Benchmark 上测试 MLLM 进行验证，发现确实很差（定量）
3. (optional) 讨论 MLLM 主观性能下降可能的原因：刷点、数据质量差、etc. 
4. 尝试最 naive 的 solution (与 LM SFT 数据混训)，发现不 work：LLM Human-Preference Benchmark 上有大提升，但多模场景下：1. 综合能力掉点；2. Human-Preference Benchmark 上未有提升。注意，这里不需要 "ensure a fair comparison“，因为你要证明的不是说 magpie / condor 比 sharegpt4 好 (这是显然的)。你可以调整掺语言数据的量，然后拉条曲线 (线上 2、3 个点就可以)
5. 基于 4 中的发现，说我们需要构建多模的主观数据


\end{comment}
\section{Human Alignment of MLLMs: The Preliminary Study}
\label{sec: preliminary}
When handling open-ended and flexible questions involving images,
state-of-the-art open-source MLLMs -- 
despite excelling in certain recognition tasks -- 
exhibit significantly weaker alignment with human preferences compared to GPT-4o (\cref{fig:internvl} in appendix).
We hypothesize that this decline in multi-modal preference alignment stems from a reduction in the language model's proficiency after the multi-modal SFT stage. 
To test this hypothesis, we evaluated state-of-the-art MLLMs on text-only human preference alignment benchmarks~\cite{liu2023alignbench,alpaca_eval,li2024crowdsourced}.
Additionally, we constructed a LLaVA~\cite{liu2023visual} baseline using InternLM-2.5-7B, fine-tuned on the LLaVA-Next-778k~\cite{liu2023improved} SFT dataset. As shown in \cref{tab: subjective language}, 
the LLM’s ability to handle text-only open-ended questions degrades significantly after multi-modal SFT. 
This degradation may be due to (1) insufficient quantity or quality of text-only samples during multi-modal SFT, or (2) the overly simplistic style of multi-modal fine-tuning data derived from traditional VQA datasets~\cite{vqav2,hudson2019gqa}, often consisting of simple questions and short, factual answers.

% We noted that when addressing more open-ended and flexible questions involving images, the current state-of-the-art (SOTA) open-source MLLMs, which claim superior performance in certain classification tasks such as OCR and visual perception, exhibit a markedly weaker alignment with human preferences compared to GPT-4o.
% prior arts hardly examine aligned LLMs’ fulfillment of user intention and human
% preference in real-world conditions(\cref{fig:internvl}),
% \textit{WHY?} Initially, we hypothesize that the observed decline in Multi-modal subjective alignment is fundamentally attributable to a reduction in the language model's proficiency following the Multi-Modal SFT training stage.
% To substantiate this hypothesis, we conducted a series of tests evaluating the performance of the SOTA MLLMs on benchmarks concerning subjective language capability, including AlignBench~\cite{liu2023alignbench}, Alpaca-Eval-V2~\cite{alpaca_eval}, and ArenaHard-V2~\cite{li2024crowdsourced}.
% Besides, we further developed a model based on LLaVA, utilizing the InternLM-2.5-7B as the language model, and trained it on the LLaVA-Next-778k SFT dataset.
% We employed three subjective language benchmarks: AlignBench, Alpaca-Eval-V2, and ArenaHard-V2. These benchmarks are specifically designed to assess a model's capacity for generating nuanced and contextually appropriate responses, thereby providing a rigorous test of its subjective language capabilities. 
% The results are shown in Tab.~\ref{tab: subjective language}. After the Multi-modal SFT training stage, the capability of LLM to handle alignment rapidly diminishes.
% The limitation can be explained in two primary ways:

% 1. The proportion of text-only training data is of inferior quality. 
% This may result in reduced model proficiency, particularly in tasks that require robust understanding and generation of textual content.

% 2. The constrained format of question-and-answer pairs in the volume of training data. Historically, training datasets for MLLMs have frequently been derived from straightforward VQA datasets, which typically feature relatively simple questions paired with answers that are often limited to single words or short phrases. 
% As a result, after undergoing fine-tuning, MLLMs may exhibit a reduced capacity to generate responses in the form of extended, coherent sentences, as demonstrated in Fig.~\ref{}.

\begin{table}
    \centering
    \resizebox{.5\textwidth}{!}{%
    \begin{tabular}{ll|ccc}
    \Xhline{0.15em}
        \textbf{Method} & \textbf{Type} & \textbf{AlignBench} & \textbf{AlpacaEval-V2} & \textbf{ArenaHard}  \\
        % \midrule
        % InternLM2.5-7B & LLM &6.36& 27.58& 27.06 \\
        % InternVL2-8B & MLLM &4.09& 3.98& 8.08 \\
        % LLaVA-Internlm & MLLM &4.5& 5.22& 8.05 \\
        \hline
        InternLM2.5-7B & LLM  &6.36& 27.58& 27.06 \\
        InternVL2-8B   & MLLM &4.04(-36.4\%)& 3.35(-87.9\%)& 4.65(-82.8\%) \\
        LLaVA-Internlm & MLLM &4.66(-26.7\%)& 4.22(-84.7\%)& 4.93(-81.8\%) \\
        \hline
        Qwen2-7B     & LLM &6.02& 24.47& 32.84 \\
        Qwen2VL-7B & MLLM &4.92(-18.3\%)& 3.85(-83.4\%)& 6.46(-80.3\%) \\
        \hline
        LLaMA3-8B     & LLM &4.88& 30.19& 31.96 \\
        MiniCPM-V2.5 & MLLM &3.84(-21.3\%)& 7.33(-75.7\%)& 8.05(-74.8\%) \\
        \hline
        InternLM2-20B & LLM  &5.49& 43.35& 33.75 \\
        InternVL2-26B & MLLM &4.39(-20.0\%)& 5.34(-87.7\%)& 11.25(-66.7\%) \\
        \hline
        Hermes2-llama3-70b & LLM &5.72 &46.09 &57.40 \\
        InternVL2-76B     & MLLM &4.33(-24.3\%) &8.32(-81.9\%) &16.17(-72.3\%) \\
        % \midrule
        % InternLM2-20B & LLM &6.06& 42.24& 42.15 \\
        % InternVL2-26B & MLLM &4.49& 5.59& 15.96 \\
        % \midrule
        % Hermes2-llama3-70b & LLM &6.34 &38.63 &62.85 \\
        % InternVL2-76B & MLLM &4.39 &7.83 &20.57 \\
    \Xhline{0.15em}
    \end{tabular}
    }%
    \caption{\textbf{Language Alignment Benchmark Results.} After multi-modal SFT, MLLMs demonstrate significant decline in human preference alignment compared to their corresponding LLMs.} 
    \label{tab: subjective language}
    \vspace{-10pt}
% \end{table*}
\end{table}

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \tablestyle{4pt}{1}
    % \sty
    \begin{tabular}{l|ccc|ccccccc}
    \Xhline{0.15em}
        & \multicolumn{3}{c|}{\textit{\textbf{Language Benchmarks}}} & \multicolumn{5}{c}{\textit{\textbf{Multi-modal Benchmarks}}}  \\
        \textbf{Model}  & \textbf{AlignBench} & \textbf{AlpacaEval-V2} & \textbf{ArenaHard} &\textbf{WildVision} &\textbf{MMVet} &\textbf{MMBench-V1.1}  &\textbf{AI2D} &\textbf{OCRBench} \\
        \hline
        % InternLM2.5-7B &6.36& 27.58& 27.06 & & & & & & & \\
        LLaVA$^I$-LLaVANext778k         &4.7& 31.7& 21.6 & 18.4/-55.1& 41.2 &73.7 &74.2 &39.7\\
        LLaVA$^I$-LLaVANext$_{mm}$738k-Magpie40k  &4.5 \color{red}{$\boldsymbol{\downarrow}$}& 65.5 \color{SeaGreen}{$\boldsymbol{\uparrow}$}& 44.6 \color{SeaGreen}{$\boldsymbol{\uparrow}$} & 16.8/-58.9 \color{red}{$\boldsymbol{\downarrow}$}& 37.7 \color{red}{$\boldsymbol{\downarrow}$} &73.1 \color{red}{$\boldsymbol{\downarrow}$} &73.4 \color{red}{$\boldsymbol{\downarrow}$} &38.7 \color{red}{$\boldsymbol{\downarrow}$}\\
        LLaVA$^I$-LLaVANext$_{mm}$738k-Condor40k  &5.6 \color{SeaGreen}{$\boldsymbol{\uparrow}$}& 72.9 \color{SeaGreen}{$\boldsymbol{\uparrow}$}& 55.7 \color{SeaGreen}{$\boldsymbol{\uparrow}$} & 16.8/-57.4 \color{red}{$\boldsymbol{\downarrow}$}& 38.3 \color{red}{$\boldsymbol{\downarrow}$} &72.6 \color{red}{$\boldsymbol{\downarrow}$}  &73.6 \color{red}{$\boldsymbol{\downarrow}$} &38.5 \color{red}{$\boldsymbol{\downarrow}$}\\
        LLaVA$^I$-LLaVANext$_{mm}$738k-Condor80k  &5.7 \color{SeaGreen}{$\boldsymbol{\uparrow}$} & 75.2 \color{SeaGreen}{$\boldsymbol{\uparrow}$} & 55.7 \color{SeaGreen}{$\boldsymbol{\uparrow}$}  & 16.8/-57.0 \color{red}{$\boldsymbol{\downarrow}$}& 38.3 \color{red}{$\boldsymbol{\downarrow}$} &72.6 \color{red}{$\boldsymbol{\downarrow}$}  &74.0 \color{red}{$\boldsymbol{\downarrow}$} &37.6 \color{red}{$\boldsymbol{\downarrow}$}\\
    \Xhline{0.15em}
    \end{tabular}
    }%
    \caption{\textbf{Performance of Incorporating High-Quality Language Data}. LLaVA$^I$ denotes LLaVA structure with InternLM2.5-7B as language model, and LLaVANext$_{mm}$738k refers to the multi-modal data in LLaVANext-778K.
    Integrating high-quality language data significantly improves alignment performance on language benchmarks.
    However, it leads to a decline in multi-modal alignment performance on benchmarks such as WildVision and MMVet. 
    For AlpacaEval and ArenaHard, we present the winning rate against GPT-3.5. }
    \label{tab: mm}
    \vspace{-5pt}
% \end{table*}
\end{table*}

To assess the impact of the first factor on multi-modal alignment, we examined the role of text-only data in multi-modal SFT.
The LLaVA-Next-778K dataset includes approximately 40K text-only samples from ShareGPT~\cite{vicuna2023}, which are relatively outdated and of lower quality. 
To improve alignment, we experimented with replacing these samples with better-quality alternatives and increasing their proportion in the multi-modal fine-tuning data. 
Specifically, we sampled 40K / 80K instances from two high-quality language SFT datasets: Magpie-Llama3.3~\cite{xu2024magpie} and Condor~\cite{cao2025condor}, using these to replace the original language data in LLaVA-Next. 
We then evaluated models fine-tuned on these different data mixtures using both text-only benchmarks and multi-modal benchmarks, including WildVision~\cite{lu2024wildvision} for human preference alignment and various benchmarks from OpenVLM Leaderboard~\cite{duan2024vlmevalkit} for assessing various fundamental multi-modal capabilities.\footnote{MMVet is open-ended, while others are closed-ended. } 



% \textit{HOW?} To address this issue, we first attempted to integrate high-quality text-only training data from LLMs into the training process. Given that the language data within LLaVA-Next-778K is sourced from ShareGPT-4 and may not be of sufficient quality, it is empirically reasoned that substituting this training data with higher-quality sources could lead to improved performance in alignment.
% Thus, we selected two high-quality language data sources: Magpie-Llama3.3~\cite{xu2024magpie} and Condor~\cite{cao2025condor}. We uniformly sampled 40K instances from each of these sources, matching the original number of language data points in the LLaVA-Next-778k dataset. These sampled datasets were then used to replace the original language data in LLaVA-Next. 
% We then compared the models using the aforementioned language benchmarks and several popular multi-modal benchmarks, including benchmarks in OpenCompass~\cite{duan2024vlmevalkit} Leaderboard, and WildVision \cite{lu2024wildvision}. Among these multi-modal benchmarks, WildVision specifically targets alignment response quality, and MMVet includes some open-ended questions. The remaining benchmarks are primarily objective in nature. The results are shown in Tab.~\ref{tab: mm}.


Results in \cref{tab: mm} reveal several key findings. 
While MLLMs tuned with higher-quality text-only samples show significant improvements on text-only alignment benchmarks, 
they unexpectedly demonstrate degraded performance on both multi-modal alignment and common VQA benchmarks. 
This counter-intuitive phenomenon suggests that language alignment capability does not directly translate to multi-modal alignment. 
We therefore argue that high-quality multi-modal human-aligned data is crucial for improving MLLMs' human preference alignment in multi-modal contexts.


% The results revealed several key observations: 

% \noindent \textit{1. Enhancement on Language Benchmarks.} 
% \noindent \textit{2. Minor Diminished Performance on Common VQA Benchmarks.} 
% % Consistent with our hypothesis, the enhancement in subjective capabilities came at the cost of performance on objective multi-modal benchmarks. This trade-off suggests that the model's focus shifted towards handling more complex linguistic structures at the expense of its ability to process straightforward multi-modal inputs.
% \noindent \textit{3. Decline in Multi-modal Alignment Benchmarks.} 

% To our surprise, despite the rapid improvement in language alignment , the performance on multi-modal benchmarks for alignment such as WildVision did not increase but rather declined to a lower level. \textbf{This unusual phenomenon may indicate that capability of language alignment and multi-modal alignment are not closely related; instead, they may develop along separate trajectories. }

% Based on the observation above, we argue that while enriching the language data can significantly boost performance on purely textual human-aligned benchmarks, it does not necessarily translate to better performance on multi-modal alignment. Instead, high-quality multi-modal human-aligned data is of vital importance for enhancing the multi-modal alignment of MLLMs. 
