\section{Evaluation Results}

\subsection{SFT with OmniAlign-V }

We conduct extensive experiments to demonstrate OmniAlign-V's effectiveness. 
We combine OmniAlign-V with LLaVA-Next-778k (excluding text-only samples), 
creating OmniAlign-V$_{mix}$ with 946K training samples. 
We evaluate various MLLMs tuned on OmniAlign-V against their counterparts tuned on LLaVA-Next-778k.

Our evaluation spans multiple multi-modal benchmarks, 
including standard VQA benchmarks~\cite{yu2023mm,liu2024mmbench,liu2023hidden,yue2023mmmu,kembhavi2016diagram} and human-preference alignment benchmarks: MM-AlignBench, WildVision ~\cite{lu2024wildvision}, and MIA-Bench~\cite{qian2024mia}. 
Results in \cref{tab: main} show that OmniAlign-V significantly improves human alignment across all benchmarks. 
Moreover, our training data improves general multi-modal capabilities, particularly on MMVet and MMMU, demonstrating a trend distinct from text-only data. 

Notably, despite excluding language samples from training data, models maintain stronger language alignment than those trained on LLaVA-Next-778k, as shown in Table~\ref{tab: main_lang}. 
This suggests that while high-quality language data alone may not significantly impact multi-modal capabilities, 
enhancing multi-modal data quality can improve both language and multi-modal performance, highlighting the crucial role of high-quality, human-aligned multi-modal training data.


% We conduct several experiments to demonstrate the efficacy of our training data. We remove the pure-text samples from LLaVA-Next-778k and combined the remaining data with our own, resulting in a total of 946k training samples OmniAlign-V$_{mix}$. We first evaluate on Multi-Modal benchmarks, encompassing both common VQA benchmarks~\cite{yu2023mm,liu2024mmbench,liu2023hidden,yue2023mmmu,kembhavi2016diagram} and those that evaluate alignment of models, MM-AlignBench, WildVision~\cite{lu2024wildvision}, and MIA-Bench~\cite{qian2024mia} which focus on the multi-modality instruction-following performance. The results are shown in Tab.~\ref{tab: main}.

% It can be observed that incorporating our training data significantly enhances the alignment of models with human. This improvement is evident not only on the MM-AlignBench but also manifests as a substantial zero-shot increase on WildVision and MIA-Bench.
% Moreover, the model's general multi-modal capabilities improve with our training data, particularly notable on MMVet and MMMU, which demonstrates a markedly different trend compared to language data.
% % This observation underscores the high quality of our data.
% \input{tables/benchmark_results}
% Furthermore, we observe that despite the absence of language samples in training data, the alignment of model's language capability remains stronger than that of LLaVA-Next-778k dataset. The results are shown in Table~\ref{tab: main_lang}.
% This phenomenon suggests that high-quality language training data alone may not significantly influence multi-modal capabilities. However, enhancing the quality of multi-modal data can lead to improvements in both language and multi-modal performance. This underscores the essential role of high-quality, human-aligned multi-modal training data.
\input{tables/dpo}

\subsection{DPO with OmniAlign-V-DPO}
We conduct DPO post-training on three models: 
LLaVA-Next trained with LLaVA-Next-778k, 
LLaVA-Next trained with OmniAlign-V$_{mix}$, 
and InternVL2-8B. 
Results in \cref{tab: dpo} show that DPO tuning significantly improves performance on real-world questions (WildVision) across all models. 
While the baseline trained solely on LLaVA-Next-778k shows minimal improvement on MM-AlignBench, 
models incorporating OmniAlign-V during SFT demonstrate substantial gains after DPO. 
Similarly, InternVL2-8B, a state-of-the-art MLLM partially trained on proprietary multi-modal corpora, shows significant improvement on MM-AlignBench post-DPO. 
% We hypothesize that sufficient and diverse multi-modal training during SFT is a prerequisite for OmniAlign-V DPO's effectiveness. 
This indicates that if a model has been trained on data aligned with human preferences, such as open-ended or long-context data  during SFT phase, the subsequent DPO training on high-quality human-aligned data can significantly activate the model’s ability, leading to a considerable improvement in alignment performance. 
In contrast, if the model has not been exposed to such alignment-focused datasets during SFT, training with open-ended data alone via DPO will not significantly improve its capabilities of alignment.
These findings demonstrate the value of OmniAlign-V in both SFT and DPO stages for enhancing human preference alignment.

% We conduct DPO post-training experiments on the following models: LLaVA-Next trained with LLaVA-Next-778k; LLaVA-Next trained with OmniAlign-V$_{mix}$; InternVL2-8B, the results are shown in Table.~\ref{tab: dpo}.

% It can be observed that after the DPO stage, all models exhibit better performance on questions in the wild(WildVision).
% On MM-AlignBench, the LLaVANext-InternLM trained solely with LLaVA-Next-778k does not show significant improvement.
% However, integrating our data during the SFT stage leads to further substantial enhancements in performance after the DPO stage.
% InternVL2-8B also shows a significant improvement on MM-AlignBench, which can be attributed to the large volume of context-rich image-text interleaved data used during SFT stage.
% These findings indicate that our data is not only effective when used in the SFT stage but also enhances model alignment with human preferences when integrated into the DPO post-training process.
\input{tables/benchmark_results}

\subsection{MM-AlignBench}
We evaluate various state-of-the-art MLLMs~\cite{2022chatgpt,team2023gemini,Claude3,bai2023qwen,chen2024far,li2024llava,minicpm2024,internlmxcomposer2,LLaMA32Vision,laurençon2024building,wang2024enhancing} on MM-Alignbench, with results shown in \cref{tab: bench}. 
Closed-source models like GPT, Claude, and Gemini demonstrate strong alignment in responding to open-ended questions. 
In contrast, while Qwen2-VL and InternVL2 excel at common VQA tasks, they show relatively lower human preference alignment. 
This highlights the importance of prioritizing MLLM alignment for improved daily human interactions. 
Our LLaVA-OA-32B, trained with OmniAlign-V, achieves exceptional performance, outperforming numerous strong MLLMs and nearly matching Qwen2VL-72B. 
After applying DPO with OmniAlign-V-DPO, \textbf{LLaVA-OA-32B-DPO achieves winning rate of 72.6 with an average reward of +33.5,  surpassing the performance of Qwen2VL-72B}. 
These results highlight the high quality and effectiveness of the OmniAlign-V dataset in improving model alignment with human preferences.


% We evaluate SOTA MLLMs on MM-Alignbench, with the results presented in Tab.~\ref{tab: bench}. It is evident that closed-source models, such as GPT, Claude, and Gemini, exhibit strong alignment when responding to open-ended questions. In contrast, Qwen2VL and InternVL2 perform exceptionally well on common VQA tasks but show comparatively lower alignment with human preferences. This suggests that the alignment of MLLMs should be prioritized to enhance their utility in daily human interactions. Our LLaVA-OA-32B, trained with OmniAlign-V, demonstrates outstanding performance, surpassing a wide range of strong MLLMs and nearly matching the performance of Qwen2VL-72B, shows the great quality of our data.

\subsection{Ablation Study}
We conduct an ablation study to evaluate each subset of OmniAlign-V, 
reporting results on MM-Alignbench, WildVision, and MMVet in \cref{tab: ablation}. 
Performance improves progressively as different tasks from OmniAlign-V are incorporated. 
Notably, Instruction-Following data significantly enhances performance across all three benchmarks, 
demonstrating its crucial role. 
The creation data subset uniquely improves performance on MM-Alignbench but not on WildVision and MMVet.
This discrepancy can be attributed to the absence of multi-modal creative question types in these two benchmarks, suggesting their incompleteness in capturing full spectrum of alignment challenges.
\input{tables/ablation_data}

% We conduct an ablation study to evaluate the impact of each component of our training data and report the results on MM-Alignbench, WildVision, and MMVet, as detailed in Table~\ref{tab: ablation}. The performance on these benchmarks improves progressively by incorporating a series of tasks within OminiAlign-V. Specifically, the integration of Instruction-Following significantly enhances performance across all three benchmarks, highlighting its importance. Additionally, we observe that the inclusion of creation data leads to performance gains only on MM-Alignbench, likely due to its distinct distribution compared to WildVision and MMVet.

\section{Conclusion}
In this paper, we introduce \textbf{OmniAlign-V}, 
a dataset designed to enhance the alignment of MLLMs with human preferences, 
as well as \textbf{MM-AlignBench}, 
a high-quality, specific-purpose benchmark for evaluating such alignment. 
We investigate the impact of both language and multi-modal training data, 
emphasizing the critical role of multi-modal open-ended training data. 
By incorporating OmniAlign-V into SFT and DPO stages, we achieve significant improvements in the alignment of MLLMs. 
% OmniAlign-V is poised to become resource in advancing the development of MLLMs.

\section{Limitation}
% Although OmniAlign-V  demonstrates strong performance in improving the alignment of MLLMs, the scale of our dataset remains limited and may not be sufficient for large-scale training. Additionally, the selected tasks may lack generality and fail to encompass all aspects of daily life. Deeper exploration into the alignment of MLLMs is still needed to address these limitations and further advance the field.
Although the OmniAlign-V pipeline can be easily scaled to support larger datasets, the scale of the dataset used in this paper may be insufficient for large-scale training due to cost limitations. 
% Besides, MM-Alignbench utilizes GPT-4o for evaluation, which may not fully align with human preferences.
Deeper exploration into the alignment of MLLMs is still needed to address these limitations and further advance the field.