\section{Related Work}
% Nowadays, Multi-Modality Large Language Models (MLLMs) have demonstrated significant advancements in Visual Question Answering (VQA), leading to their widespread application across numerous downstream visual tasks. As MLLMs continue to evolve rapidly, the quality of multi-modality instruction-following data has become a focal point of increasing attention. 

% In the early stages, Visual Question Answering (VQA) datasets were relatively simple in format. Popular VQA datasets such as GQA~\cite{}, and VQA-v2~\cite{}, predominantly featured straightforward questions that required single-word or short-sentence answers. Visual classification datasets like LAION~\cite{} typically included only brief descriptions referring to the main object in the image.

% LLaVA~\cite{} leveraged advanced large language models to generate instruction-following formatted data from traditional VQA datasets~\cite{}, resulting in an extensive collection of 665K instruction-following data combinations, which are specifically designed for the Supervised Fine-Tuning (SFT) stage.
% With the advancement of MLLMs, there has been an increased trend toward utilizing powerful MLLM for dataset annotation.
% % thereby accelerating the efficiency and quality of data preparation. 
% ShareGPT-4V~\cite{} pioneered the use of GPT-4V to provide detailed descriptions of images within the COCO dataset, resulting in 100k high-quality image-caption pairs. Similarly, ALLaVA~\cite{} employed GPT-4V to generate complex questions for each image, leading to the creation of 700k sophisticated Image Question-Answering pairs.
% These powerful MLLMs are increasingly being employed to generate data for various downstream visual tasks, including but not limited to visual chat applications such as video~\cite{}, medical~\cite{}, and remote sensing~\cite{}.

% While the quantity and quality of multi-modality training data are rapidly increasing, much of this data remains focused on accurate and concise QA pairs. However, there is a notable gap in the representation of subjective multi-modality data, which is crucial for aligning model outputs with human preferences. Therefore, this study aims to highlight the importance of considering such data and propose methods for integrating subjective elements into training datasets to improve model performance in real-world applications.

\textbf{Alignment of LLMs.} Alignment, encompassing the ability to follow human instructions and provide helpful assistance~\cite{liu2024trustworthyllmssurveyguideline}, has long been a critical aspect of LLMs. 
Recent works~\cite{wang2023openchat,xu2024magpie,cao2025condor} focused on generating high-quality SFT training data to enhance the alignment of LLMs. 
Besides, recent benchmarks~\cite{liu2023alignbench,alpaca_eval,li2024crowdsourced} have introduced open-ended and challenging questions to assess the alignment performance of LLMs. 
However, there is a notable lack of benchmarks designed to evaluate the alignment of MLLMs.

\noindent\textbf{Visual Question Answering Datasets.} In the early stages, Visual Question Answering (VQA) datasets were primarily used for basic semantic alignment between images and text~\cite{radford2021learning,li2022blip, zhao2024open}. These datasets were relatively simple in structure. Popular VQA datasets~\cite{hudson2019gqa, antol2015vqa} predominantly featured straightforward questions that elicited single-word or short-sentence answers. Similarly, visual classification and detection datasets~\cite{lin2014microsoft, schuhmann2021laion, sharma2018conceptual} typically consisted of brief descriptions that focused on the main object within the image.

\noindent\textbf{Instruction Tuning data.} With the rapid development of MLLMs~\cite{chen2024far, bai2023qwen}, the instruction tuning data has gained more attention. LLaVA~\cite{liu2023visual} leveraged advanced LLMs to generate instruction-following formatted data from traditional VQA datasets. Recent works have further expanded this approach by employing MLLM to generate captions~\cite{chen2023sharegpt4v}, complex question-answer pairs~\cite{ chen2024allava, gu2024infinity}, OCR data~\cite{textocr-gpt4v} and math-related data~\cite{shi2024math}. Other efforts~\cite{tong2024cambrian, li2024llava} aggregates multiple publicly available document image datasets into unified resources. However, these datasets primarily focus on fundamental visual capabilities with short dialogues and factual questions, resulting in suboptimal alignment with human preferences.

