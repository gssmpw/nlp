\section{Introduction}

With the rapid advancement of large language models (LLMs)~\cite{2022chatgpt,touvron2023llama2},  multi-modal large language models (MLLMs)~\cite{OpenAI2023GPT4TR,team2023gemini} have also seen significant improvements. 
Most open-source MLLMs~\cite{liu2023visual,chen2023internvl} are developed by 
connecting a vision encoder~\cite{dosovitskiy2020image} to a pretrained LLM,
followed by vision instruction tuning.
Existing vision instruction tuning datasets~\cite{liu2023visual,chen2023sharegpt4v,chen2024allava} and 
multi-modal evaluation benchmarks~\cite{liu2023mmbench,yue2023mmmu,lu2023mathvista} primarily focus on assessing fundamental capabilities (object recognition, OCR, \emph{etc.}) of MLLMs,
while paying little attention to human preference alignment.
Consequently, 
while open-source MLLMs achieve comparable performance to proprietary counterparts on objective metrics of these foundational skills,
% \wwy{citation to prove your claim}
they display a significant gap in the alignment with human preferences, 
which detrimentally impacts user experience in multi-modal conversational interactions, as demonstrated in Appx.~\ref{appd: align}.
%
% \wwy{What is human preference?}

In this work, 
a preliminary investigation was conducted to quantitatively assess the degradation of human preference alignment in MLLMs(\cref{sec: preliminary}). 
Experimental results on text-only subjective evaluation benchmarks~\cite{dubois2024length,li2024crowdsourced}
revealed that MLLMs exhibited a substantial performance drop compared to their corresponding LLMs. 
A plausible hypothesis suggests that MLLMs suffer from catastrophic forgetting during vision instruction tuning. 
Following this hypothesis, a straightforward solution would be incorporating both multi-modal and text-only SFT data~\cite{xu2024magpie,cao2025condor,ding2023enhancing} for joint learning. 
% However, empirical evidence indicates that this approach not only failed to yield improvements
However, our experiments (Tab.~\ref{tab: mm}) indicate that this approach not only failed to yield improvements
% \wwy{citation to prove your claim}
in human preference alignment under multi-modal contexts but also demonstrated negative impacts on the foundational multi-modal skills of MLLMs. 
These observations suggest that enhancing human alignment in multi-modal scenarios necessitates the development of specialized multi-modal instruction tuning datasets.

Existing multi-modal instruction tuning datasets predominantly focus on foundational capabilities, featuring simple language patterns and uniform response styles (\cref{fig:badsft} in appendix).
% \wwy{it is important to prove your claim, or qualitative examples}
This study argues that effective multi-modal datasets for enhancing human preference alignment should incorporate several critical characteristics: \textbf{open-ended questions}, \textbf{broad topic coverage}, \textbf{diverse response formats} (varying in length and style), and \textbf{strict adherence to instructions}. 
Based on these principles, we constructed \textbf{OmniAlign-V}.
In terms of image sources, OmniAlign-V encompasses natural images and infographics such as posters and charts. 
Furthermore, a novel solution was developed to filter out semantically rich images from natural image collections.
With respect to tasks, complex knowledge-based question answering, creative tasks, and reasoning tasks were designed for different image types. 
Each task category incorporates diverse subtasks, and state-of-the-art MLLMs were leveraged to obtain diverse, high-quality responses. 
Ultimately, OmniAlign-V comprises $\sim$200K multi-modal instruction tuning samples, 
exhibiting a significantly different overall data distribution compared to existing multi-modal SFT datasets. 

Leveraging OmniAlign-V, comprehensive experiments were conducted to explore its full potential for enhancing human alignment in MLLMs. 
Integrating OmniAlign-V into the SFT stage of the LLaVA-NeXT~\cite{liu2024llavanext} structure with InternLM2.5-7B and Qwen2.5-32B yielded significant improvements in human preference alignment across various powerful LLM encoders, 
including InternLM2.5~\cite{cai2024internlm2} and Qwen2.5~\cite{qwen}. 
Furthermore, on ground-truth-based VQA benchmarks like MMMU~\cite{yue2023mmmu} or OCRBench~\cite{liu2023hidden}, 
MLLMs finetuned with OmniAlign-V displayed comparable or superior performance. 
Beyond its application in SFT, 
OmniAlign-V also demonstrated additional value when applied to Direct Preference Optimization (DPO)~\cite{rafailov2024direct}. 
Experimental results indicate that implementing OmniAlign-V for DPO yields further improvements in preference alignment, 
surpassing baseline models finetuned on the same dataset. After the SFT and DPO stage, \textbf{our LLaVA-Next baseline with Qwen2.5-32B surpasses the state-of-the-art model Qwen2VL-72B}~\cite{chen2024far} finetuned on extensive proprietary datasets.


Throughout the exploration, 
we also observed that existing multi-modal human preference benchmarks~\cite{lu2024wildvision,qian2024mia} lack diversity in image sources, contain repetitive questions, and lack clarity. 
To address these limitations, 
we introduce \textbf{MM-AlignBench},
a high-quality benchmark comprising 252 carefully curated samples with diverse image sources and meticulously crafted questions by human annotators, enabling comprehensive evaluation of MLLMs' alignment with human preferences.
MM-AlignBench, along with other MLLM human preference benchmarks, 
was employed throughout this study for evaluation.



% To further enhance the performance, we further propose OmniAlign-V-DPO, a high-quality dataset for Direct Preference Optimization(DPO) based on OmniAlign-V.

% Existing multi-modal subjective benchmarks~\cite{} often encounter issues such as limited diversity in image sources, repetitive questions, and lack of clarity. 
% To more accurately assess the alignment of MLLMs with human preferences, we introduce MM-Alignbench, a high-quality benchmark comprising 252 carefully curated samples. 
% The image sources are drawn from a range of diverse datasets, while the questions are meticulously crafted by human annotators to ensure both variety and quality. 
% This design allows for a more comprehensive evaluation of the MLLM's true performance in subjective alignment with human judgment.

% We conduct comprehensive experiments involving both SFT and DPO training across various model structures and scales. 
% By incorporating OminiAlign-V in SFT stage, we observe a significant improvement in subjective alignment for both LLaVA-Next with InternLM2.5-7B and Qwen2.5-32B models. Notably, performance on VQA benchmarks, such as MMVet and MMMU, also shows substantial gains.
% Furthermore, after applying DPO with OmniAlign-V-DPO, the subjective alignment of our models, as well as InternVL2-8B, is further enhanced, demonstrating the efficacy of our data in improving model performance.

The contributions of this study are as follows:

1. An in-depth investigation into the degradation of MLLM human alignment, 
analyzing the impact of both text-only and multi-modal tuning data.

2. The introduction of OmniAlign-V, 
a comprehensive open-ended multi-modal SFT dataset, complemented by OmniAlign-V-DPO. Extensive experiments demonstrate the effectiveness of these datasets in improving human preference alignment.

3. The development of MM-AlignBench, a carefully curated benchmark comprising high-quality, human-annotated samples specifically designed to evaluate MLLMs' human preference alignment.