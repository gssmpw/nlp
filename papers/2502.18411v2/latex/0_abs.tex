\begin{abstract}

Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, 
leaving a significant gap in human preference alignment. 
This paper introduces \textbf{OmniAlign-V},  a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. 
% Our investigation reveals that existing vision instruction tuning approaches and joint learning with text-only data fail to address the alignment challenge effectively. 
% To tackle this issue, OmniAlign-V incorporates diverse image sources, complex knowledge-based questions, and varied response formats, resulting in approximately 200K high-quality training samples. 
We also present \textbf{MM-AlignBench}, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. 
Experimental results show that finetuning MLLMs with OmniAlign-V, 
using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO),
significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities.
Our datasets, benchmark, code and checkpoints have been released at \url{https://github.com/PhoenixZ810/OmniAlign-V}.

% Although open-source multi-modal large language models(MLLMs) have matched or even surpassed proprietary models in common VQA tasks, a significant gap persists in their alignment with human preferences. In this paper, we investigate the impact of text-only and multi-modal training data on the alignment of MLLMs, emphasizing the critical role of multi-modal open-ended training data. To address this gap, we introduce OmniAlign-V , a high-quality training dataset comprising open-ended, comprehensive question-answer pairs, along with MM-AlignBench , a human-annotated reliable benchmark designed to evaluate the alignment of MLLMs. Through experiments using both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), we demonstrate the substantial effectiveness of our dataset in enhancing MLLM's alignment with human preference.
\end{abstract}