\begin{comment}
Sec 4 主要讲我们怎样构造多模态数据 (for Human Alignment)，按这个 pipeline 写：
1. 既然要构造，首先讲现在数据有什么样的问题，我们希望构造怎样的数据 (长回复、有格式、follow instruction, etc.)？
2. 总结我们设计 data synthesis pipeline 的版图。两个维度：1. 图片类型：自然图像，人工图像 (艺术创作、专业领域图像 (电路图、学科图示等)、图表 (目的为展示数据：饼图、折线图等)、Infographics (由复杂图文组成，用以传递信息，如海报等))；2. 任务类型：blahblah；3. 图像类型与任务类型间存在关系 (若画成二维应该是一个稀疏矩阵)。
3. 如何挑图 (自然图像)：须有实验结果佐证
4. 如何生成 QA (拣值得一提的部分说)：1. 如何生成回复；2. 如何 refine 回复；3. DPO 数据生成
\end{comment}

\section{OmniAlign-V}

Current MLLM instruction tuning datasets primarily focus on enhancing basic capabilities like perception, OCR, and mathematical reasoning. 
These datasets typically contain simple, brief question-answer pairs that inadequately capture human preferences and real-world interaction complexity, as shown in \cref{fig:badsft}. 
We propose that multi-modal training data should also incorporate: (1) \textbf{Open-ended, Diverse, and Creative Questions requiring interdisciplinary knowledge}, (2) \textbf{Comprehensive and Knowledge-rich Responses}. To address these requirements, we develop a novel data synthesis pipeline to generate high-quality human-aligned multi-modal training data, resulting in the creation of \textbf{OmniAlign-V}.

% Currently, the majority of training datasets are designed with an emphasis on enhancing the objective capabilities of MLLMs, such as their proficiency in perception, OCR, mathematical reasoning, and other similar domains. However, the question-answer pairs within these datasets tend to be simplistic and brief, lacking both complexity and diversity. Consequently, they do not adequately reflect the nuances of human preferences or the intricacies of real-world interactions, as illustrated in Fig.~\ref{fig:badsft}.
% We argue that multi-modal training data should encompass the following characteristics: \textit{Open-ended, Diverse, and Creative Questions that require Interdisciplinary Knowledge}; \textit{Comprehensive and Knowledge-Rich Responses}; \textit{Flexible Instruction-Following Pairs};. 
% To address these, we propose our data engine OmniAlign-V, to generate high-quality human-aligned multi-modal training data.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/framework_local.pdf}
    \caption{\textbf{Overall pipeline of OmniAlign-V}. By utilizing an image filter and employing a customized pipeline for distinct tasks, we curate semantically rich images paired with high-quality open-ended question-answer sets. Post-refinement further enhances both the variety and quality of our dataset.}
    \label{fig:pipeline}
    \vspace{-10pt}
\end{figure*}

\subsection{Task Taxonomy: A Big Picture}


Image content plays a crucial role in constructing multi-modal training data. 
To ensure comprehensive coverage, we classify images into two major categories: natural images and infographic images, as shown in \cref{fig:data_combination}(a). 
Our data synthesis pipeline first determines whether an input image belongs to natural images (captured from real-world scenes) or infographic images (human-created to convey information). 
Based on this classification, different vision-language tasks are assigned.

For natural images, we define three primary tasks: \textbf{Knowledge}, \textbf{Inferential}, and \textbf{Creation}, 
each requiring diverse and complex question formats with comprehensive, reasoned responses. 
These tasks enhance the model's ability to interpret real-world scenes effectively.

For infographic images, given their diverse content, 
we identify four key types that elicit intricate and challenging questions: \textbf{Arts}, \textbf{Charts}, \textbf{Diagrams}, and \textbf{Posters}. 
These categories necessitate a deep understanding of human-designed visuals, 
including both abstract and detailed elements.


% Then in natural images, we propose three main tasks: Knowledge, Inferential, and Creation. The three tasks require diverse, complexible format of questions and comprehensive, reasonable response.
% For unnatural images, considering the content of image types, we select four image types that can raise diverse and complicated questions, includes Arts, Charts, Diagrams, and Posters. 
% Within the category of natural images, we propose three primary tasks: \textit{Knowledge}, \textit{Inferential}, and \textit{Creation}. Each task demands questions in diverse and complex formats, along with comprehensive and reasoned responses. These tasks are designed to challenge and enhance the model's ability to understand and interpret real-world scenes effectively.
% For infographic images, given their content diversity, we selected four specific types that are likely to elicit a wide range of intricate and challenging questions: \textit{Arts}, \textit{Charts}, \textit{Diagrams}, and \textit{Posters}. These categories were chosen because they require the model to demonstrate a deep understanding of human-generated imagery, encompassing both abstract and detailed visual elements.



\subsection{Image Selection Strategy}
\label{sec: filter}

For natural images, rich semantic content leads to more comprehensive and insightful QA pairs. 
To enhance training data quality, 
we developed a pipeline to select semantically rich images from diverse sources, 
as shown in \cref{fig:pipeline}(a). This pipeline consists of two key steps.

First, we use the Image Complexity (IC) recognition model IC9600~\cite{feng2022ic9600} to assign IC scores to images. 
Images with low semantic content —- 
characterized by few objects or simple, uniform backgrounds —- 
receive low IC scores and are then excluded. 
While IC9600 effectively filters out low-complexity images, high IC scores alone do not guarantee semantic richness. 
For instance, as shown in \cref{fig:imagefilter}, an image filled with tents may have a high IC score but lacks sufficient semantic information for multi-modal training.

To refine selection, we employ the Recognize Anything Model~\cite{zhang2024recognize} to identify objects within images, filtering out those with high complexity but minimal meaningful content. This two-step approach ensures our pipeline accurately selects images that are both complex and semantically rich. 
Experiment results have validated the effectiveness of our proposed image selection strategy, as shown in \cref{appx: filterablation}.



% The pipeline is divided into two key steps. Image complexity (IC) refers to the level of detail and variety in an image. First, we utilize IC9600~\cite{feng2022ic9600} model to assess IC score of the images.
% Image complexity (IC) is defined as the intricacy contained within an image. Objectively, IC can be considered as the amount of detail and variety in an image. human-alignedly, it is the degree of difficulty for a human audience to understand or describe an image, regarding both global abstract and local details or textures. 
% Image complexity (IC) refers to the level of detail and variety in an image, both objectively (in terms of intricacy) and human-alignedly (based on the difficulty for humans to understand or describe global and local features).
% Consequently, images with low semantic content—characterized by few objects or simple, uniform backgrounds—are excluded from our dataset. 
% This ensures that only semantically rich images, which contain sufficient detail and complexity, are selected for generating high-quality Q-A pairs.

% While the IC9600 model effectively filters out images with low complexity, it sometimes encounters high-complexity images that contain repetitive elements with low semantic information. Examples are provided in Fig.~\ref{fig:imagefilter}.
% To address this, we further employ the RAM model \cite{zhang2024recognize} to recognize the objects within the image. This additional step ensures that images with high complexity but minimal meaningful information are filtered out. 
% By combining both two models, our pipeline can accurately select images that are not only complex but also rich in semantic detail.
% thereby enhancing the quality and relevance of the generated Q-A pairs.

\subsection{Data Generation Pipeline}
\label{sec: sft generate}
% The overall generation pipeline consists of five distinct modules, as illustrated in Fig.~\ref{fig:pipeline}.
% For each type of multi-modal task, a specific pipeline has been devised to generate high-quality question-answer pairs.

\noindent\textbf{SFT QA Generation. } 
In \cref{fig:pipeline}(b), we outline the generation process of SFT QA pairs.
For vision-language tasks involving natural images (knowledge, creative, inferential), 
we first apply our image selection strategy to filter semantically rich images from CC-3M\cite{sharma2018conceptual}, Flickr30k \cite{plummer2015flickr30k}, and GQA~\cite{hudson2019gqa}.
For infographic image tasks, images are collected from various existing sources~\cite{masry2022chartqa,Li2018TextbookQA,kembhavi2016diagram}. 
More details are provided in \cref{sources}.

% The generation of QA-pairs is detailed in Fig.~\ref{fig:pipeline}(b). For tasks involving natural images, which encompass knowledge-based, creative, and inferential tasks, we first employ the image filter detailed in Section~\ref{sec: filter} to select images rich in semantic content from CC-3M~\cite{sharma2018conceptual}, Flickr30k~\cite{plummer2015flickr30k}, and GQA~\cite{hudson2019gqa} datasets.

\textit{Knowledge \& Inferential Tasks. }
In preliminary experiments, we found that GPT-4o can generate diverse, content-relevant questions for the two tasks when provided with well-designed few-shot prompts. 
Consequently, we carefully designed a single prompt for each task category, incorporating comprehensive instructions and selected few-shot examples. 
These prompts were then used with GPT-4o across diverse images to generate QA pairs.


\textit{Creative Tasks. }We noticed that a single prompt cannot generate sufficiently diverse, 
content-relevant creative questions. 
Therefore, we developed a more sophisticated pipeline inspired by Condor~\cite{cao2025condor}. 
First, we created a set of seed creative questions:
\begin{equation}
    \mathcal{Q}_s =\{Q_1, Q_2, ... Q_N\}
\end{equation}
where each seed question corresponds to a distinct creative task. 
Since directly using all seed questions as few-shot examples leads to repetition and lack of diversity, 
we employ a light-weight MLLM~\cite{luo2024mono} to generate detailed captions $C$ for each image. An LLM $\mathcal{M}$ then selects a relevant subset of seed questions according to the caption:
\begin{equation}
    \mathcal{Q}_s' = \mathcal{M}(C, \mathcal{Q}_s), \|Q_s'\| \ll \|Q_s\|
\end{equation}
Finally, we randomly select three question types from $\mathcal{Q}_s'$ as few-shot examples for GPT-4o, preserving both quality and diversity of synthesized data.

% we first designed specific pools of question types:
% \begin{equation}
%     \mathcal{Q}_s =\{Q_1, Q_2, ... Q_N\}, 
% \end{equation}
% where $N$ is the nunmber of seed question types, drawing inspiration from Condor~\cite{cao2025condor}. 
% In practice, we found that directly prompting GPT-4o to select question types from all available options within the pools can result in significant repetition and a lack of diversity in the generated questions. 
% % The reason is that the variety of suitable question types is finite, with certain categories, such as prose writing, can be broadly applicable to most natural images. 
% To address this challenge, we employ Mono-InternVL \cite{luo2024mono} to quickly produce detailed caption $C_i$ for each image. 
% These captions are then used to guide an LLM $\mathcal{M}$ in selecting appropriate question types that are contextually relevant:
% \begin{equation}
%     \mathcal{Q}_m = \mathcal{M}(C_i, \mathcal{Q}_s),
% \end{equation}
% where $\mathcal{Q}_m = \{Q_1, Q_2, \dots, Q_n\}, \quad n \ll N$. Following this, we randomly chose three question types from $\mathcal{Q}_m$ to serve as few-shot examples, which we use to prompt GPT-4o to generate both the questions and their corresponding answers. This method aims to maintain the diversity of the creative questions as much as possible.
 
\textit{Infographic Tasks. }
For infographic image tasks (\textit{Charts}, \textit{Diagrams}, \emph{etc.}), 
questions and answers are closely tied to specific image content. 
Unlike natural images, these visuals convey information primarily through text, colors, lines, and symbolic elements, 
making evaluation based on image complexity or object categories unsuitable. Therefore, instead of applying image selection strategies, 
we carefully select image sources containing rich, detailed information. 
We then design specialized prompts for GPT-4o to generate questions that require comprehensive background knowledge understanding. The difference is shown below:

\noindent\textit{InfographicVQA: What is the respiratory disease death rate for individuals aged 70+? }

\noindent\textit{OmniAlign-V: How does the respiratory disease death rate for individuals aged 70+ compare to the other age groups, and what might this suggest?}

% For tasks involving infographic images, such as \textit{Arts}, \textit{Charts}, \textit{Diagrams}, and \textit{Posters}, the questions and answers are intricately tied to the specific content of the images. Unlike natural images, the information conveyed in these types is primarily described through text, colors, lines, or other symbolic elements, making it unsuitable to evaluate based solely on image complexity or object categories.
% Thus, we carefully select image sources that contain rich and detailed information, 
% % tailored to the specific characteristics of each type of unnatural image. 
% the details are shown in Appx.~\ref{sources}.
% We then design specialized, high-quality prompts for GPT-4o to generate questions that require an understanding of the background knowledge.


\noindent\textbf{Post Refinement. } 
To further improve the quality of the synthesized data, 
 we implemented a series of post-processing methods for refinement,
as illustrated in \cref{fig:pipeline}(c).

\textit{Instruction Augmented Knowledge QAs. }
Instruction following is a crucial capability significantly impacting human preference. 
To enhance this, we incorporate complex instructions and restrictions into our knowledge QA pairs and reformulate responses accordingly. 
As shown in \cref{fig:pipeline}(c1), for each knowledge QA, 
we use a powerful LLM to select an appropriate instruction type that can be integrated into the question without depending on visual content. 
The instruction is then incorporated into the existing question, and an LLM adjusts the corresponding answer to ensure alignment with both the modified question and original context, resulting in instruction-augmented knowledge QAs.


% For \textit{Knowledge} data, inspired by the composition of language training data~\cite{cao2025condor}, we posit the complicated format of questions and answers will lead to a superior performance of language capability. Specifically, we focus on incorporating intricate instructions into question formulations.
% Therefore, we have designed a series of instruction types tailored for Knowledge data, which can be applied independently without the need for additional information, as demonstrated in Fig.~\ref{fig:pipeline}(c1).
% For each item in Knowledge Question-Answer pairs, we prompt a powerful LLM to select an instruction type that can be integrated into the question without requiring access to the associated image. The selected instruction is then seamlessly incorporated into the existing question, resulting in an instruction-enhanced question. Subsequently, an LLM is employed to adjust the corresponding answer, ensuring alignment with both the modified question and the original context.

\textit{Enriched Inferential QAs. }
For many inferential QAs, 
the answers lack sufficient detail to fully explain underlying logic and background knowledge. 
To address this, we employ a knowledge-rich LLM to enrich responses with detailed explanations, relevant background information, and logical reasoning (\cref{fig:pipeline}(c2)). 
This refinement enhances alignment with user preferences.


% For \textit{Inferential} data, we found that some of the model's responses lack sufficient detail to fully articulate the underlying logic or background knowledge inherent in questions. 
% For instance, consider a scenario where the model is presented with an image of a street covered in yellow fallen leaves, accompanied by the question, "Which season might this be?" The model may respond succinctly: "It's autumn because the leaves are turning yellow and falling from the trees." While this answer appears clear, it overlooks the explanatory depth required to understand why these leaf changes signify autumn. Specifically, it does not explain the biological processes—such as the cessation of chlorophyll production leading to color change—that occur as part of the seasonal transition.
% To address this gap, we employ a knowledge-rich LLM to supplement the response with relevant background information and logical reasoning(\cref{fig:pipeline}(c2)). The LLM is tasked with enriching the answers by incorporating detailed explanations of the phenomena involved, thereby providing a more comprehensive understanding of the context behind each question and answer pair. This enhancement leads to better alignment with user preferences.

\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{figs/data&bench_combination.pdf}
    \includegraphics[width=\linewidth]{figs/data_combination3.pdf}
    \caption{\textbf{Data distribution of OmniAlign-V.} Our dataset includes a diverse range of tasks, characterized by a more balanced distribution of answer lengths compared to those observed in ALLaVA and ShareGPT4V.}
    \label{fig:data_combination}
    \vspace{-10pt}
\end{figure}

\textit{Quality Improved Infographic QAs. }
For infographic tasks, particularly those involving \textit{Chart} data, 
we observed that even SOTA MLLMs struggle with complex charts and detailed questions.
While GPT-4o excels at explaining background knowledge but often produces inaccurate OCR results, 
SOTA open-source MLLMs~\cite{chen2024far,bai2023qwen} show superior OCR accuracy but lack detailed explanations. 
Therefore, we developed a refinement pipeline to generate responses combining rich background knowledge and accurate OCR results. 
We filter out questions where GPT-4o and SOTA open-source MLLM responses show significant discrepancies in trends. 
For remaining questions, the responses from different MLLMs are merged to produce a final response that is both precise and comprehensive.
The merged answers are further reviewed by human experts to ensure their quality and consistency. More details are demonstrated in \cref{appd: chart post}.


% For \textit{Chart} data, we have observed that even SOTA MLLMs struggle to generate satisfactory responses when faced with complex chart images and detailed questions. 
% GPT-4o tends to excel in explaining background knowledge but frequently produces inaccurate OCR results. 
% Conversely, the best open-source MLLM such as InternVL2~\cite{chen2024far} and Qwen2VL~\cite{bai2023qwen}, are less adept at providing detailed explanations but demonstrate superior performance in OCR accuracy.
% Thus, we propose a post-processing pipeline to generate responses that incorporate both rich background knowledge and accurate OCR results.
% We utilize the current SOTA MLLMs to generate answers, then filter out questions for which these models provide significantly differing answers. Finally, we merge the remaining answers to produce a final response that is both precise and knowledge-rich, details are provided in Appx~\ref{appd: chart post}.
% % Thus, we propose a post-processing filter and merge pipeline to generate high-quality responses that incorporate both rich background knowledge and accurate OCR results.
% The merged answers are further reviewed and supervised by human experts to ensure their quality and consistency.
% This integration leverages the strengths of both models, ensuring that the final response is both accurate and informative.


OmniAlign-V comprises 39K Knowledge QAs, 37K Inferential QAs, 10K Creative QAs, 
38K Instruction-Following (Knowledge) QAs, and 44K Infographic QAs (2K Art, 8K Diagram, 11K Chart, 23K Poster). 
Additionally, we prompt GPT-4o to generate 35K QAs focusing on image details, resulting in a total of 205K high-quality SFT training samples. Examples are shown in \cref{fig:datasample1,fig:datasample2,fig:datasample3,fig:datasample4}.


\input{tables/main}
\input{tables/main_language}
\noindent\textbf{DPO Data Generation. } 
OmniAlign-V's high-quality, human-aligned QA pairs can serve as positive samples for DPO training. 
Inspired by Reject Sampling~\cite{casella2004generalized}, 
we generate negative samples by prompting a LLaVA-Next baseline (generator $G$) trained on LLaVA-Next-778k. 
For each question $Q_i$, the generator produces $N$ responses with high temperature,
$\mathbf{R} = \{r^i_1, r^i_2,... r^i_N\}$. An LLM Judger $J$ then evaluates these responses to select the one that most  deviates from the original question's intent and context as the negative sample $r^i_{Neg}$, ensuring clear contrast between positive and negative samples.


% Direct Preference Optimization(DPO)~\cite{rafailov2024direct} serves as an alternative to Reinforcement Learning from Human Feedback (RLHF) and is currently employed to align MLLM's response with human preferences effectively. 
% Our data comprises high-quality Q-A pairs that exhibit a high degree of alignment with human preferences, which makes it ideal for use as positive samples in DPO training. 
% We propose methods inspired by Reject Sampling~\cite{casella2004generalized} to generate corresponding negative samples that exhibit low quality. The LLaVA-Next-Internlm2.5-7B model trained on LLaVA-Next-778k dataset is employed as our baseline generator $G$. For each question $Q_i$, the generator $G$ randomly generates $N$ responses with high temperature, $\mathbf{R} = \{r^i_1, r^i_2,... r^i_N\}$. Subsequently, an LLM Judger $J$ evaluates these responses to identify the one that most significantly diverges from the original question's intent and context, thereby serving as the negative sample $r^i_{Neg}$:
% \begin{equation}
%     r^i_{Neg}=J(Q_i, [r^i_1, r^i_2, ... r^i_N]),
% \end{equation}

% This process ensures a stark contrast between the positive and the negative samples. Finally, we curate around 150k pairs as OmniAlign-V-DPO.
% This process ensures a stark contrast between the positive samples—characterized by their high alignment with human preferences—and the negative samples, which are intentionally selected for their poor relevance and quality.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/appendix_bench.pdf}
    \caption{\textbf{Samples in MM-AlignBench}.}
    \label{fig:bench}
    \vspace{-10pt}
\end{figure}

\section{MM-AlignBench}

Current benchmarks for assessing multi-modal alignment capabilities are limited. While WildVision~\cite{lu2024wildvision} aims to evaluate human preferences in real-world interactions, 
it employs repetitive and simplistic question formats that inadequately assess response quality, as shown in \cref{fig:badbench}. 
To address this, we developed MM-AlignBench, featuring human-curated images and questions for more nuanced evaluation.

We selected high-quality images from SAM-1B~\cite{kirillov2023segment}, CC-3M-Test~\cite{sharma2018conceptual}, AI2D~\cite{kembhavi2016diagram}, ChartQA~\cite{masry2022chartqa}, and InfographicVQA~\cite{mathew2022infographicvqa}. 
For natural images, we applied our image selection strategy from \cref{sec: filter} to identify 2,000 semantically rich images and combine them with 1,000 carefully selected infographics images. 
GPT-4o was then used to generate diverse questions for these images. 
Human experts reviewed and refined the image-question pairs, filtering out low-quality, repetitive, or contextually weak samples. 
This process resulted in 252 high-quality question-image pairs, 
featuring diverse question types and semantically rich images.
Several examples are shown in \cref{fig:bench}.

For evaluation, we follow WildVision’s approach, using GPT-4o as the judge model to compare model responses with reference responses generated by Claude3V-Sonnet~\cite{Claude3}, reporting winning rates and reward scores.


% Currently, there are limited benchmarks available for assessing multi-modal human-aligned capabilities. One such benchmark, WildVision, aims to gauge human preferences in real-world multimodal interactions. However, it employs repetitive and simplistic question formats that fail to adequately assess the quality of responses, as shown in Fig.\ref{fig:badbench}. To address this limitation and to evaluate our data more effectively, we have developed MM-AlignBench. This new benchmark features a curated set of images and questions selected by humans to provide a more nuanced evaluation framework.

% To select high-quality images for our benchmark, we opted for SAM-1B~\cite{kirillov2023segment}, CC-3M-Test~\cite{sharma2018conceptual}, AI2D~\cite{kembhavi2016diagram}, ChartQA~\cite{masry2022chartqa}, InfographicVQA~\cite{mathew2022infographicvqa} as our image sources. Following the application of the image filter mentioned in\cref{sec: filter} to the images, we initially collected 3,000 images that are rich in semantic information. Subsequently, we employed GPT-4o to generate a diverse set of question types pertaining to the selected images.

% The generated question-image pairs were subsequently evaluated and regenerated by human experts to ensure quality. This process involved filtering out low-quality, similar, or repetitive image-question sets, as well as those that lacked sufficient meaning or context. Following this, we curated 252 high-quality question-image pairs. Each pair includes contextually relevant, diverse-type questions paired with semantically rich and varied images. 
% The evaluation metric is designed as arena following AlpacaEval, where 5 levels of comparison results are defined, which are "Much Better", "Better", "Tie", "Worse", and "Much Worse" respectively.
% Following WildVision, we adapt GPT-4o as the judge model, and compute the winning rate and rewards of input model compared with the reference model Claude-3-Sonnet.




% \begin{figure}[t]
%     \centering
%     \
%     \includegraphics[width=.6\linewidth]{figs/bench_combination.pdf}
%     \caption{Distribution of human-alignedBench.}
%     \label{fig:bench_combination}
% \end{figure}

