\begin{table}[t]\small
    \centering
    \resizebox{.5\textwidth}{!}{%
    \tablestyle{4pt}{1}
    \begin{tabular}{l|l|cc}
    \shline
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Type}} &\textbf{AlpacaEvalv2}  &\textbf{ArenaHard}  \\
                       &   & \multicolumn{2}{c}{\textit{v.s. GPT-3.5/GPT-4}} \\
        \hline
        LLaVANext$^I$ & MLLM    & 29.8 / 3.8 & 21.4 / 4.93  \\
        \rowcolor{gray!20}
        LLaVANext$^I$-OA&MLLM &50.1 / 7.8 &30.4 / 9.33  \\
        InternLM2.5-7B  & LLM        &78.3 / 26.2 & 47.5 / 19.1   \\
        \hline
        LLaVANext$^Q$ & MLLM     &50.6 / 7.0 &54.5 / 18.0 \\
        \rowcolor{gray!20}
        LLaVANext$^Q$-OA& MLLM &77.6 / 18.0 &87.2 / 58.1 \\
        Qwen2.5-32B  & LLM        &92.1 / 37.1 &94.8 / 75.9 \\
    \shline
    \end{tabular}
    }%
    \caption{\textbf{Performance on text-only alignment benchmarks.} OA denotes models trained with OmniAlign-V.
    We present the winning rate against GPT-3.5 and GPT-4 (original setting).  OmniAlign-V can also enhance MLLM's performance on text-only alignment benchmarks.}
    \label{tab: main_lang}
    \vspace{-10pt}
\end{table}