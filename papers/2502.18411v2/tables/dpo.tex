\begin{table}[t]
    \centering
    \resizebox{.48\textwidth}{!}{%
    \tablestyle{4pt}{1.2}
    \begin{tabular}{l|l|ccc|ccccc}
    \Xhline{0.11em}
        \textbf{Model} & \textbf{Stage} &\textbf{MM-AlignBench}  &\textbf{WildVision}\\
        \hline
        LLaVANext$^I$ & SFT            &9.5 / -69.2 &30.4 / -34.2 \\
        \rowcolor{gray!20}
        LLaVANext$^I$ & SFT+DPO        &11.1 / -64.5 & 35.5 / -23.4 \\
        \hline
        LLaVANext$^I$-OA & SFT       &57.1 / +11.1 &29.6 / -31.3   \\
        \rowcolor{gray!20}
        % LLaVANext$^I$-OA & SFT+DPO   &65.5 / +23.2 &35.2 / -22.8  \\
        LLaVANext$^I$-OA & SFT+DPO   &64.3 / +22.4 &41.8 / -10.1  \\
        \hline
        InternVL2-8B   & SFT                &31.4 / -21.8 & 48.6 / +1.4\\
        \rowcolor{gray!20}
        InternVL2-8B   & SFT+DPO            &64.7 / +19.4 & 51.4 / +1.9\\
    \Xhline{0.11em}
    \end{tabular}
    }%
    \caption{\textbf{Performance of applying DPO with OmniAlign-V-DPO}. For models finetuned with human-aligned data, by employing DPO training, the model's alignment with human perference further improved.}
    \label{tab: dpo}
    \vspace{-10pt}
\end{table}