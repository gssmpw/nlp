\section{Related Work}
\subsection{Collaborative Perception}
 Typically, collaborative perception involves fusing information perceived by infrastructure's sensors with vehicle's sensor perception information to enhance the global perception capability of the vehicle's. Based on different collaboration stages, previous work can be broadly categorized into early, intermediate, and late collaboration.
\subsubsection{Early Collaboration and Late Collaboration}
Cooper ____ primarily shares multi-precision LiDAR points, projecting its sparse representation into a compact space, followed by a sparse point cloud object detection network to adapt to low-density point clouds. However, early fusion comes with significant computational overhead. In contrast, late fusion involves directly communicating and fusing perception results from different intelligent agents. TruPercept ____ introduces a trust mechanism for secure message selection, adjusting based on the trust model of the intelligent agent providing perception results. It enhances perception performance beyond the line of sight and at a distance from the vehicle by fusing communication messages, relying on local-verified reports of object detection accuracy. However, late fusion is subjective, and once the perception of a collaborator is compromised, the impact is also shared by the collaborator.

\subsubsection{Intermediate Collaboration}
To strike a balance between perception accuracy and reasoning latency, intermediate fusion methods have been widely explored. V2VNet proposes a graph-based approach, capturing and updating the geographical information of each vehicle iteratively through Convolutional Gated Recurrent Units (ConvGRU). To emphasize the importance of agents, DiscoNet discards highly similar pixels between vehicles through an edge weight matrix and constructs an overall geometric topology through knowledge distillation. To simulate the impact of real-world transmission delays, Who2com ____ proposes a three-step handshake communication protocol, including request, match, and connect, to determine which collaborator to interact with. Additionally, When2com ____ considers a learnable self-attention mechanism to infer whether the self-agent engages in additional communication for more information. Where2comm develops a novel sparse confidence map to mask unimportant elements used for feature compression. Investigating fine-grained and dense predictions for in-vehicle cameras, CoBEVT ____ studies a pure camera map prediction framework below the Bird's Eye View (BEV) plane. This framework utilizes a novel Fusion Axis (FAX) attention to reconstruct dynamic scenes on the ground plane.

Despite the excellent performance of the above algorithms, they are all built on the foundation of high-resolution sensors, without considering the cost issues associated with practical deployment. In this work, we propose a V2I framework that leverages low-resolution sensors to achieve high-precision perception. We further optimize the intermediate features to achieve more efficient bandwidth utilization.

% \subsection{3D Object Detection}
% In 3D object detection, there are roughly three approaches: LiDAR-based, image-based, and multimodal fusion-based methods.

% LiDAR-based 3D object detection methods can be categorized into three main types based on the form of data they process: using raw point cloud data, using voxelized data, and using a combination of point cloud and voxel data. When using raw point cloud data for object detection, it is common to employ PointNet ____ and PointNet++ ____ for direct extraction of features from the raw point cloud data. PointNet ____ processes each point in the input point cloud, learning its corresponding spatial encoding. Subsequently, it aggregates features from all points to obtain a global point cloud feature. PointNet++ ____, an improvement upon PointNet ____, introduces a multi-level feature extraction structure to effectively capture both local and global features. 

% % PointRCNN ____, for the first time, applies point clouds to the task of 3D object detection. Its overall approach involves generating 3D proposals from the bottom up, i.e., directly creating high-quality 3D proposals from point clouds that have already been categorized into foreground and background points. Subsequently, the pooled points of each proposal are transformed into a regular coordinate system to learn better local spatial features. These features are then combined with the previously learned global semantic features for accurate box refinement and confidence prediction.

% Voxel-based detection methods discretize point clouds into a grid and use convolutional networks to process features within each voxel. Pioneering work, such as VoxelNet ____, designs a voxel feature encoding (VFE) layer for point-wise information extraction, followed by 3D convolution for local intermediate feature aggregation. Voxel Transformer ____ introduces sparse voxel modules and submanifold voxel modules, which can effectively compute positions for both empty and non-empty voxels. To further expand the attention range, it proposes two attention mechanisms: local attention and extended attention. Additionally, the Voxel Transformer ____ introduces a fast voxel query to accelerate the query process in multi-head attention. Additionally, it introduces a fast voxel query to expedite the query process in multi-head attention. To enhance inference speed, PointPillars ____ compresses point clouds into a 2D representation and utilizes sparse convolution as a feature extraction network. 

% % The recent research trend involves combining point-wise features with voxel-wise features for 3D object detection. To address challenges such as the loss of local information in point cloud data by algorithms like VoxelNet ____ and the computational intensity of algorithms directly processing point cloud data, as seen in PointNet ____, STD ____ proposes a detection paradigm following a sparse-to-dense approach. This paradigm uses innovative spherical anchors to obtain accurate proposals from raw points and generates a compact representation from sparse point expressions using point pooling.

% In the past two years, there has been significant attention on the 3D object detection based on the pure visual Bird's Eye View (BEV) approach. This all-in-one method has indeed substantially improved the performance of 3D detection algorithms based on cameras, even approaching the capabilities of LiDAR-based solutions. DETR3D ____ extracts 2D features from multiple images, then uses a sparse 3D object query set to index these 2D features, followed by linking the 3D positions to the multi-view images using a camera transformation matrix. BEVFormer ____ is based on a transformer encoder and a multi-layer perceptron encoder with residual connections. It utilizes spatial cross-attention modules for feature extraction from different perspective cameras and employs a temporal self-attention module to extract correlation information between historical and current moment features.

% Camera-LiDAR fusion perception ____ has proven its superiority and garnered significant attention in recent 3D detection. It supplements LiDAR sensors with feature information for distant small targets and provides depth information to camera sensors. 
% % CLOCs ____, prior to Non-Maximum Suppression (NMS) in any 2D and 3D detectors, merges the outputted candidate targets, training on the geometric and semantic consistency of both modalities to produce more accurate final 3D and 2D detection results. 
% % The working principle of PointPainting ____ involves projecting LiDAR points into the output of an image semantic segmentation network and appending category scores to each point. The appended (painted) point cloud can then be fed into any LiDAR-only method. 
% SparseFusion ____ transforms camera candidate boxes to LiDAR coordinate space by decoupling object representations. Subsequently, a lightweight self-attention module is employed to fuse multi-modal candidate boxes in a unified 3D space. In this paper, to strike a balance between efficiency and accuracy, we employ PointPillars ____ as the feature extraction network for LiDAR. Simultaneously, we use resnet50 ____ to extract image features and employ the method proposed in this paper for cross-modal feature fusion, obtaining fused features for subsequent processing.


\begin{figure*}[h]
	\centering
	\includegraphics[height=85mm]{sys_view.pdf}
	\caption{System architecture diagram of LCV2I}
	\label{fig: sys}
\end{figure*}