%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{natbib}
\setcitestyle{numbers,square}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx} 
\usepackage{algpseudocode}
\usepackage{algorithm}  
\usepackage{amsmath}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{LCV2I: Communication-Efficient and High-Performance Collaborative \\ Perception Framework with Low-Resolution LiDAR}
\author{
    %Authors
    % All authors must be in the same font size and format.
%     \thanks{ The authors are with the Fujian Key Lab for Intelligent
% Processing and Wireless Transmission of Media Information,
% College of Physics and Information Engineering, Fuzhou
% University, Fuzhou 350108, China (e-mail: fxx1116@fzu.edu.cn;
% 221127183@fzu.edu.cn;zhenghf@fzu.edu.cn)}
    Xinxin Feng,
    Haoran Sun,
    HaiFeng Zheng
    \thanks{Corresponding author}
   }
   \affiliations{
    %Afiliations
     Fujian Key Lab for Intelligent Processing and Wireless Transmission of Media Information,\\
     College of Physics and Information Engineering, Fuzhou University, Fuzhou 35108, China\\
    E-mails:\{fxx1116,221127183,zhenghf\}@fzu.edu.cn
%
% See more examples next
}
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2}, 
%     % J. Scott Penberthy\textsuperscript{\rm 3}, 
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}

Vehicle-to-Infrastructure (V2I) collaborative perception leverages data collected by infrastructure's sensors to enhance vehicle perceptual capabilities.  
LiDAR, as a commonly used sensor in cooperative perception, is widely equipped in intelligent vehicles and infrastructure. However, its superior performance comes with a correspondingly high cost. To achieve low-cost V2I, reducing the cost of LiDAR is crucial. 
Therefore, we study adopting low-resolution LiDAR on the vehicle to minimize cost as much as possible.
However, simply reducing the resolution of vehicle's LiDAR results in sparse point clouds, making distant small objects even more blurred. Additionally, traditional communication methods have relatively low bandwidth utilization efficiency. These factors pose challenges for us.
To balance cost and perceptual accuracy, we propose a new collaborative perception framework, namely LCV2I.  
LCV2I uses data collected from cameras and low-resolution LiDAR as input. It also employs feature offset correction modules and regional feature enhancement algorithms to improve feature representation. Finally, we use regional difference map and regional score map to assess the value of collaboration content, thereby improving communication bandwidth efficiency.
In summary, our approach achieves high perceptual performance while substantially reducing the demand for high-resolution sensors on the vehicle. To evaluate this algorithm, we conduct 3D object detection in the real-world scenario of DAIR-V2X, demonstrating that the performance of LCV2I consistently surpasses currently existing algorithms.
  
\end{abstract}

\section{Introduction}

The emergence of collaborative sensing technology has resolved the limitations of individual intelligent vehicles in effectively detecting distant small targets and occluded objects.
Currently, commonly used collaborative perception technologies include Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V). As the names suggest, V2I involves deploying sensors on the infrastructure to assist vehicle driving, while V2V utilizes sensors on surrounding vehicles to assist the ego vehicle. Each method has its pros and cons, but V2I, with sensors often deployed at elevated positions on the infrastructure, allows the sensors to assist intelligent vehicles from a higher perspective, providing a broader view to better sense the surrounding environment. This helps overcome the limitations of the single perspective of intelligent vehicles. 

Based on existing V2X simulation datasets such as DAIR-V2X \cite{9879243}, OpenV2V \cite{opv2v}, V2X-Sim \cite{9294660}, and V2XSet \cite{10.1007/978-3-031-19842-7_7}, previous researchers have made significant contributions. Where2comm \cite{Where2comm:22} introduces a collaborative approach based on a spatial confidence map to reduce transmission bandwidth. V2VNet \cite{10.1007/978-3-030-58536-5_36} explores end-to-end learning with source code encoding, and DiscoNet \cite{NEURIPS2021_f702defb} uses 1D convolution to compress messages.  
However, it's important to note that all these approaches were developed under the assumption of high-resolution LiDAR sensors on both vehicles and infrastructure. While high-resolution LiDAR provides higher detection accuracy, it also increases unnecessary costs.

% we consider reducing the resolution of the vehicle's LiDAR to half or even one-fourth of the original resolution. Since the infrastructure is responsible for providing a broad auxiliary field of view, the resolution of the infrastructure's LiDAR remains unchanged. 
To address the issue mentioned above, we consider reducing the resolution of the vehicle's LiDAR by lowering the number of scan lines of LiDAR to half or even one-fourth of the original, while keeping the infrastructure LiDAR's resolution unchanged to provide an auxiliary field of view. However, we will face the following four issues: Firstly, LiDAR has poor perception capability for distant small targets, and this weakness is more pronounced with low-resolution LiDAR, thus requiring additional modality data for supplementation.
Secondly, due to the different fields of view of various sensors for the same area, even though data from different sensors have been projected onto a unified coordinate system, there will still be areas only sensed by a single sensor. 
Therefore, during the fusion process, some areas will be well fused, while others will be insufficiently fused.
Thirdly, as the point cloud becomes sparser, the regional features of the target will become blurred, leading to increased false detections and missed detections.
Finally, since most current collaborative strategies send all auxiliary information to the vehicle, the utilization of communication bandwidth is extremely low.

To address the aforementioned issues, we propose a Low-Cost and high-performance V2I collaborative perception framework (LCV2I), which adopts multi-modal data as input. This framework primarily consists of three key modules:
% \begin{itemize}
%     \item Voxel-Wise Fusion Module (VWF): This module uses pixel features from camera data to complement voxel features on a voxel-by-voxel basis.
%     \item Feature Offset Correction Module (FOCM): This module adjusts the feature weights offset after cross-modal fusion.
%     \item Region Feature Enhancement Algorithm (RFEA): This algorithm enhances the region features of the target to be perceived. Simultaneously, it generates a feature difference matrix to guide the generation of region differences map and optimize the region scoring map to reduce transmission bandwidth.
% \end{itemize}
(1) Voxel-Wise Fusion Module (VWF), which uses pixel features from camera data to complement voxel features on a voxel-by-voxel basis.
(2) Feature Offset Correction Module (FOCM), which adjusts the feature weights offset after cross-modal fusion.
(3) Regional Feature Enhancement Algorithm (RFEA), which enhances the region features of the target to be perceived. Simultaneously, RFEA generates a feature difference matrix to guide the generation of regional difference map and optimize the region scoring map to reduce transmission bandwidth.
The main contributions of our work are as follows:
\begin{itemize}
    \item Considering the high cost of vehicles' sensors, we propose LCV2I, a low-cost, high-bandwidth-utilization, and high-performance collaborative perception framework. LCV2I allows vehicles to be equipped with low-resolution sensors to achieve high-performance 3D object detection, while only transmitting critical features, thereby ensuring high bandwidth utilization.
    \item We propose VWF and FOCM to utilize image features to supplement voxel features, addressing the issue of inaccurate voxel feature generation caused by the reduced resolution of LiDAR.
    % \item We propose  to address the issue of insufficient fusion in certain areas caused by the different viewing ranges of sensors during the fusion process.
    \item We propose RFEA to enhance the regional features of targets while generating a regional difference map to guide the filtering of invalid features during communication. This further improves communication bandwidth utilization, achieving a balance between bandwidth and performance.
    \item To evaluate LCV2I, we conducted a collaborative 3D object detection task on the real-world dataset DAIR-V2X \cite{9879243}. The results indicate that our approach maintains high perceptual accuracy and low transmission bandwidth, even with a significant reduction in sensor resolution.
\end{itemize}
% Firstly, by utilizing infrastructure's sensors to provide a broader auxiliary field of view to all vehicles on the current road segment, we can afford to reduce the resolution of the onboard sensors to half or even a quarter of the original, significantly lowering deployment costs. 
% Secondly, we optimized the content transmitted during the collaboration process by removing unnecessary data. This optimization results in a more precise request map, further reducing the bandwidth required for communication and achieving a balance between bandwidth and performance.










\section{Related Work}

\subsection{Collaborative Perception}
 Typically, collaborative perception involves fusing information perceived by infrastructure's sensors with vehicle's sensor perception information to enhance the global perception capability of the vehicle's. Based on different collaboration stages, previous work can be broadly categorized into early, intermediate, and late collaboration.
\subsubsection{Early Collaboration and Late Collaboration}
Cooper \cite{8885377} primarily shares multi-precision LiDAR points, projecting its sparse representation into a compact space, followed by a sparse point cloud object detection network to adapt to low-density point clouds. However, early fusion comes with significant computational overhead. In contrast, late fusion involves directly communicating and fusing perception results from different intelligent agents. TruPercept \cite{9304695} introduces a trust mechanism for secure message selection, adjusting based on the trust model of the intelligent agent providing perception results. It enhances perception performance beyond the line of sight and at a distance from the vehicle by fusing communication messages, relying on local-verified reports of object detection accuracy. However, late fusion is subjective, and once the perception of a collaborator is compromised, the impact is also shared by the collaborator.

\subsubsection{Intermediate Collaboration}
To strike a balance between perception accuracy and reasoning latency, intermediate fusion methods have been widely explored. V2VNet proposes a graph-based approach, capturing and updating the geographical information of each vehicle iteratively through Convolutional Gated Recurrent Units (ConvGRU). To emphasize the importance of agents, DiscoNet discards highly similar pixels between vehicles through an edge weight matrix and constructs an overall geometric topology through knowledge distillation. To simulate the impact of real-world transmission delays, Who2com \cite{9197364} proposes a three-step handshake communication protocol, including request, match, and connect, to determine which collaborator to interact with. Additionally, When2com \cite{Liu_2020_CVPR} considers a learnable self-attention mechanism to infer whether the self-agent engages in additional communication for more information. Where2comm develops a novel sparse confidence map to mask unimportant elements used for feature compression. Investigating fine-grained and dense predictions for in-vehicle cameras, CoBEVT \cite{pmlr-v205-xu23a} studies a pure camera map prediction framework below the Bird's Eye View (BEV) plane. This framework utilizes a novel Fusion Axis (FAX) attention to reconstruct dynamic scenes on the ground plane.

Despite the excellent performance of the above algorithms, they are all built on the foundation of high-resolution sensors, without considering the cost issues associated with practical deployment. In this work, we propose a V2I framework that leverages low-resolution sensors to achieve high-precision perception. We further optimize the intermediate features to achieve more efficient bandwidth utilization.

% \subsection{3D Object Detection}
% In 3D object detection, there are roughly three approaches: LiDAR-based, image-based, and multimodal fusion-based methods.

% LiDAR-based 3D object detection methods can be categorized into three main types based on the form of data they process: using raw point cloud data, using voxelized data, and using a combination of point cloud and voxel data. When using raw point cloud data for object detection, it is common to employ PointNet \cite{8099499} and PointNet++ \cite{10.5555/3295222.3295263} for direct extraction of features from the raw point cloud data. PointNet \cite{8099499} processes each point in the input point cloud, learning its corresponding spatial encoding. Subsequently, it aggregates features from all points to obtain a global point cloud feature. PointNet++ \cite{10.5555/3295222.3295263}, an improvement upon PointNet \cite{8099499}, introduces a multi-level feature extraction structure to effectively capture both local and global features. 

% % PointRCNN \cite{8954080}, for the first time, applies point clouds to the task of 3D object detection. Its overall approach involves generating 3D proposals from the bottom up, i.e., directly creating high-quality 3D proposals from point clouds that have already been categorized into foreground and background points. Subsequently, the pooled points of each proposal are transformed into a regular coordinate system to learn better local spatial features. These features are then combined with the previously learned global semantic features for accurate box refinement and confidence prediction.

% Voxel-based detection methods discretize point clouds into a grid and use convolutional networks to process features within each voxel. Pioneering work, such as VoxelNet \cite{Zhou_2018_CVPR}, designs a voxel feature encoding (VFE) layer for point-wise information extraction, followed by 3D convolution for local intermediate feature aggregation. Voxel Transformer \cite{9710835} introduces sparse voxel modules and submanifold voxel modules, which can effectively compute positions for both empty and non-empty voxels. To further expand the attention range, it proposes two attention mechanisms: local attention and extended attention. Additionally, the Voxel Transformer \cite{9710835} introduces a fast voxel query to accelerate the query process in multi-head attention. Additionally, it introduces a fast voxel query to expedite the query process in multi-head attention. To enhance inference speed, PointPillars \cite{8954311} compresses point clouds into a 2D representation and utilizes sparse convolution as a feature extraction network. 

% % The recent research trend involves combining point-wise features with voxel-wise features for 3D object detection. To address challenges such as the loss of local information in point cloud data by algorithms like VoxelNet \cite{Zhou_2018_CVPR} and the computational intensity of algorithms directly processing point cloud data, as seen in PointNet \cite{8099499}, STD \cite{9008777} proposes a detection paradigm following a sparse-to-dense approach. This paradigm uses innovative spherical anchors to obtain accurate proposals from raw points and generates a compact representation from sparse point expressions using point pooling.

% In the past two years, there has been significant attention on the 3D object detection based on the pure visual Bird's Eye View (BEV) approach. This all-in-one method has indeed substantially improved the performance of 3D detection algorithms based on cameras, even approaching the capabilities of LiDAR-based solutions. DETR3D \cite{pmlr-v164-wang22b} extracts 2D features from multiple images, then uses a sparse 3D object query set to index these 2D features, followed by linking the 3D positions to the multi-view images using a camera transformation matrix. BEVFormer \cite{10.1007/978-3-031-20077-9_1} is based on a transformer encoder and a multi-layer perceptron encoder with residual connections. It utilizes spatial cross-attention modules for feature extraction from different perspective cameras and employs a temporal self-attention module to extract correlation information between historical and current moment features.

% Camera-LiDAR fusion perception \cite{9726893} has proven its superiority and garnered significant attention in recent 3D detection. It supplements LiDAR sensors with feature information for distant small targets and provides depth information to camera sensors. 
% % CLOCs \cite{10.1109/IROS45743.2020.9341791}, prior to Non-Maximum Suppression (NMS) in any 2D and 3D detectors, merges the outputted candidate targets, training on the geometric and semantic consistency of both modalities to produce more accurate final 3D and 2D detection results. 
% % The working principle of PointPainting \cite{Vora_2020_CVPR} involves projecting LiDAR points into the output of an image semantic segmentation network and appending category scores to each point. The appended (painted) point cloud can then be fed into any LiDAR-only method. 
% SparseFusion \cite{10204403} transforms camera candidate boxes to LiDAR coordinate space by decoupling object representations. Subsequently, a lightweight self-attention module is employed to fuse multi-modal candidate boxes in a unified 3D space. In this paper, to strike a balance between efficiency and accuracy, we employ PointPillars \cite{8954311} as the feature extraction network for LiDAR. Simultaneously, we use resnet50 \cite{7780459} to extract image features and employ the method proposed in this paper for cross-modal feature fusion, obtaining fused features for subsequent processing.


\begin{figure*}[h]
	\centering
	\includegraphics[height=85mm]{sys_view.pdf}
	\caption{System architecture diagram of LCV2I}
	\label{fig: sys}
\end{figure*}




\section{Method}

% This section introduces LCV2I, a collaborative perception framework based on multimodal data with low cost and low communication bandwidth, utilizing intermediate fusion, 
LCV2I is applied in vehicle-road collaborative perception scenarios, enabling better perception results with the assistance of infrastructure even when the vehicle's LiDAR resolution is significantly reduced. The system architecture of LCV2I is shown in Fig. \ref{fig: sys}. Both the vehicle and infrastructure use the same network structure, consisting of the following components: Modality Encoder, VWF, FOCM, RFEA, and Collaborative Perception Module. The VWF module uses image features to supplement the missing feature information from the low-resolution LiDAR, the FOCM module corrects feature offsets generated during fusion, the RFEA module enhances the regional characteristics of features and generates a regional difference map, and the Collaboration Module uses the regional difference map to achieve higher communication bandwidth utilization.

\subsection{Modality Encoder}
The modality encoder extracts features from sensor data. In LCV2I, it uses two modalities of data, namely LiDAR point clouds and RGB images, as inputs. For the LiDAR point cloud data collected from the LiDAR sensor, it is voxelized and then encoded using the PointPillars \cite{8954311}. For the RGB images captured by the camera, they are processed using ResNet50 \cite{he2016deep} to extract features. Additionally, the features are represented from a Bird's Eye View (BEV) perspective, and the perception information from all intelligent agents is projected onto a unified global coordinate system to facilitate better collaboration and avoid complex coordinate transformations.

For the input RGB image $X_c$ and point cloud $X_l$, we can extract their features $F_{img}^{ori}=F_{enc}(X_c)\in\mathbb{R}^{b\times c\times {h_c}\times {w_c}}$ and $F_{voxel}^{ori}=F_{enl}(voxelize(X_l))\in\mathbb{R}^{b\times c\times z\times h\times w}$, where ${F_{enc}}$ and ${F_{enl}}$ are the image feature encoder and voxel feature encoder, respectively, and $b$, $c$, $z$, $h$, $w$, $h_c$ and $w_c$ represent the batch size, number of channels, height, length, and width of voxel features, as well as height and width of image features.
\subsection{Voxel-Wise Fusion module}
% To address the weak perception of distant small targets by low-resolution LiDAR, we use image features to supplement the potentially missing features of distant small targets in the voxels.Additionally, as the collected point clouds become sparse due to the reduced LiDAR beams, resulting in feature deviations in the voxels, we designed a module called Voxel-Wise Fusion module to supplement features voxel by voxel.
To address the weak perception of distant small targets by low-resolution LiDAR, we use image features to supplement the potentially missing features of distant small targets in the voxels. However, due to the sparse point clouds collected by low-resolution LiDAR, the generated voxel features may be biased. Therefore, we designed a module called the Voxel-Wise Fusion module (VWF) to supplement the features voxel by voxel. This module utilizes image features at different scales to complement voxel features, thereby achieving more accurate feature supplementation. As shown in Fig. \ref{fig: VWF}.

\begin{figure}
	\centering
	\includegraphics[height=55mm]{VWF.pdf}
	\caption{Structure of VWF}
	\label{fig: VWF}
\end{figure}

First, we need to obtain image features at different scales. We perform pooling operations on the original image features to obtain $F_{img}^{s_i}$,
\begin{equation}
    F_{img}^{s_i} = maxpool_i(F_{img}^{ori}),
\end{equation}
where, $F_{img}^{s_i}\in\mathbb{R}^{b\times c\times l_{i}\times w_{i}}$ represents the image features at the $i$th scale, $maxpool_i$ denotes the pooling layer used to obtain image features at the $i$th scale, and $F_{img}^{ori}\in\mathbb{R}^{b\times c\times h_c\times w_c}$ represents the original image features extracted from the raw input features.

Next, in order to process voxel features voxel by voxel, we flatten the obtained voxels,
\begin{equation}
    F_{voxel}^{flatten} = flatten(F_{voxel}^{ori}),
\end{equation}
where, $F_{voxel}^{flatten}\in\mathbb{R}^{h*l*w\times b*c}$ represents the voxel features after being flattened into a two-dimensional tensor, $F_{voxel}^{ori}\in\mathbb{R}^{h\times l\times w\times b\times c}$ represents the original voxel features. 

% To efficiently supplement voxel features, we adopted a mechanism similar to attention fusion based on reference points. 
To effectively supplement the voxel features, we employ a mechanism similar to attention-based fusion using reference points.
This method allows each query point to only query the regions of interest, meaning attention is computed by sampling a certain number of points around the reference points. We use each voxel feature block from the flattened voxel features to query image features at different scales, obtaining updated flattened voxel features. Each voxel feature block is updated using the following formula: 
\begin{equation}
    \begin{split}
    CrossATTN(F_{voxel}^{flatten}, F_{img}^{s_i}, P) = \\ \sum\limits_{k=1}^J{{A_k^{s_i}}{W_k^{s_i}}{F_{img}^{s_i}(P+{\delta{P_k^{s_i}}})}},
    \end{split}
\end{equation}


where, $P$ represents the reference point, and $J$ denotes the number of points sampled around the reference point, ${s_i}$ represents the $i$th scale, ${A_k^{s_i}}\in\lbrack0,1\rbrack$ represents learnable attention weights, $W_{k}^{s_i}\in\mathbb{R}^{c\times c}$ represents learnable weights generated from the $F_{img}^{s_i}$, $\delta{P_k^{s_i}}$ represents the predicted offset to the reference point $P$, ${F_{img}^{s_i}(P+{\delta{P_k^{s_i}}})}$ represents the feature at location ${P+\delta{P_k^{s_i}}}$.

After fusing the image features with voxel features at all scales, we obtain the fused features for each scale, and we reshape them back to the original dimensions, obtaining $F_{fused}^{s_i}\in\mathbb{R}^{h\times l\times w\times b\times c}$. Then, we concatenate all the fused features along the channel dimension. Finally, we apply a fully connected layer to obtain the ultimate fused feature,
\begin{equation}
F_{fused}^{s_{all}} = concat(F_{fused}^{s_1}, F_{fused}^{s_2}, ..., F_{fused}^{s_i}),
\end{equation}
\begin{equation}
F_{fused} = {W_F}{F_{fused}^{s_{all}}} + {B_F},
\end{equation}
where, $F_{fused}^{s_{all}}\in\mathbb{R}^{h\times l\times w\times b\times i*c}$ represents the feature obtained by concatenating all the fused features across different scales, $F_{fused}\in\mathbb{R}^{h\times l\times w\times b\times c}$ represents the ultimate fused feature, $W_F$ and $B_F$ represent the learnable parameters in the linear layer.

\subsection{Feature Offset Correction Module}
When supplementing voxel features, image features are queried based on $J$ sampled points around the reference point. Therefore, if the LiDAR perceives an object at a certain location while the camera may fail to perceive it due to occlusion or field of view issues, the reference point will be inaccurate. Consequently, the attention scores for querying at this location become relatively lower than elsewhere. We refer to this discrepancy as feature offset.

% Although both locations on the same feature map contain target features, the updated feature values for the target feature with the mentioned issue will be lower than the updated feature values for the target feature at another location without such issues. 

 \begin{figure}[h]
	\centering
	\includegraphics[height=36mm]{focm.pdf}
	\caption{Structure of FOCM}
	\label{fig: focm}
\end{figure}

To address this issue, we designed the Feature Offset Correction Module (FOCM), as shown in Fig. \ref{fig: focm}. We obtain respective weight matrices, $W_{voxel}$ and $W_{fused}$, for the original voxel feature $F_{voxel}^{ori}$ and the fused voxel feature $F_{fused}$ through a weight matrix generator. The weight matrix generator consists of a fully connected layer that is used to obtain the attention scores of the feature maps. We then multiply the original voxel feature and the weight matrix for fused voxel feature, as well as the fused voxel feature and the weight matrix for the original voxel feature. This yields two feature maps with weighted scores. Subsequently, these two feature maps are concatenated and passed through a linear layer for full connection, resulting in the corrected feature $F_{oc}$. The mathematical expression is as follows:
\begin{equation}
    F_{oc}= W_{oc}(F_{voxel}W_{fused} \copyright  F_{fused}W_{voxel})+B_{oc},
\end{equation}
 where $F_{oc}$ represents the corrected feature, $W_{fused}$ denotes the feature weight matrix generated from cross-modal fusion features, $W_{voxel}$ represents the feature weight matrix generated from the original voxel features, {\copyright} denotes concatenation, and $W_{oc}$ and $B_{oc}$ represent trainable parameters in the linear layer, $W_{voxel}$ and $W_{fused}$ represent the weight matrices generated by the original voxel features and the fused voxel features, respectively.

 \subsection{Regional Feature Enhancement Algorithm}
Due to a significant reduction in the laser LiDAR's resolution at the vehicle, it results in a decrease in point cloud density, leading to the regional features becoming indistinct. Therefore, we propose the Regional Feature Enhancement Algorithm (RFEA) to address this issue. RFEA first applies Gaussian filtering to the feature map. Subsequently, we calculate the difference between each point and its surrounding points, obtaining a difference matrix with the same feature scale as the original. 
% We then assess this difference matrix, setting the value at a particular location to 1 when it exceeds a certain threshold. 
Then we assess this difference matrix and empirically set a threshold, which is determined as the optimal value through repeated experiments. When the value at a particular location exceeds this threshold, we set it to 1.
Conversely, when it is below the threshold, we set the value at that location to zero. The algorithmic process is shown in Algorithm \ref{alg:RFEA}. Afterwards, we obtain the regional difference map $M_d$$\in\left(0,1\right)$. 
\begin{algorithm}[h]
\caption{RFEA}\label{alg:RFEA}
\begin{algorithmic}[1]
\Require
feature map $F_{oc}$
\Ensure
regional difference map $M_d$ 

\State Smoothing the input feature map with Gaussian filtering.
\State Determine whether the current point $P_c$ has one or more of the neighboring points, including the top $P_t$, bottom $P_b$, left $P_l$, and right $P_r$ neighbors, and the corresponding values are $V_c, V_t,V_b, V_l, V_r$.
\State Create an empty tensor $F_{RFEA}^{empty}$ with the same size as the input feature map $F_{oc}$.
\While{not all points on $F_{oc}$ have been traversed}
\If{having neighboring points}
\State the values of the missing neighboring points are set to zero.
\State $V_d = \frac{|(V_c-V_t)|+|(V_c-V_b)|+|(V_c-V_l)|+|(V_c-V_r)|}{4}$
\ElsIf{no neighboring points}
\State $V_d = V_c$
\EndIf
\If{$V_d>V_{thres}$}
\State $V_d=1$
\ElsIf{$V_d<V_{thres}$}
\State $V_d=0$
\EndIf
\State Place $V_d$ into $F_{RFEA}^{empty}$ at the corresponding position of $P_c$.
\EndWhile
\State Obtain the regional difference map $M_d$
\end{algorithmic}
\end{algorithm}



\subsection{Collaborative Perception Module}
To address the issue of limited bandwidth while still needing to transmit large amounts of data in collaborative perception, we designed a feature selection mechanism to efficiently utilize the bandwidth.

\textbf{Feature selection.} First, we utilize a regional score map generator $\Phi_{gen}(\cdot)$. We input the obtained BEV feature map $F_{oc}$ into $\Phi_{gen}(\cdot)$ to generate confidence scores for each position in the feature map. Regions with confidence scores greater than a certain threshold are considered high-confidence areas, and all values in those regions are set to 1. Conversely, regions with confidence scores less than the threshold are considered low-confidence areas, and all values in those regions are set to 0.
The setting of this threshold is based on experience.
The binary map obtained as described above is referred to as the regional score map $M_s$. Then, we complement the regional difference map $M_d$ obtained from RFEA with $M_s$ to obtain the regional confidence map $M_{sd}$, which guides where the features should be transmitted, the principle of $M_{sd}$ is illustrated in Fig. \ref{fig: difference_map_feature}. 

\begin{figure}[h]
	\centering
	\includegraphics[height=35mm]{difference_map_feature.pdf}
	\caption{Regional difference map feature}
	\label{fig: difference_map_feature}
\end{figure}


\textbf{Communication process.} At the beginning of communication, the vehicle needs to convey its own requirements before the infrastructure can accurately assist the vehicle in perception. The overall communication process is illustrated in Fig. \ref{fig: comm}. Therefore, based on $M_{sd}$, we will utilize the following equation to obtain the request map $M_{re}$: 
\begin{equation}
    M_{re}=1-M_{sd}.
\end{equation}

\begin{figure}[h]
	\centering
	\includegraphics[height=48mm]{comm.pdf}
	\caption{Diagram of cooperative perception}
	\label{fig: comm}
\end{figure}



During the communication process, the vehicle first sends the request map $M_{re}$ to the collaborator. The collaborator then compares the received request map $M_{re}$ with its own regional confidence map $M_{sd}$ to identify the features that can satisfy the vehicle's request for transmission:

\begin{equation}
    F_{comm}^{I \to V}=F_{oc}^I \times (M_{re}^V \odot M_{sd}^I),
\end{equation}
where $F_{comm}^{I \to V}$ represents the feature map that satisfies the perception request of the vehicle, with $I \to V$ indicating that the feature map is sent from the infrastructure to the vehicle, $F_{oc}^I$ is the unprocessed feature map at the infrastructure, $M_{re}^V$ denotes the request map of the vehicle, and $M_{sd}^I$ represents the regional confidence map at the infrastructure, $\odot$ represents performing the XNOR operation on two matrices.

\textbf{Message fusion.} After receiving $F_{comm}^{I \to V}$ from the infrastructure, it is further supplemented and fused with the local feature map $F_{oc}^V$ of the vehicle to obtain the final feature map $F^V$. The specific process is as follows:
\begin{equation}
    F^V = SA(stack(F_{comm}^{I \to V},F_{oc}^V)),
    % F_{comm}^{I \to V}=F_{RFEA}^I \times (M_{re}^V \oplus M_{sd}^I),
\end{equation}
where stack represents stacking the two feature maps, and $SA(\cdot)$ represents using the self-attention method to integrate the vehicle feature maps into the infrastructure's feature maps.

\textbf{Object detection.} Use the decoder to decode the features into objects,  including class and regression output.
\begin{equation}
    {\mathcal{O}^V}=\phi_{dec}(F^V).
\end{equation}
Through the aforementioned selection of transmitted features, unnecessary features for the vehicle are removed, thereby transmitting only the features of interest to the vehicle, achieving efficient utilization of communication bandwidth. 

\subsection{Loss Function}
To train the overall system, we supervise two tasks during training: the generation of the regional score map and object detection. Our regional score map generator reuses the parameters of the detection decoder. Supervised with one detection loss, the overall loss is $L=L_{det}(\mathcal{O}^V,\mathcal{O})$, where $\mathcal{O}$ is vehicle's ground-truth objects, $L_{det}$ is the detection loss \cite{DBLP}.

\section{Experiment and Analysis}
We conducted 3D object detection tasks on the DAIR-V2X dataset in the real-world scenario to validate the performance of our work. The detection results are evaluated using the Average Precision (AP) at Intersection over Union (IoU) thresholds of 0.50 and 0.70. Communication results are calculated in bytes on a logarithmic scale (base 2) for message size. To fairly compare communication results intuitively, we do not take into account any additional data/feature/model compression.
\subsection{Dataset and Experimental Setup}
DAIR-V2X \cite{9879243} is a publicly available collaborative perception dataset collected in real-world environments. The dataset consists of two main components: data captured from the perspective of intelligent vehicles, including images and point cloud data, and data captured from the perspective of infrastructure's sensors, also including images and point cloud data. The perception range covers an area of 201.6m × 80m.

The original DAIR-V2X dataset did not annotate targets outside the field of view. Therefore, we utilized the version of DAIR-V2X re-annotated by Yifan Lu and others \cite{lu2023robust}, which covers a 360-degree detection range, as our experimental dataset. We represent the field of view as a Bird's Eye View (BEV) image with dimensions of (200, 504, 64), and a precision of 0.4m per pixel for both length and width.

% We will compare with the following collaborative perception algorithms under different vehicle's LiDAR precisions, which are
% \begin{itemize}
% \item 
% V2VNet \cite{10.1007/978-3-030-58536-5_36}: V2VNet proposes a new Perception and Prediction (P$\&$P) model, utilizing a spatial Graph Neural Network (GNN) to aggregate information received from all nearby SDVs, intelligently combining information from different time points and perspectives in the scene.
% \item 
% V2X-ViT \cite{10.1007/978-3-031-19842-7_7}:V2X-ViT uses ViT (Vision Transformer) methods to capture the interaction relationships between agents and the spatial relationships of each agent. This is used to address issues such as information sharing asynchrony and pose estimation heterogeneity.
% \item 
% DiscoNet \cite{NEURIPS2021_f702defb}:DiscoNet employs a knowledge distillation model compression method to facilitate better feature extraction and aggregation. It also introduces a trainable edge weight matrix, allowing vehicles to adaptively highlight regions of information.
% \item 
% Where2comm \cite{Where2comm:22}:Where2comm introduces a collaborative approach based on a spatial confidence map to reduce transmission bandwidth. 
% \item 
% BM2CP \cite{pmlr-v229-zhao23a}:BM2CP utilizes LiDAR and cameras to achieve efficient multimodal perception. It employs LiDAR-guided modality fusion, collaborative depth generation, and modality-guided intermediate fusion to achieve deep interactions between the modalities of different agents.

% \end{itemize}

We compared the cooperative perception algorithms under different vehicle LiDAR precisions. Among them, V2VNet, V2X-ViT \cite{10.1007/978-3-031-19842-7_7}, and DiscoNet are unimodal algorithms with point cloud data as input. Where2comm is also a unimodal algorithm, but it supports point cloud or image data as input; here, we use point cloud data as input. BM2CP \cite{pmlr-v229-zhao23a} is a multimodal algorithm, using point cloud and image data as input for the model.

% The detection results are evaluated by AP@0.5 and AP@0.7. The communication results count the message size by byte in log scale with base 2. To compare communication results straightforward and fair, we do not consider any extra data/feature/model compression.


\subsection{Quantitative Evaluation}
Tables \ref{table: diffbeam} demonstrate the performance of various collaboration methods under different accuracies of vehicle's LiDAR, especially under conditions of the original resolution, half of the original resolution, and one-fourth of the original resolution. The results indicate that our method performs well under different LiDAR accuracies. Under the condition of LiDAR accuracy being the original resolution, compared with state-of-the-art methods, our performance has improved by 2.55\% in AP@0.5 and 1.27\% in AP@0.7. When the resolution is reduced by half, we achieve performance improvement by 3.18\% in AP@0.5 and 2.99\% AP@0.7. When the resolution is reduced by one-fourth, we also achieve performance improvement by 4.40\% in AP@0.5 and 3.65\% AP@0.7.

At the same time, the above data also indicates that when LiDAR accuracy decreases, other methods are significantly affected. This is shown in the figures as a sharp drop in target detection accuracy as LiDAR accuracy declines. However, our method, although affected by reduced sensor accuracy, not only maintains a lead in accuracy compared to other methods, but also does not experience a sharp decline.

We believe that when other methods reduce the accuracy of LiDAR, resulting in a sparser point cloud and insufficiently clear regional features of the target, the detection capability for distant small objects significantly decreases. In contrast, our approach addresses this issue by introducing camera assistance and enhancing the regional features of the target. This is why our method exhibits such performance.


\begin{table}[h]
% calculated using a formula.}
\centering
	\scalebox{0.71}{
 \begin{tabular}{|c|c|c|c|c|}
    \hline
    Method & Comm & AP@0.5 & AP@0.7 \\
    \hline
    No Collaboration  & 0 & 0.5009 / 0.4892 / 0.4253 & 0.4359 / 0.4104 / 0.3997 \\  
    \hline
    V2VNet  & 24.21 & 0.5625 / 0.5511 / 0.4918 & 0.4304 / 0.4023 / 0.3877 \\ 
    \hline
    V2X-ViT  &22.62 &0.5394 / 0.5238 / 0.5008 &0.4329 / 0.4179 / 0.4084 \\
    \hline
    DiscoNet  & 22.62 & 0.5411 / 0.5299 / 0.5038 & 0.4332 / 0.4186 / 0.4037 \\ 
    \hline
    Where2comm  & 22.62 & 0.6479 / 0.6286 / 0.5981 & 0.4955 / 0.4750 / 0.4592 \\ 
    \hline
    BM2CP  &   22.62   &  \textbf{0.6762} / 0.6474 / 0.6156    &  0.5068 / 0.4901 / 0.4726      \\ 
    \hline
    LCV2I(Ours)  & 22.62 & 0.6734 / \textbf{0.6604} / \textbf{0.6421} & \textbf{0.5082} / \textbf{0.5049} / \textbf{0.4957} \\ 
    \hline
\end{tabular}
}
\caption{The performance of different methods under various vehicle's LiDAR precisions. "Comm" represents the communication bandwidth, and in AP@0.5 and AP@0.7, from left to right, the LiDAR precisions are the original resolution, half the original resolution, and a quarter of the original resolution.} 
\label{table: diffbeam}
\end{table}

% We believe that when other methods reduce the accuracy of LiDAR, resulting in a sparser point cloud and insufficiently clear regional features of the target, the detection capability for distant small objects significantly decreases. In contrast, our approach addresses this issue by introducing camera assistance and enhancing the regional features of the target. This is why our method exhibits such performance.

Additionally, we introduced Gaussian noise into the network to test the robustness of LCV2I. In this experiment, we selected only BM2CP and Where2comm, whose accuracy is similar to LCV2I, for comparison. The accuracy of the other models differs significantly from LCV2I, so robustness experiments were not conducted for them. During this experiment, our LiDAR resolution is maintained at one-quarter of the original resolution, as shown in Fig. \ref{fig: noise}.

\begin{figure}[h]
	\centering
	\includegraphics[height=35mm]{AP_noise.pdf}
	\caption{Robustness test with LiDAR at one-quarter of the original resolution}
	\label{fig: noise}
\end{figure}

From Fig. \ref{fig: noise}, it can be seen that when Gaussian noise is introduced, the detection accuracy of all models decreases. However, when the noise standard deviation exceeds 0.4, under an IoU threshold of 0.5, the AP of LCV2I decreases more slowly compared to the other two models. Additionally, under an IoU threshold of 0.7, while the AP decrease rate of LCV2I is similar to BM2CP, it is still slower than that of Where2comm.

Therefore, compared to existing methods that rely on high-resolution LiDAR for vehicles and infrastructure, LCV2I not only reduces costs and improves detection accuracy but also offers better robustness.





\subsection{Ablation study}
As shown in Table \ref{table: ablation}, we use Where2comm as the baseline. With the original LiDAR accuracy reduced to one-fourth on the vehicle side, the baseline achieves an accuracy of 59.81\%AP@0.5 and 45.92\%AP@0.7. Introducing camera data as a supplement and using VWF to fuse data from these two sensors, we obtain accuracy gains of 2.36\%AP@0.5 and 0.18\%AP@0.7. Subsequently, by incorporating FOCM to correct the fused features, our work achieves accuracy gains of 1.10\%AP@0.5 and 2.11\%AP@0.7. Finally, building upon the aforementioned, we employ RFEA to enhance the regional features of the targets, observing optimal performance and obtaining accuracy gains of 0.94\%AP@0.5 and 1.72\%AP@0.7.

\begin{table}[h]
\centering
	\scalebox{0.8}{
 \begin{tabular}{|ccc|c|c|c|}
    \hline
    VWF  & FOCM & RFEA &LiDAR resolution & AP@0.5 & AP@0.7 \\
    \hline
     &  &  & Ori/4  &0.5981 &0.4592 \\
    \hline
    $\surd$ &  &  & Ori/4  & 0.6217 & 0.4574\\
    \hline
    $\surd$ & $\surd$ &  & Ori/4  & 0.6327 & 0.4785 \\
    \hline
    $\surd$ & $\surd$ & $\surd$ & Ori/4  & 0.6421 & 0.4957 \\
    \hline
\end{tabular}
}
\caption{Performance comparison of various approaches with the vehicle LiDAR restricted to quarter of original resolution in DAIR-V2X.}
\label{table: ablation}
\end{table}

Table \ref{table: Communication Strategies} demonstrates the impact of different communication strategies on detection performance under the same LiDAR accuracy at the vehicle end. The results indicate that when our framework's communication strategy uses only the confidence map proposed by Where2comm, the performance is 0.96\% lower compared to using both regional score map and regional difference map under the same communication bandwidth in AP@0.5. In other words, our communication strategy can achieve higher performance with the same communication bandwidth. Without using our communication strategy, achieving similar performance would require a higher communication bandwidth.


\begin{table}[h]

\centering
	\scalebox{0.8}{
 \begin{tabular}{|c|c|c|c|c|}
    \hline
    Method & LiDAR resolution & Comm & AP@0.5 & AP@0.7 \\
    \hline
    Where2comm & Ori/4 & 22.62 & 0.5981 & 0.4592 \\
    \hline
    ours' & Ori/4 & 22.62 & 0.6325 & 0.4703 \\
    \hline
    ours & Ori/4 & 22.62 & \textbf{0.6421} & \textbf{0.4957} \\
    \hline
\end{tabular}
}
\caption{Performance comparison of various approaches with the vehicle LiDAR restricted to quarter of original resolution in DAIR-V2X.}
\label{table: Communication Strategies}
\end{table}



\subsection{Qualitative evaluation}
Finally, we conducted qualitative experiments to demonstrate the effectiveness of our work. We selected the same road section with the most complex vehicle conditions to showcase our results. We compared our algorithm with Where2Comm and DiscoNet. From Fig. \ref{fig: scene1}, it can be observed that while the other two methods also achieved good results, their detection capabilities for distant targets decreased significantly as the accuracy of the LiDAR decreased. However, our approach, which incorporates a camera sensor and utilizes region-based feature enhancement algorithms, exhibits stronger detection capabilities for distant targets even under significant reduction of the sensor's resolution. 

\begin{figure}[ht]
	\centering
	\includegraphics[height=45mm]{scene1.pdf} %45
	\caption{Visual comparison of detection results}
	\label{fig: scene1}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[height=18mm]{feature_map.pdf} %18
	\caption{Visual comparison of feature maps}
	\label{fig: feature map}
\end{figure}


Additionally, we compared the feature maps processed with the confidence map proposed by Where2comm with the feature maps processed with both regional  score map and regional difference map simultaneously. The results, as shown in the Fig. \ref{fig: feature map}, reveal that the original feature maps are relatively blurry, and the region-based features of the targets are not prominent enough. Applying the confidence map proposed by Where2Comm did not significantly improve the situation. However, when we further processed the feature maps using our REFA-generated regional difference map, it is evident that the region-based features of the targets in our method are more prominent. Moreover, irrelevant information has been removed, resulting in a relatively sparse overall feature map. This further saves transmission bandwidth.



\section{Conclusion}
In this paper, we introduce LCV2I, a framework that enhances perception by incorporating data from the vehicle's camera. Leveraging the semantic information from both the camera and LiDAR sensors under the influence of VWF, the framework enables better perception of distant small targets. To address feature misalignment caused by insufficient fusion, we propose FOCM to correct feature offset. Additionally, to tackle the issue of regional feature blurring in data collected by low-resolution LiDAR, we design RFEA. Utilizing the regional feature map obtained from RFEA, we further optimize the communication strategy. Our framework, using low-resolution sensors, achieves high detection performance, significantly reducing costs. Experimental results on the DAIR-V2X dataset demonstrate that our proposed LCV2I achieves higher detection accuracy and lower deployment costs compared to existing algorithms. 
% Future work will continue to explore collaborative perception under adverse conditions, such as modalities missing.

% \textbf{Limitaions and future work.} Although our work has achieved high detection accuracy, it also has some limitations. The use of a cross-attention mechanism to fuse multi-modal data at multiple scales results in slower inference times. Additionally, due to the complexity of road conditions, sensor heterogeneity is inevitable, but our work cannot handle this situation effectively.








\bibliography{aaai25}
% \bibliographystyle{plain}

\end{document}
