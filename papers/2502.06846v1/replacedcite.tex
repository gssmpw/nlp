\section{Related work}
\subsection*{Protein Representation Learning}

Proteins are fundamental components of cells, essential for their biological activities and diverse functions. Previous studies on protein characterization have explored various methods to learn protein representations based on different forms of protein information. Protein sequences, often referred to as the "language of life," have been extensively studied using advanced natural language processing techniques. For instance, Tranception____ employs the Transformer____ model to encode amino acid sequences, capturing relationships between residues and autoregressively reconstructing protein sequences from large-scale databases. Similarly, sequential modeling approaches such as ESM____ leverage masked language modeling (MLM) to develop attention patterns that correspond to residue-residue contact maps, enhancing sequence-based protein representations.
On the other hand, structure-based approaches____ directly indicates protein function and encodes geometric information of the protein for topology-sensitive tasks such as protein property prediction. Foldseek____ introduces the concept of a structural alphabet, encoding protein structures into a discrete representation space. Similarly, Saprot____ introduces a structure-aware vocabulary, embedding structural information into model inputs to enhance representational capacity, achieving significant success in protein function prediction tasks.


\subsection*{Multi-Modal Alignment}


Enabling large language models (LLMs) to understand additional modalities has been a rapidly evolving research direction, with notable examples including image-text models____, video-text models like VideoLlama____, audio-text models such as Macaw-LLM____, and molecular-text models, including ReLM____, and MolTC____. This line of research was pioneered by advancements in visual language modeling (VLM), which has been successfully applied to tasks such as few-shot image classification, image captioning, and image Q\&A____.
To enable LLMs to understand images, leading VLM methods adopt different strategies. Some, like BLIP-2____, employ nonlinear and expressive cross-modal projectors, while others, such as PaLM-E____, utilize visual encoders and fine-tune LLMs on multimodal datasets. These approaches have also been increasingly applied to protein analysis and understanding. For instance, Galactica____ leverages scientific literature to model protein sequences and SMILES representations, enabling the model to interpret sequence properties. 
ProtNote____, utilize two encoders to encode protein sequence information and text information, respectively, and Multilayer Perceptron (MLP) to fuse these inputs, achieving the goal of using text to achieve supervised and zero-shot protein functional prediction. 
ProteinChat____ further uses the protein structure and the corresponding description to model for the protein Q\&A tasks.
ProtChatGPT____ uses two independent modules to encode sequence and structural information respectively, and aligns them with natural language to explore protein functions. 
InstructBioMol____ integrates multimodal biomolecular inputs through a feature extraction module, allowing researchers to articulate design goals in natural language. 
InstructProtein____ employs a supervised training framework based on instruction generation using knowledge graphs, enabling bidirectional generation between natural language and protein language. This model can predict protein functional descriptions and generate protein sequences based on natural language prompts. Prot2Text____ combines graph neural networks (GNNs) with LLMs to predict protein functions in free-text form. Similarly, ProtT3____ provides an efficient protein-to-text generation framework by integrating protein language models (PLMs) with LLMs. FAPM____ utilizes a contrastive learning framework to implement a generative-like Q\&A model, while BiomedGPT____ bridges the gap between biological and human natural language through a multimodal generative pre-trained transformer (GPT). This allows users to "communicate" with biological modalities using free-text queries, facilitating interactions with biological data in natural language. 



\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|ccr}
        \hline
        Model & Input & Output \\
        \hline
        Prot2Text & Sequence & Structured Text  \\
        ProtNote & Sequence\&Description & Function\&Probability    \\
        ProtChatGPT & Structure\&Sequence\&Question & Free Text  \\
        Evola & Structure\&Sequence\&Question & Free Text  \\
        InstructProtein & Sequence\&Question & Structured Text  \\
        FAPM & Sequence & Structured Text  \\
        BioMedGPT & Sequence\&Question & Free Text  \\
        Prot2Chat & [Structure+Sequence]\&Question & Free Text  \\
        \hline
    \end{tabular}
    \caption{Comparison of input-output of protein Q\&A models, "Description" is a text related to protein functions, such as the description of Gene Ontology (GO) terms, "[Structure + Sequence]" represents the early fusion of structure and sequence. }
    \label{tab:base models}
\end{table*}


\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|cccr }
        \hline
        Dataset & Train & Valid & Test\\
        \hline
        Mol-Instructions & 404640 & 16859 & 11072 \\
        UniProtQA & 25820 & 1075 & 6734  \\
        \hline
    \end{tabular}
    \caption{The count and division of the datasets we used.}
    \label{tab:dataset} 
\end{table}

\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{p{0.95\textwidth}}
    \hline
    \textbf{Prompt} \\ 
    Please select the sentence that is closest and most accurate in meaning to the Target sentence and rank sentences A, B, and C accordingly, with the first place meaning the most identical and accurate in meaning to the Target. Only provide the answer, for example, `B A C' or `A C B', etc. \\
    Target: \{Ground Truth\} A: \{Model1 Generated Text\} B: \{Model2 Generated Text\} C: \{Model3 Generated Text\} \\ \hline
    \end{tabular}
    \caption{Prompt to KIMI for evaluation}
    \label{tab:prompt}
\end{table*}



\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|c|c|c|c}
    \hline
        Model & Bleu-2 & Rouge-1 & Rouge-2 & Rouge-L    \\ 
        \hline
        LLaMA3-8B-Instruct & 4.84 & 23.07 & 4.31 & 15.36  \\ \hline
        LLaMA3-FT with sequence& 6.42 & 24.50  & 6.32 & 17.03\\ \hline
        BioMedGPT-LM-10B & 1.02 & 10.93 & 1.57 & 7.84  \\ \hline
        KIMI (zero-shot)& 4.79 & 22.21 & 4.62 & 14.70  \\ \hline
        KIMI (few-shot) & 12.05 & 31.21 & 11.38 & 24.18   \\ \hline
        
        Prot2Chat  & \textbf{33.25}  & \textbf{54.88} & \textbf{35.26} & \textbf{47.90}  \\ \hline
       
        
        w/o fine-tuned LLM& 12.87 & 30.89 & 14.67 & 26.81  \\ \hline
        w/o Protein sequence & 31.88 & 52.90 & 33.43 & 46.17 \\ \hline
    \end{tabular}
    \caption{Result in Mol-instructions protein oriented dataset, the best performances are marked in bold.}
    \label{tab:result1}
\end{table*}

\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|c|c|c|c}
    \hline
        Model & Bleu-2 & Rouge-1 & Rouge-2 & Rouge-L    \\ 
        \hline
        LLaMA3-8B-Instruct & 2.41  & 13.66 & 1.10  & 9.07  \\ \hline
        BioMedGPT-LM-10B & 5.80  & \textbf{15.88} & 7.32 & 14.68  \\ \hline
        Prot2Chat (zero-shot)& 2.11 & 13.78 & 1.51 & 9.64  \\ \hline
        
        Prot2Chat (fine-tuned) & \textbf{5.87} & 15.65 & \textbf{9.00}  & \textbf{14.99}  \\ \hline
    \end{tabular}
    \caption{Result in UniprotQA dataset, the best performances are marked in bold.}
    \label{tab:result2}
\end{table*}

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|l|l|l|c}
    \hline
        
        Model & 1st & 2nd & 3rd & Avg.  \\ \hline
        Prot2Chat & 591 & 386 & 22 & 1.43  \\ \hline
        BioMedGPT-LM-10B & 218 & 214 & 567 & 2.35  \\ \hline
        LLaMA3-FT with sequence  & 190 & 399 & 410 & 2.22  \\ \hline
    \end{tabular}
    \caption{Results of KIMI evaluation, "Avg." represents the average ranking.}
    \label{tab:select1}
\end{table}


\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|l|l|l|c}
    \hline
        Model & 1st & 2nd & 3rd & Avg.  \\ \hline
        Prot2Chat & 454 & 115 & 78 & 1.42  \\ \hline
        BioMedGPT-LM-10B & 157 & 203 & 287 & 2.20  \\ \hline
        LLaMA3-FT with sequence & 36 & 329 & 282 & 2.38  \\ \hline
    \end{tabular}
    \caption{Results of expert manual evaluation, "Avg." represents the average ranking.}
    \label{tab:select2}
\end{table}

\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{2} 
    \begin{tabular}{m{0.2\textwidth}|m{0.32\textwidth}|m{0.18\textwidth}|m{0.15\textwidth}}
        \hline
        Inputs & LLaMA3-FT with sequence & Prot2Chat & Ground Truth \\
        \hline
        \begin{minipage}[c][0.1\textwidth][c]{0.2\textwidth}
            \begin{center}
            
            \includegraphics[width=\linewidth]{location.png} \\
            \textbf{[O58787]}Using the provided structure and amino acid sequence of a protein, estimate its primary location within the cell: 
            \end{center}
        \end{minipage} & Based on the provided amino acid sequence, it appears that the protein is a membrane protein, likely \color{red}localized to the endoplasmic reticulum, Golgi apparatus, and/or plasma membrane.\color{black} The presence of multiple transmembrane domains and the distribution of hydrophobic and hydrophilic residues suggest that \color{red}this protein is an integral membrane protein.\color{black} Additionally, the sequence contains several potential phosphorylation sites and potential N-glycosylation sites, which may be involved in protein-protein interactions and/or post-translational modification. & The protein characterized by the structure and amino acid sequence demonstrates rRNA binding and is implicated in the translation. Its subcellular \color{blue}localization is primarily within the cytosol. & Upon analyzing the given protein structure and sequence, it appears that the protein is likely to be \textbf{localized in the cytosol compartment of the cell}. \\
        \hline
        \begin{minipage}[c][0.01\textwidth][c]{0.2\textwidth} 
            \centering
            \includegraphics[width=0.8\linewidth]{function.png} \\
            \textbf{[O58736]}Using the protein structure and the following protein sequence as input, assess its potential function by comparing it to known proteins with similar sequences or structures: 
        \end{minipage}  &The provided protein sequence is approximately 240 amino acids long and shows no significant similarity to known proteins in the database. However, it can be analyzed for potential functional regions and motifs using various bioinformatic tools. Here are some observations and predictions: 1. \color{red}**Transmembrane regions**\color{black}: The sequence contains several regions that are \color{red}predicted to be transmembrane,\color{black} including the following: * 24-45 amino acids (ILLLVGMPGSGKGEVAKAFRRRGIPVI) * 63-82 amino acids (MGGDAIREEAEKRGIPKTPEGLKYVSLKVREE) & With the provided structure and amino acid sequence, the protein is expected to \color{blue}display adenylate kinase activity and participate in the phosphorylation, ATP biosynthetic process, phosphorylation. & The protein with the structure and amino acid sequence has been analyzed, and its structure suggests a potential \textbf{involvement in ATP binding}.\\
        \hline
    \end{tabular}
    \caption{Case study. The bold font represents the key points of the answers, the blue font represents the correct answers of the key points, and the red font represents the incorrect answers of the key points.}
    \label{tab:example}
\end{table*}