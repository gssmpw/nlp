
\typeout{IJCAI--24 Instructions for Authors}

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai24}

\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage{amsfonts}

\usepackage{array} 
\usepackage{tabularx}



\urlstyle{same}


\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure}

\author{
Zhicong Wang$^1$
\and
Zicheng Ma$^{2}$\and
Ziqiang Cao$^{1*}$\and
Changlong Zhou$^{1}$\and
Jun Zhang$^{2}$\And
Yiqin Gao$^2$\\
\affiliations
$^1$School of Computer Science and Technology, Soochow University, Suzhou, China\\
$^2$Changping Laboratory, Academy for Advanced Interdisciplinary Studies, Peking University, Beijing, China\\
\emails
\{wangzc025, 17625808660\}@163.com,
2301111585@stu.pku.edu.cn,
20235227105@stu.suda.edu.cn,
jzhang@cpl.ac.cn,
gaoyq@pku.edu.cn
}


\begin{document}

\maketitle

\begin{abstract}

Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q\&A systems. 
To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. 
Our model incorporates a modified ProteinMPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. 
To optimize training efficiency, we freeze the encoder and employ LoRA techniques for the decoder. 
We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. 
Furthermore, zero-shot prediction results highlight its strong generalization capabilities. 
This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research.


\end{abstract}

\section{Introduction}

With the continuous advancement of biotechnology, gaining a deeper understanding of protein sequences, structures, and their functions has become increasingly important. 
To elucidate the complex mechanisms underlying protein-mediated physiological processes, researchers have focused on studying and predicting the physicochemical properties of proteins. 
The widespread availability of protein sequences and structures, enabled by novel high-throughput sequencing technologies and advanced structure prediction tools~\cite{jumper2021highly,abramson2024accurate}, has greatly facilitated the exploration of intricate relationships between protein sequences, structures, and functions. 
In this context, protein Q\&A systems have emerged as a research hotspot, aiming to help scientists and researchers efficiently access information about protein properties and functions~\cite{zhou2023survey}. 
These systems are of considerable significance in fields such as drug discovery and disease research. 
Besides, just as some proteins mentioned in AF-Cluster~\cite{wayment2024predicting}, it indicates that even if the protein sequences are the same, they may present different 3D structures due to different environmental conditions, binding partners, or point mutations, thereby performing different biological functions. 


Traditional approaches, such as those based on Gene Ontology (GO) classifications, predict protein functions by leveraging sequence information through convolutional models or sequence similarity-based computations, as exemplified by methods like DeepGOPlus~\cite{kulmanov2020deepgoplus}.
Such models primarily utilize protein sequence information and are limited to classification tasks, which oversimplifies the complexity of protein functions.
The recent rise of large language models (LLMs) has propelled protein Q\&A systems into a cutting-edge research domain, allowing scientists to efficiently access and interpret critical information about protein properties and functions~\cite{zhou2023survey}. 
These systems hold immense potential for transformative applications in areas such as drug discovery and disease research. 
To support this growing field, several datasets, including Mol-Instructions~\cite{fang2023mol} and UniProtQA~\cite{luo2023biomedgpt}, have been developed, providing essential resources for advancing protein Q\&A research.


An increasing number of models have fine-tuned LLMs using protein-specific Q\&A datasets to generate contextually relevant text or answer questions related to protein structure and function. 
However, as highlighted in Table \ref{tab:base models}, existing approaches often face significant limitations, including a lack of multimodal information integration, limited flexibility, and high computational costs. 
These shortcomings restrict their ability to provide precise and contextually appropriate responses. 
For instance, FAPM~\cite{xiang2024fapm} and InstructProtein~\cite{wang2023instructprotein}, despite incorporating generative Q\&A functionalities, remain heavily influenced by classification-based frameworks. 
This often results in the generation of extraneous or irrelevant information, deviating from the intended scope of the answers and limiting their utility in addressing complex protein-related queries. 
BiomedGPT~\cite{luo2023biomedgpt} introduces a generative framework tailored for protein Q\&A tasks but relies solely on sequence information, which significantly limits its predictive capabilities. 

We compared the generation effects of large language models trained with protein structures and sequences, respectively, on different types of questions, as shown in Figure \ref{fig:QA}. 
The results show that when answering questions related to protein subcellular localization and domains or motifs, the generation effect of the model jointly trained with protein structures and sequences is significantly better than that trained only with protein sequences. 
Some following works such as ProtChatGPT~\cite{wang2024protchatgpt} and Evola~\cite{zhou2025decoding} have tried to employ two separate modules for encoding sequence and structural information.
However, such practice makes it difficult for protein sequence and structural features to interact, and significantly increases computational complexity.
Moreover, no existing model has systematically examined the relative contributions of sequence and structural information to the performance of protein Q\&A systems. 
This gap underscores the critical need for future research to evaluate how integrating both sequence and structural data can enhance the accuracy, robustness, and interpretability of protein-related Q\&A.


As the sequence and structural features of proteins are one-to-one correspondence, we propose a novel framework called Prot2Chat that seamlessly incorporates the spatial structure and sequence information into LLMs. 
This approach seeks to enhance both the performance and applicability of protein Q\&A systems. 
As illustrated in Figure \ref{fig:model}, the overall architecture of our model comprises three primary components: the protein encoder, the protein-to-text adapter, and the large language model. 
Specifically, we extended the structure encoder ProteinMPNN~\cite{dauparas2022robust} to fuse sequence information during node initialization. 
The sequence features we introduced are inherent to the model, thus eliminating the need for additional training parameters.
Then following the methodology of InstructPLM~\cite{qiu2024instructplm}, a lightweight cross-attention layer was incorporated to align the protein encoder with a LoRA~\cite{hu2022lora} fine-tuned LLM. 
This approach preserves the primary sequence and structural information of proteins during the compression process. 
The processed protein structure and sequence data are provided as a soft prompt to the LLM. 
Leveraging the flexible text generation capabilities of the LLM, our model effectively follows residue-level protein instructions, significantly enhancing its utility in protein-related tasks. 

To evaluate the model's performance, we conducted comparative experiments on the Mol-Instructions and UniprotQA datasets. 
And we conducted zero-shot experiments using a part of the UniprotQA dataset to verify the generalization ability of the model. 
The generated answers were assessed using common metrics such as BLEU and ROUGE, as well as online KIMI~\cite{qin2024mooncake} scoring metrics and expert evaluations, ensuring the reliability and relevance of the outputs. 
Experimental results demonstrate that the model employing a unified encoder to integrate protein sequence and structural information outperforms those using sequence-only pretraining or ESM-based encoders~\cite{lin2022language}. 
Furthermore, the high consistency between expert evaluations and online KIMI assessments confirms the robustness of our model.


In summary, our contributions are as follows:
\begin{enumerate}
    \item We extended the existing structure encoder ProteinMPNN to realize structure and sequence early fusion without the need for training.
    \item Based on the protein encoder, we achieved a lightweight and effective protein large language model with only 93M training parameters. 
    \item We performed various systematic assessments across various evaluation datasets to validate our model's generative and generalization capabilities. These included traditional metrics, large model evaluations online, and manual expert evaluations.
    
\end{enumerate}




\begin{figure}[t]
  \includegraphics[width=0.98\linewidth]{QA_3.png} 
  \caption {Comparison of Q\&A results between Prot2Chat and LLaMA3-FT with sequence.}
  \label{fig:QA}
\end{figure}


\begin{figure*}[t]
  \includegraphics[width=0.98\linewidth]{124.pdf} 
  \caption {Model Structure of Prot2Chat. The red font represents the input, the snowflake represents freezing, and the fire represents the parameters to be trained.}
  \label{fig:model}
\end{figure*}




\section{Related work}

\subsection*{Protein Representation Learning}

Proteins are fundamental components of cells, essential for their biological activities and diverse functions. Previous studies on protein characterization have explored various methods to learn protein representations based on different forms of protein information. Protein sequences, often referred to as the "language of life," have been extensively studied using advanced natural language processing techniques. For instance, Tranception~\cite{notin2022tranception} employs the Transformer~\cite{vaswani2017attention} model to encode amino acid sequences, capturing relationships between residues and autoregressively reconstructing protein sequences from large-scale databases. Similarly, sequential modeling approaches such as ESM~\cite{rives2021biological,lin2023evolutionary,hayes2024simulating} leverage masked language modeling (MLM) to develop attention patterns that correspond to residue-residue contact maps, enhancing sequence-based protein representations.
On the other hand, structure-based approaches~\cite{gligorijevic2021structure,achiam2023gpt} directly indicates protein function and encodes geometric information of the protein for topology-sensitive tasks such as protein property prediction. Foldseek~\cite{van2022foldseek} introduces the concept of a structural alphabet, encoding protein structures into a discrete representation space. Similarly, Saprot~\cite{su2023saprot} introduces a structure-aware vocabulary, embedding structural information into model inputs to enhance representational capacity, achieving significant success in protein function prediction tasks.


\subsection*{Multi-Modal Alignment}


Enabling large language models (LLMs) to understand additional modalities has been a rapidly evolving research direction, with notable examples including image-text models~\cite{tsimpoukelli2021multimodal,li2023blip}, video-text models like VideoLlama~\cite{zhang2023video}, audio-text models such as Macaw-LLM~\cite{lyu2023macaw}, and molecular-text models, including ReLM~\cite{shi2023relm}, and MolTC~\cite{fang2024moltc}. This line of research was pioneered by advancements in visual language modeling (VLM), which has been successfully applied to tasks such as few-shot image classification, image captioning, and image Q\&A~\cite{li2023blip,alayrac2022flamingo}.
To enable LLMs to understand images, leading VLM methods adopt different strategies. Some, like BLIP-2~\cite{li2023blip}, employ nonlinear and expressive cross-modal projectors, while others, such as PaLM-E~\cite{driess2023palm}, utilize visual encoders and fine-tune LLMs on multimodal datasets. These approaches have also been increasingly applied to protein analysis and understanding. For instance, Galactica~\cite{taylor2022galactica} leverages scientific literature to model protein sequences and SMILES representations, enabling the model to interpret sequence properties. 
ProtNote~\cite{char2024protnote}, utilize two encoders to encode protein sequence information and text information, respectively, and Multilayer Perceptron (MLP) to fuse these inputs, achieving the goal of using text to achieve supervised and zero-shot protein functional prediction. 
ProteinChat~\cite{guo2023proteinchat} further uses the protein structure and the corresponding description to model for the protein Q\&A tasks.
ProtChatGPT~\cite{wang2024protchatgpt} uses two independent modules to encode sequence and structural information respectively, and aligns them with natural language to explore protein functions. 
InstructBioMol~\cite{zhuang2024instructbiomol} integrates multimodal biomolecular inputs through a feature extraction module, allowing researchers to articulate design goals in natural language. 
InstructProtein~\cite{wang2023instructprotein} employs a supervised training framework based on instruction generation using knowledge graphs, enabling bidirectional generation between natural language and protein language. This model can predict protein functional descriptions and generate protein sequences based on natural language prompts. Prot2Text~\cite{abdine2024prot2text} combines graph neural networks (GNNs) with LLMs to predict protein functions in free-text form. Similarly, ProtT3~\cite{liu2024prott3} provides an efficient protein-to-text generation framework by integrating protein language models (PLMs) with LLMs. FAPM~\cite{xiang2024fapm} utilizes a contrastive learning framework to implement a generative-like Q\&A model, while BiomedGPT~\cite{luo2023biomedgpt} bridges the gap between biological and human natural language through a multimodal generative pre-trained transformer (GPT). This allows users to "communicate" with biological modalities using free-text queries, facilitating interactions with biological data in natural language. 



\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|ccr}
        \hline
        Model & Input & Output \\
        \hline
        Prot2Text & Sequence & Structured Text  \\
        ProtNote & Sequence\&Description & Function\&Probability    \\
        ProtChatGPT & Structure\&Sequence\&Question & Free Text  \\
        Evola & Structure\&Sequence\&Question & Free Text  \\
        InstructProtein & Sequence\&Question & Structured Text  \\
        FAPM & Sequence & Structured Text  \\
        BioMedGPT & Sequence\&Question & Free Text  \\
        Prot2Chat & [Structure+Sequence]\&Question & Free Text  \\
        \hline
    \end{tabular}
    \caption{Comparison of input-output of protein Q\&A models, "Description" is a text related to protein functions, such as the description of Gene Ontology (GO) terms, "[Structure + Sequence]" represents the early fusion of structure and sequence. }
    \label{tab:base models}
\end{table*}


\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|cccr }
        \hline
        Dataset & Train & Valid & Test\\
        \hline
        Mol-Instructions & 404640 & 16859 & 11072 \\
        UniProtQA & 25820 & 1075 & 6734  \\
        \hline
    \end{tabular}
    \caption{The count and division of the datasets we used.}
    \label{tab:dataset} 
\end{table}

\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{p{0.95\textwidth}}
    \hline
    \textbf{Prompt} \\ 
    Please select the sentence that is closest and most accurate in meaning to the Target sentence and rank sentences A, B, and C accordingly, with the first place meaning the most identical and accurate in meaning to the Target. Only provide the answer, for example, `B A C' or `A C B', etc. \\
    Target: \{Ground Truth\} A: \{Model1 Generated Text\} B: \{Model2 Generated Text\} C: \{Model3 Generated Text\} \\ \hline
    \end{tabular}
    \caption{Prompt to KIMI for evaluation}
    \label{tab:prompt}
\end{table*}



\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|c|c|c|c}
    \hline
        Model & Bleu-2 & Rouge-1 & Rouge-2 & Rouge-L    \\ 
        \hline
        LLaMA3-8B-Instruct & 4.84 & 23.07 & 4.31 & 15.36  \\ \hline
        LLaMA3-FT with sequence& 6.42 & 24.50  & 6.32 & 17.03\\ \hline
        BioMedGPT-LM-10B & 1.02 & 10.93 & 1.57 & 7.84  \\ \hline
        KIMI (zero-shot)& 4.79 & 22.21 & 4.62 & 14.70  \\ \hline
        KIMI (few-shot) & 12.05 & 31.21 & 11.38 & 24.18   \\ \hline
        
        Prot2Chat  & \textbf{33.25}  & \textbf{54.88} & \textbf{35.26} & \textbf{47.90}  \\ \hline
       
        
        w/o fine-tuned LLM& 12.87 & 30.89 & 14.67 & 26.81  \\ \hline
        w/o Protein sequence & 31.88 & 52.90 & 33.43 & 46.17 \\ \hline
    \end{tabular}
    \caption{Result in Mol-instructions protein oriented dataset, the best performances are marked in bold.}
    \label{tab:result1}
\end{table*}

\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|c|c|c|c}
    \hline
        Model & Bleu-2 & Rouge-1 & Rouge-2 & Rouge-L    \\ 
        \hline
        LLaMA3-8B-Instruct & 2.41  & 13.66 & 1.10  & 9.07  \\ \hline
        BioMedGPT-LM-10B & 5.80  & \textbf{15.88} & 7.32 & 14.68  \\ \hline
        Prot2Chat (zero-shot)& 2.11 & 13.78 & 1.51 & 9.64  \\ \hline
        
        Prot2Chat (fine-tuned) & \textbf{5.87} & 15.65 & \textbf{9.00}  & \textbf{14.99}  \\ \hline
    \end{tabular}
    \caption{Result in UniprotQA dataset, the best performances are marked in bold.}
    \label{tab:result2}
\end{table*}

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|l|l|l|c}
    \hline
        
        Model & 1st & 2nd & 3rd & Avg.  \\ \hline
        Prot2Chat & 591 & 386 & 22 & 1.43  \\ \hline
        BioMedGPT-LM-10B & 218 & 214 & 567 & 2.35  \\ \hline
        LLaMA3-FT with sequence  & 190 & 399 & 410 & 2.22  \\ \hline
    \end{tabular}
    \caption{Results of KIMI evaluation, "Avg." represents the average ranking.}
    \label{tab:select1}
\end{table}


\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \begin{tabular}{l|l|l|l|c}
    \hline
        Model & 1st & 2nd & 3rd & Avg.  \\ \hline
        Prot2Chat & 454 & 115 & 78 & 1.42  \\ \hline
        BioMedGPT-LM-10B & 157 & 203 & 287 & 2.20  \\ \hline
        LLaMA3-FT with sequence & 36 & 329 & 282 & 2.38  \\ \hline
    \end{tabular}
    \caption{Results of expert manual evaluation, "Avg." represents the average ranking.}
    \label{tab:select2}
\end{table}

\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{2} 
    \begin{tabular}{m{0.2\textwidth}|m{0.32\textwidth}|m{0.18\textwidth}|m{0.15\textwidth}}
        \hline
        Inputs & LLaMA3-FT with sequence & Prot2Chat & Ground Truth \\
        \hline
        \begin{minipage}[c][0.1\textwidth][c]{0.2\textwidth}
            \begin{center}
            
            \includegraphics[width=\linewidth]{location.png} \\
            \textbf{[O58787]}Using the provided structure and amino acid sequence of a protein, estimate its primary location within the cell: 
            \end{center}
        \end{minipage} & Based on the provided amino acid sequence, it appears that the protein is a membrane protein, likely \color{red}localized to the endoplasmic reticulum, Golgi apparatus, and/or plasma membrane.\color{black} The presence of multiple transmembrane domains and the distribution of hydrophobic and hydrophilic residues suggest that \color{red}this protein is an integral membrane protein.\color{black} Additionally, the sequence contains several potential phosphorylation sites and potential N-glycosylation sites, which may be involved in protein-protein interactions and/or post-translational modification. & The protein characterized by the structure and amino acid sequence demonstrates rRNA binding and is implicated in the translation. Its subcellular \color{blue}localization is primarily within the cytosol. & Upon analyzing the given protein structure and sequence, it appears that the protein is likely to be \textbf{localized in the cytosol compartment of the cell}. \\
        \hline
        \begin{minipage}[c][0.01\textwidth][c]{0.2\textwidth} 
            \centering
            \includegraphics[width=0.8\linewidth]{function.png} \\
            \textbf{[O58736]}Using the protein structure and the following protein sequence as input, assess its potential function by comparing it to known proteins with similar sequences or structures: 
        \end{minipage}  &The provided protein sequence is approximately 240 amino acids long and shows no significant similarity to known proteins in the database. However, it can be analyzed for potential functional regions and motifs using various bioinformatic tools. Here are some observations and predictions: 1. \color{red}**Transmembrane regions**\color{black}: The sequence contains several regions that are \color{red}predicted to be transmembrane,\color{black} including the following: * 24-45 amino acids (ILLLVGMPGSGKGEVAKAFRRRGIPVI) * 63-82 amino acids (MGGDAIREEAEKRGIPKTPEGLKYVSLKVREE) & With the provided structure and amino acid sequence, the protein is expected to \color{blue}display adenylate kinase activity and participate in the phosphorylation, ATP biosynthetic process, phosphorylation. & The protein with the structure and amino acid sequence has been analyzed, and its structure suggests a potential \textbf{involvement in ATP binding}.\\
        \hline
    \end{tabular}
    \caption{Case study. The bold font represents the key points of the answers, the blue font represents the correct answers of the key points, and the red font represents the incorrect answers of the key points.}
    \label{tab:example}
\end{table*}



\section{Method}
\subsection{Overview of Prot2Chat}


As illustrated in Figure~\ref{fig:model}, our model leverages the 3D coordinate structure information of protein residue atoms along with the protein sequence. The protein encoder processes this information to generate a protein embedding representation, which is then transformed into a text-aligned protein prompt via the protein-text adapter. This prompt, combined with the input question, serves as the input to the large language model (LLM), ultimately producing the output answer.
\subsection{Sequence and Structure Fused Protein Encoder}

We modified the structure encoder ProteinMPNN to realize the sequence and structure early fusion.
ProteinMPNN is originally dedicated to design protein sequences based on backbone structures. 
The input is structural information $E$, namely the 3D coordinates of protein residue atoms (N, C$\alpha$, C, O). 
Protein features $h_V$ are obtained through the encoder layer and the decoder layer. 
\begin{align}
    h_E &= \text{Linear}(E) \\
    h_V &=\text{Zeros}(n,Dc) \label{EQ:originalHv}\\
    h_V, h_E &= \text{Encoderlayers}(h_V, h_E) \\
    h_V &= \text{Decoderlayers}(h_V, h_E) \label{EQ:origndecoderlayers}
\end{align}
where Linear, Encoderlayers and Decoderlayers are all modules in ProteinMPNN, $E$ is the structual features and $n$ is the number of protein residues and $Dc$ is the embedding dimension of the protein encoder.


We find that a variant version of ProteinMPNN additionally contains the Embedding module for protein sequences $S$, and the Decoderlayer is sequence-aware, which changes Equation~\ref{EQ:origndecoderlayers} to:
\begin{equation}
    h_V = \text{Decoderlayers}(h_V, h_E, \text{Embedding}(S))
\end{equation}
To be specific, the Decoderlayer combines the node embeddings, the output features of the encoder, and the sequence embeddings to construct the context features for decoding through neighborhood feature aggregation. 
Using the message passing mechanism, the hidden state $h_V$ is continuously updated through multi-layer stacking to simulate the dynamic generation process of the decoder. 

On this basis, we further initialized $h_V$ with the off-the-shelf sequence embedding to make the sequence and structural information early fused.
Specifically, Equation~\ref{EQ:originalHv} is replaced by:
\begin{equation}
    h_V =\text{Embedding}(S)
\end{equation}

In particular, all of the model weights we used are from ProteinMPNN, with no need for additional training. 
The final protein node vector ${h}_{V}$ is used as the protein feature for the protein-text adapter. 
Inspired by InstructPLM~\cite{qiu2024instructplm}, we concatenate the protein representations encoded by nine released ProteinMPNN models. 
As a result, $Dc = 128*9 = 1152$.




\subsection{Protein-Text Adapter}

We implemented a protein-text adapter to semantically align the information obtained by the protein encoder with natural language, and provided both types of information as input to the large model simultaneously. 
The feature $h_V$ obtained from the modified ProteinMPNN model is output through the linear projection layer, positional encoding and the cross-attention mechanism.  
Specifically, the input protein feature first passes through a linear projection layer \( W_{\text{proj}} \) to transform the input features to the target output dimension $D_{\text{o}}$:
\begin{equation}
X_{\text{proj}} = W_{\text{proj}} \cdot h_V + PE
\end{equation}
where $PE$ is the Dynamic Positional Encoding~\cite{vaswani2017attention} used to capture the positional information of amino acids.



The complete protein encoding is too long. 
We adopt the idea of BLIP-2 to extract the important semantic features from it. 
In particular, we introduce \( n_\text{q} \) learnable queries $\boldsymbol{Q} \in \mathbb{R}^{n_\text{q}\times D_\text{o}}$ and also apply $PE$ to them. 
Then the multi-head attention layer is adapted to capture key protein information based on queries: 
\begin{equation}
A^{k}=\text{softmax}\left(\frac{Q^{k}K^{k\top}}{\sqrt{D_{k}}}\right)V^{k}
\end{equation}
where $Q^{k}=\boldsymbol{Q}W_{Q}^{k}$, $ K^{k}=X_\text{proj}W_{K}^{k}$ and $V^{k}=X_\text{proj}W_{V}^{k}$ stand for queries, keys and values. 
Among them, $W_{Q}^{k},W_{K}^{k},W_{V}^{k}$ are related parameters, $k$ is the head index.
The multi-head attention outputs are then concatenated across heads and linearly transformed to the final protein prompt:
\begin{equation}
\boldsymbol{X_\text{protein}}=\text{Concat}(\boldsymbol{A}^{1},\boldsymbol{A}^{2},\cdots,\boldsymbol{A}^{M})\boldsymbol{W}_{out}
\end{equation}


\subsection{LLM Decoder}

We combine $X_\text{protein}$ representing protein information with the text question and input it into the existing LLM to obtain the response. 
\begin{equation}
\text{response} = \text{LLM}(X_\text{protein}, \text{question})
\end{equation}
To improve domain adaptability, we fine-tuned the LLM with LoRA while training the adapter. 
The number of adapter training parameters is 89,702,400, while the number of LLaMA3 training parameters is 3,407,872. 
The total number of training parameters of the Prot2Chat is 93M. 
Meanwhile, the number of training parameters of BiomedGPT is 3B, that of ProtT3 is approximately 2B, the range of trainable parameters of Prot2Text is from 256M to 898M, and that of FAPM is 188M. 
This also shows the advantage of our model in significantly reducing the computational cost. 
Besides, we adpot the CrossEntropyLoss function as common.  


\section{Experiment}

\subsection{Datasets}

We evaluated the performance of our model using the Mol-Instructions~\cite{fang2023mol} and UniProtQA~\cite{luo2023biomedgpt} datasets. Mol-Instructions is a comprehensive instruction dataset specifically designed for the biomolecular domain, comprising both human-constructed and machine-generated data. For our experiments, we utilized the protein-oriented instruction subset, which is primarily derived from entries in the UniProtKB/Swiss-Prot database~\cite{uniprot2018uniprot}. This subset encompasses tasks such as predicting protein domains, functions, and activities. Our model was predominantly trained on this dataset. 

Additionally, we employed a portion of the UniProtQA dataset, introduced by BiomedGPT~\cite{luo2023biomedgpt}, to assess the generalization capability of our model through zero-shot evaluation. Fine-tuning was also performed on this dataset to further validate performance. UniProtQA consists of textual descriptions of proteins, including their functions and properties. It was curated by UniProt~\cite{uniprot2018uniprot} and includes four types of questions related to protein function, formal name, protein family, and subcellular location. The detailed sizes of our dataset splits are provided in Table~\ref{tab:dataset}.

To incorporate structural and sequence information, we retrieved the corresponding PDB files for proteins listed in Swiss-Prot. These PDB files, combined with the associated questions, serve as the input to Prot2Chat.

\subsection{Baselines}

We introduced the following baselines that represent protein information using sequence.
\begin{itemize}
    \item \textbf{LLaMA3: } LLaMA3~\cite{dubey2024llama} series has attracted a lot of research attention due to its outstanding capabilities in the general domain. We use LLaMA3-8B-Instruct and input the protein sequence and question to achieve zero-shot.  

    \item \textbf{LLaMA3-FT: } We also fine-tuned LLaMA3-8B-Instruct using textual protein sequence information and used this model as a reference model.

    \item \textbf{BioMedGPT-10B: } BioMedGPT is a domain-specific LLM trained on a large selection of corpora of human scientific knowledge. The model encodes the protein sequence with the ESM-3B~\cite{lin2022language} and uses BioMedGPT-LM-7B as the decoder to generate the response. 
    
    \item \textbf{KIMI: } We used the online large model KIMI as a control.
\end{itemize}
In order to make a fair comparison, we used the same input template for each model through prompt engineering, so that the model could output the answer as we instructed. 
Some other open-source models in related fields, such as FAPM, Prot2Text, and InstructProtein, were not included in the comparison because these models complete protein Q\&A tasks based on classification, or there are differences in the formats of the model's inputs and outputs, as shown in Table \ref{tab:base models}. 
Therefore, it is hard to conduct comparisons. 

\subsection{Model Setting}


We jointly trained the protein adapter and LLaMA3 on the Mol-Instructions dataset, employing full training for the adapter and LoRA (Low-Rank Adaptation) fine-tuning for LLaMA3. Inspired by BLIP-2~\cite{li2023blip}, we configured the adapter with 256 queries. For the LoRA setup, we set the rank \( r \) to 8, LoRA alpha to 16, and targeted the "q\_proj" and "v\_proj" modules, with a LoRA dropout rate of 0.1. The adapter comprises 89,702,400 trainable parameters, while LLaMA3 has 3,407,872 trainable parameters. 

We utilized the Adam optimizer and implemented gradient accumulation to optimize the training process. The initial learning rate was set to \(10^{-4}\), with a batch size of 2 and a maximum context length of 1024 tokens, which includes both the question and answer text. The model was fine-tuned for 2 epochs, with each training session requiring approximately 1600 hours on an NVIDIA RTX 3090 GPU.
\subsection{Evaluation Metrics}


To evaluate the effectiveness of the model's text generation, we employed performance metrics including BLEU~\cite{papineni-etal-2002-bleu} and ROUGE~\cite{lin-2004-rouge}. Additionally, we utilized the prompt templates shown in Table~\ref{tab:prompt} to simultaneously input the target text and the text generated by different models into the online KIMI model. This allowed us to determine which generated answer was closer to the target text, thereby assisting in the evaluation of the model's output quality. The results, as shown in Table~\ref{tab:select1}, demonstrate that our model outperforms other comparative models overall. Furthermore, we conducted expert manual evaluations, where professional biology PhDs ranked the responses of different models based on their alignment with the target text. The results of these evaluations (Table~\ref{tab:select2}) are consistent with those obtained using KIMI.


\subsection{Main Results}

The results of the assessment are shown in Table \ref{tab:result1} and Table \ref{tab:result2}. 
The results show that there is a significant modal gap between the protein sequence and the natural language, and it is incomprehensible to directly input the protein sequence as text and problem into the large language model, resulting in the disordered and meaningless response of the model. 
Aligning protein language with human language is an effective solution to solve this problem, and we have observed that the performance of the model trained with the addition of structural information is significantly better than that of other models. 
This suggests that we can directly use the structure and sequence information of proteins to help language models better understand and generate technical terms through feature space alignment. 
Establishing a link between proteins and natural language can provide guidance for researchers in their research on unknown proteins. 
The results of our model's prediction on the UniprotQA~\cite{luo2023biomedgpt} dataset verify the generalization performance of our model. 
Further illustrating the role of our model in protein Q\&A tasks.



\subsection{Ablation Experiments}

As shown in Table~\ref{tab:result1}, we conducted a comparative analysis to evaluate the impact of incorporating protein structure during training, thereby highlighting the importance of structural information for protein understanding. We also examined the effect of fine-tuning LLMs, revealing that training adapters alone was insufficient for achieving optimal performance. Furthermore, by comparing the results obtained from training with sequence text alone versus training with both sequence and structural information, we observed that LLMs struggle to effectively interpret protein sequence information presented in textual form. These findings underscore the necessity of integrating protein structural information alongside a unified multimodal alignment module, which enables LLMs to better understand and generate domain-specific technical terms.
\subsection{Case Study}

As shown in Table \ref{tab:example}, the results we obtained using the protein Q\&A test dataset indicate that the model trained with the addition of protein structure information generates responses significantly better than the model trained with protein sequence. 
Further prove that it is difficult for LLM to discover the corresponding protein functions and effects only through the text information of protein sequences, and thus draw the conclusion that the protein structure is vital for the comprehensive understanding of proteins. 

\section{Conclusion}

This paper introduces a novel protein Q\&A model that addresses the limitations of traditional protein Q\&A tasks by leveraging the modified ProteinMPNN and an adapter module to integrate protein structure and sequence information with natural language text. By harnessing the flexible generation capabilities of large language models (LLMs), our approach effectively bridges the gap between protein data and textual understanding. Experimental results demonstrate that the model trained with structural information significantly outperforms other baseline models, underscoring the critical role of protein structure in protein understanding and analysis. Furthermore, the results validate the effectiveness and strong generalization ability of our model in protein Q\&A tasks.


\bibliographystyle{named}
\bibliography{ijcai24}

\end{document}

