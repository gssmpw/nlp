\section{The NUTSHELL Dataset}
\label{sec:dataset}
In this section, we introduce the new \DATASETNAME{} resource. We chose 
to build our corpus upon the
the ACL Anthology\footnote{\label{footnote_acl}https://aclanthology.org} 
since it provides a rich collection of multimodal resources (talks and abstracts) and open-access licensing. Starting from 2017, a significant number of papers published in the main *ACL conferences (ACL, EMNLP, and NAACL) include a video of the presentation, all released under the Creative Commons Attribution 4.0 license. This makes ACL an ideal resource for building a multimodal dataset 
for the SAG task.

In the following, we present a feasibility assessment of SAG through human evaluation (\S\ref{subsec:human_feasibility}). Then, we describe the collection process performed to create \DATASETNAME{}, together with the final dataset statistics (\S\ref{subsec:collection}).

 
\subsection{Are paper abstracts \enquote{good} talk summaries?}
\label{subsec:human_feasibility}
Before creating the corpus,
we establish the validity of our data by investigating 
whether abstracts represent a good summary of the associated talk. To this aim, we conduct a qualitative check on a data sample of 30 talk-abstract pairs from the ACL anthology. 
We involve a total of 5 annotators, who are all domain experts and thus familiar with scientific material.\footnote{Annotators include the paper authors and their colleagues, whose work will be acknowledged upon acceptance.}
To verify Inter-Annotator Agreement (IAA),
a double annotation by different experts was carried out on 15 pairs.


Since we are interested in understanding whether 
paper abstracts are informative enough to represent a good summary
of the talk,
we asked evaluators to annotate:
    (1) Whether the information in the abstract is \textbf{all} uttered by the presenter;
    (2) The span of information present in the abstract that was not contained in the talk, if any;
    (3) Whether they think that the abstract summarizes 
    all \textbf{important} information presented in the talk.
The human evaluation template is
provided in \cref{fig:annoation_instructions_good_abstracts} of \cref{app:human_eval_good_abstracts}.


The results indicate that $70.0\%$ of the abstracts are considered good summaries by annotators as they contain important information about the talk. 
However, $63.3\%$ of the abstracts contain  information that is not present in the talk.
For this reason, we analyzed the annotated spans of the missing information. We observed that this phenomenon is mainly due to missing dataset, model, and shared task (e.g., evaluation campaigns) names or URLs (e.g., link to the resource or model being released), 
which are typically not spelled by  presenters.\footnote{This issue could be  overcome by exploiting the videos, as this information is typically shown in the slides. While out of scope for SAG, \DATASETNAME{} includes the videos, making it a useful resource also for more complex multimodal tasks.}
Despite this drawback, the evaluation of automatic models against the same ground truth abstract can be considered fair, as models are equally penalized by this category of missing information. Moreover, establishing a unique ground truth for summarization tasks is still an open research question  \citep{zhang-etal-2024-benchmarking} as humans often produce very different summaries. Both, questions (1) and (3) have an inter-annotator agreement of $\kappa=0.466$, indicating moderate agreement \citep{IAA-agreement}, which is satisfactory given the subjective nature of evaluating summaries. Therefore, the manual evaluation revealed the feasibility of the 
SAG tasks and the validity of our resource.

\subsection{Collection and Dataset
Statistics}
\label{subsec:collection}
We collected talks from 16 ACL Anthology events: 6 ACL, 6 EMNLP, and 4 NAACL, including workshops.
For each paper (both long and short format), we extracted the video and the associated abstract already available on the paper website. We exclude papers with invalid URLs, videos without audio, or abstracts missing from the paper page.

Lastly, we split the dataset into training (years  2017 to 2021), dev (ACL 2022), and test (EMNLP/NAACL 2022).
These splits reflect a realistic evaluation setup, where models are trained on past data and tested on the most recent, unseen examples.
In total, the corpus contains 1,172 hours of audio content corresponding to 6,316 different presentations (full statistics are reported in \cref{tab:data_statistics}).

