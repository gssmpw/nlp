\section{Introduction}
Abstracts are essential in scientific communication, allowing researchers
to quickly grasp the key contributions of a paper.
With the ever-growing number of publications, abstracts help researchers stay informed without reading full papers. Beyond their practical utility, abstracts also pose a significant challenge for natural language generation models: 
abstracts are a specialized form of summarization that not only condenses content but also promotes the work, often using domain-specific terminology and structured language.

Scientific summarization has been widely studied in natural language processing, including summarizing entire articles \citep{collins-etal-2017-supervised, liu-etal-2024-sumsurvey}, particularly in the medical domain \citep{kedzie-etal-2018-content, cohan-etal-2018-discourse, gupta-etal-2021-sumpubmed}, generating abstracts from citations \citep{yasunaga-scisumm, Zanzotto_Bono_Vocca_Santilli_Croce_Gambosi_Basili_2020}, summarizing specific paper sections \citep{takeshita-etal-2024-aclsum}, and leveraging knowledge graphs for 
abstract generation \citep{koncel-kedziorski-etal-2019-text}.

With the growing availability of recorded conference talks, a new challenge emerges: generating abstracts from spoken content or \TASKNAME{} (SAG).  The abstracts offer researchers 
a quick way to assess relevant talks without watching entire recordings. Additionally, as conferences include more virtual content, automatically generated summaries enable efficient engagement with recorded talks \citep{murray-etal-2010-generating}.

While speech summarization has been explored in domains like news \citep{matsuura2024sentencewisespeechsummarizationtask}, YouTube videos \citep{sanabria2018how2largescaledatasetmultimodal}, and meeting minutes \citep{mccowan-ami, janin-icsi}, large-scale datasets for scientific talk abstract generation are lacking. 
Existing work \citep{lev-etal-2019-talksumm} aligns transcripts with the corresponding papers and extracts overlapping textual segments as summaries. However, these segments are drawn from the paper rather than the talk itself, failing to capture the distinct contributions, framing, and nuances conveyed in spoken presentations. Other studies have focused on summarizing TED Talks \citep{Koto-ted, DBLP:conf/asru/KanoODW21, vico-tedtalk-2022, shon-etal-2023-slue}, which target a broad audience and prioritize inspiration and engagement over technical content.

To bridge this gap, we introduce \DATASETNAME{} a new multimodal dataset for abstract generation from scientific talks. Built from recorded 
presentations of *ACL conferences, the dataset pairs abstracts with their corresponding spoken content and video, offering 
a valuable resource for future research. To validate the quality of the abstracts as concise and well-structured summaries of the talks -- i.e., capturing the essence of the presentations \textit{in a nutshell} -- we performed 
a human assessment, which confirmed
their effectiveness and suitability for the SAG task.


To establish baselines for SAG using our dataset, we evaluate three model types: (1) a cascaded model combining automatic speech recognition (ASR) with text-based summarization, (2) a state-of-the-art speech-language model (SpeechLLM) without fine-tuning, and (3) a SpeechLLM fine-tuned on our dataset. 

Our contributions are three-fold:
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
    \item  We introduce \DATASETNAME{}, a novel dataset for abstract generation from scientific talks comprising 1,172 hours, which is released under
    CC-BY 4.0 License on HuggingFace;\footref{footnote_data_hf}
    \item We provide baselines with different model types for comparison in future research, evaluated using both standard automatic metrics (e.g., ROUGE) and the emerging LLM-as-a-judge approach \citep{shen-etal-2023-large};
    \item We conduct human evaluations to assess the quality of 
        the abstracts and  validate the suitability of automatic metrics for the SAG task.

\end{enumerate}
\input{tables/data_stats}
   


