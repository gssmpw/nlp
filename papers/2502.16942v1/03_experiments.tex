%\section{Experiments}
\section{Analysis}

 To demonstrate the quality and usability of our corpus, as well as 
 provide baselines for future works, we 
 develop and evaluate four different models using
 both automatic metrics and
 human evaluation. 

 \subsection{Experimental Setting}\label{subsec:exp_setting}
 
 \subsubsection{Models}
To establish baselines for the SAG task, we analyze the performance of four models described as follows. Prompts, model, generation, and additional training details are provided in \cref{sec:app:baselines}.

\paragraph{Whisper + LLama3.1-8B-Instruct.}  A cascaded solution, where the audio is first transcribed with
\texttt{openai/whisper\--large\--v3} \citep{radford2022robustspeechrecognitionlargescale}, and then
\texttt{meta\--llama/\-Llama\--3.1\--8B\--Instruct} \citep{dubey2024llama3herdmodels} is prompted to generate the abstract from the generated transcript.

\paragraph{Qwen2-Audio-7B-Instruct.} The \texttt{Qwen/\-Qwen2\--Audio\--7B\--Instruct} \citep{chu2024qwen2audiotechnicalreport} model, an existing SpeechLLM\footnote{By \textit{SpeechLLM}, we refer to the combination of a speech encoder and an LLM through a learned modality adapter \citep{gaido-etal-2024-speech}.}, which is used out of the box without any fine-tuning.

\paragraph{End2End Zero-Shot.} A SpeechLLM composed of HuBERT \citep{hubert-2021} as speech encoder, \texttt{meta\--llama/\-Llama\--3.1\--8B\--Instruct} as LLM, and a QFormer \citep{Li2023BLIP2BL} as adapter. The SpeechLMM is built to handle long audio inputs (\cref{sec:app:baselines}) and obtained by training only the adapter in two steps: (a) contrastive pretraining \citep{z√ºfle2024contrastivelearningtaskindependentspeechllmpretraining} to align the LLM representations for the speech and text modalities using MuST-C \citep{di-gangi-etal-2019-must} and Gigaspeech \citep{chen-2021-gigaspeech}, and (b) fine-tuning on instruction-following tasks, including ASR, speech translation, and spoken question answering using MuST-C and Spoken-SQuAD \citep{lee2018spoken}. Therefore, the model is not trained or fine-tuned on \DATASETNAME{} and operates in zero-shot for the SAG task.

\paragraph{End2End Finetuned.} A SpeechLLM trained using the same contrastive pretraining procedure as End2End Zero-Shot but subsequently fine-tuned on our \DATASETNAME{} dataset. 
This not only evaluates the direct impact of task-specific datasets on the SAG performance, but it also ensures the feasibility of the task and the suitability of the collected data.

\input{tables/baseline_models}

\subsubsection{Evaluation}
\paragraph{Metrics.} 
We use standard
(text) summarization metrics: \textbf{ROUGE} \citep{lin-2004-rouge} -- a text similarity metric that has been widely adopted for LM evaluation \citep{grusky-2023-rogue} that focuses on n-gram overlap between the hypothesis and reference --, and \textbf{BERTScore} \citep{DBLP:conf/iclr/ZhangKWWA20} -- a neural-based metric that measures the pairwise similarity of contextualized token embeddings between the summary and its reference. 
Also, we rely on \textbf{LLM-as-a-judge} \citep{shen-etal-2023-large,zheng-llm-judge-2024} 
 where the LLM\footnote{We use \texttt{Llama-3.1-8B-Instruct} \citep{dubey2024llama3herdmodels} as the judge using the prompts reported in \cref{fig:llm_as_a_judge} in \cref{sec:app-llm-as-a-judge}.} is prompted to assign a score to each output, using the reference abstract as context (Score with Expl.). The score is
  based on four criteria: 
(1) relevance, (2) coherence, (3) conciseness, and (4) factual accuracy.\footnote{(1) \textit{Does the predicted abstract capture the main points of the gold abstract?}, (2) \textit{Is the predicted abstract logically organized and easy to follow?}, (3) \textit{Is the predicted abstract free from unnecessary details?}, (4) \textit{Are the claims in the predicted abstract consistent with the gold abstract?}}
 We also report results where the LLM judge provides a single score without explanations (Plain Score), as well as results where it ranks the given abstracts instead of scoring them individually (Avg. Rank).
 
 All these metrics have known limitations and no metric is conclusively best for evaluating the SAG task: both ROUGE and BERTScore are known to fail to fully capture the extent to which two summaries share information \citep{deutsch-roth-2021-understanding} while LLM-as-a-judge is sensitive to prompt complexity and the length of input \citep{thakur2024judgingjudgesevaluatingalignment} and struggle to distinguish similar candidates \citep{shen-etal-2023-large}. For this reason, we complement 
the automatic scores with
 human evaluation.


\paragraph{Human Evaluation.}
For the human evaluation, 
9 annotators -- all experts in the field -- were provided 
with the generated abstracts and the ground truth abstract. We use the same randomly sampled 30 test set examples as in \cref{subsec:human_feasibility} and validate their representativeness, which is discussed in \cref{app:human_eval_ranking_model_outputs}.
Each sample is evaluated by three annotators. 
They follow the same criteria as the LLM evaluation but rank models instead of assigning scores. 
Detailed instructions are in \cref{app:human_eval_ranking_model_outputs}. 
As the End2End Zero-Shot model performance was comparable to that of Qwen2-Audio -- also being a zero-shot model -- and given that Qwen2-Audio is an established SpeechLLM with a distinct architecture, we exclude the End2End Zero-Shot from this analysis.





