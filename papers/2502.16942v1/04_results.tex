\subsection{Results}
\paragraph{Automatic Evaluation.}

\cref{tab:baselines} presents the performance of our models on the \DATASETNAME{} test set. Among them, the cascaded model (Whisper + Llama3.1-8B-Instruct) achieves the highest scores across all LLM-based evaluation metrics. Instead, looking at both n-gram- and neural-based metrics, the End2End Finetuned model achieves the highest RougeL and BERTScore. In addition, Qwen2-Audio and our End2End Zero-Shot models
demonstrate similar performance across all automatic metrics, showing a noticeable gap compared to the 
other two models. These results
highlight the importance of 
our dataset for building high-performing end-to-end models, as the substantial gap between the cascaded and End2End Zero-Shot models is effectively bridged through fine-tuning on the \DATASETNAME{} dataset.

For a more granular analysis, \cref{tab:baselines_with_llama_eval} in \cref{sec:app-llm-as-a-judge} provides 
results for the LLM-based 
metrics.
Given that all models except Qwen2-Audio rely on Llama3.1-8B-Instruct, one might 
question whether the Llama-based judge could introduce bias in favor of these models. To address this, we perform additional evaluations using \texttt{Qwen/Qwen2-7B} \citep{yang2024qwen2technicalreport} as the judge (\cref{tab:baselines_with_qwen_eval} in \cref{sec:app-llm-as-a-judge}), which confirm the same ranking,  eliminating any concerns about evaluator bias.

\paragraph{Human Evaluation.}
As shown in Table \ref{tab:baselines}, the human evaluation results closely align with the LLM-based  judgments: the cascaded model 
ranks first, followed closely by the finetuned model while Qwen2-Audio ranks last. Notably, the 
gap between the first two models is small, whereas the difference between the second and third models is substantial
-- consistent with the LLM-based evaluation. This suggests that automatic metrics reliably capture both subtle and large performance differences between models. IAA, measured using pairwise rankings \citep{bojar-etal-2016-findings} reached $\kappa = 0.53$, which is acceptable
given the close ranking of the top two systems.
