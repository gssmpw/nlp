\clearpage
\appendix
\input{fig/system_prompt_eval}
\input{tables/llama_as_a_judge}
\input{tables/qwen_as_judge}

\section{Human Evaluation: Are abstracts good summaries of the talk? }\label{app:human_eval_good_abstracts}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{img/talks_annotation_template.png}
    \caption{Instructions for annotators to evaluate whether the paper abstracts are good and informative abstracts for the ACL talks.}
    \label{fig:annoation_instructions_good_abstracts}
\end{figure*}

We want to analyze whether the paper abstracts can serve as good abstracts for the  *ACL talks. Therefore, we conduct a human evaluation, sampling 30 examples from our dataset. Detailed instructions for the human annotators can be found in \cref{fig:annoation_instructions_good_abstracts}.


\section{Baseline Details}\label{sec:app:baselines}
\paragraph{Generation Settings}
We evaluate four different models to establish baselines for abstract generation from spoken ACL talks. The evaluations were conducted on a single NVIDIA A100-SXM4-40GB GPU.

For all models, we use the default generation parameters and apply greedy search, following the usage instructions for \texttt{meta-llama/Llama-3.1-8B-Instruct}\footnote{\label{footnote_llama_url}\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} \citep{dubey2024llama3herdmodels}, \texttt{Qwen/Qwen2-Audio-7B-Instruct}\footnote{\label{footnote_qwen_url}\url{https://github.com/QwenLM/Qwen2-Audio}} \citep{chu2024qwen2audiotechnicalreport} and the contrastively pretrained models from \citet{züfle2024contrastivelearningtaskindependentspeechllmpretraining}\footnote{\label{footnote_llava_url}\url{https://github.com/MaikeZuefle/contr-pretraining}}.

\paragraph{Cascaded Model}
For the cascaded model, we segment the audio into 30-second chunks and transcribe them using \texttt{openai/whisper-large-v3} \citep{radford2022robustspeechrecognitionlargescale}. The transcribed chunks are then concatenated and processed by \texttt{meta-llama/Llama-3.1-8B-Instruct} \citep{dubey2024llama3herdmodels} to generate the abstract.  Inference took 5:40 hours on a single NVIDIA A100-SXM4-40GB GPU, including transcribing and summarizing.

Since the model’s outputs often included a title and category for the talk, we explicitly prompt it to generate only the abstract. This adjustment was not necessary for the other models.

We use the following prompt:

\lstset{basicstyle=\small} 
\noindent System Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n
\end{lstlisting}
Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
Summarize the following talk to create an abstract for an ACL Paper, don't include the title or other information, only the abstract:\n<transcription>\n
\end{lstlisting}

\paragraph{Qwen2-Audio}
For \texttt{Qwen/\-Qwen2-\-Audio-\-7B-\-Instruct} \citep{chu2024qwen2audiotechnicalreport}, inference took 50 minutes on a single NVIDIA A100-SXM4-40GB GPU.
We use the system prompt as provided in the code documentation\footref{footnote_qwen_url}.

\lstset{basicstyle=\small} 
\noindent System Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
You are a helpful assistant.
\end{lstlisting}
Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
Summarize this talk to create an abstract for an ACL Paper:\n
\end{lstlisting}
\input{tables/baseline_results_subset}


\paragraph{Contrastively Pretained Models}
For the contrastively pretrained model, we follow \citet{züfle2024contrastivelearningtaskindependentspeechllmpretraining} and adopt their settings\footref{footnote_llava_url}, including training configurations, hyperparameters, and system prompts. The SpeechLLM consists of HuBERT \citep{hubert-2021} as speech encoder, \texttt{meta\--llama/\-Llama\--3.1\--8B\--Instruct} as LLM, and a QFormer \citep{Li2023BLIP2BL} as adapter. We choose HuBERT as an encoder in contrast to the bigger and more powerful \texttt{openai/\-whisper-\-large-\-v3} \citep{radford2022robustspeechrecognitionlargescale}, as it needs less memory and is therefore more suitable for the summarization task of longer audio. 
However, due to the extended duration of the audio inputs, we additionally introduce two modifications:
\begin{enumerate}
    \item     We segment the audio into one-minute chunks, encode each chunk using the encoder and then concatenate the encoded representations before passing them through the adapter and LLM backbone.
    \item     We use a batch size of 1 for fine-tuning with NUTSHELL.
\end{enumerate}

Despite these adjustments, we encountered memory limitations for audio files exceeding 35 minutes. In such cases, we truncate the audio to 35 minutes, which affects one example in the test set.

The training of the models was conducted on four NVIDIA A100-SXM4-40GB GPUs. The contrastive pretraining took 33 hours on four GPUS. Finetuning on ASR, speech translation, and spoken question answering data took 30 hours, finetuning on the NUTSHELL dataset took 2:10 hours.
Generating the outputs of the test set (on a single NVIDIA A100-SXM4-40GB GPU) took 2:35 hours.

\noindent System Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n
\end{lstlisting}
Prompt:
\begin{lstlisting}[breaklines=true, breakindent=0pt]
Summarize this talk to create an abstract for an ACL Paper:
\end{lstlisting}

\section{Evaluation Details}
We evaluate the results of our models using automatic metrics including ROUGE, BERTScore, and LLM-as-a-judge.
\subsection{ROUGE and BERT Score}
As automatic metrics, we use ROUGE\footnote{} \citep{lin-2004-rouge} and BERTScore  \citep{DBLP:conf/iclr/ZhangKWWA20}. Concretely, we compute ROUGE-L, which focuses on the longest common subsequence, with \texttt{DD/sacrerouge} \citep{deutsch-roth-2020-sacrerouge}, as recommended by \citet{grusky-2023-rogue} and for BERTScore, we use the \texttt{bertscore} implementation from HuggingFace\footnote{\url{https://huggingface.co/spaces/evaluate-metric/bertscore}} and report the F1-score.


\subsection{LLM as a judge}\label{sec:app-llm-as-a-judge}
To evaluate the model outputs, we also use an LLM \textit{as a judge}, specifically \texttt{meta\--llama/\-Llama\--3.1\--8B\--Instruct}  \citep{dubey2024llama3herdmodels}. The LLM assigns a score to each output using the reference abstract as context, based on four criteria: (1) relevance (\textit{Does the predicted abstract capture the main points of the gold abstract?}), (2) coherence (\textit{Is the predicted abstract logically organized and easy to follow?}), (3) conciseness (\textit{Is the predicted abstract free from unnecessary details?}), and (4) factual accuracy (\textit{Are the claims in the predicted abstract consistent with the gold abstract?}). Additionally, we report results where the LLM provides a single overall score without explanations and results where it ranks the given abstracts instead of scoring them individually. The prompts are given in \cref{fig:llm_as_a_judge}.
If the model fails to return a valid json dictionary, we instead take the first number after the score name in the output. 
We present the results for all four criteria, the average score, the score without explanations, and the ranking in \cref{tab:baselines_with_llama_eval}.  One potential concern is that this LLM might be biased, as all our models except Qwen2-Audio are based on Llama-3.1. However, we find this is not the case. When using \texttt{Qwen/Qwen2-7B} \citep{yang2024qwen2technicalreport} as the judge, we obtain the same ranking as with Llama. The results with Qwen-as-a-judge can be found in \cref{tab:baselines_with_qwen_eval}.



\section{Human Evaluation for Ranking Model Outputs}\label{app:human_eval_ranking_model_outputs}

We evaluate the models using ROUGE \citep{lin-2004-rouge}, BERTScore \citep{DBLP:conf/iclr/ZhangKWWA20}, and LLM-as-a-judge. However, the first two metrics may not fully capture information overlap \citep{deutsch-roth-2021-understanding}, while LLM-as-a-judge is sensitive to different prompts \citep{thakur2024judgingjudgesevaluatingalignment} and struggles with distinguishing similar candidates \citep{shen-etal-2023-large}. To address these limitations, we complement our evaluation with human analysis. Specifically, we ask 9 domain experts to rank model outputs relative to the reference abstract, with each example annotated by three independent annotators. The annotation instructions are provided in \cref{fig:annotation_instructions_ranking}.

We conduct this human evaluation on a randomly selected subset of 30 test examples. We consider this subset representative, as the model rankings based on automatic metrics remain consistent with those on the full test set. The corresponding automatic scores for this subset are reported in \cref{tab:baselines_subset}. We want to include three diverse models in our human evaluation: a zero-shot model, a cascaded model, and a model finetuned on our dataset. Since we have two zero-shot models (Qwen2-Audio and our contrastively pretrained zero-shot model) that perform similarly, we decided to exclude one for efficiency in the human evaluation. We keep the Qwen2-Audio model as this is an already existing and widely used SpeechLLM. 


\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{img/ranking_annotation_template.png}
    \caption{Instructions for human annotators for ranking model outputs.}
    \label{fig:annotation_instructions_ranking}
\end{figure*}
