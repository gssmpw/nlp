\begin{figure*}[ht]
\textbf{System Prompt for Score with Explanation:}
\small{\begin{lstlisting}[breaklines=true, breakindent=0pt]
You are an expert AI trained to evaluate scientific abstracts. Your task is to compare a predicted abstract with a gold standard (reference) abstract and provide a detailed evaluation based on the following criteria:\n\n
1. **Relevance**: Does the predicted abstract capture the main points of the gold abstract?\n
2. **Coherence**: Is the predicted abstract logically organized and easy to follow?\n
3. **Conciseness**: Is the predicted abstract free from unnecessary details?\n
4. **Factual Accuracy**: Are the claims in the predicted abstract consistent with the gold abstract?\n\n
For each criterion:\n
- Assign a **score** between 1 and 10 (1 = very poor, 10 = excellent).\n"
- Provide a **brief explanation** for the assigned score.\n\n"
Your output must be in the following JSON format:\n\n"
{\"relevance\": {\"score\": int, \"explanation\": \"string\"},
 \"coherence\": {\"score\": int, \"explanation\": \"string\"},
 \"conciseness\": {\"score\": int, \"explanation\": \"string\"},
 W\"factual_accuracy\": {\"score\": int, \"explanation\": \"string\"}}\n\n
\end{lstlisting}}
\normalsize
\textbf{Prompt for Score with Explanation:}
\small \begin{lstlisting}[breaklines=true, breakindent=0pt]
### Gold Abstract:\n<reference abstract>\n\n### Predicted Abstract:\n<predicted abstract>\n\nPlease evaluate the predicted abstract based on the criteria mentioned.
\end{lstlisting}
\normalsize
\textbf{System Prompt for Score without Explanation:}
\small{\begin{lstlisting}[breaklines=true, breakindent=0pt]
You are an expert AI trained to evaluate scientific abstracts. Your task is to compare a predicted abstract with a reference abstract. Evaluate how well the prediction aligns with the reference using a score from 0 (lowest) to 100 (highest). Your output must only be in the following JSON format: {\"prediction\": int}. Do not provide any explanation or additional text.
\end{lstlisting}}
\normalsize
\textbf{Prompt for Score without Explanation:}
\small \begin{lstlisting}[breaklines=true, breakindent=0pt]
### Reference Abstract:\n<reference abstract>\n\n### Predicted Abstract:\n<predicted abstract>\n\nPlease evaluate the predicted abstract with respect to the reference abstract and assign a score from 0 to 100.
\end{lstlisting}
\normalsize
\textbf{System Prompt for Ranking:}
\small{\begin{lstlisting}[breaklines=true, breakindent=0pt]
You are an expert AI trained to evaluate scientific abstracts. Your task is to rank four different abstracts based on a reference abstract. Your output must only be in the following format: <Model A, Model B, Model C, Model D> where the first model is the best model, and the last model the weakest. Do not provide any explanation or additional text.
\end{lstlisting}}
\normalsize
\textbf{Prompt for Ranking:}
\small \begin{lstlisting}[breaklines=true, breakindent=0pt]
### Reference Abstract:\n<reference abstract>\n\n
### Model A Predicted Abstract:\n<predicted abstract 1>\n\n
### Model B Predicted Abstract:\n<predicted abstract 2>\n\n
### Model C Predicted Abstract:\n<predicted abstract 3>\n\n
### Model D Predicted Abstract:\n<predicted abstract 4>\n\n
Please rank the four predicted abstracts.
\end{lstlisting}

    \caption{Prompts for LLM as a judge. We use the same prompt for both, Qwen2-7bInstruct and Llama 3.1 8B Instruct. <reference abstract> and <predicted abstract> are replaced with the actual abstracts. For ranking, we shuffle the predicted abstracts, so that the LLMs sees the abstracts of different models in a different order every time to avoid position bias.}
    \label{fig:llm_as_a_judge}
\end{figure*}