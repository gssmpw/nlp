\begin{table*}[ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccccccccc}
    \toprule
       Model & \multicolumn{2}{c}{RougeL} & \multicolumn{2}{c}{BERTScore} & \multicolumn{4}{c}{Llama31-Instruct} & Human    \\

      &  Rec & F1 &  Rec & F1 & Expl & Score & Coherence & Relevance & Avg.\\
    \midrule
       Whisper + LLama31-Instruct  &  \textbf{0.5} & 0.40 & \textbf{0.7} & \textbf{0.465} & \textbf{0.70} &  \textbf{0.53} &  0.4  & 0.37 & \textbf{0.533} \\
       Qwen2-Audio & 0.0 & 0.07 & 0.0 & 0.07\phantom{0} &  0.03 & 0.0\phantom{0} & 0.0\phantom{0} & 0.0\phantom{0} & 0.033 \\
       Finetuned &   \textbf{0.5} & \textbf{0.53} & 0.3 & \textbf{0.465} & 0.27 & 0.47  & \textbf{0.6}\phantom{0} & \textbf{0.63} & 0.433 \\
    \bottomrule
    \end{tabular}%
    }
    \caption{Results of the human evaluation in comparison to automatic metrics. We report the average number of times the model was ranked first by the metric/human annotators (avg).}
    \label{tab:human_eval_ranking}
\end{table*}