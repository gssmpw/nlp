\begin{table*}[!ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccccccc}
    \toprule
       Model & \multicolumn{1}{c}{RougeL} & \multicolumn{1}{c}{BERTScore} & \multicolumn{3}{c}{Llama3.1-7B-Instruct}   & Human  (on subset) \\

      &   F1 $\uparrow$ &   F1 $\uparrow$ & Score with Expl. $\uparrow$ & Plain Score $\uparrow$ & Avg. Rank $\downarrow$ &  Avg. Rank $\downarrow$\\
    \midrule
       Whisper + LLama3.1-8B-Instruct  &  22.14 & 86.62 & \textbf{77.84} & \textbf{82.47} & \textbf{ 1.24} &  \textbf{1.53} \\
       Qwen2-Audio-7B-Instruct &  15.02 & 84.65 & 45.57 &   36.81 & 3.43 & 2.87\\
       End2End Finetuned &  \textbf{23.89} & \textbf{86.66} & 68.78 &  73.53 & 1.98 & 1.6\phantom{0} \\
       End2End Zero-Shot & 16.08 & 84.13 & 45.97 & 39.90 & 3.35 & N/A\\
    \bottomrule
    \end{tabular}%
    }
    \caption{We report results on the \DATASETNAME{} test set for four models: a cascaded approach (Whisper+Llama-3.1-8B-Instruct), an existing SpeechLLM (Qwen2-Audio), and an end-to-end \texttt{HuBERT+\-QFormer+\-Llama3.1-\-8B-\-Instruct} model, either finetuned on our data (\textit{End2End Finetuned }) or trained on audio instruction-following data (\textit{End2End Zero-Shot}). Avg. Rank, assigned by an LLM judge or human annotators, reflects the mean ranking per model. \vspace{-0.3cm}}
    
    \label{tab:baselines}
\end{table*}



    % 