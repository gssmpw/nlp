\begin{table*}[!ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccccccccccc}
    \toprule
       Model & \multicolumn{7}{c}{Llama-3.1-8B-Instruct}  \\

      & Relevance $\uparrow$ & Coherence $\uparrow$ & Conciseness $\uparrow$ & Factual Accuracy $\uparrow$ & Avg. Score $\uparrow$ & Plain Score $\uparrow$ &  Avg. Rank $\downarrow$\\
    \midrule
       Whisper + LLama31-Instruct  &  \textbf{77.12} & \textbf{86.00} & \textbf{61.13} & \textbf{87.13} & \textbf{77.84} & \textbf{82.47} & \textbf{1.24}\\
       Qwen2-Audio & 37.21 & 52.52 & 45.91 & 46.63 & 45.57 & 36.81 & 3.43\\
       End2End Finetuned & 66.41 & 78.24 & 50.25 & 80.22 & 68.78 & 73.53 & 1.98 \\
       End2End Zero-Shot & 40.28 &   48.02&  37.69 &  57.89&  45.97 & 39.90 & 3.35\\
    \bottomrule
    \end{tabular}%
    }
    \caption{Results using  Llama-3.1-8B-Instruct as a judge. We report results on the \DATASETNAME{} test set for four models: a cascaded approach (\texttt{openai/whisper-large-v3} + \texttt{meta/Llama-3.1-8B-Instruct}), \texttt{Qwen/Qwen2-Audio-7B-Instruct}, and an end-to-end \texttt{HuBERT+QFormer+Llama3.1-7B-Instruct} model, either finetuned on our data (\textit{End2End Finetuned }) or trained on audio instruction-following data (\textit{End2End Zero-Shot}). Avg. Rank reflects the mean ranking per model.}
    \label{tab:baselines_with_llama_eval}
\end{table*}

