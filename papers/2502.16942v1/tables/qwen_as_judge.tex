\begin{table*}[!ht]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccccccccccc}
    \toprule
       Model & \multicolumn{7}{c}{Qwen2-7bInstruct} \\

      & Relevance $\uparrow$ & Coherence $\uparrow$ & Conciseness $\uparrow$ & Factual Accuracy $\uparrow$ & Avg. Score $\uparrow$ & Plain Score $\uparrow$ &  Avg. Rank $\downarrow$\\
    \midrule
       Whisper + LLama31-Instruct  & \textbf{79.61} & \textbf{83.54} & 72.08 & \textbf{86.07} & \textbf{80.33} & \textbf{74.60} & \textbf{1.66} \\
       Qwen2-Audio & 56.99 & 75.35 & \textbf{75.91} & 59.28 & 66.88 & 49.55 & 3.18\\
       End2End Finetuned & 75.13 & 81.78 & 75.04 & 81.16 & 78.28 & 70.83 & 2.12 \\
       End2End Zero-Shot & 57.93 & 68.02& 69.34 & 66.65& 65.49& 53.61 & 3.04 \\
    \bottomrule
    \end{tabular}%
    }
    \caption{Results using Qwen2-7bInstruct as a judge. We report results on the \DATASETNAME{} test set for four models: a cascaded approach (\texttt{openai/whisper-large-v3} + \texttt{meta/Llama-3.1-8B-Instruct}), \texttt{Qwen/Qwen2-Audio-7B-Instruct}, and an end-to-end \texttt{HuBERT+QFormer+Llama3.1-7B-Instruct} model, either finetuned on our data (\textit{End2End Finetuned }) or trained on audio instruction-following data (\textit{End2End Zero-Shot}). Avg. Rank reflects the mean ranking per model.}
    \label{tab:baselines_with_qwen_eval}
\end{table*}

