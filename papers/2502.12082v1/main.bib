@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}

@article{zaheer2020bigbird,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{velivckovic2024softmax,
  title={softmax is not enough (for sharp out-of-distribution)},
  author={Veli{\v{c}}kovi{\'c}, Petar and Perivolaropoulos, Christos and Barbero, Federico and Pascanu, Razvan},
  booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
  year={2024}
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{thakur2021beir,
 author = {Thakur, Nandan and Reimers, Nils and R\"{u}ckl\'{e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
 booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
 editor = {J. Vanschoren and S. Yeung},
 pages = {},
 title = {BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
 url = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper-round2.pdf},
 volume = {1},
 year = {2021}
}


@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{triton-paper,
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
title = {Triton: an intermediate language and compiler for tiled neural network computations},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315508.3329973},
doi = {10.1145/3315508.3329973},
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {10–19},
numpages = {10},
keywords = {neural networks, compiler, GPU},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}

@misc{veličković2024softmaxforsharpoutofdistribution,
      title={softmax is not enough (for sharp out-of-distribution)}, 
      author={Petar Veličković and Christos Perivolaropoulos and Federico Barbero and Razvan Pascanu},
      year={2024},
      eprint={2410.01104},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01104}, 
}

@misc{ye2024differentialtransformer,
      title={Differential Transformer}, 
      author={Tianzhu Ye and Li Dong and Yuqing Xia and Yutao Sun and Yi Zhu and Gao Huang and Furu Wei},
      year={2024},
      eprint={2410.05258},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.05258}, 
}




@article{cao2022mobivqa,
	title = {{MobiVQA}: {Efficient} {On}-{Device} {Visual} {Question} {Answering}},
	volume = {6},
	shorttitle = {{MobiVQA}},
	url = {https://doi.org/10.1145/3534619},
	doi = {10.1145/3534619},
	number = {2},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Cao, Qingqing and Khanna, Prerna and Lane, Nicholas D. and Balasubramanian, Aruna},
	month = jul,
	year = {2022},
	keywords = {edge computing, mobile computing, on-device applications, visual question answering},
	pages = {44:1--44:23},
}

@article{kim2005adaptive,
  title={Adaptive weighted-sum method for bi-objective optimization: Pareto front generation},
  author={Kim, Il Yong and De Weck, Oliver L},
  journal={Structural and multidisciplinary optimization},
  volume={29},
  number={2},
  pages={149--158},
  year={2005},
  publisher={Springer}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  url = {https://arxiv.org/abs/2107.03374},
  year={2021}
}

@Article{tsallis,
author={Tsallis, Constantino},
title={Possible generalization of Boltzmann-Gibbs statistics},
journal={Journal of Statistical Physics},
year={1988},
month={Jul},
day={01},
volume={52},
number={1},
pages={479-487},
abstract={With the use of a quantity normally scaled in multifractals, a generalized form is postulated for entropy, namelySq≡k [1 -- ∑i=1Wpiq]/(q-1), whereq∈ℝ characterizes the generalization andpi are the probabilities associated withW (microscopic) configurations (W∈ℕ). The main properties associated with this entropy are established, particularly those corresponding to the microcanonical and canonical ensembles. The Boltzmann-Gibbs statistics is recovered as theq{\textrightarrow}1 limit.},
issn={1572-9613},
doi={10.1007/BF01016429},
url={https://doi.org/10.1007/BF01016429}
}

@ARTICLE{sort-and-scan,
  title    = "Validation of subgradient optimization",
  author   = "Held, Michael and Wolfe, Philip and Crowder, Harlan P",
  abstract = "The ``relaxation'' procedure introduced by Held and Karp for
              approximately solving a large linear programming problem related
              to the traveling-salesman problem is refined and studied
              experimentally on several classes of specially structured
              large-scale linear programming problems, and results on the use
              of the procedure for obtaining exact solutions are given. It is
              concluded that the method shows promise for large-scale linear
              programming",
  journal  = "Mathematical Programming",
  volume   =  6,
  number   =  1,
  pages    = "62--88",
  month    =  dec,
  year     =  1974
}

@inproceedings{pivot-partition,
author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
title = {Efficient projections onto the l1-ball for learning in high dimensions},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390191},
doi = {10.1145/1390156.1390191},
abstract = {We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the l1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with l1 projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {272–279},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{chalkidis-etal-2021-paragraph-ecthr2,
    title = "Paragraph-level Rationale Extraction through Regularization: A case study on {E}uropean Court of Human Rights Cases",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Manos  and
      Tsarapatsanis, Dimitrios  and
      Aletras, Nikolaos  and
      Androutsopoulos, Ion  and
      Malakasiotis, Prodromos",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.22/",
    doi = "10.18653/v1/2021.naacl-main.22",
    pages = "226--241",
    abstract = "Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research."
}


@inproceedings{chalkidis-etal-2019-neural-ecthr1,
    title = "Neural Legal Judgment Prediction in {E}nglish",
    author = "Chalkidis, Ilias  and
      Androutsopoulos, Ion  and
      Aletras, Nikolaos",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1424/",
    doi = "10.18653/v1/P19-1424",
    pages = "4317--4323",
    abstract = "Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case`s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT`s length limitation."
}


@inproceedings{torchcompile,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640366},
doi = {10.1145/3620665.3640366},
abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {929–947},
numpages = {19},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{
chen2021scatterbrain,
title={Scatterbrain: Unifying Sparse and Low-rank Attention},
author={Beidi Chen and Tri Dao and Eric Winsor and Zhao Song and Atri Rudra and Christopher R{\'e}},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=SehIKudiIo1}
}

@article{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{
peng2021randomattention,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=QtTKTdVrFBB}
}

@inproceedings{
choromanski2021rethinkingperformer,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@article{warner2024smarter,
  title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
  author={Warner, Benjamin and Chaffin, Antoine and Clavi{\'e}, Benjamin and Weller, Orion and Hallstr{\"o}m, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and others},
  journal={arXiv preprint arXiv:2412.13663},
  year={2024}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@misc{lozhkov2024fineweb-edu,
    author       = { Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas },  
    title        = { FineWeb-Edu: the Finest Collection of Educational Content }, 
    year         = 2024,  
    url          = { https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu },  
    doi          = { 10.57967/hf/2497 },
    publisher    = { Hugging Face }
}

@article{jiang2024minference,
    title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
    author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
    journal={arXiv preprint arXiv:2407.02490},
    year={2024}
}

@misc{shah2024flashattention3,
      title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}, 
      author={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
      year={2024},
      eprint={2407.08608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08608}, 
}

@misc{dong2024flexattentionprogrammingmodel,
      title={Flex Attention: A Programming Model for Generating Optimized Attention Kernels}, 
      author={Juechu Dong and Boyuan Feng and Driss Guessous and Yanbo Liang and Horace He},
      year={2024},
      eprint={2412.05496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05496}, 
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446/",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
}


@article{condats,
author = {Condat, Laurent},
title = {Fast projection onto the simplex and the $\ell_1$ ball},
year = {2016},
issue_date = {July      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {158},
number = {1–2},
issn = {0025-5610},
url = {https://doi.org/10.1007/s10107-015-0946-6},
doi = {10.1007/s10107-015-0946-6},
abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an $$l_1$$l1-norm ball. It can be viewed as a Gauss---Seidel-like variant of Michelot's variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
journal = {Math. Program.},
month = jul,
pages = {575–585},
numpages = {11},
keywords = {$$l_1$$l1-Norm ball, 49M30, 65C60, 65K05, 90C25, Large-scale optimization, Simplex}
}

@article{softmax-varform,
author = {Wainwright, Martin J. and Jordan, Michael I.},
title = {Graphical Models, Exponential Families, and Variational Inference},
year = {2008},
issue_date = {January 2008},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {1},
number = {1–2},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000001},
doi = {10.1561/2200000001},
abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
journal = {Found. Trends Mach. Learn.},
month = jan,
pages = {1–305},
numpages = {305}
}

@article{halleys-method,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2975033},
 author = {T. R. Scavo and J. B. Thoo},
 journal = {The American Mathematical Monthly},
 number = {5},
 pages = {417--426},
 publisher = {Mathematical Association of America},
 title = {On the Geometry of Halley's Method},
 urldate = {2025-01-17},
 volume = {102},
 year = {1995}
}


@article{bisection-linear-proof,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2322546},
 author = {Edwin H. Kaufman and Terry D. Lenker},
 journal = {The American Mathematical Monthly},
 number = {1},
 pages = {48--51},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Linear Convergence and the Bisection Algorithm},
 urldate = {2025-01-17},
 volume = {93},
 year = {1986}
}



@InProceedings{blondel-entmax,
  title = 	 {Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms},
  author =       {Blondel, Mathieu and Martins, Andre and Niculae, Vlad},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {606--615},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/blondel19a/blondel19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/blondel19a.html},
  abstract = 	 {This paper studies Fenchel-Young losses, a generic way to construct convex loss functions from a regularization function.  We analyze their properties in depth, showing that they unify many well-known loss functions and allow to create useful new ones easily.  Fenchel-Young losses constructed from a generalized entropy, including the Shannon and Tsallis entropies, induce predictive probability distributions.  We formulate conditions for a generalized entropy to yield losses with a separation margin, and probability distributions with sparse support.  Finally, we derive efficient algorithms, making Fenchel-Young losses appealing both in theory and practice.}
}


@InProceedings{martins-sparsemax,
  title = 	 {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
  author = 	 {Martins, Andre and Astudillo, Ramon},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1614--1623},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/martins16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/martins16.html},
  abstract = 	 {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.}
}


@inproceedings{correia-etal-2019-adaptively,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M.  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1223/",
    doi = "10.18653/v1/D19-1223",
    pages = "2174--2184",
    abstract = "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter {--} which controls the shape and sparsity of alpha-entmax {--} allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
}


@article{bender-friedman-2018-data,
    title = "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
    author = "Bender, Emily M.  and
      Friedman, Batya",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1041",
    doi = "10.1162/tacl_a_00041",
    pages = "587--604",
    abstract = "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
}



@article{thompson2020computational,
  title={The computational limits of deep learning},
  author={Thompson, Neil C and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F},
  journal={arXiv preprint arXiv:2007.05558},
  url={https://arxiv.org/abs/2007.05558},
  year={2020}
}

@article{liu2021gpt,
  title={{GPT understands, too}},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  url={https://arxiv.org/abs/2103.10385},
  year={2021}
}

@inproceedings{moosavi-etal-2022-adaptable,
    title = "Adaptable Adapters",
    author = "Moosavi, Nafise  and
      Delfosse, Quentin  and
      Kersting, Kristian  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.274",
    doi = "10.18653/v1/2022.naacl-main.274",
    pages = "3742--3753",
    abstract = "State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture{---}i.e., the same adapter layer on top of each layer of the pretrained model{---}for every dataset, regardless of the properties of the dataset or the amount of available training data. In this work, we introduce adaptable adapters that contain (1) learning different activation functions for different layers and different input data, and (2) a learnable switch to select and only use the beneficial adapter layers. We show that adaptable adapters achieve on-par performances with the standard adapter architecture while using a considerably smaller number of adapter layers. In addition, we show that the selected adapter architecture by adaptable adapters transfers well across different data settings and similar tasks. We propose to use adaptable adapters for designing efficient and effective adapter architectures. The resulting adapters (a) contain about 50{\%} of the learning parameters of the standard adapter and are therefore more efficient at training and inference, and require less storage space, and (b) achieve considerably higher performances in low-data settings.",
}

@inproceedings{pfeiffer-etal-2020-adapterhub,
    title = "{A}dapter{H}ub: A Framework for Adapting Transformers",
    author = {Pfeiffer, Jonas  and
      R{\"u}ckl{\'e}, Andreas  and
      Poth, Clifton  and
      Kamath, Aishwarya  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.7",
    doi = "10.18653/v1/2020.emnlp-demos.7",
    pages = "46--54",
    abstract = "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters{---}small learnt bottleneck layers inserted within each layer of a pre-trained model{---} ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic {``}stiching-in{''} of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml",
}


@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{lee-etal-2020-empowering,
    title = "{E}mpowering {A}ctive {L}earning to {J}ointly {O}ptimize {S}ystem and {U}ser {D}emands",
    author = "Lee, Ji-Ung  and
      Meyer, Christian M.  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.390",
    doi = "10.18653/v1/2020.acl-main.390",
    pages = "4233--4247",
    abstract = "Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.",
}

@inproceedings{klie-etal-2020-zero,
    title = "{F}rom {Z}ero to {H}ero: {H}uman-{I}n-{T}he-{L}oop {E}ntity {L}inking in {L}ow {R}esource {D}omains",
    author = "Klie, Jan-Christoph  and
      Eckart de Castilho, Richard  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.624",
    doi = "10.18653/v1/2020.acl-main.624",
    pages = "6982--6993",
    abstract = "Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35{\%} compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.",
}

@inproceedings{margatina-etal-2021-active,
    title = "Active Learning by Acquiring Contrastive Examples",
    author = {Margatina, Katerina  and
      Vernikos, Giorgos  and
      Barrault, Lo{\"\i}c  and
      Aletras, Nikolaos},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.51",
    doi = "10.18653/v1/2021.emnlp-main.51",
    pages = "650--663",
    abstract = "Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.",
}
@inproceedings{
Ash2020Deep,
title={Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds},
author={Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryghZJBKPS}
}

@InProceedings{cortes2008sampling-bias,
author="Cortes, Corinna
and Mohri, Mehryar
and Riley, Michael
and Rostamizadeh, Afshin",
editor="Freund, Yoav
and Gy{\"o}rfi, L{\'a}szl{\'o}
and Tur{\'a}n, Gy{\"o}rgy
and Zeugmann, Thomas",
title="Sample Selection Bias Correction Theory",
booktitle="Algorithmic Learning Theory",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="38--53",
abstract="This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of distributional stability which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm.",
isbn="978-3-540-87987-9"
}


@inproceedings{
wu2021when,
title={When Do Curricula Work?},
author={Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=tW4QEInpni}
}

@inproceedings{baldock2021,
 author = {Baldock, Robert and Maennel, Hartmut and Neyshabur, Behnam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10876--10889},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning Through the Lens of Example Difficulty},
 url = {https://proceedings.neurips.cc/paper/2021/file/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html},
 volume = {34},
 year = {2021}
}

@inproceedings{liu-etal-2018-learning,
    title = "Learning to Actively Learn Neural Machine Translation",
    author = "Liu, Ming  and
      Buntine, Wray  and
      Haffari, Gholamreza",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-1033",
    doi = "10.18653/v1/K18-1033",
    pages = "334--344",
    abstract = "Traditional active learning (AL) methods for machine translation (MT) rely on heuristics. However, these heuristics are limited when the characteristics of the MT problem change due to e.g. the language pair or the amount of the initial bitext. In this paper, we present a framework to learn sentence selection strategies for neural MT. We train the AL query strategy using a high-resource language-pair based on AL simulations, and then transfer it to the low-resource language-pair of interest. The learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions.",
}

@inproceedings{li-etal-2020-active-learning,
    title = "Active Learning for Coreference Resolution using Discrete Annotation",
    author = "Li, Belinda Z.  and
      Stanovsky, Gabriel  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.738",
    doi = "10.18653/v1/2020.acl-main.738",
    pages = "8320--8331",
    abstract = "We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.",
}

@inproceedings{bodo2011active,
  title={Active learning with clustering},
  author={Bod{\'o}, Zal{\'a}n and Minier, Zsolt and Csat{\'o}, Lehel},
  booktitle={Active Learning and Experimental Design workshop In conjunction with AISTATS 2010},
  pages={127--139},
  year={2011},
  url={http://proceedings.mlr.press/v16/bodo11a/bodo11a.pdf},
  organization={JMLR Workshop and Conference Proceedings}
}

@proceedings{ws-2018-nlp,
    title = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    editor = "Park, Eunjeong L.  and
      Hagiwara, Masato  and
      Milajevs, Dmitrijs  and
      Tan, Liling",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2500",
}

@article{zhang-duh-2020-reproducible,
    title = "Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems",
    author = "Zhang, Xuan  and
      Duh, Kevin",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.26",
    doi = "10.1162/tacl_a_00322",
    pages = "393--408",
    abstract = "Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model{'}s architecture or training recipe can mean the difference between a positive and negative research result or between a state-of-the-art and underperforming system. While recent literature has proposed methods for automatic hyperparameter optimization (HPO), there has been limited work on applying these methods to neural machine translation (NMT), due in part to the high costs associated with experiments that train large numbers of model variants. To facilitate research in this space, we introduce a lookup-based approach that uses a library of pre-trained models for fast, low cost HPO experimentation. Our contributions include (1) the release of a large collection of trained NMT models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for evaluating HPO methods on NMT, and (3) a reproducible benchmark of several HPO methods against our model library, including novel graph-based and multiobjective methods.",
}

@inproceedings{dong2017learning,
 author = {Dong, Xin and Chen, Shangyu and Pan, Sinno},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon},
 url = {https://proceedings.neurips.cc/paper/2017/file/c5dc3e08849bec07e33ca353de62ea04-Abstract.html},
 volume = {30},
 year = {2017}
}

@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks.},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}

@inproceedings{sun-etal-2020-mobilebert,
    title = "{M}obile{BERT}: a Compact Task-Agnostic {BERT} for Resource-Limited Devices",
    author = "Sun, Zhiqing  and
      Yu, Hongkun  and
      Song, Xiaodan  and
      Liu, Renjie  and
      Yang, Yiming  and
      Zhou, Denny",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.195",
    doi = "10.18653/v1/2020.acl-main.195",
    pages = "2158--2170",
    abstract = "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT{\_}LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT{\_}LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT{\_}BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT{\_}BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT{\_}BASE).",
}



@inproceedings{hinton-2015,
    title	= {Distilling the Knowledge in a Neural Network},
    author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
    year	= {2015},
    URL	= {http://arxiv.org/abs/1503.02531},
    booktitle	= {NeurIPS Deep Learning and Representation Learning Workshop}
}




@article{PAULLADA2021100336,
title = {Data and its (dis)contents: A survey of dataset development and use in machine learning research},
journal = {Patterns},
volume = {2},
number = {11},
pages = {100336},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100336},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001847},
author = {Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily Denton and Alex Hanna},
keywords = {datasets machine learning},
abstract = {Summary
In this work, we survey a breadth of literature that has revealed the limitations of predominant practices for dataset collection and use in the field of machine learning. We cover studies that critically review the design and development of datasets with a focus on negative societal impacts and poor outcomes for system performance. We also cover approaches to filtering and augmenting data and modeling techniques aimed at mitigating the impact of bias in datasets. Finally, we discuss works that have studied data practices, cultures, and disciplinary norms and discuss implications for the legal, ethical, and functional challenges the field continues to face. Based on these findings, we advocate for the use of both qualitative and quantitative approaches to more carefully document and analyze datasets during the creation and usage phases.}
}


@InProceedings{iofinova2021well,
    author    = {Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
    title     = {How Well Do Sparse ImageNet Models Transfer?},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    url       = {https://openaccess.thecvf.com/content/CVPR2022/html/Iofinova_How_Well_Do_Sparse_ImageNet_Models_Transfer_CVPR_2022_paper.html},
    pages     = {12266-12276}
}


@article{liu2021video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}
@inproceedings{vaswani2021scaling,
  title={Scaling local self-attention for parameter efficient visual backbones},
  author={Vaswani, Ashish and Ramachandran, Prajit and Srinivas, Aravind and Parmar, Niki and Hechtman, Blake and Shlens, Jonathon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12894--12904},
  year={2021}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{dehghani2018universal,
    title={Universal Transformers},
    author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

@inproceedings{lan2019albert,
  title={{ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@inproceedings{reid-etal-2021-subformer-exploring,
    title = "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers",
    author = "Reid, Machel  and
      Marrese-Taylor, Edison  and
      Matsuo, Yutaka",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.344",
    doi = "10.18653/v1/2021.findings-emnlp.344",
    pages = "4081--4090",
    abstract = "Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.",
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={International conference on machine learning},
  pages={4651--4664},
  year={2021},
  organization={PMLR}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{levine2020pmi,
  title={PMI-Masking: Principled masking of correlated spans},
  author={Levine, Yoav and Lenz, Barak and Lieber, Opher and Abend, Omri and Leyton-Brown, Kevin and Tennenholtz, Moshe and Shoham, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@misc{mosaicml2022composer,
    author = {The Mosaic ML Team},
    title = {composer},
    year = {2021},
    howpublished = {\url{https://github.com/mosaicml/composer/}},
}

@inproceedings{kundu2021attentionlite,
  title={Attentionlite: Towards efficient self-attention models for vision},
  author={Kundu, Souvik and Sundaresan, Sairam},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2225--2229},
  year={2021},
  organization={IEEE}
}
@inproceedings{he2019streaming,
  title={Streaming end-to-end speech recognition for mobile devices},
  author={He, Yanzhang and Sainath, Tara N and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and others},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6381--6385},
  year={2019},
  organization={IEEE}
}

@inproceedings{Cao2021IrEneIE,
    title = "{I}r{E}ne: Interpretable Energy Prediction for Transformers",
    author = "Cao, Qingqing  and
      Lal, Yash Kumar  and
      Trivedi, Harsh  and
      Balasubramanian, Aruna  and
      Balasubramanian, Niranjan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.167",
    doi = "10.18653/v1/2021.acl-long.167",
    pages = "2145--2157",
}

@inproceedings{cao-etal-2020-towards,
    title = "Towards Accurate and Reliable Energy Measurement of {NLP} Models",
    author = "Cao, Qingqing  and
      Balasubramanian, Aruna  and
      Balasubramanian, Niranjan",
    booktitle = "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sustainlp-1.19",
    doi = "10.18653/v1/2020.sustainlp-1.19",
    pages = "141--148",
    abstract = "Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https://github.com/csarron/sustainlp2020-energy.",
}

@misc{Barham2022,
  doi = {10.48550/ARXIV.2203.12533},
  
  url = {https://arxiv.org/abs/2203.12533},
  
  author = {Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey, Laurent El and Thekkath, Chandramohan A. and Wu, Yonghui},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Pathways: Asynchronous Distributed Dataflow for ML},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{RFA,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah and Kong, Lingpeng},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?continueFlag=7fcad3444ce73135efa053c0f6709de5&id=QtTKTdVrFBB}
}

@article{lewis2020retrieval,
  title={{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Abstract.html}
}

@article{khandelwal2019generalization,
  title={\href{https://arxiv.org/abs/1911.00172}{Generalization through memorization: Nearest neighbor language models}},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{gu2018search,
	doi = {10.48550/ARXIV.1705.07267},
	author = {Gu, Jiatao and Wang, Yong and Cho, Kyunghyun and Li, Victor O. K.},
	title = {Search Engine Guided Non-Parametric Neural Machine Translation},
	year = {2018},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	url={https://ojs.aaai.org/index.php/AAAI/article/view/12013},
}

@inproceedings{martins2022efficient,
    title = "Efficient Machine Translation Domain Adaptation",
    author = "Martins, Pedro  and
      Marinho, Zita  and
      Martins, Andre",
    booktitle = "Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.spanlp-1.3",
    doi = "10.18653/v1/2022.spanlp-1.3",
    pages = "23--29",
}

@inproceedings{he2021efficient,
  title={\href{https://arxiv.org/abs/2109.04212}{Efficient Nearest Neighbor Language Models}},
  author={He, Junxian and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5703--5714},
  year={2021}
}

@article{yogatama2021adaptive,
  title={\href{https://arxiv.org/abs/2102.02557}{Adaptive Semiparametric Language Models}},
  author={Yogatama, Dani and de Masson d’Autume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  publisher={MIT Press}
}

@misc{Rajbhandari2022,
  doi = {10.48550/ARXIV.2201.05596},
  url = {https://arxiv.org/abs/2201.05596},
  
  author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{kaleab2021,
  doi = {10.48550/ARXIV.2102.01670},
  author = {Tessera, Kale-ab and Hooker, Sara and Rosman, Benjamin},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization},
  
  publisher = {arXiv},
  url={https://arxiv.org/abs/2102.01670},
  year = {2021},
  journal={arXiv preprint arXiv:2102.01670},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{xu-etal-2021-beyond,
    title = "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of {BERT} Compression",
    author = "Xu, Canwen  and
      Zhou, Wangchunshu  and
      Ge, Tao  and
      Xu, Ke  and
      McAuley, Julian  and
      Wei, Furu",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.832",
    doi = "10.18653/v1/2021.emnlp-main.832",
    pages = "10653--10659",
    abstract = "Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module replacing with loyalty and robustness. By combining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness.",
}

@inproceedings{Lepikhin2021GShardSG,
    title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@inproceedings{liu-etal-2020-fastbert,
    title = "{F}ast{BERT}: a Self-distilling {BERT} with Adaptive Inference Time",
    author = "Liu, Weijie  and
      Zhou, Peng  and
      Wang, Zhiruo  and
      Zhao, Zhe  and
      Deng, Haotang  and
      Ju, Qi",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.537",
    doi = "10.18653/v1/2020.acl-main.537",
    pages = "6035--6044",
    abstract = "Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.",
}

@inproceedings{jiao-etal-2020-tinybert,
    title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
    author = "Jiao, Xiaoqi  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Chen, Xiao  and
      Li, Linlin  and
      Wang, Fang  and
      Liu, Qun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.372",
    doi = "10.18653/v1/2020.findings-emnlp.372",
    pages = "4163--4174",
    abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}


@inproceedings{Jiaao2022,
author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
title = {FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508418},
doi = {10.1145/3503221.3508418},
abstract = {The current trend in deep learning is to scale models to extremely large sizes with the objective of increasing their accuracy. Mixture-of-Expert (MoE) is the most popular pre-trained model that makes feasible the training of models with parameters beyond trillion-scale. Thanks to the dynamic activation of experts, i.e., shallow layers specialized in certain domains, it allows for sparse training of bigger models, removing the linearity between model size and computation. However, different from traditional deep learning models, it draws huge challenges to the efficiency of these training systems, including dynamic load imbalance, inefficient synchronous execution mode, and congested all-to-all communication.To address these challenges, we first propose a performance model that can both accurately predict the latency of different operations of a specific training task, and intuitively analyze its end-to-end performance via a novel roofline-like model. Then, guided by this model, we invent a dynamic shadowing approach to cope with load imbalance, and a smart fine-grained schedule that splits different operations and executes them concurrently. We design a congestion-avoiding expert selection strategy that relieves network congestion for the lower latency of iterations, when modification of expert selection is allowed. We implement and integrate the above optimizations as a general system, FasterMoE, empowering efficient distributed MoE model training. FasterMoE is evaluated on different cluster systems using up to 64 GPUs. It achieves 1.37X - 17.87X speedup compared with state-of-the-art systems for large models, including ZeRO, GShard, and BASE Layer.Source code of FasterMoE is now available at https://github.com/thu-pacman/FasterMoE.},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {120–134},
numpages = {15},
keywords = {parallelism, distributed deep learning, performance modeling},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}


@InProceedings{pmlr-v119-evci20a,
  title = 	 {Rigging the Lottery: Making All Tickets Winners},
  author =       {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2943--2952},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/evci20a/evci20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/evci20a.html},
  abstract = 	 {Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.}
}


@article{Shen2019ArgusAE,
  title={Argus: An End-to-End Framework for Accelerating CNNs on FPGAs},
  author={Yongming Shen and Tianchu Ji and Michael Ferdman and Peter Milder},
  journal={IEEE Micro},
  year={2019},
  volume={39},
  pages={17-25}
}

@inproceedings{choromanski2020rethinking,
    title={Rethinking Attention with Performers},
    author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{roy2021efficient,
    title = "Efficient Content-Based Sparse Attention with Routing Transformers",
    author = "Roy, Aurko  and
      Saffar, Mohammad  and
      Vaswani, Ashish  and
      Grangier, David",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.4",
    doi = "10.1162/tacl_a_00353",
    pages = "53--68",
}

@article{zaheer2020big,
  title={{Big Bird: Transformers for Longer Sequences}},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17283--17297},
  year={2020},
  url={https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html}
}

@inproceedings{
stock2021training,
title={Training with Quantization Noise for Extreme Model Compression},
author={Pierre Stock and Angela Fan and Benjamin Graham and Edouard Grave and R{\'e}mi Gribonval and Herve Jegou and Armand Joulin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=dV19Yyi1fS3}
}

@inproceedings{kitaev2020reformer,
    title={Reformer: The Efficient Transformer},
    author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@inproceedings{rae2019compressive,
    title={Compressive Transformers for Long-Range Sequence Modelling},
    author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SylKikSYDH}
}

@inproceedings{dao2021,
    title={Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
    author={Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=Nfl-iXa-y7R}
}


@inproceedings{dai2019transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
}

@article{Hooker2021,
author = {Hooker, Sara},
year = {2021},
month = {12},
pages = {58-65},
title = {The hardware lottery},
volume = {64},
journal = {Communications of the ACM},
doi = {10.1145/3467017}
}

@article{wu2022extreme,
  title={Extreme Compression for Pre-trained Transformers Made Simple and Efficient},
  author={Wu, Xiaoxia and Yao, Zhewei and Zhang, Minjia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01859},
  url={https://arxiv.org/abs/2206.01859},
  year={2022}
}

@proceedings{ngt-2020-neural,
    title = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Heafield, Kenneth  and
      Junczys-Dowmunt, Marcin  and
      Konstas, Ioannis  and
      Li, Xian  and
      Neubig, Graham  and
      Oda, Yusuke",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.0",
}

@inproceedings{ruckle-etal-2021-adapterdrop,
    title = "{AdapterDrop}: {O}n the Efficiency of Adapters in Transformers",
    author = {R{\"u}ckl{\'e}, Andreas  and
      Geigle, Gregor  and
      Glockner, Max  and
      Beck, Tilman  and
      Pfeiffer, Jonas  and
      Reimers, Nils  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.626",
    doi = "10.18653/v1/2021.emnlp-main.626",
    pages = "7930--7946",
    abstract = "Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",
}

@InProceedings{pmlr-v139-hubara21a,
  title = 	 {Accurate Post Training Quantization With Small Calibration Sets},
  author =       {Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4466--4475},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hubara21a/hubara21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hubara21a.html},
  abstract = 	 {Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations’ dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations’ dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy degradation — with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code \textit{https://github.com/papers-submission/CalibTIP}.}
}


@InProceedings{pmlr-v162-dong22a,
  title = 	 {Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks},
  author =       {Dong, Runpei and Tan, Zhanhong and Wu, Mengdi and Zhang, Linfeng and Ma, Kaisheng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5343--5359},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/dong22a/dong22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/dong22a.html},
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}


@inproceedings{yao2021hawq,
  title={Hawq-v3: Dyadic neural network quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael and others},
  booktitle={International Conference on Machine Learning},
  pages={11875--11886},
  year={2021},
  organization={PMLR}
}

@inproceedings{ibis2009efficient,
  title={Efficient Euclidean projections in linear time},
  author={Liu, Jun and Ye, Jieping},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={657--664},
  year={2009}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{devoto-etal-2024-simple,
    title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Cache Compression",
    author = "Devoto, Alessio  and
      Zhao, Yu  and
      Scardapane, Simone  and
      Minervini, Pasquale",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1027/",
    doi = "10.18653/v1/2024.emnlp-main.1027",
    pages = "18476--18499",
}

@inproceedings{luohe2024keep,
title={Keep the Cost Down: A Review on Methods to Optimize {LLM}{\textquoteright}s {KV}-Cache Consumption},
author={Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=8tKjqqMM5z}
}


@inproceedings{pagliardini2023fast,
title={Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention},
author={Matteo Pagliardini and Daniele Paliotta and Martin Jaggi and Fran{\c{c}}ois Fleuret},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=UINHuKeWUa}
}

@inproceedings{xiao2024efficient,
    title={Efficient Streaming Language Models with Attention Sinks},
    author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=NG7sS51zVF}
}

@inproceedings{ahia-etal-2021-low-resource,
    title = "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
    author = "Ahia, Orevaoghene  and
      Kreutzer, Julia  and
      Hooker, Sara",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.282",
    doi = "10.18653/v1/2021.findings-emnlp.282",
    pages = "3316--3333",
    abstract = "A {``}bigger is better{''} explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind.",
}

@article{Mohammadshahi2022,
  doi = {10.48550/ARXIV.2205.10828},
  url = {https://arxiv.org/abs/2205.10828},
  author = {Mohammadshahi, Alireza and Nikoulina, Vassilina and Berard, Alexandre and Brun, Caroline and Henderson, James and Besacier, Laurent},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {What Do Compressed Multilingual Machine Translation Models Forget?},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2205.10828},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Alwani2016FusedlayerCA,
  title={Fused-layer CNN accelerators},
  author={Manoj Alwani and Han Chen and Michael Ferdman and Peter Milder},
  journal={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2016},
  pages={1-12}
}

@inproceedings{pfeiffer-etal-2022-xgqa,
    title = "x{GQA}: Cross-Lingual Visual Question Answering",
    author = "Pfeiffer, Jonas  and
      Geigle, Gregor  and
      Kamath, Aishwarya  and
      Steitz, Jan-Martin  and
      Roth, Stefan  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.196",
    doi = "10.18653/v1/2022.findings-acl.196",
    pages = "2497--2511",
    abstract = "Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingual visual question answering. We further propose new adapter-based approaches to adapt multimodal transformer-based models to become multilingual, and{---}vice versa{---}multilingual models to become multimodal. Our proposed methods outperform current state-of-the-art multilingual multimodal models (e.g., M3P) in zero-shot cross-lingual settings, but the accuracy remains low across the board; a performance drop of around 38 accuracy points in target languages showcases the difficulty of zero-shot cross-lingual transfer for this task. Our results suggest that simple cross-lingual transfer of multimodal models yields latent multilingual multimodal misalignment, calling for more sophisticated methods for vision and multilingual language modeling.",
}


@misc{leclerc2022ffcv,
    author = {Guillaume Leclerc and Andrew Ilyas and Logan Engstrom and Sung Min Park and Hadi Salman and Aleksander Madry},
    title = {ffcv},
    year = {2022},
    howpublished = {\url{https://github.com/libffcv/ffcv/}},
    note = {commit xxxxxxx}
}

@inproceedings{LeeThorp2021FNetMT,
    title = "{FN}et: Mixing Tokens with {F}ourier Transforms",
    author = "Lee-Thorp, James  and
      Ainslie, Joshua  and
      Eckstein, Ilya  and
      Ontanon, Santiago",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.319",
    doi = "10.18653/v1/2022.naacl-main.319",
    pages = "4296--4313",
    abstract = "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that {``}mix{''} input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97{\%} of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80{\%} faster on GPUs and 70{\%} faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the {``}efficient Transformers{''} on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
}



@inproceedings{NEURIPS2020_eae15aab,
 author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20378--20389},
 publisher = {Curran Associates, Inc.},
 title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
 url = {https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Abstract.html},
 volume = {33},
 year = {2020}
}


@misc{Hooker2020,
  doi = {10.48550/ARXIV.2010.03058},
  url = {https://arxiv.org/abs/2010.03058},
  journal={arXiv preprint arXiv:2010.03058},
  author = {Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Characterising Bias in Compressed Models},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Hoffmann2022,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  url = {https://arxiv.org/abs/2203.15556},
  year={2022}
}

@article{gale2019,
  doi = {10.48550/ARXIV.1902.09574},
  url = {https://arxiv.org/abs/1902.09574},
  author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The State of Sparsity in Deep Neural Networks},
  journal={arXiv preprint arXiv:1902.09574},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{lee-etal-2022-deduplicating,
    title = "Deduplicating Training Data Makes Language Models Better",
    author = "Lee, Katherine  and
      Ippolito, Daphne  and
      Nystrom, Andrew  and
      Zhang, Chiyuan  and
      Eck, Douglas  and
      Callison-Burch, Chris  and
      Carlini, Nicholas",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.577",
    doi = "10.18653/v1/2022.acl-long.577",
    pages = "8424--8445",
    abstract = "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1{\%} of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets{---}for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4{\%} of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.",
}


@article{dehghani2021,
  author    = {Mostafa Dehghani and
               Anurag Arnab and
               Lucas Beyer and
               Ashish Vaswani and
               Yi Tay},
  title     = {The Efficiency Misnomer},
  journal   = {CoRR},
  volume    = {abs/2110.12894},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.12894},
  eprinttype = {arXiv},
  eprint    = {2110.12894},
  timestamp = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-12894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{shazeer2017outrageously,
    title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
    author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=B1ckMDqlg}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{khan_npe_2021,
	title = {{NPE}: {An} {FPGA}-based {Overlay} {Processor} for {Natural} {Language} {Processing}},
	shorttitle = {{NPE}},
	url = {http://arxiv.org/abs/2104.06535},
	doi = {10.1145/3431920.3439477},
	abstract = {In recent years, transformer-based models have shown state-of-the-art results for Natural Language Processing (NLP). In particular, the introduction of the BERT language model brought with it breakthroughs in tasks such as question answering and natural language inference, advancing applications that allow humans to interact naturally with embedded devices. FPGA-based overlay processors have been shown as effective solutions for edge image and video processing applications, which mostly rely on low precision linear matrix operations. In contrast, transformer-based NLP techniques employ a variety of higher precision nonlinear operations with significantly higher frequency. We present NPE, an FPGA-based overlay processor that can efficiently execute a variety of NLP models. NPE offers software-like programmability to the end user and, unlike FPGA designs that implement specialized accelerators for each nonlinear function, can be upgraded for future NLP models without requiring reconfiguration. We demonstrate that NPE can meet real-time conversational AI latency targets for the BERT language model with \$4{\textbackslash}times\$ lower power than CPUs and \$6{\textbackslash}times\$ lower power than GPUs. We also show NPE uses \$3{\textbackslash}times\$ fewer FPGA resources relative to comparable BERT network-specific accelerators in the literature. NPE provides a cost-effective and power-efficient FPGA-based solution for Natural Language Processing at the edge.},
	urldate = {2021-07-01},
	journal = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	author = {Khan, Hamza and Khan, Asma and Khan, Zainab and Huang, Lun Bin and Wang, Kun and He, Lei},
	month = feb,
	year = {2021},
	note = {arXiv: 2104.06535},
	keywords = {Computer Science - Hardware Architecture},
	pages = {227--227},
}

@inproceedings{langhammer_stratix_2021,
	address = {New York, NY, USA},
	series = {{FPGA} '21},
	title = {Stratix 10 {NX} {Architecture} and {Applications}},
	isbn = {978-1-4503-8218-2},
	url = {http://doi.org/10.1145/3431920.3439293},
	doi = {10.1145/3431920.3439293},
	abstract = {The advent of AI has driven the adoption of high density low precision arithmetic on FPGAs. This has resulted in new methods in mapping both arithmetic functions as well as dataflows onto the fabric, as well as some changes to the embedded DSP Blocks. Technologies outside of the FPGA realm have also evolved, such as the addition of tensor structures for GPUs, and also the introduction of numerous AI ASSPs, all of which have a higher claimed performance and efficiency than current FPGAs. In this paper we will introduce the Stratix 10 NX device (NX), which is a variant of FPGA specifically optimized for the AI application space. In addition to the computational capabilities of the standard programmable soft logic fabric, a new type of DSP Block provides the dense arrays of low precision multipliers typically used in AI implementations. The architecture of the block is tuned for the common matrix-matrix or vector-matrix multiplications in AI, with capabilities designed to work efficiently for both small and large matrix sizes. The base precisions are INT8 and INT4, along with shared exponent support for support block floating point FP16 and FP12 numerics. All additions/accumulations can be done in INT32 or IEEE754 single precision floating point (FP32), and multiple blocks can be cascaded together to support larger matrices. We will also describe methods by which the smaller precision multipliers can be aggregated to create larger multiplier that are more applicable to standard signal processing requirements. In terms of overall compute throughput, Stratix 10 NX achieves 143 INT8/FP16 TOPs/FLOPs, or 286 INT4/FP12 TOPS/FLOPs at 600MHz. Depending on the configuration, power efficiency is in the range of 1-4 TOPs or TFLOPs/W.},
	urldate = {2021-07-01},
	booktitle = {The 2021 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Langhammer, Martin and Nurvitadhi, Eriko and Pasca, Bogdan and Gribok, Sergey},
	month = feb,
	year = {2021},
	keywords = {AI tensor block, dot product, Stratix 10 NX},
	pages = {57--67},
}

@inproceedings{martins2022infty,
    title = "$\infty$-former: Infinite Memory Transformer",
    author = "Martins, Pedro Henrique  and
      Marinho, Zita  and
      Martins, Andre",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.375",
    doi = "10.18653/v1/2022.acl-long.375",
    pages = "5468--5485",
}

@inproceedings{gu2021efficiently,
    title={Efficiently Modeling Long Sequences with Structured State Spaces},
    author={Albert Gu and Karan Goel and Christopher Re},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=uYLFoz1vlAC}
}



@inproceedings{treviso2022predicting,
    title = "Predicting Attention Sparsity in Transformers",
    author = "Treviso, Marcos  and
      G{\'o}is, Ant{\'o}nio  and
      Fernandes, Patrick  and
      Fonseca, Erick  and
      Martins, Andre",
    booktitle = "Proceedings of the Sixth Workshop on Structured Prediction for NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.spnlp-1.7",
    doi = "10.18653/v1/2022.spnlp-1.7",
    pages = "67--81"
}


@inproceedings{rei2022searching,
    title = "Searching for {COMETINHO}: The Little Metric That Could",
    author = "Rei, Ricardo  and
      Farinha, Ana C  and
      de Souza, Jos{\'e} G.C.  and
      Ramos, Pedro G.  and
      Martins, Andr{\'e} F.T.  and
      Coheur, Luisa  and
      Lavie, Alon",
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.9",
    pages = "61--70"}

@InProceedings{borgeaud2021improving,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html},
}




@article{fedus2021switch,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  url     = {http://jmlr.org/papers/v23/21-0998.html}
}

@inproceedings{gmlp,
 author = {Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9204--9215},
 publisher = {Curran Associates, Inc.},
 title = {Pay Attention to {MLP}s},
 url = {https://proceedings.neurips.cc/paper/2021/file/4cc05b35c2f937c5bd9e7d41d3686fff-Abstract.html},
 volume = {34},
 year = {2021}
}



@article{Dao2022-yl,
  title={{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  url = {https://arxiv.org/abs/2205.14135},
  year={2022}
}



@INPROCEEDINGS{Bach2022-di,
  title     = "{PromptSource}: An Integrated Development Environment and
               Repository for Natural Language Prompts",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics: System Demonstrations",
  author    = "Bach, Stephen and Sanh, Victor and Yong, Zheng Xin and Webson,
               Albert and Raffel, Colin and Nayak, Nihal V and Sharma, Abheesht
               and Kim, Taewoon and Bari, M Saiful and Fevry, Thibault and
               Alyafeai, Zaid and Dey, Manan and Santilli, Andrea and Sun,
               Zhiqing and Ben-david, Srulik and Xu, Canwen and Chhablani,
               Gunjan and Wang, Han and Fries, Jason and Al-shaibani, Maged and
               Sharma, Shanya and Thakker, Urmish and Almubarak, Khalid and
               Tang, Xiangru and Radev, Dragomir and Jiang, Mike Tian-Jian and
               Rush, Alexander",
  publisher = "Association for Computational Linguistics",
  pages     = "93--104",
  month     =  may,
  year      =  2022,
  url = "https://aclanthology.org/2022.acl-demo.9",
  doi = "10.18653/v1/2022.acl-demo.9",
  address   = "Dublin, Ireland"
}



@misc{fnet,
	doi = {10.48550/ARXIV.2105.03824},
	url = {https://arxiv.org/abs/2105.03824},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	title = {FNet: Mixing Tokens with Fourier Transforms},
	year = {2021},
	note = {{arXiv}:2105.03824},
}

@misc{flash,
	doi = {10.48550/ARXIV.2202.10447},
	url = {https://arxiv.org/abs/2202.10447},
	author = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V.},
	title = {Transformer Quality in Linear Time},
	year = {2022},
	note = {{arXiv}:2202.10447},
}

@book{jevons1866coal,
  title={The Coal Question; An Inquiry Concerning the Progress of the Nation, and the Probable Exhaustion of Our Coal Mines},
  author={Jevons, William Stanley},
  year={1866},
  publisher={Macmillan \& Co. London},
  isbn={978-0-678-00107-3},
}

@inproceedings{swayamdipta-etal-2020-dataset,
    title = "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
    author = "Swayamdipta, Swabha  and
      Schwartz, Roy  and
      Lourie, Nicholas  and
      Wang, Yizhong  and
      Hajishirzi, Hannaneh  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.746",
    doi = "10.18653/v1/2020.emnlp-main.746",
    pages = "9275--9293",
}

@book{Settles2012,
  author = {Settles, Burr},
  title = {{Active Learning}},
  year = {2012},
  series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {18},
  publisher = {Morgan \& Claypool},
}

@inproceedings{lowell-etal-2019-practical,
    title = "Practical Obstacles to Deploying Active Learning",
    author = "Lowell, David  and
      Lipton, Zachary C.  and
      Wallace, Byron C.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1003",
    doi = "10.18653/v1/D19-1003",
    pages = "21--30",
}


@inproceedings{yuan-etal-2020-cold,
    title = "Cold-start Active Learning through Self-supervised Language Modeling",
    author = "Yuan, Michelle  and
      Lin, Hsuan-Tien  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.637",
    doi = "10.18653/v1/2020.emnlp-main.637",
    pages = "7935--7948",
}

@inproceedings{ein-dor-etal-2020-active,
    title = "{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy",
    author = "Ein-Dor, Liat  and
      Halfon, Alon  and
      Gera, Ariel  and
      Shnarch, Eyal  and
      Dankin, Lena  and
      Choshen, Leshem  and
      Danilevsky, Marina  and
      Aharonov, Ranit  and
      Katz, Yoav  and
      Slonim, Noam",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.638",
    doi = "10.18653/v1/2020.emnlp-main.638",
    pages = "7949--7962",
}

@inproceedings{settles2008active,
  title={Active learning with real annotation costs},
  author={Settles, Burr and Craven, Mark and Friedland, Lewis},
  booktitle="Proceedings of the NIPS workshop on cost-sensitive learning (Vol. 1).",
  year=2008
}

@article{lee-klie-gurevych-2022,
    author = {Lee, Ji-Ung and Klie, Jan-Christoph and Gurevych, Iryna},
    title = "{Annotation Curricula to Implicitly Train Non-Expert Annotators}",
    journal = {Computational Linguistics},
    volume = {48},
    number = {2},
    pages = {343-373},
    year = {2022},
    month = {06},
    abstract = "{Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowdsourcing scenarios where domain expertise is not required. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances to be annotated according to a learning curriculum. To do so, this work formalizes annotation curricula for sentence- and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, we provide a proof of concept for annotation curricula in a carefully designed user study with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results indicate that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can be a promising research direction to improve data collection. To facilitate future research—for instance, to adapt annotation curricula to specific tasks and expert annotation scenarios—all code and data from the user study consisting of 2,400 annotations is made available.1}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00436},
    url = {https://doi.org/10.1162/coli\_a\_00436},
    eprint = {https://direct.mit.edu/coli/article-pdf/48/2/343/2029108/coli\_a\_00436.pdf},
}

@inproceedings{silva-etal-2021-towards,
    title = "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
    author = "Silva, Andrew  and
      Tambwekar, Pradyumna  and
      Gombolay, Matthew",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.189",
    doi = "10.18653/v1/2021.naacl-main.189",
    pages = "2383--2389",
    abstract = "The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",
}

@article{xu2022can,
  title={Can model compression improve nlp fairness},
  author={Xu, Guangxuan and Hu, Qingyuan},
  journal={arXiv preprint arXiv:2201.08542},
  url={https://arxiv.org/abs/2201.08542},
  year={2022}
}

@article{10.1145/3472291,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3472291},
doi = {10.1145/3472291},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {180},
numpages = {40},
keywords = {active learning, deep active learning, Deep learning}
}

@inproceedings{karamcheti-etal-2021-mind,
    title = "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering",
    author = "Karamcheti, Siddharth  and
      Krishna, Ranjay  and
      Fei-Fei, Li  and
      Manning, Christopher",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.564",
    doi = "10.18653/v1/2021.acl-long.564",
    pages = "7265--7281",
}


@inproceedings{yuan-etal-2022-adapting,
    title = "Adapting Coreference Resolution Models through Active Learning",
    author = "Yuan, Michelle  and
      Xia, Patrick  and
      May, Chandler  and
      Van Durme, Benjamin  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.519",
    doi = "10.18653/v1/2022.acl-long.519",
    pages = "7533--7549",
}


@article{kirsch2019batchbald,
  title={{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}},
  author={Kirsch, Andreas and Van Amersfoort, Joost and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/file/95323660ed2124450caaac2c46b5ed90-Abstract.html}
}

@inproceedings{gal2017deep,
  title={Deep bayesian active learning with image data},
  author={Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  booktitle={International Conference on Machine Learning},
  pages={1183--1192},
  year={2017},
  organization={PMLR}
}

@inproceedings{
sener2018active,
title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
author={Ozan Sener and Silvio Savarese},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1aIuk-RW},
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive structured matrices for efficient and accurate training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={4690--4721},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/dao22a.html}
}

@article{kreutzer-etal-2022-quality,
    title = "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
    author = {Kreutzer, Julia  and
      Caswell, Isaac  and
      Wang, Lisa  and
      Wahab, Ahsan  and
      van Esch, Daan  and
      Ulzii-Orshikh, Nasanbayar  and
      Tapo, Allahsera  and
      Subramani, Nishant  and
      Sokolov, Artem  and
      Sikasote, Claytone  and
      Setyawan, Monang  and
      Sarin, Supheakmungkol  and
      Samb, Sokhar  and
      Sagot, Beno{\^\i}t  and
      Rivera, Clara  and
      Rios, Annette  and
      Papadimitriou, Isabel  and
      Osei, Salomey  and
      Suarez, Pedro Ortiz  and
      Orife, Iroro  and
      Ogueji, Kelechi  and
      Rubungo, Andre Niyongabo  and
      Nguyen, Toan Q.  and
      M{\"u}ller, Mathias  and
      M{\"u}ller, Andr{\'e}  and
      Muhammad, Shamsuddeen Hassan  and
      Muhammad, Nanda  and
      Mnyakeni, Ayanda  and
      Mirzakhalov, Jamshidbek  and
      Matangira, Tapiwanashe  and
      Leong, Colin  and
      Lawson, Nze  and
      Kudugunta, Sneha  and
      Jernite, Yacine  and
      Jenny, Mathias  and
      Firat, Orhan  and
      Dossou, Bonaventure F. P.  and
      Dlamini, Sakhile  and
      de Silva, Nisansa  and
      {\c{C}}abuk Ball{\i}, Sakine  and
      Biderman, Stella  and
      Battisti, Alessia  and
      Baruwa, Ahmed  and
      Bapna, Ankur  and
      Baljekar, Pallavi  and
      Azime, Israel Abebe  and
      Awokoya, Ayodele  and
      Ataman, Duygu  and
      Ahia, Orevaoghene  and
      Ahia, Oghenefego  and
      Agrawal, Sweta  and
      Adeyemi, Mofetoluwa},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.4",
    doi = "10.1162/tacl_a_00447",
    pages = "50--72",
    abstract = "With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50{\%} sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.",
}

@article{al2015efficient,
  title={Efficient machine learning for big data: A review},
  author={Al-Jarrah, Omar Y and Yoo, Paul D and Muhaidat, Sami and Karagiannidis, George K and Taha, Kamal},
  journal={Big Data Research},
  volume={2},
  number={3},
  pages={87--93},
  year={2015},
  publisher={Elsevier}
}

@article{lee-2021-resource,
  author    = {JunKyu Lee and
               Lev Mukhanov and
               Amir Sabbagh Molahosseini and
               Umar Ibrahim Minhas and
               Yang Hua and
               Jes{\'{u}}s Mart{\'{\i}}nez del Rinc{\'{o}}n and
               Kiril Dichev and
               Cheol{-}Ho Hong and
               Hans Vandierendonck},
  title     = {Resource-Efficient Deep Learning: {A} Survey on Model-, Arithmetic-,
               and Implementation-Level Techniques},
  journal   = {CoRR},
  volume    = {abs/2112.15131},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.15131},
  eprinttype = {arXiv},
  eprint    = {2112.15131},
  timestamp = {Wed, 05 Jan 2022 17:44:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-15131.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mishra-sachdeva-2020-need,
    title = "Do We Need to Create Big Datasets to Learn a Task?",
    author = "Mishra, Swaroop  and
      Sachdeva, Bhavdeep Singh",
    booktitle = "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sustainlp-1.23",
    doi = "10.18653/v1/2020.sustainlp-1.23",
    pages = "169--173",
    abstract = "Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as {`}Entailment{'}, {`}Neutral{'}, and {`}Contradiction{'} for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2{\%} data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.",
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{press-etal-2021-shortformer,
    title = "Shortformer: Better Language Modeling using Shorter Inputs",
    author = "Press, Ofir  and
      Smith, Noah A.  and
      Lewis, Mike",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.427",
    doi = "10.18653/v1/2021.acl-long.427",
    pages = "5493--5505",
    abstract = "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
}


@article{wettig2022should,
  title={Should You Mask 15\% in Masked Language Modeling?},
  author={Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2202.08005},
  url={https://arxiv.org/abs/2202.08005},
  year={2022}
}

@inproceedings{wan-etal-2020-self,
    title = "Self-Paced Learning for Neural Machine Translation",
    author = "Wan, Yu  and
      Yang, Baosong  and
      Wong, Derek F.  and
      Zhou, Yikai  and
      Chao, Lidia S.  and
      Zhang, Haibo  and
      Chen, Boxing",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.80",
    doi = "10.18653/v1/2020.emnlp-main.80",
    pages = "1074--1080",
    abstract = "Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.",
}


@inproceedings{agrawal-etal-2021-role,
    title = "On the Role of Corpus Ordering in Language Modeling",
    author = "Agrawal, Ameeta  and
      Singh, Suresh  and
      Schneider, Lauren  and
      Samuels, Michael",
    booktitle = "Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sustainlp-1.15",
    doi = "10.18653/v1/2021.sustainlp-1.15",
    pages = "142--154",
    abstract = "Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.",
}


@article{klie2022annotation,
  title={Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future},
  author={Klie, Jan-Christoph and Webber, Bonnie and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2206.02280},
  url={https://arxiv.org/abs/2206.02280},
  year={2022}
}


@inproceedings{northcutt2021pervasive,
  title={Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks},
  author={Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}


@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  url={https://arxiv.org/abs/2205.01068},
  year={2022}
}


@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{gissin2019discriminative,
  title={Discriminative active learning},
  author={Gissin, Daniel and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1907.06347},
  url={https://arxiv.org/abs/1907.06347},
  year={2019}
}


@inproceedings{Bengio2009,
 author = {Bengio, Yoshua and Louradour, J{\'e}r\^{o}me and Collobert, Ronan and Weston, Jason},
 title = {Curriculum Learning},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 year = {2009},
 pages = {41--48},
 numpages = {8},
} 

@inproceedings{tay-etal-2019-simple,
    title = "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives",
    author = "Tay, Yi  and
      Wang, Shuohang  and
      Luu, Anh Tuan  and
      Fu, Jie  and
      Phan, Minh C.  and
      Yuan, Xingdi  and
      Rao, Jinfeng  and
      Hui, Siu Cheung  and
      Zhang, Aston",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1486",
    doi = "10.18653/v1/P19-1486",
    pages = "4922--4931",
    abstract = "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51{\%} relative improvement on BLEU-4 and 17{\%} relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.",
}


@inproceedings{xu-etal-2020-curriculum,
    title = "Curriculum Learning for Natural Language Understanding",
    author = "Xu, Benfeng  and
      Zhang, Licheng  and
      Mao, Zhendong  and
      Wang, Quan  and
      Xie, Hongtao  and
      Zhang, Yongdong",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.542",
    doi = "10.18653/v1/2020.acl-main.542",
    pages = "6095--6104",
}

@inproceedings{zhao2020reinforced,
  title={Reinforced curriculum learning on pre-trained neural machine translation models},
  author={Zhao, Mingjun and Wu, Haijiang and Niu, Di and Wang, Xiaoli},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={9652--9659},
  year={2020},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/6513}
}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M and Packer, Benjamin and Koller, Daphne},
  journal={Advances in neural information processing systems},
  volume={23},
  url={https://papers.nips.cc/paper/2010/file/e57c6b956a6521b28495f2886ca0977a-Abstract.html},
  year={2010}
}


@inproceedings{zhu-etal-2021-combining-curriculum-learning,
    title = "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
    author = "Zhu, Qingqing  and
      Chen, Xiuying  and
      Wu, Pengfei  and
      Liu, JunFei  and
      Zhao, Dongyan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.111",
    doi = "10.18653/v1/2021.findings-emnlp.111",
    pages = "1284--1295",
    abstract = "Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facilitate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield significant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for efficient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is difficult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demonstrate that our methods achieve improvements compared with competitive baselines.",
}

@inproceedings{zhan2021meta,
  title={Meta-curriculum learning for domain adaptation in neural machine translation},
  author={Zhan, Runzhe and Liu, Xuebo and Wong, Derek F and Chao, Lidia S},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={14310--14318},
  year={2021}
}

@inproceedings{lewis1994sequential,
  title={A sequential algorithm for training text classifiers},
  author={Lewis, David D and Gale, William A},
  booktitle={SIGIR’94},
  pages={3--12},
  year={1994},
  organization={Springer},
  url={https://link.springer.com/chapter/10.1007/978-1-4471-2099-5_1}
}

@inproceedings{tang-etal-2002-active,
    title = "Active Learning for Statistical Natural Language Parsing",
    author = "Tang, Min  and
      Luo, Xiaoqiang  and
      Roukos, Salim",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1016",
    doi = "10.3115/1073083.1073105",
    pages = "120--127",
}

@inproceedings{zhang-etal-2019-curriculum,
    title = "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
    author = "Zhang, Xuan  and
      Shapiro, Pamela  and
      Kumar, Gaurav  and
      McNamee, Paul  and
      Carpuat, Marine  and
      Duh, Kevin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1189",
    doi = "10.18653/v1/N19-1189",
    pages = "1903--1915",
    abstract = "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
}


@inproceedings{zhou-etal-2020-uncertainty,
    title = "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
    author = "Zhou, Yikai  and
      Yang, Baosong  and
      Wong, Derek F.  and
      Wan, Yu  and
      Chao, Lidia S.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.620",
    doi = "10.18653/v1/2020.acl-main.620",
    pages = "6934--6944",
    abstract = "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
}



@InProceedings{pmlr-v70-graves17a,
  title = 	 {Automated Curriculum Learning for Neural Networks},
  author =       {Alex Graves and Marc G. Bellemare and Jacob Menick and R{\'e}mi Munos and Koray Kavukcuoglu},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1311--1320},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/graves17a/graves17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/graves17a.html},
  abstract = 	 {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.}
}

@inproceedings{platanios-etal-2019-competence,
    title = "Competence-based Curriculum Learning for Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios  and
      Stretcu, Otilia  and
      Neubig, Graham  and
      Poczos, Barnabas  and
      Mitchell, Tom",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1119",
    doi = "10.18653/v1/N19-1119",
    pages = "1162--1172",
}



@inproceedings{he2019rethinking,
  title={Rethinking {ImageNet} pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2019},
  url={https://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html}
}

@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/0607f4c705595b911a4f3e7a127b44e0-Abstract.html}
}

@inproceedings{kovaleva2019revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1445",
    doi = "10.18653/v1/D19-1445",
    pages = "4365--4374",
}

@inproceedings{zaken2021bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@inproceedings{bapna2019simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
}

@article{rebuffi2017learning,
  title={Learning multiple visual domains with residual adapters},
  author={Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{mahabadi2021compacter,
 author = {Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
 volume = {34},
 year = {2021}
}

@inproceedings{karimi-mahabadi-etal-2022-prompt,
    title = "Prompt-free and Efficient Few-shot Learning with Language Models",
    author = "Karimi Mahabadi, Rabeeh  and
      Zettlemoyer, Luke  and
      Henderson, James  and
      Mathias, Lambert  and
      Saeidi, Marzieh  and
      Stoyanov, Veselin  and
      Yazdani, Majid",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.254",
    doi = "10.18653/v1/2022.acl-long.254",
    pages = "3638--3652",
    abstract = "Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/rabeehk/perfect.",
}


@inproceedings{hu2022lora,
  title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
  author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle={International Conference on Learning Representations},
  url={https://openreview.net/forum?id=nZeVKeeFYf9},
  year={2022},
}


@inproceedings{sung2021training,
    title={Training Neural Networks with Fixed Sparse Masks},
    author={Yi-Lin Sung and Varun Nair and Colin Raffel},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=Uwh-v1HSw-x}
}

@inproceedings{guo2021parameter,
    title = "Parameter-Efficient Transfer Learning with Diff Pruning",
    author = "Guo, Demi  and
      Rush, Alexander  and
      Kim, Yoon",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.378",
    doi = "10.18653/v1/2021.acl-long.378",
    pages = "4884--4896",
}

@inproceedings{dai-etal-2022-revisiting,
    title = "Revisiting Transformer-based Models for Long Document Classification",
    author = "Dai, Xiang  and
      Chalkidis, Ilias  and
      Darkner, Sune  and
      Elliott, Desmond",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.534/",
    doi = "10.18653/v1/2022.findings-emnlp.534",
    pages = "7212--7230",
    abstract = "The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods.We examine several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on long document classification tasks."
}


@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021}
}


@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  journal={arXiv preprint arXiv:2205.05638},
  url={https://arxiv.org/abs/2205.05638},
  year={2022}
}

@inproceedings{aghajanyan2021intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
}


@inproceedings{li2018measuring,
    title={Measuring the Intrinsic Dimension of Objective Landscapes},
    author={Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=ryup8-WCW},
}

@article{ruder2017overview,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  url={https://arxiv.org/abs/1706.05098},
  year={2017}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
}

@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  journal={Proceedings of machine learning and systems},
  volume={2},
  pages={129--146},
  url={https://proceedings.mlsys.org/paper/2020/file/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html},
  year={2020}
}

@inproceedings{peters2019tune,
  title={To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks},
  author={Peters, Matthew E and Ruder, Sebastian and Smith, Noah A},
  booktitle={Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)},
  year={2019}
}

@inproceedings{kim-rush-2016-sequence,
    title = "Sequence-Level Knowledge Distillation",
    author = "Kim, Yoon  and
      Rush, Alexander M.",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1139",
    doi = "10.18653/v1/D16-1139",
    pages = "1317--1327",
}

@inproceedings{lacoste2019quantifying,
  title={Quantifying the Carbon Emissions of Machine Learning},
  author={Luccioni, Sasha and Schmidt, Victor and Lacoste, Alexandre and Dandres, Thomas},
  booktitle={NeurIPS 2019 Workshop on Tackling Climate Change with Machine Learning},
  url={https://www.climatechange.ai/papers/neurips2019/22},
  year={2019}
}

@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={248},
  pages={1--43},
  year={2020}
}

@article{hershcovich2022towards,
  title={Towards Climate Awareness in NLP Research},
  author={Hershcovich, Daniel and Webersinke, Nicolas and Kraus, Mathias and Bingler, Julia Anna and Leippold, Markus},
  journal={arXiv preprint arXiv:2205.05071},
  url = {https://arxiv.org/abs/2205.05071},
  year={2022}
}


@inproceedings{anthony2020carbontracker,
  title={{CarbonTracker: Tracking and predicting the carbon footprint of training deep learning models}},
  author={Anthony, Lasse F Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  booktitle={Proceedings of the workshop on Challenges in Deploying and monitoring Machine Learning Systems, ICML},
  year={2020}
}

@misc{https://doi.org/10.48550/arxiv.1701.06549,
  doi = {10.48550/ARXIV.1701.06549},
  url = {https://arxiv.org/abs/1701.06549},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning to Decode for Future Success},
  publisher = {arXiv},
  year = {2017},
}



@misc{Dodge:2020,
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah A.}, 
  title={Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping},
  url = "https://arxiv.org/abs/2002.06305",
  note= "{arXiv}:2002.06305",
  year={2020}
}

@misc{Phang:2018,
	doi = {10.48550/ARXIV.1811.01088},
	url = {https://arxiv.org/abs/1811.01088},
	author = {Phang, Jason and Févry, Thibault and Bowman, Samuel R.},
	title = {Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},
	year = {2018},
	note = {{arXiv}:1811.01088},
}


@inproceedings{schwartz-etal-2020-right,
    title = "The Right Tool for the Job: Matching Model and Instance Complexities",
    author = "Schwartz, Roy  and
      Stanovsky, Gabriel  and
      Swayamdipta, Swabha  and
      Dodge, Jesse  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.593",
    doi = "10.18653/v1/2020.acl-main.593",
    pages = "6640--6651",
    abstract = "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) {``}exit{''} from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.",
}


@inproceedings{xin-etal-2020-deebert,
    title = "{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference",
    author = "Xin, Ji  and
      Tang, Raphael  and
      Lee, Jaejun  and
      Yu, Yaoliang  and
      Lin, Jimmy",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.204",
    doi = "10.18653/v1/2020.acl-main.204",
    pages = "2246--2251",
    abstract = "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to {\textasciitilde}40{\%} inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",
}

@inproceedings{Dabre:2020,
    title = "Balancing Cost and Benefit with Tied-Multi Transformers",
    author = "Dabre, Raj  and
      Rubino, Raphael  and
      Fujita, Atsushi",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.3",
    doi = "10.18653/v1/2020.ngt-1.3",
    pages = "24--34",
}

@inproceedings{Elbayad:2020,
    title={Depth-Adaptive Transformer},
    author={Maha Elbayad and Jiatao Gu and Edouard Grave and Michael Auli},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SJg7KhVKPH}
}

%%% model selection

@article{yang2021tuning,
  title={Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  author={Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url={https://papers.nips.cc/paper/2021/hash/8df7c2e3c3c3be098ef7b382bd2c37ba-Abstract.html}
}

@InProceedings{yang2021tensor,
  title = 	 {{Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel Training Dynamics}},
  author =       {Yang, Greg and Littwin, Etai},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11762--11772},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21f/yang21f.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21f.html},
}



@inproceedings{wei2021meta,
  title={Meta-learning Hyperparameter Performance Prediction with Neural Processes},
  author={Wei, Ying and Zhao, Peilin and Huang, Junzhou},
  booktitle={International Conference on Machine Learning},
  pages={11058--11067},
  year={2021},
  url={https://proceedings.mlr.press/v139/wei21c.html},
  organization={PMLR}
}

@inproceedings{liu-wang-2021-empirical,
    title = "An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models",
    author = "Liu, Xueqing  and
      Wang, Chi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.178",
    doi = "10.18653/v1/2021.acl-long.178",
    pages = "2286--2300",
    abstract = "The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms{'} performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO{'}s failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in \url{https://github.com/microsoft/FLAML/tree/main/flaml/nlp/}",
}



@inproceedings{li2020system,
title	= {{A System for Massively Parallel Hyperparameter Tuning}},
author	= {Liam Li and Kevin Jamieson and Afshin Rostamizadeh and Ekaterina Gonina and Jonathan Ben-tzur and Moritz Hardt and Benjamin Recht and Ameet Talwalkar},
year	= {2020},
booktitle	= {Third Conference on Systems and Machine Learning}
}



@article{feurer2015efficient,
  title={Efficient and robust automated machine learning},
  author={Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015},
  url={https://proceedings.neurips.cc/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Abstract.html}
}


@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={Jamieson, Kevin and Talwalkar, Ameet},
  booktitle={Artificial intelligence and statistics},
  pages={240--248},
  year={2016},
  organization={PMLR},
  url={https://proceedings.mlr.press/v51/jamieson16.html}
}

@article{snoek2012practical,
  title={Practical {Bayesian} optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}, 
  url={https://papers.nips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html}
}

@article{lindauer2022smac3,
  title={{SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization}},
  author={Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr{\'e} and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, Ren{\'e} and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={54--1},
  year={2022},
  url={https://www.jmlr.org/papers/volume23/21-0888/21-0888.pdf}
}


%%%%%%


@inproceedings{Fan:2020,
    title={Reducing Transformer Depth on Demand with Structured Dropout},
    author={Angela Fan and Edouard Grave and Armand Joulin},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SylO2yStDr}
}


@inproceedings{renduchintala-etal-2021-gender,
    title = "Gender bias amplification during Speed-Quality optimization in Neural Machine Translation",
    author = "Renduchintala, Adithya  and
      Diaz, Denise  and
      Heafield, Kenneth  and
      Li, Xian  and
      Diab, Mona",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.15",
    doi = "10.18653/v1/2021.acl-short.15",
    pages = "99--109",
    abstract = "Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.",
}

@inproceedings{Lan:2020,
	doi = {10.48550/ARXIV.1909.11942},
	url = {https://arxiv.org/abs/1909.11942},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	year = {2020},
	booktitle = {Proc. of ICLR},
}


@inproceedings{Clark:2020,
	doi = {10.48550/ARXIV.2003.10555},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	title = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
	year = {2020},
	booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=r1xMH1BtvB}
}


@article{he2021debertav3,
  title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  url={https://arxiv.org/abs/2111.09543},
  year={2021}
}

@inproceedings{Devlin:2019,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}


@inproceedings{Dettmers:2021,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  booktitle={Proc. ICLR 2022},
  year={2022},
  note_={bits and bytes},
}

@inproceedings{Bitton:2021,
    title = "Data Efficient Masked Language Modeling for Vision and Language",
    author = "Bitton, Yonatan  and
      Elhadad, Michael  and
      Stanovsky, Gabriel  and
      Schwartz, Roy",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.259",
    doi = "10.18653/v1/2021.findings-emnlp.259",
    pages = "3013--3028",
}


@inproceedings{Bergstra:2011,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
 volume = {24},
 year = {2011}
}


@article{Li:2018,
  author  = {Lisha Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar},
  title   = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {185},
  pages   = {1--52},
  url     = {http://jmlr.org/papers/v18/16-558.html}
}




@inproceedings{Lakim2022AHA,
    title = "A Holistic Assessment of the Carbon Footprint of Noor, a Very Large {A}rabic Language Model",
    author = "Lakim, Imad  and
      Almazrouei, Ebtesam  and
      Abualhaol, Ibrahim  and
      Debbah, Merouane  and
      Launay, Julien",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.8",
    doi = "10.18653/v1/2022.bigscience-1.8",
    pages = "84--94",
    note_={model_size_by_param.png figure}
}




@article{Schwartz:2020,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green {AI}},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
journal = {Communications of the ACM (CACM)},
month = nov,
pages = {54-63},
numpages = {10}
}


@inproceedings{Strubell:2019,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
}

@inproceedings{hsu2005towards,
  title={Towards efficient supercomputing: A quest for the right metric},
  author={Hsu, C-H and Feng, W-C and Archuleta, Jeremy S},
  booktitle={19th IEEE International Parallel and Distributed Processing Symposium},
  pages={8--pp},
  year={2005},
  organization={IEEE}
}

@article{raffel2019exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{aghajanyan2021muppet,
    title = "Muppet: Massive Multi-task Representations with Pre-Finetuning",
    author = "Aghajanyan, Armen  and
      Gupta, Anchit  and
      Shrivastava, Akshat  and
      Chen, Xilun  and
      Zettlemoyer, Luke  and
      Gupta, Sonal",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.468",
    doi = "10.18653/v1/2021.emnlp-main.468",
    pages = "5799--5811",
}

@inproceedings{aribandi2021ext5,
    title={ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
    author={Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=Vzh1BFUCiIX}
}

@article{radford2017learning,
  title={Learning to generate reviews and discovering sentiment},
  author={Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1704.01444},
  url={https://arxiv.org/abs/1704.01444},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{DarasSMYRF2020,
    author = {Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
    pages = {6476--6489},
    publisher = {Curran Associates, Inc.},
    title = {SMYRF - Efficient Attention using Asymmetric Clustering},
    url = {https://proceedings.neurips.cc/paper/2020/file/47d40767c7e9df50249ebfd9c7cfff77-Abstract.html},
    volume = {33},
    year = {2020}
}

@inproceedings{wang-etal-2021-cluster,
    title = "Cluster-Former: Clustering-based Sparse Transformer for Question Answering",
    author = "Wang, Shuohang  and
      Zhou, Luowei  and
      Gan, Zhe  and
      Chen, Yen-Chun  and
      Fang, Yuwei  and
      Sun, Siqi  and
      Cheng, Yu  and
      Liu, Jingjing",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.346",
    doi = "10.18653/v1/2021.findings-acl.346",
    pages = "3958--3968",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  url   = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@inproceedings{LeCun:1989,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Abstract.html},
 volume = {2},
 year = {1989}
}

@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  booktitle={International Conference on Learning Representations},
  url={https://openreview.net/forum?id=9Vrb9D0WI4},
  year={2022}
}





@inproceedings{wei2022finetuned,
    title={Finetuned Language Models are Zero-Shot Learners},
    author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@inproceedings{Livni:2014,
	doi = {10.48550/ARXIV.1410.1141},
	url = {https://arxiv.org/abs/1410.1141},
	author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
	title = {On the Computational Efficiency of Training Neural Networks},
	year = {2014},
	booktitle = {Proc. of NeurIPS},
}

@inproceedings{quinn-ballesteros-2018-pieces,
    title = "Pieces of Eight: 8-bit Neural Machine Translation",
    author = "Quinn, Jerry  and
      Ballesteros, Miguel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans - Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-3014",
    doi = "10.18653/v1/N18-3014",
    pages = "114--120",
    abstract = "Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago. Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs. In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values. Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy.",
}

@inproceedings{seide2014-bit,
author = {Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
title = {1-Bit Stochastic Gradient Descent and Application to Data-Parallel Distributed Training of Speech DNNs},
booktitle = {Interspeech 2014},
year = {2014},
month = {September},
abstract = {We show empirically that in SGD training of deep neural networks, one can, at no or nearly no loss of accuracy, quantize the gradients aggressively—to but one bit per value—if the quantization error is carried forward across minibatches (error feedback). This size reduction makes it feasible to parallelize SGD through data-parallelism with fast processors like recent GPUs.

We implement data-parallel deterministically distributed SGD by combining this finding with AdaGrad, automatic minibatch-size selection, double buffering, and model parallelism. Unexpectedly, quantization benefits AdaGrad, giving a small accuracy gain.

For a typical Switchboard DNN with 46M parameters, we reach computation speeds of 27k frames per second (kfps) when using 2880 samples per minibatch, and 51kfps with 16k, on a server with 8 K20X GPUs. This corresponds to speed-ups over a single GPU of 3.6 and 6.3, respectively. 7 training passes over 309h of data complete in under 7h. A 160M-parameter model training processes 3300h of data in under 16h on 20 dual-GPU servers—a 10 times speed-up—albeit at a small accuracy loss.},
url = {https://www.microsoft.com/en-us/research/publication/1-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/},
edition = {Interspeech 2014},
}

@inproceedings{kim-etal-2019-research,
    title = "From Research to Production and Back: Ludicrously Fast Neural Machine Translation",
    author = "Kim, Young Jin  and
      Junczys-Dowmunt, Marcin  and
      Hassan, Hany  and
      Fikri Aji, Alham  and
      Heafield, Kenneth  and
      Grundkiewicz, Roman  and
      Bogoychev, Nikolay",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5632",
    doi = "10.18653/v1/D19-5632",
    pages = "280--288",
    abstract = "This paper describes the submissions of the {``}Marian{''} team to the WNGT 2019 efficiency shared task. Taking our dominating submissions to the previous edition of the shared task as a starting point, we develop improved teacher-student training via multi-agent dual-learning and noisy backward-forward translation for Transformer-based student models. For efficient CPU-based decoding, we propose pre-packed 8-bit matrix products, improved batched decoding, cache-friendly student architectures with parameter sharing and light-weight RNN-based decoder architectures. GPU-based decoding benefits from the same architecture changes, from pervasive 16-bit inference and concurrent streams. These modifications together with profiler-based C++ code optimization allow us to push the Pareto frontier established during the 2018 edition towards 24x (CPU) and 14x (GPU) faster models at comparable or higher BLEU values. Our fastest CPU model is more than 4x faster than last year{'}s fastest submission at more than 3 points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is slightly faster than last year{'}s fastest RNN-based submissions, but outperforms them by more than 4 BLEU and 10 BLEU points respectively.",
}
@inproceedings{bogoychev-etal-2020-edinburghs,
    title = "{E}dinburgh{'}s Submissions to the 2020 Machine Translation Efficiency Task",
    author = "Bogoychev, Nikolay  and
      Grundkiewicz, Roman  and
      Aji, Alham Fikri  and
      Behnke, Maximiliana  and
      Heafield, Kenneth  and
      Kashyap, Sidharth  and
      Farsarakis, Emmanouil-Ioannis  and
      Chudyk, Mateusz",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.26",
    doi = "10.18653/v1/2020.ngt-1.26",
    pages = "218--224",
    abstract = "We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.",
}
@inproceedings{behnke-heafield-2021-pruning,
    title = "Pruning Neural Machine Translation for Speed Using Group Lasso",
    author = "Behnke, Maximiliana  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.116",
    pages = "1074--1086",
    abstract = "Unlike most work on pruning neural networks, we make inference faster. Group lasso regularisation enables pruning entire rows, columns or blocks of parameters that result in a smaller dense network. Because the network is still dense, efficient matrix multiply routines are still used and only minimal software changes are required to support variable layer sizes. Moreover, pruning is applied during training so there is no separate pruning step. Experiments on top of English-{\textgreater}German models, which already have state-of-the-art speed and size, show that two-thirds of feedforward connections can be removed with 0.2 BLEU loss. With 6 decoder layers, the pruned model is 34{\%} faster; with 2 tied decoder layers, the pruned model is 14{\%} faster. Pruning entire heads and feedforward connections in a 12{--}1 encoder-decoder architecture gains an additional 51{\%} speed-up. These push the Pareto frontier with respect to the trade-off between time and quality compared to strong baselines. In the WMT 2021 Efficiency Task, our pruned and quantised models are 1.9{--}2.7x faster at the cost 0.9{--}1.7 BLEU in comparison to the unoptimised baselines. Across language pairs, we see similar sparsity patterns: an ascending or U-shaped distribution in encoder feedforward and attention layers and an ascending distribution in the decoder.",
}

@inproceedings{liutowards,
    title = "Towards Efficient {NLP}: A Standard Evaluation and A Strong Baseline",
    author = "Liu, Xiangyang  and
      Sun, Tianxiang  and
      He, Junliang  and
      Wu, Jiawen  and
      Wu, Lingling  and
      Zhang, Xinyu  and
      Jiang, Hao  and
      Cao, Zhao  and
      Huang, Xuanjing  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.240",
    doi = "10.18653/v1/2022.naacl-main.240",
    pages = "3288--3303",
}


@inproceedings{de2021hyperparameter,
    title = "Hyperparameter Power Impact in Transformer Language Model Training",
    author = "Puvis de Chavannes, Lucas H{\o}yberg  and
      Kongsbak, Mads Guldborg Kjeldgaard  and
      Rantzau, Timmie  and
      Derczynski, Leon",
    booktitle = "Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sustainlp-1.12",
    doi = "10.18653/v1/2021.sustainlp-1.12",
    pages = "96--118",
}

@article{derczynski2020power,
  title={Power consumption variation over activation functions},
  author={Derczynski, Leon},
  journal={arXiv preprint arXiv:2006.07237},
  url={https://arxiv.org/abs/2006.07237},
  year={2020}
}

@article{fedus2022review,
  title={A Review of Sparse Expert Models in Deep Learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  url={https://arxiv.org/abs/2209.01667},
  year={2022}
}

@inproceedings{ethayarajh2022understanding,
  title={Understanding Dataset Difficulty with V-Usable Information},
  author={Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle={International Conference on Machine Learning},
  pages={5988--6008},
  year={2022},
  organization={PMLR}
}



@misc{palm,
	doi = {10.48550/ARXIV.2204.02311},
	url = {https://arxiv.org/abs/2204.02311},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	title = {{PaLM: Scaling Language Modeling with Pathways}},
	year = {2022},
	note = {{arXiv}:2204.02311},
}

@inproceedings{khandelwal2020nearest,
  title={\href{https://arxiv.org/abs/2010.00710}{Nearest neighbor machine translation}},
  author={Khandelwal, Urvashi and Fan, Angela and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={Proccedings of International Conference on Learning Representations},
  year={2021}
}

@inproceedings{meng2021fast,
    title = "Fast Nearest Neighbor Machine Translation",
    author = "Meng, Yuxian  and
      Li, Xiaoya  and
      Zheng, Xiayu  and
      Wu, Fei  and
      Sun, Xiaofei  and
      Zhang, Tianwei  and
      Li, Jiwei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.47",
    doi = "10.18653/v1/2022.findings-acl.47",
    pages = "555--565",
}

@article{martins2022chunk,
  title={Chunk-based Nearest Neighbor Machine Translation},
  author={Martins, Pedro and Marinho, Zita and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2205.12230},
  url={https://arxiv.org/abs/2205.12230},
  year={2022}
}

@article{wang2021faster,
  title={Faster Nearest Neighbor Machine Translation},
  author={Wang, Shuhe and Li, Jiwei and Meng, Yuxian and Ouyang, Rongbin and Wang, Guoyin and Li, Xiaoya and Zhang, Tianwei and Zong, Shi},
  year={2021},
  url={https://arxiv.org/abs/2112.08152},
  journal={arXiv preprint arXiv:2112.08152},
}

@inproceedings{alon2022neuro,
  title={\href{https://arxiv.org/abs/2201.12431}{Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval}},
  author={Alon, Uri and Xu, Frank F and He, Junxian and Sengupta, Sudipta and Roth, Dan and Neubig, Graham},
  booktitle={Procceedings of International Conference on Machine Learning},
  year={2022}
}

@inproceedings{wang2022non,
  title={\href{https://arxiv.org/abs/2109.11136}{Non-parametric online learning from human feedback for neural machine translation}},
  author={Wang, Dongqi and Wei, Haoran and Zhang, Zhirui and Huang, Shujian and Xie, Jun and Chen, Jiajun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={11431--11439},
  year={2022}
}

@article{li2022survey,
  title={A Survey on Retrieval-Augmented Text Generation},
  author={Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
  journal={arXiv preprint arXiv:2202.01110},
  url={https://arxiv.org/abs/2202.01110},
  year={2022}
}

@inproceedings{treviso-etal-2022-predicting,
    title = "Predicting Attention Sparsity in Transformers",
    author = "Treviso, Marcos  and
      G{\'o}is, Ant{\'o}nio  and
      Fernandes, Patrick  and
      Fonseca, Erick  and
      Martins, Andre",
    booktitle = "Proceedings of the Sixth Workshop on Structured Prediction for NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.spnlp-1.7",
    doi = "10.18653/v1/2022.spnlp-1.7",
    pages = "67--81",
}

@misc{wei_emergent_2022,
    title = {Emergent {{Abilities}} of {{Large Language Models}}},
    author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
    year = {2022},
    month = jun,
    number = {arXiv:2206.07682},
    eprint = {2206.07682},
    eprinttype = {arxiv},
    primaryclass = {cs},
    institution = {{arXiv}},
    doi = {10.48550/arXiv.2206.07682},
    url = {http://arxiv.org/abs/2206.07682},
    urldate = {2022-06-22},
    abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
    archiveprefix = {arXiv}
}

@inproceedings{
    micikevicius2018mixed,
    title={Mixed Precision Training},
    author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=r1gs9JgRZ},
}


@inproceedings{micikevicius_mixed_2018,
    title = {Mixed {{Precision Training}}},
    booktitle = {International {{Conference}} on {{Learning Representations}}},
    author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
    year = {2018},
    month = feb,
    url = {https://openreview.net/forum?id=r1gs9JgRZ},
    urldate = {2021-11-15},
    abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural...}
}


@article{kalamkar_study_2019,
    title = {A {{Study}} of {{BFLOAT16}} for {{Deep Learning Training}}},
    author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
    year = {2019},
    month = may,
    url = {https://arxiv.org/abs/1905.12322v3},
    journal={arXiv preprint arXiv:1905.12322v3},
    urldate = {2022-02-02},
    abstract = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.}
}


@inproceedings{ren_zero-offload_2021,
    title = {\{\vphantom\}{{ZeRO-Offload}}\vphantom\{\}: {{Democratizing}} \{\vphantom\}{{Billion-Scale}}\vphantom\{\} {{Model Training}}},
    shorttitle = {\{\vphantom\}{{ZeRO-Offload}}\vphantom\{\}},
    booktitle = {2021 {{USENIX Annual Technical Conference}} ({{USENIX ATC}} 21)},
    author = {Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
    year = {2021},
    pages = {551--564},
    url = {https://www.usenix.org/conference/atc21/presentation/ren-jie},
    urldate = {2022-05-17},
    isbn = {978-1-939133-23-6}
}

@inproceedings{
    dettmers2022bit,
    title={8-bit Optimizers via Block-wise Quantization},
    author={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=shpkpVXzo3h}
}

@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  url={https://arxiv.org/abs/2107.13586},
  year={2021}
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
}

@article{tay2020efficient,
    author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
    title = {Efficient Transformers: A Survey},
    year = {2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3530811},
    doi = {10.1145/3530811},
    journal = {ACM Comput. Surv.},
    month = {apr},
    keywords = {neural networks, transformers, attention, deep learning}
}


@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
}

@inproceedings{schick-schutze-2021-just,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
}
@inproceedings{shin-etal-2020-autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
}
@inproceedings{peters-martins-2021-smoothing,
    title = "Smoothing and Shrinking the Sparse {S}eq2{S}eq Search Space",
    author = "Peters, Ben  and
      Martins, Andr{\'e} F. T.",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.210",
    doi = "10.18653/v1/2021.naacl-main.210",
    pages = "2642--2654",
}

@article{rae2022scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  url={https://arxiv.org/abs/2112.11446},
  year={2021}
}

@techreport{J1WhitePaper,
  author = {Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  title = {Jurassic-1: Technical Details And Evaluation},
  institution = {AI21 Labs},
  year = 2021,
  month = aug,
}

@inproceedings{Peters:2018,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
}

@article{feurer2020auto,
  title={Auto-sklearn 2.0: The next generation},
  author={Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
  journal={arXiv preprint arXiv:2007.04074},
  volume={24},
  year={2020}
}
@article{zimmer2021auto,
  title={Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL},
  author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={9},
  pages={3079--3090},
  year={2021},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/9382913}
}


@article{Gou:2021,
author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
title = {Knowledge Distillation: A Survey},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {129},
number = {6},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-021-01453-z},
doi = {10.1007/s11263-021-01453-z},
abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
journal = {Int. J. Comput. Vision},
month = {jun},
pages = {1789–1819},
numpages = {31},
keywords = {Teacher–student architecture, Knowledge transfer, Knowledge distillation, Model compression, Deep neural networks}
}



@inproceedings{Dodge:2022,
    author = {Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
    title = {{Measuring the Carbon Intensity of AI in Cloud Instances}},
    year = {2022},
    isbn = {9781450393522},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3531146.3533234},
    doi = {10.1145/3531146.3533234},
    booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {1877–1894},
    numpages = {18},
    keywords = {cloud, CO2, emissions, carbon awareness, grid, carbon intensity},
    location = {Seoul, Republic of Korea},
    series = {FAccT '22}
}



@article{ponti2022combining,
  title={Combining modular skills in multitask learning},
  author={Ponti, Edoardo M and Sordoni, Alessandro and Reddy, Siva},
  journal={arXiv preprint arXiv:2202.13914},
  url={https://arxiv.org/abs/2202.13914},
  year={2022}
}

@article{wu2022sustainable,
  title={Sustainable ai: Environmental implications, challenges and opportunities},
  author={Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={795--813},
  url={https://arxiv.org/abs/2111.00364},
  year={2022}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  url={https://arxiv.org/abs/2104.10350},
  year={2021}
}

@article{ma2022mega,
  title={Mega: Moving Average Equipped Gated Attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2209.10655},
  url={https://arxiv.org/abs/2209.10655},
  year={2022}
}

@article{zhai2021attention,
  title={An attention free transformer},
  author={Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
  journal={arXiv preprint arXiv:2105.14103},
  url={https://arxiv.org/abs/2105.14103},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url={https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{peters-etal-2019-sparse,
    title = "Sparse Sequence-to-Sequence Models",
    author = "Peters, Ben  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1146",
    doi = "10.18653/v1/P19-1146",
    pages = "1504--1519",
}

@inproceedings{xia-etal-2022-structured,
    title = "Structured Pruning Learns Compact and Accurate Models",
    author = "Xia, Mengzhou  and
      Zhong, Zexuan  and
      Chen, Danqi",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.107",
    doi = "10.18653/v1/2022.acl-long.107",
    pages = "1513--1528",
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{radford2019language-gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{
penedo2024the-fineweb,
title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale},
author={Guilherme Penedo and Hynek Kydl{\'\i}{\v{c}}ek and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=n6SCkn2QaG}
}


@article{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	urldate = {2020-11-13},
	journal = {arXiv:2004.05150 [cs]},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05150},
	keywords = {Computer Science - Computation and Language},
}


@article{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	url = {http://arxiv.org/abs/1904.10509},
	urldate = {2020-09-17},
	journal = {arXiv:1904.10509 [cs, stat]},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.10509
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tay_efficient_2020,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	urldate = {2020-09-16},
	journal = {arXiv:2009.06732 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.06732
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018}
}

@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580/",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808",
}

@inproceedings{martins2016softmax,
  title = 	 {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
  author = 	 {Andre Martins and Ramon Astudillo},
  pages = 	 {1614--1623},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  booktitle ={International Conference on Machine Learning (ICML)},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/martins16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/martins16.html}
}

@Article{Tsallis1988,
    author="Tsallis, Constantino",
    title="Possible generalization of Boltzmann-Gibbs statistics",
    journal="Journal of Statistical Physics",
    year="1988",
}