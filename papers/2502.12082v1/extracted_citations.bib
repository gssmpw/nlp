@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}

@InProceedings{blondel-entmax,
  title = 	 {Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms},
  author =       {Blondel, Mathieu and Martins, Andre and Niculae, Vlad},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {606--615},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/blondel19a/blondel19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/blondel19a.html},
  abstract = 	 {This paper studies Fenchel-Young losses, a generic way to construct convex loss functions from a regularization function.  We analyze their properties in depth, showing that they unify many well-known loss functions and allow to create useful new ones easily.  Fenchel-Young losses constructed from a generalized entropy, including the Shannon and Tsallis entropies, induce predictive probability distributions.  We formulate conditions for a generalized entropy to yield losses with a separation margin, and probability distributions with sparse support.  Finally, we derive efficient algorithms, making Fenchel-Young losses appealing both in theory and practice.}
}

@article{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	url = {http://arxiv.org/abs/1904.10509},
	urldate = {2020-09-17},
	journal = {arXiv:1904.10509 [cs, stat]},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.10509
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{condats,
author = {Condat, Laurent},
title = {Fast projection onto the simplex and the $\ell_1$ ball},
year = {2016},
issue_date = {July      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {158},
number = {1–2},
issn = {0025-5610},
url = {https://doi.org/10.1007/s10107-015-0946-6},
doi = {10.1007/s10107-015-0946-6},
abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an $$l_1$$l1-norm ball. It can be viewed as a Gauss---Seidel-like variant of Michelot's variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
journal = {Math. Program.},
month = jul,
pages = {575–585},
numpages = {11},
keywords = {$$l_1$$l1-Norm ball, 49M30, 65C60, 65K05, 90C25, Large-scale optimization, Simplex}
}

@inproceedings{correia-etal-2019-adaptively,
    title = "Adaptively Sparse Transformers",
    author = "Correia, Gon{\c{c}}alo M.  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1223/",
    doi = "10.18653/v1/D19-1223",
    pages = "2174--2184",
    abstract = "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter {--} which controls the shape and sparsity of alpha-entmax {--} allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{devoto-etal-2024-simple,
    title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Cache Compression",
    author = "Devoto, Alessio  and
      Zhao, Yu  and
      Scardapane, Simone  and
      Minervini, Pasquale",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1027/",
    doi = "10.18653/v1/2024.emnlp-main.1027",
    pages = "18476--18499",
}

@misc{dong2024flexattentionprogrammingmodel,
      title={Flex Attention: A Programming Model for Generating Optimized Attention Kernels}, 
      author={Juechu Dong and Boyuan Feng and Driss Guessous and Yanbo Liang and Horace He},
      year={2024},
      eprint={2412.05496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.05496}, 
}

@inproceedings{ibis2009efficient,
  title={Efficient Euclidean projections in linear time},
  author={Liu, Jun and Ye, Jieping},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={657--664},
  year={2009}
}

@inproceedings{kitaev2020reformer,
    title={Reformer: The Efficient Transformer},
    author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{luohe2024keep,
title={Keep the Cost Down: A Review on Methods to Optimize {LLM}{\textquoteright}s {KV}-Cache Consumption},
author={Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=8tKjqqMM5z}
}

@inproceedings{martins2016softmax,
  title = 	 {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
  author = 	 {Andre Martins and Ramon Astudillo},
  pages = 	 {1614--1623},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  booktitle ={International Conference on Machine Learning (ICML)},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/martins16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/martins16.html}
}

@inproceedings{pagliardini2023fast,
title={Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention},
author={Matteo Pagliardini and Daniele Paliotta and Martin Jaggi and Fran{\c{c}}ois Fleuret},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=UINHuKeWUa}
}

@inproceedings{peters-etal-2019-sparse,
    title = "Sparse Sequence-to-Sequence Models",
    author = "Peters, Ben  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1146",
    doi = "10.18653/v1/P19-1146",
    pages = "1504--1519",
}

@article{roy2021efficient,
    title = "Efficient Content-Based Sparse Attention with Routing Transformers",
    author = "Roy, Aurko  and
      Saffar, Mohammad  and
      Vaswani, Ashish  and
      Grangier, David",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.4",
    doi = "10.1162/tacl_a_00353",
    pages = "53--68",
}

@ARTICLE{sort-and-scan,
  title    = "Validation of subgradient optimization",
  author   = "Held, Michael and Wolfe, Philip and Crowder, Harlan P",
  abstract = "The ``relaxation'' procedure introduced by Held and Karp for
              approximately solving a large linear programming problem related
              to the traveling-salesman problem is refined and studied
              experimentally on several classes of specially structured
              large-scale linear programming problems, and results on the use
              of the procedure for obtaining exact solutions are given. It is
              concluded that the method shows promise for large-scale linear
              programming",
  journal  = "Mathematical Programming",
  volume   =  6,
  number   =  1,
  pages    = "62--88",
  month    =  dec,
  year     =  1974
}

@inproceedings{treviso-etal-2022-predicting,
    title = "Predicting Attention Sparsity in Transformers",
    author = "Treviso, Marcos  and
      G{\'o}is, Ant{\'o}nio  and
      Fernandes, Patrick  and
      Fonseca, Erick  and
      Martins, Andre",
    booktitle = "Proceedings of the Sixth Workshop on Structured Prediction for NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.spnlp-1.7",
    doi = "10.18653/v1/2022.spnlp-1.7",
    pages = "67--81",
}

