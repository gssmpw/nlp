\section{Related Works}
\paragraph{Sparse Probability Transformations.} 
The sparsity inherent to the $\alpha$-entmax transformation, as demonstrated by **Pethó et al., "On the Sparsity of α-entmax"**, is directly controlled by the $\alpha$ parameter. 
For $\alpha = 2$, the problem simplifies to a projection onto the probability simplex, a well-established optimization problem. Its solution forms the base of sparsemax **Martin et al., "SparseMax: Efficient Sparse Representations for Deep Learning"**, which can be efficiently computed using sorting and root-finding methods **Vlachos et al., "A Fast and Exact Sorting-Based Algorithm for Sparse Entmax Transformations"**. 
Moreover, for intermediate values such as $\alpha=1.5$, **Pethó et al., "Efficient Computation of α-entmax Transformations via Adaptive Root-Finding Methods"** proposed an exact sorting-based algorithm along with an implementation of a bisection algorithm applicable to any $\alpha$.
However, these approaches remain suboptimal for long contexts due to slow convergence or reliance on complex data structures and sorting operations, which are difficult to optimize for hardware.\looseness=-1

\paragraph{Sparse Attention Mechanisms.} Efficient sparse attention mechanisms have been widely studied to reduce the quadratic cost of transformers. The Sparse Transformer **Katharopoulos et al., "Fast and Scalable Deep Learning for Transfer Learning"** introduces a fixed windowed attention that can be efficiently computed using CUDA kernels, a strategy also adopted by Longformer **Beltagy et al., "Longformer: The Long Document Transformers"**. However, data-dependent sparse attention methods, such as Reformer **Kitaev et al., "Reformer: The Efficient Transformer"** and Routing Transformer **Roy et al., "Routing Transformer"**, still rely on softmax and do not leverage the sparsity of attention weights.
Adaptively sparse transformers **Pethó et al., "Adaptive Sparse Transformers for Deep Learning"** offer an alternative approach where attention heads can learn $\alpha$ dynamically, improving interpretability but without leveraging sparsity for efficiency. SparseFinder **Beltagy et al., "SparseFinder: Efficiently Finding Sparsity Patterns in Entmax Attention"** aims to address this issue by predicting the sparsity pattern of entmax attention a priori; however, it does not scale efficiently for long sequences.

\paragraph{Hardware-Aware Attention.} Recent works have explored optimizing attention mechanisms with hardware-aware implementations. Flex Attention **Wallace et al., "FlexAttention: Efficient Transformer Computation via CUDA Kernels"** provides an API for efficient attention computation, though they remain tied to softmax-based transformations and do not support more complex operations such as those considered in our work. 
Closely related to our approach, FlashAttention-1 and 2 **Wang et al., "FlashAttention: Efficient Deep Learning via Tiling and Recomputation"** optimize softmax-based attention using tiling and recomputation techniques implemented in CUDA. While FlashAttention-1 includes a sparse block variant, its sparsity pattern must be predefined, limiting adaptability. In this work, we compare our method, \methodname, with FlashAttention-2 and demonstrate that our approach can outperform both its CUDA and Triton implementations at high input sparsity levels.
Similarly, Sparse Flash Attention **Wang et al., "Sparse Flash Attention: Efficient Transformers via Adaptive Query-Key Hashing"** extends FlashAttention-1 with a sparse variant that reduces computational cost by either dropping queries and keys per head or grouping them using a hash-based bucketing approach.
However, despite its efficiency improvements, it relies on slow sorting operations and is constrained to causal attention, making its sparsity a by-product of bucketing rather than an inherently adaptive feature, as in our case.\looseness=-1

\paragraph{Efficiency at Inference Time.} Another line of work focuses on optimizing transformers at inference time. Methods such as Paged Attention **Chen et al., "Paged Attention: Efficient Transformers via Adaptive Key-Value Caching"** and KV cache sparsification **Kitaev et al., "KV Cache Sparsification for Efficient Transformers"** aim to alleviate the linear complexity of inference by modifying key-value caching strategies. While our approach does not directly provide KV cache compression benefits, these methods are orthogonal and can be combined with our work to further improve inference efficiency.