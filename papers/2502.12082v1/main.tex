
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\usepackage{graphicx}
\usepackage{comment}
\usepackage{xspace}
\usepackage{enumitem}

\usepackage{amsfonts}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{dsfont}

\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{arydshln}

\makeatletter
\def\adl@drawiv#1#2#3{%
        \hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}%
                #2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{%
  \noalign{\vskip 2pt
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}[.4pt/2pt]
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip 2pt}}
\makeatother


\DeclareMathOperator*{\onehot}{one-hot}
\DeclareMathOperator*{\topk}{top-$k$}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argtopk}{arg\,topk}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\bigO}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\softmax}[1]{\text{softmax}\left(#1\right)}
\newcommand{\entmax}[1]{\text{entmax}\left(#1\right)}
\newcommand{\sparsemax}[1]{\text{sparsemax}\left(#1\right)}
\newcommand{\aentmax}[1]{\alpha\text{-entmax}\left(#1\right)}

\renewcommand\ttdefault{cmtt}
\renewcommand\sfdefault{cmss}


\definecolor{set10-red}{HTML}{e41a1c}
\definecolor{set10-blue}{HTML}{377eb8}
\definecolor{set10-green}{HTML}{4daf4a}



\newcommand{\methodname}{\textsc{AdaSplash}\xspace}


\icmltitlerunning{\methodname: Adaptive Sparse Flash Attention}

\begin{document}

\twocolumn[
\icmltitle{\methodname: 
Adaptive Sparse Flash Attention
}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nuno Gonçalves}{ist}
\icmlauthor{Marcos Treviso}{it}
\icmlauthor{André F. T. Martins}{ist,it,ell,unb}
\end{icmlauthorlist}

\icmlaffiliation{ist}{Instituto Superior Técnico, Universidade de Lisboa}
\icmlaffiliation{it}{Instituto de Telecomunicações}
\icmlaffiliation{unb}{Unbabel}
\icmlaffiliation{ell}{ELLIS Unit Lisbon}

\icmlcorrespondingauthor{Nuno Gonçalves}{nuno.m.goncalves@tecnico.pt}



\icmlkeywords{Sparsity, Flash Attention}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %

\begin{abstract}
The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks.
Adaptive sparsity, of which $\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains.  
In this work, we propose \methodname, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\alpha$-entmax. 
We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. 
Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\alpha$-entmax implementations. 
It approaches---and in some cases surpasses---the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.%





\end{abstract}

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/relative_speeds_attention_2.pdf}
    \caption{Runtime (Fwd+Bwd) as a function of input sparsity for non-causal attention. 
    While the highly-optimized FlashAttention-2 maintains a constant runtime across varying levels of sparsity, \methodname effectively leverages sparsity to obtain speed-ups, eventually outperforming FlashAttention-2 as sparsity grows.}
    \label{fig:sparsity-speedup}
    \vspace{-0.5cm}
\end{figure}

Central to the success of transformers \citep{vaswani2017attention} lies the attention mechanism, where each token in a sequence attends directly to every other token. Attention probabilities are computed through the \textbf{softmax} transformation, which always assigns a nonzero probability to every token. However, for long context inputs, the accumulation of small probabilities can lead to dispersion \citep{velivckovic2024softmax}. 
In fact, 
previous research shows that attention probabilities tend to peak around a small number of tokens~\citep{voita-etal-2019-analyzing,treviso-etal-2022-predicting}, which suggests that model performance and computational efficiency can be increased by leveraging attention sparsity. 
This has motivated methods that predefine sparse masks \citep{beltagy_longformer_2020, zaheer2020bigbird}, rely on clustering-based strategies \cite{kitaev2020reformer}, or low-rank approximate attention \citep{choromanski2021rethinkingperformer, peng2021randomattention, xiong2021nystromformer, chen2021scatterbrain}. Some of these techniques show the potential of sparsity to mitigate memory and computation bottlenecks, but they often require architectural modifications or crude approximations, limiting their flexibility and generality.

A related line of research explores adaptive and differentiable sparse activations as surrogates of softmax, such as \textbf{sparsemax} \citep{martins2016softmax} and, more broadly, the \textbf{$\alpha$-entmax} family \cite{peters-etal-2019-sparse, correia-etal-2019-adaptively}. 
By assigning zero probability to irrelevant tokens, these activations eliminate their residual influence, reducing the dilution of attention scores and potentially improving both performance and interpretability. 
Unfortunately, existing algorithms and implementations for these adaptive sparse activations do not exploit the sparsity, being slower than softmax-based attention and struggling to scale effectively with context length, primarily due to the lack of hardware-optimized implementations like FlashAttention-2~\citep{dao2023flashattention2} or support from programming models like FlexAttention \cite{dong2024flexattentionprogrammingmodel}.\looseness=-1

This paper addresses this problem by providing new  algorithms and implementations to improve 
the computational efficiency of the family of $\alpha$-entmax activations.  %
Our main contributions include a faster and GPU-friendly algorithm for calculating $\alpha$-entmax, alongside a Triton kernel \citep{triton-paper} for computing entmax-based attention, which we call \methodname. 
In particular, \methodname advances the goal of supporting training of adaptively sparse models with longer context lengths, as shown in Figure~\ref{fig:sparsity-speedup}. 
We demonstrate the potential and scalability of our approach through experiments with synthetic data and with several natural language processing benchmarks for encoder-only and decoder-only models, achieving substantial improvements over previous $\alpha$-entmax implementations and approaching (sometimes surpassing) the efficiency of softmax-based attention with FlashAttention-2, with strong performance on downstream tasks.\footnote{Code at: \url{https://github.com/deep-spin/adasplash}} 



\section{Background}


\subsection{Hardware Performance}

Modern GPUs, such as the Nvidia H100, are designed for efficient parallel computation using a hierarchical memory architecture, with high-bandwidth memory (HBM) providing large capacity but slower access compared to the smaller, faster on-chip SRAM. Efficient use of SRAM is critical to minimize the memory bottlenecks caused by frequent HBM accesses. GPUs execute operations (kernels) via thousands of threads organized into blocks and warps, where data is loaded from HBM into SRAM for computation before being written back. Kernel fusion is a key optimization strategy that combines multiple operations into a single kernel, reducing intermediate HBM accesses by directly computing and storing final results. While compilers like \texttt{torch.compile} can automate fusion for simple operations~\citep{torchcompile}, complex tasks such as attention mechanisms require custom strategies to reorder operations and optimize memory usage effectively. Our method leverages this GPU memory organization by implementing block-wise computations, recomputation strategies, and kernel fusion specifically tailored for sparse attention, as detailed in \S\ref{sec:fwd-pass} and \S\ref{sec:bwd-pass}.


\subsection{Standard Attention}

Given a set of matrices $\bm{Q}, \bm{K},\bm{V} \in \mathbb{R}^{n \times d}$ containing $d$-dimensional representations for $n$ queries, keys and values, the \textit{dot-product
self-attention} at a single head is computed in the following way~\citep{vaswani2017attention}:
\begin{equation}\label{eq:dotproduct-attention}
    \bm{O} = \pi
    \Bigg(\underbrace{
        \frac{\bm{Q}\bm{K}^\top}{\sqrt{d}}
    }_{\bm{S} \in \mathbb{R}^{n \times n}}\Bigg) 
    \bm{V} \in \mathbb{R}^{n \times d}.
\end{equation}
The $\pi$ transformation usually maps rows to distributions, with $\pi(\bm{S})_{ij} = \softmax{\bm{s}_i}_j$ being a common choice. 
For decoder-only models, $\bm{S}$ is masked in order to ignore the contribution from future tokens.
Notably, a naive implementation of Equation~\ref{eq:dotproduct-attention} leads to a $\bigO{n^2}$ time and memory complexity for training.






\subsection{FlashAttention}

To address the costs of naive attention implementations, \citet{dao2022flashattention} introduced FlashAttention, an algorithm that avoids the  materialization of quadratic matrices via a GPU-aware implementation of online softmax~\citep{milakov2018online}, bringing the overall memory complexity to $\bigO{n}$. 
Subsequent versions of FlashAttention further improved GPU usage by reordering the loops, reducing the number of non-GEMM (general matrix multiply) operations \citep{dao2023flashattention2}, and exploiting the asynchronicity and support for FP8 low-precision on the new Hopper GPUs \citep{shah2024flashattention3}. The key idea of FlashAttention is to split the inputs $\bm{Q}, \bm{K}, \bm{V}$ into blocks, load them from  slow GPU high bandwidth memory (HBM) to the fast GPU on-chip SRAM, then compute the attention output regarding those blocks and, at the end, scale the output by the right normalization factor. 










\subsection{Sparse Attention}

The original softmax-based attention is {\it dense}, i.e., it puts \textit{some} probability mass on all tokens---not only a computational disadvantage, but also making interpretation and generalization harder~\citep{voita-etal-2019-analyzing,treviso-etal-2022-predicting,veličković2024softmaxforsharpoutofdistribution}. An alternative to softmax is the {\bf $\boldsymbol{\alpha}$-entmax transformation}  \citep{peters-etal-2019-sparse}, which is differentiable and leads to sparse outputs: 
\begin{equation}\label{eq:solution_entmax}
    \aentmax{\bm{s}} = [(\alpha - 1)\bm{s} - \tau\bm{1}]_{+}^{\nicefrac{1}{\alpha-1}},
\end{equation}
where $[\cdot]_{+}$ is the ReLU function, and $\tau \in \mathbb{R}$ is a normalizing constant to ensure the output is a valid probability distribution. 
Importantly, entries with score $s_i \leq \frac{\tau}{\alpha-1}$ get exactly zero probability. 
In the limit $\alpha \rightarrow 1$, $\alpha$-entmax recovers the softmax function, while for any value of $\alpha>1$ this transformation returns increasingly sparser probability vectors. 
When $\alpha=2$, we recover the sparsemax transformation \citep{martins2016softmax}. 
However, in contrast to fixed sparse patterns, such as windowed sparse attention~\citep{child_generating_2019,beltagy_longformer_2020} and block-sparse variants~\citep{zaheer2020bigbird,dao2022flashattention}, $\alpha$-entmax's sparsity patterns are dynamic and hence difficult to exploit in order to reduce the quadratic burden of self-attention because we still need to materialize $\bm{S} = \bm{Q}\bm{K}^\top$ before applying the  transformation. 




In the next section (\S\ref{sec:method}), we outline \methodname, our new method for computing $\alpha$-entmax attention, along with a novel custom Triton kernel~\citep{triton-paper} that enables efficient training of transformers for extremely long context lengths.
As shown in \S\ref{sec:experiments}, our implementation maintains competitiveness with state-of-the-art algorithms such as FlashAttention by leveraging the sparsity given by $\alpha$-entmax, effectively exploiting the advantages of sparse attention at scale. 





















\section{\methodname} \label{sec:method} 

We start by revisiting the computation of $\alpha$-entmax for general values of $\alpha$ in \S\ref{subsec:computing_entmax}, and proposing a new algorithm that has a fast empirical convergence.
We design an efficient Triton kernel in \S\ref{subsec:kernel_entmax}, dubbed \methodname, that effectively leverages adaptive sparsity patterns in both the forward and backward passes of $\alpha$-entmax in order to minimize runtime.


\subsection{$\alpha$-entmax Computation} \label{subsec:computing_entmax}

In order to compute Equation \ref{eq:solution_entmax} for a given $\bm{s} \in \mathbb{R}^n$, we need to find the threshold $\tau \in \mathbb{R}$ such that the resulting output sums to $1$. Mathematically, this is equivalent to finding the root of the following equation:
\begin{equation} \label{eq:rootfind-func}    
f(\tau) = \sum_{i} \left[ (\alpha - 1) s_i - \tau \right]_+^{1/(\alpha-1)} - 1. 
\end{equation}

\paragraph{Exact algorithms for $\alpha \in \{1.5, 2\}$.} In particular, for $\alpha = 2$, the computation is reduced to an Euclidean projection onto the probability simplex, for which efficient algorithms have been extensively studied \citep{sort-and-scan, pivot-partition, condats}. %
Similarly, for $\alpha=1.5$, \citet{peters-etal-2019-sparse} introduced an exact sort-based algorithm. 
However, these methods either require complex data structures that are not efficiently handled in GPUs, or sorting-based algorithms, which require the materialization of the entire input. 

\paragraph{Bisection algorithm for $\alpha > 1$.} For a general $\alpha$, \citet{blondel-entmax} introduced a bisection update rule to approximate $\tau$ by iteratively refining its lower ($\tau_{\text{lo}}$) and higher ($\tau_{\text{hi}}$) bounds:
\begin{equation} \label{eq:bisection_update}
    B_f(\tau) =
    \begin{cases}
        (\tau_{\text{lo}}, \tau) & \text{if } f(\tau) < 0, \\
        (\tau, \tau_{\text{hi}}) & \text{otherwise},
    \end{cases}
\end{equation}
obtaining $\tau = \frac{1}{2}(\tau_{\text{lo}} + \tau_{\text{hi}})$ after the last iteration.
While the bisection algorithm is simple and effective, it converges at a linear rate \citep{bisection-linear-proof}, meaning the absolute error decreases by approximately half at each iteration. Achieving high precision often requires many iterations, resulting in frequent memory accesses. 
As a result, in memory-bound scenarios where the time taken is mostly determined by the number of memory accesses---such as in attention---the number of iterations can significantly impact the runtime cost.




\begin{algorithm}[t]
   \caption{Halley-bisection algorithm for $\alpha$-entmax.}
   \label{alg:entmax_halley}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} logits $\bm{s} \in \mathbb{R}^n$, param. $\alpha \in \mathbb{R}$, iterations $T$
   \STATE Define $f(\tau) := \sum_i[s_i - \tau]_+^{1/(\alpha-1)} - 1$
   \STATE Set $\bm{s} \leftarrow (\alpha - 1) \bm{s}$
   \STATE Initialize $\tau_{\text{lo}} = \max(\bm{s}) - 1$
   \STATE Initialize $\tau_{\text{hi}} = \max(\bm{s}) - n^{1-\alpha}$
   \STATE Initialize $\tau = (\tau_{\text{lo}}+\tau_{\text{hi}})/2$
   \REPEAT
   \STATE Compute $\tau_{\text{lo}}, \tau_{\text{hi}} = B_f(\tau)$ (Equation~\ref{eq:bisection_update})
   \STATE Compute $\tau_{H} = H_f(\tau)$ (Equation~\ref{eq:halley_update})
   \IF{$\tau_{H} \in \left[\tau_{\text{lo}}, \tau_{\text{hi}}\right]$} 
       \STATE $\tau \leftarrow \tau_{H}$ \hfill \(\triangleright\) (Halley's Update)
   \ELSE
       \STATE $\tau \leftarrow \frac{1}{2}(\tau_{\text{lo}} + \tau_{\text{hi}})$ \hfill \(\triangleright\) (Bisection Update)
   \ENDIF
   \UNTIL{$T$ iterations are completed}
   \STATE {\bfseries Output:} $[\bm{s} - \tau\bm{1}]_+^{1/(\alpha-1)}$
\end{algorithmic}
\end{algorithm}


\paragraph{Halley-bisection algorithm.} 
In order to obtain a faster runtime, we propose a hybrid algorithm for solving Equation~\ref{eq:rootfind-func} for any $\alpha > 1$ that combines the convergence guarantee of bisection with the faster convergence of Halley's method \citep{halleys-method}.
As we show in \S\ref{subsec:efficiency_benchmark}, this approach achieves significant wall-clock speed-ups while requiring fewer iterations to attain the same precision.


The function defined in Equation \ref{eq:solution_entmax} enjoys a cheap computation of its derivatives. Thus, methods that incorporate second-order information, such as Halley's method, can be leveraged to improve the approximation of $\tau$ at each iteration. 
Halley's method, which uses both the first and second derivatives, updates the solution using the following rule:
\begin{equation} \label{eq:halley_update}
H_f(\tau) = \tau - \frac{2f(\tau)f'(\tau)}{2f'(\tau)^2 - f(\tau)f''(\tau)},
\end{equation}

where the derivatives are given as follows:
\begin{align} \label{eq:derivatives_f1}
f'(\tau) &= -\frac{1}{\alpha-1}\sum_{i} \left[ (\alpha - 1) s_i - \tau \right]_+^{1/(\alpha-1) - 1}, \\ \label{eq:derivatives_f2}
f''(\tau) &= \frac{2-\alpha}{(\alpha-1)^2}\sum_{i} \left[ (\alpha - 1) s_i - \tau \right]_+^{1/(\alpha-1) - 2}.
\end{align}


While Halley's method offers faster convergence under ideal conditions, it does not always converge, particularly when the initial guess is far from the solution. 
To ensure convergence, we introduce a fail-safe mechanism that integrates the convergence guarantee of bisection: whenever Halley's method produces an update that moves the solution out of the bisection bounds, the algorithm reverts to a bisection update $B_f(\tau)$. This ensures that the algorithm converges, even in the worst cases, while leveraging the cubic convergence of Halley's method wherever possible. We outline our hybrid algorithm in Algorithm~\ref{alg:entmax_halley}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/iters_lineplot.pdf}
    \caption{Comparison of mean absolute error magnitudes between Halley-bisection and Torch's bisection methods across iterations, measured against the exact solution for $\alpha=1.5$.
    }
    \label{fig:iterations_convergence}
    \vspace{-0.15cm}
\end{figure}





\paragraph{Efficiency Benchmark.} 
We compare the runtime of Halley-bisection against existing algorithms for computing $\alpha$-entmax implemented in Torch.
Specifically, we generate random tensors from a standard Gaussian distribution ($\mu = 0, \sigma^2 = 1$) with a fixed sequence length of $n=8192$. For each configuration, we measure the average runtime over 1000 runs. 
Overall, we observe that Halley-bisection is significantly more efficient than the standard bisection algorithm implemented in Torch. Halley-bisection achieves a runtime of 2.38 ms, compared to 36.67 ms for the standard bisection algorithm, making it approximately \textbf{15$\times$ faster}. In addition, Halley-bisection reduces memory usage by \textbf{1.75$\times$}, requiring only 512 MB compared to 896.15 MB for bisection. 
Furthermore, in Figure~\ref{fig:iterations_convergence} we show that Halley-bisection ($\alpha=1.5$) requires only 3 iterations to converge to machine precision for both the output and the gradient. On the other hand, the standard bisection algorithm takes 23 iterations to achieve the same precision for both cases.





\subsection{Flash $\alpha$-entmax Attention} \label{subsec:kernel_entmax}


Given an algorithm to compute the entmax mapping that requires $T$ iteration steps, a naive implementation of entmax attention proceeds as follows: 
(1) multiply $\bm{S} = \bm{Q}\bm{K}^\top \in \mathbb{R}^{n \times n}$ and write the result to slow HBM on the GPU; 
(2) load $\bm{S}$ from HBM $T$ times to compute $\bm{\tau}$; 
(3) load $\bm{S}$ from HBM again, and write the result $\bm{P} = \aentmax{\bm{S}}$ to HBM; 
(4) perform a matrix multiplication to get the output $\bm{O} = \bm{P}\bm{V}$. 
However, since most of these operations are memory-bound, the excessive number of HBM accesses leads to slow wall-clock times. Moreover, having to materialize $\bm{S}$ and $\bm{P}$ in memory poses a major bottleneck, as their sizes quickly exceed GPU memory capacity when the sequence length $n$ increases.
To address these issues and speed up $\alpha$-entmax attention on hardware accelerators like GPUs, we propose an algorithm that reduces HBM reads and writes while producing the same outputs as the naive implementation. %



\begin{algorithm}[t]
   \caption{\methodname forward pass (w/o masking)}
   \label{alg:entattention}
\begin{algorithmic}[1]
   \STATE {\bfseries Require:} Matrices $\bm{Q}, \bm{K}, \bm{V} \in \mathbb{R}^{n \times d}$ in HBM, block sizes $B_c, B_r$, param. $\alpha \in \mathbb{R}$
   \STATE Divide $\bm{Q}$ into $T_r = \lceil n / B_r \rceil$ blocks $\bm{Q}_1, \dots, \bm{Q}_{T_r}$ of size $B_r \times d$
   \STATE Divide $\bm{K}, \bm{V}$ into $T_c = \lceil n / B_c \rceil$ blocks $\bm{K}_1, \dots, \bm{K}_{T_c}$, $\bm{V}_1, \dots, \bm{V}_{T_c}$ of size $B_c \times d$
   \STATE Divide $\bm{O} \in \mathbb{R}^{n \times d}$ into $T_r$ blocks $\bm{O}_1, \dots, \bm{O}_{T_r}$ of size $B_r \times d$
   \STATE Divide $\bm{\tau}$ into $T_r$ blocks $\bm{\tau}_1, \dots, \bm{\tau}_{T_r}$ of size $B_r$
   \FOR{$i = 1$ to $T_r$}
       \STATE Load $\bm{Q}_i$ from HBM to on-chip SRAM
       \STATE On chip, initialize $\bm{O}_i$
       \STATE On chip, compute $\bm{\tau}_i$ using Hybrid Halley's with pre-defined $\alpha$, using a block version of Algorithm~\ref{alg:entmax_halley}.
       \FOR{$j = 1$ to $T_c$}
           \STATE Load $\bm{K}_j$, $\bm{V}_j$ from HBM to on-chip SRAM
           \STATE Compute $\bm{S}_i^{(j)} = \bm{Q}_i \bm{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$
           \STATE Compute $\bm{P}_i^{(j)} = \left[(\alpha-1)\bm{S}_i^{(j)}-\bm{\tau}_i\right]_{+}^{\nicefrac{1}{\alpha-1}}$
           \STATE Accumulate $\bm{O}_i \leftarrow \bm{O}_i + \bm{P}_i^{(j)}\bm{V}_j$
       \ENDFOR
       \STATE Write $\bm{O}_i$ and $\bm{\tau}_i$ to HBM
   \ENDFOR
   \STATE {\bfseries Return:} Output $\bm{O}$ and $\bm{\tau}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Forward Pass}\label{sec:fwd-pass}

We outline the forward pass in Algorithm \ref{alg:entattention} (without masking full-zero blocks, which we introduce later on this section).
Concretely, given the inputs $\bm{Q}, \bm{K}, \bm{V} \in \mathbb{R}^{n \times d}$ stored in HBM, the goal is to compute the attention output $\bm{O} \in \mathbb{R}^{n \times d}$ efficiently and write it back to HBM. Akin to the approach taken in FlashAttention~\citep{dao2022flashattention}, we employ two well-known techniques---\textbf{tiling} and \textbf{recomputation}---to address the challenge of materializing the matrices $\bm{S} \in \mathbb{R}^{n \times n}$ and $\bm{P} \in \mathbb{R}^{n \times n}$.  

\paragraph{Tiling.} The key idea involves splitting the inputs $\bm{Q}, \bm{K}, \bm{V}$ into smaller blocks, and then computing attention block by block. 
We start by loading only $\bm{Q}$ and $\bm{K}$ from the slower HBM to the faster SRAM to compute $\boldsymbol{\tau} \in \mathbb{R}^n$ using the  Halley-bisection algorithm (Alg.~\ref{alg:entmax_halley}). 
In order to use the aforementioned algorithm, we need to accumulate three values: $f(\boldsymbol{\tau})$, $f'(\boldsymbol{\tau})$, $f''(\boldsymbol{\tau})$. %
Since $f$, as well at its derivatives, is additive over its inputs, their computation can also be computed in blocks. 
Let $B_r$ and $B_c$ be the row and column block sizes, respectively, and define $T_r = \lceil n/B_r \rceil$ and $T_c = \lceil n/B_c \rceil$. Divide $\bm{Q}$ into $\bm{Q}_1, ..., \bm{Q}_{T_r}$ blocks, and $\bm{K}$ into $\bm{K}_1, ..., \bm{K}_{T_c}$ blocks.
Then, $f(\bm{\tau})$ can be computed as:
\begin{equation}
 f(\bm{\tau}_i) = \sum_{j=1}^{T_c} f(\bm{\tau}_i; \bm{S}_i^{(j)})   
\end{equation}
where $\bm{S}_i^{(j)} = \bm{Q}_i \bm{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$ and $\bm{\tau}_i$ represents the $i$\textsuperscript{th} sliced block of $\bm{\tau}$ with size $T_r$.
Thus, these quantities do not need to ever be materialized and can be accumulated directly in fast memory. 
Afterwards, we load $\bm{V}$ to compute the attention output $\bm{O}$ for those blocks. 
In contrast to FlashAttention, our approach requires loading $\bm{K}$ to compute $\bm{S}$ at least two additional times. Therefore, the forward pass is bound to always be slower than FlashAttention's due to the extra HBM reads and computation.\looseness=-1

\paragraph{Recomputation.} 
In order to avoid the materialization of the matrices $\bm{S}$ and $\bm{P}$, we recompute them again in Algorithm~\ref{alg:entmax_halley}, which is used to compute $\bm{\tau}$, and also recompute them for obtaining the gradients for the backward pass. 
By doing this we are increasing the required FLOPs to reduce the maximum amount of memory required. While this might suggest an increase in runtime, the opposite is observed \citep{dao2022flashattention}. 
Despite the need for additional matrix multiplications, the reduction in total HBM reads and writes more than offsets the extra FLOPs, leading to improved performance overall.

\paragraph{Sparsity-aware implementation.} 
The key challenge of $\alpha$-entmax attention lies in finding the threshold $\bm{\tau}$, which requires multiple evaluations of the function $f(\bm{\tau})$, which, in turn, depends on the score matrix $\bm{S}$. 
While our proposed Halley-bisection algorithm alleviates the number of iterations needed to recompute $\bm{S}_{i}^{(j)}$ by providing a faster empirical convergence,
our current implementation still iterates over all blocks of $\bm{S}$, including \textbf{null blocks}---blocks where the corresponding entries of the sparse attention matrix $\bm{P}$ are zero.\looseness=-1


Furthermore, empirical evidence from \citet{jiang2024minference} and \cite{xiao2024efficient} suggests that for long inputs (e.g., 128k tokens in LLaMa-3-8b), approximately 3\% of the entries in $\bm{P}$ suffice to capture over 96\% of the total attention, which motivates an approach to leverage the adaptive and unstructured sparsity of $\alpha$-entmax attention weights. 
To this end, we propose to only compute necessary blocks of $\bm{P}$ by skipping the null blocks. 
Concretely, let $\mathcal{I}(i)$ denote the set of all indices $i'$ such that $\lfloor i' / T_r \rfloor = i$, and $\mathcal{J}(j)$ denote the set of all indices $j'$ such that $\lfloor j' / T_c \rfloor = j$. We construct a \textbf{block mask}  matrix $\bm{M} \in \{0, 1\}^{T_r \times T_c}$ as follows:
\begin{equation}\label{eq:block_mask}
M_{ij} =
\begin{cases}
1 &  \text{if $\exists_{i' \in \mathcal{I}(i), j' \in \mathcal{J}(j)}: S_{i',j'} > \tau_{i'}$}, \\
0 &  \text{otherwise},
\end{cases}
\end{equation}
Importantly, $\bm{M}$ is created dynamically after a small predefined number of Halley-bisection iterations. 

While the introduction of $\bm{M}$ breaks the linear memory complexity of dense fused-attention by requiring $T_r \times T_c$ extra memory, the overhead is still manageable as it only contains binary values and is substantially smaller than the full $\bm{P} \in \mathbb{R}^{n \times n}$ matrix. Furthermore, $\bm{M}$ needs to be materialized only once and its memory can be shared across all attention layers. 
To leverage $\bm{M}$ in practice, we propose to create two \textbf{pointer-increment lookup tables}:
\begin{enumerate}
\item $\mathcal{K}_j = \{i \mid M_{ij} = 1\}$: A table containing the row indices $i$ of $\bm{M}$ that lead to 
non-null blocks in $\bm{P}_{i}^{(j)}$.
\item $\mathcal{Q}_i = \{j \mid M_{ij} = 1\}$: A table containing the column indices $j$ of $\bm{M}$ that lead to non-null blocks in $\bm{P}_{i}^{(j)}$.
\end{enumerate}

These tables enable efficient skipping of $\bm{K}$ and $\bm{V}$ blocks that do not contribute to the final attention output $\bm{O}$, significantly reducing unnecessary computations. Moreover, the same mechanism can be extended to accelerate the backward pass, where gradients with respect to $\bm{Q}$, $\bm{K}$, and $\bm{V}$ are computed, which we describe next.









\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figs/runtime_large_no_x_new.pdf}
\vskip -0.3cm
\caption{Efficiency of algorithms for computing non-causal attention in terms of the average training step time for increasingly longer sequence lengths. 
We use $\alpha = 1.5$ for $\alpha$-entmax based methods (Bisection and \methodname). 
}
\label{fig:runtimes_comparison}
\end{figure*}

\subsubsection{Backward Pass}\label{sec:bwd-pass}
In FlashAttention~\citep{dao2022flashattention}, the backward pass is executed using a single kernel that parallelizes computation across batch, head, and sequence dimensions. 
However, following Triton's  official implementation of FlashAttention,\footnote{\url{https://github.com/triton-lang/triton/blob/main/python/tutorials/06-fused-attention.py}}
we separate the backward pass into two kernels: one for $\bm{dQ}$ (the gradient w.r.t. $\bm{Q}$) and another for $\bm{dK}$ and $\bm{dV}$ (the gradients w.r.t. $\bm{K}$ and $\bm{V}$). 

\paragraph{Sparse Jacobian of $\alpha$-entmax.} 

The sparsity in the Jacobian of $\alpha$-entmax plays a crucial role in the backward pass. For $\bm{p} = \alpha\text{-entmax}(\bm{s})$, the Jacobian is~\citep{peters-etal-2019-sparse}
\begin{equation}
\frac{\partial \alpha\text{-entmax}(\bm{s})}{\partial \bm{s}} = \text{Diag}(\bm{u}) - \frac{\bm{u} \bm{u}^\top}{\|\bm{u}\|_1},
\end{equation}
where $u_j = (p_j)^{2-\alpha}$.
Importantly, this Jacobian is sparse and only depends on $\bm{p}$, which, in turn, is a function of $\tau$ computed during the forward pass.
We denote by $\bm{U} \in \mathbb{R}^{n \times n}$ the matrix defined element-wise 
as $U_{lk} = P_{lk}^{2-\alpha}$, and by $\bm{U}_i^{(j)} \in \mathbb{R}^{B_r \times B_c}$ its $(i,j)\textsuperscript{th}$ block. 
Using this information, the gradient w.r.t. the score matrix $\bm{S}_{i}^{(j)} \in \mathbb{R}^{B_r \times B_c}$ can be efficiently computed as: 
\begin{equation}
\bm{dS}_{i}^{(j)} = \bm{U}_{i}^{(j)} \odot \bm{dP}_{i}^{(j)} - \text{Diag}(\bm{\delta}_i) \bm{U}_{i}^{(j)},
\end{equation}
where $\bm{dP}_i^{(j)} = \bm{dO}_i \bm{V}_j^\top \in \mathbb{R}^{B_r \times B_c}$, 
with $\bm{dO}_i \in \mathbb{R}^{B_r \times n}$ and $\bm{V}_j \in \mathbb{R}^{B_c \times n}$, 
and $\bm{\delta}_i \in \mathbb{R}^{B_r}$ denotes the $i\textsuperscript{th}$ block of the vector $\bm{\delta} \in \mathbb{R}^{n}$ defined element-wise as $\delta_l = (\sum_{k}U_{lk}dP_{lk})/(\sum_{k}U_{lk})$. 

\paragraph{Efficient gradient computation.}
In \methodname, instead of storing $\bm{P}$, we store the lookup tables $\mathcal{K}$ and $\mathcal{Q}$ 
computed during the forward pass, allowing us to 
to efficiently skip the computations of null blocks during backpropagation.
Given $\bm{dS}_i$, the gradients for $\bm{Q}_i$, $\bm{K}_i$, $\bm{V}_i \in \mathbb{R}^{B_r \times d}$ are computed as follows using the pointer-increment lookup tables:
\begin{align}
\bm{dQ}_i &= \sum_{j \in \mathcal{Q}_i} \bm{dS}_{i}^{(j)} \cdot \bm{K}_j, \\
\bm{dK}_j &= \sum_{i \in \mathcal{K}_j} \bm{dS}_{i}^{(j)} \cdot \bm{Q}_i, \\
\bm{dV}_j &= \sum_{i \in \mathcal{K}_j} \bm{P}_{i}^{(j)} \cdot \bm{dO}_i.\label{eq:derivatives_with_lookup_tables}
\end{align}
Hence, by splitting the backward pass into separate kernels and exploiting the sparsity of $\alpha$-entmax through the Jacobian structure, we can achieve efficient gradient computation. 
Overall, \methodname allows users to choose between memory efficiency (without block masking) and computational speed (with block masking) depending on the task requirements and hardware constraints.
We provide a detailed derivation of $\alpha$-entmax attention's backward pass and its implementation in Appendix \ref{app:backpass-derivation}.







\section{Experiments} 
\label{sec:experiments}

We evaluate \methodname across various scenarios to show its computational efficiency and impact on downstream tasks. Our experiments address the following questions:
\begin{itemize}
    \item Performance efficiency: How does \methodname compare with baseline methods in terms of runtime 
    as sequence length and sparsity vary?

    \item Generalization to architectures: How does \methodname perform when integrated with encoder-only and decoder-only models?

    \item Effectiveness in finetuning: Can \methodname-pretrained models outperform or match their dense counterparts in short and long-context tasks?
\end{itemize}


\subsection{Efficiency Benchmark}
\label{subsec:efficiency_benchmark}

We compare the efficiency of \methodname against 
FlashAttention-2 and naive implementations of $\alpha$-entmax. 
For a fair comparison, we also include a variant of FlashAttention-2 implemented in Triton that follows closely our kernel implementation of \methodname.
We set the number of iterations of \methodname to 3 and Bisection to 10.
As input, we generate random tensors from a Gaussian distribution ($\mu = 0$), simulating attention scores with a high level of sparsity by setting the Gaussian variance to $\sigma^2 = 6$ of query vectors.
Sequence lengths range from 1k to 64k, with a fixed head size of $d=64$. 
\paragraph{Runtime.} 
We show the average training step time for each method in Figure~\ref{fig:runtimes_comparison}. 
\methodname demonstrates superior scalability, efficiently handling sequences up to 64k, unlike the Bisection method implemented in Torch, which runs out of memory beyond 4k context length. 
We also note that, as context length increases, the amount of block sparsity  naturally increases as well, leading to an advantage for our method over both implementations of FlashAttention-2. 



\subsection{Performance on Real Tasks}


Encoder-only models, such as RoBERTa~\citep{liu2019robertarobustlyoptimizedbert} and ModernBERT~\citep{warner2024smarter}, exhibit higher attention sparsity than decoder-only models, making them well-suited for adaptive sparse attention mechanisms like \methodname. 
Following ModernBERT's evaluation setup, we opt to evaluate these models on standard 
NLP tasks, such as text classification, natural language inference, textual similarity, and information retrieval.
Moreover, following FlashAttention’s evaluation setup~\citep{dao2022flashattention}, we also benchmark \methodname with GPT-2, a decoder-only model, to assess its efficiency in autoregressive settings where attention patterns are denser. 
This ensures a comprehensive comparison with optimized softmax-based methods while validating the benefits of sparsity across different architectures.
We provide more training and evaluation details for each task in Appendix~\ref{sec:app_experimental_setup}. 



\paragraph{Continuous pretraining.} 
We conducted continuous pretraining of RoBERTa-base and ModernBERT-base on 2B tokens of the English subset of Fineweb-edu~\citep{lozhkov2024fineweb-edu} using \methodname for $\alpha \in \{1.5, 2\}$, and PyTorch's \texttt{scaled\_dot\_product\_attention} for $\alpha=1.0$. 
To ensure a smooth transition from dense to sparse attention, we linearly increased $\alpha$ from $\alpha=1.0$ to the target values $\alpha \in \{1.5, 2.0\}$ over the first 1B tokens and kept it fixed afterwards. 
We provide more details on the continuous pretraining phase in Appendix~\ref{subsec:app_continuous_pretraining}, including efficiency results.









\begin{table}[t]
    \small
    \centering
    \caption{Results for single-vector retrieval models on different tasks from the BEIR benchmark in terms of nDCG@10.
    } 
    \label{tab:results_retrieval}
    \vskip 0.1in
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{l r cccc}
    \toprule
    Model & Seq. 
    & SciFact & NFC & FiQA & TREC-C \\
    \midrule
    RoBERTa                     & 512    & 51.7 & 23.1 & 27.8 & 60.1  \\
    RoBERTa ($\alpha = 1.5$)    & 512    & 50.8 & 24.2 & 27.6 & 71.0  \\
    RoBERTa ($\alpha = 2.0$)    & 512    & 52.2 & 23.8 & 25.7 & 65.5  \\
    ModernBERT                  & 8192   & 57.7 & 22.4 & 25.7 & 67.6 \\
    ModernBERT ($\alpha = 1.5$) & 8192   & \bf 58.4 & \bf 25.7 & \bf 29.6 & \bf 75.2 \\
    ModernBERT ($\alpha = 2.0$) & 8192   & 58.0 & 25.4 & 29.3 & 71.1 \\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph{Single-vector retrieval.} 
We evaluate our pretrained models on single-vector retrieval performance using the BEIR benchmark (SciFact, NFCorpus, FiQA2018, TREC-COVID), following the setup in \citep{warner2024smarter}.
Table~\ref{tab:results_retrieval} highlights the performance of RoBERTa and ModernBERT models using $\alpha$-entmax attention in terms of the standard nDCG@10 metric. 
ModernBERT with $\alpha = 1.5$ consistently outperformed its dense counterpart, achieving the highest scores on all tasks, demonstrating its ability to focus on relevant signals effectively. While ModernBERT with $\alpha = 2.0$ remained competitive, its higher sparsity might have excluded relevant information, affecting task performance.
Finally, sparse versions of ModernBERT achieve better results than the sparse versions of RoBERTa on all tasks, highlighting the benefit of modeling long contexts.



\begin{table}[t]
    \small
    \centering
    \caption{Long document classification performance ($F_1$ micro) with softmax and $\alpha$-entmax attention.}
    \label{tab:entmax_softmax_comparison}
    \vskip 0.1in
    \begin{tabular}{lcccccc}
    \toprule
    & \multicolumn{5}{c}{Sequence Length} \\
    \cmidrule(lr){2-6}
    Model & 512 & 1024 & 2048 & 4096 & 8192 \\
    \midrule
    RoBERTa  & 71.5          & 74.4          & 75.1          &  77.9 & \bf 79.2 \\
    RoBERTa ($\alpha=1.5$)   & \bf 71.8 & \bf 75.5 & \bf 76.4 & \bf 78.0 & 78.6 \\
    \bottomrule
    \end{tabular}
\end{table}







\paragraph{Long document classification.} 

We fine-tuned a pretrained RoBERTa model \citep{liu2019robertarobustlyoptimizedbert} on the ECtHR \citep{chalkidis-etal-2019-neural-ecthr1, chalkidis-etal-2021-paragraph-ecthr2} dataset while progressively increasing the sequence length up to 8192 tokens. 
Positional embeddings were extended by repetition, following the approach of \citet{beltagy_longformer_2020}. 
As a baseline, we fine-tuned the model using standard softmax-based attention. 
For $\alpha$-entmax attention, we linearly increased the $\alpha$ from 1.0 to 1.5 during training to ensure smooth convergence.
The results, summarized in Table~\ref{tab:entmax_softmax_comparison}, show a consistent improvement in model performance with longer context lengths. Notably, despite the base model being pretrained with standard attention, $\alpha$-entmax attention was capable of effectively learning the task, achieving a slightly higher micro $F_1$ score than the model fine-tuned with standard attention up to a sequence length of 4096 tokens.



\paragraph{Language understanding.} 
We also evaluate RoBERTa and ModernBERT models with $\alpha$-entmax attention on the GLUE benchmark~\citep{wang-etal-2018-glue} in Appendix~\ref{subsec:app_glue_and_bier}.
Overall, the results indicate that models with sparse attention achieve comparable performance to their dense counterparts, which underscores the ability to efficiently train $\alpha$-entmax models without sacrificing accuracy. 






\begin{table}[t]
    \small
    \centering
    \caption{Results on language modeling with GPT-2 in terms of final validation loss and accuracy on the HellaSwag task~\citep{zellers2019hellaswag}, along with the average runtime per training step (in seconds) and peak memory usage (GB) per GPU.}
    \label{tab:gpt2_hellaswag}
    \vskip 0.1in
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{lcccc}
    \toprule
    Model               & Val. Loss & HS Acc.  & Runtime & Memory \\
    \midrule
    GPT-2                & 3.283     & 30.4      & \bf 0.98   & \bf 52.5       \\
    GPT-2 ($\alpha=1.5$) & \bf 3.263 & \bf 30.6  & 1.03      & \bf 52.5  \\
    \ \ w/ Torch sorting & - & - & 3.61 &  73.8\\
    \ \ w/ Torch bisection & - & - & 7.78 & 77.6\\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph{Language modeling.} 
Following \citep{dao2022flashattention}, we trained a small 124M GPT-2 model~\citep{radford2019language-gpt2} from scratch on 10B tokens of the FineWeb dataset~\citep{penedo2024the-fineweb} with a context length of 1024 tokens. 
For a consistent evaluation between softmax and $\alpha$-entmax attention, we also trained a softmax-based GPT-2 to serve as baseline. 
After training, we evaluated both models on the HellaSwag task~\citep{zellers2019hellaswag}. 
Table \ref{tab:gpt2_hellaswag} presents a side-by-side comparison of the final validation loss and accuracy on HellaSwag, along with runtime and memory usage numbers. 
Sparse GPT-2 achieves a slight improvement in validation loss (3.263 vs. 3.283) and final accuracy (30.6\% vs. 30.4\%) compared to its softmax counterpart, while obtaining comparable runtime and memory efforts.
Furthermore, our approach achieves a runtime comparable to the GPT-2 using the highly optimized FA2 (1.03 s/step vs. 0.98 s/step) and matches its memory footprint (52.5 GB), while outperforming the sorting and bisection variants by large margins in both speed (1.03 s/step vs. 3.61 and 7.78 s/step) and memory usage (52.5 GB vs. 73.8 and 77.6 GB).
In Appendix~\ref{subsec:app_language_modeling}, we report all training and evaluation details, including the validation loss curves of each method.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{figs/sparsity_heatmap_gpt2.pdf}
    \caption{Ratio of non-zero attention scores for GPT-2 ($\alpha = 1.5$).}
    \label{fig:sparsity-gpt2}
\end{figure}























\paragraph{Sparsity in attention heads.} 
Figure~\ref{fig:sparsity-gpt2} presents the sparsity observed in attention heads for all layers for an input of 1024 tokens for our sparse GPT-2 model ($\alpha = 1.5$).
Except for the first layer, all subsequent layers exhibit a high degree of sparsity, highlighting the potential efficiency gains from leveraging this property. 
Moreover, in Figure~\ref{fig:heatmaps_mlm} (Appendix~\ref{subsec:app_continuous_pretraining}), we illustrate the sparsity patterns in ModernBERT-base attention heads for $\alpha \in \{1.5, 2.0\}$, reinforcing similar conclusions.







\section{Related Works}


\paragraph{Sparse Probability Transformations.} 
The sparsity inherent to the $\alpha$-entmax transformation, as demonstrated by \citet{blondel-entmax}, is directly controlled by the $\alpha$ parameter. 
For $\alpha = 2$, the problem simplifies to a projection onto the probability simplex, a well-established optimization problem. Its solution forms the base of sparsemax \citep{martins2016softmax}, which can be efficiently computed using sorting and root-finding methods \citep{sort-and-scan,condats,ibis2009efficient}. 
Moreover, for intermediate values such as $\alpha=1.5$, \citet{peters-etal-2019-sparse} proposed an exact sorting-based algorithm along with an implementation of a bisection algorithm applicable to any $\alpha$.
However, these approaches remain suboptimal for long contexts due to slow convergence or reliance on complex data structures and sorting operations, which are difficult to optimize for hardware.\looseness=-1

\paragraph{Sparse Attention Mechanisms.} Efficient sparse attention mechanisms have been widely studied to reduce the quadratic cost of transformers. The Sparse Transformer \citep{child_generating_2019} introduces a fixed windowed attention that can be efficiently computed using CUDA kernels, a strategy also adopted by Longformer \citep{Beltagy2020Longformer}. However, data-dependent sparse attention methods, such as Reformer \citep{kitaev2020reformer} and Routing Transformer \citep{roy2021efficient}, still rely on softmax and do not leverage the sparsity of attention weights.
Adaptively sparse transformers \citep{correia-etal-2019-adaptively} offer an alternative approach where attention heads can learn $\alpha$ dynamically, improving interpretability but without leveraging sparsity for efficiency. SparseFinder \citep{treviso-etal-2022-predicting} aims to address this issue by predicting the sparsity pattern of entmax attention a priori; however, it does not scale efficiently for long sequences.

\paragraph{Hardware-Aware Attention.} Recent works have explored optimizing attention mechanisms with hardware-aware implementations. Flex Attention \citep{dong2024flexattentionprogrammingmodel} provides an API for efficient attention computation, though they remain tied to softmax-based transformations and do not support more complex operations such as those considered in our work. 
Closely related to our approach, FlashAttention-1 and 2~\citep{dao2022flashattention,dao2023flashattention2} optimize softmax-based attention using tiling and recomputation techniques implemented in CUDA. While FlashAttention-1 includes a sparse block variant, its sparsity pattern must be predefined, limiting adaptability. In this work, we compare our method, \methodname, with FlashAttention-2 and demonstrate that our approach can outperform both its CUDA and Triton implementations at high input sparsity levels.
Similarly, Sparse Flash Attention \citep{pagliardini2023fast} extends FlashAttention-1 with a sparse variant that reduces computational cost by either dropping queries and keys per head or grouping them using a hash-based bucketing approach.
However, despite its efficiency improvements, it relies on slow sorting operations and is constrained to causal attention, making its sparsity a by-product of bucketing rather than an inherently adaptive feature, as in our case.\looseness=-1

\paragraph{Efficiency at Inference Time.} Another line of work focuses on optimizing transformers at inference time. Methods such as Paged Attention \citep{kwon2023efficient} and KV cache sparsification~\citep{devoto-etal-2024-simple,luohe2024keep} aim to alleviate the linear complexity of inference by modifying key-value caching strategies. While our approach does not directly provide KV cache compression benefits, these methods are orthogonal and can be combined with our work to further improve inference efficiency.





\section{Conclusion}


In this work, we introduced \methodname, a hardware-aware and efficient implementation of $\alpha$-entmax attention, bridging the gap between adaptive sparse activations and efficient long-context modeling. 
Our approach leverages a hybrid Halley-bisection algorithm for faster empirical convergence and custom Triton kernels to exploit the inherent sparsity of $\alpha$-entmax.
Our experiments show that \methodname not only achieves substantial computational improvements over existing $\alpha$-entmax implementations, but can often match or even surpass the efficiency of highly optimized softmax-based attention algorithms like FlashAttention-2. 
Moreover, \methodname enables long-context training while maintaining strong task performance across diverse benchmarks, such as language understanding, information retrieval, document classification,  and language modeling.
Overall, our work unlocks the viability of dynamically sparse attention mechanisms in large-scale training, which was previously hindered by computational inefficiencies. 








\section*{Impact Statement}

Efficient attention mechanisms are crucial for scaling transformers to long-context tasks. 
Our work provides a practical implementation by making adaptive sparse attention efficient, overcoming previous computational limitations of $\alpha$-entmax. 
Therefore, the improved efficiency of \methodname has potential applications in large-scale NLP, where sparsity can be leveraged to reduce computational costs. 
We do not foresee direct societal consequences from sparsity itself, but its integration into decision-making models may still reflect biases in training data. 
As such, we encourage careful evaluation when deploying sparse attention mechanisms in high-stakes applications, ensuring that efficiency gains do not come at the cost of fairness or transparency.


\section*{Acknowledgments}

We thank António Farinhas, Ben Peters, Duarte Alves, Giuseppe Attanasio, Saul Santos, Edoardo Ponti, and Vlad Niculae for their helpful and constructive feedback. 
This work was supported by EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI), and by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicações.







\bibliography{main}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Algorithm Details}
We first derive a high-level view of the forward and backward passes of the entmax attention and then present the full algorithms for both mentioned versions. 
For consistency and ease of comparison, we follow the notation adopted by FlashAttention-1 \citep{dao2022flashattention}.

\subsection{$\alpha$-entmax Attention Forward Pass}\label{app:forward-derivation}
We recall that given the input sequences $\bm{Q}, \bm{K}, \bm{V} \in \mathbb{R}^{n\times d}$, we want to compute the attention output $\bm{O} \in \mathbb{R}^{n\times d}$ as follows:
$$
\bm{S} = \bm{QK^\top} \in \mathbb{R}^{n\times n},\,\, \bm{P} = \alpha\text{-entmax}(\bm{S}) \in \mathbb{R}^{n\times n},\,\, \bm{O}= \bm{P}\bm{V} \in \mathbb{R}^{n\times d}
$$
Therefore all we need is the $\bm{\tau} \in \mathbb{R}^{n}$ that solves Equation \ref{eq:solution_entmax}, for which we can use Algorithm \ref{alg:entmax_halley}. 
We note that we do not need to materialize $\bm{S}$ as we only need to accumulate the derivatives of $f(\bm{\tau})$, defined in Equation~\ref{eq:rootfind-func}. Once $\bm{\tau}$ is computed, we can compute each row of $\bm{O}$ as follows:

\begin{equation}\label{getting-oi}
\bm{O}_i = \bm{P}_{i}\bm{V} = \sum_j P_{ij}\bm{V}_j = \sum_{j=1}^n \max\left(0, (\alpha - 1) \bm{Q}_i^\top \bm{K}_j - \tau_i \right)^{\nicefrac{1}{\alpha-1}} \bm{V}_j
\end{equation}

As in FlashAttention, we can compute $\bm{O}_i$ without extra memory by  incrementally summing the contributions of each $\alpha\text{-entmax}(\bm{Q}_i^\top \bm{K}_j)\bm{V}_j$ term. We can then compute the forward pass with $\bigO{n}$ extra memory as follows:
\begin{enumerate}
    \item Compute $\tau_i$ for all $1 \leq i \leq n$ according to Algorithm \ref{alg:entmax_halley}, which takes $\bigO{n}$ extra memory.
    \item Compute $\bm{O}_i$ for all $1 \leq i \leq n$ according to Equation \ref{getting-oi} which takes $\bigO{n}$ extra memory.
\end{enumerate}


\subsection{$\alpha$-entmax Attention Backward Pass}\label{app:backpass-derivation}

For the $\alpha$-entmax attention backward pass, we need to compute the gradients with respect to $\bm{V}$, $\bm{K}$, and $\bm{Q}$. 
Let $\mathcal{L}$ be a scalar loss function, and $\bm{dO} \in \mathbb{R}^{n \times d}$ denote $\frac{\partial \mathcal{L}}{\partial\bm{O}}$. 
Our goal is to compute the input gradients $\bm{dV}, \bm{dK}, \bm{dQ} \in \mathbb{R}^{n \times d}$.

\subsection*{1. Gradient of $\bm{V}$}
Using reverse-mode autodifferentiation, we first compute $\bm{dV}$:
\begin{equation}
\bm{dV} = \bm{P}^\top \bm{dO},
\end{equation}
where $\bm{P} = \alpha\text{-entmax}(\bm{S})$ is the output of the $\alpha$-entmax transformation applied row-wise to the score matrix $\bm{S} = \bm{Q}\bm{K}^\top$. 
Expressed element-wise, we obtain:
\begin{equation}\label{eq:getting-dvj}
\bm{dV}_j = \sum_{i=1}^n P_{ij} \bm{dO}_i,
\end{equation}
which is analogous to the softmax case. Since $P_{ij}$ is sparse due to the nature of $\alpha$-entmax, we can skip $\bm{Q}_i$ blocks that leads to blocks of $\bm{P}$ full of zeros using the pointer increment tables, as shown in Equation~\ref{eq:derivatives_with_lookup_tables}. 

\subsection*{2. Gradient of $\bm{P}$ and $\bm{S}$}
The next step involves computing $\bm{dP}$ and $\bm{dS}$. From $\bm{O}=\bm{P}\bm{V}$, we have:
\begin{equation}
dP_{ij} = \bm{dO}_i^\top \bm{V}_j.    
\end{equation}

Next, let us recall the Jacobian of the $\alpha$-entmax mapping \citep{peters-etal-2019-sparse}. Defining $\bm{p} = \alpha\text{-entmax}(\bm{s})$, the Jacobian is: 

\begin{equation}
\frac{\partial \alpha\text{-entmax}(\bm{s})}{\partial \bm{s}} = \text{Diag}(\bm{u}) - \frac{\bm{u}\bm{u}^\top}{\|\bm{u}\|_1},
\end{equation}
where $\bm{u}$ is defined element-wise as:
\begin{equation}
u_k =
\begin{cases} 
(p_k)^{2 - \alpha}, & \text{if } p_k > 0 \\
0, & \text{otherwise.}
\end{cases}
\end{equation}


Let $\bm{U}$ denote a stack of $[\bm{u}_1, ..., \bm{u}_n]$ for each row of $\bm{P}$. From the relationship $\bm{P} = \alpha\text{-entmax}(\bm{S})$, and the Jacobian of the $\alpha$-entmax function, we can propagate the gradients back to $\bm{S}$ as follows:
\begin{align}
\bm{dS}_i &= \left[ \text{Diag}(\bm{U}_i) - \frac{\bm{U}_i \bm{U}_i^\top}{\|\bm{U}_i\|_1} \right] \bm{dP}_i \\
    &= \bm{U}_i \odot \bm{dP}_i - \left( \frac{\bm{U}_i^\top \bm{dP}_i}{\|\bm{U}_i\|_1} \right) \bm{U}_i.
\end{align}


We can further simplify by defining a new quantity $\bm{\delta} \in \mathbb{R}^n$:


\begin{align}\label{eq:getting_di}
\delta_i &= \frac{\bm{U}_i^\top \bm{dP}_i}{\|\bm{U}_i\|_1} \\
&= \frac{1}{\|\bm{U}_i\|_1} \sum_{j=1}^n U_{ij} \left( \bm{dO}_i^\top \bm{v}_j \right) \\ 
&= \bm{dO}_i^\top \underbrace{\frac{\left( \sum_{j=1}^n U_{ij} \bm{V}_j \right)}{\|\bm{U}_i\|_1}}_{\bm{O}^{(2)}_i} \label{eq:getting_do2}
\end{align}

In standard softmax attention, instead of the right-side term in the above product, we would simply obtain $\bm{O}_i$.  
Since this new quantity is required for the backward pass, and to avoid passing once more through $\bm{Q}$, $\bm{K}$ and $\bm{V}$, we compute and store this quantity during the forward pass solely during training. 
Unlike in softmax attention, however, the backward pass for $\alpha$-entmax does not require saving the output matrix $\bm{O}$; instead, we only require this new quantity, which we label $\bm{O}^{(2)}$.
Then, we can simplify the computation of $\bm{dS}$ to:
\begin{align}
\bm{dS}_{i} &= \bm{U}_{i} \odot \left( \bm{dP}_{i} - \delta_i \right)
\end{align}

Again, we can use the sparsity stored in $\bm{M}$ (see Equation~\ref{eq:block_mask}) from the forward pass to efficiently skip the computation of null blocks of $\bm{P}$. 

\subsection*{3. Gradients of $Q$ and $K$}
Using the definition of $S_{ij} = \bm{Q}_i^\top \bm{K}_j$, the gradients for $Q$ and $K$ are:
\begin{align}
\bm{dQ}_i &= \sum_{j=1}^n dS_{ij} \bm{K}_j, \\
\bm{dK}_j &= \sum_{i=1}^n dS_{ij} \bm{Q}_i.
\end{align}

Substituting $dS_{ij}$, we get:
\begin{align}
\bm{dQ}_i = \sum_{j=1}^n U_{ij} \left( dP_{ij} - \delta_i \right) \bm{K}_j    \label{eq:getting-dqi} \\
\bm{dK}_j = \sum_{i=1}^n U_{ij} \left( dP_{ij} - \delta_i \right) \bm{Q}_i \label{eq:getting-dkj}
\end{align}

Effectively, we can only iterate through the blocks that will result in $P_{ij} \neq 0$. As in FlashAttention, the backward pass can also be computed with $\bigO{n}$ extra memory:
\begin{enumerate}
    \item Compute  $\bm{dV}_j$ for all $j$ according to Equation~\ref{eq:getting-dvj}, which takes $\bigO{d}$ extra memory.
    
    \item Compute $\delta_i$ for all $i$ according to Equation~\ref{eq:getting_di}, which takes $\bigO{n}$ extra memory.

    \item Compute $\bm{O}^{(2)}_i$ for all $i$, as defined in Equation~\ref{eq:getting_do2}, which takes $\bigO{d}$ extra memory.
    
    \item Compute $\bm{dQ}_i$ for all $i$ according to Equation~\ref{eq:getting-dqi}, which takes $\bigO{d}$ extra memory.
    
    \item Compute $\bm{dK}_j$ for all $j$ according to Equation~\ref{eq:getting-dkj}, which takes $\bigO{d}$ extra memory.
    
\end{enumerate}

We note that the only extra memory requirement compared to FlashAttention is in having to additionally compute and storing $\bm{O}^{(2)} \in \mathbb{R}^{n \times d}$. When using block masking, we also need $\bigO{T_r \times T_c}$ extra memory to store the binary mask $\bm{M}$. However, we recall that this memory can be shared across attention layers, as it is merely a temporary matrix used to compute the pointer-increment tables.


\subsection{\methodname: Forward Pass (without block masking)}

The full \methodname's forward pass is presented in Algorithm \ref{alg:entattention}. 
For completeness, we also provide in Algorithm \ref{alg:block-tau} the steps for approximating $\bm{\tau}$ without the need to materialize $\bm{S}$ in a block-wise manner.

\begin{algorithm}[h]
   \caption{Halley-bisection for computing $\bm{\tau}$ -- Block Version}
   \label{alg:block-tau}
\begin{algorithmic}[1]
   \REQUIRE Matrices $\bm{Q}, \bm{K} \in \bm{R}^{n \times d}$ in HBM, block sizes $B_c, B_r$ and number of iterations $M$.
   
   \STATE Divide $\bm{Q}$ into $T_r = \lceil n / B_r \rceil$ blocks $\bm{Q}_1, \dots, \bm{Q}_{T_r}$ of size $B_r \times d$
   \STATE Divide $\bm{K}$ into $T_c = \lceil n / B_c \rceil$ blocks $\bm{K}_1, \dots, \bm{K}_{T_c}$ of size $B_c \times d$
   \STATE Divide $\bm{\tau}$ into $T_r$ blocks $\bm{\tau}_1, \dots, \bm{\tau}_{T_r}$ of size $B_r$

   
   \FOR{$i = 1$ to $T_r$}
       \STATE Load $\bm{Q}_i$ from HBM to on-chip SRAM
       
       \STATE On chip, initialize $\bm{\tau}_i$, $\bm{\tau}_{\text{lo}_i}$, $\bm{\tau}_{\text{hi}_i}$ according to Algorithm \ref{alg:entmax_halley}. \hfill \(\triangleright\) Note: this requires one pass over $\bm{K}_j$ for all $j$.

       \REPEAT
            \STATE On chip, initialize $f, f^{'}, f^{''} = \bm{0} \in \mathbb{R}^{B_r}$  
           \FOR{$j = 1$ to $T_c$}
               \STATE Load $\bm{K}_j$, $\bm{V}_j$ from HBM to on-chip SRAM
               \STATE Compute $\bm{S}_i^{(j)} = \bm{Q}_i \bm{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$
               \STATE Accumulate $f, f^{'}, f^{''}$ according to Equations \ref{eq:rootfind-func}, \ref{eq:derivatives_f1} and \ref{eq:derivatives_f2}, respectively.
           \ENDFOR
           \STATE Update $\boldsymbol{\tau}_i, \boldsymbol{\tau}_{\text{lo}_i}$, $\boldsymbol{\tau}_{\text{hi}_i}$ according to Algorithm \ref{alg:entmax_halley}.
        \UNTIL{$M$ iterations are completed}
       \STATE Write $\boldsymbol{\tau}_i$ to HBM
   \ENDFOR
   \STATE {\bfseries Return:} $\boldsymbol{\tau}$
\end{algorithmic}
\end{algorithm}

\subsection{\methodname: Backward Pass (without block masking)}

As mentioned in \S\ref{sec:bwd-pass}, in contrast to FlashAttention, we propose to separate the kernels that compute the gradients $\bm{dQ}, \bm{dK}, \bm{dV}$. 
However, as in FlashAttention, we need to compute $\bm{\delta}$ before being able to compute the gradients, which we do in a separate kernel following Equation~\ref{eq:getting_do2}.
We present the full steps for computing $\bm{dK}$ and $\bm{dV}$ in Algorithm~\ref{alg:entattention-backward-dkdv}, and for computing $\bm{dQ}$ in Algorithm~\ref{alg:entattention-backward-dq}.


\begin{algorithm}[tb]
\caption{\methodname Backward Pass for $\bm{dK}$ and $\bm{dV}$}
\label{alg:entattention-backward-dkdv}
\begin{algorithmic}[1]
\REQUIRE Matrices $\bm{Q}, \bm{K}, \bm{V}, \bm{O}, \bm{dO} \in \mathbb{R}^{n \times d}$ in HBM, vector $\bm{\tau} \in \mathbb{R}^n$ in HBM, block sizes $B_c, B_r$, parameter $\alpha$
\STATE Divide $\bm{Q}$ into $T_r = \lceil n / B_r \rceil$ blocks $\bm{Q}_1, \dots, \bm{Q}_{T_r}$ of size $B_r \times d$ each, and divide $\bm{K}, \bm{V}$ into $T_c = \lceil n / B_c \rceil$ blocks $\bm{K}_1, \dots, \bm{K}_{T_c}$, $\bm{V}_1, \dots, \bm{V}_{T_c}$ of size $B_c \times d$ each.
\STATE Divide  $\bm{dO}$ into $T_r$ blocks $\bm{dO}_1, \dots, \bm{dO}_{T_r}$ of size $B_r \times d$ each.
\STATE Divide $\bm{\tau}$ into $T_r$ blocks $\bm{\tau}_1, \dots, \bm{\tau}_{T_r}$ of size $B_r$ each.
\STATE Initialize and divide $\bm{dK}, \bm{dV} \in \mathbb{R}^{n \times d}$ into $T_c$ blocks $\bm{dK}_1, \dots, \bm{dK}_{T_c}$ and $\bm{dV}_1, \dots, \bm{dV}_{T_c}$ of size $B_c \times d$ each.
\STATE Divide $\boldsymbol{\delta}$ into $T_r$ blocks $\boldsymbol{\delta}_1, \dots, \boldsymbol{\delta}_{T_r}$ of size $B_r$ each.
\FOR{$1 \leq j \leq T_c$}
    \STATE Load $\bm{K}_j, \bm{V}_j$ from HBM to on-chip SRAM.
    \STATE Initialize $\bm{dK}_j = \bm{0}_{B_c \times d}$ on SRAM.
     \STATE Initialize $\bm{dV}_j = \bm{0}_{B_c \times d}$ on SRAM.
    \FOR{$1 \leq i \leq T_r$}
        \STATE Load $\bm{Q}_i, \bm{dO}_i, \boldsymbol{\tau}_i, \boldsymbol{\delta}_i$ from HBM to on-chip SRAM.
        \STATE On chip, compute $\bm{S}_i^{(j)} = \bm{Q}_i \bm{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute 
        $\bm{P}_i^{(j)} = \max(0, (\alpha-1)\bm{S}_i^{(j)}-\bm{\tau}_i)^{\nicefrac{1}{\alpha-1}} \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\bm{dV}_j \leftarrow \bm{dV}_j + (\bm{P}_i^{(j)})^\top \bm{dO}_i \in \mathbb{R}^{B_c \times d}$.
        \STATE On chip, compute $\bm{dP}_i = \bm{dO}_i \bm{V}_j^\top \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\bm{U}_i^{(j)} = {\bm{P}_i^{(j)}}^{2-\alpha} \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\bm{dS}_i^{(j)} = \bm{U}_i^{(j)} \odot (\bm{dP}_i^{(j)} - \boldsymbol{\delta}_i) \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\bm{dK}_j \leftarrow \bm{dK}_j + (\bm{dS}_i^{(j)})^\top \bm{Q}_i \in \mathbb{R}^{B_c \times d}$.
    \ENDFOR
    \STATE Write $\bm{dK}_j, \bm{dV}_j$ to HBM.
\ENDFOR
\STATE {\bfseries Return:} Gradients $\bm{dK}, \bm{dV}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
   \caption{\methodname Backward Pass for $\bm{dQ}$}
   \label{alg:entattention-backward-dq}
\begin{algorithmic}[1]
   \REQUIRE Matrices $\bm{Q}, \bm{K}, \bm{V}, \bm{O}, \bm{dO} \in \mathbb{R}^{n \times d}$ in HBM, vector $\boldsymbol{\tau} \in \mathbb{R}^n$ in HBM, block sizes $B_c, B_r$, parameter $\alpha$.
   \STATE Divide $\bm{Q}$ into $T_r = \lceil n / B_r \rceil$ blocks $\bm{Q}_1, \dots, \bm{Q}_{T_r}$ of size $B_r \times d$ each, and divide $\bm{K}, \bm{V}$ into $T_c = \lceil n / B_c \rceil$ blocks $\bm{K}_1, \dots, \bm{K}_{T_c}$, $\bm{V}_1, \dots, \bm{V}_{T_c}$ of size $B_c \times d$ each.
    \STATE Divide $\bm{dO}$ into $T_r$ blocks $\bm{dO}_1, \dots, \bm{dO}_{T_r}$ of size $B_r \times d$ each. 
    \STATE Divide $\bm{\tau}$ into $T_r$ blocks $\bm{\tau}_1, \dots, \bm{\tau}_{T_r}$ of size $B_r$ each.
    \STATE Initialize $\bm{dQ}$ in HBM and divide it into $T_r$ blocks $\bm{dQ}_1, \dots, \bm{dQ}_{T_r}$ of size $B_r \times d$ each. 
    \STATE Divide $\boldsymbol{\delta}$ into $T_r$ blocks $\boldsymbol{\delta}_1, \dots, \boldsymbol{\delta}_{T_r}$ of size $B_r$ each.
   \FOR{$i = 1$ to $T_r$}
       \STATE Load $\bm{Q}_i, \bm{dO}_i, \boldsymbol{\delta}_i, \boldsymbol{\tau}_i$,  from HBM to on-chip SRAM
       \STATE Initialize $\bm{dQ}_i = \bm{0}_{B_c \times d}$ on SRAM.
       \FOR{$j = 1$ to $T_c$}
           \STATE On chip, compute $\bm{S}_i^{(j)} = \bm{Q}_i \bm{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$.
            \STATE On chip, compute $\bm{P}_i^{(j)} = \max(0, (\alpha-1)\bm{S}_i^{(j)}-\bm{\tau}_i)^{\nicefrac{1}{\alpha-1}} \in \mathbb{R}^{B_r \times B_c}$.
            \STATE On chip, compute $\bm{dP}_i = \bm{dO}_i \bm{V}_j^\top \in \mathbb{R}^{B_r \times B_c}$.
            \STATE On chip, compute $\bm{U}_i^{(j)} = {\bm{P}_i^{(j)}}^{2-\alpha} \in \mathbb{R}^{B_r \times B_c}$.
            \STATE On chip, compute $\bm{dS}_i^{(j)} = \bm{U}_i^{(j)} \odot (\bm{dP}_i^{(j)} - \boldsymbol{\delta}_i) \in \mathbb{R}^{B_r \times B_c}$.
            \STATE On chip, compute $\bm{dQ}_i \leftarrow \bm{dQ}_i + \bm{dS}_i^{(j)} \bm{K}_j \in \mathbb{R}^{B_r \times d}$.
       \ENDFOR
       \STATE Write $\bm{dQ}_i$ to HBM
   \ENDFOR
   \STATE {\bfseries Return:} Gradient $\bm{dQ}$
\end{algorithmic}
\end{algorithm}



\subsection{\methodname: Block Masked Version}
In this version, as outlined in Section~\ref{sec:method}, a boolean block mask $\bm{M} \in \mathbb{R}^{T_r \times T_c}$ is created dynamically in the forward pass, allowing the exploitation of the sparsity in the matrix $\bm{P}$ at the cost of linear memory complexity. 
The mask is populated during the final iteration of the Halley-bisection algorithm (Algorithm~\ref{alg:block-tau}) by evaluating the condition $\textsf{any}(\bm{S}_i^{(j)} > \bm{\tau}_i)$ and storing the result as a boolean value. Thus, the mask $\bm{M}$ indicates whether a specific $\bm{Q},\bm{K}$ block pair contributes to the output.
This process enables the creation of a lookup table that associates each query block with the set of key blocks that contribute non-zero values, thereby allowing to skip unnecessary computations for future computations. Similarly, a reverse lookup table can be created for each key block. Both tables can be used in the backward pass (Line 10 in Algorithm~\ref{alg:entattention-backward-dkdv} and Line 9 in Algorithm~\ref{alg:entattention-backward-dq}) to avoid looping over unnecessary query/key blocks.

In practice, to create the lookup tables, we use the \texttt{torch.argwhere} function to extract the $(i,j)$ indices of entries where $M_{ij} = 1$. 
Combined with row-wise summation of non-zero entries, this approach efficiently skips computations for irrelevant blocks within the remaining kernels. 
Consequently, during the forward pass, only the $\bm{K},\bm{V}$ pairs identified in the lookup table are loaded, avoiding redundant memory and computational overhead. 
As mentioned, for the backward pass, given that we separated the computation of $\bm{dQ}$ and $\bm{dK},\bm{dV}$, we can further use both tables ($\mathcal{Q}$ and $\mathcal{K}$) to speedup the gradient computation.





\section{Experimental Setup}
\label{sec:app_experimental_setup}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\linewidth]{figs/sparsity_heatmap_alpha1.5.pdf}
    \quad \includegraphics[width=0.3\linewidth]{figs/sparsity_heatmap_alpha2.0.pdf}
    \caption{Ratio of non-zeros for non-local layers of ModernBERT-base with $\alpha=1.5$ (left) and $\alpha=2.0$ (right).}
    \label{fig:heatmaps_mlm}
\end{figure}


\subsection{Continuous Pre-training} \label{subsec:app_continuous_pretraining}

We conducted continuous pretraining of RoBERTa-base\footnote{\url{https://huggingface.co/FacebookAI/roberta-base}} and ModernBERT-base\footnote{\url{https://huggingface.co/answerdotai/ModernBERT-base}} models with our custom sparse attention Triton kernel, \methodname. 
The pretraining process was carried on 2B tokens of the FineWeb-Edu dataset,\footnote{\url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu}} due to its high-quality, diverse and large-scale content. 
We used the HuggingFace Transformers library for model training and implementation and the Datasets library for data handling.
Concretely, we used a batch size of 32 and a learning rate of $5 \times 10^{-5}$, optimized with the AdamW optimizer. Training was conducted for 100,000 steps using mixed-precision (\texttt{fp16}). The sparsity parameter ($\alpha$) was initialized at 1.01 and annealed linearly to a final value of 1.5 or 2.0 over 50,000 steps. 
We kept ModernBERT's window attention layers untouched, only replacing the full softmax layers by $\alpha$-entmax.
Finally, we also performed continuous pretraining of RoBERTa and ModernBERT with standard softmax attention with a fixed $\alpha=1.0$.

As shown in Figure~\ref{fig:heatmaps_mlm}, the attention mechanisms of our sparse ModernBERT model ($\alpha = 1.5$) obtain high sparsity levels in practice, with an overall sparsity of 95\% for $\alpha=1.5$ and 99\% for $\alpha = 2.0$. 
For this reason, we used the version of \methodname that leverages the pointer increment tables for training ModernBERT, which has a maximum sequence length of 8,192. 
For RoBERTa, which has a sequence length of 512, we opted to use the Halley-bisection algorithm implemented in Triton. 
In Table~\ref{tab:benchmark_efficiency_mlm} we report efficiency results in terms of runtime and memory usage for different attention algorithms with ModernBERT-base. 
Overall, we observe that the sorting approach is slower than bisection, which is slower than our Halley-bisection and \methodname, in that order. 


\begin{table}[t]
    \small
    \centering
    \caption{Runtime (s) of ModernBERT-base ($\alpha=1.5$) for varying context lengths.
    } 
    \label{tab:benchmark_efficiency_mlm}
    \vskip 0.1in
    \begin{tabular}{l ccccc}
    \toprule
     & \multicolumn{5}{c}{Sequence Length} \\
    \cmidrule(lr){2-6}
    Algorithm & 512 & 1024 & 2048 & 4096 & 8192 \\
    \midrule

    Sorting (Torch) & 0.09 & 0.11 & 0.26 & 0.76 & OOM \\
    
    Bisection (Torch) & 0.11 & 0.15 & 0.42 & 1.35 & 4.99 \\
    
    Halley-bisection (Triton) & 0.10 & 0.11 & 0.26 & 0.46 & 1.61 \\
    
    \methodname (Triton) & 0.10 & 0.12 & 0.21 & 0.48 & 1.53 \\
    
    \bottomrule
    \end{tabular}
\end{table}










\subsection{GLUE and BIER tasks} \label{subsec:app_glue_and_bier}

For GLUE tasks, we used the checkpoints of continuous pre-trained models for both RoBERTa-base and ModernBERT-base. Then, we fine-tuned them on each GLUE task with the default hyperparameters from the Transformer library.\footnote{\url{https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification}} Importantly, we capped the maximum sequence length at 128 tokens to reduce computational cost while preserving task-relevant context and used \texttt{fp16} for training. 


To evaluate the generalization of \methodname in retrieval tasks, we fine-tuned ModernBERT-base and RoBERTa-base models on the MS MARCO dataset~\citep{bajaj2016ms} and evaluated them on the BEIR benchmark~\citep{thakur2021beir}. 
This benchmark suite assesses performance across diverse information retrieval tasks, including SciFact, NFCorpus, FiQA-2018, and TREC-COVID.
The fine-tuning and evaluation process closely follows the approach proposed in the ModernBERT paper~\citep{warner2024smarter}. 
Fine-tuning was performed using the SentenceTransformers library.\footnote{\url{https://sbert.net/}} 
The models were evaluated on BEIR tasks using the MTEB benchmark toolkit.\footnote{\url{https://github.com/embeddings-benchmark/mteb}} 
The evaluation metric for each task was nDCG@10 (Normalized Discounted Cumulative Gain), following standard information retrieval practices.



\begin{table*}[t]
    \small
    \centering
    \caption{Results on different tasks from the GLUE benchmark~\citep{wang-etal-2018-glue}.} 
    \label{tab:results_glue}
    \vskip 0.1in
    \begin{tabular}{lrrccccccccc}
    \toprule
    & & & \multicolumn{2}{c}{Single Sentence} & \multicolumn{3}{c}{Paraphrase and Similarity} & \multicolumn{3}{c}{Natural Language Inference} \\
    \cmidrule(lr){4-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
    Model & Params & Seq. & CoLA & SST-2 & MRPC & STS-B & QQP & MNLI & QNLI & RTE & Avg. \\
    \midrule
    BERT        & 110M & 512    & 58.6 & 91.9 & 86.9 & 89.0 & 89.3 & 84.0 & 91.0 & 69.3 & 82.5 \\
    RoBERTa     & 125M & 512    & 59.8 & 93.7 & 89.5 & 89.6 & 89.8 & 87.7 & 92.3 & 69.3 & 83.9 \\
    RoBERTa ($\alpha = 1.5$)    & 125M & 512    & 58.5 & 93.2 & 91.5 & 90.2 & 89.7 & 87.3 & 92.5 & 68.6 & 83.9 \\
    RoBERTa ($\alpha = 2.0$)    & 125M & 512    & 56.8 & 93.0 & 90.9 & 88.8 & 89.0 & 86.7 & 91.9 & 67.2 & 83.0 \\
    ModernBERT  & 149M & 8192   & 63.2 & 95.0 & 88.2 & 90.3 & 90.4 & 87.9 & 93.0 & 61.7 & 83.7 \\
    ModernBERT ($\alpha = 1.5$)  & 149M & 8192   & 62.2 & 96.1 & 87.7 & 89.4 & 90.2 & 87.9 & 92.6 & 61.7 & 83.5 \\
    ModernBERT ($\alpha = 2.0$) & 149M & 8192   & 62.2 & 94.8 & 89.0 & 89.9 & 90.5 & 87.8 & 93.1 & 62.5 & 83.7 \\
    \bottomrule
    \end{tabular}
\end{table*}


\subsection{Long Document Classification} \label{subsec:app_long_doc_classification}

The European Court of Human Rights (ECtHR) dataset comprises legal cases from the European Court of Human Rights, each associated with specific articles of the Convention on Human Rights allegedly violated. 
For this task, we fine-tuned the RoBERTa base model \cite{liu2019robertarobustlyoptimizedbert} with a classification head. Since this is a multi-label classification task, we used a binary cross-entropy loss.
To accommodate longer contexts, we followed the approach proposed by \citep{beltagy_longformer_2020}, repeating the 512 position embeddings until the target context size was reached. We used the AdamW optimizer for training. For hyperparameters, we follow the recipe of \citet{dai-etal-2022-revisiting}. For the attention mechanism, \texttt{bfloat16} precision was used.


\begin{table}[ht]
    \small
    \centering
    \caption{Runtime per epoch (hh:mm:ss) and peak memory usage (GB) for different sequence lengths on RoBERTa (FA2) and RoBERTa ($\alpha=1.5$) with AdaSplash and Torch's bisection for the Long Document Classification task. In cases where the full batch could not fit in memory, gradient accumulation was used. Memory values represent the effective peak memory required to process a batch of 16, given the gradient accumulation steps.}
    \label{tab:runtime_memory}
    \vskip 0.1in
    \setlength{\tabcolsep}{6pt} %
    \begin{tabular}{l ccccc ccccc}
    \toprule
    & \multicolumn{5}{c}{Runtime (hh:mm:ss)} & \multicolumn{5}{c}{Memory (GB)} \\
    \cmidrule(lr){2-6} \cmidrule(lr){7-11}
    Model & 512 & 1024 & 2048 & 4096 & 8192 & 512 & 1024 & 2048 & 4096 & 8192 \\
    \midrule
    RoBERTa (FA2)          & 2:39  & 5:00  & 9:35   & 18:36   & 35:51   & 6.75  & 11.43 & 20.35  & 37.49   & 75.00   \\
    RoBERTa ($\alpha=1.5$)     & 2:45  & 5:20  & 10:24  & 19:54   & 38:08   & 6.75  & 11.45 & 20.38  & 39.17   & 79.88   \\
    \ \  w/ Torch Bisect    & 4:51  & 8:44  & 22:48  & 1:11:53 & 4:12:34 & 7.75  & 16.92 & 44.06  & 142.76  & 508.16  \\
    \bottomrule
    \end{tabular}
\end{table}




\subsection{Language Modeling} \label{subsec:app_language_modeling}


We trained both the standard GPT-2 model and sparse GPT-2 ($\alpha=1.5$) using the configuration provided in the \texttt{llm.c} repository,\footnote{\url{https://github.com/karpathy/llm.c}} following their training recipe. Specifically, we trained a GPT-2 (124M parameters) from scratch on 10B tokens of the FineWeb dataset, with a maximum sequence length of 1024 tokens. Training was conducted using \texttt{bfloat16} precision. We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory. We use the AdamW optimizer, with learning rate $6\times 10^{-4}$ and weight decay of 0.1. The learning rate followed a warm-up phase, linearly ramping from zero to a maximum of $6\times 10^{-4}$ over the first 700 iterations, equivalent to 350 million tokens. Subsequently, the learning rate decayed to zero across the remaining training steps.
We show the validation loss curves for both softmax and $\alpha$-entmax ($\alpha = 1.5$) in Figure~\ref{fig:comparing-gpt2}. 

Given that, for this task, the context size was not high enough, for sparse attention we opted to use the algorithm that does not take advantage of the pointer increment tables. For the benchmarking of the time spent per step, we averaged across 50 steps after the model had trained for at least 100 steps.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth]{figs/sparsegpt2_vs_gpt2.pdf}
    \caption{FineWeb withheld validation loss comparison between GPT-2 and Sparse GPT-2 during training.}
    \label{fig:comparing-gpt2}
\end{figure}



\section{Computational Details}

Experiments on masked language modeling, text classification, GLUE tasks and BIER tasks were carried on Nvidia RTX A6000 GPUs with 48GB VRAM. 
Experiments with GPT-2 and the efficiency benchmark in Figures~\ref{fig:sparsity-speedup} and \ref{fig:runtimes_comparison} were carried on a single Nvidia H100 Tensor Core GPU with 80GB VRAM. The runtime experiments with ModernBERT were carried on a single A6000 GPU.


\end{document}
