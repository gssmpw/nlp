\section{Related Work}
In this section, we discuss the prior work and put this paper in the context of existing results. 

\paragraph{Necessity of memorization in learning.} 
Our work is most similar in spirit to the works of____ and____. In ____, the authors studied the traceability of algorithms for mean estimation in $\ell_\infty$ norm. In particular, they demonstrated that, in the accuracy regime where DP learning is impossible, every algorithm is traceable with large recall. 
More recently,____ showed that a similar conclusion holds for learners in SCO in $\ell_2$ geometry. Our work builds on top of these results on a number of fronts. Our first novelty is a generic traceability theory (outlined in~\Cref{sec:technical}) that allows to seamlessly convert fingerprinting lemmas into traceability results and DP lower bounds. 
Another key difference is the structure of hard problems and the novelty of fingerprinting lemma. First, in our fingerprinting lemmas, the parameters of the priors in our constructions necessarily need to \emph{scale} with the risk level, in order to show that the number of memorized samples is the same as sample complexity. 
We emphasize that this scaling is indeed necessary to achieve an optimal result. For example, in the context of SCO in $\ell_2$ geometry, one could hope to use the fingerprinting lemma with \emph{uniform prior} from____ along with the reduction in~\cite[Sec.~5.1]{bassily2014private}; however, it would only yield a recall of $\Omega(1/\alpha)$ samples, while the sample complexity is $\Theta(1/\alpha^2)$.  Second, $\ell_p$-SCO with $p > 2$ is distinct in the sense that the hardest problems have a sparse structure (see____). It is different from the usual constructions for mean estimation and $\ell_2$-SCO for which a ``hypercube-like'' construction suffices.  These differences necessitate new techniques, leading us to devise novel constructions and a new fingerprinting lemma specifically suited to sparse data vectors, which can be of independent interest.

A parallel line of work investigated memorization using the notion of \emph{label memorization} in supervised setups.
As per this definition, a learner is said to memorize its training samples if it ``overfits'' at these points.____ showed that, in some classification tasks, if the underlying distribution is \emph{long-tailed}, then a learner is forced to memorize many training labels.____ showed this phenomenon also occurs in the setting of linear regression. While this framework is suitable to study memorization in supervised tasks, the notion of ``labels'' in SCO in not well-defined and thus calls for alternative definitions.

Another line of work studied memorization through the lens of information theoretic measures. ____ used \emph{input-output mutual information} (IOMI) as a memorization metric and showed that IOMI can scale linearly with the training sample's entropy, indicating that a constant fraction of bits is memorized. 
In the context of SCO in $\ell_2$ geometry, lower bounds on IOMI have been studied in ____. Specifically____ demonstrated that, for every accurate algorithm, its IOMI must scale with dimension $d$. Our approach to the study of memorization is conceptually different since we focus on the number of \emph{samples} memorized as opposed to the number of \emph{bits}. 
Nevertheless, it can be shown using~\Cref{lemma:threshold-tracing} and~\cite[Thm.~2.1]{haghifam2020sharpened} that the recall \emph{lower bounds} IOMI of an algorithm (provided that FPR $\xi$ is small enough, e.g., $\xi = 1/n^2$). However, because of the Lipschitzness of loss functions in $\ell_p$-SCO, we can use discretization of $\Theta$ and design algorithms with IOMI that is significantly smaller that the entropy of the training set, thus, memorization in the sense of ____ does not arise here. 

\paragraph{Membership inference.} Membership inference is an important practical problem ____. In these works, the focus is on devising strategies for the tracer in modern machine learning settings, particularly neural networks. Our work takes a more fundamental perspective, aiming to determine whether membership inference is inherently unavoidable or simply a byproduct of specific training algorithms. An interesting aspect of our results is that, for $1 < p \le 2$, the optimal strategy for tracing depends \emph{only} on the loss function, which is in line with empirical studies____. 



\paragraph{Private Stochastic Convex Optimization.}
DP-SCO has been extensively studied in $\ell_2$ geometry (see, for instance, ____). For $\ell_p$ with $p \in [1,2)$, the optimal DP excess risk was established in ____. The best known upper bounds for DP-SCO in $\ell_p$ geometry for $p>2$ are due to ____. In this setting, there is a long-standing gap between upper and lower bounds, and the best known lower bounds are due to____, which our paper improves on.  

\paragraph{Fingerprinting Lemmas and DP Lower Bounds.}
Fingerprinting codes were introduced by____. The work____ was first to relate them to lower bounds on differential privacy, and they were used extensively afterwards____. Intuitively, fingerprinting lemmas formalize the intuition that the output of an accurate algorithm must be correlated with its training set ____. To put our \emph{sparse fingerprinting lemma} into the context of prior work, it can be seen to generalize the results of____ to sparse sets. 
Another ``sparse fingerprinting lemma'' in the literature is given by____ to address DP lower bounds in the sparse generalized linear model. Besides visual and naming similarities, our results are distinct. Indeed, sparsity enters the lemmas in different forms: in____ the \emph{mean vector} is sparse (and data is dense), and in our case, the mean is dense and the \emph{data vectors} are sparse. The proof techniques also differ substantially.