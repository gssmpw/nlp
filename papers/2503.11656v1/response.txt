\section{Related Works}
%Srihan
% In Towards Understanding Sycophancy in LLMs**Stoyanovich, "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"**, they noticed a trend in the shift of the GPT-4 answers. Answer choices tended to shift more to make the user more satisfied. Both papers have a similar methodology, analyzing the AI model's responses to different inputs and whether the answers given by the model is an answer from its own or from external pressure. Past works have raised concerns regarding sycophancy in AI Models, our experiment touches on this, viewing how each AI model from the ones listed above changes their answers. 

% We aim to answer **Stoyanovich, "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"** and **Haimo, "The Impact of Sycophancy on Large Language Models"** questions and statements on Sycophancy. We used this core argument in our paper, understanding how the models will changed based of question difficulty. In the 2021 paper, they mention deceptive behaviors in AI models, specifically how AI models might change their answer as the question becomes more complicated. 

% Alongside these two papers, **Haimo and Lin, "The Effects of Sycophancy on Language Models"**. Despite being a little different, our paper and tests are based off whether the models will change their answers and base it off what the user states, different from Zheng's paper. Their paper focuses on AI models and whether they will conform to one of the user's opinion more than the others. 

% **Lin and Haimo, "The Consequences of Sycophancy in Language Models"** differs from our paper in small ways. Lin and his team tested whether model's will generate false answers and information based on common and learned misconceptions. Our test is different as we focused more on whether models provide agreeable responses to the user. Lin's paper does teach that models can create these false answers and information due to falsehood presents in their training data. Their training data could lead them to agree with the user more than what might be true, hence creating this false information, therefore misguiding the user. These papers have greatly inspired our tests, helping us understand current inaccuracies in AI models, and guiding us on every step we took while running tests and creating this paper. 
Prior studies have highlighted sycophantic tendencies in language models, showing that language models shift responses to align with user preferences. **Stoyanovich et al., "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"** analyzed GPT-4’s behavior and found that responses often changed to increase user satisfaction. Our study builds on this by examining sycophancy over multiple turns, revealing progressive factual degradation rather than just single-step shifts. Reinforcement learning from human feedback (RLHF) has become a prominent technique for training large language models (LLMs) to align with human preferences and values **Lin et al., "The Effects of Sycophancy on Language Models"**. This training method creates preferences in the LLM. For example, it causes Large language Models to avoid telling the user “I don’t know” if they do not know the correct answer **Haimo and Lin, "The Consequences of Sycophancy in Language Models"**.

We also explore LLM deception in response to increasing question complexity, as discussed in **Stoyanovich et al., "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"**. Our findings reinforce that models not only adjust answers based on difficulty but also amplify sycophantic behavior over extended interactions. Similarly, **Lin and Haimo, "The Consequences of Sycophancy in Language Models"** studied the conformity in language models, although their focus was on whether the models favor certain user opinions. Our work extends this analysis by explicitly measuring factual consistency under user influence.

%Another relevant study, TruthfulQA **Haimo and Lin, "The Effects of Falsehood Generation due to Learned Misconceptions"** examined falsehood generation due to learned misconceptions. While their work focused on preexisting biases in model training, our study investigates how sycophantic agreement reinforces user-driven inaccuracies. By bridging insights from these works, our research contributes a deeper understanding of how sycophancy compounds over multiturn dialogues, underscoring the need for more robust mitigation strategies.

%**Haimo and Lin, "The Consequences of Sycophancy in Language Models"** directly relates with our work, as this paper addresses LLM response shifts and bias based on user influence. Our research contributes to a deeper understanding of the average percentage in which a model will switch their answers based on user influence. **Stoyanovich et al., "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"** created a benchmark that challenges and addresses AI models in long multi-step conversations with a user. This paper specifically viewed how well the AI models can follow the user and if they will comply to the user's opinions. 

**Haimo and Lin, "The Consequences of Sycophancy in Language Models"** and **Stoyanovich et al., "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"** discuss factual degradation in language models after long-form conversations. **Lin and Haimo, "The Effects of Hallucinations in LLMs after Long Conversations"** specifically touches on hallucinations in LLMs after long conversations, while **Stoyanovich et al., "Sociotechnical Sycophancy: An Analysis of GPT-4's Responses"** talks about how the language model's behavior switches throughout the conversation. Both papers are highly relevant to our paper as we view the factual degradation of our language model after several turns, with each turn viewing the result and percentage on how the language model will switch its answer.