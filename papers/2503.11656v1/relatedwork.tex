\section{Related Works}
%I thought this was to long Srihan
% In Towards Understanding Sycophancy in LLMs\cite{sharma2023understandingsycophancylanguagemodels}, they noticed a trend in the shift of the GPT-4 answers. Answer choices tended to shift more to make the user more satisfied. Both papers have a similar methodology, analyzing the AI model's responses to different inputs and whether the answers given by the model is an answer from its own or from external pressure. Past works have raised concerns regarding sycophancy in AI Models, our experiment touches on this, viewing how each AI model from the ones listed above changes their answers. 

% We aim to answer \cite{cotra2021Whyaialignmentcouldbehardwithmoderndeeplearning} questions and statements on Sycophancy. We used this core argument in our paper, understanding how the models will changed based of question difficulty. In the 2021 paper, they mention deceptive behaviors in AI models, specifically how AI models might change their answer as the question becomes more complicated. 

% Alongside these two papers, \cite{Zheng_2022}. Despite being a little different, our paper and tests are based off whether the models will change their answers and base it off what the user states, different from Zheng's paper. Their paper focuses on AI models and whether they will conform to one of the user's opinion more than the others. 

% \cite{lin2022truthfulqameasuringmodelsmimic} differs from our paper in small ways. Lin and his team tested whether model's will generate false answers and information based on common and learned misconceptions. Our test is different as we focused more on whether models provide agreeable responses to the user. Lin's paper does teach that models can create these false answers and information due to falsehood presents in their training data. Their training data could lead them to agree with the user more than what might be true, hence creating this false information, therefore misguiding the user. These papers have greatly inspired our tests, helping us understand current inaccuracies in AI models, and guiding us on every step we took while running tests and creating this paper. 
Prior studies have highlighted sycophantic tendencies in language models, showing that language models shift responses to align with user preferences. \citet{sharma2023understandingsycophancylanguagemodels} analyzed GPT-4’s behavior and found that responses often changed to increase user satisfaction. Our study builds on this by examining sycophancy over multiple turns, revealing progressive factual degradation rather than just single-step shifts. Reinforcement learning from human feedback (RLHF) has become a prominent technique for training large language models (LLMs) to align with human preferences and values \cite{sharma2023understandingsycophancylanguagemodels}. This training method creates preferences in the LLM. For example, it causes Large language Models to avoid telling the user “I don’t know” if they do not know the correct answer \cite{miehling2024languagemodelsdialogueconversational}.

We also explore LLM deception in response to increasing question complexity, as discussed in \citet{cotra2021Whyaialignmentcouldbehardwithmoderndeeplearning}. Our findings reinforce that models not only adjust answers based on difficulty but also amplify sycophantic behavior over extended interactions. Similarly, \citet{Zheng_2022} studied the conformity in language models, although their focus was on whether the models favor certain user opinions. Our work extends this analysis by explicitly measuring factual consistency under user influence.

%Another relevant study, TruthfulQA \cite{lin2022truthfulqameasuringmodelsmimic}, examined falsehood generation due to learned misconceptions. While their work focused on preexisting biases in model training, our study investigates how sycophantic agreement reinforces user-driven inaccuracies. By bridging insights from these works, our research contributes a deeper understanding of how sycophancy compounds over multiturn dialogues, underscoring the need for more robust mitigation strategies.

%\cite{wei2024simplesyntheticdatareduces} directly relates with our work, as this paper addresses LLM response shifts and bias based on user influence. Our research contributes to a deeper understanding of the average percentage in which a model will switch their answers based on user influence. \cite{sirdeshmukh2025multichallengerealisticmultiturnconversation} created a benchmark that challenges and addresses AI models in long multi-step conversations with a user. This paper specifically viewed how well the AI models can follow the user and if they will comply to the user's opinions. 

\citet{malmqvist2024sycophancylargelanguagemodels} and \citet{laban2024surechallengingllmsleads} discuss factual degradation in language models after long-form conversations. \citet{malmqvist2024sycophancylargelanguagemodels} specifically touches on hallucinations in LLMs after long conversations, while \citet{laban2024surechallengingllmsleads} talks about how the language model's behavior switches throughout the conversation. Both papers are highly relevant to our paper as we view the factual degradation of our language model after several turns, with each turn viewing the result and percentage on how the language model will switch its answer.