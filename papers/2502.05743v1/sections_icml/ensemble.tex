% In this section, we first validate our theoretical insights into the representation dynamics of diffusion models under both theoretical and practical settings in \Cref{subsec:emp_verify}. 
We examine the practical implications of our findings in \Cref{subsec:exp_ensemeble}, leveraging feature information at different levels of granularity to enhance robustness. Additionally, we discuss the advantages of diffusion models over traditional single-step DAEs in \Cref{subsec:weight_share}.
% and analyze the influence of data complexity on diffusion-based representation learning in \Cref{subsec:mem_gen}. Detailed experimental setups are provided in \Cref{app:exp_detail}.

% \begin{figure*}[t]
% \begin{center}
%     \begin{subfigure}{0.47\textwidth}
%     \includegraphics[width = 0.955\textwidth]{figs/ucurve_cifar_featureacc.pdf}
%     \caption{Probing Acc. of diffusion models and DAEs} 
%     \end{subfigure} \quad %\hspace*{\fill}
%     \begin{subfigure}{0.47\textwidth}
%     \includegraphics[width = 0.955\textwidth]{figs/ucurve_cifar_csnr.pdf}
%     \caption{CSNR and Posterior Acc.} 
%     \end{subfigure}
%     \end{center}
% \caption{\textbf{Dynamics of feature probing accuracy and \CSNR~on CIFAR10.} Panel (a) shows the feature probing accuracy and Panel (b) shows the \CSNR~and posterior probing accuracy trends computed using the CIFAR10 test dataset, both exhibiting a \textcolor{blue}{matching} unimodal pattern.}
% \label{fig:cifar}
% \end{figure*}

% \subsection{Empirical Validation}\label{subsec:emp_verify}

% \qq{condense, and move this as a subsection of Section 3 instead. This section needs to focus on practical insights: (i) ensemble, (ii) weight sharing, and (iii) memorization to generalization}

% \xiao{Section 4.1.1 has been discussed in Section 2. Section 4.1.2 will be removed to Section 3}
% {\color{blue}
% We divide this subsection into two parts: in \Cref{subsubsec:feature-extraction}, we justify our design choice in the theoretical analysis by considering clean inputs $\bm x_0$ as the input to diffusion models, rather than the conventional approach of using noisy inputs $\bm x_t$. Subsequently, in \Cref{subsubsec:theory_verify}, we validate our theoretical insights by demonstrating that \CSNR~aligns closely with both feature and posterior probing performance.

% \subsubsection{Clean Inputs Enhance Representation Performance Compared to Noisy Inputs}\label{subsubsec:feature-extraction}

% If we treat a diffusion model at each noise level (i.e., $\bm x_{\bm \theta}(\cdot, t)$) as an independent model and aim to compare their representation performance, using the noisy input $\bm x_t$ does not provide a fair basis for comparison. This is because the varying input noise levels at different values of $t$ can significantly influence the results. Therefore, in both the definition of \CSNR~\eqref{eq:csnr_true} and the subsequent analysis, we consistently consider clean inputs $\bm x_0$ to eliminate the confounding effects of input degradation.

% A potential concern, however, is whether using clean inputs during inference contradicts the training paradigm, as diffusion models are trained on corrupted inputs. To address this, we compare the representation performance of models using $\bm x_0$ and $\bm x_t$ as inputs across classification and segmentation tasks, with results presented in \Cref{fig:clean_feature}. Our findings highlight two key insights: (1) using clean inputs $\bm{x}_0$ consistently matches or surpasses the performance achieved with corrupted inputs $\bm{x}_t$, and (2) the performance gap widens as $t$ (or equivalently, $\sigma_t$) increases. These observations suggest that the core representation ability of diffusion models is primarily derived from their denoising objective, while the diffusion process—characterized by the gradual addition and removal of Gaussian noise—plays a minimal role in representation quality.

% This phenomenon draws a parallel with traditional supervised and self-supervised learning paradigms. In these settings, data augmentations such as cropping~\citep{caron2021emerging}, color jittering, or masking~\citep{he2022masked} are applied during training to enhance model robustness and generalization. However, clean, unaugmented images are typically used during inference to ensure optimal performance. Similarly, in diffusion models, additive Gaussian noise serves as a form of data augmentation critical for training~\citep{chen2024deconstructing}, but when the focus shifts to representation learning at inference, using clean images $\bm{x}_0$ is both sufficient and aligned with standard representation learning practices. Thus, we advocate for the use of clean images as the standard input protocol for feature extraction during inference throughout our analysis and future work.

% }

\subsection{Finding I: Feature Ensembling Across Timesteps Improves Representation Robustness}\label{subsec:exp_ensemeble}

Our theoretical insights imply that features extracted at different timesteps capture varying levels of granularity. Given the high linear separability of intermediate features, we propose a simple ensembling approach across multiple timesteps to construct a more holistic representation of the input. Specifically, in addition to the optimal timestep, we extract feature representations at four additional timesteps—two from the coarse (larger $\sigma_t$) and two from the fine-grained (smaller $\sigma_t$) end of the spectrum. We then train linear probing classifiers for each set and, during inference, apply a soft-voting ensemble by averaging the predicted logits before making a final decision.(experiment details in \Cref{app:exp_detail})

We evaluate this ensemble method against results obtained from the best individual timestep, as well as a self-supervised method MAE \citep{he2022masked}, on both the pre-training dataset and a transfer learning setup. The results, reported in \Cref{tab:ensemble_results} and \Cref{tab:ensemble_results_transfer}, demonstrate that ensembling significantly enhances performance for both EDM \citep{karras2022elucidating} and DiT \citep{peebles2023scalable}, consistently outperforming their vanilla diffusion model counterparts and often surpassing MAE. Moreover, ensembling substantially improves the robustness of diffusion models for classification under label noise.

 \begin{table}[t]
\centering
\resizebox{0.98\linewidth}{!}{
	\begin{tabular}	{l c c c c c } 
		
        \toprule
            % \rule{0pt}{0.1ex} \\ 
		 \textbf{Method} & \multicolumn{5}{c}{\textit{MiniImageNet$^\star$} Test Acc. \%}\\
            % \rule{0pt}{0.1ex} \\ 
		 \midrule
		 \textbf{Label Noise} & Clean & 20\% & 40\% & 60\% & 80\% \\
   %       \midrule
   %       \textit{CIFAR10} \\ 
		 % \midrule
		 % SimCLR ResNet50 & 93.2 & 92.5 & 92.2 & 91.6 & 90.2 \\ 
   
   %        EDM & 96.0 & 95.8 & 95.9 & 95.5 & 94.6  \\ 
          
		 % \textbf{EDM (Ensemble)} & 95.7 & 95.8 & 95.9 & 95.4 & 95.0 \\ 
         
         \midrule
   %      \textit{MiniImageNet$^\star$} \\  
		 % \midrule
		 % SimCLR ResNet50 & 75.1 & 72.1 & 70.1 & 66.6 & \textbf{60.4} \\ 
         MAE & 73.7 & 70.3 & 67.4 & 62.8 & 51.5 \\ 
   
          EDM & 67.2 & 62.9 & 59.2 & 53.2 & 40.1  \\ 
          
		 \textbf{EDM (Ensemble)} & 72.0 & 67.8 & 64.7 & 60.0 & 48.2 \\ 

         
   
          DiT & 77.6 & 72.4 & 68.4 & 62.0 & 47.3  \\ 
          
		 \textbf{DiT (Ensemble)} & \textbf{78.4} & \textbf{75.1} & \textbf{71.9} & \textbf{66.7} & \textbf{56.3} \\ 
         
        \bottomrule
	\end{tabular}}
    \caption{\textbf{Comparison of test performance across different methods under varying label noise levels.} All compared models are publicly available and pre-trained on ImageNet-1K \citep{deng2009imagenet}, evaluated using MiniImageNet classes. Bold font highlights the best result in each scenario.}
    \label{tab:ensemble_results}
    % \vspace{-6mm}
\end{table}

 \begin{table*}[t]
\centering
\resizebox{0.98\linewidth}{!}{
	\begin{tabular}	{l c c c c c | c c c c c | c c c c c} 
		
        \toprule
          % \rule{0pt}{0.05in} \\ 
		 \textbf{Method} & \multicolumn{15}{c}{Transfer Test Acc. \%}\\
         % \rule{0pt}{0.2ex} \\ 
           % \rule{0pt}{0.05in} \\ 
         \midrule
        & \multicolumn{5}{c|}{\textbf{CIFAR100}} & \multicolumn{5}{c|}{\textbf{DTD}} & \multicolumn{5}{c}{\textbf{Flowers102}} \\  
		 \textbf{Label Noise} & Clean & 20\% & 40\% & 60\% & 80\% & Clean & 20\% & 40\% & 60\% & 80\% & Clean & 20\% & 40\% & 60\% & 80\% \\ 
         
         
		 \midrule
		 % SimCLR ResNet50 & 65.8 & 62.9 & \textbf{60.5} & \textbf{56.6} & \textbf{48.4} & 57.1 & 54.6 & 50.0 & 43.5 &	28.3 & 59.5 & 50.8 & 37.5 & 24.4 & 9.5 \\ 
         MAE & 63.0 & 58.8 & 54.7 & 50.1 & 38.4 & 61.4 & 54.3 & 49.9 & 40.5 & 24.1 & 68.9 & 55.2 & 40.3 & 27.6 & 9.6 \\ 
   
          EDM & 62.7 & 58.5 & 53.8 & 48.0 & 35.6 & 54.0 & 49.1 & 45.1 & 36.4 & 21.2 & 62.8 & 48.2 & 37.2 & 24.1 & 9.7 \\ 
          
		 \textbf{EDM (Ensemble)} & \textbf{67.5} & \textbf{64.2} & \textbf{60.4} & \textbf{55.4} & \textbf{43.9} & 55.7 & 49.5 & 45.2 & 37.1 & 22.0 & 67.8 & 53.9 & 41.5 & 25.0 & 10.4 \\ 

         
   
          DiT & 64.2 & 58.7 & 53.5 & 46.4 & 32.6 & 65.2 & 59.7 & 53.0 & 43.8 & 27.0 & 78.9 & 65.2 & 52.4 & 34.7 & 13.3 \\ 
          
		 \textbf{DiT (Ensemble)} & 66.4 & 61.8 & 57.6 & 51.3 & 39.2 & \textbf{65.3} & \textbf{60.6} & \textbf{56.1} & \textbf{46.3} & \textbf{30.6} & \textbf{79.7} & \textbf{67.0} & \textbf{54.6} & \textbf{36.6} & \textbf{14.7}  \\ 
         
        \bottomrule
	\end{tabular}}
    \caption{\textbf{Comparison of transfer learning performance across different methods under varying label noise levels.} All compared models are publicly available and pre-trained on ImageNet-1K \citep{deng2009imagenet}, evaluated on different downstream datasets. Bold font highlights the best result in each scenario.  }
    \label{tab:ensemble_results_transfer}
    % \vspace{-6mm}
\end{table*}
%\qq{save space of the first line} 
% \begin{table}[t]
%     \centering
%     \begin{tabular}{c|c c c c}
%     label noise & 0 & 0.4 & 0.6 & 0.8\\
%     \hline
%     cifar10 & 95.8 & 94.72 & 93.7 & 82.83\\
%     % cifar10 (ensemble) & 95.75 & 95.52& 95.12 & 94.04 \\
%     cifar10 (ensemble w/ BN) & 95.65 & 95.63 & 95.36 & 94.98\\
%     MAE(ImageNet transfer)& 83.94 & 80.53 & 76.69 & 63.46\\
%     ResNet(ImageNet transfer) \\
%     \hline
%     % miniImageNet(edm) & 65.64 & 54.48  & 42.26 & 23.58\\
%     % miniImageNet(DiT) & 74.77 & 50.91 & 38.09 & \\
%     miniImageNet(DiT w/BN) & 77.56 & 63.42 & 49.59 & 27.11\\
%     miniImageNet(DiT w/BN ensemble) & 78.44 & 67.91 & 54.30 & 30.18\\
%     % miniImageNet (edm ensemble) & 70.9 & 60.53 & 48.13 & 24.96\\
%     % MAE(ImageNet transfer w/o BN) & 73.37 & 66.27 & 56.32 & 33.37\\
%     MAE(ImageNet transfer w/ BN) & 72.87 & 65.79 & 55.15 & 29.32\\
%     % MAE(ImageNet transfer w/trick) & 69.01 & 61.70 & 52.16 & 29.57\\
%     \hline
%     cifar100 \\
%     MAE(w/ BN)& 63.01 & 53.06 & 42.10 & 22.11\\
%     % MAE(w/o BN)& 58.24 & 51.52 & 40.74 & 21.94\\
%     \end{tabular}
%     \caption{Feature ensemble enhance the overall linear probing accuracy, especially robustness againist label noise}
%     \label{tab:ensemble}
% \end{table}

\subsection{Finding II: Weight Sharing in Diffusion Models Facilitates Representation Learning}\label{subsec:weight_share}

\begin{figure*}[t]
    \begin{center}
    \begin{subfigure}{0.47\textwidth}
    \includegraphics[width = 0.955\textwidth]{figs/dae_diffusion_c10.pdf}
    \caption{CIFAR10} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.47\textwidth}
    \includegraphics[width = 0.955\textwidth]{figs/dae_diffusion_c100.pdf}
    \caption{CIFAR100} 
    \end{subfigure}
    \end{center}
    \vspace{-0.1in}
\caption{\textbf{Diffusion models exhibit higher and smoother feature accuracy and similarity compared to individual DAEs.} We train DDPM-based diffusion models and individual DAEs on the CIFAR datasets and evaluate their representation learning performance. Feature accuracy, and feature differences from the optimal features (indicated by {\color{cyan} $\star$}) are plotted against increasing noise levels. The results reveal an inverse correlation between feature accuracy and feature differences, with diffusion models achieving both higher/smoother accuracy and smaller/smoother feature differences compared to DAEs.}
\vspace{-0.05in}
\label{fig:dae_diffusion}
\end{figure*}

% Now that we have established the denoising objective as the primary driver of diffusion models' superior representation learning capabilities, we now turn to a key question: what makes diffusion models outperform traditional single-step denoising autoencoders (DAEs) \citep{vincent2008extracting,vincent2010stacked} and achieve on-par or superior representation performance compared to state-of-the-art self-supervised methods? In this subsection, we reveal that the key advantage of diffusion models over traditional DAEs lies in their inherent weight-sharing mechanism.

Second, we reveal why diffusion models, despite sharing the same denoising objective with classical DAEs, achieve superior representation learning due to their inherent weight-sharing mechanism. By minimizing loss across all noise levels (\ref{eq:dae_loss}), diffusion models enable parameter sharing and interaction among denoising subcomponents, creating an implicit "ensemble" effect. This improves feature consistency and robustness across noise scales compared to DAEs \citep{chen2024deconstructing}, as illustrated in \Cref{fig:dae_diffusion}.

To test this, we trained 10 DAEs, each specialized for a single noise level, alongside a DDPM-based diffusion model on CIFAR10 and CIFAR100. We compared feature quality using linear probing accuracy and feature similarity relative to the optimal features at $\sigma_t = 0.06$ (where accuracy peaks) via sliced Wasserstein distance (\SWD) \citep{doan2024assessing}.

The results in \Cref{fig:dae_diffusion} confirm the advantage of diffusion models over DAEs. Diffusion models consistently outperform DAEs, particularly in low-noise regimes where DAEs collapse into trivial identity mappings. In contrast, diffusion models leverage weight-sharing to preserve high-quality features, ensuring smoother transitions and higher accuracy as noise increases. This advantage is further supported by the \SWD~curve, which reveals an inverse correlation between feature accuracy and feature differences. Notably, diffusion model features remain significantly closer to their optimal state across all noise levels, demonstrating superior representational capacity.

Our finding also aligns with prior results that sequentially training DAEs across multiple noise levels improves representation quality \citep{chandra2014adaptive,geras2014scheduled,zhang2018convolutional}. Our ablation study further confirms that multi-scale training is essential for improving DAE performance on classification tasks in low-noise settings (details in \Cref{app:add_exp}, \Cref{tab:dae_trial}).
