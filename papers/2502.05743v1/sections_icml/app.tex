The Appendix is organized as follows: in \Cref{app:related}, we discuss related works; in \Cref{app:add_exp}, we provide complementary experiments; in \Cref{app:exp_detail}, we present the detailed experimental setups for the empirical results in the paper. Lastly, in \Cref{app:proofs}, we provide proof details for \Cref{sec:main}.


\subsection{Related Works}\label{app:related}

\paragraph{Denoising auto-encoders.}
Denoising autoencoders (DAEs) are trained to reconstruct corrupted images to extract semantically meaningful information, which can be applied to various vision \citep{vincent2008extracting, vincent2010stacked} and language downstream tasks \citep{lewis2019bart}. Related to our analysis of the weight-sharing mechanism, several studies have shown that training with a noise scheduler can enhance downstream performance \citep{chandra2014adaptive, geras2014scheduled, zhang2018convolutional}. On the theoretical side, prior works have studied the learning dynamics \citep{pretorius2018learning,steck2020autoencoders} and optimization landscape \citep{kunin2019loss} through the simplified linear DAE models.

\paragraph{Diffusion-based representation learning.} Diffusion-based representation learning \citep{fuest2024diffusion} has demonstrated significant success in various downstream tasks, including image classification \citep{xiang2023denoising, mukhopadhyay2023diffusion, deja2023learning}, segmentation \citep{baranchuk2021label}, correspondence \citep{tang2023emergent}, and image editing \citep{shi2024dragdiffusion}. To further enhance the utility of diffusion features, knowledge distillation \citep{yang2023diffusion, li2023dreamteacher,stracke2024cleandift,luo2024diffusion} methods have been proposed, aiming to bypass the computationally expensive grid search for the optimal $t$ in feature extraction and improving downstream performance. Beyond directly using intermediate features from pre-trained diffusion models, research efforts has also explored novel loss functions \citep{abstreiter2021diffusion, wang2023infodiffusion} and network modifications \citep{hudson2024soda, preechakul2022diffusion} to develop more unified generative and representation learning capabilities within diffusion models. Unlike the aforementioned efforts, our work focuses more on understanding the representation learning capabilities of diffusion models.

% \paragraph{\textcolor{orange}{Coarse to fine generation within diffusion process.}}
% \textcolor{orange}{\citep{wang2023diffusion} highlight the coarse-to-fine generation phenomenon observed in diffusion models. Similarly, \citep{yue2024exploring} propose a framework to explore the emergence of attributes of different granularity during the diffusion process, leveraging this to learn disentangled representations at different time steps. We also connect our findings to the coarse-to-fine effect, but we focus on understanding the features of diffusion models through a DDAE perspective. Furthermore, we introduce a computable metric, \CSNR, to characterize the optimal time step at which diffusion models produce the most effective representations}

\subsection{Additional Experiments}\label{app:add_exp}

\paragraph{Influence of data complexity in diffusion representation learning}%\label{subsec:mem_gen}

Our analyses in the main body of the paper are based on the assumption that the training dataset contains sufficient samples for the diffusion model to learn the underlying distribution. Interestingly, if this assumption is violated by training the model on insufficient data, the unimodal representation learning dynamic disappears and the probing accuracy also drops severely. 


As illustrated in \Cref{fig:phase_transit_2}, we train $2$ different UNets following the EDM \citep{karras2022elucidating} configuration with training dataset size ranging from $2^5$ to $2^{15}$. The unimodal curve emerges only when the dataset size exceeds $2^{12}$, whereas smaller datasets produce flat curves.


The underlying reason for this observation is that, when training data is limited, diffusion models memorize all individual data points rather than learn the true underlying data structure \citep{wang2024diffusion}. In this scenario, the model memorizes an empirical distribution that lacks meaningful low-dimensional structures and thus deviates from the setting in our theory, leading to the loss of the unimodal representation dynamic. To confirm this, we calculated the generalization score, which measures the percentage of generated data that does not belong to the training dataset, as defined in \citep{zhang2024emergence}. As shown in \Cref{fig:phase_transit}, representation learning only achieves strong accuracy and displays the unimodal dynamic when the generalization score approaches $1$, aligning with our theoretical assumptions.

\begin{figure*}[t]
    \begin{center}
    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width = 0.975\textwidth]{figs/unet64_acc_trend.pdf}
    \caption{UNet-64 accuracy trend} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width = 0.975\textwidth]{figs/unet128_acc_trend.pdf}
    \caption{UNet-128 accuracy trend} 
    \end{subfigure}
    \end{center}
    \vspace{-0.1in}
\caption{\textbf{The influence of data complexity in diffusion-based representation learning.} With the same model trained in \Cref{fig:phase_transit}, we plot the representation learning dynamics for each trained model as a function of changing noise levels.}
\vspace{-0.1in}
\label{fig:phase_transit_2}

\end{figure*}

\begin{figure*}[t]
    \begin{center}
    % \begin{subfigure}{0.42\textwidth}
    % \includegraphics[width = 0.955\textwidth]{figs/unet64_acc_trend.pdf}
    % \caption{UNet-64 accuracy trend} 
    % \end{subfigure} \quad %\hspace*{\fill}
    % \begin{subfigure}{0.42\textwidth}
    % \includegraphics[width = 0.955\textwidth]{figs/unet128_acc_trend.pdf}
    % \caption{UNet-128 accuracy trend} 
    % \end{subfigure}

    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width = 0.975\textwidth]{figs/mem_gen_gl_score.pdf}
    \caption{Phase transition in generalization score} 
    \end{subfigure}
    \begin{subfigure}{0.43\textwidth}
    \includegraphics[width = 0.975\textwidth]{figs/mem_gen_peak_acc.pdf}
    \caption{Phase transition in representation learning} 
    \end{subfigure}
    \end{center}
\vspace{-0.1in}
\caption{\textbf{Better representations are learned in the generalization regime.} We train EDM-based \citep{karras2022elucidating} diffusion models on the CIFAR-10 dataset using different training dataset sizes, ranging from $2^6$ to $2^{15}$. (a) The change in the generalization score \citep{zhang2024emergence} as the dataset size increases, where regions with a generalization score close to $0$ are labeled as the memorization regime, and those close to $1$ are labeled as the generalization regime. (b) The peak representation learning feature accuracy achieved as a function of dataset size.}
\label{fig:phase_transit}
\end{figure*}

% \paragraph{Dynamics of learned solution magnitude}
% \ZK{Probably not needed anymore?}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{example-image}
%     \caption{A wrapfigure: left-DM vs DAE vs DAE tuned (vs DM tuned?), right-weight norm of dae with different noise scale}
%     \label{fig:mog_optimization}
% \end{figure}



\paragraph{Posterior quality decides feature quality}\label{subsec:posterior}
% \xiao{Use posterior image in appendix to demonstrate that posterior quality directly reflects feature quality, motivate the next section}
% \xiao{Do we need this?}

\begin{figure*}[t]
    \begin{center}
    \begin{subfigure}{0.98\textwidth}
    \includegraphics[width = 0.985\textwidth]{figs/noise_image_posterior.pdf}
    \caption{$\hat{\bm x}_{\bm \theta}(\bm x_t, t)$: Posterior estimation using \textbf{noise image} as inputs.} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.98\textwidth}
    \includegraphics[width = 0.985\textwidth]{figs/clean_image_posterior.pdf}
    \caption{$\hat{\bm x}_{\bm \theta}(\bm x_0, t)$: Posterior estimation using \textbf{clean image} as inputs.} 
    \end{subfigure}
    \end{center}
    \vspace{-0.1in}
\caption{\textbf{Using clean images as inputs improves posterior estimation quality.} We use a pre-trained DDPM diffusion model on CIFAR10 to visualize posterior estimation for clean inputs and noisy inputs across varying noise scales $\sigma_t$. Clean inputs produce smooth and descriptive estimations even at high noise levels, whereas noisy inputs result in blurry and lossy estimations at large $\sigma_t$, making it difficult to extract meaningful representations.}
\label{fig:clean_vs_noise}
\end{figure*}

Diffusion models $\hat{\bm{x}}_{\bm{\theta}}$ are trained to perform posterior estimation at a given time step $t$ using corrupted inputs, with the features for representation learning emerging as an intermediate byproduct of this process. This leads to a natural conjecture:
\emph{changes in feature quality should directly correspond to changes in posterior estimation quality.}

% \textcolor{orange}{(Zekai: I suggest to put this in a rect box)}


% \textcolor{orange}{(Zekai: while posterior estimation can also be seen as the last-layer representaion somehow)}. 
To test this hypothesis, we visualize the posterior estimation results for clean inputs ($\hat{\bm{x}}_{\bm{\theta}}(\bm{x}_0, t)$) and noisy inputs ($\hat{\bm{x}}_{\bm{\theta}}(\bm{x}_t, t)$) across varying noise scales $\sigma_t$ in \Cref{fig:clean_vs_noise}. The results reveal that, similar to the findings for feature representation, clean inputs yield superior posterior estimation compared to corrupted inputs, with the performance gap widening as the noise scale increases. Furthermore, as illustrated in the \Cref{fig:use_clean}, if we consider posterior estimation as the last-layer features and directly use it for classification, the accuracy curve reveals a unimodal trend as noise level progresses, similar to the behavior observed in feature classification accuracy. These findings strongly validate the conjecture. 
% \textcolor{orange}{(Zekai: only validating the conjecture with current figure 2 is too vague... Maybe we should refer to the x0 acc vs fearure acc/peak shift part in following sections?)}

Building on this insight, we use posterior estimation as a proxy to analyze the dynamics of diffusion-based representation learning in \Cref{sec:main}. Moreover, since the unimodal representation dynamic persists across different network architectures and feature extraction layers, analyzing posterior estimation also enables us to study the problem without relying on specific architectural or layer-dependent assumptions.

\paragraph{Additional representation learning experiments on DDPM.}
Apart from EDM and DDPM* models pre-trained using the framework proposed by \citep{karras2022elucidating}, we also experiment with the features extracted by classic DDPM models \citep{ho2020denoising} to make sure the observations do not depend on the specific training framework. We use the same groups of noise levels and also test using clean or noisy images as input to extract features at the bottleneck layer, and then conduct the linear probe. The DDPM models we use are trained on the Flowers-102 \citep{nilsback2008automated} and the CIFAR10 dataset accordingly. Different from the framework proposed by \citep{karras2022elucidating}, the input to the classic DDPM model is the same as the input to the UNet inside. Therefore, we calculate the scaling factor $\sqrt{\bar{\alpha}_t} = 1/\sqrt{\sigma^2(t)+1}$, and use $\sqrt{\bar{\alpha}_t} \bm x_0$ as the clean image input. Besides, for noisy input, we set $\bm x_t = \sqrt{\bar{\alpha}_t} (\bm x_0 + \bm n)$, with $\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$. The linear probe results are presented in \Cref{fig:ddpm_actual}, where we consistently see an unimodal curve, as well as compatible or even superior representation learning performance of clean input $\bm x_0$.

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.42\linewidth]{figs/teaser_edm.pdf}
    \includegraphics[width=0.48\linewidth]{figs/app_actual_ddpm.pdf}
    \caption{\textbf{Performance comparison: clean vs. noisy inputs.} We use pre-trained DDPM/EDM model on the CIFAR10/CIFAR100 datasets and Flowers-102 \citep{nilsback2008automated}. The feature probing accuracy is plotted to compare the performance when using clean versus noisy inputs.}
    \label{fig:ddpm_actual}
\end{figure}


\paragraph{Extend \CSNR~on feature representations.}
\begin{figure*}[h]
\centering
\includegraphics[width=0.52\textwidth]{figs/mog_f_csnr.pdf}
\caption{\textbf{Dynamics of feature probing accuracy and associated \CSNR~on \MoLRG~data} Feature probing accuracy is plotted alongside $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$. Probing accuracy is evaluated on the test set, while the empirical \CSNR~is computed from the training set. Both exhibit an aligning unimodal pattern.} \label{fig:mog_2}
\end{figure*}

\begin{figure*}[h]
\begin{center}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/feature_curve_c10.pdf}
    \caption{CIFAR10} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/feature_curve_mini.pdf}
    \caption{MiniImageNet} 
    \end{subfigure}

    \end{center}
\caption{\textbf{Dynamics of feature probing accuracy and associated \CSNR~on CIFAR10 and MiniImageNet.} Feature probing accuracy is plotted alongside $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$. Probing accuracy is evaluated on the test set, while the empirical \CSNR~is computed from the training set. Both exhibit an aligning unimodal pattern.}
\label{fig:cifar_2}
\end{figure*}

In the main body of the paper, we define \CSNR~with respect to the posterior estimation function. Given that the intermediate representations of diffusion models exhibit near-linear properties \citep{kwon2022diffusion}, we extend the definition of \CSNR~to feature extraction functions:
\begin{align*}
    \mathrm{CSNR}(t, f_{\bm \theta}) := \E_k \left[\frac{\E_{\bm x_0}[\|\hat{\bm U}_k\hat{\bm U}_k^Tf_{\bm \theta}(\bm x_0, t)\|^2 \mid k]}{\E_{\bm x_0}[\sum_{l\neq k}\|\hat{\bm U}_l\hat{\bm U}_l^Tf_{\bm \theta}(\bm x_0, t)\|^2 \mid k]} \right]
\end{align*}

Here, $f_{\bm \theta}(\cdot, t)$ represents a diffusion feature extraction function that includes all layers up to the feature extraction layer of a diffusion model. The matrix $\hat{\bm U}_k$ denotes the extracted basis corresponding to the correct class of the features, while $\hat{\bm U}_l (l \neq k)$ represents the bases of incorrect classes. 

We validate this extension of \CSNR~as a measure of feature representation quality. Using the same models from \Cref{fig:csnr_molrg_match} and \Cref{fig:cifar}, we extract intermediate features at each time step and evaluate classification performance on the test set via linear probing. For \CSNR~calculation, we compute the basis using features extracted at each time step and subsequently calculate \CSNR~for the extracted features, denoted as $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$. The results are presented in \Cref{fig:mog_2} and \Cref{fig:cifar_2} for synthetic and real datasets, respectively. As shown in the plots, $\mathrm{CSNR}(f_{\bm \theta},t)$ consistently follows a unimodal pattern, mirroring the trend of feature probing performance as the noise scale increases.



\paragraph{Validation of $\hat{\bm x}_{\textit{approx}}^{\star}$ approximation in \Cref{app:thm1_proof}.}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.23\linewidth]{figs/assump/K2.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/K3.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/K5.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/K10.pdf}

    \includegraphics[width=0.23\linewidth]{figs/assump/d2.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/d3.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/d5.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/d10.pdf}

    \includegraphics[width=0.23\linewidth]{figs/assump/b1.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/b2.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/b3.pdf}
    \includegraphics[width=0.23\linewidth]{figs/assump/b6.pdf}
    \caption{\textbf{Comparison between \CSNR~calculated using the optimal model $\hat{\bm x}_{\bm \theta}^{\star}$ and the \CSNR~calculated with our approximation in \Cref{lem:main}.} We generate \MoLRG~data and calculate \CSNR~using both the corresponding optimal posterior function $\hat{\bm x}_{\bm \theta}^{\star}$ and our approximation $\hat{\bm x}_{\textit{approx}}^{\star}$ from \Cref{lem:main}. Default parameters are set as $n=50$, $d=5$, $K=3$, and $\delta=0.1$. In each row, we vary one parameter while keeping the others fixed, comparing the actual and approximated \CSNR.}
    \label{fig:assump_validate}
\end{figure}

In \Cref{lem:main_formal}, we approximate the optimal posterior estimation function $\hat{\bm x}_{\bm \theta}^{\star}$ using $\hat{\bm x}_{\textit{approx}}^{\star}$ by taking the expectation inside the softmax with respect to $\bm x_0$. To validate this approximation, we compare the \CSNR~calculated from $\hat{\bm x}_{\bm \theta}^{\star}$ and from $\hat{\bm x}_{\textit{approx}}^{\star}$ using the definition in \Cref{lem:E[x_0]_multi} and (\ref{eq:approx_score}) in \Cref{app:thm1_proof}, respectively. We use a fixed dataset size of $2400$ and set the default parameters to $n=50$, $d=5$, $K=3$, and $\delta=0.1$ to generate \MoLRG~data. We then vary one parameter at a time while keeping the others constant, and present the computed \CSNR~in \Cref{fig:assump_validate}. As shown, the approximated \CSNR~score consistently aligns with the actual score.

\paragraph{Visualization of the \MoLRG~posterior estimation and \CSNR~across noise scales.} In \Cref{fig:csnr_molrg_match}, we show that both the posterior classification accuracy and \CSNR~exhibit a unimodal trend for the \MoLRG~data. To further illustrate this behavior, we provide a visualization of the posterior estimation and \CSNR~at different noise scales in \Cref{fig:vis_molrg_3class}. In the plot, each class is represented by a colored straight line, while deviations from these lines correspond to the $\delta$-related noise term. Initially, increasing the noise scale effectively cancels out the $\delta$-related data noise, resulting in a cleaner posterior estimation and improved probing accuracy. However, as the noise continues to increase, the class confidence rate drops, leading to an overlap between classes, which ultimately degrades the feature quality and probing performance.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/vis_molrg.pdf}
%     \vspace{-0.1in}
%     \caption{\textbf{Visualization of posterior estimation for a clean input, higher \CSNR~correspondings to higher classification accuracy.} The \textbf{same} \MoLRG~data is fed into the models; each row represents a different denoising model, and each column corresponds to a different time step with noise scale ($\sigma_t$). The \textcolor{red}{red} box indicates the best posterior estimation and feature probing accuracy.}
%     \label{fig:vis_molrg_3class}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/vis_molrg_v2.pdf}
    \vspace{-0.1in}
    \caption{\textbf{Visualization of posterior estimation for a clean input, higher \CSNR~correspondings to higher classification accuracy.} The \textbf{same} \MoLRG~data is fed into the models; each row represents a different denoising model, and each column corresponds to a different time step with noise scale ($\sigma_t$). The \textcolor{c1}{teal} box indicates the best generation quality and The \textcolor{c2}{red} box indicates the best representation quality. }
    \label{fig:vis_molrg_3class}
\end{figure}

%The {\color{c1} curve with pentagon markers} demonstrates the transition from fine to coarse granularity in posterior estimation as noise levels increase, corresponding to the monotonic rise in FID. In contrast, the {\color{c2} curve with square markers} reveals an unimodal trend in posterior classification accuracy, achieving peak performance at intermediate noise levels. This occurs when high-level details are filtered out while essential low-level semantic information is preserved, as illustrated by the posterior estimations according to different noise levels shown at the bottom of the figure.

\paragraph{Mitigating the performance gap between DAE and diffusion models.}
Throughout the empirical results presented in this paper, we consistently observe a performance gap between individual DAEs and diffusion models, especially in low-noise regions. Here, we use a DAE trained on the CIFAR10 dataset with a single noise level $\sigma = 0.002$, using the NCSN++ architecture \citep{karras2022elucidating}. In the default setting, the DAE achieves a test accuracy of $32.3$. We then explore three methods to improve the test performance: (a) adding dropout, as noise regularization and dropout have been effective in preventing autoencoders from learning identity functions \citep{steck2020autoencoders}; (b) adopting EDM-based preconditioning during training, including input/output scaling, loss weighting, etc.; and (c) multi-level noise training, in which the DAE is trained simultaneously on three noise levels $[0.002, 0.012, 0.102]$. Each modification is applied independently, and the results are reported in \Cref{tab:dae_trial}. As shown, dropout helps improve performance, but even with a dropout rate of $0.95$, the improvement is minor. EDM-based preconditioning achieves moderate improvement, while multi-level noise training yields the most promising results, demonstrating the benefit of incorporating the diffusion process in DAE training.

\begin{table}[t]
\caption{\textbf{Improve DAE representation performance at low noise region.} A vanilla DAE trained on the CIFAR-10 dataset with a single noise level of $\sigma = 0.002$ serves as the baseline. We evaluate the performance improvement of dropout regularization, EDM-based preconditioning, and multi-level noise training ($\sigma=\{0.002, 0.012, 0.102\}$). Each technique is applied independently to assess its contribution to performance enhancement.}
\centering
\resizebox{0.42\linewidth}{!}{
        \vspace{-0.1in}
	\begin{tabular}	{c | c } 
            Modifications & Test acc.  \\
            \toprule
            Vanilla DAE & $32.3$ \\
            +Dropout (0.5) & $35.3$ \\
            +Dropout (0.9) & $36.4$ \\
            +Dropout (0.95) & $38.1$ \\
            +EDM preconditioning & $49.2$ \\
            +Multi-level noise training & \textbf{58.6} \\
		  \bottomrule
	\end{tabular}}
    \label{tab:dae_trial}
\end{table}


% {\color{blue}

% \paragraph{Additional experiment on ImageNet \citep{deng2009imagenet}.} We use a DiT-XL \citep{peebles2023scalable} pre-trained on ImageNet and follow the settings in previous work \citep{chen2024deconstructing}. Specifically, we extract features from the 1/2-L layer, treated as the bottleneck layer, and apply mean-pooling over all tokens for linear probing. Due to computational constraints, we limit our analysis to the 100 class labels used in miniImageNet \citep{vinyals2016matching}. For computing \CSNR~, since DiT employs latent diffusion, we compute \CSNR~on the latent space. This involves first passing the images through a VAE to obtain the latent representation, flattening the output, and then computing the basis from the SVD of the flattened latent vectors. As shown in \Cref{fig:imagenet_dit}, both the feature probing accuracy and \CSNR~exhibit a similar curve, consistent with findings on other datasets and network architectures discussed in the main body of the paper.

% \begin{figure}[t]
%     \begin{center}
%     \includegraphics[width = 0.485\textwidth]{figs/dit_imagenet.pdf}
%     \end{center}
% \caption{{\color{blue}\textbf{Additional results on ImageNet.} We employ a pre-trained DiT diffusion model on ImageNet. Intermediate features used for linear probing are extracted from the 1/2-L layer. The \CSNR~metric is computed on the latent codes of clean images, obtained after processing them through a VAE.}}
% \label{fig:imagenet_dit}
% \end{figure}


% \paragraph{Additional experiments on \MoLRG~data.}
% \begin{figure}[htbp]
%     \begin{center}
%     \includegraphics[width = 0.485\textwidth]{figs/rebut_csnr_mog.pdf}
%     \end{center}
% \caption{{\color{blue}\textbf{\CSNR~comparison on \MoLRG~data.} Using the same model and data as in \Cref{fig:ucurve_molrg3class}, we plot the \CSNR~results for the (tuned) DAEs and (tuned) diffusion model. Solid lines represent normal training results (as in \Cref{fig:ucurve_molrg3class}), while dashed lines indicate results with more nuanced optimization strategy for improved performance.}}
% \label{fig:rebut_csnr}
% \end{figure}

% % When the model is fully optimized, the DAE \CSNR~(orange dashed curve) approximate optimal \CSNR~(green curve), indicating the DAE has converged to the optimal denoiser. However, for diffusion since the model capacity is limited, even we use careful optimization, the gap (between blue dashed curve and green curve) still exist.
% In \Cref{fig:ucurve_molrg3class}, we can observe that \CSNR~of both the learned DAE and diffusion model have a gap to the optimal \CSNR~. We hypothesize that the discrepancy between the trained network and the optimal solution may arise from the following two factors:

% \begin{itemize}
%     \item \textbf{Network Capacity.} A single DAE is tailored to handle a specific noise scale, enabling its \CSNR~to closely align with the optimal \CSNR~across multiple noise scales. Conversely, the diffusion model must simultaneously accommodate all noise scales, which compromises its performance on individual noise scales. To test this hypothesis, we conducted an experiment in which we tuned the learning rate and extended the training duration to 1000 epochs. The results, shown in \Cref{fig:rebut_csnr}, reveal that while the tuned diffusion model outperforms its untuned counterpart, it still exhibits a substantial gap compared to the optimal \CSNR~, thus verifies the conjecture.

%     \item \textbf{Optimization Difficulty.} As described in Equation \eqref{eq:E_MoG}, the optimal posterior function requires projecting $\bm x_t$ onto different subspaces. At higher noise levels, the magnitude of this projection diminishes (since $\sigma_t$ appears only in the denominator), making optimization increasingly challenging. To explore this hypothesis, we employed more nuanced optimization strategies for the DAE models. These include increasing training epochs (from 200 to 1000), decreasing learning rates (from $1e^{-3}$ to $1e^{-4}$), and scaling down the initialization magnitude as the noise level increases. While these strategies effectively drive the \CSNR~closer to the optimal \CSNR~for small noise scales, a persistent gap remains at larger noise scales due to the enlarged optimization difficulty.
% \end{itemize}

% Furthermore, we conduct a preliminary study to investigate the potential of \CSNR~as a metric for model comparison by plotting the posterior probing accuracy and intermediate probing accuracy for the (tuned) DAEs and (tuned) diffusion models in \Cref{fig:rebut_molrg_acc}. As shown in the plot, \CSNR~is directly linked to posterior accuracy, with higher \CSNR~values correlating with improved posterior accuracy. Regarding to the feature probing accuracy, although comparing DAEs with diffusion models in this case is challenging due to the weight-sharing mechanism discussed in \Cref{subsec:weight_share}, we can still observe \CSNR~serves as a reliable metric for reflecting feature probing accuracy within the same model (e.g., tuned DAEs and diffusion models compared to their untuned counterparts)).


% \begin{figure}[htbp]
%     \begin{center}
%     \begin{subfigure}{0.48\textwidth}
%     \includegraphics[width = 0.985\textwidth]{figs/rebut_x0acc_mog.pdf}
%     \caption{Posterior probing accuracy} 
%     \end{subfigure} \quad %\hspace*{\fill}
%     \begin{subfigure}{0.48\textwidth}
%     \includegraphics[width = 0.985\textwidth]{figs/rebut_featureacc_mog.pdf}
%     \caption{Intermediate feature probing accuracy} 
%     \end{subfigure}
%     \end{center}
% \caption{{\color{blue}\textbf{Connection between \CSNR~and posterior/intermediate feature probing accuracy.} We use the DAE and diffusion models trained for \Cref{fig:rebut_csnr} and plot the corresponding posterior and intermediate feature probing accuracy.}}
% \label{fig:rebut_molrg_acc}
% \end{figure}

% }


\subsection{Experimental Details}\label{app:exp_detail}
In this section, we provide technical details for all the experiments in the main body of the paper. 

\paragraph{Experimental details for \Cref{fig:clean_feature}.}
\begin{itemize}
    \item \textit{Experimental details for \Cref{fig:clean_feature}(a).} We train diffusion models based on the unified framework proposed by \citep{karras2022elucidating}. Specifically, we use the DDPM+ network, and use VP configuration for \Cref{fig:clean_feature}(a). \citep{karras2022elucidating} has shown equivalence between VP configuration and the traditional DDPM setting, thus we call the models in \Cref{fig:use_clean}(a) as DDPM* models. We train two models on CIFAR10 and CIFAR100, respectively. After training, we conduct linear probe on CIFAR10 and CIFAR100. At a specific noise level $\sigma(t)$, we either use clean image $\bm x_0$ or noisy image $\bm x_t = \bm x_0 + \bm n$ as input to the DDPM* models for extracting features after the '8x8\_block3' layer. Here, $\bm n$ represents random noise and $\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$. We train a logistic regression on features in the train split and report the classification accuracy on the test split of the dataset. We perform the linear probe for each of the following noise levels: [0.002, 0.008, 0.023, 0.060, 0.140, 0.296, 0.585, 1.088, 1.923, 3.257].

    \item \textit{Experimental details for \Cref{fig:clean_feature}(b).}  We exactly follow the protocol in \citep{baranchuk2021label}, using the same datasets which are subsets of CelebA \citep{karras2018progressive} and FFHQ \citep{karras2019style}, the same training procedure, and the same segmentation networks (MLPs). The only difference is that we use a newer latent diffusion model \citep{rombach2022high} pretrained on CelebAHQ from Hugging Face and the noise are added to the latent space. For feature extraction we concatenate the feature from the first layer of each resolution in the UNet's decoder (after upsampling them to the same resolution as the input). We perform segmentation for each of the following noise levels:[0.015, 0.045, 0.079, 0.112, 0.176, 0.342, 0.724, 2.041].
\end{itemize}

\paragraph{Experimental details for \Cref{fig:use_clean}.}
We use a pre-trained EDM CIFAR10 model from the official GitHub repository \citep{karras2022elucidating} and extract 10 sets of posterior estimations corresponding to $\sigma_t$ values ranging from 0.002 to 8.401. MLP probing is applied to these posterior estimations to evaluate posterior accuracy, and FID \citep{heusel2017gans} is computed relative to the original CIFAR10 dataset. For the posterior visualizations in the bottom figure, we randomly select a sample and display its posterior estimations according to the same $\sigma_t$ schedule.

% \paragraph{Experimental details for \Cref{fig:use_clean} (a)-(b).}
% We utilize a minimal implementation of the original DDPM model from an online public repository \citep{ddpm_repo}, consisting of a 12-layer UNet (including input/output embedding layers), and train it on the CIFAR10 dataset with $T=1000$ time steps for 200 epochs with an AdamW optimizer and learning rate $1\times 10^{-4}$. Features are extracted as 512-dimensional vectors from the output of the 7th layer (i.e., the bottleneck layer) at time steps [1, 5, 10, 20, 30, 40, 60, 80, 100, 200, 400, 500, 600], each corresponding to a specific $\sigma_t$ ranging from 0.01 to 6.17. Linear probing is applied to the extracted features, as in \citep{xiang2023denoising}, to plot the feature probing accuracy curve in \Cref{fig:use_clean}(a). For the posterior estimation ($\bm x_\theta(\bm x_0, t)$) probing accuracy curve, also shown in \Cref{fig:use_clean}(a), we use a two-layer MLP probe with ReLU activation. The estimated posterior at these time steps is visualized in \Cref{fig:use_clean}(b). 

% \paragraph{Experimental details for \Cref{fig:use_clean} (c)-(d).}
% We train diffusion models based on the unified framework proposed by \citep{karras2022elucidating}. Specifically, we use the DDPM+ network, and use EDM configuration for \Cref{fig:use_clean} (c) while taking VP configuration \Cref{fig:use_clean} (d). \citep{karras2022elucidating} has shown equivalence between VP configuration and the traditional DDPM setting, thus we call the models in \Cref{fig:use_clean} (d) as DDPM* models. For each of EDM and VP configuration, we train two models on CIFAR10 and CIFAR100, respectively. After training, we conduct linear probe on CIFAR10 and CIFAR100. At a specific noise level $\sigma(t)$, we either use clean image $\bm x_0$ or noisy image $\bm x_t = \bm x_0 + \bm n$ as input to the EDM or the DDPM* models for extracting features after the '8x8\_block3' layer. Here, $\bm n$ represents random noise and $\boldsymbol{n} \sim \mathcal{N}\left(\mathbf{0}, \sigma(t)^2 \mathbf{I}\right)$. We train a logistic regression on features in the train split and report the classification accuracy on the test split of the dataset. We perform the linear probe for each of the following noise levels: [0.002, 0.008, 0.023, 0.060, 0.140, 0.296, 0.585, 1.088, 1.923, 3.257].
% using 4 A40 GPUS , 5.315, 8.401, 12.910, 19.352, 28.375, 40.786, 57.586

\paragraph{Experimental details for \Cref{fig:csnr_molrg_match} and \Cref{fig:vis_molrg_3class}.}
For the \MoLRG~experiments, we train a 3-layer MLP with ReLU activation and a hidden dimension of 1024, following the setup provided in an open-source repository \citep{tiny_diffusion_repo}. The MLP is trained for 200 epochs using DDPM scheduling with $T=500$, employing the Adam optimizer with a learning rate of $5 \times 10^{-4}$. For feature extraction, we use the activations of the second layer of the MLP (dimension 1024) as intermediate features for linear probing. For \CSNR~computation, we follow the definition in \Cref{subsec:rep-quality} since we have access to the ground-truth basis for the \MoLRG~data, i.e., $\bm U_1, \bm U_2$, and $\bm U_3\in \mathbb{R}^{50\times 15}$ .  For probing we simply train a linear probe on the posterior and estimations, noting that we take the absolute value of the posterior estimations before feeding them to the probe. 

For both panels in \Cref{fig:csnr_molrg_match}, we train our probe the same training set used for diffusion and test on five different \MoLRG~datasets generated with five different random seeds, reporting the average accuracy and \CSNR~at time steps [1, 5, 10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 240, 260, 280]. In \Cref{fig:vis_molrg_3class}, we switch to a 3-class \MoLRG~with $d=1, n=10, \delta=0.3$ and visualize the posterior estimations at time steps [1, 20, 80, 200, 260] by projecting them onto the union of $\bm U_1, \bm U_2$, and $\bm U_3$ (a 3D space), then further projecting onto the 2D plane by a random $3 \times 2$ matrix with orthonormal columns. The subtitles of each visualization show the corresponding \CSNR~calculated as explained above.

\paragraph{Experimental details for \Cref{fig:cifar}.}
We use pre-trained EDM models \citep{karras2022elucidating} for CIFAR10 and ImageNet, extracting feature representations from the best-performing layer and posterior estimations as the network outputs at each timestep . For the ImageNet model, features are extracted using images and classes from MiniImageNet \citep{vinyals2016matching}. Feature accuracy is evaluated via linear probing, while posterior accuracy is assessed using a two-layer MLP, where posterior estimations pass through a linear layer and ReLU activation before the final linear classifier. The bases for the \CSNR~metric on features are computed via singular value decomposition (SVD) on feature representations at each timestep for each class, followed by \CSNR~calculation using its definition in \Cref{subsec:rep-quality}. For posterior estimation, we directly use bases $\bm U_k$ derived from raw dataset images. In all cases, the first $5$ right singular vectors of each class are used to extract $\bm U_k$.
% ; we didn't enforce orthogonality on the class basis because of the discrepancy between real data and the \MoLRG~data.

\paragraph{Experimental details for \Cref{fig:dae_diffusion}.}
We train individual DAEs using the DDPM++ network and VP configuration outlined in \citet{karras2022elucidating} at the following noise scales: 
\begin{align*}
    [0.002, 0.008, 0.023, 0.06, 0.14, 0.296, 0.585, 1.088, 1.923, 3.257].
\end{align*} 
Each model is trained for 500 epochs using the Adam optimizer \citep{kingma2014adam} with a fixed learning rate of $1 \times 10^{-4}$. For the diffusion models, we reuse the model from \Cref{fig:use_clean}(d). The sliced Wasserstein distance is computed according to the implementation described in \citet{doan2024assessing}.

\paragraph{Experimental details for \Cref{fig:phase_transit_2} and \Cref{fig:phase_transit}.}
We use the DDPM++ network and VP configuration to train diffusion models\citep{karras2022elucidating} on the CIFAR10 dataset, using two network configurations: UNet-$64$ and UNet-$128$, by varying the embedding dimension of the UNet. Training dataset sizes range exponentially from $2^6$ to $2^{15}$. For each dataset size, both UNet-$64$ and UNet-$128$ are trained on the same subset of the training data. All models are trained with a duration of $50$K images following the EDM training setup. After training, we calculate the generalization score as described in \citet{zhang2024emergence}, using $10$K generated images and the full training subset to compute the score.

\paragraph{Experimental details for \Cref{tab:ensemble_results} and \Cref{tab:ensemble_results_transfer}}

For EDM, we use the official pre-trained checkpoints on ImageNet $64\times64$ from \citep{karras2022elucidating}, and for DiT, we use the released DiT-XL/2 model pre-trained on ImageNet $256\times256$ from \citep{peebles2023scalable}. As a baseline, we include the Hugging Face pre-trained MAE encoder (ViT-B/16) \citep{he2022masked}.

For diffusion models, features are extracted from the layer and timestep that achieve the highest probing accuracy, following \citep{xiang2023denoising}. After feature extraction, we adopt the probing protocol from \citep{chen2024deconstructing}, passing the extracted features through a probe consisting of a BatchNorm1d layer followed by a linear layer. To ensure fair comparisons, all input images are cropped or resized to $224\times224$, matching the resolution used for MAE training.

For ensembling, we extract features from two additional timesteps on either side of the optimal timestep. Independent probes are trained on these timesteps, yielding five probes in total. At test time, we apply a soft-voting ensemble by averaging the output logits from all five probes for the final prediction. Specifically, let $\bm W_t \in \mathbb{R}^{K \times d}$ be the linear classifier trained on features from timestep $t$, and let $\bm h_t \in \mathbb{R}^{d}$ denote the feature representation of a sample at timestep $t$. Considering neighboring timesteps $t-2, t-1, t+1$, and $t+2$, our ensemble prediction is computed as:
\begin{align*} 
    \bm \hat{y} = \argmax\left(\frac{1}{5}\sum_{t=t-2}^{t+2} \bm W_t \bm h_t\right). 
\end{align*}

We evaluate each method under varying levels of label noise, ranging from $0\%$ to $80\%$, by randomly mislabeling the specified percentage of training labels before applying linear probing. Performance is assessed on both the pre-training dataset and downstream transfer learning tasks. For pre-training evaluation, we use the images and classes from MiniImageNet \citep{vinyals2016matching} to reduce computational cost. For transfer learning, we evaluate on CIFAR100 \citep{krizhevsky2009learning}, DTD \citep{cimpoi14describing}, and Flowers102 \citep{nilsback2008automated}.


\subsection{Proofs}\label{app:proofs}

\subsubsection{Proof of \Cref{lem:E[x_0]_multi}}\label{app:proof_prop}
\begin{proof}
    We follow the same proof steps as in \citep{wang2024diffusion} Lemma $1$ with a change of variable. Let $\bm c_k = \begin{bmatrix}
        \bm a_k \\
        \bm e_k
    \end{bmatrix}$ and $\widetilde{\bm U_k} = \begin{bmatrix}
        \bm U_k & \delta \bm U_k^{\perp}
    \end{bmatrix}$, we first compute
    \begin{align*}
        \;\;\;& p_t(\bm x \vert Y = k) \\
        & = \int  p_t\left(\bm x \vert Y = k, \bm c_k) \mathcal{N}(\bm c_k; \bm 0, \bm I_{d+D}\right) {\rm d}\bm c_k \\
        &= \int p_t (\bm x \vert \bm x_0 = \widetilde{\bm U_k} \bm c_k) \mathcal{N}\left(\bm c_k; \bm 0, \bm I_{d + D}\right) {\rm d}\bm c_k \\
        & = \int \mathcal{N}(\bm x; s_t\widetilde{\bm U_k} \bm c_k, \gamma_t^2\bm I_n) \mathcal{N}\left(\bm c_k; \bm 0, \bm I_{d+D}\right) {\rm d}\bm c_k \\
        & = \frac{1}{(2\pi)^{n/2}(2\pi)^{(d+D)/2}\gamma_t^n} \int \exp\left(-\frac{1}{2\gamma_t^2}\|\bm x - s_t \widetilde{\bm U_k} \bm c_k\|^2 \right)\exp\left( -\frac{1}{2}\|\bm c_k\|^2 \right) {\rm d}\bm c_k \\
        & = \frac{1}{(2\pi)^{n/2}(2\pi)^{(d+D)/2}\gamma_t^n} \int \exp\left(-\frac{1}{2\gamma_t^2}\left(\bm x^T \bm x - 2s_t \bm x^T \widetilde{\bm U_k} \bm c_k + s_t^2 \bm c_k^T \widetilde{\bm U_k}^T \widetilde{\bm U_k} \bm c_k + \gamma_t^2 \bm c_k^T \bm c_k \right) \right){\rm d}\bm c_k \\
        & =  \frac{1}{(2\pi)^{n/2}\gamma_t^n}\left( \frac{s_t^2 + \gamma_t^2}{\gamma_t^2} \right)^{-d/2} \left( \frac{s_t^2 \delta^2 + \gamma_t^2}{\gamma_t^2} \right)^{-D/2}  \exp\left( -\frac{1}{2\gamma_t^2} \bm x^T\left(\bm I_n - \frac{s_t^2}{s_t^2 + \gamma_t^2} \bm U_k \bm U_k^{T} -\frac{s_t^2 \delta^2}{s_t^2\delta^2 + \gamma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \right)\bm x \right) \\
            &\ \int \frac{1}{(2\pi)^{d/2}} \left( \frac{\gamma_t^2}{s_t^2 + \gamma_t^2} \right)^{-d/2}\exp\left(-\frac{s_t^2 + \gamma_t^2}{2\gamma_t^2}\left\|\bm a_k - \frac{s_t}{s_t^2 + \gamma_t^2}\bm U_k^{T}\bm x\right\|^2 \right) {\rm d}\bm a_k\\
            &\ \int \frac{1}{(2\pi)^{D/2}} \left( \frac{\gamma_t^2}{s_t^2\delta^2 + \gamma_t^2} \right)^{-D/2}\exp\left(-\frac{s_t^2\delta^2 + \gamma_t^2}{2\gamma_t^2}\left\|\bm e_k - \frac{s_t \delta}{s_t^2 \delta^2 + \gamma_t^2}\bm U_k^{\perp T}\bm x\right\|^2 \right) {\rm d}\bm e_k\\
        & = \frac{1}{(2\pi)^{n/2}}\frac{1}{(s_t^2 + \gamma_t^2)^{d/2}(s_t^2\delta^2 + \gamma_t^2)^{D/2}}\exp\left(-\frac{1}{2\gamma_t^2}\bm x^T\left(\bm I_n - \frac{s_t^2}{s_t^2+\gamma_t^2} \bm U_k \bm U_k^{T} - \frac{s_t^2 \delta^2}{s_t^2 \delta^2 +\gamma_t^2} \bm U_k^{\perp} \bm U_k^{\perp T} \right)\bm x \right) \\
        & = \frac{1}{(2\pi)^{n/2}\det^{1/2}(s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)}\\
        &\;\;\;\;\;\exp\left( -\frac{1}{2}\bm x^T\left(s_t^2\bm U_k\bm U_k^{T} +s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T}  + \gamma_t^2\bm I_n \right)^{-1}\bm x \right) \\
        & = \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n),
    \end{align*}
where we repeatedly apply the pdf of multi-variate Gaussian and the second last equality uses $\det(s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n) = (s_t^2 + \gamma_t^2)^{d}(s_t^2\delta^2 + \gamma_t^2)^{D}$ and $(s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)^{-1} = \left(\bm I_n - s_t^2/(s_t^2+\gamma_t^2) \bm U_k\bm U_k^{T} - s_t^2\delta^2/(s_t^2\delta^2+\gamma_t^2) \bm U_k^{\perp}\bm U_k^{\perp T} \right)/\gamma_t^2$ because of the Woodbury matrix inversion lemma. Hence, with $\P{Y = k} = \pi_k$ for each $k \in [K]$, we have
    \begin{align*}
        p_t(\bm x) & = \sum_{k=1}^K p_{t}(\bm x\vert Y = k) \mathbb{P}(Y = k) = \sum_{k=1}^K \pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n).
    \end{align*}

Now we can compute the score function
\begin{align*}
    \nabla \log p_t(\bm x) & = \frac{\nabla p_t(\bm x)}{p_t(\bm x)} =  \frac{\splitfrac{\sum_{k=1}^K \pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)}{\left( -\frac{1}{\gamma_t^2}\bm x + \frac{s_t^2}{\gamma_t^2(s_t^2 + \gamma_t^2)} \bm U_k\bm U_k^{T}  \bm x + \frac{s_t^2 \delta^2}{\gamma_t^2(s_t^2\delta^2 + \gamma_t^2)} \bm U_k^{\perp}\bm U_k^{\perp T}  \bm x  \right)}}{\sum_{k=1}^K \pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)} \\
    & = -\frac{1}{\gamma_t^2} \left( \bm x -\frac{\splitfrac{\sum_{k=1}^K\pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)}{\left( \frac{s_t^2}{s_t^2 + \gamma_t^2} \bm U_k\bm U_k^{T}  \bm x ) + \frac{s_t^2 \delta^2}{s_t^2 \delta^2 + \gamma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T}  \bm x ) \right)}}{\sum_{k=1}^K \pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2\delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)}  \right). 
\end{align*}

    According to Tweedie's formula, we have
    \begin{align*}
        \E\left[\bm x_0 \vert \bm x_t \right] & = \frac{\bm x_t + \gamma_t^2 \nabla \log p_t(\bm x_t)}{s_t}\\
        &= \frac{s_t}{s_t^2 + \gamma_t^2} \frac{\sum_{k=1}^K\pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n) \bm U_k\bm U_k^{T}  \bm x }{\mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)} \\ 
        &\;\;+ \frac{s_t\delta^2}{s_t^2\delta^2 + \gamma_t^2} \frac{\sum_{k=1}^K\pi_k \mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n) \bm U_k^{\perp}\bm U_k^{\perp T}  \bm x }{\mathcal{N}(\bm x; \bm 0, s_t^2\bm U_k\bm U_k^{T} + s_t^2 \delta^2\bm U_k^{\perp}\bm U_k^{\perp T} + \gamma_t^2\bm I_n)} \\
        & = \dfrac{s_t}{s_t^2 + \gamma_t^2}  \frac{\sum_{k=1}^K \pi_k \exp\left(\phi_t \|\bm U_k^{T}\bm x_t\|^2\right)\exp\left(\psi_t \|\bm U_k^{\perp T}\bm x_t\|^2\right) \bm U_k\bm U_k^{T} \bm x_t}{\sum_{k=1}^K \pi_k \exp\left(\phi_t \|\bm U_k^{T}\bm x_t\|^2\right)\exp\left(\psi_t \|\bm U_k^{\perp T}\bm x_t\|^2\right)} \\
        &\;\; + \dfrac{s_t \delta^2}{s_t^2 \delta^2 + \gamma_t^2}  \frac{\sum_{k=1}^K \pi_k \exp\left(\phi_t \|\bm U_k^{T}\bm x_t\|^2\right)\exp\left(\psi_t \|\bm U_k^{\perp T}\bm x_t\|^2\right) \bm U_k^{\perp}\bm U_k^{\perp T} \bm x_t}{\sum_{k=1}^K \pi_k \exp\left(\phi_t \|\bm U_k^{T}\bm x_t\|^2\right)\exp\left(\psi_t \|\bm U_k^{\perp T}\bm x_t\|^2\right)},
    \end{align*}
    with $\phi_t = s_t^2 / (2 \gamma_t^2 (s_t^2 + \gamma_t^2))$ and $\psi_t = s_t^2\delta^2 / (2 \gamma_t^2 (s_t^2 \delta^2 + \gamma_t^2))$. The final equality uses the pdf of multi-variant Gaussian and the matrix inversion lemma discussed earlier.

    Now since $\pi_k$ is consistent for all $k$ and $s_t = 1$, we have
    \begin{align*}
        \E\left[ \bm x_0\vert \bm x_t\right] &=  \sum_{k=1}^K w^{\star}_k(\bm x_t) \left( \frac{1}{1 + \sigma_t^2} \bm U_k\bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \right) \bm x_t \\
        &\text{where}\ w^{\star}_k(\bm x_t) := \frac{\exp\left( \frac{1}{2\sigma_t^2 (1 + \sigma_t^2)} \|\bm U_k^T \bm x_t\|^2 + \frac{\delta^2}{2 \sigma_t^2 (\delta^2 + \sigma_t^2)} \| \bm U_k^{\perp T} \bm x_t \|^2 \right)}{\sum_{k=1}^K \exp\left( \frac{1}{2\sigma_t^2 (1 + \sigma_t^2)} \|\bm U_k^T \bm x_t\|^2 + \frac{ \delta^2}{2 \sigma_t^2 (\delta^2 + \sigma_t^2)} \| \bm U_k^{\perp T} \bm x_t \|^2 \right)}.
    \end{align*}
\end{proof}


\subsubsection{Proof of \Cref{lem:main}}\label{app:thm1_proof}
We first state the formal version of \Cref{lem:main}.

To simplify the calculation of \CSNR~as introduced in \Cref{subsec:rep-quality} on posterior estimation, which involves the expectation over the softmax term $w^{\star}_k$, we approximate $\hat{\bm x}_{\bm \theta}^{\star}$ as follows: 
% \qq{needs to specific what the expectation is taking w.r.t.?}

\begin{align}\label{eq:approx_score}
    \begin{split}
    \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) &= \sum_{k=1}^K \hat{w}_k \left( \frac{1}{1 + \sigma_t^2} \bm U_k\bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \right) \bm x , \\
    &\text{where}\ \hat{w}_k := \frac{\exp\left( \E_{\bm x_0} [g_k(\bm x_0, t)] \right)}{\sum_{k=1}^K \exp\left( \E_{\bm x_0} [g_k(\bm x_0, t)] \right)}.
    \end{split}
\end{align}

In other words, we use $\hat{w}_k$ in \eqref{eq:approx_score} to approximate $w^{\star}_k(\bm x_0)$
in \Cref{lem:E[x_0]_multi} by taking expectation inside the softmax with respect to $\bm x_0$. This allows us to treat $\hat{w}_k$ as a constant when calculating \CSNR, making the analysis more tractable while maintaining $\E[\|\bm U_l\bm U_l^T\hat{\bm x}_{\bm \theta}^{\star}(\bm x_0, t)\|^2] \approx \E[\|\bm U_l\bm U_l^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]$ for all $l \in [K]$. We verify the tightness of this approximation at \Cref{app:add_exp} (\Cref{fig:assump_validate}). With this approximation, we state the theorem as follows:

\begin{theorem}\label{lem:main_formal}
Let data $\bm x_0$ be any arbitrary data point drawn from the \MoLRG~distribution defined in Assumption \ref{assum:subspace} and let $k$ denote the true class $\bm x_0$ belongs to. Then \CSNR~introduced in \Cref{subsec:rep-quality} depends on the noise level $\sigma_t$ in the following form: 
    \begin{align}\label{eq:csnr_2}
        \mathrm{CSNR}(\hat{\bm x}_{\textit{approx}}^{\star},t) = \frac{1}{(K-1)\delta^2} \cdot\left(\frac{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_k, \delta)}{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_l, \delta)}\right)^2
    \end{align}
    
where $h(w, \delta) := (1 - \delta^2)w + \delta^2$. Since $\delta$ is fixed, $h(w,\delta)$ is a monotonically increasing function with respect to $w$. Note that here $\delta$ represents the magnitude of the fixed intrinsic noise in the data where $\sigma_t$ denotes the level of additive Gaussian noise introduced during the diffusion training process.

\end{theorem}

\begin{proof}
    Following the definition of \CSNR~as defined in \Cref{subsec:rep-quality}, \Cref{lem:decomp} and the fact that $k \sim \text{Mult}(K,\pi_k)$ with $\pi_1 = \dots = \pi_K = 1/K$, we can write
    \begin{align*}
        \mathrm{CSNR}(\hat{\bm x}_{\textit{approx}}^{\star},t) &= \frac{\E_{\bm x_0}[\|\bm U_k\bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]}{\E_{\bm x_0}[\sum_{l\neq k}\|\bm U_l\bm U_l^T\bm \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} = \frac{\E_{\bm x_0}[\|\bm U_k\bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]}{\sum_{l\neq k}\E_{\bm x_0}[\|\bm U_l\bm U_l^T\bm \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} \\
        &= \frac{\left( \frac{\hat{w}_k}{1 + \sigma_t^2} + \frac{(K-1)\delta^2\hat{w}_l}{\delta^2+\sigma_t^2}\right)^2 d}{(K-1)\left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2(\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d} \\
        &= \frac{1}{(K-1)\delta^2} \cdot \left(\frac{\hat{w}_k\delta^2 + \hat{w}_k\sigma_t^2 + (K-1)\delta^2 \hat{w}_l + (K-1)\delta^2 \hat{w}_l\sigma_t^2}{\hat{w}_l\delta^2 + \hat{w}_l\sigma_t^2 + \delta^2 \hat{w}_k + (K-2)\delta^2\hat{w}_l + \delta^2 \hat{w}_k \sigma_t^2 + (K-2)\delta^2\hat{w}_l\sigma_t^2}\right)^2 \\
        &= \frac{1}{(K-1)\delta^2} \cdot \left(\frac{\delta^2 + \sigma_t^2 \left( \hat{w}_k + (K-1)\delta^2 \hat{w}_l \right) }{\delta^2 + \sigma_t^2 \left( \hat{w}_l + \delta^2\hat{w}_k + (K-2)\delta^2 \hat{w}_l \right) } \right)^2\\
        &= \frac{1}{(K-1)\delta^2} \cdot \left(\frac{1 + \frac{\sigma_t^2}{\delta^2}\left( (1 - \delta^2)\hat{w}_k + \delta^2 (\hat{w}_k + (K-1)\hat{w}_l) \right)}{1 + \frac{\sigma_t^2}{\delta^2}\left( (1-\delta^2)\hat{w}_l + \delta^2 (\hat{w}_l + \hat{w}_k + (K-2)\hat{w}_l) \right)}\right)^2 \\
        &= \frac{1}{(K-1)\delta^2} \cdot \left(\frac{1 + \frac{\sigma_t^2}{\delta^2}\left( (1 - \delta^2)\hat{w}_k + \delta^2\right)}{1 + \frac{\sigma_t^2}{\delta^2}\left( (1 - \delta^2)\hat{w}_l + \delta^2\right)}\right)^2 \\
        &= \frac{1}{(K-1)\delta^2} \cdot \left(\frac{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_k, \delta)}{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_l, \delta)}\right)^2 \\
    \end{align*}
    where $h(w, \delta) := (1 - \delta^2)w + \delta^2$.
\end{proof}

\begin{lemma}\label{lem:decomp}
    With the set up of a K-class \MoLRG~data distribution as defined in (\ref{eq:MoG noise}), and define the noise space as $\mb U_{\perp} = \bigcap_{k=1}^K \bm U_k^{\perp} \in \mathcal{O}^{n \times (n-Kd)}$ (i.e., mutual noise for all classes). Consider the following the function:

    \begin{align}
        \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) &=  \sum_{k=1}^K \hat{w}_k(\bm x) \left( \frac{1}{1 + \sigma_t^2} \bm U_k\bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \right) \bm x, \\
        &\text{where}\ \hat{w}_k(\bm x) := \frac{\exp\left( \E_{\bm x} [g_k(\bm x, t)] \right)}{\sum_{k=1}^K \exp\left( \E_{\bm x} [g_k(\bm x, t)] \right)}, \\
        &\text{and}\ g_k(\bm x) = \frac{1}{2\sigma_t^2 (1 + \sigma_t^2)} \|\bm U_k^T \bm x\|^2 + \frac{\delta^2}{2 \sigma_t^2 (\delta^2 + \sigma_t^2)} \| \bm U_k^{\perp T} \bm x \|^2 .
    \end{align}

    I.e., we consider a simplified version of the expected posterior mean as in \Cref{lem:E[x_0]_multi} by taking expectation of $g_k(\bm x)$ prior to the softmax operation. Under this setting, for any clean $\bm x_0$ from class $k$ (i.e., $\bm x_0 = \bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i$), we have: 

    \begin{align}
        &\E_{\bm x_0}[\| \bm U_k \bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] = \left( \frac{\hat{w}_k}{1 + \sigma_t^2} + \frac{(K-1)\delta^2\hat{w}_l}{\delta^2+\sigma_t^2}\right)^2 d \label{eq:bin_correct}\\
        &\E_{\bm x_0}[\| \bm U_l \bm U_l^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] = \left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2(\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d \label{eq:bin_other}\\
        &\E_{\bm x_0}[\| \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] = \frac{\delta^6 (n-kd)}{(\delta^2 + \sigma_t^2)^2} \label{eq:bin_noise_space} 
    \end{align}

    \begin{align} \label{eq:bin_overall}
        \begin{split}
        \E[\|\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] &= \underbrace{\left( \frac{\hat{w}_k}{1 + \sigma_t^2} + \frac{(K-1)\delta^2\hat{w}_l}{\delta^2+\sigma_t^2}\right)^2 d}_{\E\|\bm U_k \bm U_k^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} \\
        &+ \underbrace{\left( K-1 \right)\left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2(\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d}_{\E[\sum_{l \neq k}^K\bm U_l \bm U_l^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} + \underbrace{\frac{\delta^6 (n-Kd)}{(\delta^2 + \sigma_t^2)^2}}_{\E[\|\bm U_{\perp} \bm U_{\perp}^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} 
        \end{split}
    \end{align}
 % a_t = \frac{1}{2\sigma_t^2(1 + \sigma_t^2)}
 % c_t = \frac{\delta^2}{2\sigma_t^2(\delta^2 + \sigma_t^2)}
 %         &\E[\|\hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t)\|^2] = \left(\frac{\hat{w_1}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_2}}{\delta^2 + \sigma_t^2} \right)^2 d + \left( \frac{\hat{w_2}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d + \frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2} \label{eq:bin_overall}
    and 
    \begin{align}
    \begin{split}
        &\hat{w}_k := \hat{w}_k(\bm x_0) = \frac{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4D}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right)}{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4D}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right) + \left(K-1\right)\exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}, \\ 
        &\hat{w}_l := \hat{w}_l(\bm x_0) = \frac{\exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4D}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right) + \left(K-1 \right)\exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}
    \end{split}
    \end{align}
    for all class index $l \neq k$.
\end{lemma}

\begin{proof}
    Throughout the proof, we use the following notation for slices of vectors.
    \begin{align*}
        \bm e_i [a:b] \;\;\;\;\;\; \text{Slices of vector} \; \bm e_i \; \text{from} \; a\text{th} \;\text{entry to} \; b\text{th} \; \text{entry.} 
    \end{align*}
    We begin with the softmax terms. Since each class has its unique disjoint subspace, it suffices to consider $g_k(\bm x_0, t)$ and $g_l(\bm x_0, t)$ for any $l \neq k$. Let $a_t = \frac{1}{2\sigma_t^2(1 + \sigma_t^2)}$ and $c_t = \frac{\delta^2}{2\sigma_t^2(\delta^2 + \sigma_t^2)}$, we have:
    \begin{align*}
        \E[g_k(\bm x_0, t)] &= \E[a_t \| \bm U_k^T \bm x_0 \|^2 + c_t \| \bm U_k^{\perp T} \bm x_0 \|^2] \\
        &= \E[a_t \| \bm U_k^T (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i) \|^2] + \E[c_t \|\bm U_k^{\perp T} (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i)\|^2] \\
        &= \E[a_t\|\bm a_i\|^2] + \E[c_t\|b \bm e_i\|^2] \\
        & = a_t d + c_t \delta^2 D
    \end{align*}
    where the last equality follows from $\bm a_i \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{d})$ and $\bm e_i \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{D})$. 
    
    Without loss of generality, assume the $j=k+1$, we have:
    \begin{align*}
        \E[g_l(\bm x_0, t)] &= \E[a_t \| \bm U_l^T \bm x_0 \|^2 + c_t \| \bm U_l^{\perp T} \bm x_0 \|^2] \\
        &= \E[a_t \| \bm U_l^T (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i) \|^2] + \E[c_t \|\bm U_l^{\perp T} (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i)\|^2] \\
        &= \E[a_t\|b\bm e_i [1:d]\|^2] + \E\left[c_t \Big\| \begin{bmatrix}
            \bm a_i \\
            \bm 0 \in \mathbb{R}^{D-d}
        \end{bmatrix} + b \begin{bmatrix}
            \bm 0 \in \mathbb{R}^{d} \\
            \bm e_i [d:D]]
        \end{bmatrix} \Big\|^2 \right] \\
        & = a_t \delta^2 d + c_t(d + \delta^2 (D-d))
    \end{align*}

    Plug $a_t$ and $b_t$ back with the exponentials, we get $\hat{w}_k$ and $\hat{w}_l$. \\

    Now we prove (\ref{eq:bin_correct}):
    \begin{align*}
        \bm U_k \bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) &= \hat{w}_k \bm U_k \bm U_k^T \left(\frac{1}{1+\sigma_t^2} \bm U_k \bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp} \bm U_k^{\perp T} \right) \bm x_0 \\
        &\;\;\;\; + \sum_{l \neq k}\hat{w}_l \bm U_k \bm U_k^T \left(\frac{1}{1+\sigma_t^2} \bm U_l \bm U_l^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_l^{\perp} \bm U_l^{\perp T} \right) \bm x_0 \\
        &= \hat{w}_k \left( \frac{1}{1 + \sigma_t^2} \bm U_k \bm U_k^T \bm x_0 \right) + \sum_{l \neq k}\hat{w}_l \left( \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k \bm U_k^T \bm x_0\right) \\
        &= \left( \frac{\hat{w}_k}{1+\sigma_t^2} + \frac{\left(K-1 \right)\delta^2 \hat{w}_l}{\delta^2 + \sigma_t^2} \right) \bm U_k \bm U_k^T (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i) \\
        &= \left( \frac{\hat{w}_k}{1+\sigma_t^2} + \frac{\left(K-1 \right)\delta^2 \hat{w}_l}{\delta^2 + \sigma_t^2} \right) \bm U_k \bm a_i
    \end{align*}
    Since $\bm U_k \in \mathcal{O}^{n \times d}$:
    \begin{align*}
        \E[\| \bm U_k \bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) \|^2] = \left( \frac{\hat{w}_k}{1+\sigma_t^2} + \frac{\left(K-1 \right)\delta^2 \hat{w}_l}{\delta^2 + \sigma_t^2} \right)^2 d
    \end{align*}
    \vspace{-0.1in}
    and similarly for (\ref{eq:bin_other}):
    \begin{align*}
        \bm U_l \bm U_l^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) &= \hat{w}_k \bm U_l \bm U_l^T \left(\frac{1}{1+\sigma_t^2} \bm U_k \bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp} \bm U_k^{\perp T} \right) \bm x_0 \\
        &\;\;\;\; + \hat{w}_l \bm U_l \bm U_l^T \left(\frac{1}{1+\sigma_t^2} \bm U_l \bm U_l^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_l^{\perp} \bm U_l^{\perp T} \right) \bm x_0 \\
        &\;\;\;\; + \sum_{j\neq k,l}\hat{w_j} \bm U_l \bm U_l^T \left(\frac{1}{1+\sigma_t^2} \bm U_j \bm U_j^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_j^{\perp} \bm U_j^{\perp T} \right) \bm x_0 \\
        &= \hat{w}_k \left( \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_l \bm U_l^T \bm x_0 \right) + \hat{w}_l \left( \frac{1}{1 + \sigma_t^2} \bm U_l \bm U_l^T \bm x_0\right) + \sum_{j \neq k,l}\hat{w_j} \left( \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_l \bm U_l^T \bm x_0 \right) \\
        &= \left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2 (\hat{w}_k + (K-2)\hat{w_j})}{\delta^2 + \sigma_t^2}\right) \bm U_l \bm U_l^T (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i) \\
        &= \left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2 (\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2}\right) b \bm U_l \bm e_i[1:d]
    \end{align*}
    where the third equality follows since $\hat{w_j} = \hat{w}_l$ for all $j \neq k,l$. Further, we have:
    \begin{align*}
        \E[\| \bm U_l \bm U_l^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) \|^2] = \left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2 (\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2}\right)^2 \delta^2 d
    \end{align*}

    Next, we consider (\ref{eq:bin_noise_space}):
    \begin{align*}
        \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) &= \hat{w}_k \bm U_{\perp} \bm U_{\perp}^T \left(\frac{1}{1+\sigma_t^2} \bm U_k \bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp} \bm U_k^{\perp T} \right) \bm x_0 \\
        &\;\;\;\; + \sum_{l \neq k}\hat{w}_l \bm U_{\perp} \bm U_{\perp}^T \left(\frac{1}{1+\sigma_t^2} \bm U_l \bm U_l^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_l^{\perp} \bm U_l^{\perp T} \right) \bm x_0 \\
        &= \hat{w}_k \left(\frac{\delta^2}{\delta^2 + \sigma_t^2 } \bm U_{\perp} \bm U_{\perp}^T \bm x_0 \right) + \sum_{l \neq k}\hat{w}_l \left(\frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm U_{\perp}^T \bm x_0 \right) \\
        &= \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm U_{\perp}^T (\bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i) \\
        &= \frac{\delta^3}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm e_i[(K-1)d:D]
    \end{align*}
    Hence:
    \begin{align*}
        \E[\| \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] &= \frac{\delta^6 (n-Kd)}{(\delta^2 + \sigma_t^2)^2}
    \end{align*}

    Lastly, we prove (\ref{eq:bin_overall}). Given that the subspaces of all classes and the complement space are both orthonormal and mutually orthogonal, we can write:
    \begin{align*}
        \E[\| \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t) \|^2] = \E[\| \bm U_k \bm U_k^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] + \E[\sum_{l \neq k}\| \bm U_l \bm U_l^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] + \E[\| \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]
    \end{align*}
    Combine terms, we get:
    \begin{align*}
        \E[\|\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] &= \left( \frac{\hat{w}_k}{1 + \sigma_t^2} + \frac{(K-1)\delta^2\hat{w}_l}{\delta^2+\sigma_t^2}\right)^2 d \\
        &+ \left( K-1 \right)\left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2(\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d + \frac{\delta^6 (n-Kd)}{(\delta^2 + \sigma_t^2)^2}.
    \end{align*}
\end{proof}



% \begin{lemma}
%     Consider \MoLRG~data as defined in \eqref{eq:MoG noise} with $K=2$ (i.e., the binary case) with class index $1$ and $2$. Consider the following the function:

%     \begin{align}
%         \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) &=  \sum_{k=1}^K \hat{w}_k(\bm x) \left( \frac{1}{1 + \sigma_t^2} \bm U_k\bm U_k^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \right) \bm x, \\
%         &\text{where}\ \hat{w}_k(\bm x) := \frac{\exp\left( \E [g_k(\bm x, t)] \right)}{\sum_{k=1}^K \exp\left( \E [g_k(\bm x, t)] \right)}, \\
%         &\text{and}\ g_k(\bm x) = \frac{1}{2\sigma_t^2 (1 + \sigma_t^2)} \|\bm U_k^T \bm x\|^2 + \frac{\delta^2}{2 \sigma_t^2 (\delta^2 + \sigma_t^2)} \| \bm U_k^{\perp T} \bm x \|^2 .
%     \end{align}

%     I.e., we consider a simplified version of the expected posterior mean as in \eqref{eq:E_MoG}. Under this setting, for any clean $\bm x$ from class $1$ (i.e., $\bm x = \bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i$), we have: 

%     \begin{align} \label{eq:bin_overall}
%         \E[\|\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2] = \underbrace{\left(\frac{\hat{w_1}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_2}}{\delta^2 + \sigma_t^2} \right)^2 d}_{\E\|\bm U_1 \bm U_1^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} + \underbrace{\left( \frac{\hat{w_2}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d}_{\E|\bm U_2 \bm U_2^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} + \underbrace{\frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2}}_{\E[\|\bm U_{\perp} \bm U_{\perp}^T\hat{\bm x}_{\textit{approx}}^{\star}(\bm x_0, t)\|^2]} 
%     \end{align}

%     \begin{align}
%         &\E[\| \bm U_2 \bm U_2^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t)\|^2] = \left( \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} + \frac{\hat{w_2}}{1 + \sigma_t^2} \right)^2 \delta^2 d \label{eq:bin_other}\\
%         &\E[\| \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t)\|^2] = \frac{\delta^6 (D-d)}{(\delta^2 + \sigma_t^2)^2} \label{eq:bin_noise_space} 
%     \end{align}
%  % a_t = \frac{1}{2\sigma_t^2(1 + \sigma_t^2)}
%  % c_t = \frac{\delta^2}{2\sigma_t^2(\delta^2 + \sigma_t^2)}
%  %         &\E[\|\hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t)\|^2] = \left(\frac{\hat{w_1}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_2}}{\delta^2 + \sigma_t^2} \right)^2 d + \left( \frac{\hat{w_2}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d + \frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2} \label{eq:bin_overall}
%     and 
%     \begin{align}
%     \begin{split}
%         &\hat{w_1} = \frac{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4d}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right)}{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4d}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right) + \exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}, \\ 
%         &\hat{w_2} = \frac{\exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}{\exp\left(\frac{d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^4d}{2\sigma_t^2(\delta^2 + \sigma_t^2)} \right) + \exp\left(\frac{\delta^2 d}{2\sigma_t^2(1 + \sigma_t^2)} + \frac{\delta^2 d + \delta^4(D-d)}{2\sigma_t^2(\delta^2 + \sigma_t^2)}\right)}
%     \end{split}
%     \end{align}
% \end{lemma}

% \begin{proof}
%     We begin with the softmax terms $\hat{w_1}, \hat{w_2}$. Since we are in the binary case, it suffices to consider $g_1(\bm x, t)$ and $g_2(\bm x, t)$. Let $a_t = \frac{1}{2\sigma_t^2(1 + \sigma_t^2)}$ and $c_t = \frac{\delta^2}{2\sigma_t^2(\delta^2 + \sigma_t^2)}$, we have:
%     \begin{align*}
%         \E[g_1(\bm x, t)] &= \E[a_t \| \bm U_1^T \bm x \|^2 + c_t \| \bm U_{1^{\perp}}^T \bm x \|^2] \\
%         &= \E[a_t \| \bm U_1^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i) \|^2] + \E[c_t \|\bm U_{1^{\perp}}^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i)\|^2] \\
%         &= \E[a_t\|\bm a_i\|^2] + \E[c_t\|b \bm e_i\|^2] \\
%         & = a_t d + c_t \delta^2 D
%     \end{align*}
%     where the last equality follows from $\bm a_i \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{d})$ and $\bm e_i \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{D})$. 
    
%     Similarly, we have:
%     \begin{align*}
%         \E[g_2(\bm x, t)] &= \E[a_t \| \bm U_2^T \bm x \|^2 + c_t \| \bm U_{2^{\perp}}^T \bm x \|^2] \\
%         &= \E[a_t \| \bm U_2^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i) \|^2] + \E[c_t \|\bm U_{2^{\perp}}^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i)\|^2] \\
%         &= \E[a_t\|b\bm e_i [1:d]\|^2] + \E\left[c_t \Big\| \begin{bmatrix}
%             \bm a_i \\
%             \bm 0 \in \mathbb{R}^{D-d}
%         \end{bmatrix} + b \begin{bmatrix}
%             \bm 0 \in \mathbb{R}^{d} \\
%             \bm e_i [d:D]]
%         \end{bmatrix} \Big\|^2 \right] \\
%         & = a_t \delta^2 d + c_t(d + \delta^2 (D-d))
%     \end{align*}

%     Plug $a_t$ and $b_t$ back with the exponentials, we get $\hat{w_1}$ and $\hat{w_2}$. \\

%     Now we prove (\ref{eq:bin_other}):
%     \begin{align*}
%         \bm U_2 \bm U_2^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) &= \hat{w_1} \bm U_2 \bm U_2^T \left(\frac{1}{1+\sigma_t^2} \bm U_1 \bm U_1^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{1^{\perp}} \bm U_{1^{\perp}}^T \right) \bm x \\
%         &\;\;\;\; + \hat{w_2} \bm U_2 \bm U_2^T \left(\frac{1}{1+\sigma_t^2} \bm U_2 \bm U_2^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{2^{\perp}} \bm U_{2^{\perp}}^T \right) \bm x \\
%         &= \hat{w_1} \left( \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_2 \bm U_2^T \bm x \right) + \hat{w_2} \left( \frac{1}{1 + \sigma_t^2} \bm U_2 \bm U_2^T \bm x\right) \\
%         &= \left( \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} + \frac{\hat{w_2}}{1+\sigma_t^2} \right) \bm U_2 \bm U_2^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i) \\
%         &= \left( \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} + \frac{\hat{w_2}}{1+\sigma_t^2} \right) b \bm U_2 \bm e_i[1:d]
%     \end{align*}
%     Therefore, since $\bm U_2 \in \mathcal{O}^{n \times d}$:
%     \begin{align*}
%         \E[\| \bm U_2 \bm U_2^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) \|^2] = \left( \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} + \frac{\hat{w_2}}{1+\sigma_t^2} \right)^2 \delta^2 d
%     \end{align*}

%     Next, we consider (\ref{eq:bin_noise_space}):
%     \begin{align*}
%         \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) &= \hat{w_1} \bm U_{\perp} \bm U_{\perp}^T \left(\frac{1}{1+\sigma_t^2} \bm U_1 \bm U_1^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{1^{\perp}} \bm U_{1^{\perp}}^T \right) \bm x \\
%         &\;\;\;\; + \hat{w_2} \bm U_{\perp} \bm U_{\perp}^T \left(\frac{1}{1+\sigma_t^2} \bm U_2 \bm U_2^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{2^{\perp}} \bm U_{2^{\perp}}^T \right) \bm x \\
%         &= \hat{w_1} \left(\frac{\delta^2}{\delta^2 + \sigma_t^2 } \bm U_{\perp} \bm U_{\perp}^T \bm x \right) + \hat{w_2} \left(\frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm U_{\perp}^T \bm x \right) \\
%         &= \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm U_{\perp}^T (\bm U_1 \bm a_i + b\bm U_{1^{\perp}} \bm e_i) \\
%         &= \frac{\delta^3}{\delta^2 + \sigma_t^2} \bm U_{\perp} \bm e_i[d:D]
%     \end{align*}
%     Hence:
%     \begin{align*}
%         \E[\| \bm U_{\perp} \bm U_{\perp}^T \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t)\|^2] &= \frac{\delta^6 (D-d)}{(\delta^2 + \sigma_t^2)^2}
%     \end{align*}

%     Lastly, we prove (\ref{eq:bin_overall}). \\
%     Let $h_k(\bm x, t) = \frac{1}{1+\sigma_t^2} \bm U_k \bm U_k^T \bm x + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_k^{\perp}\bm U_k^{\perp T} \bm x$. Then we have:
%     \begin{align*}
%         \E[\| \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) \|^2] &= \E[(\hat{w_1} h_1(\bm x, t) + \hat{w_2} h_2(\bm x, t))^T(\hat{w_1} h_1(\bm x, t) + \hat{w_2} h_2(\bm x, t))] \\
%         &= \hat{w_1}^2\E[\|h_1(\bm x, t)\|^2] + \hat{w_2}^2\E[\|h_2(\bm x, t)\|^2] + 2\hat{w_1}\hat{w_2}\E[\Tr(h_1(\bm x, t) h_2(\bm x, t)^T)]
%     \end{align*}
%     Consider each term separately, we have:
%     \begin{align*}
%         \E[\|h_1(\bm x, t)\|^2] &= \E[\left(\frac{1}{1+\sigma_t^2} \bm x^T \bm U_1 \bm U_1^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm x^T \bm U_{1^{\perp}}\bm U_{1^{\perp}}^T \right) \cdot \\
%         &\;\;\;\;\;\;\;\; \left(\frac{1}{1+\sigma_t^2} \bm U_1 \bm U_1^T \bm x + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{1^{\perp}}\bm U_{1^{\perp}}^T \bm x \right) ] \\
%         &= \frac{1}{(1 + \sigma_t^2)^2} \E[\|\bm U_1^T \bm x\|^2] + \frac{\delta^4}{(\delta^2 + \sigma_t^2)^2 } \E[\|\bm U_{1^{\perp}}^T \bm x\|^2] \\
%         &= \frac{1}{(1 + \sigma_t^2)^2} \E[\|\bm a_i\|^2] + \frac{\delta^4}{(\delta^2 + \sigma_t^2)^2 } \E[\|b \bm e_i\|^2] \\
%         &= \frac{d}{(1 + \sigma_t^2)^2} + \frac{\delta^6 D}{(\delta^2 + \sigma_t^2)^2};
%     \end{align*}
    
%     \begin{align*}
%         \E[\|h_2(\bm x, t)\|^2] &= \E[\left(\frac{1}{1+\sigma_t^2} \bm x^T \bm U_2 \bm U_2^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm x^T \bm U_{2^{\perp}}\bm U_{2^{\perp}}^T \right) \cdot \\
%         &\;\;\;\;\;\;\;\; \left(\frac{1}{1+\sigma_t^2} \bm U_2 \bm U_2^T \bm x + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{2^{\perp}}\bm U_{2^{\perp}}^T \bm x \right) ] \\
%         &= \frac{1}{(1 + \sigma_t^2)^2} \E[\|\bm U_2^T \bm x\|^2] + \frac{\delta^4}{(\delta^2 + \sigma_t^2)^2 } \E[\|\bm U_{2^{\perp}}^T \bm x\|^2] \\
%         &= \frac{1}{(1 + \sigma_t^2)^2} \E[\|b \bm e_i[1:d]\|^2] \\ 
%         &\;\;\;\;\;\;+ \frac{\delta^4}{(\delta^2 + \sigma_t^2)^2 } \E\left[\Big\|\begin{bmatrix}
%             \bm a_i \\
%             \bm 0 \in \mathbb{R}^{D-d}
%         \end{bmatrix} + b \begin{bmatrix}
%             \bm 0 \in \mathbb{R}^{d} \\
%             \bm e_i [d:D]
%         \end{bmatrix}\Big\|^2 \right] \\
%         &= \frac{\delta^2 d}{(1 + \sigma_t^2)^2} + \frac{\delta^4 d + \delta^6 (D-d)}{(\delta^2 + \sigma_t^2)^2};
%     \end{align*}

%     \begin{align*}
%         &\E[\Tr(h_1(\bm x, t) h_2(\bm x, t)^T)] \\
%         &= \E[\Tr\left(\frac{1}{1+\sigma_t^2} \bm x^T \bm U_1 \bm U_1^T + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm x^T \bm U_{1^{\perp}}\bm U_{1^{\perp}}^T \right) \left(\frac{1}{1+\sigma_t^2} \bm U_2 \bm U_2^T \bm x + \frac{\delta^2}{\delta^2 + \sigma_t^2} \bm U_{2^{\perp}}\bm U_{2^{\perp}}^T \bm x \right) ] \\ 
%         &= \E[\Tr( \left(\frac{1}{1+\sigma_t^2} \frac{\delta^2}{\delta^2 + \sigma_t^2} \right) \delta^2 \bm e_i[1:d]^T \bm U_2^T \bm U_{1^{\perp}} \bm e_i \\
%         &\;\;\;\;\;\;\;\;\;\; + \left(\frac{1}{1+\sigma_t^2} \frac{\delta^2}{\delta^2 + \sigma_t^2}\right) \left(\begin{bmatrix}
%             \bm a_i^T & \bm 0 \in \mathbb{R}^{D-d}
%         \end{bmatrix} \bm U_{2^{\perp}}^T b \bm U_{1^{\perp}} \bm a_i + b \begin{bmatrix}
%             \bm 0 \in \mathbb{R}^{D-d} & \bm e_i[d:D]
%         \end{bmatrix} \bm U_{2^{\perp}}^T \bm U_{1^{\perp}} \bm a_i \right) \\
%         &\;\;\;\;\;\;\;\;\;\; + \left(\frac{1}{1+\sigma_t^2} \frac{\delta^2}{\delta^2 + \sigma_t^2}\right) \left(\begin{bmatrix}
%             \bm a_i^T & \bm 0 \in \mathbb{R}^{D-d}
%         \end{bmatrix} \bm U_{2^{\perp}}^T b \bm U_{1^{\perp}} \bm e_i + \delta^2 \begin{bmatrix}
%             \bm 0 \in \mathbb{R}^{D-d} & \bm e_i[d:D]
%         \end{bmatrix} \bm U_{2^{\perp}}^T \bm U_{1^{\perp}} \bm e_i \right) ] \\
%         &= \E[\left(\frac{1}{1+\sigma_t^2} \frac{\delta^2}{\delta^2 + \sigma_t^2} \right) \delta^2 \|\bm e_i[1:d]\|^2 + \left(\frac{1}{1+\sigma_t^2} \frac{\delta^2}{\delta^2 + \sigma_t^2} \right) \|\bm a_i\|^2 + \left( \frac{\delta^2}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 \|\bm e_i[d:D]\|^2 ] \\
%         &= \frac{\delta^4 d + \delta^2 d}{(1 + \sigma_t^2)(\delta^2 + \sigma_t^2)} + \frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2}
%     \end{align*}

%     Combine terms, we get:
%     \begin{align*}
%         \E[\| \hat{\bm x}_{\textit{approx}}^{\star}(\bm x, t) \|^2] &= \hat{w_1}^2\left(\frac{d}{(1 + \sigma_t^2)^2} + \frac{\delta^6 D}{(\delta^2 + \sigma_t^2)^2}\right) + \hat{w_2}^2 \left(\frac{\delta^2 d}{(1 + \sigma_t^2)^2} + \frac{\delta^4 d + \delta^6 (D-d)}{(\delta^2 + \sigma_t^2)^2} \right) \\
%         & + 2\hat{w_1}\hat{w_2}\left(\frac{\delta^4 d + \delta^2 d}{(1 + \sigma_t^2)(\delta^2 + \sigma_t^2)} + \frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2}\right) \\
%         &= \left(\frac{\hat{w_1}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_2}}{\delta^2 + \sigma_t^2} \right)^2 d + \left( \frac{\hat{w_2}}{1+\sigma_t^2} + \frac{\delta^2 \hat{w_1}}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d + \frac{\delta^6(D-d)}{(\delta^2 + \sigma_t^2)^2}.
%     \end{align*}
% \end{proof}