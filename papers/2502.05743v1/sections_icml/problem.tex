% {\color{blue} Talk about diffusion model foundations, mixture of low-rank gaussian data model, relationship between score-matching and DAE}

% \qq{In this section, let us only focus on: (i) problem setup and preliminary, (ii) empirical studies in the generalization regime, (iii) the demonstration of uni-modal curve }
% In

% In this section, we first review the fundamentals of diffusion models and outline the feature extraction method used in this work. Following this, we illustrate the connection between diffusion posterior estimation and representation learning, which serves as the foundation for the subsequent analysis in \Cref{sec:main}.
% %\subsection{Preliminaries on Diffusion Models for Representation Learning}

\begin{figure*}[t]
    \begin{center}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/teaser_ddpm.pdf}
    \caption{Classification} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/teaser_seg.pdf}
    \caption{Segmentation} 
    \end{subfigure}
    \end{center}
    \vspace{-0.1in}
\caption{\textbf{Unimodal representation dynamic: using clean images as inputs improves representation quality.} We train DDPM/EDM-based diffusion models on various datasets and evaluate downstream performance using clean images ($\bm{x}_0$) versus noisy images ($\bm{x}_t$). Both scenarios show a unimodal performance trend, peaking at intermediate noise levels. While clean images perform similarly to noisy inputs at low noise, they outperform as noise increases. }
%\qq{change $\bm x_t$ $\bm x_0$ to bold in the figure}\zk{remake (a), delete DDPM, change yticks} 
% \zk{illustration of unimodal performance curve of diffusion representation. We extract the feature with ? for ; and observed the unimodal curve of the performance as in \citep{xiang2023denoising}, more importantly we find that clean images achieve comparable performance at low noise levels and significantly outperform corrupted inputs as noise levels increase; which is also one of the main setups for our theoretical findings; subject to change} \xiao{change title to emphasize unimodal}}
\label{fig:clean_feature}
\end{figure*}

 \subsection{Preliminaries on Denoising Diffusion Models}\label{subsec:diffusion_prelim}

\paragraph{Basics of diffusion models.} Diffusion models are a class of probabilistic generative models that aim to reverse a progressive noising process by mapping an underlying data distribution, $p_{\text{data}}$, to a Gaussian distribution.
\begin{itemize}[leftmargin=*]
    \item \emph{The forward diffusion process.} Starting from clean data $\bm{x}_0$, noise is progressively added following a schedule based on time step $t$ until the data becomes pure Gaussian noise. At each step $t$, the noised data is expressed as $\bm{x}_t = s_t \bm{x}_0 + s_t\sigma_t \bm{\epsilon}$, where $\bm{\epsilon} \sim \mathcal{N}(\bm{0}, \bm{I})$ is Gaussian noise, and $s_t$, $s_t\sigma_t$ scale the signal and noise, respectively.
    \item \emph{The reverse diffusion process.}  We can run a reverse SDE \citep{anderson1982reverse} to sample from $\bm x_1$ as:
\begin{align*} %\label{eq:reve}
    \mathrm{d}\bm{x}_t = \left(  f(t)\bm{x}_t - g^2(t) \nabla \log p_t(\bm{x}_t)\right) \mathrm{d}t +g(t)\mathrm{d}\bar{\bm{w}}_t,
\end{align*}
where $\{\bar{\bm{w}_t}\}_{t \in [0,1]}$ is the standard Wiener process running backward in time from $t=1$ to $t=0$ and the functions $f(t), g(t): \R \to \R$ respectively denote the drift and diffusion coefficients. 
%Notably, if both $\bm x_1$ and $\nabla \log p_t$ are known, the reverse process mirrors the forward process at each time step $t \geq 0$ . 
\end{itemize}

 %and based on the relationship between the score $s_{\bm{\theta}}$ and the posterior mean $\E[\hat{\bm x}_0 \vert \bm x_t]$ \citep{vincent2011connection, wang2024diffusion}:
%prior works \citep{ho2020denoising, song2020score} train neural networks to approximate the score function across timesteps.
\paragraph{Training with denoising auto-encoder (DAE) based objective.} Since the score function $\nabla \log p_t(\bm x_t)$ depends on the unknown data distribution $p_{\rm data}$ and satisfies
\begin{align} \label{eq:Tweedie} s_t\E\left[ \bm x_0 \vert \bm x_t \right] = \bm x_t + s_t^2\sigma_t^2 \nabla \log p_t(\bm x_t), \end{align}
we can estimate $\nabla \log p_t(\bm x_t)$ by training a network $\bm x_{\bm \theta}(\bm x_t, t)$ to approximate the posterior mean $\E[\bm x_0 \vert \bm x_t]$ \citep{chen2024deconstructing, xiang2023denoising, kadkhodaie2023generalization}. This is achieved by minimizing the loss $\mathcal{L}(\mb \theta)$ via
\begin{align}
   \min_{\bm \theta}\; \sum_{i=1}^N \int_0^1 \lambda_t   \E_{\bm \epsilon } \left[\left\|\bm x_{\bm \theta}(\bm x_t^{(i)}, t) -  \bm x_0^{(i)} \right\|^2\right] \mathrm{d}t, \label{eq:dae_loss}
\end{align}
where $\bm \epsilon \sim \mathcal{N}(\bm 0, \bm I_n)$, $\lambda_t$ represents the weight associated with each noise level, and $N$ denotes the dataset size. Moreover, $\bm x_i^{(0)} \overset{i.i.d.}{\sim} p_{\rm data}$ is the training sample and the corresponding noisy sample is given by $\bm x_t^{(i)} = s_t \bm x_0^{(i)} + s_t\sigma_t\bm \epsilon$ for each $i\in [N]$. To simplify the analysis, we assume throughout the paper that $s_t = 1$ and $\lambda_t$ remain constant across all noise levels, with the noise level denoted as $\sigma_t$. It is worth noting if we only minimize the error at one specific timestep, we are exactly training a single step DAE proposed in \citep{vincent2011connection}, and in \Cref{subsec:weight_share} we discuss the superiority of diffusion models over DAEs.

%where $\bm \epsilon \sim \mathcal{N}(\bm 0, \bm I_n)$, $\bm x_t^{(i)} = s_t \bm x_0^{(i)} + s_t\sigma_t\bm \epsilon$, $N$ represents the size of the training dataset, and $\lambda_t$ denotes the weighting for each noise level. To simplify the analysis, we assume throughout the paper that $s_t = 1$ and $\lambda_t$ remain constant across all noise levels, with the noise level denoted as $\sigma_t$. 
%\lambda_t

%that minimizes a least squares loss $\mathcal{L}(\mb \theta)$
%An alternative approach \citep{chen2024deconstructing, xiang2023denoising, kadkhodaie2023generalization} minimizes a least squares loss $\mathcal{L}(\mb \theta)$ to align clean images $\bm x_0$ with posterior mean approximations $\E[\bm x_0 \vert \bm x_t]$ using a DAE network $\bm x_{\bm \theta}(\bm x_t, t)$:
%based on the aforementioned relationship to estimate the posterior mean $\E[\bm x_0 \vert \bm x_t]$ via a DAE network $\bm x_{\bm \theta}(\bm x_t, t)$: % \qq{this should be $\bm x_{\bm \theta}(\bm x_t, t)$?} %\xiao{the equation below is once again too long..}

%\qq{this is weird, where is $\bm \theta$? this is our optimization variable but not shown in the loss}
%\qq{I do not quite understand what is the message we want to convey in the following paragraph. I think we should highlight the benefits of timesteps over single step DAE, and refer to the section of our study. In other words, we should motivate why we study DAE diffusion over traditional DAE. Maybe we should move Figure 4 here to explain why diffusion is better than single step DAE, so that studying the problem is important.} If we remove the integration in (\ref{eq:dae_loss}) and fix $t$, the loss reduces to the traditional single-step DAE loss \citep{vincent2008extracting}, where the DAE is trained at a single noise level. Prior work \citep{chen2024deconstructing} has decomposed the diffusion model training objective into the denoising process (via the denoising loss) and the diffusion process (integrating loss across noise levels in (\ref{eq:dae_loss})). To examine their distinct roles in representation learning, we compare diffusion models with individual DAEs, using the latter as a control to isolate the denoising process.


%We note that if we remove the integration in (\ref{eq:dae_loss}) and fix $t$, the loss simplifies to the traditional single-level DAE loss \citep{vincent2008extracting}, where the DAE is trained at a single noise level. Previous work \citep{chen2024deconstructing} has decomposed the training objective of diffusion models into the denoising process (through the denoising loss) and the diffusion process (integrating the loss across all noise levels in (\ref{eq:dae_loss})). To comprehensively investigate the distinct roles of these two processes in representation learning, we consider both diffusion models and individual DAEs in our experiments where the individual DAEs serve as a control group, allowing us to isolate and analyze the effects of the denoising process alone.
% \xiao{Talk about relation with one-step DAE model}



\subsection{Representation Learning via Diffusion Models} 

% \qq{we need to first introduce how to use diffusion models for representation learning, and then go into the basic setup of the studying unimodal curves, CSNR maybe here as well. We cannot directly go into unimodal curve, this is too abrupt }
%We introduce the basics of representation extraction and measuring the representation quality in diffusion models.

%\subsubsection{Representation extraction}
Once the diffusion model $\bm x_{\mb \theta}$ is trained via \eqref{eq:dae_loss}, we extract and evaluate the representation from data $\bm x_0$ as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Using clean images as network inputs.}  We propose to use clean images, $\bm x_0$, as inputs to the network $\bm x_{\bm \theta}(\bm x_0,t)$ for extracting representation across different noise levels $t$. This is different from existing approaches \citep{xiang2023denoising, baranchuk2021label, tang2023emergent} that use noisy images $\mb x_t$ for representation extraction.
    \item \textbf{Layer selection for representations.} Following the protocol in \citep{xiang2023denoising}, we freeze the entire model and extract representations from the layer of the diffusion model $\bm x_{\bm \theta}(\bm x_0,t)$ that yields the best downstream performance.\footnote{After feature extraction, we apply a global average pooling to the features. For instance, given a feature map of dimension $256 \times 4 \times 4$, we pool the last two dimensions, resulting in a 256-dimensional vector.} Typically, we select a layer near the bottleneck layer of U-Net and the exact midpoint layer of DiT to balance between data compression and performance.
    \item \textbf{Evaluation of representation.} Once the model is trained, we freeze the model and assess its representation quality based on downstream performance metrics, such as accuracy for classification and mean intersection over union (mIoU) for segmentation tasks.
    % \qq{I remembered some reviewers asked, maybe we should also clarify here.}
\end{itemize}

%. As illustrated in \Cref{fig:clean_feature}, our work shows that extracting representations from clean images yields comparable or superior quality to

\paragraph{Remarks on network inputs.} Beyond simplifying the analysis, our choice of using \emph{clean} images as network inputs $\bm x_{\bm \theta}(\bm x_0,t)$ (rather than $\bm x_{\bm \theta}(\bm x_t,t)$) for representation extraction is driven by two primary considerations.
\begin{itemize}[leftmargin=*]
    \item \emph{Empirical performance gains.} As demonstrated in \Cref{fig:clean_feature}, using clean inputs outperforms using noisy inputs $\bm x_t$ on both classificaiton and segmentation tasks, a result further supported by our studies on the posterior estimation; see \Cref{fig:clean_vs_noise} in the Appendix. These findings imply that the denoising objective is the primary driver of representation capabilities in diffusion models, whereas the progressive denoising procedure has a relatively minor impact on representation quality.
    \item \emph{Alignment with existing learning paradigms.} Moreover, our approach is also consistent with standard supervised and self-supervised learning, where data augmentations (e.g., cropping \citep{caron2021emerging}, color jittering, masking \citep{he2022masked}) are applied during training to improve robustness, but clean, unaugmented images are typically used at inference. In diffusion models, similarly, additive Gaussian noise serves as a form of data augmentation during training \citep{chen2024deconstructing}, while clean images are used for inference.
\end{itemize}




%Beyond simplifying analysis, this choice of using clean images $\bm x_0$ as input for representation is motivated by several key factors: (i) first, this approach aligns with traditional supervised and self-supervised learning paradigms, where data augmentations such as cropping \citep{caron2021emerging}, color jittering, or masking \citep{he2022masked} are employed during training to improve robustness, while clean, unaugmented images are typically used during inferencee. Similarly, in diffusion models, additive Gaussian noise acts as a critical form of data augmentation during training \citep{chen2024deconstructing}. (ii) As shown in \Cref{fig:clean_feature}, clean inputs yield better overall performance compared to noisy inputs $\bm x_t$. This superiority of clean inputs is further supported in posterior estimation results (see \Cref{fig:clean_vs_noise} in the Appendix). These findings indicate that the core representation ability of diffusion models is primarily derived from their denoising objective, whereas the diffusion process—characterized by the gradual addition and removal of Gaussian noise—plays a minimal role in determining representation quality.


%We use clean images, $\bm x_0$, as inputs to the network, in contrast to conventional approaches that utilize noisy images, $\bm x_t$ \citep{xiang2023denoising, baranchuk2021label, tang2023emergent}. This approach aligns with traditional supervised and self-supervised learning paradigms, where data augmentations such as cropping \citep{caron2021emerging}, color jittering, or masking \citep{he2022masked} are employed during training to enhance robustness and generalization, while clean, unaugmented images are typically used during inference to ensure optimal performance. Similarly, in diffusion models, additive Gaussian noise acts as a critical form of data augmentation during training \citep{chen2024deconstructing}, and for representation learning at inference, using clean images $\bm x_0$ is sufficient. As shown in \Cref{fig:clean_feature}, clean inputs yield better overall performance compared to noisy inputs $\bm x_t$. This superiority of clean inputs is further supported in posterior estimation results (see \Cref{fig:clean_vs_noise} in the Appendix). These findings indicate that the core representation ability of diffusion models is primarily derived from their denoising objective, whereas the diffusion process—characterized by the gradual addition and removal of Gaussian noise—plays a minimal role in determining representation quality. Therefore, we advocate for the use of clean images as the standard input protocol for feature extraction during inference in all our analyses. Specifically, we conside $\bm x_{\bm \theta}(\bm x_0, t)$ where $t$ serves only as an indicator of the noise level adopted by the diffusion model during feature extraction.

%\begin{itemize}
 %   \item \textbf{Using clean images as network inputs.} We use clean images, $\bm x_0$, as inputs to the network, in contrast to conventional approaches that utilize noisy images, $\bm x_t$ \citep{xiang2023denoising, baranchuk2021label, tang2023emergent}. This approach aligns with traditional supervised and self-supervised learning paradigms, where data augmentations such as cropping \citep{caron2021emerging}, color jittering, or masking \citep{he2022masked} are employed during training to enhance robustness and generalization, while clean, unaugmented images are typically used during inference to ensure optimal performance. Similarly, in diffusion models, additive Gaussian noise acts as a critical form of data augmentation during training \citep{chen2024deconstructing}, and for representation learning at inference, using clean images $\bm x_0$ is sufficient. As shown in \Cref{fig:clean_feature}, clean inputs yield better overall performance compared to noisy inputs $\bm x_t$. This superiority of clean inputs is further supported in posterior estimation results (see \Cref{fig:clean_vs_noise} in the Appendix). These findings indicate that the core representation ability of diffusion models is primarily derived from their denoising objective, whereas the diffusion process—characterized by the gradual addition and removal of Gaussian noise—plays a minimal role in determining representation quality. Therefore, we advocate for the use of clean images as the standard input protocol for feature extraction during inference in all our analyses. Specifically, we conside $\bm x_{\bm \theta}(\bm x_0, t)$ where $t$ serves only as an indicator of the noise level adopted by the diffusion model during feature extraction.

%    \item \textbf{Layer selection for representations.} For feature extraction, we follow the protocol in \citep{xiang2023denoising}, selecting representations from the layer of the diffusion model that yields the best downstream performance.\footnote{After feature extraction, we apply a global average pooling to the features. For instance, given a feature map of dimension $256 \times 4 \times 4$, we pool the last two dimensions, resulting in a 256-dimensional vector.}  
%\end{itemize}




% Here, \CSNR~is defined for any function $f$, providing greater flexibility. Specifically, if we consider $f$ as a diffusion-based feature extractor that outputs intermediate representations (i.e., $f$ can represent the first $n$ layers of a diffusion model), then \CSNR~is computed directly on these representations that are utilized in downstream representation tasks, as detailed in \citep{xiang2023denoising,baranchuk2021label}. On the other hand, since intermediate representations in diffusion models are byproducts of the posterior estimation process, and to better leverage the benign structure of the \MoLRG~distribution, we adopt a more direct approach by analyzing \CSNR~by considering $f$ to be posterior estimation functions such as $\hat{\bm x}_{\bm \theta}^{\star}$. To verify the applicability of \CSNR~, in \Cref{subsec:emp_verify}, we empirically demonstrate that \CSNR~correlates well with the probing performance of both intermediate representations and posterior estimations.




% Once trained, the network iteratively removes the noise, starting from an entirely noisy input $\bm x_T$. Through this process, a clean image $\bm x_0$ from the underlying data distribution $p_{\text{data}}$ is eventually recovered. \qq{Here is not very clear, I think we should describe this through the Tweedie's formula instead}

% Modern diffusion models are typically trained by minimizing the score-matching loss \citep{ho2020denoising,song2020score}. At the meantime, due to the equivalence between the score function $\nabla\log p_t(\bm x_t)$ and the posterior mean $\mathbb{E}[\bm x_0 | \bm x_t]$ \citep{vincent2011connection,wang2024diffusion}, an alternative DAE-based training objective could be considered:
% \begin{align}\label{eq:dae_loss}
%     \min_{\bm \theta}\ \ell(\bm \theta) := \frac{1}{2N} \sum_{i=1}^N \int_0^1 \lambda_t \E_{\bm \epsilon \sim \mathcal{N}(\bm 0, \bm I_n)} \left[\left\| \bm x_{\bm \theta}(s_t \bm x_0^{(i)} + \gamma_t\bm \epsilon, t) -  \bm x_0^{(i)} \right\|^2\right] \mathrm{d}t, 
% \end{align}
% where $N$ represents the size of the training dataset and $\lambda_t$ denotes the weighting for each noise level. 
% To simplify the analysis, we assume throughout the paper that $s_t = 1$ and $\lambda_t$ remain constant across all noise scales.


% \paragraph{Data Setup.} Each training example can be represented as $\bm x^{(i)} = \bm U_k^\star \bm a_i + b\bm e_i,\;\text{with probability}\;\pi_k, \; \ \forall i \in [N]$. For each class, we can write $\bm X_k = \bm U_k \bm A_k + b\bm E_k$ where $\bm A_k = \begin{bmatrix}
%     \bm a_1 & \bm a_2 & ... & \bm a_{N_k}
% \end{bmatrix} \in \mathbb{R}^{d \times N_k}$ and $\bm E_k = \begin{bmatrix}
%     \bm e_1 & \bm e_2 & \cdots & \bm e_{N_k}
% \end{bmatrix} \in \mathbb{R}^{n \times N_k}$. We consider the balanced case where $N_1 = \dots = N_k = \dots = N_K$ and thus $K \cdot N_k = N$.
% We then stack the data points from all classes to formulate the data matrix $\bm X = \begin{bmatrix}
%     \bm X_1 & \bm X_2 & \cdots & \bm X_K
% \end{bmatrix} \in \mathbb{R}^ {n \times N}$. 

% \paragraph{DAE setup.} At each time step, the DAE can be written as
% \begin{align}%\label{eq:para DAE}
%      \bm x_{\bm \theta_t}(\bm x_t) = \bm W_2 \bm W_1^T \bm x_t,
%     \end{align}
% where we minimize the following loss (after taking expectation):
% \begin{align}%\label{eq:loss}
%     \min_{\bm \theta_t}\ \ell(\bm \theta_t) = \frac{1}{2N}||\bm W_2 \bm W_1^T \bm X - \bm X||_F^2 + \frac{1}{2}s^2||\bm W_2 \bm W_1^T||_F^2. 
% \end{align}

% We consider complete DAE where $\bm W_1, \bm W_2^T \in \mathbb{R}^{n \times n}$. For notation convenience, we let $\bm U = \begin{bmatrix}
%     \bm U_1^{\star} & ... & \bm U_K^{\star} \end{bmatrix} \in \mathbb{R}^{n \times Kd}$ to denote the entire subspace spanned by the basis from all classes.

% Consider $\bm W = \bm W_2 \bm W_1^T$, we can get a closed form solution for the weight $\bar{\bm W} = \frac{1}{N} \bm X \bm X^T (\frac{1}{N} \bm X \bm X^T + s^2 \bm I)^{-1}$. Further, we can also have a learned solution: $\hat{\bm W}$.

% \begin{figure}[t]
%     \begin{center}
%     \begin{subfigure}{0.42\textwidth}
%     \includegraphics[width = 0.905\textwidth]{figs/draft/cifar100_vp_separate.png}
%     \caption{DDPM Cifar-100} 
%     \end{subfigure} \quad %\hspace*{\fill}
%     \begin{subfigure}{0.42\textwidth}
%     \includegraphics[width = 0.905\textwidth]{figs/draft/flowers_ddpm_separate.png}
%     \caption{DDPM Flowers-102} 
%     \end{subfigure}
%     \end{center}
% \caption{\textbf{Decoupling representation and denoising ability.} \xiao{Subject to change}}
% \label{fig:use_clean}
% \end{figure}


% \subsection{Decoupling representation and denoising ability}
% \qq{let us change the order in the following: describe how we extract the feature first, and then compare with existing literature. The following is quite confusing: first, what network are you using? $\mb x_\theta$ or $\mb s _\theta$? The description of clean image is also not clear.}

% High-quality image generation demand our model to capture both coarse and fine details of the data. In contrast, for representation, we often map our data into a low-dimensional latent space for data compression. Therefore, a good representation would only keep the coarse structures while loses the fine details of our data. In fact, finer image details may even act as `noise' that hinders performance as suggested by recent results \citep{allen2022feature}. Therefore, when the noise level increases, the predicted posteriors for clean input $\bm x_0$ transition from `fine' to `coarse' \citep{choi2022perception, wang2023diffusion}, gradually removing fine-grained details. As such, the best representation is achieved when the posterior estimation retains the essential information while discarding some class-irrelevant details. 
 
%This disparity explains the unimodal curve of representation quality across different noise levels as shown in \Cref{fig:use_clean}(b). Furthermore, \Cref{fig:use_clean}(c)-(d) indicates that the unimodal representation learning dynamic is a universal phenomenon, independent of specific network architectures. 



%\subsection{Prevalent phenomena in learned representations of diffusion models}

%\xiao{Please avoid expanding on the memory/generalization discussion. The only relevant takeaway for representation learning from that plot is the expected increase in test accuracy with more samples, which is not very surprising. Naturally, training with more samples improves test accuracy. Overemphasizing the phase transition may lead reviewers to question the paper's focus. We can include it in the experiment section as a nice bonus, but better not upfront.}

%\paragraph{Good representations are \emph{only} learned when diffusion models learn underlying distributions.} 

%\paragraph{Prevalence of uni-modal curves of representation quality.}



%Furthermore, \Cref{fig:use_clean}(c)-(d) indicates that the unimodal representation learning dynamic is a universal phenomenon, independent of specific network architectures. To systematically investigate the root cause of this dynamic in a manner agnostic to network architecture and training configurations, we focus directly on the differences in posterior prediction quality in our subsequent analysis.

% \subsection{Denoising process is the primary drive for diffusion representation learning dynamics}\label{subsec:model_decompose}
%Decoupling diffusion and denoising process: the advantages of diffusion model
% \color{brown}{}
% \begin{figure}[t]
%     \begin{center}
%     \begin{subfigure}{0.42\textwidth}
%     \includegraphics[width = 0.905\textwidth]{figs/edm_dae_acc.pdf}
%     \caption{Test Accuracy} 
%     \end{subfigure} \quad %\hspace*{\fill}
%     \begin{subfigure}{0.42\textwidth}
%     \includegraphics[width = 0.905\textwidth]{figs/edm_dae_swd.pdf}
%     \caption{Sliced Wasserstein Distance} 
%     \end{subfigure}
%     \end{center}
% \caption{\textbf{Comparison of representation learning performance between diffusion model (EDM-VE) and separate DAEs.} \xiao{Subject to change}}
% \label{fig:dae_diffusion}
% \end{figure}


% Diffusion models inherently combine both diffusion and denoising processes \citep{chen2024deconstructing}. To gain deeper insights into their representation learning dynamics, it is important to determine whether the representational differences at various noise levels arise from a single process or from the interplay of both. To investigate this, we conducted an experiment by training separate Denoising Autoencoders (DAEs) on CIFAR-10 \citep{krizhevsky2009learning}, each at a distinct noise level, and compared the test accuracy trend across noise levels with trend the EDM (VE) diffusion model. The results, presented in \Cref{fig:dae_diffusion}(a), show that while there is a consistent performance gap between the diffusion model and the DAEs\footnote{\citet{chen2024deconstructing} notices that training a DAE using a single noise level of $\sqrt{1/3}$ does not significantly differ in representation performance from multi-level training. Our results provide a more complete picture, showing that the magnitude of the performance gap varies significantly across different noise levels.}, the overall unimodal trend is similar in both settings. This suggests that the denoising process is the primary driver of the representation dynamics in diffusion models. Based on this observation, we can simplify our analysis and get rid of the integration over all noise levels by studying separate DAEs instead of the original diffusion model.  In the meantime, we note that denoising alone cannot fully explain the performance gap seen in \Cref{fig:dae_diffusion}, particularly the substantial difference at lower noise levels. To account for this, we must examine the diffusion process in more detail, which will be discussed further in \Cref{subsec:weight_share}.
