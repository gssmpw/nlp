% In \Cref{sec:main}, we analyzed diffusion representation dynamics with a focus on the denoising process, assuming sufficient training data for learning the underlying distribution. In this section, we explore the impact of the diffusion process (\Cref{subsec:weight_share}) and data complexity (\Cref{subsec:mem_gen}) in shaping diffusion models' representation learning dynamics.




With the setup in \Cref{sec:problem}, this section theoretically investigates the representation dynamics of diffusion models across the noise levels, providing new insights for understanding the representation-generation tradeoff. Moreover, our theoretical studies are corroborated by experimental results on real datasets.

%{\color{blue} In this section, we investigate the representation dynamics of diffusion models, focusing on the intriguing unimodal trend in their representation learning performance across varying noise levels. To uncover the underlying reasons for this behavior, we conduct a theoretical analysis of the posterior estimation quality, $\E[\bm{x}_0 \mid \bm{x}_t]$, in low-dimensional distributions.}





% \vspace{-0.2in}
\subsection{Assumptions of Low-Dimensional Data Distribution}\label{subsec:model}

\begin{wrapfigure}[14]{R}{0.44\textwidth}
% \begin{figure}[t]
    \vspace{-0.1in}
    \begin{center}
    % \includegraphics[width=0.38\textwidth]{figs/molrg_illustration.pdf} % Replace with your image
    \includegraphics[width=0.4\textwidth]{figs/molrg_illustration_v2.pdf}
    \end{center}
    \vspace{-0.25in}
    \caption{\textbf{An illustration of \MoLRG\;with different noise levels.} We visualize samples drawn from noisy~\MoLRG~with noise levels $\delta = 0.1,\;0.3$ and $K=3$.}
    \label{fig:sample}
    \vspace{-0.1in}
% \end{figure}
\end{wrapfigure}

% \ZK{Refer to latent diffusion? The latent space seems more likely to have a low-dimensional subsuapce structure.}

In this work, we assume that the input data follows a noisy version of the mixture of low-rank Gaussians (\MoLRG) distribution \citep{wang2024diffusion,elhamifar2013sparse, wang2022convergence}, defined as follows.


%\zk{K-subspace or K-class? Need consistency}
\begin{assum}[$K$-Class Noisy \MoLRG~Distribution]\label{assum:subspace}
\emph{For any sample $\mb x_0$ drawn from the noisy \MoLRG~distribution with $K$ subspaces, the following holds: }
% \qq{edit the equation}
\begin{align}\label{eq:MoG noise}
    \bm x_0 = \bm U_k \bm a + \delta \bm U_k^{\perp} \bm e,\;\text{with prob.}\;\pi_k \geq 0,\; k \in [K].
\end{align}
\emph{Here, $k$ represents the class of $\bm x_0$ and follows a multinomial distribution $k \sim \text{Mult}(K,\pi_k)$, $\bm U_k \in \mathcal{O}^{n \times d_k}$ denotes an orthonormal basis for the $k$-th subspace with its complement $\mb U_k^\perp \in \mathcal{O}^{n \times (n-d_k)}$, $d_k$ is the subspace dimension with $d_k \ll n$, and the coefficient $\bm a \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{d_k})$ is drawn from the normal distribution. The level of the noise $\bm e \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{n-d_k})$ is controlled by the scalar $\delta < 1$. }
% $\sum_{k=1}^K \pi_k=1$%Additionally, $\mb U_k^\perp \in \mathcal{O}^{n \times (n-d_k)}$ is the orthogonal complement of $\mb U_k$.
%Here, $\sum_{k=1}^K \pi_k=1$, $\bm U_k \in \mathcal{O}^{n \times d_k}$ denotes an orthonormal basis for the $k$-th subspace, $d_k$ is the subspace dimension with $d_k \ll n$, and the coefficient $\bm a \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{d_k})$ is drawn from a standard normal distribution \qq{normal means standard Gaussian, remove standard}. For the noise, we assume $\bm e \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_{n-d_k})$ with magnitude controlled by the scalar $\delta < 1$. Additionally, $\mb U_k^\perp \in \mathcal{O}^{n \times (n-d_k)}$ is the orthogonal compliment of $\mb U_k$. 
\end{assum}

As shown in \Cref{fig:sample}, data from \MoLRG~resides on a union of low-dimensional subspaces, each following a Gaussian distribution with a low-rank covariance matrix representing its basis. The study of Noisy \MoLRG\; distributions is further motivated by the fact that

%The data drawn from \MoLRG~lie on a union of low-dimensional subspaces. Within each subspace, the data follows a Gaussian distribution with a low-rank covariance matrix that represents the subspace basis. Moreover, the study of the Noisy \MoLRG\; distributions is motivated by the facts that
\begin{itemize}[leftmargin=*]
    \vspace{-0.1in}
    \item \emph{\MoLRG\;captures the intrinsic low-dimensionality of image data.} Although real-world image datasets are high-dimensional in terms of pixel count and data volume, extensive empirical studies \citep{gong2019intrinsic,pope2021intrinsic,stanczuk2022your} demonstrated that their intrinsic dimensionality is considerably lower. Additionally, recent work \citep{huang2024denoising,liang2024low} has leveraged the intrinsic low-dimensional structure of real-world data to analyze the convergence guarantees of diffusion model sampling. The \MoLRG~distribution, which models data in a low-dimensional space with rank $d_k \ll n$, effectively captures this property.
    % \qq{add explanation why MolRG captures the low-dimensionality here}
    \item \emph{The latent space of latent diffusion models is approximately Gaussian.} State-of-the-art large-scale diffusion models \citep{peebles2023scalable, podell2023sdxl} typically employ autoencoders \citep{kingma2013auto} to project images into a low-dimensional latent space, where a KL penalty encourages the learned latent distribution to approximate standard Gaussians \citep{rombach2022high}. Furthermore, recent studies \citep{jing2022subspace, chen2024deconstructing} show that diffusion models can be trained to leverage the intrinsic subspace structure of real-world data.
    % \qq{add: In their training loss, Gaussianity is typically enforced for training the encoder.}\xiao{this is not always true, better be integrated with the previous point}
    
    % \item \emph{Modeling the visual details of real-world image datasets.}\zk{I will shrink} The noise term $\delta \bm U_k^{\perp} \bm e_i$ captures perturbations outside the $k$-th subspace via the orthogonal complement $\bm U_k^{\perp}$, aligning the model with real-world scenarios. These perturbations represent attributes irrelevant to the subspace, such as the background in a bird image or the color and texture of a car. While this additional noise term may be less significant for representation learning, it plays a crucial role in enabling diffusion models to generate high-fidelity samples.
    \item \emph{Modeling the complexity of real-world image datasets.} The noise term $\delta \bm U_k^{\perp} \bm e_i$ captures perturbations outside the $k$-th subspace via the orthogonal complement $\bm U_k^{\perp}$, analogous to insignificant attributes of real-world images, such as the background of an image. While this additional noise term may be less significant for representation learning, it plays a crucial role in enhancing the fidelity of generated samples. %\qq{I feel here there is a bit overstatement. Fine-details are of high frequency, but noise is not. We need to be careful, not mention visual details.}
    % \zk{we can mention the connections with Difan Zou's data assumptions here}
\end{itemize}

Moreover, the noisy \MoLRG\; is analytically tractable. For simplicity, we assume equal subspace dimensions ($d_1 = \dots = d_K = d$), orthogonal bases ($\bm U_k^{T} \bm U_l = \bm 0$ for $k \neq l$), uniform mixing weights ($\pi_1 = \dots = \pi_K = 1/K$), and define the noise space as $\mb U_{\perp} = \bigcap_{k=1}^K \bm U_k^{\perp} \in \mathcal{O}^{n \times (n-Kd)}$. Then, we can derive the ground truth posterior mean $\E\left[\bm x_0 \vert \bm x_t\right]$ for the noisy \MoLRG\; distribution as:


\begin{figure*}[t]
    \begin{center}
    \includegraphics[width = 0.9\textwidth]{figs/Fig1_teaser_zekai.pdf}
    % {figs/teaser_final.pdf}
    \end{center}
\vspace{-0.1in}
\caption{\textbf{Trade-offs between representation quality and generation quality.} The {\color{c1} curve with pentagon markers} demonstrates the transition from fine to coarse granularity in posterior estimation as noise levels increase, corresponding to the monotonic rise in FID. In contrast, the {\color{c2} curve with square markers} reveals an unimodal trend in posterior classification accuracy, achieving peak performance at intermediate noise levels. This occurs when high-level details are filtered out while essential low-level semantic information is preserved, as illustrated by the posterior estimations according to different noise levels shown at the bottom of the figure.}
\label{fig:use_clean}
\end{figure*}

\begin{prop}\label{lem:E[x_0]_multi}
Suppose the data $\bm x_0$ is drawn from a noisy \MoLRG~data distribution with $K$-class and noise level $\delta$. Let  $\zeta_t = \frac{1}{1 + \sigma_t^2}$ and $\xi_t = \frac{\delta^2}{\delta^2 + \sigma_t^2}$, where $\sigma_t$ is the noise scaling in \eqref{eq:Tweedie}. Then for each time $t > 0$, the optimal posterior estimator $\E\left[\bm x_0 \vert \bm x_t\right]$ has the analytical form: 
\begin{align*}
    \E\left[ \bm x_0 \vert \bm x_t\right] = \sum_{l=1}^K w^{\star}_l(\bm x_t,t) \left( \zeta_t \bm U_l\bm U_l^T + \xi_t \bm U_l^{\perp}\bm U_l^{\perp T} \right) \bm x_t.
\end{align*}
where $w^{\star}_l(\bm x_t,t) = \frac{\exp\left(g_l(\bm x_t, t) \right)}{\sum_{l=1}^K \exp\left(g_l(\bm x_t, t) \right)}$ is a soft-max operator for $g_l(\bm x,t) = \frac{1}{2\sigma_t^2}\zeta_t \|\bm U_l^T \bm x\|^2 + \frac{\delta^2}{2 \sigma_t^2}\xi_t \| \bm U_l^{\perp T} \bm x \|^2 $.
%    \begin{align}
%         \ &w^{\star}_l(\bm x_t,t) := \frac{\exp\left(g_l(\bm x_t, t) \right)}{\sum_{l=1}^K \exp\left(g_l(\bm x_t, t) \right)}, \label{eq:w-k} \\
%           \ &g_l(\bm x,t) = \frac{1}{2\sigma_t^2}\zeta_t \|\bm U_l^T \bm x\|^2 + \frac{\delta^2}{2 \sigma_t^2}\xi_t \| \bm U_l^{\perp T} \bm x \|^2 . \label{eq:softmax}
%    \end{align}
\end{prop}
%    \begin{align}\label{eq:E_MoG}
%        \begin{split}
%        &\hat{\bm x}_{\bm \theta}^{\star}(\bm x_t, t) := \E\left[ \hat{\bm x_0}\vert \bm x_t\right] \\
%        &\quad =\sum_{l=1}^K w^{\star}_l(\bm x_t,t) \left( \zeta_t \bm U_l\bm U_l^T + \xi_t \bm U_l^{\perp}\bm U_l^{\perp T} \right) \bm x_t 
%        \end{split}
%    \end{align}
%\qq{I feel the discussion on generation quality is unnecessary and can be deferred if possible. We should discuss why we can use the posterior estimation as an indicator of representation quality here instead.}
%\paragraph{Remark.}
The proof can be found in \Cref{app:proof_prop} and it is an extension of the result in \citep{wang2024diffusion}. For $\bm x_0$ following noisy \MoLRG, note that the optimal solution $\hat{\bm x}_{\bm \theta}^{\star}(\bm x_t, t) $ of the training loss \eqref{eq:dae_loss} would exactly be $ \E\left[\bm x_0 \vert \bm x_t\right]$. As such, as illustrated in \Cref{fig:use_clean}, the analytical form of the posterior estimation facilitates the study of generation-representation tradeoff across timesteps:
%In the above proposition, we present the \textbf{ground truth} posterior estimation function that a diffusion model aims to approximate by minimizing the training objective defined in (\ref{eq:dae_loss}). We denote this optimal model as $\hat{\bm x}_{\bm \theta}^{\star}$. Under this optimal setting, the trade-off between generation and representation learning dynamics can be analyzed by evaluating posterior estimations at different time steps $t$, .
\begin{itemize}[leftmargin=*]
    \item \emph{The generation quality.} The generation quality of posterior estimation cam be measured by $||\hat{\bm x}_{\bm \theta}^{\star}(\bm x_t, t) - \bm x_0||^2$. As shown in \Cref{lem:E[x_0]_multi}, this error is minimized at $t=0$ with $\sigma_t=0$, where the true class weight satisfies $w^{\star}_k(\bm x_t) = 1$, yielding $\hat{\bm x}_{\bm \theta}^{\star}(\bm x_t, t) = \bm x_0$. As $t$ increases, higher noise levels $\sigma_t$ decrease $w^{\star}_k(\bm x_t)$, causing a monotonic increase in FID, as seen in \Cref{fig:use_clean}.
    \item \emph{The representation quality.} The representation quality follows a unimodal trend across timesteps \citep{xiang2023denoising,tang2023emergent}, which can be measured through the posterior estimator $\hat{\bm x}_{\bm \theta}^{\star}(\bm x_t, t)$ (see \Cref{subsec:rep-quality}). As shown in \Cref{fig:use_clean}, this unimodal behavior creates a trade-off between generation and representation quality, particularly at smaller $t$ when closer to the original image.
    % \zk{This seems to overlap with the theorem?}
%    To better understand the tradeoff between generation FID and posterior accuracy as illustrated in \Cref{fig:use_clean}, we analyze the unimodal representation dynamics using the ground truth posterior estimation function. This is done by studying the \CSNR~with this function, which we discuss in more detail in the following section.
    % Similarly, to analyze the unimodal behavior of representation quality, an appropriate evaluation metric is required. We discuss this metric in the following section.
\end{itemize}




% \subsection{Measuring Representation Quality}\label{subsec:rep-quality}

\subsection{Measuring Posterior Representation Quality}\label{subsec:rep-quality}
% \qq{this might be moved to earlier sections} 



%In this work, we use classification tasks to study representation learning, focusing on two types of accuracies: (i) feature accuracy: This refers to the classification accuracy achieved by applying linear probing on extracted feature representations, and (ii) posterior accuracy: Since intermediate representations in diffusion models are a byproduct of the posterior estimation process, we directly evaluate the classification accuracy using the posterior estimations ($\hat{\bm x_0} = \bm x_{\bm \theta}(\bm x_0, t)$) to analyze the trade-off between generation quality and representation quality. 

For understanding diffusion-based representation learning, we introduce a metric termed Class-specific Signal-to-Noise Ratio (\CSNR) to quantify the posterior representation quality as follows.

\begin{definition}[Class-specific Signal-to-Noise Ratio]
\emph{Suppose the data $\bm x_0$ follows the noisy \MoLRG\;introduced in \Cref{assum:subspace}. Without loss of generality, let $k$ denote the true class of $\mb x_0$. For its associated posterior estimator $\hat{\bm x}_{\bm \theta}$,} %we define \CSNR\; as:
\begin{align*}%\label{eq:csnr_true}
    \mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t) := \E_k \left[\frac{\E_{\bm x_0}[\|\bm U_k\bm U_k^T\hat{\bm x}_{\bm \theta}(\bm x_0, t)\|^2 \mid k ] }{\E_{\bm x_0}[\sum_{l\neq k}\|\bm U_l\bm U_l^T\hat{\bm x}_{\bm \theta}(\bm x_0, t)\|^2 \mid k ]}\right]
\end{align*}
\emph{Here, $\bm U_k$ represents the basis of the subspace corresponding to the true class to which $\bm x_0$ belongs and the $\bm U_l$s with $l \neq k$ denotes the bases of the subspaces for other classes. }
\end{definition}

%\paragraph{Posterior representation quality based upon noisy \MoLRG.} 


% \begin{align}\label{eq:csnr_true}
%     \mathrm{CSNR}(t, f) := \frac{\E_{\bm x_0}[\|\hat{\bm U}_k\hat{\bm U}_k^Tf(\bm x_0, t)\|^2]}{\E_{\bm x_0}[\sum_{l\neq k}\|\hat{\bm U}_l\hat{\bm U}_l^Tf(\bm x_0, t)\|^2]}
% \end{align}

Intuitively, successful prediction of the class for $\bm x_0$ is achieved when the projection onto the correct class subspace, $\|\bm U_k\bm U_k^T \hat{\bm x}_{\bm \theta}(\bm x_0, t)\|$, preserves larger energy than the projections onto subspaces of any other class, $\|\bm U_l\bm U_l^T \hat{\bm x}_{\bm \theta}(\bm x_0, t)\|$. Thus, \CSNR~measures the ratio of the true class signal to irrelevant noise from other classes at a given noise level $t$, serving as a practical metric for evaluating classification performance and hence the representation quality. In this work, we use posterior representation quality as a proxy for studying the representation dynamics of diffusion models for the following reasons:
%, rather than directly analyzing feature quality, for the following reasons:
\begin{itemize}[leftmargin=*]
    \item \emph{Posterior quality reflects feature quality.} Diffusion models $\hat{\bm{x}}_{\bm{\theta}}$ are trained to perform posterior estimation at a given time step $t$ using corrupted inputs, with the intermediate features emerging as a byproduct of this process. Thus, a more class-representative posterior estimation inherently implies more class-representative intermediate features.
    \item \emph{Model-agnostic analysis.} Our goal is to provide a general analysis independent of specific network architectures and feature extraction protocols. Posterior representation quality offers a unified metric that avoids assumptions tied to particular architectures, making the analysis broadly applicable.
\end{itemize}
%\textcolor{orange}{[zzk: posterior representation quality?]} 
%\zk{Also we need to note we use intermediate feature and inner representation interchangeably in our paper.}.
% \zk{The representativeness of the posterior reflects representation quality.}
% Moreover, we note that \CSNR~is applicable to any feature extracting function $f$ \zk{that has subspace structures/linearity within its range}, whether it represents feature extraction or posterior estimation. Accordingly, we adopt \CSNR~as a proxy for classification performance in subsequent analyses and experiments.

% Here, $\hat{\bm U}_k$ represents the basis of the subspace corresponding to the true class to which $f(\bm x_0, t)$ belongs and $\hat{\bm U}_l,l \neq k$ denotes the bases of the subspaces for other classes. Intuitively, successful prediction of the class for $\bm x_0$ is achieved when the projection onto the correct class subspace, $\|\hat{\bm U}_k\hat{\bm U}_k^T f(\bm x_0, t)\|$, is greater than the projections onto subspaces of any other class, $\|\hat{\bm U}_l\hat{\bm U}_l^T f(\bm x_0, t)\|$. Thus, \CSNR~measures the ratio of the true class signal to irrelevant noise from other classes at a given noise level $t$, serving as a practical metric for evaluating classification performance. We note that \CSNR~is applicable to any function $f$, whether it represents feature extraction or posterior estimation. Accordingly, we adopt \CSNR~as a proxy for classification performance in subsequent analyses and experiments.

\subsection{Main Theoretical Results}



% \qq{I feel the main theorem is overly long. You need to introduce the CSNR first, build some intutions on what you want to show, and then describe your results. Otherwise, the reviewer does not understand what you want to show.}

%\ZK{need to be consistent}

% As we discussed in \Cref{subsec:posterior}, based upon the strong correlation between representation quality and the posterior mean estimation, we analyze $\hat{\bm x}_{\mb \theta}^{\star}(\bm x_0, t)$ across different time step $t\in[0,1]$. Here, we use $\mb x_0$ as the input instead of $\mb x_t$ according to our discussion in \Cref{subsec:feature-extraction}. 
Based upon the setup in \Cref{subsec:model} and \Cref{subsec:rep-quality}, we obtain the following results.
%Now, we are ready to state our main theorem as follows:

\begin{theorem}\label{lem:main}(Informal)
% Let data $\bm x_0$ be any arbitrary data point drawn from the \MoLRG~distribution defined in Assumption \ref{assum:subspace} and let $k$ denote the true class $\bm x_0$ belongs to. Then \CSNR~introduced in \eqref{eq:csnr_true} depends on the noise level $\sigma_t$ in the following form: 
%     % Without loss of generality, for any clean $\bm x_0$ from class k (i.e., $\bm x_0 = \bm U_k \bm a_i + \delta\bm U_k^{\perp} \bm e_i$), 
%     % \qq{maybe we should write the decomposition out in the following}
%     \begin{align}\label{eq:csnr}
%         \mathrm{CSNR}(t, \hat{\bm x}_{\textit{approx}}^{\star}) = \frac{1}{(K-1)\delta^2} \cdot\left(\frac{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_k, \delta)}{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_l, \delta)}\right)^2
%     \end{align}
% \qq{I feel maybe we should change $b$ to some greek characters} 
% \qq{needs to remind reviewers what is $\sigma_t$ here}
Suppose the data $\bm x_0$ follows the noisy \MoLRG\;introduced in \Cref{assum:subspace} with $K$ classes and noise level $\delta$, then  the \CSNR~of the optimal denoiser $\hat{\bm x}_\theta^{\star}$ takes the following form:
    % Without loss of generality, for any clean $\bm x_0$ from class k (i.e., $\bm x_0 = \bm U_k \bm a_i + \delta\bm U_k^{\perp} \bm e_i$), 
    % \qq{maybe we should write the decomposition out in the following}
    \begin{align}\label{eq:csnr}
        \mathrm{CSNR}(\hat{\bm x}_\theta^{\star},t) = \frac{1}{(K-1)\delta^2}\cdot \left(\frac{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_t^+, \delta)}{1 + \frac{\sigma_t^2}{\delta^2}h(\hat{w}_t^-, \delta)}\right)^2.
    \end{align}
Here, $h(w, \delta) := (1 - \delta^2)w + \delta^2$ is a monotonically increasing function with respect to $w$. Additionally, $h(\hat{w}_t^+, \delta)$ and $h(\hat{w}_t^-, \delta)$ denote positive and negative class confidence rates with% \qq{does the function below depends on $\sigma_t$? we need to reflect that}\zk{It is a function of $\sigma_t$(or t) and $\delta$, but we omit $\delta$ and use t as a subscript to make it one line} \qq{we can add a bracket below to denote those}
\begin{align*}
\begin{cases}
\hat w_t^+(\sigma_t, \delta) &=\; \mathbb E_k[ \mathbb{E}_{\bm x_0}[w_k(\bm x_0, t)\mid k]], \\
\hat w_t^-(\sigma_t, \delta) &=\; \mathbb E_{k}[\mathbb{E}_{\bm x_0}[w_{l}(\bm x_0, t) \mid k \neq l ]],
\end{cases}
\end{align*}
whose analytical forms can be found in \Cref{app:thm1_proof}.  
\end{theorem}



%$\hat w_t^+ = \mathbb{E}_{\bm x_0}[w_k(\bm x_0, t)]$, $\hat w_t^-= \mathbb{E}_{\bm x_0}[w_{l}(\bm x_0, t)] \;\text{for}\; l\neq k$, and $h(w, \delta) := (1 - \delta^2)w + \delta^2$.
%\begin{align*}&\hat w_t^+ := \mathbb{E}_{\bm x_0}[w_k(\bm x_0, t)]\\
%    &\hat w_t^-:= \mathbb{E}_{\bm x_0}[w_{l}(\bm x_0, t)], l\neq k\\
%    &h(w, \delta) := (1 - \delta^2)w + \delta^2
%\end{align*} 
%where samples $\bm x_0$ are drawn from class $k$ (i.e., $\bm x_0 = \bm U_k \bm a_i + b\bm U_k^{\perp} \bm e_i$)
%we leave the analytical form of $\hat w_t^+$ and $\hat w_t^- $ to the appendix, with which we are able to show that it exhibits a unimodal trend. Note that here $\sigma_t$ denotes the level of additive Gaussian noise introduced during the diffusion training process.
% and $h(w, \delta) := (1 - \delta^2)w + \delta^2$. 
%Since $\delta$ is fixed, $h(w,\delta)$ is a monotonically increasing function with respect to $w$. 
% The term $\hat{\bm x}_{\textit{approx}}^{\star}$ serves as an approximation of $\hat{\bm x}_{\bm \theta}^{\star}$ as defined in \eqref{eq:E_MoG}, obtained by approximating $w_l^{\star}$ in \eqref{eqn:w-k} with $\hat{w}_l$ by taking the expectation inside the softmax with respect to $\bm x_0$.\footnote{We empirically validate the tightness of this approximation in \Cref{fig:assump_validate}.}  
% Note that here $\delta$ represents the magnitude of the fixed intrinsic noise in the data 


    % \xiao{I moved the other part of the thm to next section, not sure if this is better.} \qq{I feel it would be better to move it back, and we can refer later on}
%\qq{just say it is a monotonically increasing function w.r.t. to w}
    % Furthermore, we can decompose $\E_{\bm x_0}[\|f^{\star}(\bm x_0, t)\|^2]$ as:
    % \begin{align}\label{eq:fx_decompose}
    % \begin{split}
    %     \E[\|f^{\star}(\bm x_0, t)\|^2] &= \E\|\bm U_k \bm U_k^Tf^{\star}(\bm x_0, t)\|^2] + \E[\sum_{l \neq k}^K\bm U_l \bm U_l^Tf^{\star}(\bm x_0, t)\|^2] + \E[\|\bm U_{\perp} \bm U_{\perp}^Tf^{\star}(\bm x_0, t)\|^2]
    % \end{split}
    % \end{align}
    % where 
    % \begin{align}\label{eq:fx_decompose_terms}
    % \begin{split}
    %     \E\|\bm U_k \bm U_k^Tf^{\star}(\bm x_0, t)\|^2] &= \left( \frac{\hat{w}_k}{1 + \sigma_t^2} + \frac{(K-1)\delta^2\hat{w}_l}{\delta^2+\sigma_t^2}\right)^2 d \\
    %     \E[\|\bm U_{\perp} \bm U_{\perp}^Tf^{\star}(\bm x_0, t)\|^2] &= \frac{b^6 (n-Kd)}{(\delta^2 + \sigma_t^2)^2} \\
    %     \E[\sum_{l \neq k}^K\bm U_l \bm U_l^Tf^{\star}(\bm x_0, t)\|^2] &= \left( K-1 \right)\left( \frac{\hat{w}_l}{1+\sigma_t^2} + \frac{\delta^2(\hat{w}_k + (K-2)\hat{w}_l)}{\delta^2 + \sigma_t^2} \right)^2 \delta^2 d.
    % \end{split}
    % \end{align}



% In the \MoLRG~setting, the ground truth posterior mean is a combination of $\bm x_t$'s projections onto the signal and noise spaces of each class, with the coefficients determined by a softmax term. Similar to the single Gaussian case, the SNR for the reconstruction is:
% \begin{align}
%     \frac{1}{1 + \sigma_t^2} \;/\; \frac{\delta^2}{\delta^2 + \sigma_t^2} = \frac{\delta^2+\sigma_t^2}{\delta^2(1+\sigma_t^2)}
% \end{align}

% increases as the noise scale grows. Given that the optimal weights involve a softmax term, a linear network is no longer sufficient to capture the optimal solution. Therefore, we employ a more practical MLP-based model to learn the optimal score function.
% \qq{use mathrm for all CSNR}

We defer the formal statement of \Cref{lem:main} and its proof to \Cref{app:thm1_proof}. In the following, we discuss the implications of our result.

\begin{wrapfigure}[12]{R}{0.50\textwidth}
% \begin{figure}[t] % 'r' for right, 'l' for left
    \centering
    \includegraphics[width=0.4\textwidth]{figs/interplay.pdf}
    \caption{Illustration of the interplay between the denoising rate and the class confidence rate.}
    \label{fig:trade_off}
% \end{figure}
\end{wrapfigure}

\paragraph{The unimodal curve of \CSNR\;across noise levels.}
Intuitively, our theorem shows that unimodal curve is mainly induced by the the interplay between the ``denoising rate" $\sigma_t^2/\delta^2$ and the positive class confidence rate $h(\hat{w}_t^+, \delta)$ as noise level $\sigma_t$ increases. As observed in \Cref{fig:trade_off}, the ``denoising rate" ($\sigma_t^2/\delta^2$) increases monotonically with $\sigma_t$ while the class confidence rate $h(\hat{w}_t^+, \delta)$ monotonically declines. Initially, when $\sigma_t$ is small, the class confidence rate remains relatively stable due to its flat slope, and an increasing ``denoising rate" improves the \CSNR, resulting in improved posterior estimation. However, as indicated by \Cref{lem:E[x_0]_multi}, when $\sigma_t$ becomes too large, $h(\hat{w}_t^+,\delta)$ approaches $h(\hat{w}_t^-,\delta)$, leading to a drop in \CSNR, which limits the ability of the model to project $\bm x_0$ onto the correct signal subspace and ultimately hurts posterior estimation. 



\paragraph{Alignment of \CSNR\;with posterior representation quality.} Although our theory is derived from the noisy \MoLRG\; distribution, it effectively captures real-world phenomena. As shown in \Cref{fig:csnr_molrg_match,fig:cifar}, we conduct experiments on both synthetic (i.e., noisy \MoLRG) and real-world datasets (i.e., CIFAR and ImageNet) to measure $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$ as well as the posterior probing accuracy. For posterior probing, we use posterior estimations at different timesteps as inputs for classification. The results consistently show that $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$ follows a unimodal pattern across all cases, mirroring the trend observed in posterior probing accuracy as the noise scale increases. This alignment provides a formal justification for previous empirical findings \citep{xiang2023denoising, baranchuk2021label, tang2023emergent}, which have reported a unimodal trajectory in the representation dynamics of diffusion models with increasing noise levels. Detailed experimental setups are provided in \Cref{app:exp_detail}.
% \qq{need to explain what is posterior probing performance}
% \qq{this sentence is a bit confusing, revise, I suggest to separate into multiple sentences} 

\paragraph{Explanation of generation and representation trade-off.}
Our theoretical findings reveal the underlying rationale behind the generation and representation trade-off: the proportion of data associated with $\delta$ represents class-irrelevant attributes. The unimodal representation learning dynamic thus captures a ``fine-to-coarse" shift \citep{choi2022perception, wang2023diffusion}, where these class-irrelevant attributes are progressively stripped away. During this process, peak representation performance is achieved at a balance point where class-irrelevant attributes are eliminated, while class-essential information is preserved. In contrast, high-fidelity image generation requires capturing the entire data distribution—from coarse structures to fine details—leading to optimal performance at the lowest noise level $\sigma_t$, where class-irrelevant attributes encoded in the $\delta$-term are maximally retained. Thus, our insights explain the trade-off between generation and representation quality. As visualized in \Cref{fig:use_clean} and \Cref{fig:vis_molrg_3class}, representation quality peaks at an intermediate noise level where irrelevant details are stripped away, while generation quality peaks at the lowest noise level, where all details are preserved.
    

% \begin{wrapfigure}[19]{R}{0.44\textwidth}
\begin{figure}[t]
    \centering
    \includegraphics[width = 0.52\textwidth]{figs/posterior_curve_molrg.pdf}
\caption{\textbf{Posterior probing accuracy and associated \CSNR~dynamics in \MoLRG~data.} We plot the posterior probing accuracy and \CSNR~with the posterior estimations obtained from a learned estimator $\bm{\hat x_\theta}$. both of which exhibit a consistent unimodal pattern. Additionally, we include the optimal \CSNR~, calculated from the ground truth posterior function $\bm{\hat x_\theta}^\star$ defined in \Cref{lem:E[x_0]_multi}, as a reference. The estimator is trained on a 3-class \MoLRG~dataset with data dimension $n=50$, subspace dimension $d=15$, and noise scale $\delta=0.5$.}
\label{fig:csnr_molrg_match}
\end{figure}
% \end{wrapfigure}


% \begin{figure*}[t]
% \begin{center}
%     \begin{subfigure}{0.47\textwidth}
%     \includegraphics[width = 0.955\textwidth]{figs/posterior_curve_molrg.pdf}
%     \caption{Posterior Probing Acc. and $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$} 
%     \end{subfigure} \quad %\hspace*{\fill}
%     \begin{subfigure}{0.47\textwidth}
%     \includegraphics[width = 0.955\textwidth]{figs/feature_curve_molrg.pdf}
%     \caption{Feature Probing Acc. and $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$} 
%     \end{subfigure}
%     \end{center}
% \caption{\textbf{Probing accuracy and associated \CSNR~dynamics in \MoLRG~data.} In panel (a), we plot the probing accuracy and the previously discussed \CSNR, both metrics exhibit a consistent unimodal pattern as depicted in \Cref{fig:clean_feature}. Additionally, in (b) we plot the metrics calculated from intermediate features, showing the universality across both posterior and features.}
% \label{fig:csnr_molrg_match}
% \end{figure*}
% mog_csnr.pdf mog_f_csnr.pdf


\begin{figure*}[h]
\begin{center}
    % \begin{subfigure}{0.47\textwidth}
    % \includegraphics[width = 0.955\textwidth]{figs/feature_curve_c10.pdf}
    % \caption{CIFAR10} 
    % \end{subfigure} \quad %\hspace*{\fill}
    % \begin{subfigure}{0.47\textwidth}
    % \includegraphics[width = 0.955\textwidth]{figs/feature_curve_mini.pdf}
    % \caption{MiniImageNet} 
    % \end{subfigure}

    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/posterior_curve_c10.pdf}
    \caption{CIFAR10} 
    \end{subfigure} \quad %\hspace*{\fill}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[width = 0.995\textwidth]{figs/posterior_curve_mini.pdf}
    \caption{MiniImageNet} 
    \end{subfigure}
    \end{center}
\caption{\textbf{Dynamics of posterior probing accuracy and associated \CSNR~on CIFAR10 and MiniImageNet.} Posterior probing accuracy is plotted alongside $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$. Probing accuracy is evaluated on the test set, while the empirical \CSNR~is computed from the training set. Both exhibit an aligning unimodal pattern. We use released EDM models \citep{karras2022elucidating} trained on the CIFAR-10 \citep{krizhevsky2009learning} and ImageNet \citep{deng2009imagenet} datasets, evaluating them on CIFAR-10 and MiniImageNet \citep{vinyals2016matching}, respectively. To compute \CSNR~, we apply PCA on the original CIFAR-10/MiniImageNet images to extract the basis $\bm{U}_k$s. Further details can be found in \Cref{app:exp_detail}. }
% \qq{moved here, please revise, this is also the setup of Figure 5? we can refer to Figure 6}}
\label{fig:cifar}
\end{figure*}
%\caption{\textbf{Dynamics of feature probing accuracy and associated \CSNR~on CIFAR10 and MiniImageNet.} Feature probing accuracy is plotted alongside $\mathrm{CSNR}(t, f)$. Probing accuracy is evaluated on the test set, while the empirical \CSNR~is computed from the training set. Both exhibit an aligning unimodal pattern.}
%\caption{\textbf{Dynamics of posterior probing accuracy and associated \CSNR~on CIFAR10 and MiniImageNet.} The top row presents feature probing accuracy alongside $\mathrm{CSNR}(t, f)$, while the bottom row illustrates posterior probing accuracy and $\mathrm{CSNR}(t, \bm x_{\bm \theta})$. Probing accuracy is evaluated on the test set, whereas \CSNR~is computed from the training set. Both feature and posterior probing accuracy exhibit unimodal patterns that align with their respective \CSNR~trends.  }
% \xiao{grid} \qq{revise} 
%\zk{this actually overlaps with fig3, have to reduce, also we can combine it with the remark}




% In contrast, high-fidelity image generation requires capturing the entire data distribution—from coarse structures to fine details—leading to optimal performance at the lowest noise level, where class-irrelevant attributes and finer image details encoded in the $\delta$-term are maximally retained. Thus, our insights clarify the trade-off between generation and representation quality. As visualized in \Cref{fig:use_clean} and \Cref{fig:vis_molrg_3class}, representation quality peaks at an intermediate noise level where irrelevant details are stripped away, while generation quality peaks at the lowest noise level, where all details are preserved.

% Previous studies \citep{xiang2023denoising, baranchuk2021label, tang2023emergent} have empirically shown that the representation dynamics of diffusion models follow a unimodal curve as the noise scale increases, across various tasks such as classification, segmentation, and image correspondence. Our theoretical findings reveal the underlying rationale behind this phenomenon: the proportion of data associated with $\delta$ represents class-irrelevant attributes or finer image details. The unimodal representation learning dynamic thus captures a ``fine-to-coarse" shift \citep{choi2022perception, wang2023diffusion}, where these details are progressively stripped away. During this process, peak representation performance is achieved at a balance point where class-irrelevant attributes are eliminated, while class-essential information is preserved.

% On the other hand, high-fidelity image generation requires capturing the entire data distribution—from coarse structures to fine details—leading to optimal performance at the lowest noise level, where class-irrelevant attributes and finer image details encoded in the $\delta$-term are maximally retained. Therefore, our theoretical insights shed light on explaining the trade-off between generation and representation quality. This explanation is supported by our visualizations in \Cref{fig:use_clean} and \Cref{fig:vis_molrg_3class}, which show that for both synthetic and real datasets, the representation peak consistently occurs at an intermediate noise level where class-irrelevant details are removed, while the generation peak is reached at the smallest noise level, where all details are preserved.
% ==============


% Previous studies \citep{xiang2023denoising, baranchuk2021label, tang2023emergent} have empirically shown that the representation dynamics of diffusion models follow a unimodal curve as the noise scale increases, across various tasks such as classification, segmentation, and image correspondence. Our findings corroborate this observation, as demonstrated in \Cref{fig:clean_feature}, where the representation quality consistently exhibits a unimodal trend, regardless of the specific network architecture, dataset or task. In the following analysis, we argue that this unimodal behavior arises from subtle differences and trade-offs between the requirements of representation learning and the generative nature of diffusion models.

% High-fidelity image generation demands that diffusion models capture every aspect of the data distribution—from coarse structures to fine details. In contrast, representation learning, particularly for high-level tasks such as classification \citep{allen2022feature}, prefers an abstract representation, where finer image details may even act as `noise' that hinders performance. As shown in \Cref{fig:use_clean}, as the noise level increases, the predicted posteriors for clean input $\bm x_0$ transition from `fine' to `coarse' \citep{wang2023diffusion,choi2022perception}, gradually removing fine-grained details. For the classification task in the plot, the best performance is achieved when the posterior estimation retains the essential information while discarding some class-irrelevant details. These findings indicate a trade-off between generative quality and representation performance \citep{chen2024deconstructing}, prompting us to attribute variations in feature quality across noise levels to differences in posterior prediction.


%\subsection{Alignment of CSNR with Representation Quality}\label{subsec:theory_verify}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/vis_molrg.pdf}
%     \vspace{-0.1in}
%     \caption{\textbf{Visualization of posterior estimation for a clean input, higher \CSNR~correspondings to higher classification accuracy.} The \textbf{same} \MoLRG~data is fed into the models; each row represents a different denoising model, and each column corresponds to a different time step with noise scale ($\sigma_t$). The \textcolor{red}{red} box indicates the best posterior estimation and feature probing accuracy.}
%     \label{fig:vis_molrg_3class}
% \end{figure*}



%We conduct experiments on both synthetic and real-world datasets to validate our theoretical insights into the dynamics of representation learning.



%The results for the synthetic and real datasets are shown in \Cref{fig:csnr_molrg_match} and \Cref{fig:cifar}, respectively. As depicted in the plots, $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$ consistently follows a unimodal pattern, aligning with the trend of posterior probing performance as the noise scale increases.

% We extract intermediate features and posterior predictions at each time step and evaluate the classification performance on the test set using linear and MLP probing, respectively. For \CSNR~calculation, we use the raw inputs (i.e., $\bm x_0$s drawn from the \MoLRG~distribution for the synthetic dataset and the original CIFAR10/MiniImageNet images) to compute the basis $\bm U_k$s, followed by the empirical computation of \CSNR~for posterior predictions, denoted as $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$. For \CSNR~on feature representations, we compute the basis using the features extracted at each time step and subsequently calculate \CSNR~for the features, denoted as $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$.

% The results for the synthetic datasets are presented in \Cref{fig:csnr_molrg_match} and real datasets in \Cref{fig:cifar,fig:cifar_2}. As illustrated in the plots, $\mathrm{CSNR}(\hat{\bm x}_{\bm \theta},t)$ consistently exhibits a unimodal pattern similar to the posterior probing performance as the noise scale increases. A similar trend is observed for $\mathrm{CSNR}(\hat{f}_{\bm \theta},t)$, which aligns with the feature probing accuracy. These findings support our theoretical predictions.

% \paragraph{Trade-offs between generation and representation quality.}
% To further illustrate the trade-offs between generation and representation quality, we visualize the posterior estimations and \CSNR~values of the synthetic \MoLRG~dataset at different noise scales in \Cref{fig:vis_molrg_3class}. As observed, the highest generation quality occurs at the smallest noise scale, where the generated samples closely resemble the original inputs. However, optimal generation quality does not necessarily translate to optimal representation quality, as measured by probing performance. Instead, the best representation performance is achieved at intermediate noise scales. This observation aligns with our theoretical insights, suggesting that initially, increasing the noise scale mitigates the $\delta$-related data noise, leading to cleaner posterior estimations and improved probing accuracy. However, as the noise scale continues to rise, the class confidence rate diminishes, causing class overlap and ultimately degrading feature quality and probing performance. Similar trends can be observed in real datasets, where CIFAR10 results are presented at the beginning of this paper in \Cref{fig:use_clean}.