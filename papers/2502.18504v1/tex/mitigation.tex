
\begin{table*}[t]
\resizebox{\linewidth}{!}{%
\centering
\small
\begin{tabular}{l|r|r|r}\toprule
\multirow{2}{*}{\textbf{Model}} &\multicolumn{1}{c|}{\textbf{ASR (\%)}} &\multicolumn{1}{c|}{\textbf{Average Queries Per Jailbreak}} &\multicolumn{1}{c}{\textbf{Number of Jailbreaking Templates}} \\
&\multicolumn{1}{c|}{(higher is better)} &\multicolumn{1}{c|}{(lower is better)} &\multicolumn{1}{c}{(higher is better)} \\\cmidrule{1-4}
Gemma 7B (Original) &100 &6.88 &30 \\
Gemma 7B (Fine-tuned) &26 &75.88 &26 \\
\bottomrule
\end{tabular}
}
\caption{\bedrockfuzz attack performance on Gemma 7B before and after fine-tuning evaluated on 200 harmful behaviors from HarmBench~\cite{mazeika2024harmbench} text standard dataset with a target model query budget of 4000.}
\label{tab:rq1_ft}
\end{table*}



\subsection{Improving In-built Defenses with Supervised Adversarial Training}
\label{sec:defense}
Jailbreaking artifacts generated by \bedrockfuzz represent high-quality data that can be utilized to develop effective defensive and mitigation techniques. One defensive technique is to adapt jailbreaking data to perform supervised fine tuning with the objective of improving in-built safety mitigation in the fine-tuned model.




We performed instruction fine tuning for Gemma 7B using HuggingFace SFTTrainer\footnote{\url{https://huggingface.co/docs/trl/sft_trainer}} with QLoRA~\cite{dettmers2023qlora} and FlashAttention~\cite{dao2022flashattention}.
We collected a total of 1171 attack prompts that were successful in jailbreaking Gemma 7B (200 from Table~\ref{tab:rq1} and 971 from Table~\ref{tab:rq3}), paired each one of them with sampled safe responses generated by Gemma 7B for the corresponding question, and used these $( \text{successful attack prompt}, \text{safe response} )$ pairs as the fine-tuning dataset.



\begin{table}[!htp]
\centering
\small
\begin{tabular}{l|rr}\toprule
\multirow{2}{*}{Metric (\%)} &\multicolumn{2}{c}{Gemma 7B} \\\cmidrule{2-3}
&Original &Fine-tuned \\\midrule
ASR &100 &35 \\\midrule
Top-1 Template ASR &75 &16 \\
Top-5 Template ASR &98 &30 \\
\bottomrule
\end{tabular}
\caption{Templates learnt with \bedrockfuzz in \textit{RQ1} (Table~\ref{tab:rq1}) evaluated on 100 harmful questions from JailBreakBench~\cite{chao2024jailbreakbench} for attacking Gemma 7B before and after fine tuning.}
\label{tab:rq3_ft}
\end{table}


Tables~\ref{tab:rq1_ft} \&~\ref{tab:rq3_ft} present the comparison of the original versus fine-tuned Gemma 7B.
We found attacking the fine-tuned model by \bedrockfuzz to generate new successful templates to become much more difficult, reaching a much lower ASR and requiring many more queries per jailbreak (Table~\ref{tab:rq1_ft}).
Similarly, the fine-tuned model showed significantly lower attack success rates when evaluated on the previously-successful templates (Table~\ref{tab:rq3_ft}).
