\section*{Ethics Statement}





Our research on jailbreaking techniques reveals potential vulnerabilities in LLMs that could be exploited to generate harmful content. While this presents inherent risks, we believe transparency and full disclosure are essential for several reasons:

\begin{itemize}
    \item The methodologies discussed are relatively straightforward and have been previously documented in existing literature. With sufficient resources and dedication, malicious actors could independently develop similar techniques.
    \item By revealing these vulnerabilities, we provide vital information to model developers to assess and enhance the robustness of their systems against adversarial attacks.
\end{itemize}

To minimize potential misuse of our research, we have taken the following precautionary measures:
\begin{itemize}
    \item We included clear content warnings about potentially harmful content.
    \item We will limit distribution of specific jailbreaking templates to verified researchers.
    \item We included \S\ref{sec:defense} that describes details about how to improve in-built defenses using red-teaming data generated with our techniques.
\end{itemize}

The incremental risk posed by our findings is minimal since many effective jailbreaking techniques are already public. Our primary goal is to advance the development of more robust and safer AI systems by identifying and addressing their vulnerabilities. We believe this research will ultimately benefit the AI community by enabling the development of better safety measures and alignment techniques.
