\section{Conclusions \& Future Work}
\label{sec:conclusion}
We presented \bedrockfuzz, a significant upgrade over~\cite{yu2023gptfuzzer} for effectively jailbreaking LLMs automatically in practice using black-box mutation-based fuzzing. Our experimental evaluation showed \bedrockfuzz achieves $\geq 95\%$ ASR consistently while requiring $\sim$3x fewer queries than GPTFuzzer. 
Templates learnt with \bedrockfuzz generalize to unseen harmful questions directly. Supervised adversarial training using jailbreaking artifacts generated with \bedrockfuzz significantly improved in-built model defenses to prompt attacks.

Future work includes presenting evaluation over an extended set of leading LLMs, comparison against latest/concurrent jailbreaking methods~\cite{liu2024autodan,pavlova2024automated, lin2024pathseeker, chen2024llm,liu2024flipattack}, conducting ablation studies for additional hyper parameters (Appendix~\ref{app:implement}), exploring new upgrades \& heuristics, and diving deep into devising effective defensive/mitigation techniques in practice.
