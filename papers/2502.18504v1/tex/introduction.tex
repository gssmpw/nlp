\section{Introduction}
\label{sec:introduction}

\begin{table*}[bp]
\resizebox{\linewidth}{!}{%
\centering
\small
\begin{tabular}{l|rr|rr|rr}\toprule
\multirow{3}{*}{\textbf{Model}} &\multicolumn{2}{c|}{\textbf{ASR (\%)}} &\multicolumn{2}{c|}{\textbf{Average Queries Per Jailbreak}} &\multicolumn{2}{c}{\textbf{Number of Jailbreaking Templates}} \\
&\multicolumn{2}{c|}{(higher is better)} &\multicolumn{2}{c|}{(lower is better)} &\multicolumn{2}{c}{(higher is better)} \\\cmidrule{2-7}
&GPTFuzzer &\bedrockfuzz &GPTFuzzer &\bedrockfuzz &GPTFuzzer &\bedrockfuzz \\\midrule
GPT-4o &28 &\textbf{98} &73.32 &\textbf{20.31} &8 &\textbf{38} \\
GPT-4o Mini &34 &\textbf{100} &60.27 &\textbf{14.43} &7 &\textbf{28} \\
GPT-4 Turbo &58 &\textbf{100} &34.79 &\textbf{13.79} &10 &\textbf{26} \\
GPT-3.5 Turbo &100 &100 &3.12 &\textbf{2.84} &8 &\textbf{12} \\\midrule
Gemma 7B &100 &100 &13.10 &\textbf{6.88} &22 &\textbf{30} \\
Gemma 2B &36 &\textbf{100} &57.13 &\textbf{10.15} &14 &\textbf{27} \\
\bottomrule
\end{tabular}
}
\caption{Comparison of \bedrockfuzz versus GPTFuzzer~\cite{yu2023gptfuzzer} on 200 harmful behaviors from HarmBench~\cite{mazeika2024harmbench} text standard dataset with a target model query budget of 4000.}
\label{tab:rq1}
\end{table*}

With the rapid advances in applications powered by large-language models (LLMs), integrating responsible AI practices into the AI development lifecycle is becoming increasingly critical.
Red teaming LLMs using automatic jailbreaking methods has emerged recently, that adaptively generate adversarial prompts to attack a target LLM effectively.
These jailbreaking methods aim to bypass the target LLM's safeguards and trick the model into generating harmful responses.

Existing jailbreaking methods can be broadly categorized into a) white-box methods like~\cite{zou2023universal, wang2024a, liao2024amplegcg, paulus2024advprompter,andriushchenko2024jailbreaking, zhou2024don}, etc., which require full or partial knowledge about the target model, and b) black-box methods like~\cite{mehrotra2023tree, chao2023jailbreaking,takemoto2024all,sitawarin2024pal,liu2023autodan, yu2023gptfuzzer, samvelyan2024rainbow, zeng2024johnny, gong2024effective, yao2024fuzzllm}, etc., which only need API access to the target model.
In particular, GPTFuzzer~\cite{yu2023gptfuzzer} proposed using mutation-based fuzzing to explore the space of possible jailbreaking templates. The generated templates (also referred as mutants) can be combined with any harmful question to create attack prompts, which are then employed to jailbreak the target model. Figure~\ref{fig:motivating} in the appendix provides a motivating example of this approach.

Our objective is to produce sets of high quality $( \text{attack prompt}, \text{harmful response} )$ pairs \textit{at scale} that can be utilized to identify vulnerabilities to prompt attacks in a target model and help in developing defensive/mitigation techniques, such as improving in-built defenses in the target model or developing effective external guardrails.\footnote{To encompass a wide variety of LLMs and situations where the system prompt is inaccessible, we limit our threat model to forcing a LLM to generate harmful responses through black box access via user prompts only.}


We found GPTFuzzer as the most fitting to our needs since it enables creating attack prompts at scale by combining arbitrary harmful questions with jailbreaking templates that are automatically learnt with black-box access to the target model. However, when applying GPTFuzzer (or its extensions) in practice, we observed several limitations that resulted in sub-optimal attack success rates and incurred high query costs.
First, the mutant search space considered is quite limited and lacked even simple refusal suppression techniques that have shown impressive effectiveness~\cite{wei2024jailbroken}.
Second, the learned templates often jailbroke the same questions, leaving more challenging questions unaddressed.
Third,  GPTFuzzer combines each generated template with each question, often unnecessarily, resulting in inefficient exploration of the mutant search space.
\begin{figure*}[t]
\centerline{\includegraphics[width=\linewidth]{figs/overview.pdf}}
\caption{Overview of \bedrockfuzz}
\label{fig:workflow}
\end{figure*}

To overcome these limitations, we developed \bedrockfuzz that (1) expands the mutation library, (2) improves search with new selection policies, and (3) adds efficiency-focussed heuristics.
\bedrockfuzz achieves a near-perfect attack success rate across a wide range of target LLMs, significantly reduces query costs, and learns templates that generalize well to new unseen harmful questions.
Our key contributions include:
\begin{itemize}
    \item We introduce a collection of upgrades to improve template-based mutation-based fuzzing to automatically generate effective jailbreaking templates efficiently.

    \item We implement our proposed upgrades in \bedrockfuzz, a fuzzing framework for automatically jailbreaking LLMs effectively in practice. \bedrockfuzz forces a target model to produce harmful responses through black box access via single-turn user prompts within average $\sim$20 queries per jailbreak.

    \item We perform an extensive experimental evaluation of \bedrockfuzz on a collection of open and closed LLMs using public datasets. \bedrockfuzz consistently achieves impressive attack success rates compared to GPTFuzzer (Table~\ref{tab:rq1}) and other state-of-the-art techniques (Table~\ref{tab:rq2}). Templates learnt with \bedrockfuzz generalize well to new unseen harmful behaviors directly (Table~\ref{tab:rq3}). We also present ablation studies indicating the contribution of each individual upgrade we added in \bedrockfuzz (Table~\ref{tab:rq4}). 

    \item We present how red-teaming data generated with \bedrockfuzz can be utilized to improve in-built model defenses through supervised adversarial training (Tables~\ref{tab:rq1_ft} \&~\ref{tab:rq3_ft}).
\end{itemize}


