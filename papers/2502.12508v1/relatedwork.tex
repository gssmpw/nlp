\section{Related Work}
\textbf{The traditional theory of benign overfitting}   
The traditional theory of benign overfitting has obtained a series of research results aimed at understanding this phenomenon in linear, kernel, and random feature models. \cite{Bartlett_2020} set risk bounds for the minimum norm interpolator, stressing overparameterization. \cite{zou2021benignoverfittingconstantstepsizesgd} provided an excess risk bound for constant stepsize SGD. \cite{Liao_2021} extended the analysis to random Fourier feature regression with fixed ratio sample size, data dimension, and number of random features. \cite{adlam2021randommatrixperspectivemixtures} incorporated bias terms into the model. \cite{tsigler2022benignoverfittingridgeregression} extended findings to ridge regression and determined optimal regularization. \cite{mallinar2024benigntemperedcatastrophictaxonomy} found tempered overfitting in certain kernels. \cite{JMLR:v25:22-1389}  extended this to the “multiple random feature model” and discovered multiple descent. 

\textbf{Benign overfitting in transformer}   Benign overfitting in transformers has attracted significant attention in the research field. Some studies utilize the feature learning framework to explore this phenomenon within transformer architectures. \cite{li2024optimizationgeneralizationtwolayertransformers} delved into symbolic gradient descent in two layer transformers, but the research was only concerned with harmful overfitting. \cite{jiang2024unveilbenignoverfittingtransformer} explored the training process of visual transformers. Their work uncovered the training dynamics and clarified the difference between benign and harmful overfitting.  Additionally, there has been new theoretical research on benign overfitting.\cite{jelassi2022visiontransformersprovablylearn,tarzanagh2024transformerssupportvectormachines,tian2023scansnapunderstandingtraining} have focused their studies on the training dynamics of transformers. Analyzing these dynamics helps to accurately describe the optimization process, providing valuable insights into how the model converges and generalizes. \cite{jin2024provableincontextlearningmixture} carried out a theoretical analysis of a Transformer for MoR context learning.\cite{huang2023incontextconvergencetransformers}  explored the training of single layer transformers. \cite{li2024nonlineartransformerslearngeneralize}  examined transformers with nonlinear self-attention. \cite{frei2024trainedtransformerclassifiersgeneralize} investigated the performance of linear Transformers in random linear classification tasks and found that benign overfitting can occur under certain conditions.