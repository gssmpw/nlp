@article{Bartlett_2020,
   title={Benign overfitting in linear regression},
   volume={117},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1907378117},
   DOI={10.1073/pnas.1907378117},
   number={48},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
   year={2020},
   month=apr, pages={30063–30070} }

@article{JMLR:v25:22-1389,
  author  = {Xuran Meng and Jianfeng Yao and Yuan Cao},
  title   = {Multiple Descent in the Multiple Random Feature Model},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {44},
  pages   = {1--49},
  url     = {http://jmlr.org/papers/v25/22-1389.html}
}

@article{Liao_2021,
   title={A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent*},
   volume={2021},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/ac3a77},
   DOI={10.1088/1742-5468/ac3a77},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
   year={2021},
   month=dec, pages={124006} }

@misc{adlam2021randommatrixperspectivemixtures,
      title={A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning}, 
      author={Ben Adlam and Jake Levinson and Jeffrey Pennington},
      year={2021},
      eprint={1912.00827},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1912.00827}, 
}

@misc{frei2024trainedtransformerclassifiersgeneralize,
      title={Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context}, 
      author={Spencer Frei and Gal Vardi},
      year={2024},
      eprint={2410.01774},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01774}, 
}

@misc{huang2023incontextconvergencetransformers,
      title={In-Context Convergence of Transformers}, 
      author={Yu Huang and Yuan Cheng and Yingbin Liang},
      year={2023},
      eprint={2310.05249},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.05249}, 
}

@misc{jelassi2022visiontransformersprovablylearn,
      title={Vision Transformers provably learn spatial structure}, 
      author={Samy Jelassi and Michael E. Sander and Yuanzhi Li},
      year={2022},
      eprint={2210.09221},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.09221}, 
}

@misc{jiang2024unveilbenignoverfittingtransformer,
      title={Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization}, 
      author={Jiarui Jiang and Wei Huang and Miao Zhang and Taiji Suzuki and Liqiang Nie},
      year={2024},
      eprint={2409.19345},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.19345}, 
}

@misc{jin2024provableincontextlearningmixture,
      title={Provable In-context Learning for Mixture of Linear Regressions using Transformers}, 
      author={Yanhao Jin and Krishnakumar Balasubramanian and Lifeng Lai},
      year={2024},
      eprint={2410.14183},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2410.14183}, 
}

@misc{li2024nonlineartransformerslearngeneralize,
      title={How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?}, 
      author={Hongkang Li and Meng Wang and Songtao Lu and Xiaodong Cui and Pin-Yu Chen},
      year={2024},
      eprint={2402.15607},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15607}, 
}

@misc{li2024optimizationgeneralizationtwolayertransformers,
      title={On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent}, 
      author={Bingrui Li and Wei Huang and Andi Han and Zhanpeng Zhou and Taiji Suzuki and Jun Zhu and Jianfei Chen},
      year={2024},
      eprint={2410.04870},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.04870}, 
}

@misc{mallinar2024benigntemperedcatastrophictaxonomy,
      title={Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting}, 
      author={Neil Mallinar and James B. Simon and Amirhesam Abedsoltan and Parthe Pandit and Mikhail Belkin and Preetum Nakkiran},
      year={2024},
      eprint={2207.06569},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.06569}, 
}

@misc{tarzanagh2024transformerssupportvectormachines,
      title={Transformers as Support Vector Machines}, 
      author={Davoud Ataee Tarzanagh and Yingcong Li and Christos Thrampoulidis and Samet Oymak},
      year={2024},
      eprint={2308.16898},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.16898}, 
}

@misc{tian2023scansnapunderstandingtraining,
      title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer}, 
      author={Yuandong Tian and Yiping Wang and Beidi Chen and Simon Du},
      year={2023},
      eprint={2305.16380},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.16380}, 
}

@misc{tsigler2022benignoverfittingridgeregression,
      title={Benign overfitting in ridge regression}, 
      author={A. Tsigler and P. L. Bartlett},
      year={2022},
      eprint={2009.14286},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2009.14286}, 
}

@misc{zou2021benignoverfittingconstantstepsizesgd,
      title={Benign Overfitting of Constant-Stepsize SGD for Linear Regression}, 
      author={Difan Zou and Jingfeng Wu and Vladimir Braverman and Quanquan Gu and Sham M. Kakade},
      year={2021},
      eprint={2103.12692},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.12692}, 
}

