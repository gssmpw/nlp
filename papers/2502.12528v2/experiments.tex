\section{Experiment}\label{sec: experiment}


In this section, we implement and evaluate our algorithm for both the delay-as-loss and delay-as-reward settings.
For simplicity, we only consider the non-contextual setting.
Since there are no existing algorithms for this problem (to the best of our knowledge), 
we consider a simple benchmark that applies the standard \texttt{LinUCB} algorithm only using the currently available observations (see \pref{alg:linUCB} in \pref{app: experiment} for details).
We point out that is simple approach to handling delayed feedback is indeed very common in the literature and in fact enjoys favorable guarantees at least for some problems~\citep{thune2019nonstochastic,van2023unified}.

\textbf{Experiment setup} The experiment setup is as follows. We set the dimension $n\in\{6,8,10\}$ and the size of the action set $|\calA|=50$. The model parameter $\theta$ is set to be $\frac{|\nu|}{\|\nu\|_2}$ where $\nu$ is drawn from the $n$-dimensional standard normal distribution and $|\nu|$ denotes the entry-wise absolute value of $\nu$ to make sure that $\theta\in \R_+^n\cap \mathbb{B}_2^n(1)$. Each action $a\in \calA$ is constructed by first sampling $a_i$ uniformly from $[0,1]$ for all $i\in [n]$ and then normalizing it to unit $\ell_2$-norm. When an action $a_t$ is chosen at round $t$, the payoff $u_t$ is defined as follows: with probability $1-\mu_{a_t}$, $u_t$ is drawn from $\calU[0,\mu_{a_t}]$, and with probability $\mu_{a_t}$, $u_t$ is drawn from $\calU[\mu_{a_t},1]$. It is a valid assignment since $\mu_{a_t}\in[0,1]$ and direct calculation shows that $\E[u_t]=(1-\mu_{a_t})\cdot \frac{\mu_{a_t}}{2}+\mu_{a_t}\cdot\frac{1+\mu_{a_t}}{2}=\mu_{a_t}$.
The number of iterations $T$ is $16000$ and the maximal possible delay $D$ is $1000$.
For simplicity, we also ignore the role of $B$ in our algorithms.

\textbf{Results} 
In \pref{fig:synthetic_dataset}, we plot the mean and the standard deviation of the regret over $8$ independent experiments with different random seeds, for each $n\in\{6,8,10\}$ (the columns) and also both delay-as-loss and delay-as-reward (the rows).
We observe that our algorithm consistently outperforms \texttt{LinUCB} in all setups. 
Also, in all runs, after about $9$ to $12$ epochs, our algorithm eliminates a significant number of bad actions, leading to almost constant regret after that point (and explaining the ``phase transition'' in the plots).
