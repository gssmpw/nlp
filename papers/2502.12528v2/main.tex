\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}



\icmltitlerunning{Contextual Linear Bandits with Delay as Payoff}
\include{command}
\begin{document}

\twocolumn[
\icmltitle{Contextual Linear Bandits with Delay as Payoff}
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mengxiao Zhang}{a}
\icmlauthor{Yingfei Wang}{b}
\icmlauthor{Haipeng Luo}{c}
\end{icmlauthorlist}

\icmlaffiliation{a}{University of Iowa}
\icmlaffiliation{b}{University of Washington}
\icmlaffiliation{c}{University of Southern California}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNotice{}


 

\begin{abstract}
A recent work by \citet{schlisselberg2024delay} studies a delay-as-payoff model for stochastic multi-armed bandits, where the payoff (either loss or reward) is delayed for a period that is proportional to the payoff itself.
While this captures many real-world applications, the simple multi-armed bandit setting limits the practicality of their results.
In this paper, we address this limitation by studying the delay-as-payoff model for contextual linear bandits.
Specifically, we start from the case with a fixed action set and propose an efficient algorithm whose regret overhead compared to the standard no-delay case is at most
$D\Delta_{\max}\log T$, where $T$ is the total horizon, $D$ is the maximum delay, and $\Delta_{\max}$ is the maximum suboptimality gap. 
When payoff is loss, we also show further improvement of the bound, demonstrating a separation between reward and loss similar to \citet{schlisselberg2024delay}.
Contrary to standard linear bandit algorithms that construct least squares estimator and confidence ellipsoid, the main novelty of our algorithm is to apply a phased arm elimination procedure by only picking actions in a \emph{volumetric spanner} of the action set, which addresses challenges arising from both payoff-dependent delays and large action sets.
We further extend our results to the case with varying action sets by adopting the reduction from~\citet{hanna2023contexts}.  
Finally, we implement our algorithm and showcase its effectiveness and superior performance in experiments.
\end{abstract}




\input{intro}
\input{preliminary}
\input{linear_bandits}
\input{contextual_bandits}
\input{experiments}

\section{Conclusion}
In this work, we initiate the study of the delay-as-payoff model for contextual linear bandits and develop provable algorithms that require novel ideas compared to standard linear bandits.
Interesting future directions include proving matching regret lower bounds and extending our results to general payoff-dependent delays~\citep{lancewicki2021stochastic} and other even more challenging settings, such as those with intermediate observations~\citep{esposito2023delayed} or
evolving observations~\citep{bar2024non}.


\bibliography{ref}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\input{appendix_linear}
\input{appendix_contextual}
\input{appendix_experiment}


\end{document}

