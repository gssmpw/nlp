

\section{Extension to Contextual Linear Bandits}\label{sec: contextual}


In this section, we extend our results to the stochastic contextual setting where the action set at each round is drawn i.i.d. from a distribution $\dist$. 
While the arm elimination procedure is critical in solving our problem in the non-contextual case with a fixed action set, it is not clear (if possible at all) to directly generalize it to the contextual setting due to the dynamic nature of the action set.


Fortunately, a recent work by \citet{hanna2023contexts} proposes a reduction from contextual linear bandits to non-contextual linear bandits (both without delay).
At a high level, this reduction utilizes a subroutine of a non-contextual linear bandits algorithm by constructing a fixed action for each possible parameter $\theta$ of the contextual bandit instance. 
Importantly, the subroutine needs to be able to deal with an $\epsilon$-misspecified model, where the loss of each $a\in \calA$ is almost linear: $\mu_a=\inner{a,\theta}+\epsilon_a\in [0,1]$, with $\epsilon \geq \max_{a\in\calA}|\epsilon_a|$ indicating the misspecification level. 
It turns out that, a simple modification of our \pref{alg:lossLB} can address such misspecification --- it only requires incorporating the misspecification level $\epsilon$ into the criteria of arm elimination;
see \pref{alg:lossLBmis} and specifically its \pref{line:eliminate-mis} for details.


We then plugin this subroutine, denoted as $\Alg_{\nctx}$, into their reduction, as shown in \pref{alg:reduction}.
Specifically, the algorithm first constructs a $\frac{1}{T}$-cover $\Theta'$ of the parameter space $\Theta=\R_+^n\cap\mathbb{B}_2^n(1)$ with size $|\Theta'| = \mathcal{O}(T^n)$. 
It then proceeds in epochs with doubling length. 
At the start of epoch $m$, 
a new \emph{fixed} action set $\mathcal{X}_m = \{g^{(m)}(\theta) : \theta \in \Theta'\}$ is constructed, where $g^{(m)}(\theta)$ is the averaged optimal action over the previous $m-1$ epochs, assuming the model parameter being $\theta$.
Then, a new instance of $\Alg_{\nctx}$ with action set $\mathcal{X}_m$ and some 
misspecification level $\epsilon_m$ is initiated and run for the entire epoch.
At each round $t$ of this epoch, $\Alg_{\nctx}$ outputs an action $g^{(m)}(\theta_t) \in \mathcal{X}_m$, and the algorithm's final decision upon receiving the true action set $\calA_t$ is $a_t=\argmin_{a\in \calA_t}\inner{a,\theta_t}$.
Finally, at the end of this round, all newly observed losses are sent to $\Alg_{\nctx}$.


\begin{figure*}[t]
\centering

\includegraphics[width=0.33\textwidth]{Figure/cost_dimension_6.pdf}
\includegraphics[width=0.33\textwidth]{Figure/cost_dimension_8.pdf}
\includegraphics[width=0.33\textwidth]{Figure/cost_dimension_10.pdf}

\includegraphics[width=0.33\textwidth]{Figure/reward_dimension_6.pdf}
\includegraphics[width=0.33\textwidth]{Figure/reward_dimension_8.pdf}
\includegraphics[width=0.33\textwidth]{Figure/reward_dimension_10.pdf}
\caption{Comparison of the empirical results of our algorithm and \texttt{LinUCB}. The top row is the delay-as-loss setting and the bottom row is the delay-as-reward setting. The left, middle, and right column correspond to $n=6,8,10$ respectively.}
\label{fig:synthetic_dataset}
\end{figure*}

\paragraph{Guarantees and Analysis}
Even though our algorithm is a direct application of the reduction of~\citet{hanna2023contexts}, it is a priori unclear whether it enjoys any favorable regret guarantee in the delay-as-loss setting.
By adopting and generalizing their analysis, we show that this is indeed the case.
Before introducing our results, we define the following quantities:
    \begin{align*}
        g(\theta) &\triangleq \E_{\calA\sim \dist}\left[\argmin_{a\in\calA}\inner{a,\theta}\right],\\
        \Delta_{\min}^{\nctx} &\triangleq\min_{\theta'\in \Theta', \inner{g(\theta'),\theta}\neq \inner{ g(\theta),\theta}}\E\left[\inner{g(\theta)-g(\theta'),\theta}\right],\\
        \Delta_{\max}^{\nctx}  &\triangleq\max_{\theta'\in \Theta'}\E\left[\inner{g(\theta)-g(\theta'),\theta}\right],\\
        \overline{d}^{\star} &\triangleq D\cdot \inner{g(\theta),\theta} = D\cdot \E_{\calA\sim \dist}\left[\min_{a\in \calA}\inner{a,\theta}\right],
    \end{align*}
    where $g(\theta)$ denotes the optimal action in expectation, $\Delta_{\min}^{\nctx}$ ($\Delta_{\max}^{\nctx}$) denotes the minimum (maximum) suboptimality gap for the reduced non-contextual linear bandit instance, and $\overline{d}^\star$ denotes the expected delay of the optimal action.

\begin{theorem}\label{thm:reduction}
    \pref{alg:reduction} with %$t^{(m)}=2^{m-1}$ and 
    $\delta = 1/T^2$ guarantees
    \begin{align*}
        &\Reg =\order\big(n\sqrt{T\log T}+\min\{V_1,V_2\} \\
        &\quad\quad\quad\quad +\log(\overline{d}^\star)\min\{W_1,W_2\}\big),
    \end{align*}
     where $V_1=\frac{n^3\log^2(T)\log(T/n)\log(\overline{d}^\star)}{\Delta_{\min}^{\nctx}}$, $V_2=n^{1.5}\sqrt{T\log(\overline{d}^\star)\log(T)}$, $W_1=\log T(n\overline{d}^\star\log(T/n)+D\Delta_{\max}^{\nctx})$, and $W_2=D\Delta_{\max}^{\nctx}\log T\log(T/n)$.
\end{theorem}
The proof is deferred to \pref{app: contextual}. 
The regret bound is in the same spirit as the one for the non-contextual case (\pref{thm:main-non-contextual}) and consists of a term for standard regret and a term for delay overhead.
The standard part unfortunately suffers higher dependence on the dimension $n$, while the delay overhead is in a similar problem-dependent form.
We remark that this is the first regret guarantee for contextual linear bandits with delay-as-payoff, resolving an open problem asked by \citep{schlisselberg2024delay}.