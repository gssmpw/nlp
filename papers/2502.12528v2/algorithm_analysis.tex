\subsection{Analysis}\label{sec: alg}
In this section, we provide a proof sketch of \pref{thm:main-non-contextual}. Detailed proofs are deferred to \pref{app:loss}.

The proof starts by proving that $\UCB_m(a)$ and $\LCB_m(a)$ are indeed valid UCB and LCB respectively for all actions in $\calA_m$. 
This follows from first using standard concentration inequalities to show that $\hat{\mu}_{m,1}^+(a)$ and $\hat{\mu}_{m,2}^+(a)$ ($\hat{\mu}_{m,1}^-(a)$ and $\hat{\mu}_{m,2}^-(a)$) are valid UCBs (LCBs) for each action in the spanner, 
and then generalizing it to every action $a \in \calA_m$ according to its decomposition over the actions in the spanner.

With this property, our analysis then proceeds to control the regret of \pref{alg:lossLB} for each guess of $B$ separately. Let $\calT_B$ be the set of rounds when \pref{alg:lossLB} runs with guess $B$. 
In \textbf{Step 1}, we first show that the use of $\LCB_{m,2}(a)$ and $\UCB_m(a)$ ensures a regret bound of $\order\left(\min\{R_1,R_2\}+D\Delta_{\max}\log(T/n)\right)$ where $R_1=\frac{n^2\log(KT)\log(T/n)}{\Delta_{\min}}$ and $R_2=n\sqrt{|\calT_B|\log(KT)}$,
and then in \textbf{Step 2}, we show that the use of $\LCB_{m,1}(a)$ and $\UCB_m(a)$ ensures a regret bound of
$\order(\min\{R_1,R_2\}+(nd^\star+DB)\log(T/n)+D\Delta_{\max})$.

\paragraph{Step 1}
For notational convenience, we define 
\begin{align*}
    \rad_{m,a}^F=\beta\sum_{i=1}^{|\calS_m|}|\lambda_{m,i}^{(a)}|\cdot\frac{\|a\|_2}{\sqrt{\unbiasSize_m(a_{m,i})}}
\end{align*}
to be the total confidence radius of action $a$ coming from the definition of $\LCB_{m,2}(a)$ and $\UCB_m(a)$. 
Via a standard analysis of arm elimination, 
we show that that if an action $a$ is not eliminated at the end of epoch $m$, we have
\begin{align*}
    \Delta_a \leq 4\max_{a\in\calA_m}\rad_{m,a}^F \leq \frac{4\sqrt{3n}\beta}{\min_{a_m\in\calS_m}\sqrt{\unbiasSize_m(a_m)}},
\end{align*}
where the second inequality uses Cauchy-Schwarz inequality and the properties of volumetric spanners, specifically that $\|\lambda_{m}^{(a)}\|_2\leq 1$ and $|\calS_m|=3n$. To provide a lower bound on $c_m(a')$ for any $a'\in\calS_m$, note that we pick each action $a'\in \calS_m$ $2^m$ times in a round-robin manner, and thus
\begin{align*}
    c_m(a') \geq 2^m - \frac{D}{|\calS_m|}-1 = 2^m - \frac{D}{3n}-1.
\end{align*}
Rearranging the terms, we then obtain
\begin{align}\label{eqn:epoch_bound_1}
    2^m\Delta_a \leq \frac{48n\beta^2}{\Delta_a} + \frac{D\Delta_a}{3n} + \Delta_a.
\end{align}
Taking summation over all $a\in\calS_m$ and $m$, and noticing that the total number of epochs is bounded by $M=\lceil\log_2(|\calT_B|/3n)\rceil$, we arrive at the following $\order(R_1+D\Delta_{\max}\log(T/n))$ regret guarantee:
\begin{align}
&\sum_{m=1}^{M}\sum_{a\in\calS_m}2^m\Delta_a \nonumber\\
    &\leq \sum_{m=1}^{M}\sum_{a\in\calS_m,\Delta_a>0}2\cdot\left(\frac{48n\beta^2}{\Delta_a}+\frac{D\Delta_a}{3n}+\Delta_a\right) \nonumber\\
    &\leq \sum_{m=1}^{M}\sum_{a\in\calS_m,\Delta_a>0}\order\left(\frac{n\log (KT)}{\Delta_a}\right) + \order\left(D\Delta_{\max}\log(T/n)\right),\nonumber \\
    &\leq \order\left(\frac{n^2\log(T/n)\log (KT)}{\Delta_{\min}}\right) + \order\left(D\Delta_{\max}\log(T/n)\right),\nonumber
\end{align}
where the first inequality is because $a\in\calS_m$ is not eliminated in epoch $m-1$ and the last inequality is by lower bounding $\Delta_a$ by $\Delta_{\min}$.

To obtain the other instance-independent regret bound $\order(R_2+D\Delta_{\max}\log(T/n))$, we bound the regret differently by considering $\Delta_a\geq \beta\sqrt{n/2^m}$ and $\Delta_a\leq \beta\sqrt{n/2^m}$ separately:
\begin{align}
    &\sum_{m=1}^{M}\sum_{a\in\calS_m}2^m\Delta_a \nonumber \\
    &\leq \sum_{m=1}^{M}\sum_{a\in\calS_m,\Delta_a\geq\beta\sqrt{n/2^m}}\left(\frac{512n\beta^2}{\Delta_a}+\frac{2D\Delta_a}{3n}+2\Delta_a\right) \nonumber\\
    &\qquad +\sum_{m=1}^{M}\sum_{a\in\calS_m,\Delta_a\leq\beta\sqrt{n/2^m}}\left(2^m\Delta_a\right) \nonumber \\
    &\leq \order(n\sqrt{|\calT_B|\log(KT)} + D\Delta_{\max}\log(T/n))\nonumber.
\end{align}

\paragraph{Step 2}
To obtain the other regret bound $\order(\min\{R_1,R_2\}+(nd^\star+DB)\log(T/n)+D\Delta_{\max})$ with a different delay overhead, we similarly define
\begin{align*}
    \rad_{m,a}^{N} &= \beta\sum_{i=1}^{|\calS_m|}|\lambda_{m,i}^{(a)}|\cdot \frac{\|a\|_2}{\sqrt{2^m}}
\end{align*}
as the total confidence radius of action $a$ coming from the definition of $\LCB_{m,1}(a)$. 
Further let $\wh{\mu}_m(a) = \frac{1}{2^m}\left(\sum_{\tau\in \obs_m(a)\cup\unobs_m(a)}u_{\tau}\right)$ be the empirical mean of action $a$'s loss within epoch $m$ (which is generally not available to the algorithm due to delay). According to the construction of $\wh{\mu}_m^{+}(a)$ and $\wh{\mu}_m^{-}(a)$, we know that for all $a\in\calS_m$,
\begin{align*}
    \wh{\mu}_m^{+}(a)\leq \wh{\mu}_m(a) + \frac{|\unobs_m(a)|}{2^m},~~\wh{\mu}_m^{-}(a)\geq \wh{\mu}_m(a) - \frac{|\unobs_m(a)|}{2^m}.
\end{align*}
Then, for any action $a\in\calA_m$ that is not eliminated at the end of epoch $m$, using the fact that $a=\sum_{i=1}^{|\calS_m|}\lambda_{m,i}^{(a)}a_{m,i}$, we obtain with high probability:
\begin{align}
    \mu_a &\leq \sum_{i=1}^{|\calS_m|}\lambda_{m,i}^{(a)}\cdot \hat{\mu}_{m}(a_{m,i}) + \rad_{m,a}^{N} \nonumber\\
    &\leq \LCB_{m,1}(a) + \rad_{m,a}^{N}+\sum_{i=1}^{|\calS_m|}|\lambda_{m,i}^{(a)}|\cdot \frac{|\unobs_m(a_{m,i})|}{2^m} \nonumber\\
    &\leq \LCB_{m,1}(a) +\rad_{m,a}^{N} \nonumber\\
    &\qquad + \sum_{i=1}^{|\calS_m|}|\lambda_{m,i}^{(a)}|\cdot\left(\frac{2D\mu_{a_{m,i}}}{2^m|\calS_m|}+\frac{16\log KT +2}{2^m}\right) \\
    &\leq B +\rad_{m,a}^{N} \nonumber\\
    &\qquad + \sum_{i=1}^{|\calS_m|}|\lambda_{m,i}^{(a)}|\cdot\left(\frac{2D\mu_{a_{m,i}}}{2^m|\calS_m|}+\frac{16\log KT +2}{2^m}\right),\label{eqn:small-loss}
\end{align}
where the first inequality is by standard Azuma-Hoeffding's inequality, the third inequality is by Lemma C.2 of \citet{schlisselberg2024delay} (included as \pref{lem:high-prob-event} in the appendix for completeness), and the last inequality is because $a$ is not eliminated at the end of epoch $m$.

\setcounter{AlgoLine}{0}
\begin{algorithm*}[htbp]
\caption{Reduction from Contextual Linear Bandits to Non-Contextual Linear Bandits~\citep{hanna2023contexts}}\label{alg:reduction}
Input: confidence level $\delta$, an instance $\Alg_{\nctx}$ of \pref{alg:lossLBmis} with $\beta=\sqrt{2\log(KT^3)}$. 

Let $\Theta'$ be a $\frac{1}{T}$-cover of $\Theta$ with size $\order(T^n)$.

\For{$m=1,2,\dots$}{
    \nl Construct action set $\calX_{m}=\{\gup{m}(\theta)~\vert~\theta\in \Theta'\}$ where   $\gup{m}(\theta)=\frac{1}{2^{m-1}}\sum_{\tau=1}^{2^{m-1}}\argmin_{a\in \calA_\tau}\inner{a,\theta}$.
    

    \nl Initiate $\Alg_{\nctx}$ with action set $\calX_m$ and misspecification level $\epsilon_m=\min\{1,2\sqrt{\log(T|\Theta'|/\delta)/2^m}\}$. \label{line:misspecific_level}
    
    \nl \For{$t=2^{m-1}+1,\dots,2^m$}{
        \nl $\Alg_{\nctx}$ outputs action $\gup{m}(\theta_t)$.

        \nl Observe $\calA_t$ and select $a_t=\argmin_{a\in \calA_t}\inner{a,\theta_t}$.

        \nl Observe the loss $u_\tau$ for all $\tau$ such that $\tau+d_{\tau}\in (t-1,t]$ and send them to $\Alg_{\nctx}$.
    }
    
}
\end{algorithm*}

Now consider two cases. When $B\geq \frac{\mu_a}{2}$, we know that $\Delta_a\leq \mu_a - \mu^\star\leq 2B$. Using the previous \pref{eqn:epoch_bound_1}, we know that
\begin{align}\label{eqn:small-loss-1}
    2^m\Delta_a\leq \order\left(\frac{n\beta^2}{\Delta_a}+\frac{DB}{n}\right).
\end{align}
Otherwise, when $B < \frac{\mu_a}{2}$, with some manipulation on \pref{eqn:small-loss}, we show that
\begin{align}\label{eqn:small-loss-2}
    2^m\Delta_a\leq \order\left(\frac{n\beta^2}{\Delta_a}+\frac{\sum_{i=1}^{|\calS_m|}D\mu_{a_{m,i}}}{n}\right).
\end{align}
Combining \pref{eqn:small-loss-1} and \pref{eqn:small-loss-2}, we then obtain that within epoch $m$, the regret is bounded by
\begin{align}\label{eqn:small-loss-3}
\order\left(\sum_{a\in\calS_m}\frac{n\beta^2}{\Delta_a}+DB+D\sum_{i=1}^{|\calS_{m-1}|}\mu_{a_{m-1,i}}\right),
\end{align} 
since all active actions in epoch $m$ are not eliminated in epoch $m-1$.
The first term $\sum_{a\in\calS_m}\frac{n\beta^2}{\Delta_a}$ in \pref{eqn:small-loss-3} eventually leads to the $\min\{R_1,R_2\}$ term in the claimed regret bound, by the exact same reasoning as in \textbf{Step 1}.
The second term explains the final $DB\log(T/n)$ term in the regret bound (recall that number of epoch is of order $\order(\log(T/n))$).
Finally, the last term in \pref{eqn:small-loss-3} can be written as
$D\sum_{i=1}^{|\calS_{m-1}|} \Delta_{a_{m-1,i}} + 3n\cdot d^\star$,
and the term $D\sum_{i=1}^{|\calS_{m-1}|} \Delta_{a_{m-1,i}}$ is one half of the regret incurred in epoch $m-1$ as long as $2^{m-1}>2D$ (otherwise, the epoch length is smaller than $D$, and we bound the regret trivially by $D\Delta_{\max}$).
Summing over all epochs and rearranging the terms thus leads to the a term $nd^\star\log(T/n)$ in the regret.
This proves the goal of the second step.

\paragraph{Combining everything} 
Finally, note that the number of different values of $B$ \pref{alg:lossLB} uses is upper bounded by $\lceil\log_2(d^\star)\rceil=\lceil\log_2(D\mu^\star)\rceil$ since the optimal action $a^\star$ will never be eliminated when $B\geq \mu^\star$. Summing up the regret over these different values of $B$ arrives at the the final bound $\order(\min\{V_1,V_2\},\log(d^\star)\min\{W_1,W_2\})$.