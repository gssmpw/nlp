\newpage
\section{Omitted Details in \pref{sec: experiment}}\label{app: experiment}
For completeness, we include the pseudo code for the benchmark used in our experiment, that is, \texttt{LinUCB} using only the observed feedback;
see \pref{alg:linUCB}.

\begin{algorithm}[h]
\caption{LinUCB with Delayed Feedback}\label{alg:linUCB}
Input: action set $\calA$, a parameter $\lambda>0$.

Initialize: $\wh{\theta}_1$ arbitrarily, $\beta_t = \sqrt{\lambda} + \sqrt{2\log T+n\log(1+\frac{t}{n\lambda})}$ for all $t\in[T]$, $H_1 = \lambda I$.

\For{$t=1,2,\dots,T$}{
     Pick 
     \begin{align*}
         a_t=
         \begin{cases}
            \argmin_{a\in\calA} \inner{a,\wh{\theta}_t} - \beta \|a\|_{H_t}^{-1}, &\mbox{in the loss case,} \\
            \argmax_{a\in\calA} \inner{a,\wh{\theta}_t} + \beta \|a\|_{H_t}^{-1}, &\mbox{in the reward case.}
        \end{cases}
     \end{align*}
     
     Observe the payoff $u_\tau$ for all $\tau$ such that $\tau+d_{\tau}\in (t-1,t]$.

     Update $H_{t+1} = H_t + \sum_{\tau:\tau+d_{\tau}\in(t-1,t]}a_{\tau}a_{\tau}^\top$ and $\wh{\theta}_{t+1}=H_{t+1}^{-1}\sum_{\tau:\tau+d_{\tau}\leq t}a_{\tau}u_{\tau}$.
     
}
\end{algorithm}
