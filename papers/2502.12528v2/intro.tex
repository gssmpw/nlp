\section{Introduction}\label{sec: intro}
Stochastic multi-armed bandit (MAB)  is a well-studied theoretical framework for sequential decision making. In recent years, considerable investigation has been given to the realistic situations where the agent observes the payoff (either reward or loss) of an arm
only after a certain delayed period of time. However, most work assumes that the delays are {\it payoff-independent}. Namely, while the delay may depend on the chosen arm, it is sampled independently from the stochastic payoff of the chosen arm.

\citet{lancewicki2021stochastic} address this limitation by studying a setting where the delay and the reward are drawn together from a joint distribution.  
Later, \citet{tang2024stochastic} consider a special case where the delay is exactly the reward. Their motivation stems from response-adaptive clinical trials that aim at maximizing survival outcomes. For example, progression-free survival (PFS)—defined as the number of days after treatment until disease progression or death—is widely used to evaluate the effectiveness of a treatment. Notably, in this context, the ``delays'' in observing the PFS are the PFS itself. \citet{schlisselberg2024delay}  builds on and refines this investigation,  extending the study to the case where delay is the loss itself. Taken together, this delay-as-payoff framework effectively captures many real-world scenarios involving time-to-event data across many domains. For example, postoperative length of stay (PLOS) is one example of time-to-event data that  specifies the length of stay after surgery. Potential surgical procedures and postoperative care can be modeled as arms.  The delay—defined as the time until the patient is discharged—can be interpreted as the loss that we aim to minimize. 
As another example, in advertising, common metrics, including Average Time on Page (ATP) and Time to Re-engagement (that
tracks the time elapsed between a user’s initial interaction with an ad and subsequent engagement such as returning to the website), can be modeled as reward or loss inherently delayed by the same duration.

Despite such recent progress, the current consideration of {\it payoff-dependent} delay remains limited to the simple multi-armed bandit (MAB) setting. While MAB frameworks are foundational in decision-making problems, they have notable practical limitations. Specifically, they fail to account for the influence of covariates that drive heterogeneous responses across different actions. This makes them less suitable for scenarios involving a large number of  (potentially dynamically changing) actions and/or situations where context is crucial in shaping outcomes.


\paragraph{Contributions.} 
Motivated by this limitation, in this work, we extend the delay-as-payoff model from MAB to contextual linear bandits, a practical framework that is widely used in real-world applications.  Specifically, our contributions are as follows.
\begin{itemize}[leftmargin=*]
    \item As a first step, in \pref{sec: linear}, we study stochastic linear bandits with a fixed action set (known as the non-contextual setting).
    We point out the difficulty of directly combining the standard \texttt{LinUCB} algorithm with the idea of~\citet{schlisselberg2024delay},
    and propose a novel phased arm elimination algorithm that  only selects actions from a \emph{volumetric spanner} of the action set.
    In the delay-as-loss case, we prove that, compared to the standard regret in the delay-free setting, the overhead caused by the payoff-dependent delay for our algorithm is 
    $\order(\min\{nd^\star
    \log(T/n)+D\Delta_{\max}, D\Delta_{\max}\log(T/n)\})$, where $n$ is the dimension of the action set, $T$ is the total horizon, $\Delta_{\max}$ is the maximum suboptimality gap,
    $d^\star$ is the expected delay of the optimal action,
    and $D$ is the maximum possible delay
    (formal definitions are deferred to \pref{sec: pre}). 
    This instance-dependent bound is in the same spirit as the one of~\citet{schlisselberg2024delay} and is small whenever the optimal action has a small loss.
    In the delay-as-reward case, a slightly worse bound is provided in \pref{app: reward}; such a separation between loss and reward is similar to the results of~\citet{schlisselberg2024delay}.
    
    \item Next, in \pref{sec: contextual}, we extend our results to the contextual case where the action set is varying and drawn i.i.d. from an unknown distribution. 
    Using a variant of our non-contextual algorithm (that can handle misspecification) as a subroutine,    
    we apply the contextual to non-contextual reduction recently proposed by \citep{hanna2023contexts} and show that the resulting algorithm enjoys a similar regret guarantee despite having varying action sets, establishing the first regret guarantee for contextual linear bandits with delay-as-payoff.

    
    \item In \pref{sec: experiment}, we implement our algorithm and test it on synthetic linear bandits instances, demonstrating its superior performance against a baseline that runs \texttt{LinUCB} with only the currently available feedback.
\end{itemize}

\paragraph{Related works.} 
Recent research has investigated different problems of learning under bandit feedback with delayed payoff, addressing various new challenges caused by the combination of delay and bandit feedback. As mentioned, most studies assume {\it payoff-independent} delays. Among this line of research, \citet{Dudk2011EfficientOL} is among the first to consider delays in stochastic MAB with a constant delay. \citet{mandel2015queue} and \citet{joulani2013online} extend the consideration to stochastic delay, with the assumption that the delay is bounded.

Subsequent studies on  i.i.d. stochastic delays differentiate between {\it arm-independent} and {\it arm-dependent} delays. For {\it arm-independent} delays, \citet{zhou2019learning,vernade2020linear,blanchet2024delay-contextual-linear} shows regret characterizations for (generalized) linear stochastic contextual bandits.
\citet{pike2018bandits} considers  aggregated anonymous feedback, under the assumption that the expected delay is bounded and known to the learner. {\it Arm-dependent} stochastic delays have been explored in various settings, including \citet{gael2020stochastic,arya2020randomized,lancewicki2021stochastic}.


Far less attention has been given to {\it payoff-dependent} stochastic delays. The setting in
\citet{vernade:hal-01545667} implies a dependency between the reward and the delay, as a current non-conversion could be the result of a delayed reward of 1.  \citet{lancewicki2021stochastic} considers the case where the stochastic delay in each round and the reward are drawn from a general joint distribution. 
\citet{tang2024stochastic} investigates strongly reward-dependent delays, specifically motivated by medical settings where the delay is equal to the reward. \citet{ schlisselberg2024delay} follows this investigation and extends the discussion to  delay as loss, and provides a tighter regret bound. Although with a slightly different focus, \citet{thune2019nonstochastic, zimmert2020optimal,gyorgy2021adapting, van2022nonstochastic,van2023unified} and several other works study non-stochastic bandits, where  both the delay and rewards are adversarial. 

Nevertheless, the {\it payoff-dependent} (either loss or reward) delays are only studied under stochastic multi-armed bandits (MAB).  In this work, we extend the study to contextual linear bandits, significantly broadening its practicality.

