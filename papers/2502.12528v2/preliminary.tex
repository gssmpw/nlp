\section{Preliminary}\label{sec: pre}
Throughout this paper, we use $[N]$ to denote $\{1,2,\dots,N\}$ for some positive integer $N$. Let $\R^n_+$ be the $n$-dimensional Euclidean space in the positive orthant and $\mathbb{B}_2^n(1)=\{v \in \R^n:~\|v\|_2\leq 1\}$ be the $n$-dimensional unit ball with respect to $\ell_2$ norm. Define $\calU[a,b]$ to be the uniform distribution over $[a,b]$. For a real number $a$, define $\sgn(a)$ as the sign of $a$. For two real numbers $a$ and $b$, define $a\vee b\triangleq \max\{a,b\}$. For a finite set $\calS$, denote $|\calS|$ as the cardinality of $\calS$. The notation $\otil(\cdot)$ hides all logarithmic dependencies.

In this paper, we consider the delay-as-payoff model proposed by \citet{schlisselberg2024delay}, in which the delay of the payoff is proportional to the payoff itself. Specifically, we study stochastic linear bandits in this model, and we start with a fixed action set as the first step (referred to as the non-contextual case) and then move on to the case with a time-varying action set (referred to as the contextual case). For conciseness, we mainly discuss the payoff-as-loss case, but our algorithm and results can be directly extended to the payoff-as-reward case (see \pref{app: reward}).

\textbf{Non-contextual stochastic linear bandits.} In this problem, the learner is first given a \emph{fixed} finite set of actions $\calA\subset \R_+^n\cap\mathbb{B}_2^n(1)$ with cardinality $|\calA|=K$. Let $D>0$ be the maximum possible delay. At each round $t\in [T]$, the learner selects an action $a_t\in \calA$ and incurs a loss $u_t=\mu_{a_t}+\eta_t\in [0,1]$ where $\eta_t$ is zero-mean random noise, $\mu_{a}=\inner{a,\theta}$ is the expected payoff of action $a$, and $\theta\in \R_+^n\cap\mathbb{B}_2^n(1)$ is the model parameter that is unknown to the learner.\footnote{We enforce both $\calA\subset \R_+^n\cap\mathbb{B}_2^n(1)$ and $\theta\in \R_+^n\cap\mathbb{B}_2^n(1)$ to make sure that the payoff (and hence the delay) is non-negative.}  Then, the loss is received by the learner at the end of round $\lceil t+d_t \rceil$ where the delay $d_t=D\cdot u_t$ (that is, proportional to the loss).  
The goal of the learner is to minimize the (expected) pseudo regret defined as follows:
\begin{align}\label{eqn:reg_linear_reward}
    \Reg \triangleq \E\left[\sum_{t=1}^T\inner{a_t,\theta}\right] - T\cdot \min_{a\in \calA}\inner{a,\theta}.
\end{align}
Let $a^\star \in \argmin_{a\in \calA}\inner{a,\theta}$ be an optimal action, $\mu^\star = \mu_{a^\star}$ be its expected loss,
and $d^\star = D\mu^\star$ be its expected delay.
For an action $a$, define $\Delta_a = \inner{a-a^\star,\theta}$ as its sub-optimality gap. Further define $\Delta_{\min} = \min_{a\in \calA, \Delta_a>0}\Delta_a$ and $\Delta_{\max} = \max_{a\in \calA}\Delta_a$ to be the minimum and maximum non-zero sub-optimality gap respectively. 

We point out that the standard multi-armed bandit (MAB) setting considered in~\citet{schlisselberg2024delay} is a special case of our setting with $\calA$ being the set of all standard basis vectors in $\R^n$.


\textbf{Contextual stochastic linear bandits.} In the contextual case, the main difference is that the action set is not fixed but \emph{changing over rounds}. Specifically, at each round $t$, the learner first receives an action set $\calA_t\subset \R_+^n\cap \mathbb{B}_2^n(1)$ (which can be seen as a context), where we assume that $\calA_t$ is drawn i.i.d. from an unknown distribution $\dist$. The rest of the protocol remains the same, and the goal of the learner is still to minimize the (expected) pseudo regret, defined as:
\begin{align*}
    \Reg\triangleq \E
    \left[\sum_{t=1}^T\inner{a_t,\theta} - \sum_{t=1}^T\min_{a_t^\star\in\calA_t}\inner{a_t^\star,\theta}\right],
\end{align*}
where the expectation is taken over both the internal randomness of the algorithm and the external randomness in the action sets and loss noises.
