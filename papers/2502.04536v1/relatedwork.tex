\section{Related work}
\subsection{Neural Decompilation}

The early history of neural decompilation is a story of evolving architectures.
Katz et al.~\cite{katz2018} use a recurrent neural network.
Coda~\cite{coda} uses a multi-stage model with tree-based encoder and decoder to generate an approximate solution followed by an ensemble of other models to edit the first stage's output.
Cao et al.~\cite{cao2022boosting} use a graph neural network.
BtC~\cite{hosseinibeyond} uses the powerful transformer architecture~\cite{transformer} in its encoder-decoder (sequence to sequence) configuration.
As the immense power of the transformer architecture across a wide variety of tasks and domains has become clear, subsequent work has focused on leveraging it for neural decompilation and scaling it in size.
SLaDe~\cite{slade} is 50\% larger than BtC~\cite{hosseinibeyond} and uses numerous small adjustments.
LLM4Decompile~\cite{llm4decompile} introduce a family of large causal transformer models with sizes in the billions of parameters.
LLM4Decompile models come in two flavors: one that takes assembly as input and one that takes deterministic decompilation from Ghidra as input.
Nova+~\cite{nova} is designed to handle assembly only.
It introduces a hierarchical attention mechanism that helps adapt attention to handling long input sequences of assembly instructions.

In contrast, in our work we recast the task of neural decompilation as necessarily one of joint code and type recovery if neural decompilation is to be applicable to real code.
We are also the first to use inter-procedural context for neural decompilation.

\subsection{Type Recovery}

Type recovery work spans a range of different approaches, from heuristics to constraint-based to probabilistic methods, including machine learning.
There is sufficient information available at the executable level to deterministically eliminate some types, but not enough to determine the full types, or the names of any UDTs or their fields.

TIE~\cite{tie} is a non-probabilistic technique that uses static analysis and constraint solving to predict types.
REWARD~\cite{reward} and Howard~\cite{howard} uses dynamic analysis to collect information about variables' types.
Dynamic approaches can be powerful, but quality inputs are difficult to obtain in reverse engineering scenarios.
In addition, all non-probabilistic techniques are fundamentally limited by the information present in the binary, which is insufficient to recover types in general and are necessarily unable to predict type or field names.

Some probabilistic techniques focus on predicting only the composition of variable types, not the names of those types or their fields.
Osprey~\cite{osprey} combines constraint solving and probabilistic random variables to predict the composition of types.
TyGr~\cite{tygr} does the same but uses a graph neural network.

DIRTY~\cite{dirty} is a joint name and type recovery model based on the transformer architecture.
DIRTY encodes as context not only the decompiled code of the target function but also an encoding of the memory layout of each variable.
(We find that the memory layout encoding is redundant because it is communicated via the type names and decompiler-provided comments indicating the source of each variable, such as the \cinline{// rax} comment on line 2 of Figure~\ref{fig:intro_decompiled}. In early experiments, it offered no benefit while being more expensive to run.)
A key limitation of DIRTY is it predicts structures out of a fixed type library, leaving it unable to generalize outside this set; further, some types in this library are pointers to UDTs without definitions.
Recently, Xie et al.~\cite{resym} introduced ReSym, which performs the same joint code-type prediction task but with an improved approach using larger language models and providing the ability to generate arbitrary field names and types for UDTs without being restricted to a fixed library.

Many of these tools struggle with nested structures (when the field of a structure is another, perhaps different, structure, or a pointer to it) or are fundamentally incapable of predicting these types.
This represents the most challenging type recovery task.
To be counted as correct in our experiments, type definitions for all nested structures must have equivalent composition; nested structures are extremely common in our real-world data.