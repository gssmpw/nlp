\section{Implementation and Evaluation}
\label{sec:evaluation}

We implement \tool with about 3,900 lines of Python code.
\tool currently supports two blockchains, i.e., Ethereum~\cite{ethereum} and BSC~\cite{bsc}, which account for over 60\% of total value locked (TVL) among all the blockchains~\cite{defillama_chains}.
To obtain raw data from specific transactions, we utilize blockchain node APIs facilitated by QuickNode~\cite{quicknode}, an external RPC service provider.

Our evaluation aims to answer three research questions (RQs):
% Save space, changed to [leftmargin=*,noitemsep,topsep=0pt]
\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{RQ1}: How effectively does \tool detect price manipulation attacks compared with the existing state-of-the-arts?
    \item \textbf{RQ2}: How significantly does the fine-tuning technique promote the accuracy of \tool?
    \item \textbf{RQ3}: How practically and efficiently does \tool detect price manipulation attacks in a real-world setting? %during real-world traffic monitoring?
    % \item \textbf{RQ3}. How many false positives does \tool yield in DeFi price manipulation attack detection?
    % \item \textbf{RQ4}. Can \tool identify new real-world price manipulation attacks?
\end{itemize}


% \begin{figure*}[t]
% \centering
%     \begin{subfigure}[t]{0.15\textwidth}
%         \includegraphics[width=\textwidth]{Figure/ds_result_v3.pdf}
%         \caption{\tool}
%     \end{subfigure}
%     \hspace{0.1in}
%     \begin{subfigure}[t]{0.15\textwidth}
%         \includegraphics[width=\textwidth]{Figure/baseline_result_v3.pdf}
%         \caption{Baseline}
%     \end{subfigure}
%     \hspace{0.1in}
%     \begin{subfigure}[t]{0.15\textwidth}
%         \includegraphics[width=\textwidth]{Figure/dt_result_v2.pdf}
%         \caption{DeFiTainter}
%     \end{subfigure}
%     \hspace{0.1in}
%     \begin{subfigure}[t]{0.15\textwidth}
%         \includegraphics[width=\textwidth]{Figure/dr_result_v2.pdf}
%         \caption{DeFiRanger}
%     \end{subfigure}
%     \hspace{0.1in}
%     \begin{subfigure}[t]{0.15\textwidth}
%         \includegraphics[width=\textwidth]{Figure/df_result_v2.pdf}
%         \caption{DeFort}
%     \end{subfigure}
%     \caption{Effectiveness of tools on various price models.}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=1\linewidth]{Figure/cdf_graph(multithread).pdf}
%     \caption{CDF graph of time consumption. The average time cost is 165s per transaction.}
%     \label{CDF_graph}
% \end{figure}
% \ye{@juantao: brief dicusss our implementation of \tool \juantao{Done.}}

\noindent
\textbf{Datasets.}
To address these three RQs, we collect three datasets from real DeFi transactions on Ethereum and BSC to evaluate \name, as shown in~\Cref{tab:benchmark}.
Specifically, we use the first dataset, \textit{D1}, which comprises 95 transactions of real-world price manipulation attacks from 90 DeFi applications, to evaluate both RQ1 and RQ2.
To demonstrate \name's practicality for RQ3, we use the second dataset, \textit{D2}, consisting of 968 suspicious transactions collected by our industry partner.
Furthermore, to measure \name's time overhead when deployed in a realistic setting with both suspicious and benign transactions, we mix the 968 suspicious transactions from \textit{D2} with 96,800 benign transactions in the third dataset, \textit{D3}.

For \textit{D1}, we scraped data from multiple sources.
Initially, we included all 54 price manipulation transactions identified by DeFort~\cite{DeFort24}.
We also collected all 55 price manipulation attacks documented by the renowned DeFi security GitHub repository, DeFiHackLabs~\cite{DeFiHackLabs}, from October 26, 2020, to October 11, 2023.
% As a renowned DeFi security GitHub repository, DeFiHackLabs has collected, tagged and organized a substantial number of attack incidents along with their POCs, garnering 5.1k stars. From this source, we identifies 55 incidents. 
To further expand our dataset, we acquired 31 transactions confirmed as price manipulation attacks from our industry partner.
%{Note that the attack incidents from the aforementioned data sources could repeat.}
% After filtering all the duplicates, we further selected transactions on either Ethereum or BSC, followed by manually analyzing each attack to remove erroneously labeled incident.
After removing duplicates among the three sources of data, we ultimately obtained 95 price manipulation attacks for \textit{D1}, which collectively caused \$381.16M in losses on Ethereum and BSC.

We also measure the distribution and monetary losses of the 95 attacks in \textit{D1} across our eight attack patterns described in \mysec\ref{sec:finalDetect}.
We observe that Pattern I has the highest number of cases, while Pattern III results in the greatest losses.
This is largely due to one of the most notorious incidents, CreamFinance~\cite{CreamFinance}, which falls under this pattern and led to a loss of \$130M.
For Patterns V and VIII, there is only one case each: ATK~\cite{ATK} and LUSD~\cite{LUSD}, respectively.

\begin{table}[t!]
    \centering
    \small
    \caption{The benchmark datasets used for evaluation.}
    \vspace{-2ex}
    \begin{tabular}{ll}
    \toprule
        Dataset & RQs \\
        \midrule
        D1: 95 historical real-world attacks & RQ1, RQ2\\
        \midrule
        D2: 968 suspicious transactions & RQ3\\
        \midrule
        D3: 96,800 benign transactions & RQ3\\
    \bottomrule
    \end{tabular}
    \label{tab:benchmark}
\end{table}

\begin{table}[!t]
    \centering
    \small
    \vspace{2ex}
    \caption{Summary of different attack patterns in \textit{D1}.}
    \vspace{-2ex}
    \begin{tabular}{lcccccccc}
    \toprule
        Pattern & I & II & III & IV & V & VI & VII & VIII
        \\
        \midrule
        \#Case & 49 & 20 & 5 & 6 & 1 & 2 & 11 & 1\\
        Loss(\$) & 55.3M & 70M & 141.2M & 43M & 61K & 83K & 71.4M & 9K\\
        \bottomrule
    \end{tabular}
    \label{tab:distr_and_loss}
    \vspace{2ex}
\end{table}


%\textit{D2} comprises 968 suspicious transactions identified by our industry partner, a web3 security company, which seeks our help to confirm how many of these transactions belong to price manipulations.
%, which performs monitoring of on-chain transactions. 
%MetaTrust Labs~\cite{Metatrust}
\textit{D2} comprises 968 suspicious transactions provided by our industry partner, a Web3 security company, which monitors blockchain transactions in real-time and automatically flags transactions that yield significant profits for initiators.
All these transactions are sufficiently complex and potentially involve price manipulation.
While manually confirming these transactions is labor-intensive and prone to errors, 155 of them have been confirmed to belong to other vulnerabilities, such as Reentrancy (OpenLeverage~\cite{repo_openLeverage}), Unverified User Input (YIELD~\cite{repo_yield}), and Access Control Bugs (SafeMoon~\cite{repo_safeMoon}), by developers and industry partners.
In this way, we can stress-test \name's detection capabilities when mixed with other types of attack transactions. 
%\juantao{emphasize the complexity of D2}
% As of June 20, 2024, they have flagged a total of 1,427 suspicious transactions that could date back to 2017. We filtered all transactions containing a transaction hash from the database on either Ethereum or BSC. This effort resulted in a collection of 968 distinct transactions.

Furthermore, to assess \name's false alarms on benign transactions and to measure its time overhead in a realistic setting, we need to collect a large number of benign transactions, as the most majority of real-world transactions are still benign.
In the absence of empirical data on the ratio of suspicious to benign transactions, we adopt a conservative yet reasonable ratio of 1:100, assuming one suspicious transaction for every 100 benign transactions.
Thus, with 968 suspicious transactions in \textit{D2}, we require 96,800 benign transactions for \textit{D3}.
To this end, we randomly sampled these 96,800 transactions from DeFort~\cite{DeFort24}’s dataset of 428,523 benign transactions, which includes 384,143 benign transactions on Ethereum and BSC.
At this sample size, we achieve a 99.999\% confidence level with a margin of error of 0.625\%.
% \juantao{New, benign dataset} \fixme{The third dataset, denoted as \textit{D3}, contains 96,800 real-world transactions sampled from the dataset studied in DeFort~\cite{DeFort24} including 428,523 \emph{likely} benign transactions, reaching a 99.999\% confidence level with a margin of error of 0.625\%.}

% \juantao{Need to revise the purpose} \fixme{We evaluate the practicality of \tool in on-chain security monitoring using \textit{D2} to assess the false positive rate towards suspicious transactions.}
% % of \tool's detection.

% \juantao{New, benign dataset} \fixme{For \textit{D3}, we constructed it through random sampling from the dataset used in DeFort~\cite{DeFort24}, which contains 428,523 benign transactions, at a 99.9\% confidence level with a margin of error of 5\%. \textit{D3} is primary employed to assess the false positive rate towards normal transactions, as well as the overhead of daily on-chain monitoring.}


\noindent
\textbf{Experimental Setup.}
All experiments were conducted on a desktop computer running Ubuntu 20.04, powered by an Intel® Xeon® W-2235 CPU (3.80 GHz, 6 cores, and 12 threads) and equipped with 16 GB of memory.
For LLMs, we use OpenAI's GPT family models due to limited data required for fine-tuning, as explained in \mysec\ref{sec:finetune}.
\name by default uses GPT-3.5-Turbo (\texttt{GPT-3.5-turbo-1106}) for its well-recognized cost-performance balance, but we also conduct an ablation study in \mysec\ref{sec:RQ2} to test the more advanced GPT-4o (\texttt{GPT-4o-2024-08-06}) for fine-tuning.
For the LLM configuration, \name employs nucleus sampling~\cite{holtzman2019curious} with a \texttt{top-p} value of 1 and sets the \texttt{temperature} to 0, ensuring highly deterministic responses for each prompt, although each was run only once.



% The big table
\input{Table/RQ1-result}

\subsection{RQ1: Detection Effectiveness}
\label{sec:RQ1}

To answer RQ1, we evaluate \tool and compare it with three SOTA tools—DeFiTainter~\cite{DeFiTainter23}, DeFort~\cite{DeFort24}, and DeFiRanger~\cite{DeFiRanger23}, using the dataset \textit{D1}.
Because DeFiRanger is not open-source, we use the results reported in their paper for the attacks they evaluated; for other attacks, we re-implemented their approach based on the description~\cite{DeFiRanger23} and conducted our evaluation.
Although DeFort is also not open-source, we obtained a copy of its source code from the authors to conduct our experiment.

\mytab\ref{tab:RQ1_res} presents the detection results. The first four columns list the name of the victim protocol, the destination chain, the hack date, and the resulting loss, respectively. Note that one protocol may face multiple attacks, so we add numerical suffixes to protocol names to differentiate them. We use \cmark to indicate an attack can be successfully detected by a tool, and \xmark to indicate a detection failure.

\tool can detect most of the price manipulation attacks.
It achieves a recall rate of 80\%, outperforming all other tools. 
% 80% = 76 / 95
Overall, \tool detected 76 attacks, followed by DeFort with 50 and DeFiRanger with 49 attacks, respectively, while DeFiTainter detected only 34 attacks.
\myfig\ref{fig:price_model_proportion} details the performance of each tool on the evaluated DeFi protocols across four application categories using different price models.
% the number of each type of DeFi applications within \textit{D1} and the proportion of three price models in each type, where 88\% of tokens that were attacked utilize the CPMM, while only five of them were exploited through the vulnerabilities in their own custom price models; for the other three types of DeFi applications, customized price models account for approximately 70\%.
Compared to other tools, \tool performs the best in every application category.
Particularly, it achieves the highest recall, 90.7\%, in Token-related protocols. 
However, \tool yields a low recall rate in Lending-related protocols, though still higher than all existing tools. 
Through further analysis, we identifies that 3 out of 12 attacks (InverseFinance\_2, SanshuInu, and VesperFinance) are cross-transaction attacks that \tool is unable to detect.
Additionally, one attack (TIFIToken) involves exploiting a closed-source custom price model, and the InverseFinance\_1 attack, a false negative, will be detailed below.
% \ye{The last undetectable case, the attack on InverseFinance, was a false negative, and we will subsequently provide a detailed analysis of the reasons for this. @juantao: what the 'false positive' means?}\juantao{Typo.False negative}
% \ye{@juantao: could we give some reasons why \tool detects only 58.3\,\% in Lending-related protocols. we should focus on the analysis of \tool itself rather than why other tools failed.\juantao{Done.}}


% \fixme{After analyzing the proportion of each type of price price within every domain of DeFi applications, we can further interpret the experimental results. For the Token-related protocols, due to the predominant use of CPMM, and the training of this pricing model during the fine-tuning process, \tool can achieve an accuracy far superior to that of other tools. The low detection accuracy of DeFiTainter on this domain primarily stems from the nature of the attacks targeting these applications, which exploit the token \texttt{transfer} function. By continuously causing unexpected increases or decreases in the number of tokens within the liquidity pool, attackers achieve the objective of price manipulation. Since the transfer function in isolation is benign, DeFiTainter, which relies on function-based detection, cannot identify such attacks.}

% \fixme{For the other three categories of DeFi applications, since the custom price model constitutes the vast majority, the high accuracy of \tool in inferring the trend of token price change within the custom price model (details in \mysec\ref{sec:RQ2}) is key to achieving higher detection accuracy compared to other tools. Meanwhile, we conducted a more thorough analysis of the results from other tools. Since the majority of attacks on yield farming protocols conform to \textit{Pattern II}, and the detection rules defined by DeFiRanger do not cover this attack pattern, the detection success rate is relatively low; On the other hand, since the attacks on DEX are readily detectable by comparing the exchange rates of two tokens before and after a swap, and 3 out of 8 rules in DeFiRanger are designed to detect attacks on it, DeFiRanger achieve significantly higher accuracy than other two tools; For the lending applications, due to the prevalent use of custom models in such apps and the difficulty in detecting attacks on these protocols by calculating the exchange rate, the accuracy of the other three tools is similar and relatively lower.}
% \ye{@juantao: could we interpret more from these figure.\juantao{Done. Further interpretation mark as red}}

% a recall rate of 80\%, outperforming all leading approaches. 
Yet, \tool missed detecting 11 price manipulation attacks. After analyzing each attack, we discovered that 8 out of 11 are cross-transaction attacks~\cite{ATK,Inuko,ARK,Inverse,SanshuInu,StarWallets,SwapX,Vesperfinance}, where detection was unsuccessful because \tool is based on analyzing individual transactions. For Zoompro, detection failure occurred because the token involved in the transaction did not adhere to the ERC20 token standard, resulting in the transfer event not being identified. The remaining two attacks, i.e., ``IndexedFinance'' and ``InverseFinance\_1'' cases, were subject to in-depth analysis.
% IndexedFinanace, a decentralized protocol for passive portfolio management on Ethereum, providing trade service for Top 5 tokens of DeFi projects of Ethereum, presents a challenge. 
The detection incapability for IndexedFinance was due to its use of an extremely complicated pricing mechanism that involves exponential calculations in its price-related function \texttt{joinswapExternAmountIn}. In the case of InverseFinance, the attacker exploited the flawed price dependency when calculating the price of tokens deposited as collateral, where the collateral price is based on the balance of multiple tokens in the liquidity pool and off-chain price oracles. As both cases require precise quantitative calculations, \tool is limited by the current LLMs' constrained capacity for scientific computation. A potential solution could be to integrate with Program-Aided Language models (PAL)~\cite{gao2023pal}, guiding the LLM to generate scripts for necessary calculations and executing them to obtain the result.

% As detailed in \mytab\ref{tab:RQ1_res}, we benchmark \tool against three pertinent approaches, i.e., DeFiTainter, DeFort and DeFiRanger, using \textit{D1} dataset. 
% Since both DeFiRanger and DeFort are not open-sourced, in order to obtain the results from DeFiRanger, for transactions that overlap with its published experimental dataset, we directly cited the results; for other transactions, we reproduced the method described in the paper and evaluated using our version. 
% To obtain the results from DeFort, we invited the author of it to assist us in the evaluation. 
% These two tools detected 49 and 50 attacks respectively. On the other hand, DeFiTainter is open-sourced and identified 34 out of 95 attacks using the version we built following its guidance~\cite{DeFiTainter_github}. In comparison, within 95 transactions, \tool successfully identified 76 attacks, thus achieving a recall of 0.8, significantly higher than other state-of-the-art tools. It is important to note that, due to the inability to accurately identify user accounts in the transaction, four (AutoSharkFinance\_2, bZx, CreamFinance and GDS) out of the 76 successfully detected attacks could not be identified under fully automated conditions. These attacks were only successfully detected after manual adjustments on the user accounts were done.

% For the remaining \ye{19} cases that were undetectable, we conducted a comprehensive analysis on each. Two instances of false negatives are discussed above. 
% \ye{todo: check later}
Among the remaining 8 undetectable transactions, 
3 were not analyzed successfully due to the unavailability of the code of price calculation functions since our method relies on code-level analysis; 
% eight cases are cross-transaction attacks, where detection was unsuccessful as \tool is based on analyzing an individual transaction; 
five cases involved compilation errors during the extraction of price calculation functions using Slither~\cite{slither}, an off-the-shelf static analyzer for Solidity and Vyper, thereby prematurely terminating the detection process. 
% the last case was due to the token involved in the transaction not adhering to the ERC20 token standard, resulting in the transfer event could be identified. 
These limitations are not inherent to \tool's methodology and can be mitigated by automated or semi-automated techniques, e.g., code decompilation~\cite{grech2019gigahorse,grech2022elipmoc,suiche2017porosity} and manual intervention.
% and they are not within the scope of targets that \tool aims to detect.
% 11+8+76=95

% \ye{\textit{D1} contains DeFi applications of different domains using varied price models dicussed in~\cref{sec:backg_priceModel}. }
% \myfig\ref{fig:price_model_proportion} represents the number of each type of DeFi applications within \textit{D1} and the proportion of three price models in each type, where 88\% of tokens that were attacked utilize the CPMM, while only five of them were exploited through the vulnerabilities in their own custom price models; for the other three types of DeFi applications, customized price models account for approximately 70\%.

\begin{figure}[t]
    \centering
    \small
    \includegraphics[width=0.8\linewidth]{Figure/Overview_of_Evaluation_Result_v7.pdf}
    \vspace{-1ex}
    \caption{The categorized detection results on DeFi protocols.}
    \label{fig:price_model_proportion}
\end{figure}
% 90.7% = 39/43, 71.4% = 20/28, 83.3% = 10/12, 58.3%=7/12
% 43+28+12+12 = 95, 39+20+10+7=76


\subsection{RQ2: Ablation Study}
\label{sec:RQ2}

In this RQ, we investigate how fine-tuning can enhance \name's detection accuracy on the same \textit{D1} dataset, as well as the impact and cost of fine-tuning different GPT models.
To this end, we test four settings shown in \myfig~\ref{fig:RQ_2}: (a) the original GPT-3.5-Turbo without fine-tuning, (b) GPT-3.5-Turbo with fine-tuning, which was used in RQ1, (c) the original GPT-4o without fine-tuning, and (d) GPT-4o with fine-tuning.
%\ye{In this experiment, we employed GPT-3.5-turbo-1106 and GPT-4o-2024-08-06 as our baseline models to investigate the impact of the fine-tuning technique, evaluate them on \textit{D1}.} 

\myfig~\ref{fig:RQ_2} shows that fine-tuning significantly enhances \name's ability to detect attacks, with the fine-tuned versions of GPT-3.5-Turbo and GPT-4o detecting 18 (31\%) and 12 (19\%) more attacks, respectively.
It also indicates that fine-tuning provides a more noticeable improvement for less powerful models, such as GPT-3.5-Turbo, compared to stronger models like GPT-4o.
With fine-tuning, the detection success rate for attacks targeting the CPMM increases to 100\% with GPT-3.5-Turbo and 95.6\% with GPT-4o\footnote{GPT-4o does not exhibit an advantage with CPMM but shows a clear advantage with custom price models, especially when the original GPT-4o compared to GPT-3.5-Turbo.}.
This very high rate can be attributed to the use of CPMM during the fine-tuning training phase, enabling the model to effectively handle this specific pricing model.
% 31% = 18/(37+18+3), 19% = 12/(35+26+2), 95.6% = 43/45

\begin{figure}[t]
\centering
  \begin{subfigure}[t]{.49\columnwidth}
        \includegraphics[width=.8\columnwidth]{Figure/baseline_result_v7.pdf}
        \caption{GPT-3.5-Turbo w/o fine-tuning}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.49\columnwidth}
        \includegraphics[width=.8\columnwidth]{Figure/ds_result_v6.pdf}
        \caption{GPT-3.5-Turbo w/ fine-tuning}
    \end{subfigure}
    %\vskip\baselineskip
    \begin{subfigure}[t]{.49\columnwidth}
        \includegraphics[width=.8\columnwidth]{Figure/gpt_4o_result.pdf}
        \caption{GPT-4o w/o fine-tuning}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.49\columnwidth}
        \includegraphics[width=.8\columnwidth]{Figure/ft_gpt_4o_result.pdf}
        \caption{GPT-4o w/ fine-tuning}
    \end{subfigure}
    \vspace{-1ex}
\caption{Effectiveness of fine-tuning for 78 attacks that go through \name's LLM inference. The cases not plotted in the figures include eight due to missing source code and compilation errors, and nine due to cross-transaction issues and non-adherence to the ERC20 token standard, as mentioned in \mysec\ref{sec:RQ1}.}
    \vspace{1ex}
    \label{fig:RQ_2}
\end{figure}
% 78 + 8 + 9 = 95

%\ye{Fine-tuning the models also presents strong transfer learning capability.}
Although \name's fine-tuned models were trained using only CPMM data, we find that they also exhibit strong transfer learning capabilities for attacks targeting custom price models.
Specifically, the detection success rate for attacks targeting custom price models increases from 60\% to 93.3\% (a relative increase of 55.5\%) for GPT-3.5-Turbo and from 86.7\% to 96.7\% (a relative increase of 11.5\%) for GPT-4o. 
Further analysis of the LLM responses generated during detection reveals that the most significant difference introduced by fine-tuning is that the fine-tuned model strictly adheres to the CoT approach we specified.
This involves initially extracting the price model from the given code and then conducting inference based on the extracted model along with the provided information about balance changes.
In contrast, although the original model produces the final evaluation scores, it does not strictly follow the instructions of the CoT prompts.
% 60%=18/(12+18), 93.3%=28/(28+2), 55.5%=(93.3-60)/60
% 86.7%=26/(26+4), 96.7%=29/(29+1), 11.5%=(96.7-86.7)/86.7


%\noindent
%\textbf{Inference accuracy.}
%To understand how accurate the fine-tuned model identify and interpret code related to price calculations, we further conduct an in-depth measurement.
%We find that after fine-tuning, LLMs' inference accuracy reaches 83\% and 87\% for GPT-3.5 turbo and GPT-4o, respectively. 
%Because virtually LLMs generate text based on the preceding context, we think, a profound analysis of price models will substantially influence the precision of the final evaluation. 
%Therefore, the fine-tuned model can achieve higher detection accuracy against attacks targeting the custom price model.
%\ye{How much higher than GPT-3.5 turbo and GPT-4o without fine-tuning. This information is not clear.\juantao{Cannot compare, since before fine-tuning, the response of LLM only contains the evaluation score of price change, the code snippet on which the inference is based on is not included.}}


\begin{table}[t!]
    \centering
    \caption{Comparison between GPT-3.5-Turbo and GPT-4o.}
    \vspace{-2ex}
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
         Model & $TP$ & $Recall$ & \makecell[c]{Ave. Cost(\$)/\\per inference} & \makecell[c]{Fine-tuning\\Cost(\$)} \\
        \midrule
        GPT-3.5 w/o fine-tuning & 58 & 0.61 & 0.0023 & -\\
        GPT-3.5 w/ fine-tuning & 76 & 0.80 & 0.0107 & 8\\
        \midrule
        GPT-4o w/o fine-tuning & 63 & 0.66 & 0.008 & -\\
        GPT-4o w/ fine-tuning & 75 & 0.79 & 0.0131 & 25\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:compare_between_models}
\end{table}

\noindent
\textbf{LLM Costs.}
\mytab\ref{tab:compare_between_models} highlights that while the original GPT-4o generally performs better than GPT-3.5-Turbo, this performance gap can be largely minimized through fine-tuning.
However, regarding costs, GPT-3.5-Turbo has a clear advantage, which is why \name uses GPT-3.5-Turbo for fine-tuning by default instead of GPT-4o.
Specifically, the fine-tuning cost of GPT-3.5-Turbo is 68\% lower than that of GPT-4o, and the per-request inference cost of GPT-3.5-Turbo is merely \$0.0107~\cite{OpenAI_Price}, which is also 18\% cheaper than that of GPT-4o.
Therefore, for a tradeoff between cost and performance, we recommend using GPT-3.5-Turbo for \name.
% 68% = (25-8)/25, 18% = (13.1-10.7)/13.1

% \fixme{Moreover, we finally used fine-tuned GPT-3.5 turbo in RQ1 and RQ3, based on a comprehensive evaluation of cost and performance. As shown in \myfig~\ref{fig:RQ_2}, two models' performances are similar after fine-tuning, demonstrating the general applicability of our fine-tuning method, despite GPT-4o outperforms GPT-3.5 turbo in its base version. From the perspective of cost, fine-tuning GPT-3.5 turbo spends only 32\% of GPT-4o, and per-million-request cost is merely \$9~\cite{OpenAI_Price}, which is 48\% of that of GPT-4o, and this represents the primary expense of \tool.}
% \begin{table}[t]
%     \centering
%     \small
%     \caption{The summary result of attack detection.}
%     \begin{tabular}{c|c|c}
%     \toprule
%         Tool  &  \#Detectable & Success Rate \\
%         \midrule
%         \tool w/o fine-tuning & 58 & \\
%         \tool & 76 &\\
%         \bottomrule
%     \end{tabular}
%     \label{tab:summary_attack_detection}
% \end{table}




% \begin{table*}[!t]
%     \centering
%     \small
%     \caption{Price manipulation attacks detected by \tool. The attack time is in UTC. The loss amount is in USD.}
%     \vspace{-2ex}
%     \begin{tabular}{llllll}
%     \toprule
%         \# & Application & Chain & Transaction hash & Date & Loss
%         \\
%         \midrule
%         1 & NovaX & BSC & 0xb1ad1188d620746e2e64785307a7aacf2e8dbda4a33061a4f2fbc9721048e012 & 06-Aug-24 & 578K\\
%         2 & iVest & BSC & 0x12f27e81e54684146ec50973ea94881c535887c2e2f30911b3402a55d67d121d & 12-Aug-24 & 172K\\
%         3 & HFLH & BSC & 0xb61ae75835854e577657c25fe1891ff1c9ffb1b6b61ec4064396a08e222f679e & 23-Aug-24 & 5K\\
%         \bottomrule
%     \end{tabular}
%     \label{tab:real-world_impact}
% \end{table*}


%\subsection{RQ3: False Positives}
%\subsection{RQ3: Real-world Impact}

\subsection{RQ3: Real-World Practicality}
\label{sec:RQ3}
% \ye{RQ3. Transfer Learning of Price Change Inference}

% After conducting a thorough analysis of the dataset used in DeFort~\cite{DeFort24}, constructed by tracking activities of high-value DeFi applications over a period of time, to evaluate false positives, we discovered that there exists a large proportion of simple transactions which are unlikely to be attacks, i.e., transactions that involve only one action such as transferring or approving. 
% Given that our \textit{D2} is constructed based on suspicious transactions, each transaction possesses certain degree of complexity, which better facilitates that evaluation of the tool's actual false positive rate.

To be practical in a real-world setting, \name not only needs to maintain high detection rates for true attacks but also minimize false alarms for benign transactions.
In this RQ, we evaluate this aspect of \name and the associated time overhead using the datasets \textit{D2} and \textit{D3}, which were introduced in the prologue of \mysec\ref{sec:evaluation}.
% from February 2021 to June 2024
Specifically, \textit{D2} comprises 968 suspicious transactions with various attacks (i.e., not limited to price manipulations), and \textit{D3} includes 96,800 benign transactions collected from DeFort~\cite{DeFort24}'s dataset. %\dao{time?}.

%Since transactions in \textit{D2} were identified as abnormal, each could potentially be an attack.
For \textit{D2}, \tool flagged 153 price manipulation attacks out of 968 suspicious transactions.
To robustly confirm these potential attacks, we cross-referenced them with attack reports or alerts published by security companies through their official channels~\cite{ancilia_twi,beosin_twi,blocksec_twi,slowMist_twi}, thereby verifying the root causes of the attacks.
Using this method, we confirm that 66 of them are previously reported price manipulation attacks.
For the remaining cases, we conducted comprehensive and in-depth analyses by combining manual review with ancillary evidence, such as identifying whether the EOA initiating the transaction was marked as a hacker by blockchain explorers~\cite{bscscan,etherscan,oklink}.
Finally, we discovered 81 previously unknown historical incidents, which were neither reported by security companies nor tagged as malicious transactions by blockchain explorers, and identified a total of six false positive cases (as detailed in \mytab~\ref{tab:false_positives} in Appendix~\ref{sec:fp_details}).
Of these, five are benign transactions initiated by the same EOA and exhibiting identical invocation flows, while the last one is indeed an attack due to a logic issue rather than a price manipulation attack, resulting in a precision of 96\% (147/153).
% 66 (confirmed) + 81 (unknown) + 6 (FP) = 153

% TP / (TP+FP) = 147 / (147 + 6)
In comparison, DeFort identified only 58 attacks, of which 9 were false positives labeled as other vulnerabilities by security researchers.
Moreover, DeFort failed to detect 114 of the malicious transactions flagged by \tool, including 54 that are officially confirmed attacks.
% 114 = 54 confirmed + 60 unconfirmed
% To evaluate the false positive rate, we subjected \tool to an analysis using the \textit{D2} dataset, composed of suspicious transactions. Given that transactions in \textit{D2} are identified by MetaTrust as abnormal transactions, each of them is a possible attack. \tool detected 151 price manipulation attacks from all 968 suspicious transactions. In our commitment to a more objective evaluation, we corroborate these detected attacks against security companies' attack reports or alerts published through their official channels~\cite{ancilia_twi}~\cite{beosin_twi}~\cite{blocksec_twi}~\cite{slowMist_twi}, confirming the root cause of the attacks. Through this method, we confirmed that 38 of them are price manipulation attacks. For the remaining cases, we employ a combination of manual analysis and other ancillary evidence, such as the EOA initiating the transaction being marked as a hacker by blockchain explorers~\cite{bscscan}~\cite{etherscan}~\cite{oklink}, to conduct comprehensive and in-depth analysis. Finally, we identified a total of six false positive cases (as detailed in \mytab\ref{tab:false_positives} Appendix \ref{sec:fp_details}), where five of them are benign transactions initiated by the same EOA and exhibiting identical invocation flows, while the last one is indeed an attack due to a logic issue rather than price manipulation attack, \fixme{achieving a precision of 96.0\%}\juantao{Add to support data in abstract}. 


% \dao{Move to appendix}
% The reason is that these transactions involve two contract accounts that were created three months ago by the transaction initiator. Any fund transfers among these accounts and the initiator should be considered benign operations rather than price manipulation operations. \ye{Yet, these accounts were incorrectly marked as closed-source DEXes in the detection, leading to false inferences.}\juantao{Add more details, as reviewer D required}
% \ye{@juantao: what means `deployed` for user accounts?\juantao{Changed to `created`, mark as red}}, 
% \fixme{resulting in the erroneous analysis of the transfer between user accounts as an action for price manipulation}
% % an erroneous assumption in subsequent price change inference that one of the user accounts was a close-sourced liquidity pool and the transfer between two user accounts constituted manipulation of the token prices within the pool
% ~\ye{@juantao: not clear, could you simplify the words\juantao{Revised, mark as red}}. Consequently, the transaction was incorrectly identified as an attack. 
% Such false positives could be mitigated by conducting a historical analysis of account ownership relationships and clustering user accounts that are controlled by the same owners.
% by back tracing the deployer of CAs and label all CAs deployed by the known user accounts as new user accounts. 


We further analyze \name's false alarms in a realistic setting mixing 968 suspicious transactions in \textit{D2} with 96,800 benign transactions in \textit{D3} (see the earlier dataset setup in the prologue of this section).
For each transaction, we set the maximum scan time to 300 seconds.
The results reveal that \tool achieves zero false alarms on benign transactions in \textit{D3}, with an average of 2.5 seconds per transaction across all transactions. % and no cases of timeouts for benign transactions; suspicious的有超时也有error：超时101个，error 85个，都是slither的compilation error.
This highlights \name's potential for large-scale, daily on-chain monitoring scenarios.
%\juantao{Is it fast enough to claim this?}
%\ye{@juantao: should illustrate some experimental details, what timeout, how many errors, average speed. what's the %distribution of real-world transactions and how developers could use \name to detect price manipulation attacks.\juantao{This part need to revise after the experiment on 96800 sample is done, and consider combine the result to RQ3 instead of RQ1}}


% For the attack whose underlying cause is the logic issue, because the transaction includes a sequence of actions that conform to \textit{Pattern I}, it is labeled by \tool as a price manipulation. In reality, the attacker exploited vulnerabilities in the contract logic by continuously performing swaps and reverse swaps, to increase the gains, and then extracted profits through another transaction. Upon manual calculation, the actual profits realized by the attack in the marked transaction are less than zero, therefore, this type of false positives could be solved by imposing constraints on the total profits obtained by the attacker account.



%\subsection{RQ4: Real-world Impact}\label{sec:RQ4}
% \noindent
% \textbf{Case Study on Latest Attacks.}
% Moreover, \tool successfully detected three recent price manipulation attacks in August 2024, as shown in \mytab\ref{tab:real-world_impact}.
% % We deployed \tool to Ethereum and BSC for real-time monitoring. 
% % \tool successfully identified three price manipulation attacks and promptly reported to security companies. 
% For the attack on NovaX, the hacker exploited the vulnerability in the function \texttt{convertInternalSwap} invoked while withdrawing staked NOVAX from the app. As illustrated in \mylist\ref{lst:convertInternalSwap} (see Appendix~\ref{sec:case_studies}), the price of NOVAX calculated during the withdrawal is inversely proportional to the balance of USDT in the liquidity pool and directly proportional to the balance of NOVAX. By first depositing NOVAX into the app and then swapping a large amount of NOVAX for USDT, the calculated price of NOVAX was dramatically deflated, allowing the hacker to withdraw a quantity of NOVAX far exceeding the amount initially deposited. This attack matches and was detected by \textit{Pattern V}.

% \tool detected the second attack using \textit{Pattern I}. The attacker manipulated the price of iVest in the liquidity pool by calling \texttt{skim}, which aligns the balance of tokens in the pool with their reserves when the balance exceeds the reserve, transferring the excess to the account specified by the user. For the last attack, the attackers transferred large amounts of HFLH to the liquidity pool, causing a liquidity imbalance that led to a dramatically inflated price of WBNB, which \tool detected using \textit{Pattern II}. To conserve space, we leave a detailed description of these two attacks in Appendix~\ref{sec:case_studies}.

% Detailed analysis of other two cases are omitted here due to page limit. Interested readers may refer to Appendix \ref{sec:case_studies}.
% \ye{@juantao: no space already, move the detailed descriptions for two cases to appendix \juantao{Moved to Appendix \ref{sec:case_studies}}}
% As shown in \mylist\ref{lst:transfer} (in Appendix \ref{sec:vulerable_code}), \texttt{\_transfer} is invoked while the liquidity transferring tokens. By setting the receiver to zero address, the condition on the fourth line held, resulting in the invocation of \texttt{\_\_MakeDonation} to burn iVest from the pool. Subsequently, in \texttt{\_transferFromLP}, the pool once again burnt an identical quantity of iVest as it did previously, leading to a double burning. Consequently, the balance of iVest in the pool was substantially lower than the correct value, and its price was raised significantly. To make profit, the attacker initially exchanged WBNB to iVest, and then repeated the operation of transferring iVest to the pool followed by skimming to the zero address for four times, finally through transferring a small quantity of iVest to the pool, the attacker drained almost all WBNB, and gained \$172K. \tool detected this attack with \textit{Pattern I}.


% As for the attack on HFLH, attackers transferred large amounts of HFLH to the liquidity pool to cause a liquidity imbalance, which lead to a dramatically inflated price of WBNB returned by the function \texttt{getPrice} (as shown in \mylist\ref{lst:getPrice}, Appendix \ref{sec:vulerable_code}). To exploit the manipulated price, attacks invoked the function \texttt{borrow}, which calculated the amount of HFLH to be loaned based on the WBNB price returned by the function \texttt{getPrice}, with small amounts of WBNB to borrow HFLH at a value far exceeding the actual price of WBNB, and then made profits by swapping back to WBNB. This attack approach conforms with \textit{Pattern II}. 
% \vspace{-15pt}
% \subsection{Threats of Validity}

% While the precision of \tool is quite satisfactory, its recall rate is primarily affected by the following two threats.
% Future work can further optimize the following two aspects:
% %enhance the performance by targeting these two issues for optimization.

% \noindent\textbf{Cross-transaction price manipulation attacks.} 
% \tool currently detects only single-transaction attacks.
% However, some price manipulation attacks are designed across multiple transactions to circumvent time restrictions coded in DeFi protocols, as studied in~\cite{chen2024demystifying}.
% Detecting cross-transaction attacks is often challenging.
% For example, attacks like INUKO~\cite{Inuko} span 48 hours (crossing around 57,000 blocks), making it extremely difficult to precisely identify all related transactions.
% Fortunately, our study observed that cross-transaction attacks are less common than single-transaction ones.

% \noindent\textbf{Closed-source price calculation functions.} 
% The availability of code for price models could affect the detection accuracy of \tool.
% According to our study, most DeFi applications are open-source to gain user trust.
% To mitigate issues with closed-source price models, we design the Type-II prompt to cover price models in those closed-source liquidity pools.

% \noindent\textbf{Limitation in supported chains.} Given the necessity for highly detailed granularity of transaction data in the detection process, only the data obtained from Ethereum and BSC through RPC providers meets the requisite level of detail, thus \tool currently only supports analysis on above two chains, accounting for most of assets on the blockchain. Once the data meets our requirements, we will extend \tool to support detection on other chains in the future.

% \ye{discuss the limitation of \tool}
% \ye{list all the factors that may affect the performance of \tool and propose some mitigation methods.}
