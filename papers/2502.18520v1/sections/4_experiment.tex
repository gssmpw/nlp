\section{Experiments\label{sec4}}
\input{tables/table1_cifar}

To evaluate the effectiveness of the proposed method, we perform extensive experiments across different benchmark datasets and network architectures, including CIFAR-10 \cite{krizhevsky2009learning}, Tiny ImageNet \cite{le2015tiny}, and GTSRB \cite{stallkamp2011german} datasets, on PreAct-ResNet18 \cite{he2016identity} and VGG19-BN \cite{simonyan2014very} networks. We evaluate the proposed method against ten state-of-the-art (SOTA) backdoor attack methods, and compare the performance with nine SOTA backdoor defense methods. Additionally, we provide some analyses in Sec. \ref{sec4.4}.

\subsection{Implementation Details}
\subsubsection{Datasets and models}
Three benchmark datasets \cite{wubackdoorbench} are used to conduct comparison experiments including CIFAR-10 \cite{krizhevsky2009learning}, Tiny ImageNet \cite{le2015tiny}, and GTSRB \cite{stallkamp2011german}. In details, 
CIFAR-10 contains a total of 60,000 images divided into ten categories. Each category has 5,000 and 1000 images for training and testing, respectively. The size of each image is $32 \times 32$.
Tiny ImageNet, which is a smaller version of the ImageNet dataset \cite{deng2009imagenet}, includes 100,000 images for training and an additional 10,000 for testing, spread across 200 different classes. The images in this dataset have $64 \times 64$ pixels in size.
GTSRB contains a total of 39,209 images for training and 12,630 for testing, grouped into 43 categories. Each image in GTSRB is $32 \times 32$ pixels in dimensions.
To demonstrate the applicability of our method across different network architectures, we conduct experiments on both PreAct-ResNet18 \cite{he2016identity} and VGG19-BN \cite{simonyan2014very}, and compare their performance with SOTA methods.


\subsubsection{Attack settings}
We evaluate the proposed three defense methods against ten well-known backdoor attacks, including BadNets \cite{gu2019badnets} (both BadNets-A2O and BadNets-A2A, representing attacking one target label and multiple target labels, respectively), Blended attack (Blended \cite{chen2017targeted}), Input-aware dynamic backdoor attack (Input-aware \cite{nguyen2020input}), Low frequency attack (LF \cite{zeng2021rethinking}), Sample-specific backdoor attack (SSBA \cite{li2021invisible}), Trojan backdoor attack (Trojannn \cite{Trojannn}), Warping-based poisoned networks (WaNet \cite{nguyen2021wanet}), Frequency domain trojan attack (FTrojan \cite{wang2022invisible}), and Adaptive Backdoor Attack (Adap-Blend \cite{qi2023revisiting}). To ensure a fair comparison, we follow  the default attack configurations outlined in BackdoorBench \cite{wubackdoorbench}. The poisoning ratio is set to 10\% and the targeted label is set to the $0^{th}$ label when evaluating against state-of-the-art defenses. To demonstrate the effectiveness of our method under multiple target labels setting, we employ an all-to-all label attack strategy, BadNets-A2A, where the target labels for original labels $y$ are set to $y_t=(y+1) \bmod C$. Detailed introductions about these attacks are provided in Appendix B.

\subsubsection{Defense settings}
We compare the proposed three defense methods against nine SOTA backdoor defense methods, \ie, FP \cite{liu2018fine}, NAD \cite{li2021neural}, NC \cite{wang2019neural}, ANP \cite{wu2021adversarial}, i-BAU \cite{zeng2022adversarial}, EP \cite{zheng2022pre}, FT-SAM \cite{zhu2023enhancing}, SAU \cite{wei2023shared}, and FST \cite{min2024towards}. All these defenses have access to 5\% benign training samples for training. All the training hyperparameters are aligned with BackdoorBench \cite{wubackdoorbench}. We evaluate the proposed three defense methods with these SOTA defenses on different datasets and networks. 

\subsubsection{Details of our methods}
For our methods, we apply an $l_2$ norm constraint to the adversarial perturbations, with a perturbation bound of 3 for the CIFAR-10 and GTSRB datasets, and 6 for the Tiny ImageNet dataset. The loss hyperparameters $\lambda_1$, $\lambda_2$, and $\lambda_3$ are set to $1.0$, $0.4$, and $0.4$ for the CIFAR-10 in our NPD, e-CNPD and a-CNPD, and set to $1.0$, $0.5$, and $0.5$ for in our r-CNPD. We train the neural polarizer for 50, 10, 100, and 200 epochs for the NPD, a-CNPD, e-CNPD, and r-CNPD methods, respectively, with a learning rate of $0.01$ on the CIFAR-10 dataset. The neural polarizer block is inserted before the final convolutional layer of the PreAct-ResNet18 architecture for CNPD and before the penultimate convolutional layer for NPD. Additional implementation details regarding state-of-the-art attacks, defenses, and our methods can be found in Appendix B.

\subsubsection{Evaluation metrics}
In this work, we utilize three primary evaluation metrics to assess the effectiveness of various defense methods: clean Accuracy (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER). 
\begin{itemize}
    \item \textbf{ACC} reflects the accuracy of benign samples.
    \item \textbf{ASR} evaluates the percentage of poisoned samples that are successfully classified as the target label.
    \item \textbf{DER} \cite{zhu2023enhancing}) is a comprehensive metric that considers both ACC and ASR:
\end{itemize}
A higher ACC, a lower ASR, and a higher DER indicate superior defense performance. 
In the main tables, the effectiveness of defense methods against various attack methods is color-coded for clarity: the most effective defense is highlighted in $\cellcolor[HTML]{F1B9B6}\text{red}$, the second most effective in $\cellcolor[HTML]{FEEBB8}\text{yellow}$, and the third in $\cellcolor[HTML]{D7E8F2}\text{blue}$.

\input{tables/table2_gtsrb}
\input{tables/table6_pratio}
\input{tables/table7_cleanratio_enpd}
\input{tables/table7_cleanratio_anpd}

\subsection{Main Results}
We first evaluate the effectiveness of our NPD and CNPD defense methods against ten SOTA backdoor attacks, using three different datasets and two network architectures. As shown in Tab. \ref{table1} and \ref{table2},  we observe that our defense methods demonstrate superior performance across all tested attacks. Specifically, our r-CNPD, e-CNPD, and a-CNPD methods achieved the first, second, and third highest average DER scores, respectively, and our a-CNPD achieves average DERs of 95.88\%, 97.90\%, and 93.32\% on CIFAR-10, GTSRB, and Tiny ImageNet datasets, respectively. Additionaly, our NPD also shows superior performance in terms of DER for almost all attacks compared to SOTA defenses. These results highlight that our methods outperform other defenses, with the a-CNPD method yielding the best overall average DER. Defense results with Tiny ImageNet dataset and VGG19-BN network are provided in Appendix C.

\textbf{Effectiveness Against SOTA defenses.}
Tab. \ref{table1} and \ref{table2} present a comparison between our method and the SOTA backdoor defenses. It is evident that our method consistently performs well across different datasets and models, achieving both high ACC and low ASR. While FP and ANP show high ACC, their effectiveness in reducing ASR is highly unstable, highlighting the limitations of existing pruning-based methods. In terms of reducing ASR, SAU and FST demonstrate strong performance, but they sometimes fall short in maintaining ACC. Although FT-SAM also demonstrates some advancements, it struggles with complex attacks on large datasets, \ie, Tiny ImageNet. In contrast, our method excels in both maintaining high ACC and reducing ASR, as reflected by the high DER. 

\textbf{Comparison of our defenses.}
The four defense methods proposed in this work achieve the best DER and ASR. Among them, a-CNPD generally delivers the best overall performance, while e-CNPD and r-CNPD show comparable results, ranking second and third, respectively. Specifically, a-CNPD is able to reduce the ASR to below 1\% while maintaining excellent model utility.

\subsection{Ablation Study}
\subsubsection{Effectiveness against low poisoning ratio}
Low poisoning-ratio attacks have long posed a significant challenge to the effectiveness of defense mechanisms \cite{zhang2024reliable,qi2023revisiting,ma2023dihba}. Tab. \ref{table6} presents the results of our e-CNPD defense against attacks with varying poisoning ratios on CIFAR-10 dataset. As shown, our method remains robust even under low poisoning ratios, likely because it effectively identifies backdoor-related signals and blocks them efficiently.
\input{tables/table8_loss}
\subsubsection{Effectiveness on low clean ratio}
In practice, training samples for fine-tuning may be scarce. Tab. \ref{table7_2} and \ref{table7_1} show the defense results of our e-CNPD and a-CNPD methods under different clean ratios. As the tables indicate, our methods remain effective even with low clean ratios, although there is a slight decrease in model performance, suggesting that some regularization may be needed to maintain performance. On the other hand, the consistently low ASR demonstrates that our methods are lightweight and efficient, as they require very few training parameters, allowing for strong defense performance even with limited training samples.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{figs/layers.pdf}
\caption{Defense performance of inserting NP into different layers.}
\label{fig:layers}
\end{figure}

\subsubsection{Performance of inserting NP into different layers}
We evaluate the impact of selecting different layers for inserting the NP layer by positioning it before each convolutional layer in the PreAct-ResNet18 network, tested on the CIFAR-10 dataset. Fig. \ref{fig:layers} presents our a-CNPD's defense performance under different attacks. The results indicate that inserting the NP layer into shallower layers leads to a drop in accuracy, as even minor perturbations in these layers can cause significant instability in the final output. However, as the insertion point moves deeper into the network, the features become more distinct, leading to improved defense performance.


\subsubsection{Effectiveness of each loss term}
We conduct an ablation study to assess the contribution of each component of the loss function to the overall performance on CIFAR-10 dataset. Specifically, we examine the first and second terms of the loss function, $\gL_{bce}$ (see Eq. (\ref{eq:bce_loss})), which we denote as $\gL_{bce1}$ and $\gL_{bce2}$, respectively. As shown in Tab. \ref{table8}, both $\gL_{bce}$ and $\gL_{asr}$ play a crucial role in enhancing overall performance, and removing any component results in a considerable drop in defense effectiveness.

\subsection{Analysis\label{sec4.4}}
\input{tables/table5_detect}
\subsubsection{Further extension to test time detection}
%intro
In this work, we demonstrate that the CNPD framework can be effectively extended to detect poisoned samples during the inference phase. 
Specifically, we introduce a poisoned sample detection mechanism that compares the network's output with and without the NP layer. By evaluating any discrepancies between the two outputs, we can identify whether a query sample is poisoned. Notably, this detection method is highly efficient and incurs only a negligible increase in inference cost compared to a single forward pass.

Our approach leverages the unique properties of the NP layer. When a backdoor attack is successful, the attacked sample is misclassified as the target label. However, the NP layer suppresses the backdoor feature, altering the network's output label for the poisoned sample, while the output for clean samples remains unchanged. This distinction allows us to design a simple yet effective poisoned sample detection method, as described by the following rule:
\begin{equation}
    R(\x;f_{\w},\hat{f}_{\vtheta})= \begin{cases}\text {1}, & \text { if } f_{\w}(\x)\neq \hat{f}_{\vtheta}(\x) \\ 0 , & \text { if } f_{\w}(\x)= \hat{f}_{\vtheta}(\x)\end{cases},
\end{equation}
where $\x$ is input, $1$ denotes that $\x$ is detected as a backdoor sample.
Unlike existing backdoor detection methods, this approach eliminates the need for repeated model queries, additional computational overhead, or extra training for optimization or estimation. In essence, it performs backdoor detection with only minimal computational cost beyond the initial inference. We denote this detection method as \textit{NPDT}

% 对比方法
Here, we compare the proposed NPDT with four SOTA backdoor detection methods, \ie, 
SCALE-UP \cite{guoscale}, SentiNet \cite{chou2020sentinet}, STRIP \cite{gao2019strip}, and IBD-PSC \cite{hou2024ibdpsc}.
% metric
 For the comparison of detection performance, we evaluate the True Positive Rate (\textbf{TPR}) and False Positive Rate (\textbf{FPR}) at a fixed threshold for each detection method. Superior detection performance is indicated by a higher TPR and a lower FPR.
Tab. \ref{table5} shows the detection performance of our NPDT method in identifying samples during the inference phase. Compared to existing detection methods, our approach demonstrates a very high TPR of 99.68\% on average and a very low FPR of 5.52\%. The FPR is not zero due to the inherent limitations in the model’s classification accuracy. However, the exceptionally high TPR highlights the outstanding performance of our method against malicious Attacks.

%%%%%%

\subsubsection{Visualization on purified features}
In this work, we mitigate backdoors by filtering the intermediate poisoned features of the network through a learnable neural polarizer (NP) layer. To further understand the impact of this mechanism, we conduct a detailed analysis of feature changes before and after passing through the NP layer. We perform two visualization experiments as follows.

\textbf{Visualization of feature map changes before and after the NP layer.}
An effective NP layer should suppress features that exhibit high activation values on backdoor-related neurons. Here, we use the TAC metric \cite{zheng2022data} to measure the correlation between neurons and the backdoor effect. We then rank these neurons according to their correlation with the backdoor and visualize the feature maps of poisoned samples on these neurons. We compare the feature maps between backdoored models with the defense models, as shown in Fig. \ref{feature_map}. Note that we use the output of the final convolutional layer of the network for visualization. In each subplot in the figure, the neurons are arranged from top to bottom according to their correlation with the backdoor, from high to low. As observed, in the backdoored model (\textbf{first row}), poisoned samples exhibit highlighted activations on the backdoor-related neurons, whereas in the defense model (\textbf{second row}), poisoned samples no longer show highlights on the backdoor-related neurons. This indicates that the backdoor-related features have successfully been suppressed.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/feature_map.pdf}
    \caption{Feature maps of poisoned samples in the final convolutional layer of PreAct-ResNet18 network, with and without the neural polarizer layer, on CIFAR-10 dataset.}
    \label{feature_map}
\end{figure*}

\textbf{Visualization of feature space changes between poisoned and clean samples.}
To further compare NP's effects on clean and poisoned samples, we analyze it from the perspective of feature subspaces. Specifically, we examine the following four types of features: 1) Features of clean samples under backdoored models (\textbf{Clean before}); 2) Features of clean samples under defense models (\textbf{Clean after}); 3) Features of poisoned samples under backdoored models (\textbf{BD before}); 4) Features of poisoned samples under defense models (\textbf{BD after}).

We perform Singular Value Decomposition (SVD) \cite{stewart1993early} on the “Clean before” and “BD before” groups, extracting two principal component vectors, \( v_{\text{clean}} \) and \( v_{\text{bd}} \). For each of the four sample groups, we compute the normalized similarity for the two vectors. The results are visualized as the components along \( v_{\text{clean}} \), as shown in Fig. \ref{visual_bar}. It can be observed that:  
\begin{itemize}
    \item After passing through the NP layer ("Clean after," orange bars), the feature activations remain largely aligned with \( v_{\text{clean}} \), suggesting that the NP layer preserves clean sample features effectively while applying minimal perturbations.
    \item The "BD after" group (green bars) exhibits strong activations on \( v_{\text{clean}} \), and suppressed activations on \( v_{\text{bd}} \), different from the "BD before" group (red bars).
\end{itemize}
The visualization demonstrates that the NP layer successfully suppresses backdoor-related feature activations while retaining the integrity of clean sample features. This selective filtering mechanism confirms the robustness of the NP layer against various types of backdoor attacks.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/bar.png}
    \caption{Visualization of clean and poisoned samples with and without the neural polarizer layer on CIFAR-10 dataset.}
    \label{visual_bar}
\end{figure*}


\input{tables/table12_time}
\subsubsection{Running time comparison}
Here, we demonstrate the training efficiency of our method. We tested the training times of all defense methods on two datasets, namely CIFAR-10 and Tiny ImageNet, with the results presented in Tab. \ref{table12}. As observed, the r-CNPD method incurs a significant training cost, while our NPD, e-CNPD and a-CNPD methods are significantly more efficient compared to existing defense approaches.






