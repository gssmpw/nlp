\begin{abstract}

Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign performance.
In this work, inspired by the concept of optical polarizer—which allows light waves of specific polarizations to pass while filtering others—we propose a lightweight backdoor defense approach, \textit{NPD}. This method integrates a \textbf{neural polarizer} (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. 
Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. 
To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (\textit{CNPD}). 
The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD.
We propose three implementations of CNPD: the first is \textit{r-CNPD}, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: \textit{e-CNPD}, which embeds class information as additional features, and \textit{a-CNPD}, which directs network attention using class information.
Additionally, we provide a theoretical guarantee for the feasibility of the proposed CNPD method, and demonstrate that CNPD establishes an upper bound on backdoor risk.
Extensive experiments show that our lightweight and effective methods outperform existing methods across various neural network architectures and datasets.
\end{abstract}

\begin{IEEEkeywords}
Backdoor learning, backdoor defense, adversarial machine learning, trustworthy machine learning.
\end{IEEEkeywords}

