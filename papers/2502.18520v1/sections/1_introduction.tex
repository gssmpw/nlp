
\section{Introduction\label{sec1}}

\IEEEPARstart{D}{eep} Neural Networks (DNNs) have achieved remarkable performance in lots of critical domains, such as facial recognition \cite{sharif2016accessorize} and autonomous driving \cite{yurtsever2020survey}.
However, the susceptibility of DNNs to deliberate threats \cite{dong2021query,goldblum2022dataset,yin2023generalizable,wu2023adversarial} poses a serious challenge, as it imperils the integrity of these systems and undermines their reliability. Notably, the risk posed by backdoor attacks is garnering increased attention due to their inherent sophistication and stealthiness \cite{wu2024backdoorbench,zhu2024breaking}.
Backdoor attacks entail the malicious manipulation of the training dataset \cite{gu2019badnets} or training process \cite{liang2024badclip}. This results in a backdoored model which classifies poisoned samples with \textit{triggers} to a predefined \textit{target label} and behaves normally on benign samples \cite{gu2019badnets}. Backdoor attacks may originate from a multitude of avenues, such as the use of contaminated training datasets, employing third-party platforms for model training, or procuring pre-trained models from untrusted sources. These circumstances considerably heighten the risk posed by backdoor attacks on DNN's applications. Concurrently, they underscore the cruciality of devising backdoor defenses against such attacks.


This work focuses on \textbf{post-training defense}, which aims to mitigate the backdoor effect in a backdoored model, using a small subset of benign data, while preserving the model's performance on benign inputs.
Current approaches in this category mainly involve global fine-tuning \cite{zeng2022adversarial,zhu2023enhancing} or network pruning \cite{liu2018fine,zheng2022pre}. However, these methods face two challenges. First, the large parameter space involved in fine-tuning or pruning makes it difficult for a limited amount of benign data to effectively remove backdoors without compromising model performance. Second, the optimization of this parameter space is computationally expensive.

To address the above constraints, we introduce a lightweight yet effective countermeasure that operates without requiring model retraining. 
Our approach is inspired by optical polarizers \cite{xiong2016optical}, which allow only light waves with specific polarizations to pass while blocking others in a mixed light wave (refer to Fig. \ref{motivation}). 
Similarly, our method involves inserting one learnable layer while keeping all layers of the original backdoored model fixed. By applying a similar principle, if we consider a poisoned sample as a combination of both trigger and benign features, we can design a \textit{neural polarizer} (NP) to filter the trigger features while conserving the benign ones. Consequently, contaminated samples could be cleansed thereby diminishing the impact of the backdoor, while benign samples remain largely unaffected.
With the lightweight NP, our objective is to weaken the correlation between the trigger and the target label, where both of them are inaccessible to the defender. To tackle it, we first formulate a bi-level optimization problem, where in inner optimization, we dynamically estimate the target label and trigger with adversarial examples (AEs), and the NP is updated in outer minimization by unlearning these AEs. We term this method as Neural Polarizer based backdoor Defense (\textit{NPD}).

In this process, we observe that the effectiveness of defense depends on the accuracy of the estimated target label. An incorrect estimation can either fail to remove the backdoors or cause oscillations in the attack success rate during training. To address this limitation, we observe that the target label is available when an attacker successfully launches an attack. Therefore, we explore leveraging the output of the backdoored model as a proxy for the target label, utilizing the ``home field'' advantage of defenders. To this end, we propose class-conditional neural polarizer-based backdoor defense (\textit{CNPD}). This method introduces a fusion module that combines class information predicted by the backdoored model with the features to be purified, effectively emulating the targeted adversarial approach. We propose three implementations of CNPD. The first, replicated Class-conditional NPD (r-CNPD),  which trains a replicated NP layer for each class in post-training phase, and in inference phase, the class predicted by the backdoored model is used to select the corresponding NP layer for feature purification. This approach demonstrates performance comparable to the targeted adversarial defense method.
Given the computational cost increases with the number of classes, two variants are designed to mitigate this complexity. The first, \textit{e-CNPD}, integrates class embeddings as additional features to assist in feature purification. The second, \textit{a-CNPD}, uses class-related information to guide the model's attention toward the features requiring purification, inspired by attention mechanisms \cite{vaswani2017attention}. These approaches enable efficient backdoor purification while keeping computational complexity manageable. Additionally, we provide a theoretical guarantee for the feasibility of CNPD, and demonstrate that it offers an upper bound on the backdoor risk \cite{wei2023shared}.

\begin{figure}
\centering
\vspace{0.5em}
\includegraphics[width=0.49\textwidth]{figs/motivation.pdf}
\caption{Comparison of Optical and Neural Polarizers. An optical polarizer allows only light waves with specific polarizations to pass through. Similarly, in a neural polarizer integrated into a compromised model, only benign features are allowed to pass, while backdoor-related features are filtered out, effectively removing the backdoor.}
\label{motivation}
% \vspace{-4.15em}
\end{figure}


Our main contributions are fourfold:
1) We propose a novel backdoor defense paradigm, NPD, that optimizes only one additional layer while keeping the original parameters of the backdoored model fixed.
2) Based on NPD, we introduce the Class-conditional NPD (CNPD), which uses the backdoored model to guide the purification of poisoned samples. We design three variations-\textit{r-CNPD}, \textit{e-CNPD}, and \textit{a-CNPD}-to implement this approach.
3) We provide a theoretical analysis to validate the feasibility of our CNPD approach, and establish an upper bound on backdoor risk.
4) We conduct extensive experiments and analyses, including comparisons with state-of-the-art (SOTA) methods, and visualization analysis.


This paper extends our conference paper (\textit{NPD} \cite{zhu2024neural}) with significant improvements. Compared to the conference version, the current version introduces the following key enhancements:
1) We introduce CNPD, which leverages the backdoored model to guide poisoned sample purification.  
2) We propose \textit{r-CNPD} and introduce two additional efficiency-boosting mechanisms—\textit{e-CNPD}  and \textit{a-CNPD} —which embed label information to more effectively filter out backdoor features.
3) We provide a theoretical analysis to validate CNPD’s feasibility and show that it offers an upper bound on backdoor risk compared to the original NPD.  
4) We conduct additional experiments, including comparisons with SOTA methods, ablation studies, and visualization analyses.

% The rest of the paper is organized as follows: Section \ref{sec2} reviews the development trends in backdoor attacks, defenses, and detections. Section \ref{sec3} provides a detailed introduction to the three proposed methods, with theoretical analysis. Section \ref{sec4} presents the results of extensive experiments and analysis. Section \ref{sec5} concludes the paper, and we hope this work contributes to enhancing the robustness of DNNs against backdoor attacks and inspiring further research into more efficient and practical defense strategies.




