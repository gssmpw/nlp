
\section{Methodology\label{sec3}}
In this section, we present the details of our methodology. First, in Sec. \ref{sec3.1}, we define the basic settings, including the notations and the threat model.  
Next, in Sec. \ref{sec3.2}, we introduce our Neural Polarizer-based Defense (NPD) method, outlining its objective, optimization, and algorithm.  
In Sec. \ref{sec3.3}, we analyze the limitation of NPD and present our Class-conditional NPD method, followed by the introduction of three distinct implementations in Sec. \ref{sec3.4}.  
Finally, in Sec. \ref{sec3.6}, we provide a theoretical analysis to support the feasibility and effectiveness of our approach.


\subsection{Basic Settings\label{sec3.1}}
\subsubsection{Notations} We consider a classification problem with $C$ classes ($C\geq 2$). Assume $\x\in \gX \subset \mathbb{R}^d$ to be a $d$-dimensional input sample, its corresponding ground truth label is denoted as $y\in \gY = \{1,\dots,C\}$. A deep neural network comprising of $L$ layers $f:\gX\times\gW\to \mathbb{R}^C$, parameterized by $\w\in\gW$, is defined as follows:
\begin{equation}
f(\x ; \w)=f^{(L)}_{\w_{L}} \circ f^{(L-1)}_{\w_{L-1}} \circ \cdots \circ f^{(1)}_{\w_{1}} (\x),
\end{equation}
where $f^{(l)}_{\w_{l}}$ embodies the function (e.g., convolution layer or batch normalization layer) with parameter $\w_{l}$ located in the $l^{\text {th }}$ layer, where $1 \leq l \leq L$. To maintain simplicity, $f(\x ; \w)$ is presented as $f_{\w}(\x)$ or $f_{\w}$. Given an input $\x$, the predicted label of $\x$ is determined by $\argmax_c f_c(\x;\w), c=1,\ldots,C$, where $f_c(\x; \w)$ represents the logit of the $c^{\text {th}}$ class.

\subsubsection{Threat model} 
\textbf{Attacker's goal.} In this work, we consider the scenario that the attacker is an unauthenticated third-party model publisher or dataset publisher. Thus the backdoored model $f_{\w}$ can be generated through the manipulation of either the training process or the training data, ensuring that $f_{\w}$ functions correctly on benign sample $\x$ (\ie, $f_{\w}(\x) = y$), and predicts the poisoned sample $\x_{\Delta} = r(\x, \Delta)$ to the targeted label $T$. Here, $\Delta$ indicates the pre-defined trigger and $r(\cdot, \cdot)$ is the fusion function that integrates $\x$ and $\Delta$. 
Considering the possibility of the attacker establishing multiple target labels, we assign $T_i$ to represent the target label for sample $\x_i$.

This threat is practical in the real world applications that heavily rely on AI-driven decisions, particularly in scenarios where organizations use third-party models or datasets without thoroughly scrutinizing their integrity due to resources limits.
The stealthiness of backdoor attacks makes them particularly hard to immediately recognize, thus posing substantial potential risks for individuals or entities.

\noindent \textbf{Defender's capabilities and objectives.}
Assume that the defender is given the backdoored model $f_{\w}$ and has access to a limited number of benign training samples, denoted as $\mathcal{D}_{bn}=\{(\x_i,y_i)\}_{i=1}^{N}$. 
The defender's goal is to obtain a new model $\hat{f}$ based on $f_{\w}$ and $\mathcal{D}_{bn}$, such that the backdoor effect is removed and the benign performance is maintained in $\hat{f}$, \ie, $\hat{f}(\x_{\Delta}) \neq y$ and $\hat{f}(\x) = y$. Note that the defender is unaware of the attacker's target labels and specified triggers, and can only leverage a limited number of samples to modify the backdoored model. This presents a stringent yet highly practical scenario; for instance, a user may lack the resources to train a model from scratch and instead obtain a model from a third-party unauthenticated organization. However, fine-tuning the model is affordable to achieve a secure model.

\begin{figure*}
\centering
\vspace{0.5em}
\includegraphics[width=\textwidth]{figs/two_methods.pdf}
\caption{\textbf{(a)}: Neural Polarizer based Backdoor Defense. Backdoor defense by integrating a trainable neural polarizer into the compromised model.
\textbf{(b)}: Class-conditional Neural Polarizer based Backdoor Defense. During training, a trainable neural polarizer is incorporated into the compromised model, with a fusion module that fuses internal features and class information. During inference, the output of the backdoored model is used to guide class-conditional neural polarizer for feature filtering.}
\label{two_methods}
% \vspace{-4.15em}
\end{figure*}

\subsection{Neural Polarizer based Backdoor Defense\label{sec3.2}}
We propose the neural polarizer (NP) to purify poisoned samples in the feature space. As shown in Fig. \ref{two_methods} \textbf{(a)}, the neural polarizer $g_{\vtheta}$ is inserted into the backdoored model $f_{\w}$ at a specific immediate layer, to obtain a combined network $f_{\w, \vtheta}$, \ie, $f_{\w, \vtheta}=f^{(L)}_{\w_{L}} \circ \cdots \circ f^{(l+1)}_{\w_{l+1}} \circ g_{\vtheta} \circ f^{(l)}_{\w_{l}} \circ \cdots \circ f^{(1)}_{\w_{1}} (\x)$. For clarity, we denote $f_{\w, \vtheta}$ as $\hat{f}_{\vtheta}$, since $\w$ is fixed.

\subsubsection{Objective of the neural polarizer\label{sec3.2.1}}
Informally, a desired neural polarizer should have the following three properties:
\begin{itemize}[leftmargin=*]
    \item \textbf{Compatible with neighboring layers}: This implies that both the input feature and output activation should be of the same shape, which can be achieved through careful architectural design.
    \item \textbf{Filtering trigger features in poisoned samples}: After applying the neural polarizer, the trigger features should be filtered out to ensure that the backdoor is deactivated, \ie, $\hat{f}_{\vtheta}(\x_{\Delta})\neq T$.
    \item \textbf{Preserving benign features in both poisoned and benign samples}: The neural polarizer should maintain benign features to ensure that $\hat{f}_{\vtheta}$ performs effectively on both poisoned and benign samples, \ie, $\hat{f}_{\vtheta}(\x_{\Delta})= \hat{f}_{\vtheta}(\x) = y$.     
\end{itemize}
The first property could be easily satisfied by carefully designing the architecture of the neural polarizer. For example, given the input feature map \(m^l(\x) \in \mathbb{R}^{C_l \times H_l \times W_l}\), the neural polarizer is implemented by a convolution layer (Conv) with $C_l$ convolution filters of shape $1\times1$, followed by a Batch Normalization (BN) layer. The Conv-BN block can be seen as a linear transformation layer. 
To satisfy the latter two properties, the parameters $\vtheta$ should be learned by solving a carefully formulated optimization problem as presented in Sec. \ref{sec3.2.2}.

\subsubsection{Learning neural polarizer\label{sec3.2.2}}
\textbf{Loss functions in oracle setting.}
To learn a desired neural polarizer $g_{\vtheta}$, we begin with an oracle scenario in which the trigger $\Delta$ and the target label $T$ are provided. We introduce several loss functions designed to encourage the NP $g_{\vtheta}$ to fulfill the two properties previously mentioned, as outlined below:
\begin{itemize}[leftmargin=*]
    \item \textbf{Loss for filtering trigger features in poisoned samples.} Given trigger $\Delta$ and target label $T$, trigger features can be filtered  by weakening the connection between \(\Delta\) and \(T\) using the following loss function:
    \begin{equation}
        \gL_{asr}(\x,\Delta, T; \vtheta) = -\log(1 - s_{T}(\x_{\Delta}; \vtheta)),
        \label{eq:asr_loss}
    \end{equation}
    where \(s_{T}(\x_{\Delta}; \vtheta)\) represents the softmax probability of the model \(\hat{f}_{\vtheta}\) predicting \(\x_{\Delta}\) as label \(T\).
    %
    \item \textbf{Loss for maintaining benign features in poisoned samples.} To ensure that a poisoned sample can be correctly classified to its true label, we utilize the boosted cross-entropy loss introduced in \cite{wang2020improving}:
    \begin{equation}
    \begin{aligned}
        \gL_{bce}(\x, y, \Delta; \vtheta) &= -\log(s_{y}(\x_{\Delta}; \vtheta)) \\
        &\quad - \log\big(1 - \max_{k \neq y} s_{k}(\x_{\Delta}; \vtheta)\big).
        \label{eq:bce_loss}
    \end{aligned}
    \end{equation}
    %
    \item \textbf{Loss for maintaining benign features in benign samples.} To preserve benign features of benign samples, the standard cross-entropy loss is employed:
    \begin{equation}
        \gL_{bn}(\x, y; \vtheta) = -\log(s_{y}(\x; \vtheta)).
    \end{equation}
\end{itemize}
% \
% \newline
% \noindent
% \quad 
% \textbf{Approximating \(T\) and $\Delta$.}
\noindent \textbf{Approximating \(T\) and $\Delta$.}
Note that the target label \(T\) and trigger \(\Delta\) is inaccessible in both \(\gL_{asr}\) and \(\gL_{bce}\). As a result, these two losses cannot be directly optimized and require an approximation. In terms of approximating \(T\),  although several methods \cite{ma2022beatrix,guo2021aeva} have been developed to detect the target label of backdoored models, we adopt a straightforward, sample-specific, and dynamic strategy: 
\begin{equation}
    T\approx y' = \argmax_{k\neq y} \hat{f}_k(\x;\vtheta).
    \label{eq:label}
\end{equation}
This strategy offers two key advantages. First, the predicted target label allows us to generate targeted adversarial perturbations. As discussed later, targeted perturbations serve as a better surrogate for the unknown trigger compared to untargeted ones. Second, the sample-specific target label prediction is versatile and works for both all2one (one trigger, one target class) and all2all (every class as a potential target) attack settings. In practice, defenders often lack certainty regarding the attack type—whether it’s all2one, all2all, or multi-trigger multi-target—leading to potential sub-optimal defense strategies due to incorrect assumptions or detection failures. Our approach, with its sample-specific target label prediction, mitigates this risk.

In terms of approximating \(\Delta\), it is dynamically approximated by the targeted adversarial perturbation (T-AP) \cite{madry2017towards} of \(\hat{f}_{\vtheta}\), \ie, 

\begin{equation}\label{eq:at}
\Delta \approx \vdelta^* = \argmin_{\|\vdelta\|_p\leq \rho} \gL_{\text{CE}}\left(\hat{f}_{\vtheta}(\x+\vdelta), y'\right),
\end{equation}
where $\|\cdot\|_p$ is the $L_p$ norm, $\rho$ is the budget for perturbations.
  

\noindent \textbf{Bi-level optimization problem.}
By utilizing the approximation of \(T\) and \(\Delta\) as shown in Eq. (\ref{eq:label}) and (\ref{eq:at}), and substituting them into the loss functions in Eq. (\ref{eq:asr_loss}) and Eq. (\ref{eq:bce_loss}), we formulate the following optimization problem to learn \(\vtheta\) based on the clean dataset \(\mathcal{D}_{bn}\):
\begin{equation}\label{loss}
\begin{aligned}
    \min_{\vtheta} \quad \quad & \frac{1}{N}\sum_{i=1}^N \lambda_1 \gL_{bn}({\x_i,y_i};\vtheta) +  \lambda_2 \gL_{asr}({\x_i,y_i,\vdelta^*_i, y'_i;\vtheta}) \\
    & + \lambda_3 \gL_{bce}({\x_i,y_i,\vdelta^*_i;\vtheta}),\\
    \mathrm{s.t.} \quad \quad & \vdelta^*_i = \argmin_{\|\vdelta_i\|_p\leq \rho} \gL_{\text{CE}}\left(\hat{f}_{\vtheta}(\x_i+\vdelta_i), y'_i\right), \\
    & y'_i = \argmax_{k_i \neq y_i} \hat{f}_{k_i}(\x_i;\vtheta),
\end{aligned}
\end{equation}
where $\lambda_1,\lambda_2,\lambda_3 > 0 $ are hyper-parameters to adjust the importance of each loss function. 
This bi-level optimization is dubbed Neural Polarizer based backdoor Defense (\textbf{NPD}).
NPD is highly efficient, as the lightweight NP layer contains only a few parameters that need optimization, allowing it to be learned with a limited number of clean samples.

\ 
\newline
\noindent
\quad 
\textbf{Optimization algorithm.} We propose Algorithm \ref{alg1_npd} to solve the above NPD. Specifically, the key steps involve alternatively updating the perturbation $\vdelta$ and $\vtheta$ as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Inner minimization:} Given parameters $\vtheta$, estimate the target label $y'$ by Eq. (\ref{eq:label}). Then the targeted Project Gradient Descent (PGD) \cite{madry2017towards} is employed to generate the perturbation $\vdelta^*$ via Eq. (\ref{eq:at}).
    %
    \item  \textbf{Outer minimization:} Given $y'$ and $\vdelta^*$ for each sample in a batch, the $\vtheta$ can be updated by taking one stochastic gradient descent \cite{bottou2007tradeoffs} (SGD) step \wrt~ the outer minimization objective in Eq. ($\ref{loss}$).
\end{itemize}

\begin{algorithm}[h]
\caption{Neural Polarizer based Backdoor Defense}\label{alg1_npd}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training set $\mathcal{D}_{bn}$, backdoored model $f_{\w}$, neural polarizer $g_{\vtheta}$, learning rate $\eta>0$, perturbation bound $\rho>0$, norm $p$, hyper-parameters $\lambda_1$,$\lambda_2$,$\lambda_3>0$, warm-up epochs $\mathcal{T}_0$, training epochs $\mathcal{T}$, number of PGD steps $T_{adv}$.
\vspace{-4mm}
\STATE \textbf{Output:} Model $\hat{f}(\w,\vtheta)$.
\STATE Initialize $g_{\vtheta}$ to be an identity function. Fix $\w$, and construct the composed network $\hat{f}(\w,\vtheta)$.
\STATE Warm-up: Train $g_{\vtheta}$ with $\mathcal{L}_{bn}$ for $\mathcal{T}_0$ epochs.
\FOR {$t=0,...,$ $\mathcal{T} -1$}
\FOR {mini-batch $\mathcal{B}=\{(\x_i,y_i)\}_{i=1}^{b}\subset \mathcal{D}_{bn}$}
\STATE Compute $\{y'_i\}_{i=1}^{b}$ by Eq. (\ref{eq:label}); 
\STATE Generate perturbations $\{(\boldsymbol{\delta_i})\}_{i=1}^{b}$ with $\boldsymbol{\|\delta_{i}\|_p}\leq \rho$ and target labels $\{y'_i\}_{i=1}^{b}$ by targeted PGD attack \cite{madry2017towards} via Eq. (\ref{eq:at});
\STATE Update $\vtheta$ via outer minimization of Eq. (\ref{loss}) by SGD.
\ENDFOR
\ENDFOR
\RETURN Model $\hat{f}(\w,\vtheta)$. 
\end{algorithmic}
\end{algorithm}


\subsection{Class-conditional NPD\label{sec3.3}}
\subsubsection{Limitations of NPD\label{sec3.3.1}}
\begin{figure}[h]
\centering
\vspace{0.5em}
\includegraphics[width=0.4\textwidth]{figs/moti_matplot_trojannn.pdf}
\caption{Defense performance of unlearning adversarial examples (AEs) using different strategies on Trojan attack \cite{Trojannn}. ``Target'' refers to unlearning AEs that use the attacker's target label. ``Wrong'' indicates unlearning AEs assigned a label that is not the attacker's target label. ``Random'' signifies unlearning AEs with randomly assigned labels. ``Untarget'' represents unlearning AEs with an untargeted objective.}
\label{moti}
% \vspace{-4.15em}
\end{figure}

One limitation of NPD lies in its reliance on the accuracy of the estimated target label (see Eq. (\ref{eq:label})). An inaccurate estimation may either fail to eliminate backdoors or result in oscillations in the attack success rate during training.
To further investigate this, we conduct experiments to compare the defense performance using different AEs. 
In detail, we use the same model architecture and bi-level optimization framework as in NPD, and substitute the AEs generation method with four different AEs generation methods. As shown in Fig. \ref{moti}, ``Target'' refers to unlearning targeted AEs (using the same target label as attacker's target label $T$), while ``Wrong'' involves unlearning AEs assigned a label different from the attacker's target. The ``Random'' consists of unlearning AEs with randomly assigned labels, and ``Untarget'' represents unlearning AEs with an untargeted objective. The experiment is conducted using the Trojan attack \cite{Trojannn} on the CIFAR-10 dataset with PreAct-ResNet18 network.
As the figure illustrates, the ``Wrong'' group fails in backdoor mitigation, and the ASR curves in the backdoor removal processes of the ``Random'' and ``Untarget'' groups exhibit oscillations. In contrast, the ``Target'' group removes the backdoor effectively and steadily with minimal impact on accuracy. This experiment highlights that different types of AEs result in significantly varying backdoor removal performance, with targeted AEs proving to be the most effective.

However, the target label remains unavailable to the defender in post-training stage. Fortunately, the target label is revealed when the attacker launches an attack during inference. Therefore, we propose utilizing the output of the backdoored model as a proxy for the target label, taking advantage of the defenders' "home field" advantage.


\subsubsection{Class-conditional NPD for DNNs\label{sec3.3.2}}
Unlike NPD, which unlearns targeted AEs based on estimated target labels, Class-conditional NPD (CNPD) designs a fusion module that is trained evenly across all classes, using class information as a condition for feature purification.
As dipicted in Fig. \ref{two_methods} \textbf{(b)}, the fusion module combines the class information \( y \) and the feature map $m^{l}(\x)$ as input, and the output of the module is:
\begin{equation}
    \hat{g}_{\vTheta}(m^{l}(\x),y) = S(g_{\vtheta}(m^{l}(\x)),y).
\end{equation}
Here, $S$ represents fusion operation, and $\vTheta$ is the parameters set of NP.
The combined network is denoted as $f_{\w, \vTheta}$ or $\hat{f}_{\vTheta}$, \ie, $f_{\w, \vTheta}=f^{(L)}_{\w_{L}} \circ \cdots \circ f^{(l+1)}_{\w_{l+1}} \circ \hat{g}_{\vTheta} \circ f^{(l)}_{\w_{l}} \circ \cdots \circ f^{(1)}_{\w_{1}} (\x)$. 


The optimization of $\hat{g}_{\vTheta}$ is similar to the optimization of $g_{\vtheta}$ as introduced in Eq. (\ref{loss}). The only difference is that the second constraint in Eq. (\ref{loss}) becomes:
\begin{equation}
    y'_i \in \{1,\ldots,C\},i = 1,\ldots,N.
\end{equation}
It means the surrogate target label is equally distributed across all classes.
In the training phase, we train NP for each label evenly for backdoor mitigation. 
During the testing phase, it involves two forward passes. First, the predicted label $y'=\argmax f_{\vtheta}(\x)$ is obtained from the backdoored model $f_{\vtheta}$ without the NP. Then, the fusion module purifies $m^{l}(\x)$ based on $y'$ to produce the final output category.
\textit{This strategy makes sense because a successful attack must imply that $y'=T$, and thus the fusion module adjusts its parameters for defense.}


\begin{figure*}
\centering
\vspace{0.5em}
\includegraphics[width=\textwidth]{figs/three_structures.pdf}
\caption{
\textbf{(a)}. Replicated CNPD: Each class is associated with an individual neural polarizer.
\textbf{(b)}. Embedding-based CNPD: Class information is embedded as features within the model.
\textbf{(c)}. Attention-based CNPD: Class information is used to guide the network's attention for purification.
}
\label{three_structures}
% \vspace{-4.15em}
\end{figure*}
\subsection{Three Implementations of CNPD\label{sec3.4}}
To implement our CNPD approach, we design three distinct fusion module architectures, each accompanied by its corresponding training algorithm.

\subsubsection{Replicated CNPD\label{sec3.4.1}}
As dipicted in Fig. \ref{three_structures} \textbf{(a)}, the replicated CNPD (\textbf{r-CNPD}) defines a separate NP for each class $c$, parameterized by $\vtheta_c$, \ie, $\vTheta=\{\vtheta_c,c=1,\cdots,C\}$. Given an input feature map $m^{l}(\x)$ with label $y$, the output of the NP layer is:
\begin{equation}
    \hat{g}_{\vTheta}(m^{l}(\x),y) = g_{\vtheta_y}(m^{l}(\x)).
\end{equation}
In the training phase, the parameters $\vtheta_y$ are sequentially trained across all classes for backdoor mitigation. The training algorithm is shown in Alg. \ref{alg2_rnpd}.

\begin{algorithm}[h]
\caption{Replicated Class-conditional NPD (\textbf{r-CNPD})}\label{alg2_rnpd}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training set $\mathcal{D}_{bn}$, backdoored model $f_{\w}$, neural polarizer $\hat{g}_{\vTheta}$,
learning rate $\eta>0$, perturbation bound $\rho>0$, norm $p$, hyper-parameters $\lambda_1$,$\lambda_2$,$\lambda_3>0$, training epochs $\mathcal{T}$, number of PGD steps $T_{adv}$.
\STATE \textbf{Output:} Model $f(\w,\vTheta)$.
\STATE Initialize $g_{\vtheta_c}$ to be an identity function for $c\in {1,\ldots,C}$. Fix $\w$, and construct the composed network $f(\w,\vTheta)$.
\FOR {$c=1,...,C$}
\FOR {$t=0,...,$ $\mathcal{T} -1$}
\FOR {mini-batch $\mathcal{B}=\{(\x_i,y_i)\}_{i=1}^{b}\subset \mathcal{D}_{bn}$}
\STATE Generate perturbations $\{(\boldsymbol{\delta_i})\}_{i=1}^{b}$ with $\boldsymbol{\|\delta_{i}\|_p}\leq \rho$ and target label $c$ by targeted PGD attack \cite{madry2017towards} via Eq. (\ref{eq:at});
\STATE Update $\vtheta_c$ via outer minimization of Eq. (\ref{loss}) by SGD.
\ENDFOR
\ENDFOR
\ENDFOR
\RETURN Model $f(\w,\vTheta)$. 
\end{algorithmic}
\end{algorithm}


\subsubsection{Embedding-based CNPD\label{sec3.4.2}}
Although our r-CNPD method performs effectively in practice, the training cost and network capacity increase as the number of categories in the classification task grows, which may hinder its practical application. To address this issue, we design a fusion module that incorporates a fixed NP structure and a label embedding set, as illustrated in Fig. \ref{three_structures} (b).
Unlike r-CNPD, the \textbf{e-CNPD} method directly incorporates category information into the network as embedded features to influence the effect of different categories on feature filtering. Let the input feature be \(m^l(\x) \in \mathbb{R}^{C_l \times H_l \times W_l}\). We predefine specific embedded features for each category, denoted as \(e(c) \in \mathbb{R}^{1 \times H_l \times W_l}\), where \(c \in \{1, \ldots, C\}\). The concatenated feature \(\text{cat}[m^l(\x), e(y)]\) is then passed through the NP for filtering, defined as:
\begin{equation}
    \hat{g}_{\vTheta}(m^{l}(\x),y) = g_{\vTheta}(\text{cat}[m^{l}(\x),e(y)]).
\end{equation}
During each batch update in the training phase, we first randomly generate target labels for all batch samples and then create targeted adversarial samples to train the NP layer.

\subsubsection{Attention-based CNPD\label{sec3.4.3}}
Although the e-CNPD method is effective, careful consideration is required when designing the scale of feature fusion to avoid excessive distortion on the original features. To address this, we propose a more refined feature filtering method called Attention-based CNPD (\textbf{a-CNPD}). As shown in Fig. \ref{three_structures} (c), we design an attention-like \cite{vaswani2017attention} mechanism, enabling different labels to modulate the network's attention to features across channels. 
We denote the embedding of class \(c\) as \(e(c)\), where \(c \in \{1, \dots, C\}\), and the parameters of the two linear layers as \(\vtheta_Q\), \(\vtheta_K\), one convolutional layer as \(\vtheta_V\). The subsequent convolutional and batch normalization layers are denoted as \(g_{\vtheta_o}\). Given an input feature map \(m^l(\x)\) with label \(y\), the output of the network \(g_{\vtheta}\) is defined as:
\begin{equation}
\label{eq:anpd}
    \begin{aligned}
    	&\hat{g}_{\vTheta}(m^l(\x), y) = \\
    	&g_{\vtheta_o}(\text{softmax}\left[\frac{(\vtheta_Q \cdot e(y))^{\top} (\vtheta_K \cdot e(y))}{\sqrt{d_k}}\right]\cdot (\vtheta_V (m^l(\x)))),
    \end{aligned}
\end{equation}
where \(d_k\) is the dimension of \(\vtheta_K\).

The detailed algorithms for solving e-CNPD and a-CNPD are slightly different from those of r-CNPD, which are provided in Appendix A.


\subsection{Theoretical Analysis\label{sec3.6}}
In this section, we analyze the effectiveness of the CNPD method theoretically. 

\subsubsection{Existence of Class-conditional Neural Polarizer}
We first provide theoretical foundation for the existence of Class-conditional Neural Polarizer. To simplify our analysis, we focus on classification task equipped with a squared loss \cite{masnadi2008design} in a reproducing kernel Hilbert space $\mathcal{H}_{X}$ \cite{rosipal2001kernel} generated by kernel $\kappa_{X}$, feature mapping $\phi_{X}$ and inner product $\langle \cdot, \cdot\rangle_{\mathcal{H}_{X}}$. Then, for a function $h\in \mathcal{H}_{X}$, we have $h(\x)=\langle \phi_{X}(\x),h\rangle_{\mathcal{H}_{X}}$. 
%Here, uppercase symbols represent random variables, while their lowercase counterparts denote specific realizations.
We introduce $M(\x)$ as a function indicating whether a data point $(\x,y)$ is poisoned ($M(\x)=1$) or not ($M(\x)=0$). 

Given a poisoned dataset $\mathcal{D}_{bd}$, 
we consider a poisoned classifier that  minimizes the expected squared loss over this poisoned dataset:
\begin{equation} 
\label{mse}
h_{bd} = \argmin_{h\in\mathcal{H}_{X}}\mathbb{E}_{(\bm{X},Y)\sim\mathcal{D}_{bd}}(h(\bm{X})-Y)^2.
\end{equation}
Then, we provide the following Theorem: 

\begin{thm}
Assume that $\phi_{X}(\x_i)\neq \phi_{X}(\x_j)$ if $\x_i\neq \x_j$. Given a poisoned model $h_{bd}$ defined in Eq.~(\ref{mse}), there exists non-trivial linear projection operators $P_{y}$ for $y$ such that
$$\text{Cov}(\langle \phi^y_{X}(\bm{X}), h_{bd}\rangle_{\mathcal{H}_{X}}|Y=y, M(\bm{X})|Y=y)=0,$$
where $\phi^y_{X}(\x) = P_y\phi_{X}(\x)$ is the projected feature of $\phi_{X}(\x)$ for the sample pair $(\x,y)$. 
\label{theorem1}
\end{thm}

The proof of Theorem~\ref{theorem1} can be found in Appendix A. The assumption in Theorem~\ref{theorem1} requires that the mapping from the input space to the feature space preserves the uniqueness of each sample and the feature of a poisoned sample is not completely overwritten by the trigger. Then, Theorem~\ref{theorem1} states that \emph{it is possible to construct a class-conditional neural polarizer for each class that decorrelates the prediction of $h_{bd}$ from the poison status of the sample without altering the trained classifier itself.}  Such an operator can be derived from the eigenfunctions associated with the class-conditional covariance operator between $M(\bm{X})$ and $\bm{X}$ in the specific kernel space \cite{wei2023mean}.  Therefore, Theorem~\ref{theorem1} supports the existence of the class-conditional neural polarizer.

\subsubsection{Mechanism of CNPD}
Building upon the backdoor risk introduced in \cite{wei2023shared}, we demonstrate that the CNPD learning process is equivalent to minimizing an upper bound on backdoor risk, providing a theoretical guarantee of CNPD’s effectiveness.

We first recall that the \textbf{backdoor risk} $\mathcal{R}_{bd}$ defined in \cite{wei2023shared} is:
\begin{align}
    \label{eq::bd_risk}
    \mathcal{R}_{bd}(h) = \frac{\sum_{\x\sim\mathcal{D}} \mathbb{I}(h(\x_{\Delta})=T)}{|\mathcal{D}|}.
\end{align}
Here, $\mathbb{I}$ is the indicator function, which equals 1 if the argument is true and 0 otherwise,  $\mathcal{D}$ is a non-target dataset that includes samples from non-target classes, and $|\cdot|$ denotes the size of the set.

Given a backdoored model $h_{bd}$, for a poisoned sample $\x_{\Delta}$, the corresponding CNPD is trained to minimize the targeted adversarial risk for target $h_{bd}(\x_{\Delta})$. Therefore, following the work of \cite{wei2023shared, wang2019improving}, we defined the following risk for CNPD, which measures the risk of classifying an adversarial example to the target $h_{bd}(\x_{\Delta})$:

\noindent\resizebox{.85\linewidth}{!}{
  \begin{minipage}{\linewidth}
   \begin{equation*}
    \mathcal{R}_{cnpd}(h)=\frac{\sum\limits_{\x\sim\mathcal{D},t\in[1,C]}\max\limits_{\|\delta\|_{p}\leq\rho}\mathbb{I}(h_{bd}(\x_{\Delta})=t)\mathbb{I}(h(\x+\vdelta)=t,t\neq y)}{|\mathcal{D}|}.
\end{equation*} 
\end{minipage}}

\noindent We then establish the following theorem:

\begin{thm}
Assume that $h_{bd}$ is a fully backdoored model, \ie, $h_{bd}(\x_{\Delta})=T$ for all $\x\in\mathcal{D}$. Furthermore, assume that $\|\x_{\Delta}-\x\|_p\leq \rho$. Then, the following inequality holds:
\begin{equation}
    \mathcal{R}_{bd}(h)\leq \mathcal{R}_{cnpd}(h).
\end{equation}
\label{theorem2}
\end{thm}

The above theorem indicates that CNPD offers an upper bound on backdoor risk. The proof of Theorem~\ref{theorem2} can be found in Appendix A. 


