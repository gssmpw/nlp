\section{Conclusion\label{sec5}}
In conclusion, we have proposed a novel and lightweight backdoor defense approach, Neural Polarizer Defense (NPD), which effectively filters out malicious triggers from poisoned samples while preserving benign features. Drawing inspiration from the concept of optical polarizers, NPD integrates a learnable neural polarizer (NP) as an intermediate layer within compromised models, effectively mitigating backdoor attacks while maintaining benign performance. 
Furthermore, to enhance the defense capabilities, we introduce class-conditional neural polarizer-based defense (CNPD), which emulates the targeted adversarial approach by combining class information predicted by the backdoored model with the features to be purified. 
Through three distinct instantiations of CNPD—r-CNPD, e-CNPD, and a-CNPD—we show how to balance backdoor mitigation with computational efficiency, especially for large class numbers.
Moreover, we provide a theoretical guarantee for the class-conditional neural polarizer, establishing an upper bound on backdoor risk.
Our method also includes an effective poisoned sample detection mechanism during inference, requiring minimal additional computational cost while achieving high detection accuracy. Extensive experiments across diverse neural network architectures and datasets show that our approach outperforms existing backdoor defense techniques, offering a robust and efficient solution with minimal reliance on clean training data.
Overall, the proposed neural polarizer offers a promising direction for enhancing backdoor defense in deep learning models, balancing attack mitigation with performance preservation.

\textbf{Limitations and future work.}
Although minimal, the method still relies on some clean training data for effective NP layer training. In scenarios with no clean data, defense performance may be compromised. Therefore, a promising direction for future work is to explore data-free training or training the neural polarizer with out-of-distribution samples. Applying our method to multimodal models or large-scale neural networks could be a promising direction, as full finetuning these models is often computationally expensive. By integrating our defense mechanism with such models, we could achieve more robust and scalable backdoor defenses.



\section*{Acknowledgments}
Baoyuan Wu is supported by the Guangdong Basic and Applied Basic Research Foundation (No. 2024B1515020095), 
Shenzhen Science and Technology Program (No. RCYX20210609103057050 and JCYJ20240813113608011), 
Sub-topic of Key R\&D Projects of the Ministry of Science and Technology (No. 2023YFC3304804), 
Longgang District Key Laboratory of Intelligent Digital Economy Security, and Guangdong Provincial Special Support Plan - Guangdong Provincial Science and Technology Innovation Young Talents Program (No. 2023TQ07A352). 
Hongyuan Zha is supported in part by the Shenzhen Key Lab of Crowd Intelligence Empowered Low-Carbon Energy Network (No.
ZDSYS20220606100601002).


