\section{Related work\label{sec2}}
\subsection{Backdoor attacks}
Deep neural networks (DNNs) have been found to be vulnerable to backdoor attacks, where the backdoored model performs normally on benign inputs while predicting a specified target label when encounters input with the pre-defined trigger. Backdoor attacks are generated by poisoning training dataset \cite{gu2019badnets,chen2017targeted,zeng2021rethinking,li2021invisible} or injecting backdoors in the training process \cite{nguyen2020input,nguyen2021wanet,liang2024badclip,foret2021sharpnessaware}.
In dataset poisoning attacks, the adversary maliciously modifies the dataset by introducing carefully crafted triggers onto images and altering the corresponding labels to a target label. As a result, the backdoored model learns incorrect associations between the triggers and the intended target label.
The seminal work by Gu \etal \cite{gu2019badnets} introduced the "BadNets", demonstrating that subtle alterations to a fraction of training samples could lead to a targeted misclassification during inference. This foundational study spurred extensive exploration into various mechanisms of backdoor insertion, particularly focusing on invisible triggers design \cite{li2021invisible,zeng2021rethinking}, dataset poisoning with low poisoning ratio \cite{song2024wpda,zhu2023boosting}, or clean-label attacks \cite{gao2021clean,shafahi2018poison,barni2019new} that crafts poisoned samples without modifying labels. These works increase the stealthiness of backdoor attacks, and highlight the necessity for improved detection and mitigation techniques. 

In training controllable backdoor attacks, the adversary strategically optimizes the triggers and backdoored models meanwhile to enhance the stealthiness and impact of the triggers, ensuring they can be effectively activated by the model. Additionally, the adversary seeks to make the model more resilient to backdoor detection and mitigation efforts.
For instance, WaNet \cite{nguyen2021wanet} proposes a novel "noise" training mode to increase the difficulty of detection. He \etal \cite{he2024sharpnessaware} proposes sharpness-aware data poisoning
attack that maintains the poisoning effect under various re-training process. Such advancements underscore the increasing sophistication of backdoor strategies, emphasizing the growing need for comprehensive defense method in AI systems.
Beyond the traditional images domain, backdoor attacks have also proven to be highly effective against self-supervised models \cite{jia2022badencoder,li2023embarrassingly}, diffusion models \cite{chou2024villandiffusion,chou2023backdoor}, large language models \cite{yang2024watch,li2024backdoorllm}, and multimodal models \cite{liang2024badclip,bai2024badclip}, posing a significant threat to deep learning.

\subsection{Backdoor defense}
Numerous studies \cite{wu2023defenses,niu2024towards} have been conducted to develop defenses against the threats posed by backdoor attacks. According to defense stages, existing studies can be roughly partitioned into four categories: pre-processing \cite{chen2022effective,zhu2024vdc}, in-training \cite{li2021anti,huang2022backdoor}, post-training \cite{wang2019neural,li2021neural}, and inference stage defenses \cite{guoscale,chou2020sentinet}. 
Pre-processing defenses aim to remove poisoned samples from a compromised dataset by input anomaly detection. 
Training-stage strategies assume that a defender is given the poisoned training dataset and the defender needs to train a secure model and inhibit backdoor injection based on the poisoned dataset. These strategies exploit differences between poisoned and clean samples to filter out potentially harmful instances \cite{chen2022effective}. 
Post-training defenses assume that a defender has access to a backdoored model and a limited number of clean samples. These defenses aim to eliminate backdoor effects from compromised models without requiring knowledge of the trigger or target labels. Techniques include pruning potentially compromised neurons \cite{liu2018fine,zheng2022pre} and performing strategic fine-tuning \cite{zeng2022adversarial,zhu2023enhancing}. For fine-tuning based methods, I-BAU \cite{zeng2022adversarial} establishes a minimax optimization framework for training the network using samples with universal adversarial perturbations. Similarly, PBE \cite{mu2023progressive} leverages untargeted adversarial attacks to purify backdoor models. SAU \cite{wei2023shared} firstly searches for shared adversarial example between the backdoored model and the purified model and unlearns the generated adversarial example for backdoor purification. 
\textit{These studies leverage adversarial examples in backdoor models to mitigate backdoor threats; however, none have discussed the impact of different adversarial examples on the effectiveness of backdoor removal.}
Inference stage defenses seek to prevent backdoor activation through sample detection and destroying trigger's feature. 
In this work, we mainly consider post-training backdoor defense, and follow their default settings.

% \subsection{Poisoned sample detection}
% Poisoned sample detection \cite{chen2019detecting,zhang2024reliable} has garnered significant attention due to its critical role in ensuring the robustness of machine learning models. Existing methods can be broadly categorized into pre-training stage detection and inference-time detection.
% The pre-training stage detection identifies and mitigates backdoor attacks through data filtering. Representative works are AC \cite{chen2019detecting}, Spectre \cite{hayase2021spectre}, and SS \cite{tran2018spectral}.
% Inference-time detection aims to identify poisoned inputs during the inference phase. These methods typically analyze the model's predictions or internal activations when processing new inputs. For example,
% SCALE-UP \cite{guoscale} employs a novel approach by analyzing prediction consistency during pixel-wise amplification, identifying poisoned samples based on their scaled prediction consistency.
% SentiNet \cite{chou2020sentinet} uses neural network interpretability techniques to localize attack triggers, effectively detecting adversarial patches and poisoned inputs.
% STRIP \cite{gao2019strip} perturbs incoming inputs and analyzes the randomness of predicted classes; low entropy in predictions indicates the presence of a poisoned input.
% IBD-PSC \cite{hou2024ibdpsc} leverages parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples remain abnormally consistent during model parameter amplification, enabling effective detection.
% These methods show robustness across various attack types. However, \textit{these methods rely on multiple inferences per sample or additional optimization steps to ensure detection}. This significantly increases inference costs, making them less practical for real-time deployment.













