%arabic 阿拉伯数字
%roman 小写的罗马数字
%Roman 大写的罗马数字
%alph 小写字母
%Alph 大写字母

% \textbf{Organization of the Supplementary Materials.}
% We have provided the table of contents below to facilitate easy navigation of the Supplementary Materials.
% \begin{itemize}
%     \item Section \ref{A} presents the detailed algorithms for the proposed e-CNPD and a-CNPD, alongside an introduction to the Projected Gradient Descent (PGD) method. Additionally, we provide the detailed proof of our Theorem \ref{theorem1} and \ref{theorem2} in the main manuscript.
%     \item Section \ref{B} covers the implementation details, including the datasets, the attack and defense methods we compare, and our proposed approaches.
%     \item Section \ref{C} displays more defense results, including the results on Tiny ImageNet dataset, as well as on VGG19-BN architecture.
% \end{itemize}

% \setcounter{section}{0}
% \setcounter{equation}{0}
% \setcounter{page}{1}
\clearpage
% \twocolumn[
% \begin{center}
%     \begin{spacing}{2}
%         {\LARGE \textbf{Supplementary Materials of ``Conditional Neural Polarizer: Leveraging Backdoor Models for Self-Purification''}}
%         \vspace{1cm}
%     \end{spacing}
% \end{center}
% ]

\long\def\comment#1{}
\renewcommand\thesection{\Alph{section}}
% \appendix
\section*{\large Appendix}
We have provided the table of contents below to facilitate easy navigation of the Appendix.
\begin{itemize}
    \item Section \ref{app_A} presents the detailed algorithms for the proposed e-CNPD and a-CNPD, alongside an introduction to the Projected Gradient Descent (PGD) method. Additionally, we provide the detailed proof of our Theorem \ref{theorem1} and \ref{theorem2} in the main manuscript.
    \item Section \ref{app_B} covers the implementation details, including the datasets, the attack and defense methods we compare, and our proposed approaches.
    \item Section \ref{app_C} displays more defense results, including the results on Tiny ImageNet dataset, as well as on VGG19-BN architecture.
\end{itemize}

\setcounter{section}{0}


\section{More Algorithmic Details and Theoretical result\label{app_A}}
\subsection{Introduction to PGD Algorithm}
In this section, we introduce the PGD attack algorithm \cite{madry2017towards}, which plays a crucial role in three of our defense methods.

Projected Gradient Descent (PGD) is a multi-step extension of the Fast Gradient Sign Method (FGSM) \cite{kurakin2018adversarial}. It performs projected gradient descent on the negative loss function. Specifically, given an input \((\x, y)\) and a model \(\hat{f}_{\vtheta}\), the adversarial example is computed iteratively as follows:

\begin{equation} 
\x^{n+1} = \Pi_{\x + \mathcal{S}}\left(\x^n + \alpha \operatorname{sgn}\left(\nabla_{\x} L_{\text{CE}}(\hat{f}_{\vtheta}(\x), y)\right)\right),
\end{equation}
where \(\Pi_{\x + \mathcal{S}}\) denotes the projection onto the \(\rho\)-ball with \(L_p\) norm: \(\|\x^{n+1} - \x\|_p \leq \rho\), and \(\x^0 = \x\), \(n = 0, \cdots, N-1\). Here, \(\alpha\) is the step size.

However, in Section 3 of the main manuscript, we utilize a confidence-guided targeted PGD approach to generate the targeted adversarial example. In this case, the adversarial example is updated as follows:

\begin{equation} \label{eq:pgd}
\x^{n+1} = \Pi_{\x + \mathcal{S}}\left(\x^n - \alpha \operatorname{sgn}\left(\nabla_{\x} L_{\text{CE}}(\hat{f}_{\vtheta}(\x), T)\right)\right),
\end{equation}
where \(T\) denotes the target label for \(\x\). For convenience, we define the adversarial perturbation as:

\begin{equation} \label{eq:pert}
\boldsymbol{\delta} = \x^{n+1} - \x^n.
\end{equation}


\subsection{Detailed Algorithms of e-NPD and a-CNPD}
In the main manuscript, we introduce the algorithm for CNPD. Here, we provide the detailed algorithms for e-CNPD and a-CNPD, as shown in Alg. \ref{alg2_enpd} and \ref{alg3_anpd}.

\begin{algorithm}[h]
\caption{Embedding based Class-conditional NPD (e-CNPD)}\label{alg2_enpd}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training set $\mathcal{D}_{bn}$, backdoored model $f_{\w}$, neural polarizer $\hat{g}_{\vTheta}$, class embedding $e(c),c\in {1,\ldots,C}$,
learning rate $\eta>0$, perturbation bound $\rho>0$, norm $p$, hyper-parameters $\lambda_1$,$\lambda_2$,$\lambda_3>0$, training epochs $\mathcal{T}$, number of PGD steps $N$.
\STATE \textbf{Output:} Model $f(\w,\vTheta)$.
\STATE Initialize $g_{\vtheta}$. Fix $\w$, and construct the composed network $f(\w,\vTheta)$.
\FOR {$t=0,...,$ $\mathcal{T} -1$}
\FOR {mini-batch $\mathcal{B}=\{(\x_i,y_i)\}_{i=1}^{b}\subset \mathcal{D}_{bn}$}
\STATE Randomly sample target labels $\{y'_i\neq y_i|y'_i\in \{1,\ldots,C\}\}_{i=1}^{b}$;
\FOR{$n=0,...,$ $N -1$}
\STATE Generate perturbations $\{(\boldsymbol{\delta_i})\}_{i=1}^{b}$ with $\boldsymbol{\|\delta_{i}\|_p}\leq \rho$ and target labels $\{y'_i\}_{i=1}^{b}$ by targeted PGD attack \cite{madry2017towards} via Eq. (\ref{eq:at});
\ENDFOR
\STATE Select the corresponding NPs for clean batches $\{\x_i\}_{i=1}^{b}$ and perturbed batches $\{\x_i+\boldsymbol{\delta_{i}}\}_{i=1}^{b}$ according to labels $\{y_i\}_{i=1}^{b}$ and target labels $\{y'_i\}_{i=1}^{b}$ respectively. 
\STATE Update $\vTheta$ via outer minimization of Eq. (\ref{loss}) by SGD.
\ENDFOR
\ENDFOR
\RETURN Model $f(\w,\vTheta)$. 
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
\caption{Attention-based Class-conditional NPD (a-CNPD)}\label{alg3_anpd}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training set $\mathcal{D}_{bn}$, backdoored model $f_{\w}$, neural polarizer $\hat{g}_{\vTheta}$, class embedding $e(c),c\in {1,\ldots,C}$,
learning rate $\eta>0$, perturbation bound $\rho>0$, norm $p$, hyper-parameters $\lambda_1$,$\lambda_2$,$\lambda_3>0$, training epochs $\mathcal{T}$, number of PGD steps $N$.
\STATE \textbf{Output:} Model $f(\w,\vTheta)$.
\STATE Initialize $g_{\vtheta}$. Fix $\w$, and construct the composed network $f(\w,\vTheta)$.
\FOR {$t=0,...,$ $\mathcal{T} -1$}
\FOR {mini-batch $\mathcal{B}=\{(\x_i,y_i)\}_{i=1}^{b}\subset \mathcal{D}_{bn}$}
\STATE Randomly sample target labels $\{y'_i\neq y_i|y'_i\in \{1,\ldots,C\}\}_{i=1}^{b}$;
\FOR{$n=0,...,$ $N -1$}
\STATE Generate perturbations $\{(\boldsymbol{\delta_i})\}_{i=1}^{b}$ with $\boldsymbol{\|\delta_{i}\|_p}\leq \rho$ and target labels $\{y'_i\}_{i=1}^{b}$ by targeted PGD attack \cite{madry2017towards} via Eq. (\ref{eq:at});
\ENDFOR
\STATE Select the corresponding NPs for clean batches $\{\x_i\}_{i=1}^{b}$ and perturbed batches $\{\x_i+\boldsymbol{\delta_{i}}\}_{i=1}^{b}$ according to labels $\{y_i\}_{i=1}^{b}$ and target labels $\{y'_i\}_{i=1}^{b}$ respectively. 
\STATE Update $\vTheta$ via outer minimization of Eq. (\ref{loss}) by SGD.
\ENDFOR
\ENDFOR
\RETURN Model $f(\w,\vTheta)$.
\end{algorithmic}
\end{algorithm}
%%%


\subsection{Proof of Theorem 1}
% In this section, we provide the proof of our theorem 1 and 2.

% \begin{thm}
% Assume that $\mathbb{P}(m=0)\in (0,1)$ and $\phi_{XM}(\x_i,m)\neq \phi_{XM}(\x_j,m)$ if $\x_i\neq \x_j$. Given a poisoned model $h_{bd}$ trained by minimizing (\ref{mse}), there exists a non-trivial linear projection operator $P$ such that
% $$\text{Cov}(\langle \hat{\phi}_{XM}(\x,m), h_{bd}\rangle_{\mathcal{H}_{XM}}, m)=0,$$
% where $\hat{\phi}_{XM}(\x,m) = P\phi_{XM}(\x,m)$ is the projected feature of $\phi_{XM}(\x,m)$ and $\langle \cdot, \cdot\rangle_{XM}$ is the inner product in $\gH_{XM}$.
% \label{theorem1}
% \end{thm}

\begin{proof}
    To prove Theorem 1, we consider two cases:
    
    \noindent \textbf{Case 1: $\text{Cov}(\phi_{X}(\bm{X})|Y=y,M(\bm{X})|Y=y) = 0$:} This case contains either exclusively clean ($P(M(\bm{X})=1|Y=y)=0$) or entirely poisoned ($P(M(\bm{X})=1|Y=y)=1$) samples in class $y$, or the feature mapping $\phi_{X}$ fail to capture the correlation between $\bm{X}$ and its poisoning status for samples in class $y$. In such case, $\text{Cov}(\langle \phi^y_{X}(\bm{X}), h_{bd}\rangle_{\mathcal{H}_{X}}|Y=y, M(\bm{X})|Y=y)=0$ holds for any non-trivial projectors by the property of covariance operator with linear transformation.

    \noindent \textbf{Case 2: $\text{Cov}(\phi_{X}(\bm{X})|Y=y,M(\bm{X})|Y=y) \neq 0$:} In this case, we first recall that for a real variable $\bm{X}$, a binary variable $M(\bm{X})$, and kernel space $\mathcal{H}_{X}$, under the assumption $\phi_{X}(\x_i)\neq \phi_{X}(\x_j)$ if $\x_i\neq \x_j$ and $\text{Cov}(\phi_{X}(\bm{X}),M(\bm{X})) \neq 0$,  \cite{wei2023mean} and \cite{zhu2024neural} say that there exists a subspace $\mathcal{H}_{sub}$ of $\mathcal{H}_{X}$, in which each function $h\in\mathcal{H}_{sub}$ satisfies $\text{Cov}(h(\bm{X}),M(\bm{X}))=0$. And a projection operator $P$ from $\mathcal{H}_{X}$ to $\mathcal{H}_{sub}$ can be constructed by the eigenfunctions of the operator $\Sigma_{XM(\bm{X})}\Sigma_{M(\bm{X})\bm{X}}$ where $\Sigma_{\bm{X}M(\bm{X})}$ is the covariance operator between $\phi_{X}(\x)$ and $\phi_{M}(M(\x))$ for  polynomial kernel $\phi_M$ with degree 2.

    Then, conditioning on $Y=y$, the above analysis can be extended to a subspace $h_{sub}^y$ with $\text{Cov}(h(\bm{X})|Y=y,M(\bm{X})|Y=y)=0$ for each $h\in h_{sub}^y$ and a projection operator $P_y$ can be constructed by the eigenfunctions of 
    $\Sigma_{\bm{X}M(\bm{X})|Y=y}\Sigma_{M(\bm{X})\bm{X}|Y=y}$ where $\Sigma_{\bm{X}M(\bm{X})|Y=y}$ is the class-conditional covariance operator. In such case, a non-zero class-conditional covariance between $\phi_{X}(\x)$ and $M(\x)$ dictates a specific, non-trivial projection operator $P_y$. Such projection operator can be applied to project the feature $\phi_{X}(\x)$ and eliminate the correlation between the prediction of $\x$ and its poisoning status.
\end{proof}

\subsection{Proof of Theorem 2}
% \begin{thm}
% Assume that $h_{bd}$ is a fully backdoored model, \ie, $h_{bd}(\vx_{\Delta})=T$ for all $\vx\in\mathcal{D}$. Furthermore, assume that $\|\vx_{\Delta}-\x\|_p\leq \rho$. Then, the following inequality holds:
% \begin{equation}
%     \mathcal{R}_{bd}(h)\leq \mathcal{R}_{cnpd}(h).
% \end{equation}
% \label{theorem2}
% \end{thm}

\begin{proof}
Assume that $h_{bd}$ is a fully backdoored model, \ie, $h_{bd}(\vx_{\Delta})=T$ for all $\vx\in\mathcal{D}$. Furthermore, assume that $\|\vx_{\Delta}-\x\|_p\leq \rho$. 

Then, we have 

\begin{align}
        & \mathcal{R}_{cnpd}(h) \\&=\frac{\sum\limits_{\x\sim\mathcal{D},t\in[1,C]}\max\limits_{\|\delta\|_{p}\leq\rho}\mathbb{I}(h_{bd}(\x_{\Delta})=t,t\neq y)\mathbb{I}(h(\x+\vdelta)=t)}{|\mathcal{D}|}\\
        &=\frac{\sum\limits_{\x\sim\mathcal{D}}\max\limits_{\|\delta\|_{p}\leq\rho}\mathbb{I}(h(\x+\vdelta)=T)}{|\mathcal{D}|}\\
        &\geq \frac{\sum\limits_{\x\sim\mathcal{D}}\mathbb{I}(h(\x_{\Delta})=T)}{|\mathcal{D}|}.
\end{align}
\end{proof}


\section{More implementation details\label{app_B}}
In this section, we provide additional implementation details, including an overview of the attacks compared, specifics of their implementation, an introduction to the defenses considered, and the implementation of the methods we propose. All experiments are conducted on one RTX 3090Ti GPU.


\subsection{Introduction of Attacks} 
Fig. \ref{visual_trigger_cifar} and \ref{visual_trigger_tiny}  illustrates visual representations of poisoned samples for various backdoor attacks, evaluated on the CIFAR-10 and Tiny ImageNet datasets. 
We follow BackdoorBench \cite{wubackdoorbench} to categorize the ten backdoor attacks according to various criteria as in Tab. \ref{app_table1}. We also follow the attack configuration as that in BackdoorBench.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/visual_cifar_allattacks.pdf}
    \caption{Visualization of poisoned samples under different backdoor attacks on CIFAR-10 dataset.}
    \label{visual_trigger_cifar}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/visual_tiny_allattacks.pdf}
    \caption{Visualization of poisoned samples under different backdoor attacks on Tiny ImageNet dataset.}
    \label{visual_trigger_tiny}
\end{figure}
\input{tables/app_table1}

Specifically, the details of these attacks are as follows:
\begin{itemize}
    \item \textbf{BadNets} \cite{gu2019badnets}: BadNets is a trigger-additive attack that inserts a fixed patch (a $3 \times 3$ white square for CIFAR-10 and GTSRB, and $6 \times 6$ for Tiny ImageNet in our paper, which is consistent with BackdoorBench.) into the image, and the corresponding label is altering to the target label.
    
    \item \textbf{Blended Backdoor Attack} (Blended) \cite{chen2017targeted}: The Blended attack involves merging a predefined image (Hello Kitty, in our work) with the original image using a blend coefficient $\alpha = 0.2$, as per the BackdoorBench standard.

    \item \textbf{Input-Aware Dynamic Backdoor Attack} (Input-Aware) \cite{nguyen2020input}: This attack is controllable during training. It first trains a trigger generator using adversarial methods. The generator then creates specific triggers for each sample during the model's training phase.

    \item \textbf{Low Frequency Attack} (LF) \cite{zeng2021rethinking}: The LF attack begins by learning a universal adversarial perturbation (UAP) while filtering out high-frequency components. The remaining low-frequency perturbation is then applied to clean samples to generate poisoned data.

    \item \textbf{Sample-Specific Backdoor Attack} (SSBA) \cite{li2021invisible}: SSBA utilizes an autoencoder, which is first trained. The autoencoder is then used to combine a trigger with the clean samples, producing poisoned images.

    \item \textbf{Trojan Backdoor Attack} (Trojan) \cite{Trojannn}: Similar to the LF attack, Trojan also generates a universal adversarial perturbation (UAP). This perturbation is then applied to clean samples to create poisoned ones.

    \item \textbf{Warping-Based Poisoned Networks} (WaNet) \cite{nguyen2021wanet}: WaNet employs a warping function that perturbs the clean samples to generate poisoned versions. During training, the adversary controls the process to ensure the model learns this specific warping transformation.
    \item \textbf{Trojaning attack the frequency domain} (Ftrojan) \cite{wang2022invisible}: Ftrojan introduces adversarial perturbations in the frequency domain, such that the poisoned image are visually imperceptible.
    \item \textbf{Adaptive Backdoor Poisoning Attack} (Adap-Blend) \cite{qi2023revisiting}: Adap-Blend designs an asymmetric trigger planting strategy that weakens triggers for data poisoning to diversify the latent representations of poisoned samples, and uses a stronger trigger during test time to improve attack success rate. 
\end{itemize}

\subsection{Introduction of Defenses} 
In this work, we compare our methods with ten SOTA backdoor defense methods. We conduct defense experiments following BackdoorBench. The detailed introduction of these methods are as follows:
\begin{itemize}
    \item \textbf{Fine-Pruning (FP)} \cite{liu2018fine}: FP first prunes neurons that remain dormant when processing benign samples. Then, it fine-tunes the pruned model to recover its utility and improve overall performance.
    
    \item \textbf{Neural Attention Distillation (NAD)} \cite{li2021neural}: NAD employs a knowledge distillation strategy that aims to distill clean knowledge from a potentially contaminated model using clean samples.
    \item \textbf{Neural Cleanse (NC)} \cite{wang2019neural}: NC detects backdoors by searching for a minimal Universal Adversarial Perturbation (UAP). Once a backdoor is detected, it purifies the model by unlearning the identified UAP.
    \item \textbf{Adversarial Neuron Pruning (ANP)} \cite{wu2021adversarial}: ANP is a pruning-based method. It observes that neurons associated with backdoors are more susceptible to adversarial perturbations and employs a minimax optimization strategy to identify and mask these compromised neurons.
    \item \textbf{Implicit Backdoor Adversarial Unlearning (i-BAU)} \cite{zeng2022adversarial}: i-BAU utilizes an implicit hyper-gradient method to optimize adversarial training, removing backdoor influences by unlearning adversarial perturbations.
    \item \textbf{Entropy-based pruning (EP)} \cite{zheng2022pre}: EP identifies significant differences in the moments of pre-activation distributions between benign and poisoned data in backdoor neurons in contrast to clean neurons, which is utilized for model pruning.
    \item \textbf{Fine-Tuning with Sharpness-Aware Minimization (FT-SAM) \cite{zhu2023enhancing}}: FT-SAM employs sharpness-aware minimization to fine-tune the poisoned model, improving its robustness by minimizing sharp loss regions.
    \item \textbf{Feature Shift Tuning (FST)} \cite{min2024towards}: FST encourages feature shifts by reinitializing the linear classifier and fine-tuning the model to counteract backdoor effects.
    \item \textbf{Shared Adversarial Unlearning (SAU)} \cite{wei2023shared}: SAU generates shared adversarial examples and then unlearns these examples to purify the model, targeting adversarial behavior.
\end{itemize}


\subsection{Implementation Details} 
In this work, starting from the concept of a neural polarizer, we propose NPD and CNPD with three instantiations: r-CNPD, e-CNPD, and a-CNPD. In practical experiments, the selection of their hyperparameters differs slightly based on their performance.

First, for the generation of adversarial examples, we use a unified $l_2$ norm for adversarial perturbation generation, with a perturbation bound of 3 for the CIFAR-10 and GTSRB datasets, and 6 for the Tiny ImageNet dataset. For each training batch, the inner loop uses PGD algorithm to generate adversarial examples, with 5 iterations and learning rate 0.1 for the inner loop.

Regarding our NPD, we use the same hyperparameters as in our Conference \cite{zhu2024neural}. Specifically, the training of neural polarizer consists of 50 epochs for all three datasets, with a warm-up period of 5 epochs. We use a learning rate of 0.01 with a weight decay of 0.0005 and a momentum of 0.9. The NP layer is inserted before the third convolution layer of the fourth layer for PreAct-ResNet18.

Next, we consider CNPD. Regarding the loss function, for the e-CNPD and a-CNPD methods, the hyperparameters $\lambda_1$, $\lambda_2$, and $\lambda_3$ are set to 1, 0.4, and 0.4 for the CIFAR-10 dataset, 1, 0.5, and 0.5 for the GTSRB dataset, and 1.0, 0.1, and 0.1 for the Tiny ImageNet dataset. For the r-CNPD method, the hyperparameters $\lambda_1$, $\lambda_2$, and $\lambda_3$ are set to 1, 0.1, and 0.1 for CIFAR-10, and 1, 0.2, and 0.1 for GTSRB.

Regarding the number of training epochs, for the a-CNPD method, we train for 10, 50, and 200 epochs on the CIFAR-10, GTSRB, and Tiny ImageNet datasets, respectively. For e-CNPD, we train for 100, 50, and 100 epochs. For r-CNPD, we consistently train for 200 epochs.

In terms of the insertion layer, we find that inserting PD into either the penultimate or last convolutional layer yields good defense results. In this paper, we uniformly choose to insert it into the last convolutional layer in the main experiments for CNPD.

For optimization, we use SGD with a learning rate of 0.01, weight decay of $5 \times 10^{-4}$, and momentum of 0.9. The learning rate is set to 0.001 for e-CNPD on Tiny ImageNet dataset. 

As for the architecture, the PD structure in the r-CNPD method is identical to that of NPD, consisting of a $1 \times 1$ convolutional layer followed by a batch normalization (BN) layer. The total number of such structures is equal to the number of classes.

For the e-CNPD method, let the output features from the previous layer be denoted as $m^l(\x) \in \mathbb{R}^{c \times h \times w}$, where $c$, $h$, and $w$ represent the number of channels, height, and width, respectively. The label embedding feature for each class is of dimension $h \times w$. Thus, the feature $m^l(\x)$ corresponding to class $y$ is concatenated with the embedding $e(y)$, resulting in a feature of dimension $(c+1) \times h \times w$, which is then passed through a Conv-BN layer, followed by a ReLU activation, and subsequently processed through another Conv-BN layer to produce an output of size $c \times h \times w$.

For the a-CNPD method, the parameters include a class-specific embedding vector $e(c) \in \mathbb{R}^{c \times h}$ for each class, and three learnable matrices: linear layers $\vtheta_Q \in \mathbb{R}^{h \times h}$, $\vtheta_K \in \mathbb{R}^{h \times h}$, and a $1 \times 1$ convolutional layer $\vtheta_V$. After performing the operations as described in Eq. (\ref{eq:anpd}), the resulting feature is passed through a final Conv-BN layer to obtain the filtered output.

\input{tables/table3_tiny}
\input{tables/table4_vgg}
\section{Defense results in comparison to SOTA defenses on more datasets and networks\label{app_C}}
To demonstrate the effectiveness of our defense methods across different datasets and networks, we conduct additional experiments on the Tiny ImageNet dataset (Tab. \ref{table3}) and on the VGG19-BN network with the CIFAR-10 dataset (Tab. \ref{table4}), comparing their performance against SOTA defenses. When evaluated on Tiny ImageNet (Tab. \ref{table3}), our methods consistently achieve remarkably low Attack Success Rates (ASRs) while maintaining competitive clean accuracy (ACC). For instance, under severe poisoning conditions (10\%), e-CNPD and a-CNPD effectively suppress ASR to near-zero for attacks like BadNets~\cite{gu2019badnets}, Trojan~\cite{Trojannn}, and SSBA~\cite{li2021invisible}. Meanwhile, our methods preserve accuracy at levels comparable to or even surpassing SOTA approaches, demonstrating that they can simultaneously ensure robustness and maintain the model’s utility. Similar trends are observed on VGG19-BN network (Tab. \ref{table4}), where e-CNPD and a-CNPD exhibit consistently low ASRs, indicating that our approaches generalize well across datasets and network architectures. Notably, the DER remains favorable, reflecting a balanced trade-off between backdoor mitigation and classification performance.

\vspace{0.5em}
\noindent \textbf{Effectiveness against state-of-the-art defenses.} Our proposed defenses outperform or match the leading defenses across multiple challenging backdoor attacks. Compared to prominent existing methods such as FP~\cite{liu2018fine}, NAD~\cite{li2021neural}, NC~\cite{wang2019neural}, ANP~\cite{wu2021adversarial}, and i-BAU~\cite{zeng2022adversarial}, our e-CNPD and a-CNPD consistently yield lower ASRs. More importantly, they do so without sacrificing clean accuracy significantly, which underscores their practical relevance. The comparison with defenses like FT-SAM~\cite{zhu2023enhancing}, FST~\cite{min2024towards}, and SAU~\cite{wei2023shared} further consolidates this point. While some existing methods may reduce ASR, they often incur substantial accuracy drops. In contrast, our methods navigate the delicate balance between robustness and accuracy more effectively. Over multiple attack strategies, including newer and more adaptive attacks (e.g., Adap-Blend~\cite{qi2023revisiting}), our defenses maintain superiority or close parity, indicating their resilience against evolving backdoor strategies.

\vspace{0.5em}
\noindent \textbf{Comparison among our four defenses.} Among the proposed approaches, e-CNPD and a-CNPD consistently emerge as the top-performing variants in terms of minimizing ASR while preserving accuracy across both Tiny ImageNet and CIFAR-10. NPD and r-CNPD, though effective, exhibit slightly higher ASRs in certain scenarios compared to e-CNPD and a-CNPD. Nevertheless, NPD and r-CNPD still surpass most baseline and SOTA defenses. E-CNPD generally provides a robust and balanced solution, maintaining low ASR with minimal impact on ACC. A-CNPD further refines this balance, often achieving the lowest ASRs and DER values observed, solidifying its role as a highly competitive defense. Thus, among the three, a-CNPD stands out as the most robust, though e-CNPD remains a strong contender, offering a complementary performance profile that may be preferable in certain application constraints.

In summary, the above analysis shows that our methods consistently achieve low attack success rates while maintaining competitive accuracy. Compared to state-of-the-art defenses, our methods outperform them, demonstrating robustness and utility across various attacks.







