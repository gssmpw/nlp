\begin{sidewaystable}
\vskip 0.15in
\caption{Important hyper-parameters of different algorithms in our experiments}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lr|cccccccccccc}
\toprule
\textbf{Policy Parameter} & & TRPO & TRPO-Lagrangian & TRPO-SL [18' Dalal]& TRPO-USL & TRPO-IPO & TRPO-FAC & CPO & PCPO &  SCPO & TRO-CVaR & CPO-CVaR & ASCPO \\
% \midrule
% Random & 100.38 $\pm$ 28.25 & 14.21 $\pm$ 11.20 & 0.18 $\pm$ 4.35 & 0.89 $\pm$ 10.96 & 49.57 $\pm$ 16.88 \\
\hline\\[-1.0em]
% \vskip 0.15in
Epochs & $N$ & 200 & 200 & 200 & 200 & 200 & 200 & 200 & 200 & 200 & 200 & 200 & 200\\
Steps per epoch & $B$& 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000 & 30000\\
Maximum length of trajectory & $L$ & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000\\
Policy network hidden layers & & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64)\\
Discount factor  &  $\gamma$ & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99\\
Advantage discount factor  & $\lambda$ & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97\\
TRPO backtracking steps & &100 &100 &100 &100 &100 &100 &100 & - &100 &100 &100 &100\\
TRPO backtracking coefficient & &0.8 &0.8 &0.8 &0.8 &0.8 &0.8 &0.8 & - &0.8 &0.8 &0.8 &0.8\\
Target KL & $\delta_{KL}$& 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 & 0.02\\


Value network hidden layers & & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64)\\
Value network iteration & & 80 & 80 & 80 & 80 & 80 & 80 & 80 & 80 & 80 & 80 & 80 & 80\\
Value network optimizer & & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam\\
Value learning rate & & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001\\

Cost network hidden layers & & - & (64, 64) & (64, 64) & (64, 64) & - & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64) & (64, 64)\\
Cost network iteration & & - & 80 & 80 & 80 & - & 80 & 80 & 80 & 80 & 80 & 80 & 80\\
Cost network optimizer & & - & Adam & Adam & Adam & - & Adam & Adam & Adam & Adam & Adam & Adam & Adam\\
Cost learning rate & & - & 0.001 & 0.001 & 0.001 & - & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001\\
Target Cost & $\delta_{c}$& - & 0.0 &  0.0 &  0.0 &  0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\

Lagrangian optimizer & & - & - & - & - & - & Adam & - & - & - & - & - & -\\
Lagrangian learning rate & & - & 0.005 & - & - & - & 0.0001 & - & - & - & - & - & -\\
USL correction iteration & & - & - & - & 20 & - & - & - & - & - & - & - & -\\
USL correction rate & & - & - & - & 0.05 & - & - & - & - & - & - & - & -\\
Warmup ratio & & - & - & 1/3 & 1/3 & - & - & - & - & - & - & - & -\\
IPO parameter  & ${t}$ & - & - & - & - & 0.01 & - & - & - & - & - & - & -\\
Cost reduction & & - & - & - & - & - & - & 0.0 & - & 0.0 & - & 0.0 & 0.0\\
Probability factor & k & - & - & - & - & - & - & - & - & - & - & - & 7.0\\
\bottomrule
\end{tabular}
}
\label{tab:policy_setting}
\end{center}
%\vskip -0.1in
\end{sidewaystable}