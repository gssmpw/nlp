% \newpage

% \section{ASCPO Notation Table}
% \label{append: ascpo notation}

\begin{table}[H]
    \centering
    \caption{Notation Table of ASCPO \textbf{I}}
    \begin{tabular}{p{2cm} p{13cm}}
        \toprule
        \textbf{Notation} & \textbf{Description} \\
        \midrule
        $\mathcal{S}$ & State space \\
        $\mathcal{A}$ & Action space \\
        $\mathcal{M}$ & Up-to-now maximum state-wise cost space: $\mathcal{M} \subset \mathbb{R}$\\
        $\hat{\mathcal{S}}$ & Augmented state space\\
        $\gamma$ & Discount factor: $ 0 \leq \gamma < 1$ \\
        $R$ & Reward function: $\mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$ \\
        $P$ & Transition probability function: $\mathcal{S} \times \mathcal{A} \times \mathcal{S} \mapsto \mathbb{R}$ \\
        $\mu$ & Initial state distribution: $\mathcal{S} \mapsto \mathbb{R}$ \\
        $C$ & Cost function: $\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$\\
        $D$ & Cost increment function: $(\mathcal{S}, \mathcal{M}) \times \mathcal{A} \times \mathcal{S} \mapsto [0, \mathbb{R}^+]$\\
        $\mathcal{P}(\mathcal{A})$ & Probability distribution over actions \\
        $\pi$ & Stationary policy: $\mathcal{S} \mapsto \mathcal{P}(\mathcal{A})$ \\
        $\Pi$ & Set of all stationary policies \\
        $t$ & Time step along the trajectory \\
        $m$ & Number of the constraints \\
        $i$ & Index of the constraints \\
        $k$ & Index of the policy update iterations \\
        $w$ & Maximum cost for constraints \\
        $s_t$  & State at step $t$:  $s\in\mathcal{S}$  \\
        $a_t$  & Action at step $t$:  $a\in\mathcal{A}$ \\
        $M_t$  & Up-to-now maximum state-wise cost at step $t$:  $M\in\mathcal{M}$ \\
        $\hat{s}_t$  & Augmented state at step $t$:  $\hat{s}\in\hat{\mathcal{S}}$  \\
        $\tau$  & Trajectory: a sequence of action and state  \\
        $H / h$  & Horizon of a trajectory  \\
        $\pi(a|s)$  & Probability of selecting action $a$ in state $s$ \\
        $\pi(\cdot | s)$ & Probability distribution of all action in state $s$\\
        $P(\cdot|s, a)$ & Probability distribution of all next state in state $s$ with action $a$\\
        $\mathcal{J}(\pi)$ & Expectation performance of policy $\pi$\\
        $\mathcal{D}_{\pi}(\hs_0)$ & Maximum state-wise cost performance sample of policy $\pi$ \\
        $R(\tau)$ & Discounted return of a trajectory with infinite horizon \\
        $V_\pi$ & Value function with infinite horizon of policy $\pi$\\
        $Q_\pi$ & Action-value function with infinite horizon of policy $\pi$\\
        $A_\pi$ & Advantage function with infinite horizon of policy $\pi$\\
        $V^H_\pi$ & Value function with $H$ horizon of policy $\pi$\\
        $Q^H_\pi$ & Action-value function with $H$ horizon of policy $\pi$\\
        $A^H_\pi$ & Advantage function with $H$ horizon of policy $\pi$\\
        
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Notation Table of ASCPO \textbf{II}}
    \begin{tabular}{p{2cm} p{13cm}}
        \toprule
        $V^H_{[D]\pi}$ & Value function of cost increment $D$ with $H$ horizon of policy $\pi$\\
        $Q^H_{[D]\pi}$ & Action-value function of cost increment $D$ with $H$ horizon of policy $\pi$\\
        $A^H_{[D]\pi}$ & Advantage function of cost increment $D$ with $H$ horizon of policy $\pi$\\
        $F_\pi$ & HJ reachability value functio\\
        $\mathcal{B}$ & Upper probability bound\\
        ${p}$ & Confidence of the probability bound\\
        $k$ & Probability factor: $k \geq 0$, $k \in \mathbb{R}$\\
        $\mathcal{V}_{min}$ & The minima of $\mathcal{D}_{\pi}(\hs_0)$\\
        $\psi$ & $\mathcal{V}_{min} \in \mathbb{R}^+$ \\
        $\mathcal{E}_{[D]}(\pi)$ & Expectation of the distribution of $\mathcal{D}_{\pi}(\hs_0)$\\
        $\mathcal{V}_{[D]}(\pi)$ & Variance of the distribution of $\mathcal{D}_{\pi}(\hs_0)$\\
        $\mathcal{B}_{[D]k}(\pi)$ & Upper probability bound of $\mathcal{D}_{\pi}(\hs_0)$ with confidence $p_k^\psi$\\
        $\mathcal{J}^l_{\pi, \pi_j}$ & Surrogate function for policy update to bound $\mathcal{J}(\pi)$ from below \\
        $\mathcal{E}^{u}_{[D]\pi, \pi_j}$ & Surrogate function for policy update to bound $\mathcal{E}_{[D]}(\pi)$ from above \\
        $\overline{MV}_{[D]\pi,\pi_j}$ & Upper bound of expected variance of the maximum state-wise cost\\
        $\overline{VM}_{[D]\pi,\pi_j}$ & Upper bound of the variance of the expected maximum state-wise cost \\
        $\mathcal{D}_{KL}(\pi \| \pi_j)[{\hat s}]$ & KL divergence between two policies $(\pi, \pi_j)$ at state $\hat s$\\
        $d_{\pi}$ & Discounted future state distribution of policy $\pi$ \\
        $\bar d_{\pi}$ & Non-discounted future state distribution of policy $\pi$ \\
        $\epsilon_{[D]}^{\pi}$ & Maximum expected advantage of policy $\pi$\\
        ${\mathcal{R}}_\pi(\hs_0)$ & Discounted return starts at state $s_0$ with infinite horizon of policy $\pi$\\
        $V_\pi(\hs_0)$ & Value of state $\hs_0$ with infinite horizon of policy $\pi$\\
        ${\mathcal{R}}_\pi^H\hs_0)$ & Discounted return starts at state $s_0$ with $H$ horizon of policy $\pi$\\
        $V_\pi^H(\hs_0)$ & Value of state $\hs_0$ with $H$ horizon of policy $\pi$\\
        $\mathcal{E}(\pi)$ & Expectation of the distribution of  ${\mathcal{R}}_\pi(\hs_0)$ of policy $\pi$\\
        $\mathcal{V}(\pi)$ & Variance of the distribution of  ${\mathcal{R}}_\pi(\hs_0)$ of policy $\pi$\\
        $\mathcal{B}_{k}(\pi, \gamma)$ & Upper probability bound of ${\mathcal{R}}_\pi(\hs_0)$\\
        $MV_\pi$ & \textbf{MeanVariance} of policy ${\pi}$\\
        $VM_\pi$ & \textbf{VarianceMean} of policy ${\pi}$\\
        $\omega_\pi^h(\hs)$ & Variance of the state-action value function ${Q^h_\pi}$ at state ${\hs}$ with $h$ horizon\\
        $\omega_{[D]\pi}^h(\hs)$ & Variance of the state-action value function $Q^h_{[D]\pi}$ at state ${\hs}$ with $h$ horizon\\
        $\bm \Omega_\pi^h$ & The vector of $\omega_\pi^h(\hs)$\\
        $\bm \Omega_{[D]\pi}^{h}$ & The vector of $\omega_{[D]\pi}^h(\hs)$\\
        $\xi$ & Action probability ratio\\
        
        \bottomrule
    \end{tabular}
\end{table}