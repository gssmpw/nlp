\section{Related Work}\label{sec::related}


% \yh{cite flowcam, flowmap, colmap-free gaussian; pose-free nvs: pf3splat, flowcam,flowmap, noposesplat, nerf--}
\noindent\textbf{Structure-from-Motion (SfM).} SfM techniques aim to estimate camera poses and reconstruct sparse 3D structures.
Conventional approaches~\citep{schonberger2016structure, snavely2006photo} employ multi-stage optimization, starting with pairwise feature matching~\citep{matas2004robust, bay2006surf, lowe2004distinctive} across views to establish correspondences, followed by camera pose optimization using incremental bundle adjustment~\citep{triggs2000bundle}.
Recently, numerous learning-based SfM methods have been proposed to enhance the traditional multi-stage pipeline. These improvements focus on three main areas: developing learning-based feature descriptors~\citep{detone2018superpoint, dusmanu2019d2, yi2016lift}, learning more accurate matching algorithms~\citep{sun2021loftr, sarlin2020superglue}, and implementing differentiable bundle adjustment~\citep{wang2023visual, lin2021barf}.
%
However, when input views are extremely sparse, accurately matching features becomes highly challenging, leading to degraded camera pose estimation performance.

\noindent\textbf{Multi-view Stereo (MVS).}
MVS techniques aim to reconstruct dense 3D geometry from multiple calibrated images. Traditional MVS methods~\citep{galliani2015massively, schonberger2016pixelwise, furukawa2015multi} typically follow a pipeline of depth map estimation, depth map fusion, and surface reconstruction. These approaches often rely on photometric consistency across views and various regularization techniques~\citep{yao2019recurrent} to handle challenging scenarios.
Recent years have witnessed a surge in learning-based MVS methods, leveraging deep neural networks to improve reconstruction quality and efficiency either with cascade cost volume matching~\citep{yao2018mvsnet, yao2019recurrent} or reconstruction supervision with differentiable rendering~\citep{chen2021mvsnerf, chen2024mvsplat, charatan2024pixelsplat, xu2024grm, hong2023lrm, li2023instant3d}.
Despite significant progress, MVS methods consistently depend on calibrated camera poses, which are typically estimated by SfM methods. This cascaded pipeline often causes MVS to perform suboptimally when the estimated poses are inaccurate.
DUSt3R~\citep{wang2024dust3r, leroy2024grounding} directly predicts the geometry of visible surfaces without any explicit knowledge of the camera parameters. However, under multi-view settings, their approach is limited to pairwise image processing followed by global alignment, failing to fully exploit multi-view information and supporting photorealistic rendering.


\noindent\textbf{3D Reconstruction from Sparse-view Images.}  Neural representations~\citep{occupancy,deepsdf,nerf,sitzmann2019scene,tewari2022advances} present a promising foundation for scene representation and neural rendering.
When applied to novel-view synthesis, these methods have demonstrated success in scenarios with dense-view training images, showcasing proficiency in single-scene overfitting.
Notably, recent advancements~\citep{pixelnerf, mvsnerf, sparseneus, ibrnet, visionnerf, dietnerf, chen2024mvsplat, zhang2024worldconsistentvideodiffusionexplicit} have extended these techniques to operate with a sparse set of views, displaying improved generalization to unseen scenes.
These methods face challenges in capturing multiple modes within large-scale datasets, resulting in a limitation to generate realistic results.
Additional works~\citep{xu2024grm, li2023instant3d, hong2023lrm, charatan2024pixelsplat} further scale up the model and datasets for better generalization with NeRF or Gaussian Splatting.
%
Unlike existing methods that rely on calibrated camera poses to supervise the neural network training, our approach can perform direct 3D reconstruction from uncalibrated images.

\noindent\textbf{Pose-free Novel-view Synthesis.}
%
Recent research has made significant progress in novel-view synthesis from uncalibrated images.
% 
One line of research focuses on jointly optimizing camera poses and radiance fields from dense-view images. BARF~\cite{lin2021barf}, NeRF\texttt{-}\texttt{-}~\cite{wang2021nerf}, and subsequent works~\cite{Jeong_2021_ICCV, bian2023nope, truong2023sparf} have advanced this approach.
%
Several recent methods~\cite{Fu_2024_CVPR, keetha2024splatam, fan2024instantsplat} have extended the 3D representation from NeRF to 3D Gaussians.
%
Another research direction~\cite{Sajjadi2022RUSTLN, kani24upfusion, xu2025sparp, jiang2024forge} focuses on developing feed-forward novel-view synthesis for unposed images. 
% 
SRT~\cite{srt22} proposes the first pose- and geometry-free framework for novel view synthesis, while LEAP~\cite{jiang2022LEAP} pioneers  pose-free radiance field reconstruction by directly estimating scene geometry and radiance fields. 
% 
FlowCam~\cite{smith2023flowcam} and FlowMap~\cite{smith2024flowmap} introduce 2D flow to enable unsupervised learning of generalizable 3D reconstruction, though their performance degrades in sparse-view settings.
%
PF-LRM~\cite{wang2023pf} estimates camera poses by predicting point maps and solving a differentiable perspective-n-point (PnP) problem, but shows limited generalization to complex 3D scenes.
%
PF3plat~\cite{hong2024pf3plat} achieves coarse alignment of 3D Gaussians by leveraging pre-trained models for monocular depth estimation and visual correspondence.
%
Splatt3R~\cite{smart2024splatt3r} and NopoSplat~\cite{ye2024no} utilize DUSt3R~\cite{wang2024dust3r} or MASt3R~\cite{leroy2024grounding} to predict point maps as proxy geometry and subsequently learn 3D Gaussians for sparse-view reconstruction.
%
However, existing approaches are either restricted to two-view scenarios or produce suboptimal rendering results due to imperfect geometry estimates from DUSt3R or MASt3R.
%
Our work presents a differentiable system that simultaneously predicts camera parameters, geometry, and appearance, achieving superior generalization across diverse real-world scenes.

% Recent efforts have addressed the novel-view synthesis from uncalibrated images.
% %
% One research line of the jointly optimizatze camera pose and radiance fileds,  BARF~\cite{lin2021barf}, NeRF\texttt{-}\texttt{-}~\cite{wang2021nerf} and others~\cite{Jeong_2021_ICCV, bian2023nope, truong2023sparf} expanded upon this foundation.
% %
% A set of methods~\cite{Fu_2024_CVPR, keetha2024splatam,fan2024instantsplat} further extend the 3D representation from NeRF to 3D Gaussians.
% %
% The other research aims to build a feed-forward 3D reconstructor from unposed images.
% %
% FlowCam~\cite{}, FlowMap~\cite{} introduce 2D flow to unsueprvised learn a generalizable 3D reconstructor while can't work well on sparse-view setting.
% %
% PF-LRM~\citep{wang2023pf} predicts a point map followed by a differentiable perspective-n-point problem (PnP) to estimate poses but fails to generalize well on 3D scenes.
% %
% PF3plat~\cite{hong2024pf3plat} leverage pre-trained monocular depth estimation  and visual correspondence  models to achieve a coarse alignment of 3D Gaussians.
% %
% Splatt3R~\citep{smart2024splatt3r}, NoposeSplat~\cite{ye2024no} leverages DUSt3R~\citep{wang2024dust3r} or MASt3R~\cite{leroy2024grounding} to predict point maps as proxy geometry and further learn 3D Gaussians for sparse-view reconstruction.
% %
% However, these works are either limited to two-view scenarios, or produce degraded rendering results due to imperfect geometry from DUSt3R or MASt3R.
% %
% Our work introduces a differentiable system that jointly optimizes camera parameters, geometry, and appearance, demonstrating better generalization ability on diverse in-the-wild scene datasets.