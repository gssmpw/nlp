\section{Introduction}\label{sec::intro}

% 1. What are we doing? Why is it important?
% \begin{itemize}
%     \item Reconstructing 3D scenes from images is a fundamental problem with applications spanning computer vision, perception, and computer graphics.
%     \item Conventional approaches typically divide the reconstruction process into two stages: first, using an SfM solver to calibrate poses, and then leveraging MVS to estimate depth for dense 3D reconstruction.
%     \item Recently, with the advancement of neural scene representation, many methods have started using differentiable rendering instead of MVS for reconstruction. However, these methods still heavily rely on poses generated by SfM solvers.
% \end{itemize}
% __________________________________

Reconstructing 3D scenes from multi-view images is a fundamental problem with wide-ranging applications across computer vision, perception, and computer graphics. 
Traditional approaches typically solve this problem in two stages by first estimating camera parameters using Structure from Motion (SfM) solvers~\citep{hartley2003multiple, schonberger2016structure, snavely2006photo} and then  predicting dense depth maps by Multi-View Stereo (MVS) to achieve dense 3D reconstruction~\citep{seitz2006comparison, schonberger2016pixelwise}. 
% 
Despite the significant success of the SfM-MVS paradigm over the past decades, it faces three key limitations. First, these methods rely heavily on handcrafted feature matching and are non-differentiable, preventing them from fully leveraging recent advancements in deep learning. Second, traditional SfM struggles with sparse views and limited viewpoints, significantly restricting its applicability in real-world scenarios. 
%
% Third, these approaches typically register and process frames sequentially, leading to potential error accumulation and be time-consuming, particularly when handling a large number of input frames.
%



% 2. What are the problems with current practice?
% \begin{itemize}
%     \item SfM has two key limitations: first, it is not differentiable with respect to its output variables and heavily depends on handcrafted feature matching. Second, it struggles to handle sparse views and generally requires dense frame inputs, often failing with sparse views.
%     \item Current methods: Dust3r/Flowmap proposes learning a two-view point map with a scalable training paradigm, but it requires a post-optimization process to obtain a multi-view map and camera poses with global registration. Additionally, it cannot perform high-quality rendering. ()
%     \item PFLRM proposes a feed-forward reconstruction system that jointly infers camera poses and object shapes. However, it fails in large-scale scenes due to the underlying 3D representation and the limitations of the training datasets.
%     \item BARF, NeRF--: optimization process, no prior, very slow.
% \end{itemize}


Recent efforts to tackle these issues have shown potential, but significant challenges remain.
Deep camera estimation methods~\citep{sinha2023sparsepose, wang2023posediffusion, lin2023relposepp, zhang2024cameras} treat sparse-view SfM as a camera parameter regression problem, yet struggle with accuracy and generalization. VGGSfM~\citep{wang2024vggsfm} improves this by incorporating multi-view tracking and differentiable bundle adjustment, but falls short in providing dense geometry.
While optimization-based approaches like BARF~\citep{lin2021barf} and NeRF\texttt{-}\texttt{-}\citep{wang2021nerf} jointly optimize camera pose and geometry, they suffer from slow processing and poor generalization to novel scenes. 
These methods often require dense captures to establish the correspondence for feature matching to estimate the camera, which is often inapplicable in more in-the-wild cases, specifically in a very sparse-view setting.
Recent works like DUSt3R\citep{wang2024dust3r} and MASt3R~\citep{} propose generating a two-view point map with pixel-wise geometry, but their reliance on post-optimization global registration is time-consuming and often yields suboptimal results. PFLRM~\citep{wang2023pf} offers feed-forward reconstruction from four images, but its tri-plane representation limits performance in large-scale scenes.
 While these methods have demonstrated promising advances in sparse-view settings, they still lack a comprehensive solution that combines scalability, accuracy, and efficiency in both geometry and camera estimation.
% Recent efforts to tackle these issues have shown potential, but significant challenges remain.
% %
% For example, deep camera estimation methods~\citep{sinha2023sparsepose, wang2023posediffusion, lin2023relposepp, zhang2024cameras} treat sparse-view SfM as a camera parameter regression problem. However, these methods often struggle with achieving accurate pose estimation and generalization across diverse scenarios.
% %
% To address this problem, VGGSfM~\citep{wang2024vggsfm} incorporates multi-view tracking into deep camera estimation and employs differentiable bundle adjustment for joint optimization, although it falls short in providing dense geometry.
% %
% Optimization-based approaches like BARF~\citep{lin2021barf} and NeRF\texttt{-}\texttt{-}\citep{wang2021nerf} offer alternatives by jointly optimizing camera pose and geometry from a set of images of a specific scene, but suffer from slow processing times and a lack of prior knowledge to generalize on novel scenes.
% %
% These methods often require dense captures to establish the correspondence for feature matching to estimate the camera, which is often inapplicable in more in-the-wild cases, specifically in a very sparse-view setting.
% %
% To overcome this significant challenge, DUSt3R\citep{wang2024dust3r} and MASt3R~\citep{} introduce a scalable learning paradigm for generating a two-view point map with pixel-wise geometry. However, it requires a post-optimization process via global registration to derive a multi-view map and camera poses, which is both time-consuming and prone to yielding suboptimal camera poses.
% %
% PFLRM~\citep{wang2023pf} introduces a feed-forward reconstruction system that jointly infers camera poses and object shapes only from four images, but fails in large-scale scenes due to limitations in tri-plane representation.
% %
% While these methods have demonstrated promising advances in sparse-view settings, they still lack a comprehensive solution that combines scalability, accuracy, and efficiency in both geometry and camera estimation.



% Despite their widespread use, SfM approaches face two key limitations. Firstly, they are not differentiable with respect to their output variables and depend heavily on handcrafted feature matching. Secondly, SfM methods struggle to handle sparse views, generally requiring dense frame inputs and often failing with limited viewpoints. Recent attempts to address these issues have shown promise but still face significant challenges. For instance, Dust3r proposes learning a two-view point map with a scalable training paradigm, but it requires a post-optimization process to obtain a multi-view map and camera poses with global registration, and cannot perform high-quality rendering. PFLRM introduces a feed-forward reconstruction system that jointly infers camera poses and object shapes, but fails in large-scale scenes due to limitations in its underlying 3D representation and training datasets. Optimization-based approaches like BARF and NeRF-- offer alternatives but suffer from slow processing times and a lack of prior knowledge. \jay{Let me think if we need to mention the trials in the area of deep sfm, as both the two claims here are somehow addressed by recent deep SfM research. }


% Drawback of Traditional: low-textured, specular and reflective regions, "hand-crafted similarity metrics and engineered regularizations (e.g., normalized cross correlation and semi-global matching [12]) "


% 3. How does our solution work? What is the difference compared to current practice?
% \begin{itemize}
%     \item In this paper, we present \textbf{ScaleGRM}, a high-quality and differentiable system for camera and geometry estimation from sparse-view images.
%     \item ScaleGRM introduces a novel cascade learning paradigm to progressively reconstruct camera poses and geometry.
%     \item The key insight is that jointly optimizing camera, geometry, and appearance from input images often presents significant learning difficulties.
%     \item Therefore, we first build a small network to perform a coarse estimation of camera poses and geometry for pretraining.
%     \item Then, we use the noisy camera poses and geometry from the pretraining stage as conditions and further build a transformer architecture to infer appearance and refine pose and geometry. This is done with reconstruction loss from differentiable rendering using Gaussian splatting.
%     \item The noisy camera poses provide strong geometric cues that guide the transformer architecture to implicitly utilize the correspondences between input views, enabling faster convergence and better reconstruction results.
%     \item Our model is trained on large-scale data and learns a general scene reconstruction and camera prior, enabling it to generalize to unseen data and estimate both camera parameters and geometry within 0.5 seconds.
%     \item As shown in Fig xxx, our system can robustly reconstruct accurate poses and realistic 3D scenes using just four sparse input images. It also supports incremental reconstruction in large-scale scenes.
% \end{itemize}


In this paper, we present \method, a novel feed-forward and differentiable system that represents scenes using 3D Gaussians, enabling high-quality geometry, appearance and camera parameter estimation from uncalibrated sparse-view images. Specifically, we represent scene geometry efficiently using 3D pointmaps formed by Gaussian centers. A primary challenge in this task is that joint optimization of these parameters presents significant learning difficulties, frequently converging to sub-optimal solutions with distorted geometry and blurry textures. We thus introduce a novel cascade learning paradigm to progressively estimate geometry, appearance and camera poses, relaxing traditional requirements for 3D reconstruction such as dense image views, accurate camera poses and wide baselines.

Our first key innovation is that decomposing this complex joint optimization problem into sequential stages substantially improves the accuracy of predicted geometry and camera poses. In our framework, we first employ a lightweight network that operates on low-resolution images to obtain initial coarse estimates of camera poses and 3D pointmaps. The refinement stage then leverages a transformer-based architecture to incorporate these preliminary results as crucial geometric priors, further refining both the 3D pointmaps and camera poses. The key innovation of the refinement is our camera-centric representation for geometry regression: instead of directly predicting global 3D pointmaps, we estimate 3D points in individual camera coordinates and introduce a neural global alignment module to unify them into a coherent global representation. This enables fast convergence of geometry learning and reduces geometry distortion for source views far from the reference view.



% 
%

%




% 4. What is the impact?
% \begin{itemize}
%     \item We trained our model on the xxxxx dataset and achieved state-of-the-art  results in both pose estimation and depth estimation \jay{not sure how good our depth estimation can be, because it is still an open problem to convert gaussian to depth/normal?}. 
%     \item Our model also enables photorealistic novel view synthesis using Gaussian splatting and significantly improves speed compared to previous optimization-based methods.
%     \item
% \end{itemize}


We trained our model on a set of  large public datasets~\citep{},  achieving state-of-the-art results in  camera pose and geometry estimation. \method also enables photorealistic novel view synthesis using Gaussian splatting and significantly improves the reconstruction speed within 0.5 seconds compared to previous optimization-based methods. 
As demonstrated in Figure X, our system can robustly reconstruct accurate poses and realistic 3D scenes using just four sparse input images, and supports incremental reconstruction in large-scale scenes.

% Our approach not only advances the field of 3D reconstruction but also opens up new possibilities for applications such as efficient view synthesis and video sequence localization from unknown poses.


% 5. Summarize the contribution.
% \begin{itemize}
%     \item We build a powerful transformer architecture and learn a strong scene reconstruction and camera prior from vast-scale data sources, allowing for applications such as view synthesis and the localization of video sequences from unknown poses less than 0.5s.
%     % \item 
%     % \item We propose a new generalizable system that can jointly estimate camera poses and geometry from sparse-view inputs in less than 0.5 seconds.
%     \item We analyze the learning difficulties associated with unposed images and propose a novel cascade learning paradigm to progressively refine camera poses and 3D geometry to high quality.
%     \item We introduce a novel training strategy that uses differentiable triangulation to estimate scene scale, allowing the use of data without depth annotations for model training.
% \end{itemize}
The primary contributions of this work are as follows:
\begin{itemize}
\item We propose an efficient, feed-forward and differentiable system for high-quality 3D Gaussian scene reconstruction from uncalibrated sparse-view images, achieving inference in less than 0.5 seconds.
\item We introduce a novel cascade learning paradigm that decomposes the challenging joint optimization of geometry, appearance, and camera parameters into progressive stages. This coarse-to-fine approach effectively leverages geometric priors from initial estimates to guide subsequent refinement, addressing the inherent difficulties in joint optimization.
\item We design a camera-centric representation with a neural global alignment module that efficiently unifies multi-view geometry into global pointmaps, improving both reconstruction quality and speed over traditional optimization-based global alignment methods.
\end{itemize}

Through these innovations, \method represents a significant step forward in the field of 3D scene reconstruction, offering a fast, accurate, and generalizable solution to the challenges of sparse-view reconstruction with uncalibrated camera poses.



