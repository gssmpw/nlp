\section{Method}\label{sec::method}

\input{figure/pipeline/pipeline}

% \gw{the following paragraph could be significantly shortened.}


% \method is a feed-forward 3D reconstruction framework that leverages point-based geometry representation to infer high-quality camera poses, dense per-pixel point maps, and 3D Gaussians for photorealistic novel view synthesis from uncalibrated sparse views. 
\method uses point maps~\cite{wang2024dust3r} as our geometry representation for two key advantages: their compatibility with neural networks and natural integration with 3D Gaussians for appearance modeling.
%
As shown in \cref{fig:pipeline}, \method is  feed-forward model to infer high-quality camera poses and 3D geometry from uncalibrated sparse-view images.
%
Our solution is a cascaded learning paradigm that first estimates camera poses from sparse views and then leverages the estimates to guide the subsequent geometry and appearance learning.
%
% In the following sections, we detail our neural pose predictor (\cref{method:pose}), two-stage geometry learning (\cref{method:geometry}), appearance modeling (\cref{method:apperance}), and training objectives (\cref{method:training}). 
We present our neural pose predictor for sparse-view pose estimation in \cref{method:pose}.
With the pose estimates, we propose a two-stage learning paradigm for geometry estimation (\cref{method:geometry}). Given the estimated geometry, we develop a 3D Gaussian reconstruction head that enables high-quality appearance modeling for photorealistic novel view synthesis (\cref{method:apperance}). Finally, we detail the training objectives for the whole framework (\cref{method:training}). 
% The  pipeline is illustrated in \cref{fig:pipeline}.


% \method is a feed-forward 3D reconstruction framework to infer high-quality camera poses, dense per-pixel point maps, and 3D Gaussians for photorealistic novel view synthesis from uncalibrated sparse views.
% %
% Considering that joint optimization of camera poses, dense geometry and appearance from sparse-view images is a challenging problem, we propose to leverage camera poses as an intermediate representation bridging 2D images and 3D geometry and to construct a neural camera pose predictor to provide good initialization for geometry learning.


% We first demonstrate the neural pose predictor to estimate a coarse camera poses from sparse-view inputs (\cref{method:pose}). 
% %
% Based on the coarse estimate pose, we propose a cascade cascade learning paradigm for geometry estimation (\cref{method:geometry}). 
% %
% Built upon the well-learned geometry, we develop a 3D Gaussian reconstruction head for photorealistic novel view synthesis (\cref{method:apperance}). 
% %
% We then detail the training objectives for the whole framework (\cref{method:training}).
% %
% The overall pipeline is illustrated in \cref{fig:pipeline}.


% Inspired by prior works, we also use 3D point maps as the geometry representation, which forms a one-to-one mapping between image pixels and
% 3D scene points.  We use it for two reasons: 1) This representation is compatible with the neural network. 2) The point-based representation can be intergreated with 3D Gaussians for apperance modeling, which we discuss it in \cref{method:apperance}.


% \subsection{Pose as Proxy for Sparse-view Reconstruction}
% \label{method:proxy}

% \gw{this subsection is redundant. all you're saying in half a page is that you first estimate coarse poses, which then guide the estimation. that was already said multiple times in the abstract and earlier in this section. Eqs. 1 and 2 are also redundant. I would omit this entire subsection. the paradigm of first estimating poses is also not new - all MVS methods use the same idea. so try not to sell it as something super new.}

% Our goal is to achieve accurate 3D reconstruction including pose $\mathcal{C}$, geometry $\mathcal{G}$ and appearance $\mathcal{A}$ estimation from uncalibrated sparse views $\mathcal{I}$, which remains a challenging problem in computer vision. Traditional SfM-MVS pipelines often struggle with sparse-view scenarios, as SfM frequently fails to estimate accurate camera poses from limited viewpoints. Recent learning-based methods shown in \cref{eq:pose2geo} attempt to directly regress 3D representations from 2D images and build a complex post-optimization process to obtain camera poses. However, reliance solely on image-based learning makes the geometry learning highly ill-posed, leading to increased optimization difficulty and inadequate surface reconstruction quality. 
% \begin{equation}
% \begin{aligned}
%     \mathcal{I} &\rightarrow \mathcal{G} \rightarrow \mathcal{C} \\
%     \mathcal{I} &\rightarrow \mathcal{G}, \mathcal{C}
% \end{aligned}
%  \label{eq:pose2geo}
% \end{equation}

% To address these limitations, our key insight is to leverage camera poses as an intermediate representation bridging 2D images and 3D geometry, as shown in \cref{eq:pose2geo-ours}, and to construct a neural camera pose predcitor to provide good initialization for geometry learning. Camera poses naturally represent the physical viewpoints where images are captured, and the corresponding view frustums efficiently define the spatial coverage of each image. Moreover, with camera poses as the intermediate representation, the network can implicitly establish correspondences across views and better reason about 3D geometry, leading to more accurate surface reconstruction. 



% \begin{equation}
% \begin{tikzcd}[column sep=1.0em, arrows={line width=0.45pt}]
% \mathcal{I} \arrow[r] \arrow[rr, bend right] & \mathcal{C} \arrow[r] & \mathcal{G}
% \end{tikzcd}
% \label{eq:pose2geo-ours}
% \end{equation}



\subsection{Neural Pose Predictor}
\label{method:pose}
Traditional pose estimation methods rely on feature matching to find correspondences, but this often fails in sparse-view scenarios where images have limited overlapping regions. 
%
Inspired by previous deep camera estimation methods~\cite{wang2023posediffusion, wang2024vggsfm}, we drop the feature matching and formulate pose estimation as a direct transformation problem from image space to camera space using an end-to-end transformer model. Given the input images $\mathcal{I} = \{\mathbf{I}_i\}_{i=1}^{N}$,  we first tokenize them into non-overlapping patches to obtain image tokens. We then initialize learnable camera latents $\mathcal{Q}_{c} = \{\mathbf{q}^{\text{coarse}}_i\}_{i=1}^{N}$. By concatenating the image patches with camera latents into a 1D sequence, we leverage a small decoder-only transformer, named ``neural pose predcitor'' $\mathrm{F}_p(\cdot)$ to estimate the coarse camera poses $\mathcal{P}_{c} = \{\mathbf{p}^{\text{coarse}}_i\}_{i=1}^{N}$:
\begin{equation}
\mathcal{P}_{c} = \mathrm{F}_p(\mathcal{Q}_{c}, \mathcal{I}).
\end{equation}
We parametrize each camera pose as a 7-dimensional vector comprising the absolute translation and a normalized quaternion. 
With this neural pose predictor, we can estimate coarse camera poses as initialization for subsequent geometry and appearance learning. 
%
We observe that the estimated poses do not need to be very accurate---only approximating the ground truth distribution is enough. This aligns with our key insight: camera poses, even imperfect, provide essential geometric priors and spatial initialization, which significantly reduces the complexity for geometry and appearance reconstruction. 

\subsection{Multi-view Geometry Estimation}\label{method:geometry}
% Previous methods attempt to directly regress global geometry from input images~\cite{wang2024dust3r, leroy2024grounding}, which poses a significant challenge as the network needs to reason about camera viewpoints and global geometric structures simultaneously.
With our estimated camera poses serving as an effective intermediate representation, we propose a two-stage geometry learning approach to improve reconstruction quality. 
Our key idea is to first learn camera-centric geometry in local frames (camera coordinate system) and then build a neural scene projector to transform it into a global world coordinate system with the guidance of estimated poses. 

\noindent\textbf{Camera-centric Geometry Estimation.} 
Learning geometry in camera space aligns with the image formation process, as each view directly observes local geometry from its perspective. This also simplifies the geometry learning by focusing on local structures visible in each view, rather than reasoning about complex global spatial relationships.
%
We tokenize images into image tokens and concatenate them with camera tokens derived from coarse pose estimates $\mathcal{P}_c$. These tokens are then fed into a transformer architecture $\mathrm{F}_l(\cdot)$ to estimate local point tokens $\mathcal{T}_l = \{\mathbf{T}^{local}_{i}\}_{i=1}^{N}$.
%
The self-attention mechanism in the transformer can perform association across different views and exploit the geometric cues from cameras.
%
The local point tokens are subsequently fed into a DPT-based~\cite{ranftl2021vision} decoder $\mathrm{D}_l(\cdot)$ for spatial upsampling to obtain dense point maps $\mathcal{G}_l = \{\mathbf{G}^{\text{local}}_{i}\}_{i=1}^{N}$ and confidence map $\mathcal{C}_l = \{\mathbf{C}^{\text{local}}_{i}\}_{i=1}^{N}$ in local camera space. Meanwhile, we further refine the initial camera poses by introducing additional learnable pose tokens $\mathcal{Q}_{f}$ into the network,  which output refined pose estimates $\mathcal{P}_f$ alongside the geometry prediction:
\begin{align}
\mathcal{T}_{l}, \mathcal{P}_{f} &= \mathrm{F}_l(\mathcal{I}, \mathcal{P}_{c}, \mathcal{Q}_{f}) \\
\mathcal{G}_{l}, \mathcal{C}_{l} &= \mathrm{D}_l(\mathcal{T}_{l}).
\end{align}
% \yh{Add confidence map}
We can process an arbitrary number of images, as long as the GPU memory does not overflow. For a detailed explanation, please refer to the \supp.
We find this multi-task learning scheme can boost each task performance by providing complementary supervision signals between pose refinement and geometry estimation, as observed in previous work~\cite{wang2023pf}. To handle potentially inaccurate pose estimates during inference, we introduce a simple yet effective pose augmentation strategy during training. Specifically, we randomly perturb the predicted camera poses by adding Gaussian noise, which allows the network to learn to adapt noisy estimated poses at inference time.

% To handle potentially inaccurate pose estimates during inference, we introduce a simple yet effective pose augmentation strategy. We randomly perturb the predicted poses by adding Gaussian noise at a certain probability, which enhances the network's robustness to pose variations at inference stage. 
% \jay{As far as I can recall, we use (pred cameras+noise) rather than (gt cameras+noise) here. May need shangzhan to double confirm.  }  \zsz{yes, we use pred cameras+noise}

\noindent\textbf{Global Geometry Projection.}
We aim to transform camera-centric geometry predictions into a consistent global geometry using refined camera poses. However, this transformation is challenging since imperfect pose estimates make direct geometric reprojection unreliable.
%
Rather than using geometric transformation, we propose a learnable geometry projector $\mathrm{F}_g(\cdot)$ that transforms local geometry $\mathcal{G}_l$ into global space, conditioned on the estimated poses $\mathcal{P}_f$. This learned approach is more robust to pose inaccuracies compared to direct geometric projection.
%
For computational efficiency, we utilize the local point tokens $\mathcal{T}_l$ rather than the dense camera geometry $\mathcal{G}_l$ as the input:
\begin{align}
    \mathcal{T}_g &= \mathrm{F}_g(\mathcal{T}_l, \mathcal{P}_f) \\
    \mathcal{G}_{g}, \mathcal{C}_{g} &= \mathrm{D}_g(\mathcal{T}_{g}).
\end{align}
% \yh{Add confidence map}
where $\mathrm{D}_g(\cdot)$, $\mathcal{G}_g$ and $\mathcal{C}_g$  denotes the DPT-based upsampling decoder, global point maps and corresponding confidence map.
%
This geometry projector $\mathrm{F}_g(\cdot)$ is also implemented with a transformer architecture, which is the same as the $\mathrm{F}_l(\cdot)$ but takes different input.


% With our two-stage geometry learning scheme, we demonstrate advantages for geometry estimation in sparse-view settings, particularly when dealing with image pairs having limited overlap regions, as shown in our experiments.

% \input{figure/alignment/alignment}

% \subsection{}

% \textbf{Coarse-to-fine Pose Estimation}

% \textbf{Pose-conditioned Geometry Learning}
% We use a transformer-based network to
% estimate the initial camera poses $\hat{\mathcal{P}}$.
% \begin{equation}
%     \mathcal{P}_c = \mathcal{T}_c(\mathcal{F}_c),
% \end{equation}
% where $\mathcal{P}_c$ represents the estimated camera pose, $\mathcal{T}_c$ denotes the transformer-based network, and $\mathcal{F}_c$ represents the input low-resolution image tokens.
% Our camera poses $\mathcal{P}_c$ are defined in the camera coordinate system of the first input image.

% Then, we feed the coarse poses $\hat{\mathcal{P}}$ and full-resolution image tokens $\mathcal{F}_f$ into our refinement transformer $\mathcal{T}_f$, which outputs the refined camera poses $\mathcal{P}$.
% \begin{equation}
%     \mathcal{P}, \mathcal{X}_c = \mathcal{T}_f(\mathcal{P}_c, \mathcal{F}_f),
% \end{equation} where $\mathcal{X}_c$ represents the camera-centric geometry, which is described in the next section.

% \subsection{Geometry Estimation} \label{subsec::refinement}

% intrinsic first--> extrinsic later.



% Our geometry estimation network consists of two parts: camera-centric geometry estimation and camera pose refinement.

% \paragraph{Camera-Centric Geometry Estimation}\label{subsec::geometry}

% We first estimate the camera-centric point tokens $\mathcal{F}_p$ using the coarse camera poses $\mathcal{P}_c$ and full-resolution image tokens $\mathcal{F}_f$. Then, we input these point tokens $\mathcal{F}_p$ into a DPT-based decoder to obtain dense point maps $\mathcal{X}_c$.
% Our point maps $\mathcal{X}_c$ is defined in the camera coordinate system of each input image, which reduces geometry distortion and speeds up geometry learning convergence.

% \paragraph{Neural Global Alignment}\label{subsec::alignment}
% We use another transformer-based network $\mathcal{T}_g$ to condition on the camera poses $\mathcal{P}$ and align the camera-centric point maps $\mathcal{X}_c$ into a global coordinate system to obtain the final point maps $\mathcal{X}$. Here, global coordinate system refers to the camera coordinate system of the first input image.

% \begin{equation}
%     \mathcal{X}, \mathcal{F}_g = \mathcal{T}_g(\mathcal{X}_c, \mathcal{P}),
% \end{equation}
% where $\mathcal{F}_g$ represents dense gaussian feature maps introduced in the next section.

\subsection{3D Gaussians for Appearance Modeling}\label{method:apperance}

Based on the learned 3D point maps, we initialize 3D Gaussians by using the point maps as the centers of 3D Gaussians. We then build a Gaussian regression head to predict other Gaussian parameters including opacity $\mathcal{O} = \{\mathbf{o}_i\}_{i=1}^N$, rotation $\mathcal{R} = \{\mathbf{r}_i\}_{i=1}^N$, scale $\mathcal{S} = \{\mathbf{s}_i\}_{i=1}^N$ and spherical harmonics coefficient $\mathcal{SH} = \{\mathbf{sh}_i\}_{i=1}^N$ for appearance modeling. Specifically, to efficiently model appearance, we utilize a pretrained VGG network to extract features from input images $\mathcal{V} = \{\mathbf{v}_i\}_{i=1}^N$ and build another DPT head on top of the $\mathrm{F}_g(\cdot)$ to obtain an appearance feature $\mathcal{A}$. This appearance feature is then fused with VGG features and fed into a shallow CNN decoder $\mathrm{F}_a(\cdot)$ for Gaussian parameter regression.
% \begin{align}
%    (\mathcal{O}, \mathcal{R}, \mathcal{S}, \mathcal{SH}) = \mathrm{F}_a(\mathcal{V}, \mathcal{A})
% \end{align}
% \zsz{The solution to address the scaling problem is not precise. I will revise this part after completing all the experiments.}

% Considering that the estimated geometry may have scale inconsistency with ground truth geometry,  directly rendering images with annotated camera poses would lead to misalignment.  To address this issue, we follow DUSt3R to normalize both estimated and ground truth geometry 
% into a unified coordinate space.  Specifically, we compute average scale factors $s = \mathrm{avg}(\mathcal{G}_g)$ from our  predicted global point maps and $s_{gt} = \mathrm{avg}(\mathcal{G}_{gt})$ from ground-truth point maps  respectively, and normalize both scenes to unit space during rendering.  The scale parameter $\mathcal{S}$ of Gaussians and the translation vector of novel-view camera pose $\mathbf{p}'$ should also be normalized for rendering.
To address scale inconsistency between estimated and ground truth geometry, we normalize both into a unified coordinate space. We compute average scale factors from predicted $s = \mathrm{avg}(\mathcal{G}_g)$ and ground truth $s_{gt} = \mathrm{avg}(\mathcal{G}_{gt})$ point maps, normalizing scenes to unit space during rendering. The Gaussian scale parameter $\mathcal{S}$ and novel-view camera position $\mathbf{p}'$ are also normalized.
%
We use the differentiable Gaussian rasterizer $\mathrm{R}(\cdot)$ to render images with the normalized 3D Gaussians:
\begin{align}
   \mathbf{I}_{\mathbf{p}'} = \mathrm{R}(\{\mathcal{G}_g/s, \mathcal{O}, \mathcal{R}, \mathcal{S}/s, \mathcal{SH}\}, \mathbf{p}'/s_{gt}),
\end{align}
where $\mathbf{I}_{\mathbf{p}'}$ is the rendered image. The entire rendering process is differentiable, enabling end-to-end optimization of the Gaussian regression head through reconstruction loss.




% Considering that different scenes exhibit large variations in spatial scale, directly regressing Gaussian scale parameters becomes challenging due to the large solution space. To address this issue, we compute an average scale factor $s = \mathrm{avg}(\mathcal{P}_g)$ from our predicted global point maps and normalize the scene to unit space during rendering. Specifically, given the novel-view camera pose $\mathbf{t}$ whose translation is normalized by the ground truth point maps, we use the differentiable Gaussian rasterizer $\mathrm{R}(\cdot)$ to obtain the final images on the normalized 3D Gaussians:
% \begin{align}
%    \mathbf{I}_t = \mathrm{R}(\{\frac{\mathcal{P}_g}{s}, \mathcal{O}, \mathcal{R}, \frac{\mathcal{S}}{s}, \mathcal{SH}\}, \mathbf{t}),
% \end{align}
% where $\mathbf{I}_t$ is the rendered image from learned 3D Gaussians.
% Due to the differentiability of the entire rendering framework, we can leverage reconstruction loss to optimize the Gaussian regression head.





% For 3D Gaussian splatting, we directly use the per-pixel global point maps $\mathcal{X}$ as our Gaussian centers, $\{\mu_i\}_{i=1}^N$. Additionally, we incorporate a shallow CNN encoder to extract per-pixel image features, $\mathcal{F}_s$ and $\mathcal{F}_g$, and pass them through a shallow CNN decoder to estimate the remaining Gaussian parameters: opacity $\{\alpha_i\}_{i=1}^N$, covariance $\{\Sigma_i\}_{i=1}^N$, and color information $\{c_i\}_{i=1}^N$. With these predicted 3D Gaussians, we are able to render novel view images through the differentiable Gaussian splatting rendering.


\subsection{Training Loss}\label{method:training}
Our model is a joint learning framework and trained with a multi-task loss function comprising three components: camera pose loss, geometry loss, and Gaussian splatting loss. 
The camera pose loss is defined as the combined sum of rotation and translation losses following the pose loss used in VGGSFM:
\begin{align}
     \mathcal{L}_{\text{pose}} &= \sum_{i=1}^{N} \ell_{\text{huber}}(\mathbf{p}_i^{\text{coarse}}, \mathbf{p}_i) + \ell_{\text{huber}}(\mathbf{p}_i^{\text{fine}}, \mathbf{p}_i),
\end{align}
where $\mathbf{p}_i$ is the ground-truth camera pose of $i$-th image and $\ell_{\text{huber}}$ is the Huber-loss between the parametrization of poses.



% \zsz{add view centric loss}
The geometry loss includes a confidence-aware 3D regression term similar to that in DUSt3R:
\begin{align}
        \mathcal{L}_{\text{geo}} &= \sum_{i=1}^{N} \sum_{j\in\mathcal{D}^i} \mathbf{C}_{i, j}^\text{camera}\ell_{\text{regr}}^{\text{camera}}(j,i) - \alpha\log \mathbf{C}_{i, j}^\text{camera} \\ &+ \sum_{i=1}^{N} \sum_{j\in\mathcal{D}^i} \mathbf{C}_{i, j}^\text{global}\ell_{\text{regr}}^{\text{global}}(j,i) - \alpha\log \mathbf{C}_{i, j}^\text{global},
\end{align}
where $\mathcal{D}^i$ denotes the valid pixel grids, $\mathbf{C}_{i, j}^\text{local}$ and $\mathbf{C}_{i, j}^\text{global}$ denote the confidence scores of pixel $j$ of $i$-th image in local and global maps. $\ell_{\text{regr}}^{\text{local}}(j,i)$ and $\ell_{\text{regr}}^{\text{global}}(j,i)$ denote the Euclidean distances of pixel $j$ of $i$-th image between the normalized predicted point maps and ground-truth point maps in camera and global coordinate frames. 


The Gaussian splatting loss is computed as the sum of the $L_2$ loss and the VGG perceptual loss $L_\text{perp}$ between the rendered $\mathbf{I}_{\mathbf{p}'}$ and ground truth $\hat{\mathbf{I}}_{\mathbf{p}'}$ images. Additionally, we include a depth loss to supervise  rendered depth maps $\mathbf{D}_{\mathbf{p}'}$ with the prediction $\hat{\mathbf{D}_{\mathbf{p}'}}$ from the monocular depth estimator~\cite{depthanything}:
\begin{align}
    \mathcal{L}_{\text{splat}} &= \sum_{\mathbf{p}' \in \mathcal{P}' }\|\hat{\mathbf{I}}_{\mathbf{p}'} - \mathbf{I}_{\mathbf{p}'}\| + 0.5L_\text{perp}(\hat{\mathbf{I}}_{\mathbf{p}'}, \mathbf{I}_{\mathbf{p}'}) \\
    &+ 0.1 \|(\mathbf{W}\hat{\mathbf{D}_{\mathbf{p}'}} + \mathbf{Q}) - \mathbf{D}_{\mathbf{p}'}\|, 
\end{align}
where $\mathbf{W}$ and $\mathbf{Q}$ are the scale and shift used to align  $\hat{\mathbf{D}_{\mathbf{p}'}}$ and $\mathbf{D}_{\mathbf{p}'}$, $\mathcal{P}'$ is the novel-view camera poses.

The total loss is represented as:
\begin{align}
    \mathcal{L}_{\text{total}} &= \lambda_{\text{pose}} \mathcal{L}_{\text{pose}} + \lambda_{\text{geo}} \mathcal{L}_{\text{geo}} + \lambda_{\text{splat}} \mathcal{L}_{\text{splat}},
\end{align}
where $\lambda_{\text{pose}}$,  $\lambda_{\text{geo}}$ and $\lambda_{\text{splat}}$ denotes the loss weight for corresponding loss.
% Joint training enables the point maps and Gaussian splatting to achieve optimal results. For detailed definitions of loss function, please refer to the supplementary materials.
% \zsz{add
% pose loss: vggsfm
% geometry: dust3r
% splatting: vgg, l2, monsdf
% }




