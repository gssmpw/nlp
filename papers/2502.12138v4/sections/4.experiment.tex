\section{Experiment}\label{sec::exp}

% \noindent\textbf{Datasets.}  
\paragraph{Datasets.}
Following DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024grounding}, we train our model on a mixture of the following public datasets: MegaDepth~\cite{li2018megadepth}, ARKitScenes~\cite{baruch2021arkitscenes}, Blended MVS~\cite{yao2020blendedmvs}, ScanNet++~\cite{yeshwanth2023scannet++}, CO3D-v2~\cite{Reizenstein2021CommonOI}, Waymo~\cite{Sun_2020_CVPR}, WildRGBD~\cite{xia2024rgbd}, and DL3DV~\cite{Ling_2024_CVPR}. These datasets feature diverse types of scenes.

% \noindent\textbf{Implementation details.} 
\input{tab/Re10K_pose}
\input{tab/eth3d}

\paragraph{Implementation details.} 
Our model is trained from scratch using 8 views as input, without any pre-trained models, except for the encoder.
The neural pose predictor $\mathrm{F}_{p}(\cdot)$ consists of 12 transformer blocks with channel width 768.
%
In cascade geometry estimation, our camera-centric geometry estimator $\mathrm{F}_{l}(\cdot)$  and global geometry projector $\mathrm{F}_{g}(\cdot)$ both use 12 transformer blocks with channel width 768.
%, and the DPT-head consists of 4 convolutional blocks  to achieve 4$\times$ upsampling.
%
We use the Adam~\cite{kingma2014adam} optimizer with an initial learning rate of $1\times10^{-4}$, gradually decreasing to $1\times10^{-5}$. We train our model on 64 NVIDIA A800 GPUs for 200 epochs with an input resolution of $512\times384$. The training takes approximately 14 days to complete. 
% 
% We first train the geometry estimation network and camera pose estimator without the Gaussian regression head for approximately 7 days using a batch size of 8 per GPU. Subsequently, we introduce the Gaussian regression head while keeping the coarse camera pose estimator frozen, training for an additional 7 days with a reduced batch size of 4 per GPU. 
%
% The loss weights $\lambda_{\text{pose}}$, $\lambda_{\text{geo}}$, and $\lambda_{\text{splat}}$ are set to 5, 1, and 4 by default.
More implementation details are presented in the \supp.

\paragraph{Inference.}
Our model, although trained with 8 views, generalizes well to scenarios ranging from as few as 2 views to as many as 25 views. We give a comprehensive study between performance and view numbers in \supp. 
% To enable our model to handle different views, we employ two camera latent codes. One for the reference image and a shared code for all source images. The source token is replicated N-1 times, enabling the model to process an arbitrary number of input images.

\subsection{Multi-view Pose Estimation}
\noindent\textbf{Dataset.}
Following PoseDiffusion~\cite{wang2023posediffusion}, we evaluate the camera pose estimates on the RealEstate10K dataset~\cite{zhou2018stereo}. We apply our method directly to the RealEstate10K dataset without fine-tuning, using 5 images as input following previous protocol. 

\noindent\textbf{Metrics.}
We evaluate pose accuracy on the RealEstate10K dataset~\cite{zhou2018stereo} using three metrics~\cite{jin2021image, wang2023posediffusion}: AUC, RRA, and RTA.
%
The AUC metric computes the area under the accuracy curve across different angular thresholds, where accuracy is determined by comparing the angular differences between predicted and ground-truth camera poses.
%
RRA (Relative Rotation Accuracy) and RTA (Relative Translation Accuracy) measure the angular differences in rotation and translation respectively. The final accuracy at a threshold $\tau$ is determined by the minimum of RRA@$\tau$ and RTA@$\tau$.


\noindent\textbf{Baseline.}
For camera pose estimation, we compare our method with recent deep optimization-based methods, including DUSt3R~\cite{wang2024dust3r}, MASt3R~\cite{leroy2024grounding}, VGGSfM~\cite{wang2024vggsfm} and RelPose~\cite{zhang2022relpose}. We also include traditional SfM methods such as COLMAP~\cite{schoenberger2016sfm} and PixSfM~\cite{schonberger2016pixelwise}, as well as feed-forward method PoseDiffusion~\cite{wang2023posediffusion}.
%
% Note that among all baselines, PoseDiffusion is trained on the RealEstate10K dataset while others are not.

\noindent\textbf{Comparison.} We show quantitative comparison in \cref{tab:pose}.
Conventional SfM methods like COLMAP and PixSfM rely on time-consuming bundle adjustment, resulting in slow inference speed for camera pose estimation. These optimization-based methods also struggle with sparse views where feature correspondences are difficult to establish.
%
DUSt3R and MASt3R employ global alignment optimization, which not only leads to slow inference but also limits their performance due to their two-view geometry learning paradigm that cannot effectively leverage multi-view associations.
%
Compared to other feed-forward approaches, our method achieves superior performance, thanks to the coarse-to-fine pose estimation strategy and multi-task learning with geometry.


\subsection{Sparse-view 3D Reconstruction}
\noindent\textbf{Dataset.}
For sparse-view geometry reconstruction, we construct a comprehensive benchmark on the ETH3D~\cite{schops2017multi}, DTU~\cite{jensen2014large}, and TUM~\cite{sturm12iros_ws} datasets, which feature diverse scenes including objects, indoor scenes, and outdoor scenes.
%
% We randomly sample 8 images with small overlaps from each scene and evaluate both point cloud quality and camera pose accuracy on these datasets. \xn{Do we need to have some discussion here fore sparse-view reconstruction?}

\noindent\textbf{Metrics and Baselines.}
We use the accuracy and completion metrics~\cite{jensen2014large} to assess point cloud quality. We compare our method against recent state-of-the-art methods like DUSt3R~\cite{wang2024dust3r}, MASt3R~\cite{leroy2024grounding}, and Spann3R~\cite{wang2024spann3r}.
%
We also include conventional SfM methods like COLMAP~\cite{schoenberger2016sfm} for comparison.
% \zsz{For COLMAP and MVSNet, some case failed and some case succeeded on this  dataset.}
\input{figure/geometry/geometry}


\noindent\textbf{Comparison.}
As shown in \cref{tab:recon}, our method achieves higher performance than optimization-based methods like DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024grounding}, as well as recent feed-forward methods such as Spann3R~\cite{wang2024spann3r}.
%
Compared to DUSt3R and MASt3R, our method not only achieves better reconstruction quality but also offers faster inference, as our feed-forward reconstructor directly leverages multi-view information, avoiding their two-stage pipeline of two-view geometry estimation followed by global alignment post-processing.
%
Our method also outperforms Spann3R by leveraging camera poses as geometric priors in our two-stage geometry learning paradigm, rather than directly regressing point maps through neural networks.
%
We show qualitative comparisons in \cref{fig:reconstruction}. Our model achieves better geometry reconstruction with less noise compared to other baselines.


% For sparse reconstruction, our method achieves higher performance than optimization-based methods like DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024grounding}, as well as recent feed-forward methods, such as Spann3R~\cite{wang2024spann3r}.
% \zsz{Our method can capture more information from additional views, so it performs better under sparse views compared to two-view scenarios. Meanwhile, SPANN3R, due to directly using network-generated point clouds, struggles in challenging cases, resulting in poor performance.}

\subsection{Novel-view Synthesis}
\noindent\textbf{Dataset.} 
We evaluate rendering quality on two datasets: the RealEstate10K~\cite{zhou2018stereo}  and DL3DV~\cite{Ling_2024_CVPR}. For RealEstate10K, a widely used benchmark for novel view synthesis, we fine-tune our network using two-view images following the NopoSplat protocol~\cite{ye2024no}, adding intrinsic as a condition. Since we focus on sparse views, our RealEstate10K tests only the results from the NopoSplat test split, specifically in the overlapping regions with low and medium overlap.
For DL3DV, we sample 8 views for each testing video sequence as the input, and another 9 views as the groundtruth for evaluation. The sampling interval is randomly chosen, selected between 8 and 24. A total of 100 scenes are used as the test set.


\noindent\textbf{Metrics and Baselines.}
To evaluate rendering quality in the RealEstate10K~\cite{zhou2018stereo} and DL3DV~\cite{Ling_2024_CVPR} datasets, we employ PSNR, SSIM, and LPIPS metrics.
To evaluate rendering quality, we compare with pose-free methods, including CoPoNeRF~\cite{hong2023unifying}, Splatt3R, as well as pose-required methods, such as MVSplat~\cite{chen2024mvsplat} and PixelSplat~\cite{Charatan_2024_CVPR}.
We evaluate both MVSplat and pixelSplat using two-view inputs, selecting the two closest input views relative to the target view. Although MVSplat supports multiple input views, we find its performance degrades with additional views, as demonstrated in \supp. 
% \input{figure/rebuttal/table_views}
\input{figure/rendering/rendering}

\input{tab/DL3DV}


\paragraph{Comparison.}
\cref{tab:nvs-dl3dv} and \cref{tab:nvs-realestate10k} report the quantitative evaluation results for novel-view synthesis on DL3DV and RealEstate10K, respectively. 
Our approach substantially outperforms pose-free methods like CoPoNeRF~\cite{hong2023unifying} and Splatt3R~\cite{smart2024splatt3r}, even better than the pose-required methods, including MVSplat~\cite{chen2024mvsplat} and PixelSplat~\cite{Charatan_2024_CVPR} that take camera poses provided by the dataset to achieve high-fidelity rendering. Our model doesn't require camera extrinsic information, which makes our method more applicable in real-world settings. 
\cref{fig:rendering-comparison} qualitatively illustrates that our pose-free method obtains higher rendering quality than the compared baselines.

\input{tab/Re10K}
\subsection{Ablation Study}
We conduct an ablation study to evaluate the effectiveness of each component in our method. For this study, we select the BlendedMVS~\cite{yao2020blendedmvs} dataset.
We randomly split the dataset of 95\% scenes in BlendedMVS dataset as the training set and keep the rest 5\% for testing.
Our ablations exclude rendering loss to focus on geometric results with various design choices.
``\textit{w/o} pose'' means our reconstructor is only conditioned on multiview images without inputting predicted camera poses. ``\textit{w/o} camera-centric'' means that we directly use a transformer with the same parameter size to output the global geometry without predicting camera-centric point maps. ``\textit{w/o} joint training'' means first training the camera predictor separately and then fixing it while training the reconstructor. 
"\textit{w/o} DPT head" denotes our ablation where we substitute the DPT decoder with a shallow MLP for point-map regression. "\textit{w/} rendering loss" refers to our configuration that incorporates  rendering loss during training to evaluate its impact on geometric accuracy. 

As demonstrated in \cref{tab:ablation-study}, using camera poses as proxies substantially improves geometry learning. Our two-stage approach with camera-centric geometry enhances performance, and the multi-task learning paradigm adds further improvements. DPT head plays a crucial role by explicitly accounting for spatial relationships during upsampling, whereas the shallow MLP lacks this consideration. The rendering loss has both positive and negative effects. While its dense supervision enhances COMP by supervising regions without ground truth point clouds, it slightly reduces ACC due to its lower accuracy compared to actual ground truth data.

\iffalse
\noindent\textbf{Impact of camera pose conditioning.}
We assess the impact of camera pose conditioning in the geometry transformer. To do this, we compare our method with a variant that omits camera pose conditioning. The results indicate that incorporating camera pose conditioning significantly enhances the performance of the geometry transformer.

\noindent\textbf{Effectiveness of cascade geometry estimation.}
To demonstrate the effectiveness of cascade geometry estimation, we replace our camera-centric geometry estimation and global geometry projection with a single global geometry estimation. Specifically, we use a 24-layer transformer to perform the global geometry estimation. We observe that the cascade geometry estimation improves the quality of the reconstructed point cloud.


% \input{tab/dtu}
\noindent\textbf{Importance of joint training of pose and geometry.}
To highlight the importance of joint training of pose and geometry, we train the pose estimation network and the geometry estimation network separately. In this setup, we condition the geometry transformer on ground truth camera poses perturbed by random Gaussian noise. Our observations show that joint training improves the accuracy of geometry estimation.
\fi
    
\input{tab/ablation}
