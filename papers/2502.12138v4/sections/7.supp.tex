\newpage 
\appendix

\section*{Appendix}

In this supplementary material, we include: (1) extended implementation details covering data processing and training strategies, (2) additional experimental results, and (3) a comprehensive description of the network architecture.


\section{Implementation Details}

\paragraph{Data Processing.} 
% 
We follow the processing protocol of DUSt3R to generate point maps for most datasets. 
% 
However, the DL3DV dataset only provides the annotation for  camera parameters.  
% 
To include DL3DV into our training framework, we use the multi-view stereo algorithm from COLMAP to annotate per-frame depth maps, which are then converted into point maps. 
% 
% We further leverage multi-view photometric consistency to filter out noisy depth~\cite{yan2020dense}.
Additionally, we utilize multi-view photometric and geometric consistency to eliminate noisy depth~\cite{yan2020dense}.
% We utilize COLMAP to recover point maps for the DL3DV dataset, followed by applying the multiview fusion algorithm~\cite{yan2020dense} to refine and clean the noisy point maps. For other datasets, we adhere to the DUSt3R dataset processing protocol to generate the point maps.
% \paragraph{Training Strategies.}
% Initially, the neural pose predictor and the multi-view geometry estimation network are jointly trained for approximately $14$ days on $32$ NVIDIA A100 GPUs.
% 
% We initially train the neural pose predictor and the multi-view geometry estimation network together for approximately four days on 32 NVIDIA A100 GPUs. 
% 
% After this period, both the pose predictor and the camera-centric geometry estimation network are frozen. 
% \jay{Double check here, "multi-view geometry estimation network" and "camera-centric geometry estimation network".}
% 
% Following this phase, we freeze both the pose predictor and the camera-centric geometry estimation network. 
% 
% Subsequently, we train the appearance estimation network and the global geometry projector for three days, also using 32 NVIDIA A100 GPUs. 
% The appearance estimation network and the global geometry projector are then trained for additional $3$ days with the same GPU setup. 
% 
For the datasets captured as video sequences, we randomly select $8$ images from a single video clip, with each video clip containing no more than 250 frames. 
% 
For multi-view image datasets, we randomly select $8$ images per scene. 
% 
% The whole training takes approximately $7$ days.
% \jay{Do you want to highlight you did not care about the overlap between frames for the multi-view image datasets? yes}


\paragraph{Baselines for Novel View Synthesis.}
We compare our novel view synthesis results with MVSplat~\cite{chen2024mvsplat}, pixelSplat~\cite{charatan2024pixelsplat}, and CoPoNeRF~\cite{hong2023unifying} on the DL3DV dataset~\cite{Ling_2024_CVPR}.
% 
% However, these methods were only trained by $2$ views and perform not well under our sparse-view setting, \ie, $8$ views. 
% 
% For fair comparison, we selected two source views closest to the target rendering view as the input for these baselines (\eg, MVSplat). 
However, these methods were originally trained on only $2$ views and perform not well under our sparse-view setting of $8$ views. To ensure a fair comparison, we selected the two source views closest to the target rendering view as inputs for these baselines (e.g., MVSplat). 
% 
We found that selecting two closest source views significantly improved their rendering quality compared to using all $8$ views directly.
% 
% For example, it improved the rendering quality of \todo{Method A} from \todo{XXX} to \todo{YYY}. \jay{shangzhan to fill this}

% this can significantly improved their rendering quality compared to directly feeding $8$ views to them.

% approach significantly improved the rendering quality.

% Our main 
% We compared our method with MVSplat~\cite{chen2024mvsplat}, pixelSplat~\cite{charatan2024pixelsplat}, and CoPoNeRF~\cite{hong2023unifying} on the DL3DV dataset~\cite{Ling_2024_CVPR} under the 8-view setting. 
% However, MVSplat, PixelSplat, and CoPoNeRF were not trained with 8 views. 
% To address this issue, we selected the two source views closest to the rendering view as the network input, achieving the best results. 
% We found that this approach significantly improved the rendering quality.


\paragraph{Numbers of Input Image.}
We have two camera latents: one for the first image (reference), and one is shared by all other images (source). The source token is duplicated N-1 times. Therefore, the model can process any number of input images.



\section{Experiments}
\input{tab/ab}
\paragraph{Relationship between MVSplat Performance and Input Views.}
We evaluated MVSplat with two views because its
performance degrades with additional input frames, as shown in~\cref{view_number}, as
demonstrated in the figure above. We therefore reported
its optimal results.



\input{tab/num_results}

\paragraph{Study between Performance and the Number of Frames.}
We analyzed the impact of varying the number of frames on pose and point map estimation using the DTU dataset.  
For this experiment, we randomly selected 2, 6, 10, 16, and 25 source views while fixing two query views for testing pose accuracy and for evaluating surface accuracy. Under the 2-view setting, our method generates a reasonable shape, but its precision remains limited. The results demonstrate that increasing the number of views leads to improvements in both pose and surface accuracy. However, these improvements gradually plateau as the number of views continues to grow, as shown in \cref{tab:ab_num}.



\input{figure/geometry/DTU}

\paragraph{Dense View 3D Reconstruction on the DTU dataset.}
% 
We present the results for dense view 3D reconstruction on the DTU dataset in \cref{dtu_dense}, although dense reconstruction is not our primary objective.
% 
% our primary objective is not.  
% 
As observed, our method achieves better results compared to DUSt3R but falls short of MASt3R. 
% 
This is expected since our approach is not tailored for dense reconstruction, whereas MASt3R is specifically optimized for it through the training of matching heads.  
% . It falls short of MASt3R, which is however expected, since our approach is not tailored for dense reconstruction, whereas MASt3R is specifically optimized for it through the training of matching heads. 


\paragraph{Visualization of Sparse-view Reconstruction.} We present the visualizations of our sparse-view reconstruction results on the DTU dataset in \cref{fig:depth}. 



% Visualizations of our reconstruction results on the DTU dataset are shown in \cref{fig:depth}. 




% \input{tab/IMC}
\input{tab/dtu}

% \input{tab/neural_pose_predictor}

% \input{tab/camera_centric}
% \input{tab/global_geometry_projector}
% \section{Network Architecture}
% We employ the CroCo encoder~\cite{croco_v2} as our pretrained encoder model, keeping its weights frozen during training. 
% %  
% The architecture of our neural pose predictor, trained from scratch, is detailed in \cref{tab:neural_pose_predictor}. Similarly, our camera-centric geometry estimation network is trained from scratch, and its architecture is also outlined in \cref{tab:camera_geometry_estimation}. Additionally, the architecture of our global geometry projector, trained from scratch, is presented in \cref{tab:global_projector}. 
% % 
% % The appearance model takes dense features from both VGG and DPT head outputs as input.
% The appearance model takes as input the dense features extracted from the VGG and DPT heads. 
% % \jay{I am confused by the original sentence here. Check if my changed version is correct.}
% % 
% These features are fused using a two-layer U-Net, which combines the information from both sources. 
% % 
% The fused features are then processed through separate convolutional layers to predict the final outputs, such as color, opacity, and high-frequency spherical harmonics coefficients.


