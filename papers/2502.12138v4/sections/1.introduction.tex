\section{Introduction}\label{sec::intro}
Reconstructing 3D scenes from multi-view images is a fundamental problem with wide-ranging applications across computer vision, perception, and computer graphics. 
Traditional approaches typically solve this problem in two stages: first, estimating camera parameters using Structure-from-Motion (SfM) solvers~\citep{hartley2003multiple, schonberger2016structure, snavely2006photo}, and then predicting dense depth maps by Multi-View Stereo (MVS) to achieve dense 3D reconstruction~\citep{seitz2006comparison, schonberger2016pixelwise}. 
% 
Despite the significant success of the SfM-MVS paradigm over the past decades, it faces two key limitations. First, these methods rely heavily on handcrafted feature matching and are non-differentiable, preventing them from fully leveraging recent advancements in deep learning. Second, traditional SfM approaches struggle with sparse views and limited viewpoints, significantly restricting their applicability in real-world scenarios. 


Recent efforts to tackle these issues have shown potential, but significant challenges remain.  Optimization-based approaches like BARF~\citep{lin2021barf} and NeRF\texttt{-}\texttt{-}\citep{wang2021nerf} jointly optimize camera poses and geometry, but they require a good initialization and suffer from poor generalization to novel scenes. 
% 
Deep camera estimation methods~\citep{sinha2023sparsepose, wang2023posediffusion, lin2023relposepp, zhang2024cameras, Rockwell2024, Rockwell2022} treat sparse-view SfM as a camera parameter regression problem, yet struggle with accuracy and generalization. VGGSfM~\citep{wang2024vggsfm} improves this by incorporating multi-view tracking and differentiable bundle adjustment but falls short in providing dense geometry.
% 
DUSt3R\citep{wang2024dust3r} and MASt3R~\citep{leroy2024grounding} propose generating a two-view point map with pixel-wise geometry, but their reliance on post-optimization global registration is time consuming and often yields suboptimal results. PF-LRM~\citep{wang2023pf} offers feed-forward reconstruction from four images, but its tri-plane representation~\cite{chan2022efficient} limits performance in large-scale scenes.
While these methods have demonstrated promising advances in sparse-view settings, they still lack a comprehensive solution that combines scalability, accuracy, and efficiency in camera, geometry, and appearance estimation.
     
% Specifically, we represent scene geometry efficiently using 3D point maps formed by Gaussian centers. 


We present \method, a novel feed-forward and differentiable system that infers high-quality geometry, appearance, and camera parameters from uncalibrated sparse-view images. 
Direct optimization of these parameters from images often presents significant learning difficulties, frequently converging to sub-optimal solutions with distorted geometry and blurry textures. To address these challenges, we introduce a novel cascade learning paradigm that progressively estimates camera poses, geometry, and appearance, relaxing traditional requirements for 3D reconstruction such as dense image views, accurate camera poses, and wide baselines.
%
Our cascade learning paradigm lies in decomposing the challenging  optimization problem into sequential stages, using camera poses as proxies for each stage. 
% 
The central concept is that a camera pose frames a 2D image within a 3D observation frustum, reducing the learning complexity for subsequent tasks. 
% 
Our method starts with a neural pose predictor that estimates coarse camera poses from sparse-view images. These initial poses provide geometric cues that facilitate a transformer-based architecture to refine the poses, compute point maps, and predict 3D Gaussians for novel-view synthesis. 
% 
For geometry prediction, we introduce a two-stage approach instead of direct global prediction. First, we estimate camera-centric point maps in individual camera coordinates. Then, a neural scene projector unifies these local point maps into a coherent global structure. With this approach, we enable faster convergence in geometry learning and reduce geometric distortion for challenging scenes.


We trained our model on a set of large public datasets~\cite{li2018megadepth, baruch2021arkitscenes, yao2020blendedmvs, yeshwanth2023scannet++, Reizenstein2021CommonOI, Sun_2020_CVPR, xia2024rgbd, Ling_2024_CVPR} . \method achieves state-of-the-art results in camera pose estimation, point cloud estimation, and novel-view synthesis. With unposed images as input, \method can produce photorealistic novel-view synthesis using Gaussian Splatting in just 0.5 seconds, which is a substantial improvement over previous optimization-based methods. 
As demonstrated in  \cref{fig:teaser}, our system reconstructs 3D scenes and estimates poses from as few as 2-8 input images.

The primary contributions of this work are as follows:
\begin{itemize}[leftmargin=1.5em]
\item We propose an efficient, feed-forward, and differentiable system for high-quality 3D Gaussian scene reconstruction from uncalibrated sparse-view images, achieving inference in less than 0.5 seconds.

\item We demonstrate that leveraging camera poses as proxies effectively simplifies complex 3D learning tasks. We thus introduce a novel cascaded learning paradigm that starts with camera pose estimation, whose results condition the subsequent learning of geometric structure and appearance. 

\item We propose a two-stage geometry learning approach that first learns camera-centric point maps and builds a global geometry
projector to unify the point maps into a global coordinate.

\end{itemize}


