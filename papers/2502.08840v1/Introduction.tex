\section{Introduction}

Graphs and hypergraphs are fundamental structures in diverse fields such as computer science, mathematics, social science, and biology, supporting a wide range of theoretical and applied research areas. 
Hypergraphs generalize graphs, with hyperedges consisting of subsets of the vertices.
Because interactions between entities often occur in groups, such as people dining together or items added to an online shopping cart, many phenomena are best captured using hypergraphs. At the same time, the vast majority of graph algorithms are designed for simple graphs, where edges constitute a \emph{pairwise} relationship.

Given a hypergraph $H$, one can construct a graph $G$ by including an edge between each pair of vertices that appear in some hyperedge in $H$. This corresponds to placing in $G$ a clique on the vertices appearing in each hyperedge of $H$. We say that $G$ is the \emph{projection} of $H$. 

Projecting hypergraphs onto graphs and leveraging graph algorithms is a common strategy for solving problems on hypergraphs. 
This approach has been pursued especially in the domain of community detection within the hypergraph stochastic block model, where algorithms aim to reconstruct communities from similarity matrices, a form of pairwise hypergraph projection
\cite{kim2018stochastic,cole2020exact,gaudio2023community}.
% In community detection for hypergraph stochastic block model, many algorithms were designed to recover the community from similarity matrix , which is a pairwise projection of the original hypergraph. 
Similar methodologies also exist in hypergraph matching, where the optimal soft matching can be obtained by considering pairwise interactions \cite{zass2008probabilistic}. 
More generally in graph data processing, projection of hypergraphs is used to improve storage efficiency and interpretability, or simply to allow use of existing data structures and algorithms. 


There can be many different hypergraphs that project to a given graph $G$, and thus the projection operation is often \emph{lossy}. 
It is not at all clear when projecting a hypergraph and solving some problem on the projected graph is optimal, and in general this depends both on the task and on the hypergraph. One scenario in which projecting to a simple graph does not degrade performance, \emph{whatever the task}, is if it is possible to efficiently reconstruct the hypergraph from the projected graph. 
This motivates the following basic question: under what conditions does projecting a hypergraph result in information loss, and conversely, when can a hypergraph be recovered from its projection? 




% A natural question is when is projecting a hypergraph to a normal graph a good strategy to develop algorithms. 

% The viability of hypergraph-to-graph projection as a strategy for algorithm development depends on the nature of the problem at hand.
% However, through exploring the inherent limits of information loss during projection, we can gain insights on this issue. 
% The answer to this question depends on specific problem of interest, but we can gain insight by studying the fundamental limit of when the projection start to loss information.

% \red{How to transition between the two motivations}
Beyond serving as a justifying principle for employing hypergraph-to-graph projections in algorithm creation, the task of recovering hypergraphs from their graph projections arises naturally in network analysis. 
The phenomenon of intrinsic hypergraphs appearing as projected graphs is common in real-world networks \cite{zhou2006learning,latapy2008basic,williamson2020random,battiston2020networks}. For instance, two scientists are listed as co-authors on Google Scholar because they collaborate on the same paper \cite{newman2004coauthorship}, two people send emails to each other because they are working on the same project \cite{klimt2004introducing}. In such scenarios, direct methods for detecting higher order interactions are often unavailable, which highlights the importance of hypergraph recovery.

Prior research on this problem has been focused on designing algorithms with good empirical performance; none of the following works have theoretical guarantees. In \cite{young2021hypergraph,lizotte2023hypergraph}, the authors assumed a prior distribution over hypergraphs, and try to sample from the posterior to approximate the original hypergraph. 
% \gbdone{Does it actually sample from posterior? If so that seems pretty good...}
The work \cite{wang2022supervised} aims to recover a hypergraph from its graph projection, for a general distribution over initial hypergraph given access to another hypergraph independently sampled from the same distribution. A scoring method was then used to select hyperedges based on their similarity to the sampled hypergraph. 

% A scoring method was used in \cite{wang2022supervised} to select hyperedges, assuming access to an example hypergraph generated from the same distribution. \cg{Explain this better.} 
% While these approaches have demonstrated success on real datasets, there remains a gap in theoretical understanding, with no rigorous guarantees available.

In this work we aim to provide a deeper understanding of the conditions under which hypergraphs can be recovered from graph projections. %, thereby contributing to both the theoretical and practical aspects of hypergraph analysis. 
We study the problem of recovering a \emph{random} $d$-uniform hypergraph, where all hyperedges are of size $d$, from its graph projection. For $d=3$ we determine a precise threshold in the hyperedge density at which recovery is feasible, and give an efficient algorithm to do so when it is. For $d\geq 4$ we provide bounds on the hyperedge density. 
Our analysis relies on analyzing the local structure of random hypergraphs, and in the process we identify useful structural properties of random hypergraphs. 


Our results hold also for mildly inhomogeneous random hypergraphs, where edge probabilities may be non-uniform but are all within constant factors of one another. This includes the hypergraph stochastic block model (HSBM).
The question of determining the information-theoretic recovery thresholds for HSBM, given the similarity matrix, was previously posed as an open problem in \cite{gaudio2023community}.
As a by-product of our results, we solve the open problem showing that the information theoretic threshold of HSBM, given the similarity matrix, coincides with that of HSBM given the original hypergraph.\footnote{One of the original motivations of the present paper was to disprove
the claim that the two thresholds are different, made in \cite{gaudio2023community-v1}. Later versions \cite{gaudio2023community} replace this with the statement that the threshold for HSBM recovery from the similarity matrix is open.} This is proven by a reduction that recovers the original hypergraph given the similarity matrix.


% We also give the information theoretically optimal algorithm that outperforms the min-bisection estimator and the SDP-relaxation algorithm studied in  \cite{kim2018stochastic,gaudio2023community}. 
% As a by-product of our results, we disprove a stated claim from COLT 2023 on recovery in hypergraph stochastic block models. 
% It is claimed in \cite{gaudio2023community} that their algorithm gives information theoretically optimal performance on the community recovery in hypergraph stochastic block model given the similarity matrix.  
% However, based on our results, the original hypergraph can be exactly recovered from the similarity matrix with high probability. So the optimal algorithm should have the same performance as the optimal algorithm given the original hypergraph, better than what is claimed in \cite{gaudio2023community}.



% \red{
% \begin{itemize}
%     \item We ask the basic question of when does projecting a hypergraph to a graph losses information, equivalently, when is it possible to recover the original hypergraph from a projected graph.
%     \item Inspiration from social network and biology. How to recover higher order interactions from a graph.
%     \item Some data are inherently hypergraph, but treated as a graph. Examples from social science. 
%     \item We want to understand when is projecting hypergraph to a normal graph a good strategy to develop algorithms. The answer depends on specific question of interest, but we can gain insight by studying the fundamental limit of when the projection start to loss information.
%     \item The same question has been studied from empirical point of view and tested on real data sets.
%     \item To understand the question in theory, we study random hypergraphs, as worst-case hypergraphs is impossible to recover.
%     \item Definition of the problem
%     \item We obtain tight bounds of exact recovery for random $d$-hypergraph when $d=3,4,5$.
%     \item An interval for $d\ge 6$? We can have some simple upper and lower bounds.
%     \item A chart for the results.
% \end{itemize}
% }


\subsection{Hypergraph Reconstruction Problem Formulation}
Before describing our problem formulation we require a couple of definitions.

\subsubsection{Random hypergraphs}
We define the following model of random hypergraphs, generalizing the \ER\ random graph.\footnote{This definition of random hypergraph is equivalent to the definition of random $d$-complex \cite{toth2017handbook} except here we use the language of hypergraphs instead of simplicial complexes. The model considered in \cite{young2021hypergraph} is an inhomogeneous generalization of our model where each hyperedge has a distinct probability of appearing. Projection of random hypergraphs was also proposed as a way to simulate network data \cite{williamson2020random}. 
% \gbdone{What does this last sentence mean?}
}

A \emph{random $d$-hypergraph} $\rhG(n,d,p)=([n],\hE_{\hG})$ is a $d$-uniform hypergraph where every size-$d$ hyperedge in $\binom{[n]}{d}$ is included in $\hE_{\rhG}$ with probability $p$ independently. We will use the parameterization $$p=n^{-d+1+\delta}\,,$$ so that the expected degree of each node is on the order $n^\delta$.\footnote{Constant factors do not affect any result of the paper. All of our results also holds with possibly different probabilities of inclusion at different edges, as long as the probability is $\Theta(n^{-d+1+\delta})$. } 

\subsubsection{Graph projection}
Given a hypergraph $H=([n],\hE)$, we consider the \emph{projection} $\proj(H)$ which takes
$d$-uniform hyperedges to ordinary (pairwise, undirected) edges by simply including an edge if  both its endpoints are in a hyperedge:
\[
\proj(\hE) \defeq \b\{(i,j)\in \textstyle{\binom{ [n]}{2}}: i,j\in \he\text{ for some } \he\in \hE\b\}.
\]
Here we overload notation, using $\proj$ both for projection of a set of hyperedges and for the projected graph. 
A random hypergraph $\rhG$ results in a \emph{random projected graph} $\pG=\proj(\rhG)=([n], \pE=\proj(\hE_{\rhG}))$. 
% 
% For any $e\in\binom{[n]}{2}$, let $\Ip_e$ be the indicator that $e\in \pE$. We also use $\proj$ to denote the mapping from the indicator variable $\Ih$ to the indicator variable $\Ip$.
For one hyperedge $\he$, we use $\proj(\he)$ to denote $ \proj(\{\he\})$. For a simple graph $G$, we say a hypergraph in $\prim (G)$ is a \emph{preimage} or a \emph{clique cover} of $G$. We will frequently use the fact that projection commutes with union: $\proj(C_1\cup C_2) = \proj(C_1)\cup \proj( C_2)$. 

Our goal is to recover the original hypergraph $\rhG$ from the projected graph $\pG$.

\subsubsection{Exact Recovery}
We say that an algorithm $\cA:\{0,1\}^{[n]\choose 2}\to \{0,1\}^{[n]\choose d}$ mapping a projected graph $\pG$ to a $d$-uniform hypergraph can achieve (asymptotically) \emph{exact recovery} if 
\begin{equation}\label{e:recovery}
    \Pr\b(\cA(\proj(\rhG)) = \rhG\b)=1-o_n(1)\,.
\end{equation}


\begin{remark}\label{rem:delta-range}
   We parameterize $p=p(\delta,d,n)=n^{-d+1+\delta}$ so that the expected degree of a node is $\Theta(n^\delta)$.
    The problem of exact recovery is only interesting when $0\le \delta\le 1$. When $\delta<0$, with high probability $\pG$ only consists of isolated $d$-cliques, so exact recovery is trivial. When $\delta>1$, with high probability $\pG$ is complete, so exact recovery is impossible.
\end{remark}

% \begin{remark}\cg{Consider deleting or putting this in footnote}
%     We consider recovering a random hypergraph instead of recovering a worst-case input. Indeed, there are $2^{\binom{n}{d}}$ possible hypergraphs and only $2^{\binom{n}{2}}$ possible graphs. So it is impossible to recover an arbitrary hypergraph.
% \end{remark}
% \begin{itemize}
%     \item results
%     \item technique overview
%     \item monotonicity in density, so translates into thresholding.
%     \item optimal algorithm
%     \item subcritical branching process when delta small, analyze possible subgraphs
% \end{itemize}

\paragraph{Information-theoretic Versus Algorithmic Feasibility.}
    The existence of an algorithm $\cA$ satisfying
    \eqref{e:recovery}
    answers the question of whether the projection operator loses information.
Exact recovery is \emph{information theoretically possible} for a certain $\delta$ if there exists an algorithm $\cA$ that can do exact recovery regardless of time complexity. 
When exact recovery is information theoretically possible, we wish to find an efficient algorithm. Exact recovery is said to be \emph{efficiently achievable} for a certain $\delta$ if there exists a polynomial-time algorithm $\cA$ that can achieve exact recovery.



\subsection{Results}

Before describing our results, it will be helpful to gain a qualitative understanding of how the density $p=n^{-d+1+\delta}$ impacts the difficulty of exact recovery. The main intuition is that as we make the hypergraph denser, recovery from the projected graph gets more difficult as projections of different hyperedges begin to overlap. The extreme case where the projected graph is complete was mentioned in Remark~\ref{rem:delta-range}. This intuition is formalized by the following lemma, which is proved in Appendix~\ref{sec:monotone}.

% \subsection{Exact Recovery is Monotone in Density}
\begin{lemma}[Monotonicity in $\delta$]\label{lem:monotone}
    For any $d\ge 4$ and any $0\le \delta_1<\delta_2\le 1$, if exact recovery is information theoretically possible (or efficiently achievable) when $\delta=\delta_2$, then exact recovery is also information theoretically possible (or efficiently achievable) for $\delta=\delta_1$.  
\end{lemma}
The lemma is proved via a simple reduction: given $G = \proj(\rhG)$ where $\rhG$ has density $p(\delta_1,n,d)$, we independently sample a random hypergraph $\rhG'$ so that $\rhG\cup \rhG'$ has density $p(\delta_2,n,d)$ and give algorithm $\cA$ (presumed to achieve exact recovery at $\delta_1$) the graph $\proj(\rhG)\cup \proj(\rhG') = \proj(\rhG\cup \rhG')$. We then remove the hyperedges in $\rhG'$ from the output of $\cA$, and this succeeds as long as $\rhG$ and $\rhG'$ have no hyperedges in common. This latter property holds for $d\geq 4$, but not for $d=3$. 


It follows that for $d\ge 4$ there must exist a threshold $\cdel d$ above which exact recovery is possible and below which exact recovery is impossible. Formally, let
\[
\cdel d \defeq \inf \{\delta: \text{exact recovery is impossible at }\delta\}\,.
\]
We have the following corollary from the lemma above.
\begin{corollary}[Threshold for Exact Recovery]\label{cor:monotone}
    For $d\ge 4$, exact recovery is information theoretically possible for any $\delta<\cdel d$ and impossible for any $\delta>\cdel d$.
\end{corollary}


% Although it is not obvious from Lemma~\ref{lem:monotone} that such threshold exists when $d=3$, it is proven this is the case.
The statement of the corollary is also true for $d=3$, but this requires a different argument.
% \red{Notion of the threshold $\delta^*$. Then express the results in terms of $\delta^*$}
We determine the location of the threshold when $d=3$ and we also prove that exact recovery precisely at the threshold is impossible.

\begin{theorem}\label{thm:main-three}
    For $d=3$, there is an efficient algorithm for exact recovery when $\delta<2/5$ and exact recovery is information theoretically impossible when $\delta\ge 2/5$.
\end{theorem}

 For $d\ge 4$, as stated in the following two theorems, we demonstrate that the threshold $\cdel d$ must lie in a certain interval. Furthermore, we find an efficient algorithm (in fact, attaining the optimal probability of reconstruction error) in the regime where we show that exact recovery is possible. It is worth noting that our algorithm does not need to know $p$ as an input parameter.
The results are summarized in Table~\ref{table:delta_bounds}.

\begin{theorem}\label{thm:main-four-five}
    For $d=4,5$, there is an efficient algorithm for exact recovery when $\delta<1/2$ and exact recovery is information theoretically impossible when $\delta\ge \frac{2d-4}{2d-1}$.
\end{theorem}

\begin{theorem}\label{thm:main-large-d}
    For $d\ge 6$, there is an efficient algorithm for exact recovery when $\delta<\frac{d-3}{d}$ and exact recovery is information theoretically impossible when $\delta\ge \frac{d^2-d-2}{d^2-d+2}$.
\end{theorem}

\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|}
\hline
Value of \(d\) & Lower Bound for \(\cdel d\) & Upper Bound for \(\cdel d\) \\ \hline
\(3\) & \(2/5\) & \(2/5\) \\
\(4\) & \(1/2\) & \(4/7\) \\
\(5\) & \(1/2\) & \(2/3\) \\
\(d\ge 6\) & \(\frac{d-3}{d}\) & \(\frac{d^2-d-2}{d^2-d+2}\) \\ \hline
\end{tabular}
\caption{Bounds for hyperedge density threshold \(\cdel d\).}
\label{table:delta_bounds}
\end{table}

For $d=4$ and $5$, we conjecture that the correct threshold is at $\frac{2d-4}{2d-1}$ (note this is the case for $d=3$). Our methodology enables proving the conjecture by verifying certain combinatorial properties for finitely many graphs, a check that can be carried out with computer assistance. However, the computation required is significant and we were unable to complete the computer verification. We elaborate on this in Section~\ref{s:mainIdeas}.




\input{HSBM}

\subsection{Notation}
We always use $H$ for hypergraphs, $\he$ for hyperedges and $\hE$ for a set of hyperedges. $G$ stands for a simple graph, $e$ is used to denote an edge, and $E$ denotes a set of edges. The \emph{size} of a graph (hypergraph) means the number of edges (hyperedges) in the graph (hypergraph). We often identify a graph or hypergraph simply by its edge set, which causes no ambiguity in the case that every vertex is in some edge (i.e., there are no isolated vertices).

All the probabilities $\Pr$ are in the probability space defined by the random $d$-hypergraph $\rhG(n,d,p)$. We denote by $X_{\hG}$ the random variable equal to the number of appearances of $\hG$ as a sub-hypergraph of $\rhG$.

% \gbdone{$K$ denotes a sub-hypergraph.}