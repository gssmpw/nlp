\subsubsection{Application to Hypergraph Stochastic Block Model}
% \begin{itemize}
%     \item one of the application of our result is for HSBM
%     \item what is the definition of hsbm and community detection
%     \item Argue through the monotonicity argument? but that does not include $d=3$. Just say log factors does not affect any result below the threshold. For the convenience of discussion we use the same value of p everywhere
% \end{itemize}
\label{s:HSBM}
We now discuss the application of our results to the Hypergraph Stochastic Block Model (HSBM).
As we explain momentarily, a byproduct of our result is that community detection from the graph projection of the HSBM is equivalent to community detection given the original HSBM hypergraph, and this is also equivalent to doing so given the similarity matrix (defined below). 

The model $\hsbm(d,n,q_1,q_2)$ describes a random $d$-uniform hypergraph on $n$ vertices, parameterized by $q_1$ and $q_2$. A sample $\rhG$
is generated as follows. First an assignment of labels $\sigma\in\{\pm 1\}^n$ for the vertices is sampled uniformly at random from all assignments with equal number of $+1$ and $-1$ ($n$ is assumed to be even). Conditional on $\sigma$, for each $\he\in\binom{[n]}{d}$, the hyperedge $\he=\{i_1,\cdots,i_d\}$ is included in $\rhG$ independently with probability
\[
\Pr(\he\in\rhG )=
\begin{cases}
    q_1 &\text{ if } \sigma_{i_1} = \sigma_{i_2} = \cdots = \sigma_{i_d}\\
    q_2 &\text{otherwise}\,.
\end{cases}
\]
The probabilities $q_1$ and $q_2$ are parameterized as 
$q_1 = \alpha\log n/\binom{n-1}{d-1}$ and $q_2 = \beta\log n/\binom{n-1}{d-1}$. 

In the \emph{community recovery} problem, we are given a sample hypergraph $\rhG\sim \hsbm(d,n,q_1,q_2)$ and we want to recover the assignment for all vertices (up to global sign flip).

The \emph{similarity matrix} $W$ of a hypergraph $H=([n],\hE)$  is defined to be 
\[
W_{ij} = |\{\he\in \hE: i,j\in \he\}|\,.
\]
In \cite{kim2018stochastic,cole2020exact,gaudio2023community}, the similarity matrix of the hypergraph is used as the algorithm input. A basic question is: does using the similarity matrix lose performance as compared to using the original hypergraph? Our result shows that this is not the case. Specifically, if there is an algorithm that recovers the assignment for some $d,\alpha$ and $\beta$ with the hypergraph as input, then there exists an algorithm that recovers the assignment for the same $d,\alpha$ and $\beta$ with the similarity matrix as input. 
This yields an algorithm for exact recovery given the similarity matrix that outperforms those in prior work.


% This contradicts the claim in \cite{gaudio2023community} that their algorithm achieves the information theoretically optimal bound given the similarity matrix, which is worse than the optimal bound given the original hypergraph.

\begin{theorem}
    For any $d, \beta$ and $\alpha$,
    given the similarity matrix $W$ of $\hsbm(d,n,q_1,q_2)$ where $q_1 = \alpha\log n/\binom{n-1}{d-1}$ and $q_2 = \beta\log n/\binom{n-1}{d-1}$, we can exactly recover the hypergraph with high probability.
\end{theorem}
\begin{proof}
In the HSBM parameter regime, the edge density is $\Theta(n^{-d+1}\log n)$, which is far below the critical threshold $n^{-d+1+\cdel d}$ and indeed also far below our \emph{lower bound} on the critical threshold (i.e., our algorithms succeed in this range). Note that the HSBM may not appear to be within the setting of this paper because: 
\begin{enumerate}
    \item The probability of having a hyperedge depends on the assignment of the nodes.
    \item There is a constant that differs across hyperedges, as well as a $\log n$ factor, in front of the probability.
\end{enumerate}
However, all of our achievability results below the critical threshold $p=n^{-d+1+\cdel d}$ only require an upper bound on the hyperedge probabilities, regardless of whether the probability depends on specific edges. For instance, in the proof of Lemma~\ref{lem:exp-dec}, we only used the fact that $p=O_n(n^{-d+1+\delta})$. In the regime of HSBM both $q_1$ and $q_2$ are $O_n(n^{-d+1+\delta})$, so the argument still holds. 
% \cg{For instance, in the proof of ...}\red{needs editing}
In this paper we nevertheless use the parameterization $p=n^{-d+1+\delta}$ for clarity of exposition.
\end{proof}

% The results here also disprove a claim from \cite{gaudio2023community} in COLT23. It is claimed in \cite{gaudio2023community} that their algorithm gives information theoretically optimal performance on the community recovery in hypergraph stochastic block model given the similarity matrix, which is projection of the hypergraph with multiplicity. The regime of hypergraph stochastic block model corresponds to $p=\Theta(\frac{\log n}{n^{d-1}})$. Based on the theorems above, we can exactly recover the original hypergraph from the similarity matrix with high probability. So the optimal algorithm should have the same performance as the optimal algorithm given the original hypergraph, better than the algorithm in \cite{gaudio2023community}.
