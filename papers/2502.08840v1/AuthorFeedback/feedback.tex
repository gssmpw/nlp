\documentclass[12pt]{colt2024} %% Anonymized submission
\usepackage{times}
\input{macros-response}
% \usepackage{fullpage}
\begin{document}
% \title{Author Feedback}                                    %% This removes title for more space 
  %% Your response goes here (at most one page pdf).
We thank the reviewers for their time and effort reviewing the paper. For brevity we paraphrase the reviewer questions as we understood them, and provide responses.  We will improve the camera-ready version to address the questions asked by the reviewers. The errors and typos found by the reviewers will also be corrected. Thank you again!

\paragraph{Question: (Reviewer \#1)} \emph{Are there results on hypergraphs with variable size or approximate recovery?}

 Neither of these is addressed by our paper, but they are certainly nice questions for the future.

\paragraph{Question:  (Reviewers \#1 and \#2)} \emph{Clarify the statement about the result of Gaudio \& Joshi.}

Gaudio \& Joshi proposed an algorithm for hypergraph SBM (HSBM), based on using the \emph{similarity matrix} $W$ where $W_{ij} = |\{h\in \rhG: \{i,j\}\subseteq h\}|$. They claimed that the algorithm was optimal among all algorithms that use only the similarity matrix $W$, noting that the performance is \emph{worse} than what is achievable given the original hypergraph $\rhG$. 
Their claim consists of two parts: (1) achievable performance of their algorithm, and (2) the statement that no other algorithm can do better using only $W$. 
The analysis for item 1, achievable performance of their algorithm, is correct as far as we know. 

However, their claim (2) is not correct. (They do not attempt to prove this, but rather attribute this result to Kim, Bandeira and Goemans, which does not actually show this.)
It follows from our results that their algorithm cannot be optimal, even amongst algorithms using only $W$. An algorithm that outperforms theirs is as follows. First (a) use our hypergraph recovery result to recover the original hypergraph and then (b) apply the existing HSBM recovery results, which as noted above achieves better performance than Gaudio \& Joshi's algorithm. It remains to argue that step (a) can be done. This follows from our theorems, but more conceptually, what is happening is that the threshold for (in)feasibility of recovery of the hypergraph is for \emph{much} denser graphs than the regime of interest for HSBM, i.e., it is \emph{feasible} to recover original HSBM from the graph projection (and thus also from $W$) and thus step (a) does not form a bottleneck.

This discussion is based on the July 1, 2023 version of their paper, which they have since updated to express that the threshold for recovery from $W$ is open. Our results nevertheless still settle this question.


% This was done by constructing an algorithm and analyzing its performance. This part of the argument is correct. However, for the lower bound they quote [blah], which however does not contain such a claim

% The guarantee on their algorithm is still valid. But their claim that the algorithm is information theoretically optimal given the similarity matrix is not correct. The following algorithm would have better performance than their algorithm: Recover the original hypergraph with similarity matrix using our result and then apply the optimal algorithm on HSBM given the original hypergraph.



% \paragraph{Question:} Results for $d=4,5$ are not mentioned on pg2. (Maybe no need to reply?)
% \\
\paragraph{Question:  (Reviewer \#2)} \emph{Is Theorem 10 a statement that holds with high probability?}

Yes. As the problem input is random, all guarantees on algorithm performance in the paper hold with high probability. We will clarify this in the updated version.


\paragraph{Question:  (Reviewer \#2)} \emph{I find the quantifiers in Lemma 17 a bit unclear. Does ``If any ambiguous graph..." mean ``there exists" or ``for all"?}

We mean ``If all ambiguous graphs satisfy ...". Apologies for the lack of clarity in the lemma statement.


\paragraph{Concern:  (Reviewer \#2)} \emph{It is hard to evaluate the difficulty of the techniques used in the paper.}

On the surface, the approach may seem straightforward, but numerous subtleties arise in the analysis. In fact, we were quite surprised on multiple occasions by the technical difficulties that arose. 

In the paper there are two critical thresholds. The first threshold is $\delta=\frac{d-1}{d+1}$, below which 2-connected components are of constant size. The second threshold is the threshold below which ambiguous graphs appears with vanishing probability. The argument developed in the paper on ambiguous graphs only holds below the first threshold. When $d\ge 6$, the second threshold is above the first threshold, and the structure of the problem completely changes. 

One main technical difficulty lies in the analysis of the first threshold. We need to optimize a probability over all possible ways for which a 2-connected component can grow. It is not even obvious that the threshold has an analytic solution for general $d$. In Appendix C, we used a series of carefully designed relaxations of the optimization problem to determine the solution. 
\end{document}