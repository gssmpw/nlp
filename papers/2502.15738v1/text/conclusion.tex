\section{Next Steps}
    While the proof-of-concept prototype presented in this paper is a promising stepping stone toward full-fledged \sname, much more research is needed to fully explore the many new avenues opened by this technique.
    
    On the one hand, our immediate future work is focused on tackling the key challenges discussed in \sect~\ref{subsec:challenges}. To this end, we aim to design a \sname module capable of providing virtualization for an arbitrarily large portion of the virtual addressing space of a user-space application running in a typical OS, \eg Linux. On the other hand, we envision that several potential paradigms are within reach after that, as summarized below.

    
    \subsection{Future Paradigms Enabled by \sname}
    
        \para{Seamless pages migration}
            Migrating one or more pages of a user process or guest OS is not uncommon and done for multiple reasons---\eg moving a page closer to the physical core in a NUMA configuration, exploiting faster/non-volatile memory. Nonetheless, page migration is an expensive operation, requiring the processes involved to be blocked while busy-copy or DMA operations are performed and the affected page table(s) are updated.
            
            With \sname, it is possible to seamlessly migrate pages. Upon migration, \ci a DMA operation is issued to copy a given page; simultaneously, \cii any \tlb entry for the page is invalidated by the \sname module, which \ciii manipulates future translations to target the new page location. Before the DMA operation completes, \civ any access performed by the \textbf{running} processes is captured at the coherence level and replied with the data of the source page. \cv Once the DMA completes, the old page can be reclaimed, and accesses to the new page are not captured anymore.
        
        \para{Multiple Hypervisors Coexistence}
            State-of-the-art virtualization support allows for, at most, a single type-1 hypervisor active in the system. By leveraging \sname, it is also possible to manipulate hypervisor-side translations. Thus, it is foreseeable that multiple type-1 hypervisors could be allowed to operate simultaneously on the same physical platform.
        
        \para{\io Device Sharing}
            Type-1 hypervisors can statically assign a given \io resource in pass-through mode to a single \vm. Exporting the same device to multiple \vm{s} requires device emulation or advanced support like SR-IOV~\cite{sr_iov}.
            
            We envision that \sname could enable \io device virtualization in two ways. \ci First, one could replace the physical \pfn of the \io device's control registers and buffers with dedicated addresses in the \fpga aperture while having a hardware module in charge of selectively forwarding commands and data to the true aperture of the memory-mapped \io device.
            \cii Second, one could change the attribute set of the mapped physical device aperture to mark any page within that as cacheable. This way, accessing them will cause snoops that can be intercepted and manipulated. In this case, cache maintenance operations are also issues on the coherence interface to prevent stale data from being retained in any \cpu-side caches.
            
            The main advantage of scenario \ci is that it provides deterministic and direct knowledge of a \vm{s'} access to the device's registers. Indeed, the intercepted read/write bus transaction contains the exact target register address and the same width. Conversely, in scenario \cii, a cache line snoop can only target a cache line size-aligned address. Also, recovering the type of access (read vs. write) could prove challenging.       
            On the other hand, approach \cii allows a more compact design and seamless interaction without dedicating a portion of the \fpga address range to the virtualized device.
        
        \para{Code Execution and Interrupt Virtualization}
            For \sname to ultimately offer full-fledged virtualization support, the ability to execute management code on the \cpu is also desirable, as typical in type-1 hypervisors. It might appear that \sname only allows some address translation \emph{tricks}. Nonetheless, the translation tricks can be leveraged to unlock code execution in \sname. This is because, by carefully steering the physical translation of instruction fetches, \sname can direct the execution flow of the \cpu to any code in memory.
            
            But that's not all. A \sname module could simply replace, at the coherence level, the payload of the next cache line of instructions that is about to be executed by the \cpu, by hijacking the corresponding snoop request. The instructions served instead might be fetched from memory or procedurally generated on the fly as needed. Remarkably, by applying this approach when interrupt handling routines are executed, interrupt virtualization in \sname can be achieved as well.
            
    \subsection{On the vendors}
        \label{subsec:on_the_vendors}
        
        While our solution is thought to be seamless and transparent to be integrates within existing off-the-shelf systems, there is some hardware support that could be easily implemented by the vendors and could make the development easier and the tool more powerful.
        Some of the most relevant challenges depend on \textit{guessing} the intention of the main CPU cluster --- core or MMU --- with a given snoop request; in fact, the latter misses information about the exact address requested and even the nature --- read or write --- of the access.
        
        Especially in the optic of the close approach of the Compute Express Link standard, it could be valuable to have some additions to the coherence protocol:
        the snoop requests should have information related to the \ci memory access direction, \cii the offset --- in the cache line --- of the requested data and \ciii the source of the request --- \ie the core or MMU ID --- in a standard way; also, as an optional feature, \civ even in the case of a cache hit, at any level, a snoop should be performed and potentially waited for, so that the coherent hardware hypervisor can apply a veto and replace the content of the cache line.
