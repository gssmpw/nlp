\section{Ancillary Concepts}

    \begin{figure}
        \begin{subfigure}{\linewidth}
        \centering
            \resizebox{\linewidth}{!}{\input{imgs/va}}
            \caption{Virtual address description semantics}
            \label{fig:va}
            \Description{Virtual address description semantics}
        \end{subfigure}
        \begin{subfigure}{\linewidth}
        \centering
            \resizebox{\linewidth}{!}{\input{imgs/translation}}
            \caption{Translation tables walk}
            \label{fig:translation}
            \Description{Translation tables walk}
        \end{subfigure}
        \caption{\va to \pa translation in \arm{64}, 4\kb page granularity and \nth{4} level of translation folded.}
        \label{fig:mmu}
        \vspace{-0.5cm}
    \end{figure}


    This section covers fundamental concepts that are key to understanding \sname.
    The terminology used in this section is based on \arm{64} systems to ease the transition between the description of the concepts and our prototype.
    
    \para{Address Translation}
        Virtual memory is ubiquitous in modern general-purpose \oslong (\os).
        Its objective is to map a contiguous set of \valong{es} (\va) seen by a process to a set of \palong{es} (\pa) that may not be contiguous at a given granule (page) size.

        To allow for the implementation of arbitrary mappings, virtualization is achieved via page tables, i.e., a $l$-level $n$-ary radix tree walked by segmenting the \va in several indices. \fig{mmu} illustrates how a page table walk is performed on \arm{64} systems. A page table is looked up using said indices, shown in green, blue, or orange in \fig{mmu}. At each translation step, the content of the \ptelong (\pte) is a \emph{pointer} to the following translation step or a leaf page. Specifically, the \pte contains the \pfnlong (\pfn) of the next page table or of the final mapped data page.
        Page table walks are handled in hardware by the \mmulong (\mmu) upon each \cpu-originated \va memory access. As translations may occur frequently and are time costly, \mmu{s} are often equipped with multiple levels of \tlblong (\tlb) to cache translation results. Usually, the \mmu{s} can access the same cache hierarchy as the CPU load/store unit.
    
    \para{Type 1 Hypervisors}
        The terminology \emph{``Type 1 Hypervisors''} refers to a class of bare-metal hypervisors that directly interfaces a hosted \vmlong (\vm) with the hardware layer. They can partition system resources (memory, I/O devices) to the \vm{s} by leveraging specialized hardware support that introduces yet another level of indirection between \va{s} as translated by \vm{s} and real \pa{s}. Similarly to the \va-to-\pa translation described above, a secondary translation stage is introduced, leveraging page tables again. In this case, \os{es} in \vm{s} are said to translate \va{s} to \ipalong{s} (\ipa{s}) whereas, hypervisor-side page tables translate \ipa{s} to \pa{s}.        
        Of course, this extra translation layer introduces additional overheads, as up to two full page table walks may be performed for a single \va memory access.
        Moreover, hypervisor bookkeeping operations and the handling of \vm exits steal \cpu time from the hosted \vm{s}.
        
    \para{Cache Coherence}
        In modern multi-processors \soc{s} where the cache hierarchy is a key performance enabler, ensuring a consistent view of the memory to all attached \pe{s} is crucial.
        Snoop-based coherence is a common coherence paradigm.
        The core idea is to attach a few bits to each cache line to encode its state (\eg \emph{Modified}, \emph{Shared}, \emph{Invalid}) in the cache hierarchy.
        The state of each cache line in the hierarchy is continuously updated as they are accessed by the \pe{s}.

        Under snoop-based coherence, \pe-originated read/write requests prompt a coherence controller (\eg interconnect) to broadcast commands to all other attached \pe{s}.
        These commands directly affect the state of the targeted cache lines in the \pe{s}' caches.
        For instance, a read-miss event from \pe{1} will prompt the controller to broadcast a snoop request.
        If the snoop results in a hit in \pe{2}'s cache, the line state in \pe{2}'s cache will be altered---\eg \ci from clean and exclusive to clean and shared or \cii from modified and exclusive to invalid~\cite{MESI_protocol}.
        Conversely, in case of a miss in the cache of all the other coherent PEs, the request is forwarded to the next level of the cache hierarchy (\eg another cache level or main memory), and the line is allocated in \pe{1}'{s} cache.
        
        On \arm{64} \soc{s}, the \acelong (\ace) is an established bus-level interface to implement snoop-based coherence protocols. For instance, ACE-enabled \cpu-side cache controllers and interconnects (\eg ARM CCI-400~\cite{CCI-400}) are the basic components of a coherent multi-cluster big.LITTLE architecture~\cite{ARM-big-LITTLE, chung2012heterogeneous}.


    \para{Coherence Backstabbing}
        Simply put, the idea behind \emph{Coherence Backstabbing} as originally proposed in~\cite{CAESAR} is to hijack the coherence protocol for purposes other than maintaining coherence among a multi-\pe system where one of the \pe{s} is a coherent \fpga~\cite{enzian2020cidr}.
        Coherence backstabbing leverages the idea that a coherent \pe can \emph{pretend} to have ownership of a cache line, fabricate its content, and feed it to the other \pe.
        In other words, by \emph{``placing itself on the back of the coherence protocol''}~\cite{CAESAR}, a custom user-defined hardware module can seamlessly intercept, modify, and reroute \emph{any} coherent traffic---\eg cache line refills.
