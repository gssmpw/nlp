\section{Related Works in Recommendation Systems}
% \textbf{LLMs}: 
\textbf{Language Models
% for Recommendation
}: 
Recent progress in language models has led to significant interest in reformulating recommendation tasks as language modeling. Early approaches such as BERT4Rec____ and Transformers4Rec____ adapted transformer architectures for sequential recommendation, primarily focusing on modeling user behavior sequences through masked token prediction. These works demonstrated the potential of transformer-based architectures in capturing complex sequential patterns but were limited to behavioral data without leveraging rich semantic information.

More recent approaches have explored using foundation models for end-to-end recommendation. Text2Tracks____ investigates generative track retrieval, directly generating music item IDs from natural language queries. This eliminates the need for explicit retrieval mechanisms but faces challenges in maintaining recommendation quality across large item catalogs. MMREC____ extends this concept to multiple modalities, demonstrating how LLMs can jointly process different types of item features. However, these approaches still maintain separate modules for dialogue management and recommendation logic.


\textbf{Conversational Retrieval}:
Traditional conversational recommendation systems employ separate modules for dialogue management and recommendation____, requiring complex pipelines to coordinate user interaction and item retrieval. Recent approaches such as UniCRS____ and ReFICR____ leverage LLMs towards unifying these components, but their systems still require separate training stages for different modules.

In the music domain, conversational recommendation is particularly effective due to the rapid feedback cycle of music consumption - users can instantly experience and provide feedback on recommended items, unlike domains such as movies or products that require longer engagement periods. While recent work has explored knowledge-enhanced architectures____ and multimodal approaches____, these systems still maintain separate modules for dialogue understanding and recommendation generation. Our work addresses these limitations by directly optimizing query-item relevance through next-token prediction, enabling seamless integration of user preferences, contextual queries, and domain knowledge.

% \textbf{Multimodality}:
\textbf{Multimodality in Music}:
In the music domain, multimodality has been actively studied in the form of content-based music understanding____ and generation tasks____. For retrieval, multimodal approaches have primarily been used to address the cold-start problem____. However, most works were limited to combining only a single modality such as audio or text. 


% \sh{
% In contrast, other domains have made significant progress in multimodal recommendation systems.
Multimodal recommendation systems have been actively studied in other domains. For example,
% }
CCF-LLM____ and CMBF____ propose cross-modal fusion techniques to jointly learn from interaction patterns and content features, while fMRLRec____ introduces hierarchical representations to capture information at different granularities. However, these methods still require careful design of modality-specific architectures and fusion mechanisms.

% Modern industry-scale recommendation systems are often hybrid (i.e. multimodal), usually combining collaborative signals (user-item interactions) with content features for cold-start cases. Traditional approaches handle this through separate modules, merging or switching between them to cover a wide range of use-cases. While effective, these hybrid systems are complex to evaluate and maintain, and their training do not benefit from a more end-to-end learning.

% Recent approaches aim to unify multiple modalities through end-to-end architectures. 

Rather than designing explicit fusion mechanisms, our work builds upon advances in both LLM-based and conversational approaches by representing different modalities through a unified token architecture. This approach allows the model to learn cross-modal relationships directly through self-attention, inheriting the semantic understanding capabilities of pre-trained LLM weights. By encoding audio features, lyrics, metadata, and playlist co-occurrences as token sequences, \modelname eliminates the need for separate modality-specific architectures while maintaining interpretability - a key advantage over traditional multimodal fusion approaches.