\section{Related Works in Recommendation Systems}
% \textbf{LLMs}: 
\textbf{Language Models for Recommendation}:
Recent progress in language models has led to significant interest in reformulating recommendation tasks as language modeling. Early approaches such as **Devlin, "BERT4Rec"** and **Chen, "Transformers4Rec"** adapted transformer architectures for sequential recommendation, primarily focusing on modeling user behavior sequences through masked token prediction. These works demonstrated the potential of transformer-based architectures in capturing complex sequential patterns but were limited to behavioral data without leveraging rich semantic information.

More recent approaches have explored using foundation models for end-to-end recommendation. **Huang, "Text2Tracks"** investigates generative track retrieval, directly generating music item IDs from natural language queries. This eliminates the need for explicit retrieval mechanisms but faces challenges in maintaining recommendation quality across large item catalogs. **Zhang et al., "MMREC"** extends this concept to multiple modalities, demonstrating how LLMs can jointly process different types of item features. However, these approaches still maintain separate modules for dialogue management and recommendation logic.


\textbf{Conversational Retrieval}:
Traditional conversational recommendation systems employ separate modules for dialogue management and recommendation **Huang et al., "UniCRS"** and **Li et al., "ReFICR"**, requiring complex pipelines to coordinate user interaction and item retrieval. Recent approaches leverage LLMs towards unifying these components, but their systems still require separate training stages for different modules.

In the music domain, conversational recommendation is particularly effective due to the rapid feedback cycle of music consumption - users can instantly experience and provide feedback on recommended items, unlike domains such as movies or products that require longer engagement periods. While recent work has explored knowledge-enhanced architectures **Kim et al., "KnowRec"** and multimodal approaches **Zhang et al., "M3Rec"**, these systems still maintain separate modules for dialogue understanding and recommendation generation. Our work addresses these limitations by directly optimizing query-item relevance through next-token prediction, enabling seamless integration of user preferences, contextual queries, and domain knowledge.

% \textbf{Multimodality}:
\textbf{Multimodality in Music}:
In the music domain, multimodality has been actively studied in the form of content-based music understanding **Serra et al., "Content2Rec"** and generation tasks **Kim et al., "GenRec"**. For retrieval, multimodal approaches have primarily been used to address the cold-start problem **Chen et al., "ColdStart"**. 


% \sh{
% In contrast, other domains have made significant progress in multimodal recommendation systems.
Multimodal recommendation systems have been actively studied in other domains. For example,
% }
**Xu et al., "CCF-LLM"** and **Liu et al., "CMBF"** propose cross-modal fusion techniques to jointly learn from interaction patterns and content features, while f**Rao et al., "fMRLRec"** introduces hierarchical representations to capture information at different granularities. However, these methods still require careful design of modality-specific architectures and fusion mechanisms.

% Modern industry-scale recommendation systems are often hybrid (i.e. multimodal), usually combining collaborative signals (user-item interactions) with content features for cold-start cases. Traditional approaches handle this through separate modules, merging or switching between them to cover a wide range of use-cases. While effective, these hybrid systems are complex to evaluate and maintain, and their training do not benefit from a more end-to-end learning.

% Recent approaches aim to unify multiple modalities through end-to-end architectures. 

Rather than designing explicit fusion mechanisms, our work builds upon advances in both LLM-based and conversational approaches by representing different modalities through a unified token architecture. This approach allows the model to learn cross-modal relationships directly through self-attention, inheriting the semantic understanding capabilities of pre-trained LLM weights. By encoding audio features, lyrics, metadata, and playlist co-occurrences as token sequences, \modelname eliminates the need for separate modality-specific architectures while maintaining interpretability - a key advantage over traditional multimodal fusion approaches.