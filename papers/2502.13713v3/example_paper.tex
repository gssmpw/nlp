%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{xspace}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\newcommand{\sh}[1]{\textcolor{blue}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}


\lstset{defaultdialect=[5.3]Lua}


\lstdefinestyle{json}{
    basicstyle=\ttfamily\small,
    numbers=none,
    breaklines=true,
    morestring=[b]",  % handles strings in double quotes
    stringstyle=\color{green!50!black},
    showstringspaces=false,
    tabsize=2,
    frame=single,
    backgroundcolor=\color{gray!5},
    moredelim=**[is][\color{blue}]{@}{@},  % custom delimiters for specific highlighting
    moredelim=**[is][\color{red}]{|}{|},   % if you need another color for some elements
    captionpos=b
}

\lstdefinestyle{docstring}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!5},
    numbers=none,
    columns=flexible,
    keepspaces=true
}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\modelname}{\textsc{TalkPlay}\xspace}

% % Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{TalkPlay: Multimodal Music Recommendation with LLMs}

\begin{document}

\twocolumn[
% \icmltitle{TalkPlay: Multimodal Music Recommendation}
\icmltitle{\modelname: Multimodal Music Recommendation with Large Language Models}
% Multimodal Music Recommendation with LLMs

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Seungheon Doh}{equal,kaist,talkpl}
\icmlauthor{Keunwoo Choi}{equal,talkpl}
\icmlauthor{Juhan Nam}{kaist}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST, South Korea}
\icmlaffiliation{talkpl}{talkpl.ai, New York, USA}

\icmlcorrespondingauthor{SeungHeon Doh}{seungheondoh@kaist.ac.kr}
% \icmlcorrespondingauthor{Keunwoo Choi}{first2.last2@www.uk}
\icmlcorrespondingauthor{Juhan Nam}{juhan.nam@kaist.ac.kr}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% kc
We present \modelname, a multimodal music recommendation system that reformulates the recommendation task as large language model token generation. \modelname represents music through an expanded token vocabulary that encodes multiple modalities - audio, lyrics, metadata, semantic tags, and playlist co-occurrence. Using these rich representations, the model learns to generate recommendations through next-token prediction on music recommendation conversations, that requires learning the associations natural language query and response, as well as music items. In other words, the formulation transforms music recommendation into a natural language understanding task, where the model's ability to predict conversation tokens directly optimizes query-item relevance. Our approach eliminates traditional recommendation-dialogue pipeline complexity, enabling end-to-end learning of query-aware music recommendations. In the experiment, \modelname is successfully trained and outperforms baseline methods in various aspects, demonstrating strong context understanding as a conversational music recommender. A demo of the recommendation system is available at: \url{https://talkpl-ai.github.io/talkplay-demo}


% We present \modelname, a multi-modal music recommendation system that reformulates the recommendation task as large language model token generation. 
% % This is done \sh{by leveraging the language understanding and sequence modeling capabilities of language models.}
% \sh{\modelname unifies diverse music modalities into the model by vocabulary expansion, with the modalities including audio content, lyrics, metadata, semantic annotations, and collaborative playlist interactions.} 
% % Using these rich representations, the model learns to generate recommendations through next-token prediction, 
% % \sh{on both playlist continuation and music discovery conversations}. 
% \sh{
% % By formulating music recommendation as next-token prediction, 
% % % i.e., 
% % % it learns multiple associations across music modalities and natural language.
% % (i) within music modalities, (ii) between music items with playlist supervision, and (iii) associations between music and natural language with conversation supervision. 
% % Our approach transcends traditional unimodal, dot-product based recommendation methods by effectively leveraging the rich multimodal nature of music through this unified learning framework.
% } 
% \sh{The experiments demonstrate that \modelname achieves strong performance across three distinct tasks: playlist continuation, conversational music retrieval, and response generation, all unified through a single language model framework.}  % kc: i'll revisit this once we have more result. 

\end{abstract}

\section{Introduction}
\label{intro}

Recent advances in recommendation systems have focused on leveraging rich multimodal data and complex user-item interactions~\cite{liu2024multimodal}. While neural architectures have shown success in modeling these aspects~\cite{salau2022state}, the emergence of Large Language Models (LLMs) presents new opportunities for recommendation. Current LLM-based approaches typically use these models either as dialogue managers for natural language interaction \cite{liu-etal-2023-conversational} or as feature extractors for multimodal understanding \cite{mmrec}. However, these methods often require complex pipelines to bridge the semantic understanding of LLMs with structured recommendation logic.

We present \modelname, a multimodal music recommendation system that reformulates the recommendation task as next token prediction. Unlike existing approaches that maintain separate modules for dialogue, retrieval, and ranking, \modelname unifies these components through a novel multimodal music tokenization and vocabulary expansion. By representing music items as sequences of tokens encoding audio features, lyrics, metadata, tags, and playlist co-occurrence, our system learns to recommend directly through next-token prediction on music discovery conversations. This formulation eliminates the need for separate recommendation logic while preserving the benefits of both content-based and collaborative filtering approaches.

The LLM and token prediction framework provides several fundamental advantages. First, it enables end-to-end learning of query-aware recommendations without requiring explicit dialogue management or retrieval mechanisms. Second, its causal nature naturally models sequential patterns in music discovery, similar to session-based approaches but with richer semantic context. 
Third, \modelname inherits broad semantic understanding from pre-trained LLM weights, which likely contain rich knowledge about musical concepts like artist names, genres, and cultural contexts. Most significantly, the large capacity of the LLM architecture enables the model to effectively utilize this knowledge alongside all available signals - user preferences, contextual queries, and interaction patterns - for generating recommendations.
% Most significantly, by leveraging pre-trained LLM weights, \modelname inherits broad semantic understanding that enables unified optimization across all available signals: user preferences, contextual queries, and domain knowledge. 
This integration of multiple information sources through a text-based interface eliminates the need for modality-specific architectures while maintaining interpretability - a key advantage over traditional recommendation systems that require separate components for query understanding, preference modeling, and item retrieval.


\section{Related Works in Recommendation Systems}

% \textbf{LLMs}: 
\textbf{Language Models
% for Recommendation
}: 
Recent progress in language models has led to significant interest in reformulating recommendation tasks as language modeling. Early approaches such as BERT4Rec~\cite{bert4rec} and Transformers4Rec~\cite{transformers4rec} adapted transformer architectures for sequential recommendation, primarily focusing on modeling user behavior sequences through masked token prediction. These works demonstrated the potential of transformer-based architectures in capturing complex sequential patterns but were limited to behavioral data without leveraging rich semantic information.

More recent approaches have explored using foundation models for end-to-end recommendation. Text2Tracks~\cite{text2tracks} investigates generative track retrieval, directly generating music item IDs from natural language queries. This eliminates the need for explicit retrieval mechanisms but faces challenges in maintaining recommendation quality across large item catalogs. MMREC~\cite{mmrec} extends this concept to multiple modalities, demonstrating how LLMs can jointly process different types of item features. However, these approaches still maintain separate modules for dialogue management and recommendation logic.


\textbf{Conversational Retrieval}:
Traditional conversational recommendation systems employ separate modules for dialogue management and recommendation~\cite{zhang2018towards, chen2019towards}, requiring complex pipelines to coordinate user interaction and item retrieval. Recent approaches such as UniCRS~\cite{unicrs} and ReFICR~\cite{reficr} leverage LLMs towards unifying these components, but their systems still require separate training stages for different modules.

In the music domain, conversational recommendation is particularly effective due to the rapid feedback cycle of music consumption - users can instantly experience and provide feedback on recommended items, unlike domains such as movies or products that require longer engagement periods. While recent work has explored knowledge-enhanced architectures~\cite{zhang2021kecrs} and multimodal approaches~\cite{liu2023m3ke}, these systems still maintain separate modules for dialogue understanding and recommendation generation. Our work addresses these limitations by directly optimizing query-item relevance through next-token prediction, enabling seamless integration of user preferences, contextual queries, and domain knowledge.

% \textbf{Multimodality}:
\textbf{Multimodality in Music}:
In the music domain, multimodality has been actively studied in the form of content-based music understanding~\cite{gardner2023llark, mugen} and generation tasks~\cite{tal2024joint}. For retrieval, multimodal approaches have primarily been used to address the cold-start problem~\cite{van2013deep, oramas2017deep}. However, most works were limited to combining only a single modality such as audio or text. 


% \sh{
% In contrast, other domains have made significant progress in multimodal recommendation systems.
Multimodal recommendation systems have been actively studied in other domains. For example,
% }
CCF-LLM~\cite{liu2024collaborative} and CMBF~\cite{chen2021cmbf} propose cross-modal fusion techniques to jointly learn from interaction patterns and content features, while fMRLRec~\cite{lai2024matryoshka} introduces hierarchical representations to capture information at different granularities. However, these methods still require careful design of modality-specific architectures and fusion mechanisms.

% Modern industry-scale recommendation systems are often hybrid (i.e. multimodal), usually combining collaborative signals (user-item interactions) with content features for cold-start cases. Traditional approaches handle this through separate modules, merging or switching between them to cover a wide range of use-cases. While effective, these hybrid systems are complex to evaluate and maintain, and their training do not benefit from a more end-to-end learning.

% Recent approaches aim to unify multiple modalities through end-to-end architectures. 

Rather than designing explicit fusion mechanisms, our work builds upon advances in both LLM-based and conversational approaches by representing different modalities through a unified token architecture. This approach allows the model to learn cross-modal relationships directly through self-attention, inheriting the semantic understanding capabilities of pre-trained LLM weights. By encoding audio features, lyrics, metadata, and playlist co-occurrences as token sequences, \modelname eliminates the need for separate modality-specific architectures while maintaining interpretability - a key advantage over traditional multimodal fusion approaches.

\section{Data}\label{sec:data}

This section introduces the data generation process of \modelname. It is prohibitively difficult to perform research on recommendation without access to real-world data -- information about items, users, and their interactions. Another challenge lies in the fact that the conversational recommendation system is in its early stage and so the publicly available dataset is scarce. 
%For \modelname, an additional difficulty lies in the novelty of our research problem: Conversational recommendation system is in its early stage to have enough datasets. 
The Conversational Playlist Curation Dataset (CPCD) is probably the most relevant one, but it comes with 917 conversations which are not sufficient to train a large model~\cite{chaganty2023beyond}.

Inspired by recent works such as ~\cite{leszczynski2023talk, doh2024music}, we generate the training data using a music playlist dataset and an LLM. In our primary reference ~\cite{doh2024music}, (1) each conversation is generated as a journey of machine-assisted music discovery; (2) each turn is generated as following: i) user's turn is stochastically chosen based on a user intention taxonomy, and ii) machine continues to react by choosing music items satisfying user's request. A notable difference is that in \modelname, the base dataset is not simply a music dataset (e.g., \cite{bertin2011million}) but a playlist dataset, in order to simulate more plausible and coherent conversation and recommendation.

\subsection{Base Dataset: Million Playlist Dataset}\label{subsec:base-data}
In the current era of music streaming, playlists are playing a crucial role in the music listening experience. They also provide unique and direct information about music items: which music items go along together well, according to the real listeners. They may suit well for the purpose of training \modelname as well, as a base dataset, since conversation consisting of tracks in the same playlist would be more diverse, coherent, and hence realistic than other hypothetical strategies such as random sampling or any other heuristics. 

Among the available playlist datasets, the Million Playlist Dataset (MPD) is chosen as our base dataset \cite{chen2018recsys}. Since being introduced in 2018, this dataset remains one of the largest playlist datasets available to the public by including one million Spotify playlists. The MPD provides track metadata and playlist co-occurrence and is extended with audio content, lyrics, and semantic annotations. The audio content are successfully crawled through the Spotify API for 1,717,148 items. The lyrics are extracted using \textsc{Whisper-Large-V3}~\cite{radford2023robust}, a well-known speech transcription model that performs decent lyrics transcription. For semantic annotations, we leverage a pretrained music captioning model, \textsc{LP-MusicCaps}~\cite{doh2023lp}, that incorporates information about genres, moods, instruments, and vocal styles.

\begin{table}[t]
\centering
\begin{tabular}{lrr}
\toprule
Dataset                       & Training         & Evaluation       \\ \midrule
\multicolumn{3}{l}{{\color[HTML]{9B9B9B} \textit{Miliion Playlist Dataset}}} \\
\# of Playlists                & 999,000          & 1,000            \\
\# of Average Tracklist        & 63.73            & 59.41            \\
\# of Warm Start Tracks        & 1,714,772        & 37,114           \\
\# of Cold Start Tracks        & -                & 2,357            \\
\# of Vocab                    & 24,129            & 713              \\
Sparsity                      & 0.99996          & 0.99850          \\ \midrule
\multicolumn{3}{l}{{\color[HTML]{9B9B9B} \textit{TalkPlay Dataset}}}        \\
\# of Playlist                 & 116,136          & 1,000             \\
\# of Dialogue                 & 532,627          & 1,000             \\
\# of Tracks                   & 406,768           & 11,022            \\
\# of Vocab                    & 318,281           & 19,796            \\
Avg. \# of turns               & 7.03             & 13.31            \\
Avg. query len.               & 17.62            & 15.35           \\ 
Avg. reponse len.               & 17.91            & 14.92           \\ 
\bottomrule
\end{tabular}
\vspace{-1mm}
\caption{Statistics of the base dataset and TalkPlay dataset.}
\vspace{-7mm}
\label{tab:dataset-stats}
\end{table}

\subsection{TalkPlay Dataset}\label{subsec:maintask-synthe-music-convo}
% kc: ok, strong opinion here.
% i insist the original structure. 
% [1. goal, motivation, assumption (flexibility, coherence, modality coverage, text format.) 
% [2. details about how we did, to be aligned with the goal movation etc.]
%
% this would be better than starting with [2].. right? becuase this whole thing would be quite new to readers. so we need to justify lots of our design choices. this playlist-based synthesis idea is what i suggested you, and do you remember i convinced you that we can _just_ use the playlist sequence, with some of them being unsatisfying recommendations? if it took me some time and effort to convince you, we have to invest some space for the same thing.
% this small sub section is equal to a dataset paper. would you start with how you did? no, you def would start with your goals and etc, THEN, based on those, you'd explain what you did. let's do the same thing here.
% please let me know how you think. after then we can make the change. 
% ps. if your intention was to provide a brief overview to begin the subsection, sorry mr. Doh, this is not brief at all. many details should go after the design principles. (the first paragraph literally explains *everything*.) 


We use an LLM and the base dataset (MPD) to generate TalkPlay Dataset, a music conversation dataset with natural language and music recommendation.\footnote{https://huggingface.co/datasets/talkpl-ai/talkplay-db-v1}
% \sh{We use an LLM to play a role of data augmentation function to generate music recommendation conversations. } 
% Given a sequence of tracks from a playlist in MPD, the LLM takes  comprehensive information about each track with a carefully designed prompt. The LLM then generates a multi-turn conversation that naturally incorporates these tracks as recommendations. By using metadata, lyrics, semantic information from all tracks within a playlist, we ensure the synthetic conversations maintain coherence while reflecting realistic music listening patterns.
% \sh{Our synthesis approach aims to create natural dialogues where recommendations emerge organically from user-system interactions.} 
% Rather than generating arbitrary conversations, we ground the synthesis in actual playlist data, leveraging the inherent musical coherence and thematic relationships present in human-curated track sequences.
% \sh{We detail our approach for generating the \modelname dataset through the LLM-based synthesis, focusing on both the key design principles and their practical implementation. }
These are the key design principles and choices to ensure the synthetic data can be used for the training of \modelname.

\textbf{Coherence}: Given a user query or reaction, the system should respond with music recommendation and relevant dialogue.

\textbf{Modality Coverage}: Conversations span all target modalities (audio, lyrics, metadata, and semantic tags) except playlist co-occurrence. We exclude the latter because: i) these implicit modality is difficult to verbalize, and ii) in our task setup, playlist co-occurrence would be inherently captured (Section~\ref{subsec:system-music-token}).

\textbf{User Simulation}: Users may skip or reject recommended tracks, even if the track sequence is based on existing playlists. Our synthetic conversation reflects this by allowing users to request alternative recommendations.

\textbf{Text Format}: For the outcome of this generation (synthesized conversation), a structured format is preferred, in order to use the synthetic conversation for our training (i.e., music tokenization in Section~\ref{subsec:system-music-token}). The JSON format is chosen and used as presented in Section~\ref{subapp:synth-convo}

Based on this, \textsc{Gemini-1.5-flash-002}~\cite{team2024gemini} is used to create music conversation given a prompt that includes i) our goal and the aforementioned conditions, ii) track identifiers and their metadata, semantic tags, lyrics. The prompt is included in Section~ \ref{subapp:synth-prompt}. Table~\ref{tab:dataset-stats} compares our 
% LLM-augmented Synthesizing Music Conversation (SMC) % kc: i) we're not using this acronym anywhere later. ii) let's release it under a cooler name later. TalkPlayData-v1?
dataset with the base dataset.
Using 116k playlists with an average of 7 tracks each, our synthesis approach yields 552k multi-turn music recommendation dialogues. By leveraging rich textual annotations of music sequences, our LLM-based data synthesis technique significantly expands the text-music associations beyond just playlist titles, increasing the token count from 24k to 318k.

 
% \sh{
Each user-assistant interaction in our synthetic conversation dataset follows a triplet structure: \{\textsc{user query, recommended music, assistant response}\}. The user query and assistant response are natural language expressions, while the recommended music is represented by a unique track identifier.
During training, both the assistant response and the recommended music are used as labels. 
This structured format resembles the typical task or chat templates. %  enables the model to learn the relationship between conversational context and appropriate music recommendations while maintaining coherence across multiple turns.

\subsection{Dataset Split}
To reflect the real-world recommendation scenarios, we perform a chronological data split based on the playlist creation date. The test set is created by randomly sampling 1,000 playlists from the last creation date (2017-11-01), and training set include all the rest of the tracks. Through this split, around 2,357 tracks in the test split are cold-start items, tracks that only appear in the test set. Assuming a real-world scenario, these items represent newly released songs, which traditional collaborative filtering algorithms has no information to make recommendation from. In contrast, being multimodal, \modelname is able to handle these items by understanding content such as audio, lyrics, metadata and semantic features.

\begin{figure*}[!t]
\centering
\includegraphics[width= 0.85 \linewidth]{figs/main.png}
\vspace{-2mm}
\caption{Overview of \modelname: (1) The multimodal music tokenizer converts source data into modality embeddings $v$ and quantizes them into codebook indices $c$, which are mapped to music tokens $i$. (2) The LLM is fine-tuned on text and music token sequences. (3) The generated music tokens are mapped back to codebook indices and are used as queries to retrieve music items from the database.}
\label{fig:talkplay-overview}
\vspace{-1mm}
\end{figure*}

\section{\modelname: The Proposed System}\label{sec:proposed-system}

% ## TODO: changes on the figure
% Check out all the variable i defined ($x$, $v$, $c$, $m$, $i$, and usd them. 
% Semantic --> Semantic Tags
% Collaborative Filtering --> User-item interaction
% Enc. --> just remove this. 
% the "three" per encoder: remove them, and instead let's put $v^1$, $v^2$, $v^3$, $v^4$, $v^5$.
% after quantization, remove the <audio-..> etc, all of them. instead, put $c_{29}^1$, $c_{7}^2$, $c_{98}^3$, $c_{361}^4$, $c_{59}^5$ (as in the main text.)
% replace the text "Codebook Lookup" with "Token Mapping". after token mapping, put the text tokens like <|audio-29|> and the others.
% change the order of modalities in the image, as we do: playlist - semantic - metadata - lyrics - audio

As illustrated in Figure~\ref{fig:talkplay-overview}, \modelname consists of two parts -- a music tokenizer and a model. The music tokenizer encodes multimodal music embeddings into tokens. A single music item is represented by multiple tokens, each of which represents a different modality. The model is a usual LLM that is presumably: i) capable of modeling music recommendation conversations (with text and music tokens), and ii) trained with a large amount of data including musical corpus for faster training and generalization.

\subsection{Music Tokenizer}\label{subsec:system-music-token}
The music tokenizer in \modelname is designed under following goals and constraints. First, to retrieve and recommend items, the model needs to be connected to the database. Second, to easily utilize LLM as a recommendation engine, the items in the database need to be discretized as tokens. Third, the embeddings of those item tokens need to be trained well, requiring some minimum token occurrence in the training data.
Given these conditions, we propose the following scheme for music tokenization in \modelname. A music item $x_i$ ($i$ is an item index) may have rich information including (1) playlist co-occurrence, (2) semantic tag information, (3) metadata, (4) lyrics, and (5) audio. 

% \textbf{Data}: A music item $x_i$ ($i$ is an item index) may have rich information including (1) audio, (2) metadata, (3) lyrics, (4) semantic information, and (5) \sh{playlist}-item interaction. 

% These are fairly separate from each other, although there would be some correlations. The audio is a 30-second preview clip. % provided by the Spotify API
% Metadata includes song title, artist name, album name, and released year. Lyrics are either provided in a given dataset or extracted using Whisper, which can transcribe multilingual lyrics reasonably~\ref{radford2023robust}. Semantic information includes music tags by human, music classification model, or music captioning model [ref, ref, ref]. User-item interaction is information about which user listened to which items, provided in music playlist dataset \cite{chen2018recsys} {and etc etc TODO}.

\textbf{Item Embeddings}: For all the data modalities, pretrained models are used to extract item embedding $v_i^m$, where $m$ stands for a modality index (1-5 for playlist co-occurrence, semantic tag, metadata, lyrics, and audio, respectively.) For playlist co-occurrence, a word2vec-style~\cite{barkan2016item2vec} item embedding model is trained using the training dataset ($v_i^1 \in \mathbb{R}^{128}$). For semantic information, metadata, and lyrics, NV-Embed-v2~\cite{lee2024nv} is chosen as its state-of-the-art performance in various text embedding benchmarks~\cite{muennighoff2022mteb} ($v_i^2, v_i^3, v_i^4 \in \mathbb{R}^{4096}$).  For audio, the MusicFM~\cite{won2024foundation} model is chosen for its strong performance in many music informatics tasks ($v_i^5 \in \mathbb{R}^{1024}$). 


\textbf{Quantization}: Due to the long-tail nature of music consumption and the limited size of available training data, all the item embeddings $v_i^m$ are vector-quantized into $K$ clusters (per modality) using K-means clustering. Here, $c_k^m$ represents a cluster centroid. In this paper, we chose $K=1024$.

As a result, the overall music database is represented by $K \times M$ indices, where the total number of modalities $M$ is 5. However, this does not mean that all music items are reduced to $1024 \times 5=5120$ unique clusters. A music item is represented by 5 modality tokens, where each modality embedding has 1024 clusters. Theoretically, the proposed music tokenization scheme is capable of representing $1024 ^ 5$ = about 1,126 trillion unique music items.

\textbf{Token Mapping}: In this final step of tokenization step, the multimodal cluster indices of each music item is used in our token format: \texttt{<|modality-index|>}. For example, a music item whose embeddings are mapped to 29th audio cluster, 2nd metadata cluster, 98th lyrics cluster, 3th semantic cluster, and 59th user-item cluster is converted to \texttt{<|playlist-59|>}\texttt{<|semantic-361|>}
\texttt{<|metadata-7|>}\texttt{<|lyrics-98|>}\texttt{<|audio-29|>}. 

This token mapping is simple and intuitive, but there is a caveat. Although powerful models are used to compute item embeddings, its rich semantic information is simplified by the cluster indices, i.e., every detailed information of the inter-cluster relationship is lost. This will be addressed in Section~\ref{subsec:exp-simloss} with the introduction of a self-similarity loss.

\subsection{LLM: The Recommender}\label{subsec:system-llm}
The LLM, our recommendation engine, is based on a pretrained open-source model. We opt for instruction finetuned model to utilize the conversationability as our goal is to build a conversational music recommender. To better understand the cultural background of various genres and languages, a multi-lingual model is preferred. At the same time, our task is highly specific compared to the scope of usual LLM training, leaving room for optimization. Altogether, Llama-3.2-1B-Instruct is chosen for our base model~\cite{dubey2024llama}. 

To enhance the LLM's understanding of multimodal music information, we expand its vocabulary to include multimodal music tokens~\cite{zhao2024llama}. In detail, the vocabulary size of the base model is expanded by $1024 \times 5=5120$, to include the multimodal music tokens. Two more special tokens, \texttt{<start\_of\_music>} and \texttt{<end\_of\_music>}, are added as well, since they are used to wrap music tokens in the training data. This modification means 5122 new embeddings ($e_k^m$ and two special token embeddings) to the model, each of which is 2048-dim in the Llama-3.2-1B model. Equivalently, it adds about 10.5M parameters to the model, increasing the model size by less than 1\%. With this vocabulary expansion, the synthetic conversation (as in Section~\ref{subapp:synth-convo}), represented with the multimodal music tokens (as in Section~\ref{subapp:synth-final}), is tokenized and fed to the model. This expansion allows the model to directly understand and generate multimodal music tokens along with regular text tokens during conversations.


\subsection{Training}\label{subsec:system-training}
The training of \modelname is similar to the typical LLM supervised finetuning, using the cross-entropy loss $\mathcal{L}_{\text{conv}}$ and the causal attention. The synthetic conversational data (Section~\ref{subsec:maintask-synthe-music-convo}) is fed into the model, after replacing the track identifies with the corresponding multimodal music tokens. The primary objective of this training process is to learn the newly introduced multimodal music token embeddings $e_k^m$ by leveraging contextual information from preceding conversations and music items. As briefly discussed in Section~\ref{subsec:system-music-token}, the added music token embeddings $e$ are initialized with the mean and covariance of the existing embeddings~\cite{hewitt2021initializing}. As a result, the model learns to make music recommendation by generating multimodal music tokens, as well as appropriate natural language responses. 

In addition to the synthetic conversation dataset, we incorporate playlist continuation data as an next-token prediction task also trained on the cross-entropy loss, $\mathcal{L}_{\text{playlist}}$. This can be seen as a pretraining task on a music token corpus, providing supervision for learning both intra-track and inter-track relationships. The intra-track relationships are learned through the connections between the five multimodal music tokens representing each track, while the inter-track relationships are captured through playlist co-occurrence patterns. We split the given playlist dataset into two versions based on random select offsets, where the task is to predict the next track sequence given a track sequence.

The final cross-entropy loss $\mathcal{L}_{\text{CE}}$ consists of two components: conversation loss $\mathcal{L}_{\text{conversation}}$ and playlist continuation loss $\mathcal{L}_{\text{playlist}}$, which are jointly optimized as:

\vspace{-3mm}
\begin{equation}
\mathcal{L}_{\text{CE}} = \mathcal{L}_{\text{conversation}} + \mathcal{L}_{\text{playlist}}
\label{eq:celoss}
\end{equation}
\vspace{-3mm}

where $\mathcal{L}_{\text{conversation}}$ is computed from the synthetic conversation dataset and $\mathcal{L}_{\text{playlist}}$ from the playlist continuation task. Both losses are standard cross-entropy losses for next-token prediction.

While the cross-entropy loss with vocabulary expansion successfully integrates multimodal tokens into the LLM, the newly added token embeddings lose the semantic relationships present in the original modality embeddings. This is because the added music token embedding $e$ is randomly initialized, although it is necessary since the semantic space of the base LLM embeddings are completely irrelevant to the semantics of $c$. To address it, a self-similarity loss is adopted as in Eq.~\eqref{eq:simloss}. By measuring the self-similarity of the token embedding $e$ and the cluster centroids $c$, during training, the token embedding is optimized for not only the music recommendation but also the information restoration.

\vspace{-4mm}
\begin{equation}
\mathcal{L}_{\text{self-sim}} = \mathbb{E}_{i,j} |S(\hat{c}_i, \hat{c}_j) - S(\hat{e}_i, \hat{e}_j)|
\label{eq:simloss}
\end{equation}
\vspace{-4mm}

where $\hat{c}_i = \frac{c_i}{\|c_i\|_2}, \hat{c}_j = \frac{c_j}{\|c_j\|_2}$ are L2-normalized centroids, $\hat{e}_i = \frac{e_i}{\|e_i\|_2}, \hat{e}_j = \frac{e_j}{\|e_j\|_2}$ are L2-normalized token embeddings, and $S(x,y) = x^T y$ is dot product between normalized vectors.
The total loss combines the cross-entropy loss for language modeling with the self-similarity loss, $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{CE}} + \lambda \mathcal{L}_{\text{self-sim}}$, where $\lambda=0.1$ to balance between the tasks.

% todo: mention later: without sim loss, after 1epoch it overfits. with it, it learns for 9epochs and still stable and slowly decreasing, reaching lower val loss. 
% \subsection{Inference: Recommendation by Generation} \label{subsec:system-inference}
% During an inference stage, we use the usual LLM inference pipeline and parameters to generate with temperature=1.0, top-p filtering ($p$=0.9), repetition penalty of 1.0. As in the training data, the recommendation is done by generating multimodal music tokens. 

% Note that these multimodal tokens are generated first, followed by the asisstant's message (e.g., ``Here is a song titled XX by YY.") This generation order is chosen to allow the model to make recommendation only based on the query. With the opposite order (message first, then multimodal music tokens), the recommendation would be based on not only the user query but also the message it generated. It may not be ideal, because the message can be popularity-biased, as observed when LLMs are used as a zero-shot recommendation system \cite{he2023large}.

% Once the multimodal music tokens are generated, they need to be mapped back to the music items -- a backward flow of the music tokenization in Section~\ref{subsec:system-music-token}. As our clustering and quantization is many-to-one mapping, this token-to-item process is stochastic. This is in line with many industry-level recommenders, where approximate nearest neighbor is used for scalability, e.g., \cite{spotify_annoy}. 

% In practice, given a series of multimodal music tokens (length of 5 corresponding to a single recommendation), the song items that share the same token index in each modality are selected. Since the space (1,126 trillion unique number of cases) is significantly larger than the number of music item in our (or any) dataset, the item distribution is sparse. In this case, the system falls back to partial matching using feature importance weighting. We assume the importance is in the order of playlist, tags, metadata, lyrics, and audio; assign $5^2, 4^2, 3^2, 2^2, 1^2$ respectively; iteratively search for matches using different combinations of tokens while prioritizing combinations with higher cumulative importance scores. This approach ensures that when exact matches are unavailable, the system finds the closest possible items while preserving the most critical aspects of the recommendation. The weighting scheme particularly emphasizes playlist co-occurrence, which typically carry the strongest indicators of user preference in music recommendation systems.

\subsection{Inference: Recommendation by Generation} \label{subsec:system-inference}
During inference, we use standard LLM generation parameters (temperature=1.0, top-p=0.9, repetition penalty=1.0) to generate multimodal music tokens. These tokens are generated before the assistant's response message to ensure recommendations are based solely on the query, avoiding potential popularity biases that could arise from conditioning on generated text \cite{he2023large}.

The generated tokens must then be mapped back to music items through a reverse lookup of the tokenization process described in Section~\ref{subsec:system-music-token}. As our clustering creates a many-to-one mapping, this process is stochastic, similar to approximate nearest neighbor searches used in industry recommenders \cite{spotify_annoy}.

Given a sequence of five multimodal tokens representing a recommendation, we select items matching the token indices across modalities. However, due to the vast token space (1,126 trillion combinations) relative to our dataset size, exact matches are often unavailable. In such cases, we fall back to partial matching using weighted feature importance, with quadratically decreasing weights (25,16,9,4,1) assigned to playlist, tags, metadata, lyrics, and audio features respectively. The system iteratively searches for matches using token combinations prioritized by cumulative importance, ensuring recommendations preserve the most critical signals, particularly playlist co-occurrence patterns.

\vspace{-1mm}
\section{Experiment}
In this section, we present the experiment results focusing on the training. Evaluation on the recommendation of \modelname is discussed in Section~\ref{sec:results}.
Using the synthetic conversation dataset (Section~\ref{sec:data}), \modelname, a vocabulary-expanded Llama-3.2-1B model is trained with a 1024-token context length, a learning rate of $1e-4$, and the AdamW optimizer \cite{loshchilov2017decoupled}. The model reported in this paper is trained on 8 GPUs with a total batch size of 48 items (or 49,152 tokens). The model is trained over 2.3M steps, and the one with the lowest validation loss at step 2.1M is selected as the best checkpoint for evaluation. 

\subsection{Training Stability and Self-Similarity Loss} \label{subsec:exp-simloss}
Although the token count in a single batch (49,152) is smaller than typical LLM pretraining (E.g., 2 million tokens in Llama 3~\cite{dubey2024llama}), the training was stable. We hypothesize this due to two reasons: First, for the existing text tokens, the finetuning stages usually do not require a large batch size; Second, for the multimodal music tokens, the self-similarity loss works a regularizer and stablizes the training. 


\begin{figure}[t]
   \centering
   \includegraphics[width=\linewidth]{loss_comparison.pdf}
   \vspace{-10mm}
   \caption{Training curves showing cross-entropy loss (top) and similarity loss (bottom). The similarity loss is identical across splits because it is computed on the token embedding, independently from any conversation data.}
   \vspace{-5mm}
   \label{fig:training}
\end{figure}

\begin{table*}[!t]
\centering
\begin{tabular}{llrrrr}
\toprule
Model              & Retrieval Methods & MRR $\uparrow$   & Hit@1 $\uparrow$ & Hit@10 $\uparrow$ & Hit@100 $\uparrow$ \\
\midrule
BM25~\cite{robertson1999okapi}               &  TF-IDF Scoring            & 0.005 & 0.001 & 0.009  & 0.037   \\
CLAP~\cite{wu2023large}               & Similarity-based  & 0.000 & 0.000 & 0.000  & 0.003   \\
NV-Embeds~\cite{lee2024nv}          & Similarity-based  & 0.020 & 0.007 & \textbf{0.042}  & 0.133   \\
\modelname (Ours) & Generative        & \textbf{0.023} & \textbf{0.014} & 0.036  & \textbf{0.142}   \\
\bottomrule
\end{tabular}
\caption{Comparison of conversational recommendation performance across different models. MRR indicates the mean reciprocal rank and Hit@K measures proportion of relevant items in top-K results. Higher is better for all metrics.}
\vspace{-3mm}
\label{tab:retrieval-results}
\end{table*}

The regularization effect of the self-similarity loss is illustrated in Figure~\ref{fig:training} (top). The model trained with self-similarity loss exhibits a consistent and small gap between validation and training cross-entropy losses, with both losses decreasing steadily over time. In contrast, without the self-similarity loss, the training loss rapidly decreases while the validation loss reaches its minimum within 25k steps, after which it keeps increasing, indicating overfitting.

The self-similarity loss curves in Figure~\ref{fig:training} (bottom) provide additional evidence. In the model without self-similarity loss, this metric consistently increases as overfitting occurs. This demonstrates that not only does a low self-similarity loss lead to low cross-entropy loss, but the inverse relationship also holds, empirically showing their close connection.



\subsection{Token Matching Analysis}
\label{subsec:token-matching}

The effectiveness of \modelname's token representation can be evaluated through the analysis of token matching patterns during inference. Given the vast combinatorial space of possible token sequences (1,126 trillion unique combinations) and a test set of only 39,471 tracks, random matching would yield practically zero matches. However, our analysis of 13,012 conversation turns reveals that 49.7\% of generated token sequences successfully match to existing items, indicating that the model has learned meaningful associations between multimodal tokens.

This non-random matching behavior demonstrates that \modelname has effectively captured the intrinsic relationships between different modalities of music items. More specifically, the model has learned valid combinations of collaborative filtering patterns, audio characteristics, metadata, lyrics, and content attributes that correspond to real music items.

Further analysis reveals interesting patterns in warm (seen during training) versus cold (unseen) items. While warm items constitute the majority of matches (93.6\%), this is primarily due to the imbalanced distribution in the test set (37,114 warm vs 2,357 cold items). When normalized by their respective test set sizes, warm and cold items show comparable matching rates - 16.5\% for warm items and 14.5\% for cold items. This near-equal utilization suggests two important properties of our model: (1) it successfully learns meaningful multimodal patterns from the training data, and (2) this understanding generalizes to unseen items, indicating that the model has captured fundamental relationships between modalities rather than merely memorizing training examples.

The substantial proportion of zero-match cases (50.3\%) can be attributed to the extreme sparsity of our test set relative to the possible token space. This suggests that expanding the size of the music catalog would likely improve the raw matching rate without compromising the model's demonstrated ability to generate valid multimodal token sequences.

% quantifies the discrepancy between the learned multimodal token embeddings in the LLM and their corresponding original embeddings. The results demonstrate that our self-similarity loss successfully minimizes this discrepancy, effectively aligning the two embedding distributions.



% \begin{figure}[t]
%    \centering
%    \includegraphics[width=\columnwidth]{modality_acc.pdf}
%    \caption{Top-5 accuracy for different modality embeddings during training. This metric indicates how well the token embeddings preserve the neighborhood structure of original embeddings - specifically, the average overlap between 5 nearest neighbors in both original ($c^m$) and token embedding spaces ($e^m$).}
%    \label{fig:modality}
% \end{figure}

% Figure~\ref{fig:modality} presents how the multimodal music tokens learn their embeddings $e_k^m$ during training, by measuring the overlap between 5 nearest neighbors with respect to the original vectors $c^m_k$ for all $k$. 

% \begin{table}[!t]
% \centering
% \label{tab:playlist-continuation}
% \begin{tabular}{lrrr}
% \toprule
% Models              & H@10           & H@100          & H@200          \\ \midrule
% \multicolumn{4}{l}{{\color[HTML]{9B9B9B} \textit{Similarity-based Retrieval}}}         \\
% Random                & 0.001          & 0.007          & 0.010          \\
% Collaborate Filtering & \textbf{0.243} & \textbf{0.521} & \textbf{0.624} \\
% Metadata              & 0.191          & 0.510          & 0.600          \\
% Attributes            & 0.013          & 0.086          & 0.128          \\
% Audio                 & 0.045          & 0.187          & 0.268          \\
% Lyrics                & 0.033          & 0.150          & 0.221          \\
% \midrule
% \multicolumn{4}{l}{{\color[HTML]{9B9B9B} \textit{Generative Retrieval}}}          \\
% TalkPlay (Ours)         & 0.173          & 0.350          & 0.417          \\
% + Self-Sim Loss     & 0.175          & 0.394          & 0.484         \\ \bottomrule
% \end{tabular}
% \caption{Playlist continuation performance comparison. Hit Rate (H@K) measures the proportion of relevant items in top-K recommendations. Higher is better.}
% \end{table}



\section{Conversational Recommendation} \label{sec:results}

\subsection{Evaluation Metrics} \label{subsec:exp-eval}

We evaluate our model's conversational recommendation capabilities along two key dimensions: (1) the ability to retrieve relevant music given multi-turn dialogue context, and (2) the ability to generate contextually appropriate responses. For the retrieval evaluation, we employ standard ranking metrics - Mean Reciprocal Rank (MRR) and Hit Rate at K (Hit@K) with K = \{1, 10, 100\}. MRR measures the average reciprocal of the rank at which the first relevant item appears, while Hit@K indicates whether a relevant item appears in the top-K retrieved results. We evaluate retrieval performance for each turn using 1,000 multi-turn conversations derived from 1,000 playlists in Table~\ref{tab:dataset-stats}.

% For the response generation evaluation (2), we analyze the model's responses to assess how well they reflect user queries beyond simple token overlap~\cite{} or embedding distance metrics~\cite{}. We examine whether the generated responses demonstrate appropriate understanding of the user's intent, maintain contextual coherence across turns, and provide relevant music recommendations that align with the expressed preferences and constraints.

\subsection{Baseline Models} \label{subsec:exp-eval}
We evaluate our model against three baseline approaches: BM25~\cite{robertson1999okapi} for classical text-based sparse retrieval, NV-Embeds-V2~\cite{lee2024nv} for deep similarity retrieval with LLM embeddings, and CLAP~\cite{wu2023large} for multimodal audio-text retrieval. See Appendix~\ref{appendix_a_4} for more details.


\begin{table*}[t]
\centering
\begin{tabular}{lll}
\toprule
Turn & Types & Contents \\ \midrule
\multicolumn{3}{l}{{\color[HTML]{9B9B9B} \textit{Chat History (Input Condition)}}} \\
1 & Query & I am looking for a mix of songs for a \sh{road trip} playlist... \sh{folk, indie pop}, and \sh{mellow} tracks. \\
1 & Music & ``Just A Game" by ``Birdy" (Last.fm Tag: \sh{indie, folk}, alternative, piano) \\
1 & Response & Let's start with a beautiful \sh{indie folk} song with some haunting vocals \\
2 & Query & That was lovely and melancholic, now something a little more \magenta{upbeat} and \magenta{folk-inspired}, perhaps? \\ \midrule
\multicolumn{3}{l}{{\color[HTML]{9B9B9B} \textit{Ground Truth}}} \\ 
2 & Music & ``Tomorrow Will Be Kinder" by ``The Hunger Games Soundtrack" (Last.fm Tag: \magenta{folk}, soundtrack) \\
2 & Response & How about this folk-pop track with a \magenta{brighter}, more \magenta{energetic} feel? \\ \midrule
\multicolumn{3}{l}{{\color[HTML]{9B9B9B} \textit{\modelname Generataed Output}}} \\
2 & Music & ``Let Her Go" by "Passenger" (Last.fm Tag: \magenta{folk}, acoustic, 2021) \\
2 & Response & Here's a song with a more \magenta{uplifting folk sound}. \\
\bottomrule
\end{tabular}
\caption{Example multi-turn music recommendation dialogue demonstrating the model's ability to adapt recommendations based on evolving user preferences, transitioning from initial \sh{folk, indie pop, and mellow} criteria to subsequent \magenta{upbeat and folk-inspired} requests.}
\label{tab:dialogue-example}
\vspace{-1mm}
\end{table*}

\subsection{Multi-turn Music Retrieval Results}
Table~\ref{tab:retrieval-results} presents the multi-turn music retrieval performance across different models. The LLM embedding-based NV-Embeds-V2~\cite{lee2024nv} significantly outperforms the traditional sparse retrieval approach BM25~\cite{robertson1999okapi} while operating in the same text modality. Despite its strong performance on audio-text alignment tasks~\cite{wu2023large}, CLAP shows near-zero performance in multi-turn retrieval scenarios, indicating limitations in handling music metadata-related queries, such as track and artist names, as it was primarily trained on semantic annotation data, including genre and instrument information.

Our generative retrieval approach \modelname demonstrates superior performance across almost all metrics compared to baseline methods. Most notably, it achieves nearly double the Hit@1 score compared to the embedding similarity-based NV-Embeds-V2~\cite{lee2024nv}.
This precision-seeking behavior is aligned with the training objective, as the next-token prediction with cross-entropy encourages the single true class while discouraging all the rest.
The relatively low Hit@10 may be due to our inference strategy when there is no exact match. Dropping each modality, as we do in such a case, is an approximate nearest neighbor method, and using it at k=10 seems to be somewhat noisy in our test setup. 
The Hit@100 performance further validates that our approach not only excels at top-1 prediction but also learns meaningful relevance rankings across a broader recommendation set.

\begin{figure}[t]
   \centering
   \includegraphics[width=\linewidth]{figs/mrr_line_comparison.png}
   \vspace{-5mm}
   \caption{Performance comparison across conversation turns. The x-axis shows the turn number in the dialogue, and the y-axis shows Mean Reciprocal Rank (MRR).}
   \vspace{-2mm}
   \label{fig:turn_wise_eval}
\end{figure}

Figure~\ref{fig:turn_wise_eval} shows how retrieval performance varies across conversation turns. The MRR scores demonstrate that \modelname maintains relatively stable performance even as the conversation context grows longer, while baseline methods show rather degradation in later turns. This demonstrates the superior context understanding capabilities of the proposed LLM-based token prediction model. While similarity-based methods merely aggregate previous chat history for dot product, our generative retrieval approach leverages the model's internal attention mechanism during item decoding to naturally aggregate contextual information. % This architectural difference explains why \modelname maintains strong performance even with longer conversation histories, while other methods show degraded performance after the third turn as their simple aggregation strategies struggle to capture complex multi-turn dependencies.





\subsection{Qualitative Results}

Table~\ref{tab:dialogue-example} illustrates an example of multi-turn music recommendation. The first row shows the multi-turn queries that serve as conditions for the LLM, the second row presents the ground truth from our synthetic dataset, and the final row displays the LLM-generated output response and top-1 recommended music. Through this qualitative example, we can observe that our LLM demonstrates the ability to understand user queries at each turn and retrieve increasingly specific music recommendations that adapt to evolving user needs.

In the initial turn, the user specifies preferences for a ``road trip'' playlist with ``folk, indie pop, and mellow'' characteristics. The model's recommendation aligns with these criteria by suggesting a track with indie folk and mellow attributes. When the user subsequently requests a shift toward ``more upbeat and folk-inspired'' music, the model demonstrates contextual understanding by adapting its recommendation - maintaining the folk element while transitioning to a more energetic selection, evidenced by both the retrieved track and the generated descriptive response.

\section{Conclusion}
We presented \modelname, demonstrating that music recommendation can be effectively reformulated as token generation within an LLM framework. Our key contributions - multimodal tokenization, self-similarity loss, and unified conversational recommendation model - enable stable training and competitive performance while significantly reducing system complexity. The empirical results show that this approach maintains recommendation quality across different modalities while eliminating the need for separate dialogue management and retrieval modules. Future work could explore scaling to larger datasets, incorporating additional modalities, and leveraging unique LLM capabilities like chain-of-thought reasoning and zero-shot scenarios. As one of the first unified recommendation systems within the LLM framework, \modelname demonstrates the potential for simpler yet more capable architectures.


% As one of the first unified recommendation systems within the LLM framework, \modelname demonstrates the potential for simpler yet more capable architectures.


% We presented \modelname, demonstrating that music recommendation can be effectively reformulated as token generation within an LLM framework. Our key contributions - multimodal tokenization, self-similarity loss, and unified recommendation-dialogue architecture - enable stable training and competitive performance while significantly reducing system complexity. The empirical results show that this approach maintains recommendation quality across different modalities while eliminating the need for separate dialogue management and retrieval modules.

% The attempt of \modelname opens several research directions. Immediate improvements would focus on scaling: larger datasets, a longer context length, adaptive clustering schemes where popular items receive dedicated tokens, and incorporation of additional modalities such as release year, language, or visual information. More fundamentally, our token-based framework could leverage unique LLM capabilities - using decoding parameters for exploration-exploitation control, implementing chain-of-thought reasoning for recommendation explanation, or extending to zero-shot scenarios. For personalization, promising directions include encoding user profiles as tokens and developing cold-start strategies for both items and users. As one of the first systems to unify recommendation entirely within the LLM framework, \modelname demonstrates the potential for simpler yet more capable recommendation architectures that fully leverage the semantic understanding of language models.

% \newpage  % impact statement 8   
\section*{Impact Statement}
\modelname introduces a novel approach to conversational music recommendation that could impact both user experience and music discovery. By unifying dialogue and recommendation through language models, our work may democratize music discovery by enlarging the recommendation pool. However, the system's recommendations may be still influenced by biases present in the training data, potentially affecting artist exposure and representation. Additionally, while our token-based approach helps protect user privacy by avoiding direct embedding storage, practitioners implementing such systems should carefully consider data handling practices and user consent mechanisms. We believe our technical contributions advance the field of machine learning while encouraging thoughtful consideration of recommendation system design's impact on users, artists, and the broader music ecosystem.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Synthetic Data} \label{app:synth}
\subsection{Prompt to Google Gemini to synthesize conversation}\label{subapp:synth-prompt}

% after adding 5127 more,
% which is 2 bom, eom
% then 1025 x 5 tokens
% including -1 index 
% new token size: 133384


\begin{lstlisting}[style=docstring]
    You are an AI assistant specializing in music. Generate an ANSWER to the QUESTION in valid LIST OF JSON:
- role: one of {"user", "assistant", "music"}
- content: 
  - for "user": string of conversation text for music request
  - for "assistant": string of conversation text for answer to user's request.
  - for "music": string of unique track_id (i.e., "3pzjHKrQSvXGHQ98dx18HI", "7vFv0yFGMJW3qVXbAd9BK9")

1. Conversation Start:
- The conversation begins with a user query.
  - This query can be a specific music item request (i.e., "superstition by steve wonder")
  - This query can be a natural language request (i.e., "party anthem with energetic rhythm", "play some 90s rock music")
- The assistant should respond with two consecutive objects:
  - A text response (role: "assistant") providing a brief reason for the recommendation
  - A music recommendation (role: "music") containing the `track_id`

2. Assistant Recommendations:
 - The assistant should recommend just one song per single turn, one that shares common musical traits (i.e, genre, artist, mood, theme, tempo) with the query song.
 - The goal is to syntehsize an engaging conversation and allow the user to explore music based on their request.
 - The assistant's music recommendations prioritize music tracks and tags, and ignore non-musical words. If there are captions and lyrics, use them as optional information as needed.

3. User Satisfaction:
 - In each turn, the user may or may not be satisfied with the recommendations and simply listen.
 - The user may express agreement with the recommendation (e.g., "This is great!").

4. User Direction Change:
 - In the real-world scenario, sometimes users are not happy with the recommendation. We need to include it in our synthetic conversation. 
 - In such a case, the user would express dissatisfaction with a recommendation.
 - The user may specify a new direction or trend of the recommended songs. (e.g., "Play something different", "Play something more upbeat").
 - Use the songs in the "NEGATIVE SET" to synthesize this recommendation that user does not like.
 - This helps evaluate how the assistant responds to negative feedback and adapts its recommendations.

5. assistant Adaptation:
The assistant should provide the next music track based on the following factors:
 - The previous conversation.
 - The current music being played.
 - The user's recent requests.
 - The assistant should recommend at least half of the songs in the provided playlist.

7. JSON Format:
 - The conversation should be formatted as a list of JSON objects.
 
8. Example:
QUESTION : 
Music Playlist:
{'5tkDCf3xKRZjT5RbCb3sZB': "Metadata:\n  Title: Big Amount\n  Artist: 2 Chainz, Drake\n  Album: Pretty Girls Like Trap Music, Industry Kings 2\nTag: drake, Hip-Hop/Rap, rap\n  Popularity: 55.0\nLyrics:  Pat downs, [REST OF THE LYRICS ARE OMITTED]\nPseudo Caption: This music features a hip hop song that consists of a flat male vocal rapping over shimmering hi hats, claps, punchy kick, groovy bass, synth lead melody and tinny hi hats. It sounds energetic, exciting and addictive - like something you would hear in clubs.\nTempo Estimate: 134.13\nKey Estimate: G# minor", 
[REST OF THE TRACKS ARE OMITTED]}

Negative Set:
{'7bJ4mu7MHa3rHiNyKjOoSl': 'Metadata:\n  Title: Ice Melts\n  Artist: Young Thug, Drake\n  Album: More Life\nTag: young thug, Hip-Hop/Rap\n  Popularity: 60.0\nLyrics:  You need to give it up to someone. [REST OF THE LYRICS ARE OMITTED]\nPseudo Caption: This music features a hip hop song that consists of a passionate male vocal, alongside harmonizing background vocals, singing over shimmering hi hats, punchy kick and claps. It sounds groovy and addictive - like something you would hear in clubs.\nTempo Estimate: 124.51\nKey Estimate: A major', 
[REST OF THE TRACKS ARE OMITTED]}

ANSWER:
[
  {"role": "user","content": "Play something like 'Big Amount' by 2 Chainz and Drake"},
  {"role": "assistant","content": "Here's a similar track with a similar vibe, featuring a catchy hook and hard-hitting flow."},
  {"role": "music","content": "4ckuS4Nj4FZ7i3Def3Br8W"},
  {"role": "user","content": "This is great! Can you play something more upbeat, but still in a hip hop style?"},
  {"role": "assistant","content": "Here's a track with a faster tempo and energetic rhythm, perfect for keeping the party going."},
  {"role": "music","content": "0B8QzDH7YWih85V5SEMnyJ"},
  {"role": "user","content": "I'm not feeling this one, play something different"},
  {"role": "assistant","content": "I understand. How about something with a bit more of a soulful feel, but still with a good groove?"},
  {"role": "music","content": "0PXp9QxBfSKGGpWUNaws2X"},
  {"role": "user","content": "Yeah, that's better, this is more my style"},
  {"role": "assistant","content": "Great choice! Let's keep it in this vein, with a focus on smooth vocals and melodic rhythms."},
  {"role": "music","content": "0H8XeaJunhvpBdBFIYi6Sh"},
  {"role": "user","content": "This is so good, this mood is perfect"},
  {"role": "assistant","content": "I'm glad you're enjoying it! Here's another track that leans into the same kind of soulful vibe."},
  {"role": "music","content": "3LwYGYpsg5SQioMX5BwFAz"},
  {"role": "user","content": "This is really good! One last request, something a little more intense with a little more aggression"},
  {"role": "assistant","content": "You got it. Here's a track with a harder edge and more energetic production. Get ready to bump!"},
  {"role": "music","content": "0utlOiJy2weVl9WTkcEWHy"}
]
\end{lstlisting}

After this prompt, the track identifiers and their metadata is provided in a similar form shown under ``8. Example". 

\subsection{Synthetic Conversation Example}\label{subapp:synth-convo}

This is one of the synthetic conversation results. 
\begin{lstlisting}[style=json]
[{"role": "user",
  "content": "I'm looking for some mellow acoustic tracks to relax to. Can you suggest something?"},
 {"role": "music",
  "content": "3kZC0ZmFWrEHdUCmUqlvgZ"},
 {"role": "assistant",
  "content": "I think I Will Follow You into the Dark by Death Cab for Cutie would suit your mood. It's known for its emotional depth and gentle acoustic sound."},
 {"role": "user",
  "content": "That was beautiful, but a little too somber.  How about something with more energy, still in the alternative rock genre?"},
 {"role": "music",
  "content": "3mcG2NI5G5vhrQtRda1YnA"},
 {"role": "assistant",
  "content": "Okay, let's try something more upbeat. The Middle by Jimmy Eat World is a popular alternative rock anthem that offers a good balance of energy and emotion."}]
\end{lstlisting}


\subsection{After Music Tokenization}\label{subapp:synth-final}

This is after applying the music tokenization process in Section~\ref{subsec:system-music-token}. The music identifiers are converted to a sequence of multimodal music tokens. This is the text data fed to the LLM.

\begin{lstlisting}[style=json]
"<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 20 Jan 2025\n\nYou are a music expert. Your task is conversational retrieval: Predict the next music item based on the given chat history.<|eot_id|>
<|start_header_id|>user<|end_header_id|>\n\nI'm looking for some mellow acoustic tracks to relax to.  Can you suggest something?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<start_of_music><|playlist-53|><|semantic-559|><|metadata-895|><|lyrics-654|><|audio-268|><end_of_music>I think I Will Follow You into the Dark by Death Cab for Cutie would suit your mood. It's known for its emotional depth and gentle acoustic sound.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThat was beautiful, but a little too somber.  How about something with more energy, still in the alternative rock genre?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<start_of_music><|playlist-86|><|semantic-165|><|metadata-767|><|lyrics-158|><|audio-743|><end_of_music>Okay, let's try something more upbeat. The Middle by Jimmy Eat World is a popular alternative rock anthem that offers a good balance of energy and emotion.<|eot_id|>"
\end{lstlisting}

\subsection{Comparison Music Recommendation Models}~\label{appendix_a_4}

We compare our approach against three representative baselines spanning different paradigms of music retrieval:

\textbf{BM25}~\cite{robertson1999okapi} represents a classical text-based sparse retrieval approach. It utilizes term frequency-inverse document frequency (TF-IDF) scoring to match user queries with music metadata and semantic tags.~\footnote{\{Title\} by \{Artist\} from \{Album\} \{Semantic tags\} \{Year\}} While computationally efficient and interpretable, this method relies solely on lexical matching and cannot capture semantic relationships or cross-modal patterns.

\textbf{NV-Embeds-V2}~\cite{lee2024nv} advances beyond sparse retrieval by implementing a text-based deep similarity retrieval framework. The model learns dense neural embeddings for both queries and music descriptions, enabling semantic matching that can capture latent relationships beyond exact word matches.This model represents a strong baseline as an LLM-based embedding extractor model that has demonstrated superior retrieval performance on the MTEB benchmark.

\textbf{CLAP}~\cite{wu2023large} extends the similarity-based paradigm to multiple modalities by jointly embedding audio and text. Through contrastive learning, it aligns the representations of music audio with corresponding textual descriptions, enabling cross-modal retrieval. This model represents the current state-of-the-art in audio-text music retrieval, providing a strong baseline for multimodal understanding.

Compare to other methods, \modelname takes a fundamentally different approach by reformulating music retrieval as a generative task. Rather than learning separate embeddings, it unifies dialogue understanding and recommendation through token generation within a language model framework. 


\end{document}

