\section{Semi-Synthetic Experiment: Fine-Tuning Llama-3-8B on HH-RLHF}
\label{app:llama}

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figs/rews.pdf}
    \caption{Reward definition for three user types in semi-synthetic experiments (\cref{subsec:exp:semi-synth}) based on the length of prompt response combination. The first user type prefers long prompt response combinations, the second user type prefers short prompt response combinations, and the third user type prefers mid-length prompt response combinations. The dashed cyan line shows the average reward across the three user types.}
    \label{fig:app:rewards}
\end{figure}

\paragraph{Reward Models.}
\cref{fig:app:rewards} shows the three distinct rewards we use for the three user types along with their average.
In order to have a reliable ground-truth reward which we can rely on in evaluation, we define these rewards as functions of the number of tokens in prompt-response combinations.

\paragraph{Anonymous Dataset.}
We use prompts and response pairs from both helpfulness and harmlessness subsets of Anthropic's HH-RLHF dataset~\citep{hh-rlhf} and relabel the \textit{chosen} and \textit{rejected} responses manually.
We filter for data points in which the sum of the number of tokens in the prompt and the number of tokens in the longer response do not exceed $512$.
This leaves us with $160,800$ training and $17,104$ test data points. 
For every data point (a prompt with a pair of responses), we sample one of the three user types uniformly at random.
Given the type of user, we sample a preference based on BT~\citep{bt} to label the two alternatives.

\paragraph{Dataset with Maximum Annotator Information.}
We use prompts and response pairs from both helpfulness and harmlessness subsets of Anthropic's HH-RLHF dataset~\citep{hh-rlhf} and relabel the \textit{chosen} and \textit{rejected} responses manually.
We filter for data points in which the sum of the number of tokens in the prompt and the number of tokens in the longer response do not exceed $512$.
This leaves us with $160,800$ training and $17,104$ test data points. 
For every data point (a prompt with a pair of responses), we keep sampling BT~\citep{bt} preferences from all user types until they agree with each other.
Once the consensus is achieved, we stop sampling and use the agreed-upon preference as the label for this data point.

\paragraph{Fine-Tuning Details.}
We fine-tune Llama-3-8B~\citep{llama3modelcard} base model with LoRA~\citep{lora}.
We fine-tune for one epoch with a batch size of $2$, and use a linear learning rate schedule that starts with $3\times10^{-5}$ and decreases to zero.
We use the Adam optimizer with a weight decay of $0.001$~\citep{adamw}.
Regarding LoRA's hyper-parameters, we use the matrix rank of $r=8$, $\alpha=32$, and the dropout probability of $0.1$.

For direct alignment experiments, we use a uniform reference policy.
When ignoring heterogeneity, we do vanilla DPO over the anonymous dataset.
When modeling heterogeneity, we use the loss function we propose in \cref{prop:consistent_loss_1} over the dataset with maximum annotator information.
We use the ordinal agreement between the ground-truth average reward and the reward induced by the aligned policy as the measure of accuracy.

For the reward learning experiments, we fine-tune the Llama-3-8B as a reward model.
When ignoring heterogeneity, we assume BT and maximize the probability of the anonymous preference dataset under the learned reward model.
When modeling heterogeneity, we use the loss function in \cref{prop:consistent_loss_1} over the dataset with maximum annotator information, but replace $h(\vy_1, \vy_2; \pi)$ with the difference in rewards, i.e., $r(\vy_2) - r(\vy_1)$.
We use the ordinal agreement between the ground-truth average reward and the learned reward as the measure of accuracy.

\begin{table}
\caption{Raw Accuracy $(\%)$ in Alignment Experiments}
\label{tab:raw-aligns}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
Seed & Ignoring Homogeneity & Modeling Heterogeneity \\
\midrule
0    &  65.61 & 66.63\\
1    &  67.55 & 69.55\\
2    &  68.77 & 75.21\\
3    &  68.70 & 72.04\\
4    &  66.28 & 74.95\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\end{table}

\begin{table}
\caption{Raw Accuracy $(\%)$ in Reward Learning Experiments}
\label{tab:raw-rews}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{ccc}
\toprule
Seed & Ignoring Homogeneity & Modeling Heterogeneity \\
\midrule
0    &  92.33 & 95.26\\
1    &  85.38 & 92.01\\
2    &  88.46 & 94.6\\
3    &  89.69 & 93.57\\
4    &  91.94 & 93.87\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\end{table}

\paragraph{Detailed Results.}
We conduct every experiment with five different random seeds.
\cref{fig:semi-synthetic} shows the average, $25^\text{th}$ percentile, and $75^\text{th}$ percentile of accuracy across the five random seeds.
\cref{tab:raw-aligns,tab:raw-rews} show the raw accuracy numbers across the five random seeds.