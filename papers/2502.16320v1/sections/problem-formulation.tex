\section{Problem Formulation}
\label{sec:formulation}

The alignment problem is traditionally framed under a preference homogeneity assumption, where a single reward is presumed to capture all individual interests. In practice, people's preferences can differ significantly. To better capture real-world settings, we formalize preference heterogeneity by allowing reward functions to vary across \emph{user types}. 

%%%%%
\niparagraph{Heterogeneous Preferences.}
The influential study of ``individual choice behavior'' by \citet{luce1959individual} and other foundational works on human decision-making in mathematical psychology such as \citet{shepard_stimulus_1957}  focus on \emph{individual} preference models. \citet{luce1959individual} uses an axiomatic approach to establish the existence of a value function for each individual that, once normalized, explains the individual's choice probabilities. The widely used BT is one such example. 

In practice, we often cannot observe individuals' identities. Therefore, standard approaches in preference modeling use a single reward function across the entire population. This homogeneity assumption makes preference learning \emph{unrealizable}: Even if a specific model family can explain preferences of every individual, we cannot ensure that a model from the same family explains population-level choices. For example, we cannot represent a mixture of BT models with a single BT (we prove this in \cref{prop:mixture_bt} for completeness).

To account for heterogeneity, we need to define individual rewards. However, learning at scale with this level of granularity is impractical, especially when working with finite data. Hence, we group individuals into multiple \emph{user types}, denoted by~$\gU$. Individuals with the same type have similar rewards, but this need not hold across types. 

For a given user of type~$u \in \gU$, we denote the corresponding reward function by~$r^*(\cdot; u)$. This function assigns a scalar reward~$r^*([\vx, \vy]; u)$ for every response~$\vy$ to a given query~$\vx$. We model the preferences of an individual of type~$u$ with
%
\begin{equation}
\label{eq:heter_BT}
    \Pr(\vy_2 \succ \vy_1 \mid r^*, u) = \sigma\big(r^*(\vy_2; u) - r^*(\vy_1; u)\big)
    \,,
\end{equation}
The population-level preferences are therefore given by: 
\begin{equation}
\label{eq:heter_marginal_BT}
    \Pr(\vy_2 \succ \vy_1 \mid r^*) = \E_u\Big[\sigma\big(r^*(\vy_2; u) - r^*(\vy_1; u)\big)\Big]
    \,.
\end{equation}
%

%%%%%
\niparagraph{The Extended Alignment Problem.}
Our goal is to find a single policy that can effectively accommodate a heterogeneous population.
This is essential when user types are not observable during inference. Furthermore, a universal policy may be preferable when personalization comes with significant drawbacks: e.g., cases where prioritizing a broadly accepted notion of truth or safety is more important than catering to individual preferences~\citep{monteiro2022epistemic,kirk2024benefits}. 

Deriving a universal policy requires aggregation of diverse rewards. As we show next, an affine combination is the only form of aggregation that guarantees a well-defined problem, i.e., a problem that yields the same optimal policy for every reward that is consistent with the preference data.
% This, in turn, requires aggregating rewards to induce a consistent ranking over possible alternatives, which is only possible with affine aggregation. We formalize this below: 
%%%
\begin{theoremEnd}[restate]{proposition}
\label{prop:only_affine}
Consider an aggregation~$f: \R^\gU \rightarrow \R$. If $f\big(\{r(\vy; u)\}_{u \in \gU}\big)$ induces the same ordering over~$\vy$ for every reward~$r$ consistent with the preferences distribution, then under weak regulatory assumptions, $f$ must be affine. 
\end{theoremEnd}
%%%
\begin{proofEnd}
    First of all, if a reward function~$r^*(\vy; u)$ can explain the preferences of a user type~$u$, any other reward function~$r(\vy; u) \coloneqq r^*(\vy; u) + c(u)$ induces the same preference distribution:
    %
    \begin{equation*}
        \Pr(\vy_2 \succ \vy_1 \mid r; u) = \sigma\big(r(\vy_2; u) - r(\vy_1; u)\big) = \sigma\big(r^*(\vy_2; u) - r^*(\vy_1; u)\big) = \Pr(\vy_2 \succ \vy_1 \mid r^*; u)
        \,.
    \end{equation*}
    %
    Therefore, $r^*$ is identifiable up to a bias term that can depend on the context and user type. 
    
    Consider a reward aggregation function~$f: \R^\gU \rightarrow \R$. Denoting all the rewards from different user types by a vector~$\vr(\vy) \in \R^\gU$, the aggregation~$f(\vr(\vy))$ should induce the same ranking for every~$\vr$ consistent with (possibly infinite) preference data. Our above argument then implies that $f(\vr^*(\vy) + \vc)$ should induce a consistent ranking for every~$\vc \in \R^\gU$. For a sufficiently large space of alternatives, where $\vr^*(\vy)$ can take any value within a closed interval of~$\R$, this is possible only if there exists a function~$\psi: \R^\gU \rightarrow \R$ such that
    %
    \begin{equation*}
        f(\vr_2 + \vc) - f(\vr_1 + \vc) = \psi(\vr_2 - \vr_1)
        \,,
    \end{equation*}
    %
    for every~$\vc$, $\vr_1$, and~$\vr_2$ in~$\R^\gU$. Choosing $\vc = -\vr_1$, this implies $f(\vr) = f(\vzero) + \psi(\vr)$ for every~$\vr$. Therefore, we have the following Cauchy functional equation for~$\psi$:
    %
    \begin{equation*}
        \psi(\vr + \vDelta) = \psi(\vr) + \psi(\vDelta)
        \,.
    \end{equation*}
    %
    Under weak regularity conditions such as the monotonicity or continuity of~$f$, it is well-known that~$\psi$ has to be a linear function. This implies that~$f$ has to be an affine function which completes the proof. 
\end{proofEnd}
%%%
This result rules out many commonly-used aggregations, such as Max-Min \citep{chakraborty2024maxmin} or Nash social welfare \citep{kaneko1979nash}. The expected reward across user types emerges as a natural choice here. Any other affine combination would weigh people unequally, which requires strong justifications and is rare in practice.

To summarize, our objective is to maximize
{\ifnotarxiv\small\fi
\begin{equation}
\label{eq:heter_alignment_problem}
    \E_{\vy \sim \ifarxiv \pi(\cdot \mid \vx) \else \pi \fi}\big[\E_u\big[r^*([\vx, \vy]; u)\big]\big] \!-\! \beta \KL\big(\pi(\cdot \!\mid\! \vx); \pi_\reff(\cdot \!\mid\! \vx)\big)
    %,
\end{equation}
}
for every prompt~$\vx$. With this extended framework in mind, we next discuss why standard approaches like RLHF or DPO do not necessarily yield the optimal policy.

%%%%%
