\section{Experiments}

We provide empirical evidence for our claims throughout the paper. \Cref{sec:sensitivity} extends our sensitivity example in \cref{sec:drawbacks} to
% quantify how sensitive $\nbc$ is to the sampling distribution in
a real-world preference dataset. In \cref{sec:simulate_dpo_with_unknown_u}, we simulate DPO and our proposed improvements in a synthetic, small-scale environment where we can visualize and compare the resulting policies. Finally, we scale this experiment in \Cref{subsec:exp:semi-synth} by fine-tuning large language models, illustrating the extent of improvement over DPO. 
\ifarxiv 
Our code for reproducing the experimental results is publicly available at \url{https://github.com/arashne/dahp}.
\fi

%%%%%
\subsection{NBC Sensitivity to Sampling Distribution}
\label{sec:sensitivity}
Recall from \Cref{sec:borda_count} that the common practice of alignment assuming homogeneity results in ordinal consistency with $\nbc$.  
Here, we analyze the sensitivity of $\nbc$ to the distribution of pairwise preference datasets in real-world cases by using Pew surveys~\citep{pew_about}, the same dataset used in \cref{sec:drawbacks}. Specifically, we address two questions: (i) Across the questions in the Pew surveys, how often would $\nbc$ rankings change when the sampling distribution of alternatives varies (while retaining support over all alternatives)? (ii) How much must the sampling distribution deviate from uniform to alter $\nbc$ rankings?

To answer these questions, we first estimate the reward of each option in each question (see \cref{sec:drawbacks,sec:pew_additional_examples} for further details). Given the rewards, we can calculate $\nbc$ under any sampling distribution using \cref{eq:nbc}.

For question (i), among 1519 questions from 19 Pew surveys, $\nbc$ rankings change due to changing the sampling distribution from uniform in 20\% of cases (306 questions), with the preferred choice changing in 136 cases. For question (ii), we find that a modest change in the sampling distribution suffices; in half the cases, a total variation (TV) distance of less than 0.23 from uniform alters the rankings. The cumulative distribution function (CDF) of the minimum TV distances required to change $\nbc$ rankings is shown in \cref{fig:cdf_TV}. Further experimental details are in \cref{sec:pew_additional_examples}.

\begin{figure}[h]
    \centering
    \ifarxiv
    \includegraphics[width=0.4\textwidth]{figs/cdf_order_change.pdf}
    \else
    \includegraphics[width=0.31\textwidth]{figs/cdf_order_change.pdf}
    \fi
    \caption{CDF of the minimum TV distances (from uniform) required to change the NBC order in the Pew surveys. A change of $0.23$ is sufficient to change the order in half the questions. }
    \label{fig:cdf_TV}
\end{figure}
%%%%%


%%%%%
\subsection{Synthetic Experiments}
\label{sec:simulate_dpo_with_unknown_u}

We generalize the discrete environment of \citet{xu2024dpo} with a heterogeneous population. This environment enables us to visualize the differences between DPO's policy and the optimal policy, as well as to evaluate the effectiveness of applying a first-order correction (\cref{sec:dpo_corrected}) and using a consistent loss function (\cref{sec:dpo_with_known_u}).

\niparagraph{Environment.}
A prompt~$x$ can take a value from~$1$ to~$n$. There are also only $n$~possible responses to each~$x$. The reward for responding~$y$ to~$x$ for a type~$u$ is $r^*([x, y]; u) = R_u({\rm dist}(x + u, y))$, where~${\rm dist}$ is a circular distance, and~$R_u$ is a linearly decreasing function floored at zero. In this experiment, we set~$n=40$ and consider three equally represented types: $\gU = \{-10, 0, 10\}$. We assume BT annotators.

Since the reward (and thus the policies) depends only on~$y - x$, we can reduce everything to a $1$D representation by setting~$\delta \coloneqq y-x$ and averaging over~$x$. For example, for a policy~$\pi$, define a~$1$D policy $\pi(\delta) \coloneqq \frac{1}{n}\sum_{x \in [n]} \pi(x + \delta \mid x)$, and similarly a $1$D reward $r^*(\delta; u)$. We also compute standard errors of these $1$D representations across~$x$. \cref{fig:sim_rewards} shows our choice of rewards as well as the expected reward across user types. 

\begin{figure}[h]
    \centering
    \ifarxiv
    \includegraphics[width=0.4\textwidth]{figs/rewards_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo.pdf}
    \else
    \includegraphics[width=0.31\textwidth]{figs/rewards_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo.pdf}
    \fi
    \vspace{-3mm}
    \caption{Rewards in the synthetic experiments}
    \label{fig:sim_rewards}
\end{figure}


\niparagraph{Policies.}
For a uniform~$\pi_\reff$ and $\beta=1$, \cref{eq:heter_opt_policy} implies $\pi^*(y \mid x) \propto \exp\big(\frac{1}{3} \sum_{u \in \gU} r^*([x, y]; u)\big)$. We generate a large dataset of preferences under uniform context and alternative distributions and use the Adam optimizer to minimize the loss for different methods. For the first-order correction of DPO, we additionally train a joint likelihood model~$J$ to estimate the variance term~$V$ from \cref{eq:var_and_J}. We use the loss from \cref{prop:consistent_loss_1} as our choice for the consistent loss. 
\ifarxiv This loss effectively utilizes less data compared to other methods since it throws away data points with disagreement. Refer to the accompanying code for more details. \fi 

\begin{figure*}[t!]
    \centering
    \subfigure[DPO vs. optimal]{
        \ifarxiv
        \includegraphics[width=0.4\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_nbc.pdf}
        \else
        \includegraphics[width=0.31\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_nbc.pdf}
        \fi
        \label{fig:opt_vs_dpo_nbc}
    }
    \ifarxiv\fi
    \subfigure[Corrected DPO]{
        \ifarxiv
        \includegraphics[width=0.4\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_estvarcorrecteddpo.pdf}
        \else
        \includegraphics[width=0.31\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_estvarcorrecteddpo.pdf}
        \fi
        \label{fig:opt_vs_corrected_dpo}
    }\ifarxiv\fi
    \subfigure[Consistent loss minimization]{
        \ifarxiv
        \includegraphics[width=0.4\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_shiralietal.pdf}
        \else
        \includegraphics[width=0.31\textwidth]{figs/aligned_policy_comparison_offline_size500000_40s40a_shifts-10_0_10_decaylinear0.075_0.1_0.075_rscale4_1.5_4_dpo_shiralietal.pdf}
        \fi
        \label{fig:opt_vs_ours}
    }
    \vspace{-2mm}
    \caption{Policies explicitly accounting for heterogeneity are more consistent with the average reward across types in a synthetic setup.}
\end{figure*}

\niparagraph{Results.}
\cref{fig:opt_vs_dpo_nbc} presents $\pi_\dpo$ along with~$\pi^*$ and $\nbc$. Unlike the optimal policy which prefers~$\delta$ around~$-10$ and~$10$, DPO prefers~$\delta \approx 0$. To a large extent, DPO's policy is ordinally consistent with~$\nbc$.

\cref{fig:opt_vs_corrected_dpo} shows that increasing correction strength~$\alpha$ brings the corrected DPO policy closer to~$\pi^*$. In particular, at $\alpha=1$, the corrected DPO already favors alternatives with~$\delta \in \{-10, 10\}$, consistent with~$\pi^*$. Furthermore, increasing~$\alpha$ makes these alternatives even more favorable. In the full-information setting, \cref{fig:opt_vs_ours} shows that minimizing the consistent loss largely leads to~$\pi^*$.
Note that minor deviations from theoretical derivations are likely due to limited data and imperfect optimization in these experiments.

%%%%%


%%%%%
\subsection{Semi-Synthetic Experiments}
\label{subsec:exp:semi-synth}

To demonstrate our findings in a realistic setup, we use LoRA~\citep{lora} to fine-tune Llama-3-8B~\citep{llama3modelcard} for both reward learning and direct alignment on two relabeled variations of the HH-RLHF dataset~\citep{hh-rlhf}, which contains user prompts with pairs of chatbot responses.
To simulate heterogeneous preferences, we define three user types with distinct length-based rewards: the first type prefers long, the second type prefers short, and the third type prefers mid-length prompt response combinations (see \cref{app:llama} for details).
Recall from \cref{sec:formulation} that we argued for the average reward across user types as the proper objective for alignment with heterogeneous preferences.
Therefore, we use agreement with the ground-truth average reward on the test set as the success metric.

First, we use vanilla reward learning and DPO with the homogeneity assumption on an anonymous preference dataset where every sample is labeled with a random user type.
As \cref{fig:semi-synthetic} shows in blue, their induced order agrees with the average reward in 89.6\% and 67.4\% of the test cases, respectively.
Next, we use the loss function in \cref{prop:consistent_loss_1} on a dataset with maximum annotator information (\cref{sec:dpo_with_known_u}). 
To create this dataset, for every response pair to a prompt, we sample the preferences of the three user types until a consensus is reached, using the agreed-upon preference as the label.
As \cref{fig:semi-synthetic} shows in red, the learned reward and the induced reward by the learned policy agree with the average reward in 93.9\% and 71.7\% of the test cases.
In summary, explicitly accounting for heterogeneity increases the agreement with the average reward across user types by 4.3\%---an additional 368 test cases---for both reward learning and direct alignment.

\begin{figure}[h]
    \centering
    \ifarxiv
    \includegraphics[width=0.6\textwidth]{figs/rew-accuracy.pdf}
    \else
    \includegraphics[width=0.95\columnwidth]{figs/rew-accuracy.pdf}
    \fi
    \vspace{-3mm}
    \caption{In the presence of preference labels from every user type, our proposed loss function produces reward models (left) and aligned policies (right) that are more consistent with the average reward across user types, compared to typical approaches that overlook heterogeneity. Bars show the mean, and whiskers denote the second and third quartiles across five random seeds.}
    \label{fig:semi-synthetic}
\end{figure}

%%%%%