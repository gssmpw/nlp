\section{Implications of Homogeneity Assumption}
\label{sec:dpo_with_unknown_u}

With heterogeneous preferences, standard RLHF or DPO cannot yield the optimal policy~$\pi^*$ that maximizes \cref{eq:heter_alignment_problem}. If they did, it would also be possible to learn the user-weighted average reward as the induced reward of~$\pi^*$. However, as we show in \cref{prop:impossible_anonymous_learning} and was previously observed
% in specific cases
by \citet{siththaranjan2023distributional,procaccia2025clone}, learning the expected reward from anonymous preferences is impossible.

To explain DPO's failure in finding~$\pi^*$, we extend its derivation to the heterogeneous setting in \cref{sec:dpo_deos_not}. This analysis lays the foundations to account for heterogeneity in DPO later on.
%
In \cref{sec:borda_count}, we show that DPO's policy aligns with Borda count and, in \cref{sec:drawbacks}, highlight its limitations. While our analysis focuses on DPO, similar insights extend to RLHF by substituting the policy with its induced reward.


%%%%%
\subsection{Objective is Not the Expected Reward}
\label{sec:dpo_deos_not}

We follow DPO's derivation from \cref{sec:prelim} but under heterogeneity. We show the closed-form connection between~$\pi^*$ and~$r^*$ is no longer sufficient to express the likelihood function. Beginning with \cref{eq:heter_alignment_problem}, the optimal policy is
%
\begin{equation}
\label{eq:heter_opt_policy}
    \pi^*(\vy) = \frac{1}{Z(\vx)} \, \pi_\reff(\vy) \cdot \exp\Big(\frac{1}{\beta} \, \E_u\big[r^*(\vy; u)\big]\Big)
    \,.
\end{equation}
%
Define $\Delta r^*(\vy_1, \vy_2; u) \coloneqq r^*(\vy_2; u) - r^*(\vy_1; u)$. The policy ratios of~$\pi^*$ are related to the expected difference in rewards:
{\ifnotarxiv\small\fi
\begin{equation}
\begin{aligned}
\label{eq:heter_diff_of_rew}
    \E_u\big[\Delta r^*(\vy_1, \vy_2; u)\big] =
    \beta \log\frac{\pi^*(\vy_2)}{\pi_\reff(\vy_2)} - \beta \log\frac{\pi^*(\vy_1)}{\pi_\reff(\vy_1)}
    .
\end{aligned}
\end{equation}
}In the homogeneous case, $\Delta r^*$ was sufficient to describe the likelihood of $\vy_2 \succ \vy_1$. However, with heterogeneous preferences, $\E_u[\Delta r^*]$ alone does not suffice to write the likelihood function in \cref{eq:heter_marginal_BT}. It is only under the approximation
%
\begin{equation}
\label{eq:dpo_approx}
    \E_u\Big[\sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)\Big] \!\approx \sigma\Big(\E_u\big[\Delta r^*(\vy_1, \vy_2; u)\big]\Big)
\end{equation}
%
that we can write $\Pr(\vy_2 \succ \vy_1 \mid r^*)$ in terms of policy ratios as in \cref{eq:likelihood}, and minimize DPO's loss to find~$\pi^*$. 

%%%%%
\subsection{Ordinal Consistency with Borda Count}
\label{sec:borda_count}

If DPO were the answer, what would the question be?
We partially answer this question by an adaptation of a result 
from~\citet{siththaranjan2023distributional} which we restate for completeness. First, define Borda count as follows.
%%%
\begin{definition}[Normalized Borda count]
For a prompt~$\vx$, let $\gD(\cdot \mid \vx)$ denote the distribution of alternative responses sampled for~$\vx$. The Normalized Borda Count (NBC) of~$\vy$ at~$\vx$ is the probability that an annotator with a random type prefers~$\vy$ over an alternative response~$\vy' \sim \gD(\cdot \mid \vx)$:
%
\begin{equation}
\label{eq:nbc}
    \nbc(\vy \mid \vx) \coloneqq \E_{\vy' \sim \gD(\cdot \mid \vx)} \Big[\Pr(\vy \succ \vy' \mid \vx; r^*)\Big] 
    \,.
\end{equation}
%
\end{definition}
%%%

We next show that DPO's policy ratios are ordinally consistent with the normalized Borda count.
%%%
\begin{theoremEnd}[restate]{proposition}
\label{prop:dpo_heter_sol}
Suppose responses to~$\vx$ in the preference dataset are drawn from $\gD(\cdot \mid \vx)$. In the limit of many data points, DPO's induced reward, or equivalently~$\frac{\pi_\dpo(\cdot \mid \vx)}{\pi_\reff(\cdot \mid \vx)}$, has the same ordering over responses as~$\nbc(\cdot \mid \vx)$.\footnote{We can view $\nbc(\vy \mid \vx)$ as an aggregation of rewards at~$\vy$. One can verify that $\nbc$ meets the order consistency condition of \cref{prop:only_affine}. However, it uses the reward value at $\vy' \neq \vy$ to define the aggregated reward at~$\vy$ and thus does not fall under \cref{prop:only_affine}. In fact, this interdependency causes the issues we discuss \cref{sec:drawbacks}.}
\end{theoremEnd}
%%%
\begin{proofEnd}
    We start from DPO's objective in \cref{eq:dpo_ml}. For notational simplicity, we assume~$\pi(\vy \mid \vx)$ already contains a normalization by~$\pi_\reff(\vy \mid \vx)$. In the limit of many data points, we can rewrite DPO's objective as the minimization of a cross-entropy loss
    %
    \begin{align*}
        \gL_\dpo(\pi) \coloneqq -\E_{\vx, \vy, \vy'} \Big[ 
        &\barsig\big(\Delta r^*(\vx, \vy', \vy)\big) \cdot \log \sigma\Big(\beta \log \frac{\pi(\vy \mid \vx)}{\pi(\vy' \mid \vx)}\Big) \\
        &+ \Big(1 - \barsig\big(\Delta r^*(\vx, \vy', \vy)\big)\Big) \cdot \log \Big(1 - \sigma\Big(\beta \log \frac{\pi(\vy \mid \vx)}{\pi(\vy' \mid \vx)}\Big) \Big)
        \Big]
        \,,
    \end{align*}
    %
    where $\barsig\big(\Delta r^*(\vx, \vy', \vy)\big)$ is shorthand for $\Pr(\vy \succ \vy' \mid \vx; r^*) = \E_u\big[\sigma\big(r^*([\vx, \vy]; u) - r^*([\vx, \vy']; u)\big)\big]$. The minimizer of~$\gL_\dpo$ should meet the first-order condition: $\pd{\gL_\dpo}{\pi(\vy \mid \vx)} = 0$, for every~$\vx$ and~$\vy$. Then, a direct calculation shows that the optimal policy~$\pi^*$ meets
    %
    \begin{equation}
    \label{eq:dpo_foc}
        \E_{\vy' \sim \gD(\cdot \mid \vx)}\Big[\sigma\Big(\beta \log \frac{\pi^*(\vy \mid \vx)}{\pi^*(\vy' \mid \vx)}\Big)\Big] 
        - \E_{\vy' \sim \gD(\cdot \mid \vx)}\Big[\barsig\big(\Delta r^*(\vx, \vy', \vy)\big)\Big]
        = 0
        \,.
    \end{equation}
    %
    Recognize that the second term is~$\nbc(\vy \mid \vx)$:
    %
    \begin{equation*}
        \nbc(\vy \mid \vx) = \E_{\vy' \sim \gD(\cdot \mid \vx)} \Big[\E_u\Big[\sigma\big(r^*([\vx, \vy]; u) - r^*([\vx, \vy']; u)\big)\Big]\Big]
        \,.
    \end{equation*}
    %
    In the absence of heterogeneity, we have $\barsig = \sigma$, so setting $\beta \log \pi^*(\vy \mid \vx) = r^*([\vx, \vy]) + C(\vx)$ for a normalizing~$C$ would solve \cref{eq:dpo_foc}. In general, we are not aware of any closed-form solution. However, we can still infer the ordering the optimal policy induces from \cref{eq:dpo_foc}: Since the first term is increasing in~$\pi^*(\vy \mid \vx)$, the optimal policy will be monotone in~$\nbc(\vy \mid \vx)$. This completes the proof.
\end{proofEnd}
%%%
\cref{prop:dpo_heter_sol} also applies to the homogeneous setting. In this case, however, $\nbc$ aligns with~$r^*$. 
It is worth mentioning that DPO is not the only method consistent with~$\nbc$; identity preference optimization (IPO)~\citep{azar2024general} uses $\nbc$ as its objective. We next highlight key differences between $\nbc$ and the user-weighted expected reward along with DPO's drawbacks in practice.

%%%%


%%%%
\subsection{Practical Drawbacks}
\label{sec:drawbacks}
In case of heterogeneous preferences, Borda count can significantly diverge from the user-weighted expected reward. This is studied under \emph{distortion} in social choice problems~\citep{anshelevich2021distortion}. Notably, $\nbc$ in \cref{eq:nbc} depends on~$\gD$. Therefore, although data collection is irrelevant in defining the optimal policy, it does affect $\pi_\dpo$.
% This issue particularly arises when $\pi_\dpo$ violates independence of irrelevant alternatives \citep{xu2023rlhf,procaccia2025clone}.

Next, we illustrate two key differences between~$\pi^*$ and~$\pi_\dpo$ using examples. Unless otherwise stated, we assume $\gD(\cdot \mid \vx)$ and~$\pi_\reff(\cdot \mid \vx)$ are uniform, and annotators follow BT.
Refer to \cref{sec:drawbacks_appendix} for further drawbacks of DPO (minority suppression and IIA violation).

%%%
\paragraph{Sensitivity to Preference Dataset Distribution.}
Suppose $\gU = \{A, B\}$ and types are equally represented. Given three possible responses, type~$A$ prefers~$\vy_1$ but type~$B$ prefers~$\vy_2$:
%
\begin{align*}
    &r^*(\vy_1; A) = 6,\, r^*(\vy_2; A) = 1,\, r^*(\vy_3; A) = 4 \,, \\
    &r^*(\vy_1; B) = 3,\, r^*(\vy_2; B) = 9,\, r^*(\vy_3; B) = 4 \,.
\end{align*}
%
One can verify when $\gD(\vy_1) = \gD(\vy_2)$, increasing $\gD(\vy_3)$ from~$0.02$ to~$0.04$ changes~$\pi_\dpo$'s preference from~$\vy_2$ to~$\vy_1$.

DPO's policy is also sensitive to the preference model. Consider a variation of BT with a temperature of~$2$: $\sigma_2(z) \coloneqq (1 + \exp(-z/2))^{-1}$. For the same users and uniform sampling of alternatives, increasing the temperature from~$1$ to~$2$ flips~$\pi_\dpo$'s ranking over~$\vy_1$ and~$\vy_2$ while the preference model has no effect on~$\pi^*$.

We have to emphasize that the dependence of $\nbc$, and consequently $\pi_\dpo$, on the dataset sampling distribution~$\gD$ is not due to finite-sample limitations or insufficient offline dataset support. This issue persists even with complete data coverage and in the limit of infinite data.
%%%

%%%
\paragraph{Mediocrity Promotion.}
Consider the task of summarization. Suppose $\gU = \{A, B, C\}$ and types are equally represented. Type~$A$ ($B$) strongly favors longer (shorter) summaries while type~$C$ slightly prefers medium-length ones:
%
\begin{align*}
    &r^*({\rm short}; A) = 0,\, r^*({\rm med}; A) = 1,\, r^*({\rm long}; A) = 4 \,, \\
    &r^*({\rm short}; B) = 4,\, r^*({\rm med}; B) = 1,\, r^*({\rm long}; B) = 0 \,, \\
    &r^*({\rm short}; C) = 0,\, r^*({\rm med}; C) = 1,\, r^*({\rm long}; C) = 0 \,.
\end{align*}
%
In this case,
$\pi^*({\rm short}) = \pi^*({\rm long}) > \pi^*({\rm med})$, however, $\nbc({\rm short}) = \nbc({\rm long}) < \nbc({\rm med})$. DPO prefers medium-length summaries not strongly favored by any type.
%%%


%%%
\paragraph{Real-World Examples.}
The examples above are not contrived; in real-world cases, $\nbc$ can produce rankings different from~$\pi^*$ and is sensitive to dataset distribution as extensively studied under distortion of social choice rules. To show this with a real example, we use Pew Research Center surveys and analyze a question to 5101 participants: ``The next time you purchase a vehicle, how likely are you to consider purchasing an electric vehicle?''
(options from A: very likely to D: not at all likely). We discuss how we select this question in \cref{sec:pew_additional_examples}. Responses come from groups of different political leanings: Republican (45\%), Democratic (48\%), and Neither/refused (7\%).

Assuming the Luce-Shepard model~\citep{shepard_stimulus_1957} (see \cref{eq:luce-shep}), we estimate the reward for each group to calculate $\nbc$ and a user-weighted average reward. To find $\nbc$, we use two distributions for alternatives: a uniform distribution~$\gD_U$ and a slightly altered distribution~$\gD_a$ with~$0.2$ total variation distance (TV) from~$\gD_U$. As shown in \cref{fig:reward_vs_nbc}, $\nbc$ (with $\gD_U$) ranks option C first despite its mediocrity: it is the second or third preference for the three groups (see \cref{fig:example7}). In contrast, the user-weighted average reward favors D: the first and second preference for Republicans and the no-lean groups, respectively.
%
Notably, altering $\gD_U$ to $\gD_a$ flips $\nbc$’s top ranking, highlighting $\nbc$’s sensitivity to dataset distribution. Similar discrepancies appear in other Pew surveys (see \cref{sec:pew_additional_examples}).

\begin{figure}%[!htb]
    \centering
    % \includegraphics[width=0.92\linewidth]{figs/PEW/reward_vs_NBC_sens_EVCAR2_W128.pdf}
    \ifarxiv
    \includegraphics[width=0.6\linewidth]{figs/PEW/ranking_comparison_sens_EVCAR2_W128.pdf}
    \else
    \includegraphics[width=0.92\linewidth]{figs/PEW/ranking_comparison_sens_EVCAR2_W128.pdf}
    \fi
    \vspace{-2mm}
    \caption{  
    ``The next time you purchase a vehicle, how likely are you to seriously consider purchasing an EV?''
    $\nbc$ ranking differs from the user-weighted average reward and is sensitive to the dataset distribution.
    }
    \label{fig:reward_vs_nbc}
\end{figure}
%%%


% \begin{figure}[ht]
%     \centering
%     % First Row
%     \subfigure[Rewards and Proportions per Type]{%
%         \includegraphics[width=0.73\linewidth]{figs/PEW/prop_rew_POLPROB_W127.pdf}
%         \label{fig:prop_rew}
%     }%
%     \hfill
%     \subfigure[Ranking]{%
%         \includegraphics[width=0.22\linewidth]{figs/PEW/ranking_comparison_POLPROB_W127.pdf}
%         \label{fig:ranking}
%     }

%     % Second Row
%     \subfigure[Avg Reward vs. NBC]{%
%         \includegraphics[width=0.7\linewidth]{figs/PEW/reward_vs_NBC_POLPROB_W127.pdf}
%         \label{fig:reward_vs_nbc}
%     }

%     \caption{Using NBC to rank answers to the survey question: "How much of a problem do you think police violence against Black people is in the United States today?
% A: Major problem; B: Minor problem; C: Not a problem" would lead to a ranking that is different from the one induced by average reward.}
%     \label{fig:combined}
% \end{figure}
% 


%%%%%
