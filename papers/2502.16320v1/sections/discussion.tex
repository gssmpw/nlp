\section{Discussion and Conclusion}
\label{sec:conclusion}

% The importance of annotator data collection
Aligning a single policy to the average reward across user types in a heterogeneous population requires collecting annotator information. This can range from minimal information such as linking two instances labeled by the same annotator, to richer information like using questionnaires to infer annotator types. We improved DPO using the former and introduced a consistent loss when annotators from all user types label every data point. With additional assumptions, unsupervised methods might be able to identify annotator types from anonymous datasets~\citep{zhang2022identifiability}.
Further research should explore the additional structures that, when used during data collection, can help with identifiability.

% Surprise + Open question
Our results revealed a tension between consistency and sample efficiency in direct alignment. Thus, an alternative approach---individual reward training and aggregation---may be more practical for addressing heterogeneity when individual rewards are identifiable.
% It also underscores the need for further research to understand the identifiability of heterogeneous rewards and its implications for the alignment problem.

% Limits
\ifarxiv
We use the average reward across user types, as the natural choice among aggregations that define a well-defined alignment problem from pairwise preferences. However, the choice of aggregation is inherently a social and policy question rather than purely a technical one.
In certain contexts, the policymaker might prefer to give higher weight to disadvantaged people to address issues such as inequality. Additionally, in some cases, the very existence of a reward function may be questionable, requiring objectives to be defined in terms of choice probabilities rather than rewards.

We believe that trained policies should not be used to elicit or represent aggregate preferences, even when reward aggregation is appropriate and estimation is consistent. While such policies may capture certain patterns in user behavior, they do not necessarily reflect the underlying interests or values of the population. In other words, we view the resulting policy as a functional tool for decision-making rather than a true representation of users' collective interests.
\fi

% Summary
In summary, while preference heterogeneity is well recognized in mathematical psychology, standard methods often bypass this complexity. As we showed, accounting for heterogeneity\ifarxiv, even when the goal remains the same as in the homogeneous setting---to derive a single policy---\fi
can render common techniques inefficient or inapplicable. Understanding these limitations calls for new approaches that explicitly incorporate heterogeneity while effectively balancing efficiency, consistency, and practicality.

%unsupervised methods to identify annotator type. collecting annotator information might not be possible. What can we do with anonymous data. There are papers that tryto identify BT people but they need a lot more assumptions. There is a lot to study here. What is possible with anonymous data? 

%impossibility of consistency and direct optimization. what is the right balance? Do we have a good method to balance these two? 


%limitations of work 
