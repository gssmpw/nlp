\section{Related Work}
\label{sec:related}

Aligning models to ``serve pluralistic human values'' \citep{sorensen_roadmap_2024} can involve personalization to the user's specific reward \citep{poddar_personalizing_2024,chen_pal_2024} or aggregation of diverse rewards. The latter, which is the subject of our study, can use insights from social choice theory \citep{ge2024axioms,conitzer2024social}.

Closest to our work, EM-DPO \citep{chidambaram2024direct} simultaneously learns the distribution of user types and their corresponding policies. However, EM introduces significant complexities and lacks guarantees. Moreover, the identifiability of types requires additional assumptions. MODPO \citep{zhou_beyond_2024} applies DPO for each user type while utilizing estimated rewards from other user types to maximize a linear combination of rewards. Neither method obtains a policy by directly minimizing a loss over preference data. For a more extensive related work, refer to \cref{sec:related_appendix}.

%%%

% \citet{siththaranjan2023distributional} studies learning a reward function with hidden contexts, or equivalently unknown user types. We drew on their insights to further understand how policy learning works under the homogeneity assumption. \citet{dumoulin2023density} also discusses ``annotator misspecification'' and its challenges for models learning from a population of annotators with diverse viewpoints. 

% \citet{chidambaram2024direct} proposed Expectation-Maximization (EM) DPO to simultaneously learn the distribution of user types and their corresponding policies. However, EM introduces significant complexities and lacks guarantees. Moreover, the identifiability of types requires additional assumptions; for instance, it is known that even the expected reward is not learnable under anonymous data (\cref{prop:impossible_anonymous_learning}). While EM-DPO aims to personalize policies, our study focuses on learning a single policy through direct optimization.

% In addition to EM-DPO, other recent studies have explored learning multiple policies, particularly the Pareto frontier of all linear combinations of reward functions \citep{rame2024rewarded,zhou2023beyond}. Our study reveals the inherent challenges in directly learning even one such aggregation.

% Aligning models to serve ``pluralistic'' human values can take different forms and require further contributions from various communities \citep{pluralistic-alignment-roadmap}. Social choice theory can certainly guide such efforts in aggregating diverse preferences \citep{conitzer2024social,ge2024axioms}. 