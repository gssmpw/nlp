\begin{abstract}


Alignment with human preferences is commonly framed using a universal reward function, even though human preferences are inherently heterogeneous. We formalize this heterogeneity by introducing \emph{user types} and examine the limits of the homogeneity assumption.
%and characterize settings where annotator data can help mitigate these limitations. 
We show that aligning to heterogeneous preferences with a single policy is best achieved using the average reward across user types. However, this requires additional information about annotators. We examine improvements under different information settings, focusing on direct alignment methods. We find that minimal information can yield first-order improvements, while full feedback from each user type leads to consistent learning of the optimal policy. Surprisingly, however, no sample-efficient consistent direct loss exists in this latter setting. These results reveal a fundamental tension between consistency and sample efficiency in direct policy alignment. 

%The problem of AI alignment from human preferences is typically formulated as maximizing a universal reward function. However, recent work has shown that desirable alignment methods must account for heterogeneous preferences. Building on this insight, we argue that, when serving a heterogeneous population using a single policy, maximizing the average reward is the right choice, but this is impossible to do with anonymous data. We therefore explore the possibilities and limitations when additional information is available from annotators. Our analysis centers on \emph{direct alignment} methods that learn a policy directly from preference data. In the extreme, when every data point includes feedback from all \emph{user types}, we propose loss functions for consistent learning of the optimal policy. Surprisingly, no sample-efficient consistent loss exists in this setting. %This reveals a fundamental tension between consistency and sample efficiency in direct policy alignment. 
%Overall, our results suggest the need for richer datasets that allow personalized learning of rewards. 
%
% In a nutshell, our findings underscore the importance of modeling heterogeneity in learning from preferences and shed light on tradeoffs in the direct alignment with heterogeneous preferences.

% Aligning a \emph{policy} with human preferences is crucial to ensure it effectively serves their interests. The standard alignment problem formulates this as maximizing a universal notion of human reward. However, human rewards are inherently heterogeneous. We identify key shortcomings of ignoring this heterogeneity and advocate for explicit modeling of it in learning from human preferences. 
% %
% When serving a heterogeneous population using a single policy, we argue for maximization of average reward. However, aligning to the average reward proves impossible with anonymous data. We then explore the possibilities and limitations when additional information is available from annotators. Our analysis centers on \emph{direct alignment} methods that learn a policy directly from preference datasets. 
% % 
% % We find that improving direct alignment objective, even to the first order, is impossible without annotator information. Nonetheless, this becomes feasible when preference data is grouped into pairs, each labeled by the same individual.
% %
% In the extreme, where every data point includes feedback from all \emph{user types}, consistent estimation of the optimal policy is possible, and we propose loss functions for this purpose. Surprisingly, no sample-efficient consistent loss exists in this setting. This reveals a fundamental tension between consistency and sample efficiency in direct policy alignment.
% %
% In a nutshell, our findings underscore the importance of modeling heterogeneity in learning from preferences and shed light on tradeoffs in the direct alignment with heterogeneous preferences.




% % My first sentence will be about alignment being useful and important these days.
% Human preferences are inherently heterogeneous.
% %make the transition smoother. Maybe add a sentence like: Current methods don't not take it into account and assume homogeneous preferenes.
% We identify key shortcomings that arise when this heterogeneity is overlooked in reward modeling or policy alignment. % I'd not discuss reward learning here and keep it to alignment. You need one sentence that introduces alignment and why it's widely used these days before this one.
% %%%%
% % I'd mention that we argue for average reward as a good objective to optimize for. You're already saying that we're formalizing some new notion of reward objective, why the suspense? tell them what it is.
% By formalizing the objective for aligning a single AI agent with a diverse population, we study the possibility of alignment and propose algorithms across varying information settings. 
% %%%%
% When preferences are anonymous, we prove the alignment is fundamentally unattainable. % I understand what you're saying but you can assume heterogeneity and align, as everyone does, and the result is not too bad. You need a more descriptive version of what's impossible. If you introduce the notion of average reward as a good objective earlier, you can say alignment with average reward of a heterogeneous population is impossible.
% %%%%%%%%%%%%%%%%%
% At the other extreme, where every pairwise choice includes feedback from all distinct user \emph{types}, consistent estimation of the optimal policy becomes possible. % and we propose a loss function for it.
% %It's the first time that you use pairwise choice without prior explanation. Consider removing it. We need the preference dataset from which we're learning to have labels from every user type. Or if you wanna be more precise, introduce the dataset with pairwise comparisons beforehand, perhaps when you mention prior methods that overlook heterogeneity, or when you introduce the alignment problem.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % I like 'loss function' more than 'estimator' for the following sentence.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% However, surprisingly, there is no sample-efficient consistent estimator in this setting.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % How do you deduce the trade off from the previous sentence? You can talk about trade-off once you say that if you give up consistency, you'll have a sampleefficient objective function.
% This reveals a fundamental tradeoff between consistency and sample efficiency in direct policy alignment.
% % This is the first time you use 'direct policy alignment' and it's a key term. Make sure you introduce it beforehand, maybe when you describe alignment problem. Also, it might be worth an \emph{}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %again I like 'loss function' or 'objective function' better than 'method'. Method is 'ERM'.
% For example, by forgoing consistency, we design an approximate alignment method that achieves sample efficiency while requiring only minimal annotator information. 
% % I'd mention what minimal information is in a short clause, but it's again a taste thing. I don't like things being unclear.
% %%%%%%
% In a nutshell, our findings underscore the importance of modeling heterogeneity in learning from preferences and shed light on tradeoffs in the direct alignment with heterogeneous preferences.

\end{abstract}