\section{Direct Alignment with \ifnotarxiv \\ \fi Maximum Annotator Information}
\label{sec:dpo_with_known_u}

Recall that learning the optimal policy from anonymous data is impossible, and an approximate improvement to DPO requires only minimal information about the annotations. But what if we collect richer data? Can we design a direct alignment method that consistently learns the optimal policy~$\pi^*$? To explore this, we consider a dataset where every sample is labeled by representatives of all user types. We show that consistent direct alignment is possible using this dataset. 
%However, this comes with an unavoidable trade-off: no sample-efficient method can achieve consistent direct alignment.

% For simplicity,
Suppose user types are in a finite set~$\gU$ and equally represented. This assumption makes our negative results stronger. Consider a rich data collection: for every context and candidate pairs $(\vx, \vy_1, \vy_2)$, we collect one preference data point from each user type. Let $\vo \in \{0,1\}^{|\gU|}$ be the vector that indicates preferences where $o_{u} = 1$ if user of type~$u \in \gU$ has preferred~$\vy_2$, and~$0$ otherwise.
Given such a dataset~$\gD$ with context, candidates, and preferences represented as $(\vx, \vy_1, \vy_2, \vo)$, our goal is to design a loss function
%
\begin{equation}
\label{eq:decompose_L}
    \gL(\gD; \pi) = \frac{1}{|\gD|} \sum_{(\vx, \vy_1, \vy_2, \vo) \in \gD} l(\vx, \vy_1, \vy_2, \vo; \pi)
\end{equation}
%
such that $\argmin_\pi \gL(\gD; \pi)$ is a consistent estimator of~$\pi^*$. 
Designing such a loss is, in fact, possible. For instance, suppose we only look into the agreement cases in~$\gD$ where~$\vo$ is either all one or zero. Conditioned on agreement, we will show that the probability of $\vy_2 \succ \vy_1$ is proportional to~$\exp\big(\sum_u \Delta r^*(\vy_1, \vy_2; u)\big)$. We can write this likelihood in terms~$\pi^*$ directly as we have a correspondence between~$\pi^*$ and the difference in user-weighted average rewards (see \cref{eq:heter_diff_of_rew}). We formally show this possibility through a temperature-adjusted DPO:
%%%
\begin{theoremEnd}[restate]{proposition}
\label{prop:consistent_loss_1}
Defining~$l$ in \cref{eq:decompose_L} as follows results in a consistent estimation of the optimal policy when preferences follow the BT model:
%
\begin{equation*}
    l(\vy_1, \vy_2, \vo; \pi) = \begin{cases}
        -\log \sigma\big( |\gU| \cdot h(\vy_1, \vy_2; \pi) \big) \,, & \vo = \vec{\vone} \,, \\
        -\log \sigma \big( |\gU| \cdot h(\vy_2, \vy_1; \pi) \big) \,, & \vo = \vec{\vzero} \,, \\
        0 & \text{o.w.}
    \end{cases}
\end{equation*}
%
Here, $h$ is the difference of $\pi$'s induced rewards (\cref{eq:def_h}).
\end{theoremEnd}
%%%
\begin{proofEnd}
    The proof follows similar steps as the derivation of DPO. First of all, conditioned on agreement, the likelihood of observing~$\vy_2 \succ \vy_1$ under the BT model is
    %
    \begin{align*}
        \Pr(\vy_2 \succ \vy_1 \mid r^*, {\rm agreement}) &= \frac{\Pi_u \sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)}{\Pi_u \sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big) + \Pi_u \sigma\big(\Delta r^*(\vy_2, \vy_1; u)\big)} \\
        &= \frac{\exp\big(\sum_u r^*(\vy_2; u)\big)}{\exp\big(\sum_u r^*(\vy_1; u)\big) + \exp\big(\sum_u r^*(\vy_2; u)\big)} \\
        &= \sigma\Big(|\gU| \cdot \E_u\big[\Delta r^*(\vy_1, \vy_2)\big]\Big)
        \,.
    \end{align*}
    %
    On the other hand, \cref{eq:heter_diff_of_rew} allows us to write $\E_u\big[\Delta r^*(\vy_1, \vy_2)\big]$ with difference $\pi$'s induced rewards, i.e., $h$: 
    %
    \begin{equation*}
        \Pr(\vy_2 \succ \vy_1 \mid \pi^*, {\rm agreement}) = \sigma\big(h(\vy_1, \vy_2; \pi^*)\big)
        \,.
    \end{equation*}
    %
    We can define the likelihood in this way for every policy~$\pi$. Then, the proposed loss function is equivalent to maximizing log-likelihood, which under mild conditions is a consistent estimator for~$\pi^*$. 
\end{proofEnd}
%%%
Consistent loss function is not unique. We give another example in \cref{prop:consistent_loss_2}, and a systematic way to find such losses in \cref{lem:characterize_l_tilde}. In both examples, loss functions reduce to the standard DPO loss when~$|\gU| = 1$.

While the loss function in \cref{prop:consistent_loss_1} benefits from consistency, it only uses samples where all user types have agreed. In other words, it discards a sample with any disagreement. A natural question arises: Can we design a loss function that uses all data, including those with disagreement, while maintaining consistency? Surprisingly, the answer is no:
%%%
\begin{theoremEnd}[restate]{thm}
\label{thm:no_good_consistent_loss}
Suppose $l$ in \cref{eq:decompose_L} only depends on $(\vx, \vy_1, \vy_2)$ through~$\pi$ and~$\pi_\reff$. If there are more than three types of user and the preferences follow BT, any loss~$\gL$ that allows a consistent estimation of the optimal policy discards samples with disagreement, i.e., those with $\vo \notin \{\vzero, \vone\}$.
\end{theoremEnd}
%%%
\begin{proofEnd}
    The proof involves three steps: First, the next lemma shows that any loss function~$l$ in \cref{eq:decompose_L} with the desired consistency property can only depend on~$\pi$ through the ratio~$\frac{\pi(\vy_2 \mid \vx)}{\pi(\vy_1 \mid \vx)}$.
    
    %%%
    \begin{theoremEnd}[restate]{lemma}
    \label{lem:l_tilde}
    Suppose $l$~in~\cref{eq:decompose_L} only depends on~$(\vx, \vy_1, \vy_2)$ through~$\pi$ and~$\pi_\reff$. Then, for any~$l$ that gives a consistent estimation of the optimal policy in \cref{eq:heter_opt_policy}, there exists an equivalent loss~$\till$ such that 
    %
    \begin{equation*}
        l\big(\vx, \vy_1, \vy_2, \vo; \pi) = \till\big(\vo; h(\vx, \vy_1, \vy_2; \pi)\big)
        \,,
    \end{equation*}
    %
    where~$h$ is defined in \cref{eq:def_h}.
    \end{theoremEnd}
    %%%
    \begin{proofEnd}
        Since $l$ only depends on~$(\vx, \vy_1, \vy_2)$ through~$\pi$ and~$\pi_\reff$, we overload the notation and use $l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big)$ to denote the loss from~$(\vx, \vy_1, \vy_2, \vo)$. In the limit of many data points, $\gL(\gD; \pi)$ converges to
        %
        \begin{equation*}
            \gL(\pi) = \E_{\vx, \vy_1, \vy_2, \vo}\big[l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big)\big]
            \,.
        \end{equation*}
        %
        Using $\Pr(o_u = 1 \mid \vx, \vy_1, \vy_2) = \sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)$, the tower rule implies
        %
        \begin{align*}
            \gL(\pi) &= \E_{\vx, \vy_1, \vy_2}\Big[\E_{\vo}\big[l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big) \mid \vx, \vy_1, \vy_2\big]\Big] \\
            &= \E_{\vx, \vy_1, \vy_2}\Big[
            \sum_{\vo \in \{0, 1\}^{\gU}}
            l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big) \cdot
            \prod_{u \in \gU} \sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)^{o_u} \big(1 - \sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)\big)^{1 - o_u}
            \Big]
            \,.
        \end{align*}
        %
        For notational simplicity, let's define
        %
        \begin{align*}
            z_u(\vx, \vy_1, \vy_2) &\coloneqq \sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big) \,, \\
            \chi_\vo(\vz) &\coloneqq \prod_{u \in \gU} z_u^{o_u} (1 - z_u)^{1 - o_u}
            \,.
        \end{align*}
        %
        Note that $\chi_\vo$ implicitly depends on $(\vx, \vy_1, \vy_2)$ through $\vz$, which we drop from the notation when it is clear from the context. Using this notation, we can rewrite the $\gL(\pi)$'s expansion as follows:
        %
        \begin{equation}
        \label{eq:_proof_l_expansion}
            \gL(\pi) = \E_{\vx, \vy_1, \vy_2}\Big[
            \sum_{\vo \in \{0, 1\}^{\gU}}
            l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big) \cdot
            \chi_\vo\big(\vz(\vx, \vy_1, \vy_2)\big)
            \Big]
            \,.
        \end{equation}
        %
    
        Overloading notation, we can always equivalently represent $\big(\pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big)$ as $\big(\pi({\vy_1, \vy_2} \mid \vx), \pi(\vy_2 \mid {\vy_1, \vy_2}, \vx)\big)$, that is, with the probability that either of the two responses is chosen and the probability that the second one is preferred. Therefore, there exists a loss~$l'$ such that
        %
        \begin{equation*}
            l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big) = 
            l'\big(\vo; \pi(\{\vy_1, \vy_2\} \mid \vx), \pi(\vy_2 \mid \{\vy_1, \vy_2\}, \vx)\big)
            \,.
        \end{equation*}
        %
        
        If $\pi$ is optimal, $\pi(\vy_2 \mid {\vy_1, \vy_2}, \vx)$ should also be optimal. Since $\pi(\vy_2 \mid {\vy_1, \vy_2}, \vx)$ appears in only one term of the expectation in \cref{eq:_proof_l_expansion}, we can conclude that
        %
        \begin{equation*}
            \argmin_{\theta'} \, \sum_{\vo \in \{0, 1\}^{\gU}}
            l'\big(\vo; \pi^*(\{\vy_1, \vy_2\} \mid \vx), \theta'\big) \cdot
            \chi_\vo\big(\vz(\vx, \vy_1, \vy_2)\big)
        \end{equation*}
        %
        is the optimal $\pi(\vy_2 \mid \{\vy_1, \vy_2\}, \vx)$ for every optimal $\pi(\{\vy_1, \vy_2\} \mid \vx)$.  
        On the other hand, a property of the optimal policy~$\pi^*$ is that 
        %
        \begin{equation*}
            \pi^*(\vy_2 \mid \{\vy_1, \vy_2\}, \vx) = \frac{\pi^*(\vy_2 \mid \vx)}{\pi^*(\vy_1 \mid \vx)} = \frac{\pi_\reff(\vy_2 \mid \vx)}{\pi_\reff(\vy_1 \mid \vx)} \cdot \exp\Big(\frac{1}{\beta}\E_u\big[\Delta r^*(\vx, \vy_1, \vy_2; u)\big]\Big)
            \,.
        \end{equation*}
        %
        Therefore, for every optimal policy~$\pi^*(\{\vy_1, \vy_2\} \mid \vx)$, we have 
        %
        \begin{equation}
        \begin{aligned}
        \label{eq:_proof_min_of_l_prime}
            &\frac{\pi_\reff(\vy_2 \mid \vx)}{\pi_\reff(\vy_1 \mid \vx)} \cdot \exp\Big(\frac{1}{\beta}\E_u\big[\Delta r^*(\vx, \vy_1, \vy_2; u)\big]\Big) = \\
            &\quad\quad\argmin_{\theta'} \, 
            \sum_{\vo \in \{0, 1\}^{\gU}} l'\big(\vo; \pi^*(\{\vy_1, \vy_2\} \mid \vx), \theta'\big) \cdot \chi_\vo\big(\vz(\vx, \vy_1, \vy_2)\big)
            \,.
        \end{aligned}
        \end{equation}
        %
        Recall from the optimal policy (\cref{eq:heter_opt_policy}) that we can modify the reward function for responses other than $(\vx, \vy_1, \vy_2)$ while keeping $\Delta r^*(\vx, \vy_1, \vy_2; u)$ constant. This allows arbitrary changes to $\pi^*(\{\vy_1, \vy_2\} \mid \vx)$ without altering the rest of \cref{eq:_proof_min_of_l_prime}. So, we can argue that $l'$ does not depend on~$\pi(\{\vy_1, \vy_2\} \mid \vx)$ and we drop it from~$l'$ notation. Define a new loss based on~$l'$:
        %
        \begin{equation*}
            \till(\vo; \theta)  \coloneqq l'\Big(\vo; \frac{\pi_\reff(\vy_2 \mid \vx)}{\pi_\reff(\vy_1 \mid \vx)} \cdot \exp\big(\frac{1}{\beta}\theta\big)\Big)
            \,.
        \end{equation*}
        %
        Note that $\till$ implicitly depends on~$(\vx, \vy_1, \vy_2)$ through~$\pi_\reff$ which we dropped from notation. Using~$\till$, we can write the original loss~$l$ as
        %
        \begin{align*}
            l\big(\vo; \pi(\vy_1 \mid \vx), \pi(\vy_2 \mid \vx)\big) &= 
            l'\big(\vo; \pi(\vy_2 \mid \{\vy_1, \vy_2\}, \vx)\big) \\
            &= l'\Big(\vo; \frac{\pi(\vy_2 \mid \vx)}{\pi(\vy_1 \mid \vx)}\Big) \\
            &= \till\Big(\vo; \beta \log \frac{\pi(\vy_2 \mid \vx)}{\pi(\vy_1 \mid \vx)} - \beta \log \frac{\pi_\reff(\vy_2 \mid \vx)}{\pi_\reff(\vy_1 \mid \vx)} \Big) \\
            &= \till\big(\vo; h(\vx, \vy_1, \vy_2; \pi)\big)
            \,.
        \end{align*}
        %
        This completes the proof.
    \end{proofEnd}
    %%%
    
    In the second step, we further limit the search space of $\till$ (as introduced by \cref{lem:l_tilde}) to those that meet certain first- and second-order conditions:
    
    %%%
    \begin{theoremEnd}[restate]{lemma}
    \label{lem:characterize_l_tilde}
    Any loss~$\till$ as in \cref{lem:l_tilde} that leads to a consistent estimation of the optimal policy meets
    %
    \begin{align*}
        &\sum_{\vo \in \{0, 1\}^{\gU}} \pd{\till}{\theta}\big(\vo; \theta^*(\vz)\big) \cdot \chi_\vo\big(\vz\big) = 0 \,, \\
        &\sum_{\vo \in \{0, 1\}^{\gU}} \pd[2]{\till}{\theta}\big(\vo; \theta^*(\vz)\big) \cdot \chi_\vo\big(\vz\big) \ge 0 
        \,,
    \end{align*}
    %
    for every $\vz \in [0, 1]^{\gU}$. Here, we define
    %
    \begin{equation*}
        \chi_\vo(\vz) \coloneqq \prod_{u \in \gU} z_u^{o_u} (1 - z_u)^{1 - o_u}
        \,.
    \end{equation*}
    %
    and 
    %
    \begin{equation*}
        \theta^*(\vz) \coloneqq \frac{1}{|\gU|} \sum_{u \in \gU} \sigma^{-1}(z_u)
        \,.
    \end{equation*}
    %
    \end{theoremEnd}
    %%%
    \begin{proofEnd}
        We will refer to the proof of \cref{lem:l_tilde} in this proof. Using $\till$ in place of~$l'$ in \cref{eq:_proof_min_of_l_prime}, since $\exp(\cdot)$ is monotone increasing, we have
        %
        \begin{equation*}
            \E_u\big[\Delta r^*(\vx, \vy_1, \vy_2; u)\big] = 
            \argmin_{\theta} \, 
            \sum_{\vo \in \{0, 1\}^{\gU}} \till(\vo; \theta) \cdot \chi_\vo\big(\vz\big)
            \,.
        \end{equation*}
        %
        On the other hand, using the fact that user types in~$\gU$ are equiprobable, we can write 
        %
        \begin{equation*}
            \E_u\big[\Delta r^*(\vx, \vy_1, \vy_2; u)\big] 
            = \frac{1}{|\gU|} \sum_{u \in \gU} \sigma^{-1}(z_u)
            \,.
        \end{equation*}
        %
        Putting these together, it is necessary to have
        %
        \begin{equation*}
            \frac{1}{|\gU|} \sum_{u \in \gU} \sigma^{-1}(z_u) = 
            \argmin_{\theta} \, 
            \sum_{\vo \in \{0, 1\}^{\gU}} \till(\vo; \theta) \cdot \chi_\vo\big(\vz\big)
        \end{equation*}
        %
        for every $\vz \in [0, 1]^{\gU}$. The rest of the proof is straightforward.
    \end{proofEnd}
    %%%
    
    Finally, we show that when preferences follow the BT model and there are more than three user types, all $\till(\vo; \theta)$ terms corresponding to $\vo \notin {\vzero, \vone}$ do not depend on~$\theta$. Therefore, these terms do not depend on~$\pi$ and can be removed from the loss function, thereby completing the proof. 
    
    %%%
    \begin{theoremEnd}[restate]{lemma}
    If $|\gU| > 3$, for any loss~$\till$ that meets the first-order condition of \cref{lem:characterize_l_tilde}, we have $\pd{\till}{\theta}(\vo; \theta) = 0$ for every~$\vo \notin \{\vzero, \vone\}$. 
    \end{theoremEnd}
    %%%
    \begin{proofEnd}
        First of all, for the BT model, a direct calculation shows
        %
        \begin{equation*}
            \theta^*(\vz) \coloneqq \frac{1}{|\gU|} \sum_{u \in \gU} \sigma^{-1}(z_u)
            = \frac{1}{|\gU|} \log \prod_{u \in \gU} \big(\frac{z_u}{1 - z_u}\big) 
            = \frac{1}{|\gU|} \log \big(\frac{\chi_\vone}{\chi_\vzero}\big)
            \,.
        \end{equation*}
        %
        Since $\theta^*(\vz)$ depends on~$\vz$ only through~$\frac{\chi_\vzero}{\chi_\vone}$, we denote $\pd{\till}{\theta}\big(\vo; \theta^*\big)$ by $g\big(\vo; \frac{\chi_\vzero}{\chi_\vone}\big)$.
        %
        The proof has two steps: In the first step, we relate $g\big(\vo; \frac{\chi_\vzero}{\chi_\vone}\big)$ to $g\big(\vo^{\oplus u'}; \frac{\chi_\vzero}{\chi_\vone}\big)$, where we define
        %
        \begin{equation*}
            \vo^{\oplus u'} \coloneqq \begin{cases}
                o_u \,, & u \neq u' \,, \\
                1 - o_u \,, & u = u' \,.
            \end{cases}
        \end{equation*}
        %
        Using this connection, in the second step, we will show that $g\big(\vo; \frac{\chi_\vzero}{\chi_\vone}\big) = 0$ for any~$\vo \in \{\vzero, \vone\}$ when $|\gU| \ge 4$.
        
        \paragraph{Step 1.}
        Consider any~$\vo \notin \{\vzero, \vone\}$. When $|\gU| \ge 3$, there exists~$u' \in \gU$ such that $\vo^{\oplus u'} \notin \{\vzero, \vone\}$. For such~$\vo$ and~$u'$, we define two non-empty sets
        %
        \begin{align*}
            \gS^1 &\coloneqq \{u \mid u \in \gU, u \neq u', o_u = 1\} \,,\\
            \gS^0 &\coloneqq \{u \mid u \in \gU, u \neq u', o_u = 0\} \,.
        \end{align*}
        %
        We set $\vz$ such that
        %
        \begin{equation}
        \begin{aligned}
        \label{eq:_proof_set_z}
            &\prod_{u \in \gS^1} z_u = \prod_{u \in \gS^0} (1 - z_u)
            \,, \\
            &z_u \rightarrow 1^-\,, \;\forall u \in \gS^1 \,,\\
            &z_u \rightarrow 0^+\,, \;\forall u \in \gS^0 \,.
        \end{aligned}
        \end{equation}
        %
        For this choice of~$\vz$, asymptotically, we have
        %
        \begin{align*}
            \frac{\chi_\vzero}{\chi_\vone} &= \frac{1 - z_{u'}}{z_{u'}} \,, \\
            \chi_\vo &= z_{u'}^{o_{u'}} (1 - z_{u'})^{1 - o_{u'}} \,, \\
            \chi_{\vo^{\oplus u'}} &= z_{u'}^{1 - o_{u'}} (1 - z_{u'})^{o_{u'}} \,, \\
            \chi_{\vo'} &= 0 \,, \; \forall \vo' \notin \{\vo, \vo^{\oplus u'}\} \,.
        \end{align*}
        %
        Using the above, we can simplify the first-order condition in \cref{lem:characterize_l_tilde} as
        %
        \begin{equation*}
            g\big(\vo; \frac{1 - z_{u'}}{z_{u'}}\big) \cdot z_{u'}^{o_{u'}} (1 - z_{u'})^{1-o_{u'}} 
            + g\big(\vo^{\oplus u'}; \frac{1 - z_{u'}}{z_{u'}}\big) \cdot z_{u'}^{1 - o_{u'}} (1 - z_{u'})^{o_{u'}}  = 0
            \,.
        \end{equation*}
        %
        This condition should be held for every~$z_{u'} \in [0, 1]$. Therefore, we can conclude
        %
        \begin{equation}
        \label{eq:_proof_g_one_bit_flipped}
            g(\vo^{\oplus u'}; \alpha) = - g(\vo; \alpha) \cdot \alpha^{1-2 o_{u'}}
            \,,
        \end{equation}
        %
        for every~$\alpha \in \R$. This completes the first part of the proof.
    
        \paragraph{Step 2.}
        When $\vo \notin \{\vzero, \vone\}$ and $|\gU| \ge 4$, there exist distinct user types~$u'$ and~$u''$ such that none of $\vo^{\oplus u'}$, $\vo^{\oplus u''}$, and $\vo^{\oplus (u', u'')}$ are in $\{\vzero, \vone\}$. Here, we used $\vo^{\oplus (u', u'')}$ as a shorthand for $(\vo^{\oplus u'})^{\oplus u''}$. For such~$\vo$,~$u'$,~and~$u''$, we define two non-empty sets
        %
        \begin{align*}
            \gS^1 &\coloneqq \{u \mid u \in \gU \setminus \{u', u''\}, o_u = 1\} \,,\\
            \gS^0 &\coloneqq \{u \mid u \in \gU \setminus \{u', u''\}, o_u = 0\} \,.
        \end{align*}
        %
        We set~$\vz$ according to \cref{eq:_proof_set_z}. Then, asymptotically, 
        %
        \begin{equation*}
            \chi_{\vo'} = 0 \,, \; \forall \vo' \notin \{\vo, \vo^{\oplus u'}, \vo^{\oplus u''}, \vo^{\oplus (u', u'')}\}
            \,.
        \end{equation*}
        %
        Using the above, we can simplify the first-order condition in \cref{lem:characterize_l_tilde} as
        %
        \begin{equation}
        \label{eq:_proof_four_g_terms}
            g\big(\vo; \frac{\chi_\vzero}{\chi_\vone}\big) \cdot \chi_\vo 
            + g\big(\vo^{\oplus u'}; \frac{\chi_\vzero}{\chi_\vone}\big) \cdot \chi_{\vo^{\oplus u'}} 
            + g\big(\vo^{\oplus u''}; \frac{\chi_\vzero}{\chi_\vone}\big) \cdot \chi_{\vo^{\oplus u''}} 
            + g\big(\vo^{\oplus (u', u'')}; \frac{\chi_\vzero}{\chi_\vone}\big) \cdot \chi_{\vo^{\oplus (u', u'')}} 
            = 0
            \,.
        \end{equation}
        %
        Because of the symmetry of this equation, we can assume without loss of generality that $o_{u'} = 0$ and $o_{u''} = 1$. Therefore, asymptotically, we have
        %
        \begin{align*}
            \frac{\chi_\vzero}{\chi_\vone} &= \big(\frac{1 - z_{u'}}{z_{u'}}\big) \big(\frac{1 - z_{u''}}{z_{u''}}\big) \,, \\
            \chi_\vo &= (1 - z_{u'}) \cdot z_{u''} \,, \\
            \chi_{\vo^{\oplus u'}} &= z_{u'} \cdot z_{u''} \,, \\
            \chi_{\vo^{\oplus u''}} &= (1 - z_{u'}) \cdot (1 - z_{u''}) \,, \\
            \chi_{\vo^{\oplus (u', u'')}} &= z_{u'} \cdot (1 - z_{u''}) \,.
        \end{align*}
        %
        \cref{eq:_proof_g_one_bit_flipped} also implies
        %
        \begin{align*}
            g(\vo^{\oplus u'}; \alpha) &= -g(\vo; \alpha) \cdot \alpha \,, \\
            g(\vo^{\oplus u''}; \alpha) &= -g(\vo; \alpha) \cdot \alpha^{-1} \,, \\
            g(\vo^{\oplus (u', u'')}; \alpha) &= g(\vo; \alpha) \,.
        \end{align*}
        %
        Plugging these into \cref{eq:_proof_four_g_terms} and simplifying equations, we obtain
        %
        \begin{equation*}
            g\Big(\vo; \big(\frac{1 - z_{u'}}{z_{u'}}\big) \big(\frac{1 - z_{u''}}{z_{u''}}\big)\Big) \cdot (2 z_{u'} - 1) (2 z_{u''} - 1)
            = 0
            \,.
        \end{equation*}
        %
        This equation should be held for every~$z_{u'}$ and~$z_{u''}$. By appropriately setting~$z_{u'}$ and~$z_{u''}$, we can conclude that $g(\vo; \alpha)$ should be zero for every~$\alpha$. This completes this proof.
    \end{proofEnd}
    %%%

\end{proofEnd}
%%%
This theorem highlights a tension: To improve efficiency, one must compromise either consistency or direct optimization. 
The approximate direct alignment method proposed in \cref{sec:dpo_corrected} exemplifies forgoing consistency. Next, we discuss an alternative that favors consistency.


\paragraph{An Indirect Practical Solution: Averaging Personalized Rewards.}
The tradeoff between sample efficiency and consistency arises from the requirement for direct optimization. 
%Direct alignment methods simplify RLHF by bypassing the need to train a reward model and apply RL.
To regain sample efficiency, we may relax the requirement for direct alignment by training reward models while still avoiding RL. Specifically, we can learn personalized reward models~$r(\cdot; u)$ for different user types~$u \in \gU$, calculate a user-weighted expected reward, and use it to relabel a preference dataset.
A dataset labeled with this average reward makes any direct alignment method applicable and avoids RL. It is both consistent and sample-efficient when personalized reward learning is feasible, but comes at the cost of additional training for each user type and memory to store multiple models.
\ifarxiv Relabeling followed by DPO is found effective in practice~\citep{frick2024evaluate}. \fi

