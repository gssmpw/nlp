\section{Additional Related Work}
\label{sec:related_appendix}
The challenge of handling heterogeneous preferences in alignment has been recognized as a significant problem in alignment research~\citep{anwar_foundational_2024, casper_open_2023,ge2024axioms,sorensen_roadmap_2024}. This problem has attracted considerable attention from researchers in the field. Here, we highlight a few representative works that address key directions in tackling this challenge.

\paragraph{Analysis of DPO.}
Our study of how standard preference learning methods, such as DPO, behave in the presence of heterogeneous preferences was inspired by \citet{siththaranjan2023distributional}'s result, which shows that RLHF aggregates preferences according to a well-known voting rule called Borda count. \citet{chakraborty2024maxmin} highlights the impossibility of aligning with a singular reward model in RLHF by providing a lower bound on the gap between the optimal policy and a subpopulation's optimal policy. \citet{dumoulin2023density} adopts a density estimation perspective on learning from human feedback to illustrate the challenges of preference learning from a population of annotators with diverse viewpoints. \citet{rosset_direct_2024} and~\citet{gao_rebel_2024} point out the limitations of point-wise reward models in expressing complex, intransitive preferences that may arise due to the aggregation of diverse preferences. Additionally, frameworks that generalize DPO and unify different alignment methods have been proposed to analyze current approaches and explore possible alternatives~\citep{chen_mallowspo_2024, tang_generalized_2024, azar2024general, meng2024simpo}.


\paragraph{Policy Personalization.} 
Many works in the literature have proposed personalization as a solution to the problem of pluralistic alignment. \citet{poddar_personalizing_2024} propose a latent variable formulation of the problem and learn rewards and policies conditioned on it. \citet{chen_pal_2024} use an ideal point model for preferences and learn latent spaces representing different preferences. Mapping user information to user representations, \citet{li_personalized_2024} perform personalized DPO to jointly learn a user model and a personalized language model. \citet{balepur2025boatdoesfloatimproving} use abductive reasoning to infer user personas and train models to tailor responses accordingly. \citet{lee_aligning_2024} explore the possibility of steering a language model to align with a user's intentions through system messages. \citet{dang_personalized_2025} extend personalized alignment to text-to-image diffusion models. \citet{jang_personalized_2023} perform personalized alignment by decomposing preferences into multiple dimensions. \citet{lau_personalized_2024} dynamically adapt the model to individual preferences using in-context learning.


\paragraph{Preference Aggregation.} 
Closely aligned with our goal of serving the entire population with a single policy, several works have explored ways to aggregate diverse preferences. The rich literature on social choice theory has proven to be a valuable source of inspiration for studying existing preference learning approaches and proposing new ones~\citep{conitzer2024social, qiu_representative_2024,alamdari_policy_2024,ge2024axioms,dai2024mapping}. Drawing insights from social choice theory, robustness to approximate clones has been proposed as a desirable property of RLHF algorithms, which current methods lack~\citep{procaccia2025clone}. The Minimax Winner, a concept in preference aggregation, has inspired the use of the proportion of wins as the reward for a particular trajectory to align a model through self-play~\citep{swamy_minimaximalist_2024}. The impact of heterogeneity on strategic behavior in feedback and its effects on aggregation are also explored in~\citet{park2024rlhf}, which further examines the use of different social welfare functions for preference aggregation.


\paragraph{Methods.} 
Solutions proposed to address different formulations of the problem span a wide range of methods. \citet{siththaranjan2023distributional} estimate a distribution of scores for alternatives to account for heterogeneity as hidden context. \citet{chidambaram2024direct} propose an Expectation-Maximization (EM) version of DPO to minimize a notion of worst-case regret. Multi-objective reinforcement learning~\citep{harland_adaptive_2024, jang_personalized_2023} and its direct optimization variant~\citep{zhou_beyond_2024} have also been proposed to align with diverse preferences. \citet{wang_arithmetic_2024} train a multi-objective reward model to capture diverse preferences. \citet{zhong_provable_2024} use meta-learning to learn diverse preferences and aggregate them using different social welfare functions. \citet{li_aligning_2024} design an optimal-transport-based loss to calibrate their model with the categorical distribution of preferences.
 Producing a Pareto front of models has also been explored as a solution. \citet{boldi_pareto-optimal_2024} employ an iterative process to select solutions, while~\citet{rame_rewarded_2023} interpolate the weights of independent networks linearly to achieve a Pareto-optimal generalization across preferences. 

\paragraph{Empirical Observations.} 
Empirical studies of alignment methods have had a significant impact on the study of preference learning. \citet{zhang_diverging_2024} demonstrate the Bradley-Terry model's failure to distinguish between unanimous agreement among annotators and the majority opinion in cases of diverging user preferences. \citet{chen_preference_2024} show that RLHF and DPO struggle to improve ranking accuracy. \citet{zeng_diversified_2024} study the role of model size and data size in the impact of diversified human preferences. \citet{bansal_peering_2024} demonstrate the significant influence of feedback protocol choice on alignment evaluation. \citet{santurkar_whose_2023} explore the opinions reflected by a language model, while \citet{bakker_fine-tuning_2022} investigate a language model's ability to generate consensus statements by training it to predict individual preferences. \citet{jiang_can_2024} propose individualistic alignment to predict an individual's values, and \citet{zollo_personalllm_2024} introduce the PersonalLLM benchmark to measure a model's adaptation to a particular user's preferences.

