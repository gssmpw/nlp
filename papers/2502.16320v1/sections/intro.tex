\section{Introduction}

% 1) Human preferences are diverse but learning often ignores that
Human rewards and preferences are heterogeneous~\citep{aroyo2015truth,hetro-1,denton2021whose,hetro-2,zhang_diverging_2024}. Despite this, learning from preference data often bypasses this insight, relying on what we dub the \emph{preference homogeneity assumption}. 
This tension in assumptions is readily apparent in standard human-AI alignment methods---such as reinforcement learning from human feedback (RLHF) \citep{ziegler2019fine,rlhf-summarize,ouyang2022training} and direct preference optimization (DPO) \citep{rafailov2024direct}---which assume a single reward function captures the interests of the entire population.


% 2) What happens if we use the homogeneity assumption?
We examine the limits of the preference homogeneity assumption when individuals belong to \emph{user types}, each characterized by a specific reward function. 
Recent work has shown that in this setting, the homogeneity assumption can lead to unexpected behavior~\citep{conitzer2024social,ge2024axioms,sorensen_roadmap_2024}. One challenge is that, under this assumption, learning from human preferences becomes \emph{unrealizable}, as a single reward function cannot capture the complexity of population preferences with multiple reward functions~\citep{dumoulin2023density,park2024rlhf}. Both RLHF and DPO rely on maximum likelihood estimation (MLE) to optimize the reward or policy. Unrealizability implies their likelihood functions cannot fully represent the underlying preference data distribution, resulting in a nontrivial optimal MLE solution. From another perspective, learning a universal reward or policy from a heterogeneous population inherently involves an \emph{aggregation} of diverse interests, and this aggregation is nontrivial. 

In the quest for a single policy that accommodates a heterogeneous population with multiple user types, we show that the only universal reward yielding a well-defined alignment problem is an affine aggregation of the reward functions across user types, with the average reward as a natural choice. However, standard methods like DPO do not maximize this user-weighted average reward. Building on insights by \citet{siththaranjan2023distributional}, we show that DPO implicitly maximizes Borda count, which comes with unexpected drawbacks, e.g., the optimal solution depends on how alternative responses are sampled, even for infinite data. 

% 4) Impossbility with no annotator information + raise the information question 
We observe that learning the average reward over user types---or equivalently, a policy that maximizes it---from anonymous data is impossible. Focusing on \emph{direct alignment methods}, which avoid explicit reward modeling, we study the benefits of using annotator data for a range of information settings. We show that improving DPO with a first-order correction to its objective is possible with minimal annotator information. Specifically, we design an approximate direct alignment method when each preference data point is paired with another one labeled by the same user.

% 6) Direct alignment with maximum information
On the other hand, we find that there are limits to what is possible even with significant annotator information. In particular, we propose a consistent loss function for direct alignment when we have feedback on each data point from each user type. But this loss is sample-inefficient, using only data where all annotator types agree. Surprisingly, we prove that no consistent loss uses the rest of the data. 

% 7) Emphasize the tradeoffs
In summary, the homogeneity assumption leads to undesirable outcomes when aligning a single AI agent to diverse preferences. Our analysis shows that there is a limited class of reward aggregation that results in a valid objective for alignment, with average reward over user types emerging as the natural candidate. This requires some annotator data, though small amounts of data can yield significant improvements. Our findings, however, uncover a fundamental tension between consistency and sample efficiency in direct alignment. To achieve both sample efficiency and consistency, we must forgo the benefits of direct optimization and instead train individualized reward models, which inevitably incurs significant training and storage costs. 
