\section{Preliminaries}
\label{sec:prelim}

In the \emph{alignment problem}, we consider a setting where a \emph{reward} function~$r^*$ evaluates responses to queries. Formally, $r^*([\vx, \vy])$ is the reward value of responding~$\vy$ to a query~$\vx$. 

The alignment problem involves designing a \emph{policy}, which chooses high-reward responses. Let~$\pi$ denote a policy, defining a probability distribution over possible responses: i.e., given a query~$\vx$, $\pi$ returns~$\vy$ with probability~$\pi(\vy \mid \vx)$. 
%
Commonly, we start with a reference policy~$\pi_\reff$, which serves as a prior over~$\vy$ \citep{korbak2022rl}. The goal is then to find a new policy~$\pi$ that, for every~$\vx$, maximizes 
%the expected reward under the distribution induced by $\pi$: 
%
\begin{equation}
\label{eq:alignment_problem}
    \E_{\vy \sim \pi(\cdot \mid \vx)}\big[r^*([\vx, \vy])\big] - \beta \KL\big(\pi(\cdot \mid \vx); \pi_\reff(\cdot \mid \vx)\big)
    \,.
\end{equation}
%
We denote the optimal policy by~$\pi^*$. In practice, $\pi_\reff$ is often a pretrained language model, and the regularization parameter 
$\beta$ controls deviation from it. 
\ifarxiv \cref{eq:alignment_problem} often includes $\E_\vx$, which is important in practice but does not affect~$\pi^*$ in theory. \fi

When $r^*$~is explicitly known, we can directly apply RL to maximize~\cref{eq:alignment_problem}. In many real-world settings, however, we do not know $r^*$ and must estimate it. In such cases, we can collect human feedback to infer the reward function, after which we can use RL to optimize the policy, commonly known as RLHF. While RLHF is widely used, tuning this approach can be challenging due to the inherent complexities of RL. 
Recently, \emph{direct alignment with preferences} has gained popularity as an alternative approach~\citep{zhao2023slic,rafailov2024direct,olaif}. Unlike RLHF, direct alignment methods bypass explicit reward modeling to instead train a policy directly from human feedback. 


\paragraph{Preference Model.}
Both direct alignment and RLHF rely on a model of human preference to relate reward values with observed preference data. 
%TODO: add a transition sentence here about the role of preference models since we don't know rewards? 
Consider the case where responses~$\vy_1, \vy_2$ are generated for a given query~$\vx$. We express the probability that $\vy_2$ is preferred to~$\vy_1$ as
%
\begin{equation}
\label{eq:BT}
    \Pr(\vy_2 \succ \vy_1 \mid \vx; r^*) = \sigma\big(r^*([\vx, \vy_2]) - r^*([\vx, \vy_1])\big)
    \,,
\end{equation}
%
where $\sigma$ is a non-decreasing function in $[0, 1]$~\citep{thurstone1927law,luce1959individual,yellott1977relationship}. A widely-used choice for~$\sigma$ is the sigmoid function corresponding to the well-known Bradley-Terry (BT) model~\citep{bt}. 
%TODO if we have ample space, it could also be nice to say something quick about why people like BT 
%%%%%


%%%%%
\paragraph{Direct Preference Optimization.}
Among direct alignment methods, DPO has emerged as the most widely used approach. It leverages a closed-form solution to~\cref{eq:alignment_problem}, which allows it to link any reward directly to its optimal policy. Thereby, rather than explicitly estimating the reward, DPO optimizes a policy whose \emph{induced reward} best explains the observed preferences. We derive this connection below:

First, maximizing~\cref{eq:alignment_problem} has a well-known solution \citep{ziebart2010modeling}. The optimal policy~$\pi^*$ takes the form:
%
\begin{equation}
\label{eq:opt_policy}
    \pi^*(\vy \mid \vx) = \frac{1}{Z(\vx)} \, \pi_\reff(\vy \mid \vx) \cdot \exp\Big(\frac{1}{\beta}  r^*([\vx, \vy])\Big)
    \,.
\end{equation}
%
Here, $Z(\vx) = \sum_{\vy'} \pi_\reff(\vy' \mid \vx) \cdot \exp(\frac{1}{\beta}  r^*([\vx, \vy']))$ is the partition function. \cref{eq:opt_policy} establishes a direct relationship between policy ratios and reward differences:
%
\begin{equation}
\label{eq:diff_of_rew}
    r^*(\vy_2) - r^*(\vy_1) = \beta \log\frac{\pi^*(\vy_2)}{\pi_\reff(\vy_2)} - \beta \log\frac{\pi^*(\vy_1)}{\pi_\reff(\vy_1)}
    \,.
\end{equation}
%
Henceforth, we omit~$\vx$ when we can do so without ambiguity. This equation shows that the difference in rewards between two responses is fully captured by the difference in their policy ratios. 
Using this formulation, we can define the \emph{induced reward} of a policy~$\pi$ by $\beta \log\frac{\pi(\vy)}{\pi_\reff(\vy)}$.  The induced reward of~$\pi$ is the reward for which $\pi$~is the optimal policy.

The difference in rewards of $\vy_1$ and $\vy_2$ is sufficient to express the likelihood of $\vy_2 \succ \vy_1$ in \cref{eq:BT}. Using \cref{eq:diff_of_rew}, we can therefore write the likelihood as a function of $\pi^*$: 
{\ifnotarxiv\small\fi
\begin{equation}
\label{eq:likelihood}
    \Pr(\vy_2 \succ \vy_1 \mid \pi^*) = \sigma \Big(\beta \log\frac{\pi^*(\vy_2)}{\pi_\reff(\vy_2)} - \beta \log\frac{\pi^*(\vy_1)}{\pi_\reff(\vy_1)} \Big)
    .
\end{equation}
}For any policy~$\pi$, we can define~$\Pr(\vy_2 \succ \vy_1 \mid \pi)$ similarly. We can then estimate~$\pi^*$ using MLE: Given a dataset~$\gD$ with query and response pairs $(\vx, \vy_l, \vy_w)$, where $\vy_w \succ \vy_l$, DPO finds~$\pi^*$ by maximizing the log-likelihood 
%
\begin{equation}
\label{eq:dpo_ml}
    \sum_{(\vx, \vy_l, \vy_w) \in \gD} \log \Pr(\vy_w \succ \vy_l \mid \vx; \pi)
    \,,
\end{equation}
%
or equivalently, minimizing the cross-entropy loss. Under mild assumptions, MLE is a consistent estimator of~$\pi^*$.

%%%%%