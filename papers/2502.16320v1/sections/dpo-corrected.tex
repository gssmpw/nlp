\section{Approximate Direct Alignment with \ifarxiv \\ \fi Minimal Annotator Information}
\label{sec:dpo_corrected}

The failure of standard alignment methods to find the optimal policy~$\pi^*$ raises the question of whether it is even possible to design a method that identifies~$\pi^*$. The answer is no without annotator information. To show this, it suffices to prove that the ranking based on the user-weighted average reward is not learnable. This implies that~$\pi^*$ is also not learnable since its induced reward corresponds to this average reward by definition. We defer the formal definition of learnability to \cref{def:learnability} in the appendix. Based on this definition, we prove the following impossibility:
%%%
\begin{theoremEnd}[restate]{proposition}
\label{prop:impossible_anonymous_learning}
If there are at least two alternatives and two user types with a continuous preference model, the ranking based on the user-weighted expected reward is not learnable without annotator information.
\end{theoremEnd}
%%%
\begin{proofEnd}
    We give three related proof strategies. The first strategy works for every preference model. The second strategy draws on a connection to a robust version of Arrow's impossibility theorem~\citep{friedgut2002boolean}. The third strategy is inspired by \citet{procaccia2025clone}. We start with the notation and definitions specific to this proof. 
    
    \paragraph{Notation and Definitions.}
    Consider a fixed prompt~$\vx$ with a set of possible responses~$\gY$. Let~$R$ denote a \emph{complete ranking} over~$\gY$, where $\vy_2 R \vy_1$ indicates whether $\vy_2 \succ \vy_1$ or vice versa. A \emph{profile} refers to a set of complete rankings. For a heterogeneous reward function~$r(\vy; u)$ and a prior~$\gP$ over user types~$\gU$, let~$R_\barr$ be the ranking according to~$\barr(\vy) \coloneqq \E_{u\sim\gP}[r(\vy; u)]$.
    
    A \emph{pairwise preference dataset}~$\gD$ consists of tuples $(\vy_1, \vy_2, o)$, where $o \coloneqq \One\{\vy_2 \succ \vy_1\}$. We assume that $\vy_1$ and~$\vy_2$ are i.i.d. draws. When a random user with a reward function~$r$ labels each instance in the dataset, we denote the resulting dataset by~$\gD_r$. A \emph{pairwise learning algorithm}~$\gA$ produces a complete ranking over~$\gY$ based on the pairwise preference dataset~$\gD$. 

    \paragraph{Proof Strategy 1.}
    Suppose there exists an algorithm~$\gA$ such that for some~$\gY$ with $|\gY| \ge 2$, for any reward function~$r$ and any preference dataset~$\gD_r$ with $|\gD_r| \ge n_\barr$, it outputs~$R_\barr$ on~$\gY$ with a probability of at least~$\frac{1}{|\gY|!} + \epsilon$.
    
    Suppose $r$ is a heterogeneous reward function that its expectation induces a complete ranking~$R_\barr$ with no tie. Define a new heterogeneous reward function~$r_\gamma$ as follows. Consider a new user type~$0 \notin \gU$. For some~$\gamma > 1$, let $r_\gamma(\vy; u) = \gamma\,r(\vy; u)$ when $u \neq 0$, and $r_\gamma(\vy; 0) = 0$. Define a new user distribution~$\gP_\gamma(u) \coloneqq (1-\frac{1}{\gamma}) \One\{u=0\} + \frac{1}{\gamma}\gP(u)$. It is straightforward to verify $\barr_\gamma \coloneqq \E_{u\sim\gP_\gamma}[r_\gamma(\vy; u)] = \barr$. Therefore, with high probability, $\gA$ outputs~$R_{\barr_\gamma}=R_{\barr}$ from~$\gD_{r_\gamma}$ for every~$\gamma > 1$:
    %
    \begin{equation*}
        \Pr\big(\gA(\gD_{r_\gamma}) = R_\barr\big) \ge \frac{1}{|\gY|!} + \epsilon
        \,.
    \end{equation*}
    %
    As we increase~$\gamma$, for any continuous preference model~$\sigma$, the pairwise preference dataset~$\gD_{r_\gamma}$ approaches a uniform preference dataset~$\gD_\unif$ labeled mostly by an indifferent annotator of type~$u=0$. So, we have
    %
    \begin{equation}
        \Pr\big(\gA(\gD_\unif) = R_\barr\big) \ge \frac{1}{|\gY|!} + \epsilon
        \,.
    \end{equation}
    %
    This is true for every~$r$. For different choices of~$r$, agreements with~$R_\barr$ are disjoint events. Since there are~$|\gY|!$ different rankings overall, the pigeonholed principle implies~$\epsilon = 0$. 

    \paragraph{Proof Strategy 2.}
    The proof is by contradiction. Suppose there exists an algorithm~$\gA$ that for any reward function~$r$ and any preference dataset~$\gD_r$ with $|\gD_r| \ge n_\barr$, it outputs~$R_\barr$ with a probability of at least~$1-\epsilon$.
    We follow \citet{friedgut2002boolean} and define a social choice function as a function that yields an asymmetric relation on the alternatives given a profile. A social choice is \emph{rational} if it is an order relation on the alternatives, and is \emph{neutral} if it is invariant under permutations of alternatives.
    
    Let~$\gY_3 = \{\vy_1, \vy_2, \vy_3\}$ be an arbitrary subset of~$\gY$ with size~$3$. Next, we construct a neutral social choice function~$f$ acting on~$\gY_3$ that is independent of irrelevant alternatives (IIA). Here is how we design~$f$: Let~$P_r$ be a profile of size~$n \ge n_\barr$ at the input, where every~$R \in P_r$ is an i.i.d. draw from a Plackettâ€“Luce (PL) ranking model with $\exp(r)$ as the weight of the alternatives. Note that the marginal distribution induced by PL on any two alternatives follows BT. Create three pairwise preference datasets~$\gD_{r,12}$,~$\gD_{r,23}$, and~$\gD_{r,13}$ where~$\gD_{r, ij} = \{\vy_i R \vy_j \mid R \in P_r\}$. The social function~$f$ applies~$\gA$ to every dataset to obtain a relation over~$\gY_3$. By construction, $f$ is neutral and IIA. 
    
    By assumption, for every internal dataset~$\gD_{r, ij}$ we have $\Pr\big(\gA(\gD_{r, ij}) = R_{r, ij}\big) \ge 1 - \epsilon$, where $R_{r, ij}$ is the projection of~$R_\barr$ to only two alternatives~$\vy_i$ and~$\vy_j$. Using union bound, we have
    %
    \begin{equation*}
        \Pr\big(f(P_r) = R_\barr\big) \ge 1 - 3 \epsilon
        \,.
    \end{equation*}
    %
    Similar to strategy~1, we can define a new reward function~$r_\gamma$ with $\barr_\gamma = \barr$ such that the above holds for every~$r_\gamma$ with~$\gamma > 1$. Then, by increasing~$\gamma$, the profile~$P_{r_\gamma}$ approaches a uniformly distributed profile~$P_\unif$. In this case, since $R_\barr$ is an order relation, we have $\Pr\big(f \text{ is rational}\big) \ge 1 - 3\epsilon$. Then Theorem 1.3 of \citet{friedgut2002boolean} implies that for some global constant~$K$, 
    %
    \begin{equation*}
        \Pr\big(f \text{ is dictatorship}\big) \ge 1 - 3 K \epsilon
        \,.
    \end{equation*}
    %
    On the other hand, we know that the order that~$R_\barr$ induces for different~$r$ is not a dictatorship, so, we have
    %
    \begin{equation*}
        \Pr\big(f \text{ is dictatorship}\big) < 3 \epsilon
        \,.
    \end{equation*}
    %
    Putting these together, we obtain a lower bound on~$\epsilon$:
    %
    \begin{equation*}
        \epsilon \ge \frac{1}{3 (K + 1)} > 0
        \,.
    \end{equation*}
    %
    The rest of the proof is similar for the second and third strategies. We can use a boosting argument to show that from any weak pairwise learner, we can obtain an arbitrarily strong weak learner corresponding at the cost of collecting a larger dataset. We show this when for the weak learner~$\epsilon < \frac{1}{2}$ but a weaker condition of choosing~$R_\barr$ better than chance level is sufficient for our argument. Consider a pairwise preference dataset~$\gD_r$ of size~$m \, n_\barr$. Partition~$\gD_r$ into $m$~equal-size datasets. Let~$R_i$ be the output of~$\gA$ on the $i^{\rm th}$ dataset. Since samples in~$\gD_r$ are independently generated, $R_i$s~err independently. Construct a meta-algorithm~$\gA_\maj$ that outputs the majority winner of~$R_i$s. A standard Hoeffding bound implies
    %
    \begin{equation*}
        \Pr\big(\gA_\maj(\gD_r) \neq R_\barr\big) \le \exp\Big(-2(1/2 - \epsilon)^2 \, m\Big)
        \,.
    \end{equation*}
    %
    A simple calculation then shows that for any arbitrarily small~$\epsilon' > 0$, by choosing $m = O\big(\log(\frac{1}{\epsilon'}) \, (\frac{1}{2} - \epsilon)^{-2}\big)$ the majority-winner algorithm agrees with~$R_\barr$ with probability of at least~$1-\epsilon'$. This contradicts the lower bound we established earlier and completes the proof.

    \paragraph{Proof Strategy 3.}
    The proof is by contradiction and is inspired by \citet{procaccia2025clone}. This proof requires at least four different user types. Suppose there are two equally represented user types~$\gU = \{A, B\}$ who follow BT. For some arbitrary response~$\vy_0 \in \gY$ and $0 < \tau < \frac{1}{3}$, consider the following reward function:
    %
    \begin{equation}
        r_\tau(\vy; u) = \begin{cases}
            0 \,, & \vy \neq \vy_0 \,, \\
            \sigma^{-1}(\frac{2}{3} + \tau) = \log \frac{\frac{2}{3} - \tau}{\frac{1}{3} + \tau} \,, & \vy = \vy_0, u = A \,, \\
            \sigma^{-1}(\frac{2}{3} - \tau) = \log \frac{\frac{2}{3} + \tau}{\frac{1}{3} - \tau} \,, & \vy = \vy_0, u = B \,.
        \end{cases}
    \end{equation}
    %
    One can see $\Pr(\vy_0 \succ \vy \mid r_\tau) = \frac{2}{3}$ for every~$\vy \neq \vy_0$. Therefore, $r_\tau$ induces the same pairwise preference distribution for every~$\tau$. On the other hand, $\barr_\tau(\vy_0) \coloneqq \E_u[r_\tau(\vy_0; u)] = \log \frac{\frac{4}{9} - \tau^2}{\frac{1}{9} - \tau^2} > 0$ is an increasing function of~$\tau$. 

    Consider two arbitrary $\tau_1$ and~$\tau_2$ such that $0 < \tau_1 < \tau_2 < \frac{1}{3}$. Sample a pairwise preference dataset at follows. Draw a random~$u$ and a random permutation~$\rho$ over~$\gY$. If $\rho$~is not identity, ask an annotator of type~$u$ with reward~$r_{\tau_1}(\cdot; u)$ to label this sample and permute the ranking with~$\rho$. If~$\rho$ is identity, ask an annotator of type~$u$ with reward~$r_{\tau_2}(\cdot; u)$ to label. This sampling is equivalent to sampling from $2*|\gY|!$ different user types. By symmetry, this pairwise preference dataset is distributionally equivalent to a dataset~$\gD_\unif$ with indifferent preferences. However, our construction implies that~$\vy_0$ has the highest expected reward and other alternatives have similar rewards:
    %%%
    \begin{equation*}
        \barr(\vy) = \frac{1}{|\gY|} \begin{cases}
            \barr_{\tau_1}(\vy) \,, & \vy \neq \vy_0 \,, \\
            \barr_{\tau_2}(\vy) \,, & \vy = \vy_0 \,.
        \end{cases}
    \end{equation*}
    %%%
    Denote the ranking based on~$\barr$ above by~$R_0$. 

    Similar to the second strategy, suppose there exists an algorithm~$\gA$ that for any reward function~$r$ and any preference dataset~$\gD_r$ with $|\gD_r| \ge n_\barr$, it outputs~$R_\barr$ with a probability of at least~$1-\epsilon$. Collect a preference dataset as explained above with at least~$n_\barr$ samples. Then, by assumption,
    %
    \begin{equation*}
        \Pr\big(\gA(\gD_\unif) = R_0\big) \ge 1 - \epsilon
        \,.
    \end{equation*}
    %
    Note that our choice of~$\vy_0$ could be any of the alternatives. Therefore, the above should be true when any of the alternatives has the highest expected reward. This implies a lower bound on~$\epsilon$:
    %
    \begin{equation*}
        \epsilon \ge 1 - \frac{1}{|\gY|} > 0
        \,.
    \end{equation*}
    %
    The rest of the proof is similar to the second strategy. 
\end{proofEnd}
%%%
\citet{siththaranjan2023distributional} (Theorem 3.4) presented a version of \cref{prop:impossible_anonymous_learning} in case of two alternatives and two types with~$\sigma(\Delta r) = \One\{\Delta r > 0\}$. \citet{procaccia2025clone} (Theorem 2.2) generalized this to BT. \cref{prop:impossible_anonymous_learning} presents a fresh perspective by generalizing the impossibility to any continuous preference model and presenting multiple proof strategies, including one that draws on a robust version of Arrow's theorem~\citep{friedgut2002boolean}.

To circumvent the impossibility in \cref{prop:impossible_anonymous_learning}, we must either relax the requirement of exactly identifying~$\pi^*$ or collect some information from the annotators. This section focuses on the former, and the latter is the subject of \cref{sec:dpo_with_known_u}. Next, we introduce an approximate alignment objective, along with the required information and algorithms to solve it.
 

%%%%%
\subsection{First-Order Approximation}

The approximation in \cref{eq:dpo_approx} is equivalent to using a zeroth-order Taylor expansion of $\sigma(\cdot)$ around the average reward to calculate the likelihood function. To improve it, we extend DPO by incorporating an additional non-zero term from the expansion, which we call \emph{first-order corrected DPO}. The derivation is as follows. Expanding~$\sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)$ around~$\Delta\barr^*(\vy_1, \vy_2) \coloneqq \E_u\big[\Delta r^*(\vy_1, \vy_2; u)\big]$ up to the second order gives
% %
% \begin{align*}
%     &\sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big) \approx \sigma\big(\Delta\barr^*(\vy_1, \vy_2)\big) \\
%     &+ \sigma'\big(\Delta\barr^*(\vy_1, \vy_2)\big) \Big(\Delta r^*(\vy_1, \vy_2; u) - \Delta\barr^*(\vy_1, \vy_2)\Big) \\
%     &+ \frac{1}{2} \sigma''\big(\Delta\barr^*(\vy_1, \vy_2)\big) \Big(\Delta r^*(\vy_1, \vy_2; u) - \Delta\barr^*(\vy_1, \vy_2)\Big)^2
%     .
% \end{align*}
% %
the below approximation for the likelihood:
%
\begin{equation}
\begin{aligned}
\label{eq:first_order_correction} 
    &\E_u\Big[\sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)\Big] \approx \sigma\big(\Delta\barr^*(\vy_1, \vy_2)\big) \\
    &+ \frac{1}{2} \sigma''\big(\Delta\barr^*(\vy_1, \vy_2)\big) \cdot \Var_u\big[\Delta r^*(\vy_1, \vy_2; u)\big]
    \,.
\end{aligned}
\end{equation}
%
Note that \cref{eq:dpo_approx} is loose when~$\sigma$ is nonlinear and preferences have high variance. We improve likelihood approximation by incorporating the variance term from \cref{eq:first_order_correction}.
To calculate \cref{eq:first_order_correction}, we can substitute~$\Delta \barr^*$ in by the difference in log policy ratios (\cref{eq:heter_diff_of_rew}). We then need to estimate the variance term. \cref{sec:var_est} offers a variance estimator. 

Once the variance is estimated by a function~$V(\vy_1, \vy_2)$, first-order corrected DPO estimates $\Pr(\vy_2 \succ \vy_1 \mid \pi)$ using
%
\begin{equation*}
    \sigma\big(h(\vy_1, \vy_2; \pi)\big)
    + \frac{\alpha}{2} \, \sigma''\big(h(\vy_1, \vy_2; \pi)\big) \cdot V(\vy_1, \vy_2) 
    \,.
\end{equation*}
%
Here, $\alpha > 0$ determines the strength of correction, and 
%
\begin{equation}
\label{eq:def_h}
    h(\vy_1, \vy_2; \pi) \coloneqq \beta \log\frac{\pi(\vy_2)}{\pi_\reff(\vy_2)} - \beta \log\frac{\pi(\vy_1)}{\pi_\reff(\vy_1)}
\end{equation}
%
denotes the difference of $\pi$'s induced rewards.
Given $\Pr(\vy_2 \succ \vy_1 \mid \pi)$ and a preference dataset~$\gD$, we can maximize the log-likelihood similar to \cref{eq:dpo_ml}. For numerical stability, we use a stable logarithm $\widetilde{\log}(z) \coloneqq \log(\max \{z, \epsilon\})$ in computations. Note that our theory suggests $\alpha = 1$ while DPO uses $\alpha = 0$. Our empirical findings in \cref{sec:simulate_dpo_with_unknown_u} show that larger values of~$\alpha$ improve the effectiveness of the correction. We next discuss the estimation of~$V(\vy_1, \vy_2)$.

%%%%


%%%%
\subsection{Impossibility without Annotator Information}
\label{sec:impossible_approx}

If we limit our algorithms to M-estimators, which encompass most practical learning methods, consistent estimation of the variance term is impossible with anonymous data:
%%%
\begin{theoremEnd}[restate]{proposition}
\label{prop:no_estimator_of_var}
There is no M-estimator that can estimate $V(\vx, \vy_1, \vy_2) \coloneqq \Var_u\big[\Delta r^*(\vx, \vy_1, \vy_2; u)\big]$ consistently without annotator information.
\end{theoremEnd}
%%%
\begin{proofEnd}
    Consider a dataset~$\gD$ of context, candidate pairs, and preference represented as $(\vx, \vy_1, \vy_2, o)$, where $o = \One\{\vy_2 \succ \vy_1\}$, and $\vy_1, \vy_2$ are independently drawn. Then consider an M-estimator
    %
    \begin{equation*}
        \argmin_V \sum_{(\vx, \vy_l, \vy_w) \in \gD} \rho(\vx, \vy_l, \vy_w; V)
        = \sum_{(\vx, \vy_1, \vy_2, o) \in \gD} o \cdot \rho(\vx, \vy_1, \vy_2; V) + (1 - o) \cdot \rho(\vx, \vy_2, \vy_1; V)
        \,.
    \end{equation*}
    %
    Under preference model of \cref{eq:BT}, for a reward~$r$, we have $\E[o \mid \vx, \vy_1, \vy_2; u] = \sigma\big(\Delta r(\vx, \vy_1, \vy_2; u)\big)$. In the limit of a large dataset, the $M$-estimator solves
    %
    \begin{align*}
        &\argmin_V \;\E_{\vx, \vy_1, \vy_2, o, u}\Big[ o \cdot \rho(\vx, \vy_1, \vy_2; V) + (1 - o) \cdot \rho(\vx, \vy_2, \vy_1; V)  \Big] \nonumber \\
        &= \E_{\vx, \vy_1, \vy_2, u}\Big[
        \sigma\big(\Delta r(\vx, \vy_1, \vy_2; u)\big) \cdot \rho(\vx, \vy_1, \vy_2; V)
        + \sigma\big(\Delta r(\vx, \vy_2, \vy_1; u)\big) \cdot \rho(\vx, \vy_2, \vy_1; V)
        \Big] && \text{($V$ has no $o$)} \\
        &= \E_{\vx, \vy_1, \vy_2}\Big[
        \E_u\big[\sigma\big(\Delta r(\vx, \vy_1, \vy_2; u)\big)\big] \cdot \rho(\vx, \vy_1, \vy_2; V)
        + \E_u\big[\sigma\big(\Delta r(\vx, \vy_2, \vy_1; u)\big)\big] \cdot \rho(\vx, \vy_2, \vy_1; V)
        \Big] && \text{($V$ has no $u$)} \\
        &= 2 \, \E_{\vx, \vy_1, \vy_2}\Big[ \E_u\big[\sigma\big(\Delta r(\vx, \vy_1, \vy_2; u)\big)\big] \cdot \rho(\vx, \vy_1, \vy_2; V) \Big] \,. && \text{($\vy_1, \vy_2$ are i.i.d)}
    \end{align*}
    %
    Here, we used the fact that $V$~does not depend on~$u$. We also relied on the assumption that $\vy_1$~and~$\vy_2$ are identically and independently distributed. The above equation suggests that regardless of how $\rho$~is designed, $V(\vx, \vy_1, \vy_2)$ can only depend on~$u$'s distribution through $\E_u\big[\sigma\big(\Delta r(\vx, \vy_1, \vy_2; u)\big)\big]$. Therefore, no consistent M-estimator can generally estimate $\Var_u\big[\Delta r(\vx, \vy_1, \vy_2; u)\big]$ even with the availability of infinite preference data.   
\end{proofEnd}
%%%
While \cref{prop:impossible_anonymous_learning} already implies that we cannot learn~$\pi^*$ without annotation information, \cref{prop:no_estimator_of_var} goes further, showing that even improving DPO with a first-order approximation is practically impossible. 
\ifarxiv Next, we show how minimal annotation information can overcome this impossibility. \fi

%%%%%


%%%%%
\subsection{Using Paired Preferences}
\label{sec:var_est}

We can get around \cref{prop:no_estimator_of_var} by collecting additional information on annotators. Specifically, it suffices to have a dataset~$\gD$ of pairs of preferences in the form $\{(\vx, \vy_1, \vy_2, o), (\vx', \vy'_1, \vy'_2, o')\}$ where $o = \One\{\vy_2 \succ \vy_1\}$, $o' = \One\{\vy'_2 \succ \vy'_1\}$ are labeled by the \emph{same person}. Using~$\gD$, we can train a \emph{joint likelihood model}~$J(\vx, \vy_1, \vy_2, \vx', \vy'_1, \vy'_2)$ by minimizing cross-entropy between~$J$ and $(o \cdot o')$ as the label. The joint likelihood model consistently estimates
%
\begin{align*}
    % &J(\vx, \vy_1, \vy_2, \vx', \vy'_1, \vy'_2) = \Pr(\vy_2 \succ \vy_1, \vy'_2 \succ \vy'_1 \mid \vx, \vx'; r) \\
    % &= 
    \E_u\Big[\sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big) \cdot \sigma\big(\Delta r^*(\vx', \vy'_1, \vy'_2; u)\big)\Big]
    \,.
\end{align*}
%
This is in fact sufficient to estimate the variance term:

%%%
\begin{theoremEnd}[restate]{lemma}
Using $J_1$ and $J_2$ as shorthands for $J(\vx, \vy_1, \vy_2, \vx, \vy_1, \vy_2)$ and $J(\vx, \vy_1, \vy_2, \vx, \vy_2, \vy_1)$, we can use the following to estimate the variance term:
%
\begin{equation}
\label{eq:var_and_J}
    V(\vy_1, \vy_2) = \frac{J_1 - (J_1 + J_2)^2}{\sigma'\big(\Delta \barr^*(\vy_1, \vy_2)\big)^2}
    \,.
\end{equation}
%
\end{theoremEnd}
%%%
\begin{proofEnd}
    First of all, $J$ can give us the likelihood itself:
    %
    \begin{align*}
        J(\vx, \vy_1, \vy_2, \vx, \vy_1, \vy_2) + J(\vx, \vy_1, \vy_2, \vx, \vy_2, \vy_1) &= \E_u\Big[
        \sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)^2 + 
        \sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big) \cdot \sigma\big(\Delta r^*(\vy_2, \vy_1; u)\big)
        \Big] \\
        &= \E_u\big[\sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)\big]
        \,.
    \end{align*}
    %
    Here, we used the property $\sigma\big(\Delta r^*(\vy_2, \vy_1; u)\big) = 1 - \sigma\big(\Delta r^*(\vy_1, \vy_2; u)\big)$. We also dropped~$\vx$ from the notation for simplicity. Since $J$ can give us both the first and second moments, we can use it to find $\Var_u\big[\sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)\big]$ as follows:
    %
    \begin{align*}
        \Var_u\big[\sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)\big] &= \E_u\big[\sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)^2\big] - \E_u\big[\sigma\big(\Delta r^*(\vx, \vy_1, \vy_2; u)\big)\big]^2 \\
        &= J(\vx, \vy_1, \vy_2, \vx, \vy_1, \vy_2) - \big(J(\vx, \vy_1, \vy_2, \vx, \vy_1, \vy_2) + J(\vx, \vy_1, \vy_2, \vx, \vy_2, \vy_1)\big)^2
        \,.
    \end{align*}
    %
    In the last piece of the proof, we connect $\Var_u\big[\sigma(\Delta r^*)\big]$ with $\Var_u\big[\Delta r^*\big]$. The Taylor expansion of $\sigma(\Delta r^*)$ around $\Delta \barr^* \coloneqq \E_u[\Delta r^*]$ gives
    %
    \begin{align*}
        \Var_u\big[\sigma(\Delta r^*)\big] &= \Var_u\Big[\sigma(\Delta \barr^*) + \sigma'(\Delta \barr^*) \cdot (\Delta r^* - \Delta \barr^*) + O\big((\Delta r^* - \Delta \barr^*)^2\big)\Big] \\
        &= \sigma'(\Delta \barr^*)^2 \cdot \Var_u\big[\Delta r^*\big] + O\Big(\E_u\big[(\Delta r^* - \Delta \barr^*)^3\big]\Big)
        \,.
    \end{align*}
    %    
    We can neglect the third-order term in calculations as first-order correction uses up to $O\Big(\E_u\big[(\Delta r^* - \Delta \barr^*)^2\big]\Big)$ in its approximation.  
\end{proofEnd}
%%%
Note that we can substitute $\Delta \barr^*(\vy_1, \vy_2)$ in terms of log policy ratios from \cref{eq:heter_diff_of_rew}. Thus, we have all the elements to calculate~$V$ in \cref{eq:var_and_J}. This completes our derivation of first-order corrected DPO.

%%%%%

