\section{Related Works}
Numerous attempts have been done regarding Parameter-Efficient Fine-Tuning to adapt modern large language models to various applications. LoRA ____ has been one of the first major attempts in imposing efficient structural assumption on the neural network weight matrices of large models, subsequent research based on LoRA involves utilizing lower-precision quantization to harness the advantages of efficient calculations on lower-precision numbers offered by contemporary tensor core-based GPUs ____, and other form of weight decompositions with better semantic understanding of the weight matrices ____. Further more, Kronecker product based factorizations of the weight matrices have also been studied to further reduce the parameter counts ____, and ____ provides a mixture of low-rank and Kronecker factorization to achieve parameter-efficient tuning for vision models.