@misc{dora,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@inproceedings{kadaptation,
  title={Parameter-efficient model adaptation for vision transformers},
  author={He, Xuehai and Li, Chunyuan and Zhang, Pengchuan and Yang, Jianwei and Wang, Xin Eric},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={817--825},
  year={2023}
}

@article{krona,
  title={Krona: Parameter efficient tuning with kronecker adapter},
  author={Edalati, Ali and Tahaei, Marzieh and Kobyzev, Ivan and Nia, Vahid Partovi and Clark, James J and Rezagholizadeh, Mehdi},
  journal={arXiv preprint arXiv:2212.10650},
  year={2022}
}

@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@misc{qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

