\section{Related Works}
Numerous attempts have been done regarding Parameter-Efficient Fine-Tuning to adapt modern large language models to various applications. LoRA \cite{lora} has been one of the first major attempts in imposing efficient structural assumption on the neural network weight matrices of large models, subsequent research based on LoRA involves utilizing lower-precision quantization to harness the advantages of efficient calculations on lower-precision numbers offered by contemporary tensor core-based GPUs \cite{qlora}, and other form of weight decompositions with better semantic understanding of the weight matrices \cite{dora}. Further more, Kronecker product based factorizations of the weight matrices have also been studied to further reduce the parameter counts \cite{krona, kadaptation}, and \citeauthor{kadaptation} provides a mixture of low-rank and Kronecker factorization to achieve parameter-efficient tuning for vision models.