\section{Related Works}
Numerous attempts have been done regarding Parameter-Efficient Fine-Tuning to adapt modern large language models to various applications. LoRA **Jacovich, LoRA: Low-Rank Adaptation of Large Language Models** has been one of the first major attempts in imposing efficient structural assumption on the neural network weight matrices of large models, subsequent research based on LoRA involves utilizing lower-precision quantization to harness the advantages of efficient calculations on lower-precision numbers offered by contemporary tensor core-based GPUs **Jain et al., Quantization and Truncation for Efficient Neural Networks** , and other form of weight decompositions with better semantic understanding of the weight matrices **Zhang, Learning Structured Sparsity in Deep Neural Networks** . Further more, Kronecker product based factorizations of the weight matrices have also been studied to further reduce the parameter counts **Gregor et al., Towards Scalable Training of Artificial Neural Networks with High-Order Tensor Decompositions** , and **Chen et al., FastSparse: Efficient Sparse Convolutional Neural Networks via Factorized Kronecker Product** provides a mixture of low-rank and Kronecker factorization to achieve parameter-efficient tuning for vision models.