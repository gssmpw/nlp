\section{Related work}
\label{sec:Related}
Our work is based on the close relationship between QIF, model counting, and knowledge compilation.
We introduce relevant work from three perspectives: (1) quantitative information flow analysis, (2) model counting, and (3) knowledge compilation.

\textbf{Quantified information flow analysis} 
At present, the QIF method based on model counting encounters two significant challenges.
The first challenge involves constructing the logical postcondition $\Pi_{proc}$ for a program $proc$~\cite{zhou2018static}.
Although symbolic execution can achieve this, existing symbolic execution tools have limitations and are often challenging to extend to more complex programs, such as those involving symbolic pointers.
The second challenge concerns model counting, a key focus of our research. 
For programs modeled by Boolean clause constraints, Shannon entropy can be computed via model counting queries, enabling the quantification of information leakage.
Golia et al.~\cite{golia2022scalable} have made notable contributions to this field. 
They proposed the first efficient Shannon entropy estimation method with PAC guarantees, utilizing sampling and model counting. 
%Their approach focuses on reducing the number of model counting queries by employing sampling techniques. 
%Nevertheless, this method yields only an approximate estimation of entropy.
%Our research is motivated by the work of Golia et al., but diverges in its approach and optimization strategy. 
%We enhance the existing model counting framework for precise Shannon entropy by reducing the number of model counting queries and concurrently improving the efficiency of model counting solutions.
Inspired by Golia et al.'s work, our research differs in approach and optimization strategy. 
We improve the existing model counting framework for precise Shannon entropy by reducing the number of model counting queries and enhancing solution efficiency. 

%Based on this idea, we propose an efficient and precise method for computing the Shannon entropy.

%Besides Boolean clause constraints, converting programs into strings and SMT constraints has also attracted extensive research.
%For string constraints, Aydin et al.~\cite{aydin2018parameterized} proposed an automata-based model counting solver, MT-ABC, which is applicable to QIF.
%Similar solvers also include SMC~\cite{luu2014model}, ABC~\cite{aydin2015automata}, etc. 
%The key distinction between their approaches lies in the advancement of automaton model counting techniques.
%Phan et al.~\cite{phan2014abstract} proposed an abstract model counting method, a quantified information leakage method based on an SMT-based framework.


\textbf{Model counting}
Since the computation of entropy relies on model counting, we reviewed advanced techniques in this domain.
The most effective methods for exact model counting include component decomposition, caching, variable decision heuristics, pre-processing, and so on.
%In our research, these methods, with the exception of component decomposition, can all be optimized and improved for application in Shannon entropy computation.
The fundamental principle of disjoint component analysis involves partitioning the constraint graph into separate components that do not share variables.
The core of \ADDAND lies in leveraging component decomposition to enhance the efficiency of construction.
%Caching techniques have long been utilized across various fields and demonstrate significant efficacy in model counting as well.
%Bacchus et al. analysis three distinct caching schemes: simple caching, component caching,and linear-space caching \cite{bacchus2003dpll}. 
%Their research suggests that component caching offers the most promising prospects.
%Combining component caching with conflict driven clause learning is an idea pioneered by Sang et al. \cite{sang2004combining} in 2004 with their model counter Cachet which is based on the well-known SAT solver zCHaff \cite{moskewicz2001chaff}. 
We also utilized caching technique in the process of computing entropy, and our experiments once again demonstrated the power of caching technique.
%Thurley \cite{thurley2006sharpsat} proposed improved component encoding schemes with a solver called sharpSAT to make cache components more concise. 
%sharpSAT \cite{thurley2006sharpsat} first employed i-BCP in model counting, which is an improvement of the well-known "look-ahead" technique based on Boolean constraint propagation. 
%The computation of implied literals relies on IBCP, and our experiments have shown that implied literals are also beneficial to the process of computing entropy.
%Sharma et al.~\cite{sharma2019ganak} proposed a probabilistic caching strategy in their model counter Ganak, demonstrating its efficiency.
%Currently, our tool PSE does not incorporate probabilistic caching methods, which is an area we plan to explore in future research.
Extensive research has been conducted on variable decision heuristics for model counting, which are generally classified into static and dynamic heuristics.
In static heuristics, the \textsf{minfill}~\cite{darwiche2009modeling} heuristic is notably effective, while in dynamic heuristics, \textsf{VSADS}~\cite{sang2005heuristics}, \textsf{DLCP}~\cite{lai2021power}, and \textsf{SharpSAT-TD  heuristic}~\cite{korhonen2021integrating} have emerged as the most significant in recent years.
%The effects of these heuristics on entropy computation are discussed in Section \ref{sec:Experiments}.
%Lagniez et al.~\cite{lagniez2017preprocessing} offer a comprehensive review of preprocessing techniques in model counting.
%Not all preprocessing methods in model counting are suitable for entropy computation. 
%Our research found that the most effective method is literal equivalence, and we have made some improvements to this preprocessing method to make it suitable for entropy computation (in Section \ref{sec:PSE}).
\begin{comment}
	
	The fundamental principle of disjoint component analysis involves partitioning the constraint graph into separate components that do not share variables.
	However, this method is unsuitable for Shannon entropy computation, as decomposing the variables in $Y$ into distinct components significantly complicates the calculation.
	Caching techniques have long been utilized across various fields and demonstrate significant efficacy in model counting as well.
	%Bacchus et al. analysis three distinct caching schemes: simple caching, component caching,and linear-space caching \cite{bacchus2003dpll}. 
	%Their research suggests that component caching offers the most promising prospects.
	%Combining component caching with conflict driven clause learning is an idea pioneered by Sang et al. \cite{sang2004combining} in 2004 with their model counter Cachet which is based on the well-known SAT solver zCHaff \cite{moskewicz2001chaff}. 
	We also utilized caching technique in the process of computing entropy, and our experiments once again demonstrated the power of caching technique.
	%Thurley \cite{thurley2006sharpsat} proposed improved component encoding schemes with a solver called sharpSAT to make cache components more concise. 
	%sharpSAT first employed i-BCP in model counting, which is an improvement of the well-known "look-ahead" technique based on Boolean constraint propagation. 
	The computation of implied literals relies on IBCP, and our experiments have shown that implied literals are also beneficial to the process of computing entropy.
	%Sharma et al.~\cite{sharma2019ganak} proposed a probabilistic caching strategy in their model counter Ganak, demonstrating its efficiency.
	%Currently, our tool PSE does not incorporate probabilistic caching methods, which is an area we plan to explore in future research.
	Extensive research has been conducted on variable decision heuristics for model counting, which are generally classified into static and dynamic heuristics.
	In static heuristics, the \textbf{minfill}~\cite{darwiche2009modeling} heuristic is notably effective, while in dynamic heuristics, \textbf{VSADS}~\cite{sang2005heuristics}, \textbf{DLCP}~\cite{lai2021power}, and \textbf{SharpSAT-TD  heuristic}~\cite{korhonen2021integrating} have emerged as the most significant in recent years.
	%The effects of these heuristics on entropy computation are discussed in Section \ref{sec:Experiments}.
	%Lagniez et al.~\cite{lagniez2017preprocessing} offer a comprehensive review of preprocessing techniques in model counting.
	%Not all preprocessing methods in model counting are suitable for entropy computation. 
	%Our research found that the most effective method is literal equivalence, and we have made some improvements to this preprocessing method to make it suitable for entropy computation (in Section \ref{sec:PSE}).
\end{comment}


\textbf{Knowledge compilation} 
%The motivation for utilizing knowledge compilation for model counting lies in the fact that it is difficult to compute the number of models for the original Boolean formula, whereas the new target language can solve the model counting problem quickly. 
%Darwiche et al. first proposed a compiler called c2d~\cite{darwiche2004new} to convert the given CNF formula into Decision-DNNF. 
%The core technique of c2d compile CNF to Decision-DNNF is decomposition.
%Based on the exact solver sharpSAT~\cite{thurley2006sharpsat}, Muise and McIlraith et al. developed a new compiler, Dsharp~\cite{muise2012d}, which converts CNF to Decision-DNNF. 
%Unlike c2d, Dsharp utilizes two significant features of sharpSAT: dynamic decomposition and implicit binary constraint propagation. 
%Lagniez and Marquis~\cite{lagniez2017improved} proposed an improved Decision-DNNF compiler, called D4, is a top-down compiler that compiles CNF formulas into d-DNNF.
%Similar to Dsharp, D4 employs dynamic decomposition, albeit with a distinct strategy. 
%To improve algorithm efficiency, D4 extensively utilizes heuristic techniques, including disjoint component analysis, component caching, conflict analysis, and non-technical backtracking, which have demonstrated effectiveness in c2d and Dsharp.
%Exploiting literal equivalence, Lai et al.~\cite{lai2021power} proposed a generalization of Decision-DNNF, called CCDD, to capture literal equivalence. 
%They demonstrate that CCDD supports model counting in linear time and design a model counter called ExactMC based on CCDD.
In order to compute the Shannon entropy, the focus of this paper is to design a compiled language that supports the representation of probability distributions.
Numerous target representations have been used to concisely model probability distributions. %providing more concise factorizations than Bayesian networks in the presence of local structure\cite{dal2021compositional}.
For example, d-DNNF can be used to compile relational Bayesian networks for exact inference~\cite{chavira2006compiling};
Probabilistic Decision Graph (PDG) is a representation language for probability distributions based on BDD~\cite{jaeger2004probabilistic}.
%Probabilistic Sentential Decision Diagram (PSDD) is a complete and canonical representation of probability distributions defined over the models of a given propositional theory~\cite{kisa2014probabilistic}. 
%Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD).
%And/Or Multi-Valued Decision Diagrams (AOMDDs)~\cite{mateescu2008and} take advantage of the AND/OR structure to represent probability distributions.
%Affine ADDs(AADDs)~\cite{sanner2005affine}, an extended form of ADDs, achieve more compression through affine transformations, which is beneficial for probabilistic inference algorithms.
%By the end of the 20th century, researchers began to focus on utilizing knowledge compilation for the precise computation of Shannon entropy.
Macii and Poncino~\cite{macii1996exact} utilized knowledge compilation to calculate entropy, demonstrating that ADD enables efficient and precise computation of entropy.
However, the size of ADD often grows exponentially for large scale circuit formulas. To simplify ADD size, we propose an extended form, \ADDAND. It uses conjunctive decomposition to streamline the graph structure and facilitate cache hits during construction. 
%However, when dealing with large-scale circuit formulas, the size of ADD often grows exponentially. 
%Therefore, to simplify the size of ADDs, we propose an extended form of ADD, namely \ADDAND. 
%It exploits the power of conjunctive decomposition to make the graph structure more concise and enables easier cache hits during the construction process.

 
%Based on the core idea of simplifying the size of ADD, we propose an extended form of ADD, \ADDAND, which utilizes the power of conjunctive decomposition to make the graph structure more concise and easier to hit the cache during the construction process.
%In addition to ADD, several studies have employed BDDs for approximating entropy.
%StankoviÄ‡ et al.~\cite{stankoviccomputing} observed that the process of computing entropy estimates could be implemented on BDDs, thus avoiding redundant calculations and ensuring the efficiency of the process.
%They used BDDs to estimate the entropy of a given vector in further studies~\cite{stankovic2007calculating}.
%The complexity of their proposed algorithm is proportional to the number of nodes in the decision graph, which is comparable to the complexity of our entropy calculation. 
%However, it is important to note that their method only estimates entropy, while our approach provides exact entropy computations. 
%Furthermore, our \ADDAND representation has a significantly smaller number of nodes compared to BDDs, theoretically resulting in greater efficiency.


