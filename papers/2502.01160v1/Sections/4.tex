\section{PSE: Scalable Precise Entropy Computation}
\label{sec:PSE}

In this section, we introduce our tool PSE, designed to compute the Shannon entropy of a specified circuit CNF formula with respect to its output variables.
Similar to other Shannon entropy tools, the main process of PSE is divided into two stages: the $Y$-stage (corresponding to outputs) and the $X$-stage (corresponding to inputs).
In the $Y$-stage, we execute search with respect to the \ADDAND framework to precisely compute the Shannon entropy.
In the $X$-stage, we perform multiple optimized model counting.
PSE, depicted in Algorithm \ref{PSE}, takes in a CNF formula $\varphi$, an input set $X$, and an output set $Y$, and returns the entropy $\mathit{H}(\varphi)$ of the formula.
Propositions \ref{prop:omega-proposition}--\ref{prop:Entropy-proposition} and the following observation (its proof is in the appendix) guarantee that the entropy of the original formula is the output of the root call.

\begin{observation}\label{prop:circuit-proposition}
	Given a circuit formula $\varphi(X, Y)$ and a partial assignment $\sigma$ without any input variables, we have the following properties:
	\begin{itemize}
		\item $\varphi[\sigma](X, Y \setminus \mathit{Vars}(\sigma))$ is a circuit formula;
		\item Each $\psi_i(X \cap \mathit{Vars}(\psi_i), Y \cap \mathit{Vars}(\psi_i))$ is a circuit formula if $\varphi = \bigwedge_{i = 1}^m\psi_i$ and for  
		$1 \le i \ne j  \le m$, $\mathit{Vars}(\psi_i) \cap \mathit{Vars}(\psi_j) = \emptyset$;
		\item If $\varphi[\sigma] \equiv \mathit{true}$, $\sigma$  contains each output variable.
	\end{itemize}
	
\end{observation}



\begin{algorithm}[h]
	\caption{PSE($\varphi$,$X$,$Y$)}
	\label{PSE}
	\LinesNumbered
    \DontPrintSemicolon
	\KwIn{A circuit CNF formula $\varphi$ with inputs $X$ and outputs $Y$}
	\KwOut{the entropy of $\varphi$}
	\lIf{$\varphi = \mathit{false}$} {\Return $0$}
	
	\lIf{$\mathit{Cache_H}(\varphi) \neq nil $} {\Return $\mathit{Cache_H}(\varphi)$}
	
	\If{$ Y = \emptyset $}
	{
		$\mathit{Cache_\#}(\varphi) \leftarrow $ \texttt{CountModels}($\Phi$) 
		
		\Return  $\mathit{Cache_H}(\varphi)  \leftarrow 0$	
	}
	
	$ \Psi = \texttt{Decompose}(\varphi) $
	
	\If{$ |\Psi| > 1 $} 
	{
		
		$Cache_\#(\varphi) = \prod_{\psi \in \Psi}{Cache_\#(\psi)}$
		
		\Return $\mathit{Cache_H}(\varphi) \leftarrow \sum_{\psi \in \Psi}{\mathbf{PSE}(\psi,X,Y_\psi)}$
	}
	
	$ y \leftarrow \texttt{PickGoodVar}(Y)$
	
	$ \varphi_0 \leftarrow \varphi[y \mapsto \mathit{false}]$; $ \varphi_1 \leftarrow  \varphi[y \mapsto \mathit{true}]$
	
	$\mathit{H}_0 \leftarrow \mathbf{PSE}$($\varphi_0,X,Y \backslash \{y\}$)\;
	$\mathit{H}_1 \leftarrow  \mathbf{PSE}$($\varphi_1,X,Y \backslash \{y\}$) 
	
	$\mathit{Cache}_\#(\varphi) \leftarrow \mathit{Cache}_\#(\varphi_0)  +  \mathit{Cache}_\#(\varphi_1)$
	
	
	$p_0 = \frac{\mathit{Cache}_\#(\varphi_0)}{\mathit{Cache}_\#(\varphi)}$; $p_1 = \frac{\mathit{Cache}_\#(\varphi_1)}{\mathit{Cache}_\#(\varphi)}$
	
	
	
	$\mathit{H} \leftarrow p_0 \cdot H_0 - p_0 \cdot \log(p_0) + p_1 \cdot H_1 - p_1 \cdot \log(p_1)$
	
	\Return $\mathit{Cache_H}(\varphi)  \leftarrow \mathit{H}$
	
\end{algorithm}

We first deal with the cases in which $\varphi$ is $\mathit{false}$ in line 1.  
In such instances where the formula $\varphi$ evaluates to $\mathit{false}$, the entropy associated with the assignment from the ancestor calls is defined as 0.
%When the formula $\varphi$ is $\mathit{true}$, we use a simplification technique where for the remaining unassigned variables $Y$, there are a total of $2^Y$ satisfying assignments.  Therefore, we have $2^{|Y|}$ terminal nodes with a weight of $2^{|\varphi|-|Y|}$ each.
In line 2, if the formula $\varphi$ is cached, return its corresponding entropy.
%Subsequently, in line 3, we utilize the \texttt{GetImpliedLiterals} function to extract the implied literals of $\varphi$.
%The implementation of \texttt{GetImpliedLiterals} relies on implicit Boolean Constraint Propagation (i-BCP)~\cite{thurley2006sharpsat}.  %\texttt{GetImpliedL\\iterals}
%In line 4, we simplify both the formula $\varphi$ and the set $Y$ utilizing the extracted implied literals $L$.
If the current $Y$ is an empty set (in line 3), it means that a satisfiable assignment under the restriction of the output set $Y$ has been obtained. 
We do not discuss the case where $\varphi$ evaluates to $\mathit{true}$, because according to Observation \ref{prop:circuit-proposition}, the scenario where set $Y$ is empty inherently includes the case where $\varphi$ is $\mathit{true}$. 
Lines 4--5 query model counting for the residual formula and calculate its $\mathit{H}$, corresponding to the terminal case of proposition \ref{prop:Entropy-proposition}.
We invoke the \texttt{Decompose} function in line 6 to determine whether the formula $\varphi$ can be decomposed into multiple components.
Lines 7--9, if the formula can be decomposed into multiple sub-components, we calculate the models and entropy of each component $\psi$, and finally obtain the entropy of the formula $\varphi$. This corresponds to the $\wedge$ case in Proposition \ref{prop:Entropy-proposition}.
When there is only one component, we make a decision on the variables in $Y$ at line 10.
The \texttt{PickGoodVar} function operates as a heuristic algorithm designed to select a variable from set $Y$, with the selection criteria being determined by the specific heuristic employed.
Moving forward, line 11 generates the residual formulas $\varphi_0$ and  $\varphi_1$, corresponding to the variable $y$ being assigned $\mathit{false}$ and $\mathit{true}$, respectively.
Following this, lines 12 and 13 are utilized to recursively determine the entropy $\mathit{H}$ for each of these derived formulas.
Since $\varphi$ is a circuit formula, all residual formulas in the recursive process after the variables in decision $Y$ are also circuit formulas.
It follows from Observation \ref{prop:circuit-proposition} that when calculating the Shannon entropy of the circuit formula, $n_0 = n_1 = 0$.
The model count of $\varphi$ is cached at line 14, corresponding to the decision node case in Proposition \ref{prop:omega-proposition}.
Finally, we compute the entropy of $\varphi$ in lines 15-16 (corresponding to the third case in Proposition \ref{prop:Entropy-proposition}), and store it in the cache, returning it as the result in line 17.


\subsection{Implementation}

We now discuss the implementation details that are crucial for the runtime efficiency of PSE. 
Specifically, leveraging the tight interplay between entropy computation and model counting, our methodology integrates a variety of state-of-the-art techniques in model counting.

Initially, we have the option to employ various methodologies for the model counting query denoted by \texttt{CountModels} in line 7.
The first considered method is to individually employ state-of-the-art model counters, such as SharpSAT-TD \cite{korhonen2021integrating}, Ganak~\cite{sharma2019ganak} and ExactMC~\cite{lai2021power}. %and D4~\cite{lagniez2017improved}.
The second method, known as \textsf{ConditionedCounting}, requires the preliminary construction of a representation for the original formula $\varphi$ to support linear model counting. 
The knowledge compilation languages that can be used for this method include d-DNNF~\cite{darwiche2004new}, \OBDDAND~\cite{lai2017new}, and SDD~\cite{choi2013compiling}.
Upon reaching line 7, the algorithm executes conditioned model counting, utilizing the compiled representation of the formula and incorporating the partial assignment derived from the ancestor calls.
%The last method, called \textsf{SharedCounting}, also solves model counting query by invoking the exact model counter.
%Unlike the first method, this method does not just query the model counting individually.
%Instead, we share the component cache in the counter for all model counting queries, and we refer to this strategy as \textsf{XCache}.
The last method, \textsf{SharedCounting}, solves model counting queries by utilizing an exact model counter. Unlike the first method, it shares the component cache within the counter for all queries through a strategy named \textsf{XCache}. 
To distinguish it from the caching approach used in the $X$-stage, the caching method employed in the $Y$-stage is referred to as \textsf{YCache}.
Our experimental observations indicate that the \textsf{SharedCounting} method is the most effective within the PSE framework.



\textbf{Conjunctive Decomposition}
We employed dynamic component decomposition (well-known in model counting and knowledge compilation) to divide a formula into components, thereby enabling the dynamic programming calculation of their corresponding entropy, as stated in Proposition \ref{prop:Entropy-proposition}.


%\textbf{Implied Literals}
%In order to reduce the number of cases for enumerating assignments to $Y$ and to solve for entropy more efficiently, we employed the implied literal extraction technique. The computation of implied literals relies on implicit Boolean Constraint Propagation(i-BCP)\cite{thurley2006sharpsat}, which is an improvement of the well-known "look-ahead" technique based on Boolean constraint propagation.



\textbf{Variable Decision Heuristic} 
We implemented the current state-of-the-art model counting heuristics for picking variable from $Y$ in the computation of Shannon entropy, including \textsf{VSADS}~\cite{sang2005heuristics}, \textsf{minfill}~\cite{darwiche2009modeling}, the \textsf{SharpSAT-TD} heuristic~\cite{korhonen2021integrating}, and \textsf{DLCP}~\cite{lai2021power}.
Our experiments consistently demonstrate that the \textsf{minfill} heuristic exhibits the best performance. Therefore, we adopt the \textsf{minfill} heuristic as the default option for our subsequent experiments.

\textbf{Pre-processing} 
%Since the core of entropy computing is model counting, we try to integrate the preprocessing strategy in model counting into our entropy tool PSE.
%Based on Literal Equivalences, which is a powerful technique in SAT solving, we implement a new preprocessing approach in PSE.
We have enhanced our entropy tool, PSE, by incorporating an advanced pre-processing technique that capitalizes on literal equivalence in model counting. 
This idea is inspired by the work of Lai et al. \cite{lai2021power} on capturing literal equivalence in model counting.
Initially, we extract equivalent literals to simplify the formula. Subsequently, we restore the literals associated with the variables in set $Y$ to prevent the entropy of the formula from becoming non-equivalent after substitution. 
This targeted restoration is sufficient to ensure the equivalence of entropy calculations.
The new pre-processing method is called \textsf{Pre} in the following.
This pre-processing approach is motivated by two primary considerations. 
Firstly, preprocessing based on literal equivalence can simplify the formula and enhance the efficiency of subsequent model counting. Secondly, and more crucially, it can reduce the treewidth of tree decomposition, which is highly beneficial for the variable heuristic method based on tree decomposition and contributes to improving the solving efficiency.

\begin{comment}
	\begin{algorithm}[h]
		\caption{entropyComputing($\Phi$)}
		\label{entropyComputing}
		\LinesNumbered
		\KwIn{A CNF formula $\Phi$ }
		\KwOut{the $entropy$ of $\Phi$}
		\eIf{$count_{opt} = Counting$}{ 
			$c \leftarrow $ \texttt{Counting}($\Phi$) 
		}
		{  
			$c \leftarrow $ \texttt{Condition}($\Phi$) 
		}
		
		$p \leftarrow \frac{c}{totalcount} $
		
		$entropy \leftarrow -p \cdot \log {p}$
		
		
		\Return $  entropy$
	\end{algorithm}
\end{comment}


\begin{comment}
	Algorithm \ref{Condition} describes another model counting method in $X$-stage.
	To implement this method, a prerequisite is that we need an ADD-L constructed based on the original minfill static variable order. The difference between this ADD-L and the ADD-L corresponding to Algorithm 2 is that the $Y$-stage of Algorithm 2 is equivalent to making decisions only on the variables of the $Y$ set based on the minfill static variable order, while our new idea is to make decisions on all variables in the order of the minfill static variable order (including var in $X$). 
	The motivation for this is that our experiments have found that the heuristic effect of the minfill linear order is much stronger (making decisions only on the variables of the $Y$ set to some extent disrupts the linear order). 
	At this point, the computing process is more like general model counting, which can efficiently construct ADD-L. 
	With such an ADD-L, we can utilize conditional model counting (or entropy computing) in knowledge compilation to solve the model counting in the $X$-stage. 
	Lines 1-2 of the algorithm indicate that when the result of a node hits the cache, we can directly retrieve the corresponding result from the cache. 
	When accessing a terminal node, we directly return the result of the corresponding terminal node (lines 4-5). 
	If the variable decision of node u is false, we recursively call the lo branch to solve it (lines 7-8). 
	Similarly, when the variable decision of node u is true, we recursively call the hi branch to solve it (lines 10-11). Otherwise, it means that the variable corresponding to the node has not been decided, so both lo and hi need to be called (line 13). 
	We cache the current result and return it in lines 14-15.
	
	
	
	If any remaining variables in $Y$ have not been assigned values, we make decisions about them in lines 8-10.
	First, a variable x is chosen heuristically.
	Next the $entropy$ of the subformula is computed recursively for two different values (true, false) of $x$. 
	Finally, the $entropy$ of the two subformulas ($\varphi[y \mapsto false]$ and $\varphi[y \mapsto true]$) are summed to obtain the $entropy$ of the current formula, and the $entropy$ associated with the corresponding formula is added to the cache.
	
	Whenever we obtain a satisfiable assignment $\sigma_{\downarrow Y}$ under the restriction of the output set $Y$, we need to calculate the number of different inputs corresponding to this assignment.
	This step can be achieved by performing model counting on the remaining formulas (corresponding to the numerator of $p_{\sigma}$).
	According to \ref{projected-proposition},  the denominator of  $p_{\sigma}$ can be represented by the total number of models of $\varphi$, from which $p_{\sigma}$ can be calculated, and then the $infor$ of the sub-formula can be derived.
	Algorithm \ref{InforComputing} describes this process. 
	
	Algorithm \ref{InforComputing} receives a remaining CNF formula $\Phi$ after all variables in $Y$ have been assigned and returns the $infor$ $H(\varphi)$ of $\Phi$.
	As mentioned above, we perform model counting on the remaining sub-formulas in lines 1-4.
	In the model counting of the $X$-stage, we propose two methods, called \textbf{Counting} and \textbf{Condition}, with $count_{opt}$ as a parameter to indicate the chosen method.
	The first approach corresponds to lines 1-2, where any exact model counting tool can be called. 
	It's worth noting that this method does not just query the model counting individually.
	Instead, we share the component cache for all model counting queries, and we refer to this strategy as \textbf{XCache}.
	The other method requires constructing in advance an ADD-L that contains all the variables in $Y$, which can also include variables from the $X$.
	Then convert the model counting into conditional model~\cite{lai2017new} counting based on the current variable assignment.
	In line 5, probability $p_{\sigma}$ is computed according to the definition in Section 2, where $totalcount$ is the number of models of the entire formula $\varphi$, which is also computed by a model counting tool. 
	We only need to calculate $totalcount$ once and store it. 
	In line 6, the $infor$ can be obtained from $p_{\sigma}$ and returned as a result in line 7.
	
\end{comment}

