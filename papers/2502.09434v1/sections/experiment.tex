\section{Experiments}
\subsection{Experimental Setup}
\label{sec:datasets}

\textbf{Datasets.}
We evaluate our method on CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, AFHQ-DOG~\cite{choi2020stargan} for training from scratch, and LAION-10k~\cite{somepalli2024understanding} for fine-tuning text-conditioned model. 
CIFAR-10 and CIFAR-100 consist of 50,000 32x32 color images, divided into 10 and 100 classes respectively. 
AFHQ-DOG is a subset of the AFHQ dataset with approximately 5,000 512x512 dog images, resized to 64x64 for our experiments.
LAION-10k is a subset of LAION~\cite{schuhmann2021laion}, comprising 10,000 image-text pairs with each image having a resolution of 256x256 pixels.
\label{sec:setup}

\textbf{Implementation Details of Training.} 
We conduct experiments on training unconditional diffusion models from scratch using the CIFAR-10, CIFAR-100, and AFHQ-DOG datasets. 
The IET framework divides CIFAR datasets into 10 shards and AFHQ-DOG into 5 shards.
Threshold $\lambda$ is set to 0.5 for CIFAR datasets and 0.7 for AFHQ-DOG. \gxlnote{The augmentation range $R$ is set to 1.7 for CIFAR-10 and CIFAR-100, and 1.2 for AFHQ-DOG.}
To demonstrate the effectiveness of our method in text-conditioned diffusion models, we fine-tune Stable Diffusion~\cite{rombach2022high} on LAION-10k following the setup of Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2024understanding}.
The IET framework divides the LAION-10k dataset into \gxlnote{4} shards, the threshold $\lambda$ is set to 0.8 with \gxlnote{the augmentation range $R$ set to $\infty$.}
For all datasets, the smoothing factor $\eta$ is 0.8, \gxlnote{and the redistribute proportion $P$ is 0.25 }. The augmentation parameter in RandAugment~\cite{cubuk2020randaugment} is set to 5 for CIFAR-100, AFHQ-DOG, and LAION-10k  and 3 for CIFAR-10. 
Further details are in the supplementary material.

\begin{table*}[t]
  \centering
  \caption{Comparisons of unconditional generation on three datasets in terms of memorized quantity denoted as MQ. We also report the FID to evaluate the quality of images produced by the model. Best in bold and second with underline. These notes are the same to other tables following.}% ``-'' represents that the model training has collapsed and there is no result.}
  %\setlength{\extrarowheight}{1.2ex}
  \begingroup %这一行要加！！！！！！
  \setlength{\tabcolsep}{5.2pt} % 调整列间距  Default value: 6pt
  \renewcommand{\arraystretch}{1} % 调整行间距  Default value: 1
   {\fontsize{7.5}{7.5}\selectfont %
    \begin{tabular}
    {l|l|ccc|c|ccc|c|ccc|c}
    % {C{1.05cm}|C{0.8cm}C{0.8cm}C{0.95cm}|C{0.55cm}|C{0.9cm}C{0.9cm}C{0.95cm}|C{0.55cm}|C{0.9cm}C{0.9cm}C{0.95cm}|C{0.55cm}}
     \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multirow{2}[4]{*}{Method} & \multirow{2}[4]{*}{Venue}&\multicolumn{4}{c|}{CIFAR-10} & \multicolumn{4}{c|}{CIFAR-100} & \multicolumn{4}{c}{AFHQ-DOG} \bigstrut\\
\cline{3-14}     &     & MQ$_{0.4}$ & MQ$_{0.5}$ & MQ$_{0.6}$$\downarrow$ & FID$\downarrow$   & MQ$_{0.4}$ & MQ$_{0.5}$ & MQ$_{0.6}$$\downarrow$ & FID$\downarrow$   & MQ$_{0.4}$ & MQ$_{0.5}$ & MQ$_{0.6}$$\downarrow$ & FID$\downarrow$ \bigstrut\\

    \hline
    \hline
    Default (DDPM)~\cite{ho2020denoising}&NeurIPS2020 & 111   & 465   & 2030  & 8.81 & 429   & 1727  & 5620  & 9.29  & 12344 & 19053 & 30795 &23.59 \bigstrut\\
    
     Adding Noise~\cite{ho2020denoising} & NeurIPS2020&197   & 593   & 2091  & 94.61 & 179   & 1037  & 4383  & 86.18  & 1170 0 & 19295 & 27224 & 61.18 \bigstrut\\
   
     Adding DP-SGD~\cite{abadi2016deep} & CCS2016 &148   & 728   & 3200  & 12.55 & -   & -  & -  & -  & - & - & - & - \bigstrut\\
     Ambient Diffusion~\cite{daras2024ambient} & NeurIPS2023& 22   & 138   & 851  & 11.7 & -   & -  & -  & -  & - & - & - & - \bigstrut\\
      
      \hline
       IET-AGC~\cite{liu2024iterative} &ECCV2024& \underline{14}    & \underline{117} & \underline{839}   & \underline{8.34} & \underline{144}  & \underline{760} & \underline{3274}  & \underline{8.51} & \underline{1811}  & \underline{5435} & \underline{15237} & \textbf{22.20} \bigstrut[t]\\
 IET-AGC+ &Ours & \textbf{10}    & \textbf{73} & \textbf{623}   & \textbf{8.33} & \textbf{124}   & \textbf{691} & \textbf{3063}  & \textbf{7.81} & \textbf{1083}  & \textbf{3208} & \textbf{9577} & \underline{24.20} \bigstrut\\
    \hline
     \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
    }
    \endgroup
  \label{tab:attack_result}%
  \vspace{-5pt}
\end{table*}%


\textbf{\gxlnote{Evaluation Metrics.}} 
\gxlnote{We evaluate the generations from three perspectives: memorization, generation quality, and text-image alignment.} For memorization, we adopt Carlini's detection rule~\cite{carlini2023extracting} for unconditional generation, considering 
$x$ as memorized if the $\ell_{2}$ distance to its nearest neighbor $\bar{x}$ is significantly lower compared to the $n$ closest neighbors $\mathbb{S}^n_{\bar{x}}$. We modify this rule to:
\begin{equation}
\ell(x,\bar{x};\mathbb{S}^n_{\bar{x}}) = \frac{\ell_{2}(x,\bar{x})}{\mathbb{E}_{y\in{\mathbb{S}^n_{\bar{x}}}}[\ell_{2}(\bar{x},y)]},
  \label{eq:attack}
\end{equation}
where $n = 50$ in our experiment. If the sample's $\ell$-loss value falls below the threshold $\delta_{V}$, it is considered to be memorized:
\begin{equation}
    IsMemo(\delta_{V}, x,\bar{x};\mathbb{S}^n_{\bar{x}}) = \mathbb{I}(\ell_(x,\bar{x};\mathbb{S}^n_{\bar{x}})\leq\delta_{V}).
  \label{eq:attack_threshold}
\end{equation}
The more images below $\delta_{V}$, the stronger the model's memorization.
We generate 65,536 images per model, calculate their $\ell$-loss, and count images below thresholds $\delta_{V}$ of 0.4, 0.5, and 0.6 to quantitatively evaluate the model's memorization, denoted as MQ$_{0.4}$, MQ$_{0.5}$ and MQ$_{0.6}$.
We adopt Somepalli 's evaluation rule~\cite{somepalli2024understanding} for text-conditioned generation, which quantifies memorization using a similarity score derived from the dot product of SSCD features~\cite{pizzi2022self} of $x$ and the nearest neighbor $\bar{x}$:
\begin{equation}
    \zeta = E(\bar{x})^T \cdot E(x),
\end{equation}
where $E(\cdot)$ is the features obtained by SSCD~\cite{pizzi2022self}.  The dataset similarity score (Sim Score) is then defined as the 95th percentile of similarity score distribution for all generated images.
\gxlnote{We use FID~\cite{heusel2017gans} to evaluate the quality of model outputs and Clip Score~\cite{hessel2021clipscore} to measure the generated images’ alignment with the input text prompts.}

\subsection{Experimental Results}
\label{sec:result}
\subsubsection{Training from Scratch}
The experimental results of our method and four competitive methods are shown in Tab.~\ref{tab:attack_result}.
``Default (DDPM)'' denotes the conventional training approach of DDPM~\cite{ho2020denoising}. 
``Adding DP-SGD'' denotes the method of adding Differentially Private Stochastic Gradient Descent~\cite{abadi2016deep}, which involves clipping and adding noise to the model's gradients to protect privacy, albeit at the cost of some image quality.
``Adding Noise'' denotes a method of directly adding Gaussian noise to the images during training, with a mean of 0 and a variance of 0.1.
``Ambient Diffusion''~\cite{daras2024ambient} protected privacy by training generative models on highly corrupted samples, preventing the model from directly observing clean training data. ``IET-AGC'' is our preliminary version ~\cite{liu2024iterative}.

Results in Tab.~\ref{tab:attack_result} show that adding noise or gradients to the training images reduces the quality of the generated images. 
However, it still does not resolve the issue of training image memorization.
Despite Ambient Diffusion also reducing memorization, it leads to a significant increase in FID (from 8.81 to 11.7), indicating a notable degradation of image quality.
Compared with the default training approach, our method maintains or even slightly improves the generative quality by reducing the FID score.
At the same time, our method significantly reduces the diffusion model's memorization of the training data. As shown in Tab.~\ref{tab:attack_result}, for the MQ$_{0.4}$ score, the number of memorized images reduces by \gxlnote{90.1\%, 74.6\%, and 91.2\%} compared with the default training on CIFAR-10, CIFAR-100, and AFHQ-DOG, respectively, illustrating the effectiveness of our method.

\subsubsection{Fine-tuning Pre-trained Diffusion Models}
\label{sec:finetune}
Training a diffusion model from scratch requires a significant amount of computational resources and time. Thus, fine-tuning a pre-trained diffusion model with limited epochs to reduce memorization is necessary.
To further demonstrate the effectiveness and applicability of our method, we finetune text-conditional Stable Diffusion.

For baselines, we compare the methods from ``Default (SD)'', Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2024understanding}, and Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}.
``Default (SD)'' denotes the conventional fine-tuning approach of SD~\cite{rombach2022high}.
The results are presented in \cref{tab:finetnue_SD}.
Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2024understanding} protected privacy by randomizing conditional information (e.g., RT, CWR, GNI, MC, RC, and CWR in \cref{tab:finetnue_SD}) during training and inference, thereby reducing the likelihood of the model replicating specific training data. 
\gxlnote{Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting} also mitigated memorization in two stages: excluding samples exceeding a certain threshold during training and adjusting prompt embeddings during inference. 
The method proposed by Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2024understanding} has limited effectiveness in mitigating memorization, both during the training phase and the inference phase.
On the other hand, the approaches designed by Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting} achieve high performance in Sim Score but excessively excluding samples limits improvements in text alignment and image quality.
However, our method effectively balances memorization and generation quality, achieving a Sim Score of 0.34, which represents a 46.7\% reduction compared to the default method, while maintaining the highest Clip Score of 31.27 and competitive FID of 16.3. }
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \caption{Fine-tuning results of Stable Diffusion model on LAION-10k. ``Phase'' refers to the phase for mitigating memorization, encompassing both the inference phase and the training phase.}
  \begingroup %这一行要加！！！！！！
  \setlength{\tabcolsep}{3.5pt} % 调整列间距  Default value: 6pt
  \renewcommand{\arraystretch}{1} % 调整行间距  Default value: 1
    \begin{tabular}{l|ll|l|ccc}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    Phase & \multicolumn{2}{l|}{Method} &Venue& Sim Score↓ & Clip Score↑ & FID↓ \bigstrut\\
    \hline
    \hline
          & \multicolumn{2}{l|}{Default (SD)~\cite{rombach2022high}} &CVPR2022& 0.638  & 30.52 & 18.7  \bigstrut\\
    \hline
    \multirow{4}[2]{*}{Infer.} & \multicolumn{2}{l|}{RT~\cite{somepalli2024understanding}}&NeurIPS2023 & 0.524  & 29.54 & 18.7  \bigstrut[t]\\
          & \multicolumn{2}{l|}{CWR~\cite{somepalli2024understanding}}&NeurIPS2023 & 0.576  & 30.13 & 18.1  \\
          & \multicolumn{2}{l|}{GNI~\cite{somepalli2024understanding}}&NeurIPS2023 & 0.615  & 30.32 & 18.9  \\
          & \multicolumn{2}{l|}{Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}}&ICLR2024 & 0.352  & 28.56 & 25.7  \bigstrut[b]\\
    \hline
    \multirow{6}[4]{*}{Train} & \multicolumn{2}{l|}{MC~\cite{somepalli2024understanding}} &NeurIPS2023& 0.420  & 30.27 & 16.6  \bigstrut[t]\\
           & \multicolumn{2}{l|}{RC~\cite{somepalli2024understanding}}&NeurIPS2023 & 0.565  & 30.64 & \textbf{16.0} \\
          & \multicolumn{2}{l|}{CWR~\cite{somepalli2024understanding}}&NeurIPS2023 & 0.614  & 30.79 & 16.7  \\
          & \multicolumn{2}{l|}{Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}}&ICLR2024 & \textbf{0.320} & \underline{30.86}  & 17.5  \bigstrut[b]\\
\cline{2-7}          & \multicolumn{2}{l|}{IET-AGC~\cite{liu2024iterative}}&ECCV2024 & 0.393  & 31.25  & 16.9 \bigstrut[t]\\
          & \multicolumn{2}{l|}{IET-AGC+} &Ours& \underline{0.340}  & \textbf{31.27} & \underline{16.3}  \bigstrut[b]\\
    \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
    \endgroup
  \label{tab:finetnue_SD}%
  \vspace{-12pt}
\end{table}%

\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{7pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-2pt} % 设置标题下方的间距为 -5pt
  \includegraphics[width=1.0\linewidth]{imgs/histogram.pdf}
  \caption{\gxlnote{Comparison of the Sim Score histograms between the generated images and the training images for both the default method and our approach. The label sim(train, train) refers to the Sim Score between images in the training set and all other training images (excluding the image itself).}}
  \label{fig:SD_histogram}
\end{figure}

\gxlnote{Additionally, we also present similarity score distribution plots of all generated images in ~\cref{fig:SD_histogram}. Compared to the default method, our approach results in overall lower similarity scores, with the majority of similarity scores of the data concentrated in the 0.2$\sim$0.3 range, showing closer similarity to the training set itself. This further demonstrates that our method significantly reduces the model's memorization ability.}

\textbf{Visualization.} To provide a more intuitive confirmation of our training method, with the conditions of the same captions, we visualize the images generated from our
method and the baseline methods  \cref{fig:Visual}.
\gxlnote{When the method without mitigation is applied, the generated images exhibit high similarity to the training images. While memorization mitigation methods show some differences from the training images, the effect is not as pronounced or the quality of the generated images slightly decreases. In contrast, the images generated by our method are more diverse in content, and their quality remains high without significant degradation.}
\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{12pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{2pt} % 设置标题下方的间距为 -5pt
    % First row: Text and image

    \begin{minipage}{0.2\linewidth}
        \centering
        Training Image
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD0.png} % Replace with your image filename
    \end{minipage}

    % Add vertical space between rows
    \vspace{1em}

    % Second row: Text and image
    \begin{minipage}{0.2\linewidth}
        \centering
    Default (SD)~\cite{rombach2022high} 
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD1.png} % Second image
    \end{minipage}
    
     % Add vertical space between rows
    \vspace{1em}
    % Second row: Text and image
    \begin{minipage}{0.2\linewidth}
        \centering
        CWR~\cite{somepalli2024understanding}
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD2.png} % Second image
    \end{minipage}
    
     % Add vertical space between rows
    \vspace{1em}
    % Second row: Text and image
    \begin{minipage}{0.2\linewidth}
        \centering
        RT~\cite{somepalli2024understanding}
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD3.png} % Second image
    \end{minipage}

     % Add vertical space between rows
    \vspace{1em}
    % Second row: Text and image
    \begin{minipage}{0.2\linewidth}
        \centering
        Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD5.png} % Second image
    \end{minipage}

     % Add vertical space between rows
    \vspace{1em}
    % Second row: Text and image
    \begin{minipage}{0.2\linewidth}
        \centering
        Ours
    \end{minipage}%
    \hspace{1em} % Horizontal space between text and image
    \begin{minipage}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/visual_SD4.png} % Second image
    \end{minipage}
    \caption{The visualizations of the generated images from our method and the baseline methods. Each column presents images generated by different methods using the same caption and random seed, alongside the corresponding training set images for that caption.}
    \label{fig:Visual}
    \vspace{-1em}
\end{figure}


\subsection{Analysis of Skipping}
\label{sec:analysis}
In this section, we conduct comparative experiments on the AFHQ-DOG dataset to delve into which types of images are prone to be skipped, as well as the relationship between memorizable images and the images that are skipped.


\subsubsection{Images Most Easily Skipped}
We believe the images are more easily skipped for two main reasons.
Firstly, data aggregation: we compute the $\ell_{2}$ distance between these easily skipped images and all other images in the dataset, as well as between those not easily skipped images and all other images in the dataset.
The left subplot in \cref{fig:Ana_hist} indicates that the distribution of the skipped images is more clustered.
Consistent with the findings of Carlini \MakeLowercase{\textit{et al.}}~\cite{carlini2023extracting}, which suggested that removing duplicate training images effectively reduces memorization capacity, skipping these clustered images can also reduce memorization capacity.
Secondly, data simplicity: we performed Fourier transforms~\cite{sneddon1995fourier} on these easily skipped and not easily skipped images to obtain their energy distributions. This process helps decompose the image into different frequency bands, where low frequencies correspond to broad, smooth structures, and high frequencies capture fine details or noise. By examining the frequency spectrum, we quantified the energy distribution, which reflects the amount of information or complexity present in the image. As is shown in the right subplot of \cref{fig:Ana_hist},
the easily skipped images have less energy, indicating that they lack finer details.
We believe both factors contribute to the model’s tendency to memorize these images, making their skipping effective in reducing memorization capacity.
\begin{figure*}[tb]
  \setlength{\abovecaptionskip}{0pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-7pt} % 设置标题下方的间距为 -5pt
  \centering
  \begin{minipage}{0.43\linewidth}
    \includegraphics[width=1.\linewidth]{./imgs/l2_ana.pdf}
    % \caption{Distribution of distances to the most similar images in the dataset.}
    \label{fig: Ana_l2_most_least}
  \end{minipage}
  % \hspace{0.03\linewidth}
  \begin{minipage}{0.43\linewidth}
    \includegraphics[width=1.\linewidth]{./imgs/energy_distribution_scatter.pdf}
    % \caption{Energy distribution. The greater the energy, the more complex the image.}
    \label{fig: Ana_spec}
  \end{minipage}
  \caption{The data distribution analysis of images skipped most and images skipped least. The left subplot shows the distribution of distances to the most similar images in the dataset. The right subplot displays energy distribution. The greater the energy, the more complex the image.}
  \label{fig:Ana_hist}
\end{figure*}

\subsubsection{Frequency of Skipped Images}
Throughout the training process, we record the identifiers of skipped images. 
As shown in \cref{fig:value_counts}, our method does not entail skipping all images. 
In our approach, about 90\% of the images are skipped fewer than 625 times (across a total of 2,278 training epochs), indicating that our method can effectively differentiate between different images. 
This suggests that we are not simply reducing memorization by constraining the model's learning. 
On the other hand, while our method requires skipping images with exceptionally low loss values, all images still contribute to the model's training.
\begin{figure}[tb]
  \setlength{\abovecaptionskip}{-0pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-7pt} % 设置标题下方的间距为 -5pt
  \centering
  \includegraphics[height=6.5cm]{imgs/afhq_value_counts_1.pdf}
  \caption{
    Distribution of skipped image counts. There are about 90\% of the images are skipped fewer than 625 times. 
  }
  \label{fig:value_counts}
\end{figure}



\subsection{Ablation Study}
\label{sec:ablation}
\subsubsection{Performance Comparisons of Each Component}
To further understand the effectiveness of our approach, we conduct ablation experiments to investigate the individual impacts of different components for training from scratch and fine-tuning Stable Diffusion on LAION-10k. 

\gxlnote{\textbf{Effectiveness of AGC:}~\cref{tab:ablation_cifar10} and ~\cref{tab:ablation_SD} show that Anti-Gradient Control (AGC) effectively mitigates model memorization by excluding easily memorized samples, in both training from scratch and fine-tuning scenarios. When training from scratch,  AGC reduces MQ$_{0.5}$ (465 to 154) by approximately 67\% compared to the conventional method. However, excessive exclusion of samples can reduce the number of training images, which in turn impacts the quality of the generated images, as evidenced by the improvement in FID shown in the ~\cref{tab:ablation_cifar10}.}

\gxlnote{\textbf{Effectiveness of IET:} For Iterative Ensemble Training (IET), it is evident that the way the model learns from the parameters of the proxy model can not only reduce memorization but also significantly improve the quality of images in ~\cref{tab:ablation_cifar10} and ~\cref{tab:ablation_SD}. When training from scratch, although it is not as effective as AGC in reducing memorization, it greatly reduces FID. Compared with AGC, FID has decreased by 26.6\%, greatly improving the quality of images. When fine-tuning, IET has the same effect. Compared with the default method, the Clip Score increases from 30.52 to 31.27. }
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \caption{Performance comparisons of each component for \emph{training from scratch}. }
  {\fontsize{7.5}{7.5}\selectfont
    \begin{tabular}{cccc|ccc|c}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multicolumn{4}{c|}{Method}   & \multicolumn{4}{c}{CIFAR-10} \bigstrut\\
    \hline
    AGC   & IET   & TAA   & MSR   &  MQ$_{0.4}$ & MQ$_{0.5}$ & MQ$_{0.6}$$\downarrow$ & FID$\downarrow$ \bigstrut\\
    \hline
    \hline
    \xmark     & \xmark     & \xmark     & \xmark     & 111   & 465   & 2030  & 8.81 \bigstrut[t]\\
    \cmark     & \xmark     & \xmark     & \xmark     & 26    & 154   & 976   & 11.36 \\
    \cmark     & \cmark     & \xmark     & \xmark     & 14   & 117   & 839   & \underline{8.34} \\
    \cmark     & \cmark     & \cmark     & \xmark     & \textbf{8} & \underline{81} & \underline{678} & 9.20 \\
    \cmark     & \cmark     & \cmark     & \cmark     & \underline{10}     & \textbf{73}    & \textbf{623}   & \textbf{8.33} \bigstrut[b]\\
    \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
    }
  \label{tab:ablation_cifar10}%
  \vspace{-7pt}
\end{table}%

\gxlnote{\textbf{Effectiveness of TAA:} Considering that memorization varies in degree and cannot be simply addressed with a hard threshold, we propose Threshold-Aware Augmentation (TAA). The fourth rows of ~\cref{tab:ablation_cifar10}  and ~\cref{tab:ablation_SD} show that augmenting samples above the skipping threshold effectively mitigates the issue of overlooked memorized samples in the AGC strategy and further reduces memorization. In training from scratch, compared with our conference version method, MQ$_{0.5}$ is decreased from 117 to 81 by 30.8\%. 
% As for fine-tuning, Sim Score is decreased by 11\% (from 0.393 to 0.350).
At the same time, by applying varying levels of augmentation based on the sample's loss, TAA does not reduce image quality to a noticeable extent. As for fine-tuning, compared with IET-AGC,  Clip Score is increased from 31.25 to 31.18.}

\gxlnote{\textbf{Effectiveness of MSR:} For Memory Samples Redistribute (MSR), we can see that in ~\cref{tab:ablation_cifar10} and ~\cref{tab:ablation_SD}, re-by engaging excessively skipped samples for learning, the MSR method significantly enhancing image quality. When training from scratch, compared to the previous row, FID is decreased from 9.20 to 8.33.  In addition,  the model's memorization capacity is not significantly affected. This suggests that once the easily memorized samples are exchanged across shards, they no longer retain their high memorization potential. For instance, when training from scratch, compared to the previous row, MQ$_{0.6}$ is decreased from 678 to 623.}


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \caption{Performance comparisons of each component for \emph{fine-tuning} Stable Diffusion on LAION-10k.}
    \begin{tabular}{cccc|ccc}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multicolumn{4}{c|}{Method}   & \multirow{2}[4]{*}{Sim Score↓} & \multirow{2}[4]{*}{Clip Score↑} & \multirow{2}[4]{*}{FID↓} \bigstrut\\
\cline{1-4}    AGC   & IET   & TAA   & \multicolumn{1}{c|}{MSR} &       &       &  \bigstrut\\
    \hline
    \hline
     \xmark     & \xmark     & \xmark     & \xmark    & 0.638  & 30.52 & 18.7  \bigstrut[t]\\
    \cmark     & \xmark     & \xmark     & \xmark     & 0.533  & 30.57  & 18.5 \\
    \cmark     & \cmark     & \xmark     & \xmark    & 0.393  & 31.25  & 16.9 \\
   \cmark     & \cmark     & \cmark     & \xmark      & 0.350  & 31.18 & 16.7 \\
    \cmark     & \cmark     & \cmark     & \cmark      & \textbf{0.340} & \textbf{31.27} & \textbf{16.3} \bigstrut[b]\\
    \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
  \label{tab:ablation_SD}%
\end{table}%



\subsubsection{Different Samples Redistribute Methods}
% To address the issue of excessively skipping, we propose the Memory
% Samples Redistribute (MSR) strategy.
MSR is designed to address the issue of excessive skipping and then to improve image quality.
To further validate the effectiveness of MSR, we experiment with random samples redistribute, where samples exchanged between shards are selected randomly. The results, shown in ~\cref{tab:ablation_MSR}, indicate that there is little difference in memory mitigation between the two redistribution methods. In contrast, memory samples redistribute achieves superior image quality, demonstrating that frequently skipped samples are essential for improving image generation quality. MSR facilitates their relearning, effectively enhancing overall performance. However, random samples redistribute lacks specificity in addressing such samples, resulting in no significant improvement in image quality.
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
% \vspace{2em}
  \centering
  \caption{The result about different samples redistribute strategies in our method. The suffix ``Random'' represents random samples redistribute, where samples exchanged between shards are selected randomly. The suffix ``Memory'' represents memory samples redistribute, \MakeLowercase{\textit{i.e.}}, MSR.}
    \begin{tabular}{ll|ccc}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multicolumn{2}{l|}{Method} & Sim Score↓ & Clip Score↑ & FID↓ \bigstrut\\
    \hline
    \hline
    % \multicolumn{2}{l|}{Default SD~\cite{rombach2022high}} & 0.638  & 30.52  & 18.72  \bigstrut[t]\\
    \multicolumn{2}{l|}{IET-AGC+$_\mathnormal{Random}$} & \textbf{0.340} & 31.10  & 16.60  \bigstrut[t]\\
    \multicolumn{2}{l|}{IET-AGC+$_\mathnormal{Memory}$} & \textbf{0.340} & \textbf{31.27} & \textbf{16.30} \\
    \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
  \label{tab:ablation_MSR}%
\end{table}%



\subsubsection{Composition of our methods with existing works}
\gxlnote{Our approach is orthogonal to existing state-of-the-art mitigation works. To demonstrate the applicability of our method, we apply our method to  Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2024understanding} and Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting} 's inference phase mitigation mechanisms. The results are shown in ~\cref{tab:addinference}. Our method can be applied to their approach to further enhance performance. Not only does it reduce memorization, but also it improves image quality and text alignment. For instance, ``GNI+Ours'' shows a 45.2\% decrease in Sim Score compared to ``GNI'', a 0.54 (30.32 to 30.86) increase in Clip Score, and a 1.9 (18.9 to 17.0) reduction in FID. }


% Table generated by Excel2LaTeX from sheet 'Sheet1'

\begin{table}[t]
  \centering
  \caption{Composition of our methods with existing works. Our approach is orthogonal to existing state-of-the-art mitigation strategies. Thus, our method can be applied to existing works and has achieved a significant improvement.}
  \vspace{1em}
    \begin{tabular}{ll|ccc}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multicolumn{2}{l|}{Method} & Sim Score↓ & Clip Score↑ & FID↓ \bigstrut\\
    \hline
    \hline
    \multicolumn{2}{l|}{RT~\cite{somepalli2024understanding}} & 0.524  & 29.54 & 18.7  \bigstrut[t]\\
    \multicolumn{2}{l|}{RT~\cite{somepalli2024understanding} + Ours}& \textbf{0.325}  & \textbf{30.83}  & \textbf{16.7}  \bigstrut[b]\\
    \hline
    \multicolumn{2}{l|}{CWR~\cite{somepalli2024understanding}} & 0.576  & 30.13 & 18.1  \bigstrut[t]\\
    \multicolumn{2}{l|}{ CWR~\cite{somepalli2024understanding}+ Ours} & \textbf{0.343}  & \textbf{30.52}  & \textbf{16.7}  \bigstrut[b]\\
    \hline
    \multicolumn{2}{l|}{GNI~\cite{somepalli2024understanding}} & 0.615  & 30.32 & 18.9  \bigstrut[t]\\
    \multicolumn{2}{l|}{GNI~\cite{somepalli2024understanding}+ Ours} & \textbf{0.337}  & \textbf{30.86}  & \textbf{17.0}  \bigstrut[b]\\
    \hline
    \multicolumn{2}{l|}{Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}} & 0.352  & \textbf{28.56} & 25.7\bigstrut[t]\\
    \multicolumn{2}{l|}{Wen \MakeLowercase{\textit{et al.}}~\cite{wen2023detecting}+ Ours} & \textbf{0.272} & 28.50  & \textbf{21.5}  \bigstrut[b]\\
    \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
  \label{tab:addinference}%
  \vspace{-1pt}
\end{table}%

\begin{table}
\centering
 % \vspace{1em}
  % \setlength{\abovecaptionskip}{2pt}
  \caption{Parameters impact on experimental results.}
  \vspace{1em}
  {
  \fontsize{7.5}{8}\selectfont %
    \begin{tabular}{lc|ccc|c}
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \hline
    \multicolumn{2}{c|}{\multirow{2}[3]{*}{Parameters}} & \multicolumn{4}{c}{CIFAR-10} \bigstrut[t]\\
\cline{3-6}    \multicolumn{2}{c|}{} & MQ$_{0.4}$ & MQ$_{0.5}$ & MQ$_{0.6}$$\downarrow$ & FID$\downarrow$ \bigstrut\\
    \hline
    \hline
    \multirow{5}[0]{*}{Number of Shards $K$} 
    
    & 1    & 111 & 465 & 2030   & 8.81 \bigstrut[t]\\
    & 2     & 14    & 79   & \textbf{501} & \textbf{7.47}\\
     & 5     & \textbf{6}    & \textbf{51}   & 507 & 8.17\\
     & 10    & 10 &73 & 623   & 8.33 \\
        & 15    & 21    & 118   & 747  & 10.68 \bigstrut[b]\\
    \hline
    \multirow{3}[2]{*}{Epoches per Interaction $E$}  & 25    & 21    & 128   & 793  & 10.89 \bigstrut[t]\\
    & 50   & \textbf{10} & \textbf{73} & \textbf{623} & \textbf{8.33} \\
    & 100    & 21    & 123   & 828  & 9.57  \bigstrut[b]\\
    \hline
    \multirow{3}[2]{*}{Redistribute Proportion $P$} & 0.10   & 12    & 86   & 638  & 9.03 \bigstrut[t]\\
     & 0.25   & \textbf{10} & \textbf{73} & \textbf{623} & \textbf{8.33} \\
          & 0.50   & 11   & 90   & 750   & 8.66 \bigstrut[b]\\
    \hline
    \multirow{3}[2]{*}{Skipping Threshold ${\lambda}$}  & 0.4   & 19    & 135   & 985  & 8.69 \bigstrut[t] \\
     & 0.5     & 10    & 73   & 623   & \textbf{8.33} \\
     & 0.6   & \textbf{9} & \textbf{52} & \textbf{372} & 12.91 \bigstrut[b]\\
    \hline
    % \multirow{3}[2]{*}{Augmentation Range $R$} & 1.6   & 25    & 139   & 861  & 9.37 \bigstrut[t]\\
    %  & 1.7   & 10 & 73 & 623 & \textbf{8.33} \\
    %       & 1.8  & \textbf{8}    & \textbf{58}   & \textbf{489}   & 9.84 \bigstrut[b]\\
    % \hline
    \specialrule{\heavyrulewidth}{0pt}{0pt} % 加粗的水平线，位于表格底部
    \end{tabular}%
    }
  \label{tab:parameters_effect}%
\end{table}%
\subsection{Exploring Parameters Impact on Experimental Results.}
\label{sec:impact}
In this study, we examine how various parameters affect our experimental outcomes.
By systematically varying these parameters, we aim to understand how they influence our results and to identify the optimal settings for our experiments. 
Specifically, we conduct a series of experiments where we change the number of shards $K$, training epochs per interaction period $E$, redistribute proportion $P$, and skipping threshold $\lambda$. For each variation, we measure the impact on MQ and FID. The default parameters are set with the number of shards $K$ as 10, epochs per shard $E$ as 50, redistribute proportion $P$ as 0.25, and skipping threshold $\lambda$ as 0.5.

The results are reported in \cref{tab:parameters_effect}.



\textbf{Number of Shards $K$.} We investigate the impact of the number of data shards on model performance by setting it to 1, 2, 5, 10, and 15. $K=1$ means the default training strategy of diffusion models. Results show that the MQ scores of $K=2, 5, 10, 15$ are all lower than $K=1$, indicating that using our IET method can effectively reduce memorization. When $K=5$, the MQ score achieves the best performance. Moreover, the effect of improving image quality becomes more pronounced as the number of data shards decreases.


\textbf{Training Epochs per Interaction Period $E$.} 
We conduct experiments by varying the number of epochs per model interaction period, \MakeLowercase{\textit{i.e.}}, the interaction frequency of parameter aggregation and sample redistribute. Results show that both high and low frequencies of aggregation will reduce the performance of the memorization mitigation and image quality. Thus, we choose $E=50$ to optimize the performance of MQ.


\gxlnote{\textbf{Redistribute Proportion $P$.}
We explore the impact of the redistribute proportion parameter by setting it to 0.10, 0.25, and 0.50. As observed, when the proportion of redistributed data is too small, many frequently skipped samples cannot be relearned, limiting improvements in image quality. However, if too much data is redistributed across shards, there is a risk of exacerbating memorization. In our experimental setup, the redistribute proportion of 0.25 yields the best results.}

\textbf{Skipping Threshold $\lambda$.} We evaluate the importance of $\lambda$ in mitigating the memorization effect by setting the values of $\lambda$ to $0.4$, $0.5$, and $0.6$. A large threshold means skipping more training samples that are easily memorized. Results in ~\cref{tab:parameters_effect} show as $\lambda$ grows, more memorable training samples are skipped and the memorization phenomenon is further reduced. However, skipping more samples will reduce the model performance, \MakeLowercase{\textit{i.e.}}, the generation quality. \gxlnote{Therefore, when selecting the skip threshold, we need to strike a balance between image quality and mitigating memorization.}