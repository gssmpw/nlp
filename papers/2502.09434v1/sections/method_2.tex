\begin{figure}[t]
  \centering
  \setlength{\abovecaptionskip}{7pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-7pt} % 设置标题下方的间距为 -5pt
  \includegraphics[height=6.5cm]{imgs/line_plot.pdf}
  \caption{
  Comparison of the training losses between memorized and non-memorized images. 
  }
  \label{fig:LossAnalysis}
\end{figure}
\section{\gxlnote{Exploring Training Loss and Memorization in Diffusion Models}}
To reduce memorization of training data, we delve into the causes of memorization phenomena, specifically analyzing it through the lens of the training loss, \gxlnote{because we suspect that images with varying degrees of memorization might exhibit different behaviors during the training process.}
We begin by establishing the fundamental notation linked with diffusion models.
Diffusion models \cite{ho2020denoising} originate from the non-equilibrium statistical physics \cite{sohl2015deep}.
They are essentially straightforward: they operate as image denoisers.
During the training process, when given a clean image $x$, time-step $t$ is sampled from the interval [$0$, $T$], along with a Gaussian noise vector $\epsilon \sim \mathit{N} (0, I)$,
resulting in a noised image $x_t$:
\begin{equation}\label{eq:noised_data}
    x_t = \sqrt{\alpha_t}x + \sqrt{1-\alpha_t} \epsilon, 
\end{equation}
where the scheduled variance $\alpha_t$ varies between $0$ and $1$, with $\alpha_0 = 1$ and $\alpha_T = 0$. 
The diffusion model then removes the noise to reconstruct the original image $x$ by predicting the noise introduced, achieved through stochastic minimization of the objective function
$\frac{1}{N} \sum_{i} \mathbb{E}_{t,\epsilon} \mathcal{L} (x_i, t, \epsilon; \theta)$, where
\begin{equation}\label{eq:loss}
  \mathcal{L} (x_i, t, \epsilon; \theta) = \| \epsilon - \epsilon_\theta(\sqrt{\alpha_t}x_i + \sqrt{1-\alpha_t}\epsilon, t) \|^2.
\end{equation}

\begin{figure*}[t]
  \centering
  \setlength{\abovecaptionskip}{7pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-7pt} % 设置标题下方的间距为 -5pt
  \includegraphics[width=1.0\linewidth]{imgs/method.pdf}
  \caption{Framework overview of our method. During the training stage, we train multiple proxy models on several data shards. Besides, we selectively skip samples based on their training loss and track how often each sample is skipped in each shard. During the interaction stage, there are two main parts: first, the proxy models are aggregated into a new model, and its weights are distributed as initial weights for the next training phase; second, each shard redistributes its top $P$ skipped samples to the next shard, assigning the last shard to the first. In the next training stage, each shard resumes training with the updated data and model.}
  \label{fig:method}
\end{figure*}
To analyze the correlation between losses and image memorization, \gxlnote{We identify memorized images on CIFAR-10 by generating 65,536 images using a pre-trained model (DDPM)~\cite{ho2020denoising}  and selecting the top 256 training images with the highest similarity to their nearest generated neighbors.} Then we calculate their loss functions at each time step. 
Similarly, we sample 256 non-memorized images from the remaining training data and compute their losses at each time step.
\cref{fig:LossAnalysis} shows the comparisons of the losses.
% when the time step is in the interval [0, 600] $(T=1000)$. 
Memorized images exhibit significantly smaller loss values during this period, indicating that the model tends to reconstruct noise into such images.


\section{Method}
In this section, we present our methodology for mitigating the memorization in diffusion models, without sacrificing excessive image quality.
\subsection{Framework Overview}
\gxlnote{
As shown in ~\cref{fig:method}, our method trains the model by the following two steps iteratively: 
1) training proxy models on each data shard, and 2) conducting two rounds of interaction: proxy model aggregation and shard data redistribution.
Specifically, during the training stage, we divide the dataset into multiple data shards ($D_1, D_2,..., D_K$) and train corresponding proxy diffusion models ($\theta_1, \theta_2,...,\theta_K$). Additionally, we selectively skip certain samples based on their training loss and keep track of the number of times each sample is skipped in each shard. During the interaction stage, the proxy diffusion models  ($\theta_1, \theta_2,...,\theta_K$) from different shards are aggregated into a new model $\hat{\theta}$ through averaging, which serves as the initial model for the next training phase. Meanwhile, each shard identifies and redistributes its top $P$ most easily skipped sample sets to the next shard, updating the data of each shard accordingly. During the next training, each shard resumes training with the updated data shard ($D_1', D_2',..., D_K'$) and model $\hat{\theta}$.
}
\subsection{\gxlnote{Threshold-Aware Control}}
\gxlnote{We first introduce the model updating step.}
In this subsection, we elaborate on how to utilize the aforementioned loss analysis to devise a training strategy to alleviate the occurrence of memorization.
\subsubsection{Anti-Gradient Control}
\textbf{Memory Bank:}
To identify images with exceptionally low loss values that are prone to memorization during training, we need to maintain the average losses for each time step. 
However, computing the average loss at each time step entails substantial computational expenses, as it necessitates evaluating the losses for all images using the model at each time step. Thus, we propose a memory bank to store and update losses during mini-batch training without increasing the time cost. However, 
the losses generally decrease with the training step growing. To address this, when calculating the average loss in the memory bank, we adjust the aggregation process by assigning higher weights to losses that are closer to the current update, rather than simply averaging all losses at the current time step.
Specifically, we initialize an array of length $T$ with zeros, termed the memory bank. 
After calculating the loss for a mini-batch, we update the memory bank using the Exponential Moving Average (EMA) ~\cite{polyak1992acceleration} method based on the loss and the sampled time step, thereby better reflecting the current state of the model:
\begin{equation}\label{eq:ema}
  l_{t} \leftarrow  \eta \cdot l_{t} + (1 - \eta) \cdot \mathcal{L} (x, t, \epsilon; \theta),
\end{equation}
where $\eta$ represents the smoothing factor, and $l_{t}$ represents the averaged loss in the memory bank at time step $t$. 

\textbf{ Loss Ratio-Based Selection:}
In previous observations, if the model exhibits memorization of a certain sample, the loss value of the model on that sample tends to be abnormally small.
Thus, we use the ratio of the training loss of a certain sample to the mean loss in the memory bank at the time step $t$ as a measure to mitigate memorization:
\begin{equation}\label{eq:ratio}
  r(x) = \frac{\mathcal{L} (x, t, \epsilon; \theta)}{l_t}.
\end{equation}
A smaller value of $r(x)$ may indicate a higher likelihood of the image being memorized. 
Then we establish a configurable threshold denoted as $\lambda$. 
If the loss ratio $r(x)$ falls below this threshold $\lambda$, we will skip the image in the mini-batch.
\begin{figure}[tb]
% \vspace{-1.5cm}
  \centering
  \setlength{\abovecaptionskip}{7pt} % 设置标题上方的间距为 -5pt
  \setlength{\belowcaptionskip}{-7pt} % 设置标题下方的间距为 -5pt
  \includegraphics[width=1.0\linewidth]{imgs/AGC+.pdf}
  \caption{\gxlnote{The proposed model update procedure (AGC with TAA). During training, we dynamically update and maintain a memory bank of losses at each timestep. For each sample's loss ratio $\frac{Loss}{l_t}$, we compare it with 
$\lambda$ and $R\lambda$  to update the loss, considering three cases: for losses less than $\lambda$, we skip the sample and update its skip times; for losses between $\lambda$ and $R\lambda$, we augment the sample and retrain to obtain a new loss; for losses greater than $R\lambda$, we keep the loss unchanged.}}
  \label{fig:AGC+}
\end{figure}


\gxlnote{\subsubsection{Threshold-Aware Augmentation}In AGC, images below the threshold are more likely to be memorized, making their exclusion a reasonable choice. However, memorization varies in degree and samples should be dynamically processed based on their level of memorization risk. Therefore, we design this strategy, dynamically enhancing samples to increase their diversity and thus mitigate memorization.}

\gxlnote{Specifically, for samples not skipped,  if their ratio $r$  does not exceed a specific value, that is, $R$ times the threshold, we apply augmentation to them as follows:
\begin{equation}
\mathcal{L}(\mathbf{Aug}(x, \rho(x), t, \epsilon; \theta) \quad \operatorname{if} \quad \lambda < r(x) <  R\lambda, 
\end{equation}
where $R$ is a multiplier with $R>1$, and $\rho(x)$ represents the relative augmentation strength. 
 For the augmentation function, we choose RandAugment\cite{cubuk2020randaugment} which introduces a vastly simplified search space for data augmentation. 
At the same time, we believe that the lower the sample’s loss value is, the higher its risk of memorization is. Therefore, we apply varying levels of augmentation based on its distance from the threshold—the closer it is, the stronger the augmentation. First, we calculate the relative distance between the loss ratio and the skip threshold:
\begin{equation}
    d(x) = \parallel \frac{ r(x) - \lambda}{\lambda} \parallel.
\end{equation}
Then we choose $e^{-Ax}$ as our negatively correlated function between the distance and the augmentation strength:
\begin{equation}
    \rho(x) =e^{-Ad(x)},
\end{equation}
where $A$ is set  as a constant value of 5.}
\gxlnote{\subsubsection{Threshold-Aware Control}With threshold-aware augmentation, the overall model updating is the following function:
\begin{equation}\label{eq:update_loss_plus}
    \mathcal{L}(x) = 
    \begin{cases} 
    0 & \operatorname{if } r(x) < \lambda \\
    % \mathcal{L}(\mathbf{Aug}(x, e^{-5\parallel \frac{ r - \lambda}{\lambda} \parallel})) & \text{if } \lambda < r(x) < R\lambda \\
    \mathcal{L}(\mathbf{Aug}(x, \rho(x)) & \operatorname{if } \lambda < r(x) < R\lambda \\
    \mathcal{L}(x) & \operatorname{otherwise},
    \end{cases}
\end{equation}
where we re-purpose it by expressing as $\mathcal{L}(x) \propto \mathcal{L}(x, t, \epsilon; \theta)$, omitting $t, \epsilon, \theta$ for simplicity. The overall process is in ~\cref{fig:AGC+}.
}

\subsection{Iterative Ensemble Training}
% \subsubsection{Iterative Ensemble Training}
\gxlnote{In traditional training approaches, directly transmitting the entire training data to the model increases the likelihood of easy samples being memorized. However, if the model learns from parameters of other models, rather than directly from the data, it may help to mitigate memorization. Thus,  we propose a framework that trains multiple proxy diffusion models on different data shards of a dataset. }


\textbf{Training on Different Data Shards.} Unlike the training methods of previous diffusion models, which train a single model on the entire dataset once, in this paper, we divide the dataset into multiple data shards and then train the corresponding proxy diffusion models on each separate part. 
\gxlnote{Specially, we suppose the dataset $D$ contains $N$ samples and $C$ classes.  We divide the dataset into $K$ parts in the IID (Independently and Identically Distributed) setting in which each data shard is randomly assigned a uniform distribution over $C$ classes. 
If the dataset does not contain class information, we divide the dataset into $K$ equal parts. In summary, each data shard contains $\frac{N}{K}$ samples.
}
Then, each shard $i$ trains a separate proxy diffusion model $\theta_{i}$ on its own dataset.

\textbf{Aggregating the Multiple Diffusion Models.} After a period of training, each shard develops a distinct proxy diffusion model. We simply average the weights of all proxy models $\theta_{i}$ to obtain a final model $\hat{\theta}$ as
\begin{equation}\label{eq:average}
    \frac{1}{K}\sum_{i = 1}^{K}\theta_{i} \rightarrow \hat{\theta}.
\end{equation}

Then, we repeat the two stages of training on separate shards of the data and aggregate proxy models, using the obtained final model as the initial model for the first stage.

\textbf{Training Time Analysis.}
As each shard contains only $\frac{1}{K}$ of the total data, the training time for each proxy model is proportionally reduced,
maintaining the overall computational cost \emph{nearly constant} compared to training a single model on the entire dataset. 
The only additional computational cost arises from periodically merging the proxy models, which is minimal and has little impact on overall training efficiency.

\vspace{0.5cm}
\subsection{Memory Samples Redistribute}
\gxlnote{Although AGC effectively mitigates memorization by skipping easily memorized samples, this exclusion may result in reducing the available training data, potentially leading to a decrease in image quality.
To address this issue, we integrated Memory Samples Redistribute (MSR) to ensure that these
samples are learned but not easily memorized.  In the IET framework, each proxy model learns from its shard, where the same data may be interpreted differently. A sample frequently memorized in its original shard may not have the same memorization tendency in a new shard. 
Thus, we allow each shard to redistribute samples that are most easily memorized to the next shard during training, which in practice corresponds to the samples that are most frequently skipped.
}

\gxlnote{Specifically, during the training process, we keep track of the number of times each sample is skipped. We define \( s_i^j \) as skip count for the $j$th sample in the \( i \)th shard's dataset and $s_i = \{s_i^1, s_i^2,..., s_i^\frac{N}{K}\}$ represents the set of skip counts for the $i$th shard.  Then each shard identifies the top $P$ of samples that are most likely to be skipped $s_i^{top}=\{\tilde{s}_i^1, \tilde{s}_i^2,..., \tilde{s}_i^{P*\frac{N}{K}} \}$, where $P$ represents the redistributed proportion of the total samples.  The dataset of most easily memorized samples is defined as:
\begin{equation}
    D_i^{\operatorname{easy}} = \{x^j | s_i^j \in s_i^{top} \}.
\end{equation}
Next, each shard distributes these samples to the next shard in a circular manner, as shown in the following function: 
\begin{equation}
    D_{i + 1} \cup D_i^{\operatorname{easy}}  \rightarrow D_{i + 1}^{\prime}, 
\end{equation}
where $i = 1, 2, \ldots, K.$ }
\gxlnote{
As is shown in ~\cref{fig:method},  the top $P$ most skipped samples from $D_1$ are redistributed to $D_2$, the samples from $D_2$ are assigned to $D_3$, and so on, with the samples from $D_K$ being assigned to $D_1$. In the next training phase, each shard’s dataset is updated accordingly.
}
