\section{Related Work}

\subsection{Memorization in Generative Models}
Several studies have examined the memorization capabilities of the generative model~\cite{wang2024replication,sun2024create}. 
Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative} have been at the forefront of this research area. 
As Webster \MakeLowercase{\textit{et al.}}~\cite{webster2021person} demonstrated when applied to face datasets, GANs can occasionally replicate.
Prior study~\cite{carlini2021extracting} explored an adversarial attack on language models like GPT-2~\cite{radford2019language}, where individual training examples can be recovered, including personally identifiable information and unique text sequences.

Recent studies have shifted their attention toward diffusion models. 
Somepalli \MakeLowercase{\textit{et al.}}~\cite{somepalli2023diffusion} found that diffusion models accurately recall and replicate training images, especially noted with models like the Stable Diffusion model~\cite{rombach2022high}.
Building upon this discovery, Carlini \MakeLowercase{\textit{et al.}}~\cite{carlini2023extracting} developed a tailored black-box attack for diffusion models. They generated images and implemented a membership inference attack to assess density.
Webster \MakeLowercase{\textit{et al.}}~\cite{webster2023reproducible} demonstrated a more efficient extraction attack with fewer network evaluations, identified "template verbatims," and discussed its persistence in newer systems. 
Recent research has shifted towards exploring the theoretical aspects of memory in diffusion models.
Yoon \MakeLowercase{\textit{et al.}}~\cite{yoon2023diffusion} discovered that generalization and memorization are mutually exclusive occurrences and further demonstrated that the dichotomy between memorization and generalization can be apparent at the class level.
Gu \MakeLowercase{\textit{et al.}}~\cite{gu2023memorization} extensively studied how factors like data dimension, model size, time embedding, and class conditions affect the memory capacity of the diffusion model.

\subsection{Memorization Mitigation} 
The mitigation measures have primarily been concerned with filtering inputs and deduplication. 
For example, Stable Diffusion employed well-trained detectors to identify unsuitable generated content. 
However, these temporary solutions can be easily bypassed~\cite{wen2024hard,rando2022red} and do not effectively prevent or lessen copying behavior on a broad scale. 
Kumari \MakeLowercase{\textit{et al.}}~\cite{kumari2023ablating} designed an algorithm to align the image distribution with a specific style, instance, or text prompt they aim to remove, to the distribution related to a core concept. 
This stopped the model from producing target concepts based on its text condition.
\gxlnote{Hintersdorf \MakeLowercase{\textit{et al.}}~\cite{hintersdorf2024finding} localized memorization of individual data samples down to the level of neurons in DMsâ€™ cross-attention layers.}
However, these approaches are inefficient because they necessitate a list of all concepts to be erased, and have not addressed the key issue of how to reduce the memory capacity of the model.
~\cite{dockhorn2022differentially,ghalebikesabi2023differentially} explored the use of differential privacy (DP)~\cite{dwork2006differential} to train diffusion models or fine-tune ImageNet pre-trained models. However, their focus was on ensuring the privacy of the training of diffusion models, not on the privacy of the images generated by the diffusion models. 
\gxlnote{Chen \MakeLowercase{\textit{et al.}}~\cite{chen2024towards} re-guides generation by measuring the similarity between generated and training images, aiming for memorization-free outputs. However, directly relying on the training set during testing is impractical.}
Daras \MakeLowercase{\textit{et al.}}~\cite{daras2024ambient} introduced a technique for training diffusion models utilizing tainted data. By incorporating additional corruption before applying noise, their methodology prevents the model from overfitting to the training data. But their training requires a considerable amount of time. 
~\cite{somepalli2024understanding,wen2023detecting, ren2024unveiling} also suggested a series of recommendations to mitigate copying such as randomly replacing the caption of an image with a random sequence of words, but most of which are limited to text-to-image models. Our work focuses on the nature of memorization in diffusion models, especially for unconditional ones. 
\vspace{-0.1cm}
\gxlnote{\subsection{Data Augmentation Theory and Practice}
Data augmentation is a widely used technique to improve the generalization of machine learning models, particularly in deep learning ~\cite{wang2021regularizing}. It is commonly employed to increase the diversity of training data by applying transformations in image-based tasks. Common data augmentation techniques include pixel erasing ~\cite{zhong2020random,devries2017improved,chen2020gridmask}, image cropping~\cite{chen2016automatic,ciocca2007self}, mixing images~\cite{hendrycks2019augmix,zhang2017mixup}, geometric transformations~\cite{wang2019perspective,jaderberg2015spatial}, kernel filter~\cite{kang2017patchshuffle}, \MakeLowercase{\textit{etc}}.
The use of data augmentation has been widely explored for vision tasks that require extensive annotation. Azizi \MakeLowercase{\textit{et al.}}~\cite{azizi2023synthetic}showed that augmenting the ImageNet training set~\cite{russakovsky2015imagenet} with samples generated by conditional diffusion models results in a significant boost in classification accuracy. Baranchuk \MakeLowercase{\textit{et al.}}~\cite{baranchuk2021label} investigated how diffusion models can be used to augment data for semantic segmentation, leveraging intermediate activations as rich pixel-level representations, especially when labeled data is scarce. Trabucco \MakeLowercase{\textit{et al.}}~\cite{trabucco2023effective} explored methods to augment individual images with a pre-trained diffusion model, showing significant improvements in few-shot scenarios. Other examples include tasks like human motion understanding~\cite{guo2022learning, izadi2011kinectfusion}, optical flow estimation~\cite{dosovitskiy2015flownet, sun2021autoflow}, and physically realistic simulation environments~\cite{de2022next,dosovitskiy2017carla,gan2020threedworld}, etc. Our study uses data augmentation to flexibly enhance model generalization, thereby mitigating memorization.}