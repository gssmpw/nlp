\section{Methodology}
This section outlines the core components of our framework, LAMD, and how they cooperate to detect and understand Android malware efficiently.

\subsection{Overall Architecture}
The primary objective of LAMD is to extract essential functionalities and their contextual information, which are then fed into LLMs to generate both detection and reasoning results. The framework consists of two key components: (1) Key Context Extraction and (2) Tier-Wise Code Reasoning, detailed as follows:
\begin{itemize}
    \item \textbf{Key Context Extraction}: This component first identifies suspicious APIs, using them as seed points for context extraction. It then analyzes the control and data dependencies associated with these APIs within the target application. By leveraging this information, we construct a set of call relationships that encapsulate potentially malicious contexts, providing a structured representation of critical interactions.
    \item \textbf{Tier-wise Code Reasoning}: To maintain contextual relationships while managing context length, we adopt a tier-wise reasoning approach for malware detection. Three tiers correspond to functions, APIs and the APK respectively. The output of each tier is served as the input to the next one. Factual consistency is verified after each first-tier analysis to mitigate error propagation from the source.
\end{itemize}
Figure shows the pipeline of our framework. Overall, the raw input to LAMD is the application files, after which the suspicious APIs and their sliced contexts are extracted as the input to LLMs. Then the malicious behavior of the application is determined by two tiers of code reasoning.

\subsection{Key Context Extraction}
\subsubsection{Suspicious API Collection}
Malware exploits system vulnerabilities or API permissions to steal data, manipulate resources, or maintain persistence. Many attacks rely on API calls indicative of malicious behavior. We perform static analysis on APKs to extract suspicious API calls as key context to identify malware. Let $\mathcal{A} = \{a_1, a_2, \ldots, a_n\}$ be the set of all API calls in an APK. A subset $\mathcal{A}_{sus} \subset \mathcal{A}$ is deemed suspicious if it interacts with sensitive components, executes malware-associated operations or exposes sensitive data. These APIs fall into two categories:
\begin{itemize}
    \item \textit{Sensitive data access APIs}: While many apps handle sensitive data, assessing developer trustworthiness is challenging. Smartphone OSs implement permission-based access control, requiring declared permissions in the APK manifest for APIs enforcing permission checks. Additionally, Some APIs, like \verb|getPrimaryClip()| (clipboard access), bypass permission enforcement. We consider these two scenarios and classify an API $a_i$ is in this category because (1) requires permissions for access control or (2) directly provides access to sensitive user data without requiring permissions.
    \item \textit{Sensitive data transmission APIs}: It is also crucial to monitor potential channels through which sensitive information may leak to an adversary. Malicious applications often exploit these APIs to exfiltrate data to external services, commonly referred to as sink APIs. an API call $a_j$ is classified as sensitive if it attends to transfer sensitive data to external environments.
\end{itemize}

To extract suspicious APIs, we leverage publicly available knowledge based on PScout~\cite{pscout}, SuSi~\cite{susi} and Flowdroid~\cite{flowdroid} to label them. In the real-world setting, not every application contains suspicious APIs, these samples should be treated individually, which is acceptable, as traditional learning-based malware detectors are also hard to handle.


\subsubsection{Backward Program Pruning}
While extracting suspicious APIs is helpful, analyzing them in isolation often fails to reveal their contextual behavior. For example, a messaging app legitimately uses \verb|sendTextMessage()|, while malware may exploit it to send premium-rate SMS without user consent. To capture behavioral intent, we extract functions invoking suspicious APIs and filter out unrelated parts. Specifically, for each API $a_i \in \mathcal{A}_{sus}$, we locate the calling function $f_i$, and construct its control flow graph(CFG), $G = <N, E>$, to prepare for analysis dependency relationships, where each node $n \in N$ is either a basic block or a single instruction, and edges $(e_1, e_2) \in E$ define control flow. CFGs can be large and may include irrelevant instructions. To refine them, we apply backward slicing, isolating only instructions affecting the suspicious API invocation. A slice $S$ is defined by a slicing criterion $C = <s, V>$ where $s$ is the statement invoking $a_i$ and $V$ includes all parameters. We classify relevant variables as: (1) \textit{Direct relevant variables}: Variables' values can affect variable $v \in V$ of $a_i$  (2) \textit{Indirect relevant variables}: Variables in branch statements whose value affects invocation of $a_i$. The backward slicing is to select the set of instructions in $\mathcal{P}$ that directly or indirectly affect the execution or parameters of $a_i$. The backward slicing algorithm consists of two steps to ensure completeness in complex branch structures:
\begin{itemize}
    \item \textbf{Variable retrieval}: Identify all variables contributing to the parameters (and internal states) used by the suspicious API and store them in a candidate set.
    \item \textbf{Slices extraction}: Append instructions related to variables collected in the first step.
\end{itemize}

After slicing, we generate sliced CFGs for each sensitive API, preserving essential control flow and statements. Notably, If undeclared variables remain in a sliced function, inter-procedural backward slicing is recursively applied to its callers until all variables are resolved. The details of slicing algorithm are shown in Appendix~\ref{slicing}

\subsection{Tier-wise Code Reasoning}
Having identified the suspicious APIs and their surrounding instructions, we adopt a two-stage summarization strategy to (a) mitigate the token-length constraints in Large Language Models (LLMs) and (b) analyze how APIs interrelate in producing potential malicious behavior. The detailed prompts used in this stage are shown in Appendix~\ref{prompt}

\subsubsection{Function Behavior Summarization}
Although we can simply feed each suspicious API to an LLM for summarization, malicious behaviors often emerge when multiple APIs work in combination. To reflect these interdependencies, we group suspicious APIs following their associations.

We define an API group $\mathcal{G}_k$ as a set of APIs that share at least one calling function or exhibit overlapping usage of variables. Formally, if $f(a_i)$ denotes the set of function where API $a_i$ is invoked, then $\mathcal{G}_k=\left\{a_i \in \mathcal{A}_{\text {sus }} \mid \exists a_j \in \mathcal{G}_k \text { such that } f\left(a_i\right) \cap f\left(a_j\right) \neq \emptyset\right\}$. Each group $\mathcal{G}_k$ therefore captures a cluster of suspicious APIs that might collectively escalate malicious actions. 

After grouping, each $\mathcal{G}_k$ and its associated sliced CFGs (from the backward slicing results) are fed into the LLM, prompting it to produce a high-level behavioral description. This helps LLM recognize the combined functionality to reveal the different semantics generated by the API's joint calls in the current application context. An intuitive example is that if the API \verb|getDeviceId()| and \verb|sendTextMessage()| are grouped, the app may be exfiltrating sensitive information via SMS. 

\subsubsection{Factual Consistency Checking}
Generating code or behavior summaries using LLMs carries the risk of hallucinations, where the model invents facts inconsistent with the actual instructions. We perform a consistency check on each function behaviour summary to mitigate error accumulation before moving to final APK-level detection.

Inspired by factual consistency verification~\cite{cloze, factasking}, we design a structured template to capture data dependencies in sliced CFGs. To enhance inference, we omit certain dependency details from textual descriptions and prompt the LLM to infer missing relationships. We define seven data dependencies in two categories: variable-to-API interactions (direct, transitive, conditional, and loop dependencies) and inter-variable relationships (parallel, derived, and control dependencies). The first category captures how variables affect API execution through assignment, call chains, control flow, or iteration. The second describes variable interactions, including joint computation, derivation, and control influence. Appendix~\ref{datadependency} shows details about these data dependencies and their descriptions.

To evaluate the reliability of generated summaries, we propose a Data Relation Coverage (DRC) metric:
\begin{equation}
DRC = \frac{\#\{\text{correctly completed dependencies}\}}{\#\{\text{all selected dependencies}\}}.
\end{equation}
If the LLM can accurately restore or infer a sufficient fraction of these omitted dependencies (i.e., $DRC \ge \theta$, where $\theta$ is a reliability threshold), we deem the function behavior summaries to be factually consistent.   Otherwise, it is revised or discarded to minimize inaccuracies.

\subsubsection{Application Maliciousness Reasoning}
In this stage, we collect all verified summaries from each API group $\mathcal{G}_k$ and feed them into the LLM again to produce a final holistic analysis. This process enables APK-level classification, where the LLM synthesizes group-level insights to determine whether the APK is benign or malicious, considering interactions between API groups that may amplify suspicious behavior. Additionally, the LLM generates Indicators of Compromise (IoCs), providing a concise explanation that highlights accessed sensitive data, external transmissions, and unusual control flow or permission requests, improving transparency and fostering user trust. 




