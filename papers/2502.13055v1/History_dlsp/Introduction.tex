\section{Introduction}
The rapid growth of Android applications has significantly increased the security risks posed by malware, which threatens user privacy, financial security, and sensitive data. Over the past decade, researchers have developed various Android malware detection techniques, yet these traditional methods continue to face fundamental challenges in real-world settings. The open and dynamic nature of the Android ecosystem makes detecting evolving malware difficult~\cite{transcend, transcending}. Moreover, the overreliance on specific datasets introduces biases in training, such as ambiguously timestamped data and randomly selected samples~\cite{tesseract}, which can degrade model reliability. Another major limitation of conventional detectors is their lack of explainabilityâ€”most tools provide only feature importance scores rather than offering a clear, human-readable interpretation of malicious behaviors.

Large Language Models (LLMs) offer a promising paradigm shift in malware detection, differing fundamentally from conventional detectors. They realize zero-shot inference relying on vast pre-trained knowledge instead of specifically labelled datasets~\cite{large_zero,toolformer}, allowing them to handle the evolving malware and potential training bias. Furthermore, existing malware detection tools that incorporate explainability techniques for deep neural networks typically focus on feature-level importance analysis instead of offering meaningful interpretations of malicious behaviors and Indicators of Compromise (IoCs). The advanced generative capabilities of LLMs present an opportunity to bridge this gap by providing human-readable comprehension, thereby enhancing malware analysis from both an accuracy and interpretability perspective.


However, despite their potential, LLMs are not omnipotent, particularly when applied to Android malware, which is characterized by complex function invocations, multi-component structures, and obfuscation techniques. Three key challenges hinder the effectiveness of LLMs: (1) \textit{\textbf{Limited LLM context window}}: Android malware usually contains thousands of classes to realize different functionalities. Directly feeding their decompiled code is infeasible, because none of the current LLMs can handle such large context window demands. Simply truncating the input leads to the loss of context information. (2) \textit{\textbf{Complex program structures}}: Unlike natural language, code follows a non-sequential structure with deeply nested dependencies across classes and intricate API interactions, especially in Android applications. This structural complexity makes it challenging for LLMs, which primarily rely on token sequences, to understand malicious behavior accurately. (3) \textit{\textbf{Sparse malicious behaviors}}: While Android malware executes unauthorized actions, it must also maintain benign functionalities to operate seamlessly across different devices. It submerges malicious patterns in normal calls which is hard to detect. At its core, these challenges highlight a fundamental requirement: 
\begin{center}
\fcolorbox{white}{gray!10}{\parbox{.9\linewidth}{\textit{Can we extract crucial structural and semantic information from complete application to guide LLMs in detecting Android malware?}}}
\end{center}
The process by which human analysts analyze malicious software in the real world follows this requirement. They identify suspicious APIs, interfaces, and function calls, analyzing their contextual relationships to detect malicious behavior within extensive application code. Our framework aims to guide LLMs in replicating this analytical process, enhancing automated Android malware detection and reasoning.

To tackle these difficulties and fulfil the demands, we propose LAMD, a novel and practical framework that enables LLMs to detect and reason about Android malware. LAMD consists of two core components: key context extraction and tier-wise code reasoning. Specifically, we analyse APK files statically and use our designed backward slicing algorithm to extract suspicious variables, instructions, and invocations indicating potentially malicious context. This process ensures that only relevant program structures are fed into the LLM, minimizing interference from unrelated functionalities. Additionally, we introduce a tier-wise reasoning mechanism coupled with factual consistency verification to ensure the reliability of summary transfer between tiers. This work lays the foundation for future research on exploring the reasoning capabilities of LLMs for detecting complex malware like Android malware in dynamic threat landscapes. The main contributions of this paper are as follows:

\begin{itemize}
    \item We propose the first LLM-powered practical Android malware detection framework, LAMD, which unlocks LLMs' ability for complex malware detection and reasoning in dynamic scenarios, providing heuristics for LLM-enhanced malware-related tasks.
    \item We enhance LAMD with a key context extractor and tier-wise code reasoning to filter out unrelated functionalities and capture semantics across API-to-function and function-to-function relationships. A targeted factual consistency verification strategy ensures intermediate correctness from the start of the tier-wise reasoning process.
    \item We evaluate LAMD on a constructed dataset\footnote{The dataset and code repository will be open-source for further research on LLM-based malware tasks.} that reflects the real-world setting and confirms the efficiency of LAMD in detecting and reasoning Android malware compared with commonly used detectors. 
\end{itemize}



% extracting essential structural and semantic information from APKs to effectively guide LLMs in identifying potentially malicious behaviors.




% Furthermore, the advanced generation and comprehension capabilities of LLM provide an opportunity to bridge the gap between feature-level explanations for malware detection and human-readable understanding.

% Furthermore, there is still a gap between feature-level explanations and human-readable comprehension for malware detection. The advanced generative capabilities of LLMs present an opportunity to offer understandable\ interpretations of malicious behaviors 
