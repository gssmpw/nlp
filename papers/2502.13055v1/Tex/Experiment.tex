\section{Evaluation}
This section presents a comprehensive evaluation of LAMD, assessing its Android malware detection performance in real-world scenarios and the quality of its generated explanations through a series of experiments. Experiment setup details are shown in Appendix~\ref{expSet}.

% \subsubsection{Human-Verified Test Set}
% To assess the quality of the generated analysis, we construct a Human-Verified Test Set by randomly selecting 100 malicious samples from the detection dataset. Each selected sample has been correctly identified through LAMD. Four cybersecurity researchers (three PhD candidates and one postdoctoral researcher) validated malicious behavior descriptions using domain expertise and LLM-assisted analysis, incorporating sample categories and family labels. To minimize bias, cross-validation was performed among the researchers. Details on dataset construction are provided in the appendix.

% Four cybersecurity researchers (3 PhD-level students and 1 postdoctoral researcher) validated malicious behavioral descriptions through iterative LLM queries, incorporating sample categories and family labels. Cross-checking is conducted among them to minimize biases. Appendix claims the details on the process of constructing dataset.


\subsection{Dataset Construction}
% \subsubsection{Detection Evaluation Test Set}
To ensure a realistic dataset, we adhere to the following principles~\cite{tesseract}: (1) Maintain temporal order in training and testing; (2) Preserve the real-world malware-to-benign ratio; (3) Ensure diversity by including packed, obfuscated, and varied market samples.

Based on these principles, we select Android APKs from Androzoo~\cite{AndroZoo} according to their discovery time, which is determined by their submission to VirusTotal~\cite{VirusTotal} (as release timestamps can be unreliable\footnote{Due to modifiable or randomly generated release times, some timestamps (dex date) are inaccurate.}). The dataset spans from 2014 to 2023, comprising 13,794 samples. %\footnote{Appendix~\ref{app:dataset} discusses the dataset selection strategy} in the training set (2014–2020)
The remaining samples are evenly split into three test sets (about 3,015 each) to represent incremental distribution drift. Details of the dataset, including its source, drift severity, and training size considerations, are provided in Appendix~\ref{app:dataset}.

\subsection{Metric}
\textbf{(1) Classification Metrics.} To address class imbalance in Android malware datasets, we use the F1-score to balance precision and recall, while also minimizing False Positive Rate (FPR) and False Negative Rate (FNR) to improve accuracy and reduce manual analysis overhead. Results are reported as percentages. \textbf{(2) Summarization Metrics.} Effective malware analysis provides interpretable insights and aids manual audits. Since malware family identification often requires expert review, initial categorization prioritizes the common sense of behavior patterns. Following prior work~\cite{malcategory}, we mainly consider six categories: Adware, Backdoor, PUA (Potentially Unwanted Applications), Riskware, Scareware, and Trojan. For evaluation, we adopt a ChatGPT-based metric~\cite{llm4codeanalysis, gptmetric1, gptmetric2}, where GPT-4o-mini~\cite{openai} assesses whether LAMD’s detection aligns with the expected behaviors of each category~\cite{humaneval}.

% \textbf{(1) Classification Metrics.}
% Given the class imbalance in the Android application dataset, we adopt the F1-score as the primary evaluation metric to balance precision and recall. In malware detection, minimizing the False Positive Rate (FPR) and False Negative Rate (FNR) is equally critical to enhance accuracy and reduce the manual analysis burden. These metrics ensure effective malware detection while maintaining practical usability. All results are reported as percentages. \textbf{(2) Summarization Metrics.} Effective application analysis describes malicious behaviors and supports manual audits. Since malware family identification often requires expert review, initial analysis prioritizes categorization and provides interpretable insights from API calls, permissions, and behavior patterns. Therefore, we classify samples into six categories based on their families and common behavior~\cite{malcategory}, containing Adware, Backdoor, PUA~(potentially unwanted applications), Riskware, Scareware, and Trojan. Following prior work~\cite{llm4codeanalysis, gptmetric1, gptmetric2}, we leverage ChatGPT-based evaluation metric. Specifically, GPT-4o-mini~\cite{openai} is asked to judge~\cite{humaneval} whether the result of the detection analysis generated by LAMD follows the behaviors of the corresponding categories. 


% To evaluate human readability, four cybersecurity researchers (3 PhD-level students and 1 postdoctoral researcher) are asked to compare the generated analysis with typical behaviors of the sample's family. 

% Therefore, we classify samples into six categories based on their families and common behavior.

% Since summarized application behaviors, IoCs, and functionalities may include suspicious API names, n-gram-based approaches are suboptimal for evaluating explanation quality in our setting. Following prior work~\cite{llm4codeanalysis, gptmetric1, gptmetric2}, we leverage ChatGPT-based evaluation metric. Specifically, GPT-4o-mini~\cite{openai} is prompted to compare detection analyses generated by LAMD against human-verified ground truth, providing a binary True/False judgment on alignment. 

% Since summarized application behaviors, IoCs, and functionalities may include suspicious API names, n-gram-based approaches are suboptimal for evaluating explanation quality. To address this, we adopt METEOR~\cite{meteor}, a semantic-oriented metric that considers word order, stemming, and synonym matching, providing a more comprehensive assessment of the generated explanations. Additionally, we assess the latent-space semantic similarity~\cite{semantic-text-similarity} using a BERT-based similarity score, providing a more robust measure of the semantic alignment between generated and reference explanations.

\input{Table/detection}
\subsection{Baseline}
In Android malware detection, an APK serves as input, containing the codebase (e.g., .dex files) and configuration files (e.g., AndroidManifest.xml), which provide behavioral insights like API calls and permissions, represented in vector or graph formats. We evaluate LAMD against Drebin~\cite{Arpdrebin}, DeepDrebin~\cite{Grossedeepdrebin}, and Malscan~\cite{malscan} which are representative learning-based methods on these feature formats, with details in Appendix~\ref{baseline}. To assess component impact, LAMD-R removes tier-wise reasoning to test structural and semantic analysis, while LAMD-F retains reasoning but omits factual consistency verification to evaluate hallucination control.

% Additionally, to assess component impact, we introduce LAMD-R and LAMD-F. LAMD-R removes tier-wise code reasoning, directly feeding extracted contexts into the LLM to evaluate structural and semantic reasoning. LAMD-F retains reasoning but omits factual consistency verification, allowing us to assess the control of hallucinations.



\subsection{Evaluation Results}
\subsubsection{Malware Detection Performance}
Table~\ref{tab:detection} compares LAMD’s detection performance with baselines. LAMD improves F1-scores by 23.12\% and reduces FNR by 71.59\% on average, enhancing detection reliability. While the FPR shows a slight increase, it is less indicative of true performance due to class imbalance, where learning-based methods often misclassify malware as benign because of the dominance of 
benign sampls. The performance drop in LAMD-R highlights the necessity of hierarchical code summarization, and the slight decline in LAMD-F underscores the role of hallucination mitigation. By refining input code and extracting key information, LAMD minimizes hallucination risks, ensuring more reliable malware detection. 

% Additionally, the False Negative Rate (FNR) is significantly reduced, as zero-shot inference mitigates the bias introduced by highly imbalanced training data, which often causes learning-based models to favor the dominant class. These results highlight LAMD’s robustness and adaptability in dynamic threat environments.


\subsubsection{Effectiveness of explanations}
To assess analysis quality, we validate 100 correctly detected malware samples. 
Results show that 81 out of 100 samples are correctly classified into their respective categories. Table~\ref{tab:explanation} summarizes the sample distribution and classification accuracy across categories. Due to their less distinct malicious patterns, Adware and Riskware pose greater challenges to accurate analysis compared to other categories, contributing to their higher misclassification rates.


\input{Table/Explanation}