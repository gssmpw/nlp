\section{Introduction}
The rapid expansion of the Android ecosystem has heightened security risks, with malware posing serious threats to user privacy, financial security, and sensitive data. Over the past decade, researchers have developed various Android malware detection techniques, yet these methods face persistent challenges in real-world scenarios. 
Firstly, the open and evolving nature of Android complicates the detection of adaptive malware~\cite{transcend, transcending}. 
Reliance on specific datasets introduces biases, such as ambiguous timestamps and randomly selected samples~\cite{tesseract}, which further compromise model reliability. 
Additionally, conventional detectors often lack explainability that fail to offer clear, human-readable insights into malicious behaviors.
% A key limitation of conventional detectors is their lack of explainability—many provide only feature importance scores without offering clear, human-readable insights into malicious behaviors.

Large Language Models (LLMs) offer a promising paradigm shift in malware detection, differing fundamentally from conventional detectors. They achieve zero-shot inference relying on vast pre-trained knowledge instead of specifically labelled datasets~\cite{large_zero,toolformer}, allowing them to handle the evolving malware and potential training bias. 
% Furthermore, existing malware detection tools that incorporate explainability techniques for deep neural networks typically focus on feature-level importance analysis instead of offering meaningful interpretations of malicious behaviors and Indicators of Compromise (IoCs).
% The advanced generative capabilities of LLMs present an opportunity to bridge this gap by providing human-readable comprehension, thereby enhancing malware analysis from both an accuracy and interpretability perspective.
Furthermore, to bridge the gap of explainability, the advanced generative capabilities of LLMs present an opportunity by providing human-readable comprehension, thereby enhancing malware analysis from both an accuracy and interpretability perspective.

% to bridge the gap of explainability, the advanced generative capabilities of LLMs present an opportunity by providing human-readable comprehension, thereby enhancing malware analysis from both an accuracy and interpretability perspective.

\begin{figure}[t]
    \centering
    % \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=0.85\linewidth]{Figure/Motivation.pdf}
    \caption{Two failure cases of applying LLMs to Android malware detection: (1) context window limitations and (2) failure to capture malicious intent. LAMD addresses these challenges by extracting key contexts and tiered reasoning to capture structures and semantics efficiently.}
    \label{fig:motivation}
\end{figure}

% \footnotetext{MD5 for case 1: c37e223e3388b31b323ad39af45180fc; MD5 for case 2: 2be97287c6af70f2074686b1a9021c06}

However, despite the potential, LLMs are not omnipotent. Two primary challenges hinder their effectiveness in Android malware detection: \textit{\textbf{(1) Excessive support codes in Android applications}}: Android malware often comprises thousands of classes to support diverse functionalities across various devices. 
While some LLMs support up to 2 million tokens~\cite{gemini1.5}, directly processing decompiled malware remains impractical, as some samples would still exceed this limit.
Truncation offers a potential alternative, but it can cause substantial contextual loss, ultimately degrading analysis accuracy~\cite{truncate}.
% Additionally, support for normal functionality and unauthorized actions leads to malicious code that is sparse in the program and hard to detect. 
More importantly, these normal functional codes make malicious codes sparse within the program and become extremely hard to detect. 
\textit{\textbf{(2) Complex program structures}}: Although code exhibits structural characteristics akin to natural language~\cite{naturalness}, it remains fundamentally distinct due to inherent structural complexity~\cite{codeandnatural}. 
% with repetitive and predictable patterns that can be modeled using statistical language models. However, it remains fundamentally distinct due to its inherent complexity~\cite{codeandnatural}. 
For instance, deeply nested dependencies, intricate API interactions, and class hierarchies all extend beyond sequential token-based modeling. 
The complexity is further amplified in Android malware, where obfuscation techniques, multi-component interactions, and convoluted function invocations obscure malicious intent. 
At its core, these challenges raise a fundamental research question: 
\begin{center}
\fcolorbox{white}{gray!10}{\parbox{.9\linewidth}{\textit{Can we extract crucial semantic and structural information from complete application to guide LLMs in detecting Android malware?}}}
\end{center}
% The process by which human analysts analyze Android malware in the real world follows this requirement. % The approach used by human analysts to examine Android malware in the real world aligns with this principle.

To address this, we draw inspiration from how analysts examine Android malware in the real world, where they identify suspicious APIs, interfaces, and function calls, analyzing contextual relationships to detect malicious behavior within extensive application code. 
Our framework aims to guide LLMs in replicating this analytical process, enhancing automated explainable Android malware detection. 
% Figure~\ref{fig:motivation} shows motivation samples about challenges and our solution.

% To tackle these difficulties and fulfil the demands, 
We propose LAMD, a novel and practical framework that enables LLMs for explainable Android malware detection. LAMD consists of two core components: key context extraction and tier-wise code reasoning. Specifically, we perform static analysis on APKs and employ a custom backward slicing algorithm to extract key variables, dependencies, and invocations for predefined suspicious APIs. These elements are transformed into graphical representations, filtering irrelevant code and preserving essential semantics for the LLM. 
Furthermore, we introduce tier-wise code reasoning combined with factual consistency verification, transferring structured knowledge across multiple tiers to understand, draw conclusions and explain detection results. 
This approach refines the LLM’s understanding of applications, progressing from fine-grained analysis to higher-level abstraction. 
% This work lays the foundation for future research on exploring the reasoning capabilities of LLMs for detecting complex malware like Android malware in dynamic threat landscapes. 
As shown in Figure~\ref{fig:motivation}, LAMD effectively addresses real-world challenges, enabling practical and explainable LLM-powered Android malware detection.
The main contributions of this paper are as follows:

\begin{itemize}
    \item We introduce LAMD, the first LLM-powered practical Android malware detection framework, unlocking LLMs' ability for explainable Android malware detection in dynamic scenarios, providing heuristics for LLM-powered malware-related tasks.
    \item LAMD integrates key context extraction and tier-wise code reasoning to filter irrelevant functionalities while capturing semantics and structural dependencies. A targeted factual consistency verification strategy is also established to ensure accurate reasoning. %ensures accuracy throughout the reasoning process.
    \item We evaluate LAMD on a collected dataset\footnote{%The dataset and code repository will be open-source for further research on LLM-based malware tasks.
    The dataset is open-source for further research on LLM-based malware tasks: https://doi.org/10.5281/zenodo.14884736
    } 
    that reflects the real-world setting, demonstrating its effectiveness in detecting and explaining Android malware, outperforming conventional detectors.
\end{itemize}

