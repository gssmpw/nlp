\section{Dataset Description}
\label{app:dataset}
\subsection{Dataset Source}
\noindent We construct a dataset spanning 2014 to 2023\footnote{as APK labels generally stabilize after about one year in the wild~\cite{labelstability}, we choose samples before 2024} using samples from the Androzoo~\cite{AndroZoo} platform, a comprehensive repository aggregating samples from sources such as Google Play, PlayDrone, VirusShare, and AppChina. Each sample's timestamp corresponds to its submission date on VirusTotal~\cite{VirusTotal}, representing the time it was discovered. The dataset includes a decade of benign and malicious samples, labeled based on VirusTotal analysis from Androzoo. Samples flagged by more than four vendors are classified as malicious. To track malware family evolution, we used Euphony~\cite{euphony} to extract family labels. Table shows details about the used dataset.

\input{Table/Dataset}

\input{Table/DataSelection}

\subsection{Dataset Drift and Training Size}
In real-world scenarios, a large number of Android applications are packed and obfuscated to bypass malware detection systems and complicate manual analysis. A notable shift occurred post-2020, as more Androzoo samples adopted these techniques. Consequently, our test set, drawn from 2020, reflects a more realistic setting. 
% Notably, the Jensen–Shannon divergence~\cite{menendez1997jensen} between the training and test sets is five times larger than that between the training and validation sets.
Notably, the Jensen–Shannon~(JS) divergence~\cite{menendez1997jensen}~(measuring the similarity between two probability distributions) between the training and test sets is five times larger than that between the training and validation sets.

Intuitively, increased training samples should enhance detection performance in learning-based methods. While LAMD excels in zero-shot learning, one may argue that traditional methods are hindered by smaller training datasets compared to the vast pretraining data leveraged by LLMs. However, the impact of significant drift warrants reconsideration. Table~\ref{tab:dataselection} highlights the performance of learning-based detectors (e.g., Drebin feature space) across varying dataset sizes, across the training dataset timeframe.
The results indicate that additional training data does not improve detection of samples with significant drift, though it enhances performance on in-distribution samples. 

% Expanding the training set introduces more information, aiding detectors in learning clearer decision boundaries, consistent with the principle of empirical risk minimization. However, significant drift emphasizes the training set, where older samples, differing greatly from recent ones, may lead models to over-rely on outdated features, thus impairing generalizability on drifted datasets.
Expanding the training set introduces more information, helping detectors learn clearer decision boundaries, consistent with the principle of empirical risk minimization (ERM).
However, ERM identifies features that distinguish samples across the entire dataset, and when the training set is heavily skewed toward older data (e.g., 2014-2019/3), the model is more likely to learn features representative of early apps rather than more recent ones. Consequently, when tested on subsequent months (e.g., Test 1), the model exhibits worse performance, as it relies on features that are less effective for newer, drifted samples. This suggests that while increasing training data is generally beneficial, it may reinforce outdated patterns in the presence of significant drift, ultimately impairing generalizability.

