\section{Motivation}
\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1.0\linewidth]{Figure/motivation_sample.pdf}
    \caption{(a), (b), and (c) show real code snippets from an early Airpush version, a later Airpush version, and the Hiddad adware family. Airpush's core behavior includes: (1) get ad data from a specific URL, (2) asynchronous execution to avoid user interruptio, and (3) push ads continuously through the background service. (a) and (b) demonstrate that both Airpush versions share invariant behaviors, with similar API calls and permissions despite implementation differences. Hiddad, while skipping step (2) for simpler ad display, shares steps (1) and (3) with Airpush, especially the newer version.}
    \label{fig:motivation_sample}
\end{figure*}

\subsection{Invariance in Malware Evolution}
% Malware families commonly evolve to circumvent new detection techniques and security measures, resulting in constant changes in their code implementations or API calls. These changes cause the feature space, extracted from applications, to gradually deviate from the initial decision boundaries of malware detectors, thereby significantly degrading detection performance. However, we have identified that during the evolution of malware, core malicious behaviors and execution logic exhibit a certain degree of invariance. This invariance is reflected in the training set through specific intents, permissions, and function calls, which are captured by feature extraction techniques. We categorize this invariance into two types: intra-family invariance and inter-family invariance. 
Malware families commonly evolve to circumvent new detection techniques and security measures, resulting in constant changes in their code implementations or API calls to bypass detection. This leads to drifts in the feature space and a decline in detection accuracy. Yet, we argue that during the evolution of malware, core malicious behaviors and execution patterns remain partially invariant, captured in training data through intents, permissions, and function calls. We define these invariances as intra- and inter-family invariance.
\begin{itemize}
    \item Intra-family invariance: While versions within a malware family may vary in implementation, their core malicious intent remains relatively stable.
    \item Inter-family invariance: Certain malicious behavior patterns are consistent across different malware families. As malware trends shift, even new families emerging after detector training may share malicious intents with families in the training set.
\end{itemize}
To illustrate invariant malicious behaviors in drift scenarios, we select APKs from Androzoo\footnote{https://androzoo.uni.lu} and decompile them using JADX\footnote{https://github.com/skylot/jadx} to obtain .java files. Our analysis focuses on core malicious behaviors in the source code. For intra-family invariance, we use versions of the Airpush family, known for intrusive ad delivery, from different periods. For inter-family invariance, we examine the Hiddad family, which shares aggressive ad delivery and tracking tactics but uses broader permissions, increasing privacy risks. Figure.\ref{fig:motivation_sample} shows code snippets with colored boxes highlighting invariant behaviors across samples. While Airpush uses asynchronous task requests, Hiddad relies on background services and scheduled tasks to evade detection.

% To highlight the invariance of malicious behaviors in drift scenarios, we selected real APK files from Androzoo~\footnote{https://androzoo.uni.lu} platform and decompiled them using the jadx tool~\footnote{https://github.com/skylot/jadx} to obtain their corresponding .java files. The invariance analysis is conducted on the core malicious behaviors represented in the source code. For intra-family invariance, we used the long-standing Airpush malware family as an example, selecting versions from different periods, which is known for its intrusive ad delivery. For inter-family invariance, we chose the Hiddad adware family, which emerged later. Airpush and Hiddad rely on aggressive ad delivery and user tracking, but Hiddad uses broader permissions, violating more privacy. Figure.\ref{fig:motivation_sample} presents code snippets from these malware families, with colored boxes highlighting the invariant malicious behaviors shared across different family samples. Since Hiddad prioritizes ad delivery via background services and scheduled tasks to avoid suspicion and detection, it omits the asynchronous task requests used in step two by Airpush.

Figure~\ref{fig:motivation_sample}(a)\footnote{MD5: 17950748f9d37bed2f660daa7a6e7439} and (b)\footnote{MD5: ccc833ad11c7c648d1ba4538fe5c0445} show core code from this family in 2014 and later years, respectively. The 2014 version uses \verb|NotifyService| and \verb|TimerTask| to notify users every 24 hours, maintaining ad exposure. The later version, adapting to Android 8.0’s restrictions, triggers \verb|NotifyService| via \verb|BroadcastReceiver| with \verb|WAKE_LOCK| to sustain background activity. In Drebin’s~\cite{Arpdrebin} feature space, these invariant behaviors are captured through features like \verb|android_app_NotificationManager;notify|, \verb|permission_READ_PHONE_STATE| and so on. Both implementations also use \verb|HttpURLConnection| for remote communication, asynchronously downloading ads and tracking user activity, and sharing Drebin features such as \verb|java/net/HttpURLConnection| and \verb|android_permission_INTERNET|.

Similarly, Figure.~\ref{fig:motivation_sample}(c)\footnote{MD5: 84573e568185e25c1916f8fc575a5222} shows a real sample from the Hiddad family, which uses HTTP connections for ad delivery, along with \verb|AnalyticsServer| and \verb|WAKE_LOCK| for continuous background services. Permissions like \verb|android_permission_WAKE_LOCK| and API calls such as \verb|getSystemService| reflect shared, cross-family invariant behaviors, whose learning would enhance model detection across variants.

Capturing the core malicious behaviors of Airpush aids in detecting both new Airpush variants and the Hiddad family, as they share similar malicious intents. These stable behaviors form consistent indicators in the feature space. However, detectors with high validation performance often fail to adapt to such variants, underscoring the need to investigate root causes and develop a drift-robust malware detector.

% Based on this analysis, learning features that represent the core malicious behaviors of the Airpush family not only aids in detecting new Airpush variants but also helps in identifying the Hiddad family. However, malware detectors trained on historical data often fail to effectively detect these malicious samples. Even if the model achieves near-perfect performance on the validation set, its performance deteriorates significantly over time, prompting us to further investigate the root causes and build a drift-robust malware detector.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: The feature space of training samples contains invariance within and among malware families to be learned.}}}
\end{center}

% The analysis suggests that learning invariant features from the Airpush family not only aids in detecting newer versions of Airpush but also improves the detection of Hiddad. However, in practice, detectors trained on these features fail to effectively identify such samples, with performance degrading over time despite near-perfect validation set results. This motivates further our exploration of an ideal malware detector that can learn these invariant behaviours and remain robust in its discrimination throughout malware evolution.

% We present several representative pseudo-code implementations of malicious software to demonstrate invariance visually. For intra-family invariance, we selected Rootkit, a common malware family present in both the training and testing phases. This family exploits system vulnerabilities to obtain root privileges, enabling high-privilege operations such as modifying system files or the kernel, and intercepting system calls to mask its behavior, showcasing a deep confrontation with the operating system’s security mechanisms. Rootkit execution can be simplified into three main steps: file hiding, root privilege acquisition, and system call hooking. With the introduction of stricter permission management and security measures in Android 6.0, this family had to rely on more complex kernel-level attacks to maintain stealth. Figure X illustrates two simplified executions of Rootkit before and after this update, with (a) representing the earlier version. While (b) introduces more complex system calls like openat and fork for file and process hiding, it retains the core semantics from (a). Both versions use the \textit{interceptSystemCall()} function to intercept system calls. The earlier version intercepted \textit{readdir} to hide malicious files, whereas later versions achieved more sophisticated file hiding by intercepting \textit{openat}. Additionally, both versions implement system-level privilege escalation. The only difference in the new version is that it intercepts the fork system call, returning an error code to hide the malicious process \textit{com.malicious.app}. 

% Privilege escalation and process hiding are also commonly used by other malware families. Figure (c) shows a pseudo-code from the Spyware family, which specializes in stealing sensitive user information while maintaining stealth. Thus, in terms of execution logic, Spyware shares root privilege escalation and process hiding with Rootkit, but additionally calls \textit{TelephonyManager} to collect SIM card serial numbers and send them to a remote server. This is understandable, as despite the diversity of malware families, core malicious behaviors can be categorized into a limited number of types\cite{malradar}. Therefore, we conclude that stable patterns indicative of malicious behavior exist in the training samples. While new functionalities will inevitably emerge, once these patterns are learned, the malware detector will exhibit some robustness against drift.








% \subsection{The Contribution of Features to Detectors}
% \subsection{Create Ideal Drift-robust Malware Detector}
\subsection{Failure of Learning Invariance}
% \subsubsection{Vanilla Malware Detector}
Let $f_r \in \mathcal{R}$ be a sample in the data space with label $y \in \mathcal{Y} = {0, 1}$, where 0 represents benign software and 1 represents malware. The input feature vector $x \in \mathcal{X}$ includes features $\mathcal{F}$ extracted from $f_r$ according to predefined rules. The goal of learning-based malware detection is to train a model $\mathcal{M}$ based on $\mathcal{F}$, mapping these features into a latent space $\mathcal{H}$ and passing them to a classifier for prediction. The process is formally described as follows:

\begin{equation}
\arg \min _{\theta} R_{erm}\left(\mathcal{F}\right)
\end{equation}
where $\theta$ is the model parameter to be optimized and $R_{erm}(\mathcal{F})$ represents the expected loss based on features space $\mathcal{F}$, defined as:
\begin{equation}
R_{erm}\left(\mathcal{F}\right)=\mathbb{E}[\ell(\hat{y}, y)].
\end{equation}
$\ell$ is a loss function. By minimizing the loss function, $\mathcal{M}$ achieves the lowest overall malware detection error. 

% Let $r \in \mathcal{R}$ represent a sample in the original data space, where the corresponding label is denoted as $y \in \mathcal{Y} = \{0, 1\}$, with 0 indicating benign software and 1 indicating malware. The input feature vector $x \in \mathcal{X}$ comprises features $\mathcal{F}$ extracted from $r$ according to specific rules. The objective of learning-based malware detection schemes is to learn a model $\mathcal{M}$ based on the feature set $\mathcal{F}$, which maps the features into a latent space $\mathcal{H}$ and feeds them into a classifier to generate predictions. The process is formally described as follows:

% However, as malware evolves, the model's performance gradually degrades. This motivates us to explore an ideal malware detector that can consistently maintain strong discriminative power throughout malware evolution.

% To explore the robustness of various features in response to malware evolution and their contribution to the performance of the detector, we define two key properties of the features used for training: stability and discriminability. Therefore, the aforementioned objective encourages the model to learn discriminative features that can minimize the loss function. However, as we know, this objective may fail during the evolution of malware. 

% This optimization objective encourages the model to learn discriminative features that minimize the loss function. However, the aforementioned objective function may fail during the evolution of malware. Therefore, to explore the robustness of various features in response to malware evolution and their contribution to the performance of the detector, we define two key properties of the features used for training: stability and discriminability. Intuitively, an ideal drift-robust malware detector would be designed to learn features that are both stable and discriminative. 


\subsubsection{Stability and Discriminability of Features}
\label{active ratio}
To investigate the drift robustness in malware evolution from the feature perspective, we introduce two key properties of features: stability and discriminability. Stability refers to a feature's ability to maintain consistent relevance across different distributions, while discriminability reflects a feature's capacity to distinguish different categories effectively. Typically, feature analysis relies on model performance and architecture, which may introduce bias in defining these feature properties. Therefore, we propose a modelless formal definition, making it applicable across various model architectures. 

Let $f_j$ represent the $j$-th feature in the feature set $\mathcal{F}$, and $S$ denote the set of all samples. To capture the behavior of feature $f_j$ under different conditions, we compute its active ratio over a subset $S^{\prime} \subseteq S$, representing how frequently or to what extent the feature is ``active'' within that subset. Specifically, for a binary feature space, feature $f_j$ takes values 0 or 1 (indicating the absence or presence of the feature, respectively), the active ratio of $f_j$ in the subset $S^{\prime}$ is defined as the proportion of samples where $f_j$ is present, which is defined as Eq.~\ref{active ratio}:
\begin{equation}
\label{active ratio}
r\left(f_j, S^{\prime}\right)=\frac{1}{\left|S^{\prime}\right|} \sum_{s \in S^{\prime}} f_j(s) 
\end{equation}
The ratio measures how frequently the feature is activated within the subset $S^{\prime}$ relative to the total number of samples in the subset. At this point, we can define the stability and discriminability of features.

\begin{myDef} 
\textbf{Stable Feature}: A feature $f_j$ is defined as stable if, for any sufficiently large subset of samples $S^{\prime} \subseteq S$, the active ratio $r\left(f_j, S^{\prime}\right)$ remains within an $\epsilon$-bound of the overall active ratio $r\left(f_j, S\right)$ across the entire sample set, regardless of variations in sample size or composition. Formally, $f_j$ is stable if:
\begin{equation}
\forall S^{\prime} \subseteq S,\left|S^{\prime}\right| \geq n_0, \quad\left|r\left(f_j, S^{\prime}\right)-r\left(f_j, S\right)\right| \leq \epsilon    
\end{equation}
where $\epsilon>0$ is a small constant, and $n_0$ represents a minimum threshold for the size of $S^{\prime}$ to ensure the stability condition holds.
\end{myDef}

When we consider discriminability, there is a need to focus on the category to which the sample belongs. Thus, let $C=\left\{C_1, C_2, \ldots, C_k\right\}$ be a set of $k$ classes, and $S_k \subseteq S$ be the subset of samples belonging to class $C_k$. The active ratio of feature $f_j$ in class $C_k$ is given by:
\begin{equation}
  r\left(f_j, S_k\right)=\frac{1}{\left|S_k\right|} \sum_{s \in S_k} f_j(s)  
\end{equation}

\begin{myDef}
\textbf{Discriminative Feature}: A feature $f_j$ is discriminative if its active ratio differs significantly between at least two classes, $C_p$ and $C_q$. Specifically, there exists a threshold $\delta > 0$ such that:
\begin{equation}
  \exists C_p, C_q \in C, p \neq q, \quad\left|r\left(f_j, S_p\right)-r\left(f_j, S_q\right)\right| \geq \delta  
\end{equation}
\end{myDef}
Furthermore, the discriminative nature of the feature should be independent of the relative class sizes, meaning that the difference in activation should remain consistent despite variations in the proportion of samples in different classes. Mathematically, for any subset $\tilde{S}_p \subseteq S_p$ and $\tilde{S}_q \subseteq S_q$, where $\left|\tilde{S}_p\right| \neq\left|S_p\right|$ or $\left|\tilde{S}_q\right| \neq\left|S_q\right|$, the discriminative property still holds:
\begin{equation}
    \left|r\left(f_j, \tilde{S}_p\right)-r\left(f_j, \tilde{S}_q\right)\right| \geq \delta
\end{equation}

% \begin{figure}
%     \centering
%     % \setlength{\abovecaptionskip}{0.5cm}
%     \includegraphics[width=\linewidth]{Figure/feature_diff_10.pdf}
%     \caption{Discriminative change of Top 10 discriminative features in the training set during the test phase}
%     \label{fig:f1_family}
% \end{figure}
\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}  
        \centering
        \includegraphics[width=1.0\textwidth]{Figure/feature_diff_10.pdf}
        \caption{}
        \label{fig:diff}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{Figure/feature_importance_10.pdf}
        \caption{}
        \label{fig:importance}
    \end{subfigure}
    
    \caption{(a) and (b) illustrate changes in the Discriminability of the top 10 discriminative training features and the top 10 important testing features, respectively. ``Discriminability'' is defined as the absolute difference in active ratios between benign and malicious samples. The grey dotted line indicates the start of the testing phase, with preceding values representing each feature's discriminability across months in the training set.}
    \label{fig:feature_discrimination}
\end{figure*}


\subsubsection{Failure Due to Learning Unstable Discriminative Features}
\label{motivation: failure}
The high test set performance of the malware detector within the same period suggests that it effectively learns discriminative features to distinguish benign software from malware. However, our analysis reveals that performance degradation over time is mainly due to the model's inability to capture stable discriminative features from the training set. To illustrate this, we sample 110,723 benign and 20,790 malware applications from the Androzoo\footnote{https://androzoo.uni.lu} platform (2014-2021). Applications are sorted by release date, with 2014 samples used for training and subsequent data divided into 30 equally spaced test intervals. We extract DREBIN~\cite{Arpdrebin} features, covering nine behavioral categories such as hardware components, permissions, and restrict API calls, and select the top 10 discriminative features based on active ratio differences to track over time.

The model configuration follows DeepDrebin~\cite{Grossedeepdrebin}, a three-layer fully connected neural network with 200 neurons per layer. We evaluate performance in each interval using macro-F1 scores. As shown in Figure~\ref{fig:feature_discrimination}, although the top 10 discriminative features maintain stable active ratios, the detector’s performance consistently declines. We further examine feature importance over time using Integrated Gradients (IG) with added binary noise, averaging results across five runs to ensure robustness, as recommended by Warnecke et al.~\cite{IG_explain}.

Figure~\ref{fig:feature_discrimination} presents the top 10 discriminative (a) and important features (b) identified by the model and their active ratio changes. While stable, highly discriminative features from the training set persist through the test phase, the ERM-based detector often relied on unstable features whose discriminative power fluctuated over time. This reliance leads to inconsistent model performance, stabilizing only when the feature discriminative power remains steady. Thus, we attribute the failure of ERM-based malware detectors in drift scenarios to their over-reliance on these unstable features and under-learning of already existing stable discriminative features, limiting its generalization to new samples.

% Figure.\ref{fig:importance} presents the top 10 important features identified by the model and their active ratio changes. The results show that while the highly discriminative features in the training set remained relatively stable during the test phase, the detector trained under empirical risk minimization (ERM) tended to rely on transient discriminative features. The discriminative power of certain features that the model focused on significantly fluctuated during the test phase, and does not seem to be significant enough in the training set. Moreover, when the discriminative power of features remained stable, the model’s performance also stabilized. Therefore, we attribute the failure of ERM-based malware detectors in drift scenarios to their over-reliance on these unstable features and under-learning of already existing stable discriminative features, limiting its generalization to new samples.

Moreover, we observe that highly discriminative features are often associated with high-permission operations and indicate potential malicious activity. For instance, features like \verb|api_calls::java/lang/Runtime;->exec| and \verb|GET_TASKS| are rarely used in legitimate applications. This aligns with malware invariance over time, where core malicious intents remain stable even as implementation details evolve.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: There are stable and highly discriminative features representing invariance in the training samples, yet current malware detectors fail to learn these features leading to decaying models' performance.}}}
\end{center}


\subsection{Create Model to Learn Invariance}
\label{learn_invariant_feature}
Our discussion highlights the importance of learning stable, discriminative features for drift-robust malware detection. ERM captures features correlated with the target variable, including both stable and unstable information~\cite{understanding}. When unstable information is highly correlated with the target, the model tends to rely on it. Thus, the key challenge is to isolate and enhance stable features, aligning with the goals of invariant learning outlined in Section~\ref{invariant_learning}.
% Our preceding discussion emphasized the importance of learning stable discriminative features for building drift-robust malware detectors. The goal of Empirical Risk Minimization (ERM) is to capture features closely related to the target variable, including both stable and unstable information, and the model is more inclined to rely on transient information when it is more relevant to the target~\cite{understanding}. The main challenge is therefore to isolate the stable component, which is consistent with the invariant learning goal described in Section~\ref{invariant_learning}. 

However, applying invariant learning methods is challenging. Its effectiveness presupposes firstly that the environment segmentation can expose the unstable information that the model needs to forget~\cite{environment_label, env_label}. In malware detection, it is uncertain which application variants will trigger distribution changes. Effective invariant learning requires the encoder to produce rich and diverse representations that provide valuable information for the invariant predictor~\cite{yang2024invariant}. Without high-quality representations, invariant learning may fail. This is also reflected in Figure~\ref{fig:feature_discrimination}, where even in the training phase, the learnt features are still deficient in discriminating between goodware and malware, and hard to fully represent the execution purpose of malware, relying instead on easily confusing features.

Thus, given arbitrary malware detectors, our intuition is to use time-aware environment segmentation to naturally expose the instability in malware distribution drift. Within each environment, ERM assumptions guide associations with the target variable, while the encoder provides both stable and unstable features for the invariant predictor. By minimizing invariant risk, unstable elements are filtered, thereby enhancing the detector's generalization capability.

% Thus, given arbitrary malware detectors, our scheme aims to further extend the learning capability of the encoder to provide more learnable information to the invariant predictor by relying only on the time-aware environment segmentation. Minimizing the invariant risk, in turn, filters out instability in the feature representation, thus further enhancing the generalization ability of the malware detector.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: Invariant learning helps to learn temporal stable features, but it is necessary to ensure that the training set can expose unstable information and the encoder can learn rich and good representations.}}}
\end{center}


