\section{Discussion}
\subsection{Influence of Malware Ground Truth}
Malware labels are determined based on the decisions of multiple security engines, which often disagree on whether a given sample is malware or not. Researchers typically establish a threshold to classify samples, balancing sensitivity and specificity. Lower thresholds tend to include more borderline or grey software that resembles benign applications, thereby blurring classification boundaries. In this paper, we select a lower threshold, vt = 4, to evaluate the performance of our approach in this challenging scenario.
% In most studies, malware labels are determined based on the decisions of multiple security engines, which often disagree on whether a given sample is malicious. Researchers typically set a threshold based on experience to determine basic facts about malware, leading to inconsistencies. A lower threshold includes more grayware, blurring decision boundaries as these samples resemble benign software. A higher threshold reduces malicious samples, potentially missing threats due to modern obfuscation techniques. In this section, we evaluate the impact of thresholds 4 and 12 on performance, using 2014 samples as the training set and the table~\ref{tab:ground_truth} shows the AUT(F1,12m) for subsequent years.


\input{Table/Regularization}
% \input{Table/Ground_truth}


\subsection{Comparison to the Regularization Method}
ERM-trained models often overfit to unstable features, limiting test set generalization~\cite{regularization}. Regularization methods, such as early stopping, $\ell$2 regularization, and dropout, help mitigate this by constraining model parameters~\cite{regularization_understanding}. While both regularization and invariant learning improve generalization, invariant learning specifically targets stable features across drift scenarios. We compare these regularization methods with our invariant learning framework during the whole test phase (Table~\ref{tab: regularization}). For reference, DeepDrebin (ERM) includes no regularization, while DeepDrebin~\cite{Grossedeepdrebin} employs dropout with a hyperparameter of 0.2. Results show that adding regularization improves performance under significant drift, with dropout performing best, while early stopping and $\ell$2 leads to poor performance as the model has not yet learnt the best feature representation. Invariant training consistently outperforms, demonstrating its ability to capture highly discriminative, stable features that sustain performance over distant distributions.

\subsection{Application Scope of Invariant Learning}
Invariant learning enhances a modelâ€™s generalization to unseen distributions, but keeping stable to all possible shifts is unrealistic. With significant distribution changes, its effectiveness diminishes, and current research indicates that addressing concept drift typically requires costly model updates, including data labeling, retraining, and redeployment. Additionally, pseudo-label noise from automated labeling can further degrade performance~\cite{labelless}. The true value of a robust malware detector lies in sustaining reliable performance under gradual drifts, extending its lifespan before retraining and reducing the cost of selective updates.
% Invariant learning improves the ability of a model to generalize to unseen distributions that may be encountered in the future. However, it is unrealistic to expect the model to adapt perfectly to all possible distributions. When significant distribution changes occur, the ability of invariant learning is weakened. Current research suggests that a complete solution to concept drift is primarily model updating, which entails significant costs, including the costs of labeling new data, retraining, and deployment. Additionally, the impact of noise from pseudo-labels used in automated labeling techniques cannot be ignored~\cite{labelless}. Therefore, the value of a robust malware detector lies in its ability to deliver reliable performance amid gentle drifts. This attribute extends the lifespan of the model before necessitating retraining and ensures that only truly significant samples are annotated and incorporated in updates, ultimately lowering the costs associated with deploying malware detectors.
