\section{Evaluation}
In this section, we comprehensively evaluate our proposed method's effectiveness in enhancing the drift robustness of Android malware detectors across different feature spaces. Additionally, we examine the specific contributions of each component within the invariant learning framework to the overall robustness improvement. Specifically, our evaluation aims to address the following research questions:
\begin{description}
    \item[RQ1.] Can our method mitigate the aging of detectors based on different feature spaces over time?
    \item[RQ2.] Can our method help the detector to be stable in different drift scenarios?
    \item[RQ3.] Does our method effectively learn the invariant features of applications?
\end{description}
To avoid randomness in the experiments, we chose 1, 42, and 2024 as random seeds to train the model and averaged the test results, respectively. All experiments were performed on RTX A6000. The dataset and code for the experiment will be made publicly available upon acceptance of the paper


\subsection{Evaluation Settings}
\subsubsection{Dataset}
To evaluate the effectiveness of our approach in the context of long-term malware evolution, we select Android APKs from the Androzoo\footnote{https://androzoo.uni.lu} platform, a comprehensive repository aggregating malicious samples from sources like Google Play, PlayDrone, VirusShare, and AppChina. The dataset spans 2014 to 2023, covering ten years of benign and malicious samples. Each sample was analyzed using VirusTotal\footnote{https://www.virustotal.com} and classified as malicious if flagged by more than four vendors. To track malware family evolution, we use Euphony~\cite{euphony} to extract family labels. The dataset includes 251,394 benign and 46,724 malicious applications across 639 families. We adhere to the methodology proposed by TESSERACT~\cite{tesseract} to eliminate potential temporal and spatial biases. The training set covers the year 2014 and the test set the remaining 9 years, from 2015 to 2021. Additionally, the training set is further split into proper training (80\%) and validation (20\%) across all the experimental settings.

%For each evaluation scenario, the samples involved in the training phase are divided into a training set and a validation set in the ratio of 80\% and 20\%.


\subsubsection{Candidate Detectors}
In Android malware detection, an APK file serves as input, containing the codebase (e.g., .dex files) and configuration files (e.g., manifest.xml), which offer behavioral insights such as API calls and permissions. These features are often represented as strings or graphs. To evaluate our invariant training framework, we selected commonly used detectors representing different feature spaces:
\begin{itemize}
    \item Drebin~\cite{Arpdrebin} is a malware detector using binary feature vectors from nine data types (e.g., hardware, API calls, permissions) for linear classification. To align with our framework's focus on neural network architectures, we include its deep learning variant, DeepDrebin~\cite{Grossedeepdrebin}, which uses the same feature space but employs a three-layer deep neural network (DNN) for feature extraction and classification.
    \item Malscan~\cite{malscan} adopts a graph-based feature space, extracting sensitive API calls from APKs and combining four centrality measures (degree, Katz, proximity, and harmonic wave centralities) as features. We concatenate these features according to the optimal method detailed in the paper for detection tasks.
    \item BERTroid~\cite{bertroid} leverages Android application permission strings, encoding them into dense representations with a BERT-based pre-trained model that captures contextual relationships among permissions. For this study, we use the pooled BERT output as the feature representation for malware detection.
    
\end{itemize}
To ensure a fair evaluation of our framework across different feature spaces, we apply two linear layers as the classifier to each of the above feature representations, with the final output layer configured for binary classification.

\subsubsection{Baseline}
To evaluate the robustness of our proposed scheme across various feature spaces, we compare it with two non-linear baseline models: (1) APIGraph~\cite{apigraph}, which enhances robustness using API-based features, and (2) Guided Retraining~\cite{guide_retraining}, which improves detector performance across multiple feature spaces. Additionally, we include T-stability, a feature stability method tailored for linear classifiers in the Drebin~\cite{Arpdrebin} feature space, to highlight the advantages of our non-linear approach. Details of each baseline are as follows:

\begin{itemize}
    \item APIGraph~\cite{apigraph}: This approach clusters APIs, exceptions, and permissions based on semantic similarities from official documentation, creating an API semantic relationship graph. Similar APIs are represented by a clustered index, forming a stable feature space. From the original method, we derive 2,000 clusters, replacing all features within the same cluster with the corresponding index. 
    \item Guided Retraining~\cite{guide_retraining}: This framework improves malware detection by re-partitioning and retraining on challenging samples. Samples are categorized using the base model’s confusion matrix (TN, TP, FN, FP), and supervised contrastive learning extracts group-specific feature representations, enabling separate processing of easy and difficult samples during inference for better accuracy.
    \item T-stability~\cite{svm_ce}: This work introduces t-stability, a metric for SVM models with the Drebin feature space, measuring feature instability via the slope of expected values over time. Then, the above unstable features are constrained to enhance robustness.
\end{itemize}
As APIGraph and T-stability are tailored to specific feature spaces—APIGraph focusing on API call methods and T-stability on manually selected Drebin features~\cite{Arpdrebin}—we use each scheme’s applicable feature space for experiments.



\subsubsection{Metrics}
In the following experiments, we employe static and dynamic evaluation metrics to compare the performance of different approaches. For static evaluation, due to class imbalance in the Android application dataset, we use the macro-F1 score to assess model performance on a fixed-time test set, as it provides a balanced measure of precision and recall.
% Additionally, since the primary goal of the detector is to identify as much malware as possible to minimize security risks, the false negative rate (FNR) is also used as a key metric, with the positive class representing malware. Ideally, a robust detector should maintain a high F1 score and a low FNR under drift, effectively reducing missed detections while keeping the false positive rate reasonable to ensure the detector's usability.

For dynamic evaluation metric, we calculate the Area Under Time (AUT), a metric proposed by TESSERACT \cite{tesseract}, which measures the area under the performance curve as it changes over time. The formula for AUT is as follows:
\begin{equation}
\operatorname{AUT}(m, N)=\frac{1}{N-1} \sum_{t=1}^{N-1}\left(\frac{m(x_t+1)+m(x_t)}{2}\right),
\end{equation}
where $ m$ denotes the performance metric. Here, we select the F1-score as the instance of $m$. $m(x_t)$ denotes the performance metric at time $t$ and $N$ is the number of time slots during the testing phase, with each slot representing a month. AUT values range from $[0, 1]$, where a classifier that performs perfectly would maintain an AUT of 1 throughout all testing time windows. 

\input{Table/RQ1}

\begin{figure}[ht]
    \centering
    % First subfigure
    \begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/Drebin_rq1.pdf}
        \caption{}
        \label{fig:sub1}
    \end{subfigure}%
    % \hspace{0.01\textwidth} % Adjust this spacing to make the figures more compact
    \hfill
    % Second subfigure
    \begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/malscan_rq1.pdf}
        \caption{}
        \label{fig:sub2}
    \end{subfigure}%
    % \hspace{0.01\textwidth} % Adjust this spacing to make the figures more compact
    \hfill
    % Third subfigure
    \begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{Figure/bertroid_rq1.pdf}
        \caption{}
        \label{fig:sub3}
    \end{subfigure}
    \caption{Monthly performance of DeepDrebin (a), Malscan (b) and BERTroid (c) feature spaces for the first test year after deployment (2015), with all models initially trained on 2014 samples. \textit{Retrained} indicates detectors updated monthly by incorporating that month's labeled test samples into the original training set.}
    \label{fig:rq1}
    \hfill
\end{figure}

\vspace{-0.5cm}

\subsection{Enhance Different Feature Space (RQ1)}
\label{rq1}
This section evaluates the temporal invariant learning framework (TIF) for mitigating detector degradation across feature spaces. The classifier is trained on 2014 samples and tested monthly from 2015 to 2023, with annual results shown in Table~\ref{tab:rq1}. To optimize short-term performance, we use monthly partitioning, with segmentation granularity discussed in Section~\ref{env seg}. Given that T-stability~\cite{svm_ce} is designed for the Drebin detector, we compare both the original Drebin~\cite{Arpdrebin} and its T-stability variant. APIGraph~\cite{apigraph} supports Drebin and DeepDrebin but is incompatible with BERTroid~\cite{bertroid} and Malscan~\cite{malscan}, while GuideRetraining~\cite{guide_retraining} applies to all detectors. We compare our proposed approach with the baseline model to obtain an absolute performance evaluation. Once deployed, frequently switching between different methods is impractical, even if the model doesn't always yield the best results. Nevertheless, our findings show that, even compared to the best yearly performance of other methods, our approach consistently outperforms them in the long term.

As drift increases over test years, AUT declines for all methods, but TIF maintains a lead, achieving up to an 8\% improvement in the first year. This aligns with real-world scenarios where detectors are retrained annually~\cite{apigraph}. Figure~\ref{fig:rq1} shows 2015 monthly performance, with the grey dashed line marking the upper performance bound. The grey dashed line represents the upper bound of performance for each month. For each month $n$, we split the samples into training and validation sets, label samples from months 1 through $n$ to simulate expert-labelled data, add these to the original training set, and retrain the detector for upper-bound performance evaluation. Without additional labels, TIF approaches this bound, demonstrating robust monthly gains. While AUT gains may diminish over time, this is manageable, as detectors are typically retrained periodically.


\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF slows down the aging of detectors in each feature space and has significant performance gains in the years just after deployment.}}}
\end{center}




% \begin{figure}
%     \centering
%     \setlength{\abovecaptionskip}{0cm}
%     \includegraphics[width=1.0\linewidth]{Figure/RQ2.pdf}
%     \caption{The variance in similarity between malware feature representations generated by different detectors in closed-world and open-world scenarios and those from the training set. \textit{Retrained} refers to detectors retrained at each time point with newly labeled samples.}
%     \label{fig:rq2}
% \end{figure}

\subsection{Robustness in Different Drift Scenarios (RQ2)}
\label{rq2}
Invariant training aims to help malware detectors learn stable representations across drift scenarios over time. To evaluate this, we separate the drift factors, i.e., the evolution of existing malware families and the emergence of new malware families, and construct two test scenarios: a closed-world scenario with only known families and an open-world scenario with novel families. We segment the test environment into 10 intervals by release date. Benign samples from matched periods are added to ensure a natural sample distribution.


We use the top-performing DeepDrebin detector to assess representation stability. For each test interval, we compute the cosine similarity between malware feature representations in each test interval and the training set. Figure~\ref{fig:rq2} shows the variance in similarity across intervals. We retrain the detector in each interval with labeled samples to establish baseline representations for comparison. Results indicate that the invariant training framework consistently yields more stable feature representations in closed and open-world scenarios, showing robustness to drift.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF generates stable feature representations for both evolved variants of existing families and new malware families.}}}
\end{center}





% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{Figure/RQ1.pdf}
%     \caption{Monthly performance of DeepDrebin (a), Malscan (b) and BERTroid (c) feature spaces for the first test year after deployment (2015), with all models initially trained on 2014 samples. \textit{Retrained} indicates detectors updated monthly by incorporating that month's labeled test samples into the original training set.}
%     \label{fig:rq1}
% \end{figure}


% To answer this question, we decouple the factors that lead to distributional drift in our test set based on the source of the drift, i.e., the evolution of existing malware families and the emergence of new families, and construct two targeted test environments, i.e., closed-world and open-world based drift scenarios. For the former, the test set contains only malicious families already present in the training set, and for the latter, only unseen families. Additionally, given the inherent trade-off between accuracy and robustness to drift~\cite{tradeoff}, we must ensure that improving robustness does not negatively impact the model's performance on non-drifted samples. Therefore, we also evaluated performance on a validation set that follows the same i.i.d. distribution as the training set. Table  presents the F1 score in non-drifted, closed-world and open-world scenarios.

% For existing families, models without TIM consistently exhibited performance degradation across the test set, indicating that malware evolution is a persistent challenge. After incorporating invariant training, the F1 score increases by xx\% and FNR decreases by xx\% for each test year. Furthermore, the emergence of new families significantly reduced the performance of all baseline models. Although the models were trained on the discriminative features of the training set, the appearance of new malware families increased the rate of ineffective discriminative features, leading to performance degradation. TIM mitigated this degradation to some extent by learning invariant features that represent core malicious behaviors in the training set. Finally, on a drift-robust basis, adding TIM does not significantly affect the learning of discriminative features in the training set, with a maximum performance change of less than xx\%.




\subsection{Effective Invariant Feature Learning (RQ3)}
\label{rq3}
We evaluate the framework’s ability to learn invariant features. Section \ref{motivation: failure} shows that ERM models underperform by relying on unstable features. To quantify stable and discriminative feature learning, we define the Feature Contribution Score (FCS) for each feature $f_j$ as follows:
\begin{equation}
    % FCS_j = r(f_j, S^{\prime}) \cdot IS_j,
    FCS_j = \left|r\left(f_j, S_m\right)-r\left(f_j, S_b\right)\right| \cdot IS_j,
\end{equation}
where $r(f_j, S_m)$ and $r(f_j, S_p)$ is the active ratio of feature $f_j$ in malware and benign category respectively and $IS_j$ is its importance score via integrated gradients (IG) with the malicious class as the target. For model $\mathcal{M}$, its discriminative ability is the sum of all FCS values, with higher scores indicating greater focus on discriminative features. We conduct interpretable Drebin feature space experiments to visually assess this capability using closed and open-world settings, as outlined in Section~\ref{rq2}. Results in Table~\ref{tab:rq3} show that invariant training significantly raises FCS, guiding the model to more stable and discriminative features, thus enhancing robustness.

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}  
        \includegraphics[width=0.9\textwidth]{Figure/similarity_variance_malware_exist_bar.pdf}
        \caption{Variance of representation similarity in closed-world setting}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}  
        \includegraphics[width=0.9\textwidth]{Figure/similarity_variance_malware_open_bar.pdf}
        \caption{Variance of representation similarity in open-world setting}
    \end{subfigure}
    \caption{The variance in the similarity between malware feature representations generated by different detectors in closed-world (a) and open-world (b) scenarios and those from the training set. \textit{Retrained} refers to detectors retrained at each time point with newly labeled samples.}
    \label{fig:rq2}
\end{figure}

We also retrain the detector with TIF then analyze the top 10 important features and tracked their discriminability. As shown in Figure~\ref{fig:feature_diff_10_irm},  invariant training improved test performance, with F1 scores exceeding those of the original model and prioritized features demonstrating greater stability. Newly highlighted permissions, such as \verb|GET_TASK| and \verb|ACCESS_FINE_LOCATION| show stronger correlations with malicious behavior, confirming that invariant training helps shift focus to more robust features.

\begin{center}
\fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{\textit{\textbf{Take Away}: TIF enables the detector to learn stable, discriminative features from the training set.}}}
\end{center}

\begin{figure}
    \centering
    % \setlength{\abovecaptionskip}{0cm}
    \includegraphics[width=1.0\linewidth]{Figure/feature_diff_10_irm.pdf}
    \caption{Discriminability for the Top-10 important features after invariant training. F1 Score (Drebin) denotes the performance of the model trained based on Section~\ref{motivation: failure} and F1 Score (Drebin + TIF) denotes the result after trained by invariant training framework TIF.}
    \label{fig:feature_diff_10_irm}
\end{figure}


% We evaluate the proposed framework's ability to learn invariant features. The empirical analysis in Section \ref{motivation: failure} shows that ERM-trained models fail due to their reliance on unstable, weakly discriminative features. Repeating the same setup with the TIM framework, we retrain the model, focusing on the top 10 important features and their discriminative power over the test intervals. The results show that TIM significantly improves F1 scores, mitigating early performance degradation. Moreover, compared with Figure.~\ref{fig:importance}, the selected features demonstrate more stable discriminative power. Newly highlighted permissions, such as \texttt{GET\_TASK} and \texttt{ACCESS\_FINE\_LOCATION} show stronger correlations with malicious behavior, confirming that invariant training helps shift focus to more robust features.

% Additionally, we evaluate the proposed framework's ability to learn invariant feature representations. Based on the type of invariance, we divided the malicious test samples into two groups: those with families present in the training set and those without. The test samples were strictly ordered by release date and split into 10 equal-length intervals. 


\subsection{Ablation Study}
This section evaluates the robustness of each component through an ablation study. Five configurations are tested: the base model, +MPC1, +MPC1 +MPC2, +MPC1 +IGA2, and +MPC1 +IGA2 +MPC2, where MPC1/MPC2 represent multi-proxy contrastive learning, and IGA2 denotes invariant gradient alignment. Table~\ref{tab: ablation} shows the AUT(F1, 12m) results for each setup with the Drebin detector.

In stage one, MPC1 refines intra-environment representations, surpassing the baseline despite minor drops in the final year. Adding MPC2 and IGA2 improves generalization by aligning gradients and learning global features, increasing resilience to drift. The full configuration achieves the highest robustness by unifying feature learning and filtering stable, risk-minimizing features.

\input{Table/RQ3}
\input{Table/Ablation}
\input{Table/Env}

\subsection{Temporal Environment Granularity Selection}
\label{env seg}
In invariant learning, environment segmentation is critical for exposing uncertainty to the model while maintaining sufficient samples per environment for effective learning. Using a one-year training set, we evaluate three segmentation granularities: monthly, quarterly, and equal-sized splits ($n=4$, $8$, and $12$). Table~\ref{tab: env} reports the AUT(F1, 12m) scores for each method across the test phase. Equal-sized splits often increase label imbalance, hindering robust representation learning. Fine-grained partitioning improves feature discrimination but reduces per-batch samples as environments increase, complicating cross-environment alignment. Coarser granularity supports long-term robustness, while finer granularity boosts short-term performance, with partitioning typically guided by validation results, e.g., preferring monthly splits when validated on 2015 samples.
