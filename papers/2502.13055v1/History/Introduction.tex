\section{Introduction}


In open, dynamic environments, even the most effective malware detectors face significant challenges posed by various forms of distribution drift{}\footnote{For clarity, we use the terms drift, shift, concept drift/shift, and distribution drift/shift interchangeably throughout the text.}, leading to performance degradation~\cite{transcending, cade}. From a conceptual perspective, without reasoning on the underlying representations, there are two main causes of drift in Android malware classification tasks: the evolution of malware behaviors and the emergence of previously unseen malware families that exhibit new behaviors~\cite{pei2024exploiting}. The emergence of new variants and families shifts the fundamental statistical properties of test samples~\cite{tesseract, overkill, transcending, Drift_forensice}, thereby weakening the performance of detectors trained on past patterns~\cite{malware_evolution_update}.

% These variants and new families may change the underlying statistical properties of the input samples at the test time~\cite{tesseract, overkill, transcending, Drift_forensice}, resulting in the underlying representations not being able to generalize to them and weakening the effectiveness of patterns learned by the detector from the training dataset~\cite{malware_evolution_update}.

Recent studies have explored incremental training techniques, which involve detecting new distributions during testing and frequently updating models through active~\cite{tesseract, continuous} or online learning~\cite{droidevolver, online_mal, labelless} to address distribution drift. However, these approaches incur significant labeling and model update costs. Although pseudo-labeling has been used to alleviate labeling burdens, it introduces noise that can lead to self-poisoning models~\cite{labelless, recda}, further complicating the learning process. Therefore, improving the robustness of the model in drifting scenarios is essential to reduce the frequency of model updates. Some studies have attempted to use static feature augmentation or select features that are less sensitive to malware evolution~\cite{scrr, apigraph, overkill}. Yet, these often require specific selection methods tailored to different feature spaces that may underperform as drift sources change. Ideally, a drift-robust detector would emphasize features in the training set that both contribute to high performance and remain stable under drift, thus capturing commonalities across different distributions. However, the current training paradigm inhibits the model's ability to learn such invariance.

% Several works focus on robustness under distributional drift~\cite{scrr}. Some use static feature enhancement to extract new feature spaces from applications that are less sensitive to malware updates~\cite{apigraph, overkill}, typically requiring carefully designed selection methods for specific feature spaces. Others adopt incremental retraining~\cite{tesseract}, periodically detecting new distributions during testing and frequently updating models through strategies like active learning~\cite{tesseract, continuous} and online learning~\cite{droidevolver, online_mal, labelless}. These approaches introduce knowledge from new distributions (e.g., new feature constraints or training data) to help models learn commonalities between old and new knowledge. Thus, a detector’s robustness to drift can be seen as its ability to learn stable, discriminative features that generalize to test samples to represent these commonalities. However, we argue that even without new knowledge, some of such invariant features already exist in the current training set and feature space, but are obscured by the existing training paradigm.

% Several works focus on robustness under distributional drift~\cite{scrr}. Some use static feature enhancement to extract new feature spaces from applications that are less sensitive to malware updates~\cite{apigraph, overkill}. This often requires feature selection tailored to a specific feature space~\cite{svm_ce}. Others adopt incremental retraining~\cite{tesseract}, detecting new distributions emerging during testing, and frequently updating the model through strategies such as active learning~\cite{tesseract, continuous} and online learning~\cite{droidevolver, online_mal, labelless}. While this allows new knowledge (e.g., new training data) to become available over time, the model cannot automatically adapt to drift scenarios when such knowledge is missing. Thus, robustness to drift largely depends on the model’s ability to learn stable and discriminative feature representations during training that can generalize to test samples. We posit that the existing feature space already contains undiscovered such patterns, i.e. invariant features, yet they are often overlooked under current training paradigms.
% To support this conclusion
In this paper, we conduct a comprehensive investigation into learning-based malware detectors. Our findings indicate that even when invariant features representing shared malicious behaviors exist within malware families or variants, detectors struggle to learn these features effectively. This inefficiency can be attributed to the limitations inherent in the empirical risk minimization (ERM) training paradigm. ERM operates under the assumption that training and testing data conform to the same distribution, requiring data shuffling or constructing k-fold cross-validation sets. However, in real-world scenarios, testing data is often collected after the training period~\cite{tesseract}, meaning the shuffled training set fails to capture the sequential transition pattern between training and testing samples, hindering the model's adaptability to the dynamic evolution of malware.

% To support this conclusion, we analyzed real malware source code from different time intervals and found invariances in Android malware evolution. Variants within the same family or across different families with similar malicious goals share some parts of core malicious behaviors, even though implementation details vary due to Android updates or various external factors. These behaviors represent underlying invariant features, providing a foundation for models to learn invariant patterns. We further examined existing learning-based detectors to identify their inefficiency in learning such temporal invariant representations. This stems from the assumption of empirical risk minimization (ERM), which requires training and test data to follow the same distribution. Typically, data is shuffled and evaluated in static settings (e.g., hold-out or k-fold validation). However, in real-world scenarios, test data appears after the training period \cite{tesseract}, meaning that current training methods overlook the learnable dynamics of malware evolution. 


% To support this hypothesis, we first reveal the invariance in Android malware evolution from the source code perspective. For malware of the same family, although they may differ in implementation details due to evolution caused by external factors, their consistent malicious intent ensures that they share a set of core features that represent malicious behavior. In addition, the objectives of malicious behavior are inherently limited~\cite{malradar}. Thus, even if certain families are absent in the training data, their execution may still resemble those in the test data with similar behaviors. These two forms of invariance provide a foundation for the model to learn an invariant paradigm. However, the failure of malware detectors based on empirical risk minimization (ERM) seems to indicate its shortcomings in learning these invariants. This motivates us to revisit its training process and find that the flaws stem from the underlying assumption of ERM, which requires training and test data to follow the same independent and identically distributed (i.i.d.) distribution. One common practice to satisfy this requirement is to shuffle training and testing datasets, and model performance is evaluated in a static setting (e.g., using hold-out or k-fold cross-validation). However, real-world environments necessitate observing test data over time following the training period\cite{tesseract}. Shuffling the training set often leads to the loss of critical information about the dynamics of malware evolution{}\footnote{While we focus on malware, the same reasoning can be applied to goodware too.}, causing the training phase to capture only transient features that are effective for the specific time window of the training data but struggle to generalize to new patterns introduced by evolving malware variants~\cite{EIC}. Angioni et al.~\cite{svm_ce} also found the importance of finding temporal invariant features and proposed a feature stability metric, t-stability, to evaluate the contribution of fluctuations in individual features to the degradation of detector performance. However, this method is based on the direct relationship between feature weights and classification performance of a linear classifier and fails to generalize to a more complex group of features and nonlinear relationships.

Invariant learning theory~\cite{IR_intro} aims to address the shortcomings of ERM by learning invariant features shared across different distributions, which aligns with our objective. This theory promotes the discovery of stable representations by dividing training data into distinct subsets or ``environments'' and encouraging the model to minimize differences between them. However, common invariant learning methods typically require prior knowledge of environment labels that reveal unstable features~\cite{environment_label, env_label} and assume the encoder is capable of learning rich, high-quality representations~\cite{yang2024invariant}. These requirements and assumptions are not trivial for malware detection, where drift arises from various non-obvious factors, compounded by imbalanced sample distributions and a diverse feature space that complicates representation learning.

% The above observations motivated us to extend the empirical risk minimization (ERM) training strategy to learn temporally invariant features for malware detectors. This aligns with invariant learning theory, which promotes the discovery of stable representations by dividing training data into distinct subsets, or "environments," and encouraging the model to minimize inter-environmental differences. However, this approach requires prior knowledge of environment labels to expose known unstable features~\cite{environment_label, env_label} and an encoder capable of learning rich representations to identify stable components~\cite{yang2024invariant}. In malware detection, these conditions are more challenging due to distribution drift influenced by multiple factors, unbalanced sample distributions, and non-intuitive features.

This paper presents a temporal invariant training framework, TIF, designed for malware detectors to facilitate the learning of invariant features under distribution drift. Specifically, we partition the training data according to the natural release times of applications, thereby exposing the complex instability of malware evolution over time and avoiding the reliance on prior environment labels. In the context of binary classification tasks, we identify that learning homogeneous representations for a merged multi-family malware class is suboptimal. Recent works have focused on constructing positive relationships within individual malware families to enhance feature representations~\cite{continuous}. However, such an overly fine-grained approach often exacerbates sample imbalance issues. To address this, we propose a multi-proxy contrastive learning module. In our proposed framework, each proxy is a representation of a similar subset within the class, with dynamic updates to multiple proxies that aim to capture feature representations reflective of the application's true semantics. Additionally, we design an invariant gradient alignment module grounded in invariant learning theory, ensuring that the encoder generates similar gradients for samples belonging to the same class across different environments, thereby promoting the learning of high-quality, cross-environment invariant representations. Our solution is orthogonal to existing robust malware detectors; it does not depend on new feature spaces, requires no changes to existing model architectures, and can be applied to any learning-based detector. The main contributions of this paper are as follows:

% This paper aims to propose a unified invariant training framework for Android malware detectors to learn invariant representations in drift scenarios. We first partition the training data by application release dates, preserving distributional changes without focusing on a single drift factor. Additionally, learning homogeneous representations for mixed malware families is suboptimal. Recent work~\cite{continuous} on fine-grained positive relations for each family may lead to excessive relations and worsen class imbalance. We propose a multi-proxy contrastive learning module that learns multiple dynamic proxy representations for each category, aligning samples with similar proxies to avoid forcing dissimilar distributions together. Building on the challenges of invariant learning, an invariant gradient alignment module is designed based on this theory. It ensures that the encoder generates similar gradients for same-class samples from different environments when processed by the same classifier. To this point, we finally develop an invariant training framework, TIF, for malware detection. Through a two-stage training process, it amplifies and compresses unstable information across environments, enabling the encoder to learn time-invariant malware representations. The proposed solution is orthogonal to existing drift-robust malware detectors. It does not rely on new feature spaces, requires no modifications to existing model architectures, and can be applied to any learning-based detector. The main contributions of this paper are as follows:

% Based on the above observations, we propose that ERM training can be extended to learn temporally invariant features for malware detection. Invariant learning~\cite{IRM_training} offers a potential solution by partitioning the training set into different environments to expose instabilities, thereby encouraging the model to discover cross-environment invariant representations by minimizing differences between environments. Inspired by this theory, an intuitive approach is to restructure the training set to incorporate the temporal evolution of malware. Specifically, we sample training data from different time-based subsets (environments) of application release dates, with each subset satisfying ERM's prerequisites. The temporal isolation between subsets naturally preserves the distribution shifts in malware. The training objective is establishing invariant feature representations between each subset while minimizing their empirical risk. This scheme also addresses the limitation of applying invariant learning theory, i.e., the acquisition of environment labels often relies on a priori knowledge and requires knowing which features the model is wrongly focusing on. Here, application timestamps naturally provide environment delineation labels while retaining information about distribution changes across time. 

% The above intuition leads us to propose TIM, an invariant training framework for Android malware detection. The framework leverages application release dates to naturally construct training environments. We then designed two core components for invariant training: Multi-Proxy Contrastive Learning and Invariant Gradient Alignment. The former models rich feature representations within both normal and malicious categories, aiming to maximize the learning of discriminative information from the training set. The latter aligns the classification gradients of same-class samples across environments based on Invariant Risk Minimization (IRM). Through these two components, the invariant training framework extracts invariant feature abstractions from the diverse representations of each environment. The proposed solution is orthogonal to existing drift robust malware detectors. It does not rely on a new feature space and needs no modifications to the existing model structure, can be applied to any learning-based detector. The main contributions of this paper are as follows:

\begin{itemize}
    \item We define invariance in malware evolution, positing that learning stable and discriminative invariant representations is key to mitigating malware vulnerability in drift scenarios.
    \item We propose a first temporal invariant learning framework, TIF, for malware detectors, which can be integrated with arbitrary detectors to learn invariant representations over time by exposing and compressing unstable information in drifts.
    % \item We propose a temporal invariant learning framework for malware detection tasks that aids arbitrary detectors in learning representation invariance in software evolution by exposing and compressing unstable information in malware drift.
    \item We carefully design multi-proxy contrastive learning and invariant gradient alignment modules to model complex distributions within malware and encourage the model to learn high-quality and stable representations across temporal variations.
    \item We construct a 10-year dataset\footnote{We will open source the dataset metadata and code repository to foster reproducibility studies.} and a series of experiments to evaluate the robustness of the TIF across various drift scenarios and feature spaces. The results show that TIF effectively slows the detector's degradation and generates invariant representations that outperform state-of-the-art methods.
\end{itemize}


% Others, instead, dynamically update models following incremental retraining~\cite{tesseract}, active learning~\cite{tesseract, continuous}, and online learning~\cite{droidevolver, online_mal, labelless} strategies. By periodically detecting new distributions that appear at test time~\cite{transcending}, the model is retrained to always have a valid feature representation relative to the test set. Essentially, these approaches learn new features, new distribution of existing features, or a combination thereof, as new knowledge (e.g., new training data) becomes available over time~\cite{overkill}. When this new knowledge (i.e., new training data) is missing, the models fail to adapt to drift scenarios as they fail to learn discriminative \emph{and} stable features that remain invariant over time. We argue existing feature space already contains as yet undiscovered learnable invariant information that is masked under existing training paradigms.

% Based on this intuition, we propose an invariant training strategy for Android malware detection. The solution naturally constructs the training environment for exposing temporal-varying features based on the release time of applications. After that, we elaborate on two core modules for invariant training, i.e., invariant representation alignment and invariant gradient alignment, which enable the model to extract uniform and invariant feature abstractions from the rich feature representation information learned through the dual alignment of the encoder and classifier. The proposed solution is orthogonal to existing drift robust malware detectors. It does not rely on a new feature space and needs no modifications to the existing model structure. We evaluate its robustness against new malware families and variants of existing families on datasets with different drift levels and time span, which significantly improves the performance of the malware detector in drifting scenarios. The main contributions of this paper are as follows:

    % \item We evaluated the invariant-enhanced model performance on datasets with various drift levels and periods, with F1 scores improving by up to 8.10\% (4.63\% on avg), FNR reducing by up to 49.89\% (17.55\% on avg) and AUT (on F1 score) up to 4.75\% (overall about 3\%) over 5 years.


