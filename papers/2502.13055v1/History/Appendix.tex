\section{Appendix}


% \subsection{Performance in Dataset with Different Distribution}

\subsection{Invariant Training Algorithm}
\input{Algorithm/Invariant_training}

The proposed invariant training process is shown in Algorithm~\ref{alg1}. This learning framework consists of two phases: discriminative information amplification and unstable information suppression. In Stage 1, the model enhances its ability to learn discriminative features through empirical risk minimization (ERM). Specifically, in Line 6, the classification loss $\mathcal{L}^{e}_{CLS}$ is calculated for each environment to ensure correct class separation, and in Line 7, a multi-proxy contrastive loss $\mathcal{L}^{e}_{MPC}$ is computed to improve the discriminative power among samples. The combined loss for ERM, including the classification and contrastive components, is computed in Line 10, and then the model is updated by backpropagation in Line 11. Stage 2 focuses on suppressing unstable information through invariant training. In Line 13, the optimizer is reset to remove the influence of previous training, while the model parameters from Stage 1 are retained in Line 14 to preserve the learned discriminative capabilities. Line 17 extracts sample representations through the encoder, and Line 21 calculates the invariant gradient alignment (IGA) loss $\mathcal{L}^e_{IGA}$ for each environment, which ensures similar responses across environments. The combined invariant risk minimization (IRM) loss is formed in Line 28, integrating the classification, contrastive, and alignment losses, followed by updating the model in Line 29. This two-phase training process enables the model to learn features that are both discriminative and stable, improving robustness and generalization in the face of distribution drift.


