\section{Introduction}
\label{sec:introduction}
The efficiency of algorithms is typically measured in the worst-case model.  The worst-case model makes a fundamental assumption that algorithmic problems are solved from scratch. In real applications, many problems are solved repeatedly on similar input instances.  There has been a growing interest in improving the running time of algorithms by leveraging similarities across problem instances.  The goal is to \emph{warm start} the algorithm; that is, initialize it with a machine-learned state, speeding up its running time.  This
initial state is learned from the past instances of the problem.   

This recent line of work has given rise to a widely-applicable model for beyond-worst-case running time analysis.   The area has come to be known as \defn{algorithms with predictions}. In the algorithms-with-predictions model, the goal is to establish strong worst-case-style guarantees for the algorithm.  The critical difference is that the running time of the algorithm is \emph{parameterized} by the quality of the learned starting state; that is, the prediction quality. The algorithms designed in this model are also frequently referred to as \emph{learning-augmented} or simply \emph{learned} algorithms.

Ideally, an algorithm outperforms the best worst-case algorithm with high quality predictions (i.e. the algorithm is \defn{consistent}). When the predictions are incorrect,  the algorithm's performance should degrade proportionally to the error (i.e. the algorithm should be \defn{smooth}) and never be worse than the best worst-case algorithm (i.e. the algorithm is \defn{robust}). If an algorithm is consistent, smooth, and robust, it is called an \defn{ideal learned algorithm}. 

Kraska et al.~\cite{KraskaBCDP18}  \emph{empirically} demonstrated how machine-learned predictions can speed up data structures.  
This seminal paper inspired Dinitz et al.~\cite{DinitzILMV21} to propose a \emph{theoretical} framework to leverage predictions to speed up \emph{offline} algorithms.
Since this work, several papers have used predictions to improve the running time of offline combinatorial optimization problems such as maximum flow~\cite{DaviesMVW,POLAK2024106487}, shortest path~\cite{LattanziSV23}, and convex optimization~\cite{SakaueO22}. Recently, Srinivas and Blum \cite{SrinivasB25} give a framework for utilizing multiple offline predictions to improve the running time of online optimization problems.
This growing body of theoretical and empirical work shows the incredible potential of using machine-learned predictions to improve the efficiency of algorithms. The prediction framework provides a rich and algorithmically interesting landscape that is yet to be understood. 

Data structures are a critical building block for dynamic algorithms for optimization problems.   
There is a growing body of theoretical work on designing ideal algorithms for data structure problems~\cite{LinLW22,
McCauleyMNS23,
McCauleyMoNi24,
BrandFNP24, liu2023predicted, 
benomar2024learningaugmented, 
zeynali2024robust, bai2023sorting,
dinitz2024binary}; see Section~\ref{sec:related} for details. 
Predictions have been particularly effective at speeding up 
data structures for dynamic graph problems, which typically incur polynomial update times in the worst case.  
In particular, McCauley et al.~\cite{McCauleyMoNi24} use
predictions to design faster data structures for incremental topological maintenance and cycle detection.  
Henzinger et al.~\cite{HenzingerSSY24} and van den Brand et al.~\cite{BrandFNP24} initiate the use of predictions for designing faster dynamic graph algorithms.
In particular, van den Brand~\cite{BrandFNP24} solve the online matrix-vector multiplication with predictions and use it to obtain faster algorithms for several dynamic graph problems such as incremental all-pairs shortest paths, reachability, and triangle detection. Liu and Srinivas~\cite{liu2023predicted} give efficient black-box transformations from  offline divide-and-conquer style algorithms to fully
dynamic learned algorithms that are given predictions about the update sequence.  
These initial results demonstrate incredible potential and there is a need to develop algorithmic techniques for designing data structures that can leverage predictions.  
Towards this goal, in this paper, we study the fundamental data structure problem of maintaining approximate single-source shortest paths in dynamic graphs with edge insertions. No learned data structure has been developed for this problem despite being a clear target in the area. 

\paragraph{Incremental Single-Source Shortest Paths.}  In this paper, we design a data structure to maintain shortest paths in a weighted directed graph when edges are inserted over time. 
Initially, all nodes $V$ of the graph are available and, in the single-source case, a source $s$ is specified. There are $m$ edges that arrive one by one:  at each time step $t$, an edge (with a positive weight) is added to the graph.   The goal is to design a data structure that approximately stores the shortest path distance from the source $s$ to every other vertex $v$ in the graph.   
Let $d^t(s,v)$ be the distance between $s$ and $v$ after $t$ edge insertions.  Let $\hat{d}^t(s,v)$ be an approximation of $d^t(s,v)$  computed by the algorithm after $t$ edges arrive. The algorithm needs to efficiently compute $\hat{d}^t(s,v)$ such that $d^t(s,v) \leq \hat{d}^t(s,v) \leq (1 + \epsilon) d^t(s,v)$ for some constant $\epsilon  > 0$.  

Incremental shortest paths is a fundamental algorithmic problem used in applications as well as a building block for other data structures~\cite{RodittyZ11,HenzingerKN16}. 
This problem has been extensively studied in the literature~\cite{GutenbergW20,BernsteinGS21,BernsteinGW20,KyngMeGu22,chechik2021incremental,henzinger2014sublinear, henzinger2015improved}. 
The best-known worst-case update times for the problem are $\tilde{O}(n
^2 \log W/\epsilon^{2.5} + m)$~\cite{probst2020new} for dense graphs and a $\tilde{O}(m^{4/3}\log W/\epsilon^2)$ algorithm~\cite{KyngMeGu22} for sparse graphs.\footnote{The $\tilde{O}$ notation suppresses log factors.}  

Recently, Henzinger et al.~\cite{HenzingerSSY24} and van den Brand~\cite{BrandFNP24} applied predictions to the problem of computing all-pairs shortest paths (APSP) in incremental graphs.  
We follow their prediction model in which the data structure is given a prediction of the online edge sequence $\hat{\sigma}$ before any edges arrive.  The performance of the algorithm is given in terms of the prediction error.  Define an edge $e$'s error $\eta_e$ as the difference between its arrival time in $\hat{\sigma}$ and $\sigma$ and the aggregate error $\eta$ as the $\max_{e} \eta_e$.  They show that for the incremental APSP problem, predictions can be used to support $O(1)$-lookup time and $O(\eta^2)$ time per edge insert, which is optimal under the Online Matrix-Vector Multiplication Hypothesis~\cite{BrandFNP24}.  The preprocessing time used in \cite{HenzingerSSY24} is $O(mn^3)$, and it is $O(n^{(3 + \omega)/2})$ in \cite{BrandFNP24} where $\omega$ is the exponent from matrix multiplication. 

Their work leaves open an important question---can predictions also help speed up the related and fundamental problem of maintaining approximate \emph{single-source} shortest paths under edge inserts, which has not yet been studied in this framework.
 
\paragraph{Our Contributions.}   The main contribution of this paper is a new learned data structure for the incremental approximate SSSP problem and a demonstration that it is ideal (consistent, robust, and smooth) with respect to the prediction error.
As a building block, we study the \emph{offline} version of the problem that has not previously been considered, which is of independent interest. 

We show that these techniques extend to the all-pairs shortest-path problem as well. 

\paragraph{Offline Incremental Single-Source Shortest Paths.}  We give a new algorithm for the \emph{offline} version of the incremental shortest path problem. In the offline version of the problem, the sequence of arriving edges is given in advance where each edge is assigned a unique time.  The goal is to maintain approximate distances $\hat{d}^t(s,v)$ for all $v$ and all times $t$ as efficiently as possible.  That is, given a query $(v,t)$, the data
structure outputs the approximate shortest path from source $s$ to
vertex $v$ in the graph with edges inserted up to time $t$.
By reversing time, the incremental and decremental versions of this problem are immediately equivalent.

Surprisingly, to our knowledge, past work in the offline setting has focused solely on exact, rather than approximate, incremental shortest path.  These exact versions have strong lower bounds. Roddity and Zwick~\cite{RodittyZ11} show that for the incremental/decremental single-source shortest paths problem in weighted directed (or undirected) graphs, the amortized query/update time must be $n^{1-o(1)}$, unless APSP can be solved in truly subcubic time (i.e. $n^{3-\epsilon}$ for constant $\epsilon>0$). 

We show that the offline \emph{approximate} version of the problem can be solved significantly faster than the exact version in the worst-case setting.  This natural problem reveals key algorithmic ideas for designing our learned online SSSP algorithm.

\begin{theorem}
\label{thm:offline-approx}
For the offline incremental SSSP problem there exists an algorithm running in worst-case total time $O(m \log (nW)  (\log^3 n)(\log \log n)/\epsilon)$ that returns $(1+\epsilon)$ approximate single-source shortest paths for each time $t$.
\end{theorem}


\paragraph{Predictions Model and Learned Online Single Source Shortest Paths.} 
Let $\sigma = e_1, \ldots, e_m$ denote the actual online sequence of edge inserts.  Before any edges arrive, the algorithm receives a prediction of this sequence, $\hat{\sigma}$.  This is the same prediction model considered by Henzinger et al.~\cite{HenzingerSSY24} and van den Brand~\cite{BrandFNP24} for the all-pairs shortest-paths problem. Let $\ind(e)$ be the index of $e$ in $\sigma$ and $\widehat{\ind}(e)$ be the index of $e$ in $\hat{\sigma}$.
Define $\widehat{\ind}(e):=m+1$ for edges $e$ that are not in $\hat{\sigma}$. 
Let $\eta_e = |\ind(e) - \widehat{\ind}(e)|$ for each edge $e$ in $\sigma$.

We first describe the performance of our learned SSSP algorithm in terms of parameters $\tau$ and $\text{HIGH}(\tau)$.  For any $\tau$, define $\text{HIGH}(\tau)$ to be the set of edges $e$ in $\sigma$ with error $\eta_e > \tau$. Then, we show that the bounds obtained are more robust than several natural measures of prediction error.  

\begin{theorem}\label{thm:online}
    There is a learned online single-source shortest path algorithm that given a prediction $\hat{\sigma}$ gives the following guarantees:
    \begin{itemize}[noitemsep, nolistsep]
        \item The algorithm maintains a $(1+\epsilon)$-approximate shortest path among edges that have arrived. 
        \item  The total running time for all edge inserts is
        $\tilde{O}(m \cdot \min_{\tau} \{\tau + |\text{HIGH}(\tau)| \} \log W/\epsilon)$.
    \end{itemize}
\end{theorem}

Our algorithm uses the offline algorithm as a black box, and therefore our results also apply to the decremental problem in which edges are deleted one by one.

This theorem can be used to give results for two natural error measures. 
We call the first error measure, the \defn{edit distance} $\text{Edit}(\sigma,\hat{\sigma})$, defined as the minimum number of insertions and deletions needed to transform $\sigma$ to the prediction $\hat{\sigma}$. To the best of our knowledge this is a new error measure.
  
The second error measure is  $ \eta = \max_{e \in \sigma} \eta_e $  the maximum error of any edge; this measure was also used by past work (e.g.~\cite{BrandFNP24, McCauleyMNS23, McCauleyMoNi24}).
The theorem gives the following corollary.

\begin{corollary}\label{cor:main}
There is a learned online algorithm that maintains $(1+\epsilon)$-approximate shortest paths and has running time at most the minimum of $\tilde{O}(m \cdot \text{Edit}(\sigma,\hat{\sigma}) \log W/\epsilon)$ and $\tilde{O}(m \eta \log W/\epsilon)$.
\end{corollary}
The corollary can be seen as follows. 
By definition, there are no edges in $\text{HIGH}(\eta)$.  Thus, setting $\tau = \eta$ in Theorem~\ref{thm:online}  gives an $\tilde{O}(m \eta \log W/\epsilon)$ running time.  Alternatively, setting $\tau = \text{Edit}(\sigma,\hat{\sigma})$  ensures that $\text{HIGH}(\tau)$
contains at most $\tau$ edges. This is because any edge not inserted or deleted in the process of transforming $\hat{\sigma}$ to $\sigma$ can move at most $\tau$ positions and so only inserted or deleted edges contribute to $\text{HIGH}(\tau)$.
This gives a running time of $\tilde{O}(m\ \text{Edit}(\sigma,\hat{\sigma}) \log W/\epsilon)$.

\paragraph{Discussion On Single-Source Shortest Paths.}   Notice that if a small number of edges have a large error $\eta_e$, the $\text{Edit}(\sigma,\hat{\sigma})$ is small even though the maximum error is large, and thus the algorithm retains strong running time guarantees on such inputs.   
Furthermore, $\text{Edit}(\sigma,\hat{\sigma})$ is small even if there is a small number of edges that are not predicted to arrive but do, or are predicted to arrive but never do (such edges are essentially insertions and deletions).  

On the other hand, the edit distance bound is a loose upper bound in some cases: for example, even if a large number of edges are incorrect, but have small relative change in position between $\sigma$ and $\hat{\sigma}$, then $\eta$ will be small. 

The algorithm is \emph{consistent} as its running time is optimal (since $\Omega(m)$ is required to read the input) up to log factors when the predictions are perfect.
It is \emph{smooth} in that it has a slow linear degradation of running time in terms of the prediction error.  We remark that \emph{robustness} to arbitrarily erroneous predictions can be achieved with respect to any worst-case algorithm simply by switching to the worst-case algorithm in the event that the learned algorithm's run time grows larger than the worst-case guarantee. 

\paragraph{Extension to All-Pairs  Shortest Paths.}  Next, we show that our techniques are generalizable by applying them to the all-pairs shortest-path (APSP) problem.  Similar to the SSSP case, we first solve the offline incremental version of the problem by running the SSSP algorithm multiple times.  We then extend it to the online setting with predictions. 

For the incremental APSP problem, it does not make sense to consider amortized cost---Bernstein~\cite{bernstein2016maintaining} gives an algorithm with nearly-optimal total work for the online problem even without predictions.  As a result, past work on approximate all-pairs shortest path with predictions has focused on improving the worst-case time for each update and query.  

For the APSP problem, we follow~\cite{HenzingerSSY24,BrandFNP24} and assume that $\hat{\sigma}$ is a permutation of $\sigma$---the set of edges predicted to arrive are exactly the set that truly arrive.\footnote{While this is in some cases a strong assumption, it seems unavoidable for worst-case update and query cost.} 

\begin{theorem}\label{thm:online-apsp}
    There is a learned online all-pairs shortest path algorithm with $\tilde{O}(nm\log W / \epsilon)$ preprocessing time, $O(\log n)$ worst-case update time, and $O(\eta^2\log\log_{1 + \epsilon} (nW))$ worst-case query time.
\end{theorem}

\paragraph{Comparison to Prior Work.} 

To the best of our knowledge, no prior work has considered the incremental SSSP problem in the algorithms-with-predictions framework. Our algorithm for the SSSP problem has a recursive tree of subproblems, that we bring online with predictions on the input sequence.  We remark that the work of   Liu and Srinivas \cite{liu2023predicted} gave a general framework for taking offline recursive tree algorithms into the online setting with predictions. Their framework is general and applies to a large class of problems that can be decomposed into recursive subproblems with smaller cost.
In contrast, our tree-based decomposition technique is tailored specifically to shortest paths which enables our efficient runtime.  Moreover, our analysis differs significantly from~\cite{liu2023predicted} as well---we cannot spread the cost evenly over smaller-cost recursive subproblems.  This is because a single edge insert can result in $\Omega(n)$ changes in shortest paths.  To avoid this, we allow subproblems to grow and shrink as necessary and charge the cost of a large subproblem to a large number of distance changes; see Section~\ref{sec:technical}.

The incremental APSP problem with predictions was studied by past work~\cite{HenzingerSSY24,BrandFNP24}.  Henzinger et al.~\cite{HenzingerSSY24} achieve $\tilde{O}(1)$ update time and $\tilde{O}(\eta^2)$ query time, after $O(mn^3)$ preprocessing for any weighted graph.
van den Brand et al.~\cite{BrandFNP24} achieve similar update and query times with $\tilde{O}(n^{(3 + \omega)/2})$ preprocessing time; however, the result is limited to unweighted graphs.  
They further show that this query time is essentially optimal: under the Online Matrix-Vector Multiplication Hypothesis, it is not possible to obtain $O(\eta^{2-\delta})$ query time for any $\delta > 0$ while maintaining polynomial preprocessing time and $O(n)$ update time.
Thus, Theorem~\ref{thm:online-apsp} obtains faster preprocessing time for sparse graphs and supports weighted graphs.  Finally, we note that Liu and Srinivas~\cite{liu2023predicted} give bounds for the \emph{fully dynamic} weighted APSP problem in the prediction-deletion model they propose and their bounds are incomparable to ours.