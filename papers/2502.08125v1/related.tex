\subsection{Additional Related Work}\label{sec:related}

\paragraph{Data Structures with Predictions.}

Data structures augmented with predictions have demonstrated empirical success in key applications such as indexing \cite{KraskaBCDP18,ding2020alex,DaiXGA20}, caching \cite{JiangP020,LykourisVassilvitskii21}, 
Bloom filters \cite{mitzenmacher2018model,vaidya2020partitioned}, frequency estimation \cite{hsu2019learning}, page migration \cite{IndykMMR22}, routing \cite{BhaskaraGKM20}.  

Initial theoretical work on data structures with predictions focused on improving their space complexity or competitive ratio, e.g. learned filters~\cite{vaidya2020partitioned,mitzenmacher2018model,bercea2022daisy} and count-min sketch~\cite{hsu2019learning,du2021putting} on stochastic learned distributions.  Several papers have since used predictions to provably improve the running time of dictionary data structures, e.g.\ learned binary-search trees~\cite{lin2022learning, chen2022power, zeynalirobust,dinitz2024binary}.  McCauley et al.~\cite{McCauleyMNS23} presented the first ``ideal'' data structures with prediction for the list-labeling problem~\cite{McCauleyMNS23}. They use a ``prediction-decomposition'' framework to obtain their bounds and extend the technique to maintain topological ordering and perform cycle detection in incremental graphs with predictions~\cite{McCauleyMoNi24}.

\paragraph{Incremental SSSP and APSP in the Worst-Case Setting.} 
We review the state-of-the-art deterministic worst-case algorithms for the incremental $(1+\epsilon)$ SSSP problem.  

The best-known deterministic algorithm for dense graphs is by Gutenberg et al.~\cite{probst2020new} with total update time $\tilde{O}(n^2 \log W/\epsilon^{O(1)})$, which is nearly optimal for very dense graphs.  For sparse graphs,
Chechik and Zhang~\cite{chechik2021incremental} give an algorithm with total time $\tilde{O}(m^{5/3}\log W/\epsilon)$, which is improved by 
Kyng et al.~\cite{KyngMeGu22} to a total time $\tilde{O}(m^{3/2}\log W /\epsilon)$; this is the best-known deterministic algorithm for sparse graphs.  Note that many of these solutions use the ES-tree data structure~\cite{shiloach1981line} as a key building block. The ES-tree can maintain exact distances in an SSSP tree for weighted graphs with total running time $O(mnW)$~\cite{henzinger1995fully}, where $W$ is the ratio of the largest to smallest edge weight. The ES-tree can be used to maintain $(1+\epsilon)$-approximate distances in total update time $\tilde{O}(mn\log W/\epsilon)$ for incremental/decremental directed SSSP; see e.g.~\cite{bernstein2009fully,bernstein2016maintaining,madry2010faster}.

This paper shows that even modestly accurate predictions can be leveraged to circumvent these high (polynomial) worst-case update costs in incremental SSSP.

For the approximate incremental APSP problem without predictions, Bernstein~\cite{bernstein2016maintaining} gave an algorithm that has nearly-optimal total runtime $O(nm \log^4 (n) \log (nW)/\epsilon)$ and $O(1)$ look-up time. 

Peng and Rubinstein consider generally how incremental or decremental algorithms can be turned into fully dynamic algorithms \cite{PengR23}.