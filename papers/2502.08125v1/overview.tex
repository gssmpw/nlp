\section{Technical Overview}
\label{sec:technical}

\subsection{Offline Algorithm}
The preliminary observation behind our algorithm is the following. Let's say we round the distance to each vertex up to the nearest power of $(1 + \epsilon)$.  Since the maximum distance to any vertex is $nW$, this means that we group distances into $O(\log_{1 + \epsilon} (nW)) = O(\log (nW)/\epsilon)$ ``buckets.''  Knowing that the distance to any vertex only decreases over time, the buckets of all vertices only change $O(n\log (nW)/\epsilon)$ times in total.  The goal of our algorithm is to list the $O(\log (nW)/\epsilon)$ times when a vertex shifts from one bucket to the next.

Thus, when an edge is inserted at time $t$, our goal is to run Dijkstra's only on vertices whose distance is changing at time $t$, charging the cost to the changing distances.  

\paragraph{Removing Vertices from Recursive Calls.} 
Our algorithm recursively divides the sequence of edge inserts in half, starting with the entire sequence of edge inserts $\{e_1, \ldots, e_m\}$, and recursing until the sequence contains a single edge.   

Consider a sequence $\{e_{\ell}, \ldots, e_r\}$, which we denote by $[\ell, r]$.  We divide this interval into two equal halves: $[\ell, x], [x, r]$.  

The observation behind our algorithm is the following.  Let's say we calculate the distance to all vertices at time $x$.  Consider a vertex $v$ that is in the same bucket at $\ell$ and $x$.
Since the distance to $v$ is non-increasing as new edges arrive, $v$ must be in this bucket throughout the interval, and we do not need to continue calculating its distance. 
Therefore, we should remove $v$ from the recursive call for the interval $[\ell, x]$.  
Similarly, if $v$ is in the same bucket at $x$ and $r$, then we should remove $v$ from $[x, r]$.

If we are successful in removing vertices this way, we immediately improve the running time.  Each vertex $v$ is included in an interval $[\ell, r]$ if and only if it shifts from one bucket to another between $\ell$ and $r$.  Thus, each time $v$ shifts buckets, it is included in at most $\log m$ intervals.  This means that each vertex is included in $O(\log m \log (nW)/\epsilon)$ intervals.  
 
Let's assume that for an interval $[\ell, r]$, we can run Dijkstra's algorithm for the midpoint $x$ in time (ignoring log factors) proportional to the number of edges that have at least one endpoint included in the interval $[\ell, r]$. 
Since each vertex contributes to the running time of $\polylog(nmW)/\epsilon$ different subproblems, we can sum to obtain $\tilde{O}(m\log W/\epsilon)$ total cost to calculate the shortest path to every vertex in every subproblem.

\paragraph{Ensuring Dijkstra's Runs are Efficient.}
Let us discuss in more detail how to ``remove'' a vertex from a recursive call.
We do not remove vertices from the graph; instead, we say that a vertex is \defn{dead} during an interval $[\ell, r]$ if its distance bucket does not change during the interval, and \defn{alive} otherwise.
Each edge $(u,v)$ is alive if and only if $v$ is alive.
  
For each time $x$  which is the midpoint of an interval $[\ell, r]$, consider building a graph $G'_x$
consisting of all alive vertices and edges in $[\ell, r]$.  If $(u,v)$ is alive, but $u$ is dead, we also add $u$ to $G'_x$.
Now, the time required to run Dijkstra's algorithm on $G'_x$ is roughly proportional to the number of edges in $G'_x$ as we desired, but the distances it gives are not meaningful---in fact, $s$ may not even be in $G'_x$.

To solve this problem, we observe that we already know (approximately) the distance to any dead vertex $u$: we marked $u$ as dead because its distance bucket does not change from $\ell$ to $r$.  Thus, we add $s$ to $G'_x$; then, for each alive edge $(u,v)$ where $u$ is dead, instead of adding $u$ to $G'_x$, we add an edge from $s$ to $v$ with weight equal to the rounded distance from $s$ to $u$ from the previous recursive call plus $w(u,v)$.
Running Dijkstra's algorithm on $G'_x$ now gives (ostensibly) meaningful distances from $s$.

We calculate the distance to each vertex in $G'_x$, round the distance up to obtain its bucket, and then recurse (marking vertices dead as appropriate) on $[\ell, x]$ and $[x, r]$.

\paragraph{Handling Error Accumulations.}

Unfortunately, the above method does not quite work, as the error accumulates during each recursive call.  

For an interval $[\ell, r]$ with midpoint $x$, we use the terms \emph{recursive call $[\ell, r]$} and \emph{recursive call $x$} interchangeably.
For a recursive call $x$, the shortest path to $v$ may begin with an edge $(s, v')$ (for some dead vertex $v'$) weighted according to the bucket of $v'$.  The bucket of $v'$ was calculated by rounding up the length of the shortest path to $v'$ in some graph $G'_{y}$ at time $y$; the length of this path again was also rounded up, and so on.

In other words, since the weight of each edge from $s$ to a dead vertex $v'$ is based on the bucket of $v'$, it has been rounded up to the nearest power of $1 + \epsilon$.  Thus, \emph{each} recursive call loses a $1 + \epsilon$ approximation factor in distance.

To solve this, we note that the recursion depth of our algorithm is $\log_2 m$.  Thus, we split into finer buckets: rather than rounding up to the nearest power of $1 + \epsilon$, we round up to the nearest power of $1 + \epsilon/\log_2 m$.  Thus, the total error accumulated over all recursive calls is $(1 + \epsilon/\log_2 m)^{\log_2 m} \leq 1 + \epsilon + \epsilon^2$ (see Section~\ref{sec:prelim}); decreasing $\epsilon$ slightly gives error $1 + \epsilon$.  

Rounding buckets up to the nearest power of $1 + \epsilon/\log m$ changes our running time analysis: most critically, each vertex now changes buckets $\log_{1 + \epsilon/\log m} nW = \Theta(\log(nW) \log m/\epsilon)$ times.  

\subsection{Ideal Algorithm with Predictions}

With perfect predictions, the algorithm described above runs in $\tilde{O}(m\log W/\epsilon)$ time: we can just run the algorithm on the predicted (in fact the actual) sequence $\hat{\sigma} = \sigma$ to obtain an approximate distance to every vertex at each point in time.  
We show how to carefully rebuild portions of the offline approach to obtain an ideal algorithm.

\paragraph{Warmup: Hamming Distance of Error.}  To start, let's give a simple algorithm that uses predictions that contain error.  
Let $Ham(\sigma, \hat{\sigma})$ be the Hamming distance between $\sigma$ and $\hat{\sigma}$---in other words, the number of edges whose predicted arrival time was not exactly correct in $\hat{\sigma}$.  We give an algorithm that runs in $\tilde{O}(m\cdot Ham(\sigma, \hat{\sigma})\log W/\epsilon)$ time.

The main idea behind this algorithm is to \emph{update} the sequence of predictions $\hat{\sigma}$ as edges come in.  At time $t$, an edge $e$ arrives; if $e$ arrived at $t$ in $\hat{\sigma}$, the algorithm does nothing---the predictions created by running the offline algorithm on $\hat{\sigma}$ took $e$ arriving at $t$ into account.  If $e$ did not arrive at $t$ in $\hat{\sigma}$, then the algorithm creates a new $\hat{\sigma}$, replacing the edge arriving at $t$ with $e$.  The algorithm then reruns the offline algorithm on the new $\hat{\sigma}$ in $\tilde{O}(m\log W/\epsilon)$ time.

Since we run the offline algorithm (in $\tilde{O}(m\log W/\epsilon)$ time) each time an edge was predicted incorrectly, we immediately obtain $\tilde{O}(m\cdot Ham(\sigma, \hat{\sigma})\log W/\epsilon)$ time.

\paragraph{Handling Nearby Edges More Effectively.}
Ideally, we would not rebuild from scratch each time an edge is predicted incorrectly---we would like the running time to be proportional to \emph{how far} an edge's true arrival time is from its predicted arrival time.  

Our final algorithm improves over the warmup Hamming distance algorithm in two ways.  First, it updates the predicted sequence $\hat{\sigma}$ more carefully.  Second, it only rebuilds parts of the recursive calls of the offline algorithm: specifically, only intervals that changed as $\hat{\sigma}$ was updated.

First, let's describe how to update $\hat{\sigma}$.  As before, when an edge $e$ arrives at time $t$, if $e$ was predicted to arrive at $t$ in $\hat{\sigma}$ the algorithm does nothing.  
If $e$ was predicted to arrive at time $t'$ in $\hat{\sigma}$, the algorithm modifies $\hat{\sigma}$ by inserting $e$ at time $t$ (shifting all edges after $t$ down one slot), and deleting $e$ at time $t'$ (shifting all edges after $t'$ up one slot).
If $e$ is not predicted in $\hat{\sigma}$, the algorithm only inserts $e$ at time $t$, shifting subsequent edges down one slot.

The only entries in $\hat{\sigma}$ that change are those between $t$ and $t'$ (inclusive).  Therefore, we only need to recalculate $G'$ for times between $t$ and $t'$.  

Finally, we update the distance array $D$.  The algorithm greedily updates $D$ during the above rebuilds to maintain the invariant that $D[i]$ stores the estimated distance $\hat{d}^t(v_i)$, which as discussed in Section~\ref{sec:prelim}, is a $(1+\epsilon)$ approximation for $d^t(v_i)$.

\paragraph{Analysis.}
For any edge $e$ that appears at time $\ind(e)$ in $\sigma$ and time $\widehat{\ind}(e)$ in $\hat{\sigma}$, we define $\eta_e = |\ind(e) - \widehat{\ind}(e)|$.  If $e$ does not appear in $\hat{\sigma}$ then $\eta_e = |m+1-\ind(e)|$.
Let $\eta$ be the maximum error: $\eta = \max_{e \in \sigma} \eta_e$.
 
We show that an edge $e$ causes a rebuild of a graph $G'_t$ only if $t$ is between its predicted arrival time $\widehat{\ind}(e)$ and its actual arrival time $\ind(e)$. 
This means that each $G'_t$ can only be rebuilt $O(\eta)$ times.  Since the total time to build all $G'_t$ is $\tilde{O}(m \log W/\epsilon)$, the total time to rebuild all $G'_t$ $\eta$ times is $\tilde{O}(m\eta\log W/\epsilon)$.

With a more careful analysis, we can get the best of the Hamming analysis and the max error analysis.  For any $\tau$, let $\text{HIGH}(\tau)$ be the set of edges with error more than $\tau$.  For all edges with error more than $\tau$, we may (in the worst case) rebuild the entire interval $\{1, \ldots, m\}$, for total cost $\tilde{O}(m |\text{HIGH}(\tau)| \log W/\epsilon)$.  For all edges with error at most $\tau$, the total rebuild cost is, as above, $\tilde{O}(m\tau\log W/\epsilon)$.  Thus, we obtain total cost $ \tilde{O}\left(m (\log W/\epsilon) \cdot \min_{\tau} \{ \tau+|\text{HIGH}(\tau)| \}\right)$.