\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{multirow}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, allcolors=blue, breaklinks=true]{hyperref}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{amsfonts}
\usepackage{authblk}  % Package for author affiliations

\usepackage{url} 

% Define color commands for different collaborators
\newcommand{\JX}[1]{\textcolor{red}{#1}}
\newcommand{\AB}[1]{\textcolor{blue}{#1}}
\newcommand{\HC}[1]{\textcolor{green}{#1}}

\title{From Abstract to Actionable: Pairwise Shapley Values for Explainable AI}
% Authors with affiliations
\author[1]{Jiaxin Xu\textsuperscript{†}}
\author[2]{Hung Chau}
\author[2,*]{Angela Burden}
\affil[1]{University of Notre Dame, USA}
\affil[2]{Zillow Group, USA}
\affil[*]{Corresponding author: \texttt{angelabu@zillow.com}}

\date{} % Removes the default date

\begin{document}
\maketitle
% Footnote with symbol
\renewcommand{\thefootnote}{\textsuperscript{†}}
\footnotetext{This work was done during Jiaxin Xu’s internship at Zillow Group.}

% Alternative titles:

% 2. Pairwise Shapley Values: A Framework for Intuitive and Efficient Explainable AI

% 3. PairSHAP: Enhancing Interpretability and Accuracy of Machine Learning Feature Attribution

\begin{abstract}
Explainable AI (XAI) is critical for ensuring transparency, accountability and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios—including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent AI systems and advance the real-world applicability of XAI. 
\end{abstract}


\textbf{Keywords}: Explainable AI, Model Interpretability, Model-Agnostic Explanations, Feature Attribution, Shapley Values

% \textbf{TL;DR (a short sentence describing your paper)}: We introduce Pairwise Shapley Values, a framework enhancing interpretability and efficiency in explainable AI through explicit and human-relatable feature attributions.

\section{Main}
% \textcolor{red}{
%     \begin{enumerate}
%     \item Background of Feature Attribution:
%         \begin{enumerate}
%         \item Discuss the importance of explainable AI (XAI) and feature attribution in machine learning and its applications (e.g., Understanding and Trust; Accountability and Monitoring; Legislation and Fairness).
%         \item Introduce Shapley values and their role/popularity/application in XAI (+ Prior Work).
%         \end{enumerate}
%     \item Problem and Issues:
%         \begin{enumerate}
%         \item Highlight the challenges with current Shapley value methods, such as interpretability (the explanation itself is difficult to explain; human understandable) and accuracy (true to the model (marginal, SHAP) vs true to the data (conditional)).
%         \item Explain the need for a more intuitive and accurate feature attribution method.
%         \end{enumerate}
%     \item Our Proposal (Pairwise Shapley): \textcolor{green}{Just a note: Some publications require the explicit inclusion of research questions that the paper aims to address.}
%         \begin{enumerate}
%         \item Introduce the Pairwise Shapley Values method (high level; section 3 is more in detail).
%         \item Summarize the key findings, emphasizing the method's interpretability and accuracy.\AB{I might mention the limitations also here}
%         \end{enumerate}
%     \end{enumerate}
% }

With the rapid advancement and widespread adoption of increasingly complex artificial intelligence (AI) black-box models across diverse applications, ensuring model outputs are transparent and understandable is becoming crucial, particularly when they have the potential to significantly impact people's lives. Consequently, Explainable Artificial Intelligence (XAI) has become an essential component of modern machine learning (ML)~\cite{arrieta2020explainable,das2020opportunities,adadi2018peeking}. The importance of XAI centers around three key aspects: (i) building user understanding and trust, (ii) promoting stakeholder accountability by making AI systems auditable, and (iii) supporting adherence to legal requirements and fairness standards~\cite{dwivedi2023explainable,saeed2023explainable,tjoa2020survey}. Among diverse XAI methods~\cite{arrieta2020explainable,das2020opportunities,adadi2018peeking,dwivedi2023explainable,saeed2023explainable,tjoa2020survey}, feature attribution~\cite{zhou2022feature,lundberg2018consistent,bilodeau2024impossibility} plays a pivotal role by identifying and quantifying the contribution of individual input features to a model's output, clarifying their influence on model predictions at both global and local levels. A common subset of feature attribution is additive feature attribution, where the attribution quantities sum to a specific value, such as the model’s prediction~\cite{lundberg2017unified}. Shapley values, derived from cooperative game theory~\cite{kuhn1953contributions}, are recognized for their mathematical fairness and consistency in assigning contributions to features by evaluating all possible combinations~\cite{sundararajan2020many}. To unify additive feature attribution methods, Lundberg and Lee~\cite{lundberg2017unified} introduced SHAP (SHapley Additive exPlanations), a framework that satisfies desirable properties like consistency and local accuracy, making it effective for interpreting models of increasing complexity.

Despite their theoretical appeal, Shapley value estimation methods in XAI face significant challenges in practical interpretability. Traditional explanations based on feature attribution can be complex and difficult for users to interpret. For instance, as shown in Fig. \ref{Fig1}, traditional feature attribution explanation is inherently \textit{implicit}, as it often relies on sampling from empirical feature distributions using reference explicands \( \textit{\textbf{X}}' \) (also referred to as the background dataset)~\cite{lundberg2017unified}. This opacity stems from the abstract baseline value \(\mathbb{E}[f(\textit{\textbf{X}}')]\), which represents the feature attribution when no features of target explicand \( x \) are known. Additionally, Shapley values for individual features, such as \(x_1\) (\(\phi_1\)), are computed from expectations based on the empirical feature distribution (\(\mathbb{E}[f(\textit{\textbf{X}}')\mid \textit{\textbf{X}}'_{:,1} = x_1]\)), obscuring the specific reference values. This abstraction makes it difficult to relate explanations to tangible comparisons, limiting their practical utility. Furthermore, while the Shapley value is uniquely defined by certain axioms~\cite{kuhn1953contributions}, its application in model explanation varies based on how the model, training data, and explanation context are utilized, leading to inconsistent outputs which challenges the applicability of its theoretical uniqueness~\cite{sundararajan2020many}. Additionally, Shapley value methods often face a trade-off between being true to the model (e.g., marginal Shapley values) and being true to the data (e.g., conditional Shapley values), complicating the attribution process and further reducing interpretability~\cite{chen2020true}. These challenges underscore the need for a more intuitive and accurate feature attribution method that balances model fidelity with human interpretability.

\begin{figure}[ht]
\centering
\includegraphics[width=1\linewidth]{Fig1_v1.pdf}
\caption{\label{Fig1} 
Shapley value attribution comparison between traditional implicit methods using empirical feature distributions and the proposed explicit pair selection method.}

% Shapley value attribution to a target explicand \(x\). For traditional implicit feature attribution, the explanation begins at the base value \(\mathbb{E}[f(\textit{\textbf{X}}')]\), representing the model's prediction when no features are known, and progresses to the current output \(f(x)\) using reference explicands \(\textit{\textbf{X}}'\) (also known as the background data) and \(x\). The Shapley value for feature \(x_1\) (\(\phi_1\)) of the target explicand is calculated from \(\mathbb{E}[f(\textit{\textbf{X}}')|\textit{\textbf{X}}'_{:,1} = x_1]\). The Shapley values for the remaining features (\(\phi_2, \ldots, \phi_4\)) are omitted for simplicity. This implicit approach often relies on sampling from the empirical feature distribution, making the explanations unintuitive due to the abstract nature of expectation. In contrast, our proposed explicit feature attribution, based on pair selection, introduces a concrete reference explicand (\(x'\)) chosen using a specific pair selection criterion. The base value is set as \(f(x')\), and Shapley values for each feature are computed using single-value imputation rather than abstract distributional imputation. For instance, the Shapley value for feature \(x_1\) (\(\phi'_1\)) can be calculated from \(f(x'|x'_1 = x_1)\). This explicit approach eliminates reliance on distributional sampling, enhancing interpretability and grounding the explanation in tangible comparisons. The key difference between implicit and explicit methods is highlighted with dashed and solid boxes, respectively.

\end{figure}

In cognitive psychology, analogical reasoning refers to the process by which individuals understand novel situations by relating them to similar and familiar ones, thereby facilitating learning and problem-solving~\cite{gentner2003analogical, goswami1996analogical}. For example, in real estate valuation, homes are valued via a comparative market analysis (CMA)~\cite{kahr2006real, yeh2018building, pagourtzi2003real}. In a CMA, a property's value (the subject home) is estimated by comparing it to recently sold properties with similar attributes (comparable homes), such as location, size, and condition. To determine a fair market price for the subject home, the sale price of the comparable home is adjusted for differences in amenities, location and time. Similarly, in drug design, the concept of a ``comparable reference'' can be applied in developing HIV treatments. Comparisons often focus on protease inhibitors sharing a common functional group essential for activity. Once this critical functional group is established, medicinal chemists can systematically modify other parts of the molecule to enhance efficacy, selectivity, and pharmacokinetics. Inspired by these human evaluative strategies, our proposed \textbf{Pairwise Shapley Value} method introduces a tangible reference point into the feature attribution process (Fig. \ref{Fig1}). By selecting a comparable baseline from the background dataset that closely matches the target explicand, the pairwise method assigns attributions to the differences in feature values between the target explicand and its reference. Pairwise Shapley values are \textit{explicit} as they ground explanations in concrete, relatable comparisons, mimicking the human decision process. Furthermore, by conditioning on certain important features in the input space, the risk of generating off-manifold data is reduced and complex feature interactions are eliminated, leading to simpler and more intuitive explanations. 


% \JX{(TODO--LIMITATIONS -- Should we move it to the last Section-Discussion?)} However, our Pairwise Shapley Values method has its limitations. First, identifying an appropriate reference baseline for certain instances of different applications may be challenging, particularly in high-dimensional or sparse datasets. Second, the computational cost of determining pairwise baselines and computing Shapley values may increase with larger datasets, limiting scalability in real-time or large-scale applications. \AB{yes, I think you are right, we should}

% \JX{TODO -- \textbf{Our Contributions:} See Section \ref{Advantages}. Should we move it up here?}

% To empirically demonstrate the advantage of the Pairwise Shapley Value method, we conduct experiments
% across diverse regression and classification tasks, including real estate pricing, polymer property prediction, and drug discovery. Results highlight several key benefits over traditional Shapley value estimation methods:

% In this work, we evaluate Pairwise Shapley Values across regression and classification tasks, including real estate pricing, polymer property prediction, and drug discovery, demonstrating key benefits over traditional Shapley value estimation methods:

% \begin{itemize}
%     \item \textit{Intuitive Explanations with Concrete Baselines:} Grounding attributions in comparisons with actual data points enhances interpretability and human understanding.
%     \item \textit{Simplification and Efficiency through Conditioning:} Pair selection isolates relevant differences, simplifying explanations and reducing computations. By ensuring reference points are close in feature space, the method avoids unrealistic interpolations, balances data fidelity with model alignment, and addresses feature multicollinearity, making it flexible and precise.
%     \item \textit{Computational Efficiency with Single-Value Imputation:} Eliminating distributional sampling significantly speeds up calculations while maintaining accuracy.
% \end{itemize}

In summary, our key contributions are:
\begin{itemize}
    \item Pairwise Shapley Values, a novel Shapley values-based approach that enhances human-relatable explainability through pairwise comparisons, offering greater interpretability, computational efficiency, and robustness.

    \item Extensive benchmarking of Pairwise Shapley Values against prevalent Shapley-based post-hoc explainability methods, evaluated on real-world datasets spanning real estate pricing, polymer property prediction, and drug discovery.

    % \item A structured protocol for data processing, designed to streamline interpretability and enhance the explainability of black-box model outputs.
\end{itemize}

% \JX{TODO: Contribution Summary}
% \begin{itemize}
%     \item \textit{Enhanced Human Understanding with a Non-Abstract Baseline:} Grounding explanations in concrete comparisons with actual data points makes them more intuitive and relatable for users.

%     \item \textit{Conditioning for Simplification, Efficiency and Accuracy:} Conditioning on specific features through pair selection isolates relevant differences, simplifying explanations and reducing the number of computations required. By focusing only on feature differences between the reference and target instances, the method reduces the influence of complex feature interactions, ensuring explanations are concise and focused. Additionally, selecting a reference close to the target instance in feature space ensures that interpolations remain within the data manifold, avoiding unrealistic or off-manifold data points. This approach achieves a balance between being "true to the data" (through pair selection that considers feature multicollinearity and can be tailored for specific use cases) and "true to the model" (through single-value imputation, assuming feature independence), offering both flexibility and precision across applications.

%     \item \textit{Single-Value Imputation for Computational Efficiency:} By removing the need for distributional sampling, the method significantly reduces computational overhead, enabling faster calculations without compromising accuracy.
% \end{itemize}

\section{Related Work}
\subsection{Feature Attribution Methods}
% \textcolor{red}{
% \begin{enumerate}
% \item Definition and Scope:
%     \begin{enumerate}
%     \item A specific subset of XAI methods that focus on quantifying the contribution or importance of individual features to a model's prediction.
%     \item Typically local, providing feature importance for individual predictions, but results can be aggregated for global insights.
%     \end{enumerate}
% \item Addictive feature attribution: Attributions sum to a specific value, such as the model’s prediction. \AB{Maybe mention that some are dependent on the model structure whereas others are model agnostic} Examples:
%     \begin{enumerate}
%     \item LIME: feature coefficient from local linear surrogate models \AB{model agnostic}
%     \item DeepLIFT: difference from reference activation of each neuron \AB{I might generalize a bit here rather than just mention DeepLift - maybe methods that extract information from the model structure such as DeepLift}
%     \item Shapley value: cooperative game theory \AB{I would split into model agnostic vs model specific}
% \end{enumerate}
% \end{enumerate}
% }

Feature attribution methods quantify individual feature contributions to an ML model’s prediction, either locally (for specific instances) or globally (aggregated across all predictions). Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s output, are notable for their clear and interpretable decomposition of predictions.
Feature attribution methods can be categorized into (i) model-agnostic methods, which generate explanations by sampling predictions from a black-box model. This includes method such as LIME (Local Interpretable Model-agnostic Explanations)~\cite{ribeiro2016should, dieber2020model, salih2024perspective,nguyen2021evaluation}, SHAP~\cite{lundberg2017unified, salih2024perspective,nguyen2021evaluation} and Shapley sampling values~\cite{strumbelj2014}; and (ii) model-dependent methods, which leverages the specific ML model architecture. This includes techniques such as using linear models, Saliency Maps~\cite{simonyan2013deep, mundhenk2019efficient, samuel2021evaluation, alqaraawi2020evaluating}, DeepLift~\cite{shrikumar2017learning, shrikumar2016not}, and Integrated Gradients~\cite{sundararajan2017axiomatic, vcik2021explaining, singh2023choose,wang2024gradient}.

% Feature attribution is a key subset of XAI that quantifies the contribution or importance of individual features to an ML model’s prediction. Typically, these methods operate locally, offering feature importance for individual predictions, but results can be aggregated to provide global insights into model behavior. Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s prediction, are particularly notable for their clear and interpretable decomposition of predictions. Feature attribution methods can be either model-agnostic, treating the model as a black box, or model-dependent, leveraging the specific structure of the ML model. Prominent approaches to feature attribution include:

% \textbf{Surrogate Models} involve fitting inherently interpretable models, such as simpler decision trees,  linear models or metamodels, to approximate complex model behaviors, enabling a simplified assessment of feature importance~\cite{craven1995extracting, paez2024understanding, alaa2019demystifying}.

% \textbf{Gradient-Based Methods} \AB{analyze the gradients of a model's output with respect to its input features to determine their importance, and backpropagation exploits the chain rule to efficiently calculate gradients during the backward pass through the neural network. Examples of gradient based methods are:
% \item Saliency Maps: gradients of a model output are computed with respect to the input. The gradients highlight which input features (pixels) are most influential to the outcome~\cite{simonyan2013deep, mundhenk2019efficient, samuel2021evaluation, alqaraawi2020evaluating}.
% \item Integrated Gradients: model gradients are (approximately) integrated along a path from a baseline input to the actual input to determine the contribution of each input feature~\cite{sundararajan2017axiomatic, vcik2021explaining, singh2023choose,wang2024gradient}.
% \item DeepLIFT (Deep Learning Important FeaTures): similar to integrated gradients, feature relevance scores are derived by comparing activations of the neurons in a neural network with those of a baseline activation; and differences are propagated back through the layers of the NN~\cite{shrikumar2017learning, shrikumar2016not}.
% \item  Layer-wise Relevance Propagation (LRP), works the same way as DeepLift but with a zero baseline.~\cite{bach2015pixel, vcik2021explaining}}

% analyze the gradients of the model's output with respect to input features to determine their importance. For example, Saliency Maps compute gradients to highlight input features most influential to the output~\cite{simonyan2013deep, mundhenk2019efficient, samuel2021evaluation, alqaraawi2020evaluating}. Integrated Gradients provide a comprehensive attribution by accumulating gradients along a path from a baseline input to the actual input~\cite{sundararajan2017axiomatic, vcik2021explaining, singh2023choose,wang2024gradient}.

% \textbf{Perturbation-Based Methods} evaluate feature importance by observing changes in the model's output when input features are altered. Examles are:
% \item LIME (Local Interpretable Model-agnostic Explanations): generates a simple interpretable model to approximate the output of a Black-box ML model locally around a specific prediction. Feature importances for a data instance are derived from the surrogate model~\cite{ribeiro2016should, dieber2020model, salih2024perspective,nguyen2021evaluation}. 
% \item SHAP leverages Shapley values from cooperative game theory to fairly distribute a prediction among input features, ensuring consistency and local accuracy~\cite{lundberg2017unified, salih2024perspective,nguyen2021evaluation}.




% \textbf{Example-Based Explanations} identify minimal feature modifications required to change a model’s prediction, emphasizing impactful features. Counterfactual Explanations are a common approach in this category, which highlight how small changes to input features would alter outcomes~\cite{wachter2017counterfactual, guidotti2024counterfactual, verma2020counterfactual,mothilal2020explaining, goyal2019counterfactual,verma2024counterfactual,slack2021counterfactual}.
% % \AB{They don't all offer actionable outcomes - only actionable recourse models, so I've removed that last part} 


% Among these, Shapley value-based methods, such as SHAP, have gained prominence due to their solid theoretical foundation and desirable properties~\cite{kuhn1953contributions, lundberg2017unified, sundararajan2020many}. However, their practical application can be complex and computationally intensive, resulting in a variety of estimation algorithms and adaptations to improve efficiency and applicability~\cite{chen2023algorithms}.

\subsection{Shapley Values}

Shapley values~\cite{shapley1953value}, originating from cooperative game theory, describe how the payout of a game should be divided fairly among a coalition of players, and have several useful properties:
(i) Efficiency: individual player payouts must sum to the total payout minus the original (null player) payout,
(ii) Symmetry: when two players contribute equally to a game (over all possible player coalitions), they should be assigned the same payout,
(iii) Dummy: a player that does not influence the game should be assigned zero payout,
(iv) Additivity: For a game that includes multiple sub-games and a summed total payout, the total payout for a single player must equal the sum of their payouts for each individual sub-game.

The Shapley value for a player \( i \) is defined as:
\begin{equation}
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \, (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right],
\label{eq:shapleyvalues}
\end{equation}
where \( N \) is the set of all players, \( S \) is a subset of \( N \) players not containing player \( i \), \( v(S) \) represents the payout (value function) based on the subset of players \( S \), and \( |S| \) denotes the cardinality of set \( S \). Equation (\ref{eq:shapleyvalues}) calculates the weighted average of the marginal contributions of player \( i \) across all possible subsets \( S \). 

This technique has been adapted to derive explanations from black-box ML models by analogizing the model prediction as the game payout and the model features as the game players~\cite{strumbelj2014, lundberg2017unified}. However, computing Shapley values in ML settings can be complex and computationally intensive, which has spawned a variety of estimation algorithms~\cite{chen2023algorithms}.

\subsection{Feature Removal in Shapley Value Estimation}
% \textcolor{red}{Discuss different approaches to feature removal in shapley value estimation, such as single value and distributional (uniform, marginal, and conditional).}

% The Shapley value for a feature \( i \) is defined as:
% \begin{equation}
% \phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \, (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right],
% \label{eq:shapleyvalues}
% \end{equation}
% where \( N \) is the set of all features, \( S \) is a subset of \( N \) not containing feature \( i \), \( v(S) \) represents the value function (e.g., model prediction) based on the subset \( S \), and \( |S| \) denotes the cardinality of set \( S \). Equation (\ref{eq:shapleyvalues}) calculates the weighted average of the marginal contributions of feature \( i \) across all possible subsets \( S \). 

During Shapley value estimation, feature removal simulates the absence of a feature by replacing or marginalizing its effect when computing the value function \( v(S) \)~\cite{chen2023algorithms}. The method of feature removal can significantly influence the resulting feature attribution values and their interpretation~\cite{sundararajan2020many, kumar2020problems, merrick2019games}. Two common strategies are single-value removal and distributional removal~\cite{chen2023algorithms}.

Single-value removal replaces the feature to be removed with a fixed reference value. The set of fixed reference values is often termed the baseline~\cite{sundararajan2020many}. A common choice is zero, but other values like the mean or median of the feature can also be used. \( v(S) \) is computed by setting the features not in \( S \) (denoted as \(\bar{S}\)) to their corresponding baseline values. While straightforward, this method may introduce unrealistic data instances in the Shapley value estimations that can result in misleading attributions~\cite{aas2021, Zhao_OOdistribution}. Rather than substitute a feature with a single value, distributional removal integrates over a range of possible values, considering the feature's empirical distribution. There are various ways to do this, we examine 3 common methods:

% \textbf{Single-Value Removal:} This approach replaces the feature to be removed with a fixed reference value. The set of fixed reference values is often termed the baseline~\cite{sundararajan2020many}. A common choice is zero, but other values like the mean or median of the feature can also comprise the baseline. While straightforward, this method may introduce unrealistic data instances in the Shapley value estimations that can result in misleading attributions.

% \textbf{Distributional Removal:} Rather than substitute a feature with a single value, distributional removal integrates over a range of possible values, considering the feature's empirical distribution~\cite{chen2023algorithms}. There are various ways to do this, we examine 3 common methods:

\begin{itemize}
    \item \textit{Uniform Distribution:} A feature is replaced with values sampled uniformly across its range. This technique assumes all feature values are equally probable, which might not align with the actual data distribution.
    \item \textit{Marginal Distribution:} A feature is replaced by sampling its marginal distribution. This approach breaks dependencies between features, potentially leading to implausible data points in the Shapley value estimation. The value function, \( v(S) \), is then the expected value of the model output when features not in \( S \) are marginalized independently:
    
    \begin{equation}
    v(S) = \mathbb{E}_{p(x_{\bar{S}})}[f(x_S, x_{\bar{S}})].
    \label{eq:shapleyvalues-marginal}
    \end{equation}
    
    \item \textit{Conditional Distribution:} A feature is replaced by sampling its conditional distribution relative to other features. This method preserves dependencies among features, resulting in more realistic data instances in the Shapley value estimation but at the cost of increased computational complexity. Furthermore, multicollinearity between features may cause non-zero attributions for features with no influence on the model outcome~\cite{chen2023algorithms}. The value function is the expected value given the features in \( S \), considering their dependencies:
    
    \begin{equation}
    v(S) = \mathbb{E}_{p(x_{\bar{S}} \mid x_S)}[f(x_S, x_{\bar{S}})].
    \label{eq:shapleyvalues-conditional}
    \end{equation}
\end{itemize}

% - \textit{Uniform Distribution:} A feature is replaced with values sampled uniformly across its range. This technique assumes all feature values are equally probable, which might not align with the actual data distribution.

% - \textit{Marginal Distribution:} A feature is replaced by sampling its marginal distribution. This approach breaks dependencies between features, potentially leading to implausible data points in the Shapley value estimation.

% - \textit{Conditional Distribution:} A feature is replaced by sampling its conditional distribution relative to other features. This method preserves dependencies among features, resulting in more realistic data instances in the Shapley value estimation but at the cost of increased computational complexity. Furthermore, multicollinearity between features may cause non-zero attributions for features with no influence on the model outcome.

% In the context of feature removal, for single-value removal, \( v(S) \) is computed by setting the features not in \( S \) (denoted as \(\bar{S}\)) to a fixed baseline value. For distributional removal, with marginal distribution, \( v(S) \) is the expected value of the model output when features not in \( S \) are marginalized independently:
% \begin{equation}
% v(S) = \mathbb{E}_{p(x_{\bar{S}})}[f(x_S, x_{\bar{S}})].
% \label{eq:shapleyvalues-marginal}
% \end{equation}
% With conditional distribution, \( v(S) \) is the expected value given the features in \( S \), considering their dependencies:
% \begin{equation}
% v(S) = \mathbb{E}_{p(x_{\bar{S}} \mid x_S)}[f(x_S, x_{\bar{S}})].
% \label{eq:shapleyvalues-conditional}
% \end{equation}

Selecting the appropriate feature removal approach is important for generating meaningful and reliable Shapley value-based explanations.


\section{Pairwise Shapley Values}

Our framework integrates a ML base model with a pair selection algorithm and feature removal based on explicit pairs (the target explicand and the similar reference explicand), as illustrated in Fig. \ref{Fig1}.

\subsection{Pair Selection Algorithm} 
To provide flexibility and balance between general applicability and domain-specific precision, we employ three pair selection algorithms to identify a reference explicand for a target explicand: 
\begin{itemize}
\item  \textit{Random:} Selecting a reference explicand (\( x' \)) randomly from the background dataset (\(\textit{X}'\)). This serves as a baseline to compare against more structured selection strategies.

\item \textit{Similar:} Choosing a reference explicand that closely matches the target explicand based on common similarity metrics, such as cosine similarity, Euclidean distance, or correlation-based measures. These algorithms represent a generalizable method applicable to any use case without requiring domain knowledge.

\item \textit{Comparable:} Defining similarity based on specific application criteria. This is a specialized approach tailored to different domains, where domain knowledge is needed to identify and condition on the ``more important" factors, ensuring relevance and interpretability in context. For example, in a CMA, comparable homes are typically selected based on similarity in attributes such as sale timing, location, size, and condition~\cite{kahr2006real, yeh2018building, pagourtzi2003real}. 

% \AB{I think this info might go somewhere earlier in the paper - thinking about it}  Appraisers evaluate a home's price by referencing a similar property sold recently, located in proximity to amenities, school districts, and desirable neighborhood features, and comparable in size (total square footage, number of bedrooms, and bathrooms) and condition (age, maintenance level, and recent renovations or upgrades) \JX{(TODO: add comps details in the Method section?@AB[TODO; notations?])}. Similarly, in drug design, the concept of a ``comparable reference'' can be applied. For instance, in developing HIV treatments, comparisons often focus on protease inhibitors sharing a common functional group essential for activity. Many HIV protease inhibitors, for example, incorporate a hydroxyethylamine scaffold that mimics the enzyme's transition state, effectively inhibiting its function. Once this critical functional group is established, medicinal chemists can systematically modify other parts of the molecule to enhance efficacy, selectivity, and pharmacokinetics. This approach ensures meaningful comparisons among drug analogs, focusing on structural differences that directly influence therapeutic performance.
\end{itemize}

\subsection{Shapley Value Estimation Based on Explicit Pairs}
\label{ExplicitPairs}

After selecting an explicit reference, we use single-value imputation to calculate the value function \( v(S) \). This involves replacing the removed features (\( \bar{S} \)) in the target instance (\( x \)) with the corresponding features in the reference instance (\( x' \)) and observing the effect on the model's prediction:

\begin{equation}
v(S) = f(x_S, x'_{\bar{S}}).
\label{eq:shapleyvalues-conditional-1}
\end{equation}

We compute the Shapley values \(\phi_i(v)\) (denoted as \(\phi'_i\) in Fig. \ref{Fig1}) using the KernelSHAP~\cite{lundberg2017unified}, a popular model-agnostic approach that approximates Shapley values by solving a weighted least squares problem. This approximation is necessary in practice because calculating exact Shapley values requires evaluating all \( 2^N \) possible feature subsets, which is computationally infeasible for models with a large number of features. However, although not implemented in this work, we note that conditioning on large subsets of features with zero variance, that we coin ``dummy pairs", where \( x_i = x'_i \), in our pair selection algorithm, allows us to omit them from the stack of evaluated feature permutations, thereby significantly decreasing the runtime of the Pairwise Shapley algorithm.

As well as adhering to the Shapley value properties (efficiency, symmetry, dummy, additivity), the pairwise method has several desirable properties not always encountered in the other Shapley value approximation methods discussed:
\begin{enumerate}
\item  \textit{Additive inverse}: Feature attributions derived by swapping the baseline and explicand have the additive inverse contribution, \(  \phi_i ( x, x' ) = -\phi_i ( x', x ) \). For example, in the home valuation problem, the contribution to the difference in home value attributed to the difference in square foot between a subject and comparable home would be \(  \phi_i \) dollars comparing subject to comparable home and \( -\phi_i \) dollars comparing comparable home to subject.
\item \textit{Dummy pairs}: Features the same in both baseline and explicand are named ``dummy pairs''. Dummy pairs are never attributed any value, but dictate the locality of the value function for evaluation. They can be considered locally independent from the remaining feature set.
\item \textit{Single feature contribution}: It follows that when differences between data pairs are isolated to a single feature, that feature (by elimination) is also independent. All changes in the model outcome will therefore be attributed to changes in that feature. 
% \item  \textit{Missing feature independence}:  Missing features are modeled as dummy pairs and therefore considered independent. This is a desirable property for modeling feature absence.
\item  \textit{Logical attribution evaluation}: Where features are expected to contribute monotonically to the output (for example, square foot) we can easily evaluate our predictive ML model for expected behavior with the pairwise method, on real data.
\end{enumerate}

% \subsection{Advantages of Pairwise Shapley Values}
% \label{Advantages}

% The Pairwise Shapley Values method addresses several key limitations of traditional Shapley value estimation methods while offering distinct advantages that enhance its utility and interpretability:

% \begin{enumerate}
%     \item \textit{Enhanced Human Understanding with a Non-Abstract Baseline:} Grounding explanations in concrete comparisons with actual data points makes them more intuitive and relatable for users.

%     \item \textit{Conditioning for Simplification, Efficiency and Accuracy:} Conditioning on specific features through pair selection isolates relevant differences, simplifying explanations and reducing the number of computations required. By focusing only on feature differences between the reference and target instances, the method reduces the influence of complex feature interactions, ensuring explanations are concise and focused. Additionally, selecting a reference close to the target instance in feature space ensures that interpolations remain within the data manifold, avoiding unrealistic or off-manifold data points. This approach achieves a balance between being "true to the data" (through pair selection that considers feature multicollinearity and can be tailored for specific use cases) and "true to the model" (through single-value imputation, assuming feature independence), offering both flexibility and precision across applications.

%     \item \textit{Single-Value Imputation for Computational Efficiency:} By removing the need for distributional sampling, the method significantly reduces computational overhead, enabling faster calculations without compromising accuracy.
% \end{enumerate}


% \subsection{Advantages of Pairwise Shapley Values:}

% The Pairwise Shapley Values method addresses several key limitations of traditional Shapley value estimation methods while offering distinct advantages that enhance its utility and interpretability:
% \begin{enumerate}

% \item \textit{Enhanced Human Understanding with Non-Abstract Baseline:} Grounding explanations in concrete comparisons makes them more understandable to users; Using actual data points as baselines avoids the pitfalls of abstract or unrealistic reference values; 

% \item \textit{Conditioning for Simplification and Accuracy:} [Simplification] Conditioning on specific features simplifies explanations by isolating relevant differences; By conditioning on shared features between the reference and target instances, the method reduces the influence of complex feature interactions, allowing the explanation to focus solely on pertinent differences; [Accuracy] electing a reference close to the target instance in feature space, the method ensures that interpolations remain within the data manifold, avoiding the generation of unrealistic or off-manifold data points; [Trade-off] Achieve a balance between "True to the data" (through pair selection considering the feature multicollinearity [also, it is flexible and can be tailored for different use case]) and "True to the model" (through single value imputation, assuming feature independence)

% \item \textit{Single-Value Imputation for Computational Efficiency:} Eliminates the need for distributional sampling, greatly reducing computational overhead.

% \end{enumerate}



% - \textit{Simplified Explanations:} Fewer features require attribution, making the explanation more concise.

% - \textit{Computational Efficiency:} Eliminates the need for distributional sampling, reducing computational overhead.

% - \textit{Enhanced Interpretability:} Grounding explanations in concrete comparisons makes them more understandable to users.

% - \textit{Improved Accuracy:} Conditioning on certain features mitigates the influence of complex interactions, focusing the explanation on pertinent differences.

% \textbf{Framework Workflow:}

% \begin{enumerate}
% \item Choose input features to condition on.

% \item Select a reference data point that matches the target instance on the conditioned features.

% \item Apply the baseline Shapley method using the reference data point as the baseline.

% \item Assign the Shapley value attributions to the differences in feature values between the reference data and the target instance.

% \item The predicted value of the target instance is the sum of the reference data point's prediction and the pairwise Shapley values.
% \end{enumerate}

% \textbf{Addressing Issues in Previous Methods:}

% \begin{enumerate}
% \item \textit{Proximity in Feature Space:} Selecting a reference close to the target instance ensures that interpolations remain within the data manifold.

% \item \textit{Conditioning for Simplification:} Conditioning on specific features simplifies explanations by isolating relevant differences.

% \item \textit{Enhanced Human Understanding:} Grounding explanations in concrete, comparable instances makes them more intuitive.

% \item \textit{Non-Abstract Baseline:} Using actual data points as baselines avoids the pitfalls of abstract or unrealistic reference values.

% \item \textit{Assumption of Conditional Independence:} The method assumes independence only among the conditioned features, aligning with realistic data distributions.
% \end{enumerate}

% \subsection{Advantages and Addressing Issues in Previous Methods}

% The Pairwise Shapley Values method addresses several key limitations of traditional Shapley value estimation methods while offering distinct advantages that enhance its utility and interpretability:

% First, by selecting a reference close to the target instance in feature space, the method ensures that interpolations remain within the data manifold, avoiding the generation of unrealistic or off-manifold data points. This proximity also allows for the conditioning of specific features, simplifying explanations by isolating relevant differences and reducing complexity. As a result, explanations are not only more concise but also easier for users to understand, grounding them in concrete, comparable instances rather than abstract baselines.

% Additionally, the use of actual data points as baselines avoids the pitfalls of abstract or unrealistic reference values, providing more meaningful and trustworthy explanations. By conditioning on shared features between the reference and target instances, the method reduces the influence of complex feature interactions, allowing the explanation to focus solely on pertinent differences. This improves the accuracy of feature attributions while ensuring the explanations remain intuitive.

% The computational efficiency of the Pairwise Shapley Values method further sets it apart. By eliminating the need for distributional sampling, the method reduces computational overhead, enabling faster calculations without compromising accuracy. Furthermore, the method assumes conditional independence only among the conditioned features, aligning closely with realistic data distributions and enhancing the validity of the resulting attributions.

% In summary, the Pairwise Shapley Values method combines simplified explanations, enhanced computational efficiency, improved accuracy, and grounded interpretability, making it a robust alternative to traditional Shapley value estimation methods. By addressing issues such as off-manifold data generation, complex feature interactions, and abstract baselines, it provides a reliable and user-friendly framework for feature attribution in machine learning models.


% \subsection{Pairwise Shapley Values}

% The pairwise method was motivated by home valuation models and the need to create explainable AVMs (Automated Valuation Machine). Home prices are dictated by a complex set of variables including local and national economic factors, climate and more tangible features such as home attributes and nearby amenities. To create accurate AVMs, models must be designed with a wide range of inputs that capture non-linearity and complex patterns in the data. However, the more complex the model and input features the more difficult it is to explain the model output in a human understandable way.

% To make our home value predictions explainable we turn to how humans think about the problem. In the home valuation domain, the industry standard for agents and appraisers is to create a CMA (comparative market analysis). This involves selecting comparable homes (comps) that have sold recently nearby to the home we want to value (the subject home) and adjusting their sale price to account for differences in features between the subject home and the comparable home. For example…. \textcolor{red}{[Generalize this ideation to broader scenarios first and then use CMA as an example, maybe also add some other counterfactual evaluation examples]}

% By choosing homes that are close by in both physical distance and feature space, the human valuer conditions on many of the complex inputs that affect the value of a home –such as economic and climate factors – and use the comparable home as a baseline for their valuation, thereby simplifying the valuation process by removing the need to estimate value changes due to differences in .. etc. When choosing homes with the same features the valuer generally implicitly assumes the identical features are independent from the broader feature set and does not attribute value to them.

% Motivated by the CMA method, we propose using pairwise data points (in our case subject home and comparable home) that are nearby in feature space, where we can condition on more complicated model inputs. The pairwise Shapley method uses similar methodology to the baseline method, however the baseline is now a tangible data point (comparable home) with real feature values. This makes the output conceptually different as attributions are assigned to real differences in the feature values from data pairs that are intentionally close in the feature manifold.

% \begin{enumerate}
% \item	Choose input features to condition on
% \item	Choose a data point that conditions on the above features and is similar to the explicand. This will be the reference baseline.
% \item	Apply the baseline Shapley method using the reference data point as the baseline.
% \item	Assign the Shapley value attributions to the differences in feature values between reference data and the explicand. 
% \item	The predicted value of the explicand is the addition of the empty set value (the prediction of the reference data point) and the pairwise Shapley values.
% \end{enumerate}

%  How does this address issues in the previous methods
% \begin{enumerate}
% \item	Close by in feature space – not off manifold
% \item	Conditioning allows us to simplify explanations
% \item	Contractive – easier for humans to understand.
% \item	Non-abstract baseline.
% \item	Only assumes that conditioned on features are independent.
% \end{enumerate}

% \textcolor{red}{[FIGURE-0: Pairwise Shapley Method Workflow]}
% \textcolor{red}{
% \begin{enumerate}
% \item Overall Model Architecture: ML base model + comps + feature removal
% \item Comps [Random, Similar, and Comparable]
% \item Feature removal \AB{Note that in this section i would talk about the advantage of having matched features between subject and comp, i.e. easier to explain as fewer features have attributions, faster to compute, easier to understand for the user. }
% \item metrics
% \end{enumerate}
% }

\section{Results}

\subsection{Base Machine Learning Models' Performance}
\label{BaseML}
% \textcolor{red}{
% \begin{enumerate}
% \item Present the performance of the base machine learning models on the home price prediction and material/drug property prediction datasets.[regression + classification] Fig \ref{Fig4-1}
% \item Discuss iterations of the base model feature engineering based on XAI feedback [+appendix]
% \end{enumerate}
% }


% Before evaluating the empirical behavior of Pairwise Shapley as a model-agnostic feature attribution method, we first present the performance of the base machine learning (ML) models on three datasets: (1) the King County home price prediction dataset~\cite{andykrause_2019_github} (referred to as ``King County''), (2) the polymer dielectric constant prediction dataset~\cite{kuenneth2021polymer} (referred to as ``Polymer''), and (3) the drug HIV replication inhibition ability prediction dataset~\cite{wu2018moleculenet} (referred to as ``Drug''). These datasets encompass both regression and classification tasks, offering a testbed for our feature attribution estimation framework. Detailed description for each dataset and task are provided in Section \ref{Methods-datasets}.

% Although the primary focus of this work is on model-agnostic feature attribution estimation, the quality of the underlying predictive model is important for generating reliable attribution estimation. Feature explanations inherently depend on the model and its predictions; hence, a sufficiently accurate predictive model is essential for meaningful and interpretable attributions. To ensure predictive performance and save time, we utilized the Tree-based Pipeline Optimization Tool (TPOT)~\cite{le2020scaling}, an automated machine learning (AutoML) tool, to automatically select the model pipeline and hyper-parameters for each dataset.

% Before evaluating the empirical behavior of Pairwise Shapley as a model-agnostic feature attribution method, we first require a set of predictive ML models.
% Although the primary focus of this work is on model-agnostic feature attribution estimation, the quality of the underlying predictive model is important for generating reliable attribution estimation. 
We first train predictive ML models to ensure reliable feature attributions. Since explanations depend on model predictions, a sufficiently accurate base model is essential. Feature explanations inherently depend on the model and its predictions; hence, a sufficiently accurate predictive model is essential for meaningful and interpretable attributions. We utilized the Tree-based Pipeline Optimization Tool (TPOT)~\cite{le2020scaling}, an automated ML (AutoML) tool, to automatically select the model pipeline and hyper-parameters for our datasets. In Table \ref{tab:baseperformance} we present the performance of the base ML models on three datasets: (1) the King County home price prediction dataset~\cite{andykrause_2019_github} (referred to as ``Home''), (2) the polymer dielectric constant prediction dataset~\cite{kuenneth2021polymer} (referred to as ``Polymer''), and (3) the drug HIV replication inhibition ability prediction dataset~\cite{wu2018moleculenet} (referred to as ``Drug''). These datasets encompass both regression and classification tasks, offering a testbed for our feature attribution estimation framework. Detailed description for each dataset and task are provided in Section \ref{Methods-datasets}.


\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Type} &  \textbf{Metric} & \textbf{Performance}& \textbf{Benchmark Performance} \\ \hline
Home& Regression & MdAPE & 0.106 & 0.100~\cite{moghimi2023rethinking}  \\ \hline
Polymer    & Regression & RMSE & 0.762 & 0.530~\cite{kuenneth2021polymer}\\ \hline
Drug       & Binary classification & ROC-AUC & 0.820 & 0.792~\cite{wu2018moleculenet} \\ \hline
\end{tabular}
\caption{Performance comparison of the base ML models and benchmarks across three datasets on the test data.}
 % The metrics are defined as follows: MdAPE (Median Absolute Percentage Error), RMSE (Root Mean Squared Error), and ROC-AUC (Receiver Operating Characteristic - Area Under the Curve).
\label{tab:baseperformance}
\end{table}

% Table \ref{tab:baseperformance} summarizes the prediction performance of the ML models selected by TPOT across the three datasets. 

The base models demonstrate competitive predictive accuracy compared with commonly used benchmarks, ensuring a adequately reliable foundation for feature attribution analysis. Details of the TPOT pipeline and model training are provided in Section \ref{Methods-basemodel}, and additional visualization of results are included in the Appendix \ref{Appendix-BaseModel}.

As an illustrative example, we also summarize a protocol for iterative explainable ML model development based on feature attribution feedback and domain knowledge, using the Home dataset as an example. Details can be found in Appendix \ref{Appendix-Protocol}. This protocol highlights how explainability-driven insights can guide enhancements in model performance and interpretability, demonstrating the complementary roles of predictive accuracy and interpretability in practical ML workflows. We hope this protocol serves as a useful guideline for researchers and practitioners in domains where both high model accuracy and interpretability are essential for informed decision-making and regulatory compliance.


% \subsection{Existing Feature Removal Methods Comparison}
\subsection{A Comparison of Existing Feature Removal Methods}


% \textcolor{red}{
% \begin{enumerate}
% \item Different feature importance results: global (Fig-\ref{Fig4-2-feature-importance}) + local (Fig-\ref{Fig4-3-local-exp}). 
% \item Different normalized feature attribution (sqft) [remove baselines] + hypothesis testing (Fig-\ref{Fig4-4-normalized}; Table \ref{tab:sqft_summary_statistics}). 
% \end{enumerate}
% }

% Global feature attributions provide insights into the importance of features across the entire dataset, aggregated over all instances, while local feature attributions focus on individual data points. 
Using the base ML models in Section \ref{BaseML}, we compare feature attribution results obtained from different feature removal methods in Shapley value estimation, evaluating both global (dataset-wide) and local (instance-level) explanations. We examine several established feature removal methods for Shapley value estimation, including single-value imputation methods using zeros (B0) and mean feature values (BM), uniform distribution imputation (UF), marginal distribution imputation using all training data (MA) or a K-means summary of the training data (MK), conditional distribution imputation using all training data (CA), and a model-specific method, TreeShap (TS), which is suited for tree-based ML models selected by the AutoML pipeline. Implementation details of these methods are listed in Section \ref{Implement-Existing}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{Fig4-2_boxplot.pdf}
\caption{\label{Fig4-2-feature-importance}Feature importance scores (mean absolute Shapley values) for features: (a) \texttt{grade}, (b) \texttt{city\_BELLEVUE}, and (c) \texttt{sqft}, derived from the base model on the test data of the Home dataset. The methods include single-value imputation using zeros (B0) and mean values (BM), uniform distribution imputation (UF), marginal distribution imputation using all training data (MA) or a K-means summary of the training data (MK), conditional distribution imputation using all training data (CA), and a model-specific method, TreeShap (TS).}
% Different Shapley value implementations are depicted by the different colored bars. 
\end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=1\linewidth]{feature_imp_local_combined.png}
% \caption{\label{feature_imp_local_combined}New fig combined}
% \end{figure}

Figure \ref{Fig4-2-feature-importance} presents the mean attributions for key features (\texttt{grade}, \texttt{sqft}, and \texttt{city\_BELLEVUE}) in the Home dataset, highlighting substantial variability across imputation methods (see initial feature importance analysis in Appendix Fig. \ref{Appendix-KingImportance}). %These features were identified as most impactful based on their mean absolute Shapley values.%
For instance, among the three selected features, MA assigns significant importance to feature \texttt{grade}, while CA emphasizes feature \texttt{sqft}. Meanwhile, although MA and MK share a similar conceptual foundation, they yield markedly different results; MA assigns significant importance to the \texttt{grade} feature, whereas MK prioritizes the \texttt{sqft} feature. Similar trends appear in the Polymer and Drug datasets (Appendix Fig. \ref{Appendix-PolyDrug}), illustrating how baseline selection impacts feature importance in Shapley value estimation.
% These differences in global attributions are expected due to the different baseline values used during imputation and they highlight the sensitivity of feature importance analysis to the specific method used for Shapley value estimation.
% This raises two key questions in feature attribution: how to explain these feature attributions and which method to choose.


Figure \ref{Fig4-3-local-exp}(a)-(b) provides an example of local feature attributions for one explicand in the Home dataset. Consistent with the global results, the local attributions from MA and CA show considerable variation. In this example, MA (Fig. \ref{Fig4-3-local-exp}(a)) assigns -97k to 1360 sqft whereas CA (Fig. \ref{Fig4-3-local-exp}(b)) assigns -316k to 1360 sqft. More examples for other datasets are in Appendix \ref{Appendix-FeatureAttributionLocal} (Fig. \ref{append_local_poly} and \ref{append_local_drug}). In addition to the variation in the explanation results, a notable limitation of these imputation methods, especially distributional sampling-based methods, is their lack of direct interpretability. The reliance on imputation obscures the exact benchmark feature value against which the explicand is compared, making it challenging to derive actionable insights from the local explanations. For example, in Fig. \ref{Fig4-3-local-exp}(a), the explicand has a \texttt{grade} of 6 and a \texttt{sqft} of 1360, which are associated with approximate reductions in the predicted home price of 183k and 97k, respectively. However, we don't know the benchmark feature values of \texttt{grade} and \texttt{sqft} against which these reductions are calculated. The reference values remain obscured due to the sampling processes employed in the imputation.

\begin{figure}[ht]
\centering
% \includegraphics[width=0.8\linewidth]{local.pdf}
\includegraphics[width=1\linewidth]{local.png}
\caption{\label{Fig4-3-local-exp}Waterfall plot of feature attributions for an explicand from the Home dataset using different methods: (a) MA, (b) CA, and (c) PC. The y-axis lists the top nine features with their values for the explicand (relative values in (c)); remaining features are aggregated as ``Other Features.'' The x-axis shows feature attributions in dollars. $f(x)$ is the model's predicted value; $E[f(X)]$ is the expected prediction based on the background data.}
\end{figure}

% The comparison of these Shapley value estimation methods underscores the variability and interpretability challenges inherent in existing approaches. Global attributions reveal method-dependent rankings of feature importance, while local attributions illustrate the profound influence of imputation strategies on individual predictions. Crucially, the use of distributional sampling introduces ambiguity in local explanations, reducing their practical utility. These observations highlight the need for feature attribution techniques that strike a balance between estimation accuracy and interpretability, providing motivation for the development and application of Pairwise Shapley in this study.

To enable a fair comparison across feature removal methods and mitigate the impact of baseline variations ($E[f(X')]$ in Fig. \ref{Fig1} and $E[f(X)]$ in Fig. \ref{Fig4-3-local-exp}), we normalize feature attributions, yielding the \textit{predicted marginal change in the target variable per unit of a specific feature}. For example, in the Home dataset, this allows us to express the \texttt{sqft} feature's impact as dollars per square foot (\$/sqft), independent of baseline selection. Normalization is performed by computing differences in Shapley values and feature values across random explicand pairs:

% \begin{equation}
% f(x_i) = \phi_0 +  \sum_{k=1}^{n}\phi_i^k,
% \label{eq:shapley-decomposition}
% \end{equation}

\begin{equation}
f(x^i) = \phi_0 +  \sum_{k=1}^{n}\phi_k^i
\label{eq:shapley-decomposition}
\end{equation}

\begin{equation}
\text{norm}_{k}^{ij} = \frac{\phi_k^i - \phi_k^j}{x_k^i - x_k^j},
\label{eq:normalized-difference}
\end{equation}
where $x^i$ is an explicand with $n$ features and the $k^{th}$ feature value is $x^i_k$; $f(x^i)$ is the ML predicted value of explicand $x^i$; $\phi_0$ is the baseline value of the feature removal method, $\phi_k^i$ is the Shapley value of the $k^{th}$ feature in explicand $x^i$, and $\text{norm}^{ij}_k$ is the normalized Shapley value of feature $k$ between two explicands $x^i$ and $x^j$.

% \begin{equation}
% \text{norm}_{ij}^k = \frac{\phi_i^k - \phi_j^k}{x_i^k - x_j^k},
% \label{eq:normalized-difference}
% \end{equation}
% where $x_i$ is an explicand with $n$ features and the $k^{th}$ feature value equals $x_i^k$; $f(x_i)$ is the ML predicted value of explicand $x_i$; $\phi_0$ is the baseline value of the feature removal method, $\phi_i^k$ is the Shapley value of the $k^{th}$ feature in explicand $x_i$, and $\text{norm}_{ij}^k$ is the normalized Shapley value of feature $k$ between two explicands $x_i$ and $x_j$.


% \[
% f(x_i) = \phi_0 + \phi_{\text{sqft},i} + \phi_{\text{grade},i}  + \dots
% \]

% \[
% f(x_j) = \phi_0 + \phi_{\text{sqft},j} + \phi_{\text{grade},j} + \dots
% \]

% \[
% f(x_j) - f(x_i) = (\phi_{\text{sqft},j} - \phi_{\text{sqft},i}) + (\phi_{\text{grade},j} - \phi_{\text{grade},i}) + \dots,
% \]
% where $x_i$ and $x_j$ represent two randomly selected explicands. $f(x)$ denotes the predicted value, which also equals to the sum of feature attribution values for the explicand $x$. $\phi_0$ represents the baseline value. $\phi_{\text{feature},i}$ denotes the Shapley value of a specific feature in explicand $x_i$.

Using feature \texttt{sqft} in the Home dataset as an example, the resulting distribution of $\text{norm}_{\texttt{sqft}}$ from 500 pairs are shown in Fig. \ref{Fig4-4-normalized}(a). The mean and standard deviation across methods are summarized in Table \ref{tab:sqft_summary_statistics}. A Kolmogorov–Smirnov (KS) test (using MA as the reference) confirms that normalized Shapley distributions significantly differ (p-values $<$ 0.01, see Table \ref{tab:sqft_summary_statistics}), underscoring the variability across feature removal techniques.

% To test whether these distributions are significantly different, we apply the Kolmogorov–Smirnov (KS) test. Using the distribution from the MA method as the reference, we compute p-values for comparisons with the distributions from other methods. With a significance threshold of \(\alpha = 0.01\), the p-values indicate that the normalized Shapley values for feature \texttt{sqft} derived from different methods are significantly distinct, reaffirming the variability of these methods.

% (\HC{What is the takeaway from 'the variability of these methods', including the pairwise ones. Do we recommend which pairwise method to use in this study?})

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Fig4-4.png}
% \includegraphics[width=0.8\linewidth]{Fig4-4.pdf}
\caption{\label{Fig4-4-normalized}Distribution of normalized Shapley values for feature \texttt{sqft} in the Home dataset for different methods. (a) Distribution plot for seven existing non-pairwise methods; (b) Distribution plot for three selected existing non-pairwise methods (UF, MA, TS) and three pairwise methods with different similarity algorithms (PR, PC, PS).}
\end{figure}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|r|r|r|r||r|r|r|r|}
% \hline
% % \textbf{Method} & \textbf{Mean} & \textbf{Std} & \textbf{Skew} & \textbf{Kurtosis} & \textbf{p-value(MA)} & \textbf{p-value(PC)}&  \textbf{p-value(PS)} & \textbf{p-value(PR)}\\

% \multicolumn{1}{|l|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Mean}}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Std}}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Skew}}} & \multicolumn{1}{l||}{\multirow{2}{*}{\textbf{Kurtosis}}} & \multicolumn{4}{l|}{\textbf{p-value}}                                                                          \\ \cline{6-9} 
% \multicolumn{1}{|l|}{}                        & \multicolumn{1}{c|}{}                      & \multicolumn{1}{l|}{}                     & \multicolumn{1}{l|}{}                      & \multicolumn{1}{l||}{}                     & \multicolumn{1}{l|}{MA} & \multicolumn{1}{c|}{PC} & \multicolumn{1}{c|}{PS} & \multicolumn{1}{c|}{PR} \\ \hline
                   
% \hline
% \hline
% \textbf{B0} & 94.74 & 249.76 & -0.56 & 2.7 & 0.0 & - & - & - \\
% \textbf{BM} & 149.43 & 213.63 & -0.99 & 5.09 & 1.1e-165 & - & - & - \\
% \textbf{UF} & 169.82 & 264.3 & -0.93 & 2.78 & 3.1e-26 & 1.4e-146 & 3.5e-151 & 1.4e-180 \\
% \textbf{CA} & 233.55 & 244.03 & -1.35 & 4.34 & 0.0 & - & - & - \\
% \textbf{MA} & 172.08 & 238.74 & -1.08 & 4.11 & - & 1.9e-102 & 2.5e-92 & 6.4e-115 \\
% \textbf{MK} & 259.86 & 269.97 & -1.24 & 3.07 & 0.0 & - & - & - \\
% \textbf{TS} & 157.95 & 235.35 & -1.03 & 4.12 & 7.8e-41 & 8.6e-111 & 8.8e-100 & 6.1e-124 \\
% \hline
% \textbf{PC} & 188.96 & 138.72 & 0.33 & 4.08 & - & - & 0.0184 & 0.00041 \\
% \textbf{PS} & 188.91 & 138.87 & 0.38 & 3.35 & - & 0.0184 & - & 0.00016 \\
% \textbf{PR} & 190.5 & 135.21 & 0.47 & 3.89 & - & 0.00041 & 0.00016 & - \\
% \hline
% \end{tabular}

% \caption{Summary statistics and Kolmogorov–Smirnov test results for the distribution of normalized \texttt{sqft} feature from different methods.}
% \label{tab:sqft_summary_statistics}
% \end{table}


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|r|r|r|r|}
\hline
\textbf{Method} & \textbf{Mean} & \textbf{Std} & \textbf{Skew} & \multicolumn{4}{c|}{\textbf{p-value}} \\ \cline{5-8}
 & & & & \textbf{MA} & \textbf{PC} & \textbf{PS} & \textbf{PR} \\ \hline
\textbf{B0} & 94.74 & 249.76 & -0.56 & 0.0 & - & - & - \\
\textbf{BM} & 149.43 & 213.63 & -0.99 & 1.1e-165 & - & - & - \\
\textbf{UF} & 169.82 & 264.3 & -0.93 & 3.1e-26 & 1.4e-146 & 3.5e-151 & 1.4e-180 \\
\textbf{CA} & 233.55 & 244.03 & -1.35 & 0.0 & - & - & - \\
\textbf{MA} & 172.08 & 238.74 & -1.08 & - & 1.9e-102 & 2.5e-92 & 6.4e-115 \\
\textbf{MK} & 259.86 & 269.97 & -1.24 & 0.0 & - & - & - \\
\textbf{TS} & 157.95 & 235.35 & -1.03 & 7.8e-41 & 8.6e-111 & 8.8e-100 & 6.1e-124 \\
\hline
\textbf{PC} & 188.96 & 138.72 & 0.33 & - & - & 0.0184 & 0.00041 \\
\textbf{PS} & 188.91 & 138.87 & 0.38 & - & 0.0184 & - & 0.00016 \\
\textbf{PR} & 190.5 & 135.21 & 0.47 & - & 0.00041 & 0.00016 & - \\
\hline
\end{tabular}
\caption{Summary statistics and Kolmogorov–Smirnov (KS) test results for the distribution of normalized \texttt{sqft} feature attribution from different methods. The p-values indicate statistical comparisons, where MA, PC, PS, and PR serve as the reference distributions.}
\label{tab:sqft_summary_statistics}
\end{table}



% (\HC{This section is `Existing Feature Removal Methods Comparison`, but the Table 2 also includes the pairwise methods.})

% \subsection{Pairwise Shapley: Enhanced Interpretability}
% We compute the Shapley values with the pairwise framework using two different pair selection strategies. First, we reuse the same random pairs of explicands from the prior analysis in normalized Shapley values, denoted as ``PR'' (Pairwise-Random). Second, we devise two similarity-based algorithms to select pairs of comparable explicands for pairwise computation, focusing on identifying explicands with close values for key attributes, denoted as ``PS'' (Pairwise-Similar) and ``PC'' (Pairwise-Comparable).

% Pairwise Shapley values show more intuitive explanations by controlling the background data and using informative background feature values, particularly in local explanations. For example, using PC method on the King County dataset, in Fig. \ref{Fig4-3-local-exp}(c), the explanation shows that a target home with 430 square feet less than the reference (a comparable home) results in a \$27k decrease in the final home price prediction. Similarly, the target home having a traffic noise level 2 units lower than the comparable reference leads to a \$38k increase in the predicted home price. This explanation closely mirrors the appraisal process in real-world home valuation, making the explanations more intuitive compared to the non-pairwise methods shown in Fig. \ref{Fig4-3-local-exp} (a)-(b). Similar advantages of the pairwise Shapley method in delivering intuitive and interpretable explanations can be observed in the Polymer and Drug datasets, as detailed in the Appendix Fig. \ref{append_local_poly} and \ref{append_local_drug}.

% When viewing the full set of predictions, the pairwise method provides a more intuitive visualization due to its reliance on relative feature values. To illustrate this, we generate beeswarm plots to compare feature values against Shapley values across multiple methods, as shown in Fig. \ref{Fig4-5-beeswarm}.  For example, in Fig. \ref{Fig4-5-beeswarm}(a), when the square footage (\texttt{sqft}) of the target home exceeds that of the reference home, the Shapley value for the square footage is expected to increase, aligning with the real-world intuition that a larger home contributes more positively to the predicted value. This is an example of the logical attribution property described above. In contrast, non-pairwise methods assign Shapley values based on absolute feature values, where square footage is always positive but may represent either an increase or a decrease in predicted home value, which can lead to explanations that are less aligned with human intuition. This discrepancy becomes particularly noticeable when the feature's monotonic correlation with the target variable is weak, such as \texttt{grade}, \texttt{noise\_traffic}, and \texttt{view\_lakewash}, as depicted in Figs. \ref{Fig4-5-beeswarm}(b)-(d). Less obvious monotonic contributions can be observed in the global plot. For instance, if we aim to explain the effect of \texttt{noise\_traffic}, \texttt{view\_lakewash}, or \texttt{grade} on home price in general based on Shapley values, only pairwise methods provide results that are intuitive and aligned with expectations. In pairwise methods, the relative feature values allow us to clearly infer how an increase or decrease in a feature (e.g., a reduction in traffic noise or the presence of a certain view) impacts the predicted home price. Conversely, non-pairwise methods often fail to provide such clear interpretations, as they rely on absolute feature values that do not directly reflect the relative change or its contextual meaning, making the results less actionable and harder to generalize.

% % Another advantage of the pairwise approach is its treatment of ``dummy players,'' where the relative feature value equals zero. For pairwise methods, dummy players always contribute zero to the prediction, as their relative feature value is zero. Non-pairwise methods, however, can mistakenly assign credits to dummy players, as shown in Fig. \ref{Fig4-5-beeswarm}(c) and (d), especially for CA method, where conditional distribution sampling is involved. This inconsistency further highlights the interpretability challenges of non-pairwise methods and underscores the intuitive nature of pairwise Shapley values for both local and global explanations \JX{[Q: Move this paragraph to the section specific for Dummy players later?]}. (\HC{Agree with Angela's comment.})


% \begin{figure}[H]
% \centering
% % \includegraphics[width=1\linewidth]{beeswarm.pdf}
% \includegraphics[width=1\linewidth]{beeswarm.png}
% \caption{\label{Fig4-5-beeswarm}Beeswarm plots showing the relationship between Shapley values and (relative) feature values for several methods on important features in King County dataset: (a) square footage (\texttt{sqft}), (b) grade, (c) traffic noise (\texttt{noise\_traffic}), and (d) Lake Washington view (\texttt{view\_lakewash}). The x-axis represents Shapley values (in dollars, the unit of the target variable), and the color bar indicates the feature value for non-pairwise methods and the relative feature value for pairwise methods.}\AB{One thing I think we should look at is removing outliers from these plots. For example, homes that have more than 5000 sqft difference, is one home really big and have an outlier sqft? Also for pairwise, I would have thought that similar homes would not be different enough in sqft to add plus minus 1 or 2 million dollars.. is the scale correct? I think one thing we could be asked is - if the pairwise methods choose similar homes, why is the magnitude of the attributions so much larger than with the other methods. }

% \end{figure}

% \subsection{Pairwise Shapley: Improved Model Insights}
% In addition to providing more intuitive explanations, Pairwise Shapley allows us to derive added insights into model behavior. In this section, we  from four perspectives: normalized Shapley value, monotonicity of feature impacts, dummy player property, and multicollinearity effect.
% \subsubsection{Normalized Shapley value}
% \label{Norm_sqft}
% % \begin{figure}[H]
% % \centering
% % \includegraphics[width=0.8\linewidth]{fig5-dist-skewness.png}
% % \caption{\label{fig:5-dist-skewness}This figure shows pairwise shapley is the only method that can recover the skewness of the real feature attribution distribution (sqft).} \AB{Unfortunately we will not be able to show the TrueFootage histogram. However we can describe the distribution in terms of skewness}
% % \end{figure}


% Different from Eq.~\ref{eq:normalized-difference}, we can calculate the normalized Shapley values of pairwise methods directly from the existing explicand pairs. 

% % \begin{equation}
% % \text{norm}_{ij}^k|_{\text{pairwise}} = \frac{\phi_{ij}^k}{x_i^k - x_j^k},
% % \label{eq:normalized-difference-pair}
% % \end{equation}

% \begin{equation}
% \text{norm}^{ij}_k|_{\text{pairwise}} = \frac{\phi^{ij}_k}{x^i_k - x^j_k},
% \label{eq:normalized-difference-pair}
% \end{equation}
% where $x^i$ is the target explicand and $x^j$ is the reference explicand in pairwise method; $\phi^{ij}_k$ is the pairwise Shapley value of feature $k$ in explicand $x^i$ compared to explicand $x^j$.

% Figure \ref{Fig4-4-normalized}(b) and Table \ref{tab:sqft_summary_statistics} show the distribution and statistics of normalized Shapley values of feature \texttt{sqft} in King County dataset for pairwise methods, compared to non-pairwise cases. As we can see, only pairwise methods successfully recover the positive skewness of the normalized \texttt{sqft} Shapley value distribution, consistent with domain knowledge from appraiser data in the Seattle area \JX{[TODO: add stats in the table for comparison]}. All non-pairwise methods, in contrast, produce negatively skewed distributions of  normalized \texttt{sqft} Shapley value. This discrepancy is attributed to the inherent nature of the pairwise Shapley method, which is inspired from the appraiser home valuation process. We believe this property is not only advantageous for home value prediction datasets, such as the King County dataset, but also extends to other datasets, such as the polymer and drug datasets. 

% Furthermore, another KS test, using each of the three pairwise distributions as the benchmark, shows no statistically significant differences among the pairwise distributions. While we cannot definitively claim that the pairwise method provides a superior representation of the dollar per square foot value—either within the model or in real-world terms (as this also depends on the model performance not only the explanation method)—we can assert that the three pairwise distributions show no significant differences among themselves, underscoring the robustness of the pairwise approach. Moreover, the pairwise method does not require additional post-processing to recover normalized feature attributions, as the pairs are inherently part of the Shapley value estimation process. These findings emphasize the simplicity and consistency of the pairwise method as an alternative to existing approaches.

% \subsubsection{Monotonicity of feature impacts}
% \label{Monotonicity}

% For representative features, such as \texttt{sqft}, \texttt{grade}, and \texttt{noise\_traffic}, we evaluate explanation accuracy from a monotonicity perspective. In home valuation, while exact dollar contributions for some features are unknown and/or subjective (e.g., the impact of the number of bedrooms, view level, or traffic noise level), the direction of their impacts can be easily inferred. For example, \texttt{sqft} and \texttt{grade} are expected to have positive contributions to the home price (i.e., higher values lead to higher prices), whereas \texttt{noise\_traffic} is expected to have a negative impact (see Appendix Fig. \ref{Appendix-KingImportance}). To assess this monotonicity, we compute the percentage of $\text{norm}_{ij}^k$ values that satisfy the following criteria: (1) $\text{norm}_{ij}^k > 0$ for positively correlated features (e.g., \texttt{sqft} and \texttt{grade}), (2) $\text{norm}_{ij}^k < 0$ for negatively correlated features (e.g., \texttt{noise\_traffic}), and (3) $\text{norm}_{ij}^k = 0$ for $x_i^k = x_j^k$ (dummy players). The results, presented in Fig. \ref{Fig4-6-mono-matchsign}, demonstrate that pairwise methods achieve the highest monotonicity measure, defined as the matched percentage, across all three features.



% \begin{figure}[H]
% \centering
% % \includegraphics[width=1.0\linewidth]{Fig4-6.pdf}
% \includegraphics[width=1.0\linewidth]{monotonicity_acc.png}
% \caption{\label{Fig4-6-mono-matchsign}Monotonicity measure (matched percentage) of Shapley value estimates across different methods for three features: \texttt{sqft}, \texttt{grade}, and \texttt{noise\_traffic}. Pairwise methods (PR, PC, PS) achieve the highest matched percentages across all features.}

% \end{figure}

% % \HC{1. We could move the legend outside (on the right of) the plot. 2. If we want to compare the methods for specific features, I would group by feature.}

% \subsubsection{Missing Feature Independence}

% Another advantage of the pairwise approach is the missing feature independence property. For pairwise methods, features modeled as missing always contribute zero to the prediction and their relative feature value is zero. Non-pairwise methods, however, can mistakenly assign credits to missing features, as shown in Fig. \ref{Fig4-5-beeswarm}(c) and (d), especially for CA method, where conditional distribution sampling is used. 

% % Additionally, the ``dummy player'' property is a critical characteristic desired for all features, regardless of whether their impacts are positive, negative, or unknown. This property ensures that features with zero relative difference contribute zero to the model prediction difference, with the assumption that identical features between the pair of explicands are independent of other features in the model \JX{[when the pairs are close to each other in the feature space.]}. 

% This property is valuable for several reasons. First, it allows us to condition on specific features and exclude them from the explanation (and computation) by selecting pairs of instances with identical values for those features which simplifies explanations. For example, in a complex home valuation model, we can exclude information about the local market conditions from our explanation by comparing similar homes within the same locality. In the context of polymers or drug molecules, we might want to focus on the effects of specific functional groups or motifs while excluding shared structural features that are not of interest. For instance, we can condition on shared backbone structures to better isolate and analyze the influence of substituents or side groups on target properties. 

% Second, the dummy player property ensures that identical features between two instances do not incorrectly contribute to the predicted difference in value. For instance, when explaining the predicted value of a target home using a comparable home, if the two homes are otherwise similar, the difference in predicted value should not be attributed to identical attributes, such as square footage. The pairwise method inherently ensures that identical features are excluded from contributing to the prediction difference.

% Figure. \ref{Fig4-7-dummy-player} shows the dummy player ratio (the ratio of $\text{norm}_{ij}^k = 0$ when $x_i^k = x_j^k$) for each method. Non-pairwise methods fail to consistently assign zero attribution to features with zero relative differences, the conditional Shapley method (CA) exhibiting particularly low ratios. This is expected, as conditional Shapley computations assume dependencies among missing features based on the original data distribution. In contrast, the pairwise method robustly satisfies the dummy player property.
% % For non-pairwise methods, relative Shapley values and relative feature values are derived using the random pair method.





% \begin{figure}[H]
% \centering
% % \includegraphics[width=1\linewidth]{Fig4-7.pdf}
% \includegraphics[width=1\linewidth]{dummy_player.png}
% \caption{\label{Fig4-7-dummy-player}Heatmap of dummy player ratios across features and methods for the King County dataset.}
% \end{figure}

% \subsubsection{Multicollinearity impact - a feature perturbation test}
% % Feature perturbation attributions

% Feature multicollinearity also impacts the feature attribution results. A feature correlated with other features in the data but irrelevant for the prediction may be assigned credit due to the imputation process used in Shapley value calculations. To evaluate explanation accuracy in the context of multicollinearity, we conduct a feature perturbation test to investigate how Shapley values change across different methods when the predicted value is updated by perturbing a single feature by a small margin. Intuitively, if all other features are held constant, the full difference in the prediction should be attributed to the perturbed feature. The ``small'' margin of perturbation is crucial for maintaining the independence assumption, as it ensures that the perturbed feature alone accounts for the entire change in the model's prediction.

% For this experiment, we focus on \texttt{sqft} in the King County data because it is a continuous variable, minimizing the risk of generating out-of-manifold data for the trained ML model. Synthetic data pairs are created by perturbing the \texttt{sqft} of 500 test instances, with perturbations applied both positively and negatively in the range \([-250, -100, -50, 50, 100, 250]\). For each perturbation, we compute the difference in predicted value and the difference in Shapley value for the \texttt{sqft} feature to determine whether the full change in prediction is allocated to that feature alone. For the pairwise method, the target home corresponds to the original test instance, and the reference home is the same instance with a perturbed \texttt{sqft}. For non-pairwise methods, we calculate the feature attribution for both the original and perturbed data points and then compare the change in predicted values with the corresponding change in Shapley values for the \texttt{sqft} feature between the original and perturbed data.

% Figure \ref{Fig4-8-perturb} shows a split violin plot, with the left side depicting prediction differences distribution and the right side showing Shapley value differences distribution for \texttt{sqft}. For non-pairwise methods (e.g., conditional-all and marginal-all), the distributions differ significantly, particularly for the conditional-all method, indicating that the prediction difference is distributed across multiple features. In contrast, for the pairwise method, the two distributions are identical, confirming that the full prediction difference is attributed to the perturbed feature. This highlights the superiority of the pairwise method, which reliably attributes the entire change to the perturbed feature, ensuring interpretability across datasets such as King County and broader applications like polymers or drug discovery.

% % In summary, this experiment demonstrates that, when a feature is perturbed while all others are held constant, the pairwise method attributes the entire prediction difference to the perturbed feature. In contrast, the marginal-all and conditional-all methods distribute the prediction difference across several features, reducing interpretability. The pairwise method offers a more intuitive explanation in our use case, as it ensures that the difference in predicted value between two similar data points (homes) is attributed solely to the differing feature. This property is desirable not only for home valuation datasets like King County but also for other applications, such as polymer or drug datasets. For instance, when modifying a specific functional group at a certain position in a material or drug, we expect the resulting property change to be attributed only to the altered part, making the pairwise method a robust and interpretable approach across domains.


% \begin{figure}[H]
% \centering
% \includegraphics[width=1.0\linewidth]{combined_sqft_perturb_violin_2.pdf}
% \caption{\label{Fig4-8-perturb}Distribution of predicted price change (blue, LHS violin) and Shapley value change for the \texttt{sqft} feature (green, RHS violin) in the King County dataset in the feature perturbation test. (a) Conditional-all method, (b) Marginal-all method, and (c) Pairwise method.}
% \end{figure}

% % \begin{table}[H]
% % \centering
% % \begin{tabular}{l|r}
% % Item & Quantity \\\hline
% % Widgets & 42 \\
% % Gadgets & 13
% % \end{tabular}
% % \caption{\label{tab:widgets}An example table.}
% % \end{table}


\subsection{Pairwise Shapley: Enhanced Interpretability}

We compute the feature attribution values with the pairwise framework using two different pair selection strategies. First, we reuse the same random pairs of explicands from the prior analysis in normalized Shapley values, denoted as ``PR'' (Pairwise-Random). Second, we devise two similarity-based algorithms to select pairs of comparable explicands for pairwise computation, focusing on identifying explicands with close values for key attributes, denoted as ``PS'' (Pairwise-Similar) and ``PC'' (Pairwise-Comparable).

Pairwise Shapley values show more intuitive explanations by controlling the background data and using informative background feature values, particularly in local explanations. For example, using PC method on the Home dataset, in Fig. \ref{Fig4-3-local-exp}(c), the explanation shows that a target home with 430 square feet less than the reference (a comparable home) results in a \$27k decrease in the final home price prediction. Similarly, the target home having a traffic noise level 2 units lower than the comparable reference leads to a \$38k increase in the predicted home price. This explanation closely mirrors the appraisal process in real-world home valuation, making the explanations more intuitive compared to the non-pairwise methods shown in Fig. \ref{Fig4-3-local-exp} (a)-(b). Similar benefits are observed in Polymer and Drug datasets (Appendix \ref{Appendix-FeatureAttributionLocal}).


\subsection{Pairwise Shapley: Improved Model Insights}
Beyond more intuitive explanations, Pairwise Shapley allows us to derive additional insights into model behavior. In this section we show how we can derive model insights by (i) comparing the distributions of ``normalized'' attribution values (ii) measuring the rate of sign matching between attribution values and feature differences, and (iii) observing global attribution values as a function of feature (difference) value.

\subsubsection{Normalized Shapley value}
\label{Norm_sqft}
% \begin{figure}[H]
% \centering
% \includegraphics[width=0.8\linewidth]{fig5-dist-skewness.png}
% \caption{\label{fig:5-dist-skewness}This figure shows pairwise shapley is the only method that can recover the skewness of the real feature attribution distribution (sqft).} \AB{Unfortunately we will not be able to show the TrueFootage histogram. However we can describe the distribution in terms of skewness}
% \end{figure}


Different from Eq.~\ref{eq:normalized-difference}, we compute the normalized Shapley values of pairwise methods directly from explicand pairs as:

% \begin{equation}
% \text{norm}_{ij}^k|_{\text{pairwise}} = \frac{\phi_{ij}^k}{x_i^k - x_j^k},
% \label{eq:normalized-difference-pair}
% \end{equation}

\begin{equation}
\text{norm}^{ij}_k|_{\text{pairwise}} = \frac{\phi^{ij}_k}{x^i_k - x^j_k},
\label{eq:normalized-difference-pair}
\end{equation}
where $x^i$ is the target explicand and $x^j$ is the reference explicand in pairwise method; $\phi^{ij}_k$ is the Pairwise Shapley value of feature $k$ in explicand $x^i$ compared to explicand $x^j$.

Figure \ref{Fig4-4-normalized}(b) and Table \ref{tab:sqft_summary_statistics} show the distribution and statistics of normalized Shapley values of feature \texttt{sqft} in the Home dataset for pairwise and non-pairwise methods. Pairwise methods recover a positively skewed distribution of normalized \texttt{sqft} attribution, aligning with our domain knowledge, while non-pairwise methods produce negative skew. A KS test finds no significant differences among pairwise distributions, reinforcing their robustness. Additionally, the pairwise approach requires no extra post-processing, as normalization is inherently built into the attribution process.

% Only pairwise methods successfully recover a positive skewness of the normalized \texttt{sqft} Shapley value distribution. The positive skew in this distribution aligns with our domain knowledge, supporting our belief that the underlying model is learning a reasonable representation of the feature. All non-pairwise methods, in contrast, produce negatively skewed distributions of  normalized \texttt{sqft} attribution values.

% A KS test, taking each of the three pairwise distributions as a benchmark, reveals no statistically significant differences among the pairwise distributions which underscores the robustness of the pairwise approach. Moreover, the pairwise method does not require additional post-processing to recover normalized feature attributions, as the pairs are inherently part of the Shapley value estimation process.


\subsubsection{Directionality of feature attributions}
\label{Monotonicity}

For key features, such as \texttt{sqft}, \texttt{grade}, and \texttt{noise\_traffic}, we evaluate the attribution values from a monotonicity perspective to show how the pairwise method allows us to evaluate logical attributions. In home valuation, while the exact dollar impact of certain features (e.g., bedrooms, view level, traffic noise) are unknown and/or subjective, the direction of their impacts can be hypothesized. For example, \texttt{sqft} and \texttt{grade} should positively impact price (i.e., higher feature values lead to higher prices), whereas \texttt{noise\_traffic} is expected to have a negative impact (see Appendix Fig. \ref{Appendix-KingImportance}). To assess the expected directionality, we compute the percentage of $\text{norm}_{k}^{ij}$ values that satisfy the following criteria: (1) $\text{norm}_{k}^{ij} > 0$ for features positively correlated with price (e.g., \texttt{sqft} and \texttt{grade}), (2) $\text{norm}_{k}^{ij} < 0$ for negatively correlated features (e.g., \texttt{noise\_traffic}), and (3) $\text{norm}_{k}^{ij} = 0$ for $x^i_k = x^j_k$ (dummy pairs). As shown in Fig. \ref{Fig4-6-mono-matchsign}, pairwise methods have the highest monotonicity rates-defined as the matched percentage-across all three features. By evaluating the monotonicity rate of these features, we can deduce if our model is behaving in an expected manner. The high monotonicy rate of these three features in the pairwise method suggests our model explanations align with human expectations and that our model is learning intuitive correlations.


\begin{figure}[ht]
\centering
% \includegraphics[width=1.0\linewidth]{Fig4-6.pdf}
\includegraphics[width=1.0\linewidth]{monotonicity_acc.png}
\caption{\label{Fig4-6-mono-matchsign}Monotonicity measure (matched percentage) of Shapley value estimates across different methods for three features: \texttt{sqft}, \texttt{grade}, and \texttt{noise\_traffic}.
% Pairwise methods (PR, PC, PS) achieve the highest matched percentages across all features.
}

\end{figure}

\subsubsection{Visualizing global attributions}
\label{global_Visualizing}
Figure \ref{Fig4-5-beeswarm} illustrates how the pairwise method provides a more informative representation of global model behavior. In Fig. \ref{Fig4-5-beeswarm}(a), Pairwise Shapley attributions for \texttt{sqft} increase when the \texttt{sqft} of the target home exceeds that of the reference, aligning with the real-world intuition. In contrast, non-pairwise methods assign Shapley values based on absolute feature values, where \texttt{sqft} is always positive but may represent either an increase or a decrease in predicted home value. This discrepancy becomes particularly noticeable when the feature's monotonic correlation with the target variable is weak, such as \texttt{grade}, \texttt{noise\_traffic}, and \texttt{view\_lakewash}, as depicted in Figs. \ref{Fig4-5-beeswarm}(b)-(d). In pairwise methods, the relative feature values allow us to clearly infer how an increase or decrease in a feature (e.g., a reduction in traffic noise or the presence of a certain view) impacts the predicted home price. Conversely, non-pairwise methods fail to provide such clear interpretations, as they rely on absolute feature values that do not directly reflect the relative change or its contextual meaning, making the results less actionable with reduced generalization.



\begin{figure}[ht]
\centering
% \includegraphics[width=1\linewidth]{beeswarm.pdf}
\includegraphics[width=1\linewidth]{Beeswarm_v2.png}
\caption{\label{Fig4-5-beeswarm}Beeswarm plots showing the relationship between Shapley values and (relative) feature values for several methods on important features in Home dataset: (a) \texttt{sqft}, (b) \texttt{grade}, (c) \texttt{noise\_traffic}, and (d) \texttt{view\_lakewash}. The x-axis represents Shapley values (in dollars), and the color bar indicates the feature value for non-pairwise methods and the relative feature value for pairwise methods.}
\end{figure}

% \HC{1. We could move the legend outside (on the right of) the plot. 2. If we want to compare the methods for specific features, I would group by feature.}

% \subsection{Missing Feature Independence}
\subsection{Pairwise Shapley: Feature Independence}

% \subsubsection{Dummy Pair and Missing Feature Independence}
\subsubsection{Dummy Pair}
% Next we evaluate missing feature independence. For pairwise methods, features modeled as missing always contribute zero to the prediction and their relative feature value is zero. Non-pairwise methods, however, can mistakenly assign credits to missing features, as shown in Fig. \ref{Fig4-5-beeswarm}(c) and (d), especially for CA method, where conditional distribution sampling is used introducing correlations between missing and observed features. 

% Next we evaluate the independence of dummy feature pairs and missing features. In these cases, pairwise methods always assign zero attribution, as the relative feature value is zero. For non-pairwise methods, we select pairs of data conditioned on certain features and compute their relative feature attribution values. We show in Fig. \ref{Fig4-5-beeswarm}(c) and (d), non-pairwise approaches, especially the CA method can mistakenly assign credits to missing features, due to conditional distribution sampling introducing correlations between missing and observed features. 


% Additionally, the ``dummy player'' property is a critical characteristic desired for all features, regardless of whether their impacts are positive, negative, or unknown. This property ensures that features with zero relative difference contribute zero to the model prediction difference, with the assumption that identical features between the pair of explicands are independent of other features in the model \JX{[when the pairs are close to each other in the feature space.]}. 

Pairwise methods always assign zero attribution to dummy features, as the relative feature value is zero. The dummy pair property is valuable as it enables conditioning on specific features to exclude them from explanations (and computations) by selecting pairs of instances with multiple identical feature values. For example, in home valuation, we can compare similar homes within the same locality to isolate local market effects. In the context of polymers or drug molecules, we can focus on the effects of specific functional groups or motifs while excluding shared structural features that are not of interest. In that case, we can condition on shared backbone structures to better isolate and analyze the influence of substituents or side groups on target properties. Figure. \ref{Fig4-7-dummy-player} shows the dummy pair ratio (the ratio of $\text{norm}^{ij}_k = 0$ when $x^i_k = x^j_k$) of each method. Non-pairwise methods fail to consistently assign zero attribution to dummy pairs, with CA exhibiting particularly low ratios. This is expected, as conditional Shapley methods assume dependencies among features based on the original data distribution. In contrast, the pairwise method always assigns zero value to a dummy feature pair.
% For non-pairwise methods, relative Shapley values and relative feature values are derived using the random pair method.


\begin{figure}[ht]
\centering
% \includegraphics[width=1\linewidth]{Fig4-7.pdf}
\includegraphics[width=0.9\linewidth]{dummy_player_v1.pdf}
\caption{\label{Fig4-7-dummy-player}Heatmap of dummy pair ratios across features and methods for the Home dataset.}
\end{figure}

\subsubsection{Single feature independence}
% Feature perturbation attributions

The dummy pair property ensures that when only one feature differs between two explicands, the entire prediction difference is attributed to that feature.
For instance, when explaining the predicted value of a target home using a comparable home, if the two homes are otherwise identical, the full difference in value should be attributed to the one different feature. However, in non-pairwise methods this does not hold true.

To evaluate single feature independence in the context of multicollinearity, we conduct a feature perturbation test by slightly modifying a single feature and analyzing how attributions change. Intuitively, if all other features are held constant, and the data points are close in feature space, the full difference in the prediction should be attributed to the perturbed feature.

For this evaluation, we focus on \texttt{sqft} in the Home data due to its continuous nature, minimizing the risk of generating out-of-manifold data for the trained ML model. Synthetic data pairs are created by perturbing the \texttt{sqft} of 500 test instances in steps of $\pm$50 \texttt{sqft} (range: $-$250 to 250). We compare the prediction change with the corresponding Shapley value change to determine whether the attribution is correctly assigned to \texttt{sqft} alone. In pairwise method, the target home is the original test instance, and the reference home is the same instance with a perturbed \texttt{sqft}. In non-pairwise methods, we calculate the feature attribution for both the original and perturbed data points and then compare the change between the original and perturbed data.

Figure \ref{Fig4-8-perturb} shows a split violin plot, with the left side depicting prediction differences distribution and the right side showing Shapley value differences distribution for \texttt{sqft}. In non-pairwise methods (e.g., conditional-all and marginal-all), the left and right distributions are different, particularly for the conditional-all method, where prediction changes are spread across multiple features. In contrast, the pairwise method maintains identical distributions, confirming the full prediction difference is attributed to the perturbed feature. This highlights the superiority of the pairwise method, which reliably attributes the entire change to the perturbed feature, ensuring interpretability.

% In summary, this experiment demonstrates that, when a feature is perturbed while all others are held constant, the pairwise method attributes the entire prediction difference to the perturbed feature. In contrast, the marginal-all and conditional-all methods distribute the prediction difference across several features, reducing interpretability. The pairwise method offers a more intuitive explanation in our use case, as it ensures that the difference in predicted value between two similar data points (homes) is attributed solely to the differing feature. This property is desirable not only for home valuation datasets like King County but also for other applications, such as polymer or drug datasets. For instance, when modifying a specific functional group at a certain position in a material or drug, we expect the resulting property change to be attributed only to the altered part, making the pairwise method a robust and interpretable approach across domains.


\begin{figure}[ht]
\centering
\includegraphics[width=1.0\linewidth]{combined_sqft_perturb_violin_2.pdf}
\caption{\label{Fig4-8-perturb}Distribution of predicted price change (blue, LHS violin) and Shapley value change for \texttt{sqft} feature (green, RHS violin) in the Home dataset under feature perturbation test. (a) Conditional-all method, (b) Marginal-all method, and (c) Pairwise method. In the Pairwise method, the predicted price change and Shapley value change are always the same, leading to the identical distributions.}
\end{figure}

% \begin{table}[H]
% \centering
% \begin{tabular}{l|r}
% Item & Quantity \\\hline
% Widgets & 42 \\
% Gadgets & 13
% \end{tabular}
% \caption{\label{tab:widgets}An example table.}
% \end{table}


\subsection{The Role of Similarity in Pairwise Shapley}

% In this section, we evaluate the role of similarity between the target and reference explicands in the Pairwise Shapley method. 
Using similar data instances as reference explicands in the pairwise method offers several key advantages. First, it reduces the likelihood of off-manifold inputs, which can distort feature attributions due to unrealistic data instances unsupported by training data~\cite{Zhao_OOdistribution}. Second, it simplifies explanations by conditioning on matched features, effectively removing them from the attribution process. To evaluate the impact of similarity on Pairwise Shapley values, we examine how monotonicity rates (as defined in Section \ref{Monotonicity}) change with explicand-reference similarity in the Home dataset. Figure \ref{Fig4-9-simscore} shows a heatmap of monotonicity scores (measured by Spearman correlation) as a function of similarity scores for several positive features in the Home dataset. The result shows that as the similarity between the target and reference explicand increases, the monotonicity rate of feature attributions generally increases. This suggests that higher similarity between data points can lead to more intuitive explanations using the pairwise method, assuming the base model has learned the expected relationship between that feature and the outcome.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{Fig4-9.pdf}
\caption{\label{Fig4-9-simscore}Heatmap of monotonicity scores (measured using Spearman correlation) as a function of similarity scores for selected positive features in the Home dataset.}
\end{figure}



% The expected monotonicity in Shapley attributions for specific features is influenced by both the model and the Shapley computation method. Our hypothesis is that as similarity between data decreases, the assumption that features can be approximated as independent breaks down, leading to non-monotonic Shapley value/feature value relationships. For example, consider comparing a home with 1000 square feet to a home with 3000 square feet. Adding three bathrooms to the smaller home may decrease its value due to size restrictions, breaking the independence approximation between bathrooms and square footage in such cases.

\subsection{Runtime Comparison}
We compare the computational time of various explanation methods for generating Shapley values, for one single explicand from the Home dataset. As shown in Table \ref{tab:computational-time}, the Pairwise method is the most efficient, requiring only 0.08 seconds, even without implementing the dummy pair speed-up described in Section \ref{ExplicitPairs}, which could further reduce runtime by many orders of magnitude. This efficiency stems from its reliance on simple single-value imputation and the ability to perform similarity calculations independently of the Shapley value computation. In contrast, marginal and conditional methods are significantly slower due to complex imputations. Detailed settings for each method are in Section \ref{Implement-Existing}. The Pairwise method’s efficiency, combined with its intuitive explanations, makes it better-suited for larger datasets and real-time applications.

% \begin{figure}[H]
% \centering
% \includegraphics[width=1\linewidth]{Fig4-10.pdf}
% \caption{\label{Fig4-10-runtime} Runtime comparison.}
% \end{figure}
\begin{table}[ht]
\centering
\caption{Comparison of computational time across methods for explaining a single explicand. Reported values are the mean over 50 independent runs, with standard deviations in parentheses.}
\label{tab:computational-time}
\begin{tabular}{|l|c|}
\hline
\textbf{Method}       & \textbf{Computational Time (s)} \\ \hline
B0             & 0.08 (0.01)                            \\ \hline
UF              & 0.21 (0.00)                            \\ \hline
CA          & 1.71 (0.21)                          \\ \hline
MA         & 1.83 (0.12)                           \\ \hline
MK         & 2.74 (0.05)                            \\ \hline
TS             & 0.19 (0.00)                            \\ \hline
Pairwise             & 0.08 (0.01)                            \\ \hline
\end{tabular}
\end{table}



\section{Discussion}

In this work, we introduced Pairwise Shapley Values, a novel method for generating intuitive and human-interpretable model explanations by leveraging explicit comparisons between proximal data points. Unlike traditional Shapley value estimation techniques that rely on abstract distributional baselines, our method directly attributes changes in model predictions to observed feature differences, enhancing interpretability across diverse domains. By ensuring that identical feature values in explicand-baseline pairs contribute no attribution, Pairwise Shapley Values maintain local independence, enabling more precise feature conditioning and computational efficiency. This property simplifies explanations by eliminating unnecessary perturbations and ensuring that isolated feature differences fully account for prediction variations. Furthermore, our approach is robust to the pair selection routine, demonstrating consistent attributions while significantly reducing computational overhead compared to traditional methods. These characteristics make Pairwise Shapley Values well-suited for tasks requiring transparent, contextually meaningful explanations, such as real estate valuation, materials discovery, and credit risk assessment.

A key distinction of our work is its contrast with the Formulate, Approximate, Explain (FAE) framework proposed by Merrick and Taly~\cite{merrick2019games}. Both approaches are motivated by the same theoretical concept of using reference points for feature attribution, but FAE constructs attributions using a distribution of reference points with a particular property, whereas our method employs a single-point pairwise comparison. This design choice enables Pairwise Shapley Values to provide more interpretable, instance-specific explanations, avoiding reliance on distributional assumptions. One advantage of FAE is its ability to derive uncertainty estimates for attributions, which is outside the scope of our work. Theoretically, our attributions are point values, and any uncertainty in attributions would propagate from the upstream model prediction uncertainty.

Pairwise Shapley Values introduce computational efficiency benefits, particularly through dummy pair independence, where features identical in both explicand and baseline receive zero attribution. This property allows us to: (i) condition on specific features, thereby reducing explanation complexity, and (ii) achieve significant runtime improvements by eliminating redundant computations. Specifically, removing these features ($n_c$) from the perturbation routine reduces computational complexity from $2^n$ to $2^{n-n_c}$, a critical bottleneck in many existing Shapley value algorithms~\cite{olsen2024comparative}. This structured feature selection not only enhances scalability but also ensures that attributions remain relevant to the given prediction task.

A crucial consideration in Shapley value estimation is feature dependence, especially when handling missing data. Traditional approaches model missing features using either marginal or conditional distributions, each of which introduces challenges. Marginal distribution-based methods assume feature independence, potentially creating unrealistic data points, while conditional distribution-based methods retain correlations among missing and observed features, leading to unexpected attributions for non-contributing features~\cite{aas2021, merrick2019games, frye2020}. Our dummy pair-based approach ensures that missing features are locally independent and do not contribute directly to attributions. However, missing values still influence the locality of the value function for evaluation. Our pair selection algorithm mitigates the risk of generating non-representative data by identifying data points close in feature space, though this effectiveness depends on dataset characteristics. In high-dimensional or sparse datasets, finding suitable reference points may be challenging, and outliers may lack appropriate pairwise comparisons, potentially reducing explanation reliability.

Another key limitation of Pairwise Shapley Values is that, like other feature attribution methods, it captures correlations rather than causal relationships. Marginal and conditional Shapley values fail to explicitly account for causality, distributing attributions among correlated features in ways that may not reflect their actual causal impact on the target variable~\cite{chen2023algorithms}. Similarly, our pairwise method removes features based on their relationship to a neighboring reference but does not model causal mechanisms. Causal inference-based Shapley methods, such as Causal Shapley Values~\cite{Heskes2020}, Asymmetric Shapley Values~\cite{frye2020b}, and Shapley Flow~\cite{pmlr-v130-wang21b}, incorporate directed causal graphs to produce more meaningful attributions. However, these methods require prior knowledge of the causal structure, which is often unavailable in real-world applications, limiting their practicality. Instead, our method provides an interpretable way to assess how model predictions change based on observed feature variations, serving as a valuable tool for model validation and debugging.

Finally, our experiments have primarily focused on tabular data, but the Pairwise Shapley framework can be extended to structured data types, including images, text, and graphs, by defining similarity measures for pair selection. For instance, in computer vision, a reference image with similar features could be selected for comparison, while in natural language processing, semantically similar text instances could serve as baselines. Future work will explore the adaptation of Pairwise Shapley Values to these domains, further expanding its utility in  XAI.

% \section{Discussion (Conference)}

% We introduced Pairwise Shapley Values, a novel feature attribution method that enhances interpretability by leveraging explicit comparisons between similar data points. Unlike traditional Shapley value methods that rely on abstract distributional baselines, our approach directly attributes changes in model predictions to feature differences, ensuring instance-specific explanations. This design improves computational efficiency, particularly through dummy pair independence, which eliminates unnecessary perturbations and reduces explanation complexity. Pairwise Shapley Values maintain local independence, leading to consistent attributions, reduced runtime, and better alignment with human decision-making in tasks such as real estate valuation, materials discovery, and credit risk assessment.

% Our method differs from the Formulate, Approximate, Explain (FAE) framework~\cite{merrick2019games}, which constructs attributions using distributions of reference points rather than single-point comparisons. While FAE can estimate attribution uncertainty, Pairwise Shapley Values provide interpretable, instance-level explanations without reliance on distributional assumptions. Additionally, our method improves computational efficiency by conditioning on selected features ($n_c$) and eliminating redundant calculations, reducing the complexity from $2^n$ to $2^{n-n_c}$, a key bottleneck in Shapley algorithms~\cite{olsen2024}.

% One limitation is the dependence on reference selection, which may be challenging in high-dimensional or sparse datasets where suitable comparisons are scarce. Furthermore, while our method improves attribution consistency, it remains correlation-based rather than causal. Unlike Causal Shapley Values~\cite{Heskes2020} and Shapley Flow~\cite{pmlr-v130-wang21b}, our approach does not infer causal mechanisms but instead provides a practical tool for assessing model behavior through observed feature variations. Finally, although our experiments focus on tabular data, the framework can be extended to images, text, and graphs by defining appropriate similarity metrics for pair selection, opening new directions for future research.


% \section{Discussion}
% In this work, we introduced Pairwise Shapley Values, a novel method for generating intuitive and human-interpretable model explanations by leveraging explicit comparisons between proximal data points. Unlike traditional Shapley value estimation techniques that rely on abstract distributional baselines, our method directly attributes changes in model predictions to observed feature differences, enhancing interpretability across diverse domains. By ensuring that identical feature values in explicand-baseline pairs contribute no attribution, Pairwise Shapley Values maintain local independence, enabling more precise feature conditioning and computational efficiency. This property also simplifies explanations by eliminating unnecessary perturbations and ensuring that isolated feature differences fully account for prediction variations. Furthermore, our approach is robust to the pair selection routine, demonstrating consistent attributions while significantly reducing computational overhead compared to traditional methods. These characteristics make Pairwise Shapley Values well-suited for tasks requiring transparent, contextually meaningful explanations, such as real estate valuation, materials discovery, and credit risk assessment.

% Despite its advantages, our method has several limitations and potential areas for future improvement. The effectiveness of Pairwise Shapley Values depends on selecting appropriate reference points, which can be challenging in high-dimensional or sparse datasets where meaningful comparisons are scarce. Additionally, while our method improves attribution consistency, it remains correlation-based and does not inherently capture causal relationships. Future work could explore integrating causal inference techniques to enhance interpretability in decision-critical applications. Another limitation is that our evaluation is currently restricted to tabular data; however, the framework can be extended to structured data types such as images, text, and graphs by defining appropriate similarity measures for pair selection. By further refining reference selection strategies and extending applicability beyond tabular datasets, Pairwise Shapley Values can serve as a broadly useful tool for explainable AI across diverse ML tasks.


% \AB{Below is copied from AB version}

% In this work, we introduced the Pairwise Shapley Values, a novel method for generating intuitive, human understandable ML model explanations by leveraging proximal data points for feature attributions. This method has several desirable and novel properties, including (i) enhanced human interpretability across several data domains, (ii) local independence for dummy pairs and single features, (iii) increased insights into global model behavior, and (iv) efficient compute run-time, with further potential for exponential speed improvements. 


% Many Shapley value estimation methods for ML models use distributional or ``null'' data representations to denote missing data fields (for an excellent review see~\cite{chen2023algorithms}). These methods lack interpretability when the goal is to relate changes in model output to deltas in feature values rather than the raw feature values. For example, traditional methods, such as SHAP~\cite{lundberg2017unified} and other popular Shapley value estimation algorithms, present the predicted value relative to a baseline drawn from a distribution where the baseline has no defined meaning in terms of the predictor variables it represents.

% Although this approach may be suitable for applications where feature importance rankings are sufficient — for example model feature selection~\cite{Kraev2024} — other scenarios (such as predicting the value of a home) require outputs that relate the change in each feature contribution to the change in prediction for meaningful interpretation. Here we show that by including a valid proximal data point as a baseline, we can output such model explanations. For example, our model values a home at \$50k higher than a comparable home, an additional \$40k is attributed to a 200sqft increase in size while a \$10k increase in value is attributed to an additional bathroom.

% Furthermore, as shown in this work and discussed in detail here~\cite{sundararajan2020many}, different Shapley value estimation methods yield remarkably different outcomes in both feature importance ranking and attribution. For the use cases presented in this work, we have shown that the Pairwise method is robust to the pairwise selection routine.

% In~\cite{merrick2019games}, motivated by the same theoretical concept, the authors use groups of contrasting reference points, and present a theoretical framework for generating and interpreting explanations for ML models referred to as Formulate, Approximate, Explain (FAE). They derive attribution values from distributions of single-reference games, where missing features are modeled in relation to a specific reference input. In contrast with the current work, the contrastive reference used is a distribution of data with a particular property as opposed to a single point pairwise comparison. A benefit of their method is the ability to derive error bars for their attribution values based on the distributional results. Uncertainty measurements of feature attributions are outside the scope of this work. Theoretically, the pairwise Shapley values are point values, in which case, all uncertainty in the attribution values would be propagated from uncertainty in the upstream model prediction. 

% Moreover, we demonstrate that the Pairwise Shapley method does not attribute any value to dummy pairs of data (where the feature in the explicand and baseline are identical). This attribute has several benefits, including:

% (i) we can condition on certain features (\(x_c\)) in our pair matching algorithm, thereby removing them from the explanation and reducing the explanation complexity. 

% (ii) we can speed up the runtime by 

% a) not computing attribution values for those features as they will always be zero,

% b) removing those features from the perturbation routine (from \(2^n\) to  \(2^{n-x_c}\)), which is shown to be a limiting factor for many existing Shapley value algorithms, see~\cite{olsen2024} for a discussion. 

% Feature dependence is an important consideration for Shapley value estimation, particularly as it pertains to modeling missing features. If missing features are modeled through conditional distributions, correlations between missing and included features are retained which may produce unexpected attribution weights to features that do not contribute to the outcome. Alternatively, assuming feature independence and performing feature removal via a marginal distribution can lead to unrealistic data instances and unexpected attribution values when features are correlated~\cite{aas2021, merrick2019games, frye2020}. In our work, missing features are modeled by dummy pairs and are locally independent of the remaining feature set and hence do not contribute directly to any attribution values. However, it should be noted that missing values indirectly contribute by dictating the locality of the value function for evaluation. We use the pair selection algorithm to identify data points close in feature space thereby alleviating the probability of generating non-representative data. However, we note that this is dataset dependent and identifying an appropriate reference baseline for certain applications may be challenging, particularly in high-dimensional and/or sparse datasets. Additionally, this method may not be appropriate for unusual data points or outliers that are not well supported by the training data and do not have any close pairs.

% An added benefit of dummy pair independence is that isolated feature differences (where only one feature differs in the pair) will be locally independent, through elimination, and the full change in prediction is attributed to the feature difference. In this work, we evaluate this property of multiple techniques by computing the Shapley value estimations before and after perturbing a single feature. While in the conditional and marginal distribution methods, the difference in Shapley values is not consistently the same as the prediction difference — it is strictly true in the Pairwise case. This is useful from an explainability perspective, provided the pairs are close in feature space, as if all else is the same between two data instances, we would expect the full difference in prediction to be attributed to the only feature that differs. Finally, we note it is extremely important to make clear to any downstream consumer of model explanation outputs that model explanations generated by these algorithms only describe the inner workings of our ML model, which is driven by feature correlation. Marginal Shapley values and conditional Shapley values, do not explicitly account for the causal relationship between features and the target variable. Conditional Shapley values tend to distribute attribution among correlated features, which can reveal hidden dependencies, whereas marginal Shapley values more closely represent the model’s underlying functional structure~\cite{chen2023algorithms}. However, neither method directly incorporates causal pathways in explaining how a particular feature influences the target outcome. Similarly, our Pairwise method, which removes features based on the relationship between a target data point and its closest neighbor, does not explicitly model causal processes. Instead, it relies on observed associations for Shapley value approximation. In contrast, causal inference approaches typically presume the existence of an underlying causal graph, a directed structure specifying how features exert causal influence on one another and, ultimately, on the target outcome. While several Shapley-based methods (e.g., causal Shapley values~\cite{Heskes2020}, asymmetric Shapley values~\cite{frye2020b}, and Shapley Flow~\cite{pmlr-v130-wang21b}) leverage this causal information to produce more interpretable attributions, they require prior knowledge of the causal graph, which is seldom available in real-world applications. Consequently, although these methods are theoretically sound, their practicality is constrained by the difficulty of establishing reliable causal structures in most settings. We can however use explanations derived from XAI methods as a tool to evaluate if a model is making decisions in an expected and appropriate way.

% Discuss how this method creates explanations that are useful for x, y, x
% \textcolor{red}{
% \begin{enumerate}
% \item Discuss the implications of the findings (human understandable/actionable; true to model vs data).
% \item Address the limitations of the Pairwise Shapley Values method (e.g., extension to other data type beyond tabular data).
% \item \AB{We can also mention that this method is applicable to the DeepLift aglorithm (maybe a follow-up paper)}
% \end{enumerate}
% }

% \JX{(TODO--LIMITATIONS -- Should we move it to the first Section-Introduction?)} However, our Pairwise Shapley Values method has its limitations. First, identifying an appropriate reference baseline for certain instances of different applications may be challenging, particularly in high-dimensional or sparse datasets. Second, the computational cost of determining pairwise baselines and computing Shapley values may increase with larger datasets, limiting scalability in real-time or large-scale applications.

% \JX{ALSO: future implementation of exact Shapley computation with pairwise methods.}

\section{Methods} 
% \textcolor{green}{Typically, I see the Method section is presented before the Result section in academic papers. \\If we have a dedicated Method section. Perhaps, the Pairwise Shapley Values section should belong here?}
% \JX{[JX-reply: The Methods section here would be like the model details (data source, packages, parameters, etc) for readers to reproduce the code/results. More on the techniques and less focused on theories and mechanisms.]}



\subsection{Datasets and Preprocessing}
\label{Methods-datasets}
This section provides detailed descriptions of the three datasets used for evaluating the effectiveness of the Pairwise Shapley Values method across both regression and classification tasks.

\subsubsection{Home Dataset}
The Home dataset contains over 560,000 records of single-family and townhome sales in King County, Washington, from 1999 to December 2023~\cite{andykrause_2019_github}. The task is to predict the sale price of a home, making it a regression problem. This dataset includes various features categorized into general information (e.g., transactional and temporal details), geographic information (e.g., locational and zoning data), physical attributes (e.g., property size, layout, and quality), and surrounding features (e.g., environmental factors and neighborhood amenities). These diverse features provide a comprehensive testbed for evaluating feature attribution methods. This dataset is divided into training, validation, and testing sets based on the sale date of the properties: the training set includes sales from January 2020 to October 2023, the validation set covers sales in November 2023, and the testing set consists of sales in December 2023.


\subsubsection{Polymer Dataset}
The Polymer dataset focuses on predicting the dielectric constant of polymers, a critical property for materials used in electronic and energy storage applications~\cite{kuenneth2021polymer}. The dielectric constant reflects a material’s ability to store electrical energy, and accurate prediction of this property is crucial for the design of advanced polymeric materials. The labels in this dataset are obtained from Density Functional Theory (DFT) calculations, which are widely used quantum mechanical modeling methods for predicting the electronic structure of molecules and materials. Each polymer is represented using its Simplified Molecular Input Line Entry System (SMILES) string, a standardized text-based format for describing chemical structures. To convert the SMILES representation into numerical vectors suitable for machine learning, we use MACCS fingerprints, which are binary vectors where each bit represents the presence or absence of a specific substructure in the molecule. Importantly, each bit in the MACCS fingerprint is interpretable, enabling direct insights into which molecular substructures influence the dielectric constant. This dataset is randomly divided into training and testing sets with a 4:1 ratio.

\subsubsection{Drug Dataset}
The Drug dataset, sourced from MoleculeNet~\cite{wu2018moleculenet}, is a well-known benchmark for molecular ML. It originates from the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability of over 40,000 compounds to inhibit HIV replication. The screening results were categorized into three labels: \textit{confirmed inactive} (CI), \textit{confirmed active} (CA), and \textit{confirmed moderately active} (CM). For this study, we used it as a binary classification task by combining the \textit{active} (CA) and \textit{moderately active} (CM) labels into a single \textit{active} class, resulting in a task to classify compounds as either \textit{inactive} or \textit{active}. Similar to the Polymer dataset, each compound is represented using its SMILES string. MACCS fingerprints are used to transform the SMILES representation into numerical vectors, with each bit corresponding to the presence or absence of specific chemical substructures. This binary encoding ensures interpretability and allows for systematic analysis of which substructures contribute to HIV replication inhibition. This dataset is stratified into training and testing sets using a 4:1 ratio to preserve the proportion of active and inactive compounds in both sets.

\subsection{Base Model Training}
\label{Methods-basemodel}
% Explain the training and evaluation process of the base models, including the use of the TPOT pipeline. \AB{We should state the exact parameters of the model as well as saying that they were derived via the TPOT pipeline for simplicity and speed, noting that the goal here is not to make the most accurate model, only a sufficiently predictive model to carryout our tests on. }

To evaluate the effectiveness of Pairwise Shapley as a model-agnostic feature attribution method, we trained base ML models on three datasets: Home, Polymer, and Drug. TPOT, an automated machine learning framework that utilizes genetic programming to optimize model pipelines~\cite{le2020scaling}, was employed for model selection and hyperparameter tuning. For each dataset, TPOT was configured with a population size of 30 and run for 3 generations. The final selected model for the Home dataset was a Random Forest regression model with 100 trees, a minimum of 15 samples per leaf node, and a minimum of 12 samples required for a split. For the Polymer dataset, the best-performing model was an XGBoost regression model with a learning rate of 0.1, a maximum tree depth of 8, a minimum child weight of 4, and a subsample ratio of 0.4. For the Drug dataset, the selected model was a Random Forest classification model consisting of 100 trees, requiring a minimum of 3 samples per leaf node, a minimum of 5 samples for splitting, and considering 20\% of the features when determining splits. While the primary objective was not to develop the most accurate models possible, the selected models provided sufficient predictive capability to facilitate an effective evaluation of the proposed Pairwise Shapley feature attribution method.

\subsection{Implementation of Existing Feature Removal Methods}
\label{Implement-Existing}

To estimate Shapley values for ML model feature attribution, we utilize multiple established feature removal methods, each corresponding to a distinct imputation strategy for missing features. We use the SHAP package~\cite{lundberg2017unified} with KernelExplainer for baseline imputation, where missing feature values are either replaced with the median values of the training dataset (BM) or set to zero (B0). For uniform distribution imputation (UF), we apply SamplingExplainer from SHAP, selecting 100 samples from the training dataset with a fixed random seed. Marginal distribution imputation (MA, MK) is implemented using KernelExplainer, with MA sampling 100 instances from the full training dataset and MK applying a k-means clustering summary (k=10) to create representative feature samples. Conditional distribution imputation (CA) is handled using the \texttt{shapr} library~\cite{sellereite2020shapr}, specifically applying the empirical approach, with 100 training samples, 100 feature combinations, and batch processing in 10 batches. For tree-based models, we use TreeExplainer from SHAP with the tree-path-dependent feature perturbation strategy. All experimentsmin this work were conducted on a high-performance computing node equipped with a 32-core Intel Xeon r6i processor and 256 GiB of RAM.

% \subsection{Metrics}
% \label{Metrics-method}
% \JX{TODO: Move details of each metrics from the RESULTS section to METHODS here.}
% \begin{enumerate}
%     \item Normalization
%     \item Monotonicity (signed attribution)
%     \item Dummy Player
%     \item Perturbation
%     \item Simscore analysis
%     \item Run time
% \end{enumerate}








% \subsection{How to add Comments and Track Changes}

% Comments can be added to your project by highlighting some text and clicking ``Add comment'' in the top right of the editor pane. To view existing comments, click on the Review menu in the toolbar above. To reply to a comment, click on the Reply button in the lower right corner of the comment. You can close the Review pane by clicking its name on the toolbar when you're done reviewing for the time being.

% Track changes are available on all our \href{https://www.overleaf.com/user/subscription/plans}{premium plans}, and can be toggled on or off using the option at the top of the Review pane. Track changes allow you to keep track of every change made to the document, along with the person making the change. 


% \subsection{How to write Mathematics}

% Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
% \[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%       = \frac{1}{n}\sum_{i}^{n} X_i\]
% denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


% \subsection{How to change the margins and paper size}

% Usually the template you're using will have the page margins and paper size set correctly for that use-case. For example, if you're using a journal article template provided by the journal publisher, that template will be formatted according to their requirements. In these cases, it's best not to alter the margins directly.

% If however you're using a more general template, such as this one, and would like to alter the margins, a common way to do so is via the geometry package. You can find the geometry package loaded in the preamble at the top of this example file, and if you'd like to learn more about how to adjust the settings, please visit this help article on \href{https://www.overleaf.com/learn/latex/page_size_and_margins}{page size and margins}.

% \subsection{How to change the document language and spell check settings}

% Overleaf supports many different languages, including multiple different languages within one document. 

% To configure the document language, simply edit the option provided to the babel package in the preamble at the top of this example project. To learn more about the different options, please visit this help article on \href{https://www.overleaf.com/learn/latex/International_language_support}{international language support}.

% To change the spell check language, simply open the Overleaf menu at the top left of the editor window, scroll down to the spell check setting, and adjust accordingly.

% \subsection{How to add Citations and a References List}

% You can simply upload a \verb|.bib| file containing your BibTeX entries, created with a tool such as JabRef. You can then cite entries from it, like this:~\cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|. You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

% If you have an \href{https://www.overleaf.com/user/subscription/plans}{upgraded account}, you can also import your Mendeley or Zotero library directly as a \verb|.bib| file, via the upload menu in the file-tree.

% \subsection{Good luck!}

% We hope you find Overleaf useful, and do take a look at our \href{https://www.overleaf.com/learn}{help library} for more tutorials and user guides! Please also let us know if you have any feedback using the Contact Us link at the bottom of the Overleaf menu --- or use the contact form at \url{https://www.overleaf.com/contact}.

\section*{Acknowledgments}  
We would like to thank Dr. Simon Nilsson for valuable insights, discussions, comments, and suggestions.

\section*{Code and Data Availability} 


The code and datasets used in this study are available~\href{https://github.com/Jiaxin-Xu/PairwiseShapley.git}{here}\footnote{\url{https://github.com/Jiaxin-Xu/PairwiseShapley.git}}.

% \bibliographystyle{acm}
\bibliographystyle{unsrt}

\bibliography{main}

\appendix

\section{Additional Results}
\subsection{Base Machine Learning Models Performance}
\label{Appendix-BaseModel}
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{Fig4-1-v3.png}
\caption{\label{Fig4-1}Base ML model performance on three different datasets. (a) King County home price prediction (regression); (b) Polymer dielectric constant prediction (regression); (c) Drug HIV replication inhibition ability prediction (classification).}
\end{figure}
% \AB{ A few of things here. 1) I would plot the scatter plots with and alpha = 0.4 so you can see the density of data better. 2) I would leave out the training data. 3) The AUC for dataset3 is 1. This suggests to me that the model is overfitting to the training data. It might be OK if we leave out the training data metrics, but ideally the model could be adjusted to prevent overfitting. }

% \HC{1. +1 to alpha = 0.4, we could also use different markers. 2. We could report actual metrics, and move the plots to Appendix to save space.}

\subsection{Feature Importance}
\label{Appendix-Featureimportance}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{Appendix_KingCounty_v3_MA_importance.png}
\caption{\label{Appendix-KingImportance}Beeswarm plot showing the initial SHAP feature importance analysis on the Home dataset. Features are ranked from top to bottom based on their average absolute SHAP values, with higher-ranked features having greater influence on the model's predictions.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{append-feature-importance-poly-drug_v2.pdf}
\caption{\label{Appendix-PolyDrug}Feature importance scores (mean absolute Shapley values) for selected important features of datasets (a) Polymer and (b) Drug. The corresponding molecular structures of each feature are included in each sub-panel.}
\end{figure}

\subsection{Examples of Feature Attribution}
\label{Appendix-FeatureAttributionLocal}
\begin{figure}[H]
\centering
% \includegraphics[width=0.8\linewidth]{local.pdf}
\includegraphics[width=0.8\linewidth]{append_local_poly.pdf}
\caption{\label{append_local_poly}Waterfall plot of feature attribution estimations for an explicand from Polymer dataset using different methods: (a) BM, (b) MA, (c) UF, and (d) PS.}
\end{figure}

\begin{figure}[H]
\centering
% \includegraphics[width=0.8\linewidth]{local.pdf}
\includegraphics[width=0.8\linewidth]{append_local_drug.pdf}
\caption{\label{append_local_drug}Waterfall plot of feature attribution estimations for an explicand from HIV dataset using different methods: (a) BM, (b) MA, (c) UF, and (d) PS.}
\end{figure}

\section{Guidance for Building an Explainable Model}
\label{Appendix-Protocol}

% \AB{I'm not sure if we should include here or in the main text (or most likely both places) a sentence about he importance of differentiating between explaining our model and causality. i.e. causation vs correlation, and that generally ML models have no concept of causality and explanations should not be misinterpreted by the end user. We can only tell a user how our model would change if an input changed.}
% \HC{I did a quick literature review and found several Shapley value methods that incorporate causal information, such as Causal Shapley Values and Shapley Flow. Our protocol is broadly applicable and can be integrated with any explanation method. Thus, I think that a discussion on causation vs correlation would be more appropriate for inclusion in the main text.}

% Two popular approaches to computing Shapley values, marginal Shapley values and conditional Shapley values, do not explicitly account for the causal relationship between features and the target variable. Conditional Shapley values tend to distribute attribution among correlated features, which can reveal hidden dependencies, whereas marginal Shapley values more closely represent the model’s underlying functional structure~\cite{chen2023algorithms}. However, neither method directly incorporates causal pathways in explaining how a particular feature influences the target outcome. Similarly, our Pairwise method, which removes features based on the relationship between a target data point and its closest neighbor, does not explicitly model causal processes. Instead, it relies on observed associations for Shapley value approximation. In contrast, causal inference approaches typically presume the existence of an underlying causal graph, a directed structure specifying how features exert causal influence on one another and, ultimately, on the target outcome. While several Shapley-based methods (e.g., causal Shapley values~\cite{Heskes2020}, asymmetric Shapley values~\cite{frye2020b}, and Shapley Flow~\cite{pmlr-v130-wang21b}) leverage this causal information to produce more interpretable attributions, they require prior knowledge of the causal graph, which is seldom available in real-world applications. Consequently, although these methods are theoretically sound, their practicality is constrained by the difficulty of establishing reliable causal structures in most settings.

As ML models become increasingly integral to decision-making in high-stakes domains, the need for model explainability has risen significantly. Ensuring that a model’s predictions are transparent and interpretable is vital for building trust among stakeholders, meeting regulatory requirements, and addressing ethical concerns. This protocol presents a systematic approach to feature auditing for tabular data prior to model construction, emphasizing techniques such as Shapley values to derive feature-level explanations. By following these steps, practitioners can ensure that final models are not only accurate but also interpretable, ethical, and aligned with domain knowledge.

\subsection{Feature Inclusion/Exclusion Criteria}
This protocol defines several criteria to determine whether a feature should be included or excluded in an explainable model. While each project may prioritize these criteria differently, the following list offers a comprehensive framework.

\begin{itemize}
    \item \textbf{Domain Expert Knowledge.} Features with clear domain relevance are more understandable and can make explanations more actionable. For example, a home price prediction model might remove “Improved Tax Value” if “Land Value” already captures the most straightforward assessment of value. Improved Tax Value may be harder to explain due to building-quality variables, whereas Land Value’s drivers (e.g., size, location, zoning) are more transparent.
    \item \textbf{Redundant Features.} If multiple features convey the same information (e.g., \textit{raw} versus \textit{log-transformed}), retaining a single representation simplifies model explanations. Redundant features can inflate model complexity and dilute interpretability. It is often preferable to keep the form that yields the clearest explanations or performs best in preliminary analyses. For example, we might have both “zip\_code” and “GPS\_coordinates” for the same property. If detailed latitude/longitude is already being used, the zip code may become redundant—or vice versa—unless there’s added interpretability or regulatory reasons to keep both.
    \item \textbf{Data Quality.} Exclude or impute features with unreliable, inaccurate, or inconsistent data. Poor-quality data undermines both model performance and the validity of subsequent explanations, rendering Shapley values or other interpretability techniques less reliable.
    \item \textbf{Low Coverage, High Cardinality, and Sparse Features.}. Features with numerous missing values or extremely high cardinality (e.g., more than 10k unique categories) can skew Shapley values unless carefully handled. For example, in our Home dataset, the \textit{subdivision} feature had over 10,000 unique values, making explanations cumbersome. The \textit{present\_use} feature was removed because 92\% of homes fell into a single category (“Single Family Detached”), limiting its contribution to predictive power and interpretability.
    \item \textbf{Ethical and Legal Restrictions.} Exclude or properly handle features tied to sensitive attributes (e.g., race, gender) to comply with regulations and prevent unethical bias. In many jurisdictions, using demographic data for certain decisions is prohibited or heavily regulated, and may also inadvertently introduce discriminatory outcomes.
    \item \textbf{Highly Correlated Features.} Including features that are highly correlated with others can distort the interpretation of Shapley values, as the importance might be unfairly attributed or spread across correlated features. Consider removing or combining these features. Among highly correlated variables, to decide which one to keep, one could use domain knowledge to identify the feature(s) with clearer causal relevance. If domain insight is unavailable, practitioners use preliminary feature importance analyses (e.g., random forest importance, correlation matrix) to retain the most impactful feature; or analyze the cross-validation model performance.
    \item \textbf{No or Low Impact Features.} Features that do not significantly influence predictions can be removed after testing with exploratory data analysis, correlation analysis, or model-based importance metrics. For example, if initial Shapley or permutation importance reveals that a feature (e.g, “view\_otherwater” in our home price prediction use case) contributes negligibly to predictions, it can be excluded, simplifying subsequent explanations.

\end{itemize}

\subsection{Step-by-Step Protocol}
The protocol can be divided into the following steps to ensure a consistent and thorough approach:

\begin{enumerate}
\item \textbf{Initial Data Collection and Preprocessing}
\begin{itemize}
    \item Gather all potential features.
    \item Apply basic cleaning (remove obvious errors, standardize formats, handle missing data).
    \item Document any transformations (e.g., log transformations, scaling) performed.
\end{itemize}

\item \textbf{Collecting Domain Expertise}
\begin{itemize}
    \item Conduct interviews or workshops with domain experts to ascertain which features have clear business or operational relevance.
    \item Flag features that are not easily explainable or contradict known domain relationships.
\end{itemize}

\item \textbf{Exploratory Data Analysis}
\begin{itemize}
    \item Generate descriptive statistics, correlation matrices, and distribution plots.
    \item Identify high-cardinality features, missing-value patterns, or candidate transformations (e.g., binning continuous variables).
\end{itemize}

\item \textbf{Feature Reduction and Preliminary Importance Scoring}
\begin{itemize}
    \item Use methods such as correlation thresholds, variance-based feature selection, or tree-based feature importance to shortlist candidate features.
    \item Document any decisions to exclude features at this stage.
\end{itemize}

\item \textbf{Ethical and Legal Review}
\begin{itemize}
    \item Ensure compliance with relevant laws and ethical standards by removing or anonymizing sensitive attributes.
    \item If neccessary, consult legal experts or ethics committees for final approval.
\end{itemize}

\item \textbf{Iterative Explainable Feature Refinement}
\begin{itemize}
    \item Train a preliminary model (e.g., gradient boosting, random forest).
    \item Apply explainability methods (e.g., SHAP) to assess each feature’s marginal contribution.
    \item Remove or modify features exhibiting minimal importance or interpretable value.
    \item Repeat iteratively until a stable set of informative, ethically sound, and interpretable features emerges (see Fig. \ref{iterative_feature_refinement}).
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{figures/iterative_feature_refinement.png}
\caption{\label{iterative_feature_refinement}An iterative workflow for feature selection and refinement in building explainable machine learning models. The process involves (1) selecting a subset of features from the previous steps in the protocol, (2) training and scoring a preliminary model, (3) conducting explainability analysis using methods like SHAP, and (4) refining features based on insights from the analysis.}
\end{figure}

\item \textbf{Final Feature Set and Documentation}
\begin{itemize}
    \item Produce a comprehensive record of features retained, rationale for exclusion, and data preprocessing steps.
    \item Ensure reproducibility by version-controlling code and storing raw data.
    \item This final set forms the basis for the deployment model used in decision-making.
\end{itemize}

\end{enumerate}

By following this protocol, practitioners can systematically evaluate and refine a robust feature set tailored for explainable machine learning models. Incorporating domain expertise, data quality checks, redundancy screening, ethical considerations, and iterative refinement processes ensure that final models maintain both high predictive performance and meaningful interpretability. This structured approach not only enhances transparency and trustworthiness of ML solutions in high-stakes domains but also supports efficient business operations and adherence to regulatory frameworks.

\end{document}