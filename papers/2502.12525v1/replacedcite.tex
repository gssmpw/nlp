\section{Related Work}
\subsection{Feature Attribution Methods}
% \textcolor{red}{
% \begin{enumerate}
% \item Definition and Scope:
%     \begin{enumerate}
%     \item A specific subset of XAI methods that focus on quantifying the contribution or importance of individual features to a model's prediction.
%     \item Typically local, providing feature importance for individual predictions, but results can be aggregated for global insights.
%     \end{enumerate}
% \item Addictive feature attribution: Attributions sum to a specific value, such as the model’s prediction. \AB{Maybe mention that some are dependent on the model structure whereas others are model agnostic} Examples:
%     \begin{enumerate}
%     \item LIME: feature coefficient from local linear surrogate models \AB{model agnostic}
%     \item DeepLIFT: difference from reference activation of each neuron \AB{I might generalize a bit here rather than just mention DeepLift - maybe methods that extract information from the model structure such as DeepLift}
%     \item Shapley value: cooperative game theory \AB{I would split into model agnostic vs model specific}
% \end{enumerate}
% \end{enumerate}
% }

Feature attribution methods quantify individual feature contributions to an ML model’s prediction, either locally (for specific instances) or globally (aggregated across all predictions). Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s output, are notable for their clear and interpretable decomposition of predictions.
Feature attribution methods can be categorized into (i) model-agnostic methods, which generate explanations by sampling predictions from a black-box model. This includes method such as LIME (Local Interpretable Model-agnostic Explanations)____, SHAP____ and Shapley sampling values____; and (ii) model-dependent methods, which leverages the specific ML model architecture. This includes techniques such as using linear models, Saliency Maps____, DeepLift____, and Integrated Gradients____.

% Feature attribution is a key subset of XAI that quantifies the contribution or importance of individual features to an ML model’s prediction. Typically, these methods operate locally, offering feature importance for individual predictions, but results can be aggregated to provide global insights into model behavior. Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s prediction, are particularly notable for their clear and interpretable decomposition of predictions. Feature attribution methods can be either model-agnostic, treating the model as a black box, or model-dependent, leveraging the specific structure of the ML model. Prominent approaches to feature attribution include:

% \textbf{Surrogate Models} involve fitting inherently interpretable models, such as simpler decision trees,  linear models or metamodels, to approximate complex model behaviors, enabling a simplified assessment of feature importance____.

% \textbf{Gradient-Based Methods} \AB{analyze the gradients of a model's output with respect to its input features to determine their importance, and backpropagation exploits the chain rule to efficiently calculate gradients during the backward pass through the neural network. Examples of gradient based methods are:
% \item Saliency Maps: gradients of a model output are computed with respect to the input. The gradients highlight which input features (pixels) are most influential to the outcome____.
% \item Integrated Gradients: model gradients are (approximately) integrated along a path from a baseline input to the actual input to determine the contribution of each input feature____.
% \item DeepLIFT (Deep Learning Important FeaTures): similar to integrated gradients, feature relevance scores are derived by comparing activations of the neurons in a neural network with those of a baseline activation; and differences are propagated back through the layers of the NN____.
% \item  Layer-wise Relevance Propagation (LRP), works the same way as DeepLift but with a zero baseline.____}

% analyze the gradients of the model's output with respect to input features to determine their importance. For example, Saliency Maps compute gradients to highlight input features most influential to the output____. Integrated Gradients provide a comprehensive attribution by accumulating gradients along a path from a baseline input to the actual input____.

% \textbf{Perturbation-Based Methods} evaluate feature importance by observing changes in the model's output when input features are altered. Examles are:
% \item LIME (Local Interpretable Model-agnostic Explanations): generates a simple interpretable model to approximate the output of a Black-box ML model locally around a specific prediction. Feature importances for a data instance are derived from the surrogate model____. 
% \item SHAP leverages Shapley values from cooperative game theory to fairly distribute a prediction among input features, ensuring consistency and local accuracy____.




% \textbf{Example-Based Explanations} identify minimal feature modifications required to change a model’s prediction, emphasizing impactful features. Counterfactual Explanations are a common approach in this category, which highlight how small changes to input features would alter outcomes____.
% % \AB{They don't all offer actionable outcomes - only actionable recourse models, so I've removed that last part} 


% Among these, Shapley value-based methods, such as SHAP, have gained prominence due to their solid theoretical foundation and desirable properties____. However, their practical application can be complex and computationally intensive, resulting in a variety of estimation algorithms and adaptations to improve efficiency and applicability____.

\subsection{Shapley Values}

Shapley values____, originating from cooperative game theory, describe how the payout of a game should be divided fairly among a coalition of players, and have several useful properties:
(i) Efficiency: individual player payouts must sum to the total payout minus the original (null player) payout,
(ii) Symmetry: when two players contribute equally to a game (over all possible player coalitions), they should be assigned the same payout,
(iii) Dummy: a player that does not influence the game should be assigned zero payout,
(iv) Additivity: For a game that includes multiple sub-games and a summed total payout, the total payout for a single player must equal the sum of their payouts for each individual sub-game.

The Shapley value for a player \( i \) is defined as:
\begin{equation}
\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \, (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right],
\label{eq:shapleyvalues}
\end{equation}
where \( N \) is the set of all players, \( S \) is a subset of \( N \) players not containing player \( i \), \( v(S) \) represents the payout (value function) based on the subset of players \( S \), and \( |S| \) denotes the cardinality of set \( S \). Equation (\ref{eq:shapleyvalues}) calculates the weighted average of the marginal contributions of player \( i \) across all possible subsets \( S \). 

This technique has been adapted to derive explanations from black-box ML models by analogizing the model prediction as the game payout and the model features as the game players____. However, computing Shapley values in ML settings can be complex and computationally intensive, which has spawned a variety of estimation algorithms____.

\subsection{Feature Removal in Shapley Value Estimation}
% \textcolor{red}{Discuss different approaches to feature removal in shapley value estimation, such as single value and distributional (uniform, marginal, and conditional).}

% The Shapley value for a feature \( i \) is defined as:
% \begin{equation}
% \phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \, (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right],
% \label{eq:shapleyvalues}
% \end{equation}
% where \( N \) is the set of all features, \( S \) is a subset of \( N \) not containing feature \( i \), \( v(S) \) represents the value function (e.g., model prediction) based on the subset \( S \), and \( |S| \) denotes the cardinality of set \( S \). Equation (\ref{eq:shapleyvalues}) calculates the weighted average of the marginal contributions of feature \( i \) across all possible subsets \( S \). 

During Shapley value estimation, feature removal simulates the absence of a feature by replacing or marginalizing its effect when computing the value function \( v(S) \)____. The method of feature removal can significantly influence the resulting feature attribution values and their interpretation____. Two common strategies are single-value removal and distributional removal____.

Single-value removal replaces the feature to be removed with a fixed reference value. The set of fixed reference values is often termed the baseline____. A common choice is zero, but other values like the mean or median of the feature can also be used. \( v(S) \) is computed by setting the features not in \( S \) (denoted as \(\bar{S}\)) to their corresponding baseline values. While straightforward, this method may introduce unrealistic data instances in the Shapley value estimations that can result in misleading attributions____. Rather than substitute a feature with a single value, distributional removal integrates over a range of possible values, considering the feature's empirical distribution. There are various ways to do this, we examine 3 common methods:

% \textbf{Single-Value Removal:} This approach replaces the feature to be removed with a fixed reference value. The set of fixed reference values is often termed the baseline____. A common choice is zero, but other values like the mean or median of the feature can also comprise the baseline. While straightforward, this method may introduce unrealistic data instances in the Shapley value estimations that can result in misleading attributions.

% \textbf{Distributional Removal:} Rather than substitute a feature with a single value, distributional removal integrates over a range of possible values, considering the feature's empirical distribution____. There are various ways to do this, we examine 3 common methods:

\begin{itemize}
    \item \textit{Uniform Distribution:} A feature is replaced with values sampled uniformly across its range. This technique assumes all feature values are equally probable, which might not align with the actual data distribution.
    \item \textit{Marginal Distribution:} A feature is replaced by sampling its marginal distribution. This approach breaks dependencies between features, potentially leading to implausible data points in the Shapley value estimation. The value function, \( v(S) \), is then the expected value of the model output when features not in \( S \) are marginalized independently:
    
    \begin{equation}
    v(S) = \mathbb{E}_{p(x_{\bar{S}})}[f(x_S, x_{\bar{S}})].
    \label{eq:shapleyvalues-marginal}
    \end{equation}
    
    \item \textit{Conditional Distribution:} A feature is replaced by sampling its conditional distribution relative to other features. This method preserves dependencies among features, resulting in more realistic data instances in the Shapley value estimation but at the cost of increased computational complexity. Furthermore, multicollinearity between features may cause non-zero attributions for features with no influence on the model outcome____. The value function is the expected value given the features in \( S \), considering their dependencies:
    
    \begin{equation}
    v(S) = \mathbb{E}_{p(x_{\bar{S}} \mid x_S)}[f(x_S, x_{\bar{S}})].
    \label{eq:shapleyvalues-conditional}
    \end{equation}
\end{itemize}

% - \textit{Uniform Distribution:} A feature is replaced with values sampled uniformly across its range. This technique assumes all feature values are equally probable, which might not align with the actual data distribution.

% - \textit{Marginal Distribution:} A feature is replaced by sampling its marginal distribution. This approach breaks dependencies between features, potentially leading to implausible data points in the Shapley value estimation.

% - \textit{Conditional Distribution:} A feature is replaced by sampling its conditional distribution relative to other features. This method preserves dependencies among features, resulting in more realistic data instances in the Shapley value estimation but at the cost of increased computational complexity. Furthermore, multicollinearity between features may cause non-zero attributions for features with no influence on the model outcome.

% In the context of feature removal, for single-value removal, \( v(S) \) is computed by setting the features not in \( S \) (denoted as \(\bar{S}\)) to a fixed baseline value. For distributional removal, with marginal distribution, \( v(S) \) is the expected value of the model output when features not in \( S \) are marginalized independently:
% \begin{equation}
% v(S) = \mathbb{E}_{p(x_{\bar{S}})}[f(x_S, x_{\bar{S}})].
% \label{eq:shapleyvalues-marginal}
% \end{equation}
% With conditional distribution, \( v(S) \) is the expected value given the features in \( S \), considering their dependencies:
% \begin{equation}
% v(S) = \mathbb{E}_{p(x_{\bar{S}} \mid x_S)}[f(x_S, x_{\bar{S}})].
% \label{eq:shapleyvalues-conditional}
% \end{equation}

Selecting the appropriate feature removal approach is important for generating meaningful and reliable Shapley value-based explanations.