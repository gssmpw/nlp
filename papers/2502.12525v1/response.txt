\section{Related Work}
\subsection{Feature Attribution Methods}
% \textcolor{red}{
% \begin{enumerate}
% \item Definition and Scope:
%     \begin{enumerate}
%     \item A specific subset of XAI methods that focus on quantifying the contribution or importance of individual features to a model's prediction.
%     \item Typically local, providing feature importance for individual predictions, but results can be aggregated for global insights.
%     \end{enumerate}
% \item Addictive feature attribution: Attributions sum to a specific value, such as the model’s prediction. \AB{Maybe mention that some are dependent on the model structure whereas others are model agnostic} Examples:
%     \begin{enumerate}
%     \item LIME: feature coefficient from local linear surrogate models \AB{model agnostic} (Freire et al., "Understanding Black-Box Predictions via Interpretable Model-Agnostic Explanations")____, SHAP (Lundberg and Lee, “A Unified Approach to Interpreting Model Predictions”)____ and Shapley value: cooperative game theory \AB{I would split into model agnostic vs model specific} (Strumbelj & Kononenko, "Explaining instance classifications with feature-importances")____; and (ii) model-dependent methods, which leverages the specific ML model architecture. This includes techniques such as using linear models, Saliency Maps \AB{I might generalize a bit here rather than just mention DeepLift - maybe methods that extract information from the model structure such as DeepLift} (Simonyan et al., "Deep Inside Convolutional Networks")____, DeepLIFT (Ancona et al., "Understanding Black-Box Predictions via Feature Importance" )____, and Integrated Gradients \AB{I would split into model agnostic vs model specific} (Sundararajan et al., "Axiomatic attribution for deep neural networks")____.

Feature attribution methods quantify individual feature contributions to an ML model’s prediction, either locally (for specific instances) or globally (aggregated across all predictions). Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s output, are notable for their clear and interpretable decomposition of predictions.
Feature attribution methods can be categorized into (i) model-agnostic methods, which generate explanations by sampling predictions from a black-box model. This includes method such as LIME \AB{Local Interpretable Model-agnostic Explanations} (Ribeiro et al., “Model-Agnostic Interpretability of Machine Learning” )____, SHAP \AB{(Lundberg and Lee, “A Unified Approach to Interpreting Model Predictions”)____} and Shapley sampling values \AB{Shapley value: cooperative game theory} (Strumbelj & Kononenko, "Explaining instance classifications with feature-importances")____; and (ii) model-dependent methods, which leverages the specific ML model architecture. This includes techniques such as using linear models, Saliency Maps \AB{(Simonyan et al., "Deep Inside Convolutional Networks")____} DeepLIFT \AB{Ancona et al., "Understanding Black-Box Predictions via Feature Importance"} (Ancona et al., “Understanding Black-Box Predictions via Feature Importance”)____ and Integrated Gradients \AB{(Sundararajan et al., "Axiomatic attribution for deep neural networks")____}.

% Feature attribution is a key subset of XAI that quantifies the contribution or importance of individual features to an ML model’s prediction. Typically, these methods operate locally, offering feature importance for individual predictions, but results can be aggregated to provide global insights into model behavior. Additive feature attribution methods, which sum feature contributions to a specific value, such as the model’s prediction, are particularly notable for their clear and interpretable decomposition of predictions. Feature attribution methods can be either model-agnostic, treating the model as a black box, or model-dependent, leveraging the specific structure of the ML model. Prominent approaches to feature attribution include:

% \textbf{Model-Agnostic Methods:} LIME (Ribeiro et al., “Model-Agnostic Interpretability of Machine Learning”)____ and SHAP \AB{(Lundberg and Lee, “A Unified Approach to Interpreting Model Predictions”)____}.

% \textbf{Model-Dependent Methods:} Techniques such as using linear models, Saliency Maps \AB{(Simonyan et al., "Deep Inside Convolutional Networks")____}, DeepLIFT (Ancona et al., “Understanding Black-Box Predictions via Feature Importance”)____, and Integrated Gradients \AB{(Sundararajan et al., "Axiomatic attribution for deep neural networks")____}.

\subsection{Shapley Values}

Selecting the appropriate feature removal approach is important for generating meaningful and reliable Shapley value-based explanations.
        === INPUT TEXT ENDS ABOVE THIS LINE (DO NOT INCLUDE THIS LINE IN OUTPUT) ===