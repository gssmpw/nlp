\section{Related Work}
\textbf{Offline Model-based RL} trains a dynamics model using offline data to approximate environmental dynamics and performs conservative policy optimization~\cite{lu2021revisiting} with model-generate data to mitigate OOD issues. 
    MOPO~\cite{mopo} penalizes state-action pairs with high model uncertainty, effectively discouraging the agent from selecting OOD actions. 
    MOReL~\cite{morel} constructs a pessimistic Markov decision process to prevent the agent from entering OOD regions, ensuring safer and more reliable policy learning. 
    COMBO~\cite{combo} extends Conservative Q-learning~\cite{kumar2020conservative} to the model-based setting by regularizing the value function on OOD samples.
    RAMBO~\cite{rambo} employs an adversarial training framework, optimizing both the policy and model jointly to ensure accurate transition predictions while maintaining robustness. 
    MOBILE~\cite{mobile} incorporates penalties into value learning by utilizing uncertainty in Bellman function estimates derived from ensemble models.
    SAMBO~\cite{luo2024sambo} imposes reward penalties and fosters exploration by incorporating model and policy shifts inferred through a probabilistic inference framework.

    \textbf{DICE-based Methods.} Distribution Correction Estimation~(DICE) is a technique that estimates stationary distribution ratios using duality theory.
    DICE has shown superior performance in evaluating discrepancies between distributions and has been widely applied in various RL domains, including off-policy evaluation~(OPE)~\cite{nachum2019dualdice}, offline imitation learning~(IL)~\cite{ma2022smodice}, and offline RL~\cite{algaedice}. 
    In offline RL, DICE-based methods correct distribution shifts between offline data and environmental dynamics, formulating a tractable maximin optimization objective. 
    For instance, AlgaeDICE~\cite{algaedice} pioneers the application of DICE-based methods in offline RL, employing regularized dual objectives and Lagrangian techniques to address distribution shift challenges. 
    OptiDICE~\cite{optidice} incorporates the Bellman flow constraint and directly estimates stationary distribution corrections for the optimal policy, eliminating the need for policy gradients. 
    ODICE~\cite{odice} combines orthogonal-gradient updates with DICE to enforce state-action-level constraints. 
    Unlike these model-free approaches, DAMO is, to the best of our knowledge, the first method to promote DICE insights in the model-based setting. 
    While model-free methods typically handle discrepancies between two distributions, DAMO skillfully extends this concept by managing the differences across three distributions, using a divergence upper bound to ensure alignment.

        \begin{figure*}[t]
            \centering
            \vspace{-0.15in}
            \includegraphics[width=0.9\linewidth]{diagram.png}
            \vspace{-0.2in}
            \caption{Intuiative examples for demystifying OOD issues in offline model-based RL. 
            The behavior policy~$\pi_{\beta}$ primarily favors the top road~($a_{1}$) with a minor representation of the bottom road~($a_{3}$) and never selects the middle road~($a_{2}$). 
            Consequently, the dynamics model lacks representation of medium road~($a_{2}$) and treats the $a_{3}$ as an OOD action.
            The policy $\pi$, trained on both offline and synthetic data, avoids $a_{3}$ when deployed in the real environment but struggles to evaluate $a_{2}$ due to its absence in $M$, resulting in occasionally getting into OOD state. 
            In DAMO, $\pi$ is constrained by $\pi_{\beta}$, guiding the agent to consistently select $a_{1}$. At the bottom, we show that while data alignment ensures a similar distribution between offline and synthetic data, it fails to guarantee behavior consistency between $M$ and $T$. 
            % Therefore, DAMO focuses on constraining the discrepancies between $\pi$ and $\pi_{\beta}$.
            }
            \label{fig-diagram}
            \vspace{-1em}
        \end{figure*}