[
  {
    "index": 0,
    "papers": [
      {
        "key": "lu2021revisiting",
        "author": "Lu, Cong and Ball, Philip J and Parker-Holder, Jack and Osborne, Michael A and Roberts, Stephen J",
        "title": "Revisiting design choices in offline model-based reinforcement learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "mopo",
        "author": "Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu",
        "title": "Mopo: Model-based offline policy optimization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "morel",
        "author": "Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten",
        "title": "Morel: Model-based offline reinforcement learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "combo",
        "author": "Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea",
        "title": "Combo: Conservative offline model-based policy optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kumar2020conservative",
        "author": "Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey",
        "title": "Conservative q-learning for offline reinforcement learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "rambo",
        "author": "Rigter, Marc and Lacerda, Bruno and Hawes, Nick",
        "title": "Rambo-rl: Robust adversarial model-based offline reinforcement learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "mobile",
        "author": "Sun, Yihao and Zhang, Jiaji and Jia, Chengxing and Lin, Haoxin and Ye, Junyin and Yu, Yang",
        "title": "Model-Bellman inconsistency for model-based offline reinforcement learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "luo2024sambo",
        "author": "Luo, Wang and Li, Haoran and Zhang, Zicheng and Han, Congying and Lv, Jiayu and Guo, Tiande",
        "title": "SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "nachum2019dualdice",
        "author": "Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong",
        "title": "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ma2022smodice",
        "author": "Ma, Yecheng Jason and Shen, Andrew and Jayaraman, Dinesh and Bastani, Osbert",
        "title": "Smodice: Versatile offline imitation learning via state occupancy matching"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "algaedice",
        "author": "Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale",
        "title": "Algaedice: Policy gradient from arbitrary experience"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "algaedice",
        "author": "Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale",
        "title": "Algaedice: Policy gradient from arbitrary experience"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "optidice",
        "author": "Lee, Jongmin and Jeon, Wonseok and Lee, Byungjun and Pineau, Joelle and Kim, Kee-Eung",
        "title": "Optidice: Offline policy optimization via stationary distribution correction estimation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "odice",
        "author": "Mao, Liyuan and Xu, Haoran and Zhang, Weinan and Zhan, Xianyuan",
        "title": "Odice: Revealing the mystery of distribution correction estimation via orthogonal-gradient update"
      }
    ]
  }
]