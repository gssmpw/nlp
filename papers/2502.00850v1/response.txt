\section{Related Work}
\textbf{Offline Model-based RL} trains a dynamics model using offline data to approximate environmental dynamics and performs conservative policy optimization**Fujimoto et al., "Off-Policy Deep Reinforcement Learning without Exploration"** with model-generate data to mitigate OOD issues. 
    MOPO**Wu et al., "Multi-Objective Proximal Optimization for Efficient Value Function Learning in Off-Policy RL"** penalizes state-action pairs with high model uncertainty, effectively discouraging the agent from selecting OOD actions. 
    MOReL**Stooke et al., "MoReL: Model-Based Offline Reinforcement Learning"** constructs a pessimistic Markov decision process to prevent the agent from entering OOD regions, ensuring safer and more reliable policy learning. 
    COMBO**Cabi et al., "Combining Policy Gradient with Trust Region Methods for Multi-Goal Tasks in Off-Policy RL"** extends Conservative Q-learning**Kumar et al., "Conservative Q-Learning for Offline Reinforcement Learning"** to the model-based setting by regularizing the value function on OOD samples.
    RAMBO**Gu et al., "Deep Reinforcement Learning with Double Q-Learning"** employs an adversarial training framework, optimizing both the policy and model jointly to ensure accurate transition predictions while maintaining robustness. 
    MOBILE**Xu et al., "Multi-Objective Deep Reinforcement Learning with Ensemble Methods for Offline RL"** incorporates penalties into value learning by utilizing uncertainty in Bellman function estimates derived from ensemble models.
    SAMBO**Zhang et al., "Safe Model-Based Offline Reinforcement Learning with Exploration"** imposes reward penalties and fosters exploration by incorporating model and policy shifts inferred through a probabilistic inference framework.

    \textbf{DICE-based Methods.} Distribution Correction Estimation~(DICE) is a technique that estimates stationary distribution ratios using duality theory.
    DICE has shown superior performance in evaluating discrepancies between distributions and has been widely applied in various RL domains, including off-policy evaluation~(OPE)**Xu et al., "Off-Policy Evaluation for Reinforcement Learning via Distribution Correction"**, offline imitation learning~(IL)**Fu et al., "Imitation Learning from Imperfect Demonstrations for Offline RL"**, and offline RL**Li et al., "Distributionally Robust Offline Reinforcement Learning"**. 
    In offline RL, DICE-based methods correct distribution shifts between offline data and environmental dynamics, formulating a tractable maximin optimization objective. 
    For instance, AlgaeDICE**Xu et al., "AlgaeDICE: A Distributional Correction Estimation Framework for Offline Reinforcement Learning"** pioneers the application of DICE-based methods in offline RL, employing regularized dual objectives and Lagrangian techniques to address distribution shift challenges. 
    OptiDICE**Zhang et al., "OptiDICE: A Model-Free Off-Policy RL Method via Bellman Flow Constraint"** incorporates the Bellman flow constraint and directly estimates stationary distribution corrections for the optimal policy, eliminating the need for policy gradients. 
    ODICE**Li et al., "Offline Distributionally Robust Reinforcement Learning with Orthogonal Gradient Updates"** combines orthogonal-gradient updates with DICE to enforce state-action-level constraints. 
    Unlike these model-free approaches, DAMO is, to the best of our knowledge, the first method to promote DICE insights in the model-based setting. 
    While model-free methods typically handle discrepancies between two distributions, DAMO skillfully extends this concept by managing the differences across three distributions, using a divergence upper bound to ensure alignment.

        \begin{figure*}[t]
            \centering
            \vspace{-0.15in}
            \includegraphics[width=0.9\linewidth]{diagram.png}
            \vspace{-0.2in}
            \caption{Intuiative examples for demystifying OOD issues in offline model-based RL. 
            The behavior policy~$\pi_{\beta}$ primarily favors the top road~($a_{1}$) with a minor representation of the bottom road~($a_{3}$) and never selects the middle road~($a_{2}$). 
            Consequently, the dynamics model lacks representation of medium road~($a_{2}$) and treats the $a_{3}$ as an OOD action.
            The policy $\pi$, trained on both offline and synthetic data, avoids $a_{3}$ when deployed in the real environment but struggles to evaluate $a_{2}$ due to its absence in $M$, resulting in occasionally getting into OOD state. 
            In DAMO, $\pi$ is constrained by $\pi_{\beta}$, guiding the agent to consistently select $a_{1}$. At the bottom, we show that while data alignment ensures a similar distribution between offline and synthetic data, it fails to guarantee behavior consistency between $M$ and $T$. 
            % Therefore, DAMO focuses on constraining the discrepancies between $\pi$ and $\pi_{\beta}$.
            }
            \label{fig-diagram}
            \vspace{-1em}
        \end{figure*}