@article{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S},
  journal={A Bradford Book},
  year={2018}
}
@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning: State-of-the-art},
  pages={45--73},
  year={2012},
  publisher={Springer}
}
@inproceedings{sinha2022s4rl,
  title={S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics},
  author={Sinha, Samarth and Mandlekar, Ajay and Garg, Animesh},
  booktitle={Conference on Robot Learning},
  pages={907--917},
  year={2022},
  organization={PMLR}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group UK London}
}
@article{emerson2023offline,
  title={Offline reinforcement learning for safer blood glucose control in people with type 1 diabetes},
  author={Emerson, Harry and Guy, Matthew and McConville, Ryan},
  journal={Journal of Biomedical Informatics},
  volume={142},
  pages={104376},
  year={2023},
  publisher={Elsevier}
}
@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{algaedice,
  title={Algaedice: Policy gradient from arbitrary experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}
@inproceedings{optidice,
  title={Optidice: Offline policy optimization via stationary distribution correction estimation},
  author={Lee, Jongmin and Jeon, Wonseok and Lee, Byungjun and Pineau, Joelle and Kim, Kee-Eung},
  booktitle={International Conference on Machine Learning},
  pages={6120--6130},
  year={2021},
  organization={PMLR}
}
@article{odice,
  title={Odice: Revealing the mystery of distribution correction estimation via orthogonal-gradient update},
  author={Mao, Liyuan and Xu, Haoran and Zhang, Weinan and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2402.00348},
  year={2024}
}
@article{ma2022smodice,
  title={Smodice: Versatile offline imitation learning via state occupancy matching},
  author={Ma, Yecheng Jason and Shen, Andrew and Jayaraman, Dinesh and Bastani, Osbert},
  journal={arXiv preprint arXiv:2202.02433},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}
@article{mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}
@article{combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28954--28967},
  year={2021}
}
@article{TT,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{rambo,
  title={Rambo-rl: Robust adversarial model-based offline reinforcement learning},
  author={Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16082--16097},
  year={2022}
}
@inproceedings{mobile,
  title={Model-Bellman inconsistency for model-based offline reinforcement learning},
  author={Sun, Yihao and Zhang, Jiaji and Jia, Chengxing and Lin, Haoxin and Ye, Junyin and Yu, Yang},
  booktitle={International Conference on Machine Learning},
  pages={33177--33194},
  year={2023},
  organization={PMLR}
}
@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ international conference on intelligent robots and systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
@article{morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}
@article{luo2024ompo,
  title={OMPO: A Unified Framework for RL under Policy and Dynamics Shifts},
  author={Luo, Yu and Ji, Tianying and Sun, Fuchun and Zhang, Jianwei and Xu, Huazhe and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2405.19080},
  year={2024}
}
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}
@article{lu2021revisiting,
  title={Revisiting design choices in offline model-based reinforcement learning},
  author={Lu, Cong and Ball, Philip J and Parker-Holder, Jack and Osborne, Michael A and Roberts, Stephen J},
  journal={arXiv preprint arXiv:2110.04135},
  year={2021}
}
@article{li2024settling,
  title={Settling the sample complexity of model-based offline reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Chi, Yuejie and Wei, Yuting},
  journal={The Annals of Statistics},
  volume={52},
  number={1},
  pages={233--260},
  year={2024},
  publisher={Institute of Mathematical Statistics}
}
@article{mao2024offline,
  title={Offline reinforcement learning with ood state correction and ood action suppression},
  author={Mao, Yixiu and Wang, Cheems and Chen, Chen and Qu, Yun and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2410.19400},
  year={2024}
}
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
@article{li2023proto,
  title={Proto: Iterative policy regularized offline-to-online reinforcement learning},
  author={Li, Jianxiong and Hu, Xiao and Xu, Haoran and Liu, Jingjing and Zhan, Xianyuan and Zhang, Ya-Qin},
  journal={arXiv preprint arXiv:2305.15669},
  year={2023}
}
@inproceedings{sikchi2023imitation,
  title={Imitation from arbitrary experience: A dual unification of reinforcement and imitation learning methods},
  author={Sikchi, Harshit and Zhang, Amy and Niekum, Scott},
  booktitle={Workshop on Reincarnating Reinforcement Learning at ICLR 2023},
  year={2023}
}

@article{eysenbach2022mismatched,
  title={Mismatched no more: Joint model-policy optimization for model-based rl},
  author={Eysenbach, Benjamin and Khazatsky, Alexander and Levine, Sergey and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23230--23243},
  year={2022}
}

@inproceedings{wang2007dual,
  title={Dual representations for dynamic programming and reinforcement learning},
  author={Wang, Tao and Bowling, Michael and Schuurmans, Dale},
  booktitle={2007 IEEE International symposium on approximate dynamic programming and reinforcement learning},
  pages={44--51},
  year={2007},
  organization={IEEE}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{arndt2020meta,
  title={Meta reinforcement learning for sim-to-real domain adaptation},
  author={Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
  booktitle={2020 IEEE international conference on robotics and automation (ICRA)},
  pages={2725--2731},
  year={2020},
  organization={IEEE}
}

@article{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen},
  journal={Cambridge UP},
  year={2004}
}

@book{fenchel2014conjugate,
  title={On conjugate convex functions},
  author={Fenchel, Werner},
  year={2014},
  publisher={Springer}
}

@article{ji2022update,
  title={When to update your model: Constrained model-based reinforcement learning},
  author={Ji, Tianying and Luo, Yu and Sun, Fuchun and Jing, Mingxuan and He, Fengxiang and Huang, Wenbing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23150--23163},
  year={2022}
}

@inproceedings{dai2017learning,
  title={Learning from conditional distributions via dual embeddings},
  author={Dai, Bo and He, Niao and Pan, Yunpeng and Boots, Byron and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1458--1467},
  year={2017},
  organization={PMLR}
}

@article{lyu2022mildly,
  title={Mildly conservative q-learning for offline reinforcement learning},
  author={Lyu, Jiafei and Ma, Xiaoteng and Li, Xiu and Lu, Zongqing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1711--1724},
  year={2022}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{wu2019behavior,
  title={Behavior regularized offline reinforcement learning},
  author={Wu, Yifan and Tucker, George and Nachum, Ofir},
  journal={arXiv preprint arXiv:1911.11361},
  year={2019}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@article{manne1960linear,
  title={Linear programming and sequential decisions},
  author={Manne, Alan S},
  journal={Management Science},
  volume={6},
  number={3},
  pages={259--267},
  year={1960},
  publisher={INFORMS}
}

@article{niu2022trust,
  title={When to trust your simulator: Dynamics-aware hybrid offline-and-online reinforcement learning},
  author={Niu, Haoyi and Qiu, Yiwen and Li, Ming and Zhou, Guyue and Hu, Jianming and Zhan, Xianyuan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36599--36612},
  year={2022}
}

@article{pomerleau1988alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@inproceedings{tang2021model,
  title={Model selection for offline reinforcement learning: Practical considerations for healthcare settings},
  author={Tang, Shengpu and Wiens, Jenna},
  booktitle={Machine Learning for Healthcare Conference},
  pages={2--35},
  year={2021},
  organization={PMLR}
}

@article{xu2022policy,
  title={A policy-guided imitation approach for offline reinforcement learning},
  author={Xu, Haoran and Jiang, Li and Jianxiong, Li and Zhan, Xianyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4085--4098},
  year={2022}
}

@article{xu2023offline,
  title={Offline rl with no ood actions: In-sample learning via implicit value regularization},
  author={Xu, Haoran and Jiang, Li and Li, Jianxiong and Yang, Zhuoran and Wang, Zhaoran and Chan, Victor Wai Kin and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2303.15810},
  year={2023}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{luo2024sambo,
  title={SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning},
  author={Luo, Wang and Li, Haoran and Zhang, Zicheng and Han, Congying and Lv, Jiayu and Guo, Tiande},
  journal={arXiv preprint arXiv:2408.12830},
  year={2024}
}