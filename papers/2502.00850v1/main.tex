
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs} 
\usepackage{mathtools}
\usepackage{nicefrac}       


\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{enumitem}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Dual Alignment Maximin Optimization for Offline Model-based RL}

\begin{document}

\twocolumn[
\icmltitle{Dual Alignment Maximin Optimization for Offline Model-based RL}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chi Zhou}{sch}
\icmlauthor{Wang Luo}{sch}
\icmlauthor{Haoran Li}{sch}
\icmlauthor{Congying Han}{sch}
\icmlauthor{Tiande Guo}{sch}
\icmlauthor{Zicheng Zhang}{comp}

\end{icmlauthorlist}


\icmlaffiliation{comp}{JD, Beijing, China}
\icmlaffiliation{sch}{School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

\icmlkeywords{Machine Learning, Offline Reinforcement learning}

\vskip 0.3in]


\printAffiliationsAndNotice{}


\begin{abstract}

Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch.
While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, 
the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies.
In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, \textbf{D}ual \textbf{A}lignment \textbf{M}aximin \textbf{O}ptimization~(DAMO).
It is a unified framework to ensure both  \textit{model-environment policy consistency} and \textit{synthetic and offline data compatibility}. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates.
Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.

\end{abstract}

\section{Introduction}

    Offline reinforcement learning~(RL)~\cite{lange2012batch, levine2020offline} aims to learn policies from a pre-collected dataset generated by a behavioral policy within the real environment. 
    This paradigm helps avoid the safety risks and high costs associated with direct interactions, making RL applicable in real-life scenarios, such as healthcare decision-making support and autonomous driving~\cite{emerson2023offline, sinha2022s4rl, mnih2015human}. 
    Offline model-based RL~\cite{mopo, morel, mobile} improves upon this by training a dynamics model and using it to generate synthetic data for policy training or planning. 
    The introduction of dynamics models enhances the sample efficiency and allows the agent to answer counterfactual queries~\cite{levine2020offline}. 
    However, despite strong performance in learned models, policies often degrade significantly when deployed in real environments due to mismatches between synthetic and real distributions.

    Most prior works have focused on enhancing the reliability of synthetic samplings from learned models, and directly incorporating off-policy optimization methods, such as SAC~\cite{haarnoja2018soft}, to address this mismatch. 
    The representative approach is MOPO~\cite{mopo}, which quantifies model uncertainty by measuring the predicted variance of learned models and subsequently penalizes model-generated trajectories with high uncertainty. 
    This methodology has inspired subsequent research like MOReL~\cite{morel} and MOBILE~\cite{mobile}, which adopt similar uncertainty-aware frameworks. 
    These approaches help generate synthetic data that remains compatible with the offline dataset and construct conservative value functions to mitigate out-of-distribution~(OOD) actions. 
    However, creating a perfect learned model from limited offline datasets is impossible, and as illustrated in Fig.~\ref{fig-mopo}, simply combining these methods with off-policy mechanisms still fails to resolve the inherent discrepancies in policy behaviors between the learned model and the underlying environment, leading to inconsistent policy performance and leaving the distribution shift problem unsolved.

    In this paper, we trace the inherent root of distribution shift challenges in offline RL to the discrepancies between behavior and learning policies in the underlying environmental dynamics. 
    While optimizing for expected returns, we introduce a regularized policy optimization objective for offline RL, that constrains the visitation distribution discrepancies between the behavioral and the learning policies in real environments. 
    We further self-consistently incorporate synthetic data into this objective, deriving a novel maximin optimization objective for offline model-based RL.
    Building on these objectives, we propose Dual Alignment Maximin Optimization~(DAMO), a consistent actor-critic framework that ensures both coherent alignments between \textit{model and environment behavior} and \textit{model-generated and offline} data.

    DAMO iteratively applies maximin optimization in two phases. 
    In the inner minimization phase, equivalent to critic training, the focus is on the dual conservative value estimation.
    This value estimation implicitly harmonizes learning policy behaviors in both the model and the environment to avoid OOD states, while explicitly aligning synthetic and offline data to prevent OOD actions.
    In the outer maximization phase, corresponding to actor learning, policy improvements are guided by the inner value estimates through objective alignment. 
    This preserves reliably conservative behavior and effectively mitigates OOD issues throughout both policy optimization and deployment.
    Additionally, DAMO incorporates classifiers to approximate the data alignment terms, facilitating practical implementation.
    Extensive experiments showcase that DAMO successfully mitigates OOD issues and achieves competitive performance across various benchmarks. 
    Our contributions are summarized as the following:
    \begin{itemize}[leftmargin=0em, itemindent=1em,topsep=0em]
    \setlength{\itemsep}{1pt}
    \setlength{\parsep}{1pt}
    \setlength{\parskip}{1pt}
        \item We derive a consistent maximin object for offline model-based RL, ensuring both alignments between \textit{model and environment policy behaviors}, and \textit{synthetic and offline} data.
        \item We develop DAMO, a novel actor-critic framework that employs maximin optimization to achieve the \textit{dual conservative value estimation} and the \textit{consistent policy improvement}.
        \item We conduct extensive experiments to validate that DAMO effectively alleviates the distribution shift challenge, delivering superior performance across varying benchmark tasks.
    \end{itemize}

    \begin{figure}[t]
        \centering
        \subfigure{
        \begin{minipage}[b]{0.96\linewidth}
            \centering
            \includegraphics[width=\linewidth]{labe_for_introl.png}
            \vspace{-2.3em}
        \end{minipage}
        }
        \setcounter{subfigure}{0}
        \subfigure[MOPO]{
        \begin{minipage}{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{MOPO_emphood.png}
        \end{minipage}
        }
        \subfigure[MOBILE]{
        \begin{minipage}{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{MOBILE_emphood.png}
        \end{minipage}
        }
        \subfigure[DAMO]{
        \begin{minipage}{0.3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{DAMO_emphood.png}
        \end{minipage}
        }
        \vspace{-0.5em}
        \caption{The distribution of transition pair $(s, a, s^{\prime})$ across three datasets (Offline Data, Real Data, and Synthetic Data) are shown for the hopper-medium-expert task, comparing two SOTA methods (MOPO and MOBILE) with our proposed DAMO framework. 
        While MOPO and MOBILE align synthetic and offline data well (indicated by a high overlap), they fail to address OOD states in the real environment (circled by an ellipse drawn with a green dotted line). 
        In contrast, DAMO effectively eliminates the OOD region.}
        \label{fig-mopo}
    \end{figure}
    
\section{Related Work}
    \textbf{Offline Model-based RL} trains a dynamics model using offline data to approximate environmental dynamics and performs conservative policy optimization~\cite{lu2021revisiting} with model-generate data to mitigate OOD issues. 
    MOPO~\cite{mopo} penalizes state-action pairs with high model uncertainty, effectively discouraging the agent from selecting OOD actions. 
    MOReL~\cite{morel} constructs a pessimistic Markov decision process to prevent the agent from entering OOD regions, ensuring safer and more reliable policy learning. 
    COMBO~\cite{combo} extends Conservative Q-learning~\cite{kumar2020conservative} to the model-based setting by regularizing the value function on OOD samples.
    RAMBO~\cite{rambo} employs an adversarial training framework, optimizing both the policy and model jointly to ensure accurate transition predictions while maintaining robustness. 
    MOBILE~\cite{mobile} incorporates penalties into value learning by utilizing uncertainty in Bellman function estimates derived from ensemble models.
    SAMBO~\cite{luo2024sambo} imposes reward penalties and fosters exploration by incorporating model and policy shifts inferred through a probabilistic inference framework.

    \textbf{DICE-based Methods.} Distribution Correction Estimation~(DICE) is a technique that estimates stationary distribution ratios using duality theory.
    DICE has shown superior performance in evaluating discrepancies between distributions and has been widely applied in various RL domains, including off-policy evaluation~(OPE)~\cite{nachum2019dualdice}, offline imitation learning~(IL)~\cite{ma2022smodice}, and offline RL~\cite{algaedice}. 
    In offline RL, DICE-based methods correct distribution shifts between offline data and environmental dynamics, formulating a tractable maximin optimization objective. 
    For instance, AlgaeDICE~\cite{algaedice} pioneers the application of DICE-based methods in offline RL, employing regularized dual objectives and Lagrangian techniques to address distribution shift challenges. 
    OptiDICE~\cite{optidice} incorporates the Bellman flow constraint and directly estimates stationary distribution corrections for the optimal policy, eliminating the need for policy gradients. 
    ODICE~\cite{odice} combines orthogonal-gradient updates with DICE to enforce state-action-level constraints. 
    Unlike these model-free approaches, DAMO is, to the best of our knowledge, the first method to promote DICE insights in the model-based setting. 
    While model-free methods typically handle discrepancies between two distributions, DAMO skillfully extends this concept by managing the differences across three distributions, using a divergence upper bound to ensure alignment.

        \begin{figure*}[t]
            \centering
            \vspace{-0.15in}
            \includegraphics[width=0.9\linewidth]{diagram.png}
            \vspace{-0.2in}
            \caption{Intuiative examples for demystifying OOD issues in offline model-based RL. 
            The behavior policy~$\pi_{\beta}$ primarily favors the top road~($a_{1}$) with a minor representation of the bottom road~($a_{3}$) and never selects the middle road~($a_{2}$). 
            Consequently, the dynamics model lacks representation of medium road~($a_{2}$) and treats the $a_{3}$ as an OOD action.
            The policy $\pi$, trained on both offline and synthetic data, avoids $a_{3}$ when deployed in the real environment but struggles to evaluate $a_{2}$ due to its absence in $M$, resulting in occasionally getting into OOD state. 
            In DAMO, $\pi$ is constrained by $\pi_{\beta}$, guiding the agent to consistently select $a_{1}$. At the bottom, we show that while data alignment ensures a similar distribution between offline and synthetic data, it fails to guarantee behavior consistency between $M$ and $T$. 
            % Therefore, DAMO focuses on constraining the discrepancies between $\pi$ and $\pi_{\beta}$.
            }
            \label{fig-diagram}
            \vspace{-1em}
        \end{figure*}
 
\section{Preliminaries}
    \textbf{MDP.}
        We consider the Markov decision process~(MDP) defined by the six-element tuple $M = (\mathcal{S}, \mathcal{A}, T, r, \mu_{0}, \gamma)$, where $\mathcal{S}$ denotes the state space, $\mathcal{A}$ denotes the action space, $T(s^{\prime} | s, a)$ is the environment transition dynamics, $r(s, a, s^{\prime}) > 0$ is the reward function, $\mu_{0}$ is the initial state distribution, and $\gamma \in (0, 1)$ is the discounted factor. Given an MDP, the objective of RL is to find a policy $\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})$ that maximizes the cumulative reward from the environment. This objective can be formally expressed as: $\pi^{\star} = \arg\max_{\pi} \mathbb{E}_{\mu_{0}, \pi, T } [\sum_{t=0}^{\infty} \gamma^{t} r(s_{t}, a_{t} ,s_{t+1}) ]$. In the derivation of DAMO, we utilize the dual form of the RL objective~\cite{puterman2014markov}, which is represented as follows:
            \vspace{-0.12in}
    	\begin{equation*}
                % \vspace{-2em}
    		\pi^{\star} = \arg\max\limits_{\pi} \mathbb{E}_{ \rho^{\pi}_{T}} [r(s, a, s^{\prime})].
    	\end{equation*}
            \vspace{-0.25in}
            
    	Here, $\rho^{\pi}_{T}(s, a, s^{\prime})$ is the transition occupancy measure, characterizing the distribution of transition pairs $(s, a, s^{\prime})$ induced by policy $\pi$ under the dynamics $T$, defined by:
        \vspace{-0.15in}
        \begin{multline*}
            \rho_{T}^{\pi} (s,a,s') = (1-\gamma) \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}[s_t = s , a_t = a , 
            \\
            s_{t+1} = s' | s_0 \sim \mu_{0} , 
            a_t \sim \pi(\cdot|s_t) , s_{t+1} \sim T(\cdot | s_t,a_t)].
        \end{multline*}
        \vspace{-0.25in}
        
        Here, $\mu_{0}$ denotes the initial state distribution. For simplicity, we use shorthand notation $\rho^{\pi}_{T}$ in the remainder of the article.

    \textbf{Offline Model-based RL.}
        Offline model-based RL aims to find the optimal policy by leveraging a learned dynamics model $M$. 
        Given the offline dataset $\mathcal{D}_{R}$, $M$ is typically trained using maximum likelihood estimation, formulated as $\min_{M} \mathbb{E}_{s, a, s^{\prime} \sim \mathcal{D}_{R}} [-\log M(s^{\prime}| s, a)] $. 
        The training policy $\pi$ will undergo $k$-step rollout in learned dynamics model $M$ and the generated data will be added to a replay buffer $\mathcal{D}_{M}$. Then $\pi$ can be optimized using mini-batches of data sampled from $\mathcal{D}_{R} \cup \mathcal{D}_{M}$, where each datapoint is sampled from the offline dataset $\mathcal{D}_{R}$ with probability $f$ and the model dataset $\mathcal{D}_{M}$ with probability $1-f$. Throughout this work, we assume that the reward function $r(s, a, s^{\prime})$ is known, though it can also be treated as part of the model if unknown.

\section{Demystify OOD in Offline Model-based RL}
        
        In this section, as illustrated in Fig.~\ref{fig-diagram}, we explore the out-of-distribution~(OOD) issues in offline model-based RL. We focus on the importance of aligning both the \textit{model and offline data}, as well as the \textit{model and environment policy behavior}, which motivates the direction of this research.

        \subsection{OOD Issues in Offline RL}
        In offline RL, OOD problems~\cite{mao2024offline}  arise when agents encounter states or select actions that fall outside the distribution of the offline data during training or testing.
        
        \textbf{OOD actions} refer to actions that the behavior policy does not choose in specific states, rather than actions that are simply absent from the offline dataset. This implies that OOD actions are inherently state-dependent. 
        Thus some studies use the term \textit{OOD state-action pairs} to describe the state-action pair $(s, a)$ that does not appear in the offline dataset. Taking OOD actions during training can lead to severely inaccurate value estimation and significantly degraded policy performance in testing. Consequently, avoiding OOD actions has been a focus in previous offline RL research. 
        
        \textbf{OOD states} primarily arise in three main scenarios:
        1) The learned policy executes unreliable OOD actions, leading to transitions into OOD states. 
        2) The initial state of the real environment lies outside the offline dataset. Additionally, stochastic dynamics could unexpectedly transition the agent into OOD states, even if it takes in-distribution~(ID) actions in ID states.
        3) Nonstationary dynamics, such as disturbances in real-world environments, can introduce unexpected changes in state distributions. 
        While this factor is outside the scope of our offline model-based setting, it is an important consideration in real-world applications.
        
    \subsection{OOD Issues under Dynamics Model Integration}
    \label{bsec-OOD actions and states in offline model-based RL}

        OOD issues in offline model-based RL are intrinsically linked to the mismatch between synthetic and real data distributions, compounded in model-based settings, where model inaccuracies introduce additional sources of error.
        More extensive discussions are provided in Appendix~\ref{app-bsec-OOD actions and states in offline model-based RL}.

        \textbf{OOD Actions.}
        The model, $M$, is used to generate synthetic data for training, but it is essential to avoid taking OOD actions when rolling out policy in the model. This can trigger negative model exploitation~\cite{levine2020offline}, resulting in poor policy performance. Aligning the distributions of synthetic and offline data can effectively mitigate these issues as it ensures accurate value estimation.

        \textbf{OOD States.}
        The performance of the learning policy during deployment is critically dependent on the alignment of transition pairs between the learned model and the real environment. 
        Significant discrepancies between these transitions can cause performance degradation and lead to OOD states, highlighting the necessity of correcting transition biases during training.
        Even if the learned policy performs well in the model, it is important, during real-environment deployment, to avoid OOD states, for which we cannot accurately estimate the value function in the offline setting.

        In summary, while optimizing the policy $\pi$ to maximize returns, we should constrain it in two directions: 1) aligning the synthetic data $(s, a, s^{\prime})_{M}^{\pi}$ with the offline data $(s, a, s^{\prime})_{T}^{\pi_{\beta}}$, and 2) ensuring that $\pi$ exhibits consistent behavior in both dynamics models and real environments.

\section{Dual Alignment Maximin Optimization}
\label{sec-Method}

    Inspired by the analysis above, we propose Dual Alignment Maximin Optimization~(DAMO), a unified approach to mitigate both OOD actions and states. 
    We further explore the distinct roles of the inner minimization and the outer maximization in addressing the distribution shift challenge.
    
    
    \subsection{Maximin Objective for Unified Shift Mitigation}
    \label{bsec-A unified framework for offline model-based RL}
        
        We start from the root cause of distribution shifts, \textit{i.e.,} the discrepancy between learned and behavior policy behaviors in the underlying environment.
        To tackle this, we introduce a regularized objective that constrains the difference between these behaviors while optimizing for expected rewards:
        \begin{equation}\notag
        	\max\limits_{\pi} \mathbb{E}_{\rho_{T}^{\pi}} [ r(s, a, s^{\prime}) ] - \alpha D_{KL}(\rho_{T}^{\pi} \Vert \rho_{T}^{\pi_{\beta}}),
        	\label{eq-align offline data with real data}
        \end{equation}
        where $\alpha$ is a regularization hyperparameter.
        Next, we self-consistently incorporate synthetic data through a divergence upper bound, deriving the following surrogate objective:
        \begin{equation}
        	\max\limits_{\pi} \mathbb{E}_{\rho_{T}^{\pi}} [ r(s, a, s^{\prime})-\alpha\log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
        	]-\alpha D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi}).
        	\label{eq-surrogate objective}
        \end{equation}
        This surrogate objective integrates two essential elements: a data alignment regularizer $\log(\rho_{M}^{\pi}/\rho_{T}^{\pi_{\beta}})$ to ensure compatibility between synthetic and offline data, and a behavior alignment regularizer $D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})$ that maintains consistent policy performance across models and real environments. 
        
        While this objective~\eqref{eq-surrogate objective} theoretically addresses the synthetic-to-real distribution mismatch (as shown later in Sec.~\ref{bsec-conservative value estimation}), it is impractical to directly compute due to the inaccessibility of $\rho_{T}^{\pi}$. 
        To make this objective tractable, we apply the convex conjugate theorem into the dual formulation of reinforcement learning, reformulating it as a maximin optimization problem. 
        This reformulation forms the core of our proposed DAMO, a unified maximin framework for mitigating distribution shifts in offline model-based RL. 
        \begin{theorem}
            \eqref{eq-surrogate objective} is equivalent to the following problem:
            \begin{multline}
                \max\limits_{\pi} \min\limits_{\Tilde{Q}(s,a)} (1-\gamma)              \mathrm{E}_{s\sim\mu_{0},a\sim \pi} [\Tilde{Q}(s,a)] 
                         \\
                + \alpha \mathrm{E}_{\rho_{M}^{\pi}} [ f_{\star}(\Phi(s,a,s')/\alpha) ],
                \label{eq-final_objective}
            \end{multline}
        where $f_{\star}$ is the conjugate function of the convex function $f$, and $\Phi(s, a, s') = \log r(s, a, s^{\prime}) - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} + \tau^{\pi} Q(s, a) - Q(s, a)$, with $\tau^{\pi} Q(s, a) = \gamma \sum_{a'} Q(s', a') \pi(a'|s')$.
        Here, $\mu_{0}$ denotes the distribution of the initial state, and $\alpha$ is a hyperparameter to control the degree of conservatism. 
        \end{theorem}
        \subsection{Practical Implementation of DAMO}
    \label{bsec-Practical algorithm for DAMO}
        
        To implement the maximin optimization~\eqref{eq-final_objective} practically, we employ a classifier to approximate the data alignment term $\log(\rho_{M}^{\pi} /  \rho_{T}^{\pi_{\beta}})$. 
        Specifically, we train a classifier $h(s, a, s^{\prime})$ using the following loss function to distinguish transitions sampled from offline data and those generated by the model:
        \begin{multline}
            \min_{h} \dfrac{1}{|\mathcal{D}_{R}|} \sum_{(s,a,s') \in \mathcal{D}_{R}} \log h(s,a,s') 
            \\
            + \dfrac{1}{|\mathcal{D}_{M}|} \sum_{(s,a,s') \in \mathcal{D}_{M}} [ \log (1 - h(s,a,s')) ].
            \label{eq-classifier_loss}
        \end{multline}
        Here, $\mathcal{D}_{R}$ and $\mathcal{D}_{M}$ represent the offline and the model-generated data, respectively. Based on the learned classifier $h^{\star}(s, a, s^{\prime})$, the data alignment modification $\log(\rho_{M}^{\pi}/\rho_{T}^{\pi_{\beta}})$ can be computed using the following analytical expression:
        \begin{equation}
            \log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} = \log \dfrac{h^{\star}(s,a,s')}{1 - h^{\star}(s,a,s')}.
            \label{eq-data alignment term}
        \end{equation}

        Note that \eqref{eq-final_objective} can be divided into two phases: an inner value estimation phase and an outer policy improvement phase, resembling the structure of the actor-critic framework. 
        This structure is directly adapted to implement DAMO. 
        The complete DAMO framework is outlined in Algorithm~\ref{alg-DAMO}.

        \begin{algorithm}[t]
            \caption{DAMO}
            \label{alg-DAMO}
            \begin{algorithmic}[1]
                \REQUIRE Offline dataset $\mathcal{D}_{R}$, dynamics model $M$, actor network $\pi_{\phi}$, critic network $Q_{\theta}$, classifier $h_{\psi}$.
                
                \STATE Train the probabilistic dynamics model $M$ on $\mathcal{D}_{R}$
                \STATE Initialize the model dataset buffer $\mathcal{D}_{M} \leftarrow \varnothing$
                \FOR{each epoch}
                    \STATE Generate synthetic rollouts by model $M$
                    \STATE Add transition data in these rollouts to $\mathcal{D}_{M}$
                    \STATE Update classifier $h_{\psi}$ according to Eq.~\eqref{eq-classifier_loss}
                    \STATE Calculate $\log(\rho_{M}^{\pi}/\rho_{T}^{\pi_{\beta}})$ according to Eq.~\eqref{eq-data alignment term}
                    \STATE Update critic $Q_{\theta}$ via minimizing Eq.~\eqref{eq-final_objective}
                    \STATE Update actor $\pi_{\phi}$ via maximizing Eq.~\eqref{eq-final_objective}
                \ENDFOR
            \end{algorithmic}
        \end{algorithm}
        \subsection{Theoretical Insights for DAMO}
    \label{bsec-conservative value estimation}

        The core structure of DAMO involves mapping the inner minimization in \eqref{eq-final_objective} to the estimation of a dual conservative value $\Tilde{Q}(s, a)$, and the outer maximization to policy improvement based on this value. 
        In this subsection, we present deeper theoretical insights into the workings of DAMO.

        \textbf{Dual Conservative Value Estimation.}
        We first demonstrate the existence of both explicit and implicit reward penalties in the optimization objective~\eqref{eq-final_objective}, which contribute to the construction of the dual conservative value estimation.
        \begin{theorem}
            \label{thm-proposition}
            The optimal solution $Q_{\pi}^{\star}$ for the inner minimization optimization in \eqref{eq-final_objective} satisfies the following equation:
            \begin{equation*}
                Q_{\pi}^{\star} = r - \alpha \log \dfrac{\rho^{\pi}_{M}}{\rho^{\pi_{\beta}}_{T}} - \alpha f^{\prime} ( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}}) + \tau^{\pi} Q_{\pi}^{\star}.
            \end{equation*}
        \end{theorem}
        This indicates that DAMO inherently introduces an explicit data alignment modification, $\log (\rho^{\pi}_{M} / \rho^{\pi{\beta}}_{T})$, and an implicit behavior alignment adjustment, $f^{\prime} (\rho^{\pi}_{T} / \rho^{\pi}_{M})$. 
        Minimizing the inner objective effectively reshapes the reward function, $r(s, a, s^{\prime})$, into a refined reward, $\Tilde{r}(s,a,s')$, which represents the dual conservative value estimation:
        \begin{equation*}
            \Tilde{r} = r - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} - \alpha f^{\prime} ( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}}).
        \end{equation*}
        The data alignment modification $\log (\rho^{\pi}_{M} / \rho^{\pi{\beta}}_{T})$ penalizes transition $(s, a, s^{\prime})$ that exhibit discrepancies between synthetic and offline data, promoting alignment with in-distribution transitions.
        This helps ensure accurate value estimation for these transitions.
        Meanwhile, the behavior alignment adjustment $f^{\prime} (\rho^{\pi}_{T} / \rho^{\pi}_{M})$ focuses on that different dynamics impact the distribution of $(s, a, s^{\prime})$. 
        By penalizing these discrepancies, we encourage the agent to select policies that exhibit similar performance in both the underlying environment and the dynamics model, thus mitigating the impact of OOD states and ensuring consistent policy behavior between dynamics models and real environments.

    \textbf{Consistent Policy Improvement.}
        Unlike conventional offline model-based approaches that optimize policies purely by maximizing estimated values, DAMO ensures the consistency between value estimation and policy improvement. 
        Specially, DAMO employs $\Tilde{Q}(s, a)$ to minimize the objective~\eqref{eq-final_objective} and utilizes $\pi$ to maximize it. 
        Conventional approaches typically follow a bilevel optimization framework, while DAMO employs a maximin paradigm, ensuring consistency and preserving dual conservatism.
        As demonstrated in Sec.~\ref{bsec-Alignment-Consistency Experiments}, this consistent structure maintains the conservatism of value estimation during policy improvement.
        
        Once the inner minimization in \eqref{eq-final_objective} is solved, the outer maximization problem is equivalent to solving the surrogate objective~\eqref{eq-surrogate objective}, which provides a lower bound for the standard RL objective $\mathcal{J}(\pi) = \mathbb{E}_{\pi} [\sum_{t=0}^{\infty} \gamma^{t} r(s_{t}, a_{t}, s_{t+1})]$:
        \begin{theorem}
            \label{thm-lower bound}
        	\eqref{eq-surrogate objective} is a lower bound of $\mathcal{J}(\pi)$ for all $\pi$:
        	\begin{equation*}
            % \scalebox{0.9}{$
        		\mathcal{J}(\pi) \geq \mathbb{E}_{\rho_{T}^{\pi}} [ r(s, a, s^{\prime})-\alpha\log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} ]
        		-\alpha D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi}).
                % $}
        	\end{equation*}
        \end{theorem}
        This demonstrates that the equivalence between objective~\eqref{eq-final_objective} and the standard reinforcement learning objective ensures that the learned policy performs well in the environment.

\section{Experiments}
\label{sec-experiments}
    In this section, we focus on the following key aspects: 1) The capability of DAMO to handle synthetic-to-real distribution mismatch, as well as the contributions of its components. 2) The comparative performance of DAMO against existing methods on standard offline RL benchmarks. 3) The significant impact of hyperparameter settings and implementation approaches on the overall performance of DAMO.

    We delve into these aspects using the D4RL benchmark~\cite{fu2020d4rl} on the MuJoCo simulator~\cite{todorov2012mujoco}. Our implementation is based on the OfflineRL-Kit library\footnote{https://github.com/yihaosun1124/OfflineRL-Kit}, a comprehensive and high-performance library for implementing offline reinforcement learning algorithms. The basic parameters of DAMO are consistent with the settings of this library. The details of each experiment and the specific hyperparameters configuration can be found in Appendix~\ref{appendix-Experiment Setting}.

    \subsection{Effectiveness of DAMO Paradigm}
    \label{bsec-Alignment-Consistency Experiments}
        In this section, we present empirical evidence demonstrating the dual capabilities of DAMO: 1) effectively aligning model-generated data with offline data distributions, and 2) consistently maintaining policy stability between dynamics models and real environments. Our detailed ablation study thoroughly investigates two key components of DAMO: inner value estimation and outer policy improvement.

        \begin{figure}[t]
            \centering
            \subfigure{
            \begin{minipage}[b]{0.37\textwidth}
                \centering
                \includegraphics[width=\textwidth]{label.png}
                \vspace{-2.3em}
            \end{minipage}
            }
            \setcounter{subfigure}{0}
            \subfigure[The distribution of w/o er]{
            \begin{minipage}[b]{0.4\textwidth}  
                % \vspace{-1em}
                \centering
                \includegraphics[width=\textwidth]{wo_er.png}
                \label{subfig-wo_er}
                \vspace{-1.5em}
            \end{minipage}
            }
            \subfigure[The distribution of w/o ir]{
            \begin{minipage}[b]{0.4\textwidth} 
                \vspace{-0.8em}
                \centering
                \includegraphics[width=\textwidth]{wo_ir.png}
                \label{subfig-wo_ir}
                \vspace{-1.5em}
            \end{minipage}
            }
            \subfigure[The distribution of DAMO]{
            \begin{minipage}[b]{0.4\textwidth}  
                \vspace{-0.8em}
                \centering
                \includegraphics[width=\textwidth]{DAMO.png}
                \label{subfig-DAMO}
                \vspace{-1.5em}
            \end{minipage}
            }
            \vspace{-1em}
            \caption{(a) (b) (c) illustrate the results of DAMO without the data alignment term~(w/o er), DAMO without the behavior alignment term~(w/o ir), DAMO. The left panels depict the distributions of state-action pair $(s, a)$ for offline and synthetic data. The right panels display the distributions of states for real and synthetic data.} 
            \label{fig-exp-visualization}
        \end{figure}
        
        \begin{figure}[t]
            \centering
            \subfigure[Normalized Score]{
            \begin{minipage}[b]{0.22\textwidth}
                \centering
                \includegraphics[width=\textwidth]{ablation_score.pdf}
                \label{subfig-score}
            \end{minipage}
            }
            \subfigure[Estimated Value]{
            \begin{minipage}[b]{0.22\textwidth}
                \centering
                \includegraphics[width=\textwidth]{ablation_value.pdf}
                \label{subfig-value}
            \end{minipage}
            }
            \caption{Training process of DAMO, w/o ir, w/o er. (a) The data alignment term is the primary factor that influences policy improvement, while the absence of the behavior alignment term results in performance degradation in the final stages. (b) The lack of the data alignment term leads to the overestimation of the value.}
            \label{fig-exp-training process}
        \end{figure}
        \subsubsection{Dual Conservative Value Estimation}
        
        In this part, we present an ablation study examining the individual contributions of the data alignment and behavior alignment terms. We systematically evaluate their impact by removing each term from DAMO and training policies on the hopper-medium-expert-v2 benchmark. To provide intuitive insights, we first visualize the state-action distributions of the final trained policies across three distinct dataset categories. These categories are defined as follows:
        \begin{itemize}[leftmargin=0em, itemindent=1em,topsep=0em]
        \setlength{\itemsep}{1pt}
        \setlength{\parsep}{1pt}
        \setlength{\parskip}{1pt}
            \item Offline Data: The transition pairs $(s, a, s^{\prime})$ are collected with behavior policy $\pi_{\beta}$ in dataset Hopper-medium-expert.
            \item Synthetic Data: The transition pairs are generated by conducting training policy $\pi$ within the dynamics model~$M$.
            \item Real Data: The transition pairs are generated by conducting training policy $\pi$ within the real environment~$T$.
        \end{itemize}
        
        \textbf{Effectiveness of Data Alignment. }
        To evaluate synthetic and offline data compatibility during training, we analyze the state-action pair distributions for both synthetic data and offline data under three distinct conditions: the complete DAMO framework, the configuration without the behavior alignment term (w/o ir), and the configuration without the data alignment term (w/o er). The visualization results are presented in the left column of Fig.~\ref{fig-exp-visualization}. Building on these initial observations, we then examine the training dynamics of DAMO under three specific conditions to further investigate their significant impacts. The results are presented in Fig.~\ref{fig-exp-training process}.
        A comparative analysis between Fig.~\ref{subfig-wo_er} and Fig.~\ref{subfig-DAMO} reveals that the w/o er configuration fails to establish proper alignment between synthetic data and offline data, thereby inadequately mitigating OOD state-action pairs. The results in Fig.~\ref{subfig-wo_ir} further highlight that the data alignment term enables w/o ir configuration to align synthetic data with offline data. To clarify the detrimental effects of incompatibility between these two data sources, Fig.~\ref{subfig-value} illustrates that w/o er overestimates the value of OOD actions, leading to suboptimal training performance. This is corroborated by the poor evaluation scores of w/o er depicted in Fig.~\ref{subfig-score}.
        
        \textbf{Effectiveness of Behavior Alignment. }
        To assess the model-environment consistency during the testing phase, we follow a similar evaluation procedure. We visualize the state distributions of real environment and model-generated data under the same conditions. The visualization results are presented in the right column of Fig.~\ref{fig-exp-visualization}.
        A comparative analysis between Fig.~\ref{subfig-wo_ir} and Fig.~\ref{subfig-DAMO} reveals that while w/o ir successfully achieves synthetic-offline data compatibility and mitigates OOD actions, the absence of the behavior alignment term leads to incomplete mitigation of OOD states, manifesting policy inconsistency between dynamics model and real environment. Although the behavior alignment term imposes constraints, the results in Fig.~\ref{subfig-wo_er} demonstrate that w/o er fails to effectively mitigate OOD states. The observed discrepancies in state distributions between real environment data and model-generated data under the w/o er highlight the critical importance of establishing robust compatibility between synthetic and offline data. This compatibility is essential for ensuring consistent policy performance across dynamics models and real environments. The influence is reflected in Fig.\ref{subfig-score}, which shows performance degradation of w/o ir in the final stage. 

        \subsubsection{Towards Consistent Policy Improvement} 

        \begin{table}[t]
            \centering
            \caption{The average value of OOD state-action pairs, $\infty$ for severe overestimation. The smallest values are bolded, which indicates the best mitigation of the overestimation problem. }
            \label{tab-policy improvement}
            \vskip 0.05in
            \begin{tabular}{l|ccc}
                \toprule 
                                & halfcheetah & hopper & walker2d
                \\ \midrule
                Consistent Version  &   \textbf{95.2}      & \textbf{213.0}  & \textbf{231.7}
                 \\ 
                 Inconsistent Version &   101.4     & 219.4  & $\infty$
                 \\
                 $Real \ value$ &   819.6     & 269.6  & 299.7
                 \\ \bottomrule
            \end{tabular}
        \end{table}

        This section presents a statistical analysis of the value estimation performance of DAMO across multiple datasets, with comparisons against the true value function. Our results validate that the compatible policy improvement framework effectively maintains the conservatism of value estimation.

        To investigate the significance of objective alignment in DAMO, we conduct a controlled comparison experiment by directly integrating DAMO with standard off-policy mechanisms~(Inconsistent Version) while evaluating the critic's estimation of state-action pair values and comparing it with the original DAMO~(Consistent Version). We perform this analysis on the medium-expert dataset across three distinct environments: HalfCheetah, Hopper, and Walker2d. The obtained quantitative results are summarized in Table~\ref{tab-policy improvement}.
        The experimental findings reveal that the Consistent Version achieves conservative value estimations across all environments. Although the Inconsistent version maintains conservatism in the HalfCheetah and Hopper environments, it exhibits significant overestimation in the Walker2d environment. These results collectively demonstrate the critical role of objective alignment in consistently ensuring conservative value estimation and preventing overestimation errors.
    \begin{table*}[t]
        \centering
        \caption{Results on the D4RL Gym benchmark. The numbers reported for DAMO are the normalized scores averaged over the final iteration of training across 4 seeds, with $\pm$ standard deviation. Since we haven't obtained the results of O-DICE on random datasets, we didn't include it in the table and used a dash (-) to indicate the absence of data. The top three scores are highlighted in bold. We use an asterisk (*) for the best score, an underscore (\_) for the second, and bold text for the third.}
        \label{tab-normalized-score}
        \vskip 0.05in
        \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccc|ccccc|c}
            \toprule  Task Name  & Algae &  Opti  & O-DICE & MOPO & COMBO & TT  & RAMBO & MOBILE & DAMO (Ours) 
            \\ \midrule
            halfcheetah-random   & -0.3  & 11.6   & -      & \textbf{38.5} & \underline{\textbf{38.8}}  &  6.1  & $\textbf{39.5}^{*}$  & 38.3  & $34.0 \pm 3.0$
            \\ 
            hopper-random        & 0.9   & 11.2   & -      & $\textbf{31.7}^{*}$ & 17.9  & 6.9  & 25.4  & \textbf{25.5}  & \underline{\textbf{31.5 $\pm$ 0.4}}
            \\ 
            walker2d-random      & 0.5   & \underline{\textbf{9.9}}    & -      & \textbf{7.4}  & 7.0   &  5.9  & 0.0   & $\textbf{21.6}^{*}$  & 0.0
            \\ \midrule
            halfcheetah-medium   & -2.2  & 38.2   & 47.4   & \underline{\textbf{72.4}} & 54.2  &  46.9 & $\textbf{77.9}^{*}$  & 69.3  & \textbf{71.3 $\pm$ 0.5}
            \\ 
            hopper-medium        & 1.2   & \textbf{94.1}   & 86.1   & 62.8 & \underline{\textbf{97.2}}  &  67.1 & 87.0  & $\textbf{102.7}^{*}$ & $93.8 \pm 8.2$
            \\ 
            walker2d-medium      & 0.3   & 21.8   & \underline{\textbf{84.9}}   & 84.1 & 81.9  &  81.3 & \underline{\textbf{84.9}}  & $\textbf{88.7}^{*}$  & $83.1 \pm 8.0 $
            \\ \midrule
            halfcheetah-medium-replay & -2.1 &39.8& 44.0   & $\textbf{72.1}^{*}$ & 55.1  &  44.1 & \textbf{68.7}  & \underline{\textbf{71.4}}  & 61.6 $\pm$ 3.0
            \\ 
            hopper-medium-replay & 1.1   & 36.4   & $\textbf{99.9}^{*}$   & 92.7 & 89.5  &  99.4 & \underline{\textbf{99.5}}  & 57.7  & $\textbf{99.9} \pm \textbf{0.9}^{*}$
            \\ 
            walker2d-medium-replay & 0.6& 21.6   & \textbf{83.6}   & \underline{\textbf{85.9}} & 56.0  &  82.6 & 89.2  & 80.5  & \textbf{89.4} $\pm$ $\textbf{3.0}^{*}$
            \\ \midrule
            halfcheetah-medium-expert &-0.8& 91.1 & 93.2   & 83.6 & 90.0  &  95.0 & 95.4  & 101.4 & \textbf{103.1} $\pm$ \textbf{0.9} 
            \\ 
            hopper-medium-expert   & 1.1   & \underline{\textbf{111.5}} & 110.8  & 74.6 & \textbf{111.0} & 110.0 & 88.2  & 107.7 & \textbf{112.3} $\pm$ $\textbf{3.6}^{*}$
            \\ 
            walker2d-medium-expert & 0.4   & 74.8 & \underline{\textbf{110.8}}  & 108.2& 103.3 &  101.9& 56.7  & $\textbf{113.0}^{*}$ & \textbf{109.4 $\pm$ 0.8}
            \\ \midrule
            average score  & 0.0 & 43.4 & - & \textbf{68.7} & 66.0 & 61.1 & 66.1 & \underline{\textbf{72.2}} & $\textbf{74.4}^{*}$
            \\ \bottomrule
        \end{tabular}
        }
    \end{table*}
    \subsection{Comparison Results}

        \textbf{Dataset.} We evaluate DAMO's performance using the D4RL benchmark~\cite{fu2020d4rl} within the MuJoCo simulation environment. Our comprehensive assessment covers 12 distinct datasets across three environments (HalfCheetah, Hopper, and Walker2d) and four dataset categories: random, medium, medium-replay, and medium-expert. Following standard evaluation protocols, we exclusively utilize the "v2" versions of all datasets for consistent comparison.

        \textbf{Baseline.} We compare DAMO with two types of offline RL baselines: DICE-based algorithms which follow a similar form of training objective with DAMO and Model-based algorithms which achieve SOTA performance. For DICE-based approaches, Algae-DICE~\cite{algaedice} transforms the intractable state-action-level constraint into a unified objective for policy training. Opti-DICE~\cite{optidice} directly estimates the stationary distribution corrections of the optimal policy to attain a high-rewarding policy. O-DICE~\cite{odice} using the orthogonal-gradient update to diminish the confliction of different gradients during policy training. For model-based approaches, MOPO~\cite{mopo} penalizes rewards via the predicted variance of ensembled dynamics models. COMBO~\cite{combo} implements CQL within a model-based framework. TT~\cite{TT} uses a transformer to model offline trajectories and employs beam search for planning. RAMBO~\cite{rambo} adversarially trains the policy and the dynamics model within a robust framework. MOBILE~\cite{mobile} penalizes the value learning of synthetic data based on the estimated uncertainty of the Bellman Q-function, derived from an ensemble of models.

        \textbf{Comparison Results.} Table~\ref{tab-normalized-score} reports the scores in D4RL benchmark. Overall, DAMO demonstrates superior performance, achieving the highest average score across all baseline methods. Notably, for high-quality datasets such as medium-expert and medium-replay, DAMO consistently attains state-of-the-art (SOTA) or near-SOTA performance. These results suggest that effective OOD constraints, combined with a high-quality behavior policy, enable the agent to learn high-reward policies within in-distribution~(ID) regions. Furthermore, DAMO maintains strong performance on medium-quality datasets. This demonstrates the robustness of DAMO in handling less-than-ideal data conditions and highlights the importance of ensuring both model-environment policy consistency and synthetic-offline data compatibility. However, the performance of DAMO on random datasets reveals limitations, indicating the need for more aggressive exploration strategies in low-quality data scenarios to identify regions beyond those covered by the offline dataset. This phenomenon is particularly pronounced in the Walker2d-random dataset, where DAMO struggles to learn an effective policy. One possible explanation is that the optimization landscape of the environment may contain numerous suboptimal solutions corresponding to saddle points in the maximin framework. If this is the case, such structural characteristics could potentially make DAMO more susceptible to becoming trapped in local optima, thereby limiting its ability to discover better solutions. However, further investigation is needed to confirm this hypothesis.
        
    \subsection{Effects of Coefficient $\alpha$}
    \label{bsec-hyperparameter tunning}
        \begin{figure}
            \centering
            \subfigure[Tuning of $\alpha$]{
            \begin{minipage}{0.22\textwidth}
                \centering
                \includegraphics[width=\textwidth]{hyperparameter.pdf}
                \label{subfig-hyperparameter}
            \end{minipage}
            }
            \subfigure[DAMO without fixing $\alpha$]{
            \begin{minipage}{0.22\textwidth}
                \centering
                \includegraphics[width=\textwidth]{trick_study.pdf}
                \label{subfig-trick}
            \end{minipage}
            }
            \caption{Two sets of experiments on hyperparameter $\alpha$. (a) The tuning experiment of $\alpha$ was performed on the hopper-medium-expert dataset. (b) The fixing experiment of $\alpha$ was performed on the halfcheetah-medium-expert dataset for validation.}
        \end{figure}
        DAMO is regulated by a key hyperparameter: the hyperparameter $\alpha$ which controls the extent of policy discrepancies. In practical implementation, we fix $\alpha$ in the actor training.
        While we consider $\alpha$ as a hyperparameter controlling the degree of conservatism, the empirical results in Fig.~\ref{subfig-hyperparameter} demonstrate that DAMO maintains robust performance across a wide range of $\alpha$ values, with its primary impact observed in the final stage performance. Notably, DAMO achieves superior performance with larger $\alpha$ values (e.g., $\alpha=3.0,5.0$), whereas insufficiently small values (e.g., $\alpha=1.0$) tend to promote excessive risk-taking during exploration, ultimately failing to converge to an optimal policy.
        To enhance training stability, we initially fix $\alpha$ to 1.0 in the actor training. However, experimental results demonstrate that DAMO achieves competitive performance even without this specific implementation. To further validate this observation, we conduct comparative experiments on the HalfCheetah environment, evaluating both the standard DAMO implementation and its variant without fixed $\alpha$ under their respective optimal hyperparameter configurations. As shown in Fig.~\ref{subfig-trick}, both settings achieve a competitive performance, confirming that DAMO maintains robust performance regardless of the implementation of fixing $\alpha$.

\section{Conclusion and Discussion}
\label{sec-Conclusion and Discussion}
    This paper critically examines limitations in existing offline model-based RL approaches, highlighting the crucial need to address behavior inconsistency caused by policy discrepancies between biased models and real environments. 
    Building upon our analysis, we introduce a unified framework that concurrently ensures model-environment policy consistency and synthetic-offline data compatibility, with comprehensive experimental validation demonstrating its effectiveness. 
    Significantly, the maximin optimization framework of DAMO demonstrates the benefits of aligning the actor and critic training objectives within the actor-critic architecture, ensuring essential conservatism during policy updates. The limitations of our work are focused on two-fold: the sampling of the initial state and the computing of the data alignment term. Although we assume the distribution of the initial state is known, the offline dataset does not explicitly label which states are initial states. Therefore, we randomly sample a state from the offline dataset to serve as the initial state, which creates a gap between theory and practice. When calculating the data alignment term, we trained a classifier to assist in the process. However, the classifier's accuracy is critical: imprecise classifiers fail to estimate the modified reward accurately, leading to unstable training and potentially invalidating the practical objective as a lower bound. Future work could extend DAMO to areas such as offline-to-online RL~\cite{li2023proto}, where methods with strong adaptability and the ability to bridge the synthetic-to-real distribution gap are in high demand.

\section*{Impact Statement}
    This work presents an effective solution for addressing the distribution shift in offline model-based RL through dual policy-environment alignment, enabling robust simulation-to-reality transfer in safety-critical domains such as medical robotics. While advancing certifiable policy deployment via implicit behavior constraints and adaptive value estimation, challenges remain in reward specification sensitivity and residual model bias mitigation. The framework highlights the critical need for continuous dynamic alignment calibration to ensure reliable real-world RL system performance.
\nocite{*}

\bibliography{cite}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\section{Proofs in the Main Text}
\label{appendix-proof}
    We present a detailed proof process for each theorem discussed in the paper. The sequence of proofs follows the logic of derivation rather than strictly adhering to their order of appearance in the main text.
    \begin{theorem}
        The KL-divergence has the following upper bound:
        \begin{equation*}
                D_{KL}(\rho_{T}^{\pi} \Vert \rho_{T}^{\pi_{\beta}})  \leq \mathrm{E}_{\rho_{T}^{\pi}} [ \log\dfrac{\rho_{T}^{\pi}}{\rho_{T}^{\pi_{\beta}}} ] 
            + D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi}) .
        \end{equation*}
        \label{thm-KL upper bound}
        \begin{proof}
    	\begin{align*}
    		D_{KL}(\rho_{T}^{\pi} \Vert \rho_{T}^{\pi_{\beta}}) & = \mathrm{E}_{\rho_{T}^{\pi}} [ \log\dfrac{\rho_{T}^{\pi}}{\rho_{T}^{\pi_{\beta}}} ]
    		\\
    		& = \mathrm{E}_{\rho_{T}^{\pi}} [ \log\dfrac{\rho_{T}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
    		\cdot \dfrac{\rho_{M}^{\pi}}{\rho_{M}^{\pi}} ]
    		\\
    		& = \mathrm{E}_{\rho_{T}^{\pi}} [ \log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
    		] + D_{KL}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})
    		\\
    		& \leq \mathrm{E}_{\rho_{T}^{\pi}} [ \log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
    			] + D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})
    	\end{align*}
        \end{proof}
    \end{theorem}
    

    \begin{corollary}
	We have the following surrogate objective
	\begin{equation*}
		\arg\max\limits_{\pi} \mathrm{E}_{\rho_{T}^{\pi}} [ r(s,a,s^{\prime})  - \log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
			] - D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})
	\end{equation*}
	A discounted factor $\alpha$ will be added for training stability.
	\begin{equation}
		\arg\max\limits_{\pi} \mathrm{E}_{\rho_{T}^{\pi}} [ r(s,a,s^{\prime})  -  \alpha\log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
			] - \alpha D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})
        \label{eq-surro}
	\end{equation}
    \end{corollary}


    \begin{lemma}
		We can represent f-divergence in a function form: $D_{f}(x \Vert p) = \mathbb{E}_{z\sim p} f(\frac{x}{p})$, then its Fenchel conjugate function is $\mathbb{E}_{z\sim p} f_{\star}(y(z))$
        \label{thm-variational form of f-divergence}
        \begin{proof}
    	We use $g_{\star}(x)$ to denote $D_{f}(x \Vert p)$
    	\begin{align*}
    		g_{\star}(x) &= \sup\limits_{y\in \mathcal{X}} \{\langle x,y \rangle - D_{f}(y \Vert p)  \}
    		\\
    		&= \sup\limits_{y\in \mathcal{X}}\{ \int_{R}x(z)y(z) dz - \int_{R} p(z)\cdot f(\frac{y(z)}{p(z)})dz \}
    		\\
    		&= \sup\limits_{y\in \mathcal{X}}\{ \int_{R} p(z)[x(z)\frac{y(z)}{p(z)} -  f(\frac{y(z)}{p(z)}) ]dz \}
    		\\
    		&= \int_{R} p(z)\sup\limits_{z}[x(z)\frac{y(z)}{p(z)} -  f(\frac{y(z)}{p(z)}) ]dz
    		\\
    		&= \int_{R} p(z)\sup\limits_{z}[x(z)y(z) -  f(y(z)) ]dz
    		\\
    		&= \int_{R} p(z)f_{\star}(x(z))dz
    		\\
    		&= \mathrm{E}_{z\sim p} f_{\star}(y(z))
    	\end{align*}
        \end{proof}
    \end{lemma}
    
	
    \begin{corollary}
	For f-divergence $D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi})$
	\begin{equation*}
		D_{f}(\rho_{T}^{\pi} \Vert \rho_{M}^{\pi}) = \max\limits_{y(s,a,s')} \mathrm{E}_{\rho_{T}^{\pi}} [ y(s,a,s') ] -
	\mathrm{E}_{\rho_{M}^{\pi}} [ f_{\star}(y(s,a,s')) ] 
	\end{equation*}
        \label{thm-f-divergence in max form}
        \begin{proof}
            Verifying $D_{f}$ is a convex, closed function. A convex and closed function's double conjugate function is itself, so $D_{f}$ can be represented in a MAX form.
        \end{proof}
    \end{corollary}
    

    \begin{theorem}
        Surrogate objective \eqref{eq-surro} is equivalent to the following maximin problem
        \begin{equation*}
		\max\limits_{\pi} \min\limits_{y(s,a,s')} \mathrm{E}_{\rho_{T}^{\pi}} [ r(s,a,s^{\prime})  -  \alpha\log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
			- \alpha y(s,a,s')] + \alpha\mathrm{E}_{\rho_{M}^{\pi}} [f_{\star}(y(s,a,s'))]
	  \end{equation*}
        \label{thm-the first maximin form}
        \begin{proof}
            According to lemma \ref{thm-f-divergence in max form}, the objective can be presented in a Min form.
        \end{proof}
    \end{theorem}
    
    
    \begin{theorem}
	The following objective is equivalent to the surrogate objective
	\begin{equation}
    	\max\limits_{\pi} \min\limits_{Q(s,a)} J(\pi,Q(s,a)) = \max\limits_{\pi}         \min\limits_{Q(s,a)} (1-\gamma) \mathrm{E}_{s \sim \mu_{0},a \sim \pi}           [Q(s,a)] + \mathrm{E}_{\rho_{M}^{\pi}} f_{\star}(\Phi(s,a,s')/\alpha)
            \label{eq-maximin}
	\end{equation}
        \label{thm-maximin}
        \begin{proof}
    	Defining $y(s,a,s') = \dfrac{ r(s,a,s^{\prime}) - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} + \tau^{\pi} Q(s,a) - Q(s,a)}{\alpha}$. For the convenience, we define $\beta_{t}(s,a,s')$
    	\begin{equation*}
    		\beta_{t}(s,a,s') = Pr[s_{t} = s, a_{t} = a, s_{t+1} = s'|s_{0}\sim \mu_{0} , a_{t}\sim \pi , s_{t+1}\sim T]
    	\end{equation*}
    	Then, $\rho_{T}^{\pi}(s,a,s') = (1-\gamma)\sum\limits_{t=0}^{\infty} \gamma^{t}\beta_{t}$.
        Similarly, we can define $\beta_{t}(s,a,s',a')$.
    	
    	Substituting $y(s,a,s')$ with $[ r(s,a,s^{\prime}) - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} + \tau^{\pi} Q(s,a) - Q(s,a) ] / \alpha$ and we have:
    	\begin{align*}
    		\max\limits_{\pi} \min\limits_{Q(s,a)} & \mathrm{E}_{\rho_{T}^{\pi}} [ Q(s,a) - \tau^{\pi} Q(s,a) ] 
    			\\
    		= & \sum\limits_{s,a,s'} \rho_{T}^{\pi}(s,a,s') [Q(s,a) - \gamma\sum\limits_{a'}Q(s',a') \pi(a'|s')] 
    			\\
    			= &  \sum\limits_{s,a,s'} [\rho_{T}^{\pi}(s,a,s')Q(s,a)] - \sum\limits_{s,a,s',a'} [\gamma \rho_{T}^{\pi}(s,a,s') Q(s',a') \pi(a'|s')] 
    			\\
    		= & (1-\gamma) [ \sum\limits_{s,a,s'} \sum\limits_{t=0}^{\infty} \gamma^{t} \beta_{t}(s,a,s')Q(s,a)  - \sum\limits_{s,a,s',a'} \sum\limits_{t=0}^{\infty} \gamma^{t+1}\beta_{t}(s,a,s')Q(s',a')\pi(a'|s') ]
    			\\
    		= & (1-\gamma) [ \sum\limits_{s,a,s'} \sum\limits_{t=0}^{\infty} \gamma^{t}
    			\beta_{t}(s,a,s')Q(s,a) - \sum\limits_{s,a,s',a'} \sum\limits_{t=0}^{\infty} \gamma^{t+1}\beta_{t}(s,a,s',a')Q(s',a')  ]
    			\\
    		= & (1-\gamma) [ \sum\limits_{s,a}\sum\limits_{t=0}^{\infty} \gamma^{t}
    			\beta_{t}(s,a)Q(s,a) - \sum\limits_{s',a'} \sum\limits_{t=0}^{\infty} \gamma^{t+1}\beta_{t}(s',a')Q(s',a') ] 
    			\\
    		= & (1-\gamma) \sum\limits_{s,a}\beta_{0}(s,a)Q(s,a)
    			\\
    		= & (1-\gamma) \mathrm{E}_{s \sim \mu_{0},a\sim \pi}[Q(s,a)] 
    	\end{align*}
        \end{proof}
    \end{theorem}
    
    \begin{theorem}
        The optimal solution of maximin question \eqref{eq-maximin}'s inner question $Q^{\star}(s,a)$ satisfies 
        \begin{equation}
            Q^{\star}(s,a) = r(s,a,s^{\prime}) - \alpha \log \dfrac{\rho^{\pi}_{M}}{\rho^{\pi_{\beta}}_{T}} - \alpha f^{\prime} ( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}}) + \tau^{\pi} Q^{\star}(s,a)
        \end{equation}
        \label{thm-inner optimal}
        \begin{proof}
            $f(x)$ is a convex function, so its conjugate function $f_{\star}$ is also convex. The objective function below is convex about $y(s,a,s^{\prime})$
            \begin{equation*}
                \max\limits_{\pi} \min\limits_{y(s,a,s')} \mathbb{E}_{\rho_{T}^{\pi}} [ r(s,a,s^{\prime})  -  \alpha\log\dfrac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} 
			- \alpha y(s,a,s')] + \alpha\mathbb{E}_{\rho_{M}^{\pi}} [f_{\star}(y(s,a,s'))]
            \end{equation*}
            Then the optimal solution of inner question is $y^{\star}(s,a,s^{\prime}) = (f^{\prime}_{\star})^{-1} ( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}})$. For any convex function, we have $(f^{\prime}_{\star})^{-1} (x) = f^{\prime}(x)$, so $y^{\star}(s,a,s^{\prime}) = f^{\prime}( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}})$.

            Recalling that we employ the variable substitution:$y(s,a,s') = [ r(s,a,s^{\prime}) - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} + \tau^{\pi} Q(s,a) - Q(s,a) ] / \alpha$, so we have
            \begin{equation*}
                y^{\star}(s,a,s') = \dfrac{ r(s,a,s^{\prime}) - \alpha \log\frac{\rho_{M}^{\pi}}{\rho_{T}^{\pi_{\beta}}} + \tau^{\pi} Q^{\star}(s,a) - Q^{\star}(s,a)}{\alpha}
            \end{equation*}
            Then the optimal solution $Q^{\star}(s,a)$ satisfies
            \begin{equation*}
                Q^{\star}(s,a) = r(s,a,s^{\prime}) - \alpha \log \dfrac{\rho^{\pi}_{M}}{\rho^{\pi_{\beta}}_{T}} - \alpha f^{\prime} ( \dfrac{\rho^{\pi}_{T}}{\rho^{\pi}_{M}}) + \tau^{\pi} Q^{\star}(s,a)
            \end{equation*}
        \end{proof}
    \end{theorem}

        
    \section{OOD Issues under Dynamics Model Integration}
    \label{app-bsec-OOD actions and states in offline model-based RL}

        OOD issues in offline model-based RL are intrinsically linked to the mismatch between synthetic and real data distributions, compounded in model-based settings, where model inaccuracies introduce additional sources of error.

        \subsection{OOD Actions}
        The dynamics model generates data that can significantly deviate from the distribution of the offline dataset, leading to OOD actions.
        This triggers negative model exploitation~\cite{levine2020offline}, resulting in poor policy performance. 
        Aligning the distributions of synthetic and offline data can effectively mitigate these issues. 
        The model, $M$, is used to generate synthetic data for training, but it is essential to avoid taking OOD actions when rolling out policy in the model, to ensure accurate value estimation.
        
        Specifically, in an offline setting, where interaction with the real environment is not possible, overestimating the value of OOD actions leads to model exploitation.
        Consequently, the agent may learn policies that perform poorly in the underlying environment compared to the model, leading to algorithmic failure during deployment.
        To avoid model exploitation, it is crucial to align synthetic with offline data.  
        Given offline data collected under the behavior policy $\pi_{\beta}$,  We aim to find a policy $\pi$ that behaves in model $M$ like $\pi_{\beta}$ does in environment $T$. 
        Specifically, we aim to match the distribution of $(s, a)_{M}^{\pi}$ with $(s, a)_{T}^{\pi_{\beta}}$, ensuring accurate value estimation of $(s, a)_{M}^{\pi}$. 

        \subsection{OOD states}
        The performance of the learning policy during deployment is critically dependent on the alignment of transition pairs between the learned model and the real environment. 
        Significant discrepancies between these transitions can cause performance degradation and lead to OOD states, highlighting the necessity of correcting transition biases during training.
        Even if the learned policy performs well in the model, it is important, during real-environment deployment, to avoid OOD states, for which we cannot accurately estimate the value function in the offline setting.
        
        For example, if the learned model predicts the transition $(s, a, s_{1})$, but the real environment produces a transition $(s, a, s_{2})$, where $s_{1}$ is a high-value state and $s_{2}$ is a low-value state,  the policy will perform poorly in the real environment.
        The situation worsens if $s_{2}$ is an OOD state that has never been encountered in model $M$ before. 
        In this scenario, the agent cannot estimate the value of the state-action pair $(s_{2}, a^{\prime})$, leading to ineffective decision-making and model-environment policy inconsistency. 
        In an offline setting, since we cannot adjust the policy by interacting with the environment, we must approximate and correct the discrepancies between the model $M$ and the real environment $T$ during training.
        This ensures that the distribution of transition $(s, a, s^{\prime})_{M}^{\pi}$ matches the distribution of $(s, a, s^{\prime})_{T}^{\pi}$ in the underlying environment.

        In summary, while optimizing the policy $\pi$ to maximize returns, we should constrain it in two directions: 1) aligning the synthetic data $(s, a, s^{\prime})_{M}^{\pi}$ with the offline data $(s, a, s^{\prime})_{T}^{\pi_{\beta}}$, and 2) ensuring that $\pi$ exhibits consistent behavior in both dynamics models and real environments.
    

\section{Implementation Details}
\label{appendix-Implementation Details}
    In this section, we present the detailed implementation of DAMO. 
    \subsection{Selection of f-divergence}
        As established in Theorem~\ref{thm-lower bound}, the f-divergence must consistently exceed the KL-divergence to guarantee that the objective in Equation~\eqref{eq-final_objective} serves as a lower bound for the standard RL objective~$\mathcal{J}(\pi)$. Based on this, we specifically choose $f(x) = \frac{1}{3}(x - 1)^{3}$, and its conjugate function is $f_{\star}(x) = \frac{2}{3}(x-1)^{\frac{3}{2}}.$ In principle, any choice of f-divergence is permissible, and we adhere to the selection made in the prior work~\cite{luo2024ompo}. Due to the tight schedule, we haven't tried other choices of f-divergence. However, we suppose that the choice of f-divergence won't enormously affect the effectiveness of DAMO, since f-divergence primarily serves as a measure of discrepancy. 

        \begin{table}[h]
            \centering
            \caption{Hyperparameter of DAMO when optimizing policy}
            \label{tab-Hyperparameter of DAMO when optimizing policy}
            \begin{tabular}{ccl}
                \toprule
                 Hyperparameter & Value & Description
                 \\ \midrule
                 $n$      & 7     & Ensemble number of dynamics model
                 \\
                 $n_{elite}$ &  5  & Number of dynamics model for prediction
                 \\
                 $d$      & 0.5   & Sampling ratio for classifier training
                 \\ 
                 $\gamma$ & 0.99  & Discounted factor.
                 \\
                $l_{r}$ of critic & $3\times 10^{-4}$ & Critic learning rate.
                 \\
                 Batch size   &  256  &  Batch size for each update.
                 \\
                 $N_{iter}$   &  3M   &  Total gradient steps.
                 \\ \bottomrule
            \end{tabular}
        \end{table}

    \subsection{Modification of actor training objective}
        When optimizing the actor network using objective~\eqref{eq-final_objective}, we observed that the substantial residual of the Bellman error often disrupts the optimization of the actor and leads to training instability. To stabilize the training process, we followed the implementation of previous DICE work~\cite{odice, sikchi2023imitation} and fixed the hyperparameter $\alpha$ to $1.0$ during the optimization of the actor network. The specific form of the actor training objective is as follows:
        \begin{equation*}
            \max\limits_{\pi} (1-\gamma)              \mathrm{E}_{s\sim\mu_{0},a\sim \pi} [\Tilde{Q}(s,a)] 
                         \\
                + \mathrm{E}_{\rho_{M}^{\pi}} [ f_{\star}(\Phi(s,a,s')) ]
        \end{equation*}
        It should be noted that the effectiveness of DAMO does not rely on this trick. We demonstrated this in the experimental section.\ref{bsec-hyperparameter tunning}.

    \subsection{Model, classifier and Policy Optimization}
        For dynamics model $M$, it was depicted as a probabilistic neural network that generates a Gaussian distribution for the subsequent state and reward, contingent upon the current state and action:
        \begin{equation*}
            m_{\theta}(s_{t+1}, r_{t}|s_{t}, a_{t}) = \mathcal{N}(\mu_{\theta}(s_{t}, a_{t}), \Sigma_{\theta}(s_{t}, a_{t})).
        \end{equation*}
        Our model training approach is consistent with the methodology used in prior works~\cite{mopo, mobile}. We train an ensemble of seven dynamics models and select the best five based on their validation prediction error from a held-out set containing 1000 transitions in the offline dataset $D_{R}$. Each model in the ensemble is a 4-layer feedforward neural network with 200 hidden units. We randomly choose one model from the best five models during model rollouts.

        For classifier $h_{\psi}$, we implement the following approach: At each gradient step, we randomly sample data from both offline dataset $D_{R}$ and synthetic dataset $D_{M}$ with a fixed ratio $d$. This method ensures a balanced utilization of data from both DL and DG during training.
    
        Policy optimization is conducted using the Actor-Critic framework in our approach, with hyperparameter configurations aligned with previous offline model-based settings. During each update, a batch of 256 transitions is sampled, where 5\% is derived from the offline dataset $D_{R}$ and the remaining 95\% from the synthetic dataset $D_{M}$. Additionally, We use two common tricks in DAMO: double Q-learning and entropy regularizer. The specific hyperparameter settings applied for the D4RL benchmark are detailed in Table.\ref{tab-Hyperparameter of DAMO when optimizing policy}
    
\section{Experiment Setting}
\label{appendix-Experiment Setting}
    \subsection{Benchamark}
    We evaluate our approach using the D4RL benchmark~\cite{fu2020d4rl}, focusing on the “v2” version of Gym tasks. For DICE-based methods (Algae-DICE, Opti-DICE, and O-DICE), we directly adopt the scores reported in their respective original papers. However, since the results for O-DICE on the random dataset are not provided in the original paper, we exclude them from our comparison. For offline model-based methods, we retrained MOPO~\cite{mopo} and MOBILE~\cite{mobile} on the “v2” Gym datasets. For the remaining methods, we directly report the scores provided in their original papers. All results are summarized in Table~\ref{tab-normalized-score}.

    \begin{table}[h]
        \centering
        \caption{Hyperparameters of DAMO used in the D4RL datasets.}
        \begin{tabular}{lccc}
            \toprule
             Task Name                  & $\alpha$ & $K$ & $l_{r}$ of actor
             \\ \midrule 
             halfcheetah-random         & 0.1 & 5 & $1 \times 10^{-4}$
             \\
             hopper-random              & 2   & 5 & $1 \times 10^{-5}$
             \\
             walker2d-random            & 2   & 5 & $1 \times 10^{-4}$
             \\ \midrule 
             halfcheetah-medium         & 0.5 & 5 & $1 \times 10^{-4}$
             \\
             hopper-medium              & 2.0 & 5 & $1 \times 10^{-5}$
             \\
             walker2d-medium            & 2   & 1 & $5 \times 10^{-5}$
             \\ \midrule 
             halfcheetah-medium-replay  & 0.1 & 5 & $1 \times 10^{-4}$
             \\
             hopper-medium-replay       & 3   & 5 & $1 \times 10^{-4}$
             \\
             walker2d-medium-replay     & 1.5 & 5 & $1 \times 10^{-4}$
             \\ \midrule
             halfcheetah-medium-expert  & 2.5 & 5 & $1 \times 10^{-4}$
             \\
             hopper-medium-expert       & 3   & 5 & $1 \times 10^{-5}$
             \\
             walker2d-medium-expert     & 1.5 & 3 & $1 \times 10^{-4}$
             \\ \bottomrule
        \end{tabular}
        \label{tab-hyperparameter}
    \end{table}

    \subsection{Hyperparameters}
    We provide a list of the hyperparameters that were tuned during our experiments. The detailed configurations are summarized in Table~\ref{tab-hyperparameter}.

    \textbf{Coefficient $\alpha$.} The coefficient $\alpha$ acts as the sole hyperparameter in our objective function, regulating the divergence between the training policy and the behavior policy. We tune $\alpha$ in the range of $[0.1, 3.0].$

    \textbf{Rollout Length $K$.} Similar to MOPO, we employ short-horizon branch rollouts in DAMO. For Gym tasks in the D4RL benchmark, the horizon parameter $K$ is tuned within the range of $\{ 1, 3, 5\}$.

    \textbf{The actor learning rate $l_{r}$} Since DAMO operates as a Maximin optimization problem and is susceptible to saddle points, we optimize the actor's learning rate to achieve robust performance. The choice of learning rate plays a pivotal role in DAMO's effectiveness. We tune the actor learning rate $l_r$ in the range of $\{1\times 10^{-5}, 5\times 10^{-5}, 1\times 10^{-4} \}$.

    \subsection{Computing Infrastructure}
    The experimental setup includes a GeForce GTX 3090 GPU and an Intel(R) Xeon(R) Gold 6330 CPU at 2.00GHz. The implementation is built upon the OfflineRL-Kit library, with DAMO adopting the software libraries and frameworks specified therein.
\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
