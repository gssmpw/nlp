@article{algaedice,
  title={Algaedice: Policy gradient from arbitrary experience},
  author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1912.02074},
  year={2019}
}

@article{combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28954--28967},
  year={2021}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{lu2021revisiting,
  title={Revisiting design choices in offline model-based reinforcement learning},
  author={Lu, Cong and Ball, Philip J and Parker-Holder, Jack and Osborne, Michael A and Roberts, Stephen J},
  journal={arXiv preprint arXiv:2110.04135},
  year={2021}
}

@article{luo2024sambo,
  title={SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning},
  author={Luo, Wang and Li, Haoran and Zhang, Zicheng and Han, Congying and Lv, Jiayu and Guo, Tiande},
  journal={arXiv preprint arXiv:2408.12830},
  year={2024}
}

@article{ma2022smodice,
  title={Smodice: Versatile offline imitation learning via state occupancy matching},
  author={Ma, Yecheng Jason and Shen, Andrew and Jayaraman, Dinesh and Bastani, Osbert},
  journal={arXiv preprint arXiv:2202.02433},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@inproceedings{mobile,
  title={Model-Bellman inconsistency for model-based offline reinforcement learning},
  author={Sun, Yihao and Zhang, Jiaji and Jia, Chengxing and Lin, Haoxin and Ye, Junyin and Yu, Yang},
  booktitle={International Conference on Machine Learning},
  pages={33177--33194},
  year={2023},
  organization={PMLR}
}

@article{mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}

@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{odice,
  title={Odice: Revealing the mystery of distribution correction estimation via orthogonal-gradient update},
  author={Mao, Liyuan and Xu, Haoran and Zhang, Weinan and Zhan, Xianyuan},
  journal={arXiv preprint arXiv:2402.00348},
  year={2024}
}

@inproceedings{optidice,
  title={Optidice: Offline policy optimization via stationary distribution correction estimation},
  author={Lee, Jongmin and Jeon, Wonseok and Lee, Byungjun and Pineau, Joelle and Kim, Kee-Eung},
  booktitle={International Conference on Machine Learning},
  pages={6120--6130},
  year={2021},
  organization={PMLR}
}

@article{rambo,
  title={Rambo-rl: Robust adversarial model-based offline reinforcement learning},
  author={Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16082--16097},
  year={2022}
}

