\section{Conclusion}
\label{sec:conclusion}



Despite their notable sample efficiency, model-based reinforcement learning methods can be extremely slow to train, which limits their usability in robotic systems.
This paper introduces S5WM, a state-space world model designed to accelerate training in model-based reinforcement learning. 
We adapt the S5 SSM architecture to handle resetability at the episode boundaries, and use it as a sequence model of the dynamics in our world model. 
Additionally, we introduce asymmetry in the world model by providing privileged state information during training in simulation. 

We compare our approach against model-based and model-free baselines, focusing on training time, performance, and sample efficiency. 
Our method achieves faster training times without compromising performance and sample efficiency. 
Additionally, we test our approach on a drone racing environment and conduct real-world experiments for two distinct tasks involving both state and image observations. 
Our results demonstrate that S5WM performs well in challenging real-world scenarios.
Furthermore, we conduct a series of ablations to highlight the key components of our approach.

While the presented approach shows promising results, we identify several directions for future work. 
For instance, one valuable area of research would be to speed up the actor-critic training in MBRL.
Additionally, our proposed approach is not as robust to hyperparameter changes as more mature MBRL methods such as Dreamer.
Future work could investigate modifications to our method to improve its robustness to hyperparameter choice.
Furthermore, it would be interesting to apply and evaluate our method on different robotic domains such as locomotion and manipulation.

In summary, our method offers a promising framework for accelerating model-based reinforcement learning, combining high sample efficiency with faster training times, making it more feasible for real-world robotic deployment.

\section*{Acknowledgments}
This work was supported by the European Unionâ€™s Horizon Europe Research and Innovation Programme under grant agreement No. 101120732 (AUTOASSESS) and the European Research Council (ERC) under grant agreement No. 864042 (AGILEFLIGHT).
