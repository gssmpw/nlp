\section{Methodology}
\label{sec:method}
In this work, we propose a model-based RL approach that leverages state-space world models. 
At each training step, we sample data from the replay buffer to train the world model as explained in section~\ref{sec:s5wm} and the actor-critic as explained in section~\ref{sec:ac}.

\subsection{State-Space World Model}
\label{sec:s5wm}
The proposed world model is based on DreamerV3~\cite{hafner2023mastering}.
However, we replace the recurrent state-space model~(RSSM)~\cite{hafner2019learning} with an SSM as the sequence model approximating the transition dynamics.
Our model includes an encoder for latent variable inference, which maps the observations $o_t$, to posterior stochastic representations $z_t$.
For a prediction horizon $H$, the sequence model predicts the deterministic representations $y_{t+1\,:\,t+H}$ given the corresponding actions $a_{t\,:\,t+H-1}$, and previous posterior $z_{t\,:\,t+H-1}$. We then infer the prior stochastic representation $\hat{z}_t$ from $y_t$.
In addition, the model includes reward and episode continuation predictors as well as an observation decoder,
\begin{equation}
\begin{alignedat}{4}
&\text{Encoder:} && \quad && z_t    && \sim \;  \Dist{q}{\phi}{\cdot}{o_t} \\
&\text{Sequence model:} && \quad x_t, \; && y_t && = \; \text{SSM}(z_{t-1}, a_{t-1}, c_{t-1}, x_{t-1})  \\
&\text{Dynamics:} && \quad && \hat{z}_t  && \sim \; \Dist{p}{\phi}{\cdot}{y_t} \\
&\text{Decoder:} && \quad && \hat{o}_t   && \sim \; \Dist{p}{\phi}{\cdot}{y_t, z_t} \\
&\text{Reward prediction:} && \quad && \hat{r}_t && \sim \; \Dist{p}{\phi}{\cdot}{y_t, z_t} \\
&\text{Continue prediction:} && \quad && \hat{c}_t && \sim\; \Dist{p}{\phi}{\cdot}{y_t, z_t}.
\end{alignedat}
\label{eq:wm}
\end{equation}
We fuse $z_t$ and $a_t$ using a multi-layer perceptron~(MLP) which outputs a single vector $u_t$. 
We then feed $u_t$ as input to the SSM Block (see Appendix~\ref{sec:s5block}).

Unlike RSSMs, which process the sequence one step at a time, SSMs parallelize over the sequence dimension, making them particularly efficient for training on long sequences.
Among the different SSM architectures, we specifically choose S5~\cite{smith2023simplified}, which employs the parallel scan operation.
One key advantage of S5 is its ability to reset the hidden state, which is not possible in other SSMs, such as S4~\cite{gu2022efficiently}.
This is crucial for preventing the model from carrying over irrelevant information from previous episodes, which is particularly relevant in scenarios where sequence boundaries or discontinuities must be handled.
We leverage the associative operator introduced in~\cite{lu2024structured}, which enables efficient resetting of the hidden state.
We extend the initial elements $e_k$ from equation~\ref{eq:sequence_scan} to include the continuity predictions $c_k\in\{0,1\}$,
\begin{equation}
\label{eq:sequence_scan_reset}
e_k = (e_{k,a}, e_{k,b}, e_{k,c}) := (\bar{A}, \bar{B} u_k, 1-c_k),
\end{equation}
where $e_{k,c}$ represents whether the episode is done.
We then adapt the binary operator from~\ref{eq:associative} to incorporate the continuity predictors,
\begin{equation}
\label{eq:associative_reset}
a_i \bullet a_j = 
\begin{cases}
(a_{j,a} \odot a_{i,a}, a_{j,a} \otimes a_{i,b} + a_{j,b}, a_{i,c}) & \text{if } a_{j,c} = 0, \\
(a_{j,a}, a_{j,b}, a_{j,c}) & \text{if } a_{j,c} = 1.
\end{cases}
\end{equation}
Using these definitions, we compute the sequence of hidden states $x_{1:L}$ as in equation~\ref{eq:full_scan}.
Since our world model uses an S5 backbone for sequence modeling, we refer to our method as S5WM.

For optimizing the world model, we use the same loss function as in~\cite{hafner2023mastering}, which is derived based on the evidence lower bound of the marginal data likelihood.
The loss combines a prediction loss $\mathcal{L}_{pred}$, a dynamic loss $\mathcal{L}_{dyn}$, and a representation loss $\mathcal{L}_{rep}$,
\begin{small}
\begin{equation}
    \mathcal{L}(\phi) \doteq  \mathbb{E}_{q_\phi}\biggl[ \sum_{t=1}^T \beta_{pred} \mathcal{L}_{pred}(\phi) + \beta_{dyn} \mathcal{L}_{dyn}(\phi) \\
      +\beta_{rep} \mathcal{L}_{rep}(\phi) \biggr],
\end{equation}
\end{small}
where $\beta_{pred}$, $\beta_{dyn}$, and $\beta_{rep}$ are hyperparameters that modulate the effects of the different loss components,
\begin{equation}
\begin{aligned}
    \mathcal{L_{\mathrm{pred}}}(\phi) & \doteq
-\ln \Dist{p}{\phi}{o_t}{y_t, z_t}
-\ln \Dist{p}{\phi}{r_t}{y_t, z_t}
\\
& \hphantom{=} \,\,
-\ln \Dist{p}{\phi}{c_t}{y_t, z_t} \\
\mathcal{L_{\mathrm{dyn}}}(\phi) & \doteq
\max\bigl(1, \KL{\sg(\Dist{q}{\phi}{z_t}{o_t})}{\Dist{p}{\phi}{z_t}{x_t}\hphantom{)}}\bigr) \\
\mathcal{L_{\mathrm{rep}}}(\phi) & \doteq
\max\bigl(1, \KL{\Dist{q}{\phi}{z_t}{o_t})}{ \sg(\Dist{p}{\phi}{z_t}{x_t})}\bigr),
\end{aligned}
\end{equation}
where $D_{\text{KL}}$ refers to the Kullback-Leibler divergence, and $\sg$ to the stop gradient operation.

Each component of $\mathcal{L}(\phi)$ contributes to training a different component of the state-space world model.
The representation loss ensures that the encoder learns an informative posterior stochastic latent $\Dist{q}{\phi}{z_t}{o_t}$.
The dynamic loss helps the world model learn the transition dynamics by encouraging the prior stochastic latent $\Dist{p}{\phi}{z_t}{x_t}$, to align with the posterior.
The prediction loss trains the decoder, as well as the reward and continue prediction heads, guiding the model to accurately predict next states.
\begin{figure}%
        \centering
        \includegraphics[width=\linewidth]{figures/deployment_imagination.pdf}
        \caption{To train the actor and critic, we leverage imaginations in the latent state and using our state-space world model.
        We obtain these imaginations by encoding the initial observation and rolling out the sequence model in the latent space.
        Despite the possibility of training the world model in a parallel fashion, the imagination step (needed to train the actor-critic) cannot be parallelized due to the dependence on the policy to generate actions needed for rolling out the trajectories.}
        \label{fig:imaginationprocess}
\end{figure}
\subsection{Actor-Critic Training}
\label{sec:ac}

In practice, the world model serves as an approximate differentiable simulator providing first-order gradients for policy updates~\cite{xu2022accelerated}. This helps reduce variance estimation, especially for systems with smooth dynamics~\cite{suh2022differentiable}.
Unlike model-free RL methods, which rely solely on actual environment interactions, in model-based RL, the actor-critic is trained using imagined trajectories generated by the world model.
For a given horizon length $H$, we generate imagined trajectories by stepping through the world model, starting from the initial states $\{\hat{z}_0, x_0\}$. For $t \in \{1, H\}$,
\begin{equation}
\begin{aligned}
x_t, \; y_t &= \text{SSM}(\hat{z}_{t-1}, a_{t-1}, \hat{c}_{t-1}, x_{t-1}) \\
\hat{z}_t &\sim \; \Dist{p}{\phi}{\cdot}{y_t}  \\
\hat{r}_t &\sim \; \Dist{p}{\phi}{\cdot}{y_t, \hat{z}_t} \\
\hat{c}_t &\sim \; \Dist{p}{\phi}{\cdot}{y_t, \hat{z}_t} \\
a_t &\sim \Dist{\pi}{\theta}{\cdot}{y_t, \hat{z}_t}.
\end{aligned}
\label{eq:imaginationproc}
\end{equation}
Unlike during world model training (see equation~\eqref{eq:wm}), where we have access to the posterior stochastic latent $z_t$, during imagination, we rely solely on the prior stochastic latent $\hat{z}_t$.
We sample the initial states $\{\hat{z}_0, x_0\}$, from the data used during world model training while ensuring sufficient context to generate meaningful predictions, and enough diversity among the initial states to learn a robust value function.

The actor $\pi_{\theta}$ is conditioned on both the deterministic representations $y_t$, and the prior stochastic latent $\hat{z}_t$, as shown in equation~\eqref{eq:imaginationproc}.
The policy follows a Gaussian distribution, with actions sampled as $a_t \sim \Dist{\pi}{\theta}{\cdot}{y_t, \hat{z}_t}$.
In practice, the policy is represented with an MLP outputting the mean and variance of a Gaussian distribution of actions.

The critic, similarly conditioned on $y_t$ and $\hat{z}_t$, as well as $\hat{r}_t$ and $\hat{c}_t$, is trained to predict the bootstrapped $\lambda$-returns~\cite{sutton2018reinforcement}, as proposed in~\cite{Hafner2020Dream}. 
The critic's loss function is given by
\begin{equation}
\begin{aligned}
    \mathcal{L}(\psi) \doteq -\textstyle\sum_{t=1}^T \ln \Dist{p}{\psi}{V^\lambda_t }{x_t}\\
V^\lambda_t \doteq \hat{r}_t + \gamma \hat{c}_t \Big(
  (1 - \lambda) v_t +
  \lambda V^\lambda_{t+1}
\Big),
\end{aligned}
\end{equation}
where $\gamma$ is the discount factor, $v_t \doteq \mathbb{E}[\Dist{v}{\psi}{.}{y_t, \hat{z}_t} ] $ is the predicted value from the critic, and $V^\lambda_T \doteq v_T$.

The actor is trained to maximize the expected returns, which are estimated using a combination of short-horizon imagined trajectories and the value function for long-term value prediction (see Fig.~\ref{fig:imaginationprocess}).
To encourage exploration, an entropy regularization term $-\eta \mathbb{H}[a_t|\hat{z}_t]$ is introduced in the actor's loss function to penalize deterministic policies and to promote more diverse actions for improved exploration,
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) \doteq
\mathbb{E}_{p_\phi,p_\theta}\Big[
  \textstyle\sum_{t=1}^{H-1} \big(
    -
    V^\lambda_t
    -\eta \mathbb{H}[a_t|\hat{z}_t]
\big)\Big],
\end{aligned}
\end{equation}
Unlike DreamerV3~\cite{hafner2023mastering}, we use the first-order gradients backpropagated through the dynamics model. 
\subsection{Privileged World Models}


In environments with high-dimensional and partially observable inputs, such as the ones encountered in vision-based RL, learning informative latent representations is challenging. 
Additional supervision signals are typically required to help the model learn meaningful representations, which are essential for efficient policy optimization~\cite{lillicrap2015continuous, kalashnikov2018scalable}.

When training policies in simulation, it is possible to take advantage of additional observations that are rarely available during real-world deployments, such information is typically referred to as privileged information and can be used in various ways to boost the training performance in simulation~\cite{pinto2018asymmetric,chen2020learning,yamada2024twist,messikommer2025studentinformed}.
We explore the use of privileged information for training the world model. 
In this setup, the world model has access to privileged information during training, which is not available during real-world deployment. 

In this work, we specifically explore the use of privileged state observations in a vision-based setting. 
During training, the image observation, $o_t$, is encoded into a posterior stochastic state, $z_t$, to be processed by the sequence model (see section~\ref{sec:s5wm}). 
We then replace the decoder from equation~\eqref{eq:wm} with a privileged decoder that reconstructs the privileged state observation, $s_t$, rather than the image observation, 

\begin{equation}
    \text{Privileged decoder:} \quad \hat{s}_t \sim \; p(\cdot|y_t, z_t),
\end{equation}
Reconstructing the state involves predicting $s \in \mathbb{R}^{24}$, which is computationally more tractable than inferring a high dimensional image using an expensive convolutional neural network~(CNN) decoder.
Furthermore, learning to reconstruct state observations, rather than images, provides a stronger signal for training the encoder.
This encourages the latent representation to capture information that is more relevant for policy optimization, rather than optimizing for accurate image reconstruction.
