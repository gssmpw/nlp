\section{Experiments}
\label{sec:experiments}
We design the experiments to answer the following questions:
\begin{itemize}
    \item Does S5WM accelerate the training of model-based RL?
    \item Does S5WM achieve comparable sample efficiency and task reward to state-of-the-art model-based RL?
    \item What are the advantages of using a privileged world model in partially observable environments?
    \item Does S5WM transfer from simulation to a real-world environment?
\end{itemize}

We validate our approach in a drone racing environment and compare our method against both model-based and model-free baselines.
We then conduct a series of ablations that highlight several design choices of our method.
\subsection{Setup}
\label{sec:setup}

Our setup is consistent across all experiments and utilizes the same quadrotor configuration. 
We first perform training in a simulation environment. 
We then validate our approach in the real world. 
For the environment setup, we use a combination of the Flightmare~\cite{yunlong2020flightmare} and Agilicious~\cite{foehn2022agilicious} software stacks.
All experiments were conducted on the same hardware under uniform conditions. 
\\
\subsubsection{Tasks}
\label{sec:task}
We investigate two tasks, both involving a quadrotor flying in a drone racing environment.

For the first task, termed \textit{state-based Split-S}, the quadrotor flies through the Split-S track and has access to the full-state observations.
This task presents highly complex dynamics, requiring the policy to push the drone's agility to the physical limits of the platform.
Due to the complexity of the track, the task demands both long-term memory and high precision, as the drone must plan ahead to navigate the complex maneuvers at high speeds.

The second task, termed \textit{vision-based Figure-8}, involves flying through the Figure-8 track and involves simpler dynamics, with the drone flying at a slower pace.
However, the challenge in this task lies in learning the dynamics model from high-dimensional image observations, which can be challenging due to the partial observability of the resulting environment.
\\
\input{figures/rewards_state_vision}
\input{figures/main_table}
\subsubsection{Observation Space}
For the \textit{state-based Split-S} task, we define the observation $s = [p, \tilde{R}, v, \omega, i, d, a_{prev}] \in \mathbb{R}^{24}$, where $p \in \mathbb{R}^3$ is the position of the drone, $\tilde{R} \in \mathbb{R}^6$ contains the first two columns of the rotation matrix, $v \in \mathbb{R}^3$ is the linear velocity, and $\omega \in \mathbb{R}^3$ is the angular velocity. The vector $i \in \mathbb{R}^2$ encodes the gate index using sine-cosine encoding to address the periodicity of the track. 
The continuous gate index $i_c$ is defined as 
\begin{equation}
    i_c = i + \frac{2}{1 + \exp(k \cdot d)},
\end{equation}
where $d \in \mathbb{R}$ is the distance to the next gate, and $a_{prev} \in \mathbb{R}^4$ represents the previous action.

For the \textit{vision-based Figure-8} task, the observation space is given by RGB images of size $64 \times 64 \times 3$, rendered from the simulator~\cite{savva2019habitat}.
The raw pixel-level input is then processed through a CNN encoder to extract relevant features.
Similarly to \cite{geles2024demonstrating}, we include the previous 3 actions in the observation to provide historical context. \\
\input{figures/timings}
\subsubsection{Action Space}
\label{sec:actions}
For both tasks, we define $a = [c, \omega_{des}] \in \mathbb{R}^4$, where $c$ is the mass-normalized collective thrust and $\omega_{des} \in \mathbb{R}^3$ represents the desired body rates.
These commands are then processed by a low-level controller, which outputs the desired motor speeds. \\

\subsubsection{Reward}
\input{figures/imagination} %
Similar to prior works on drone racing~\cite{romero2024actor, song2023reaching,xing2024multi}, we encode the task using a dense reward function $r_t$,
\begin{equation}
r_t = 
\begin{cases}
r_{\text{crash}} & \text{crashed} \\
r_{\text{pass}} & \text{gate passed} \\
r_{\text{prog}} + r_{\text{omega}} + r_{\text{cmd}} + r_{\Delta\text{cmd}} & \text{otherwise}.
\end{cases}
\end{equation}
where $r_{\text{crash}}$ is a terminal penalty for collisions, $r_{\text{pass}}$ encourages passing a gate, $r_{\text{prog}}$ encourages progress along the track, $r_{\text{omega}}$ penalizes excessive body rates, $r_{\text{cmd}}$ penalizes aggressive actions, and $r_{\Delta\text{cmd}}$ encourages smooth actions. The individual reward components are detailed in the appendix~\ref{sec:rewards}.
\subsection{Baselines}
\label{sec:baselines}
We compare our approach against both model-based and model-free baselines. For the model-based baseline, we use DreamerV3~\cite{hafner2023mastering, romero2024dream}, while for the model-free baseline, we choose PPO~\cite{schulman2017ppo}.
For the vision-based task, we introduce an additional baseline that modifies DreamerV3 to decode privileged state information instead of observation decoding.
We refer to this baseline as "DreamerV3-P".
We also compare against a variant of our method which omits decoding the privileged information and instead decodes raw observations.
Our approach uses the same model architecture and hyperparameters as DreamerV3, with the exception of the world model configuration, where we replace the RSSM with S5WM, as introduced in section~\ref{sec:method}.
We design S5WM to have a comparable number of parameters to RSSM, as detailed in the appendix~\ref{sec:hyperparams}.
We tune the hyperparameters of the baselines to ensure a fair comparison.
For the \textit{state-based Split-S} task, we use standard PPO, and for the \textit{vision-based Figure-8} task, we use PPO with the asymmetric actor-critic architecture~\cite{pinto2018asymmetric}, which provides privileged information to the critic, and employs the same CNN encoder architecture as used in our method and DreamerV3.
We train the CNN encoder jointly with the policy.
For each task, all models share the same observation, action, and reward configurations, as described in Section \ref{sec:setup}.

\subsection{Simulation Results}
We train the policies in a high-fidelity simulator on an A100 GPU.
For all experiments, we simulate 50 environments in parallel and limit the number of interactions with the environment to $\text{10}^{7}$. 
We use a fixed-size replay buffer containing ${10}^{6}$ samples, from which we uniformly sample at each training step.
During evaluation, we rollout the policy over 1000 steps, which corresponds to 20s of flight given a 50Hz control rate.
We report the average task reward obtained per episode.
We evaluate our approach based on performance, accuracy and training efficiency. \\
\input{figures/real_world}
\subsubsection{Performance}
In Fig.~\ref{fig:rewards_state_vision}, we evaluate the average task reward over the number of environment interactions for each task.
Overall, we observe a clear advantage of MBRL approaches in terms of sample efficiency.
This result corroborates previous findings from the MBRL literature~\cite{Hafner2020Dream, janner2019trust, yu2020mopo}.
For the \textit{state-based Split-S} task, we find that both S5WM and DreamerV3 achieve similar sample efficiency and converge to the maximum reward, while PPO struggles to yield competitive policies within the limited data budget.

For the \textit{vision-based Figure-8} task, we compare both S5WM and DreamerV3 with and without the privileged world model, as well as the asymmetric PPO.
We find that leveraging privileged information results in higher sample efficiency for both S5WM and DreamerV3.
These benefits are more pronounced for S5WM, where the posterior stochastic latent $z_t$ is inferred directly from the observation.
In contrast, RSSMs also leverage the deterministic state $y_t$ to infer the posterior $z_t$.
These differences are further discussed in section~\ref{sec:ablation_recurrent}.
Overall, our method performs on par with the DreamerV3 variants while being substantially faster to train as we discuss in the next section.
This is evident from the final lap time and success rate achieved by our method compared to the baselines, as shown in Table~\ref{tab:sim_real_results}.

The trajectories obtained after reward convergence are shown in appendix~\ref{sec:appendix_trajectories}.
For both tasks, we find that the most difficult part is completing the track after learning how to fly through the gates.
Additionally, for the \textit{vision-based Figure-8} task, the drone struggles to transition from Gate 3 to Gate 4, as it tends to overshoot and is unable to \textit{see} the next gate.

\subsubsection{Training Efficiency}
We profile S5WM, along with the baselines, to assess the differences in training times.
Each training step is divided into three stages: i) training the world model (observation), ii) optimizing the policy (imagination), and iii) collecting new data by interacting with the environment.
Fig.~\ref{fig:timings} shows the average times for each stage, as well as the overall duration per step.
We find that S5WM outperforms DreamerV3 in terms of overall training speed in both tasks, as the observations in S5WM are parallelized over the sequence length.
In the \textit{state-based Split-S} task, the observation step is up to 10 times faster, leading to an overall speedup of up to 4 times.
In the \textit{vision-based Figure-8} task, the observation step is up to 4 times faster, leading to an overall speedup of up to 2 times.
These benefits are most pronounced when the main computational bottleneck lies in modeling the dynamics, such as in the \textit{state-based Split-S} task.
In tasks that require learning a latent representation from complex visual inputs, such as the \textit{vision-based Figure-8} task, parallelizing the sequence model contributes less to the overall speedup.

\subsubsection{Prediction Accuracy}
We further evaluate the prediction accuracy of the world model.
We provide an initial context length $C=16$ to build a history of hidden states $x_{1:C}$, followed by imagination over the horizon $H$.
During the context period, the world model has access to the true observations from the environment, while during the imagination period, the world model predicts the trajectory without access to the observations.
Fig.~\ref{fig:imaginationtraj} shows the trajectories imagined by S5WM for $H=50$ for different parts of the track, along with the ground truth, which is obtained by interacting with the environment.
A more detailed representation of the imagined trajectories can be found in appendix~\ref{sec:imag_state}.

\subsection{Real World Deployment}

We test our approach in the real world using a high-performance racing drone~\cite{foehn2022agilicious}.
At deployment time, our method runs on an offboard desktop computer equipped with an Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz and Quadro RTX 4000 GPU. 
A Radix FC board equipped with the Betaflight\footnote{\url{https://www.betaflight.com}} firmware is used as the low-level controller, which takes as inputs the desired body rates and collective thrusts. 
An RF bridge is employed to transmit commands to the drone. 
For state estimation, we use a VICON system with 36 cameras that provide the platform with millimeter accuracy measurements of position and orientation at a rate of 400 Hz.
We select the best policy obtained during training and deploy it in the real world.
A video of the real-world deployment of both tasks can be found in the supplementary material.

In addition, Fig.~\ref{fig:real_world} shows the trajectories from the real-world experiments. 
 For the \textit{state-based Split-S} task, the Split-S maneuver at $x=-4.3\unit{m}$ and $y=-5.1\unit{m}$, stands out as a critical test of each approach's characteristics.
This complex maneuver requires the drone to fly through a higher gate and then immediately descend through a second gate located directly below the first one, with both gates sharing the same $x,y$ coordinates. 
This is the most challenging maneuver of the Split-S track, significantly influencing the overall lap time.

For the \textit{vision-based Figure-8} task, we use a hardware-in-the-loop~(HIL) setup, where images are rendered in real-time from the simulator based on state estimation from a motion-caption system estimating the position of the real robot.
For both tasks, our method enables high-speed flight on very smooth trajectories.
\subsection{Ablations}
\label{sec:ablations}
We discuss several key components that we find to be essential to the success of our approach. 
\subsubsection{Horizon Length}
In model-based RL, the actor-critic uses the world model to plan over a prediction horizon, which can substantially improve sample efficiency compared to model-free RL.
While a longer horizon is typically preferred, prediction errors tend to compound over time and therefore limit the effectiveness of imagination. 
Additionally, imagination steps are computationally expensive and cannot be parallelized, making longer horizons computationally more expensive. 
To investigate the influence of the horizon length on our method, we train the \textit{state-based Split-S} task using different horizon lengths. 
Fig.~\ref{fig:horizon} shows the task reward over the number of environment interactions for $H \in \{5, 10, 15\}$, as well as the time it takes till convergence. 
We find that $H=10$ offers a good balance between sample efficiency and computational complexity, and we therefore use it for all of our experiments.
\input{figures/rewards_horizon}
\subsubsection{Reward Smoothing}
Reward smoothing is essential for enhancing training. 
In model-free RL, it is common to adapt the reward function during training to increase the task difficulty.
While this helps the agent adapt to progressively harder settings, we find that directly altering the reward function in a model-based RL setting can be counterproductive, because the world model needs to \textit{unlearn} the old reward signal and relearn the new one.

To address this issue, we propose a smoothing strategy that decouples the total reward into two complementary components,
\begin{equation}
r = r_{\text{nom}} + r_{\text{aug}},
\end{equation}
where $r_{\text{nom}}$ (the nominal reward) is active from the start of training, and $r_{\text{aug}}$ (the smoothing reward) is learned as well from the start but only applied once the agent becomes sufficiently proficient. 
Initially, the value function only receives $r_{\text{nom}}$, and later $r_{\text{aug}}$ is also added.
We use this strategy to encourage smoother actions, which is particularly important for real-world deployment. 
We define
\begin{equation}
\begin{aligned}
r_{\text{nom}}&=r_{\text{prog}}+r_{\text{omega}}+r_{\text{cmd}} \\
r_{\text{aug}}&=r_{\Delta\text{cmd}},
\end{aligned}
\end{equation}
where we follow the definitions from section~\ref{sec:rewards}. 
We apply $r_{\text{aug}}$ halfway through training, after $\text{5}\times\text{10}^{6}$ steps. 
If $r_{\text{aug}}$ is instead applied from the beginning, the agent struggles to learn the task.
By delaying the smoothing reward, the agent first explores to discover a feasible solution, and later refines it to produce smoother actions.
Fig.~\ref{fig:smooth_actions} shows the actions learned with and without reward smoothing.
By introducing this smoothing curriculum, we significantly improve the smoothness of the control commands, preventing motor damage and reducing the sim-to-real gap by ensuring the policy does not learn infeasible command sequences in simulation.

\input{figures/rew_smooth.tex}
\input{figures/rewards_recurrent}

\subsubsection{Recurrent vs Parallel}
\label{sec:ablation_recurrent}
One main difference between our S5WM architecture (see section~\ref{sec:s5wm}) and RSSMs lies in how the posterior stochastic latent $z_t$ is computed.
In RSSMs, each step computes a prior $\hat{z}_t\sim\;\Dist{p}{\phi}{\cdot}{y_t}$ and a posterior $z_t\sim\;\Dist{q}{\phi}{\cdot}{y_t,o_t}$), both of which include the deterministic representation $y_t$.
Therefore, the posterior receives the same information as the prior plus access to the latest observation $o_t$.
By contrast, since S5WM processes the entire sequence in parallel, the posterior is determined only from the observation, $z_t\sim\;\Dist{q}{\phi}{\cdot}{o_t}$, and does not have access to historical context encoded in $y_t$.
To compensate for this, we append past actions to $o_t$, as explained in section \ref{sec:actions}, providing S5WM with additional temporal context.

We investigate this difference by implementing a \textit{recurrent S5WM} variant where the posterior also depends on $y_t$, i.e., $z_t\sim\;\Dist{q}{\phi}{\cdot}{y_t,o_t}$.
We compare the \textit{recurrent S5WM} to the standard parallel S5WM on the \textit{vision-based Figure-8} task.
Note that the \textit{recurrent S5WM} reintroduces the sequential dependencies of RSSMs, and therefore offers no speed-up in training.
This comparison is included mainly to highlight the importance of historical context in partially observable environments.
