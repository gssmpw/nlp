\section{Introduction}
\label{sec:intro}

    Robot learning has proven to be a very successful paradigm for acquiring complex robotic skills. 
    Recent work has demonstrated the applicability of reinforcement learning~(RL) approaches to manipulation~\cite{andrychowicz2020learning,handa2023dextreme,aljalbout2024role,o2024open}, locomotion~\cite{tan2018sim, hwangbo2019learning}, and aerial robotics~\cite{kaufmann23champion,song2023reaching}.
    For such methods to succeed, they either require encoding the task into a reward function or access to a dataset of expert demonstrations.
    Learning control policies reduces the barrier to skill acquisition in robotics, requiring less human interventions and efforts to equip a robot with a new skill.
    In addition, RL methods impose minimal constraints in the design of reward functions.
    As a result, they allow for greater flexibility in the design of robotic systems, which can lead to outperforming classical control approaches~\cite{song2023reaching, lee2020learning}.
    
    Despite their success, one major challenge in robot learning is the need for large amounts of physical interaction data, which can be very expensive and challenging to obtain. 
    In addition, RL suffers from high-variance gradients, often leading to unstable training and raising the need for even more data samples.
    This problem is further exacerbated when learning vision-based control policies due to the high-dimensionality of image-based observations.

    
    \begin{figure}[t!]
        \centering
        \includegraphics[width=\linewidth]{figures/fig1_wmssm.pdf}
        \caption{State-of-the-art model-based RL~(MBRL) methods typically employ recurrent state-space models (RSSMs) as the world model backbone, which are slow in training due to the sequential nature of RNNs.
        We leverage state-space models (SSMs) to parallelize the sequence dimension of the world model, thereby reducing the computational complexity of training the world model. 
        Moreover, we propose reconstructing privileged observations of lower dimensionality $s_t$, rather than high-dimensional image observations $o_t$.}
        \label{fig:fig1}
    \end{figure}
    
    To alleviate this problem, multiple solutions have been proposed to leverage prior knowledge in RL.
    These solutions range from pretraining the policy or parts of it using either imitation learning~\cite{xing2024bootstrapping} or self-supervised learning objectives~\cite{yarats2021improving, aljalbout2021learning,lee2020making}, to embedding inductive biases into RL policy architectures and training pipelines to reduce the policy complexity~\cite{lutter2021differentiable,funk2022learn2assemble}.
    Model-based RL has emerged as a promising alternative to improve sample efficiency in comparison to model-free RL~\cite{deisenroth2011learning, heess2015learning,Hafner2020Dream, aljalbout2024limt}.
    
    However, model-based RL methods are typically slower in training than their model-free counterparts because they additionally train the world model~(WM), which can be slow due to the sequential nature of the dynamics.
    This aspect limits the applicability of MBRL, particularly for cases where fast training is desirable.
    
    In this work, we propose a method for accelerating model-based reinforcement learning~(MBRL).
    We leverage state-space models~(SSMs) to parallelize the sequence dimension of the world model, thereby reducing the computational complexity of MBRL.
   We build on the family of Dreamer-based MBRL methods~\cite{Hafner2020Dream, hafner2023mastering}.
    In our approach, we replace the recurrent state-space model~(RSSM) with a modern parallelizable SSM as the dynamics model.
    
    To evaluate our approach, we perform experiments in a drone racing environment, involving complex dynamics, for both fully and partially observable environments. We then compare our method to state-of-the-art model-free and model-based RL methods.
    Our method achieves a significant speedup, reducing the overall training time by up to 4 times.
    This benefit comes without compromising performance, as our method attains similar sample efficiency and task rewards as state-of-the-art methods.

\subsection*{Contributions}
Our contributions are summarized as follows:
\begin{itemize}
    \item We present a method for leveraging state-space models to accelerate the sequence model training in MBRL world models.
    \item Our method achieves comparable sample efficiency to state-of-the-art MBRL methods while reducing world model training time by up to 10 times, and the overall MBRL training time by up to 4 times.
    \item We present an approach to facilitate the sim-to-real transfer of our vision-based policies that leverage the privileged state information while training the vision-based world model.
    \item We demonstrate our approach in the real-world robotic task of agile quadrotor flight, which involves nonlinear complex dynamics.%
\end{itemize}
