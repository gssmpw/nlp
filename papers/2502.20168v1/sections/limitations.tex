\section{Limitations}
While S5WM offers significant advantages in training speed, it also has several shortcomings. 
First, SSMs can be sensitive to hyperparameters, especially compared to RSSMs, which appear more robust across different tasks.
For instance, choosing a suitable architecture of the S5 Block (see appendix~\ref{sec:s5block}), which wraps the S5 layer with nonlinearities and a suitable gating mechanism, is crucial to S5WM's success.
This sensitivity may stem from the rigid, linear nature of the SSMs compared to more flexible architectures like Gated Recurrent Units~(GRUs).
However, a direct comparison of hyperparameter robustness with DreamerV3 would be unfair, given that the latter is fairly mature and has been extensively refined over the years to improve robustness.
A more comprehensive hyperparameter search for S5WM could improve our understanding of its internal workings, though such an analysis would be computationally expensive and remains to be explored in future work.

Finally, the computational benefits of SSMs are most prominent when the bottleneck lies in modeling the dynamics. 
In tasks where the bottleneck shifts to other factors, such as in learning latent representations from high-dimensional visual observations, the advantages of SSMs become less pronounced.
