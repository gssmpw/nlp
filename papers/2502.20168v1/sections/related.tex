\section{Related Work}
\label{sec:relatedwork}

\subsection{Model-Based RL}
While model-free RL methods, such as proximal policy optimization~(PPO), have been prevalent in robot learning~\cite{hwangbo2019learning, andrychowicz2020learning,alles2022learning,song2023reaching, handa2023dextreme,aljalbout2024role}, model-based RL is gaining popularity due to its sample efficiency.
The main difference between the two paradigms is that MBRL learns a model of the environment's dynamics, in addition to the policy~\cite{sutton2018reinforcement}.
This model, known as world model, can either be used for i) planning and control~\cite{nagabandi2018neural, hansen2022temporal}, ii) sampling environment interactions for policy training~\cite{janner2019trust,yu2020mopo,kidambi2020morel}, or iii) obtaining first-order policy gradients through imagination~\cite{heess2015learning,Hafner2020Dream,hafner2023mastering}.
World models play a crucial role in MBRL, with recurrent neural networks (RNNs) being the most widely adopted architecture. 
One popular RNN-based world model architecture is the Recurrent State-Space Model (RSSM), introduced in the Dreamer framework~\cite{hafner2019learning}.
However, RSSMs struggle to scale efficiently to long sequences, as their computational complexity increases significantly with the sequence length.

Recent works have explored alternative architectures, such as Transformers and SSMs. 
Transformers, in particular, have been widely adopted as world model backbones in MBRL, showing advantages both for sample efficiency and computational complexity~\cite{robine2023twm, zhang2023storm, iris2023, chen2022transdreamer}.
Similarly, in~\cite{deng2024facing} an S4-based world model is introduced.
However, this work does not explore the usage of such world models for MBRL and focuses on mastering predictions in long-range memory tasks. 

One of the earliest applications of MBRL to robotics was shown on a low-cost manipulator and using a Gaussian process-driven policy representation~\cite{deisenroth2011learning}.
Later work adopted a combination of deep dynamics models with model predictive control to scale this concept to vision-based robotics tasks~\cite{Wahlstrom2015c}.
\citet{nagabandi2018neural} proposed using MBRL to gather expert data for training an initial policy using model-free methods.
By doing so, they managed to alleviate the inferior task performance of MBRL methods.
\citet{chua2018deep} proposed using ensembles of dynamics model to enable uncertainty propagation during trajectory sampling.
These approaches have also been extended to jointly learn a value function together with the dynamics sequence model~\cite{hansen2022temporal,hansen2024tdmpc}.
Most of these methods perform receding horizon control with a sampling scheme.

Another line of work leverages first-order policy gradients that are backpropagated through a learned latent dynamics model~\cite{heess2015learning,Hafner2020Dream,hafner2023mastering}, which has been successfully applied to multiple robotic tasks in manipulation, locomotion, and drone flight~\cite{wu2023daydreamer, becker2020learning,brunnbauer2022latent,richard2022learning,aljalbout2024limt,bi2024sample,yamada2024twist}.


\subsection{Vision-Based Robot Learning}
While state-based RL can be challenging, learning vision-based policies presents additional difficulties. First, the higher dimensionality of the observation space makes policy search more complex, as the model must process significantly larger inputs. Second, tasks relying on visual inputs often suffer from partial observability, where the raw input does not provide a complete representation of the state. Therefore, such policies typically require additional supervision signals to effectively learn informative latent representations from raw pixel data. As a result, vision-based RL often requires a large amount of data to achieve good performance, as individual samples tend to be less informative compared to state-based environments~\cite{lillicrap2015continuous, kalashnikov2018scalable}.

Several strategies have been proposed to improve the sample-efficiency of vision-based RL. One prominent approach is to embed auxiliary losses to provide additional signals for training the perception components of the policy.
Auxiliary losses are typically inspired by the self-supervised learning literature and include methods like auto-encoding and contrastive learning~\cite{yarats2021improving,laskin2020curl}.
These methods were mostly successful in simulation~\cite{yarats2021improving,laskin2020curl,aljalbout2021learning}, but have also been deployed in real-world scenarios~\cite{chen2021robust,xing2024contrastive,silwal2024we,van2016stable}.
Motivated by the success of imitation learning for vision-based policy learning~\cite{fu2024mobile,o2024open}, multiple efforts have been made to bootstrap RL with imitation learning~\cite{DBLP:conf/rss/Zhu0MRECTKHFH18,huflare,xing2024bootstrapping}.
Model-based RL offers a more generic approach that bypasses intermediate abstractions and directly learns a control policy from visual inputs in an end-to-end fashion. Recent works have shown impressive performance across a variety of manipulation, locomotion, and agile flight tasks in real-world scenarios~\cite{wu2023daydreamer, romero2024dream}.
