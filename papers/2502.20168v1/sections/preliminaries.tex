\section{Preliminaries}

\subsection{Reinforcement Learning}
Given a reward function, RL  automates policy learning or search, by  maximizing the cumulative reward in a given environment~\cite{sutton2018reinforcement}.
Tasks are usually formulated as Markov Decision Processes~(MDP)~\cite{bellman1957markovian}. A finite-horizon, discounted MDP is defined by the tuple $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho_0, \gamma, T )$, where $\mathcal{S}$ and $\mathcal{A}$ are the state and action spaces respectively, $\mathcal{P}: \mathcal{S} \times \mathcal{A} \to \mathcal{S}$ is the transition dynamics, and $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ the reward.
Additionally, the MDP definition includes an initial state distribution $\rho_0$, a discount factor $\gamma \in [0,1]$, and a horizon length $T$. 
The optimal policy $\pi : \mathcal{S} \to P(\mathcal{A})$ maximizes the expected discounted reward as an objective function,
\begin{equation}
\label{eq:RL}
J(\pi) =  E_{\pi} \left[ \sum_{t=0}^{T-1} \gamma^t r(s_t, a_t) \right].
\end{equation}
In MBRL, in addition to learning the policy, we learn a model of the environment dynamics, usually referred to as world model of the form $\text{WM}: \mathcal{O} \times \mathcal{A} \to \mathcal{O} \times \mathbb{R}$.
The world model approximates the environment's transition dynamics $p(s_{t+1}\mid{s_t, a_t})$ and rewards $r(s_t, a_t)$ providing a simulated environment in which the agent can perform planning or optimization without requiring actual environment interactions.
This approach is often referred to as \textit{learning in imagination}.
Training typically follows an iterative process composed of three steps.
The first step is to train the world model using the collected data.
In the second step, we optimize the policy within the WM-based simulated environment.
In the third step, we collect new data by interacting with the actual environment using our latest policy.


\subsection{State-Space Models}
State space models (SSMs) provide an efficient framework for sequence modeling tasks and have shown great success in capturing long-term temporal and spatial dependencies~\cite{gu2022efficiently,smith2023simplified,patro2024mamba}. At time $t$, a first-order linear system maps the input $u(t) \in \mathbb{R}^H$ to the output $y(t) \in \mathbb{R}^P$ via the following system dynamics,
\begin{equation}
\begin{aligned}
\frac{dx(t)}{dt} = A x(t) + B u(t) \\
y(t) = C x(t) + D u(t),
\end{aligned}
\end{equation}
where $A, B, C, D$ are learnable parameters which capture the specific dependencies in the data, and $x(t)$ is the hidden state. To model sequences with a fixed time step $\Delta$, the system is discretized as follows,
\begin{equation}
\begin{aligned}
x_t = \bar{A} x_{t-1} + \bar{B} u_t \\
y_t = \bar{C} x_t + \bar{D} u_t,
\end{aligned}
\end{equation}
where $\bar{A}, \bar{B}, \bar{C}, \bar{D}$ are the discretized matrices, computed as functions of the continuous-time matrices $A, B, C, D$.

Given a sequence of inputs $e_{1:L}$, the parallel scan operation efficiently applies an associative binary operator, $\bullet$, to its inputs. Specifically, for a sequence of length $L$, the parallel scan computes:
\begin{equation}
\label{eq:scan}
[e_1, e_1 \bullet e_2, e_1 \bullet e_2 \bullet e_3, \dots, e_1 \bullet e_2 \bullet \dots \bullet e_L].
\end{equation}
Equation \ref{eq:scan} can be computed in $\mathcal{O}(\log(L))$ using $L$ parallel processors. S5~\cite{smith2023simplified} is an SSM that leverages the parallel scan to compute the sequence of hidden states $x_{1:L}$.
In the context of S5, the binary operator is defined as
\begin{equation}
\label{eq:associative}
a_i \bullet a_j = (a_{j,a} \odot a_{i,a}, a_{j,a} \otimes a_{i,b} + a_{j,b}),
\end{equation}
where $\odot$ represents matrix-matrix multiplication and $\otimes$ represents matrix-vector multiplication.
The operator is applied to the initial elements $e_k$, which are given by the pairs of matrices
\begin{equation}
\label{eq:sequence_scan}
e_k = (e_{k,a}, e_{k,b}) := (\bar{A}, \bar{B} u_k).
\end{equation}
Using this definition, the sequence of hidden states $x_{1:L}$ is computed iteratively as 
\begin{equation}
\label{eq:full_scan}
\begin{aligned}
e_1 &= (\bar{A}, \bar{B} u_1) = (\bar{A}, x_1) \\
e_1 \bullet e_2 &= (\bar{A}^2, \bar{A} x_1 + \bar{B} u_2) = (\bar{A}^2, x_2) \\
e_1 \bullet e_2 \bullet e_3 &= (\bar{A}^3, \bar{A} x_2 + \bar{B} u_3) = (\bar{A}^3, x_3).
\end{aligned}
\end{equation}
In general, matrix-matrix multiplications with a matrix $\bar{A}$ would incur a time complexity of $\mathcal{O}(P^3)$. However, by diagonalizing the systemâ€™s matrices, the complexity is reduced to $\mathcal{O}(P L)$, where $P$ is the size of the matrix and $L$ is the length of the sequence. As a result, the parallel scan operation is computed in $\mathcal{O}(\log(P L))$, making it scalable to long sequences.
