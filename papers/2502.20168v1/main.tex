\documentclass[conference]{IEEEtran}
\usepackage{times}

\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage[bookmarks=true]{hyperref}
\usepackage{siunitx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\let\labelindent\rela
\usepackage{enumitem}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.18}
\usepackage{xcolor}
\definecolor{tabblue}{rgb}{0.121568627, 0.466666667, 0.705882353}
\definecolor{taborange}{rgb}{1.0, 0.498039216, 0.0}
\usepackage{multirow}

\newcommand{\davide}[1]{{\color{red} Davide: #1}}
\newcommand{\maria}[1]{{\color{blue} Maria: #1}}

\pdfinfo{
   /Author (anonymous authors)
   /Title  (S5WM)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (MBRL, SSM)
}

\begin{document}

\title{Accelerating Model-Based Reinforcement Learning with State-Space World Models}


\author{\authorblockN{Maria Krinner\authorrefmark{1}, Elie Aljalbout\authorrefmark{1}, Angel Romero, Davide Scaramuzza} 
\authorblockA{
Robotics and Perception Group, University of Zurich, Switzerland\\
\authorrefmark{1}These authors contributed equally\vspace{15pt}
}
}







\maketitle

\begin{abstract}
Reinforcement learning~(RL) is a powerful approach for robot learning.
However, model-free RL~(MFRL) requires a large number of environment interactions to learn successful control policies.
This is due to the noisy RL training updates and the complexity of robotic systems, which typically involve highly non-linear dynamics and noisy sensor signals.
In contrast, model-based RL (MBRL) not only trains a policy but simultaneously learns a world model that captures the environment's dynamics and rewards.
The world model can either be used for planning, for data collection, or to provide first-order policy gradients for training.
Leveraging a world model significantly improves sample efficiency compared to model-free RL. However, training a world model alongside the policy increases the computational complexity, leading to longer training times that are often intractable for complex real-world scenarios.
In this work, we propose a new method for accelerating model-based RL using state-space world models.
Our approach leverages state-space models (SSMs) to parallelize the training of the dynamics model, which is typically the main computational bottleneck.
Additionally, we propose an architecture that provides privileged information to the world model during training, which is particularly relevant for partially observable environments.
We evaluate our method in several real-world agile quadrotor flight tasks, involving complex dynamics, for both fully and partially observable environments. 
We demonstrate a significant speedup, reducing the world model training time by up to 10 times, and the overall MBRL training time by up to 4 times.
This benefit comes without compromising performance, as our method achieves similar sample efficiency and task rewards to state-of-the-art MBRL methods.

\end{abstract}

\IEEEpeerreviewmaketitle

\newcommand{\Dist}[4]{#1_{#2}\left(#3 \,\middle|\, #4\right)}
\newcommand{\KL}[2]{D_{\text{KL}}\left[#1 \,\middle\|\, #2\right]}
\newcommand{\sg}{\ensuremath{\operatorname{sg}}}

\input{sections/intro}
\input{sections/related}
\input{sections/preliminaries}
\input{sections/method}
\input{sections/experiments}
\input{sections/limitations}
\input{sections/conclusion}



\bibliographystyle{unsrtnat}
\bibliography{references}
\clearpage
\newpage
\appendix
\input{sections/appendix}

\end{document}
