\method{} is organized as an iteration. As shown in Fig.\ref{fig:overall}, we divide the whole procedure into following three stages: 1) Initialization \& Generation 2) Online Signal Collection 3) LLM Post Training for query rewrite.

In each iteration: rewrites are firstly generated based on open-source / commercial LLMs or post-trained LLMs in previous iteration. The generated rewrites are then de-duplicated with online rewrites and deployed online, which served to retrieve items for the corresponding query. The clicked / purchased items indicate positive feedback from user, leading to labeling positive for the rewrites. These positive rewrites are collected, together with auxiliary tasks such as Quality, Relevance for post-training.

\subsection{Initialization \& Generation}\label{sec:rewrite-generation}
This stage generates rewrites given queries by LLMs, including open-source / commercial LLMs in the very beginning iteration and post-trained LLMs in subsequent iterations.

Prior to generation, we separate queries into three categories based on their frequency proportion: high frequency, mid frequency and tail. 
High frequency queries includes most frequent search queries in our system, such as `beef noodles', `snack'. These queries represent a majority of user preferences and contribute to the largest Gross Merchandise Volume (GMV). Besides, the number of high frequency queries is limited.
Mid frequency queries normally comprise specific search intention, such as ``Luosifen'', ``Xijiade''\footnote{Xijiade Dumplings is a well-known Chinese restaurant chain specializing in dumplings and traditional northeastern Chinese cuisine}. Its volume is lower while the number increases compared to the high frequency counterpart.
Tail queries are paid less attention before. Basically it includes typo of user, synonyms, particular local food / restaurant, equivocal search intention, natural language and etc., such as ``KFC'', ``Fat Uncle'', ``Extra Spicy'', ``What is suitable to eat when having a cold?''. 
It is difficult to infer user's intention based on tail queries. 

We further set up 5 rewrite directions for comprehensive rewrites: 1) Key Word Extraction 2) Correction 3) Alias \& Synonyms 4) Main Dish 5) Low Relevance. The exact definition, positive/negative examples are listed in App.\ref{app:rewrite-direction}.

During rewrite generation by LLMs, prompt is designed as followed: 
We set different prompts for queries in different categories, with explanation of the category and rewrite emphasis. CoT and RAG are also employed in this process.

\subsubsection{Chain-of-Thoughs (CoT)}
CoT is utilized to formulate task of query rewrite to comprehensive query understanding and process.
We instruct LLM to first determine the meaning of the search query: Whether it is unambiguous. If not, the query should be corrected or rephrased.
Then, LLM is required to determine the search intent: whether user tend to match cuisine names or restaurant names under the query.
Finally, LLM generates rewrites under the information inferred by itself.

\subsubsection{Retrieval-Augmented Generation (RAG)}
Generation process is further enhanced by incorporating relevant information. Specially, for each query, the associate restaurants and cuisine with interaction are added to the prompt.

Finally we let LLMs to choose suitable rewrite directions and conduct final rewrite. 
Prompt for query generation and examples is listed in Appendix \ref{app:rewrite-prompts}. 

\subsection{Online Signal Collection}\label{sec:signal-collection}
Most previous rewrite methods find it difficult to determine whether a rewrite is ``good'' or not automatically. Inspection method includes manual / GPT labeling for selected rewrites, bringing high-cost manpower efforts and indirect connection to final results.
In \method{}, we proposed to use real-world serving system to determine the quality of rewrites: the rewrites contributes to click / purchase action from users are collected and regarded as ``good'' rewrites. 

Specially, during serving in our online system, input query is rewritten to multiple rewrites. Each rewrite is used to retrieve candidate items for ranking and expose. 
Besides rewrite retrieval, the system comprises other retrieval methods, including origin query retrieval, embedding retrieval, user-to-item (U2I) retrieval and etc., whose retrieved items are mixed and de-duplicated. 
% We specially mark the item: 1) Retrieved by rewrites (retrieval by other methods as well) 2) Only retrieved by rewrites. For the marked 
We specially mark the items that are retrieval (only) by rewrite retrieval. 
During expose, the clicked item that are only retrieved by rewrites is regarded as ``Level-1'' positive signal to the rewrite, meaning that the rewrite provides unique contribution to the click action. For the clicked item retrieved by multiple method include the rewrite, we regard it as ``Level-2'' positive signal, where the rewrite satisfies the search intention, however without increment to the search results.
``Level-1/2'' rewrites are collected and utilized as positive label for latter process. 

\subsection{Multi-Task Post Training for Query Rewrite}\label{sec:llm-post-training}
Public LLM are trained for general usage including reasoning, conversion and etc. \method{} aims at training a specific LLM for query rewrite, based on the understanding of search query and matching of candidate items in our online system. 
Specially, the rewrite is supposed to 1) precisely represent user's intention in Meituan, 2) reason to specific item for query in natural language format, 3) relevance between query and 4) capture inherent user's interest.

To achieve these goals, three objectives and corresponding loss are established during stage of post training: 1) Rewrite Generation 2) Rewrite Quality 3) Relevance. 
% Specially, given query and corresponding positive rewrites from Sec.\ref{sec:signal-collection}, we firstly inference query's intention using our query understanding module, relevance between query and rewrites using relevance module \footnote{GPT-4 is actually used in experiments}. 
% Then we construct a prompt based on Sec.\ref{sec:rewite-generation} with similar CoT, leaving positive rewrites and their corresponding intention, relevance score masked for learning.
\subsubsection{Rewrite Generation}\label{sec:llm-post-training:rewrite-generation}
Rewrite generation serves as the main task in post training. LLM is trained to produce rewrites to match user's intention and interest.
With the collected rewrites with positive signal, instead of training LLM simply to generate them, we further rephrase the generation task using CoT by dividing it into several subtasks: 
Given the query in training samples, We first instruct LLM to tell whether the query is typo, i.e. wrongly written words. 
Then the LLM distinguishes the intention of query from choices of cuisine, restaurant and neither.
Finally, LLM is required to generate rewrites.
The prompt is arranged by concatenation of the three above mentioned subtasks. 
User input is consist of query and its corresponding names of most frequently clicked cuisine and restaurant.
Assistant output comprises the label of the subtasks: 
We use other LLMs (such as GPT-4o) to tell whether the query is typo or not in advance. 
Intention of query is inferred with our query understanding module. 
The rewrites collected in Sec.\ref{sec:signal-collection} are utilized. 
The training instruction shares the same with prompt in Sec.\ref{sec:rewrite-generation}. Besides, the content of assistant output is replaced by the positive rewrites.

\subsubsection{Rewrite Quality}\label{sec:rewrite-quality}
With the generated rewrites by LLM, we need to distinguish whether the rewrite are suitable for the system or not. Though rewrites with positive signals are trained in Sec.\ref{sec:llm-post-training:rewrite-generation}, rewrites without signals are unseen for LLM, leading to repeated occurrence in LLM rewrite generation. 
To endow LLM with the ability to generate ``good'' rewrites instead of ``bad'', we establish a classification task for LLM: During post-training, LLM is instructed to produce ``good'' or ``bad'' labels for a input pair of query and corresponding multiple rewrites. 

Similar with prompt in Sec.\ref{sec:llm-post-training:rewrite-generation}, we instruct LLM of the criterion of quality in instruction. 
Then user input is consist of query, cuisine and restaurant that are most interacted and rewrites generated from previous iteration. 
Assistant output comprises the labels, which are generated based on the online signal in Sec.\ref{sec:signal-collection} and auxiliary labeling using other LLMs (such as GPT-4o): the rewrites with positive signal is regarded automatically as ``good''. The label of remaining rewrites are inferred with other LLM. 
Training sample is listed in Appendix \ref{app:post-training:rewrite-quality}.

\subsubsection{Relevance}
Relevance between query and search results is a main constraint in search system. 
To fulfill the restriction, the generated rewrite is supposed to match high relevance with query. 

We instruct LLM to be aware of the relevance between query and rewrite. 
Specially, the instruction is a detailed description of relevance inference procedure. 
User input is composed of query, cuisine and restaurant under the query. 
Assistant output is the label of relevance including high/low/non relevance.
Similar with Sec.\ref{sec:rewrite-quality}, we use our relevance measuring module for labeling query and rewrites into the three levels. 
Besides, a auxiliary LLM is utilized to perform labeling. We choose the training samples with agreement.
Training sample is listed in Appendix \ref{app:post-training:relevance}.

As shown in Alg.\ref{alg:overall}: After post training of LLM, we use the trained LLM for generation using prompt in Sec.\ref{sec:rewrite-generation}, whose output rewrites are fed online for signal collection.
In each iteration, we add more queries and modify prompts based on online performance.
After iterations of collection and post-training, the generated rewrites tend to fulfill the objectives and lead to better online performance. 
\begin{algorithm}
\caption{\method{}: An Iterative Procedure for Rewrite Generation}\label{alg:overall}
\begin{algorithmic}[1]
% \Require $n \geq 0$
% \Ensure $y = x^n$
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
\State $\text{ite}=0$
\While{True}
% \If{$N$ is even}
%     \State $X \gets X \times X$
%     \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
% \ElsIf{$N$ is odd}
%     \State $y \gets y \times X$
%     \State $N \gets N - 1$
% \EndIf
\State Generate query rewrite using public LLMs using prompt in Sec.\ref{sec:rewrite-generation}.
\If{$\text{ite}>0$}
    Generate query rewrite using post-trained LLM using prompt in Sec.\ref{sec:rewrite-generation}.
\EndIf
\State Rewrites de-duplicated and deployed.
\State Collect positive rewrites.
\State LLM post-training with multi-tasks in Sec.\ref{sec:llm-post-training}
\State $\text{ite}+=1$
\EndWhile
\State Collected positive rewrites are deployed.
\end{algorithmic}
% \caption{Algorithm description of \method{}.}
\end{algorithm}