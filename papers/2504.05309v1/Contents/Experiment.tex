Experiments of \method{} mainly answer the following questions:
\begin{itemize}
    \item Whether rewrites generated by \method{} contributes to online improvement in our real-world service, i.e. search in Meituan delivery.
    \item Whether \method{} generates better rewrites than public LLMs in the domain of Meituan delivery, especially in searching of cuisine and restaurant.  
    \item Whether the iterative framework produces better rewrites automatically. 
\end{itemize}
To address first concerns, we first conduct online A/B test to verify the effectiveness of \method{} in real-world scenario. 
Specially, in retrieval phrase of Meituan Delivery's search, \method{} is added as another channel for retrieval. The retrieved items of \method{}, together with items from other channels are truncated, de-duplicated and merged for phrase of rank.
Finally, the ranked items are exposed to user. 
Online performance for search system with and without \method{} are recorded and compared as A/B test.

% Besides, for further verifying the performance of rewrites before deploy and providing guidance of the rewrite refinement, we design and establish offline evaluation, including 3 sets of experiments.

The second question serves as an pre-deployment validation for the effectiveness of \method{}, providing guidance for the development of rewrite generation.
It is difficult to directly measure the online metric before deployment, offline but related metrics are established.
It is subjective to judge the quality of generated content \cite{guo2023close}.
However, rewrites contain meaningful representation and serves in a specific task: reformulate user input to similar query for available and satisfied candidates matching.
Given an input, it is possible to enumerate suitable rewrites as ground-truth, measuring the relevance between generated rewrites and input to ensure consistent of search intention and estimate the efficiency of rewrites by simulation of online serving process.
Based on these assumption, 3 sets of offline experiments are set to evaluate the performance of rewrites.

Specially, a benchmark containing queries and multiple corresponding rewrites as ground-truth is prepared. 
During offline experiments, after rewrites are generated, the coverage rate of generated rewrites by different methods w.r.t ground-truth is calculated as the ``precision'' of methods.
This metric measures whether the method is able to produce ``good'' rewrites that are labeled manually by common sense.
Furthermore, A relevance score between generated rewrites and queries is calculated by our relevance module, which serves as the metric for measuring the consistence of search intention during rewrite process.
This metric is named as ``Relevance''.

Another benchmark is established to simulate the online procedure.
Specially, given a user query, corresponding restaurants and cuisines (with interaction labels) serve as candidates for retrieval. Embedding-based retrieval is used: query and title of candidates are embedded, the inner product between query and candidates is calculated as score for retrieval. A higher score represents higher probability for exposure. 
A good query is supposed to generate higher score for restaurants and cuisines with interaction labels.
During rewrite experiments, we replace origin query with rewrites generated from different methods for embedding and retrieval.
Though real-world search system comprises sophisticated modules, the offline simulation is able to roughly measure the efficiency performance of rewrites: whether the generated rewrite retrieve more interacted items than original query. 
This experiment measure the ``Efficiency'' of rewrites in an offline environment by simulating the real-world procedure of rewrite retrieval.

The third concern can be partial answered by the experiments above mentioned. 
Besides, we focus more on the rewrites generated by post trained LLM during each iteration: whether \method{} continuously motivates new rewrites.
Compared with the LLM based query rewrite methods \cite{peng2024large}, \cite{wang2024one} where one-step rewrite generation is conducted, \method{} employs iterative generation of rewrites. 
The proportion and performance newly generated is measured.

\subsection{Online Experiments}
Specially, a new recall channel is set up for the rewrites generated after iterations of rewrite generation.
For a user query, 10 arbitrary corresponding rewrites are sampled to retrieve items from candidate sets\footnote{Normally, the candidate sets for recall are the available restaurants and cuisine according to user's current location.}. 
These retrieved items, together with the counterparts in query text/embedding recall,  user historical behavior recall and etc, are de-duplicated for ranking and exposure.
Finally, user's purchase information is reported for comparison. Specially,  
we select order volume, conversion rate from exposure to order per expose (PV\_CXR) / query (QV\_CXR) / user (UV\_CXR). 
% (UV\_CXR: Conversion rate from exposure to purchase per user; UV\_RPM: Revenue Per Mille for every exposed user) are utilized to measure the performance of the overall system. 
We set a search system without \method{} as baseline and system with \method{} as target. Performance metrics 
(Order Volume, PV\_CXR, QV\_CXR, UV\_CXR) 
of target above baseline is reported as the improvement brought by \method{}.
Besides, we set multiple baseline systems, whose differences are recorded as fluctuation.

\method{} is deployed online for 7 days. As shown in Table \ref{tab:abtest}: during experiment, consistent improvement is observed for \method{}.   

\begin{table}[!htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c}
        \hline \hline
        Experiments & Order Volume(\%) & PV\_CXR(\%) & QV\_CXR(\%) & UV\_CXR(\%) \\ \hline
        AA & 0.04 & 0.02 & 0.09 & 0.03 \\  \hline
        AB(\method{}) & 0.27 & 0.34 & 0.27 & 0.21 \\  \hline  \hline
    \end{tabular}
    }
    \caption{Online experiment of \method{}.}
    \label{tab:abtest}
\end{table}

\subsection{Offline Experiments}\label{sec:offline-experiments}
Online experiment evaluates the overall performance of \method{}. 
% We dissect the working procedure of rewrites
To examine the performance of rewrites in detail and foresee online performance before deployment, % 3 offline experiments are established.
we establish $3$ offline experiments and $2$ corresponding benchmarks.

Specially, benchmark I contains $3597$ queries and average $5.19$ positive rewrites for each query. 
The positive rewrites serve as ground truth rewrites that are supposed to be generated, which is generated by GPT-4o and examined manually for correctness.
These rewrites never appear in generation or post-training phase to avoid knowledge leakage.

Benchmark II contains $10000$ queries and corresponding recall candidate set with average volume of $500$, the candidate set comprises the name of restaurants within the location constraint for the request. Statistics of the benchmarks is listed in Table \ref{tab:benchmark}.

\begin{table}[!htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \hline \hline
        Benchmark ID & $\#$Query & Label Content & $\#$Candidates & $\#$Label \\ \hline
        I & $3597$ & Positive rewrites & - & $5.19$ \\ \hline
        II & $10000$ & Clicked items & $500$ & 2.3 \\ \hline \hline
    \end{tabular}}
    \caption{Benchmark specification and statistics. $\#$ represents the number of selected content.}
    \label{tab:benchmark}
\end{table}
For evaluation methods, besides \method{}, we use GPT-4o, DeepSeek-v2 as close source baselines; Qwen2.5 as open source baseline.
\method{} is built on Qwen2.5, with CoT \& RAG in generation and multi-task post-training in an iterative training framework.
Ablation study is also conducted on these components.

\begin{table}[!htbp]
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{r|c|c|c|c|c}
    \hline \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{Precision} & \multirow{2}{*}{Relevance} & \multicolumn{3}{c}{Efficiency} \\ \cline{4-6}
        &   &    & Top1 & Top5 & Top10 \\ \hline
    GPT-4o & 0.2131 & 0.1901 & 0.0033 & 0.0231 & 0.0534 \\ \hline
    DeepSeek-v2 & 0.2245 & 0.2173 & 0.0043 & 0.0247 & 0.0567 \\ \hline
    Qwen2.5 & 0.1589 & 0.1280 & 0.0034 & 0.0219 & 0.0522 \\ \hline
    Qwen2.5 + PT(RG) & 0.3649 & 0.4503 & 0.0067 & 0.0458 & 0.0971 \\ \hline
    Qwen2.5 + PT(RG) + RAG & 0.4752 & 0.4149 & 0.0057 & 0.0413 & 0.0943 \\ \hline
    % Qwen2.5 + PT + RAG + CoT & & & & \\ \hline
    % Qwen2.5 + PT + RAG + CoT + Iter (\method{}) & & & & \\ \hline
    % \method{} w.o. RAG & 0.4752 & & & \\ \hline
    \method{} w.o. Iter\&PT(Rel) & 0.4807 & 0.4110 & 0.0063 & 0.0442 & 0.0952\\ \hline
    \method{} w.o. Iter & 0.4833 & \textbf{0.4149} & \textbf{0.0086} & \textbf{0.0465} & 0.0955\\ \hline
    \method{} & \textbf{0.5040} & 0.3362 & 0.0058 & 0.0364 & \textbf{0.1017} \\ \hline \hline
    \end{tabular}
}
\caption{Overall offline experiment results of \method{} compared with baselines. PT stands for Multi-task post training; PT(RG) means post training with only Rewrite Generation. PT(Rel) represents post training with only Relevance.}
\label{tab:offline}
\end{table}

\subsubsection{Precision Generation of Rewrites}
For each query, we evaluate the precision of generated rewrites: for each query in the benchmark I, we generate rewrites using different methods. 
% During generation, we use the same prompt for all methods for fairness.
% Then the coverage of generated rewrite with respect to the positive rewrites are calculated:
Then we calculate the coverage portion of positive rewrites:
A good rewrite LLM is able to generate all positive rewrites, leading to higher coverage. 

As shown in Table \ref{tab:offline}, open/close source LLMs with general usage demonstrate apparent degeneration than LLMs post trained with domain data. 
A single rewrite generation training task on Qwen2.5 improves by large margin (0.3649 v.s. 0.1589), showing that domain knowledge contributes significantly. 
Besides, RAG  assistants a post-trained LLMs to better performance. 
By adding multi-task post training and CoT/RAG in generation, \method{} without iteration leads to higher precision.
Finally, iterative framework further boost performance to 0.5040, meaning that half of the reserved rewrites can be generated.

\subsubsection{Relevance of Rewrites}\label{sec:exp:relevance}
In this experiment, relevance of rewrites with respect to query is evaluated. 
%  all generated rewrites' relevance with respect to query are inferenced using our own relevance module. 
We use our own relevance module to inference the relevance scores.
The module outputs prediction for a pair of query and rewrite: high-relevance, low-relevance and non-relevance. The portion high is reported as the capacity of LLM to generate relevant rewrites.

As shown in Table \ref{tab:offline}, general LLMs demonstrate poor performance before post training, showing that though public LLMs perform well in conversation, reasoning, relevance knowledge in e-commercial domain is lacking.
Besides, post-training in rewrite generation leads to significant improvement in relevance.
It is found that an ablation experiment on \method{} without Relevance demonstrates degradation in both precision and relevance metric.
However, iterative training on Relevance does not leads to improvement in relevance metric. We conjecture that Relevance is over fitted during multiple training.

\subsubsection{Efficiency of Rewrites}\label{sec:exp:efficiency}
Finally, we use the benchmark II to simulate the online procedure and estimate the efficiency of rewrites.
Similarly, we first generate rewrites for all methods. 
Then the rewrites, together with the candidate items are embedded to numerical representation. 
We use inner product between rewrites and items for scoring to simulate the online recall and ranking process.
The items with user's click are regarded as positive samples, whose scores are supposed to be higher than other samples. 
We use recall@$K$ to measure the performance of the offline simulation: for the $K$ items with highest score, the number of positive samples within is recorded as numerator while the number of positive samples is recorded as denominator.

As shown in Table \ref{tab:offline}, general LLMs show inferior performance in retrieval efficiency, compared with post trained LLMs.
Post training with Rewrite Generation (Qwen2.5+PT(RG)) contributes significantly to improvement. Some auxiliary tasks such as Relevance and RAG may lead to degradation.
Finally, all multi tasks and prompt techniques integrated in \method{} reaches to best performance in all metrics of Efficiency.

\subsection{New Rewrites in Iterations}
During the procedure of \method{}, the initial set of rewrites is generated by public LLMs (GPT-4o, DeepSeek-v2).
Starting from the first iteration, the post trained LLM in \method{} generates rewrites.
Besides, the previous rewrites is filtered by Stage 2 (Online Signal Collection) in last iteration, leaving only the positive rewrite in the current iteration.
These rewrites are de-duplicated for Stage 2. 
To examine the performance of newly generated rewrites in post trained LLM, we only use the unique rewrites for experiments:
The portion of these rewrites is recorded, demonstrating the contribution of post trained LLM in motivating rewrites.
Furthermore, similar with the experiments in Sec.\ref{sec:offline-experiments}, we compare the offline performance of the unique rewrite.
Considering only the unique rewrites is compared, most rewrites are covered by the common parts, experiment of Precision is omitted.
During the experiment, we use three times of iterations in \method{}.

\begin{table}[!htbp]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c}
     \hline \hline
    \multirow{2}{*}{Iteration Index} & \multirow{2}{*}{Relevance} & \multicolumn{3}{c}{Efficiency} \\ \cline{3-5}
        &        & Top1  & Top5    & Top10 \\ \hline
    1   & 0.3466 & 0.0034 & 0.0412 & 0.0767 \\ \hline
    2   & 0.4021 & 0.0045 & 0.0308 & 0.0923 \\ \hline
    3   & 0.3677 & 0.0054 & 0.0428 & 0.0917 \\ 
    \hline \hline
    \end{tabular}}
    \caption{Offline experiments (Relevance \& Performance) of unique rewrites in three iterations.}
    \label{tab:exp:unique-new-rewrite}
\end{table}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{Figures/unique-rewrite-portion.pdf}
    \caption{Portion of unique rewrites generated by \method{} during iterations}
    \label{fig:unique-rewrite-portion}
\end{figure}

\subsubsection{Portion of New Rewrites}
During iteration, the rewrites generated by post trained LLM in Stage 1, together with the filtered rewrites in Stage 2 (previous iteration) are de-duplicated and fed for online signal collection.
The unique rewrites provided by post trained LLM represents the contributed rewrites of \method{} for the iteration. 

The portion of unique rewrites during the three iterations is demonstrates in Fig.\ref{fig:unique-rewrite-portion}. 
In iteration 0, rewrites are generated by public LLMs. New rewrites motivate in iteration 1, where new rewrites occupy 0.852\% over the all rewrites used in this iteration.
Though it is of minor occupation, it actually contributes new information for the search system. 

As iteration progresses, portion of new rewrites keep decreasing, showing that the overall available rewrites for the search system achieve stability in iterations.

\subsubsection{Performance of New Rewrites}
We further evaluate the offline performance for the new rewrites during iterations.
Specially, after rewrite generations of post trained LLM, the rewrites are de-duplicated with the positive rewrites. 
The remaining rewrites' Relevance and Efficiency are evaluated similarly in Sec.\ref{sec:exp:relevance} and Sec.\ref{sec:exp:efficiency}.

As shown in Table \ref{tab:exp:unique-new-rewrite}, during iteration progresses, metrics of relevance and efficiency maintain similar results. 
It demonstrates that the performance of new rewrites does not change dramatically. The performance of post trained LLM is stable during training in multiple iterations.
The data also resemble corresponding results in Table \ref{tab:offline}. It shows that the new rewrites' performance is similar with the positive rewrites.