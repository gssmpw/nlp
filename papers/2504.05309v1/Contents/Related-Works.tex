Query rewrite, or query expansion has long been a crucial technique to enhance the efficiency and accuracy of information retrieval and database querying. It serves as an important component for search in both general Web search (Google, Baidu) and e-Commercial search (Taobao, Meituan Delivery). 
The methods for query rewrite can be broadly categorized into two phases: pre-LLM methods and LLM based methods.

Before the advent of LLMs, query rewrite techniques primarily relied on rule-based systems and traditional machine learning approaches. These methods often involved manually crafted rules or heuristics to transform user queries into more effective or efficient forms. 
One of the early approaches to query rewrite involved using thesauri and ontologies to expand queries with synonyms and related terms. \cite{voorhees1994query} explored the use of WordNet, a lexical database, to automatically expand user queries with semantically related terms.
Another significant pre-LLM approach was the application of statistical machine translation (SMT) models to query rewriting. \cite{berger2017information} proposed using SMT techniques to translate user queries into a language model that better represents the document space. This method treated query rewriting as a translation problem, leveraging co-occurrence statistics from large corpora to infer possible rewrites that could enhance retrieval accuracy.
\cite{jones2006generating} investigated the use of query logs to identify common reformulations that led to successful search outcomes.
\cite{cui2002probabilistic} proposed a method that utilized query expansion through the use of frequently co-occurring terms in large text corpora to improve search engine performance. Similarly, \cite{baeza2004query} explored query transformations using statistical translation models to enhance information retrieval systems. 
These approaches, while effective to some extent, often required significant manual effort to maintain and update the rule sets and were limited by their inability to generalize well across diverse query contexts.

With the introduction of LLMs, such as BERT\cite{devlin2019bert} and GPT\cite{radford2018gpt1}, query rewrite methods have seen significant advancements. These models leverage deep learning and vast amounts of training data to automatically learn complex patterns and relationships in language, enabling more sophisticated and context-aware query transformations. 
For example, \cite{nogueira2019passage} demonstrated the use of BERT for query reformulation in search engines, showing substantial improvements in retrieval performance. 
\cite{peng2024large} pioneer to deploy LLM-based query rewrite in e-Commercial search system by training a LLM for query rewrite with multi-instruction supervised fine tuning, offline feedback, and objective alignment.
\cite{wang2024one} models the query rewrite as an end-to-end keyword generation task based on LLM by multi-match prompt tuning and prefix tree-based constrained beam search in generation. Then it employs feedback tuning for LLM post training.
These LLM-based methods heavily reply on prompt tuning and post-training of LLMs in a one-time generation, without consideration of rewrites update.