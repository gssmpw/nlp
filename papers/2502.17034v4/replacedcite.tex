\section{Related Work}
The development of autonomous systems for tool design, fabrication, and utilization hinges on advancements in three areas: Vision-Language Models (VLMs) for environmental interpretation, Vision-Language Action (VLA) models for robotic manipulation, and Text-to-3D models for tool generation. VLMs enable robots to process multimodal inputs—images, videos, and natural language—to make informed decisions. Recent improvements in VLMs, such as BLIP-2____, Flamingo____, Kosmos-2____, Molmo and pixmo____, have significantly enhanced visual and textual data interpretation by leveraging transformer-based architectures. Notably, Qwen2-VL____ excels in generalization and efficient multimodal learning, making it ideal for real-time robotic reasoning.

Concurrently, VLA models have enabled robots to interpret natural language instructions alongside visual data to perform complex manipulation tasks____,____. These models generate precise 7D action outputs—translational and rotational displacements and gripper actuation—using transformer architectures. Pioneering work like PaLM-E____ integrated sensory data with natural language for long-horizon robotic tasks, while the RT-series models (RT-1____, RT-2____, RT-X____) leveraged large-scale datasets to learn versatile, task-conditioned policies. RT-2, in particular, builds on RT-1 by incorporating vision-language representations to enable cross-domain generalization without retraining. Efforts like OpenVLA____ and MiniVLA____ have further enhanced model accessibility and real-time performance, though challenges remain in real-time adaptation and robustness.

Text-to-3D models ____, ____, ____, ____ are key for converting language descriptions into tangible 3D geometries, enabling robots to autonomously create task-specific tools. Early methods like Text2Shape____ used deep generative models for this purpose, while subsequent approaches such as ShapeCrafter____ improved controllability and fidelity. Recent innovations include the Industry 6.0 framework, which employed GPT-4o____ to generate Signed Distance Functions (SDFs)____ for additive manufacturing. However, high computational demands have led to more efficient models like LLaMa-Mesh____ and MeshGPT____, with our work adopting LLaMa-Mesh for its efficiency and adaptability.

The convergence of VLMs, VLA models, and Text-to-3D technologies offers a transformative opportunity for creating autonomous systems capable of tool design and utilization. Integrating VLMs' environmental reasoning with VLA's action generation and Text-to-3D's tool creation enables robots to dynamically generate and employ task-specific tools. This approach enhances flexibility in diverse environments and facilitates real-time execution, reducing reliance on predefined tools and human intervention. However, seamless integration requires addressing challenges in data consistency, real-time processing, and structural integrity. Our work builds upon these advancements to establish a comprehensive framework for autonomous tool design and usage, positioning Evolution 6.0 as a novel application that merges these capabilities into versatile robotic systems capable of dynamic tool generation and deployment.

% The development of autonomous systems for tool design, fabrication, and utilization hinges on advancements in three main areas: Vision-Language Models (VLMs) for environmental interpretation, Vision-Language Action (VLA) models for robotic manipulation, and Text-to-3D models for tool generation. VLMs are crucial for enabling robots to process multimodal inputs—images, videos, and natural language descriptions—to make informed decisions. Recent improvements in VLMs, such as BLIP-2____, Flamingo____, and Kosmos-2____, Molmo and pixmo ____, have significantly enhanced the interpretation of visual and textual data. These models utilize transformer-based architectures to merge visual and textual information, allowing context-aware decision-making in complex scenarios. Notably, Qwen2-VL____ stands out as an advanced open-source model that excels in generalization and efficient multimodal learning, making it well-suited for robotic applications requiring real-time reasoning.
% Concurrently, VLA models have emerged as effective tools that enable robots to interpret natural language instructions alongside visual data to perform complex manipulation tasks ____. These models also leverage transformer architectures to produce precise 7D action outputs, including translational and rotational displacements and gripper actuation. Pioneering work like PaLM-E____ introduced a multimodal language model capable of reasoning over long-horizon robotic tasks by integrating sensory data with natural language input. The RT-series models—RT-1____, RT-2____, and RT-X____—further advanced this field by utilizing large-scale robotic datasets to learn task-conditioned policies applicable across diverse environments. RT-2 notably enhances RT-1 by incorporating vision-language representations that facilitate cross-domain generalization without explicit retraining. Recent efforts to improve the accessibility of VLA models have resulted in frameworks like OpenVLA____, which allows fine-tuning of pre-trained models across various robotic platforms using consumer-grade hardware. Additionally, MiniVLA____ proposed a lightweight architecture optimized for real-time inference, making VLA models more suitable for mobile applications. Despite these advancements, challenges persist regarding real-time adaptation to novel tasks and robustness in unstructured environments.
% Text-to-3D models ____, ____, ____, ____, ____ are essential for bridging abstract language descriptions with tangible 3D geometries, enabling robots to autonomously create task-specific tools based on user instructions or environmental needs. Early methods like Text2Shape____ translated textual descriptions into corresponding 3D representations using deep generative models. Subsequent developments such as ShapeCrafter____ improved the controllability and fidelity of generated structures. Recent innovations have demonstrated enhanced accuracy and generalization capabilities; for instance, the Industry 6.0 framework utilized GPT-4o____ to generate Signed Distance Functions (SDFs) ____ representing complex geometries for additive manufacturing processes. However, high computational demands have spurred the development of more efficient alternatives like LLaMa-Mesh____ and MeshGPT____, which feature specialized architectures for 3D object generation. We adopt LLaMa-Mesh due to its computational efficiency and adaptability across various robotic applications.
% The convergence of VLMs, VLA models, and Text-to-3D technologies presents a transformative opportunity for creating autonomous systems capable of tool design and utilization. Integrating VLMs' environmental reasoning with VLA's action generation and Text-to-3D's tool creation allows robots to dynamically generate and employ task-specific tools. This integration fosters adaptive tool creation based on high-level task requirements while enhancing flexibility in diverse environments. It also facilitates real-time task execution, minimizing reliance on predefined tools and human intervention. However, achieving seamless integration requires overcoming challenges related to data consistency, real-time processing, and structural integrity of tools. Our work builds upon advancements in these areas to establish a comprehensive framework for autonomous tool design and usage, empowering robots to perform complex tasks with minimal human oversight. Evolution 6.0 introduces a novel application by merging these capabilities into versatile robotic systems that can autonomously generate and utilize tools in dynamic settings.
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/picture.png}
    (a)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (b)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (c)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (d)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (e)
    \caption{Evaluation tasks. (a) Visual generalization: presence of several unseen distractor objects. (b) Visual generalization: unseen background. (c) Motion generalization: the plate is elevated from its original position. (d) Physical generalization: the plate has a different size than the one used in the dataset. (e) All types of generalization: the cake is initialized in unseen position and orientation, the background is changed, distractor object presents in the scene.}
  
  
  \label{fig:scenarios}
\end{figure*}