%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper


\usepackage{graphicx}

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Evolution 6.0: Evolving Robotic Capabilities Through Generative Design
}


\author{Muhammad Haris Khan, Artyom Myshlyaev, Artem Lykov,  Miguel Altamirano Cabrera, \\ and Dzmitry Tsetserukou% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{The authors are with the Intelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology. 
 {\tt\small\{haris.khan, Artyom.Myshlyaev, Artem.Lykov, m.altamirano, d.tsetserukou\}@skoltech.ru}
}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI. When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal. Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution. The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions. It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation. Evaluation results demonstrate a 90\% success rate for tool generation with a 10-second inference time and action generation achieving 83.5\% in physical and visual generalization, 70\% in motion generalization, and 37\% in semantic generalization. Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability.

\end{abstract}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/Evolution.png}
  \caption{The Evolution 6.0 system workflow, which autonomously analyzes a scene, generates a task-specific tool, and executes actions. The process includes scene interpretation, tool generation via an autoregressive model, rendering, 3D printing, and action execution using a 7D action vector derived from task instructions via VLA.}
  \label{fig:overview}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The rapid development of generative AI and its associated tools has unlocked extraordinary opportunities for automation in robotics ~\cite{xu2024surveyroboticsfoundationmodels}. Visual Language Action (VLA) models enable cognitive robots to perform complex manipulation tasks in dynamic environments by leveraging task instructions and visual information to generate control signals for manipulator movements, bridging perception and action. Simultaneously, Industry 6.0 ~\cite{lykov2024industry} envisions a heterogeneous swarm of robots autonomously designing, manufacturing, delivering, and assembling products based on user specifications. This paradigm eliminates human involvement across all stages of production, empowering cognitive robots to utilize and fabricate tools autonomously. However, Industry 6.0 lacks the ability to assess its environment and autonomously determine the required tools, relying instead on direct user instructions. This limits its adaptability in environments with unforeseen challenges.

Unlike Industry 6.0, which operates in predefined industrial settings, we propose Evolution 6.0 illustrated in Fig. \ref{fig:overview}, a framework that extends these capabilities to unpredictable environments, such as Mars, where tool scarcity and environmental variability are major challenges. Evolution 6.0 empowers autonomous robots to analyze unfamiliar surroundings, interpret contextual cues via VLM, and generate actionable solutions with VLA models. This enables robots to autonomously determine, design, and fabricate tools as needed, eliminating dependency on predefined instructions and enhancing operational flexibility.
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{figures/system_arch_evolution.png}
  \caption{Evolution 6.0 system architecture.}
  \label{fig:arch}
\end{figure*}

Inspired by the evolutionary leap of early humans who crafted tools to overcome challenges, Evolution 6.0 allows robots to autonomously perceive their surroundings, conceptualize solutions, and fabricate tools in real time. By integrating VLA models with Text-to-3D generative models, this framework enables robots to analyze novel challenges, dynamically design task-specific tools, and apply them seamlessly. In environments like Mars, where carrying a predefined toolset is impractical, this approach offers transformative potential, allowing robots to respond to unforeseen challenges with high autonomy. The Evolution 6.0 system is inherently designed to generalize across an open-ended range of tasks by dynamically interpreting new scenarios and autonomously generating the required tools. The presented evaluation with specific tasks serves as a benchmark to illustrate the system’s effectiveness and does not imply a limitation in its capabilities. Evolution 6.0 represents a paradigm shift by fostering adaptability in environments where traditional manufacturing infrastructures are unavailable, paving the way for a future of self-sufficient, adaptable robotic systems.
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{figures/CAD_GenAi.png}
  \caption{CAD model design by Generative AI.}
  \label{fig:cad}
\end{figure*}
\section{Related Work}


The development of autonomous systems for tool design, fabrication, and utilization hinges on advancements in three areas: Vision-Language Models (VLMs) for environmental interpretation, Vision-Language Action (VLA) models for robotic manipulation, and Text-to-3D models for tool generation. VLMs enable robots to process multimodal inputs—images, videos, and natural language—to make informed decisions. Recent improvements in VLMs, such as BLIP-2~\cite{li2023blip}, Flamingo~\cite{alayrac2022flamingo}, Kosmos-2~\cite{peng2023kosmos}, Molmo and pixmo~\cite{deitke2024molmo}, have significantly enhanced visual and textual data interpretation by leveraging transformer-based architectures. Notably, Qwen2-VL~\cite{wang2024qwen2} excels in generalization and efficient multimodal learning, making it ideal for real-time robotic reasoning.

Concurrently, VLA models have enabled robots to interpret natural language instructions alongside visual data to perform complex manipulation tasks~\cite{gbagbe2024bi},~\cite{khan2025shake}. These models generate precise 7D action outputs—translational and rotational displacements and gripper actuation—using transformer architectures. Pioneering work like PaLM-E~\cite{driess2023palm} integrated sensory data with natural language for long-horizon robotic tasks, while the RT-series models (RT-1~\cite{brohan2022rt}, RT-2~\cite{brohan2023rt}, RT-X~\cite{o2023open}) leveraged large-scale datasets to learn versatile, task-conditioned policies. RT-2, in particular, builds on RT-1 by incorporating vision-language representations to enable cross-domain generalization without retraining. Efforts like OpenVLA~\cite{kim2024openvla} and MiniVLA~\cite{belkhale2024minivla} have further enhanced model accessibility and real-time performance, though challenges remain in real-time adaptation and robustness.

Text-to-3D models ~\cite{liu2022towards}, ~\cite{jain2022zero}, ~\cite{yu2023text}, ~\cite{liu2024sherpa3d} are key for converting language descriptions into tangible 3D geometries, enabling robots to autonomously create task-specific tools. Early methods like Text2Shape~\cite{chen2019text2shape} used deep generative models for this purpose, while subsequent approaches such as ShapeCrafter~\cite{fu2022shapecrafter} improved controllability and fidelity. Recent innovations include the Industry 6.0 framework, which employed GPT-4o~\cite{openai2024gpt4technicalreport} to generate Signed Distance Functions (SDFs)~\cite{osher2004level} for additive manufacturing. However, high computational demands have led to more efficient models like LLaMa-Mesh~\cite{wang2024llama} and MeshGPT~\cite{siddiqui2024meshgpt}, with our work adopting LLaMa-Mesh for its efficiency and adaptability.

The convergence of VLMs, VLA models, and Text-to-3D technologies offers a transformative opportunity for creating autonomous systems capable of tool design and utilization. Integrating VLMs' environmental reasoning with VLA's action generation and Text-to-3D's tool creation enables robots to dynamically generate and employ task-specific tools. This approach enhances flexibility in diverse environments and facilitates real-time execution, reducing reliance on predefined tools and human intervention. However, seamless integration requires addressing challenges in data consistency, real-time processing, and structural integrity. Our work builds upon these advancements to establish a comprehensive framework for autonomous tool design and usage, positioning Evolution 6.0 as a novel application that merges these capabilities into versatile robotic systems capable of dynamic tool generation and deployment.

% The development of autonomous systems for tool design, fabrication, and utilization hinges on advancements in three main areas: Vision-Language Models (VLMs) for environmental interpretation, Vision-Language Action (VLA) models for robotic manipulation, and Text-to-3D models for tool generation. VLMs are crucial for enabling robots to process multimodal inputs—images, videos, and natural language descriptions—to make informed decisions. Recent improvements in VLMs, such as BLIP-2\cite{li2023blip}, Flamingo\cite{alayrac2022flamingo}, and Kosmos-2\cite{peng2023kosmos}, Molmo and pixmo \cite{deitke2024molmo}, have significantly enhanced the interpretation of visual and textual data. These models utilize transformer-based architectures to merge visual and textual information, allowing context-aware decision-making in complex scenarios. Notably, Qwen2-VL\cite{wang2024qwen2} stands out as an advanced open-source model that excels in generalization and efficient multimodal learning, making it well-suited for robotic applications requiring real-time reasoning.
% Concurrently, VLA models have emerged as effective tools that enable robots to interpret natural language instructions alongside visual data to perform complex manipulation tasks \cite{gbagbe2024bi}. These models also leverage transformer architectures to produce precise 7D action outputs, including translational and rotational displacements and gripper actuation. Pioneering work like PaLM-E~\cite{driess2023palm} introduced a multimodal language model capable of reasoning over long-horizon robotic tasks by integrating sensory data with natural language input. The RT-series models—RT-1~\cite{brohan2022rt}, RT-2~\cite{brohan2023rt}, and RT-X~\cite{o2023open}—further advanced this field by utilizing large-scale robotic datasets to learn task-conditioned policies applicable across diverse environments. RT-2 notably enhances RT-1 by incorporating vision-language representations that facilitate cross-domain generalization without explicit retraining. Recent efforts to improve the accessibility of VLA models have resulted in frameworks like OpenVLA~\cite{kim2024openvla}, which allows fine-tuning of pre-trained models across various robotic platforms using consumer-grade hardware. Additionally, MiniVLA~\cite{belkhale2024minivla} proposed a lightweight architecture optimized for real-time inference, making VLA models more suitable for mobile applications. Despite these advancements, challenges persist regarding real-time adaptation to novel tasks and robustness in unstructured environments.
% Text-to-3D models \cite{liu2022towards}, \cite{jain2022zero}, \cite{yu2023text}, \cite{liu2024sherpa3d}, \cite{bensadoun2024meta} are essential for bridging abstract language descriptions with tangible 3D geometries, enabling robots to autonomously create task-specific tools based on user instructions or environmental needs. Early methods like Text2Shape~\cite{chen2019text2shape} translated textual descriptions into corresponding 3D representations using deep generative models. Subsequent developments such as ShapeCrafter~\cite{fu2022shapecrafter} improved the controllability and fidelity of generated structures. Recent innovations have demonstrated enhanced accuracy and generalization capabilities; for instance, the Industry 6.0 framework utilized GPT-4o~\cite{openai2024gpt4technicalreport} to generate Signed Distance Functions (SDFs) \cite{osher2004level} representing complex geometries for additive manufacturing processes. However, high computational demands have spurred the development of more efficient alternatives like LLaMa-Mesh~\cite{wang2024llama} and MeshGPT~\cite{siddiqui2024meshgpt}, which feature specialized architectures for 3D object generation. We adopt LLaMa-Mesh due to its computational efficiency and adaptability across various robotic applications.
% The convergence of VLMs, VLA models, and Text-to-3D technologies presents a transformative opportunity for creating autonomous systems capable of tool design and utilization. Integrating VLMs' environmental reasoning with VLA's action generation and Text-to-3D's tool creation allows robots to dynamically generate and employ task-specific tools. This integration fosters adaptive tool creation based on high-level task requirements while enhancing flexibility in diverse environments. It also facilitates real-time task execution, minimizing reliance on predefined tools and human intervention. However, achieving seamless integration requires overcoming challenges related to data consistency, real-time processing, and structural integrity of tools. Our work builds upon advancements in these areas to establish a comprehensive framework for autonomous tool design and usage, empowering robots to perform complex tasks with minimal human oversight. Evolution 6.0 introduces a novel application by merging these capabilities into versatile robotic systems that can autonomously generate and utilize tools in dynamic settings.
\begin{figure*}[!t]
  \centering
  \includegraphics[width=\linewidth]{figures/picture.png}
    (a)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (b)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (c)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (d)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (e)
    \caption{Evaluation tasks. (a) Visual generalization: presence of several unseen distractor objects. (b) Visual generalization: unseen background. (c) Motion generalization: the plate is elevated from its original position. (d) Physical generalization: the plate has a different size than the one used in the dataset. (e) All types of generalization: the cake is initialized in unseen position and orientation, the background is changed, distractor object presents in the scene.}
  
  
  \label{fig:scenarios}
\end{figure*}
\section{System Architecture of Evolution 6.0}

\subsection{Tool generation module}
The Tool Generation Module is responsible for analyzing the robot’s environment and autonomously generating the necessary tools to accomplish a given task. It begins with the Robot Scene Input, where sensors such as cameras capture the robot’s environment to identify objects requiring interaction. The captured data is processed through a Qwen2-VL-2B-Instruct, where a Vision Encoder extracts relevant visual features, which are then interpreted by the QwenLM decoder to generate contextual textual descriptions. Based on the scene analysis, the system formulates a prompt, such as “Create a 3D model of a knife,” which is passed to an Autoregressive Language Model (LlamMa-Mesh). This model synthesizes a 3D representation of the required tool in mesh format, containing vertex (v) and face (f) data that define the tool’s geometry.  The generated mesh proceeds to the Mesh Rendering phase, where it is visualized and validated to ensure it meets the task requirements. Once validated, the finalized mesh is converted into G-Code, enabling the tool to be fabricated using a 3D printer. It is important to note that the system scales the tool by a factor, depending on the size of the item that needs to be manipulated in the scene. Fig. \ref{fig:cad} shows several tool examples produced by the LlamMa-Mesh. 

The Evolution 6.0 is illustrated in Fig. \ref{fig:arch}. Our system integrates two connected modules that work together to enable flexible robotic manipulation. Tool generation module focuses on creating situation-specific tools, while the Action generation module determines how to perform each task. This design adapts its actions based on observations of the environment and instructions expressed in everyday language. The approach uses QwenVLM for interpreting surroundings, OpenVLA for determining task steps, and LlamMa-Mesh for shaping three-dimensional tools. Through this arrangement, robots can operate effectively in environments that require tailored solutions and adaptive behaviors.


Overall, the Tool Generation Module provides a dynamic and adaptive mechanism for equipping the robot with the right instruments needed to effectively perform tasks in ever-changing environments.

\subsection{Action generation module}
The Action Generation Module plays a crucial role in converting high-level natural language instructions into precise robotic actions using a multimodal approach. This module is built on OpenVLA, a model developed by the Stanford AI Lab, and has been fine-tuned on a dataset of cake-cutting and cake-picking tasks. Each episode in the dataset corresponds to a specific task performed by a remotely controlled robotic manipulator, helping the model adapt effectively to dynamic environments. The architecture of the Vision-Language-Action (VLA) model remains consistent with OpenVLA, retaining its multimodal input-output structure. The system takes third-person camera frames and natural language task descriptions as input. It outputs a 7D action vector that directly controls the robotic manipulator. This action vector consists of spatial displacements \( \Delta X \), angular movements \( \Delta \theta \), and gripping actions \( \Delta Grip \).

The system operates iteratively, continuously processing environmental input and adjusting its actions in real time. Once it receives a task description, the module continuously analyzes third-person visual data and textual instructions to compute and execute actions. This continuous processing approach ensures smooth and uninterrupted task execution by eliminating delays between movements. As a result, the manipulator can respond in real time to changes in the environment, improving overall task efficiency and adaptability. 

To facilitate real-time processing, the QwenVLM and LlamMa-Mesh models were optimized by reducing numerical precision to int8, while the fine-tuned Vision-Language Action (VLA) model was kept in its original precision. This optimization, in conjunction with the utilization of a high-performance NVIDIA RTX 4090 24Gb GPU and streamlined communication protocols, enables the system to operate at a frequency of 5 Hz. This approach strikes an effective balance between computational efficiency and responsiveness, allowing Evolution 6.0 to meet the operational requirements of complex manipulation tasks.





\begin{table*}[t]
\centering
\vspace{3mm} % Adjust this value to your preference
\caption{Phase one results for tool generation, showing success rates and average inference times.}
\label{table:tool_generation_results}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Task} & \textbf{Success Rate} & \textbf{Average Inference Time (seconds)} \\
\hline
Environmental interpretation via VLM & 85\% & 4 \\
3D tool generation & 90\% & 10 \\
\hline
\end{tabular}
\end{table*}


\begin{figure*}[!t]
  \centering
  \includegraphics[width=\textwidth]{figures/download.png}
  \caption{Phase two results: success rates across generalization categories.}
  \label{fig:res}
\end{figure*}


\section{Dataset Collection and Training Pipeline}

The dataset for training was collected using a UR10 robotic arm equipped with a Logitech C920e HD 1080p camera to capture data for robotic manipulation tasks. The dataset comprises 20 episodes, evenly divided into two tasks: cutting a piece of cake and picking and placing the cake. Throughout the data collection process, the robot’s gripper position, orientation, and state were recorded, alongside synchronized high-definition visual feedback. Each episode includes language instructions such as “Cut one piece of cake” or “Pick one piece of cake and place on plate”. The collected data was formatted into the RLDS (Reinforcement learning dataset) structure, ensuring compatibility with OpenVLA for efficient organization of actions, images, and task instructions, making it suitable for imitation learning and task-specific action prediction. The collected dataset was used to fine-tune an OpenVLA-7b model using a parameter-efficient LoRA approach, applying rank-32 adapters to optimize memory usage while adjusting a minimal set of trainable weights. The training setup included a batch size of 16, a learning rate of  $5 \times 10^{-4}$, and 4000 gradient steps, with image augmentation disabled. Training was conducted on a single A100 GPU, with periodic checkpointing. Key metrics such as training loss, action accuracy, and L1 loss were monitored to ensure model performance. Additionally, smoothed metrics were employed for stable progress tracking. 

\section{Experiments}
\subsection*{Evaluation Methodology}
The evaluation was conducted in two phases. In the first phase, the tool generation module was assessed using success rate and inference time. In the second phase, the action generation module was evaluated on a UR-10 robot from a third-person view, with success rate as the primary metric. This phase encompassed five scenarios: the first scenario provided the model with previously encountered instructions and scenes from training, while the remaining four scenarios tested its generalization abilities. Specifically, physical generalization addressed variations in the target object's size or color, motion generalization involved changes in the object's position, semantic generalization required adapting to new instructions (such as replacing “cut the cake” with “cut the banana” or “cut the tomato” and “pick up the cake” with “pick up the cube”), and visual generalization evaluated performance when the scene was altered or distractors were introduced. The selection of Task 1 and Task 2 served as a validation step to demonstrate the system’s feasibility rather than its limitations, while Evolution 6.0's architecture supports seamless adaptation to new tasks without retraining due to robust multimodal integration.
% The evaluation was conducted in two phases. The first phase assessed the performance of the tool generation module, focusing on two primary metrics: success rate and inference time. In the second phase, the action generation module was evaluated using a UR-10 robot, with a third-person view to monitor the robot's actions. In this phase, the success rate was the main evaluation metric. The action generation module was tested in five distinct scenarios. The first scenario involved providing the model with previously encountered instructions and scenes during training. The remaining four scenarios were designed to evaluate the model’s generalization ability in various contexts: physical, motion, visual, and semantic generalization. Physical generalization in this context refers to variations in the size or color of the target object. Motion generalization involved changing the position of the target object, while semantic generalization tested the model's adaptability to new instructions (for instance, changing “cut the cake” to “cut the banana” or “cut the tomato” and replacing “pick up the cake” with “pick up the cube”). Lastly, visual generalization evaluated the model’s performance when the scene was altered or when distractors were introduced. The selection of Task 1 and Task 2 is a validation step to demonstrate the feasibility of the system, rather than an indication of its limitations. Evolution 6.0's architecture allows for seamless adaptation to new tasks without requiring retraining, thanks to its robust multimodal integration.


\subsubsection{Phase One: Tool Generation}
The first phase focused on the tool generation module, which was evaluated based on success rate and inference time. In this phase, the model was tasked with generating the required tools for different actions. The Qwen2-VL-2B-Instruct model serves as a sophisticated tool for interpreting environmental contexts and generating instructions for an autoregressive language model (LM) to create tools. This model is tested with 10 distinct robotic scenarios, successfully producing relevant prompts for the Text-to-3D’s model to facilitate tool generation. Out of these 10 scenarios, the visual language model (VLM) accurately described 8, yielding appropriate prompts. 





The average inference time for these interpretations was approximately 4 seconds, contingent upon the specific scene. However, the model encountered challenges when presented with scenes containing multiple identical target objects or objects belonging to the same class as the target. For instance, when tasked with distinguishing between screws and bolts, the model consistently misinterpreted the prompt as "Create the 3D model of the screwdriver," which is incompatible with the bolt. Despite this limitation, the autoregressive LM demonstrated a high success rate in generating tools based on these prompts, achieving successful outputs in 9 out of 10 instances, with an average generation time of 10 seconds, varying according to the complexity of the tool. Moreover, the model struggled with generating complex curves or fillets in its outputs. As illustrated in accompanying Fig. \ref{fig:cad}, all generated tools predominantly exhibited either sharp edges or simple circular cross-sections. This highlights a significant area for improvement in the model's ability to handle more intricate geometrical features in tool design. The results are summarized in Table 1.



\subsubsection{Phase Two: Action Generation}
In the second phase of the study, the action generation module was evaluated using a UR-10 robotic arm, with success rate serving as the primary metric. This phase involved testing the module across 10 distinct scenarios, including variations in physical, motion, visual, and semantic generalization. The evaluation aimed to assess the module's adaptability and robustness under different conditions.

\textbf{For Task 1}, the module demonstrated strong performance, achieving success rates of 87\% in physical generalization, 83\% in visual generalization, 75\% in motion generalization, and 41\% in semantic generalization. Notably, when provided with scenarios closely aligned with training settings, the module achieved a perfect success rate of 100\%.

\textbf{In Task 2}, performance slightly declined across most categories, with success rates of 80\% in physical generalization, 84\% in visual generalization, 65\% in motion generalization, and 33\% in semantic generalization. However, similar to Task 1, the module maintained a perfect success rate of 100\% when tested on scenarios identical to its training settings.

Experimental results (summarized in Fig. \ref{fig:res}) highlight the module's proficiency in physical and visual generalization and its resilience under varying conditions. However, there is significant room for improvement in motion and semantic generalization. For instance, when presented with instructions such as "Cut the banana" or "Place the tomato on the plate," the module exhibited inconsistent behavior. Examples of these scenarios are illustrated in Fig. ~\ref{fig:scenarios}. 

Overall, while the action generation module shows promise in handling familiar settings and certain types of generalizations, further refinement is needed to enhance its reliability in more complex or novel scenarios.


\section{Conclusion and Future Work}
% Evolution 6.0 is a novel framework that integrates Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models to enable autonomous robotic tool design and utilization in dynamic environments. Our experimental results demonstrated the system’s effectiveness, achieving a tool generation success rate of 90\% with an average inference time of 10 seconds, while the action generation module achieved average success rates of 83.5\% in physical generalization, 83.5\% in visual generalization, 70\% in motion generalization, and 37\% in semantic generalization. These results highlight the system's adaptability and potential for addressing real-world challenges, but also indicate areas for improvement, particularly in semantic generalization and complex geometric tool generation. Future work will focus on enhancing the system’s capabilities by extending it to bimanual manipulation, allowing for coordinated control of dual robotic arms to handle more complex tasks such as cooperative object handling and assembly. Additionally, an extended set of tasks will be incorporated to improve the system’s versatility in diverse environments, including industrial applications requiring higher precision and dexterity. Another key direction is the optimization of environmental interpretation by integrating more advanced models and imitation learning techniques to improve accuracy in complex, cluttered, or ambiguous scenarios. By pursuing these enhancements, Evolution 6.0 aims to become a more robust, flexible, and autonomous robotic framework capable of addressing a broader range of operational challenges with improved efficiency and adaptability. 
Evolution 6.0 is a novel framework that integrates Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models to enable autonomous robotic tool design and utilization in dynamic environments. Our experimental results demonstrate the system’s effectiveness, with the tool generation module achieving a 90\% success rate and an average inference time of 10 seconds, while the action generation module reached average success rates of 83.5\% in physical and visual generalization, 70\% in motion generalization, and 37\% in semantic generalization. These outcomes highlight the system's adaptability and potential for real-world applications, while also indicating areas for improvement in semantic generalization and complex geometric tool generation. Future work will extend the framework to bimanual manipulation for coordinated dual-arm control in complex tasks like cooperative object handling and assembly, incorporate an expanded set of tasks to enhance versatility in industrial environments requiring higher precision and dexterity, and optimize environmental interpretation by integrating more advanced models and imitation learning techniques to improve accuracy in complex, cluttered, or ambiguous scenarios.

% \bibliographystyle{IEEEtran}
% \bibliography{ref}
% \balance


\input{root.bbl}


\end{document}
