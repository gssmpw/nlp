[
  {
    "index": 0,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millicah, Katie and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "peng2023kosmos",
        "author": "Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu",
        "title": "Kosmos-2: Grounding multimodal large language models to the world"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "deitke2024molmo",
        "author": "Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others",
        "title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gbagbe2024bi",
        "author": "Gbagbe, Koffivi Fid{\\`e}le and Altamirano Cabrera, Miguel  and Alabbas, Ali and Alyunes, Oussama and Lykov, Artem and Tsetserukou, Dzmitry",
        "title": "Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "khan2025shake",
        "author": "Khan, Muhamamd Haris and Asfaw, Selamawit and Iarchuk, Dmitrii and Altamirano Cabrera, Miguel and Moreno, Luis and Tokmurziyev, Issatay and Tsetserukou, Dzmitry",
        "title": "Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "driess2023palm",
        "author": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
        "title": "Palm-e: An embodied multimodal language model"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "brohan2022rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others",
        "title": "Rt-1: Robotics transformer for real-world control at scale"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "brohan2023rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others",
        "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "o2023open",
        "author": "O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others",
        "title": "Open x-embodiment: Robotic learning datasets and rt-x models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "belkhale2024minivla",
        "author": "Suneel Belkhale and Dorsa Sadigh",
        "title": "MiniVLA: A Better VLA with a Smaller Footprint"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2022towards",
        "author": "Liu, Zhengzhe and Wang, Yi and Qi, Xiaojuan and Fu, Chi-Wing",
        "title": "Towards Implicit Text-Guided 3D Shape Generation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jain2022zero",
        "author": "Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben",
        "title": "Zero-Shot Text-Guided Object Generation with Dream Fields"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yu2023text",
        "author": "Xin Yu and Yuan-Chen Guo and Yangguang Li and Ding Liang and Song-Hai Zhang and Xiaojuan Qi",
        "title": "Text-to-3D with Classifier Score Distillation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2024sherpa3d",
        "author": "Liu, Fangfu and Wu, Diankun and Wei, Yi and Rao, Yongming and Duan, Yueqi",
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "chen2019text2shape",
        "author": "Chen, Kevin\nand Choy, Christopher B.\nand Savva, Manolis\nand Chang, Angel X.\nand Funkhouser, Thomas\nand Savarese, Silvio",
        "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "fu2022shapecrafter",
        "author": "Fu, Rao and Zhan, Xiao and Chen, Yiwen and Ritchie, Daniel and Sridhar, Srinath",
        "title": "ShapeCrafter: a recursive text-conditioned 3D shape generation model"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "openai2024gpt4technicalreport",
        "author": "OpenAI and others",
        "title": "GPT-4 Technical Report"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "osher2004level",
        "author": "Osher, Stanley and Fedkiw, Ronald and Piechor, K",
        "title": "Level set methods and dynamic implicit surfaces"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wang2024llama",
        "author": "Zhengyi Wang and Jonathan Lorraine and Yikai Wang and Hang Su and Jun Zhu and Sanja Fidler and Xiaohui Zeng",
        "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "siddiqui2024meshgpt",
        "author": "Siddiqui, Yawar and Alliegro, Antonio and Artemov, Alexey and Tommasi, Tatiana and Sirigatti, Daniele and Rosov, Vladislav and Dai, Angela and Nie{\\ss}ner, Matthias",
        "title": "Meshgpt: Generating triangle meshes with decoder-only transformers"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "alayrac2022flamingo",
        "author": "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millicah, Katie and Reynolds, Malcolm and others",
        "title": "Flamingo: a visual language model for few-shot learning"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "peng2023kosmos",
        "author": "Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu",
        "title": "Kosmos-2: Grounding multimodal large language models to the world"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "deitke2024molmo",
        "author": "Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others",
        "title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "wang2024qwen2",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "gbagbe2024bi",
        "author": "Gbagbe, Koffivi Fid{\\`e}le and Altamirano Cabrera, Miguel  and Alabbas, Ali and Alyunes, Oussama and Lykov, Artem and Tsetserukou, Dzmitry",
        "title": "Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "driess2023palm",
        "author": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
        "title": "Palm-e: An embodied multimodal language model"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "brohan2022rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others",
        "title": "Rt-1: Robotics transformer for real-world control at scale"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "brohan2023rt",
        "author": "Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others",
        "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "o2023open",
        "author": "O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others",
        "title": "Open x-embodiment: Robotic learning datasets and rt-x models"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "belkhale2024minivla",
        "author": "Suneel Belkhale and Dorsa Sadigh",
        "title": "MiniVLA: A Better VLA with a Smaller Footprint"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "liu2022towards",
        "author": "Liu, Zhengzhe and Wang, Yi and Qi, Xiaojuan and Fu, Chi-Wing",
        "title": "Towards Implicit Text-Guided 3D Shape Generation"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "jain2022zero",
        "author": "Jain, Ajay and Mildenhall, Ben and Barron, Jonathan T. and Abbeel, Pieter and Poole, Ben",
        "title": "Zero-Shot Text-Guided Object Generation with Dream Fields"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "yu2023text",
        "author": "Xin Yu and Yuan-Chen Guo and Yangguang Li and Ding Liang and Song-Hai Zhang and Xiaojuan Qi",
        "title": "Text-to-3D with Classifier Score Distillation"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "liu2024sherpa3d",
        "author": "Liu, Fangfu and Wu, Diankun and Wei, Yi and Rao, Yongming and Duan, Yueqi",
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "bensadoun2024meta",
        "author": "Bensadoun, Raphael and Monnier, Tom and Kleiman, Yanir and Kokkinos, Filippos and Siddiqui, Yawar and Kariya, Mahendra and Harosh, Omri and Shapovalov, Roman and Graham, Benjamin and Garreau, Emilien and others",
        "title": "Meta 3d gen"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "chen2019text2shape",
        "author": "Chen, Kevin\nand Choy, Christopher B.\nand Savva, Manolis\nand Chang, Angel X.\nand Funkhouser, Thomas\nand Savarese, Silvio",
        "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "fu2022shapecrafter",
        "author": "Fu, Rao and Zhan, Xiao and Chen, Yiwen and Ritchie, Daniel and Sridhar, Srinath",
        "title": "ShapeCrafter: a recursive text-conditioned 3D shape generation model"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "openai2024gpt4technicalreport",
        "author": "OpenAI and others",
        "title": "GPT-4 Technical Report"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "osher2004level",
        "author": "Osher, Stanley and Fedkiw, Ronald and Piechor, K",
        "title": "Level set methods and dynamic implicit surfaces"
      }
    ]
  },
  {
    "index": 44,
    "papers": [
      {
        "key": "wang2024llama",
        "author": "Zhengyi Wang and Jonathan Lorraine and Yikai Wang and Hang Su and Jun Zhu and Sanja Fidler and Xiaohui Zeng",
        "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models"
      }
    ]
  },
  {
    "index": 45,
    "papers": [
      {
        "key": "siddiqui2024meshgpt",
        "author": "Siddiqui, Yawar and Alliegro, Antonio and Artemov, Alexey and Tommasi, Tatiana and Sirigatti, Daniele and Rosov, Vladislav and Dai, Angela and Nie{\\ss}ner, Matthias",
        "title": "Meshgpt: Generating triangle meshes with decoder-only transformers"
      }
    ]
  }
]