\clearpage
\maketitlesupplementary
% \title{\textcolor{cyan}{OverLoCK}: An \textcolor{cyan}{Over}view-first-\textcolor{cyan}{Lo}ok-\textcolor{cyan}{C}losely-next ConvNet\\ with Context-Mixing Dynamic \textcolor{cyan}{K}ernels}

% %%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{Meng Lou 
% \and
% Yizhou Yu 
% \and
% School of Computing and Data Science, The University of Hong Kong \\
% {\tt\small loumeng@connect.hku.hk}, {\tt\small yizhouy@acm.org}
% }

% \newpage
% \counterwithin{table}{section}
% \counterwithin{figure}{section}
% \renewcommand{\thetable}{\thesection.\alph{table}}
% \renewcommand{\thefigure}{\thesection.\alph{figure}}
% \setcounter{page}{1}
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\thefigure}{\Alph{figure}}


\section{More Ablation Studies}
\label{sec:appendix_ab}
On the basis of the training settings outlined in Section \textcolor{red}{4.4}, we additionally conduct a series of in-depth ablation experiments to meticulously examine the impact of every component in our proposed method.
% Section \ref{sec:ab_study} 4.4
\par
% Table \ref{tab:kernel_size}
\textbf{Impact of Kernel Sizes}. We compared the performance under various settings of kernel sizes, as outlined in Table \textcolor{red}{6} (the definition of the kernel size in our proposed method is given in Section \textcolor{red}{3.3}). The results indicate that the configuration $\left\{[17, 15, 13], [7], [13, 7]\right\}$ yields the optimal performance on both image classification and semantic segmentation tasks. Further enlarging the kernels does not lead to additional improvements.
% Section \ref{sec:net} 3.3
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Ablation study of the kernel size setting.}
    \resizebox{0.49\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{l}{Kernel Sizes} & \# F (G) & \# P (M) & Top-1 (\%) & mIoU (\%) \\
    \midrule
    \multicolumn{1}{l}{{$\left\{[19, 17, 15], [7], [15, 7]\right\}$}} & 2.8   & 16.5  & 80.7  & 43.8  \\
    \rowcolor[rgb]{ .867,  .922,  .969}\multicolumn{1}{l}{{$\left\{[17, 15, 13], [7], [13, 7]\right\}$}} & 2.6   & 16.4  & \textbf{80.8} & \textbf{43.8} \\
    \multicolumn{1}{l}{{$\left\{[13, 11, 9], [7], [9, 7]\right\}$}} & 2.6   & 16.3  & 80.5  & 43.5  \\
    \multicolumn{1}{l}{{$\left\{[9, 9, 7], [7], [7, 7]\right\}$}} & 2.6   & 16.1  & 80.6  & 43.3  \\
    {$\left\{[7, 7, 7], [7], [7, 7]\right\}$} & 2.5   & 16.1  & 80.4  & 43.1  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:kernel_size}%
\end{table}%

\textbf{Impact of Stage Ratio}. The \textit{Stage Ratio} means the ratio between the number of blocks in the last stage of Base-Net and the number of blocks in the first stage of Focus-Net. In the default setting of the OverLoCK model, the stage ratio is 1:2 with the intention of allocating more network blocks to Focus-Net for extracting robust contextual information. 
%To avoid a significant increase in parameters, the number of layers in the last stage of the Overview-Net is kept consistent with the number of layers in the last stage of the Focus-Net. 
In this section, we investigate the impact of \textit{Stage Ratio}. Apart from the default setting of 1:2, we further set \textit{Stage Ratio} to 1:1 and 1:3 while maintaining the total number of network blocks constant. The results presented in Table~\ref{tab:stage_ratio} demonstrate that a \textit{Stage Ratio} of 1:2 yields the best outcomes. We posit that this is because a too small \textit{Stage Ratio} results in insufficient number of blocks in Focus-Net, thereby hindering the extraction of discriminative deep features. Conversely, an excessively large \textit{Stage Ratio} leads to a shortage of blocks in Base-Net, 
%causing a lack of shallow local details and weak context prior encompassing meaningful semantic information, 
thereby providing insufficient contextual guidance.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Ablation study of different stage ratio settings.}
      \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    \multicolumn{1}{l}{Stage Ratio} & \# F (G) & \# P (M) & Top-1 (\%) & mIoU (\%) \\
    \midrule
    1:1   & 2.7   & 16.1  & 80.4  & 42.9  \\
    \rowcolor[rgb]{ .867,  .922,  .969}1:2   & 2.6   & 16.4  & \textbf{80.8} & \textbf{43.8} \\
    1:3   & 2.7   & 15.9  & 80.6  & 43.6  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:stage_ratio}%
\end{table}%

\textbf{Impact of Channel Reduction Factor}. In the default configuration of the OverLoCK model, we employ a 1$\times$1 convolution to reduce the number of output channels of Overview-Net by a factor of 4 and concatenate this result with the output of Base-Net before forwarding it to Focus-Net. We term this reduction as the \textit{Channel Reduction Factor (CRF)}. Therefore, the value of \textit{CRF} determines the number of channels in the \textit{context prior}, thereby influencing the guidance capability. In this regard, we investigate the effects of different \textit{CRF} settings. It is important to note that during the adjustment of \textit{CRF}, we also modify the number of channels of Focus-Net to maintain similar complexities across different model variants. The results in Table~\ref{tab:crf} demonstrate that \textit{CRF}=4 yields the optimal performance.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
   \caption{Ablation study of channel reduction factor settings.}
    \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    CRF & \# F (G) & \# P (M) & Top-1 (\%) & mIoU (\%) \\
    \midrule
    2     & 2.6   & 16.1  & 80.5  & 42.9  \\
    \rowcolor[rgb]{ .867,  .922,  .969}4     & 2.6   & 16.4  & \textbf{80.8} & \textbf{43.8} \\
    6     & 2.7   & 16.6  & 80.7  & 43.4  \\
    8     & 2.7   & 16.7  & 80.6  & 43.0  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:crf}%
\end{table}%

\textbf{Impact of Auxiliary Loss}. To explore the effects of applying the auxiliary loss to Overview-Net, we adjust the weight of the auxiliary loss, drawing inspiration from prior research~\cite{zhao2017pyramid}. Given that the architectures of the models in this comparison are consistent, we opt not conduct further experiments on segmentation tasks for the sake of simplicity. The results presented in Table~\ref{tab:auxloss} indicate that the utilization of an auxiliary loss improves accuracy, while varying the weight of the auxiliary loss does not lead to a notable impact on performance. This observation aligns with findings in previous study \cite{zhao2017pyramid}.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Ablation study of auxiliary loss.}
    \resizebox{0.425\textwidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    Aux Loss Ratio & 0     & 0.2   & \cellcolor[rgb]{ .867,  .922,  .969}0.4   & 0.8   & 1.0 \\
    \midrule
    Top-1 (\%) & 80.4  & 80.7  & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{80.8} & 80.7  & 80.7 \\
    \bottomrule
    \end{tabular}%
  \label{tab:auxloss}%
}
\end{table}%
% ``Baseline" represents the ContMix utilized in our OverLoCK model.
% Table~\ref{tab:roadmap} 6
\textbf{Effectiveness of our DDS-based Top-down Network}. To evaluate the effectiveness of the proposed DDS, we reconstruct our OverLoCK-XT model as a standard hierarchical network. To be specific, we eliminate the top-down attention mechanism by removing the Overview-Net while keeping the same types of layers in the Base-Net and Focus-Net. To maintain comparable complexity with other models, the number of channels and layers in the four stages are set to [64, 112, 256, 360] and [2, 2, 9, 4], respectively. This model is denoted as the ``Hierarchical Model". Additionally, we compare it with the ``Baseline" model in Table \textcolor{red}{6} which is a fully static ConvNet. As shown in Table~\ref{tab:dds_compare}, the ``Hierarchical Model" results in a noticeable performance drop, demonstrating the effectiveness of our DDS-based top-down context guidance. However, when compared with the ``Baseline" model, it still exhibits significant advantages, clearly indicating the superiority of our proposed dynamic convolution module.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Effectiveness of the proposed DDS-based top-down network.}
        \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{l}{Method} & \# F (G) & \# P (M) & Top-1 (\%) & mIoU (\%) \\
    \midrule
    \multicolumn{1}{l}{Baseline Model} & 2.6   & 16.3  & 78.5  & 41.1 \\
    \multicolumn{1}{l}{Hierarchical Model} & 2.7   & 16.2  & 79.2  & 41.9  \\
    \rowcolor[rgb]{ .867,  .922,  .969}OverLoCK-XT & 2.6   & 16.4  & \textbf{80.7} & \textbf{43.8} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:dds_compare}%
\end{table}%

\par
\textbf{Ablation Study of ContMix}. We conduct a comprehensive comparison of various components within our proposed ContMix framework, as presented in Section \textcolor{red}{3.2}. As listed in Table~\ref{tab:ab_contmix}, we initially compute $\mathbf{Q}$ and $\mathbf{K}$ using the fused feature map instead of utilizing the channels of $\mathbf{X}$ corresponding to $\mathbf{Z}_i$ and $\mathbf{P}_i$ (the latest \textit{context prior}). This model variant, referred to as ``Fusion Affinity", results in a marginal performance decline. Subsequently, we interchange the features used to generate the $\mathbf{Q}$ and $\mathbf{K}$ matrices. This model, denoted as ``Reverse QK", also exhibits a decrease in performance. Furthermore, we individually eliminate the Softmax function (referred to as ``w/o Softmax"), remove the RepConv (referred to as ``w/o RepConv"), and substitute small kernels with large kernels (referred to as ``w/o Small Kernel"). These alterations decrease performance on both classification and segmentation tasks.
% Section \ref{sec:dyconv} 3.2
% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Ablation study of ContMix.}
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    \multicolumn{1}{l}{Method} & \# F (G) & \# P (M) & Top-1 (\%) & mIoU (\%) \\
    \midrule
    \rowcolor[rgb]{ .867,  .922,  .969}\multicolumn{1}{l}{Baseline} & 2.6   & 16.4  & \textbf{80.8} & \textbf{43.8} \\
    \multicolumn{1}{l}{Fusion Affinity} & 2.7   & 16.6  & 80.7  & 43.5  \\
    \multicolumn{1}{l}{Reverse QK} & 2.7   & 16.4  & 80.6  & 42.9  \\
    \multicolumn{1}{l}{w/o Softmax} & 2.6   & 16.4  & 80.5  & 43.5  \\
    \multicolumn{1}{l}{w/o RepConv} & 2.5   & 16.1  & 80.6  & 43.4  \\
    \multicolumn{1}{l}{w/o Small Kernel} & 2.8   & 16.6  & 80.7  & 43.3  \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:ab_contmix}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{A comparison of image classification with 384$\times$384 inputs.}
\resizebox{0.4\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    Method & Type  & \# F (G) & \# P (M) & Acc. (\%) \\
    \midrule
    Swin-B & T     & 47.1  & 88    & 84.5  \\
    MaxViT-B & T     & 74.2  & 120   & 85.7  \\
    ConvNeXt-B & C     & 45.2  & 88    & 85.1  \\
    InceptionNeXt-B & C     & 43.6  & 87    & 85.2  \\
    RDNet-L & C     & 101.9  & 186   & 85.8  \\
    PeLK-B-101 & C     & 68.3  & 90    & 85.8  \\
    \rowcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-B} & C     & 50.4  & 95    & \textbf{86.2} \\
    \bottomrule
    \end{tabular}%
}
  \label{tab:in1k-384}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[ht]
  \centering
  \caption{Robustness comparisons of different models.}
      \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    Models & \# F (G) & \# P (G) & 1K & V2    & A     & R     & Sketch \\
    \midrule
    Swin-T  & 4.5   & 28    & 81.3  & 69.7  & 21.1  & 41.5  & 29.3  \\
    VMamba-T & 4.9   & 29    & 82.6  & 72.0  & 27.0  & 45.4  & 32.9  \\
    ConvNeXt-T & 4.5   & 29    & 82.1  & 72.5  & 24.2  & 47.2  & 33.8  \\
    HorNet-T & 4.0   & 22    & 82.8  & 72.3  & 26.6  & 46.6  & 34.1  \\
    SLaK-T & 5.0   & 30    & 82.5  & 72.0  & 30.0  & 45.3  & 32.4  \\
    NAT-T  & 4.3   & 28    & 83.2  & 72.2  & 33.0  & 44.9  & 31.9  \\
    RDNet-T & 5.0   & 24    & 82.8  & 72.9  & 27.7  & 49.0  & 37.0  \\
    UniRepLKNet-T & 4.9   & 25    & 83.2  & 72.8  & 34.8  & 49.4  & 36.9  \\
    MogaNet-S & 5.0   & 33    & 83.4  & 72.6  & 33.4  & 49.7  & 37.8  \\
    \rowcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-T} & 5.5   & 33    & \textbf{84.2} & \textbf{74.0} & \textbf{39.4} & \textbf{53.3} & \textbf{40.6} \\
    \midrule
    Swin-S & 8.7   & 50    & 83.0  & 72.0  & 32.5  & 45.2  & 32.3  \\
    VMamba-S & 8.7   & 50    & 83.6  & 73.2  & 33.2  & 49.4  & 37.0  \\
    ConvNeXt-S  & 8.7   & 50    & 83.1  & 72.5  & 31.3  & 49.6  & 37.1  \\
    HorNet-S  & 8.8   & 50    & 84.0  & 73.6  & 36.2  & 49.7  & 36.9  \\
    SLaK-S & 9.8   & 55    & 83.8  & 73.6  & 39.3  & 50.9  & 37.5  \\
    NAT-S  & 7.8   & 51    & 83.7  & 73.2  & 37.4  & 47.3  & 34.3  \\
    RDNet-S & 8.7   & 50    & 83.7  & 73.8  & 33.5  & 52.8  & 39.8  \\
    UniRepLKNet-S & 9.1   & 56    & 83.9  & 73.7  & 38.3  & 50.6  & 36.9  \\
    MogaNet-B & 9.9   & 44    & 84.3  & 74.3  & 40.4  & 50.1  & 38.6  \\
    \rowcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-S} & 9.7   & 56    & \textbf{84.8} & \textbf{74.9} & \textbf{45.0} & \textbf{57.2} & \textbf{45.8} \\
    \midrule
    Swin-B & 15.4  & 88    & 83.5  & 72.4  & 35.4  & 46.5  & 32.7  \\
    VMamba-B & 15.4  & 89    & 83.9  & 73.5  & 37.2  & 49.5  & 38.5  \\
    ConvNeXt-B  & 15.4  & 89    & 83.8  & 73.7  & 36.7  & 51.2  & 38.2  \\
    HorNet-B  & 15.6  & 87    & 84.3  & 73.9  & 39.9  & 51.2  & 38.1  \\
    SLaK-B & 17.1  & 95    & 84.0  & 74.0  & 41.6  & 50.8  & 38.5  \\
    NAT-B  & 13.7  & 90    & 84.3  & 74.1  & 41.4  & 49.7  & 36.6  \\
    RDNet-B & 15.4  & 87    & 84.4  & 74.2  & 38.1  & 52.7  & 40.1  \\
    MogaNet-L & 15.9  & 83    & 84.7  & 74.0  & 41.0  & 52.2  & 39.0  \\
    \rowcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-B} & 16.7  & 95    & \textbf{85.1} & \textbf{75.4} & \textbf{47.7} & \textbf{58.5} & \textbf{46.0} \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:robust_compare}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[th]
  \centering
  \caption{Speed comparison among various models. Throughput (Thr.) is tested on a single NVIDIA L40S GPU with a batch size of 128 and an image size of 3$\times$224$\times$224.}
  \resizebox{0.875\textwidth}{!}{
    \begin{tabular}{lccccrlcccc}
\cmidrule{1-5}\cmidrule{7-11}    Method & \# F (G) & \# P (M) & Thr. (imgs/s) & Acc. (\%) &       & Method & \# F (G) & \# P (M) & Thr. (imgs/s) & Acc. (\%) \\
\cmidrule{1-5}\cmidrule{7-11}    Swin-T & 4.5   & 28    & 1324  & 81.3  &       & FocalNet-T & 4.5   & 29    & 1251  & 82.3  \\
    Swin-S & 8.7   & 50    & 812   & 83.0  &       & FocalNet-S & 8.7   & 50    & 777   & 83.5  \\
    Swin-B & 15.4  & 88    & 544   & 83.5  &       & FocalNet-B & 15.4  & 89    & 481   & 83.7  \\
\cmidrule{1-5}\cmidrule{7-11}    MaxViT-T & 5.6   & 31    & 683   & 83.7  &       & SLaK-T & 5.0   & 30    & 1126  & 82.5  \\
    MaxViT-S & 11.7  & 69    & 439   & 84.5  &       & SLaK-S & 9.8   & 55    & 747   & 83.8  \\
    MaxViT-B & 24.0  & 120   & 241   & 84.9  &       & SLaK-B & 17.1  & 95    & 478   & 83.7  \\
\cmidrule{1-5}\cmidrule{7-11}    NAT-M & 2.7   & 20    & 1740  & 81.8  &       & InternImage-T & 5.0   & 30    & 1084  & 83.5  \\
    NAT-T & 4.3   & 28    & 1287  & 83.2  &       & InternImage-S & 8.0   & 50    & 740   & 84.2  \\
    NAT-S & 7.8   & 51    & 823   & 83.7  &       & InternImage-B & 16.0  & 97    & 481   & 84.9  \\
\cmidrule{7-11}    NAT-B & 13.7  & 90    & 574   & 84.3  &       & UniRepLKNet-N & 2.8   & 18    & 1792  & 81.6  \\
\cmidrule{1-5}    BiFormer-T & 2.2   & 13    & 1103  & 81.4  &       & UniRepLKNet-T & 4.9   & 31    & 1094  & 83.2  \\
    BiFormer-S & 4.5   & 26    & 527   & 83.8  &       & UniRepLKNet-S & 9.1   & 56    & 707   & 83.9  \\
\cmidrule{7-11}    BiFormer-B & 9.8   & 57    & 341   & 84.3  &       & MogaNet-S & 5.0   & 25    & 766   & 83.4  \\
\cmidrule{1-5}    VMamba-T & 4.9   & 29    & 1179  & 82.6  &       & MogaNet-B & 9.9   & 44    & 373   & 84.3  \\
    VMamba-S & 8.7   & 50    & 596   & 83.6  &       & MogaNet-L & 15.9  & 83    & 282   & 84.7  \\
\cmidrule{7-11}    VMamba-B & 15.4  & 89    & 439   & 83.9  &       & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-XT} & \cellcolor[rgb]{ .867,  .922,  .969}2.6  & \cellcolor[rgb]{ .867,  .922,  .969}16  & \cellcolor[rgb]{ .867,  .922,  .969}1672 & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{82.7} \\
\cmidrule{1-5}    ConvNeXt-T & 4.5   & 29    & 1507  & 82.1  &       & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-T} & \cellcolor[rgb]{ .867,  .922,  .969}5.5  & \cellcolor[rgb]{ .867,  .922,  .969}33  & \cellcolor[rgb]{ .867,  .922,  .969}810 & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{84.2} \\
    ConvNeXt-S & 8.7   & 50    & 926   & 83.1  &       & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-S} & \cellcolor[rgb]{ .867,  .922,  .969}9.7  & \cellcolor[rgb]{ .867,  .922,  .969}56  & \cellcolor[rgb]{ .867,  .922,  .969}480 & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{84.8} \\
    ConvNeXt-B & 15.4  & 89    & 608   & 83.8  &       & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{OverLoCK-B} & \cellcolor[rgb]{ .867,  .922,  .969}16.7  & \cellcolor[rgb]{ .867,  .922,  .969}95  & \cellcolor[rgb]{ .867,  .922,  .969}306 & \cellcolor[rgb]{ .867,  .922,  .969}\textbf{85.1} \\
\cmidrule{1-5}\cmidrule{7-11}    
\end{tabular}%
  }
  \label{tab:speed}%
\end{table*}%

\section{Additional Experiments on Image Classification}
\subsection{Large Resolution Evaluation}
Following previous works \cite{liu2022convnet,yu2023inceptionnext,kim2024densenets}, we further investigate the image classification performance on the ImageNet-1K dataset at a higher resolution (i.e., 384$\times$384). Specifically, we pre-train the base model on 224$\times$224 inputs and then fine-tune it on 384$\times$384 inputs for 30 epochs. As shown in Table \ref{tab:in1k-384}, our OverLock-B model achieves superior performance under high-resolution input conditions. Notably, OverLock-B surpasses MaxViT-B by 0.5\% in Top-1 accuracy while reducing the parameter count by over one-third. Compared to PeLK-B, a large kernel ConvNet, our method also demonstrates significant improvements. These results further validate the robustness of our proposed method in handling large-resolution inputs.

\subsection{Robustness Evaluation}
We further assess the robustness of our models using the ImageNet out-of-distribution (OOD) benchmarks, including ImageNet-V2~\cite{imagenetv2}, ImageNet-A~\cite{imagenet-A}, ImageNet-R~\cite{imagenet-R}, and ImageNet-Sketch~\cite{ImageNetSketch}. As shown in Table~\ref{tab:robust_compare}, our method demonstrates excellent robustness on different datasets, outperforming representative ConvNets, Vision Transformers, and Vision Mamba. Notably, although OverLoCK-B improves over MogaNet-L by 0.4\% in Top-1 accuracy on ImageNet-1K, it achieves significant gains on OOD datasets, with improvements of 1.4\% on ImageNet-V2, 6.7\% on ImageNet-A, 6.3\% on ImageNet-R, and 6.8\% on ImageNet-Sketch. These results showcase the strong robustness of our pure ConvNet.

\begin{figure}[thb]
    \centering
    \includegraphics[width=0.325\textwidth]{cam.pdf}
    \caption{Class activation maps of the proposed OverLoCK network. (a), (b), and (c) show the input images, class activation maps of Overview-Net, and class activation maps of Focus-Net, respectively.}
    \label{fig:cam}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=0.47\textwidth]{erf.pdf}
    \caption{Comparison of ERF among various models.}
    \label{fig:erf}
\end{figure}

% \vspace{-8.75pt}
\section{Speed Analysis}
% \vspace{-7.5pt}
% Figure~\ref{fig:acc_plot} I
% Table~\ref{tab:speed} I
We provide a comparison of speed-accuracy trade-off in Figure \textcolor{red}{1}. More details are listed in Table \textcolor{red}{1}, where an OverLoCK variant often achieves faster speed and higher accuracy simultaneously than a larger variant of another network, demonstrating an excellent trade-off between speed and accuracy. For instance, OverLoCK-XT achieves 1672 imgs/s in throughput, improving upon Swin-T by over 300 imgs/s, while significantly enhancing Top-1 accuracy by 1.4\%. Also, OverLoCK-T achieves about 200 imgs/s improvement in throughput compared to ConvNeXt-B while achieving better performance at the cost of only around one-third of the FLOPS. When compared to more advanced models, OverLoCK still exhibits significant advantages. For example, OverLoCK-S surpasses MogaNet-B by over 100 imgs/s in throughput while increasing Top-1 accuracy from 84.3\% to 84.8\%. Likewise, OverLoCK-XT surpasses BiFormer-T by over 600 imgs/s in throughput while remarkably improving Top-1 accuracy by 1.3\%.

\section{Visualization Analysis}
\subsection{Effect of Context Guidance}
To visually understand the effect of context guidance, we separately visualize the class activation maps generated by Overview-Net and Focus-Net in OverLoCK-T using Grad-CAM~\cite{selvaraju2020grad} for the ImageNet-1K validation set. As shown in Figure~\ref{fig:cam}, Overview-Net first produces a coarse localization of an object, and when this signal is used as the top-down guidance for Focus-Net, the object's location and shape becomes more accurate.

% \begin{algorithm*}[t]
% \caption{Pseudo code of ContMix in a PyTorch style.}
% % linenos=true,
% \begin{minted}[fontsize=\small]{python}
% import torch
% from torch import nn
% from einops import rearrange, einsum
% from natten.functional import na2d_av

% class ContMix(nn.Module):
%     def __init__(self, dim, kernel_size, groups):
%         super().__init__()
%         self.groups = groups
%         self.K = kernel_size
        
%         self.q = nn.Conv2d(dim, dim, kernel_size=1)
%         self.k = nn.Conv2d(dim, dim, kernel_size=1)
%         self.d = nn.Conv2d(49, kernel_size**2, kernel_size=1)
%         self.pool = nn.AdaptiveAvgPool2d(7)
%         self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size,
%                                 padding=kernel_size//2, groups=dim)
    
%     def forward(self, x):
%         B, C, H, W = x.shape
%         y = self.dwconv(x)
        
%         # compute affinity matrix
%         q = rearrange(self.q(x), 'b (g c) h w -> b (h w) g c', g=self.groups)
%         k = rearrange(self.k(self.pool(x)), 'b (g c) h w -> b g c (h w)', g=self.groups)
%         affinity = einsum(q, k, 'b l g c, b g c s -> b g l s')
        
%         # transition affinity matrix to dynamic kernels
%         affinity = rearrange(affinity, 'b g l s -> b s g l')
%         kernel = rearrange(self.d(affinity), 'b s g (h w) -> b g h w s', h=H, w=W)
        
%         # spatially varying convolution
%         x = rearrange(x, 'b (g c) h w -> b g h w c', g=self.groups)
%         x = na2d_av(kernel.softmax(dim=-1), x, kernel_size=self.K)
%         x = rearrange(x, 'b g h w c -> b (g c) h w')
        
%         return x + y
% \end{minted}
% \label{alg:dyconv}
% \end{algorithm*}

\subsection{Effective Receptive Field Analysis}
To visually demonstrate the representation capacity of OverLoCK, we compare the Effective Receptive Field (ERF)~\cite{luo2016understanding} of our OverLoCK-T with that of other representative models with comparable complexity. The visualizations are generated using over 300 randomly sampled images with a resolution of 224$\times$224 from the ImageNet-1K validation set. As shown in Figure~\ref{fig:erf}, our model not only produces global responses but also exhibits significant local sensitivity, indicating that OverLoCK can effectively model both global and local contexts simultaneously.
% \vspace{-6.6pt}
% \section{Code}
% % \vspace{-6.6pt}
% We provide the pseudo code of ContMix in Algorithm~\ref{alg:dyconv}.


% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.