\section{Related Work}
In this section, we discuss recent text-music datasets, metadata imputation research as well as music feature retrieval systems. MusicCaps____, one of the most popular text-music datasets, contains human-annotated captions for a limited set of around 5,000 music samples. Despite the high-quality descriptions, its small scale is not sufficient for data-hungry deep learning models. MusicBench____ further augments MusicCaps 11x and modifying the captions accordingly using an LLM. Still, it is limited to around 52k instances, each 10s in duration. The WavCaps dataset____ expanded upon this concept by collecting over 400,000 audio-caption pairs from multiple online sources, employing heuristic filtering methods to ensure relevance. However, WaveCaps is more focused on generic audio rather than musical sounds. The recently released SongDescriber dataset____, contains 1.1k human-written text captions of 706 songs. The authors intended this dataset to be used as an evaluation set for text-to-music systems. SunoCaps____ contains 256 prompt-generated music samples, showing an alternative way creating text-music dataset. Finally, Noice2music____ introduced the MuLaMCap dataset (MuLan-LaMDA Music Caption) which consists of 400k music-text pairs created by annotating AudioSet content with a data mining pipeline, but the dataset is not publicly available. 

The recently released Midicaps dataset____ contains over 178,000 music MIDI files with associated text captions. They used predefined feature extractors to get relevant features such as key, chords, and time signature, and used an LLM to generate captions. This dataset contains captions for MIDI music, however, not audio music files. The approach from  ____ aligns with existing methods that integrate symbolic representations with large language models for caption generation. In the waveform domain, lp-music caps____ explored a tag-to-caption approach, where pre-existing metadata from audio libraries was used as input for a large language model, producing pseudo-captions to describe musical content. However, they relied on pre-existing metadata, which often exhibits notable incompleteness and inconsistencies in real-world scenarios. This limitation can hinder the comprehensive representation of musical content and potentially introduce biases in the generated captions.

Recent research has also explored metadata imputation to address incomplete or missing annotations in music datasets. For example, ____ proposed a collaborative filtering approach that combines content-based and user-generated metadata for automatic music tagging, leveraging deep learning to predict missing attributes based on musical embeddings. Similarly,____ introduced a transformer-based model that infers missing genre and mood labels from audio spectrograms, highlighting the role of multimodal learning in metadata completion. These works align with our use of large language models for metadata imputation, although our method uniquely incorporates a retrieval-based in-context learning strategy to enhance imputation accuracy.

In addition, music feature retrieval systems have played a crucial role in improving dataset utility. CLAP____ employed a contrastive learning approach for aligning music and text embeddings, enabling zero-shot music retrieval based on textual queries. MERT____ provided a powerful audio feature extractor designed specifically for music representation learning. Both models demonstrate the effectiveness of using pre-trained embeddings for understanding musical content. Salmonn____ is an integrated multiple-source audio features including Q-Former queries____, Whisper features____, BEATs features____ to a generate music captions . Our approach builds upon these works by integrating retrieval-based metadata imputation with local large language models, aiming to ensure more contextually relevant annotations.

Despite the advancements made by these prior works, challenges remain in ensuring accurate, and scalable metadata imputation for large-scale music-caption datasets. The \textbf{\dataset} dataset aims to address these gaps by leveraging a hybrid approach that combines musical representation, retrieval-based contextualization, and in-context learning via locally hosted large language models.