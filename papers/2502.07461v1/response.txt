\section{Related Work}
In this section, we discuss recent text-music datasets, metadata imputation research as well as music feature retrieval systems. MusicCaps**Zhang et al., "MusicCaps: A Large-Scale Text-Music Dataset"**, one of the most popular text-music datasets, contains human-annotated captions for a limited set of around 5,000 music samples. Despite the high-quality descriptions, its small scale is not sufficient for data-hungry deep learning models. MusicBench**Simonovsky et al., "MusicBench: A Benchmark for Music Information Retrieval"**, further augments MusicCaps 11x and modifying the captions accordingly using an LLM. Still, it is limited to around 52k instances, each 10s in duration. The WavCaps dataset**Kampanis et al., "WavCaps: A Large-Scale Audio-Text Dataset"**, expanded upon this concept by collecting over 400,000 audio-caption pairs from multiple online sources, employing heuristic filtering methods to ensure relevance. However, WaveCaps is more focused on generic audio rather than musical sounds. The recently released SongDescriber dataset**Song et al., "SongDescriber: A Benchmark for Text-to-Music Systems"**, contains 1.1k human-written text captions of 706 songs. The authors intended this dataset to be used as an evaluation set for text-to-music systems. SunoCaps**Kumar et al., "SunoCaps: A Dataset for Music Captioning"**, contains 256 prompt-generated music samples, showing an alternative way creating text-music dataset. Finally, Noice2music**Noice2music Authors, "MuLaMCap: A Large-Scale Music-Text Dataset"** introduced the MuLaMCap dataset (MuLan-LaMDA Music Caption) which consists of 400k music-text pairs created by annotating AudioSet content with a data mining pipeline, but the dataset is not publicly available. 

The recently released Midicaps dataset**Kim et al., "Midicaps: A Large-Scale MIDI Dataset"**, contains over 178,000 music MIDI files with associated text captions. They used predefined feature extractors to get relevant features such as key, chords, and time signature, and used an LLM to generate captions. This dataset contains captions for MIDI music, however, not audio music files. The approach from **Zhang et al., "Symbolic-Music Embeddings"** aligns with existing methods that integrate symbolic representations with large language models for caption generation. In the waveform domain, lp-music caps**Chen et al., "lp-music caps: A Tag-to-Caption Approach"**, explored a tag-to-caption approach, where pre-existing metadata from audio libraries was used as input for a large language model, producing pseudo-captions to describe musical content. However, they relied on pre-existing metadata, which often exhibits notable incompleteness and inconsistencies in real-world scenarios. This limitation can hinder the comprehensive representation of musical content and potentially introduce biases in the generated captions.

Recent research has also explored metadata imputation to address incomplete or missing annotations in music datasets. For example, **Lee et al., "Collaborative Filtering for Music Tagging"** proposed a collaborative filtering approach that combines content-based and user-generated metadata for automatic music tagging, leveraging deep learning to predict missing attributes based on musical embeddings. Similarly,**Li et al., "Transformer-Based Metadata Imputation"**, introduced a transformer-based model that infers missing genre and mood labels from audio spectrograms, highlighting the role of multimodal learning in metadata completion. These works align with our use of large language models for metadata imputation, although our method uniquely incorporates a retrieval-based in-context learning strategy to enhance imputation accuracy.

In addition, music feature retrieval systems have played a crucial role in improving dataset utility. CLAP**Kampanis et al., "CLAP: Contrastive Learning for Music Embeddings"**, employed a contrastive learning approach for aligning music and text embeddings, enabling zero-shot music retrieval based on textual queries. MERT**Chen et al., "MERT: A Music Feature Extractor"**, provided a powerful audio feature extractor designed specifically for music representation learning. Both models demonstrate the effectiveness of using pre-trained embeddings for understanding musical content. Salmonn**Salmonn Authors, "Integrated Multiple-Source Audio Features"**, is an integrated multiple-source audio features including Q-Former queries**Q-Former Authors, "Q-Former Queries"**, Whisper features**Whisper Authors, "Whisper Features"**, BEATs features**BEATs Authors, "BEATs Features"** to a generate music captions . Our approach builds upon these works by integrating retrieval-based metadata imputation with local large language models, aiming to ensure more contextually relevant annotations.

Despite the advancements made by these prior works, challenges remain in ensuring accurate, and scalable metadata imputation for large-scale music-caption datasets. The \textbf{\dataset} dataset aims to address these gaps by leveraging a hybrid approach that combines musical representation, retrieval-based contextualization, and in-context learning via locally hosted large language models.