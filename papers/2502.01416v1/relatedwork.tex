\section{Background and Related Works}
\subsection{The Static Schrödinger Bridge Problem}
Consider two distributions $p_0, p_1 \in \mathcal{P}(\mathcal{X})$ and all distributions $q\in\mathcal{P}(\mathcal{X}^{2})$ whose marginal distributions are $p_0,p_1$, respectively. The set of such distributions $\Pi(p_0, p_1) \subset \mathcal{P}(\mathcal{X}^2)$ is called the set of \emph{transport plans}. In addition, suppose we are given a reference distribution $q^{\text{ref}} \in \mathcal{P}(\mathcal{X}^2)$.

\emph{The Static Schrödinger Bridge (SB) problem} \cite{schrodinger1931umkehrung, leonard2013survey} consists of finding the transport plan $q \in \Pi(p_0, p_1)$ closest to $q^{\text{ref}}$ in terms of the Kullback–Leibler (KL) divergence:
\begin{equation}
    \label{eq:static_sb}
    q^*(x_0, x_1) = \argmin_{q \in \Pi(p_0, p_1)}\text{KL}(q(x_0, x_1)||q^{\text{ref}}(x_0, x_1)),
\end{equation}
With mild assumptions on components of the problem ($\mathcal{X}, p_0, p_1, q^{\text{ref}}$), the solution $q^*$ to this problem uniquely exists; it is called the static SB.

Notably, the static SB problem is equivalent to another well-celebrated problem -- the \emph{Entropic Optimal Transport} \citep[EOT]{cuturi2013sinkhorn}. Indeed, \eqref{eq:static_sb} can be written as
\begin{eqnarray}
    \min_{q \in \Pi(p_0, p_1)}  \mathbb{E}_{q(x_0, x_1)}\log \frac{q(x_0,x_1)}{q^{\text{ref}}(x_0,x_1)}
    =
    \nonumber
    \\
    \min_{q \in \Pi(p_0, p_1)}\big\{ \mathbb{E}_{q(x_0, x_1)}\underbrace{\left[-\log q^{\text{ref}}(x_0, x_1)\right]}_{\defeq c(x_0,x_1)} - H(q)\big\}=
    \nonumber
    \\
    \min_{q \in \Pi(p_0, p_1)} \left\{ \mathbb{E}_{q(x_0, x_1)}c(x_0,x_1) - H(q)\right\}.
    \label{eq:eot}
\end{eqnarray}

where $H(q)$ denotes the entropy of transport plan $q(x_0, x_1)$ and $c(x_0,x_1)$ is transport cost function.

\subsection{Practical Learning Setup of SB}
\label{sec:background-setup-generative}

Over the last decade, researchers have approached SB/EOT problems in various studies because of their relevance to real-world tasks \cite{peyre2019computational,gushchin2023building}. In our paper, we consider the following learning setup, which is usually called the \textit{generative} setup.

We assume that a learner is given empirical datasets $\{x^m_0\}_{m=1}^M \subset \mathcal{X}$ and $\{x^k_1\}_{k=1}^K \subset \mathcal{X}$, which are i.i.d. samples from unknown data distributions $p_0$ and $p_1$, respectively. The goal is to leverage these samples to find a solution $\widehat{q}\approx q^{*}$ to the SB problem \eqref{eq:eot} between the distributions $p_0, p_1$. The solution should permit the \textbf{out-of-sample estimation}, i.e., for any $x_{0}^{\text{new}}$, one should be able to generate new $x_{1}^{\text{new}}\sim \widehat{q}(x_1|x_{0}^{\text{new}})$.

In the related literature, this setup is mainly explored in the context of unpaired (unsupervised) domain translation. In this task, the datasets consist of samples from two different data distributions (domains), and the goal is to learn a transformation from one domain to the other \citep[Figure 2]{zhu2017unpaired}. The problem is inherently ill-posed because theoretically, there may be multiple possible transformations. In many applications of unpaired learning, it is crucial to preserve semantic information during the translation, for example, the image content in image-to-image translation. Therefore, SB and EOT are suitable tools for this task as they allow controlling the properties of the learned translation by selecting the reference distribution $q^{\text{ref}}$ in \eqref{eq:static_sb} or the transport cost $c$ in \eqref{eq:eot}. Over the last several years, many such SB/EOT methods for unpaired learning have been developed, see \citep{gushchin2023building} for a survey.

\subsection{Discrete and Continuous State Space $\mathcal{X}$ in SB}
\label{sec:bg-space}

Most methods \cite{mokrov2023energy,de2021diffusion,vargas2021solving,gushchin2023entropic,gushchin2024adversarial,korotin2024light,gushchin2024light,shi2023diffusion,liu2022deep,chen2022likelihood} use neural networks to approximate $q^{*}$ and \textit{specifically} focus on solving SB in \textbf{continuous state spaces}, e.g., ${\mathcal{X}=\mathbb{R}^{D}}$. This allows us to apply SB to many unpaired translation problems, e.g., the above-mentioned image-to-image translation or biological tasks related to the analysis and modeling of the single-cell data \cite{pariset2023unbalanced,tong2024simulation}.

Despite advances in computational SB methods, significant challenges remain when adapting these generative approaches to \textbf{discrete state spaces} $\mathcal{X}$:
\begin{enumerate}[leftmargin=*]
    \item Their underlying methodological principles are mostly incompatible with discrete spaces $\mathcal{X}$. For example, \cite{shi2023diffusion,gushchin2023entropic,vargas2021solving,liu2022deep} use stochastic differential equations (SDE) which are not straightforward to generalize and use in discrete spaces; \citep{mokrov2023energy} heavily relies on MCMC sampling from unnormalized density which is also a separate challenge for large discrete spaces $\mathcal{X}$; \citep{gushchin2024light,korotin2024light,gushchin2024adversarial} theoretically work only for the EOT problem with the quadratic cost on $\mathcal{X}=\mathbb{R}^{D}$, etc. 
    \item Extending any generative modeling techniques to discrete data is usually a challenge. For example, models such as GANs \citep{goodfellow2014generative} require backpropagation through the generator -- for discrete data is usually done via heuristics related to the Gumbel trick \citep{jang2017categorical}; flow matching methods \cite{liu2022flow} can be used for discrete data \citep{gat2024discrete} but require numerous methodological changes, etc.
\end{enumerate}

At the same time, a significant portion of modern data is inherently discrete (recall \wasyparagraph\ref{sec:intro}). Despite such data's prevalence, Schrödinger Bridges's framework for discrete spaces remains underdeveloped, which motivates our focus on addressing this gap. 

We assume that the state space $\mathcal{X}$ is discrete and represented as $\mathcal{X}=\mathbb{S}^{D}$. Here $\mathbb{S}$ is a finite set and, for convenience, we say that it is the space of categories, e.g., $\mathbb{S}=\{1,2,\dots, S\}$. One may also consider $\mathcal{X}=\mathbb{S}_{1}\times \dots \times \mathbb{S}_{D}$ for $D$ categorical sets. This does not make any principal difference, so we use $\mathbb{S}_{1}=\dots=\mathbb{S}_{D}$ to keep the paper exposition simple.

\paragraph{Discrete EOT methods.} We would like to mention, for the sake of completeness, that there is a broad area of research known as discrete EOT, which might appear to be closely related to our work. It includes, e.g., the well-celebrated Sinkhorn algorithm \citep{cuturi2013sinkhorn} and gradient-based methods \cite{dvurechensky2018computational,dvurechenskii2018decentralize}. However, such algorithms \textbf{are not relevant} to our work as they consider a different to the generative setting (\wasyparagraph\ref{sec:background-setup-generative}) and target different problems. Specifically, discrete EOT assumes that the available data samples are themself discrete distributions, i.e., $p_0 =\frac{1}{M}\sum^M_{m=1}\delta_{x_0^m},$ $p_1 = \frac{1}{K}\sum^K_{k=1}\delta_{x^k_0}$ (the weights be may not equal), and the goal is to find a bi-stochastic matrix $\in\mathbb{R}^{M\times K}$ (a.k.a. the discrete EOT plan) which optimally matches the given samples. Since this matrix is a discrete object, such methods are called discrete. Works \citep{hutter2021minimax, pooladian2021entropic, manole2021plugin, deb2021rates} aim to advance discrete EOT methods to be used in generative setups by providing out-of-sample estimators. However, they work only for continuous state space $\mathcal{X}=\mathbb{R}^{D}$. It remains an open question whether discrete solvers can be adapted for generative scenarios in discrete space $\mathcal{X}=\mathbb{S}^{D}$.

\subsection{From Static to Dynamic SB Problems}
The static SB problem \eqref{eq:static_sb} can be thought of as a problem of finding a stochastic process acting at times $t=0,1$. Usually, one considers an extension of this problem by incorporating additional time moments \cite{de2021diffusion,gushchin2024adversarial}. Let us introduce $N \geq 1$ intermediate time points $0 = t_0 < t_1 < \dots < t_N < t_{N+1} = 1$, extending $q$ to these moments. Consequently, $q$ becomes a process over the states at all time steps, i.e., $q \in \mathcal{P}(\mathcal{X}^{N+2})$. Similarly to the static formulation \eqref{eq:static_sb}, let us given marginal distributions $p_0,p_1 \in \mathcal{P}(\mathcal{X})$ with a reference process $q^{\text{ref}}\in\mathcal{P}(\mathcal{X}^{N+2})$. Then the \emph{dynamic Schrödinger Bridge} problem is
\begin{multline}
    \label{eq:disc_dyn_sb}
    \min_{q \in \Pi_{N}(p_0, p_1)}\text{KL}(q(x_0, x_{\text{in}}, x_1)||q^{\text{ref}}(x_0, x_{\text{in}}, x_1)),
\end{multline}
where $\Pi_{N}(p_0, p_1) \subset \mathcal{P}(\mathcal{X}^{N+2})$ is a set of all discrete-time stochastic processes in which initial and terminal marginal distributions are $p_0$ and $p_1$. In turn, the solution $q^{*}$ to this itself becomes an $\mathcal{X}$-valued stochastic process.

Note that:
\begin{eqnarray}
    \label{eq:disc_disintegration}
    \text{KL}(q(x_0, x_{\text{in}}, x_1)|| q^{\text{ref}}(x_0, x_{\text{in}}, x_1)) = 
    \nonumber
    \\
    \text{KL}(q(x_0, x_1)||q^{\text{ref}}(x_0, x_1)) + 
    \nonumber
    \\ \mathbb{E}_{q(x_0,x_1)} \left[\text{KL}(q(x_{\text{in}}|x_0, x_1)||q^{\text{ref}}(x_{\text{in}}|x_0, x_1)) \right]. 
    \label{kl-reciprocal-zero}
\end{eqnarray}

Since conditional distributions $q(x_{\text{in}}|x_0, x_1)$ can be chosen independently of $q(x_0, x_1)$, we can consider $q(x_{\text{in}}|x_0, x_1) = q^{\text{ref}}(x_{\text{in}}|x_0, x_1)$. It follows that the second term becomes $0$ for every $x_0,x_1$. As a result, we see that the joint distribution $q^{*}(x_0,x_1)$ for time $t=0,1$ of the dynamic SB \eqref{eq:disc_dyn_sb} is the solution to the static SB \eqref{eq:static_sb} for the reference distribution given by the $q^{\text{ref}}(x_0,x_1)$.

At this point, a reader may naturally wonder: \textit{why does one consider the more complicated Dynamic SB, especially taking into account that it boils down to simpler Static SB}?

In short, the dynamic solution adds additional properties for $q^{*}$ which can be efficiently exploited for designing computational algorithms for SB. In fact, \textbf{most} of the computational methods listed at the beginning of \wasyparagraph\ref{sec:bg-space} operate with the dynamic SB formulation. While some methods \cite{de2021diffusion,gushchin2024adversarial} consider formulation \eqref{eq:disc_dyn_sb} with discrete time and finite amount $N$ of time moments, \citep{shi2023diffusion,chen2022likelihood,gushchin2024light} work with continuous time $t\in [0,1]$. \textbf{Informally}, one may identify it with discrete time but $N=\infty$. In discussions, we will refer to this case this way in the rest of the paper \textit{to avoid unnecessary objects and notations}. The scope of our paper is exclusively the discrete-time in dynamic SB ($N<\infty$) as it is more transparent and feasible to analyze.

To conclude this section, we introduce an important definition that is specifically relevant to the dynamic SB.

\paragraph{Reciprocal processes.} A process $r \in \mathcal{P}(\mathcal{X}^{N+2})$ is called a reciprocal process with respect to the reference process $q^{\text{ref}}$ if its conditional distributions given the endpoints $x_0, x_1$ match those of the reference process, i.e.:
\begin{equation*}
    r(x_{\text{in}} \mid x_0, x_1) = q^{\text{ref}}(x_{\text{in}} \mid x_0, x_1).
\end{equation*}
    
The set of all reciprocal processes for the reference process $q^{\text{ref}}$ is denoted by $\mathcal{R}^{\text{ref}}(\mathcal{X}^{N+2}) \subset \mathcal{P}(\mathcal{X}^{N+2})$.

\subsection{Iterative Markovian Fitting (IMF) Procedure}
\label{sec:imf}
\begin{table*}[t]
    \centering
    \begin{tblr}{colspec={Q[c,m]|[0.7pt]Q[c,m]|Q[c,m]|Q[c,m]|Q[c,m]}}
        \toprule
         & \SetCell[c=2]{c} {\textbf{Continuous time} \\ ($N=\infty$)} & & \SetCell[c=2]{c,m} {\textbf{Discrete time} \\ ($N<\infty$)} \\
         \cline{2-5}
         & {\textit{Theory} \\ (SB characterization)} & {\textit{Practice} \\ (SB algorithm)} & {\textit{Theory} \\ (SB characterization)} & {\textit{Practice} \\ (SB algorithm)} \\
        \midrule
        {\textbf{Continuous space} \\ $\mathcal{X}=\mathbb{R}^D$} & \SetCell[r=2]{c,m} {Theorem 3.2 \\  \cite{leonard2014reciprocal}} & {DSBM \wasyparagraph 4 \\ \cite{shi2023diffusion}} & {Theorem 3.1 \\ \cite{gushchin2024adversarial}} & {ASBM \wasyparagraph 3.5 \\ \cite{gushchin2024adversarial}}  \\ 
        \cline{1,3-5}
        {\textbf{Discrete space} \\ $\mathcal{X}=\mathbb{S}^D$} &  & {DDSBM \wasyparagraph 3.1 \\ \cite{kim2024discrete}} & \SetCell[c=2]{c,m} \textbf{Our work} (\wasyparagraph\ref{sec-main}) \\
        \bottomrule
    \end{tblr}
    \caption{A summary of SB problem setups and existing (D-)IMF-related results. The table lists theoretical statements characterizing the SB solution (\textit{as the unique both Markovian and reciprocal process between two given distributions}) which allows to apply the IMF (D-IMF) procedure to provably get the SB solution $q^{*}$, see \citep[Theorem 8]{shi2023diffusion}. The table also lists related computational algorithms.}
    \label{tab:sb_setups}
\end{table*}

In practice, the most commonly considered case of dynamic SB is when $q^{\text{ref}}\in\mathcal{M}(\mathcal{X}^{N+2})\subset \mathcal{P}(\mathcal{X}^{N+2})$, i.e., $q^{\text{ref}}$ is a \textit{Markovian process}. In this case, the solution $q^{*}$ to SB is also known to be a Markovian process. This feature motivated the researchers to develop the \textit{Iterative Markovian Fitting} (IMF) procedure for solving SB based on Markovian and reciprocal projections of stochastic processes.

Originally, the procedure \citep{peluchetti2023diffusion,shi2023diffusion} was considered the continuous time $(N=\infty)$, but recently it has been extended to the finite amount of time moments \citep{gushchin2024adversarial}, i.e., $N<\infty$. We recall their definitions of the projections for finite $N$. In this case, the procedure is called the \textbf{D-IMF} (discrete-time IMF).

\paragraph{Reciprocal projection.} Consider a process $q\!\in\! \mathcal{P}(\mathcal{X}^{N+2})$. Then the reciprocal projection $\text{proj}_{\mathcal{R}^{\text{ref}}}(q)$ with respect to the reference process $q^{\text{ref}}$ is a process given by:
\begin{equation}
    \label{eq:recip_proj}
    \left[proj_{\mathcal{R}^{\text{ref}}}(q)\right](x_0, x_{\text{in}}, x_1) = q^{\text{ref}}(x_{\text{in}}| x_0, x_1)q(x_0, x_1)
    \nonumber.
\end{equation}

\paragraph{Markovian projection.} Consider a process ${q\!\in \!\mathcal{P}(\mathcal{X}^{N+2})}$. Then the Markovian projection $\text{proj}_{\mathcal{M}}(q)$ is given by:
\begin{multline}
    \left[proj_{\mathcal{M}}(q)\right](x_0, x_{\text{in}}, x_1) = \\ = \underbrace{q(x_0)\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}})}_{\text{forward representation}} = \\ = \underbrace{q(x_1)\prod_{n=1}^{N+1}q(x_{t_{n-1}}|x_{t_{n}})}_{\text{backward representation}}
    \label{eq:markov_proj}
\end{multline}

The reciprocal projection obviously preserves the joint distribution $q(x_0,x_1)$ of a process at time moments $t=0,1$. The Markovian projection, in general, alters $q(x_0,x_1)$ but preserves the joint distributions $\{q(x_{t_n},x_{t_{n-1}})\}_{n=1}^{N+1}$ at neighboring time moments and the marginal distributions $q(x_{t_{n}})$.

\textbf{The D-IMF procedure} is initialized with any process $q^0 \in \Pi_{N}(p_0, p_1)$. Then the procedure alternates between reciprocal $proj_{\mathcal{R}^{\text{ref}}}$ and Markovian $proj_{\mathcal{M}}$ projections:
\begin{equation}
    \label{eq:d_imf}
    \begin{gathered}
        q^{2l+1} = proj_{\mathcal{R}^{\text{ref}}}\left(q^{2l}\right),
        \\
        q^{2l+2} = proj_{\mathcal{M}}\left(q^{2l+1}\right).
    \end{gathered}
\end{equation}
Since both the Markovian and reciprocal projections preserve marginals $p_0,p_1$ at times $t=0,1$, respectively, we have that each $q^{l}\in\Pi_{N}(p_0,p_1)$. In certain configurations of $N$, $\mathcal{X}$, $q^{\text{ref}}$, IMF provably converges to the dynamic SB $q^{*}$ in KL, i.e., $\lim_{l\rightarrow\infty}\KL{q^{l}}{q^{*}}=0$. Specifically, the convergence easily follows from the generic proof argument in \citep[Theorem 8]{shi2023diffusion} \textit{as soon it is known that $q^{*}$ is the unique process in $\Pi_{N}(p_0,p_1)$ that is both Markovian and reciprocal}. We provide Table \ref{tab:sb_setups} summarizing the configurations for which this \textbf{characterization} of SB is known. We also list the related practical algorithms which implement the (D-)IMF procedure.

Finally, we would like to emphasize that the \textit{convergence rate of (D-)IMF procedure notably depends on the number $N$ of time steps}. In fact, for each $N$ it is its own separate procedure with different Markovian projection \eqref{eq:markov_proj}, see \citep[Figure 6a]{gushchin2024adversarial}.

\subsection{Object of Study}
As it is clear from Table \ref{tab:sb_setups}, for the setup with the discrete space $\mathcal{X}=\mathbb{S}^{D}$ and finite amount of time moments $N<\infty$, there is still no theoretical guarantee that the SB is the unique Markovian and reciprocal process. This leaves a large gap in D-IMF usage in this case, and we close it in our paper. 

At the same time, we note that there is a very recent IMF-based algorithm DDSBM \cite{kim2024discrete} for the discrete state space $\mathcal{X}$ but continuous time ($N=\infty$). However, since working with continuous time is infeasible in practice, the authors discretize the time grid to large finite $N$. Due to this, in fact, the authors apply the D-IMF procedure, although it still lacks any theoretical ground in this case. In contrast, our work shows that \textit{theoretically} even $N=1$ is enough.