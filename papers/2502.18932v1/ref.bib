@article{shin2019sparse,
  title={Sparse depth enhanced direct thermal-infrared SLAM beyond the visible spectrum},
  author={Shin, Young-Sik and Kim, Ayoung},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={3},
  pages={2918--2925},
  year={2019},
  publisher={IEEE}
}
@article{chen2022eil,
  title={EIL-SLAM: Depth-enhanced edge-based infrared-LiDAR SLAM},
  author={Chen, Wenqiang and Wang, Yu and Chen, Haoyao and Liu, Yunhui},
  journal={Journal of Field Robotics},
  volume={39},
  number={2},
  pages={117--130},
  year={2022},
  publisher={Wiley Online Library}
}
@InProceedings{Buemi_2021_ICCV,
    author    = {Buemi, Antonio and Bruna, Arcangelo and Petinot, Sylvain and Roux, Nicolas},
    title     = {ORB-SLAM With Near-Infrared Images and Optical Flow Data},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {10},
    year      = {2021},
    pages     = {1799-1804}
}
@inproceedings{li2017line,
  title={Line Features of Detail Enhancement Thermal Infrared Image for SLAM},
  author={Li, Fan and Zhang, XueZhao and Zhang, YaoHui and Song, HeLun},
  booktitle={MATEC Web of Conferences},
  volume={139},
  pages={00211},
  year={2017},
  organization={EDP Sciences}
}
@ARTICLE{9440682,
  author={Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and M. Montiel, José M. and D. Tardós, Juan},
  journal={IEEE Transactions on Robotics}, 
  title={ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM}, 
  year={2021},
  volume={37},
  number={6},
  pages={1874-1890},
  keywords={Simultaneous localization and mapping;Computer vision;Feature extraction;Inertial navigation;Robustness;Optimization;Computer vision;inertial navigation;simult- aneous localization and mapping},
  doi={10.1109/TRO.2021.3075644}}
@article{Mur_Artal_2017,
   title={ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras},
   volume={33},
   ISSN={1941-0468},
   url={http://dx.doi.org/10.1109/TRO.2017.2705103},
   DOI={10.1109/tro.2017.2705103},
   number={5},
   journal={IEEE Transactions on Robotics},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Mur-Artal, Raul and Tardos, Juan D.},
   year={2017},
   month=oct, pages={1255–1262} }

@article{bruno2021lift,
  title={LIFT-SLAM: A deep-learning feature-based monocular visual SLAM method},
  author={Bruno, Hudson Martins Silva and Colombini, Esther Luna},
  journal={Neurocomputing},
  volume={455},
  pages={97--110},
  year={2021},
  publisher={Elsevier}
}
@article{CHOI2023222,
title = {A sensor fusion system with thermal infrared camera and LiDAR for autonomous vehicles and deep learning based object detection},
journal = {ICT Express},
volume = {9},
number = {2},
pages = {222-227},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.12.016},
author = {Ji Dong Choi and Min Young Kim},
keywords = {Sensor fusion, LiDAR, Thermal infrared camera, Autonomous vehicles, Object detection, Convolution neural network},
abstract = {Vision, Radar, and LiDAR sensors are widely used for autonomous vehicle perception technology. Especially object detection and classification are primarily dependent on vision sensors. However, under poor lighting conditions, dazzling sunlight, or bad weather an object might be difficult to be identified with general vision sensors. In this paper, we propose a sensor fusion system that combines a thermal infrared camera and a LiDAR sensor that can reliably detect and identify objects even in environments with poor visibility, such as day or night. The proposed method obtains the external parameters of the two sensors by designing and manufacturing a 3D calibration target to externally calibrate the thermal infrared camera and the LiDAR sensor. To verify the performance, experiments were conducted in day and night environments. The proposed sensor system and fusion algorithm show that it can reliably detect and identify objects even in environments with poor visibility, such as day or night.}
}
@ARTICLE{9804793,
  author={Jiang, Jiajun and Chen, Xingxin and Dai, Weichen and Gao, Zelin and Zhang, Yu},
  journal={IEEE Robotics and Automation Letters}, 
  title={Thermal-Inertial SLAM for the Environments With Challenging Illumination}, 
  year={2022},
  volume={7},
  number={4},
  pages={8767-8774},
  keywords={Simultaneous localization and mapping;Image processing;Thermal noise;Feature extraction;Cameras;Thermal sensors;Lighting;SLAM;localization;visual-inertial SLAM},
  doi={10.1109/LRA.2022.3185385}}
@ARTICLE{10111061,
  author={Chen, Xingxin and Dai, Weichen and Jiang, Jiajun and He, Bin and Zhang, Yu},
  journal={IEEE Robotics and Automation Letters}, 
  title={Thermal-Depth Odometry in Challenging Illumination Conditions}, 
  year={2023},
  volume={8},
  number={7},
  pages={3988-3995},
  keywords={Cameras;Odometry;Feature extraction;Lighting;Thermal sensors;Point cloud compression;Simultaneous localization and mapping;Localization;SLAM},
  doi={10.1109/LRA.2023.3271510}}
@INPROCEEDINGS{10587403,
  author={Lv, Chunming and Li, Leilei and Wei, Ranfeng and Wang, Xia and Zuo, Tao},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Visual-Inertial SLAM Technology Based on Monocular Infrared Camera}, 
  year={2024},
  volume={},
  number={},
  pages={2009-2014},
  keywords={Degradation;Visualization;Simultaneous localization and mapping;Accuracy;Lighting;Cameras;Feature extraction;Infrared Image Processing;Localization;Visual-Inertial Odometry;Pose Estimation},
  doi={10.1109/CCDC62350.2024.10587403}}
@INPROCEEDINGS{7989668,
  author={Chen, Long and Sun, Libo and Yang, Teng and Fan, Lei and Huang, Kai and Xuanyuan, Zhe},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RGB-T SLAM: A flexible SLAM framework by combining appearance and thermal information}, 
  year={2017},
  volume={},
  number={},
  pages={5682-5687},
  keywords={Simultaneous localization and mapping;Cameras;Lighting;Feature extraction;Robot vision systems;Three-dimensional displays},
  doi={10.1109/ICRA.2017.7989668}}
@article{shin2022maximizing,
  title={Maximizing self-supervision from thermal image for effective self-supervised learning of depth and ego-motion},
  author={Shin, Ukcheol and Lee, Kyunghyun and Lee, Byeong-Uk and Kweon, In So},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={3},
  pages={7771--7778},
  year={2022},
  publisher={IEEE}
}

@inproceedings{zhou2017unsupervised,
  title={Unsupervised learning of depth and ego-motion from video},
  author={Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2017}
}
@misc{godard2019diggingselfsupervisedmonoculardepth,
      title={Digging Into Self-Supervised Monocular Depth Estimation}, 
      author={Clément Godard and Oisin Mac Aodha and Michael Firman and Gabriel Brostow},
      year={2019},
      eprint={1806.01260},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1806.01260}, 
}
@inproceedings{zhang2023lite,
  title={Lite-mono: A lightweight cnn and transformer architecture for self-supervised monocular depth estimation},
  author={Zhang, Ning and Nex, Francesco and Vosselman, George and Kerle, Norman},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}
@inproceedings{kim2018multispectral,
  title={Multispectral transfer network: Unsupervised depth estimation for all-day vision},
  author={Kim, Namil and Choi, Yukyung and Hwang, Soonmin and Kweon, In So},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
@inproceedings{lu2021alternative,
  title={An alternative of lidar in nighttime: Unsupervised depth estimation based on single thermal image},
  author={Lu, Yawen and Lu, Guoyu},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3833--3843},
  year={2021}
}
@article{shin2021self,
  title={Self-supervised depth and ego-motion estimation for monocular thermal video using multi-spectral consistency loss},
  author={Shin, Ukcheol and Lee, Kyunghyun and Lee, Seokju and Kweon, In So},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={2},
  pages={1103--1110},
  year={2021},
  publisher={IEEE}
}

@INPROCEEDINGS{9156697,
  author={Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={11531-11539},
  keywords={Convolution;Complexity theory;Dimensionality reduction;Kernel;Adaptation models;Computational modeling;Convolutional neural networks},
  doi={10.1109/CVPR42600.2020.01155}}


@InProceedings{10.1007/978-3-031-25555-7_3,
author="V{\"o}disch, Niclas
and Cattaneo, Daniele
and Burgard, Wolfram
and Valada, Abhinav",
editor="Billard, Aude
and Asfour, Tamim
and Khatib, Oussama",
title="Continual SLAM: Beyond Lifelong Simultaneous Localization and Mapping Through Continual Learning",
booktitle="Robotics Research",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="19--35",
abstract="Robots operating in the open world encounter various different environments that can substantially differ from each other. This domain gap also poses a challenge for Simultaneous Localization and Mapping (SLAM) being one of the fundamental tasks for navigation. In particular, learning-based SLAM methods are known to generalize poorly to unseen environments hindering their general adoption. In this work, we introduce the novel task of continual SLAM extending the concept of lifelong SLAM from a single dynamically changing environment to sequential deployments in several drastically differing environments. To address this task, we propose CL-SLAM leveraging a dual-network architecture to both adapt to new environments and retain knowledge with respect to previously visited environments. We compare CL-SLAM to learning-based as well as classical SLAM methods and show the advantages of leveraging online data. We extensively evaluate CL-SLAM on three different datasets and demonstrate that it outperforms several baselines inspired by existing continual learning-based visual odometry methods. We make the code of our work publicly available at http://continual-slam.cs.uni-freiburg.de.",
isbn="978-3-031-25555-7"
}


@INPROCEEDINGS{9709990,
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Emerging Properties in Self-Supervised Vision Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={9630-9640},
  keywords={Training;Image segmentation;Computer vision;Semantics;Layout;Image retrieval;Computer architecture;Representation learning;Recognition and classification;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00951}}
@INPROCEEDINGS{10023305,
  author={Mun, Sung Hwan and Jung, Jee-weon and Han, Min Hyun and Kim, Nam Soo},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Frequency and Multi-Scale Selective Kernel Attention for Speaker Verification}, 
  year={2023},
  volume={},
  number={},
  keywords={Adaptation models;Protocols;Frequency-domain analysis;Conferences;Kernel;speaker verification;selective kernel attention;multi-scale module},
  doi={10.1109/SLT54892.2023.10023305}}

@INPROCEEDINGS{9009796,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  keywords={Training;Estimation;Predictive models;Cameras;Image color analysis;Image reconstruction;Image matching},
  doi={10.1109/ICCV.2019.00393}}

@article{diakogiannis2020resunet,
  title={ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data},
  author={Diakogiannis, Foivos I and Waldner, Fran{\c{c}}ois and Caccetta, Peter and Wu, Chen},
  journal={ISPRS Journal of Photogrammetry and Remote Sensing},
  volume={162},
  pages={94--114},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@INPROCEEDINGS{10209029,
  author={Vödisch, Niclas and Cattaneo, Daniele and Burgard, Wolfram and Valada, Abhinav},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={CoVIO: Online Continual Learning for Visual-Inertial Odometry}, 
  year={2023},
  volume={},
  number={},
  pages={2464-2473},
  keywords={Training;Codes;Machine vision;Network architecture;Multitasking;Real-time systems;Pattern recognition},
  doi={10.1109/CVPRW59228.2023.00245}}

@INPROCEEDINGS{5979949,
  author={Kümmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
  booktitle={2011 IEEE International Conference on Robotics and Automation}, 
  title={G2o: A general framework for graph optimization}, 
  year={2011},
  volume={},
  number={},
  keywords={Optimization;Simultaneous localization and mapping;Sparse matrices;Jacobian matrices;Barium;Linear systems},
  doi={10.1109/ICRA.2011.5979949}}

@INPROCEEDINGS{8100183,
  author={Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Learning of Depth and Ego-Motion from Video}, 
  year={2017},
  volume={},
  number={},
  keywords={Cameras;Training;Pose estimation;Three-dimensional displays;Geometry;Pipelines},
  doi={10.1109/CVPR.2017.700}}
@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}
@article{bian2019unsupervised,
  title={Unsupervised scale-consistent depth and ego-motion learning from monocular video},
  author={Bian, Jiawang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{10.5555/3045118.3045167,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
%pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}
@article{diederik2014adam,
  title={Adam: A method for stochastic optimization},
  author={Diederik, P Kingma},
  journal={(No Title)},
  year={2014}
}
@article{InfiniTAM_ISMAR_2015,
  author = {Kahler, O. and Prisacariu, V.~A. and Ren, C.~Y. and Sun, X. 
        and Torr, P.~H.~S and Murray, D.~W.},
  title = "{Very High Frame Rate Volumetric Integration of Depth Images 
        on Mobile Device}",
  journal = "IEEE Transactions on Visualization and Computer Graphics",
  volume = {22},
  number = {11},
  year = 2015
}
@article{bao2022using,
  title={Using segmentation with multi-scale selective kernel for visual object tracking},
  author={Bao, Feng and Cao, Yifei and Zhang, Shunli and Lin, Beibei and Zhao, Sicong},
  journal={IEEE Signal Processing Letters},
  volume={29},
  pages={553--557},
  year={2022},
  publisher={IEEE}
}
@article{WU2023265,
title = {Improving autonomous detection in dynamic environments with robust monocular thermal SLAM system},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {203},
pages = {265-284},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271623002174},
author = {Yuzhen Wu and Lingxue Wang and Lian Zhang and Yu Bai and Yi Cai and Shuigen Wang and Yanqiu Li},
keywords = {Monocular thermal SLAM, Scene-based denoising chain, Dynamic environment, Pose estimation, Graph optimization, Shutterless thermal camera},
abstract = {Thermal SLAM outperforms visual SLAM under conditions of nighttime, low visibility (e.g., fog, smoke, and dust), and direct glare. However, the unique non-uniformity correction for thermal imaging will cause data interruption, and the weak texture of the thermal image itself will lead to poor localization accuracy. To overcome these limitations, this paper presents a monocular thermal camera-based simultaneous localization and mapping (SLAM) system that can be used for high-precision and robust localization in challenging dynamic environments with visual degradation. We also conducted extensive experiments on small-scale indoor and outdoor sequences and on large-scale driving sequences, totaling over 180,000 real-world thermal images. Our results show that the developed MonoThermal-SLAM system can achieve camera localization and sparse structured map reconstruction in the face of visual degradation and moving objects in dynamic environments without the need for other sensors. Compared with state-of-the-art monocular SLAM systems, MonoThermal-SLAM was the only system that successfully tracked all sequences to offer unparalleled robustness and higher positioning accuracy. Relative position error (RPE) was less than 0.1 m of the ground-truth trajectory in both the indoor and outdoor sequences. Absolute trajectory error (ATE) improves averagely over 68.57% compared to other best-performing SLAM systems in driving sequences. The average relative error of tracking path length to the ground-truth was less than 5.19%. The superior performance of MonoThermal-SLAM is due to the introduction of several key innovations. First, the developed real-time scene-based denoising chain not only avoids thermal camera data interruptions but also significantly improves the thermal image quality and widens the boundaries of the application of visual SLAM systems for thermal images. Second, the combination of epipolar constraints and semantic segmentation reduces the interference of dynamic objects and improves the robustness of thermal SLAM in dynamic scenes. Third, the implemented SLAM system based on point and line features consists of well-designed initialization, tracking, local mapping, and loop closing to overcome the drawbacks of poor spatial texture distribution of thermal images. Therefore, the system can be used as a novel positioning solution to replace expensive commercial navigation systems, especially in challenging urban dynamic environments with changing light and low visibility in the air.}
}
@ARTICLE{9047170,
  author={Li, Ruihao and Wang, Sen and Gu, Dongbing},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={DeepSLAM: A Robust Monocular SLAM System With Unsupervised Deep Learning}, 
  year={2021},
  volume={68},
  number={4},
  pages={3577-3587},
  keywords={Simultaneous localization and mapping;Visualization;Training;Three-dimensional displays;Optimization;Pose estimation;Depth estimation;machine learning;recurrent convolutional neural network (RCNN);simultaneous localization and mapping (SLAM);unsupervised deep learning (DL)},
  doi={10.1109/TIE.2020.2982096}}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}
@ARTICLE{10048516,
  author={Wang, Yu and Chen, Haoyao and Liu, Yufeng and Zhang, Shiwu},
  journal={IEEE Robotics and Automation Letters}, 
  title={Edge-Based Monocular Thermal-Inertial Odometry in Visually Degraded Environments}, 
  year={2023},
  volume={8},
  number={4},
  pages={2078-2085},
  keywords={Image edge detection;Cameras;Feature extraction;Visualization;State estimation;Robustness;Noise measurement;Localization;search and rescue robots;visual-inertial SLAM},
  doi={10.1109/LRA.2023.3246381}}
