\section{Complexity of equilibria in symmetric min-max optimization}


This section characterizes the complexity of computing symmetric first-order Nash equilibria\\ (\Cref{def:FONE}) in symmetric min-max optimization problems in the sense of~\Cref{def:symmetric}; namely, when $f(\vx, \vy) = - f(\vy, \vx)$ for all $(\vx, \vy) \in \calX \times \calY$ and $\calX = \calY$. 


\subsection{Problem definitions and hardness results for symmetric equilibria}
\label{sec:symmetric}

Given a continuously differentiable function $f : \mathcal{D} \to \R$, we set $F_{\textrm{GDA}}:\mathcal{D} \to \mathcal{D}$ to be
$$F_{\textrm{GDA}}(\vx,\vy) \defeq \prod_{\mathcal{D}} \left[\vx - \nabla_{\vx}f(\vx,\vy),\vy + \nabla_{\vy}f(\vx,\vy)\right] \textrm{ for }(\vx,\vy)\in \mathcal{D},$$
the norm of which measures the fixed-point gap and corresponds to the update rule of GDA with stepsize equal to one; we recall that Player $\vx$ is the minimizer, while Player $\vy$ is the maximizer. The domain $\mathcal{D}$ is a compact subset of $\R^d$ for some $d \in \mathbb{N}$. Moreover, the projection operator $\prod$ is applied jointly on $\mathcal{D}$.\footnote{This is referred to as the ``safe'' version of GDA because it ensures that the mapping always lies in $\mathcal{D}$. One could also project independently on $\mathcal{D}(\vy)=\{\vx': (\vx',\vy)\in\mathcal{D}\}$ and $\mathcal{D}(\vx)=\{\vy': (\vx,\vy')\in\mathcal{D}\}$; see \citet{DSZ21} for further details and the polynomial equivalence for finding fixed points for both versions.} When $\mathcal{D}$ can be expressed as a \emph{Cartesian} product $\calX\times\calY$, the domain set is called \emph{uncoupled} (and the projection can happen independently), otherwise it is called \emph{coupled/joint}.

We begin by introducing the problem of computing fixed points of gradient descent/ascent (GDA) for domains expressed as the Cartesian product of polytopes, modifying the computational problem $\gdaFixed$ introduced by~\citet{DSZ21}.

\begin{nproblem}[\gdaFixed]
  \textsc{Input:} 
  \begin{itemize}
  \item Precision parameter $\epsilon > 0$ and smoothness parameter $L$,
  \item Polynomial-time Turing
  machine $\calC_f$ evaluating a $L$-smooth function $f : \mathcal{X} \times \mathcal{Y} \to \R$ and its gradient
  $\nabla f: \calX\times \calY \to \R^{d}$, where
   $\mathcal{X} = \{\vx:\matA_x \vx \leq \vecb_x\}$ and $\mathcal{Y} = \{\vy:\matA_y \vy \leq \vecb_y\}$ are nonempty, bounded polytopes described by input matrices
  $\matA_x \in \R^{m_x \times d_x}$, $\matA_y \in \R^{m_y \times d_y}$ and vectors $\vecb_x \in \R^{m_x}, \vecb_y \in \R^{m_y}$, with $d \defeq d_x + d_y$.
\end{itemize}
  \noindent \textsc{Output:} A point
  $(\vxstar,\vystar)\in \calX\times\calY$ such that
  $\norm{(\vx^*, \vy^*) - F_{GDA}(\vx^*,\vy^*)}_2 \leq \epsilon$.
\end{nproblem}

Based on $\gdaFixed$, we introduce the problem $\symgdaFixed$, which captures the problem of computing \emph{symmetric} (approximate) fixed points of GDA for symmetric min-max optimization problems. We would like to note that we define our computational problems as promise problems.
  
\begin{nproblem}[\symgdaFixed]
 \textsc{Input:}    \begin{itemize}
  \item Precision parameter $\epsilon > 0$ and smoothness parameter $L$, 
  \item Polynomial-time Turing
  machine $\calC_f$ evaluating a $L$-smooth, antisymmetric function $f : \mathcal{X} \times \mathcal{X} \to \R$ and its gradient
  $\nabla f: \calX\times \calX \to \R^{2d}$, where
  $\mathcal{X} = \{\vx:\matA \vx \leq \vecb\}$ is a nonempty, bounded polytope described by an input matrix
  $\matA \in \R^{m \times d}$ and vector $\vecb \in \R^{m}$.
\end{itemize}

  \noindent \textsc{Output:} A point
  $(\vxstar,\vxstar)\in \calX\times\calX$ such that
  $\norm{(\vx^*, \vx^*) - F_{GDA}(\vx^*,\vx^*)}_2 \leq \epsilon$.
\end{nproblem}

We start by showing that the problem $\symgdaFixed$ also lies in $\PPAD$; the fact that $\gdaFixed$ is in $\PPAD$---even under coupled domains---was shown to be the case by~\citet{DSZ21}.

\begin{lemma}\label{lem:membership}
  $\symgdaFixed$ is a total
search problem and lies in \PPAD.
  \end{lemma}
\begin{proof}  
We first define the function (as in~\Cref{lem:exists}) $M: \calX \to \calX$ as
\begin{equation*}
M(\vx') \defeq \prod_{\calX} \left[\vx' - \nabla _{\vx}  f(\vx,\vy)\Big|_{(\vx,\vy)=(\vx',\vx')} \right],
\end{equation*}
where we recall that $\Pi$ is the projection operator on $\calX.$ Assuming that the input function $f$ is $L$-smooth, it follows that $M(\vx')$ is $(L+1)$-Lipschitz. Furthermore, projecting on the polytope $\calX$ takes polynomial time, and so $M$ is polynomial-time computable. As a result, we can use~\citet[Proposition 2, part 2]{Etessami10:On} (see also~\citet[Proposition D.1]{Fearnley23:Complexity}), where it was shown that finding an $\epsilon$-approximate fixed point of a Brouwer function that is efficiently computable and continuous, when the domain is a bounded polytope, lies in \PPAD.
\end{proof}

Having established that $\symgdaFixed$ belongs in \PPAD, we now prove the first main hardness result of this section.

\begin{theorem}[Complexity for symmetric equilibrium]\label{thm:symmetricminmax} 
$\symgdaFixed$ is \PPAD-complete, even for quadratic functions.
\end{theorem}
\begin{proof}
We $P$-time reduce the problem of finding approximate symmetric NE in two-player symmetric games to $\symgdaFixed$.  
Given any two-player symmetric game with payoff matrices $(\mat{R},\mat{R}^{\top})$ of size $n\times n$, we set 
\begin{equation}\label{eq:matrices}
\mat{A} \defeq \frac{1}{2} \left(\mat{R}+\mat{R}^{\top}\right) \textrm{ (symmetric matrix) and } 
\mat{C} \defeq \frac{1}{2}\left(\mat{R}-\mat{R}^{\top}\right) \textrm{(skew-symmetric matrix)}.
\end{equation}

\noindent We define the \emph{quadratic}, antisymmetric function
\begin{equation}
    \label{eq:hard-quad}
 f(\vx,\vy) \defeq \frac{1}{2} \langle \vy, \mat{A}\vy \rangle - \frac{1}{2} \langle \vx, \mat{A}\vx \rangle + \langle \vy, \mat{C}\vx \rangle   
\end{equation}
with domain $\Delta^n \times \Delta^n$. Indeed, to see that $f$ is antisymmetric, one can observe that $$f(\vy,\vx) = \frac{1}{2} \langle \vx, \mat{A} \vx \rangle - \frac{1}{2} \langle \vy, \mat{A} \vy \rangle + \langle \vx, \mat{C} \vy \rangle = \frac{1}{2} \langle \vx, \mat{A}\vx \rangle -\frac{1}{2} \langle \vy, \mat{A} \vy \rangle - \langle \vy, \mat{C}^{\top} \vx \rangle = -f(\vx,\vy).$$
Assuming that all entries of $\mat{R}$ lie in $[-1,1],$ it follows that the singular values of $\mat{A}$ and $\mat{C}$ are bounded by $n.$ As a result $f$ and $\nabla_{\vx} f = -\mat{A}\vx-\mat{C}\vy, \nabla_{\vy} f = \mat{A}\vy +\mat{C}\vx$ are polynomial time computable and continuous, and $\nabla_{\vx} f, \nabla_{\vy}f$ are $L$-Lipschitz for $L \leq 2n,$ thus $f$ is $4n$-smooth.

We assume $\vx$ is the minimizer and $\vy$ is the maximizer, and let $(\vx^*,\vx^*)$ be an $\epsilon$-approximate fixed point of GDA. We shall show that $(\vx^*,\vx^*)$ is an $4n\epsilon$-approximate NE of the symmetric two-player game $(\mat{R},\mat{R}^{\top})$. Since $(\vx^*,\vx^*)$ is an $\epsilon$-approximate fixed point of GDA, we can use~\Cref{lem:approxsmooth} (\Cref{sec:proofs2}) and obtain the following variational inequalities:
\begin{equation*}
\max_{\vx^*+\bm{\delta} \in \Delta^n, \norm{\bm{\delta}}_2\leq 1} \bm{\delta}^{\top} (\mat{A}\vx^*+\mat{C}\vx^*)\leq \epsilon\left(2n+1\right),
\end{equation*}
implying that (since the diameter of $\Delta^n$ is $\sqrt{2}$ in $\ell_2$)
\begin{equation}
\tag{VI for NE}\label{eq:VIforNE}
\langle \vx - \vx^*,(\mat{A}+\mat{C})\vx^*  \rangle \leq \sqrt{2}\epsilon\left(2n+1\right) \textrm{ for any }\vx\in\Delta^n. 
\end{equation}

Now, we observe that \eqref{eq:VIforNE} implies that $(\vx^*,\vx^*)$ is a $\sqrt{2}\epsilon\left(2n+1\right)$-approximate symmetric NE in the two-player symmetric game with payoff matrices $(\mat{A}+\mat{C},\mat{A}-\mat{C})$ (recall Definition \eqref{def:NE}). Since $\sqrt{2}\epsilon\left(2n+1\right) \leq 4n\epsilon$ for $n\geq 2$, our claim follows.
%\textrm{ or equivalently } \langle \vx - \vx^*,  \rangle \leq \]

By~\Cref{theorem:PPAD_for _symmetric} and~\Cref{lem:membership}, we conclude that $\symgdaFixed$ is \PPAD-complete, even for quadratic functions that are $O(n)$-smooth, $O(n)$-Lipschitz and $\epsilon \leq \nicefrac{1}{n^{1+c}}$, for any $c>0$.
\end{proof}

For symmetric first-order Nash equilibria, the same argument establishes \PPAD-hardness for any $\epsilon \leq \nicefrac{1}{n^c}$, where $c > 0$ (as claimed in~\Cref{theorem:main}). Moreover, leveraging the hardness result of~\citet{Rubinstein16:Settling}, we can also immediately obtain constant inapproximability under the so-called \emph{exponential-time hypothesis (ETH)} for \PPAD---which postulates than any algorithm for solving \textsc{EndOfALine}, the prototypical \PPAD-complete problem, requires $2^{\tilde{\Omega}(n)}$ time.

\begin{corollary}
    \label{cor:constant}
    Computing an $\Theta(1)$-approximate first-order Nash equilibrium in symmetric $n$-dimensional min-max optimization requires $n^{\tilde{\Omega}(\log n)}$ time, assuming ETH for \PPAD.
\end{corollary}

\begin{remark}[Comparison with \citet{DSZ21}] The argument of~\Cref{thm:symmetricminmax} can be slightly modified to imply one of the main results of~\citet{DSZ21}---with simplex constraints instead of box constraints. We provide a simple proof of this fact below (\Cref{thm:simple}). The main idea is to introduce coupled constraints in order to \emph{force symmetry}, that is, constraints of the form $-\delta \leq x_i - y_i \leq \delta$ for all $i \in [n]$, where, if $\epsilon$ is the approximation accuracy, $\delta$ is of order $\Theta\left(\epsilon^{1/4}\right)$. Our result pertaining to symmetric equilibria is stronger in that it accounts for deviations in the whole domain, not merely on the coupled feasibility set. 
\end{remark}

\begin{theorem}[\PPAD-hardness for coupled domains]\label{thm:simple} The problem $\gdaFixed$ is \PPAD-hard when the domain is a joint polytope, even for quadratic functions.
\end{theorem}
\begin{proof}
The proof follows similar steps with \Cref{thm:symmetricminmax}, namely, we $P$-time reduce the problem of finding approximate symmetric NE in two-player symmetric games to $\gdaFixed$ with coupled domains.  
Given a two-player symmetric game with payoff matrices $(\mat{R},\mat{R}^{\top})$ of size $n\times n$, we set 
$\mat{A} \defeq \frac{1}{2} \left(\mat{R}+\mat{R}^{\top}\right)$, 
$\mat{C} \defeq \frac{1}{2}\left(\mat{R}-\mat{R}^{\top}\right)$ and define again the quadratic, antisymmetric function
\begin{equation*}
    %\label{eq:hard-quad}
 f(\vx,\vy) \defeq \frac{1}{2} \langle \vy, \mat{A}\vy \rangle - \frac{1}{2} \langle \vx, \mat{A}\vx \rangle + \langle \vy, \mat{C}\vx \rangle.   
\end{equation*}
Moreover, given a parameter $\delta > 0$ (to be specified shortly), we define the joint  domain of $f$ to be
\begin{equation}\label{eq:polytope}\tag{joint Domain}
\mathcal{D} := \left\{(\vx,\vy) \in \Delta^n \times \Delta^n: -\delta\leq x_i - y_i \leq \delta \textrm{ for all }i\in[n]\right\}.
\end{equation}
Let $(\vx^*,\vy^*)$ be an $\epsilon$-approximate fixed point of GDA. We will show that $\left(\frac{\vx^*+\vy^*}{2},\frac{\vx^*+\vy^*}{2}\right)$ is an $O(\epsilon^{1/4})$-approximate (symmetric) NE of the game $(\mat{R},\mat{R}^{\top})$ for an appropriate choice of $\delta.$ 

\smallskip

We set $\mathcal{D}(\vx^*) = \{\vy:(\vx^*,\vy)\in\mathcal{D}\}$ and 
$\mathcal{D}(\vy^*) = \{\vx:(\vx,\vy^*)\in\mathcal{D}\}.$
In words, $\mathcal{D}(\vx^*)$ and $\mathcal{D}(\vy^*)$ capture the allowed deviations for $\vy$ and $\vx$ respectively. It also holds that $f$ is $G$-Lipschitz continuous with $G=4n$ and also $4n$-smooth (using the same reasoning as in Theorem~\ref{thm:symmetricminmax}). 

\smallskip
\noindent Since $(\vx^*,\vy^*)$ is an $\epsilon$-approximate fixed point of GDA, using~\Cref{lem:safe} (\Cref{sec:proofs2}), the following VIs must hold for some positive constant $K<10$ and $n$ sufficiently large:

\begin{equation}
\label{eq:VIforcoupled}
\begin{array}{cc}
\langle \vx - \vx^*,-\mat{A}\vx^*+\mat{C}^{\top}\vy^*  \rangle \geq -Kn^{3/2}\sqrt{\epsilon} \textrm{ for any }\vx \in \mathcal{D}(\vy^*) \textrm{ and }\\ 
\langle \vy - \vy^*,\mat{A}\vy^*+\mat{C}\vx^*  \rangle \leq Kn^{3/2}\sqrt{\epsilon} \textrm{ for any }\vy \in \mathcal{D}(\vx^*).
\end{array}
\end{equation}

\noindent Let $\overline{\mathcal{D}} = \left\{\vz \in \Delta^n: \left\|\vz-\frac{\vx^*+\vy^*}{2}\right\| _{\infty}\leq \frac{\delta}{2}\right\}.$ By triangle inequality, it follows that 
$\overline{\mathcal{D}} \subseteq \mathcal{D}(\vy^*)\cap\mathcal{D}(\vx^*).$
We express the VIs of \eqref{eq:VIforcoupled} using a single variable $\vz$ and common deviation domain:
\begin{equation*}
\label{eq:VIforcoupledz}
\begin{array}{cc}
\langle \vz - \vx^*,-\mat{A}\vx^*+\mat{C}^{\top}\vy^*  \rangle \geq -Kn^{3/2}\sqrt{\epsilon} \textrm{ and } 
\langle \vz - \vy^*,\mat{A}\vy^*+\mat{C}\vx^*  \rangle \leq Kn^{3/2}\sqrt{\epsilon} \textrm{ for any }\vz \in \overline{\mathcal{D}}.
\end{array}
\end{equation*}
Multiplying the first inequality by $-1/2$ and the second with $1/2$ and adding them up gives
\begin{equation}
\label{eq:touse}
\left\langle \vz - \frac{\vx^*+\vy^*}{2}, (\mat{A}+\mat{C})\frac{\vx^*+\vy^*}{2} \right\rangle \leq \frac{1}{4}\left\langle \vx^*-\vy^*,\mat{A}(\vx^*-\vy^*)\right\rangle + Kn^{3/2}\sqrt{\epsilon}. 
\end{equation}
Since $\vx^*,\vy^* \in \mathcal{D}$, it follows that $\left\langle \vx^*-\vy^*,\mat{A}(\vx^*-\vy^*)\right\rangle \leq n \|\vx^*-\vy^*\|^2_2 \leq n^2 \delta^2$. 
Combining this fact with \eqref{eq:touse}, we conclude that
\begin{equation}
\tag{VImedian}
\label{eq:lastVINE}
\left\langle \vz - \frac{\vx^*+\vy^*}{2}, (\mat{A}+\mat{C})\frac{\vx^*+\vy^*}{2} \right\rangle \leq n^2 \delta^2 + Kn^{3/2}\sqrt{\epsilon} \textrm{ for any }\vz \in \overline{\mathcal{D}}. 
\end{equation}

\eqref{eq:lastVINE} shows that by deviating from $\left(\frac{\vx^*+\vy^*}{2},\frac{\vx^*+\vy^*}{2}\right)$ to some $\vz$ in $\overline{\mathcal{D}}$, the payoff cannot increase by more than $\left(n^2\delta^2 + Kn^{3/2}\sqrt{\epsilon}\right)$ in the two-player symmetric game with matrices $(\mat{R},\mat{R}^{\top})$.

\noindent We consider any pure strategy $\bm{e}_j$ for $j\in[n]$. If $\left\|\bm{e}_j - \frac{\vx^*+\vy^*}{2}\right\|_{\infty} \leq \frac{\delta}{2}$ then $\bm{e}_j \in \overline{\mathcal{D}}$ 
and it is captured by \eqref{eq:lastVINE}. Suppose that 
$\left\|\bm{e}_j - \frac{\vx^*+\vy^*}{2}\right\|_{\infty} > \frac{\delta}{2}$
and consider the point $\vz' \in \overline{\mathcal{D}}$ on the line segment between $\bm{e}_j$ and $\frac{\vx^*+\vy^*}{2}$ that intersects the boundary of $\overline{\mathcal{D}}.$ It holds that $\bm{e}_j - \frac{\vx^*+\vy^*}{2} = c\left(\vz' - \frac{\vx^*+\vy^*}{2} \right)$ for some positive $c \leq \frac{2}{\delta}$ (it cannot be larger because otherwise the infinity norm of the difference between $\bm{e}_j$ and $\frac{\vx^*+\vy^*}{2}$ would exceed one, which is impossible as they both belong to $\Delta^n$). Therefore, 
\begin{equation}
\label{eq:lastlastVINE}
\left\langle \bm{e}_j - \frac{\vx^*+\vy^*}{2}, (\mat{A}+\mat{C})\frac{\vx^*+\vy^*}{2} \right\rangle \leq 2n^2 \delta + \frac{2Kn^{3/2}\sqrt{\epsilon}}{\delta} \textrm{ for any pure strategy }j. 
\end{equation}

From \eqref{eq:lastlastVINE}, we conclude that $\frac{\vx^*+\vy^*}{2}$ is $\left(2n^{2}\delta + \frac{2Kn^{3/2}\sqrt{\epsilon}}{\delta}\right)$-approximate NE of the symmetric two-player game $(\mat{R},\mat{R}^{\top}).$ We choose $\delta = \epsilon^{1/4} n^{-1/4}$ and we get that $\frac{\vx^*+\vy^*}{2}$ is an $O(n^{7/4}\epsilon^{1/4})$-approximate NE for $(\mat{R},\mat{R}^{\top})$, and thus the hardness result holds for $\epsilon$ of order  $O\left(\frac{1}{n^{7+c}}\right)$ for any constant $c>0.$ We note that if instead of an $\epsilon$-approximate fixed point of GDA, we were given an $\epsilon$-approximate First-order NE, then the hardness result would hold for any $\epsilon$ of order $\frac{1}{n^{c}}$ with $c>0.$
\end{proof}

\paragraph{Hardness results for symmetric dynamics}

Another interesting consequence of~\Cref{thm:symmetricminmax} is that it immediately precludes convergence under a broad class of iterative algorithms in general min-max optimization problems. 

\begin{definition}[Symmetric learning algorithms for min-max]
    \label{def:sym-dynamics}
    Let $T \in \mathbb{N}$. A deterministic, polynomial-time learning algorithm $\calA$ proceeds as follows for any time $t \in [T]$. It outputs a strategy as a function of the history $\mathcal{H}^{(t)}$ it has observed so far (where $\mathcal{H}^{(1)} \defeq \emptyset$ ), and then receives as feedback $\vec{g}^{(t)}$. It then updates $\mathcal{H}^{(t+1)} \defeq ( \mathcal{H}^{(t)}, \vec{g}^{(t)})$. 
    
    A \emph{symmetric} learning algorithm in min-max optimization consists of Player $\vx$ employing algorithm $\calA$ with history $\mathcal{H}_x^{(t)} \defeq (\nabla_{\vx} f(\vx^{(t)}, \vy^{(t)}) )_{t=1}^T$, and Player $\vy$ employing the \emph{same} algorithm with history $\mathcal{H}_y^{(t)} \defeq (- \nabla_{\vy} f(\vx^{(t)}, \vy^{(t)}) )_{t=1}^T$.
\end{definition}

(A consequence of the above definition is that both players initialize from the same strategy.) Many natural and well-studied algorithms in min-max optimization adhere to~\Cref{def:sym-dynamics}. Besides the obvious example of gradient descent/ascent, we mention extragradient descent(/ascent), optimistic gradient descent(/ascent), and optimistic multiplicative weights---all assumed to be executed simultaneously. A simple non-example is \emph{alternating} gradient descent(/ascent)~\citep{Wibisono22:Alternating,Bailey20:Finite}, wherein players do not update their strategies simultaneously.

\begin{theorem}
    \label{theorem:sym-dyn}
    No symmetric learning algorithm (per~\Cref{def:sym-dynamics}) can converge to $\epsilon$-first-order Nash equilibria in min-max optimization in polynomial time when $\epsilon = \nicefrac{1}{n^c}$, unless $\PPAD = \P$.
\end{theorem}

Indeed, this is a direct consequence of our argument in~\Cref{thm:symmetricminmax}: under~\Cref{def:sym-dynamics} and the min-max optimization problem~\eqref{eq:hard-quad}, it follows inductively that $\vx^{(t)} = \vy^{(t)}$ and $\mathcal{H}_x^{(t)} = \mathcal{H}_y^{(t)}$ for all $t \in [T]$. But~\Cref{thm:symmetricminmax} implies that computing a symmetric first-order Nash equilibrium is $\PPAD$-hard when $\epsilon = \nicefrac{1}{n^c}$.

Assuming that $\P \neq \PPAD$, \Cref{theorem:sym-dyn}, and in particular its instantiation in team zero-sum games (\Cref{theorem:team-hard}), recovers and significantly generalizes some impossibility results shown by~\citet{kalogiannis2021teamwork} concerning lack of convergence for certain algorithms, such as optimistic gradient descent(/ascent)---our hardness result goes much further, precluding any algorithm subject to~\Cref{def:sym-dynamics}, albeit being conditional.

\subsection{The complexity of non-symmetric fixed points}
\label{sec:nonsymmetric}

An immediate question raised by~\Cref{thm:symmetricminmax} concerns the computational complexity of finding \emph{non-symmetric} fixed points of GDA for symmetric min-max optimization problems. Since totality is not guaranteed, unlike $\symgdaFixed$, we cannot hope to prove membership in \PPAD. In fact, we show that finding a non-symmetric fixed point of GDA is \FNP-hard (\Cref{theorem:non-symmetric}). To do so, we first define formally the computational problem of interest.

\begin{nproblem}[\nsymgdaFixed]
 \textsc{Input:}  \begin{itemize}
   \item Parameters $\epsilon,\delta>0$ and Lipschitz constant $L$ and
  \item Polynomial-time Turing
  machine $\calC_f$ evaluating a $L$-smooth antisymmetric function $f : \mathcal{X} \times \mathcal{X} \to \R$ and its gradient
  $\nabla f: \calX\times \calX \to \R^{2d}$, where
  $\mathcal{X} = \{\vx:\matA \vx \leq \vecb\}$ is a nonempty, bounded polytope described by a matrix
  $\matA \in \R^{m \times d}$ and vector $\vecb \in \R^{m}$.
\end{itemize}

  \noindent \textsc{Output:} A point
  $(\vxstar,\vystar)\in \calX\times\calX$ such that $\norm{\vxstar-\vystar}_2 \geq \delta$ and
  $\norm{(\vx^*, \vy^*) - F_{GDA}(\vx^*,\vy^*)}_2 \leq \epsilon$ if it exists, otherwise return \textsf{NO}.
\end{nproblem}

\noindent We establish that $\nsymgdaFixed$ is \FNP-hard. Our main result is restated below.

\nonsymmetric*

Our reduction builds on the hardness result of~\citet{MCLENNAN2010683}---in turn based on earlier work by~\citet{gilboa1989nash,Conitzer08:New}---which we significantly refine in order to account for $\poly(1/n)$-Nash equilibria. We begin by describing their basic approach. Let $G = ([n], E)$ be an $n$-node, undirected, unweighted graph, and construct
\begin{equation}
    \label{eq:matA}
    \mat{A}_{i, j} = 
    \begin{cases}
        -1 & \text{if $i = j$}, \\
        0 & \text{if $\{i, j\} \in E$}, \\
        -2 & \text{otherwise.}
    \end{cases}
\end{equation}
(\Cref{fig:graph} depicts an illustrative example.) Based on this matrix, \citet{MCLENNAN2010683} consider the symmetric, identical-payoff, two-player game $(\mat{A}, \mat{A})$---by construction, $\mat{A} = \mat{A}^\top$, and so this game is indeed symmetric. They were able to show the following key property.
\input{figs/graph}

\begin{lemma}[\citep{MCLENNAN2010683}]
    \label{lemma:Nashgap}
    Let $C_k \subseteq [n]$ be a maximum clique of $G$ with size $k$ and $\vx^* = \frac{1}{k} \sum_{i \in C_k} \ve_{i}$. Then, $(\vx^*, \vx^*)$ is a Nash equilibrium of $(\mat{A}, \mat{A})$ that attains value $-\nicefrac{1}{k}$. Furthermore, any symmetric Nash equilibrium not in the form described above has value at most $- \nicefrac{1}{k - 1}$.
\end{lemma}
The idea now is to construct a new symmetric, identical-payoff game $(\mat{B}, \mat{B})$, for
    \begin{align}
        \mat{B} \defeq \begin{bmatrix}
         \mat{A}_{1, 1} & \cdots & \mat{A}_{1, n}  & r\\
         \vdots & \ddots & \vdots & \vdots \\
         \mat{A}_{n, 1} & \cdots & \mat{A}_{n,n} & r\\
         r & \cdots & r  & V\\
    \end{bmatrix}, \label{eq:unique_NP}
    \end{align}
    where $V \defeq - \frac{1}{k}$ and $r = \frac{1}{2} (- \frac{1}{k} - \frac{1}{k - 1}) = -\frac{2k-1}{2(k - 1)k}$. Coupled with~\Cref{lemma:Nashgap}, this new game yields the following \NP-hardness result.

\begin{theorem}[\citep{MCLENNAN2010683}]
    \label{theorem:knownNP}
    It is \NP-hard to determine whether a symmetric, identical-payoff, two-player game has a unique symmetric Nash equilibrium.
\end{theorem}

Our goal here is to prove a stronger result, \Cref{theorem:symmetric-new}, that characterizes the set of $\epsilon$-Nash equilibria even for $\epsilon = \nicefrac{1}{n^c}$; this will form the basis for our hardness result in min-max optimization and adversarial team games. To do so, we first derive some basic properties of game~\eqref{eq:unique_NP}.%, which will be useful for us as well.% (even though we will have to rely on a slightly different matrix $\mat{A}$).

Game~\eqref{eq:unique_NP} always admits the trivial (symmetric) Nash equilibrium $(\vec{e}_{n+1}, \vec{e}_{n+1})$. Now, consider any symmetric Nash equilibrium $(\vx^*, \vx^*)$ with $x^*_{n+1} \neq 1$. If $x^*_{n+1} = 0$, it follows that $(\vx^*_{[i \cdots n] }, \vx^*_{[i \cdots n]})$ is a Nash equilibrium of $(\mat{A}, \mat{A})$, which in turn implies that $G$ admits a clique of size $k$; this follows from~\Cref{lemma:Nashgap}, together with the fact that $- \nicefrac{1}{k - 1} < r < -\nicefrac{1}{k}$.

We now analyze the case where $x^*_{n+1} \in (0, 1)$. It then follows that $(\nicefrac{\vx^*_{[1 \cdots n]}}{1 - x^*_{n+1}}, \nicefrac{\vx^*_{[1 \cdots n]}}{1 - x^*_{n+1}})$ is a (symmetric) Nash equilibrium of $(\mat{A}, \mat{A})$. Furthermore, the utility of playing action $a_{n+1}$ is $(1 - x^*_{n+1}) r + x^*_{n+1} V > r $. By~\Cref{lemma:Nashgap}, it follows that $(\nicefrac{\vx^*_{[1 \cdots n] }}{1 - x^*_{n+1}}, \nicefrac{\vx^*_{[1 \cdots n]}}{1 - x^*_{n+1}})$ has a value of $V$ and $G$ admits a clique of size $k$. As a result, the utility of playing any action $a_i$, with $i \in \text{supp}(\vx^*)$ and $i \neq n+1$, is $( 1 - x^{*}_{n+1}) V + x^*_{n+1} r$. At the same time, the utility of playing action $a_{2n+1}$ reads $(1 - x_{n+1}^*) r + x_{n+1}^* V$. Equating those two quantities, it follows that $x^*_{n+1} = \nicefrac{1}{2}$.

In summary, $G$ contains a clique of size $k$ if and only if game~\eqref{eq:unique_NP} admits a unique symmetric Nash equilibrium, which implies~\Cref{theorem:knownNP}. What is more, we have shown a stronger property. Namely, any symmetric Nash equilibrium of $(\mat{B}, \mat{B})$ has to be in one of the following forms:
\begin{enumerate} \label{eq:three_exact_NE}
        \item $(\vx^*, \vx^*)$ with $\vx^* \defeq \ve_{n+ 1}$;\label{item:case1}
        \item $(\vx^*, \vx^*)$ with $\vx^* \defeq \frac{1}{k} \sum_{i \in C_k} \ve_i$, where $C_k \subseteq [n]$ is a clique in $G$ of size $k$;\label{item:case2}
        \item $(\vx^*, \vx^*)$ with $\vx^* \defeq \frac{1}{2} \ve_{n + 1} + \frac{1}{2k} \sum_{i \in C_k} \ve_i$, where $C_k \subseteq [n]$ is a clique in $G$ of size $k$.\label{item:case3}
\end{enumerate}
In particular, the equilibria in Items~\ref{item:case2} or~\ref{item:case3}---which exist iff $G$ contains a clique of size $k$---are always far from the one in Item~\ref{item:case1}. However, this characterization only applies to exact Nash equilibria. In two-player games, when $\epsilon$ is sufficiently small with $\log(1/\epsilon) \leq \poly(|\Gamma|)$, \citet{Etessami10:On} have shown that any $\epsilon$-Nash equilibrium is within $\ell_1$-distance $\delta$ from an exact one, and so the above characterization can be applied; unfortunately, this does not apply (for general games) in the regime we are interested, namely $\epsilon = \poly(1/n)$.

We address this challenge by refining the result of~\citet{MCLENNAN2010683}. Our main result, which forms the basis for~\Cref{theorem:non-symmetric} and~\Cref{theorem:uniqueATG}, is summarized below.

\begin{theorem}
    \label{theorem:symmetric-new}
    For symmetric, identical-interest, two-player games, constants $c_1, c_2 > 0$, and $\epsilon = n^{-c_1}$, it is \NP-hard to distinguish between the following two cases:
    \begin{itemize}[noitemsep,topsep=0pt]
        \item any two symmetric $\epsilon$-Nash equilibria have $\ell_1$-distance at most $n^{-c_2}$, and
        \item there are two symmetric $\epsilon$-Nash equilibria that have $\ell_1$-distance $\Omega(1)$.
    \end{itemize}
\end{theorem}

Our reduction proceeds similarly, but defines $\lineA$ to be the adjacency matrix of $G$ with $\delta \in (0, 1)$ in each diagonal entry. Using $\lineA$, we show that we can refine~\Cref{lemma:Nashgap} of~\citet{MCLENNAN2010683}. Before we state the key property we prove in \Cref{lemma:well_supported_nash_value}, we recall the following definition.

\begin{definition}[Well-supported NE]
    A symmetric strategy profile $(\vx, \vx)$ is an \emph{$\epsilon$-well-supported} Nash equilibrium of the symmetric, identical-payoff game $(\lineA, \lineA)$ if for all $i \in [n]$,
    \begin{align*}
        x_i > 0 \implies (\lineA \vx)_i \geq \max_{j \in [n]} (\lineA \vx)_j - \epsilon.
    \end{align*}
\end{definition}

\begin{restatable}{lemma}{wellsupclique}
\label{lemma:well_supported_nash_value}
    Suppose that the maximum clique in $G$ is of size $k$. For any symmetric $\epsilon$-well-supported NE $(\hat{\vx}, \hat{\vx})$ of $(\lineA, \lineA)$ not supported on a clique of size $k$, we have $u(\hat{\vx}, \hat{\vx}) \leq 1 - \frac{1}{k} + \frac{\delta}{k} - \frac{2\delta}{ n^2k^4} + 2 \epsilon$.
\end{restatable}

Equipped with this property, we show (in~\Cref{sec:proofs3}) that a similar argument to the one described earlier concerning game~\eqref{eq:unique_NP} establishes~\Cref{theorem:symmetric-new}. We proceed now with~\Cref{theorem:non-symmetric}.
\begin{proof}[Proof of~\Cref{theorem:non-symmetric}]
It suffices to consider the antisymmetric function $f(\vx,\vy) \defeq \vy^{\top} \mat{B}\vy - \vx^{\top} \mat{B}\vx$, where symmetric matrix $\mat{B}$ is defined as in \eqref{eq:unique_NP}, using our new matrix $\lineA$ instead of $\mat{A}$ (see \eqref{eq:unique_NP_matrix}). Any $\epsilon$-first-order Nash equilibrium $(\vx^*, \vy^*)$ of this (separable) min-max optimization problem induces, two symmetric $\epsilon$-Nash equilibria---namely, $(\vx^*, \vx^*)$ and $(\vy^*, \vy^*)$---in the symmetric, identical-interest, game $(\mat{B}, \mat{B})$. Using~\Cref{theorem:symmetric-new}, the claim follows.
\end{proof}

Finally, the proof of~\Cref{theorem:uniqueATG} that was claimed earlier follows immediately by combining~\Cref{theorem:symmetric-new} with the reduction of~\Cref{sec:cls}, and in particular, \Cref{lemma:closeness,lemma:small_z}.

\iffalse

\begin{corollary}\label{cor:either} Given antisymmetric, continuous, $L$-smooth functions $f,g$, the problem of finding either an approximate symmetric fixed point of GDA for $f$ or an approximate non-symmetric fixed point of GDA for $g$ is \PPAD-complete.
\end{corollary}
\begin{proof} We consider the problem $\mathcal{A}$ to be $\symgdaFixed$ and $\mathcal{B}$ $\nsymgdaFixed.$ Using the same reasoning as by~\citet{Daskalakis11:Continuous}, we conclude that the problem $\textrm{EITHER}(\mathcal{A},\mathcal{B})$ is $(\PPAD \cap \NPP)$-complete, thus \PPAD-complete.
\end{proof}
\begin{remark} If one can show Corollary \ref{cor:either} for a single function, this would imply \PPAD-completeness of finding fixed points of GDA for Cartesian product domains.
\end{remark}
\fi

\subsection{Team zero-sum games}
\label{sec:teamzero}

Our previous hardness result concerning symmetric min-max optimization problems does not have any immediate implications for (normal-form) team zero-sum games since the class of hard instances we constructed earlier contains a quadratic term. Our next result provides such a hardness result by combining the basic gadget we introduced in~\Cref{sec:cls} in the context of adversarial team games; the basic pieces of the argument are similar to the ones we described in~\Cref{sec:cls}, and so the proof is deferred to~\Cref{sec:proofs4}. Our goal is to prove the following.

\teamhard*

We begin by describing the class of $3$ vs. $3$ team zero-sum games upon which our hardness result is based on. To do so, based on~\eqref{eq:util-atg}, let us define the auxiliary function
\begin{equation*}
    \delta: \Delta^n \times \Delta^n \times \Delta^{2n} \ni (\vx, \vy, \vz) \mapsto \frac{|\Amin|}{\epsilon} \sum_{i=1}^n ( z_i (x_i - y_i) + z_{n+i} (y_i - x_i)) + |\Amin| z_{2n+1}.
\end{equation*}
In what follows, the $3$ players of the one team will be identified with $(\vx, \vy, \vz)$, while the $3$ players of the other team with $(\hvx, \hvy, \hvz)$. Now, we define the utility of the latter team to be
\begin{equation}
    \label{eq:team-sym}
        u(\vx, \vy, \vz , \hat{\vx}, \hat{\vy}, \hat{\vz})  = \langle \vx, \mat{A} \vy \rangle - \langle \hvx, \mat{A} \hvy \rangle + \langle \vx, \mat{C} \hvx \rangle  + \delta(\vx, \vy, \hvz) - \delta(\hvx, \hvy, \vz),
\end{equation}
where again $\mat{A}$ is a symmetric matrix and $\mat{C}$ is skew-symmetric. As in~\Cref{sec:cls}, it is assumed for convenience that $\mat{A}_{i, j} \leq -1$ for all $i, j \in [n]$; we denote by $\Amin$ the minimum entry of $\mat{A}$. That the game defined above is symmetric is clear: $u(\vx, \vy, \vz, \hvx, \hvy, \hvz) = - u(\hvx, \hvy, \hvz, \vx, \vy, \vz)$ for all joint strategies (since $\mat{C} = - \mat{C}^\top$). It is also evident that~\eqref{eq:team-sym} is a polymatrix game, as promised.

The first key lemma, which mirrors~\Cref{lemma:closeness}, shows that, in an approximate Nash equilibrium of~\eqref{eq:team-sym}, $\vx \approx \vy$ and $\hvx \approx \hvy$. This is crucial as it allows us to construct---up to some small error---quadratic terms in the utility function, as in our hardness result for symmetric min-max optimization.

\begin{restatable}{lemma}{closeteams}
    \label{lemma:close-teams}
    Let $(\vx^*, \vy^*, \vz^*, \hvx^*, \hvy^*, \hvz^*)$ be an $\epsilon^2$-Nash equilibrium of~\eqref{eq:team-sym} with $\epsilon^2 \leq \nicefrac{1}{2}$. Then, $\| \vx^* - \vy^* \|_\infty \leq 2 \epsilon$ and $\| \hvx^* - \hvy^* \|_\infty \leq 2 \epsilon$.
\end{restatable}

Next, following the argument of~\Cref{lemma:small_z}, we show that, in equilibrium, Players $\vz$ and $\hvz$ place most of their probability mass on action $a_{2n + 1}$, thereby having only a small effect on the game between Players $\vx$ and $\vy$ vs. $\hvx$ and $\hvy$.

\begin{restatable}{lemma}{smallz}
    \label{lemma:smallzteam}
    Let $(\vx^*, \vy^*, \vz^*, \hvx^*, \hvy^*, \hvz^*)$ be an $\epsilon^2$-Nash equilibrium of~\eqref{eq:team-sym} with $\epsilon \leq \nicefrac{1}{10}$. Then, $z_j, \hat{z}_j \leq 9 \epsilon$ for all $j \in [2n]$.
\end{restatable}

Armed with those two basic lemmas, we complete the proof of~\Cref{theorem:team-hard} in~\Cref{sec:proofs4}.

