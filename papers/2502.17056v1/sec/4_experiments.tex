\label{sec:syn performance}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Visual_Quality-SS.pdf}
    \vspace{-5mm}
    \caption{\textbf{Generated samples-SegMunich.} We visualize several pairs of HSIs (shown in false-color) and corresponding segmentation maps generated by the baseline method LDM and our method respectively, comparing to the real samples.}
    \label{fig:Visual Quality-SS}
\end{figure*}
\section{Experiments}
\label{sec:experiments}
\subsection{Datasets}
\noindent
\textbf{SegMunich.} The SegMunich dataset is selected to perform the semantic segmentation data synthesis and the downstream task. This dataset, captured in Munich's urban from Sentinel-2 spectral satellite,  was first created and utilized in the published work SpectralGPT \cite{spectralgpt}. It consists of 13 bands with a spatial resolution of 10 meters, including the segmentation mask that meticulously delineates 13 Land Use and Land Cover (LULC) classes. The original work \cite{spectralgpt} chooses to combine the 10-meter spectral bands (B1, B2, B3, and B4) with resampled 20-meter spectral bands (B5, B6, B7, B8A, B11, B12) to get the 10-bands patches, to create a comprehensive feature representation for semantic segmentation. Our work keeps the same band configuration. The original dataset consists 39402 pairs for training and 9846 pairs for validation. We removed the patches which contain a lot of blank background (e.g., the entire image is occupied by the blank background). The cleaned dataset has 21680 pairs for training and 5410 pairs for validation with a patch size $128\times 128$.

\noindent
\textbf{OSCD.} The Onera Satellite Change Detection (OSCD) dataset \cite{oscd} is utilized to perform the change detection data synthesis and the downstream task. This dataset comprises 24 cities of Sentinel-2 images, captured between 2015 and 2018. The original images have 13 bands. Since the OSCD dataset is captured by the same satellite as the SegMunich dataset, we select the same bands combination as SegMunich for convenience. The images and masks are cropped to 237 pairs for training and 86 pairs for validation, with a 60\% overlap rate and a patch size $256 \times 256$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Spectral_analysis.pdf}
    \vspace{-10mm}
    \caption{\textbf{Spectral profile comparison.} We visualize the spectral response of our generated samples, comparing to real samples. We sample the pixels of several typical landforms according to the annotations. The intensity of spectral responses of the same landform keep consistent in different HSIs and are close to the real samples.}
    \label{fig:Spectral analysis}
\end{figure*}

\subsection{Synthesis Performance}
\noindent
\textbf{Visual Quality.} We utilized typical LDM \cite{ldm} as the baseline to evaluate the sample quality of our synthesis method. In the first stage training, we simply concatenate the image and mask in the channel dimension to get the input. Hence LDM can be regarded as encoding the image and mask using only single-stream VAE. We use Frechet Inception Distance (FID) \cite{fid} to measure the similarity of distributions of real dataset and synthetic dataset. The comparison results are displayed in Table \ref{tab:visual quality-ss}, confirming the superior visual quality of our generated samples.

\begin{table}[t]
  \centering
    \begin{tabular}{lccc}
    \toprule
    Method & FID (Image)$\downarrow$ & FID (Mask)$\downarrow$ & mSAD$\downarrow$ \\
    \midrule
    LDM-SS & 70.44 & 50.01 & 0.13\\
    Ours-SS & 5.19 & 10.79 & 0.03\\
    \bottomrule
  \end{tabular}
  \caption{\textbf{Quantitative evaluation of synthetic dataset-SegMunich.} The first two columns display the FID scores of image and label respectively. The last column displays the mSAD scores to evaluate the spectral fidelity of generated samples. In both tables, ($\downarrow$) indicates lower metric values are better, whereas ($\uparrow$) denotes higher values are better.}
  \label{tab:visual quality-ss}
\end{table}

We further provide qualitative samples of generated training pairs in Fig. \ref{fig:Visual Quality-SS}, compared to LDM method, and the distribution of landform classes in Fig. \ref{fig:Distribution of landform classes}. We can observe that the samples generated by our method have the similar spatial distribution with the real dataset. The edges of landforms in image also have great consistency with the mask. The proportion of main types of landforms, such as Arable land, Pastures and Forests, is close to the real dataset. On the contrary, the samples generated by LDM have a large deviation from the original real dataset. The proportion of different types of landforms also shows a large difference from the real data.

\begin{figure}[t]
    \includegraphics[width=1.0\linewidth]{figs/Distribution_of_landform_classes.pdf}
    \vspace{-7mm}
    \caption{\textbf{Distribution of landform classes,} illustrating that the set of our generated samples closely matches the real distribution. The proportion of several main classes are very close (Arable land, Pastures and Forests).}
    \label{fig:Distribution of landform classes}
\end{figure}

\noindent
\textbf{Spectral Fidelity.} Since this work focuses on the spectral data synthesis, the quality of generated spectra is essential in evaluation. We calculate the average spectral response of each class of landform, and compare the mean SAD with the real data. The results are displayed in the last column in Table \ref{tab:visual quality-ss}, which illustrate that the spectra of each class generated by our method is close to the real dataset. We also display the spectral profiles of several typical landforms, sampled from real and our generated samples, showed in Fig. \ref{fig:Spectral analysis}. Our generated samples exhibit strong spectral consistency with the real data for the same landform.

\begin{figure*}[th]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/CD_Samples.pdf}
    \vspace{-4mm}
    \caption{\textbf{Generated samples-OSCD.} We visualize several samples generated by our method and highlight the changed regions. The masks can annotate these changes in high accuracy.}
    \label{fig:CD samples}
\end{figure*}

\noindent
\textbf{Change Detection Dataset Synthesis.}
We further evaluate the visual quality of our synthetic data for change detection. The FID scores are displayed in Table \ref{tab:visual quality-cd}. Compared to LDM, our generated samples have lower FID scores and are closer to the real distribution. Qualitative samples generated by our method are shown in Fig. \ref{fig:CD samples}. The obvious changed area are highlighted. Comparing these regions, we can observe that the generator indeed generated changed images. Moreover, the generated masks annotated these changes in high accuracy.

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    Method & FID (Image$_1$)$\downarrow$ & FID (Image$_2$)$\downarrow$ & FID (Mask)$\downarrow$ \\
    \midrule
    LDM-CD & 40.21 & 45.99 & 11.90\\
    Ours-CD & 10.15 & 10.74 & 0.05\\
    \bottomrule
  \end{tabular}}
  \vspace{-3mm}
  \caption{\textbf{Quantitative evaluation of synthetic dataset-OSCD.} Our method outperforms LDM in both image and mask generation.}
  \label{tab:visual quality-cd}
\end{table}

\subsection{Downstream Task Evaluation}
\label{sec:downstream}

\begin{table*}[h]
\centering
    \subtable[Segmantic Segmentation]{
    \label{tab:downstream-ss}
    \resizebox{0.47\textwidth}{!}{
    \begin{tabular}{l|cc|cc}
    \hline
         Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
    \hline
        \multirow{3}*{PFSegNet-r50 \cite{pfsegnet}}    &2k &-  &0.3250 &0.4444\\
         ~  &-  &10k    &0.3193  &0.4327\\
         ~  &2k &10k    &\textbf{0.3763} &\textbf{0.5021}\\
     \hline
        \multirow{3}*{PFSegNet-r101 \cite{pfsegnet}}    &2k &-  &0.3654 &0.4943\\
         ~  &-  &10k    &0.2532  &0.3609\\
         ~  &2k &10k    &\textbf{0.3697} &\textbf{0.4987}\\
    \hline
        \multirow{3}*{SegFormer-B0 \cite{segformer}}    &2k &-  &0.3377 &0.4605\\
         ~  &-  &10k    &0.2724  &0.3879\\
         ~  &2k &10k    &\textbf{0.3512} &\textbf{0.4788}\\
    \hline
        \multirow{3}*{SegFormer-B5 \cite{segformer}}    &2k &-  &0.3574 &0.4852\\
         ~  &-  &10k    &0.2932  &0.4088\\
         ~  &2k &10k    &\textbf{0.3772} &\textbf{0.5092}\\
    \hline  
    \end{tabular}}
    }
    \hfill
    \subtable[Change Detection]{
    \label{tab:downstream-cd}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|cc|cc}
    \hline
         Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
    \hline
        \multirow{3}*{SiamCRNN-r50 \cite{siamcrnn}}    &100 &-  &0.5292 &0.5947\\
         ~  &-  &500    &0.5210  &0.5809\\
         ~  &100 &500    &\textbf{0.5680} &\textbf{0.6486}\\
    \hline
        \multirow{3}*{SiamCRNN-r101 \cite{siamcrnn}}    &100 &-  &0.5191 &0.5766\\
         ~  &-  &500    &0.5269  &0.5970\\
         ~  &100 &500    &\textbf{0.5702} &\textbf{0.6494}\\
    \hline
        \multirow{3}*{ChangeFormerV1 \cite{changeformer}}    &100 &-  &0.5090 &0.5541\\
         ~  &-  &500    &0.5310  &0.5903\\
         ~  &100 &500    &\textbf{0.5395} &\textbf{0.6022}\\
    \hline
        \multirow{3}*{ChangeFormerV3 \cite{changeformer}}    &100 &-  &0.5460 &0.6274\\
         ~  &-  &500    &0.5561  &0.6390\\
         ~  &100 &500    &\textbf{0.5733} &\textbf{0.6617}\\
    \hline
    \end{tabular}}
    }
\caption{\textbf{Downstream task evaluation results} of (a) semantic segmantion and (b) change detection. With the augmentation of our synthetic data, the performance of downstream tasks on all methods get improvement, highlighted in \textbf{Bold}.}
\label{tab:downstream}
\end{table*}

We perform the corresponding downstream task experiments: semantic segmentation and change detection respectively, to further validate the effectiveness of our generated dataset.

\noindent
\textbf{Semantic Segmentation.} We limit the size of real dataset (using 2k pairs) and utilize 10k synthetic pairs to augment it. Then we train SS algorithms on these different dataset configuration (real data only, synthetic data only and augmented data) and evaluate on the same test set (real data). The mIoU and F1 score are used to evaluate the performance of the task. Table \ref{tab:downstream}(a) shows the segmentation results of all methods in different data configuration. As can be seen, without the supervised training of real data, the performance of SS algorithms will degrade. However, after augmenting the original real data with synthetic data, both SS models can achieve better results. Since the dataset obtained by the generative model is still learned from the real dataset, using only synthetic data to train the downstream SS model does not guarantee that the model can learn more knowledge of the feature of images, compared to training only on real data. During testing, performance degradation occurs due to the distribution difference between synthetic set and test set. Under the premise of ensuring real data supervision, using synthetic data for augmentation can enable the model to learn more knowledge to achieve better performance.

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|cc}
%     \hline
%          Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
%     \hline
%         \multirow{3}*{PFSegNet-r50 \cite{pfsegnet}}    &2k &-  &0.3250 &0.4444\\
%          ~  &-  &10k    &0.3193  &0.4327\\
%          ~  &2k &10k    &\textbf{0.3763} &\textbf{0.5021}\\
%      \hline
%         \multirow{3}*{PFSegNet-r101 \cite{pfsegnet}}    &2k &-  &0.3654 &0.4943\\
%          ~  &-  &10k    &0.2532  &0.3609\\
%          ~  &2k &10k    &\textbf{0.3697} &\textbf{0.4987}\\
%     \hline
%         \multirow{3}*{SegFormer-B0 \cite{segformer}}    &2k &-  &0.3377 &0.4605\\
%          ~  &-  &10k    &0.2724  &0.3879\\
%          ~  &2k &10k    &\textbf{0.3512} &\textbf{0.4788}\\
%     \hline
%         \multirow{3}*{SegFormer-B5 \cite{segformer}}    &2k &-  &0.3574 &0.4852\\
%          ~  &-  &10k    &0.2932  &0.4088\\
%          ~  &2k &10k    &\textbf{0.3772} &\textbf{0.5092}\\
%     \hline  
%     \end{tabular}}
%     \caption{\textbf{Semantic segmentation results.} With the augmentation of our synthetic data, the segmentation results of all methods get improvement, highlighted in \textbf{Bold}.}
%     \label{tab:downstream-ss}
% \end{table}

\noindent
\textbf{Change Detection.} Table \ref{tab:downstream}(b) presents the change detection results of all methods on three training configuration. We limit the size of real data to 100 and utilize 500 synthetic samples for augmentation. Both CD models achieve the significant improvement with the data augmentation. Moreover, due to the small size of the training set, training with only synthetic datasets dose not cause much performance degradation for CD models. For ResNet-101 backbone, SiamCRNN \cite{siamcrnn} even achieves better results when training only on the synthetic data compared to training only on the real data.

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|cc}
%     \hline
%          Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
%     \hline
%         \multirow{3}*{SiamCRNN-r50 \cite{siamcrnn}}    &100 &-  &0.5292 &0.5947\\
%          ~  &-  &500    &0.5210  &0.5809\\
%          ~  &100 &500    &\textbf{0.5680} &\textbf{0.6486}\\
%     \hline
%         \multirow{3}*{SiamCRNN-r101 \cite{siamcrnn}}    &100 &-  &0.5191 &0.5766\\
%          ~  &-  &500    &0.5269  &0.5970\\
%          ~  &100 &500    &\textbf{0.5702} &\textbf{0.6494}\\
%     \hline
%         \multirow{3}*{ChangeFormerV1 \cite{changeformer}}    &100 &-  &0.5090 &0.5541\\
%          ~  &-  &500    &0.5310  &0.5903\\
%          ~  &100 &500    &\textbf{0.5395} &\textbf{0.6022}\\
%     \hline
%         \multirow{3}*{ChangeFormerV3 \cite{changeformer}}    &100 &-  &0.5460 &0.6274\\
%          ~  &-  &500    &0.5561  &0.6390\\
%          ~  &100 &500    &\textbf{0.5733} &\textbf{0.6617}\\
%     \hline
%     \end{tabular}}
%     \caption{\textbf{Change detection results.} With the augmentation of our synthetic data, the change detection results of all methods get improvement.}
%     \label{tab:downstream-cd}
% \end{table}

\begin{table*}[t]
  \centering
    \begin{tabular}{l|ccc|ccc}
    \hline
    \multirow{2}*{Method} & \multicolumn{3}{c|}{Reconstruction Quality} & \multicolumn{3}{c}{Synthesis Quality}\\
    \cline{2-7}
    ~ & RMSE$\downarrow$ & SAD$\downarrow$ & Cross Entropy$\downarrow$ & FID (Image$_1$)$\downarrow$ & FID (Image$_2$)$\downarrow$ & FID (Mask)$\downarrow$ \\
    \hline
    Ours-SS w/o SAD loss & 0.072 & 0.217 & \textbf{0.031} & 21.05 & - & 15.12\\
    Ours-SS & \textbf{0.025} & \textbf{0.103} & 0.051 & \textbf{5.19} & - & \textbf{10.79}\\
    Ours-CD w/o SAD loss & 0.051 & 0.179 & 0.020 & 31.30 & 32.49 & 0.26 \\
    Ours-CD & \textbf{0.034} & \textbf{0.086} & \textbf{0.014} & \textbf{10.15} & \textbf{10.74} & \textbf{0.05}\\
    \hline
  \end{tabular}
  \caption{\textbf{Ablation study-SAD loss.} After eliminating the SAD loss term, the reconstruction quality degrades in the first-stage training, leading to the degradation of synthesis quality.}
  \label{tab:Ablation study-sad}
\end{table*}

\noindent
\textbf{Comparison with LDM.} We further compare the effectiveness of synthetic data generated by LDM and our method. We choose SegFormer-B5 \cite{segformer} and ChangeFormerV3 \cite{changeformer} training only on the real dataset as the baseline for SS and CD task, respectively. The dataset configuration is set to use 2k real data for SS and 100 real data for CD, and augmented with 5 times synthetic data. Table \ref{tab:downstream-ldm} presents the comparison results of two method on these two tasks. Our method outperforms LDM on both tasks, which demonstrates that our generated samples not only have better visual effects, but also more helpful in promoting downstream tasks.

\begin{table}[t]
    \centering
    \begin{tabular}{l|cc|cc}
    \hline
        \multirow{2}*{Method}&\multicolumn{2}{c|}{SS}&\multicolumn{2}{c}{CD}\\
        \cline{2-5}
        ~&mIoU$\uparrow$&F1$\uparrow$&mIoU$\uparrow$&F1$\uparrow$\\
    \hline
        Baseline & 0.3574 &0.4852 &0.5460 &0.6274\\
        LDM &0.3499 &0.4803 &0.5341 &0.5975\\
        Ours &\textbf{0.3772} &\textbf{0.5092} &\textbf{0.5733} &\textbf{0.6617}\\
    \hline
    \end{tabular}
    \caption{\textbf{Downstream tasks results comparing to LDM.} SegFormer-B5 \cite{segformer} and ChangeFormerV3 \cite{changeformer} are used as the baseline for SS and CD task. Our synthetic data has more promotion for the baseline.}
    \vspace{-0.2in}
    \label{tab:downstream-ldm}
\end{table}

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|cc}
%     \hline
%          Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
%     \hline
%         \multirow{3}*{SegFormer-B0}    &2k &-  &0.3377 &0.4605\\
%          ~  &-  &10k    &0.2724  &0.3879\\
%          ~  &2k &10k    &0.3512 &0.4788\\
%     \hline
%         \multirow{3}*{SegFormer-B5}    &2k &-  &0.3574 &0.4852\\
%          ~  &-  &10k    &0.2932  &0.4088\\
%          ~  &2k &10k    &0.3772 &0.5092\\
%     \hline
%     \end{tabular}}
%     \caption{Semantic segmentation results. }
% \end{table}

% \begin{table}[h]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|cc}
%     \hline
%          Method &Real Data   &Synthetic Data  &mIoU$\uparrow$ &F1$\uparrow$\\
%     \hline
%         \multirow{3}*{SegFormer-B0}    &2k &-  &0.3377 &0.4605\\
%          ~  &-  &10k    &-  &-\\
%          ~  &2k &10k    &0.3512 &0.4788\\
%     \hline
%         \multirow{3}*{SegFormer-B5}    &2k &-  &0.3574 &0.4852\\
%          ~  &-  &10k    &-  &-\\
%          ~  &2k &10k    &0.3772 &0.5092\\
%     \hline
%     \end{tabular}}
%     \caption{Change detection results.}
% \end{table}

\subsection{Ablation Study}

In this work, we propose to take the two-stream VAE to learn the latent representations of input HSIs and semantic annotations respectively. In experiments of Sec. \ref{sec:syn performance} and Sec. \ref{sec:downstream}, we have demonstrated the effectiveness of this approach, by comparing it with typical LDM method. We now assess the impact of SAD loss proposed in Eq. (\ref{eq:image loss}) on the reconstruction and synthesis quality, and the impact of the size of synthetic dataset on downstream tasks.

\noindent
\textbf{SAD Loss.} The SAD loss is utilized to ensure the spectral fidelity while reconstructing the HSIs in the first-stage training. We eliminate this term and use only $L_1$ loss as the reconstruction loss. Table \ref{tab:Ablation study-sad} displays the results of these two configurations. In the first-stage training, the reconstruction quality degrades much after eliminating the SAD term, especially for the SAD metric, which leads to the degradation of image synthesis quality. For the reconstruction of masks, SAD loss will not influence the parameter update of mask branch, hence the reconstruction and synthesis quality of mask is not largely affected.

\noindent
\textbf{Size of Synthetic Dataset.} In Sec. \ref{sec:downstream}, we set the size of the synthetic dataset to be 5 times that of the real dataset. We further explore the impact of more augmentation configurations. Same as Sec. \ref{sec:downstream}, we choose SegFormer-B5 \cite{segformer} and ChangeFormerV3 \cite{changeformer} as the baseline for SS and CD task, training on the real data only. We use $1\times$ and $3\times$ synthetic data for augmentation. The results are displayed in Table \ref{tab:Ablation Study-train size}. As can be seen, the performance improves as the size of synthetic set increases.

\begin{table}[t]
    \centering
    \begin{tabular}{l|cc|cc}
    \hline
        \multirow{2}*{Syn Data}&\multicolumn{2}{c|}{SS}&\multicolumn{2}{c}{CD}\\
        \cline{2-5}
        ~&mIoU$\uparrow$&F1$\uparrow$&mIoU$\uparrow$&F1$\uparrow$\\
    \hline
        Baseline &0.3574 &0.4852 &0.5460 &0.6274\\
        $\times1$ &0.3664 &0.4942 &0.5666  &0.6546\\
        $\times3$ &0.3757 &0.5086 &0.5694  &0.6561\\
        $\times5$ &\textbf{0.3772} &\textbf{0.5092} &\textbf{0.5733}  &\textbf{0.6617}\\
    \hline
    \end{tabular}
    \caption{\textbf{Ablation study-size of synthetic dataset.} SegFormer-B5 \cite{segformer} and ChangeFormerV3 \cite{changeformer} are used as the baseline for SS and CD task. We gradually add the size of synthetic dataset. Results have shown that the performance improves as the size of synthetic set increases.}
    \vspace{-0.1in}
    \label{tab:Ablation Study-train size}
\end{table}