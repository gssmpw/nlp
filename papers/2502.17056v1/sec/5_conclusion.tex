\section{Conclusion}
\label{sec:conclusion}
Our work demonstrates the value and potential of using diffusion models to generate synthetic data in a context where hyperspectral images are scarce and annotation is expensive. By using a two-stream VAE to simultaneously compress images and labels into the latent space and learn their joint distribution, it is possible to generate high-dimensional spectral data with semantic annotations. We have designed our generative model for two of the most widely used dense prediction tasks in hyperspectral remote sensing images: semantic segmentation and change detection, which can generate high-quality HSIs and pixel-level semantic annotations automatically, and validated the effectiveness of our synthetic dataset on these tasks. In data-hunger circumstances, augmenting the traing set with synthetic data can bring positive impacts on models of downstream tasks. 

\noindent
\textbf{Limitation}. For generated annotations, we currently have no suitable method to verify their pixel-level alignments with genetated images without the reference of ground truth. The reliability of generated samples can only be verified by downstream tasks right now. We will continue to explore how to evaluate the reliability of generated samples in the future.


% With the application of our proposed generative models, we have already produced a vast quantity of diverse, high-fidelity synthetic HSIs, which have shown initial positive impacts on these tasks. It demonstrates a closed-loop from method proposal to application verification. 

% The concept of two-stream encoding we proposed can be further extended to adapt to different visual tasks that require annotations for supervised learning, such as image restoration for cloud removal, salient object detection and object tracking. Additionally, we can also consider generating the specified image-label pairs by conditional control. Above are possible directions we will improve in the future. 