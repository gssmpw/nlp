\section{Introduction}
\label{sec:intro}
Hyperspectral image, with its 3D data structure, provides more detailed spectral information compared to RGB image, which makes it take advantages in various applications such as face recognition \cite{face1, face2, face3}, vegetation detection \cite{vegetation1, vegetation2, vegetation3} and geological observation \cite{earth1, earth2, earth3}. However, owing to the performance of the equipments, the requirements of scenes and objects, and the limitations of the environment, it is costly to obtain the HSI data \cite{scarse1, scarse2}. For some visual tasks with dense prediction, the cost of label annotation cannot be ignored either, especially for remote sensing scenes with large fields. In addition to the cost of annotation, the sensitivity of some hyperspectral data also makes it difficult for ordinary researchers to access the data. Due to the above reasons, both the construction of large-scale hyperspectral dataset platforms, and the research of data-dependent AI models in the field of HSI are currently severely hindered \cite{challenge1, challenge2}.
To address the scarcity of HSI data, some researchers usually use techniques such as affine transformation to enhance data \cite{affine}, or use physical modeling based synthetic data \cite{physics}. Some research also explore to reconstruct the spectral information from RGB images \cite{he2023spectral}. However, such techniques either fail to substantially increase the diversity of data, or produce high-quality data limited by the physical model.

Recently, generative AI models, such as Variational Autoencoder (VAE) \cite{vae1, vae2}, Generative Adversarial Network (GAN) \cite{gan1, gan2} and Diffusion Model (DM) \cite{dm1, dm2, ldm}, have achieved great success in the field of natural image synthesis. In most visual tasks, especially supervised learning, high-quality data annotation is as significant as the image data itself. While working on generating images with rich visual effects, some works are also devoted to exploring the generation of annotated datasets \cite{datasetgan, bigdatasetgan, diffumask, datasetdm}. For example, DatasetDM \cite{datasetdm} designed a unified perception decoder which can generate different perception annotations to meet the demands of various downstream tasks. In optical remote sensing field, SatSynth \cite{satsynth} used DDPM \cite{dm2} to generate images and segmentation masks simultaneously. For HSI synthesis, it is difficult to automatically generate such annotations through algorithms since most existing dense prediction methods like SAM \cite{sam} are designed for RGB images and cannot be directly applied to high-dimensional HSIs. Hence existing research is still at the stage of pure image generation \cite{yu2024unmixing, yu2024unmixdiff, hsigene} and cannot meet the demands of downstream tasks which need pixel-level annotations.

In this work, we focus on filling the gap in the field of hyperspectral data generation, exploring the potential of diffusion model to augment existing hyperspectral datasets in a generative manner. In addition to image data, our work can also simultaneously generate semantic labels suitable for downstream dense prediction tasks, specifically, for semantic segmentation and change detection, which are two significant tasks in hyperspectral remote sensing field. To the best of our knowledge, it is the first work to generate high-dimensional HSIs with pixel-level annotations. Instead of additionally designing a segmentation or change detection algorithm to generate annotations of HSIs, our work directly learn the joint distribution of image-label pairs by designing a two-stream training paradigm for the first-stage training, based on the classic Latent Diffusion Model (LDM) \cite{ldm}. Specifically, we implement a two-stream variational autoencoder, corresponding to the image data stream and the label data stream respectively. Due to the different distribution between the HSI pixel value and mask value \cite{satsynth}, the two VAE branches use different network parameters. In the second-stage diffusion and denoising process, we concatenate the latent features of image and semantic mask in the channel dimension and to learn their joint distribution. When generating, we sample from the joint distribution to get latent codes, then decouple and decode them to obtain high-quality images and semantic labels separately.

To summarize, the contributions of our work are as below:

\begin{itemize}
    \item We propose SpecDM: a new dataset synthesis method for hyperspectral images utilizing the generative diffusion model, which can generate high quality training data instances with pixel-level semantic labels.

    \item In order to solve the distribution difference between image and label value domains, we design the two-stream VAE to separately learn the latent representation of image and label. In addition to semantic segmantation, we expand this training paradigm to change detection. 

    \item Experiments demonstrate that the existing models trained on augmented data generated by our method exhibit significant improvements on semantic segmentation and change detection, which are two main downstream tasks in hyperspectral remote sensing fields.
\end{itemize}