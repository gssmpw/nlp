\section{Method}
\label{sec:method}

Our proposed SpecDM comprises two stages, which is illustrated in Fig. \ref{fig:approach}. The \textbf{Training} stage involves compressing the data through the two-stream VAE to obtain the latent representations of image and semantic label separately, and learning the mapping from Gaussion distribution to the joint distribution of image-mask pairs by training a denoising U-Net \cite{unet}. In the \textbf{Inference} stage, we sample the joint latent representations from Gaussian distribution and denoise it through the denoising U-Net. The clean latent representation is then decomposed to the image and label parts, and decoded by the corresponding decoders to obtain HSIs and annotations.

\subsection{Two-stream Encoding for Data Compression}

Due to the high-dimensional spectral information of HSI, training DMs in original image space is computationally expensive. Previous works using unmixing to map the HSI to the low-dimensional abundance space to ensure the fidelity of spectral response of synthetic HSI \cite{yu2024unmixdiff, yu2024unmixing}. While generating high-quality HSI, such a compression approach faces two challenges: (i) The dimension of unmixing is corresponded to the number of endmembers. When the dataset covers a larger variety of materials, the dimension of abundance is still high after unmixing, which is not suitable for segmentation datasets with more types of landforms. (ii) As a dimension reduction method, unmixing cannot handle the low-dimensional annotation images, such as binary masks. In this case, forced unmixing will lose its original physical meaning.

In this work, we propose to use two-stream encoding for data compression. Specifically, two branches of VAE in original LDM are used to encode the input data pairs, while one branch is used to encode the image data, and the other branch encodes the annotation data. Given an HSI $x\in\mathbb{R}^{H\times W\times C}$ with the semantic mask $y\in\mathbb{Z}^{H\times W}$, the image branch encoder $\mathcal{E}_{hsi}$ and mask branch encoder $\mathcal{E}_{msk}$ encode $(x, y)$ pairs into the joint latent representations $(z_x, z_y) = (\mathcal{E}_{hsi}(x), \mathcal{E}_{msk}(y))$, where $z_x, z_y \in\mathbb{R}^{h\times w\times c}$. Downsampling factor is defined as $f = H/h = W/w$. The decoders $\mathcal{D}_{hsi}$ and $\mathcal{D}_{msk}$ reconstructs the image and mask from $(z_x, z_y)$ pairs. To reconstruct HSI, we add the spectral angle distance (SAD) measurement as a part of loss function in additional to original loss to ensure the spectral fidelity. Then the loss function of image branch $\mathcal L_{hsi}$ is defined as:
\begin{equation}
\label{eq:image loss}
    \mathcal L_{hsi} (x, \hat{x}) = \mathcal L_1(x, \hat{x}) + \lambda \arccos(\frac{x\hat{x}^T}{\Vert x\Vert_2\Vert \hat{x}\Vert_2}),
\end{equation}
where $\mathcal L_1$ represents the $L_1$ loss and $\lambda$ is used to balance the two items. To reconstruct semantic mask, we use cross entropy loss, then the total loss of the two-stream VAE is defined as:
\begin{equation}
    \mathcal{L} = \mathcal L_{hsi}(x, \hat{x}) +\mathcal L_{CE} (y, \hat{y}),
\end{equation}

It should be noted that the two branches have totally different parameters for the great difference between continuous image pixel values in $\mathbb{R}$ and discrete mask values in $\mathbb{Z}$. In this manner, we can perform image and annotation data compression simultaneously without being constrained by the form of unmixing. In order to take into account both the computational efficiency in the subsequent diffusion process and reconstruction quality, we choose a downsampling factor $f = 4$ \cite{ldm}.

\subsection{Diffusion Model of Joint Representations}

After getting the joint latent representations $z = (z_x, z_y)$ of image and semantic mask inputs in the first-stage, we approximates the posterior distribution $q(z_{1:T}|z_0)$ through the diffusion forward process, and then training the denoising U-Net to denoise from $p_\theta(z_{t-1}|z_t)(t=\{1, ..., T\})$ to obtain the clean reconstruction step by step. Here we provide a brief introduction of this process.

Given a joint latent representation $z = (z_x, z_y) \in\mathbb R^{h\times w\times 2c}$, the diffusion process gradually adds Gaussian noise following a pre-defined noise schedule $\beta_1, . . . , \beta_T$:
\begin{equation}
    q(z_t|z_{t-1}):=\mathcal N(z_t;\sqrt{1-\beta_t}z_{t-1}, \beta_t\mathrm{\mathbf{I}}),
\end{equation}
where $t$ represents the $t$-th time step. After sufficiently large $T$ steps, we obtain a Gaussion random noise sample $z_T\sim \mathcal N(0, \mathrm{\mathbf{I}})$. The reversed denoising process is performed through the U-Net by optimizing the following objective function:

\begin{equation}
    \mathcal L=\mathbb E_{z, \epsilon\sim\mathcal N(0, \mathrm{\mathbf{I}}), t}\left[\Vert {\epsilon - \epsilon_\theta(z_t, t)}\Vert_2^2\right],
\end{equation}
Thus we have completed the reconstruction from Gaussian distribution to the input training data distribution.

\subsection{Data Synthesis with Semantic Annotation}

In this work, we preset two types of dataset generation tasks, one for semantic segmentation and the other for change detection, which are two typical pixel-level dense prediction tasks.

\noindent
\textbf{Synthesis for Semantic Segmentation.}  To synthesize image-mask pairs for semantic segmentation, we take the following steps:

\begin{itemize}
    \item Train the two-stream VAE on data pairs $(x, y)$ to get the joint latent representations $z = (z_x, z_y)$.
    \item Train the diffusion model $\mathcal G$ in the latent space.
    \item Sample from $\mathcal G$ to get synthetic latent representations $z_{syn}$.
    \item Decode $z_{syn}$ using the trained decoders $\mathcal D_{hsi}$ and $\mathcal D_{msk}$ to get synthetic pairs $(x_{syn}, y_{syn})$.
\end{itemize}

\noindent
\textbf{Synthesis for Change Detection.}  Such paradigm can be expanded to change detection dataset synthesis. For change detection, a data instance consists of two images at different temporal phases and a mask to represent the change. While expanding to change detection, the mask branch keeps the same, and the image branch accepts the two images as inputs. Since the image branch is utilized to compress image data only, there is no need to add additional branches with different parameters even if the the interface for input images is increased. In this case, the inputs is encoded as $z = (z_{x_1}, z_{x_2}, z_y)\in\mathbb R^{h\times w\times 3c}$, where $(z_{x_1}, z_{x_2}) = \mathcal E_{hsi}(x_1, x_2)$ and $z_y = \mathcal E_{msk}(y)$.

\subsection{Implementation Details}
\noindent
\textbf{Latent Diffusion.} We follow the LDM \cite{ldm} to set our experiments configurations. For two-stream VAE training, we take KL-regularized VAE as the backbone of both image and mask branches. Image branch accepts multi-channel HSIs as inputs, and the mask branch accepts one-hot encodings as inputs.
The SAD tradeoff $\lambda$ in Eq. (\ref{eq:image loss})  is set to 0.1, and the initial learning rate is set to $4.5\times 10^{-6}$. For diffusion model training, we apply $T=1000$ denoising steps with a linear $\beta$ schedule from 0.0015 to 0.0155. The learning rate is set to $5.0\times 10^{-6}$.

\noindent
\textbf{Downstream Task.} For \textbf{semantic segmentation}, we choose SegFormer \cite{segformer} and PFSegNet \cite{pfsegnet} algorithms to evaluate the performance trained on the original dataset and augmented dataset, respectively. Since the SegFormer was designed for RGB semantic segmentation, we add a mapping layer before the backbone to map the input HSI to 3 channels and load the pre-trained backbone model. For \textbf{change detection}, we use SiamCRNN \cite{siamcrnn} and ChangeFormer \cite{changeformer} algorithms to evaluate the performance.

All of above experiments were carried out using 4 NVIDIA 3090 GPUs.