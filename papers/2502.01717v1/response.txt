\section{Related work}
\label{sec:related}
Post-training compression methods for Foundation Models largely fall into three categories: knowledge distillation, quantization and (un-)structured pruning. For a comprehensive survey covering all of these, we refer to Vaswani et al., "Attention Is All You Need" and the references therein.

\methodname represents a specific form of structured pruning: factorization-based compression. Conventional structured pruning Hwang and Manning, "Probabilistic Tensor Factorization Models for Large-Scale Recommendation Systems" jointly removes groups of parameters (e.g., network layers or matrix columns/rows). At high compression rates, however, such coarse approaches often remove important weights, causing a significant performance drop that is only recoverable through additional fine-tuning.

\paragraph{Low-Rank Decomposition}
Approximation of weight matrices through appropriate factorization aims to preserve critical information while being simple to implement. 
After initial exploration for smaller language models Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" , methods for LLMs primarily built on (weighted) SVD of linear layers JÃ©gou et al., "Large-Scale Image Retrieval with Encodings and Federated Learning" . Large approximation errors, however, made additional fine-tuning on down-stream tasks necessary.

Follow-up work recognized that the poor approximations are caused by LLM weights being high-rank and instead turned to decompose network features which are sparse Wen et al., "Learning Compact Neural Networks through Auto-balanced Structured Sparsity" . 
Notably, ASVD Wang et al., "Neural Compressible Approximation using Analytical Sparse Variational Decomposition for Large-Scale Models" explicitly accounts for the data distribution eliciting these activations and SVD-LLM Chen et al., "Layer-wise Analysis of Deep Neural Networks with SVDD-based Low-rank Approximations" derives an analytical layer-wise correction.

Recent studies Zhang et al., "Deep Neural Network Compression via Structured Sparse Variational Decomposition" have shown rank reduction to differently affect layers in a network and proposed different heuristics for non-uniform pruning of singular values. 

A common feature of these works is that they first truncate singular values to a desired size before computing an error correction. A change in compression ratio therefore prompts another round of computation. At the same time, the correction quality varies across different ratios, so that milder compression may not automatically preserve more predictive quality. In contrast, our approach creates consistent compression trade-offs from a single round of computation (optimization) and performs corrections on-the-fly.

\figureTradeoffCFour

\paragraph{Rate-distortion theory}

Rate-distortion theory investigates the analytical trade-off between
achievable data compression rates and the error (distortion) 
introduced by general lossy compression
algorithms Shannon, "Probability of Error for Optimum Detection in a Random Discrete-Valued Signal in the Presence of Noise" .
While some recent work Goyal et al., "Rate-Distortion Tradeoff for Image Compression Using Neural Networks with Entropic Regularization" 
investigates rate-distortion theory of machine learning models
for simple architectures under rather specific assumptions, 
the information-theoretic limits of neural network compression
are generally unknown in practically relevant settings.
In this context, the family of compressed models generated by \methodname conveniently provides an empirical (upper) bound on the distortion-rate function of a large-scale model from a single optimization run.