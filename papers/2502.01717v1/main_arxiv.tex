%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

%
%
%
%
\usepackage{hyperref}
\PassOptionsToPackage{hyphens}{url}

%
\newcommand*{\img}[1]{%
    \raisebox{-.15\baselineskip}{%
        \includegraphics[
        height=\baselineskip,
        width=\baselineskip,
        keepaspectratio,
        ]{#1}%
    }%
}

\usepackage{tikz}
\definecolor{mxm}{HTML}{381D59}

\newcommand\circlenum[1]{%
\tikz[baseline] {%
        \node[circle, fill=mxm,
        inner sep=1pt, scale=.8, anchor=base,
        font=\bfseries, text=white] {#1};
    }%
}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[accepted]{icml2025Arxiv}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{enumitem}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}

%
\usepackage{commands}
\usepackage{listings}
\definecolor{codeblue}{rgb}{0.25, 0.5, 0.5}
\definecolor{codekw}{rgb}{0.35, 0.35, 0.75}
\lstdefinestyle{Pytorch}{
    language         = Python,
    backgroundcolor  = \color{white},
    basicstyle = \fontsize{8.0pt}{9pt}\selectfont\ttfamily\bfseries,
    columns          = fullflexible,
    breaklines       = true,
    captionpos       = b,
    commentstyle     = \fontsize{4pt}{4pt}\color{codeblue},
    keywordstyle     = \fontsize{4pt}{4pt}\color{codekw},
    morekeywords     = {with,scatter_,norm,sort},
}

\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{xspace}

%
%
%
%
%

%
\usepackage{colortbl}

%
\usepackage[skip=5pt]{caption}

%

%

%
%
\icmltitlerunning{\methodnamefull}

\begin{document}
%
\setlength{\textfloatsep}{8pt}
%

\twocolumn[
\icmltitle{Choose Your Model Size: Any Compression by a Single Gradient Descent}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Martin Genzel}{equal,mxm}
\icmlauthor{Patrick Putzky}{equal,mxm}
\icmlauthor{Pengfei Zhao}{equal,mxmwhile,umi}
\icmlauthor{Sebastian Schulze}{mxm}
\icmlauthor{Mattes Mollenhauer}{mxm}
\icmlauthor{Robert Seidel}{mxm}
\icmlauthor{Stefan Dietzel}{mxm}
\icmlauthor{Thomas Wollmann}{mxm}
\end{icmlauthorlist}

\icmlaffiliation{mxm}{Merantix Momentum GmbH, Berlin, Germany}
\icmlaffiliation{mxmwhile}{Work done while at Merantix Momentum.}
\icmlaffiliation{umi}{Understandable Machine Intelligence Lab, Leibniz Institute for Agriculture and Bioeconomy (ATB), Potsdam, Germany.}

\icmlcorrespondingauthor{Martin Genzel}{martin.genzel@merantix-momentum.com}

%
%
%
\icmlkeywords{Model Compression, Weight Factorization, SVD, Structured Pruning, LLM, Optimization path}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{\icmlEqualContribution} %


%
\newcommand{\figureFirstpage}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/7b_only.pdf}
    \caption{Compression-performance trade-off curves generated by \methodname on \textbf{C4} for a variety of LLMs. Each curve is obtained by a single gradient descent run and all compressed models can be materialized without recomputation.}
    \label{fig:firstpage}
\end{figure}
}

\newcommand{\figureSecondpage}{
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/all-in-one-revised-new.pdf}
    \caption{A visual overview of \methodname.
    \circlenum{1} The linear layers
    of the base model are reparametrized in terms
    of their singular value decomposition 
    $ \U \maskmat \singmat  \V\transpose$\!,
    with a (binary) singular value mask $\maskmat$, 
    and a low-rank adapter $\boldsymbol\Delta$.
    \circlenum{2} The objective function is optimized over the mask parameters $\maskvecraw$ and adapters $\boldsymbol\Delta$, where sparsity is induced on $\maskvecraw$ by an $\ell_1$-penality leading to pruned entries in the mask $\maskmat$. The optimization path of $\maskvecraw$ gives rise to a score map that determines the global importance
    of the singular values across the full model. 
    Potential compression errors are compensated by $\boldsymbol\Delta$.
    \circlenum{3} 
    Based on the parameter scores, the base 
    model can be flexibly compressed to any target size by masking the entries of $\singmat$.
    The learned adapters $\boldsymbol\Delta$ are used as correction for any compression level.}
    \label{fig:method}
\end{figure*}
}


\newcommand{\figureProgressiveShrinkage}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/param_shrinkage.pdf}
    \caption{Progressive shrinkage of exemplary mask para\-meters $\maskvecraw$ in \texttt{Attn-V} layer $l=30$ of \textbf{LLaMA-7B} based on \cref{eq:shrinkage}. 
    The starting points of shrinkage are predictive of the pruning order, a typical phenomenon in $\ell_1$-regularization. In \methodname, this pruning order determines the score of associated singular values $\singvec$ (compare \cref{alg:score_update}).}
    \label{fig:progressiveShrinkage}
\end{figure}
}

\newcommand{\figureTradeoffCFour}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/all_models_ppl_c4.pdf}
    \caption{Compression-performance trade-off curves generated by \methodname on \textbf{C4}. Compared to \cref{fig:firstpage}, we plot over the actual model parameter sizes to enable cross-model comparison. Square marks denote the base model performance.}
    \label{fig:tradeoff:c4}
\end{figure}
}

\newcommand{\figureTradeoffLMEval}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/all_models_lm_eval_average.pdf}
    \caption{Compression-performance trade-off curves generated by \methodname, using average accuracy on all \textbf{LM-Eval} tasks as metric.}
    \label{fig:tradeoff:lmeval}
\end{figure}
}

\newcommand{\figureFinetuningQuant}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/FT+QZ_ppl_c4_eff_model_size.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4} showing the impact of \textbf{fine-tuning} and \textbf{quantization} \emph{after} compression with \methodname. The horizontal axis measures size in terms of required (weight) memory to visualize the gains of quantization more clearly.}
    \label{fig:finetuning_quant}
\end{figure}
}

\newcommand{\figureQuantTransfer}{
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/experiments/transfer.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4}, showing that \textbf{quantization} \emph{before} \methodname leads to similar results as without.}
    \label{fig:transfer}
\end{figure}
}

\newcommand{\figureAblationNoLora}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/no_lora.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4} with and without using a LoRA-adapter for correction in \methodname.}
    \label{fig:ablation:nolora}
\end{figure}
}

\newcommand{\figureAblationSweepStop}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/sweep_stop.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4}, using different stopping compression ratios~ $r_{\text{stop}}$ for \methodname.}
    \label{fig:ablation:sweep_stop}
\end{figure}
}

\newcommand{\figureAblationStopScore}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/stop_score.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4}, stopping updates of the score map before the actual stopping criterion of \methodname.}
    \label{fig:ablation:stop_score}
\end{figure}
}

\newcommand{\figureAblationSVScoreMap}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/sv_score_map.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4}, using a trivial score map based on the initial singular values of the base model.}
    \label{fig:ablation:sv_score_map}
\end{figure}
}

\newcommand{\figureAblationPostTuning}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/post_train.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA-7B} on \textbf{C4} with different numbers of post-tuning steps in \methodname.}
    \label{fig:ablation:post_tuning}
\end{figure}
}

\newcommand{\figureAblationUpProj}{
\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{assets/experiments/up_proj.pdf}
    \caption{Compression-performance trade-off curves for \textbf{LLaMA2-13B} on \textbf{C4}, (not) ignoring the up projection layers in \methodname.}
    \label{fig:ablation:up_proj}
\end{figure}
}

\newcommand{\figureTradeoffSupplement}{
\begin{figure}[p]
    \centering
    \begin{subfigure}[WikiText-2]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_ppl_wikitext.pdf}
        \label{fig:tradeoff:wikitext}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[ARC Challenge]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_arc_challenge.pdf}
        \label{fig:tradeoff:arc_c}}
    \end{subfigure}

    \begin{subfigure}[ARC Easy]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_arc_easy.pdf}
        \label{fig:tradeoff:arc_e}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[HellaSwag]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_hellaswag.pdf}
        \label{fig:tradeoff:hella}}
    \end{subfigure}

    \begin{subfigure}[MathQA]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_mathqa.pdf}
        \label{fig:tradeoff:mathqa}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[OpenBookQA]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_openbookqa.pdf}
        \label{fig:tradeoff:openb}}
    \end{subfigure}

    \begin{subfigure}[PIQA]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_piqa.pdf}
        \label{fig:tradeoff:piqa}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Winogrande]{
        \includegraphics[width=.43\textwidth]{assets/experiments/all_models_lm_eval_winogrande.pdf}
        \label{fig:tradeoff:wino}}
    \end{subfigure}
    \caption{Compression-performance trade-off curves generated by \methodname on \textbf{WikiText-2} and \textbf{individual LM-Eval tasks}, complementing the results of \cref{fig:tradeoff:c4} and \cref{fig:tradeoff:lmeval}.}
    \label{fig:tradeoff:all}
\end{figure}
}

\newcommand{\figureScoreMapLlama}{
\begin{figure}[p]
    \centering
    \begin{subfigure}[Down Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_down_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Up Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_up_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Gate Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_gate_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Attention-O]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_o_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Attention-V]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_v_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Attention-Q]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_q_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Attention-K]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_llama1_7b_k_proj.pdf}}
    \end{subfigure}
    \caption{Example score maps generated by \methodname for \textbf{LLaMA-7B}. The negative values (cf.~\cref{alg:score_update}) are normalized to $-1$ for the purpose of visualization.}
    \label{fig:score_map:llama7b}
\end{figure}
}

\newcommand{\figureScoreMapQwen}{
\begin{figure}[p]
    \centering
    \begin{subfigure}[Down Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_down_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Up Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_up_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Gate Projections]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_gate_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Attention-O]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_o_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Attention-V]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_v_proj.pdf}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[Attention-Q]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_q_proj.pdf}}
    \end{subfigure}

    \begin{subfigure}[Attention-K]{
        \includegraphics[width=.43\textwidth]{assets/experiments/score_map_qwen25_7b_k_proj.pdf}}
    \end{subfigure}
    \caption{Example score maps generated by \methodname for \textbf{Qwen2.5-7B}. The negative values (cf.~\cref{alg:score_update}) are normalized to $-1$ for the purpose of visualization.}
    \label{fig:score_map:qwen25_7b}
\end{figure}
}
 
%
\begin{abstract}
The adoption of Foundation Models in resource-constrained environments remains challenging due to their large size and inference costs. A promising way to overcome these limitations is post-training compression, which aims to balance reduced model size against performance degradation. This work presents \methodnamefull (\methodname), a novel algorithmic approach to determine a compression-performance trade-off from a single stochastic gradient descent run. To ensure parameter efficiency, we use an SVD-reparametrization of linear layers and iteratively prune their singular values with a sparsity-inducing penalty. The resulting pruning order gives rise to a global parameter ranking that allows us to materialize models of any target size. Importantly, the compressed models exhibit strong predictive downstream performance without the need for costly fine-tuning. We evaluate \methodname on a large selection of open-weight LLMs and tasks, and demonstrate state-of-the-art results compared to existing factorisation-based compression methods. We also show that \methodname seamlessly complements common quantization-based compression techniques.
\end{abstract}
 %
\section{Introduction}
\label{sec:intro}

\figureFirstpage
\figureSecondpage

Post-training compression of Foundation Models, especially large language models (LLMs), promises access to powerful tools where resources are limited, e.g., in e-commerce, mobile deployments, or on the shop floor. Typical reasons for scarcity include constrained access to hardware, monetary limitations, high inference speed requirements, and environmental concerns \cite{hohman_model_2024}.
However, model compression is not for free --- it typically comes with a trade-off between model size and desired downstream performance. 
In order to support decision-making, it is of fundamental importance to find the right balance between these two objectives.
In fact, when deploying large-scale models, questions like the following ones arise:
\begin{compactquote}
    \emph{%
    I am willing to give up on $X\%$ of model performance, how much can I compress model $Y$?
    \\[\medskipamount]
    My infrastructure allows me to run an $N$ parameter model, what is the best model within this budget?
    \\[\medskipamount]
    What are the minimal hardware requirements for an LLM with a benchmark performance of at least $Z$?
    }
    \vspace{-1pt}
\end{compactquote} 
To make model compression more accessible, the algorithmic characterization of the underlying trade-off should be a high priority in the development of new algorithms \cite{boggust_compress_2025}. 
In the scientific literature, however, this aspect is often secondary, and instead, performance is compared at preset compression levels \cite{zhu_survey_2024}.

Apart from resource-intensive model distillation \cite{hinton_distilling_2015}, there are currently two dominating approaches to efficient post-training model compression: (i) model quantization, which limits weight fidelity by reducing numerical precision, and 
(ii) (un-)structured weight pruning which aims to delete unnecessary or redundant weights from a model \cite{zhu_survey_2024}. While many of these approaches lead to good compression results, they typically allow only for constant compression factors in case of quantization or require specialized hardware, e.g., to implement $n{:}m$-sparsity in unstructured pruning or low-bit representations in quantization. Due to these structural constraints, such methods do not enable a full characterization of the compression-performance trade-off for a given base model.

Inspired by the effectiveness of low-rank adapters in LLMs \cite{hu_lora_2021}, factorization-based compression can be applied without any integral changes to a network's architecture, by simply reparametrizing linear layers. This particular form of structured pruning allows, in principle, for compression factors on a fine-grained scale without additional hardware constraints.
However, the performance of existing algorithms falls behind the above-described compression techniques, so that competitive performance for stronger size-reduction is only achieved with additional fine-tuning \cite{wang_svd-llm_2024, yuan_asvd_2024,jaiswal_galore_2024}.

In this work, we follow the promising idea of factorization-based compression and find a positive answer to our initial quest:
\begin{compactquote}
    \textit{Design an efficient compression procedure that algorithmically determines the compression-performance trade-off for a given pre-trained model.}
\end{compactquote}
The basic idea of our proposed method, \methodnamefull (\methodname),\!\footnote{\textit{Pronounced} like ``a sip'' of coffee.} is to induce sparsity in the singular values of the weight matrices, which leads to low-rank and therefore compressed parametrizations. Instead of layer-wise updates, we rely on a simple end-to-end objective function that is optimized through stochastic gradient descent.
Crucially, the optimization trajectory of the pruned singular values gives rise to a score map representing a global importance ranking across all linear network layers.
This allows us to materialize compressed models of any target size without additional computation and produce trade-off curves like in \cref{fig:firstpage}.
See also \cref{fig:method} for a visualization of \methodname and \cref{sec:method} for a more detailed description.

We will demonstrate that \methodname satisfies the following practical requirements, which at the same time, also form the key contributions of our work:
\begin{description}[topsep=0pt,itemsep=0pt]
\item[Easy-to-Use \& Fast]
    \methodname allows practitioners to compress pre-trained models to \emph{any} size from a single optimization run. It is parameter-efficient, requires no hyperparameter tuning and no feature engineering due to its end-to-end nature.
\item[Flexible \& Complementary] 
    \methodname can be applied to any model that has linear layers as central building blocks without further structural assumptions. It can be used with arbitrary task loss functions and seamlessly complements quantization-based compression approaches.
\item[Effective \& Robust] 
    \methodname exhibits state-of-the-art results among factorization-based pruning approaches. It provides consistent and robust compression-performance trade-offs for a variety of popular LLMs and zero-shot evaluation tasks. 
\end{description}
%
Our main results supporting these claims are presented in \cref{sec:experiments}.
The interested reader is also referred to the supplementary material, where we address some design decisions and conceptual aspects (\cref{sec:appendix:qa}), provide implementation and runtime details (\cref{sec:appendix:implementation}), as well as additional experiments (\cref{sec:appendix:experiments:supp} and \cref{sec:appendix:experiments:further}).
 %
\section{Method}
\label{sec:method}
\methodname is an easy-to-use method to build models at \textit{any compression} rate based on an SVD-weight factorization (\cref{sec:svd}). 
We leverage ideas from sparse recovery (\cref{sec:l1reg}) to iteratively prune model parameters and then use the resulting pruning order to derive a global importance ranking (\cref{sec:parameter_scoring}).
With these preliminaries at hand, the algorithmic details of \methodname are formulated in \cref{sec:acip}.

\subsection{Preliminaries}
\label{sec:method:prelim}

\subsubsection{Score-based Parameter Pruning}
\label{sec:parameter_scoring}
As motivated in the introduction, the goal of this work is to determine the compression-performance trade-off of a pre-trained model. To do this, we use score-based parameter pruning, a framework that has been successfully applied to model compression since the 1980s \cite{lecun_optimal_1989, hassibi_optimal_1993}. In score-based pruning, a score map~$\boldsymbol\rho$ is created that assigns an importance score~$\rho_i$ to all target parameters~$\theta_i$. Naturally, this approach gives rise to a (global) ranking of parameters that allows for model compression at any desired rate.

There are many ways to design useful score maps. For example, they can be derived based on curvature at a local optimum \cite{lecun_optimal_1989,hassibi_optimal_1993} or by using hand-designed local features \cite{sun_simple_2024,frantar_sparsegpt_2023}. 
In \cref{sec:iterative_scoring}, we present a novel, data-driven approach to score maps that does not require any handcrafting or feature engineering.

\subsubsection{Compressing Linear Layers}
\label{sec:svd}
This work builds on the compression of the weight matrices in linear layers of a model. These weights typically account for the largest portion of parameters in deep learning models.
We consider linear layers of the form 
\begin{equation}
    \label{eq:linear_layer}
    \yy = \W \xx + \bb,
\end{equation}
where $\W$ is a $m \times n$ matrix and $\bb$ is a bias term. 
We aim for a \emph{low-rank compression} based on a matrix factorization such that
\begin{equation}
    \label{eq:lowrank_linear_layer}
    \yy = \PP \QQ \xx + \bb,
\end{equation}
with $\PP$ and $\QQ$ being matrices of sizes
$m \times k$ and $k \times n$, respectively,
where $k \ll \min(m,n)$. 
%
The layer parametrization in \eqref{eq:lowrank_linear_layer} may
be interpreted as a (lossy) compression of the parametrization in \eqref{eq:linear_layer},
leading to a smaller memory footprint whenever $k(m+n) < m n$. Note that matrix factorization can be used to compress the parameters of any linear layer, including dense layers as well as efficiently parametrized layers such as convolutional layers \cite{idelbayev_low-rank_2020}. In this work, we focus on the compression of dense layers in transformers \cite{vaswani_attention_2017}.

\paragraph{Singular Value Decomposition}
For low-rank factorizations in \methodname, we choose the \emph{singular value decomposition} (SVD) which has been used extensively in model compression \cite{idelbayev_low-rank_2020,yuan_asvd_2024, wang_svd-llm_2024}. SVD factorizes any $m \times n$ weight matrix $\W_l$ of rank $r \leq \min(m,n)$ at layer $l$ as
\begin{equation}
\label{eq:svd}
    \W_l = \U_l \singmat_l \V_l\transpose,
\end{equation}
where $\U_l$ and $\V_l$ are $m \times r$ and $n \times r$ matrices of (orthonormal) singular vectors, and $\singmat_l$ is a $r \times r$ diagonal matrix containing the singular values $\singvec_l^{(i)} > 0$.
Note that we have used the 
compact SVD which ignores the null-space vectors of the full SVD. 
We may reduce the inner dimension $r$ of the SVD to $k < r$ with
\begin{equation}
    \W_l \approx \trunc{\U}_l \trunc{\singmat}_l
    \trunc{\V}_l\transpose
\end{equation}
by defining
$\trunc{\U}_l$ and $\trunc{\V}_l\transpose$ to only contain $k$ singular vectors associated with a selected subset of singular values. 
To preserve the matrix norm of $\W_l$, selecting the $k$ largest singular 
values leads to an optimal approximation of $\W_l$ under rank constraints \citep{mirsky_symmetric_1960}. It has been previously argued, however, that this approach does not yield satisfactory results in deep learning as is does not take into account the (training) data \cite{hsuLanguageModelCompression2022}. Different approaches have been presented to address this problem \citep{hsuLanguageModelCompression2022,yuan_asvd_2024, wang_svd-llm_2024, chen_drone_2021}. 

\subsubsection{$\ell_1$-regularization}
\label{sec:l1reg}
Consider a generic loss functional $\mathcal{L}(\X; \boldsymbol\theta)$ evaluated on some model that is specified through a parameter vector $\boldsymbol{\theta}$ and data $\X$.
During model training, sparsity in $\boldsymbol{\theta}$ is typically encouraged by solving the penalized optimization problem
\begin{equation}
    \label{eq:lasso}
\min_{\boldsymbol\theta} \mathcal{L}\bigl(\X; \boldsymbol\theta\bigr) + \lambda \left\lVert \boldsymbol\theta \right\rVert_1, \ \quad \lambda > 0, 
\end{equation}
which is known as $\ell_1$-regularization, or
\emph{least absolute shrinkage and selection operator} (LASSO) in case of (generalized) linear models \cite{tibshirani_regression_1996,hastie_statistical_2015}. Feature selection plays a key role in this optimization problem. Consider the solution to \eqref{eq:lasso} as a function of regularization parameter $\lambda$: Increasing $\lambda$ leads to a sparser solution in $\boldsymbol{\theta}$. This effectively gives a ranking of feature importances for the solution of \eqref{eq:lasso} through the so-called regularization path \cite{tibshirani_regression_1996,efron_least_2004, mairal_complexity_2012} --- an idea that inspired our approach. Furthermore, previous results indicate that optimization paths of iterative schemes can be related to regularization paths \citep{suggala_connecting_2018}. Intuitively, this suggests that general optimization paths of iterative schemes for $\ell_1$-objectives reveal information about feature importance in the context of sparse models as well. 

In this work, we will successively increase $\lambda$ to introduce
a higher degree of model compression in terms of parameter sparsity in a controlled manner (see also \cref{rmk:lambda_scheduler} below).

\subsection{\emph{\methodnamefull} (\methodname)}
\label{sec:acip}
We now present \methodnamefull (\methodname), which consists of the following three key steps:
\begin{description}[topsep=0pt,itemsep=0pt]
    \item[Step 1. (Model Reparametrization)]
    Apply SVD to the weights $\W_l$ of all (dense) linear layers according to~\eqref{eq:reparam}, and introduce low-rank adapters and singular value masks as tunable parameters.
    \item[Step 2. (Scoring via Iterative Pruning)] Choose a surrogate loss $\mathcal{L}$ and a calibration data set $\X$. Perform iterative pruning of singular values and simultaneous training of low-rank adapters by applying gradient-based optimization as in \eqref{eq:shrinkage}. Obtain a parameter score map $\boldsymbol\rho$ by using \cref{alg:score_update}.
    \item[Step 3. (Any Compression)] Choose any desired compression rate. Use score map $\boldsymbol\rho$ and low-rank adapters $\boldsymbol\Delta$ to materialize the compressed model. 
    %
\end{description}

We provide a more detailed description of these steps in the following.
For a visual presentation of \methodname, we refer back to \cref{fig:method}.

\subsubsection{Step 1. Model Reparametrization}
\label{sec:model_reparametrization}

We start by reparametrizing all linear layers of a network\footnote{Following common practice, we ignore the embedding layer and classification head in (decoder-only) transformers.} using SVD as described in \eqref{eq:svd} and assign
\begin{equation}
\label{eq:reparam}
    \W_l \leftarrow \U_l \maskmat_l \singmat_l \V_l\transpose + \boldsymbol\Delta_l,
\end{equation}
where $\maskmat_l$ is a binary diagonal matrix with entries $\maskvec^{(i)}_l \in \{0,1\}$ masking the singular values $\singvec_l^{(i)}$ in $\singmat$, and $\boldsymbol\Delta_l$ is a low-rank adapter (LoRA) \cite{hu_lora_2021}. We find that the addition of a low-rank adapter helps to compensate for potential errors that are introduced through pruning. We initialize $\maskmat_l$ to be the identity matrix and $\boldsymbol\Delta_l$ to zero weights. In this way, the reparametrized model is initialized to be identical to the original model up to numerical precision.

We assign the binary masks $\maskvec_l^{(i)}$ such that $\tilde{\singvec}^{(i)}_l = \maskvec^{(i)}_l \cdot \singvec^{(i)}_l$ represents the pruned or retained singular values, respectively. Thus, $\maskvec^{(i)}_l$ decouples the magnitude of a singular value and the pruning decisions based on its importance. 

The above parametrization leads to a parameter-efficient compression. Indeed, given an $m \times n$ matrix $\W$, the number of non-zero singular values is bounded by $\min(m, n)$, which means the number of tunable mask parameters scales linearly in the feature dimensions.

\paragraph{Mask parametrization}
We parametrize binary masks through a thresholding operation of the form
\begin{equation}
    \label{eq:mask_parametrization}
    \maskvec_l^{(i)} = \begin{cases}
        0, & \text{for } \maskvecraw_l^{(i)} \leq 0\\
        1, & \text{for } \maskvecraw_l^{(i)} > 0
    \end{cases},
\end{equation}
where $\maskvecraw_l^{(i)}$ are scalar learnable parameters. As this operation is not differentiable, we use the straight-through estimator for backpropagation \cite{bengio_estimating_2013, yin_understanding_2018}.

\subsubsection{Step 2. Scoring via Iterative Pruning}
\label{sec:iterative_scoring}

Our goal is to find a global score map over all singular values in reparametrized layers that will guide model compression in Step 3. Leveraging the sparsity-inducing property of $\ell_1$-regularization (see \cref{sec:l1reg}), we progressively shrink the mask parameters $\maskvecraw_l^{(i)}$ to zero and build a score map based on the pruning order. As such, iterative scoring has two key components: (i) $\ell_1$-regularized iterative pruning, and (ii) the generation of the score map. We next describe each component in more detail.

\figureProgressiveShrinkage

\paragraph{Iterative Pruning}
The optimization problem solved by \methodname takes the form
\begin{equation}
\label{eq:shrinkage}
\min_{\maskvecraw, \boldsymbol\Delta} \mathcal{L}\bigl(\X; \boldsymbol\theta, \maskvecraw, \boldsymbol\Delta\bigr) + \lambda \left\lVert \maskvecraw \right\rVert_1,
\end{equation}
where $\mathcal{L}$ denotes a suitable calibration loss for the model,
$\maskvecraw = \{\maskvecraw_0, \dots, \maskvecraw_L\}$ the set of all mask parameters,
$\boldsymbol\Delta = \{\boldsymbol\Delta_0, \dots, \boldsymbol\Delta_L\}$ the set of all low-rank adapters, 
and $\boldsymbol\theta$ the set of all remaining model parameters that are frozen during optimization. 
We perform optimization until a preset stopping compression rate $r_{\text{stop}}$ is reached (see \cref{sec:appendix:experiments:further:stop} for further discussion).
Optionally, we perform post-tuning for a fixed number of steps by freezing the masks and continue optimizing the low-rank adapters.

\begin{remark}[Scaling of $\lambda$]\label{rmk:lambda_scheduler}
If $\lambda$ is chosen too small, the stopping rate $r_{\text{stop}}$ might 
never be reached. If $\lambda$ is too large, training might become 
unstable and the score map ambiguous. Therefore, we use a simple \emph{linear scheduler} that scales $\lambda$ by a fixed factor $>$1 every $j$ optimization steps, so that pruning becomes increasingly aggressive over time. 
\end{remark}

\paragraph{From Iterative Pruning to Score Map}
We use the optimization process of \eqref{eq:shrinkage} to inform our score map.
Based on the discussion of \cref{sec:l1reg}, we hypothesize that there is a close relationship between the order in which the parameters~$\maskvecraw_l^{(i)}$ are pruned and their importance for the model --- the least important parameters are pruned first and so on. During experimentation, we observed a shrinkage behavior that supports this hypothesis (see \cref{fig:progressiveShrinkage}).

\cref{alg:score_update} describes how the score map is updated after each optimization step. In plain words, the score map is built based on the pruning order. A negative number in the map indicates how many steps ago a parameter was pruned. For all parameters that have not been pruned, the score is set to the value of the corresponding parameter.
We refer to \cref{sec:appendix:experiments:further:score_maps} for visual examples of \methodname score maps.

\cref{alg:score_update} ensures that (i) the score map stores the pruning history, and (ii) it estimates future pruning based on the parameter magnitudes. Note that absolute values of the score are irrelevant for parameter ranking.

\begin{algorithm}[t]
\caption{Score map generation}
\label{alg:score_update}
\begin{lstlisting}[style=Pytorch,keepspaces=True]
# params: parameters p, vectorized;
# scores: pruning scores, vectorized;
#         initialized to NaN

def update_scores(scores, params):
 # previously pruned parameters
 score_mask = scores <= 0.
 scores[not score_mask] = params[not score_mask]
 scores[score_mask] -= 1.
 return scores
\end{lstlisting}
\end{algorithm}
\subsubsection{Step 3. Any Compression}
\label{sec:compression}
From the optimization of \eqref{eq:shrinkage}, we obtain two components: (i) the score map that informs model compression and (ii)~the tuned low-rank adapters $\boldsymbol\Delta$. The pruned masks $\maskvec$ are discarded, as they are irrelevant for compression at this stage (cf.~\cref{fig:method}). As motivated above, the score map allows us to globally rank all singular vectors based on their score. We can flexibly create a model for any compression rate $r$ by pruning the singular values
$\singvec$ (and corresponding singular vectors) according to their score $\rho_l^{(i)}$.
Note that there is a monotonic, but non-linear relationship between the number of total pruned singular values $k$ and compression rate $r$ (cf.~\cref{sec:svd}). 
Given a target rate $r$, we can easily find $k$ via a binary search.

Finally, for layers with determined rank $k \geq \frac{mn}{m+n}$, we
avoid excess storage required by the reparametrization by simply recovering $\W$ from its SVD components.
 %
\section{Related work}
\label{sec:related}
Post-training compression methods for Foundation Models largely fall into three categories: knowledge distillation, quantization and (un-)structured pruning. For a comprehensive survey covering all of these, we refer to \cite{zhu_survey_2024} and the references therein.

\methodname represents a specific form of structured pruning: factorization-based compression. Conventional structured pruning \cite{frantar_sparsegpt_2023, xia_sheared_2024, ashkboos_slicegpt_2024} jointly removes groups of parameters (e.g., network layers or matrix columns/rows). At high compression rates, however, such coarse approaches often remove important weights, causing a significant performance drop that is only recoverable through additional fine-tuning.

\paragraph{Low-Rank Decomposition}
Approximation of weight matrices through appropriate factorization aims to preserve critical information while being simple to implement. 
After initial exploration for smaller language models \cite{edalati_kronecker_2022, tahaei_kroneckerbert_2022}, methods for LLMs primarily built on (weighted) SVD of linear layers \cite{ben_noach_compressing_2020, hsuLanguageModelCompression2022}. Large approximation errors, however, made additional fine-tuning on down-stream tasks necessary.

Follow-up work recognized that the poor approximations are caused by LLM weights being high-rank and instead turned to decompose network features which are sparse \cite{kaushal_lord_2023, yu_compressing_2023}. 
Notably, ASVD \cite{yuan_asvd_2024} explicitly accounts for the data distribution eliciting these activations and SVD-LLM \cite{wang_svd-llm_2024} derives an analytical layer-wise correction.

Recent studies \cite{sharma_truth_2023, yuan_asvd_2024, jaiswal_galore_2024} have shown rank reduction to differently affect layers in a network and proposed different heuristics for non-uniform pruning of singular values. 

A common feature of these works is that they first truncate singular values to a desired size before computing an error correction. A change in compression ratio therefore prompts another round of computation. At the same time, the correction quality varies across different ratios, so that milder compression may not automatically preserve more predictive quality. In contrast, our approach creates consistent compression trade-offs from a single round of computation (optimization) and performs corrections on-the-fly.

\figureTradeoffCFour

\paragraph{Rate-distortion theory}

Rate-distortion theory investigates the analytical trade-off between
achievable data compression rates and the error (distortion) 
introduced by general lossy compression
algorithms \citep{cover_elements_2006}.
While some recent work \citep{gao_rate_2019,isik_information-theoretic_2022}
investigates rate-distortion theory of machine learning models
for simple architectures under rather specific assumptions, 
the information-theoretic limits of neural network compression
are generally unknown in practically relevant settings.
In this context, the family of compressed models generated by \methodname conveniently provides an empirical (upper) bound on the distortion-rate function of a large-scale model from a single optimization run.
 %
\section{Experiments}
\label{sec:experiments}

\paragraph{Experimental Setup}
\begin{table*}[t]
    \caption{Zero-shot evaluation of \textbf{LLaMA-7B}, comparing with existing SVD-based compression methods ASVD \cite{yuan_asvd_2024} and SVD-LLM \cite{wang_svd-llm_2024}. 
    $\uparrow$: larger is better; $\downarrow$: smaller is better; best results for each task and size ratio are marked in \textbf{bold}.
    The scores for ASVD and SVD-LLM are taken from \citet{wang_svd-llm_2024}.} \label{tab:competitors}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{5.6pt}
    \def\arraystretch{1.2}
    %
\clearpage{}%
\begin{tabular}{l|l|rr|rrrrrrrr}
\toprule
 &  & C4 ↓ & WikiText-2 ↓ & Openb. ↑ & ARC\_e ↑ & WinoG. ↑ & HellaS. ↑ & ARC\_c ↑ & PIQA ↑ & MathQA ↑ & LM Eval Avg. ↑ \\
Size & Method &  &  &  &  &  &  &  &  &  &  \\
\midrule
100\% & Original & \bfseries 7.34 & \bfseries 5.68 & \bfseries 0.28 & \bfseries 0.67 & \bfseries 0.67 & \bfseries 0.56 & \bfseries 0.38 & \bfseries 0.78 & \bfseries 0.27 & \bfseries 0.52 \\
\cline{1-12}
\multirow[c]{3}{*}{80\%} & ASVD & 15.93 & 11.14 & 0.25 & 0.53 & \bfseries 0.64 & 0.41 & 0.27 & 0.68 & \bfseries 0.24 & 0.43 \\
 & SVD-LLM & 15.84 & \bfseries 7.94 & 0.22 & 0.58 & 0.63 & 0.43 & 0.29 & 0.69 & \bfseries 0.24 & 0.44 \\
 & \texttt{ACIP} (ours) & \bfseries 10.92 & 8.83 & \bfseries 0.28 & \bfseries 0.66 & 0.63 & \bfseries 0.49 & \bfseries 0.32 & \bfseries 0.74 & 0.23 & \bfseries 0.48 \\
\cline{1-12}
\multirow[c]{3}{*}{70\%} & ASVD & 41.00 & 51.00 & 0.18 & 0.43 & 0.53 & 0.37 & 0.25 & 0.65 & 0.21 & 0.38 \\
 & SVD-LLM & 25.11 & \bfseries 9.56 & 0.20 & 0.48 & 0.59 & 0.37 & 0.26 & 0.65 & 0.22 & 0.40 \\
 & \texttt{ACIP} (ours) & \bfseries 12.22 & 10.35 & \bfseries 0.28 & \bfseries 0.64 & \bfseries 0.62 & \bfseries 0.47 & \bfseries 0.31 & \bfseries 0.73 & \bfseries 0.23 & \bfseries 0.47 \\
\cline{1-12}
\multirow[c]{3}{*}{60\%} & ASVD & 1109.00 & 1407.00 & 0.13 & 0.28 & 0.48 & 0.26 & 0.22 & 0.55 & 0.19 & 0.30 \\
 & SVD-LLM & 49.83 & 13.11 & 0.19 & 0.42 & 0.58 & 0.33 & 0.25 & 0.60 & 0.21 & 0.37 \\
 & \texttt{ACIP} (ours) & \bfseries 13.91 & \bfseries 12.46 & \bfseries 0.25 & \bfseries 0.61 & \bfseries 0.59 & \bfseries 0.44 & \bfseries 0.30 & \bfseries 0.71 & \bfseries 0.24 & \bfseries 0.45 \\
\cline{1-12}
\multirow[c]{3}{*}{50\%} & ASVD & 27925.00 & 15358.00 & 0.12 & 0.26 & 0.51 & 0.26 & 0.22 & 0.52 & 0.19 & 0.30 \\
 & SVD-LLM & 118.57 & 23.97 & 0.16 & 0.33 & 0.54 & 0.29 & 0.23 & 0.56 & 0.21 & 0.33 \\
 & \texttt{ACIP} (ours) & \bfseries 16.47 & \bfseries 16.16 & \bfseries 0.21 & \bfseries 0.57 & \bfseries 0.57 & \bfseries 0.40 & \bfseries 0.27 & \bfseries 0.68 & \bfseries 0.22 & \bfseries 0.42 \\
\cline{1-12}
\multirow[c]{3}{*}{40\%} & ASVD & 43036.00 & 57057.00 & 0.12 & 0.26 & 0.49 & 0.26 & 0.21 & 0.51 & 0.18 & 0.29 \\
 & SVD-LLM & 246.89 & 42.30 & 0.14 & 0.28 & 0.50 & 0.27 & 0.22 & 0.55 & 0.21 & 0.31 \\
 & \texttt{ACIP} (ours) & \bfseries 21.05 & \bfseries 23.99 & \bfseries 0.19 & \bfseries 0.49 & \bfseries 0.55 & \bfseries 0.35 & \bfseries 0.24 & \bfseries 0.64 & \bfseries 0.21 & \bfseries 0.38 \\
\cline{1-12}
\bottomrule
\end{tabular}
\clearpage{}%
%

\end{table*}
To demonstrate effectiveness across architectural differences in LLMs, we evaluate \methodname on a range of popular open-weight models: LLaMA-7B/13B \cite{touvron_llama_2023-1}, LLaMA-2-7B/13B \cite{touvron_llama_2023}, LLaMA-3.1-8B \cite{grattafiori_llama_2024}, Qwen2.5-7B/14B \cite{qwen_qwen25_2024}, and Mistral-7B-v0.3 \cite{jiang_mistral_2023}. More detailed analyses and ablations are carried out with LLaMA-7B as it was most extensively studied in previous research on structured weight pruning.
Regarding evaluation tasks, we follow \cite{wang_svd-llm_2024} and report perplexity on validation held-outs of C4 \cite{raffel_exploring_2019} and WikiText-2 \cite{merity_pointer_2016}, and consider seven zero-shot tasks from EleutherAI LM Evaluation Harness (LM-Eval) \cite{eval-harness}.
Implementation details about \methodname and choices of hyperparameters can be found in \cref{sec:appendix:implementation}.
\figureTradeoffLMEval

\subsection{Analyzing Compression-Performance Trade-Offs}
\label{sec:experiments:tradeoffs}
Picking up the introductory discussion of \cref{fig:firstpage}, we now study compression-performance trade-offs of \methodname in more detail. \cref{fig:tradeoff:c4} and \cref{fig:tradeoff:lmeval} confirm smooth and consistent curve shapes for all considered models; analogous results for Wikitext-2 and individual zero-shot LM-Eval tasks can be found in \cref{fig:tradeoff:all}. A remarkable observation is that the oldest models, LLaMA-7B/13B, perform best perplexity-wise, while newer, more capable models like Qwen2.5-7B/14B dominate on LM-Eval as expected, especially on the lower compression levels.
This apparent contradiction is likely caused by a deviation of the LLM pre-training data distributions from C4. In fact, more recent models use much larger and more diverse training corpora.

A second noteworthy outcome of \cref{fig:tradeoff:c4} and \cref{fig:tradeoff:lmeval} are the gaps between LLMs of different base model sizes in the same family. 
Indeed, \methodname cannot match the performance of base models of smaller size, e.g., compare the compressed Qwen2.5-7B with the original Qwen2.5-3B. This is not surprising because the corresponding smaller-size base models were obtained by pre-training or knowledge distillation, which are orders of magnitudes more expensive than \methodname.

\paragraph{Comparison to Existing Works}
\label{sec:experiments:sota}

We now compare \methodname to two recent works focusing on SVD-based structured pruning, namely ASVD \cite{yuan_asvd_2024} and SVD-LLM \cite{wang_svd-llm_2024}. Both approaches are backpropagation-free and perform (activation-aware) layer-wise updates instead. \cref{tab:competitors} shows that \methodname consistently outperforms both methods with a growing gap for higher compression levels. Note that SVD-LLM was calibrated on Wikitext-2 instead of C4, which might explain slightly better results on the former dataset for 70\% and 80\% size.
We think that these results underpin the benefits of an end-to-end scheme: (i) a simultaneous correction, e.g., by LoRA, can drastically improve performance, and (ii) robust pruning patterns can be found without leveraging any specific features of the SVD factorization.
Moreover, note that recomputations are required to generate each row of \cref{tab:competitors} for ASVD and SVD-LLM, whereas \methodname only needs a single run. 
Analogous results for \methodname applied to all other models can be found in \cref{tab:all_models}.

\subsection{Improving Performance Through Fine-Tuning}
\label{sec:experiments:finetuning}
While the main goal of this work is to produce a full family of accurate, compressed models from a few optimization steps, their performance can be certainly improved through continued fine-tuning.
\cref{fig:finetuning_quant} highlights the gains of fine-tuning LLaMA-7B; see \cref{tab:all_models} for more detailed numerical results on all other models. We observe that fine-tuning leads to a performance offset that is almost constant across all compression levels, which underlines the predictive capacity of \methodname.
Note that we even observe a jump at zero compression because inserting the low-rank adapter learned by \methodname leads to a slight initial performance drop.

We argue that post-compression fine-tuning should be still seen as an independent (and much more costly) algorithmic step for two reasons. (i) Its outcome strongly depends on the specific training protocol and data, making a fair comparison challenging (cf.~Q3 in \cref{sec:appendix:qa}); (ii) it requires us to fix a compression level, which breaks the crucial any-compression-feature of \methodname.

\subsection{Combining \methodname with Quantization}
\label{sec:experiments:quantization}

In the field of low-cost compression for LLMs, quantization is still considered as the gold standard \cite{hohman_model_2024, zhu_survey_2024}, so that a practitioner might not be willing to exchange its gains for the benefits of \methodname. Fortunately, \methodname only tunes a tiny fraction of weights with high precision, so that all remaining modules are suitable for quantization. In our experiments, we quantize all parameterized and unparametrized linear layers to $4$-bit in \texttt{fp4}-format \cite{dettmers_qlora_2023} using the \texttt{bitsandbytes}-Package (W4A16), except for the embedding layer and final classification head. We study the gains of quantization for \methodname in the following two ways.

\figureFinetuningQuant

\paragraph{Compress first, then quantize.}
We first apply \methodname as usual, compress the model to a given target size, and then quantize all remaining linear layers.
\cref{fig:finetuning_quant} confirms that this approach works fairly well, only producing a slight performance drop compared to non-quantized versions; see \cref{tab:quantization} for a full evaluation on all other metrics.
We also observe that an optional fine-tuning step as in \cref{sec:experiments:finetuning} can almost fully compensate for the errors introduced by quantization after compression. This finding is well in line with the effectiveness of the popular QLoRA approach \cite{dettmers_qlora_2023}.
Moreover, \cref{fig:finetuning_quant} reveals a drastic improvement through quantization in terms of required memory. Here, the \methodname-trade-off allows practitioners to study and apply a more fine-grained compression on top of quantization.

\paragraph{Quantize first, then compress and transfer.}
Compared to layer-wise methods like ASVD and SVD-LLM, \methodname has a higher demand in GPU memory due to backpropagation.
A quantization of all frozen weight matrices can be an effective remedy in this respect. For the experiment shown in \cref{fig:transfer}, we have applied quantization before \methodname, which leads to very similar compression-performance trade-offs as in the non-quantized case. 
Going one step further, we transfer the score maps and low-rank adapters from this quantized version of \methodname back to full precision: We load the base model in \texttt{bf16}, apply layer-wise SVD-parametrization, insert the low-rank adapters learned by quantized \methodname, and use the corresponding score map to obtain a compressed model (W16A16).
The resulting trade-off curve in \cref{fig:transfer} confirms that this simple strategy works fairly well, especially for lower compression levels. 

\figureQuantTransfer

\paragraph{Further Experiments and Ablations}
Several additional experiments are presented in \cref{sec:appendix:experiments:further}, analyzing the impact of several key components and design aspects of \methodname.
 %
\section{Conclusion}
\label{sec:discussion}

In this work, we have introduced \methodnamefull (\methodname), a simple end-to-end algorithm to determine the compression-performance trade-off of large pre-trained models. 
The underlying score map ranking allows us to materialize models of any compression rate from a single optimization run. 
We have demonstrated empirically that the downstream performance of the resulting models is superior to existing, layer-wise factorization approaches.
The flexibility and efficiency of \methodname make it a practical tool for deploying large-scale models in resource-constrained settings, especially in combination with other compression techniques such as quantization.

\paragraph{Limitations and Outlook} 
Our main results in \cref{fig:tradeoff:c4} and \cref{fig:tradeoff:lmeval} resemble the well-known phenomenon of scaling laws \cite{kaplan_scaling_2020, hoffmann_training_2022}. 
We believe that this association is not a coincidence. But revealing a more rigorous connection goes beyond the scope of this work.
In a similar vein, we observe that more recent models tend to be less compressible (e.g., compare the slopes of LLaMA-13B and Qwen2.5-14B in \cref{fig:tradeoff:lmeval}). We hypothesize that this relates to the fact that newer models carry more dense information per weight, since they were trained on much larger datasets \cite{allen-zhu_physics_2024}. Also, the distribution of the calibration dataset (C4 in our case) might play an important role in this context.

A notable limitation of our work is that we have only focused on models that are tunable on a single (NVIDIA H100) GPU in \texttt{bf16}-precision. Hence, the scaling behavior of \methodname for larger LLMs (30B+) remains to be explored.
Finally, we emphasize that \methodname could be transferred to other modalities, architectures, and tasks without any notable modifications.
 %
\section*{Software and Data}

Code will be made available on Github soon.

%
\section*{Acknowledgements}
The authors thank Brennan Wilkerson and Ziyad Sheebaelhamd for helpful discussions. 
We thank John Arnold and Jannis Klinkenberg for their technical cluster support.

We kindly acknowledge funding by the German Federal Ministry of Education and Research (BMBF) within the project ``More-with-Less: Effiziente Sprachmodelle für KMUs'' (grant no. 01IS23013A).
Moreover, we thank \text{WestAI} for providing compute resources that were used to conduct the numerical experiments of this work.


%

%
%
%
%
%
%
%
%
%
%

%

%
%
%

%
%
%
%
 
%
\bibliography{references}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
%
\appendix
\onecolumn
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}

\section{Question \& Answers}
\label{sec:appendix:qa}

In this section, we discuss a few common questions about our method and experimental design that were not (fully) addressed in the main part due to length restrictions.

\begin{itemize}
    \question{Why did you not directly compare your results to quantization and full-weight (unstructured) pruning?}
    \answer{We argue that these are fundamentally different compression approaches.
    Full weight manipulations, in principle, have the potential to lead to more powerful compressions because they have more degrees of freedom (analogously to full-weight fine-tuning vs.~PEFT).
    Therefore, they should not be seen as competing methods but complementary ones. 
    We admit that practitioners probably would not favor \methodname over well-established and widely supported quantization techniques. 
    However, the adapter-style nature of \methodname makes it suitable for a combination which can lead to extra gains, as demonstrated in \cref{sec:experiments:quantization}.
    }

    \question{Why did you not compare with model distillation or combined \methodname with it?}
    \answer{
        While model distillation can lead to outstanding compression results, e.g., see \cite{Raschka_2024}, this approach requires significantly more resources than \methodname, typically orders of magnitudes more.
        A direct comparison is therefore not meaningful from our point of view, as it should at least be based on approximately the same computational budget.
    }

    \question{Why did you not compare your fine-tuning results with other factorization-based methods?}
    \answer{
        Some previous works on SVD-based pruning \cite{wang_svd-llm_2024,jaiswal_galore_2024} report results on post-compression fine-tuning as well.
        While our own results in \cref{tab:all_models} are highly competitive in this respect, we decided to omit a direct comparison because: (i) the downstream performance strongly depends on the exact fine-tuning protocol (datasets, compute budget, hyperparameters, etc.) which we were not able to fully recover from published code; and (ii) as pointed out in \cref{sec:experiments:finetuning}, promoting a costly fine-tuning step after compression is not the primary concern of our work.
    }

    \question{Why do you propose a backpropagation-based algorithm instead of layer-wise weight updates?}
    \answer{
        Let us first summarize several benefits of our end-to-end optimization approach from the main paper: (i) it is conceptually simple and requires no feature engineering, (ii) an error correction can be injected with almost no extra costs, (iii) it allows us to perform efficient and accurate any-size compression.

        Apart from that, and to the best of our knowledge, existing compression algorithms that use layer-wise updates like ASVD \cite{yuan_asvd_2024}, SVD-LLM \cite{wang_svd-llm_2024}, or WeLore \cite{jaiswal_galore_2024} require a separate fine-tuning step to achieve competitive downstream performance.
        Therefore, the lower costs of layer-wise compression are actually dominated by a more expensive backpropagation-based step.
        It remains open if similar results can be obtained by a fully tuning-free algorithm.
    }

    \question{Why do you use matrix factorization, and SVD in particular?}
    \answer{
        Committing to a backpropagation-based algorithm (see Q4) means that we have to deal with increased memory requirements.
        As such, matrix factorization is not helpful in that respect because the number of parameters might even increase initially (for instance, an SVD-parametrization basically doubles the size of a quadratic weight matrix).
        On the other hand, tuning and pruning only the bottleneck layer (i.e., the singular value mask in case of \methodname) has the potential for drastic size reductions and is highly parameter-efficient. For example, the number of tunable mask parameters for LLaMA-7B with \methodname is $<$1M.
        
        With this in mind, SVD as a specific matrix factorization is an obvious candidate due to its nice mathematical and numerical properties, in particular, optimal low-rank matrix approximation and stable matrix operations due to orthogonality.
    }
\end{itemize}

\newpage

\section{Implementation Details}
\label{sec:appendix:implementation}

In this section, we report more technical details and hyperparameters used for our experiments.

\paragraph{Dataset and Models}
Following previous work on LLM compression, we use C4 for training as it is a good proxy of a general-purpose dataset. 
In the context of \methodname, it should be primarily seen as a calibration dataset that allows us to propagate meaningful activations through a pre-trained model while performing structured pruning.
Overfitting to the distribution of C4 is implicitly mitigated, since we only tune very few parameters (masks and LoRA-adapters) compared to the total model size.

All considered (evaluation) datasets and pre-trained models are imported with the HuggingFace \texttt{transformers}-library in \texttt{bfloat16}-precision.
Our experiments were implemented with \texttt{PyTorch} \cite{paszke_pytorch_2019} and the \texttt{Lightning} package.

\paragraph{\methodname-Specifics}
As mentioned in \cref{rmk:lambda_scheduler}, we apply a linear scheduler that increases the regularization parameter $\lambda$ dynamically over the pruning process. This ensures that the pruning becomes more and more aggressive over time and the stopping criterion will be reached at some point.
Across all experiments, we use $\lambda = 1e{-}3$ as initial value and increase it by a factor of $1.01$ every $4$ steps (this amounts to a doubling $\lambda$ at about every $280$ steps).

As pointed out in \cref{sec:iterative_scoring}, we choose a (maximum) target compression rate as a stopping criterion for \methodname.
In most experiments, a rate of $r_{\text{stop}} = 0.4$ is reasonable (i.e., only 40\% or the original parameters remain), and we refer to \cref{sec:appendix:experiments:further:stop} for further discussion and analysis. After the stopping criterion is reached with tune the low-rank adapter for 1K more steps while the masks are frozen (see \cref{sec:iterative_scoring}).

The mask parameters in \eqref{eq:mask_parametrization} are rescaled by a fixed factor of $0.02$ to ensure a better alignment with the numerical range of the remaining network weights.
The low-rank adapters are created with $r = 32$, $\alpha=16$, and dropout $0.05$. For LLaMA-7B, the number of tunable parameters amounts to $<$1M mask parameters and approximately 80M low-rank adapter parameters.

For sample data from C4, we use $1024$ tokens per sample and a batch size of $4$. We use Adam \cite{DBLP:journals/corr/KingmaB14} as optimizer without weight decay and a learning rate of $5e{-}5$.

\paragraph{Runtime Analysis} \methodname requires significantly fewer steps than fine-tuning. Depending on when the stopping criterion is reached, it typically takes 1.5K - 2.5K steps, including 1K post-tuning steps of the low-rank adapters. For LLaMA-7B, for example, this amounts to a wall clock runtime of $<30$ minutes, including the initial SVD computations for the base model parametrization. All runs were performed on single NVIDIA H100 GPUs.

\paragraph{Fine-Tuning} In all post-compression fine-tunings (see \cref{sec:experiments:finetuning}), we simply continue training \methodname's low-rank adapters (the optimizer states are reset). We train for $20$K-$25$K steps on C4 with a batch size of $4$ and a learning rate of $2e{-}4$.

\section{Supplementary Results for \cref{sec:experiments:tradeoffs} -- \cref{sec:experiments:quantization} }
\label{sec:appendix:experiments:supp}

\cref{fig:tradeoff:all} complements the trade-off curves in \cref{fig:tradeoff:c4} and \cref{fig:tradeoff:lmeval} by all other considered evaluation metrics (see~\cref{sec:experiments:tradeoffs}).
\cref{tab:all_models} reports these results in terms of numbers, including all fine-tuning results for all models (see~\cref{sec:experiments:finetuning}).
\cref{tab:quantization} provides more detailed evaluation results on fine-tuning a quantized and compressed LLaMA-7B model (see~\cref{sec:experiments:quantization}).

\begin{table}
\caption{Evaluation results for \methodname on all considered LLMs. Scores on C4 and WikiText-2 are measured in perplexity (smaller is better), and the LM-Eval zero-shot tasks are measured in accuracy (higher is better). $^*$The results of LLaMA2-13B were achieved by ignoring all up-projection layers in \methodname (see \cref{sec:appendix:experiments:further:llama2} for more details).} \label{tab:all_models}
\tiny
\setlength{\tabcolsep}{3.7pt}
%
\clearpage{}%
\begin{tabular}{l|l|rr!{\color{lightgray}\vrule}rr|rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr!{\color{lightgray}\vrule}rr}
\toprule
 &  & \multicolumn{2}{c}{C4 ↓} & \multicolumn{2}{c}{WikiText-2 ↓} & \multicolumn{2}{c}{ARC\_c ↑} & \multicolumn{2}{c}{ARC\_e ↑} & \multicolumn{2}{c}{HellaS. ↑} & \multicolumn{2}{c}{MathQA ↑} & \multicolumn{2}{c}{Openb. ↑} & \multicolumn{2}{c}{PIQA ↑} & \multicolumn{2}{c}{WinoG. ↑} & \multicolumn{2}{c}{LM Eval Avg. ↑} \\
 & Type & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} & \texttt{ACIP} & \texttt{FT} \\
Model & Size &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\midrule
\multirow[c]{8}{*}{LLaMA-7B} & 40\% & 21.05 & 15.66 & 23.99 & 17.33 & 0.24 & 0.24 & 0.49 & 0.53 & 0.35 & 0.38 & 0.21 & 0.21 & 0.19 & 0.20 & 0.64 & 0.67 & 0.55 & 0.57 & 0.38 & 0.40 \\
 & 50\% & 16.47 & 12.89 & 16.16 & 11.88 & 0.27 & 0.27 & 0.57 & 0.59 & 0.40 & 0.43 & 0.22 & 0.23 & 0.21 & 0.23 & 0.68 & 0.71 & 0.57 & 0.60 & 0.42 & 0.44 \\
 & 60\% & 13.91 & 11.14 & 12.46 & 9.63 & 0.30 & 0.32 & 0.61 & 0.64 & 0.44 & 0.47 & 0.24 & 0.23 & 0.25 & 0.26 & 0.71 & 0.73 & 0.59 & 0.63 & 0.45 & 0.47 \\
 & 70\% & 12.22 & 9.84 & 10.35 & 8.27 & 0.31 & 0.34 & 0.64 & 0.69 & 0.47 & 0.50 & 0.23 & 0.24 & 0.28 & 0.28 & 0.73 & 0.75 & 0.62 & 0.66 & 0.47 & 0.49 \\
 & 80\% & 10.92 & 8.81 & 8.83 & 7.19 & 0.32 & 0.38 & 0.66 & 0.71 & 0.49 & 0.53 & 0.23 & 0.24 & 0.28 & 0.31 & 0.74 & 0.77 & 0.63 & 0.68 & 0.48 & 0.52 \\
 & 90\% & 9.75 & 7.69 & 7.56 & 6.12 & 0.33 & 0.39 & 0.67 & 0.73 & 0.50 & 0.56 & 0.25 & 0.25 & 0.27 & 0.33 & 0.76 & 0.78 & 0.63 & 0.69 & 0.49 & 0.53 \\
 & 100\% & 9.52 & 7.32 & 7.20 & 5.75 & 0.33 & 0.40 & 0.69 & 0.75 & 0.51 & 0.57 & 0.25 & 0.26 & 0.26 & 0.34 & 0.76 & 0.79 & 0.63 & 0.70 & 0.49 & 0.54 \\
 & Original & 7.31 &  & 5.68 &  & 0.42 &  & 0.75 &  & 0.57 &  & 0.27 &  & 0.34 &  & 0.79 &  & 0.70 &  & 0.55 &  \\
\cline{1-22}
\multirow[c]{8}{*}{LLaMA-13B} & 40\% & 16.64 & 13.38 & 17.66 & 13.42 & 0.28 & 0.28 & 0.57 & 0.59 & 0.41 & 0.43 & 0.22 & 0.23 & 0.23 & 0.24 & 0.69 & 0.70 & 0.60 & 0.62 & 0.43 & 0.44 \\
 & 50\% & 13.06 & 11.12 & 12.42 & 10.49 & 0.32 & 0.34 & 0.63 & 0.63 & 0.47 & 0.48 & 0.23 & 0.23 & 0.26 & 0.28 & 0.72 & 0.73 & 0.62 & 0.64 & 0.47 & 0.47 \\
 & 60\% & 11.33 & 9.76 & 9.79 & 8.30 & 0.35 & 0.33 & 0.67 & 0.68 & 0.50 & 0.51 & 0.24 & 0.24 & 0.28 & 0.30 & 0.74 & 0.76 & 0.65 & 0.67 & 0.49 & 0.50 \\
 & 70\% & 10.10 & 8.72 & 8.17 & 7.06 & 0.38 & 0.38 & 0.70 & 0.69 & 0.53 & 0.54 & 0.24 & 0.26 & 0.31 & 0.31 & 0.76 & 0.77 & 0.67 & 0.70 & 0.51 & 0.52 \\
 & 80\% & 9.06 & 7.91 & 6.91 & 6.21 & 0.41 & 0.41 & 0.74 & 0.74 & 0.55 & 0.57 & 0.26 & 0.27 & 0.32 & 0.34 & 0.77 & 0.78 & 0.68 & 0.69 & 0.53 & 0.54 \\
 & 90\% & 8.04 & 7.06 & 5.98 & 5.40 & 0.42 & 0.44 & 0.75 & 0.76 & 0.57 & 0.59 & 0.29 & 0.29 & 0.32 & 0.33 & 0.79 & 0.79 & 0.70 & 0.72 & 0.55 & 0.56 \\
 & 100\% & 7.86 & 6.79 & 5.83 & 5.15 & 0.42 & 0.46 & 0.75 & 0.77 & 0.57 & 0.60 & 0.29 & 0.30 & 0.31 & 0.32 & 0.79 & 0.79 & 0.70 & 0.73 & 0.55 & 0.57 \\
 & Original & 6.77 &  & 5.09 &  & 0.47 &  & 0.77 &  & 0.60 &  & 0.30 &  & 0.33 &  & 0.79 &  & 0.73 &  & 0.57 &  \\
\cline{1-22}
\multirow[c]{8}{*}{LLaMA-2-7B} & 40\% & 24.62 & 16.74 & 29.20 & 18.00 & 0.21 & 0.22 & 0.46 & 0.50 & 0.34 & 0.37 & 0.20 & 0.21 & 0.18 & 0.19 & 0.62 & 0.66 & 0.51 & 0.55 & 0.36 & 0.39 \\
 & 50\% & 18.36 & 13.32 & 19.64 & 12.25 & 0.25 & 0.27 & 0.53 & 0.57 & 0.38 & 0.42 & 0.22 & 0.22 & 0.24 & 0.23 & 0.67 & 0.69 & 0.53 & 0.56 & 0.40 & 0.42 \\
 & 60\% & 15.27 & 11.15 & 14.47 & 9.54 & 0.28 & 0.31 & 0.59 & 0.63 & 0.43 & 0.46 & 0.23 & 0.24 & 0.25 & 0.23 & 0.69 & 0.72 & 0.57 & 0.62 & 0.44 & 0.46 \\
 & 70\% & 12.96 & 9.73 & 10.47 & 7.74 & 0.33 & 0.35 & 0.62 & 0.68 & 0.46 & 0.50 & 0.25 & 0.24 & 0.27 & 0.26 & 0.73 & 0.74 & 0.60 & 0.64 & 0.47 & 0.49 \\
 & 80\% & 11.31 & 8.63 & 8.46 & 6.54 & 0.33 & 0.37 & 0.66 & 0.70 & 0.49 & 0.53 & 0.25 & 0.26 & 0.28 & 0.31 & 0.74 & 0.77 & 0.63 & 0.66 & 0.48 & 0.51 \\
 & 90\% & 9.46 & 7.43 & 6.69 & 5.45 & 0.34 & 0.43 & 0.69 & 0.75 & 0.51 & 0.56 & 0.26 & 0.28 & 0.28 & 0.33 & 0.76 & 0.78 & 0.63 & 0.69 & 0.50 & 0.54 \\
 & 100\% & 9.34 & 7.06 & 6.54 & 5.13 & 0.34 & 0.43 & 0.70 & 0.76 & 0.51 & 0.57 & 0.26 & 0.28 & 0.27 & 0.32 & 0.76 & 0.78 & 0.64 & 0.69 & 0.50 & 0.55 \\
 & Original & 7.04 &  & 5.11 &  & 0.44 &  & 0.76 &  & 0.57 &  & 0.28 &  & 0.31 &  & 0.78 &  & 0.69 &  & 0.55 &  \\
\cline{1-22}
\multirow[c]{8}{*}{LLaMA-2-13B} & 40\% & 27.55 & 84.28 & 41.22 & 145.79 & 0.23 & 0.23 & 0.43 & 0.46 & 0.33 & 0.30 & 0.21 & 0.21 & 0.18 & 0.18 & 0.62 & 0.63 & 0.52 & 0.52 & 0.36 & 0.36 \\
 & 50\% & 17.10 & 12.76 & 17.89 & 13.12 & 0.30 & 0.32 & 0.58 & 0.61 & 0.41 & 0.44 & 0.21 & 0.23 & 0.27 & 0.27 & 0.69 & 0.70 & 0.56 & 0.58 & 0.43 & 0.45 \\
 & 60\% & 13.29 & 10.05 & 11.11 & 8.43 & 0.32 & 0.36 & 0.65 & 0.68 & 0.47 & 0.50 & 0.22 & 0.23 & 0.29 & 0.31 & 0.72 & 0.74 & 0.60 & 0.63 & 0.47 & 0.49 \\
 & 70\% & 11.04 & 8.64 & 8.40 & 6.71 & 0.35 & 0.41 & 0.70 & 0.74 & 0.51 & 0.54 & 0.23 & 0.25 & 0.30 & 0.33 & 0.74 & 0.77 & 0.62 & 0.66 & 0.49 & 0.53 \\
 & 80\% & 9.54 & 7.68 & 6.80 & 5.66 & 0.37 & 0.44 & 0.72 & 0.76 & 0.54 & 0.57 & 0.25 & 0.28 & 0.30 & 0.34 & 0.77 & 0.78 & 0.64 & 0.71 & 0.51 & 0.55 \\
 & 90\% & 8.26 & 6.86 & 5.70 & 4.87 & 0.40 & 0.47 & 0.74 & 0.78 & 0.55 & 0.60 & 0.26 & 0.31 & 0.31 & 0.34 & 0.78 & 0.79 & 0.66 & 0.72 & 0.53 & 0.57 \\
 & 100\% & 7.87 & 6.56 & 5.42 & 4.61 & 0.41 & 0.47 & 0.75 & 0.79 & 0.56 & 0.60 & 0.28 & 0.32 & 0.31 & 0.35 & 0.78 & 0.79 & 0.69 & 0.71 & 0.54 & 0.58 \\
 & Original & 6.52 &  & 4.57 &  & 0.48 &  & 0.79 &  & 0.60 &  & 0.32 &  & 0.35 &  & 0.79 &  & 0.72 &  & 0.58 &  \\
\cline{1-22}
\multirow[c]{7}{*}{LLaMA-3.1-8B} & 50\% & 43.32 & 26.52 & 61.77 & 29.52 & 0.23 & 0.27 & 0.51 & 0.58 & 0.33 & 0.37 & 0.22 & 0.23 & 0.17 & 0.19 & 0.64 & 0.68 & 0.53 & 0.54 & 0.38 & 0.41 \\
 & 60\% & 31.55 & 21.00 & 36.69 & 19.26 & 0.29 & 0.29 & 0.60 & 0.61 & 0.37 & 0.42 & 0.24 & 0.24 & 0.22 & 0.23 & 0.69 & 0.72 & 0.55 & 0.56 & 0.42 & 0.44 \\
 & 70\% & 24.90 & 17.08 & 23.06 & 13.55 & 0.33 & 0.32 & 0.64 & 0.66 & 0.41 & 0.47 & 0.26 & 0.27 & 0.23 & 0.27 & 0.71 & 0.73 & 0.59 & 0.62 & 0.45 & 0.48 \\
 & 80\% & 20.78 & 14.21 & 15.60 & 10.12 & 0.38 & 0.40 & 0.69 & 0.72 & 0.46 & 0.51 & 0.28 & 0.30 & 0.28 & 0.29 & 0.74 & 0.77 & 0.61 & 0.66 & 0.49 & 0.52 \\
 & 90\% & 16.25 & 11.28 & 9.80 & 7.24 & 0.41 & 0.48 & 0.74 & 0.80 & 0.52 & 0.57 & 0.33 & 0.35 & 0.27 & 0.31 & 0.77 & 0.79 & 0.67 & 0.71 & 0.53 & 0.57 \\
 & 100\% & 14.57 & 9.42 & 8.04 & 5.95 & 0.40 & 0.51 & 0.75 & 0.82 & 0.53 & 0.60 & 0.36 & 0.40 & 0.27 & 0.34 & 0.78 & 0.80 & 0.66 & 0.74 & 0.54 & 0.60 \\
 & Original & 9.31 &  & 5.86 &  & 0.51 &  & 0.82 &  & 0.60 &  & 0.39 &  & 0.33 &  & 0.80 &  & 0.74 &  & 0.60 &  \\
\cline{1-22}
\multirow[c]{8}{*}{Mistral-7B-v0.3} & 40\% & 28.92 & 19.21 & 44.29 & 23.24 & 0.24 & 0.26 & 0.48 & 0.53 & 0.35 & 0.38 & 0.21 & 0.21 & 0.18 & 0.19 & 0.66 & 0.67 & 0.55 & 0.57 & 0.38 & 0.40 \\
 & 50\% & 21.44 & 14.86 & 28.60 & 16.53 & 0.28 & 0.28 & 0.57 & 0.59 & 0.40 & 0.43 & 0.21 & 0.23 & 0.22 & 0.21 & 0.69 & 0.70 & 0.58 & 0.60 & 0.42 & 0.43 \\
 & 60\% & 16.89 & 12.49 & 21.19 & 12.29 & 0.32 & 0.32 & 0.63 & 0.66 & 0.45 & 0.48 & 0.24 & 0.24 & 0.20 & 0.23 & 0.72 & 0.73 & 0.60 & 0.62 & 0.45 & 0.47 \\
 & 70\% & 13.75 & 10.95 & 13.28 & 9.69 & 0.35 & 0.34 & 0.67 & 0.68 & 0.49 & 0.52 & 0.27 & 0.28 & 0.21 & 0.26 & 0.74 & 0.76 & 0.63 & 0.63 & 0.48 & 0.49 \\
 & 80\% & 11.80 & 9.84 & 8.70 & 7.49 & 0.38 & 0.39 & 0.70 & 0.73 & 0.52 & 0.55 & 0.29 & 0.29 & 0.23 & 0.26 & 0.76 & 0.77 & 0.65 & 0.69 & 0.50 & 0.53 \\
 & 90\% & 10.42 & 8.84 & 6.51 & 5.85 & 0.40 & 0.43 & 0.72 & 0.75 & 0.54 & 0.59 & 0.31 & 0.33 & 0.25 & 0.31 & 0.78 & 0.79 & 0.68 & 0.70 & 0.53 & 0.56 \\
 & 100\% & 9.85 & 8.31 & 6.04 & 5.31 & 0.40 & 0.45 & 0.73 & 0.77 & 0.55 & 0.60 & 0.33 & 0.34 & 0.26 & 0.33 & 0.78 & 0.79 & 0.68 & 0.72 & 0.53 & 0.57 \\
 & Original & 8.05 &  & 4.96 &  & 0.49 &  & 0.79 &  & 0.61 &  & 0.36 &  & 0.34 &  & 0.80 &  & 0.73 &  & 0.59 &  \\
\cline{1-22}
\multirow[c]{8}{*}{Qwen2.5-3B} & 40\% & 71.23 & 36.85 & 91.51 & 39.44 & 0.20 & 0.22 & 0.45 & 0.51 & 0.29 & 0.31 & 0.21 & 0.22 & 0.15 & 0.16 & 0.60 & 0.64 & 0.50 & 0.52 & 0.34 & 0.37 \\
 & 50\% & 57.17 & 29.43 & 62.42 & 26.92 & 0.22 & 0.25 & 0.49 & 0.57 & 0.32 & 0.34 & 0.22 & 0.21 & 0.18 & 0.19 & 0.63 & 0.67 & 0.52 & 0.53 & 0.37 & 0.39 \\
 & 60\% & 43.30 & 23.30 & 38.26 & 18.38 & 0.26 & 0.29 & 0.57 & 0.63 & 0.35 & 0.39 & 0.23 & 0.23 & 0.21 & 0.24 & 0.67 & 0.69 & 0.54 & 0.56 & 0.40 & 0.43 \\
 & 70\% & 34.24 & 19.04 & 25.81 & 13.50 & 0.31 & 0.34 & 0.62 & 0.68 & 0.39 & 0.43 & 0.25 & 0.26 & 0.24 & 0.26 & 0.70 & 0.72 & 0.57 & 0.58 & 0.44 & 0.47 \\
 & 80\% & 25.50 & 16.16 & 17.02 & 10.68 & 0.34 & 0.36 & 0.68 & 0.73 & 0.43 & 0.47 & 0.28 & 0.31 & 0.26 & 0.24 & 0.72 & 0.74 & 0.58 & 0.57 & 0.47 & 0.49 \\
 & 90\% & 20.10 & 13.82 & 11.99 & 8.57 & 0.37 & 0.42 & 0.73 & 0.77 & 0.46 & 0.52 & 0.33 & 0.36 & 0.27 & 0.33 & 0.74 & 0.78 & 0.60 & 0.67 & 0.50 & 0.55 \\
 & 100\% & 18.73 & 12.81 & 10.63 & 7.70 & 0.36 & 0.47 & 0.72 & 0.78 & 0.46 & 0.54 & 0.33 & 0.41 & 0.24 & 0.31 & 0.74 & 0.78 & 0.60 & 0.69 & 0.49 & 0.57 \\
 & Original & 12.90 &  & 7.64 &  & 0.45 &  & 0.77 &  & 0.55 &  & 0.37 &  & 0.30 &  & 0.78 &  & 0.68 &  & 0.56 &  \\
\cline{1-22}
\multirow[c]{8}{*}{Qwen2.5-7B} & 40\% & 46.43 & 29.26 & 49.04 & 27.24 & 0.23 & 0.24 & 0.52 & 0.58 & 0.31 & 0.34 & 0.22 & 0.21 & 0.19 & 0.20 & 0.64 & 0.67 & 0.53 & 0.53 & 0.38 & 0.40 \\
 & 50\% & 34.90 & 23.26 & 29.96 & 19.72 & 0.27 & 0.30 & 0.59 & 0.64 & 0.35 & 0.39 & 0.22 & 0.23 & 0.23 & 0.25 & 0.67 & 0.70 & 0.54 & 0.57 & 0.41 & 0.44 \\
 & 60\% & 27.84 & 18.73 & 21.98 & 13.73 & 0.31 & 0.33 & 0.63 & 0.68 & 0.39 & 0.45 & 0.25 & 0.26 & 0.25 & 0.28 & 0.70 & 0.73 & 0.55 & 0.60 & 0.44 & 0.48 \\
 & 70\% & 22.97 & 15.96 & 15.72 & 10.71 & 0.35 & 0.42 & 0.68 & 0.74 & 0.44 & 0.49 & 0.28 & 0.30 & 0.29 & 0.30 & 0.73 & 0.76 & 0.57 & 0.63 & 0.48 & 0.52 \\
 & 80\% & 19.68 & 14.02 & 12.07 & 8.89 & 0.40 & 0.46 & 0.73 & 0.78 & 0.48 & 0.53 & 0.33 & 0.36 & 0.30 & 0.33 & 0.75 & 0.78 & 0.59 & 0.67 & 0.51 & 0.56 \\
 & 90\% & 17.09 & 12.59 & 9.82 & 7.63 & 0.44 & 0.48 & 0.75 & 0.80 & 0.52 & 0.56 & 0.39 & 0.41 & 0.31 & 0.32 & 0.77 & 0.79 & 0.64 & 0.70 & 0.54 & 0.58 \\
 & 100\% & 15.34 & 11.43 & 8.38 & 6.60 & 0.43 & 0.50 & 0.75 & 0.82 & 0.53 & 0.59 & 0.43 & 0.46 & 0.28 & 0.34 & 0.77 & 0.79 & 0.66 & 0.72 & 0.55 & 0.60 \\
 & Original & 11.47 &  & 6.55 &  & 0.48 &  & 0.81 &  & 0.60 &  & 0.43 &  & 0.34 &  & 0.79 &  & 0.73 &  & 0.60 &  \\
\cline{1-22}
\multirow[c]{8}{*}{Qwen2.5-14B} & 40\% & 36.51 & 25.58 & 33.78 & 22.22 & 0.26 & 0.29 & 0.55 & 0.61 & 0.36 & 0.38 & 0.22 & 0.23 & 0.24 & 0.26 & 0.67 & 0.68 & 0.54 & 0.57 & 0.41 & 0.43 \\
 & 50\% & 26.27 & 19.53 & 20.15 & 14.57 & 0.32 & 0.33 & 0.65 & 0.68 & 0.43 & 0.44 & 0.25 & 0.25 & 0.26 & 0.27 & 0.70 & 0.71 & 0.57 & 0.59 & 0.45 & 0.47 \\
 & 60\% & 21.29 & 15.63 & 14.87 & 10.48 & 0.36 & 0.40 & 0.70 & 0.73 & 0.49 & 0.51 & 0.28 & 0.32 & 0.28 & 0.31 & 0.74 & 0.76 & 0.62 & 0.66 & 0.50 & 0.53 \\
 & 70\% & 17.99 & 13.99 & 11.25 & 8.89 & 0.42 & 0.43 & 0.73 & 0.76 & 0.53 & 0.54 & 0.33 & 0.36 & 0.31 & 0.32 & 0.77 & 0.78 & 0.64 & 0.68 & 0.53 & 0.55 \\
 & 80\% & 15.23 & 11.97 & 8.73 & 7.11 & 0.44 & 0.48 & 0.77 & 0.78 & 0.57 & 0.58 & 0.39 & 0.44 & 0.34 & 0.36 & 0.78 & 0.79 & 0.68 & 0.73 & 0.57 & 0.60 \\
 & 90\% & 13.05 & 10.77 & 6.86 & 6.04 & 0.48 & 0.52 & 0.80 & 0.82 & 0.59 & 0.60 & 0.49 & 0.49 & 0.32 & 0.35 & 0.79 & 0.81 & 0.70 & 0.74 & 0.60 & 0.62 \\
 & 100\% & 12.37 & 9.98 & 6.23 & 5.11 & 0.47 & 0.53 & 0.79 & 0.82 & 0.58 & 0.62 & 0.51 & 0.53 & 0.32 & 0.35 & 0.79 & 0.81 & 0.73 & 0.77 & 0.60 & 0.63 \\
 & Original & 9.99 &  & 5.05 &  & 0.56 &  & 0.82 &  & 0.63 &  & 0.53 &  & 0.35 &  & 0.81 &  & 0.75 &  & 0.64 &  \\
\cline{1-22}
\bottomrule
\end{tabular}
\clearpage{}%
%

\end{table}

\figureTradeoffSupplement

\begin{table}
\centering
\caption{More detailed evaluation results for our quantization experiments from \cref{sec:experiments:quantization}, reported in terms of numbers.} \label{tab:quantization}
\scriptsize
\setlength{\tabcolsep}{2.2pt}
%
\clearpage{}%
\begin{tabular}{l|l|r|rr|rrrrrrrr}
\toprule
 &  & Eff. model size [GB] & C4 ↓ & WikiText-2 ↓ & ARC\_c ↑ & ARC\_e ↑ & HellaS. ↑ & MathQA ↑ & Openb. ↑ & PIQA ↑ & WinoG. ↑ & LM Eval Avg. ↑ \\
Size & Ablation &  &  &  &  &  &  &  &  &  &  &  \\
\midrule
\multirow[c]{4}{*}{40\%} & \texttt{ACIP} & 5.47 & 21.05 & 24.00 & 0.24 & 0.49 & 0.35 & 0.21 & 0.19 & 0.65 & 0.55 & 0.38 \\
 & \texttt{ACIP → FT} & 5.47 & 15.66 & 17.33 & 0.24 & 0.53 & 0.38 & 0.21 & 0.20 & 0.67 & 0.57 & 0.40 \\
 & \texttt{ACIP → W4A16} & 1.89 & 27.12 & 35.40 & 0.22 & 0.46 & 0.33 & 0.20 & 0.18 & 0.62 & 0.53 & 0.36 \\
 & \texttt{ACIP → W4A16 → FT} & 1.89 & 16.67 & 18.90 & 0.23 & 0.52 & 0.37 & 0.22 & 0.20 & 0.67 & 0.57 & 0.40 \\
\cline{1-13}
\multirow[c]{4}{*}{50\%} & \texttt{ACIP} & 6.70 & 16.47 & 16.17 & 0.28 & 0.58 & 0.40 & 0.22 & 0.21 & 0.68 & 0.57 & 0.42 \\
 & \texttt{ACIP → FT} & 6.70 & 12.89 & 11.88 & 0.27 & 0.59 & 0.43 & 0.23 & 0.23 & 0.71 & 0.60 & 0.44 \\
 & \texttt{ACIP → W4A16} & 2.21 & 19.28 & 19.96 & 0.26 & 0.54 & 0.37 & 0.21 & 0.20 & 0.67 & 0.55 & 0.40 \\
 & \texttt{ACIP → W4A16 → FT} & 2.21 & 13.85 & 13.33 & 0.25 & 0.58 & 0.41 & 0.23 & 0.23 & 0.70 & 0.59 & 0.43 \\
\cline{1-13}
\multirow[c]{4}{*}{60\%} & \texttt{ACIP} & 7.88 & 13.91 & 12.46 & 0.30 & 0.61 & 0.43 & 0.23 & 0.25 & 0.71 & 0.60 & 0.45 \\
 & \texttt{ACIP → FT} & 7.88 & 11.14 & 9.63 & 0.32 & 0.64 & 0.47 & 0.23 & 0.26 & 0.73 & 0.63 & 0.47 \\
 & \texttt{ACIP → W4A16} & 2.51 & 15.84 & 14.64 & 0.29 & 0.58 & 0.42 & 0.22 & 0.22 & 0.69 & 0.57 & 0.43 \\
 & \texttt{ACIP → W4A16 → FT} & 2.51 & 11.77 & 10.31 & 0.29 & 0.64 & 0.45 & 0.22 & 0.26 & 0.72 & 0.61 & 0.46 \\
\cline{1-13}
\multirow[c]{4}{*}{70\%} & \texttt{ACIP} & 9.10 & 12.22 & 10.34 & 0.31 & 0.64 & 0.47 & 0.23 & 0.27 & 0.73 & 0.62 & 0.47 \\
 & \texttt{ACIP → FT} & 9.10 & 9.84 & 8.27 & 0.34 & 0.69 & 0.50 & 0.24 & 0.28 & 0.75 & 0.66 & 0.49 \\
 & \texttt{ACIP → W4A16} & 2.83 & 13.45 & 11.80 & 0.29 & 0.63 & 0.45 & 0.23 & 0.24 & 0.72 & 0.60 & 0.45 \\
 & \texttt{ACIP → W4A16 → FT} & 2.83 & 10.38 & 8.74 & 0.32 & 0.67 & 0.48 & 0.23 & 0.28 & 0.75 & 0.64 & 0.48 \\
\cline{1-13}
\multirow[c]{4}{*}{80\%} & \texttt{ACIP} & 10.30 & 10.91 & 8.83 & 0.33 & 0.67 & 0.49 & 0.23 & 0.28 & 0.74 & 0.63 & 0.48 \\
 & \texttt{ACIP → FT} & 10.30 & 8.81 & 7.19 & 0.38 & 0.71 & 0.53 & 0.24 & 0.31 & 0.77 & 0.68 & 0.52 \\
 & \texttt{ACIP → W4A16} & 3.13 & 12.61 & 9.87 & 0.32 & 0.65 & 0.47 & 0.23 & 0.28 & 0.74 & 0.61 & 0.47 \\
 & \texttt{ACIP → W4A16 → FT} & 3.13 & 9.26 & 7.60 & 0.36 & 0.69 & 0.52 & 0.24 & 0.30 & 0.76 & 0.66 & 0.50 \\
\cline{1-13}
\multirow[c]{4}{*}{90\%} & \texttt{ACIP} & 11.50 & 9.75 & 7.56 & 0.34 & 0.68 & 0.50 & 0.25 & 0.27 & 0.75 & 0.63 & 0.49 \\
 & \texttt{ACIP → FT} & 11.50 & 7.69 & 6.12 & 0.39 & 0.73 & 0.56 & 0.25 & 0.33 & 0.78 & 0.69 & 0.53 \\
 & \texttt{ACIP → W4A16} & 3.44 & 10.25 & 7.90 & 0.32 & 0.66 & 0.50 & 0.24 & 0.27 & 0.75 & 0.63 & 0.48 \\
 & \texttt{ACIP → W4A16 → FT} & 3.44 & 7.97 & 6.39 & 0.38 & 0.73 & 0.56 & 0.25 & 0.35 & 0.78 & 0.69 & 0.53 \\
\cline{1-13}
\multirow[c]{4}{*}{100\%} & \texttt{ACIP} & 12.70 & 9.52 & 7.20 & 0.33 & 0.69 & 0.51 & 0.25 & 0.26 & 0.76 & 0.63 & 0.49 \\
 & \texttt{ACIP → FT} & 12.70 & 7.32 & 5.75 & 0.40 & 0.75 & 0.57 & 0.26 & 0.34 & 0.79 & 0.70 & 0.54 \\
 & \texttt{ACIP → W4A16} & 3.75 & 9.75 & 7.37 & 0.34 & 0.69 & 0.51 & 0.25 & 0.27 & 0.76 & 0.63 & 0.49 \\
 & \texttt{ACIP → W4A16 → FT} & 3.75 & 7.52 & 5.94 & 0.40 & 0.75 & 0.56 & 0.27 & 0.34 & 0.78 & 0.69 & 0.54 \\
\cline{1-13}
\bottomrule
\end{tabular}
\clearpage{}%
%

\end{table}

\newpage

\section{Further Experiments and Ablations}
\label{sec:appendix:experiments:further}

In this section, we present several supplementary experiments analyzing the impact of some key algorithmic components and design choices of \methodname in more detail.

\subsection{Impact of Low-Rank Adapters}
The primary purpose of the low-rank adapters used in \methodname is to correct compression errors on-the-fly during the optimization.
A surprising finding of our work is that the final adapters are ``universal'' in the sense that they can be used across all seen compression levels.
While we expect that other PEFT-style approaches would lead to similar findings, it is natural to ask how \methodname would perform without any correction, i.e., just the mask parameters are tuned according to~\eqref{eq:shrinkage}. This ablation study is shown in \cref{fig:ablation:nolora}.
While performing significantly worse than with LoRA, we observe that the perplexity does not blow up and the results are even slightly better than SVD-LLM (see \cref{tab:competitors}).
This stable behavior of \methodname is closely related to our parameterization of the mask in \eqref{eq:mask_parametrization} which ensures that the forward pass corresponds to the actual outputs of the pruned model with binary masks. On the other hand, the straight-through estimator still enables backpropagation.

\figureAblationNoLora

\subsection{Impact of the Stopping Criterion}
\label{sec:appendix:experiments:further:stop}

In most experiments, we have used a compression ratio $r_{\text{stop}}$ of $0.4$ as stopping criterion, i.e., the pruning of masks is stopped if the size of the model is only 40\% of the original one (measured in number of parameters of all target weight matrices). 
We have observed that at this point, the model performance has typically dropped so much that even a fine-tuned model would be of limited practical use. 

Nevertheless, it is interesting to explore the sensitivity of compression-performance curves against different stopping ratios.
The comparison shown in \cref{fig:ablation:sweep_stop} provides several insights in this respect: (i) ``Forecasting'' compressed models beyond the stopping ratio does not work very well, especially when stopping very early ($>0.8$). (ii) The predictive capacity of \methodname remains valid for even stronger stopping compression ratios than $0.4$. However, finding the largest reasonable stopping ratio is highly model-dependent.
For less compressible models like LLaMA-3.1-8B, it could make sense to stop even earlier than $0.4$ (cf.~\cref{fig:firstpage}).
In general, we hypothesize that older models are more compressible than new ones, as the latter ``carry'' more information per weight due to significantly more training data \cite{allen-zhu_physics_2024}.

\figureAblationSweepStop

\subsection{Impact of the Score Map -- Forecasting Pruning Patterns}

Here, we pick up the observation from \cref{sec:appendix:experiments:further:stop} that forecasting the performance of compressed models beyond the stop ratio leads to inaccurate predictions, i.e., the model is compressed more strongly than it has been done by \methodname itself.
However, it turns out that the score map itself exhibits a certain forecasting capability. 
To this end, we run \methodname as usual until a stop ratio is reached, say $r_{\text{stop}} = 0.4$, but we stop updating the score map earlier in the optimization process. A few compression-performance curves with this modification are reported in \cref{fig:ablation:stop_score}. We observe very similar curve shapes even if the score map is frozen after only a tiny fraction of mask parameters was pruned.
This underpins our intuition from \cref{sec:iterative_scoring} that the pruning path of each parameter is fully determined at very early stage of \methodname.

\figureAblationStopScore

\subsection{Impact of the Score Map -- A Trivial One Does Not Work}
There are certainly alternative ways to design useful score maps.
For example, simply accumulating the gradients of all mask parameters entrywise over an \methodname-run works equally well as the strategy proposed in \cref{sec:method}.
It is therefore valid to ask whether one could even design score maps without any optimization.
We demonstrate that perhaps the most obvious approach, namely setting the score map equal to the singular values of the weight matrices, does not work very well.
\cref{fig:ablation:sv_score_map} shows that this training-free approach does not produce any reasonable compressed models and decent performance cannot be easily recovered with LoRA-finetuning.
This simple experiment confirms that designing useful score maps is not a trivial endeavour and requires a carefully crafted algorithmic approach.

\figureAblationSVScoreMap

\subsection{Impact of Post-Tuning}
\label{sec:appendix:experiments:further:post_tuning}

Our main experiments are performed with 1K post-tuning steps in \methodname (see the description in \cref{sec:iterative_scoring}).
\cref{fig:ablation:post_tuning} shows analogous compression-performance trade-off curves for fewer or no post-tuning steps.
We observe that post-tuning can indeed notably increase performance for higher compression ratios.

\figureAblationPostTuning

\subsection{Impact of Individual Layers -- Example of LLaMA2-13B}
\label{sec:appendix:experiments:further:llama2}

As pointed out in the caption of \cref{tab:all_models}, the linear layers targeted by \methodname were slightly modified for LLaMA2-13, namely all up projection layers were ignored. \cref{fig:ablation:up_proj} shows what would happen if they are compressed as well. While the performance predictions for $\geq 0.6$ look decent, the perplexity explodes for stronger compression; note that even additional fine-tuning does not recover a reasonable performance in this situation.
We hypothesize that \methodname has pruned one or more singular values of the up projection layers that are crucial for model's integrity.
This finding might be related to the recent work by \citet{yu_super_2024} on pruning so-called super weights.
In any case, \methodname is capable of revealing this undesirable behavior as demonstrated in \cref{fig:ablation:up_proj}.

\figureAblationUpProj

\subsection{Examples of Score Maps Generated by \methodname}
\label{sec:appendix:experiments:further:score_maps}

\cref{fig:score_map:llama7b} and \cref{fig:score_map:qwen25_7b} show two typical score maps generated by \methodname for LLaMA-7B and Qwen2.5-7B, respectively.
A characteristic feature is that attention layers can be pruned more aggressively than the MLP layers.
Similarly, we observe non-uniform pruning patterns for layers of the same type across all transformer layers.
This confirms the findings of \cite{yuan_asvd_2024, jaiswal_galore_2024} and demonstrates that non-uniform structured compression can be achieved without any feature engineering.

\figureScoreMapLlama

\figureScoreMapQwen

\subsection{Examples of Generated Text by Compressed Models}

\cref{tab:text_samples} shows examples of generated text by compressed versions of LLaMA-7B.

\begin{table}
\centering
\caption{Example texts for two prompts generated for \textbf{LLaMA-7B} under different compressions produced by \methodname.}
\label{tab:text_samples}
\small
\setlength{\tabcolsep}{3.7pt}
%
\clearpage{}%
\begin{tabular}{c|p{7cm}|p{7cm}}
\hline
Size & \textbf{``How to compress Large Language Models?''} & \textbf{``What is the concept of time?''} \\
\hline
Original 
& I am trying to run a large Language model generation, but I have a few files
      on disk of around 3gb each. I was wondering if there is a way to compress these
      files? I have searched the internet but have found nothing.

      Re: How to compress Large Language Models?

      I suggest compressing it to HDF5, you can find instructions on how to do this
      using TensorPacker and PNet on the TensorFlow Github repository \ldots
& Have you ever tried to ask a baby what it sees? Time, is it real? What is the concept of time? Have you ever tried to
      ask a baby what it sees? Let’s see what they can understand.

      The concept of time. What it is about, who discovered the idea of time, and
      how was it discovered? How is time measured? We can’t define it. We can’t point
      at what we are defining, we can’t even show a picture of it \ldots \\
\hline
90\% 
& Jeffrey Hunt, a linguist and applied deep learning researcher at the University
      of Technology, Sydney, talks with Caleigh Blackstock''s to discuss how to compress
      a large language model to speed up training and model execution.

      What does the language model compression actually do to the overall model? \ldots 
& Please let me know what is the concept of time.

      The concept of time is all about the measurement of time.

      This measurement was first done using the hourglass.

      When this hour Glass was invented, it was the only thing that could represent
      the passage of time. \ldots \\
\hline
80\% 
& Jeffrey Shafman, Zsolt Nemeth, Jeffrey Shafman, Zsolt Nemeth, Jeffrey Shafman,
      Zsolt Nemeth. (2013). How to compress Large Language Models? In: ICML proceedings,
      Vol. 22, pp. 462-463.

      Magnus Skallman. (2013). How to compress Large Language Models? In: ICML13 Proceedings,
      pp. 462-463. \ldots 
& You can’t even imagine that in that time,

      The life of a tree in nature is as brief

      As a flash of a lightning bolt,

      But in this human world, it may endure

      Through many millennia, if the will of man.

      So tell me, what is the concept of time?

      Hey, I’ve got one, and it’s this: \ldots \\
\hline
70\% 
& Apr 04, 2018 at 12:05 pm In the last 5-6 years, Natural Language Systems have
      been an important direction for my work and the research I do in the course
      of my PhD. I have been using them to teach myself Swedish and improve my Swedish
      by machine time in this post. Let’s see if I’ve got it! \ldots 
& Please tell me where is the concept of time in this poem.

      What is the significance of the poem "I Am Wine" by H.W.C. Carson"?

      What does he mean by the poem title?

      What does the stairs, the wine, \& the man have in common?

      What does he mean by "I am wine"?

      What do you mean by "I am wine"? \ldots \\
\hline
60\% & This is a 14 day free trial.

      Please use the following link to sign in.

      Select the country you need here to view pricing and purchase.

      Whenever you use our web site, we will track your website traffic, the page
      where you first connected to our site, and the search keywords you use to find
      us. We’ll use this information to help us continually improve our website. \ldots
  & You may be wondering what is the concept of time.

      The concept of time is all about the phenomenon of time. It says that there
      is never no time. It states that there is always a moment in time. At the moments
      of time is when we refer the time of life.

      So, as a matter of the moment, time is just a point in space.
 \ldots \\
\hline
50\%  &
Jeffrey Huan, Shu-Hua Ji, Xiao-Hong Li, Xiao-Xiao Li, Layin Ma''s. 2019:5, Springer.0000.0001/13/8/19/12/720-22-F43.

      Miller A. "The Structure of the Language in Unsupervised Deep-Processing" in
      Journal of the Acoustical Actors (v. 11(6). 2006:, pp. 278-295).

      Dong Ming, Sun Jin, He Mei-Chen,' \ldots & Getting to this answer isn’t that hard.

      The concept of time is all about the study of time. The study of time isn’t
      so hard to get a hold on.

      When you’re younger, you can begin thinking of how much you could have had if
      you had known how much time you had...if only we had it back then \ldots \\
\hline
40\% &
Apr 14, 2018 Resumes at The University of Florida will be released on Friday,
      May 29, 2018 2.05:53pm.

      Cover your next job announcement with the help of our new Resume Template Builder.

      To create your own Resume Template in minutes.

      Improving the quality of your Resume.

      To improve your Resume \ldots & You may think that it is just a fancy word, or just the idea it had in the earlier
      world. But there exists a way to understand it.

      To understand the idea of time by using the example of a clock, you can learn
      the very importance of time with a simple strategy.

      The clock ticks with a watch. The clock has it time to operate. \ldots \\
\end{tabular}\clearpage{}%
%

\end{table}
 

\end{document}
