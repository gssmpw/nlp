@misc{ashkboos_slicegpt_2024,
	title = {{SliceGPT}: {Compress} {Large} {Language} {Models} by {Deleting} {Rows} and {Columns}},
	shorttitle = {{SliceGPT}},
	publisher = {arXiv},
	author = {Ashkboos, Saleh and Croci, Maximilian L. and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
	year = {2024},
	note = {arXiv:2401.15024 [cs]},
}

@inproceedings{ben_noach_compressing_2020,
	title = {Compressing {Pre}-trained {Language} {Models} by {Matrix} {Decomposition}},
	booktitle = {Proceedings of the 1st {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 10th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Noach, Matan and Goldberg, Yoav},
	editor = {Wong, Kam-Fai and Knight, Kevin and Wu, Hua},
	year = {2020},
	pages = {884--889},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	edition = {2nd},
	title = {Elements of {Information} {Theory}},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}

@inproceedings{edalati_kronecker_2022,
	title = {Kronecker {Decomposition} for {GPT} {Compression}},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid and Clark, James and Rezagholizadeh, Mehdi},
	year = {2022},
	pages = {219--226},
}

@misc{frantar_sparsegpt_2023,
	title = {{SparseGPT}: {Massive} {Language} {Models} {Can} {Be} {Accurately} {Pruned} in {One}-{Shot}},
	shorttitle = {{SparseGPT}},
	publisher = {arXiv},
	author = {Frantar, Elias and Alistarh, Dan},
	year = {2023},
	note = {arXiv:2301.00774 [cs]},
}

@inproceedings{gao_rate_2019,
	title = {Rate {Distortion} {For} {Model} {Compression}: {From} {Theory} {To} {Practice}},
	shorttitle = {Rate {Distortion} {For} {Model} {Compression}},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Weihao and Liu, Yu-Han and Wang, Chong and Oh, Sewoong},
	year = {2019},
	pages = {2102--2111},
}

@misc{hsuLanguageModelCompression2022,
	title = {Language model compression with weighted low-rank factorization},
	author = {Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
	note = {arxiv:2207.00112 [cs]},
	year = {2022},
}

@inproceedings{isik_information-theoretic_2022,
	title = {An {Information}-{Theoretic} {Justification} for {Model} {Pruning}},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Isik, Berivan and Weissman, Tsachy and No, Albert},
	year = {2022},
	pages = {3821--3846},
}

@misc{jaiswal_galore_2024,
	title = {From {GaLore} to {WeLore}: {How} {Low}-{Rank} {Weights} {Non}-uniformly {Emerge} from {Low}-{Rank} {Gradients}},
	shorttitle = {From {GaLore} to {WeLore}},
	abstract = {Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, in this work we first study the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Our findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, we present Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. Our gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50{\textbackslash}\% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with {\textasciitilde}3x better throughput and {\textasciitilde}0.6x GPU requirement. Our codes are available at {\textbackslash}url\{https://github.com/VITA-Group/welore\}},
	author = {Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
	year = {2024},
	note = {arXiv:2407.11239 [cs]},
}

@misc{kaushal_lord_2023,
	title = {{LORD}: {Low} {Rank} {Decomposition} {Of} {Monolingual} {Code} {LLMs} {For} {One}-{Shot} {Compression}},
	shorttitle = {{LORD}},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Kaushal, Ayush and Vaidhya, Tejas and Rish, Irina},
	year = {2023},
	note = {arXiv:2309.14021 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{sharma_truth_2023,
	title = {The {Truth} is in {There}: {Improving} {Reasoning} in {Language} {Models} with {Layer}-{Selective} {Rank} {Reduction}},
	booktitle = {ICLR},
	urldate = {2024-11-11},
	author = {Sharma, Pratyusha and Ash, Jordan T. and Misra, Dipendra},
	year = {2023},
}

@inproceedings{tahaei_kroneckerbert_2022,
	address = {Seattle, United States},
	title = {{KroneckerBERT}: {Significant} {Compression} of {Pre}-trained {Language} {Models} {Through} {Kronecker} {Decomposition} and {Knowledge} {Distillation}},
	shorttitle = {{KroneckerBERT}},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tahaei, Marzieh and Charlaix, Ella and Nia, Vahid and Ghodsi, Ali and Rezagholizadeh, Mehdi},
	year = {2022},
	pages = {2116--2127},
}

@misc{wang_svd-llm_2024,
	title = {{SVD}-{LLM}: {Truncation}-aware {Singular} {Value} {Decomposition} for {Large} {Language} {Model} {Compression}},
	shorttitle = {{SVD}-{LLM}},
	publisher = {arXiv},
	author = {Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
	year = {2024},
	note = {arXiv:2403.07378 [cs]},
}

@misc{xia_sheared_2024,
	title = {Sheared {LLaMA}: {Accelerating} {Language} {Model} {Pre}-training via {Structured} {Pruning}},
	shorttitle = {Sheared {LLaMA}},
	abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
	year = {2024},
	note = {arXiv:2310.06694 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{yu_compressing_2023,
	title = {Compressing {Transformers}: {Features} {Are} {Low}-{Rank}, but {Weights} {Are} {Not}!},
	volume = {37},
	shorttitle = {Compressing {Transformers}},
	abstract = {Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks,  but high computational costs and reliance on large training datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been effective in compressing CNN models, but its application to transformers has been less explored and is less effective. Existing methods require the complete dataset to fine-tune compressed models, which are both time-consuming and data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layer's output features rather than the model weights. A second stage, GFM, optimizes the entire compressed network holistically. Both AAFM and GFM only use few training samples without labels, that is, they are few-shot, unsupervised, fast and effective. For example, with only 2K images without labels, 33\% of the parameters are removed in DeiT-B with 18.8\% relative throughput increase, but only a 0.23\% accuracy loss for ImageNet recognition. The proposed methods are successfully applied to the language modeling task in NLP, too. Besides, the few-shot compressed models generalize well in downstream tasks.},
	language = {en},
	number = {9},
	urldate = {2024-05-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yu, Hao and Wu, Jianxin},
	year = {2023},
	note = {Number: 9},
	keywords = {SNLP: Language Models},
	pages = {11007--11015},
}

@misc{yuan_asvd_2024,
	title = {{ASVD}: {Activation}-aware {Singular} {Value} {Decomposition} for {Compressing} {Large} {Language} {Models}},
	shorttitle = {{ASVD}},
	abstract = {In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from the distribution variance in the LLM activations and the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10\%-30\%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50\% KV cache reductions without performance drop in a training-free manner. Code is anonymously available in supplementary materials.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
	year = {2024},
	note = {arXiv:2312.05821 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{zhu_survey_2024,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	volume = {12},
	urldate = {2024-12-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	year = {2024},
	pages = {1556--1577},
}

