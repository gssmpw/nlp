
@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	year = {2023},
	note = {arXiv:2310.06825 [cs]},
}

@misc{eval-harness,
	title = {A framework for few-shot language model evaluation},
	publisher = {Zenodo},
	author = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
	year = {2023},
	doi = {10.5281/zenodo.10256836},
	note = {tex.version: v0.4.0},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	journal = {ArXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, A. and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, K. and Elsen, Erich and Rae, Jack W. and Vinyals, O. and Sifre, L.},
	note = {arXiv:2203.15556 [cs]},
	year = {2022},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	author = {Kaplan, J. and McCandlish, Sam and Henighan, T. and Brown, Tom B. and Chess, B. and Child, R. and Gray, Scott and Radford, Alec and Wu, Jeff and Amodei, Dario},
	note = {arXiv:2001.08361 [cs]},
	year = {2020},
}

@inproceedings{dettmers_qlora_2023,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {10088--10115},
 publisher = {Curran Associates, Inc.},
 title = {{QLoRA: Efficient Finetuning of Quantized LLMs}},
 volume = {36},
 year = {2023}
}


@misc{kaushal_lord_2023,
	title = {{LORD}: {Low} {Rank} {Decomposition} {Of} {Monolingual} {Code} {LLMs} {For} {One}-{Shot} {Compression}},
	shorttitle = {{LORD}},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Kaushal, Ayush and Vaidhya, Tejas and Rish, Irina},
	year = {2023},
	note = {arXiv:2309.14021 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	year = {2013},
	note = {arXiv:1308.3432 [cs]},
}

@misc{bengio_estimating_2013-1,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
	year = {2013},
	note = {arXiv:1308.3432 [cs]},
}

@book{wainwright_high-dimensional_2019,
	address = {New York, NY},
	edition = {1st ed},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}},
	shorttitle = {High-{Dimensional} {Statistics}},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Wainwright, Martin J.},
	year = {2019},
}

@inproceedings{mairal_complexity_2012,
	title = {Complexity analysis of the lasso regularization path},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Mairal, Julien and Yu, Bin},
	year = {2012},
	pages = {1835--1842},
}

@misc{ashkboos_slicegpt_2024,
	title = {{SliceGPT}: {Compress} {Large} {Language} {Models} by {Deleting} {Rows} and {Columns}},
	shorttitle = {{SliceGPT}},
	publisher = {arXiv},
	author = {Ashkboos, Saleh and Croci, Maximilian L. and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
	year = {2024},
	note = {arXiv:2401.15024 [cs]},
}

@book{hastie_statistical_2015,
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}},
	shorttitle = {Statistical {Learning} with {Sparsity}},
	publisher = {Chapman and Hall},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
}

@article{efron_least_2004,
	title = {Least angle regression},
	volume = {32},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	year = {2004},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
}

@inproceedings{idelbayev_low-rank_2020,
	title = {Low-{Rank} {Compression} of {Neural} {Nets}: {Learning} the {Rank} of {Each} {Layer}},
	shorttitle = {Low-{Rank} {Compression} of {Neural} {Nets}},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Idelbayev, Yerlan and Carreira-Perpiñán, Miguel A.},
	year = {2020},
	pages = {8046--8056},
}

@inproceedings{isik_information-theoretic_2022,
	title = {An {Information}-{Theoretic} {Justification} for {Model} {Pruning}},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Isik, Berivan and Weissman, Tsachy and No, Albert},
	year = {2022},
	pages = {3821--3846},
}

@article{boggust_compress_2025,
	title = {Compress and {Compare}: {Interactively} {Evaluating} {Efficiency} and {Behavior} {Across} {ML} {Model} {Compression} {Experiments}},
	volume = {31},
	shorttitle = {Compress and {Compare}},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Boggust, Angie and Sivaraman, Venkatesh and Assogba, Yannick and Ren, Donghao and Moritz, Dominik and Hohman, Fred},
	year = {2025},
	pages = {809--819},
}

@article{raffel_exploring_2019,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.},
	urldate = {2025-01-30},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam M. and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2019},
	keywords = {C4},
}

@inproceedings{gao_rate_2019,
	title = {Rate {Distortion} {For} {Model} {Compression}: {From} {Theory} {To} {Practice}},
	shorttitle = {Rate {Distortion} {For} {Model} {Compression}},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Weihao and Liu, Yu-Han and Wang, Chong and Oh, Sewoong},
	year = {2019},
	pages = {2102--2111},
}

@article{zhu_survey_2024,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	volume = {12},
	urldate = {2024-12-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	year = {2024},
	pages = {1556--1577},
}
@article{zhu_survey_2025,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	volume = {12},
	urldate = {2024-12-10},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	year = {2024},
	pages = {1556--1577},
}

@misc{zhu_survey_2024-1,
	title = {A {Survey} on {Model} {Compression} for {Large} {Language} {Models}},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	year = {2024},
	note = {arXiv:2308.07633 [cs]},
}

@inproceedings{merity_pointer_2016,
  author       = {Stephen Merity and
                  Caiming Xiong and
                  James Bradbury and
                  Richard Socher},
  title        = {Pointer Sentinel Mixture Models},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year         = {2017},
}

@inproceedings{hohman_model_2024,
	title = {Model {Compression} in {Practice}: {Lessons} {Learned} from {Practitioners} {Creating} {On}-device {Machine} {Learning} {Experiences}},
	shorttitle = {Model {Compression} in {Practice}},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hohman, Fred and Kery, Mary Beth and Ren, Donghao and Moritz, Dominik},
	year = {2024},
	pages = {1--18},
}

@article{mirsky_symmetric_1960,
	title = {Symmetric {Gauge} {Functions} and {Unitarily} {Invariant} {Norms}},
	volume = {11},
	number = {1},
	urldate = {2025-01-23},
	journal = {The Quarterly Journal of Mathematics},
	author = {Mirsky, L.},
	year = {1960},
	pages = {50--59},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	urldate = {2025-01-30},
	note = {arXiv:2307.09288 [cs]},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin R. and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, D. and Blecher, Lukas and Ferrer, Cristian Cantón and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, A. and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel M. and Korenev, A. and Koura, Punit Singh and Lachaux, M. and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, R. and Tan, Xia and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zhengxu and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, M. and Narang, Sharan and Rodriguez, Aurélien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	year = {2023},
}

@touvron{touvron_llama_2023-1,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, M. and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurélien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	note = {arXiv:2302.13971 [cs]},
	year = {2023},
}

@inproceedings{DBLP:journals/corr/KingmaB14,
	title = {Adam: {A} method for stochastic optimization},
	booktitle = {{ICLR}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
}

@misc{Raschka_2024,
	title = {New {LLM} pre-training and post-training paradigms},
	publisher = {Ahead of AI},
	author = {Raschka, Sebastian},
	year = {2024},
	note = {Blog post. Accessed 1/31/2025.},
}

@misc{yu_super_2024,
	title = {The {Super} {Weight} in {Large} {Language} {Models}},
	abstract = {Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01\%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: Pruning as few as a single parameter can destroy an LLM's ability to generate text -- increasing perplexity by 3 orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. We additionally find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can improve simple round-to-nearest quantization to become competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Yu, Mengxia and Wang, De and Shan, Qi and Reed, Colorado and Wan, Alvin},
	year = {2024},
	note = {arXiv:2411.07191 [cs]},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: an imperative style, high-performance deep learning library},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	number = {721},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8026--8037},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year = {2015},
	note = {arXiv:1503.02531 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@inproceedings{edalati_kronecker_2022,
	title = {Kronecker {Decomposition} for {GPT} {Compression}},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Edalati, Ali and Tahaei, Marzieh and Rashid, Ahmad and Nia, Vahid and Clark, James and Rezagholizadeh, Mehdi},
	year = {2022},
	pages = {219--226},
}

@inproceedings{tahaei_kroneckerbert_2022,
	address = {Seattle, United States},
	title = {{KroneckerBERT}: {Significant} {Compression} of {Pre}-trained {Language} {Models} {Through} {Kronecker} {Decomposition} and {Knowledge} {Distillation}},
	shorttitle = {{KroneckerBERT}},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tahaei, Marzieh and Charlaix, Ella and Nia, Vahid and Ghodsi, Ali and Rezagholizadeh, Mehdi},
	year = {2022},
	pages = {2116--2127},
}

@misc{xia_sheared_2024,
	title = {Sheared {LLaMA}: {Accelerating} {Language} {Model} {Pre}-training via {Structured} {Pruning}},
	shorttitle = {Sheared {LLaMA}},
	abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and the concurrent TinyLlama models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building competitive small-scale LLMs},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
	year = {2024},
	note = {arXiv:2310.06694 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kim_squeezellm_2024,
	title = {{SqueezeLLM}: {Dense}-and-{Sparse} {Quantization}},
	shorttitle = {{SqueezeLLM}},
	url = {http://arxiv.org/abs/2306.07629},
	doi = {10.48550/arXiv.2306.07629},
	abstract = {Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W. and Keutzer, Kurt},
	month = jun,
	year = {2024},
	note = {arXiv:2306.07629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{deletang_language_2023,
	title = {Language {Modeling} {Is} {Compression}},
	url = {https://openreview.net/forum?id=jznbgiynus},
	abstract = {It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models exhibit impressive predictive capabilities, they are well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4\% and LibriSpeech samples to 16.4\% of their raw size, beating domain-specific compressors like PNG (58.5\%) or FLAC (30.3\%), respectively. Finally, we show that the prediction-compression equivalence allows us to use any compressor (like gzip) to build a conditional generative model.},
	language = {en},
	urldate = {2025-01-30},
	author = {Deletang, Gregoire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and Hutter, Marcus and Veness, Joel},
	month = oct,
	year = {2023},
}

@misc{qwen_qwen25_2024,
	title = {Qwen2.5 {Technical} {Report}},
	author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
	year = {2024},
	note = {arxiv:2412.15115 [cs]},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and {Wang} and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	note = {arXiv:2407.21783 [cs]},
	year = {2024},
}

@inproceedings{louizos_bayesian_2017,
	title = {Bayesian {Compression} for {Deep} {Learning}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html},
	abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
	urldate = {2025-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
	year = {2017},
}

@inproceedings{ullrich_soft_2017,
	title = {Soft {Weight}-{Sharing} for {Neural} {Network} {Compression}},
	url = {https://openreview.net/forum?id=HJGwcKclx},
	abstract = {The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of "soft weight-sharing" (Nowlan \& Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.},
	language = {en},
	urldate = {2025-01-30},
	author = {Ullrich, Karen and Meeds, Edward and Welling, Max},
	month = feb,
	year = {2017},
}

@misc{hsuLanguageModelCompression2022,
	title = {Language model compression with weighted low-rank factorization},
	author = {Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
	note = {arxiv:2207.00112 [cs]},
	year = {2022},
}

@misc{saha_minimax_2022,
	title = {Minimax {Optimal} {Quantization} of {Linear} {Models}: {Information}-{Theoretic} {Limits} and {Efficient} {Algorithms}},
	shorttitle = {Minimax {Optimal} {Quantization} of {Linear} {Models}},
	url = {http://arxiv.org/abs/2202.11277},
	doi = {10.48550/arXiv.2202.11277},
	abstract = {High-dimensional models often have a large memory footprint and must be quantized after training before being deployed on resource-constrained edge devices for inference tasks. In this work, we develop an information-theoretic framework for the problem of quantizing a linear regressor learned from training data \$({\textbackslash}mathbf\{X\}, {\textbackslash}mathbf\{y\})\$, for some underlying statistical relationship \${\textbackslash}mathbf\{y\} = {\textbackslash}mathbf\{X\}{\textbackslash}boldsymbol\{{\textbackslash}theta\} + {\textbackslash}mathbf\{v\}\$. The learned model, which is an estimate of the latent parameter \${\textbackslash}boldsymbol\{{\textbackslash}theta\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, is constrained to be representable using only \$Bd\$ bits, where \$B {\textbackslash}in (0, {\textbackslash}infty)\$ is a pre-specified budget and \$d\$ is the dimension. We derive an information-theoretic lower bound for the minimax risk under this setting and propose a matching upper bound using randomized embedding-based algorithms which is tight up to constant factors. The lower and upper bounds together characterize the minimum threshold bit-budget required to achieve a performance risk comparable to the unquantized setting. We also propose randomized Hadamard embeddings that are computationally efficient and are optimal up to a mild logarithmic factor of the lower bound. Our model quantization strategy can be generalized and we show its efficacy by extending the method and upper-bounds to two-layer ReLU neural networks for non-linear regression. Numerical simulations show the improved performance of our proposed scheme as well as its closeness to the lower bound.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Saha, Rajarshi and Pilanci, Mert and Goldsmith, Andrea J.},
	month = aug,
	year = {2022},
	note = {arXiv:2202.11277 [cs]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Information Theory, Statistics - Machine Learning},
}

@inproceedings{yin_understanding_2018,
	title = {Understanding {Straight}-{Through} {Estimator} in {Training} {Activation} {Quantized} {Neural} {Nets}},
	abstract = {Training activation quantized neural networks involves minimizing a piecewise constant training loss whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass only, so that the "gradient" through the modified chain rule becomes non-trivial. Since this unusual "gradient" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual "gradient" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments.},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
	year = {2018},
}

@article{chen_lottery_2020,
	title = {The {Lottery} {Ticket} {Hypothesis} for {Pre}-trained {BERT} {Networks}},
	abstract = {In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40\% to 90\% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at this https URL.},
	urldate = {2024-04-23},
	journal = {ArXiv},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	month = jul,
	year = {2020},
}

@inproceedings{gao_adaptive_2024,
	title = {Adaptive {Rank} {Selections} for {Low}-{Rank} {Approximation} of {Language} {Models}},
	abstract = {Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for different layers in a language model. However, such a uniform rank selection is sub-optimal since different operations (layers) have non-uniform demand in capacity. In other words, a desired SVD strategy should allocate more ranks for important operations and vice versa. However, a globally-optimized selection of ranks for neural networks is still an open problem, and this is a non-trivial challenge since the selection is discrete. In this work, we propose a novel binary masking mechanism for optimizing the number of ranks in a differentiable framework. Our strategy uses a novel regularization to enable the masking to comply with the SVD property where the ranks have sorted singular values. The experiments examined both types of language models, encoder-only and decoder-only models, including large language models like LLaMA. Our compressed model achieves much better accuracy than previous SVD and their SOTA variants. More interestingly, our method retains significantly better accuracy with zero or limited fine-tuning, proving the substantial advantage of adaptive rank selection.},
	urldate = {2024-06-24},
	author = {Gao, Shangqian and Hua, Ting and Hsu, Yen-Chang and Shen, Yilin and Jin, Hongxia},
	year = {2024},
}

@misc{hafez-kolahi_information_2019,
	title = {Information {Bottleneck} and its {Applications} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1904.03743},
	doi = {10.48550/arXiv.1904.03743},
	abstract = {Information Theory (IT) has been used in Machine Learning (ML) from early days of this field. In the last decade, advances in Deep Neural Networks (DNNs) have led to surprising improvements in many applications of ML. The result has been a paradigm shift in the community toward revisiting previous ideas and applications in this new framework. Ideas from IT are no exception. One of the ideas which is being revisited by many researchers in this new era, is Information Bottleneck (IB); a formulation of information extraction based on IT. The IB is promising in both analyzing and improving DNNs. The goal of this survey is to review the IB concept and demonstrate its applications in deep learning. The information theoretic nature of IB, makes it also a good candidate in showing the more general concept of how IT can be used in ML. Two important concepts are highlighted in this narrative on the subject, i) the concise and universal view that IT provides on seemingly unrelated methods of ML, demonstrated by explaining how IB relates to minimal sufficient statistics, stochastic gradient descent, and variational auto-encoders, and ii) the common technical mistakes and problems caused by applying ideas from IT, which is discussed by a careful study of some recent methods suffering from them.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Hafez-Kolahi, Hassan and Kasaei, Shohreh},
	month = apr,
	year = {2019},
	note = {arXiv:1904.03743 [cs]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory, Statistics - Machine Learning},
}

@misc{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	doi = {10.48550/arXiv.physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv:physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@inproceedings{hafez-kolahi_rate-distortion_2021,
	title = {Rate-{Distortion} {Analysis} of {Minimum} {Excess} {Risk} in {Bayesian} {Learning}},
	url = {https://proceedings.mlr.press/v139/hafez-kolahi21a.html},
	abstract = {In parametric Bayesian learning, a prior is assumed on the parameter 𝑊WW which determines the distribution of samples. In this setting, Minimum Excess Risk (MER) is defined as the difference between the minimum expected loss achievable when learning from data and the minimum expected loss that could be achieved if 𝑊WW was observed. In this paper, we build upon and extend the recent results of (Xu \& Raginsky, 2020) to analyze the MER in Bayesian learning and derive information-theoretic bounds on it. We formulate the problem as a (constrained) rate-distortion optimization and show how the solution can be bounded above and below by two other rate-distortion functions that are easier to study. The lower bound represents the minimum possible excess risk achievable by {\textbackslash}emph\{any\} process using 𝑅RR bits of information from the parameter 𝑊WW. For the upper bound, the optimization is further constrained to use 𝑅RR bits from the training set, a setting which relates MER to information-theoretic bounds on the generalization gap in frequentist learning. We derive information-theoretic bounds on the difference between these upper and lower bounds and show that they can provide order-wise tight rates for MER under certain conditions. This analysis gives more insight into the information-theoretic nature of Bayesian learning as well as providing novel bounds.},
	language = {en},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hafez-Kolahi, Hassan and Moniri, Behrad and Kasaei, Shohreh and Baghshah, Mahdieh Soleymani},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3998--4007},
}

@article{bu_information-theoretic_2020,
	title = {Information-{Theoretic} {Understanding} of {Population} {Risk} {Improvement} with {Model} {Compression}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5730},
	doi = {10.1609/aaai.v34i04.5730},
	abstract = {We show that model compression can improve the population risk of a pre-trained model, by studying the tradeoff between the decrease in the generalization error and the increase in the empirical risk with model compression. We first prove that model compression reduces an information-theoretic bound on the generalization error; this allows for an interpretation of model compression as a regularization technique to avoid overfitting. We then characterize the increase in empirical risk with model compression using rate distortion theory. These results imply that the population risk could be improved by model compression if the decrease in generalization error exceeds the increase in empirical risk. We show through a linear regression example that such a decrease in population risk due to model compression is indeed possible. Our theoretical results further suggest that the Hessian-weighted K-means clustering compression approach can be improved by regularizing the distance between the clustering centers. We provide experiments with neural networks to support our theoretical assertions.},
	language = {en},
	number = {04},
	urldate = {2025-01-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bu, Yuheng and Gao, Weihao and Zou, Shaofeng and Veeravalli, Venugopal},
	month = apr,
	year = {2020},
	note = {Number: 04},
	pages = {3300--3307},
}

@misc{li_basis_2024,
	title = {Basis {Selection}: {Low}-{Rank} {Decomposition} of {Pretrained} {Large} {Language} {Models} for {Target} {Applications}},
	shorttitle = {Basis {Selection}},
	url = {http://arxiv.org/abs/2405.15877},
	doi = {10.48550/arXiv.2405.15877},
	abstract = {Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Li, Yang and Zhao, Changsheng and Lee, Hyungtak and Chang, Ernie and Shi, Yangyang and Chandra, Vikas},
	month = may,
	year = {2024},
	note = {arXiv:2405.15877 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
}

@misc{lin_autoregressive_2020,
	title = {Autoregressive {Knowledge} {Distillation} through {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2009.07253},
	doi = {10.48550/arXiv.2009.07253},
	abstract = {The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Lin, Alexander and Wohlwend, Jeremy and Chen, Howard and Lei, Tao},
	month = oct,
	year = {2020},
	note = {arXiv:2009.07253 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang_adalora_2023,
	title = {{AdaLoRA}: {Adaptive} {Budget} {Allocation} for {Parameter}-{Efficient} {Fine}-{Tuning}},
	shorttitle = {{AdaLoRA}},
	url = {http://arxiv.org/abs/2303.10512},
	doi = {10.48550/arXiv.2303.10512},
	abstract = {Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA .},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
	month = dec,
	year = {2023},
	note = {arXiv:2303.10512 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	edition = {2nd},
	title = {Elements of {Information} {Theory}},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}

@misc{li_losparse_2023,
	title = {{LoSparse}: {Structured} {Compression} of {Large} {Language} {Models} based on {Low}-{Rank} and {Sparse} {Approximation}},
	shorttitle = {{LoSparse}},
	url = {http://arxiv.org/abs/2306.11222},
	doi = {10.48550/arXiv.2306.11222},
	abstract = {Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
	month = jun,
	year = {2023},
	note = {arXiv:2306.11222 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_scaling_2024,
	title = {Scaling {Laws} for {Post} {Training} {Quantized} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2410.12119},
	doi = {10.48550/arXiv.2410.12119},
	abstract = {Generalization abilities of well-trained large language models (LLMs) are known to scale predictably as a function of model size. In contrast to the existence of practical scaling laws governing pre-training, the quality of LLMs after post-training compression remains highly unpredictable, often requiring case-by-case validation in practice. In this work, we attempted to close this gap for post-training weight quantization of LLMs by conducting a systematic empirical study on multiple LLM families quantized to numerous low-precision tensor data types using popular weight quantization techniques. We identified key scaling factors pertaining to characteristics of the local loss landscape, based on which the performance of quantized LLMs can be reasonably well predicted by a statistical model.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Xu, Zifei and Lan, Alexander and Yazar, Wanzin and Webb, Tristan and Sharify, Sayeh and Wang, Xin},
	month = dec,
	year = {2024},
	note = {arXiv:2410.12119 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{lingam_svft_2024,
	title = {{SVFT}: {Parameter}-{Efficient} {Fine}-{Tuning} with {Singular} {Vectors}},
	shorttitle = {{SVFT}},
	url = {http://arxiv.org/abs/2405.19597},
	doi = {10.48550/arXiv.2405.19597},
	abstract = {Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights {\textbackslash}(W{\textbackslash}) and inject learnable matrices {\textbackslash}({\textbackslash}Delta W{\textbackslash}). These {\textbackslash}({\textbackslash}Delta W{\textbackslash}) matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT, a simple approach that fundamentally differs from existing methods: the structure imposed on {\textbackslash}({\textbackslash}Delta W{\textbackslash}) depends on the specific weight matrix {\textbackslash}(W{\textbackslash}). Specifically, SVFT updates {\textbackslash}(W{\textbackslash}) as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96\% of full fine-tuning performance while training only 0.006 to 0.25\% of parameters, outperforming existing methods that only recover up to 85\% performance using 0.03 to 0.8\% of the trainable parameter budget.},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
	month = may,
	year = {2024},
	note = {arXiv:2405.19597 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{suggala_connecting_2018,
	title = {Connecting {Optimization} and {Regularization} {Paths}},
	volume = {31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
	year = {2018},
}

@article{daubechies_iterative_2004,
	title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
	volume = {57},
	issn = {0010-3640, 1097-0312},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.20042},
	doi = {10.1002/cpa.20042},
	abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted  p- penalties on the coefﬁcients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. Use of such  p-penalized problems with p {\textless} 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. c© 2004 Wiley Periodicals, Inc.},
	language = {en},
	number = {11},
	urldate = {2025-01-23},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Daubechies, I. and Defrise, M. and De Mol, C.},
	month = nov,
	year = {2004},
	pages = {1413--1457},
}

@article{beck_fast_2009,
	title = {A {Fast} {Iterative} {Shrinkage}-{Thresholding} {Algorithm} for {Linear} {Inverse} {Problems}},
	volume = {2},
	issn = {1936-4954},
	url = {https://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
}

@misc{yuan_asvd_2024,
	title = {{ASVD}: {Activation}-aware {Singular} {Value} {Decomposition} for {Compressing} {Large} {Language} {Models}},
	shorttitle = {{ASVD}},
	abstract = {In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from the distribution variance in the LLM activations and the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10\%-30\%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50\% KV cache reductions without performance drop in a training-free manner. Code is anonymously available in supplementary materials.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
	year = {2024},
	note = {arXiv:2312.05821 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{golub_matrix_2013,
	address = {Baltimore},
	edition = {Fourth edition},
	series = {Johns {Hopkins} studies in the mathematical sciences},
	title = {Matrix computations},
	isbn = {978-1-4214-0794-4},
	publisher = {The Johns Hopkins University Press},
	author = {Golub, Gene H. and Van Loan, Charles F.},
	year = {2013},
	note = {OCLC: ocn824733531},
	keywords = {Data processing, Matrices},
}

@article{tibshirani_valerie_nodate,
	title = {Valerie and {Patrick} {Hastie}},
	language = {en},
	author = {Tibshirani, Sami and Wainwright, John},
}

@article{hastie_statistical_nodate,
	title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {Generalizations}},
	language = {en},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal {Brain} {Surgeon} and general network pruning},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	author = {Hassibi, B. and Stork, D.G. and Wolff, G.J.},
	year = {1993},
	keywords = {Backpropagation, Benchmark testing, Biological neural networks, Data mining, Hardware, Machine learning, Pattern recognition, Statistics, Surges, Training data},
	pages = {293--299},
}

@misc{srinivas_learning_2016,
	title = {Learning {Neural} {Network} {Architectures} using {Backpropagation}},
	url = {http://arxiv.org/abs/1511.05497},
	doi = {10.48550/arXiv.1511.05497},
	abstract = {Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Srinivas, Suraj and Babu, R. Venkatesh},
	month = aug,
	year = {2016},
	note = {arXiv:1511.05497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{rajaraman_greedy_2023,
	title = {Greedy {Pruning} with {Group} {Lasso} {Provably} {Generalizes} for {Matrix} {Sensing}},
	url = {https://openreview.net/forum?id=BTRcVP7ZJn},
	abstract = {Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. In fact, several practical studies have shown that if the pruned model is fine-tuned with some gradient-based updates it generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem with the ground truth denoted \$U\_{\textbackslash}star {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{d {\textbackslash}times r\}\$ and the overparameterized model \$U {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{d {\textbackslash}times k\}\$ with \$k {\textbackslash}gg r\$. We study the approximate local minima of the mean square error, augmented with a smooth version of a group Lasso regularizer, \${\textbackslash}sum\_\{i=1\}{\textasciicircum}\{k\} {\textbackslash}lVert Ue\_i {\textbackslash}rVert\_2 \$. In particular, we provably show that pruning all the columns below a certain explicit \${\textbackslash}ell\_2\$-norm threshold results in a solution \$U\_\{{\textbackslash}text\{prune\}\}\$ which has the minimum number of columns \$r\$, yet close to the ground truth in training loss. Moreover, in the subsequent fine-tuning phase, gradient descent initialized at \$U\_\{{\textbackslash}text\{prune\}\}\$ converges at a linear rate to its limit. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which \{are not suitable for greedy pruning\}, i.e., many columns could have their \${\textbackslash}ell\_2\$ norm comparable to that of the maximum. Lastly, we show that our results also extend for the training and pruning of two-layer neural networks with quadratic activation functions. To the best of our knowledge, our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well.},
	language = {en},
	urldate = {2025-01-15},
	author = {Rajaraman, Nived and Devvrit, Fnu and Mokhtari, Aryan and Ramchandran, Kannan},
	month = nov,
	year = {2023},
}

@inproceedings{denton_exploiting_2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html},
	abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2×, while keeping the accuracy within 1\% of the original model.},
	urldate = {2025-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	year = {2014},
}

@article{rajaraman_greedy_nodate,
	title = {Greedy {Pruning} with {Group} {Lasso} {Provably} {Generalizes} for {Matrix} {Sensing}},
	language = {en},
	author = {Rajaraman, Nived},
}

@misc{hoefler_sparsity_2021,
	title = {Sparsity in {Deep} {Learning}: {Pruning} and growth for efficient inference and training in neural networks},
	shorttitle = {Sparsity in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2102.00554},
	doi = {10.48550/arXiv.2102.00554},
	abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
	urldate = {2025-01-13},
	publisher = {arXiv},
	author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
	month = jan,
	year = {2021},
	note = {arXiv:2102.00554 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{yin_outlier_2024,
	title = {Outlier {Weighed} {Layerwise} {Sparsity} ({OWL}): {A} {Missing} {Secret} {Sauce} for {Pruning} {LLMs} to {High} {Sparsity}},
	shorttitle = {Outlier {Weighed} {Layerwise} {Sparsity} ({OWL})},
	url = {http://arxiv.org/abs/2310.05175},
	doi = {10.48550/arXiv.2310.05175},
	abstract = {Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70\%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Li, Gen and Jaiswal, Ajay and Pechenizkiy, Mykola and Liang, Yi and Bendersky, Michael and Wang, Zhangyang and Liu, Shiwei},
	month = may,
	year = {2024},
	note = {arXiv:2310.05175 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{allen-zhu_physics_2024,
	title = {Physics of {Language} {Models}: {Part} 3.3, {Knowledge} {Capacity} {Scaling} {Laws}},
	shorttitle = {Physics of {Language} {Models}},
	publisher = {arXiv},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	year = {2024},
	note = {arXiv:2404.05405 [cs]},
}

@misc{noauthor_158-bit_nodate,
	title = {1.58-bit {FLUX}},
	url = {https://chenglin-yang.github.io/1.58bit.flux.github.io/},
	urldate = {2025-01-09},
}

@misc{das_beyond_2024,
	title = {Beyond {Size}: {How} {Gradients} {Shape} {Pruning} {Decisions} in {Large} {Language} {Models}},
	shorttitle = {Beyond {Size}},
	url = {http://arxiv.org/abs/2311.04902},
	doi = {10.48550/arXiv.2311.04902},
	abstract = {Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguingly, by incorporating gradients, unstructured pruning with our method tends to reveal some structural patterns, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various benchmarks show that GBLM-Pruner surpasses magnitude pruning, Wanda and SparseGPT by significant margins. We further extend our approach on Vision Transformer. Our code and models are available at https://github.com/VILA-Lab/GBLM-Pruner.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Das, Rocktim Jyoti and Sun, Mingjie and Ma, Liqun and Shen, Zhiqiang},
	month = apr,
	year = {2024},
	note = {arXiv:2311.04902 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{lecun_optimal_1989,
	title = {Optimal {Brain} {Damage}},
	volume = {2},
	abstract = {We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network,  sev(cid:173) eral  improvements  can  be  expected:  better  generalization,  fewer  training examples required,  and improved speed  of learning and/or  classification.  The  basic  idea  is  to  use  second-derivative  informa(cid:173) tion to make a  tradeoff between  network  complexity  and  training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John and Solla, Sara},
	year = {1989},
}

@misc{lu_all--one_2024,
	title = {All-in-{One} {Tuning} and {Structural} {Pruning} for {Domain}-{Specific} {LLMs}},
	url = {http://arxiv.org/abs/2412.14426},
	doi = {10.48550/arXiv.2412.14426},
	abstract = {Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88\% and 91\% performance of the dense model when pruning 40\% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Lu, Lei and Wang, Zhepeng and Bao, Runxue and Wang, Mengbing and Li, Fangyi and Wu, Yawen and Jiang, Weiwen and Xu, Jie and Wang, Yanzhi and Gao, Shangqian},
	month = dec,
	year = {2024},
	note = {arXiv:2412.14426 [cs]},
}

@inproceedings{ma_llm-pruner_2023,
	title = {{LLM}-{Pruner}: {On} the {Structural} {Pruning} of {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Pruner}},
	url = {https://openreview.net/forum?id=J8Ajf9WfXP},
	abstract = {Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code will be made public.},
	language = {en},
	urldate = {2025-01-03},
	author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	month = nov,
	year = {2023},
}

@inproceedings{kuzmin_pruning_2023,
	title = {Pruning vs {Quantization}: {Which} is {Better}?},
	shorttitle = {Pruning vs {Quantization}},
	url = {https://openreview.net/forum?id=0OU1ZXXxs5&referrer=%5Bthe%20profile%20of%20Arash%20Behboodi%5D(%2Fprofile%3Fid%3D~Arash_Behboodi1)},
	abstract = {Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date, only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question of which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower and upper bounds for the per-layer pruning and quantization error in trained networks and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models trained on 3 tasks and provide insights into the representations learned during fine-tuning with quantization and pruning in the loop. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with a very high compression ratio, compression might be beneficial from an accuracy standpoint.},
	language = {en},
	urldate = {2024-12-17},
	author = {Kuzmin, Andrey and Nagel, Markus and Baalen, Mart Van and Behboodi, Arash and Blankevoort, Tijmen},
	month = nov,
	year = {2023},
}

@misc{yuan_llm_2024,
	title = {{LLM} {Inference} {Unveiled}: {Survey} and {Roofline} {Model} {Insights}},
	shorttitle = {{LLM} {Inference} {Unveiled}},
	url = {http://arxiv.org/abs/2402.16363},
	doi = {10.48550/arXiv.2402.16363},
	abstract = {The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., Knowledge Distillation and Quantization), algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Our survey stands out by analyzing these methods with roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Zhou, Zhe and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and Yan, Yan and Chen, Beidi and Sun, Guangyu and Keutzer, Kurt},
	month = may,
	year = {2024},
	note = {arXiv:2402.16363 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{mozaffari_slim_2024,
	title = {{SLiM}: {One}-shot {Quantized} {Sparse} {Plus} {Low}-rank {Approximation} of {LLMs}},
	shorttitle = {{SLiM}},
	url = {http://arxiv.org/abs/2410.09615},
	doi = {10.48550/arXiv.2410.09615},
	abstract = {Large Language Models (LLMs) have revolutionized natural language understanding and generation tasks but suffer from high memory consumption and slow inference times due to their large parameter sizes. Traditional model compression techniques, such as quantization and pruning, mitigate these issues but often require retraining to maintain accuracy, which is computationally expensive. This paper introduces SLiM, a novel approach for compressing LLMs using a one-shot Quantized Sparse Plus Low-rank Approximation. SLiM eliminates the need for costly retraining by combining a symmetric quantization method (SLiM-Quant) with a saliency-based low-rank approximation. Our method reduces quantization error while leveraging sparse representations compatible with accelerated hardware architectures. Additionally, we propose a parameter-efficient fine-tuning recipe that significantly reduces overhead compared to conventional quantization-aware training. SLiM achieves up to a 5.4\% improvement in model accuracy for sparsity patterns like 2:4, and the fine-tuning step further enhances accuracy by up to 5.8\%, demonstrating state-of-the-art performance. This work provides a pathway for efficiently deploying large models in memory-constrained environments without compromising accuracy.},
	urldate = {2024-12-14},
	publisher = {arXiv},
	author = {Mozaffari, Mohammad and Dehnavi, Maryam Mehri},
	month = oct,
	year = {2024},
	note = {arXiv:2410.09615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance},
}

@inproceedings{chen_drone_2021,
	title = {{DRONE}: {Data}-aware {Low}-rank {Compression} for {Large} {NLP} {Models}},
	volume = {34},
	shorttitle = {{DRONE}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Patrick and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui},
	year = {2021},
	pages = {29321--29334},
}

@article{blalock_what_2020,
	title = {What is the {State} of {Neural} {Network} {Pruning}?},
	url = {https://www.semanticscholar.org/paper/What-is-the-State-of-Neural-Network-Pruning-Blalock-Ortiz/de66ada65cd9d36e46f1f8dd2c8be480180038ec},
	abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	urldate = {2024-12-09},
	journal = {ArXiv},
	author = {Blalock, Davis W. and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, J.},
	month = mar,
	year = {2020},
}

@inproceedings{shao_omniquant_2023,
	title = {{OmniQuant}: {Omnidirectionally} {Calibrated} {Quantization} for {Large} {Language} {Models}},
	shorttitle = {{OmniQuant}},
	url = {https://openreview.net/forum?id=8Wuvhh0LYW},
	abstract = {Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (\${\textbackslash}textbf\{OmniQuant\}\$) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at {\textbackslash}url\{https://github.com/OpenGVLab/OmniQuant\}.},
	language = {en},
	urldate = {2024-11-20},
	author = {Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping},
	month = oct,
	year = {2023},
}

@misc{ji_feature-based_2024,
	title = {Feature-based {Low}-{Rank} {Compression} of {Large} {Language} {Models} via {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2405.10616},
	abstract = {In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank compression, a promising technique, reduces non-essential parameters by decomposing weight matrices into products of two low-rank matrices. Yet, its application in LLMs has not been extensively studied. The key to low-rank compression lies in low-rank factorization and low-rank dimensions allocation. To address the challenges of low-rank compression in LLMs, we conduct empirical research on the low-rank characteristics of large models. We propose a low-rank compression method suitable for LLMs. This approach involves precise estimation of feature distributions through pooled covariance matrices and a Bayesian optimization strategy for allocating low-rank dimensions. Experiments on the LLaMA-2 models demonstrate that our method outperforms existing strong structured pruning and low-rank compression techniques in maintaining model performance at the same compression ratio.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Ji, Yixin and Xiang, Yang and Li, Juntao and Chen, Wei and Liu, Zhongyi and Chen, Kehai and Zhang, Min},
	month = may,
	year = {2024},
	note = {arXiv:2405.10616},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hasan_pruning_2024,
	title = {Pruning for {Protection}: {Increasing} {Jailbreak} {Resistance} in {Aligned} {LLMs} {Without} {Fine}-{Tuning}},
	shorttitle = {Pruning for {Protection}},
	url = {http://arxiv.org/abs/2401.10862},
	doi = {10.48550/arXiv.2401.10862},
	abstract = {This paper investigates the impact of model compression on the way Large Language Models (LLMs) process prompts, particularly concerning jailbreak resistance. We show that moderate WANDA pruning can enhance resistance to jailbreaking attacks without fine-tuning, while maintaining performance on standard benchmarks. To systematically evaluate this safety enhancement, we introduce a dataset of 225 harmful tasks across five categories. Our analysis of LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning benefits correlate with initial model safety levels. We interpret these results by examining changes in attention patterns and perplexity shifts, demonstrating that pruned models exhibit sharper attention and increased sensitivity to artificial jailbreak constructs. We extend our evaluation to the AdvBench harmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much safer on AdvBench prompts than on our dataset when evaluated with manual jailbreak attempts, and that pruning is effective against both automated attacks and manual jailbreaking on Advbench.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hasan, Adib and Rugina, Ileana and Wang, Alex},
	month = oct,
	year = {2024},
	note = {arXiv:2401.10862},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{dao_learning_2019,
	title = {Learning {Fast} {Algorithms} for {Linear} {Transforms} {Using} {Butterfly} {Factorizations}},
	url = {https://proceedings.mlr.press/v97/dao19a.html},
	abstract = {Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural prior they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the 𝑂(𝑁log𝑁)O(Nlog⁡N)O(N {\textbackslash}log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions 𝑁NN up to 102410241024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points—the first time a structured approach has done so—with 4X faster inference speed and 40X fewer parameters.},
	language = {en},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dao, Tri and Gu, Albert and Eichhorn, Matthew and Rudra, Atri and Re, Christopher},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1517--1527},
}

@misc{qiu_controlling_2024,
	title = {Controlling {Text}-to-{Image} {Diffusion} by {Orthogonal} {Finetuning}},
	url = {http://arxiv.org/abs/2306.07280},
	doi = {10.48550/arXiv.2306.07280},
	abstract = {Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images given a few images of a subject and a text prompt, and controllable generation where the goal is to enable the model to take in additional control signals. We empirically show that our OFT framework outperforms existing methods in generation quality and convergence speed.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Qiu, Zeju and Liu, Weiyang and Feng, Haiwen and Xue, Yuxuan and Feng, Yao and Liu, Zhen and Zhang, Dan and Weller, Adrian and Schölkopf, Bernhard},
	month = mar,
	year = {2024},
	note = {arXiv:2306.07280},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@misc{lingam_svft_2024-1,
	title = {{SVFT}: {Parameter}-{Efficient} {Fine}-{Tuning} with {Singular} {Vectors}},
	shorttitle = {{SVFT}},
	url = {http://arxiv.org/abs/2405.19597},
	doi = {10.48550/arXiv.2405.19597},
	abstract = {Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights {\textbackslash}(W{\textbackslash}) and inject learnable matrices {\textbackslash}({\textbackslash}Delta W{\textbackslash}). These {\textbackslash}({\textbackslash}Delta W{\textbackslash}) matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT, a simple approach that fundamentally differs from existing methods: the structure imposed on {\textbackslash}({\textbackslash}Delta W{\textbackslash}) depends on the specific weight matrix {\textbackslash}(W{\textbackslash}). Specifically, SVFT updates {\textbackslash}(W{\textbackslash}) as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96\% of full fine-tuning performance while training only 0.006 to 0.25\% of parameters, outperforming existing methods that only recover up to 85\% performance using 0.03 to 0.8\% of the trainable parameter budget.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Lingam, Vijay and Tejaswi, Atula and Vavre, Aditya and Shetty, Aneesh and Gudur, Gautham Krishna and Ghosh, Joydeep and Dimakis, Alex and Choi, Eunsol and Bojchevski, Aleksandar and Sanghavi, Sujay},
	month = may,
	year = {2024},
	note = {arXiv:2405.19597},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{sun_singular_2022,
	title = {Singular {Value} {Fine}-tuning: {Few}-shot {Segmentation} requires {Few}-parameters {Fine}-tuning},
	shorttitle = {Singular {Value} {Fine}-tuning},
	url = {https://openreview.net/forum?id=LEqYZz7cZOI},
	abstract = {Freezing the pre-trained backbone has become a standard paradigm to avoid overfitting in few-shot segmentation. In this paper, we rethink the paradigm and explore a new regime: \{{\textbackslash}em fine-tuning a small part of parameters in the backbone\}. We present a solution to overcome the overfitting problem, leading to better model generalization on learning novel classes. Our method decomposes backbone parameters into three successive matrices via the Singular Value Decomposition (SVD), then \{{\textbackslash}em only fine-tunes the singular values\} and keeps others frozen. The above design allows the model to adjust feature representations on novel classes while maintaining semantic clues within the pre-trained backbone. We evaluate our \{{\textbackslash}em Singular Value Fine-tuning (SVF)\} approach on various few-shot segmentation methods with different backbones. We achieve state-of-the-art results on both Pascal-5\${\textasciicircum}i\$ and COCO-20\${\textasciicircum}i\$ across 1-shot and 5-shot settings. Hopefully, this simple baseline will encourage researchers to rethink the role of backbone fine-tuning in few-shot settings.},
	language = {en},
	urldate = {2024-11-18},
	author = {Sun, Yanpeng and Chen, Qiang and He, Xiangyu and Wang, Jian and Feng, Haocheng and Han, Junyu and Ding, Errui and Cheng, Jian and Li, Zechao and Wang, Jingdong},
	month = oct,
	year = {2022},
}

@misc{saha_compressing_2024,
	title = {Compressing {Large} {Language} {Models} using {Low} {Rank} and {Low} {Precision} {Decomposition}},
	url = {http://arxiv.org/abs/2405.18886},
	doi = {10.48550/arXiv.2405.18886},
	abstract = {The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces \${\textbackslash}rm CALDERA\$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix \${\textbackslash}mathbf\{W\}\$ by approximating it via a low-rank, low-precision decomposition as \${\textbackslash}mathbf\{W\} {\textbackslash}approx {\textbackslash}mathbf\{Q\} + {\textbackslash}mathbf\{L\}{\textbackslash}mathbf\{R\}\$. Here, \${\textbackslash}mathbf\{L\}\$ and \${\textbackslash}mathbf\{R\}\$ are low rank factors, and the entries of \${\textbackslash}mathbf\{Q\}\$, \${\textbackslash}mathbf\{L\}\$ and \${\textbackslash}mathbf\{R\}\$ are quantized. The model is compressed by substituting each layer with its \${\textbackslash}mathbf\{Q\} + {\textbackslash}mathbf\{L\}{\textbackslash}mathbf\{R\}\$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, \${\textbackslash}mathbf\{L\}\$ and \${\textbackslash}mathbf\{R\}\$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. \${\textbackslash}rm CALDERA\$ obtains this decomposition by formulating it as an optimization problem \${\textbackslash}min\_\{{\textbackslash}mathbf\{Q\},{\textbackslash}mathbf\{L\},{\textbackslash}mathbf\{R\}\}{\textbackslash}lVert({\textbackslash}mathbf\{Q\} + {\textbackslash}mathbf\{L\}{\textbackslash}mathbf\{R\} - {\textbackslash}mathbf\{W\}){\textbackslash}mathbf\{X\}{\textasciicircum}{\textbackslash}top{\textbackslash}rVert\_\{{\textbackslash}rm F\}{\textasciicircum}2\$, where \${\textbackslash}mathbf\{X\}\$ is the calibration data, and \${\textbackslash}mathbf\{Q\}, {\textbackslash}mathbf\{L\}, {\textbackslash}mathbf\{R\}\$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of \${\textbackslash}rm CALDERA\$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-\$2\$ \$7\$B/\$13B\$/\$70\$B and LlaMa-\$3\$ \$8\$B models using \${\textbackslash}rm CALDERA\$ outperforms existing post-training LLM compression techniques in the regime of less than \$2.5\$ bits per parameter. The implementation is available at: https://github.com/pilancilab/caldera.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Saha, Rajarshi and Sagan, Naomi and Srivastava, Varun and Goldsmith, Andrea J. and Pilanci, Mert},
	month = nov,
	year = {2024},
	note = {arXiv:2405.18886},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{liu_parameter-efficient_2023,
	title = {Parameter-{Efficient} {Orthogonal} {Finetuning} via {Butterfly} {Factorization}},
	url = {https://openreview.net/forum?id=7NzgkEdGyr},
	abstract = {Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in computer vision and natural language. The results validate the effectiveness of BOFT as a generic finetuning method.},
	language = {en},
	urldate = {2024-11-14},
	author = {Liu, Weiyang and Qiu, Zeju and Feng, Yao and Xiu, Yuliang and Xue, Yuxuan and Yu, Longhui and Feng, Haiwen and Liu, Zhen and Heo, Juyeon and Peng, Songyou and Wen, Yandong and Black, Michael J. and Weller, Adrian and Schölkopf, Bernhard},
	month = oct,
	year = {2023},
}

@article{biderman_lora_2024,
	title = {{LoRA} {Learns} {Less} and {Forgets} {Less}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=aloEru2qCG},
	abstract = {Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (\${\textbackslash}approx\$100K prompt-response pairs) and continued pretraining (\${\textbackslash}approx\$20B unstructured tokens) data regimes. Our results show that, in the standard low-rank settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA better maintains the base model's performance on tasks outside the target domain. We show that LoRA mitigates forgetting more than common regularization techniques such as weight decay and dropout; it also helps maintain more diverse generations. Finally, we show that full finetuning learns perturbations with a rank that is 10-100\${\textbackslash}times\$ greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.},
	language = {en},
	urldate = {2024-11-14},
	journal = {Transactions on Machine Learning Research},
	author = {Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and Blakeney, Cody and Cunningham, John Patrick},
	month = may,
	year = {2024},
}

@misc{luo_empirical_2024,
	title = {An {Empirical} {Study} of {Catastrophic} {Forgetting} in {Large} {Language} {Models} {During} {Continual} {Fine}-tuning},
	url = {http://arxiv.org/abs/2308.08747},
	doi = {10.48550/arXiv.2308.08747},
	abstract = {Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale increases, the severity of forgetting intensifies. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that ALPACA maintains more knowledge and capacity compared to LLAMA during continual fine-tuning, suggesting that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning processes.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
	month = apr,
	year = {2024},
	note = {arXiv:2308.08747},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{velingker_clam_2024,
	title = {{CLAM}: {Unifying} {Finetuning}, {Quantization}, and {Pruning} by {Chaining} {LLM} {Adapter} {Modules}},
	shorttitle = {{CLAM}},
	url = {https://openreview.net/forum?id=1mKtFkElnu},
	abstract = {As LLMs have grown in size and applicability, so too have the number of methods that adapt them for downstream tasks. Recent works to address challenges in memory consumption, task performance, and inference efficiency have led to the fields of parameter-efficient finetuning (PEFT), quantization, and pruning, among others. While it is useful to combine their benefits, composing these techniques in flexible ways is challenging due to the changes each method makes to the model and any restrictions they might impose. To address these challenges, we develop an algebraic abstraction called CLAM that enables unlimited chaining of popular resource-efficient methods on nearly every modern LLM with minimal overhead. We demonstrate that CLAM can create new compositions of techniques that achieve SOTA performance on specializing compressed models across multiple benchmarks.},
	language = {en},
	urldate = {2024-11-12},
	author = {Velingker, Neelay and Liu, Jason and Sethi, Amish and Dodds, William and Xu, Zhiqiu and Dutta, Saikat and Naik, Mayur and Wong, Eric},
	month = jul,
	year = {2024},
}

@article{velingker_clam_nodate,
	title = {{CLAM}: {Unifying} {Finetuning}, {Quantization}, and {Pruning} by  {Chaining} {LLM} {Adapter} {Modules}},
	abstract = {As LLMs have grown in size and applicability, so too have the number of methods that adapt them for downstream tasks. Recent works to address challenges in memory consumption, task performance, and inference efficiency have led to the fields of parameter-efficient finetuning (PEFT), quantization, and pruning, among others. While it is useful to combine their benefits, composing these techniques in flexible ways is challenging due to the changes each method makes to the model and any restrictions they might impose. To address these challenges, we develop an algebraic abstraction called CLAM that enables unlimited chaining of popular resource-efficient methods on nearly every modern LLM with minimal overhead. We demonstrate that CLAM can create new compositions of techniques that achieve SOTA performance on specializing compressed models across multiple benchmarks.},
	language = {en},
	author = {Velingker, Neelay and Sethi, Amish and Liu, Jason and Dodds, William and Xu, Zhiqiu and Dutta, Saikat and Naik, Mayur and Wong, Eric},
}

@misc{noauthor_spp_nodate,
	title = {{SPP}: {Sparsity}-{Preserved} {Parameter}-{Efficient} {Fine}-{Tuning} for {Large} {Language} {Models}},
	url = {https://arxiv.org/html/2405.16057v1},
	urldate = {2024-11-12},
}

@misc{outmezguine_decoupled_2024,
	title = {Decoupled {Weight} {Decay} for {Any} \$p\$ {Norm}},
	url = {http://arxiv.org/abs/2404.10824},
	abstract = {With the success of deep neural networks (NNs) in a variety of domains, the computational and storage requirements for training and deploying large NNs have become a bottleneck for further improvements. Sparsification has consequently emerged as a leading approach to tackle these issues. In this work, we consider a simple yet effective approach to sparsification, based on the Bridge, or \$L\_p\$ regularization during training. We introduce a novel weight decay scheme, which generalizes the standard \$L\_2\$ weight decay to any \$p\$ norm. We show that this scheme is compatible with adaptive optimizers, and avoids the gradient divergence associated with \$0{\textless}p{\textless}1\$ norms. We empirically demonstrate that it leads to highly sparse networks, while maintaining generalization performance comparable to standard \$L\_2\$ regularization.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Outmezguine, Nadav Joseph and Levi, Noam},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10824 
version: 2},
}

@article{hansen_truncatedsvd_1987,
	title = {The {truncatedSVD} as a method for regularization},
	volume = {27},
	issn = {1572-9125},
	url = {https://doi.org/10.1007/BF01937276},
	doi = {10.1007/BF01937276},
	abstract = {The truncated singular value decomposition (SVD) is considered as a method for regularization of ill-posed linear least squares problems. In particular, the truncated SVD solution is compared with the usual regularized solution. Necessary conditions are defined in which the two methods will yield similar results. This investigation suggests the truncated SVD as a favorable alternative to standard-form regularization in cases of ill-conditioned matrices with well-determined numerical rank.},
	language = {en},
	number = {4},
	urldate = {2024-11-12},
	journal = {BIT Numerical Mathematics},
	author = {Hansen, Per Christian},
	month = dec,
	year = {1987},
	keywords = {65F20, 65F30, numerical rank, perturbation theory for truncated SVD, regularization in standard form, truncated SVD},
	pages = {534--553},
}

@misc{bae_relaxed_2024,
	title = {Relaxed {Recursive} {Transformers}: {Effective} {Parameter} {Sharing} with {Layer}-wise {LoRA}},
	shorttitle = {Relaxed {Recursive} {Transformers}},
	url = {http://arxiv.org/abs/2410.20672},
	doi = {10.48550/arXiv.2410.20672},
	abstract = {Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Bae, Sangmin and Fisch, Adam and Harutyunyan, Hrayr and Ji, Ziwei and Kim, Seungyeon and Schuster, Tal},
	month = oct,
	year = {2024},
	note = {arXiv:2410.20672},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{sun_simple_2024,
	title = {A {Simple} and {Effective} {Pruning} {Approach} for {Large} {Language} {Models}},
	publisher = {arXiv},
	author = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
	year = {2024},
	note = {arXiv:2306.11695},
}

@misc{wu_reft_2024,
	title = {{ReFT}: {Representation} {Finetuning} for {Language} {Models}},
	shorttitle = {{ReFT}},
	url = {http://arxiv.org/abs/2404.03592},
	abstract = {Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
	month = may,
	year = {2024},
	note = {arXiv:2404.03592},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{kim_memory-efficient_2023,
	title = {Memory-{Efficient} {Fine}-{Tuning} of {Compressed} {Large} {Language} {Models} via sub-4-bit {Integer} {Quantization}},
	url = {https://openreview.net/forum?id=2jUKhUrBxP},
	abstract = {Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) – a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to \$65\$ billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.},
	language = {en},
	urldate = {2024-11-11},
	author = {Kim, Jeonghoon and Lee, Jung Hyun and Kim, Sungdong and Park, Joonsuk and Yoo, Kang Min and Kwon, Se Jung and Lee, Dongsoo},
	month = nov,
	year = {2023},
}

@misc{leconte_reallm_2024,
	title = {{ReALLM}: {A} general framework for {LLM} compression and fine-tuning},
	shorttitle = {{ReALLM}},
	url = {http://arxiv.org/abs/2405.13155},
	abstract = {We introduce ReALLM, a novel approach for compression and memory-efficient adaptation of pre-trained language models that encompasses most of the post-training quantization and fine-tuning methods for a budget of {\textless}4 bits. Pre-trained matrices are decomposed into a high-precision low-rank component and a vector-quantized latent representation (using an autoencoder). During the fine-tuning step, only the low-rank components are updated. Our results show that pre-trained matrices exhibit different patterns. ReALLM adapts the shape of the encoder (small/large embedding, high/low bit VQ, etc.) to each matrix. ReALLM proposes to represent each matrix with a small embedding on \$b\$ bits and a neural decoder model \${\textbackslash}mathcal\{D\}\_{\textbackslash}phi\$ with its weights on \$b\_{\textbackslash}phi\$ bits. The decompression of a matrix requires only one embedding and a single forward pass with the decoder. Our weight-only quantization algorithm yields the best results on language generation tasks (C4 and WikiText-2) for a budget of \$3\$ bits without any training. With a budget of \$2\$ bits, ReALLM achieves state-of-the art performance after fine-tuning on a small calibration dataset.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Leconte, Louis and Bedin, Lisa and Nguyen, Van Minh and Moulines, Eric},
	month = may,
	year = {2024},
	note = {arXiv:2405.13155},
}

@misc{liu_llm-qat_2023,
	title = {{LLM}-{QAT}: {Data}-{Free} {Quantization} {Aware} {Training} for {Large} {Language} {Models}},
	shorttitle = {{LLM}-{QAT}},
	url = {http://arxiv.org/abs/2305.17888},
	abstract = {Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	month = may,
	year = {2023},
	note = {arXiv:2305.17888},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhao_galore_2024,
	title = {{GaLore}: {Memory}-{Efficient} {LLM} {Training} by {Gradient} {Low}-{Rank} {Projection}},
	shorttitle = {{GaLore}},
	url = {http://arxiv.org/abs/2403.03507},
	doi = {10.48550/arXiv.2403.03507},
	abstract = {Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5\% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5\% and total training memory by 63.3\%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
	month = jun,
	year = {2024},
	note = {arXiv:2403.03507},
}

@misc{teja_compression-aware_2020,
	title = {Compression-aware {Continual} {Learning} using {Singular} {Value} {Decomposition}},
	url = {http://arxiv.org/abs/2009.01956},
	doi = {10.48550/arXiv.2009.01956},
	abstract = {We propose a compression based continual task learning method that can dynamically grow a neural network. Inspired from the recent model compression techniques, we employ compression-aware training and perform low-rank weight approximations using singular value decomposition (SVD) to achieve network compaction. By encouraging the network to learn low-rank weight filters, our method achieves compressed representations with minimal performance degradation without the need for costly fine-tuning. Specifically, we decompose the weight filters using SVD and train the network on incremental tasks in its factorized form. Such a factorization allows us to directly impose sparsity-inducing regularizers over the singular values and allows us to use fewer number of parameters for each task. We further introduce a novel shared representational space based learning between tasks. This promotes the incoming tasks to only learn residual task-specific information on top of the previously learnt weight filters and greatly helps in learning under fixed capacity constraints. Our method significantly outperforms prior continual learning approaches on three benchmark datasets, demonstrating accuracy improvements of 10.3\%, 12.3\%, 15.6\% on 20-split CIFAR-100, miniImageNet and a 5-sequence dataset, respectively, over state-of-the-art. Further, our method yields compressed models that have {\textasciitilde}3.64x, 2.88x, 5.91x fewer number of parameters respectively, on the above mentioned datasets in comparison to baseline individual task models. Our source code is available at https://github.com/pavanteja295/CACL.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Teja, Varigonda Pavan and Panda, Priyadarshini},
	month = sep,
	year = {2020},
	note = {arXiv:2009.01956},
}

@misc{yang_learning_2020,
	title = {Learning {Low}-rank {Deep} {Neural} {Networks} via {Singular} {Vector} {Orthogonality} {Regularization} and {Singular} {Value} {Sparsification}},
	url = {http://arxiv.org/abs/2004.09031},
	doi = {10.48550/arXiv.2004.09031},
	abstract = {Modern deep neural networks (DNNs) often require high memory consumption and large computational loads. In order to deploy DNN algorithms efficiently on edge or mobile devices, a series of DNN compression algorithms have been explored, including factorization methods. Factorization methods approximate the weight matrix of a DNN layer with the multiplication of two or multiple low-rank matrices. However, it is hard to measure the ranks of DNN layers during the training process. Previous works mainly induce low-rank through implicit approximations or via costly singular value decomposition (SVD) process on every training step. The former approach usually induces a high accuracy loss while the latter has a low efficiency. In this work, we propose SVD training, the first method to explicitly achieve low-rank DNNs during training without applying SVD on every step. SVD training first decomposes each layer into the form of its full-rank SVD, then performs training directly on the decomposed weights. We add orthogonality regularization to the singular vectors, which ensure the valid form of SVD and avoid gradient vanishing/exploding. Low-rank is encouraged by applying sparsity-inducing regularizers on the singular values of each layer. Singular value pruning is applied at the end to explicitly reach a low-rank model. We empirically show that SVD training can significantly reduce the rank of DNN layers and achieve higher reduction on computation load under the same accuracy, comparing to not only previous factorization methods but also state-of-the-art filter pruning methods.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Yang, Huanrui and Tang, Minxue and Wen, Wei and Yan, Feng and Hu, Daniel and Li, Ang and Li, Hai and Chen, Yiran},
	month = apr,
	year = {2020},
	note = {arXiv:2004.09031},
}

@inproceedings{sharma_truth_2023,
	title = {The {Truth} is in {There}: {Improving} {Reasoning} in {Language} {Models} with {Layer}-{Selective} {Rank} {Reduction}},
	booktitle = {ICLR},
	urldate = {2024-11-11},
	author = {Sharma, Pratyusha and Ash, Jordan T. and Misra, Dipendra},
	year = {2023},
}

@inproceedings{compressing-llms,
	title = {Compressing llms: {The} truth is rarely pure and never simple},
	url = {https://arxiv.org/abs/2310.01382},
	booktitle = {{ICLR}},
	author = {Jaiswal, Ajay and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zhangyang and Yang, Yinfei},
	year = {2024},
}

@misc{ding_sparse_2023,
	title = {Sparse {Low}-rank {Adaptation} of {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2311.11696},
	doi = {10.48550/arXiv.2311.11696},
	abstract = {Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70\% retained parameters and 70\% training time.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11696},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{he_what_2024,
	title = {What {Matters} in {Transformers}? {Not} {All} {Attention} is {Needed}},
	shorttitle = {What {Matters} in {Transformers}?},
	url = {http://arxiv.org/abs/2406.15786},
	doi = {10.48550/arXiv.2406.15786},
	abstract = {While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4{\textbackslash}\% speedup with only a 2.4{\textbackslash}\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90{\textbackslash}\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: {\textbackslash}url\{https://github.com/Shwai-He/LLM-Drop\}.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {He, Shwai and Sun, Guoheng and Shen, Zheyu and Li, Ang},
	month = oct,
	year = {2024},
	note = {arXiv:2406.15786},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{achtibat_attnlrp_2024,
	title = {{AttnLRP}: {Attention}-{Aware} {Layer}-{Wise} {Relevance} {Propagation} for {Transformers}},
	shorttitle = {{AttnLRP}},
	url = {https://proceedings.mlr.press/v235/achtibat24a.html},
	abstract = {Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.},
	language = {en},
	urldate = {2024-10-23},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Achtibat, Reduan and Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Jain, Aakriti and Wiegand, Thomas and Lapuschkin, Sebastian and Samek, Wojciech},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {135--168},
}

@misc{orgad_llms_2024,
	title = {{LLMs} {Know} {More} {Than} {They} {Show}: {On} the {Intrinsic} {Representation} of {LLM} {Hallucinations}},
	shorttitle = {{LLMs} {Know} {More} {Than} {They} {Show}},
	url = {http://arxiv.org/abs/2410.02707},
	doi = {10.48550/arXiv.2410.02707},
	abstract = {Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02707},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{hatefi_pruning_2024,
	title = {Pruning {By} {Explaining} {Revisited}: {Optimizing} {Attribution} {Methods} to {Prune} {CNNs} and {Transformers}},
	shorttitle = {Pruning {By} {Explaining} {Revisited}},
	url = {http://arxiv.org/abs/2408.12568},
	doi = {10.48550/arXiv.2408.12568},
	abstract = {To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of eXplainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer- and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at \${\textbackslash}href\{https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch\}\{{\textbackslash}text\{this https link\}\}\$.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Achtibat, Reduan and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
	month = aug,
	year = {2024},
	note = {arXiv:2408.12568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{patel_learning_2024,
	title = {Learning to {Compress}: {Local} {Rank} and {Information} {Compression} in {Deep} {Neural} {Networks}},
	shorttitle = {Learning to {Compress}},
	url = {http://arxiv.org/abs/2410.07687},
	doi = {10.48550/arXiv.2410.07687},
	abstract = {Deep neural networks tend to exhibit a bias toward low-rank solutions during training, implicitly learning low-dimensional feature representations. This paper investigates how deep multilayer perceptrons (MLPs) encode these feature manifolds and connects this behavior to the Information Bottleneck (IB) theory. We introduce the concept of local rank as a measure of feature manifold dimensionality and demonstrate, both theoretically and empirically, that this rank decreases during the final phase of training. We argue that networks that reduce the rank of their learned representations also compress mutual information between inputs and intermediate layers. This work bridges the gap between feature manifold rank and information compression, offering new insights into the interplay between information bottlenecks and representation learning.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Patel, Niket and Shwartz-Ziv, Ravid},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07687},
}

@misc{kopf_cosy_2024,
	title = {{CoSy}: {Evaluating} {Textual} {Explanations} of {Neurons}},
	shorttitle = {{CoSy}},
	url = {http://arxiv.org/abs/2405.20331},
	doi = {10.48550/arXiv.2405.20331},
	abstract = {A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While various methods exist to connect neurons to textual descriptions of human-understandable concepts, evaluating the quality of these explanation methods presents a major challenge in the field due to a lack of unified, general-purpose quantitative evaluation. In this work, we introduce CoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to evaluate the quality of textual explanations for latent neurons. Given textual explanations, our proposed framework leverages a generative model conditioned on textual input to create data points representing the textual explanation. Then, the neuron's response to these explanation data points is compared with the response to control data points, providing a quality estimate of the given explanation. We ensure the reliability of our proposed framework in a series of meta-evaluation experiments and demonstrate practical value through insights from benchmarking various concept-based textual explanation methods for Computer Vision tasks, showing that tested explanation methods significantly differ in quality.},
	urldate = {2024-10-19},
	publisher = {arXiv},
	author = {Kopf, Laura and Bommer, Philine Lou and Hedström, Anna and Lapuschkin, Sebastian and Höhne, Marina M.-C. and Bykov, Kirill},
	month = may,
	year = {2024},
	note = {arXiv:2405.20331},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through \${L}\_0\$ {Regularization}},
	url = {http://arxiv.org/abs/1712.01312},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = jun,
	year = {2018},
	note = {arXiv:1712.01312},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ferrando_information_2024,
	title = {Information {Flow} {Routes}: {Automatically} {Interpreting} {Language} {Models} at {Scale}},
	shorttitle = {Information {Flow} {Routes}},
	url = {https://arxiv.org/abs/2403.00824v2},
	abstract = {Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.},
	language = {en},
	urldate = {2024-09-18},
	journal = {arXiv.org},
	author = {Ferrando, Javier and Voita, Elena},
	month = feb,
	year = {2024},
}

@misc{zhuang_hydra_2024,
	title = {{HYDRA}: {Model} {Factorization} {Framework} for {Black}-{Box} {LLM} {Personalization}},
	shorttitle = {{HYDRA}},
	url = {http://arxiv.org/abs/2406.02888},
	doi = {10.48550/arXiv.2406.02888},
	abstract = {Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01\% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at https://github.com/night-chen/HYDRA.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Zhuang, Yuchen and Sun, Haotian and Yu, Yue and Qiang, Rushi and Wang, Qifan and Zhang, Chao and Dai, Bo},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02888 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{bykov_labeling_nodate,
	title = {Labeling {Neural} {Representations} with {Inverse} {Recognition}},
	abstract = {Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical signiﬁcance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical signiﬁcance. We demonstrate the applicability of INVERT in various scenarios, including the identiﬁcation of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models.},
	language = {en},
	author = {Bykov, Kirill and Kopf, Laura and Nakajima, Shinichi and Kloft, Marius},
}

@misc{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://arxiv.org/abs/1711.11279},
	doi = {10.48550/arXiv.1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jun,
	year = {2018},
	note = {arXiv:1711.11279 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{fel_craft_2023,
	title = {{CRAFT}: {Concept} {Recursive} {Activation} {FacTorization} for {Explainability}},
	shorttitle = {{CRAFT}},
	url = {http://arxiv.org/abs/2211.10154},
	doi = {10.48550/arXiv.2211.10154},
	abstract = {Attribution methods, which employ heatmaps to identify the most influential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, attributed in part to their narrow focus on the most prominent regions of an image -- revealing "where" the model looks, but failing to elucidate "what" the model sees in those areas. In this work, we try to fill in this gap with CRAFT -- a novel approach to identify both "what" and "where" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-centered utility benchmark, we find that our approach significantly improves on two of the three test scenarios. Our code is freely available at github.com/deel-ai/Craft.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Fel, Thomas and Picard, Agustin and Bethune, Louis and Boissin, Thibaut and Vigouroux, David and Colin, Julien and Cadène, Rémi and Serre, Thomas},
	month = mar,
	year = {2023},
	note = {arXiv:2211.10154 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ferrando_information_2024-1,
	title = {Information {Flow} {Routes}: {Automatically} {Interpreting} {Language} {Models} at {Scale}},
	shorttitle = {Information {Flow} {Routes}},
	url = {http://arxiv.org/abs/2403.00824},
	doi = {10.48550/arXiv.2403.00824},
	abstract = {Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Ferrando, Javier and Voita, Elena},
	month = apr,
	year = {2024},
	note = {arXiv:2403.00824 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{mao_selora_2024,
	title = {{SeLoRA}: {Self}-{Expanding} {Low}-{Rank} {Adaptation} of {Latent} {Diffusion} {Model} for {Medical} {Image} {Synthesis}},
	shorttitle = {{SeLoRA}},
	url = {https://arxiv.org/abs/2408.07196v1},
	abstract = {The persistent challenge of medical image synthesis posed by the scarcity of annotated data and the need to synthesize `missing modalities' for multi-modal analysis, underscored the imperative development of effective synthesis methods. Recently, the combination of Low-Rank Adaptation (LoRA) with latent diffusion models (LDMs) has emerged as a viable approach for efficiently adapting pre-trained large language models, in the medical field. However, the direct application of LoRA assumes uniform ranking across all linear layers, overlooking the significance of different weight matrices, and leading to sub-optimal outcomes. Prior works on LoRA prioritize the reduction of trainable parameters, and there exists an opportunity to further tailor this adaptation process to the intricate demands of medical image synthesis. In response, we present SeLoRA, a Self-Expanding Low-Rank Adaptation Module, that dynamically expands its ranking across layers during training, strategically placing additional ranks on crucial layers, to allow the model to elevate synthesis quality where it matters most. The proposed method not only enables LDMs to fine-tune on medical data efficiently but also empowers the model to achieve improved image quality with minimal ranking. The code of our SeLoRA method is publicly available on https://anonymous.4open.science/r/SeLoRA-980D .},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Mao, Yuchen and Li, Hongwei and Pang, Wei and Papanastasiou, Giorgos and Yang, Guang and Wang, Chengjia},
	month = aug,
	year = {2024},
}

@misc{zhao_factorllm_2024,
	title = {{FactorLLM}: {Factorizing} {Knowledge} via {Mixture} of {Experts} for {Large} {Language} {Models}},
	shorttitle = {{FactorLLM}},
	url = {https://arxiv.org/abs/2408.11855v1},
	abstract = {Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge confusion stemming from their monolithic and redundant architectures, which calls for more efficient solutions with minimal computational overhead, particularly for LLMs. In this paper, we explore the FFN computation paradigm in LLMs and introduce FactorLLM, a novel approach that decomposes well-trained dense FFNs into sparse sub-networks without requiring any further modifications, while maintaining the same level of performance. Furthermore, we embed a router from the Mixture-of-Experts (MoE), combined with our devised Prior-Approximate (PA) loss term that facilitates the dynamic activation of experts and knowledge adaptation, thereby accelerating computational processes and enhancing performance using minimal training data and fine-tuning steps. FactorLLM thus enables efficient knowledge factorization and activates select groups of experts specifically tailored to designated tasks, emulating the interactive functional segmentation of the human brain. Extensive experiments across various benchmarks demonstrate the effectiveness of our proposed FactorLLM which achieves comparable performance to the source model securing up to 85\% model performance while obtaining over a 30\% increase in inference speed. Code: https://github.com/zhenwuweihe/FactorLLM.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Zhao, Zhongyu and Dong, Menghang and Zhang, Rongyu and Zheng, Wenzhao and Zhang, Yunpeng and Yang, Huanrui and Du, Dalong and Keutzer, Kurt and Zhang, Shanghang},
	month = aug,
	year = {2024},
}

@misc{lin_modegpt_2024,
	title = {{MoDeGPT}: {Modular} {Decomposition} for {Large} {Language} {Model} {Compression}},
	shorttitle = {{MoDeGPT}},
	url = {http://arxiv.org/abs/2408.09632},
	doi = {10.48550/arXiv.2408.09632},
	abstract = {Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces {\textbackslash}textbf\{Mo\}dular {\textbackslash}textbf\{De\}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr{\textbackslash}"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98\% of compute costs on compressing a 13B model. On {\textbackslash}textsc\{Llama\}-2/3 and OPT models, MoDeGPT maintains 90-95\% zero-shot performance with 25-30\% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46\%.},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Lin, Chi-Heng and Gao, Shangqian and Smith, James Seale and Patel, Abhishek and Tuli, Shikhar and Shen, Yilin and Jin, Hongxia and Hsu, Yen-Chang},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09632 [cs, stat]},
	keywords = {15A23 (Primary), Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, Statistics - Machine Learning},
}

@misc{cha_towards_2024,
	title = {Towards {Robust} and {Cost}-{Efficient} {Knowledge} {Unlearning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.06621},
	doi = {10.48550/arXiv.2408.06621},
	abstract = {Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, training LLMs on human-written text entails significant risk of privacy and copyright violations, which demands an efficient machine unlearning framework to remove knowledge of sensitive data without retraining the model from scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing the likelihood of generating unwanted information, the unboundedness of increasing the cross-entropy loss causes not only unstable optimization, but also catastrophic forgetting of knowledge that needs to be retained. We also discover its joint application under low-rank adaptation results in significantly suboptimal computational cost vs. generative performance trade-offs. In light of this limitation, we propose two novel techniques for robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge loss that suppresses unwanted tokens by increasing the probability of the next most likely token, thereby retaining fluency and structure in language generation. We also propose to initialize low-rank adapter weights based on Fisher-weighted low-rank approximation, which induces faster unlearning and better knowledge retention by allowing model updates to be focused on parameters that are important in generating textual data we wish to remove.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Cha, Sungmin and Cho, Sungjun and Hwang, Dasol and Lee, Moontae},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06621 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_finetuning-lora_llama3-8b-base_imdb_sanity_check_2024-08-02_02-52-23_nodate,
	title = {finetuning-lora\_llama3-8b-base\_imdb\_sanity\_check\_2024-08-02\_02-52-23 {\textbar} mxm-mwl – {Weights} \& {Biases}},
	url = {https://merantix-momentum.wandb.io/mxm/mxm-mwl/runs/6yvfya7g?nw=nwuserpengfei},
	urldate = {2024-08-02},
}

@misc{liu_dora_2024,
	title = {{DoRA}: {Weight}-{Decomposed} {Low}-{Rank} {Adaptation}},
	shorttitle = {{DoRA}},
	url = {http://arxiv.org/abs/2402.09353},
	doi = {10.48550/arXiv.2402.09353},
	abstract = {Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing {\textbackslash}ours, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. {\textbackslash}ours{\textasciitilde}consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code is available at https://github.com/NVlabs/DoRA.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
	month = jul,
	year = {2024},
	note = {arXiv:2402.09353 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{muralidharan_compact_2024,
	title = {Compact {Language} {Models} via {Pruning} and {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2407.14679},
	doi = {10.48550/arXiv.2407.14679},
	abstract = {Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive. In this paper, we investigate if pruning an existing LLM and then re-training it with a fraction ({\textless}3\%) of the original training data can be a suitable alternative to repeated, full retraining. To this end, we develop a set of practical and effective compression best practices for LLMs that combine depth, width, attention and MLP pruning with knowledge distillation-based retraining; we arrive at these best practices through a detailed empirical exploration of pruning strategies for each axis, methods to combine axes, distillation strategies, and search techniques for arriving at optimal compressed architectures. We use this guide to compress the Nemotron-4 family of LLMs by a factor of 2-4x, and compare their performance to similarly-sized models on a variety of language modeling tasks. Deriving 8B and 4B models from an already pretrained 15B model using our approach requires up to 40x fewer training tokens per model compared to training from scratch; this results in compute cost savings of 1.8x for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16\% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. We have open-sourced Minitron model weights on Huggingface, with corresponding supplementary material including example code available on GitHub.},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Muralidharan, Saurav and Sreenivas, Sharath Turuvekere and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
	month = jul,
	year = {2024},
	note = {arXiv:2407.14679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_ddk_2024,
	title = {{DDK}: {Distilling} {Domain} {Knowledge} for {Efficient} {Large} {Language} {Models}},
	shorttitle = {{DDK}},
	url = {http://arxiv.org/abs/2407.16154},
	doi = {10.48550/arXiv.2407.16154},
	abstract = {Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a high-performing LLM (i.e., the teacher model). Prevailing techniques in LLM distillation typically use a black-box model API to generate high-quality pretrained and aligned datasets, or utilize white-box distillation by altering the loss function to better transfer knowledge from the teacher LLM. However, these methods ignore the knowledge differences between the student and teacher LLMs across domains. This results in excessive focus on domains with minimal performance gaps and insufficient attention to domains with large gaps, reducing overall performance. In this paper, we introduce a new LLM distillation framework called DDK, which dynamically adjusts the composition of the distillation dataset in a smooth manner according to the domain performance differences between the teacher and student models, making the distillation process more stable and effective. Extensive evaluations show that DDK significantly improves the performance of student models, outperforming both continuously pretrained baselines and existing knowledge distillation methods by a large margin.},
	urldate = {2024-07-30},
	publisher = {arXiv},
	author = {Liu, Jiaheng and Zhang, Chenchen and Guo, Jinyang and Zhang, Yuanxing and Que, Haoran and Deng, Ken and Bai, Zhiqi and Liu, Jie and Zhang, Ge and Wang, Jiakai and Wu, Yanan and Liu, Congnan and Su, Wenbo and Wang, Jiamang and Qu, Lin and Zheng, Bo},
	month = jul,
	year = {2024},
	note = {arXiv:2407.16154 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{hu_lora_2021,
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle = {ICLR},
  title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
  year = {2022},
}

@misc{noauthor_moduledict_nodate,
	title = {{ModuleDict} — {PyTorch} 2.4 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html},
	urldate = {2024-07-25},
}

@misc{noauthor_job_nodate,
	title = {Job {\textbar} {Ray} {Dashboard}},
	url = {http://localhost:8265/#/jobs/06000000},
	urldate = {2024-07-24},
}

@misc{jaiswal_galore_2024,
	title = {From {GaLore} to {WeLore}: {How} {Low}-{Rank} {Weights} {Non}-uniformly {Emerge} from {Low}-{Rank} {Gradients}},
	shorttitle = {From {GaLore} to {WeLore}},
	abstract = {Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, in this work we first study the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Our findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, we present Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. Our gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50{\textbackslash}\% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with {\textasciitilde}3x better throughput and {\textasciitilde}0.6x GPU requirement. Our codes are available at {\textbackslash}url\{https://github.com/VITA-Group/welore\}},
	author = {Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
	year = {2024},
	note = {arXiv:2407.11239 [cs]},
}

@misc{jaiswal_galore_2024-1,
	title = {From {GaLore} to {WeLore}: {How} {Low}-{Rank} {Weights} {Non}-uniformly {Emerge} from {Low}-{Rank} {Gradients}},
	shorttitle = {From {GaLore} to {WeLore}},
	publisher = {arXiv},
	author = {Jaiswal, Ajay and Yin, Lu and Zhang, Zhenyu and Liu, Shiwei and Zhao, Jiawei and Tian, Yuandong and Wang, Zhangyang},
	year = {2024},
	note = {arXiv:2407.11239 [cs]},
}

@misc{kalajdzievski_rank_2023,
	title = {A {Rank} {Stabilization} {Scaling} {Factor} for {Fine}-{Tuning} with {LoRA}},
	url = {http://arxiv.org/abs/2312.03732},
	doi = {10.48550/arXiv.2312.03732},
	abstract = {As large language models (LLMs) have become increasingly compute and memory intensive, parameter-efficient fine-tuning (PEFT) methods are now a common strategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA), which adds trainable low-rank "adapters" to selected layers. Each adapter consists of a low-rank matrix product, multiplicatively scaled by a rank-dependent factor. This scaling factor, which divides adapters by a factor of the rank, results in slowed learning and stunted performance for LoRA with higher-rank adapters. Consequently, the use of LoRA in practice has generally been limited to very low ranks. In this work, we study the impact of the scaling factor on the learning process and prove that LoRA adapters should be divided by a factor of the square root of the rank. Modifying LoRA with the appropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA) method, easily provides for a fine-tuning compute/performance trade-off, where larger ranks can be used to trade off increased computational resources during training for better fine-tuning performance, with no change in inference computing cost.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Kalajdzievski, Damjan},
	month = nov,
	year = {2023},
	note = {arXiv:2312.03732 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
}

@misc{meng_pissa_2024,
	title = {{PiSSA}: {Principal} {Singular} {Values} and {Singular} {Vectors} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{PiSSA}},
	url = {http://arxiv.org/abs/2404.02948},
	doi = {10.48550/arXiv.2404.02948},
	abstract = {To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes \${\textbackslash}Delta W {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{m {\textbackslash}times n\}\$ through the product of two matrices \$A {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{m {\textbackslash}times r\}\$ and \$B {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{r {\textbackslash}times n\}\$, where \$r {\textbackslash}ll {\textbackslash}min(m, n)\$, \$A\$ is initialized with Gaussian noise, and \$B\$ with zeros. LoRA freezes the original model \$W\$ and updates the "Noise \& Zero" adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices \$A\$ and \$B\$ with the principal components of the original matrix \$W\$, and put the remaining components into a residual matrix \$W{\textasciicircum}\{res\} {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{m {\textbackslash}times n\}\$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the "residual" parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86\%, surpassing LoRA's 67.7\% by 5.16\%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05\%, exceeding the performances of QLoRA at 81.73\%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
	month = may,
	year = {2024},
	note = {arXiv:2404.02948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{noauthor_eric_nodate,
	title = {Eric {L}. {Buehler} - {Google} {Search}},
	url = {https://www.google.com/search?q=Eric+L.+Buehler&sca_esv=20269cc155e0fb69&sca_upv=1&ei=a0yRZqaiGbeL9u8P7vWbkAU&ved=0ahUKEwjmg8ii6aGHAxW3hf0HHe76BlIQ4dUDCA8&uact=5&oq=Eric+L.+Buehler&gs_lp=Egxnd3Mtd2l6LXNlcnAiD0VyaWMgTC4gQnVlaGxlcjIFEAAYgAQyCBAAGIAEGKIEMggQABiABBiiBDIIEAAYgAQYogRI2xBQ_AlY1g9wAngBkAEAmAG_AaABqASqAQMzLjK4AQPIAQD4AQGYAgagAvoCwgIKEAAYsAMY1gQYR8ICBxAAGIAEGA2YAwCIBgGQBgKSBwM1LjGgB8oL&sclient=gws-wiz-serp},
	urldate = {2024-07-12},
}

@inproceedings{hwang_pc-lora_2024,
	title = {{PC}-{LoRA}: {Low}-{Rank} {Adaptation} for {Progressive} {Model} {Compression} with {Knowledge} {Distillation}},
	shorttitle = {{PC}-{LoRA}},
	url = {https://www.semanticscholar.org/paper/PC-LoRA%3A-Low-Rank-Adaptation-for-Progressive-Model-Hwang-Park/4fd873eca820d2c3d269246009c6550b75de1ed4?utm_source=alert_email&utm_content=PaperCitation&utm_campaign=AlertEmails_WEEKLY&utm_term=PaperCitation&email_index=0-0-0&utm_medium=35709943},
	abstract = {Low-rank adaption (LoRA) is a prominent method that adds a small number of learnable parameters to the frozen pre-trained weights for parameter-efficient fine-tuning. Prompted by the question, ``Can we make its representation enough with LoRA weights solely at the final phase of finetuning without the pre-trained weights?'' In this work, we introduce Progressive Compression LoRA{\textasciitilde}(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously perform model compression and fine-tuning. The PC-LoRA method gradually removes the pre-trained weights during the training process, eventually leaving only the low-rank adapters in the end. Thus, these low-rank adapters replace the whole pre-trained weights, achieving the goals of compression and fine-tuning at the same time. Empirical analysis across various models demonstrates that PC-LoRA achieves parameter and FLOPs compression rates of 94.36\%/89.1\% for vision models, e.g., ViT-B, and 93.42\%/84.2\% parameters and FLOPs compressions for language models, e.g., BERT.},
	urldate = {2024-06-17},
	author = {Hwang, Injoon and Park, Haewon and Lee, Youngwan and Yang, Jooyoung and Maeng, SunJae},
	month = jun,
	year = {2024},
}

@misc{noauthor_coastalcphlex_glue_2024,
	title = {coastalcph/lex\_glue · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/coastalcph/lex_glue},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-06-05},
	month = mar,
	year = {2024},
}

@misc{colombo_saullm-7b_2024,
	title = {{SaulLM}-{7B}: {A} pioneering {Large} {Language} {Model} for {Law}},
	shorttitle = {{SaulLM}-{7B}},
	url = {http://arxiv.org/abs/2403.03883},
	doi = {10.48550/arXiv.2403.03883},
	abstract = {In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the MIT License.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Colombo, Pierre and Pires, Telmo Pessoa and Boudiaf, Malik and Culver, Dominic and Melo, Rui and Corro, Caio and Martins, Andre F. T. and Esposito, Fabrizio and Raposo, Vera Lúcia and Morgado, Sofia and Desa, Michael},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03883 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {CRFM}},
	url = {https://crfm.stanford.edu/2022/12/15/biomedlm.html},
	urldate = {2024-06-05},
}

@misc{noauthor_bigbiomed_qa_2024,
	title = {bigbio/med\_qa · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/bigbio/med_qa},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-06-05},
	month = may,
	year = {2024},
}

@misc{noauthor_bigbiomed_qa_2024-1,
	title = {bigbio/med\_qa · {Datasets} at {Hugging} {Face}},
	url = {https://huggingface.co/datasets/bigbio/med_qa},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-06-05},
	month = may,
	year = {2024},
}

@misc{kapur_diffusion_2024,
	title = {Diffusion {On} {Syntax} {Trees} {For} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/2405.20519},
	abstract = {Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program’s output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts “noise” applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Kapur, Shreyas and Jenner, Erik and Russell, Stuart},
	month = may,
	year = {2024},
	note = {arXiv:2405.20519 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{kapur_diffusion_2024-1,
	title = {Diffusion {On} {Syntax} {Trees} {For} {Program} {Synthesis}},
	url = {http://arxiv.org/abs/2405.20519},
	abstract = {Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program’s output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts “noise” applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Kapur, Shreyas and Jenner, Erik and Russell, Stuart},
	month = may,
	year = {2024},
	note = {arXiv:2405.20519 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{clark_think_2018,
	title = {Think you have {Solved} {Question} {Answering}? {Try} {ARC}, the {AI2} {Reasoning} {Challenge}},
	shorttitle = {Think you have {Solved} {Question} {Answering}?},
	url = {http://arxiv.org/abs/1803.05457},
	doi = {10.48550/arXiv.1803.05457},
	abstract = {We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
	month = mar,
	year = {2018},
	note = {arXiv:1803.05457 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{rein_gpqa_2023,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {http://arxiv.org/abs/2311.12022},
	doi = {10.48550/arXiv.2311.12022},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12022 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{rein_gpqa_2023-1,
	title = {{GPQA}: {A} {Graduate}-{Level} {Google}-{Proof} {Q}\&{A} {Benchmark}},
	shorttitle = {{GPQA}},
	url = {http://arxiv.org/abs/2311.12022},
	abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are “Google-proof”). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4–based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions—for example, when developing new scientific knowledge—we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
	language = {en},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
	month = nov,
	year = {2023},
	note = {arXiv:2311.12022 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{bisk_piqa_2019,
	title = {{PIQA}: {Reasoning} about {Physical} {Commonsense} in {Natural} {Language}},
	shorttitle = {{PIQA}},
	url = {http://arxiv.org/abs/1911.11641},
	doi = {10.48550/arXiv.1911.11641},
	abstract = {To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95\% accuracy), large pretrained models struggle (77\%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
	month = nov,
	year = {2019},
	note = {arXiv:1911.11641 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bojar_findings_2014,
	address = {Baltimore, Maryland, USA},
	title = {Findings of the 2014 {Workshop} on {Statistical} {Machine} {Translation}},
	url = {https://aclanthology.org/W14-3302},
	doi = {10.3115/v1/W14-3302},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the {Ninth} {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Bojar, Ondřej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Saint-Amand, Herve and Soricut, Radu and Specia, Lucia and Tamchyna, Aleš},
	editor = {Bojar, Ondřej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Monz, Christof and Post, Matt and Specia, Lucia},
	month = jun,
	year = {2014},
	pages = {12--58},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{xu_survey_2024,
	title = {A {Survey} on {Knowledge} {Distillation} of {Large} {Language} {Models}},
	abstract = {In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: {\textbackslash}textit\{algorithm\}, {\textbackslash}textit\{skill\}, and {\textbackslash}textit\{verticalization\} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
	year = {2024},
	note = {arXiv:2402.13116 [cs]},
}

@misc{kurtic_sparse_2023,
	title = {Sparse {Fine}-tuning for {Inference} {Acceleration} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.06927},
	abstract = {We consider the problem of accurate sparse fine-tuning of large language models (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based fine-tuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translation), Whisper (speech translation), and open GPT-type (MPT for text generation). For MPT text generation, we show for the first time that sparse fine-tuning can reach 75\% sparsity without accuracy drops, provide notable end-to-end speedups for both CPU and GPU inference, and highlight that sparsity is also compatible with quantization approaches. Models and software for reproducing our results are provided in Section 6.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Kurtic, Eldar and Kuznedelev, Denis and Frantar, Elias and Goin, Michael and Alistarh, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{kurtic_sparse_2023-1,
	title = {Sparse {Fine}-tuning for {Inference} {Acceleration} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.06927},
	abstract = {We consider the problem of accurate sparse fine-tuning of large language models (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based fine-tuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translation), Whisper (speech translation), and open GPT-type (MPT for text generation). For MPT text generation, we show for the first time that sparse fine-tuning can reach 75\% sparsity without accuracy drops, provide notable end-to-end speedups for both CPU and GPU inference, and highlight that sparsity is also compatible with quantization approaches. Models and software for reproducing our results are provided in Section 6.},
	language = {en},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Kurtic, Eldar and Kuznedelev, Denis and Frantar, Elias and Goin, Michael and Alistarh, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{agarwalla_enabling_2024,
	title = {Enabling {High}-{Sparsity} {Foundational} {Llama} {Models} with {Efficient} {Pretraining} and {Deployment}},
	url = {http://arxiv.org/abs/2405.03594},
	doi = {10.48550/arXiv.2405.03594},
	abstract = {Large language models (LLMs) have revolutionized Natural Language Processing (NLP), but their size creates computational bottlenecks. We introduce a novel approach to create accurate, sparse foundational versions of performant LLMs that achieve full accuracy recovery for fine-tuning tasks at up to 70\% sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT one-shot pruning method and sparse pretraining of those models on a subset of the SlimPajama dataset mixed with a Python subset of The Stack dataset. We exhibit training acceleration due to sparsity on Cerebras CS-3 chips that closely matches theoretical scaling. In addition, we establish inference acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are realized via sparsity alone, thus enabling further gains through additional use of quantization. Specifically, we show a total speedup on CPUs for sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results across diverse, challenging tasks, including chat, instruction following, code generation, arithmetic reasoning, and summarization to prove their generality. This work paves the way for rapidly creating smaller and faster LLMs without sacrificing accuracy.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Agarwalla, Abhinav and Gupta, Abhay and Marques, Alexandre and Pandit, Shubhra and Goin, Michael and Kurtic, Eldar and Leong, Kevin and Nguyen, Tuan and Salem, Mahmoud and Alistarh, Dan and Lie, Sean and Kurtz, Mark},
	month = may,
	year = {2024},
	note = {arXiv:2405.03594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhang_pruning_2024,
	title = {Pruning as a {Domain}-specific {LLM} {Extractor}},
	url = {http://arxiv.org/abs/2405.06275},
	doi = {10.48550/arXiv.2405.06275},
	abstract = {Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task-agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Zhang, Nan and Liu, Yanchi and Zhao, Xujiang and Cheng, Wei and Bao, Runxue and Zhang, Rui and Mitra, Prasenjit and Chen, Haifeng},
	month = may,
	year = {2024},
	note = {arXiv:2405.06275 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ren_low-rank_2023,
	title = {Low-{Rank} {Prune}-{And}-{Factorize} for {Language} {Model} {Compression}},
	url = {http://arxiv.org/abs/2306.14152},
	doi = {10.48550/arXiv.2306.14152},
	abstract = {The components underpinning PLMs -- large weight matrices -- were shown to bear considerable redundancy. Matrix factorization, a well-established technique from matrix theory, has been utilized to reduce the number of parameters in PLM. However, it fails to retain satisfactory performance under moderate to high compression rate. In this paper, we identify the {\textbackslash}textit\{full-rankness\} of fine-tuned PLM as the fundamental bottleneck for the failure of matrix factorization and explore the use of network pruning to extract low-rank sparsity pattern desirable to matrix factorization. We find such low-rank sparsity pattern exclusively exists in models generated by first-order pruning, which motivates us to unite the two approaches and achieve more effective model compression. We further propose two techniques: sparsity-aware SVD and mixed-rank fine-tuning, which improve the initialization and training of the compression procedure, respectively. Experiments on GLUE and question-answering tasks show that the proposed method has superior compression-performance trade-off compared to existing approaches.},
	urldate = {2024-05-21},
	publisher = {arXiv},
	author = {Ren, Siyu and Zhu, Kenny Q.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14152 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{guo_lq-lora_2024,
	title = {{LQ}-{LoRA}: {Low}-rank {Plus} {Quantized} {Matrix} {Decomposition} for {Efficient} {Language} {Model} {Finetuning}},
	shorttitle = {{LQ}-{LoRA}},
	url = {http://arxiv.org/abs/2311.12023},
	doi = {10.48550/arXiv.2311.12023},
	abstract = {We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Guo, Han and Greengard, Philip and Xing, Eric P. and Kim, Yoon},
	month = jan,
	year = {2024},
	note = {arXiv:2311.12023 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{yu_compressing_2023,
	title = {Compressing {Transformers}: {Features} {Are} {Low}-{Rank}, but {Weights} {Are} {Not}!},
	volume = {37},
	shorttitle = {Compressing {Transformers}},
	abstract = {Transformer and its variants achieve excellent results in various computer vision and natural language processing tasks,  but high computational costs and reliance on large training datasets restrict their deployment in resource-constrained settings. Low-rank approximation of model weights has been effective in compressing CNN models, but its application to transformers has been less explored and is less effective. Existing methods require the complete dataset to fine-tune compressed models, which are both time-consuming and data-hungry. This paper reveals that the features (i.e., activations) are low-rank, but model weights are surprisingly not low-rank. Hence, AAFM is proposed, which adaptively determines the compressed model structure and locally compresses each linear layer's output features rather than the model weights. A second stage, GFM, optimizes the entire compressed network holistically. Both AAFM and GFM only use few training samples without labels, that is, they are few-shot, unsupervised, fast and effective. For example, with only 2K images without labels, 33\% of the parameters are removed in DeiT-B with 18.8\% relative throughput increase, but only a 0.23\% accuracy loss for ImageNet recognition. The proposed methods are successfully applied to the language modeling task in NLP, too. Besides, the few-shot compressed models generalize well in downstream tasks.},
	language = {en},
	number = {9},
	urldate = {2024-05-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yu, Hao and Wu, Jianxin},
	year = {2023},
	note = {Number: 9},
	keywords = {SNLP: Language Models},
	pages = {11007--11015},
}

@misc{hua_numerical_2022,
	title = {Numerical {Optimizations} for {Weighted} {Low}-rank {Estimation} on {Language} {Model}},
	url = {http://arxiv.org/abs/2211.09718},
	doi = {10.48550/arXiv.2211.09718},
	abstract = {Singular value decomposition (SVD) is one of the most popular compression methods that approximate a target matrix with smaller matrices. However, standard SVD treats the parameters within the matrix with equal importance, which is a simple but unrealistic assumption. The parameters of a trained neural network model may affect task performance unevenly, which suggests non-equal importance among the parameters. Compared to SVD, the decomposition method aware of parameter importance is the more practical choice in real cases. Unlike standard SVD, weighted value decomposition is a non-convex optimization problem that lacks a closed-form solution. We systematically investigated multiple optimization strategies to tackle the problem and examined our method by compressing Transformer-based language models. Further, we designed a metric to predict when the SVD may introduce a significant performance drop, for which our method can be a rescue strategy. The extensive evaluations demonstrate that our method can perform better than current SOTA methods in compressing Transformer-based language models.},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Hua, Ting and Hsu, Yen-Chang and Wang, Felicity and Lou, Qian and Shen, Yilin and Jin, Hongxia},
	month = dec,
	year = {2022},
	note = {arXiv:2211.09718 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{frantar_sparsegpt_2023,
	title = {{SparseGPT}: {Massive} {Language} {Models} {Can} {Be} {Accurately} {Pruned} in {One}-{Shot}},
	shorttitle = {{SparseGPT}},
	publisher = {arXiv},
	author = {Frantar, Elias and Alistarh, Dan},
	year = {2023},
	note = {arXiv:2301.00774 [cs]},
}

@inproceedings{jaggi_simple_2010,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	url = {https://www.semanticscholar.org/paper/A-Simple-Algorithm-for-Nuclear-Norm-Regularized-Jaggi-S%C3%BAlovsk%C3%BD/32ea73a18a30b48e6a46113047706d344de4706f},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon the recent sparse approximate SDP solver of (Hazan, 2008). The experimental efficiency of our method is demonstrated on large matrix completion problems such as the Netflix dataset. The algorithm comes with strong convergence guarantees, and can be interpreted as a first theoretically justified variant of Simon-Funk-type SVD heuristics. The method is free of tuning parameters, and very easy to parallelize.},
	urldate = {2024-05-15},
	author = {Jaggi, Martin and Súlovský, Marek},
	month = jun,
	year = {2010},
}

@misc{shi_domain_2023,
	title = {Domain {Generalization} via {Nuclear} {Norm} {Regularization}},
	url = {http://arxiv.org/abs/2303.07527},
	doi = {10.48550/arXiv.2303.07527},
	abstract = {The ability to generalize to unseen domains is crucial for machine learning systems deployed in the real world, especially when we only have data from limited training domains. In this paper, we propose a simple and effective regularization method based on the nuclear norm of the learned features for domain generalization. Intuitively, the proposed regularizer mitigates the impacts of environmental features and encourages learning domain-invariant features. Theoretically, we provide insights into why nuclear norm regularization is more effective compared to ERM and alternative regularization methods. Empirically, we conduct extensive experiments on both synthetic and real datasets. We show nuclear norm regularization achieves strong performance compared to baselines in a wide range of domain generalization tasks. Moreover, our regularizer is broadly applicable with various methods such as ERM and SWAD with consistently improved performance, e.g., 1.7\% and 0.9\% test accuracy improvements respectively on the DomainBed benchmark.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Shi, Zhenmei and Ming, Yifei and Fan, Ying and Sala, Frederic and Liang, Yingyu},
	month = dec,
	year = {2023},
	note = {arXiv:2303.07527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{jaggi_simple_nodate,
	title = {A {Simple} {Algorithm} for {Nuclear} {Norm} {Regularized} {Problems}},
	abstract = {Optimization problems with a nuclear norm regularization, such as e.g. low norm matrix factorizations, have seen many applications recently. We propose a new approximation algorithm building upon the recent sparse approximate SDP solver of (Hazan, 2008). The experimental eﬃciency of our method is demonstrated on large matrix completion problems such as the Netﬂix dataset. The algorithm comes with strong convergence guarantees, and can be interpreted as a ﬁrst theoretically justiﬁed variant of Simon-Funk-type SVD heuristics. The method is free of tuning parameters, and very easy to parallelize.},
	language = {en},
	author = {Jaggi, Martin and Sulovský, Marek},
}

@misc{noauthor_low-rank_nodate,
	title = {Low-{Rank} {Matrix} {Completion}},
}

@article{chi_low-rank_nodate,
	title = {Low-{Rank} {Matrix} {Completion}},
	language = {en},
	author = {Chi, Yuejie},
}

@misc{zhang_accelerating_2015,
	title = {Accelerating {Very} {Deep} {Convolutional} {Networks} for {Classification} and {Detection}},
	url = {http://arxiv.org/abs/1505.06798},
	doi = {10.48550/arXiv.1505.06798},
	abstract = {This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., {\textgreater}=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3\% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
	month = nov,
	year = {2015},
	note = {arXiv:1505.06798 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{lee_self-supervised_2018,
	title = {Self-supervised {Knowledge} {Distillation} {Using} {Singular} {Value} {Decomposition}},
	url = {http://arxiv.org/abs/1807.06819},
	doi = {10.48550/arXiv.1807.06819},
	abstract = {To solve deep neural network (DNN)'s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and the knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1{\textbackslash}\% better than the T-DNN in terms of classification accuracy. Also assuming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79{\textbackslash}\%. code is available on https://github.com/sseung0703/SSKD{\textbackslash}\_SVD.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Lee, Seung Hyun and Kim, Dae Ha and Song, Byung Cheol},
	month = jul,
	year = {2018},
	note = {arXiv:1807.06819 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ben_noach_compressing_2020,
	title = {Compressing {Pre}-trained {Language} {Models} by {Matrix} {Decomposition}},
	booktitle = {Proceedings of the 1st {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 10th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ben Noach, Matan and Goldberg, Yoav},
	editor = {Wong, Kam-Fai and Knight, Kevin and Wu, Hua},
	year = {2020},
	pages = {884--889},
}

@misc{wang_svd-llm_2024,
	title = {{SVD}-{LLM}: {Truncation}-aware {Singular} {Value} {Decomposition} for {Large} {Language} {Model} {Compression}},
	shorttitle = {{SVD}-{LLM}},
	publisher = {arXiv},
	author = {Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
	year = {2024},
	note = {arXiv:2403.07378 [cs]},
}

@inproceedings{kim_sequence-level_2016,
	address = {Austin, Texas},
	title = {Sequence-{Level} {Knowledge} {Distillation}},
	url = {https://aclanthology.org/D16-1139},
	doi = {10.18653/v1/D16-1139},
	urldate = {2024-05-07},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon and Rush, Alexander M.},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	pages = {1317--1327},
}

@misc{kundu_efficiently_2024,
	title = {Efficiently {Distilling} {LLMs} for {Edge} {Applications}},
	url = {http://arxiv.org/abs/2404.01353},
	abstract = {Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain highquality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Kundu, Achintya and Lim, Fabian and Chew, Aaron and Wynter, Laura and Chong, Penny and Lee, Rhui Dih},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01353 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{liu_norm_2023,
	title = {{NORM}: {Knowledge} {Distillation} via {N}-to-{One} {Representation} {Matching}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{NORM}},
	url = {https://arxiv.org/abs/2305.13803},
	doi = {10.48550/ARXIV.2305.13803},
	abstract = {Existing feature distillation methods commonly adopt the One-to-one Representation Matching between any pre-selected teacher-student layer pair. In this paper, we present N-to-One Representation (NORM), a new two-stage knowledge distillation method, which relies on a simple Feature Transform (FT) module consisting of two linear layers. In view of preserving the intact information learnt by the teacher network, during training, our FT module is merely inserted after the last convolutional layer of the student network. The first linear layer projects the student representation to a feature space having N times feature channels than the teacher representation from the last convolutional layer, and the second linear layer contracts the expanded output back to the original feature space. By sequentially splitting the expanded student representation into N non-overlapping feature segments having the same number of feature channels as the teacher's, they can be readily forced to approximate the intact teacher representation simultaneously, formulating a novel many-to-one representation matching mechanism conditioned on a single teacher-student layer pair. After training, such an FT module will be naturally merged into the subsequent fully connected layer thanks to its linear property, introducing no extra parameters or architectural modifications to the student network at inference. Extensive experiments on different visual recognition benchmarks demonstrate the leading performance of our method. For instance, the ResNet18{\textbar}MobileNet{\textbar}ResNet50-1/4 model trained by NORM reaches 72.14\%{\textbar}74.26\%{\textbar}68.03\% top-1 accuracy on the ImageNet dataset when using a pre-trained ResNet34{\textbar}ResNet50{\textbar}ResNet50 model as the teacher, achieving an absolute improvement of 2.01\%{\textbar}4.63\%{\textbar}3.03\% against the individually trained counterpart. Code is available at https://github.com/OSVAI/NORM},
	urldate = {2024-04-26},
	author = {Liu, Xiaolong and Li, Lujun and Li, Chao and Yao, Anbang},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{zhao_decoupled_2022,
	title = {Decoupled {Knowledge} {Distillation}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/9879819/},
	doi = {10.1109/CVPR52688.2022.01165},
	abstract = {State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the “difficulty” of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller.},
	urldate = {2024-04-26},
	journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	month = jun,
	year = {2022},
	note = {Conference Name: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
ISBN: 9781665469463
Place: New Orleans, LA, USA
Publisher: IEEE},
	pages = {11943--11952},
}

@misc{wu_tinyclip_2023,
	title = {{TinyCLIP}: {CLIP} {Distillation} via {Affinity} {Mimicking} and {Weight} {Inheritance}},
	shorttitle = {{TinyCLIP}},
	url = {http://arxiv.org/abs/2309.12314},
	doi = {10.48550/arXiv.2309.12314},
	abstract = {In this paper, we propose a novel cross-modal distillation method, called TinyCLIP, for large-scale language-image pre-trained models. The method introduces two core techniques: affinity mimicking and weight inheritance. Affinity mimicking explores the interaction between modalities during distillation, enabling student models to mimic teachers' behavior of learning cross-modal feature alignment in a visual-linguistic affinity space. Weight inheritance transmits the pre-trained weights from the teacher models to their student counterparts to improve distillation efficiency. Moreover, we extend the method into a multi-stage progressive distillation to mitigate the loss of informative weights during extreme compression. Comprehensive experiments demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of the pre-trained CLIP ViT-B/32 by 50\%, while maintaining comparable zero-shot performance. While aiming for comparable performance, distillation with weight inheritance can speed up the training by 1.4 - 7.8 \${\textbackslash}times\$ compared to training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M, achieves an impressive zero-shot top-1 accuracy of 41.1\% on ImageNet, surpassing the original CLIP ViT-B/16 by 3.5\% while utilizing only 8.9\% parameters. Finally, we demonstrate the good transferability of TinyCLIP in various downstream tasks. Code and models will be open-sourced at https://aka.ms/tinyclip.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Wu, Kan and Peng, Houwen and Zhou, Zhenghong and Xiao, Bin and Liu, Mengchen and Yuan, Lu and Xuan, Hong and Valenzuela, Michael and Xi and Chen and Wang, Xinggang and Chao, Hongyang and Hu, Han},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12314 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_wasserstein_2021,
	title = {Wasserstein {Contrastive} {Representation} {Distillation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	url = {https://ieeexplore.ieee.org/document/9577601/},
	doi = {10.1109/CVPR46437.2021.01603},
	abstract = {The primary goal of knowledge distillation (KD) is to encapsulate the information of a model learned from a teacher network into a student network, with the latter being more compact than the former. Existing work, e.g., using Kullback-Leibler divergence for distillation, may fail to capture important structural knowledge in the teacher network and often lacks the ability for feature generalization, particularly in situations when teacher and student are built to address different classification tasks. We propose Wasserstein Contrastive Representation Distillation (WCoRD), which leverages both primal and dual forms of Wasserstein distance for KD. The dual form is used for global knowledge transfer, yielding a contrastive learning objective that maximizes the lower bound of mutual information between the teacher and the student networks. The primal form is used for local contrastive knowledge transfer within a mini-batch, effectively matching the distributions of features between the teacher and the student networks. Experiments demonstrate that the proposed WCoRD method outperforms state-of-the-art approaches on privileged information distillation, model compression and cross-modal transfer.},
	urldate = {2024-04-26},
	journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Chen, Liqun and Wang, Dong and Gan, Zhe and Liu, Jingjing and Henao, Ricardo and Carin, Lawrence},
	month = jun,
	year = {2021},
	note = {Conference Name: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
ISBN: 9781665445092
Place: Nashville, TN, USA
Publisher: IEEE},
	pages = {16291--16300},
}

@misc{liang_module-wise_2023,
	title = {Module-wise {Adaptive} {Distillation} for {Multimodality} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2310.04550},
	doi = {10.48550/arXiv.2310.04550},
	abstract = {Pre-trained multimodal foundation models have demonstrated remarkable generalizability but pose challenges for deployment due to their large sizes. One effective approach to reducing their sizes is layerwise distillation, wherein small student models are trained to match the hidden representations of large teacher models at each layer. Motivated by our observation that certain architecture components, referred to as modules, contribute more significantly to the student's performance than others, we propose to track the contributions of individual modules by recording the loss decrement after distillation each module and choose the module with a greater contribution to distill more frequently. Such an approach can be naturally formulated as a multi-armed bandit (MAB) problem, where modules and loss decrements are considered as arms and rewards, respectively. We then develop a modified-Thompson sampling algorithm named OPTIMA to address the nonstationarity of module contributions resulting from model updating. Specifically, we leverage the observed contributions in recent history to estimate the changing contribution of each module and select modules based on these estimations to maximize the cumulative contribution. We evaluate the effectiveness of OPTIMA through distillation experiments on various multimodal understanding and image captioning tasks, using the CoCa-Large model (Yu et al., 2022) as the teacher model.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Liang, Chen and Yu, Jiahui and Yang, Ming-Hsuan and Brown, Matthew and Cui, Yin and Zhao, Tuo and Gong, Boqing and Zhou, Tianyi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04550 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{fang_compressing_2021,
	title = {Compressing {Visual}-linguistic {Model} via {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2104.02096},
	abstract = {Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer-based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student's detector while the features are from Teacher's own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
	month = apr,
	year = {2021},
	note = {arXiv:2104.02096 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sun_contrastive_2020,
	title = {Contrastive {Distillation} on {Intermediate} {Representations} for {Language} {Model} {Compression}},
	url = {http://arxiv.org/abs/2009.14167},
	doi = {10.48550/arXiv.2009.14167},
	abstract = {Existing language model compression methods mostly use a simple L2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Sun, Siqi and Gan, Zhe and Cheng, Yu and Fang, Yuwei and Wang, Shuohang and Liu, Jingjing},
	month = sep,
	year = {2020},
	note = {arXiv:2009.14167 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{li_bert-emd_2020,
	address = {Online},
	title = {{BERT}-{EMD}: {Many}-to-{Many} {Layer} {Mapping} for {BERT} {Compression} with {Earth} {Mover}'s {Distance}},
	shorttitle = {{BERT}-{EMD}},
	url = {https://aclanthology.org/2020.emnlp-main.242},
	doi = {10.18653/v1/2020.emnlp-main.242},
	abstract = {Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for different NLP tasks. In addition, we leverage Earth Mover's Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables effective matching for the many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model's performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression},
	urldate = {2024-04-25},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Jianquan and Liu, Xiaokang and Zhao, Honghong and Xu, Ruifeng and Yang, Min and Jin, Yaohong},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {3009--3018},
}

@misc{wen_f-divergence_2023,
	title = {f-{Divergence} {Minimization} for {Sequence}-{Level} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2307.15190},
	abstract = {Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
	month = jul,
	year = {2023},
	note = {arXiv:2307.15190 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.6, I.2.7, I.2.m, I.5.1, I.7.m},
}

@misc{xu_tebmerawesome-knowledge-distillation--llms_2024,
	title = {Tebmer/{Awesome}-{Knowledge}-{Distillation}-of-{LLMs}},
	url = {https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs},
	abstract = {This repository collects papers for "A Survey on Knowledge Distillation of Large Language Models". We break down KD into Knowledge Elicitation and Distillation Algorithms, and explore the Skill \& Vertical Distillation of LLMs.},
	urldate = {2024-04-24},
	author = {Xu, Shawn},
	month = apr,
	year = {2024},
	note = {original-date: 2024-02-08T16:04:45Z},
	keywords = {alignment, compression, data-augmentation, data-synthesis, feedback, instruction-following, kd, knowledge-distillation, large-language-model, llm, multi-modal, self-distillation, self-training, supervised-finetuning, survey},
}

@misc{wen_f-divergence_2023-1,
	title = {f-{Divergence} {Minimization} for {Sequence}-{Level} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2307.15190},
	abstract = {Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
	month = jul,
	year = {2023},
	note = {arXiv:2307.15190 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.6, I.2.7, I.2.m, I.5.1, I.7.m},
}

@misc{ko_distillm_2024,
	title = {{DistiLLM}: {Towards} {Streamlined} {Distillation} for {Large} {Language} {Models}},
	shorttitle = {{DistiLLM}},
	url = {http://arxiv.org/abs/2402.03898},
	doi = {10.48550/arXiv.2402.03898},
	abstract = {Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3\${\textbackslash}times\$ speedup compared to recent KD methods.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03898 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{liang_mixkd_2021,
	title = {{MIXKD}: {TOWARDS} {EFFICIENT} {DISTILLATION} {OF} {LARGE}-{SCALE} {LANGUAGE} {MODELS}},
	abstract = {Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-speciﬁc data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efﬁcient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher’s behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach.},
	language = {en},
	author = {Liang, Kevin J and Hao, Weituo and Shen, Dinghan and Zhou, Yufan and Chen, Weizhu and Chen, Changyou and Carin, Lawrence},
	year = {2021},
}

@misc{tunstall_zephyr_2023,
	title = {Zephyr: {Direct} {Distillation} of {LM} {Alignment}},
	shorttitle = {Zephyr},
	url = {http://arxiv.org/abs/2310.16944},
	doi = {10.48550/arXiv.2310.16944},
	abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Clémentine and Habib, Nathan and Sarrazin, Nathan and Sanseviero, Omar and Rush, Alexander M. and Wolf, Thomas},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16944 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{gu_minillm_2024,
	title = {{MINILLM}: {KNOWLEDGE} {DISTILLATION} {OF} {LARGE} {LANGUAGE} {MODELS}},
	abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MINILLM. Extensive experiments in the instruction-following setting show that MINILLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.},
	language = {en},
	author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
	year = {2024},
	keywords = {KD},
}

@inproceedings{agarwal_-policy_2023,
	title = {On-{Policy} {Distillation} of {Language} {Models}: {Learning} from {Self}-{Generated} {Mistakes}},
	shorttitle = {On-{Policy} {Distillation} of {Language} {Models}},
	url = {https://www.semanticscholar.org/paper/On-Policy-Distillation-of-Language-Models%3A-Learning-Agarwal-Vieillard/a57ef1f5c3af185af79751855b8033b7fc6d89b3},
	abstract = {Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.},
	urldate = {2024-04-22},
	author = {Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stańczyk, P. and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
	month = jun,
	year = {2023},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jiao_tinybert_2020,
	title = {{TinyBERT}: {Distilling} {BERT} for {Natural} {Language} {Understanding}},
	shorttitle = {{TinyBERT}},
	url = {http://arxiv.org/abs/1909.10351},
	doi = {10.48550/arXiv.1909.10351},
	abstract = {Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT. TinyBERT with 4 layers is empirically effective and achieves more than 96.8\% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28\% parameters and about 31\% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
	month = oct,
	year = {2020},
	note = {arXiv:1909.10351 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{panigrahi_task-specific_2023,
	title = {Task-{Specific} {Skill} {Localization} in {Fine}-tuned {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2302.06600},
	doi = {10.48550/ARXIV.2302.06600},
	abstract = {Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (\${\textbackslash}sim0.01\$\% of model parameters) responsible for (\$\&gt;95\$\%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution (\$40\$-\$90\$\% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning.},
	urldate = {2024-04-23},
	publisher = {[object Object]},
	author = {Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}
