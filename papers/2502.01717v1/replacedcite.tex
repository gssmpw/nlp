\section{Related work}
\label{sec:related}
Post-training compression methods for Foundation Models largely fall into three categories: knowledge distillation, quantization and (un-)structured pruning. For a comprehensive survey covering all of these, we refer to ____ and the references therein.

\methodname represents a specific form of structured pruning: factorization-based compression. Conventional structured pruning ____ jointly removes groups of parameters (e.g., network layers or matrix columns/rows). At high compression rates, however, such coarse approaches often remove important weights, causing a significant performance drop that is only recoverable through additional fine-tuning.

\paragraph{Low-Rank Decomposition}
Approximation of weight matrices through appropriate factorization aims to preserve critical information while being simple to implement. 
After initial exploration for smaller language models ____, methods for LLMs primarily built on (weighted) SVD of linear layers ____. Large approximation errors, however, made additional fine-tuning on down-stream tasks necessary.

Follow-up work recognized that the poor approximations are caused by LLM weights being high-rank and instead turned to decompose network features which are sparse ____. 
Notably, ASVD ____ explicitly accounts for the data distribution eliciting these activations and SVD-LLM ____ derives an analytical layer-wise correction.

Recent studies ____ have shown rank reduction to differently affect layers in a network and proposed different heuristics for non-uniform pruning of singular values. 

A common feature of these works is that they first truncate singular values to a desired size before computing an error correction. A change in compression ratio therefore prompts another round of computation. At the same time, the correction quality varies across different ratios, so that milder compression may not automatically preserve more predictive quality. In contrast, our approach creates consistent compression trade-offs from a single round of computation (optimization) and performs corrections on-the-fly.

\figureTradeoffCFour

\paragraph{Rate-distortion theory}

Rate-distortion theory investigates the analytical trade-off between
achievable data compression rates and the error (distortion) 
introduced by general lossy compression
algorithms ____.
While some recent work ____
investigates rate-distortion theory of machine learning models
for simple architectures under rather specific assumptions, 
the information-theoretic limits of neural network compression
are generally unknown in practically relevant settings.
In this context, the family of compressed models generated by \methodname conveniently provides an empirical (upper) bound on the distortion-rate function of a large-scale model from a single optimization run.
 %