\section{Model Detail} \label{sec:methods}
In this section, we introduce the details of our LCM-LAI to implement the final learning objective mentioned above, which can be divided into three main parts: \textbf{sentence embedding module}, \textbf{basic interaction module} (\textbf{BIM}), and \textbf{legal interaction module} (\textbf{LIM}).

\subsection{Sentence Embedding Module}
Firstly, we try to encode each legal case sentence to an embedding because our LCM-LAI models the interaction between legal cases from the sentence level.
In practice, we follow the typical method mentioned in~\cite{yu2022Explainable} that uses the output of the [CLS] token of a BERT model pre-trained on a Chinese legal case corpus\footnote{The pre-trained models we used are available at \url{https://github.com/thunlp/OpenCLaP}. 
Notice, that LCM-LAI also applies to the other pre-train language models with the corresponding embeddings.} as the sentence embedding. 
In the same way, we also encode legal articles because they are generally short enough to be treated as sentences.
For the specific sentences $x_i\in X$, $y_j\in Y$, and the specific law article $L_k\in \mathcal{L}$, we formalize the calculation process as follows,
$$
\mathbf{x}_i = \text{BERT}(x_i)[\textbf{CLS}], \ \ i \in \{1, \ldots, n_x\}
$$
$$
\mathbf{y}_j = \text{BERT}(y_j)[\textbf{CLS}], \ \ j \in \{1, \ldots, n_y\}
$$
$$
\mathbf{L}_k = \text{BERT}(L_k)[\textbf{CLS}], \ \ k \in \{1, \ldots, n_L\},
$$
where $\mathbf{x}_i, \mathbf{y}_i, \mathbf{L}_k \in \mathbb{R}^{d_b}$ are the corresponding sentence embeddings.

\subsection{Basic Interaction Module}
Referring to Sec.~\ref{subsec: framework}, the two interaction information $\langle \mathbf{X}, \mathbf{Y} \rangle$ and $\langle \mathbf{X}_L, \mathbf{Y}_L \rangle$ are both critical to the LCR \& LCM tasks.
To decouple $\langle \mathbf{X}, \mathbf{Y} \rangle$ with the legal-rational interaction $\langle \mathbf{X}_L, \mathbf{Y}_L \rangle$ that we emphasize, in the BIM, we understand and model it from the perspective of general semantic similarity following the consistent idea of general text matching~\cite{yu2022Explainable, reimers2019sentence-BERT, yin2016ABCNN}.
% {\color{blue} To explain the design of sentence-level full-match.}

\subsubsection{Semantic Correlation Matrix.} \label{subsubsec: semantic correlation matrix}
From a sentence-level perspective to capture the semantic matching information between legal cases, we construct a semantic matrix $\mathbf{C}^{(S)} \in \mathbb{R}^{n_x \times n_y}$ to indicate the semantic correlation between cross-case sentence pairs.
Inspired by the process in~\cite{yu2022Explainable}, we choose the euclidean-distance-based similarity to measure used to evaluate the semantic similarity between two sentences, i.e., $c_{i, j}^{(S)} = \text{dis-sim}(\mathbf{x}_i, \mathbf{y}_j)$, where $\text{dis-sim}(\cdot)$ denotes the euclidean-distance-based similarity computation function.
As with most multi-task frameworks, to improve the robustness of the model, we map the input sentence embeddings with a trainable two-layer MLP.
The specific formula is,
\begin{equation} \label{eq: semantic correlation matrix}
    c^{(S)}_{i,j} = -||\text{MLP}(\mathbf{x}_i) - \text{MLP}(\mathbf{y}_j)||_{2}=
    -\sqrt{(\text{MLP}(\mathbf{x}_i) - \text{MLP}(\mathbf{y}_j))^\mathsf{T}(\text{MLP}(\mathbf{x}_i) - \text{MLP}(\mathbf{y}_j))},
\end{equation}
where $\text{MLP}(\cdot)$ denotes the MLP operator, $||\cdot||_{2}$ denotes the euclidean norm and $c^{(S)}_{i,j} \in \mathbf{C}^{(S)}$.
% $$I
% c^{(S)}_{i,j} = \frac{1}{1 + \sqrt{(\text{MLP}(\mathbf{x}_i) - \text{MLP}(\mathbf{y}_j))^\mathsf{T}(\text{MLP}(\mathbf{x}_i) - \text{MLP}(\mathbf{y}_j))}},
% $$
% which normalizes the euclidean distance from $[0, +\infty)$ to $(0, 1]$ by the formula $\frac{1}{1+d}$.
\subsubsection{Semantic Interaction Encoder.} \label{subsubsec: sentence interaction}
To achieve fine-grained extraction of semantic matching information between legal cases, our first step is to model the interaction between each cross-case sentence pair to capture their matching behavior.
To be precise, in the legal case pair $(X, Y)$, we begin by using a softmax function to calculate the semantic interaction weight $\alpha_{i,j}^{(S)}$  (resp. $\beta_{i,j}^{(S)}$) for each sentence $\textbf{x}_i$ (resp. $\textbf{y}_j$) in response to the corresponding row $c_{i, :}^{(S)} $ (resp. column vector $c_{:, j}^{(S)}$) in the semantic correlation matrix $\mathbf{C}^{(S)}$ about the other legal case, i.e., 
$\alpha_{i,:}^{(S)} = \text{softmax}\left(c_{i, :}^{(S)}\right)$ and $\beta_{:,j}^{(S)} = \text{softmax}\left(c_{:, j}^{(S)}\right)$.
To show this in more detail, we provide the following element-wise expansion formula,
\begin{equation} \label{eq: Semantic Interaction Attention}
\alpha_{i,j}^{(S)} = \frac{\exp(c_{i,j}^{(S)})}
{\sum_{j=1}^{n_y} \exp(c_{i, j}^{(S)})}; \ \ 
\beta_{i,j}^{(S)} = \frac{\exp(c_{i,j}^{(S)})}
{\sum_{i=1}^{n_x} \exp(c_{i, j}^{(S)})}, \ \ 
\end{equation}
% where $\alpha_{i,j}^{(S)}$ represents the weight score of $\textbf{y}_j$ relative to $\textbf{x}_i$, which is normalized by the row vector $c_{i, :}^{(S)}$, and $\beta_{i,j}^{(S)}$ denotes the score of $\textbf{y}_j$ with respect to $\textbf{x}_i$ based on the column vector $c_{:, j}^{(S)}$.
Then, we weighted aggregate all sentences in another legal case to update the new representation after semantic interaction, that is,
\[
\mathbf{x}_i^{(S)} = \mathbf{x}_i \oplus \sum_{j=1}^{n_y}\alpha_{i, j}^{(S)}\mathbf{y}_j;
\ \ 
\mathbf{y}_j^{(S)} = \mathbf{y}_j \oplus \sum_{i=1}^{n_x}\beta_{i, j}^{(S)}\mathbf{x}_i,
\]
where $\mathbf{x}_i^{(S)}, \mathbf{y}_j^{(S)} \in \mathbb{R}^{2d_b}$ denote the semantic interaction sentence representations and $\oplus$ denotes the concatenation operator.

Then, referring to the practice of Shao et al~\cite{shao2020Bert_PLI}, we also leverage a bidirectional GRU (Bi-GRU) model to further aggregate contextual information of each case, i.e., 
\begin{equation} \label{eq: Semantic RNN}
\begin{split}
& \mathbf{H}_{X}^{(S)} = [\mathbf{h}_{x, 1}^{(S)}, \cdots, \mathbf{h}_{x, n_x}^{(S)}] = \text{Bi-GRU}([\mathbf{x}_1^{(S)}, \mathbf{x}_2^{(S)}, \cdots, \mathbf{x}_{n_x}^{(S)}]) \\ 
& \mathbf{H}_{Y}^{(S)} = [\mathbf{h}_{y, 1}^{(S)}, \cdots, \mathbf{h}_{y, n_y}^{(S)}] = \text{Bi-GRU}([\mathbf{y}_1^{(S)}, \mathbf{y}_2^{(S)}, \cdots, \mathbf{y}_{n_y}^{(S)}]),
\end{split}
\end{equation}
where $\mathbf{h}_{x, i}^{(S)}, \mathbf{h}_{y, j}^{(S)} \in \mathbb{R}^{d_s}$ is the hidden state of input sentence embeddings $\mathbf{x}_i^{(S)}$ and $\mathbf{y}_j^{(S)}$, and $d_s$ denotes the dimension of legal cases' semantic interaction representations.
Finally, we use the element maximum pooling operator to obtain the semantic interaction representation of each legal case, that is,
\begin{equation} \label{eq: Semantic Representation}
\mathbf{X}^{(S)} = \text{Max-pooing}(\mathbf{H}_{X}^{(S)});
\ \ 
\mathbf{Y}^{(S)} = \text{Max-pooing}(\mathbf{H}_{Y}^{(S)}),
\end{equation}
where $\mathbf{X}^{(S)}, \mathbf{Y}^{(S)} \in \mathbb{R}^{d_s}$ separately denote the semantic interaction representation of cases $X$ and $Y$.

\subsection{Legal Interaction Module}
To capture the legal interaction information $\langle \mathbf{X}_L, \mathbf{Y}_L \rangle$ of a legal case pair, the strategy of LIM involves two key steps. First, we capture the legal-rational information of each case through the law article prediction sub-task (cf. Sec.~\ref{subsubsec: article prediction subtask}).
Next, with the assistance of the law-distribution-aware correlation matrix (cf. Sec.~\ref{subsubsec: law correlation matrix}), we use the proposed article-intervened interaction encoder (cf. Sec.~\ref{subsubsec: law interaction encoder}) to model the legal interaction between cross-case sentence pairs.
For ease of understanding, we have shown the core of the legal interaction module in Fig.~\ref{fig: Law Interaction}.

\subsubsection{Article Prediction Sub-task.}\label{subsubsec: article prediction subtask}
Since a legal case may be associated with multiple law articles, each indicating different corresponding jurisprudential attributes, a multi-label classification approach is adopted for the article prediction sub-task, in contrast to the single-label classification method more commonly utilized in LJP. 
This enables us to capture a comprehensive range of jurisprudence-related information in a legal case.

Take the case $X$ as an example, we employ a Bi-GRU to capture contextual information for each sentence $x_i$, that is,
\[
[\mathbf{h}_1, \cdots, \mathbf{h}_{n_x}] = \text{Bi-GRU}([\mathbf{x}_1, \cdots, \mathbf{x}_{n_x}]),
\]
where $\mathbf{h}_i \in \mathbb{R}^{d_b}$ denotes the corresponding hidden state of the input sentence embedding $\mathbf{x}_i$.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/Law_Interaction_Module_crop.pdf}
\caption{
Body Operations of the Legal Interaction Module.
It cleverly exploits the intermediate representations of the attention mechanism of the law prediction sub-task.
It not only takes the inner value vectors to compute the legal representations but also innovatively regards the attention score vectors of law prediction as the law article distribution for computing the legal-rational correlation between sentences across cases.
}
\label{fig: Law Interaction}
\end{figure*}

\noindent {\bf Article-aware Attention.} 
Then, to capture the legal-rational information of the legal case $X$ related to a specific law article $L_k$, we propose an article-aware attention mechanism. 
Our article-aware attention mechanism borrows from the self-attention mechanism of Transformer~\cite{vaswani2017Transformer} with three parts: query, key, and value.
The difference is that in LCM-LAI, we define a memory vector for each law article and the query of our attention mechanism is from these memory vectors instead of from the input sentence embeddings themselves.
That is, $\mathbf{q}_k= W_q\mathbf{m}_k$, $\mathbf{k}_i = W_k\mathbf{h}_i$, $\mathbf{v}_i = W_v\mathbf{h}_i$,
where $\textbf{m}_k \in \mathbb{R}^{d_b}$ is the memory vector associated to law article $L_k$ and
$W_k, W_q, W_v \in \mathbb{R}^{d_b \times d_h}$ are the trainable weighted parameters.
For simplicity, we denote the set of these memory vectors as $\mathcal{M} = \{ \mathbf{m}_1, \ldots, \mathbf{m}_{n_L}\}$.
% {\color{red} and $\mathcal{M} \in \mathbb{R}^{n_L \times d_b}$.}
Then, based on the query $\mathbf{q}_k$ and key $\mathbf{k}_i$, we compute the correlation score $\lambda_{i, k}$ between sentence $x_i$ and law article $L_k$, i.e.,
\begin{equation} \label{eq: sub_attention}
\lambda_{i, k} = \mathbf{q}_k^{\intercal} \mathbf{k}_i.
\end{equation}
Further, the output of our article-aware attention mechanism is calculated as
\begin{equation} \label{eq: sub_attention_sum}
\mathbf{X}_k = \sum_{i=1}^{n_x}\gamma_{i,k}\mathbf{v}_i,
\end{equation}
where $\gamma_{i, k} = \frac{\exp(\lambda_{i, k})}{\sum_{i=1}^{n_x} \exp(\lambda_{i, k})}$ denotes the normalized attention score,
$\mathbf{X}_k \in \mathbb{R}^{d_h}$ represents the legal-rational representation of legal case $X$ related to law article $L_k$.
For convenience narration, we call $\mathbf{X}_k$ as the $L_k$-rational representation of case $X$.
Intuitively, on the one hand, our attention mechanism captures the most relevant legal information for each law $L_k$ in case $X$, which is conducive to the article prediction sub-task.
On the other hand, we generate an interesting intermediate variable $\lambda_{i, k}$, which evaluates the legal-rational correlation of each sentence $x_i$ concerning the corresponding law $L_k$, which provides a basis for our subsequent calculation of the legal-rational correlation between sentences across cases from the perspective of each law article.

\noindent {\bf Article based Classifier.} 
Since the focus of our work is not to solve the multi-label classification, we simply treat the article prediction sub-task as multiple binary classifications.
To predict whether a law article is suitable for a case, we first compute the prototype of each law article.
Formally, we define the prototype of law article $L_k$ as $\mathbf{Proto}_k = \text{MLP}(\mathbf{L}_k) \in \mathbb{R}^{d_h}$.
Then, we use the cosine distance-based classifier to compute the probability that law article $L_k$ applies to case $X$, i.e.,
\begin{equation} \label{eq: article prediction}
\text{P}(L_k|X) = \sigma \left( \cos \left( \mathbf{X}_k, \mathbf{Proto}_k \right) \right)= \text{Sigmoid}\left(\frac{\mathbf{X}_k^\mathsf{T} \mathbf{Proto}_k}
{ |\mathbf{X}_k| \cdot |\mathbf{Proto}_k|}\right).
\end{equation}
In this way, we get the set of predicted law articles for the legal case $X$ by the following formula,
\[
\hat{\mathcal{L}}_X = \{ L_k | \text{P}(L_k|X) > 0.5, L_k \in \mathcal{L}\}.
\]

\subsubsection{Legal Correlation Matrix.} \label{subsubsec: law correlation matrix}
To measure the legal-rational correlation between across-case sentences, a straightforward way is directly computing the cosine similarity based on their corresponding value vectors as Eq.~\ref{eq: semantic correlation matrix} does.
However, such an absolute distance-based measure is still semantic and cannot comprehensively evaluate the legal-rational relevance from the perspective of multiple legal attributes.
Fortunately, our article-aware attention mechanism has computed the legal-rational correlation between sentences and law articles, i.e., 
$\lambda_{i, k}$ in Eq.~\ref{eq: sub_attention}.
Combining with the inspiration from Yurochkin et al~\cite{yurochkin2019topicdistribution}, who use the distribution of topic distribution to evaluate the relevance between documents, we consider evaluating the legal-rational correlation between across-case sentences from the perspective of law distribution.
% which inspires us to evaluate the legal-rational correlation between across-case sentences from the perspective of law distribution.
Denoting the matrix with entries $\lambda_{i, k}$ as $\Lambda$,
the column vector $\lambda_{:, k}$ of $\Lambda$ can be used to capture the $L_k$-rational representation as it records which sentences are more related to the corresponding law article $L_k$ in legal-ration (cf. Eq.~\ref{eq: sub_attention}).
Meanwhile, we observe that the row vector $\lambda_{i, :}$ can also reflect the legal-rational correlation between the $i$-th sentence and all law articles, which can be regarded as the law distribution vector of the sentence, as depicted in Fig.~\ref{fig: Law Interaction}.
In what follows we simply write $\lambda_{i, :}$ as $\lambda_{i}$ when no confusion arises.
To compute the element $c_{i,j}^{(L)}$ of legal correlation matrix $\mathbf{C}^{(L)} \in \mathbb{R}^{n_x \times n_y}$, we compute the cosine similarity between the law distribution vectors of $x_i$ and $y_j$, that is,
\[
c_{i,j}^{(L)} = \cos(\mathrm{\lambda}_{i}^{X}, \mathbf{\lambda}_{j}^{Y}) = 
 \frac{{\mathbf{\lambda}_{i}^{X}}^\mathsf{T} \mathbf{\lambda}_{j}^{Y}}
 { |\mathbf{\lambda}_{i}^{X}| \cdot |\mathbf{\lambda}_{j}^{Y}|},
\]
where $\mathrm{\lambda}_{i}^{X}, \mathrm{\lambda}_{j}^{Y} \in \mathbb{R}^{n_L}$ separately denote the law distribution vectors of the corresponding sentence $x_i$ in $\Lambda^{X} $ and $y_j$ in $\Lambda^{Y}$.
Such a distribution-based similarity can effectively measure the law-rational correlation because the high-similarity sentence pair must have almost similar legal attributes (i.e., similar relation to all law articles) instead of simply having a small distance in the semantic space.

\subsubsection{Article-intervened Interaction Encoder.}\label{subsubsec: law interaction encoder}
To extract the legal-rational matching information between pairwise legal cases, according to the process of Sec.~ \ref{subsubsec: sentence interaction}, we first get the weight matrices of $\alpha_{i, j}^{(L)}$ and $\beta_{i, j}^{(L)}$ referring to the Eq.~\ref{eq: Semantic Interaction Attention}.
As the value vectors in the article-aware attention module (i.e., $\textbf{v}_i$ in Eq.~\ref{eq: sub_attention_sum}) are closely associated with the article prediction sub-task, we leverage them to obtain the legal interaction representation of each sentence (cf. Fig.~\ref{fig: Law Interaction}), that is,
\begin{equation} \label{eq: legal_inter_rep}
\mathbf{x}_i^{(L)} = \mathbf{v}_i^X \oplus \sum_{j=1}^{n_y}\alpha_{i, j}^{(L)}\mathbf{v}_j^Y;
\ \ 
\mathbf{y}_j^{(L)} = \mathbf{v}_j^Y \oplus \sum_{i=1}^{n_x}\beta_{i, j}^{(L)}\mathbf{v}_i^X,
\end{equation}
where $\mathbf{v}_i^X, \mathbf{v}_j^Y \in \mathbb{R}^{d_h}$ denote the value vectors of the corresponding sentences $x_i$ and $y_j$, 
and $\mathbf{x}_i^{(L)}, \mathbf{y}_j^{(L)} \in \mathbb{R}^{2d_h}$ denote the corresponding legal interaction representations.
Following similar calculation of Eqs.~\ref{eq: Semantic RNN} and~\ref{eq: Semantic Representation}, we get the legal hidden states (i.e., $\textbf{H}_{X}^{(L)} = [\mathbf{h}_{x,1}^{(L)}, \cdots, \mathbf{h}_{x,n_x}^{(L)}]$ and $\textbf{H}_{Y}^{(L)} = [\mathbf{h}_{y,1}^{(L)}, \cdots, \mathbf{h}_{y,n_y}^{(L)}]$) and the legal interaction representations (i.e., $\mathbf{X}^{(L)}$ and $\mathbf{Y}^{(L)}$) for pairwise legal cases.
We use $d_l$ to represent the dimension of the legal interaction representations, i.e., $\mathbf{X}^{(L)}, \mathbf{Y}^{(L)} \in \mathbb{R}^{d_l}$.

Besides, to avoid the interference caused by similar background statements (i.e., sentences with very uniform law distribution), we propose an \textbf{a}rticle-\textbf{i}ntervened \textbf{a}ttention (AIA) mechanism to focus on the interaction representation of legal-rational prominent sentences. 
Take the specific legal case $X$ as an example, where we first use a mean operation to aggregate the predicted applicable law article to a context vector, i.e.,
\[
\Phi_{X} = \frac{1}{|\hat{\mathcal{L}}_{X}|} \sum_{L_k\in \hat{\mathcal{L}}_X} \mathbf{L}_k,
\]
where $\Phi_{X} \in \mathbb{R}^{d_b}$ denotes the context vector and $|\hat{\mathcal{L}}_{X}|$ is the number of predicted law article set in the article prediction subtask.
Then, we first calculate the legal-ration-related attention score of each sentence according to the context vector, that is,
\[
\psi_i = \frac{\exp\left((\mathbf{W}_h\mathbf{h}_{x,i}^{(L)})^\mathsf{T}(\mathbf{W}_{\Phi}\Phi_{X})\right)}{\sum_{i=1}^{n_x}\exp\left((\mathbf{W}_h\mathbf{h}_{x,i}^{(L)})^\mathsf{T}(\mathbf{W}_{\Phi}\Phi_{X})\right)},
\]
where $\mathbf{W}_h \in \mathbb{R}^{d_h \times d_l} $ and $\mathbf{W}_{\Phi} \in \mathbb{R}^{d_h \times d_b}$ denote the trainable parameter matrices.
Thus, we attentively aggregate the hidden states into a novel representation, i.e.,
\[
\mathbf{X}^{(A)} = \sum_{i=1}^{n_x}\psi_i\mathbf{h}_{i}^{(L)},
\]
where $\mathbf{X}^{(A)} \in \mathbb{R}^{d_l}$ and we call it the article-intervened representation of case $X$.

\subsection{Matching Prediction}
When we predict the final matching results, we concatenate the semantic interaction representation, legal interaction representation, and article-intervened representation as the final representation, that is,
\[
\mathbf{X}_f = \mathbf{X}^{(S)} \oplus \mathbf{X}^{(L)} \oplus \mathbf{X}^{(A)};
\ \ 
\mathbf{Y}_f = \mathbf{Y}^{(S)} \oplus \mathbf{Y}^{(L)} \oplus \mathbf{Y}^{(A)},
\]
where $\mathbf{X}_f, \mathbf{Y}_f \in \mathbb{R}^{d_s + 2d_l}$ denote the final representations of legal cases $X$ and $Y$.

For the LCM task treated as a classification task, we adopt the classification object function of Sentence-BERT~\cite{reimers2019sentence-BERT} to obtain matching results.
Specifically, we concatenate the case embedding $\mathbf{X}_f$ and $\mathbf{Y}_f$ with their element-wise difference $|\mathbf{X}_f - \mathbf{Y}_f|$ and put them into a linear classifier:
\[ 
\hat{\mathbf{Z}}(X, Y) = \text{softmax}\left(\mathbf{W}_p\left(\mathbf{X}_f \oplus \mathbf{Y}_f \oplus |\mathbf{X}_f - \mathbf{Y}_f|\right)\right),
\]
where $\mathbf{W}_p \in \mathbb{R}^{3(d_s + 2d_l) \times |Z|}$ is a trainable parameter matrix, $\hat{\textbf{Z}}(X, Y) \in \mathbb{R}^{|Z|}$ is the predicted matching vector of the legal case pair $(X, Y)$, and $|Z|$ is the number of matching labels.

As the LCR task, we treat it as a rank task and use the cosine distance to measure the matching score of pairwise legal cases:
\[ 
\hat{S}(X, Y) = \frac{\mathbf{X}_f^\mathsf{T} \mathbf{Y}_f}
{|\mathbf{X}_f| \cdot |\mathbf{Y}_f|}.
\]

\subsection{Loss Function}
We treat the article prediction, LCM, and LCR tasks as different basic deep learning tasks, which also face some additional challenges for model training.
In this subsection, we introduce our loss function selection and why we choose them.

\noindent {\bf Article Prediction Loss.}
When treating the article prediction sub-task as a multi-label classification, it faces the imbalance problem that the number of negative samples is far greater than that of positive ones for most law articles, since common charges constitute the vast majority of legal cases.
Such a challenge leads to the non-convergence of this sub-task when we apply the traditional binary cross-entropy loss function for training, which further affects the performances of final LCR \& LCM tasks.
Since the existing works~\cite{cai2024ner,tong2024legal, chen2024knowledge} usually adopt the ZLPR loss proposed by Su et al.~\cite{su2022ZLPR_loss} as the objective function when solving similar challenge tasks, we choose it as our loss function.
Take the specific case $X$ as an example, the ZLPR loss first computes the matching vector $\mathbf{S}_{X} \in \mathbb{R}^{n_L}$ about all classes, that is,
\[
\mathbf{S}_{X} = \tau_{a} \cdot \left[\cos(\mathbf{X}_1, \mathbf{Proto}_1), \ldots, \cos(\mathbf{X}_{n_L}, \mathbf{Proto}_{n_L})\right],
\]
where $\tau_{a}$ is a temperature coefficient. Then, according to the article ground-truth vector $\mathbf{Z}_{L}^{X} \in \mathbb{R}^{n_L}$, the ZLPR loss uses a rank-based function to constrain the score of the positive label to be higher than the score of the negative label. The specific formula is as follows,
\[
\mathscr{L}_{a}^{X} = \log\left(1 + \mathbf{Z}_{L}^{X} \odot e^{-\mathbf{S}_{X}}\right) + \log\left(1 + \left(1-\mathbf{Z}_{L}^{X}\right) \odot e^{\mathbf{S}_{X}}\right),
\]
where $\odot$ denotes the inner product operator.
Different from the BCE is only focused on the binary relevance of every single label, 
the ZLPR loss additionally considers the correlation between labels.
This makes it more comprehensive and robust than the former.

\noindent {\bf Loss for LCR.} 
When we solve the LCR task as a rank task, to fully use the fine-grained match-level labels,
referring to the existing works~\cite{gong2023transferable,liu2022ynu},
we choose the CoSENT loss~\cite{huang2024cosent} as the objective function to optimize the cosine similarity between legal cases.
The main idea of CoSENT loss is to constrain that the samples with the higher-level label must get higher scores than those with a lower-level label. Here, we give the specific formula,
\[
\mathscr{L}_{m} = \log\left(1+ \sum_{\text{sim}(X_{i}, Y_{j}) \textgreater \text{sim}(X_{m}, Y_{n})} e^{\tau_{m}(\hat{S}(X_{i}, Y_{j})- \hat{S}(X_{m}, Y_{n}))}\right),
\]
where $\tau_{m}$ is a temperature coefficient.
Note that $\text{sim}(X_{i}, Y_{j}) \textgreater \text{sim}(X_{m}, Y_{n})$ means the legal case pair $(X_i, Y_j)$ has the higher match-level than the legal case pair $(X_m,Y_n)$.
Thus, compared with the typical triplet loss~\cite{chechik2010TripletLoss} whose contrastive form targets an anchor sample, CoSENT loss has a more general contrastive form.

\noindent {\bf Loss for LCM.} As for the LCM task, we take it as a typical classification task and compute the cross-entropy (CE) loss as its objective function, that is,
\[
\mathscr{L}_{m} = -\sum_{i=1}^{|Z|} {z}_{i}\log({\hat{z}_{i}}),
\]
where $\hat{z}_{i}$ is the $i$-th element of the predict vector $\hat{\textbf{Z}}(X, Y)$ and $\textbf{Z}(X, Y) = [z_1, \ldots, z_{|Z|}]$ denotes the one-hot ground-truth matching vector of legal case pair $(X, Y)$.

In summary, the overall loss for full training is the sum of the subtask and the main task, that is,
\[
\mathscr{L} = \mathscr{L}_{a} + \mathscr{L}_{m}.
\]

