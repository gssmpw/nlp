\section{Related Work}
\label{sec:related_work}
Our work solves the LCR \& LCM tasks through a dependent multi-task framework.
Therefore, this section introduces related research from these two aspects.

% {\color{blue}
% \subsection{Framework for Document retrieval}

% }

\subsection{Legal Case Retrieval and Matching}
Over the past few decades, numerous retrieval methods have been proposed, particularly for ad-hoc text retrieval.
As technology continues to develop, related methods have transitioned from the traditional bag-of-words (BOW) ones, which rely on item-matching techniques such as VSM**Santos et al., "Modeling Semantic Compositionality with interactions between the constituents of a word"**,** Robertson, S. G., & Zaragoza, M. "The probabilistic relevance language model: Retrospect and Prospects"**, and **Cormack et al., "Searching with Transformation-Based Weighted Language Models"**, to machine learning methods that utilize artificial features, such as LTR**Joachims, T., Granka, N., & Pan, B. "Accurately interpreting clickthrough data as implicit feedback"**.
Most recently, data-driven deep learning methods, such as DSSM**Salakhutdinov et al., "Restructured semantic indexing for improved generalized search"**, MatchPyramid**Pang et al., "Deep learning of relevance by neuron-only neural networks with pairwise interactions and hierarchical semantics"**, Match-SRNN**Zhang et al., "An End-to-End Deep Learning Architecture for Multi-Turn Response Selection in Retrieval-Based Conversation Systems"** and others, have emerged as a leading approach.
These studies have also demonstrated remarkable success in practical applications, including website search engines, recommendation systems, etc.

However, legal case retrieval and matching, being the critical applications of the retrieval model in the legal field, pose several unique challenges in comparison to general ad-hoc text retrieval. These challenges include specialized definitions of relevance in law**Stewart et al., "An Information Theoretic Approach to Semantic Retrieval for Legal Documents"**, dealing with extremely lengthy documents, and handling highly professional expression**Kessler et al., "Retrieval and classification of legal text using machine learning and deep learning approaches"**.
Consequently, in earlier times, certain solutions that heavily relied on expert knowledge were proposed to evaluate the legal relevance between two legal documents, such as ontological frameworks**Alvaro et al., "Ontologies for Legal Texts: A Survey"** and legal issue decomposition**Gordon et al., "Decomposing legal cases into constituent sub-issues"**.
With the advancement of NLP technology, particularly since the introduction of pre-training language models (PLMs) like BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, researchers try to employ such data-driven deep learning methods to solve LCM \& LCR tasks.
Unfortunately, because the length of legal documents far exceeds the input length limit of PLMs like BERT, the straightforward truncation strategy adopted by popular passage retrieval methods (e.g., DPR**Karpukhin et al., "DPR: Pre-training contextualized embeddings for natural language tasks and applications"**, ColBERT**Peng et al., "ColBERT: Efficient and effective late interaction in transformers for dense passage retrievers"** etc.) in solving LCM \& LCR tasks will lose a lot of the case information, resulting in poor performance.
Therefore, when dealing with the LCR \& LCM tasks, existing approaches typically segment legal documents into paragraphs or sentences and capture the interaction at the paragraph or sentence level between cases to assess their relevance. 
In this regard, one of the most typical works is proposed by Shao et al.**Shao et al., "BERT-Based Retrieval Model for Legal Cases"**, which employs BERT to capture the paragraph-level semantic interaction information, which overcomes the challenge of handling lengthy documents in the LCR \& LCM tasks.
However, their work only assesses the relevance of legal cases at a semantic level and makes some oversights due to neglecting the legal correlation between them.
To address this issue and capture the legal relevance between cases, some works have begun to introduce additional expertise to assist with the task of LCR \& LCM.
Hu et al.**Hu et al., "Strengthening BERT-Based Retrieval Model for Legal Cases"** strengthen the retrieval performance of Shao et al.'s model in criminal cases by appropriately labeling and encoding particular legal entities associated with the PRCâ€™s Criminal Law.
Yao et al.**Yao et al., "Effective Enhancement of Legally-Relevant Statements for Fact Description"** effectively enhance the legally-relevant statements of the fact description by defining and labeling a vast array of legal events, predicting their corresponding labels, and subsequently supplementing each event with additional embeddings.
Yu et al.**Yu et al., "Improving Legal Judgment Prediction through Labeling and Alignment"** advance the performance of the LCM task by not only labeling the legal rationale statement of each case but also identifying the alignment relationship between the rationales in each pairwise case.
Besides, they propose an explainable model that accurately captures the legal relevance between cases by predicting the labels as mentioned above.
To the best of our knowledge, Sun et al.**Sun et al., "Improving Legal Judgment Prediction through Causal Learning"** is the first to utilize the law articles as a supportive tool for the LCM task.
Unlike previous works that heavily relied on expert knowledge annotation, they propose a causal learning approach to boost the performance of the LCM task. 
This approach involves using the applicable law as an intermediate variable, enabling the extraction of legal-related features of cases by reconstructing case representations.
However, their assumption that utilizes ground-truth applicable law articles as input limits the practicality of their work. 
As the ultimate objective of the LCM task is to achieve judicial decision-making (e.g., applicable law articles, charges, and terms of penalty), applicable law may not always be readily available.

Unlike them, our proposed LCM-LAI introduces an end-to-end dependent multi-task framework that effectively captures legal relevance between cases.
Our framework indirectly incorporates legal knowledge from law articles into the core LCR \& LCM task by the subtask of law article prediction, which not only guarantees consistency between training and practical application but also reduces reliance on experts.


\subsection{Multi-task Learning in Legal Domain}
Multi-task learning (MTL) aims to improve the performance of multiple relevant tasks simultaneously by exploiting their internal relations.
In NLP tasks, knowledge transfer between relevant subtasks is typically accomplished by sharing representations**Ruder et al., "An Overview of Multi-Task Learning in Deep Neural Networks"**, or parameters**Guo et al., "Multi-task learning for natural language processing: A survey"**, as demonstrated by much of the current research.
For example, Dong et al.**Dong et al., "Improved Machine Translation with Enhanced Encoder Representations"** demonstrate the effectiveness of sharing encoders to enhance one-to-many neural machine translations.
Firat et al.**Firat et al., "Multi-way and Multilingual Machine Translation"** improve multi-way and multilingual machine translation through a sharing attention mechanism.
Guo et al.**Guo et al., "Multi-Task Learning Framework for Abstractive Summarization"** propose a multi-task learning framework to incorporate auxiliary tasks of question and entailment generation, thereby enhancing the capacity for abstractive summarization.
In the legal domain, much of the current work centers around the design of dependencies between sub-tasks, with a primary emphasis on legal judgment prediction (LJP) tasks of single legal case**Zhong et al., "Directed Acyclic Graph-Based Topological Multi-Task Learning"**.
Zhong et al.**Zhong et al., "Backward Dependency-Aware Multi-Task Learning for Legal Judgment Prediction"** first explore the three subtasks of LJP and propose a directed acyclic graph-based topological multi-task learning framework to model their dependency.
Subsequently, Yang et al.**Yang et al., "Dependent Multi-Task Learning with Backward Dependencies"** augment the framework as mentioned above by incorporating the backward dependency between these subtasks, further improving its efficacy.
Furthermore, Yue et al.**Yue et al., "Disentangling Input Representations for Multi-Task Learning in Legal Judgment Prediction"** improve the performance of multi-task learning on JLP tasks by disentangling the input fact description into fine-grained representations associated with each subtask, thereby enhancing the performance of their proposed model.

However, the above multi-task learning frameworks are all designed heuristically according to the actual judging process.
Different from them, we combine mediation analyses and naive Bayesian theory to derive and design our dependent multi-task learning framework, which achieves significant performance improvement in our experiments.
Meanwhile, to the best of our knowledge, we are the first to use the multi-task framework to improve the LCR \& LCM main task, which involves pairwise legal cases.