\section{Methodology} \label{sec:methodology}
\begin{table*}[t]
% \color{blue}
\caption{Main mathematical notations.}\label{tab:Notation}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c}
\toprule 
Notation & Description\\
% \cmidrule(lr){1-2} \cmidrule(lr){3-4}
\midrule 

$\mathcal{L}\triangleq\{L_1, \ldots, L_{n_L}\}$ &
the set of all involved law articles \\

$X = \{x_1, \ldots, x_{n_x}\}$ and $Y = \{y_1, \ldots, y_{n_y}\} $ & 
the sentences sequence of query case $X$ and candidate case $Y$ \\

$X_L \subseteq X$ and $Y_L \subseteq Y$ & 
the set of legal-rational relevant sentences in cases $X$ and $Y$ \\

$\mathcal{L}_X, \mathcal{L}_Y \subseteq \mathcal{L} $ & 
the case $X$ and $Y$'s corresponding applicable law article set\\

$R = \langle X, Y \rangle$ & 
the simplified notation for a case pair containing $X$ and $Y$ \\

$R_{L} = \langle X_L, X_L \rangle $ &
the simplified notation for the set pair containing $X_L$ and $Y_L$ \\

$\mathcal{L}_{R} = \langle \mathcal{L}_X, \mathcal{L}_Y \rangle$ &
the simplified notation for a pair of applicable law article sets containing $\mathcal{L}_X$ and $\mathcal{L}_Y$ \\

$\mathbf{x}_i$, $\mathbf{y}_j$, and $\mathbf{L}_k$ & 
the encoded sentence embedding of sentence $x_i \in X $, $y_j \in Y$ and law article $L_k \in \mathcal{L}$ \\

$\textbf{Z}_{L}^{X} = [z_{1}^{X}, \ldots, z_{n_L}^{X}]$ and $ \textbf{Z}_{L}^{Y} = [z_{1}^{Y}, \ldots, z_{n_L}^{Y}]$ &
the law article label vector of case $X$ and $Y$, where $z_{k}^{*} \in \{0, 1\}$ \\

$\textbf{Z}(X, Y) = [z_1, \ldots, z_{|Z|}]$ & 
the one-hot ground-truth LCM vector of legal case pair $(X, Y)$\\

$S(X,Y) \in [0, 1]$ & 
the matching score between query $X$ and candidate $Y$\\

$\mathcal{M} = \{ \textbf{m}_1, \ldots, \textbf{m}_{n_L}\}$ & 
the set for memory vectors of law articles\\

$\mathbf{C}^{(S)}, \mathbf{C}^{(L)} \in \mathbb{R}^{n_x \times n_y}$ & 
the semantic and legal correlation matrix for each pairwise cross-case sentence \\

$\mathbf{X}^{(S)}, \mathbf{Y}^{(S)}$ &
the semantic interaction representations of case $X$ and case $Y$\\

$\mathbf{X}^{(L)}, \mathbf{Y}^{(L)}$ &
the legal interaction representations of case $X$ and case $Y$\\

$\mathbf{X}^{(A)}, \mathbf{Y}^{(A)}$ &
the article-intervened representations of case $X$ and case $Y$\\

$\mathbf{X}_f, \mathbf{Y}_f$ &
the final interaction representations of case $X$ and case $Y$\\

\bottomrule
\end{tabular}%
}
\end{table*}
In this section, we first formalize the LCR and LCM tasks. 
Subsequently, we employ the mediation variable analysis method and naive Bayes theory to analyze the problem, culminating in the framework of our LCM-LAI based on our analysis conclusion.

\subsection{Problem Formulation}
Here, we first show some notations in Tab.~\ref{tab:Notation} and then formulate the LCR and the LCM tasks in the following.

\noindent {\bf Legal Cases and Law Articles.} 
As our method tries to add the law articles as the external knowledge to improve the performance of LCR tasks, we first formalize the legal cases and law articles here.
In our work, we treat each legal case $X$ as a sequence of sentences, i.e., $X = \{ x_1, \ldots, x_{n_x}\}$, where $x_i$ denoted the $i$-th sentence of case $X$, and $n_x$ represents the number of sentences in the case $X$.
In civil law countries, legal cases are often analyzed and adjudicated according to statutory law (also known as written law).
We denote the statutory law as a set of law articles $\mathcal{L}\triangleq\{L_1, \ldots, L_{n_L}\}$, where $n_L$ is the article number of the full law article set. 
Since law articles' length is generally short, we represent each law article $L_i$ as a sentence.
Besides, for each legal case $X$, we also denote its cited law article set with $\mathcal{L}_{X}$, where $\mathcal{L}_{X} \subseteq \mathcal{L}$ and $\|\mathcal{L}_{X}\| > 0$.

\noindent {\bf Legal Case Retrieval.}
Formally, given a query case $X$, and a set of candidate cases $\mathcal{Y} = \{Y_1, \ldots, Y_n\}$, 
the goal of the LCR task is to identify the relevant and instructive cases $\mathcal{Y}^{*} = \{Y_i^{*} | Y_i^{*}\in \mathcal{Y} \land \mathit{related}(Y_i^{*}, X) \}$, where $\mathit{related}(Y_i^{*}, X)$ denotes that $Y_i^{*}$ is related to the given query case $X$.
To evaluate the relevance between the query and candidates, existing methods generally learn a matching model $ \mathrm{F}(X, Y_i) \rightarrow S_i $, where the higher output score $S_i$ indicates the stronger correlation between the query case $X$ and candidate case $Y_i$.

\noindent {\bf Legal Case Matching.}
As a critical step for the LCR task, we also formalize the LCM task here.
Given a set of labeled data tuples $\mathcal{D}\triangleq\{(X, Y, z)_{i}\}_{i=1}^{N}$, where $X$ and $Y$ denote two source legal cases, $z \in Z$ denotes the human-annotated matching label and $N$ denotes the tuple number in the set.
$Z$ could be the manually defined match levels.
For example, when $Z = \{0, 1, 2\}$, the values $0$, $1$, and $2$ correspond to a mismatch, a partial match, and a complete match, respectively.
The LCM task aims to learn a matching model $\mathrm{F}(X, Y) \rightarrow Z$ to predict the matching label $\hat{z} \in Z$ by inputting two legal cases $X$ and $Y$, which is trained based on the labeled tuples in dataset $\mathcal{D}$.

Since the forms are similar and for the convenience of subsequent discussion, we unify the goal of LCR \& LCM tasks, i.e., $\mathrm{F}(X, Y) \rightarrow Z$, where the only difference is that the label field $Z$ of LCR task is continuous while that of LCM task is discrete.

\begin{figure}[t]
\centering
\includegraphics[width=0.70\linewidth]{fig/problem_analyse.pdf}
\caption{The causal graph of two DMT frameworks. (a) The inconsistency between the training and reasoning stages leads to the train-test discrepancy. (b) The potential intermediate variable $R_L$ is introduced, making the training and reasoning process consistent.}
\label{fig: problem_analyse}
\end{figure}

\subsection{Problem Analysis}\label{sec:analysis}
According to the above definition, given a labeled sample $(X, Y, z)$, the objective of a desired case matching model is to maximize the prediction probability $\mathrm{P}(z|X, Y)$.
As mentioned in Sec.~\ref{sec: introduction}, relevant cases should be assessed from both semantic and jurisprudential relevance.
Thus, following the official guide document (referenced in Sec.~\ref{sec: introduction}), we introduce the applicable law articles of cases to assess the jurisprudential relevance between them.
Take $\mathcal{L}_X$ and $\mathcal{L}_Y$ as the applicable law articles of case $X$ and $Y$ respectively, 
a straightforward way to model LCR \& LCM tasks is to maximize the objective $\mathrm{P}(z|\mathcal{L}_X, \mathcal{L}_Y, X, Y)$, similar to the method employed by Sun et al.~\cite{sun2023law}.
However, such a model ignores the reality that in the actual judicial case hearing process, the judgment of the applicable law occurs after the retrieval of similar cases, making it difficult to apply such models in practice.
Fortunately, inspired by related work on legal judgment prediction~\cite{xu2020LADAN,LJP@TOIS2024_XN,zhong2018Topology,yue2021Neurjudge}, we find that the definition of applicable law articles serves as a high-dimensional summary of the legal-ration information contained in the fact description of the corresponding legal cases.
Thus, to capture the jurisprudential relevance between legal cases, our method treats the applicable law articles as the intermediate variables and forms the LCR \& LCM task as a dependent multi-task (DMT) learning problem.
From this perspective, the most direct modeling approach is to utilize the case facts to predict the corresponding applicable law articles (whose objectives are $\mathrm{P}(\mathcal{L}_X|X)$ and $\mathrm{P}(\mathcal{L}_Y|Y)$), and then use the case facts and the predicted applicable law articles to jointly predict the matching relationship between cases (whose objective is $\mathrm{P}(z|\mathcal{L}_X, \mathcal{L}_Y, X, Y)$).
Thus, such a model needs to maximize the following overall objective,
$$
\mathrm{P}(z, \mathcal{L}_X, \mathcal{L}_Y | X, Y ) = \mathrm{P}(\mathcal{L}_X|X) \cdot \mathrm{P}(\mathcal{L}_Y|Y) 
\cdot \mathrm{P}(z| \mathcal{L}_X, \mathcal{L}_Y,  X, Y),
$$
To simplify the following formulas, we simplify the case pair $\langle X, Y \rangle$\footnote{Note that in this paper, $<\cdot, \cdot>$ only represents the tuple relationship instead of any computational operation.} as $R$ 
and simplify the pair of the corresponding applicable law article set $\langle \mathcal{L}_X, \mathcal{L}_Y \rangle$ as $\mathcal{L}_R$.
Thus, we get a simplified DMT formula as follows
\footnote{Considering $\mathcal{L}_X$ and $\mathcal{L}_Y$ are two independent variables, and case $X$ and $Y$ are also independent, we get $\mathrm{P}(\mathcal{L}_X|X) \cdot \mathrm{P}(\mathcal{L}_Y|Y)=\mathrm{P}(\mathcal{L}_X, \mathcal{L}_Y|X, Y)=\mathrm{P}(\mathcal{L}_R|R)$.},
% we added some symbolic references, i.e., 
% $\langle X, Y \rangle \rightarrow R $, $\langle \mathcal{L}_x, \mathcal{L}_y \rangle \rightarrow \mathcal{L}_R$. Then, we get a simplified DMT formula\footnote{Consider $\mathcal{L}_x$ and $\mathcal{L}_y$ are two independent variables, and case $X$ and $Y$ are also independent.},
\begin{equation} \label{eq: original model}
    \mathrm{P}(z, \mathcal{L}_R|R) = \mathrm{P}(\mathcal{L}_R|R) \cdot \mathrm{P}(z|\mathcal{L}_R, R).
\end{equation}
However, there is a train-test discrepancy in Eq.~\ref{eq: original model} in the causal view. 
The causal graph of such a framework is shown in Fig~\ref{fig: problem_analyse}(a).
During the training stage (the left half of Fig.~\ref{fig: problem_analyse}(a)), we can use the gold label of applicable law articles $\mathcal{L}_R$ to pre-train the object $P(z|\mathcal{L}_R, R)$, while in the inference stage (the right half of Fig.~\ref{fig: problem_analyse}(a)), we can only use the predicted label $\hat{\mathcal{L}_R}$ to predict the matching result, as the query cases are typically pending adjudication (i.e., lacking explicitly applicable law articles).
Referring to previous works~\cite{chen2020Causal_1, chen2021Causal_2}, such a discrepancy would hurt the inference performance.

To overcome such a train-test discrepancy, we reanalyze the DMT learning in LCR \& LCM tasks. 
As mentioned in Sec. \ref{sec: introduction} and Fig.~\ref{fig: LCR_example}, for each legal case, there is always some legal-rational part of the fact description that corresponds to its applicable law articles. 
Designating this legal-rational portion as $R_L$, our focus lies in isolating this element within case fact descriptions and determining its relevance across cases.
Thus, we can update Eq.~\ref{eq: original model} as,
\begin{equation} \label{eq: new model}
P(z, R_L|R) = P(R_L|R) \cdot P(z|R_L, R).  
\end{equation}
However, it is not easy for a deep-learning model to separate the legal-rational part $R_L$ directly from the input $R$ due to the non-differentiability of the operation.
Inspired by previous works~\cite{xu2020LADAN, luo2017FLA}, the law article prediction task based on the attention mechanism can well extract the legal-rational part's vector representation in the vector space. 
Therefore, we tend to inject a term of legal prediction task $P(\mathcal{L}_R|R)$ into Eq.~\ref{eq: new model}.
In addition, it's worth noting that $R_L$ ideally has two particular properties, i.e., $R_L \in R$ and $\mathcal{L}_R$ is only relevant to $R_L$. 
Thus, we approximate two formulas by these two properties, i.e., $ P(\mathcal{L}_R| R_L, R) \geq P(\mathcal{L}_R|R) $ and $ P(\mathcal{L}_R|R_L) \approx \alpha$. 
In this way, we further deduce the formula,
\begin{equation} \label{eq: final model}
\begin{aligned}
P(z, R_L|R) &= P(R_L|R) \cdot P(z|R_L, R)\\
&=\frac{P(\mathcal{L}_R|R_L, R)}{P(\mathcal{L}_R|R_L)} \cdot P(z|R_L, R)\\
&\geq \frac{1}{\alpha} P(\mathcal{L}_R|R) \cdot P(z|R_L, R).
\end{aligned}
\end{equation}

So far, the two terms of Eq.~\ref{eq: final model} inspire us that the reasonable model design has two necessary components: the first term $P(\mathcal{L}_R|R)$ indicates the subtask of law article prediction, and the second term $P(z|R_L, R)$ states that the model needs to combine the original input $R$ and the vector representation $R_L$ of the legal-rational part to predict the results of LCR \& LCM tasks.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/framework_crop.pdf}
\caption{
Overview of our framework LCM-LAI. 
This framework operates by taking textual descriptions of legal cases and text definitions of law articles as inputs. 
On the one hand, it leverages the \textbf{Basic Interaction Module (BIM)} to extract the semantic interaction information between cases based on the semantic similarity between across-case sentences. 
On the other hand, it uses the \textbf{Legal Interaction Module (LIM)} to capture the legal interaction information from the perspective of the similarity of legal distribution, in which the \textbf{article prediction sub-task} is introduced to capture the legal distribution of each sentence.
Besides, LIM uses an \textbf{article-intervened attention (AIA)} mechanism to highlight the key jurisprudence-related parts of cases that rely on the predicted law articles.
Finally, LCM-LAI combines the semantic and legal interaction representations to compute the matching score.
% It leverages the Basic Interaction Module and Legal Interaction Module to extract the semantic interaction representation and legal interaction representation for a given pair of input cases. 
% These interaction representations are then combined to facilitate subsequent matching prediction.
% \markblue{[Add some detailed explanations]}
}
\label{fig: framework}
\end{figure*}

\subsection{Framework of LCM-LAI} \label{subsec: framework}
When we substitute the simplifications, i.e., $R \rightarrow \langle X, Y \rangle$, $R_L \rightarrow \langle X_L, Y_L \rangle$, and $\mathcal{L}_R \rightarrow \langle \mathcal{L}_X, \mathcal{L}_Y \rangle$ into Eq.~\ref{eq: final model}, then we get the following formula,
\begin{equation} \label{eq: model framework}
\mathrm{P}(z, X_L, Y_L | X, Y ) = \mathrm{P}(\mathcal{L}_X|X) \cdot \mathrm{P}(\mathcal{L}_Y|Y) 
\cdot \mathrm{P}(z| X, Y, X_L, Y_L).
\end{equation}
This formula inspires the framework of our LCM-LAI model, which includes: the inspiration of the term $\mathrm{P}(z|X, Y, X_L, Y_L)$ is to extract the features from both semantic (i.e., $X, Y$) and legal-rational (i.e., $X_L, Y_L$) levels to model the interaction between two cases. The inspiration of the term $\mathrm{P}(\mathcal{L}_X|X)$ and $\mathrm{P}(\mathcal{L}_Y|Y)$ is to use the applicable law article prediction sub-task to make the model learn how to extract the legal-rational features in the case.

Thus, the framework of our LCM-LAI is shown in Fig.~\ref{fig: framework}, which can be divided into four main modules, i.e., sentence embedding module, basic interaction module (BIM), legal interaction module (LIM), and matching module.
The sentence embedding module encodes the sentences of cases' fact descriptions into vector representations.
Subsequently, when capturing the interaction information between legal cases, our LCM-LAI employs two parallel interaction modules, namely the basic interaction module and the legal interaction module, to effectively model the paired terms $\langle X, Y \rangle$ and $\langle X_L, Y_L \rangle$ in Eq.~\ref{eq: model framework}.
On the one hand, the BIM computes the basic interaction representations $\langle \mathbf{X}, \mathbf{Y} \rangle$, which contain the general semantic matching information between two legal cases, just like the conventional text matching models work.
On the other hand, the LIM generates the legal interaction representations $\langle \mathbf{X}_L, \mathbf{Y}_L \rangle$, which capture not only the legal information of each case but also the matching information from a legal perspective.
Besides, to satisfy the formula terms $\mathrm{P}(\mathcal{L}_X|X)$ and $\mathrm{P}(\mathcal{L}_Y|Y)$, we also construct the law article prediction sub-task (i.e., the Article Prediction Sub-task in Fig.~\ref{fig: framework}) in the LIM, which also helps capture the legal interaction between cases effectively.
Finally, the $\langle \mathbf{X}, \mathbf{Y} \rangle$ and $\langle \mathbf{X}_L, \mathbf{Y}_L \rangle$ are fed into the subsequent matching module to predict the matching score or label of the subsequent LCR \& LCM tasks.

Note that the core of our framework is to learn how to capture key legal-rational features under the supervision of historical precedents' applicable law article labels during training. 
As for the inference stage, our framework only focuses on the legal-rational information extracted by the model and the legal interaction between cases instead of taking the prediction results of the applicable law as the evaluation basis.
Thus, such a framework does not contravene the judicial practice procedure of the civil law system, in which the judicial judgment conclusions should be made after the similar case retrieval.
