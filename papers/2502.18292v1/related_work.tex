\section{Related Work} \label{sec:related_work}
Our work solves the LCR \& LCM tasks through a dependent multi-task framework.
Therefore, this section introduces related research from these two aspects.

% {\color{blue}
% \subsection{Framework for Document retrieval}

% }

\subsection{Legal Case Retrieval and Matching}
Over the past few decades, numerous retrieval methods have been proposed, particularly for ad-hoc text retrieval.
As technology continues to develop, related methods have transitioned from the traditional bag-of-words (BOW) ones, which rely on item-matching techniques such as VSM~\cite{salton1988VSM}, BM25~\cite{robertson1994BM25}, and LMIR~\cite{song1999LMIR}, to machine learning methods that utilize artificial features, such as LTR~\cite{liu2009LTR}.
Most recently, data-driven deep learning methods, such as DSSM~\cite{hu2014DSSM}, MatchPyramid~\cite{pang2016MatchPyramid}, Match-SRNN~\cite{wan2016Match-SRNN} and others, have emerged as a leading approach.
These studies have also demonstrated remarkable success in practical applications, including website search engines, recommendation systems, etc.

However, legal case retrieval and matching, being the critical applications of the retrieval model in the legal field, pose several unique challenges in comparison to general ad-hoc text retrieval. These challenges include specialized definitions of relevance in law~\cite{van2017SpecialDefinition}, dealing with extremely lengthy documents, and handling highly professional expression~\cite{turtle1995Legalworld}.
Consequently, in earlier times, certain solutions that heavily relied on expert knowledge were proposed to evaluate the legal relevance between two legal documents, such as ontological frameworks~\cite{saravanan2009LCR_1} and legal issue decomposition~\cite{zeng2005LCR_2}.
With the advancement of NLP technology, particularly since the introduction of pre-training language models (PLMs) like BERT~\cite{devlin2018bert} was proposed, researchers try to employ such data-driven deep learning methods to solve LCM \& LCR tasks.
Unfortunately, because the length of legal documents far exceeds the input length limit of PLMs like BERT, the straightforward truncation strategy adopted by popular passage retrieval methods (e.g., DPR~\cite{karpukhin2020DPR}, ColBERT~\cite{khattab2020colbert}, etc.) in solving LCM \& LCR tasks will lose a lot of the case information, resulting in poor performance.
Therefore, when dealing with the LCR \& LCM tasks, existing approaches typically segment legal documents into paragraphs or sentences and capture the interaction at the paragraph or sentence level between cases to assess their relevance. 
In this regard, one of the most typical works is proposed by Shao et al.~\cite{shao2020Bert_PLI}, which employs BERT to capture the paragraph-level semantic interaction information, which overcomes the challenge of handling lengthy documents in the LCR \& LCM tasks.
However, their work only assesses the relevance of legal cases at a semantic level and makes some oversights due to neglecting the legal correlation between them.
To address this issue and capture the legal relevance between cases, some works have begun to introduce additional expertise to assist with the task of LCR \& LCM.
Hu et al.~\cite{hu2022Bert_LF} strengthen the retrieval performance of Shao et al.'s model in criminal cases by appropriately labeling and encoding particular legal entities associated with the PRCâ€™s Criminal Law.
Yao et al.~\cite{yao2022LEVEN} effectively enhance the legally-relevant statements of the fact description by defining and labeling a vast array of legal events, predicting their corresponding labels, and subsequently supplementing each event with additional embeddings.
Yu et al.~\cite{yu2022Explainable} advance the performance of the LCM task by not only labeling the legal rationale statement of each case but also identifying the alignment relationship between the rationales in each pairwise case.
Besides, they propose an explainable model that accurately captures the legal relevance between cases by predicting the labels as mentioned above.
To the best of our knowledge, Sun et al.~\cite{sun2023law} is the first to utilize the law articles as a supportive tool for the LCM task.
Unlike previous works that heavily relied on expert knowledge annotation, they propose a causal learning approach to boost the performance of the LCM task. 
This approach involves using the applicable law as an intermediate variable, enabling the extraction of legal-related features of cases by reconstructing case representations.
However, their assumption that utilizes ground-truth applicable law articles as input limits the practicality of their work. 
As the ultimate objective of the LCM task is to achieve judicial decision-making (e.g., applicable law articles, charges, and terms of penalty), applicable law may not always be readily available.

Unlike them, our proposed LCM-LAI introduces an end-to-end dependent multi-task framework that effectively captures legal relevance between cases.
Our framework indirectly incorporates legal knowledge from law articles into the core LCR \& LCM task by the subtask of law article prediction, which not only guarantees consistency between training and practical application but also reduces reliance on experts.


\subsection{Multi-task Learning in Legal Domain}
Multi-task learning (MTL) aims to improve the performance of multiple relevant tasks simultaneously by exploiting their internal relations.
In NLP tasks, knowledge transfer between relevant subtasks is typically accomplished by sharing representations~\cite{liu2015SharingRep} or parameters~\cite{liu2018SharingParameter}, as demonstrated by much of the current research.
For example, Dong et al.~\cite{dong2015MultiTranlate} demonstrate the effectiveness of sharing encoders to enhance one-to-many neural machine translations.
Firat et al.~\cite{firat2016multi} improve multi-way and multilingual machine translation through a sharing attention mechanism.
Guo et al.~\cite{guo2018soft} propose a multi-task learning framework to incorporate auxiliary tasks of question and entailment generation, thereby enhancing the capacity for abstractive summarization.
In the legal domain, much of the current work centers around the design of dependencies between sub-tasks, with a primary emphasis on legal judgment prediction (LJP) tasks of single legal case~\cite{zhong2018Topology,yang2019Bi-feedback,yue2021Neurjudge,lyu2022CEEN,zhaochun2023multi,zhaochun2023syllogistic,LJP@TOIS2024_XN}.
Zhong et al.~\cite{zhong2018Topology} first explore the three subtasks of LJP and propose a directed acyclic graph-based topological multi-task learning framework to model their dependency.
Subsequently, Yang et al.~\cite{yang2019Bi-feedback} augment the framework as mentioned above by incorporating the backward dependency between these subtasks, further improving its efficacy.
Furthermore, Yue et al.~\cite{yue2021Neurjudge} improve the performance of multi-task learning on JLP tasks by disentangling the input fact description into fine-grained representations associated with each subtask, thereby enhancing the performance of their proposed model.

However, the above multi-task learning frameworks are all designed heuristically according to the actual judging process.
Different from them, we combine mediation analyses and naive Bayesian theory to derive and design our dependent multi-task learning framework, which achieves significant performance improvement in our experiments.
Meanwhile, to the best of our knowledge, we are the first to use the multi-task framework to improve the LCR \& LCM main task, which involves pairwise legal cases.