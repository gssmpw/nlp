\section{Experiments} \label{sec:experiments}
In this section, we conduct experiments to demonstrate the effectiveness of LCM-LAI by answering the following research questions:

\noindent {\bf RQ1:} Can LCM-LAI outperform the existing state-of-the-art
baselines on LCR \& LCM tasks?

\noindent {\bf RQ2:} Is jurisprudential relevance essential for legal case retrieval and matching? Does the dependent multi-task learning framework with the law article prediction subtask improve the performance of LCM-LAI?

\noindent {\bf RQ3:} Can LCM-LAI meet the efficiency limitations of practical applications?

\noindent {\bf RQ4:} Whether LCM-LAI is explicable for the LCR \& LCM prediction results?

\begin{table}[t]
\centering
\caption{ Statistics of LeCaRD, LeCaRDv2, ELAM, and eCAIL datasets.}\label{tab: Data Statistics}
% \resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
\toprule
     & LeCaRd   & LeCaRDv2     & ELAM  & eCAIL\\ 
 \midrule
 Suitable for LCR           & \checkmark    & \checkmark    & $\times$  & $\times$ \\
 \# Query                   & $107$         & $800$         & $--$      & $--$ \\
 \# Candidates per query    & $30$         & $30$      & $--$      & $--$ \\
 \# Train Query             & $85$          & $640$         & $--$      & $--$ \\
 \# Valid Query             & $11$          & $80$          & $--$      & $--$ \\
 \# Test Query              & $11$          & $80$         & $--$      & $--$ \\
 % \# Law articles            & $92$          & $ $           & $--$      & $--$ \\
 \midrule
 Suitable for LCM           & \checkmark    & $\times$  & \checkmark    & \checkmark \\
 \# Case pairs              & $3,210$       & $--$      & $5,000$       & $7,500$ \\
 \# Label levels            & $4$           & $--$      & $3$           & $3$ \\
 \# Train pairs             & $2,582$       & $--$      & $4,000$       & $6,000$ \\
 \# Valid pairs             & $323$         & $--$      & $500$         & $750$ \\
 \# Test pairs              & $323$         & $--$      & $500$         & $750$ \\
 \midrule
 \# Law articles            & $54$          & $86$      & $42$          & $ 107 $ \\
 Avg. \# cited law articles per case     & $ 12.74 $   & $4.84$   & $5.04$   & $4.59$ \\
 Avg. \# sentences per case     & $97.86$   & $16.42$   & $16.29$   & $124.10$ \\
 \bottomrule
\end{tabular}
% }
\end{table}
\subsection{Experimental Settings}
\subsubsection{Datasets.} We validate our model on experiments of both LCR and LCM tasks. Here we introduce the four available datasets we used:
LeCaRD~\cite{ma2021LeCaRD}, LeCaRDv2, ELAM~\cite{yu2022Explainable}, and eCAIL~\cite{yu2022Explainable}.

\textbf{LeCaRD}\footnote{\url{https://github.com/myx666/LeCaRD}} is a legal case retrieval dataset that contains 107 query cases and 100 candidate cases per query selected from more than 43,000 criminal legal cases, which are published by the Supreme People’s Court of China.
As the top 30 candidate cases for each query are manually annotated with a $4$-level relevance (matching) label, this dataset is used for both LCR and LCM tasks.

\textbf{LeCaRDv2}\footnote{\url{https://github.com/THUIR/LeCaRDv2}} is an extension of the LeCaRD dataset.
Compared with LeCaRD, the data size of LeCaRDv2 is approximately $8$ times larger. 
Specifically, the LeCaRDv2 dataset comprises 800 query cases and 55,192 candidate cases extracted from 4.3 million criminal case documents. 
It gives a candidate set with $30$ legal cases for each query, which is suitable for evaluation on the LCR task.

\textbf{ELAM}\footnote{\url{https://github.com/ruc-wjyu/IOT-Match}} is annotated to solve the explainable legal case match task.
For each pair of legal cases, a $3$-level manually assigned matching label is given: $0$, $1$, and $2$ refer to a mismatch, a partial match, and a complete match, respectively.
In addition to the matching labels, fine-grain explainable labels are also provided in this dataset, such as the rationale labels of sentences, the alignments of rationales, and the literal explanations.
In the experiment, we only use the fact description of legal cases and the matching labels for training and evaluation.

\textbf{eCAIL} is extended from the CAIL 2021 dataset\footnote{The open sourced data for the Fact Prediction Track: \url{http://cail.cipsc.org.cn/}}, following the practice in~\cite{yu2022Explainable}. 
Each legal case is annotated in the original dataset with some tags about private lending.
Each case pair of the eCAIL dataset gets a $3$-level matching label based on the number of overlapping tags: overlapping $\textgreater 10$ and overlapping $\textless 1$ refer to matching and mismatching respectively, while other ranges indicate partial matching.
Similar to the ELAM dataset, we also only use matching labels for the training of LCM-LAI.

As for the selection of law articles, referring to previous works on the JLP task, we only keep the law articles applied to not less than $10$ corresponding case samples in the dataset.
Notice that, for LeCaRD, LeCaRDv2, and ELAM, we use the PRC Criminal Law.
For eCAIL, we use the PRC Contract Law\footnote{The contents of law articles can be downloaded from: \url{http://flk.npc.gov.cn/}.}.
The detailed statistics of these datasets are shown in Tab.~\ref{tab: Data Statistics}.

\subsubsection{Baselines.}
We compare our LCM-LAI with the following LCR \& LCM baselines, including the universal models (with the \textbf{[R\&M]} tag), LCR-specific models (with the \textbf{[R]} tag), and LCM-specific models (with the \textbf{[M]} tag).

\noindent{\bf [R\&M] Sentence-BERT~\cite{reimers2019sentence-BERT}:} 
a BERT-based text-matching model that uses BERT~\cite{devlin2018bert} to encode two input cases separately and then uses an MLP to conduct matching by inputting the concatenation of two case embeddings.

\noindent{\bf [R\&M] Lawformer~\cite{xiao2021Lawformer}:}
a Longformer-based pre-trained language model designed to enhance the representation of lengthy legal documents. 
This model has been trained using millions of Chinese legal cases.
In our experiment, we send the concatenation of two cases to Lawformer and use the mean pooling of output to conduct matching.

\noindent{\bf [R\&M] BERT-PLI~\cite{shao2020Bert_PLI}:}
a text-matching model that first uses BERT to capture paragraph-level semantic relationships of two documents and leverages an RNN with an attention mechanism to aggregate the relation embeddings to calculate the matching score by a binary classifier.

\noindent{\bf [R\&M] Thematic Similarity \cite{bhattacharya2020LCR_survey}:} another text-matching model considering the fine-grain similarity that segments two legal cases into paragraphs and computes the paragraph-level similarity. Unlike BERT-PLI, this method uses maximum or average similarities for matching prediction. Combined with the CoSENT loss function, we also compare LCM-LAI to it on the LCR task.

\noindent{\bf [R\&M] ColBERT \cite{khattab2020colbert}:} a classical multi-representation dense search method with a late interaction framework.
Different from Thematic Similarity, which learns only one embedding for each sentence or paragraph, this method learns embeddings for each token and then uses a “MaxSim” operation to compute the matching score between sentences across the query and candidates.


\noindent{\bf [R\&M] ColBERT-X\footnote{In this paper, we load the checkpoint in a bilingual Chinese and English version: \url{https://huggingface.co/hltcoe/plaidx-large-zho-tdist-mt5xxl-engzho}} \cite{ecir2022colbert-x}}: a pre-trained dense retrieval mode with the ColBERT framework for the cross-language information retrieval (CLIR) task.

\noindent{\bf [M] Law-Match~\cite{sun2023law}:} a model-agnostic method that uses the corresponding cited law article to reconstruct the representation of legal cases.
In the setting of the inference stage of our LCR task, the query is original without labels for applicable law articles, which makes Law-Match unfit for the LCR task. 
On the LCM task, our experiment only reports the best performance of the three variants that connect the Law-Match module with Sentence-BERT, Lawformer, or BERT-PLI backbone.

\noindent{\bf [M] IOT-Match~\cite{yu2022Explainable}:} an explainable legal case matching model that additionally marks the rationale labels of case sentences and the alignments labels of rationales with a significant cost. Then, it leverages these elaborate labels to not only improve the performance of the LCM task but also generate natural language-based explanations for the predicted matching results. 
Due to the lack of rationale labels in the LeCaRD and LeCaRDv2 datasets, this method only suits the ELAM and eCAIL datasets for the LCM task. 

\noindent{\bf [R] Some BOW retrieval models.} The typical models include TF-IDF, BM25~\cite{robertson1994BM25}, and LMIR~\cite{song1999LMIR}.

\subsubsection{Evaluation metrics.} For different tasks, we introduce the selected suitable labels for performance evaluation here.

For the LCR task, we employ the top-$k$ Normalized Discounted Cumulative Gain (N@$k$), Precision (P@$k$), and Mean Average Precision (MAP) as evaluation metrics. Among them, we mainly evaluate the P@$k$ and MAP metrics because the LCR task focuses on the candidate cases that perfectly match the query case.

For the LCM task, we choose four typical metrics that are widely used for classification tasks, including accuracy (Acc.), macro-precision (MP), macro-recall (MR), and macro-F1 score. 
Among them, we mainly evaluate all models with the F1 score, which more objectively reflects the effectiveness of our LCM-LAI and other baselines. 
Besides, we also use a Mean-F1 score to evaluate the overall performance, which is the mean of the F1 score on all three datasets.


\subsubsection{Implementation details.} \label{sec: hyperparametter_setting}
For input of the LCM-LAI, we set the maximum case length as 15 sentences and the maximum sentence length as 150 tokens.
As for the hyper-parameters, we use grid search on the validation set of the LCM tasks with AdamW~\cite{loshchilov2018AdamW} optimizer to obtain the best set of LCM-LAI.
The dimension of interaction-related representations (i.e., $d_s$ and $d_l$) is set as $2 \times 768$ as the dimension of all other latent states (i.e., $d_b$ and $d_h$) is set as $768$.
The temperature coefficient $\tau_{a}$ and $\tau_{m}$ respectively are $10$ and $20$.
For training, we set the learning rate of the AdamW optimizer to $3e-5$ and the batch size to $8$.
We set the hyper-parameters as the optimal values in the original paper for all baselines.
In either the LCR task or the LCM task, we train each model for 50 epochs and choose the model with the best score on our focused metric (i.e., MAP and F1 score) on the validation set for testing.
To ensure the reliability of the experimental results, we take the $5$-fold settings for all tasks.
Specifically, as referenced in Tab.~\ref{tab: Data Statistics}, we have partitioned all datasets to ensure that the training set occupies 4/5 of the total, with the remaining 1/5 being evenly divided into the validation set and test set.
Thus, in our experiment, we repeat the experiment five times and make sure the 1/5 data used for validation and testing in each experiment is non-overlapping.
Finally, for each metric, we report the mean of five experiments for comparison.
In addition, for all models that take BERT as the backbone (i.e., our LCM-LAI and all baselines except Lawformer, ColBERT-X, and BOW retrieval models), we initialize them with the parameters of the Legal-Bert\footnote{\url{https://github.com/thunlp/OpenCLaP}}.
Besides, for the ColBERT and ColBERT-X, we split the case into paragraphs and use the MaxP trick (taking the maximum passage score among a document as the document score) to make them adaptive to the document length in LCR \& LCM tasks.
\begin{table*}[t]
\centering
\caption{
The retrieval performances on LeCaRD and LeCaRDv2 test sets.
The best results are in \textbf{bold} and the second ones are \underline{underlined}.
$\dagger$ denotes that LCM-LAI achieves significant improvements over all baselines in paired t-test with $p$-value $\textless 0.05$.
}
\label{tab: Retrieval Performance}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccccc}
\toprule
 Datasets            & \multicolumn{6}{c}{LeCaRD}        & \multicolumn{6}{c}{LeCaRDv2}\\ 
\cmidrule(lr){2-7} \cmidrule(lr){8-13} 
 Metrics        & N@10       & N@20       & N@30       & MAP       & P@10      & P@5
                & N@10       & N@20       & N@30       & MAP       & P@10      & P@5\\
 \midrule
 BM25               & $ 74.27 $     & $ 80.01 $     & $ 88.98 $     & $ 46.18 $     & $ 39.50 $      & $ 37.57 $ 
                    & $ 82.97 $     & $ 86.84 $     & $ 93.16 $     & $ 84.35 $     & $ 82.25 $      & $ 82.00 $     \\
                    
 TF-IDF             & $ 68.09 $     & $ 76.21 $     & $ 86.34 $     & $ 41.04 $     & $ 33.46 $      & $ 32.57 $ 
                    & $ 80.84 $     & $ 85.03 $     & $ 92.32 $     & $ 82.84 $     & $ 79.25 $      & $ 78.75 $     \\

 LMIR               & $ 75.52 $     & $ 80.50 $     & $ 88.50 $     & $ 47.74 $     & $ 39.69 $      & $ 43.11 $ 
                    & $ 83.06 $     & $ 86.62 $     & $ 93.05 $     & $ 84.70 $     & $ 84.00 $      & $ 83.25 $     \\
\midrule
 BERT               & $ 77.41 $     & $ 81.80 $     & $ 89.36 $     & $ 51.67 $     & $ 41.76 $      & $ 45.25 $ 
                    & $ 86.99 $     & $ 89.98 $     & $ 94.53 $     & $ 89.38 $     & $ 88.38 $      & $ 89.75 $     \\
                    
 sentence-BERT      & $ 79.25 $     & $ 83.66 $     & $ 89.89 $     & $ 50.93 $     & $ 43.20 $      & $ 46.71 $ 
                    & $ 86.66 $     & $ 89.82 $     & $ 94.36 $     & $ 89.44 $     & $ 89.00 $      & $ 89.75 $     \\
            
 % Bert-LF            & $ $     & $ $     & $ $     & $ $     & $ $      & $ $ 
 %                    & $ $     & $ $     & $ $     & $ $     & $ $      & $ $     \\
 
 BERT-PLI           & $ 79.33 $     & $ 84.06 $     & $ 90.23 $     & $ 53.64 $     & $ 44.47 $      & $ 47.50 $ 
                    & $ 87.83 $     & $ 90.91 $     & $ 94.83 $     & $ 89.93 $     & $ 88.87 $      & $ 90.50 $     \\
                    
 Lawformer          & $ 78.30 $     & $ 84.06 $     & $ 90.24 $     & $ 52.84 $     & $ 43.40 $      & $ 46.77 $ 
                    & $ \underline{89.15} $     & $ 90.47 $     & $ 95.36 $     & $ 89.53 $     & $ \underline{89.75} $      & $ 90.50 $     \\

 Thematic Similarity(avg) 
                    & $ 79.88 $     & $ 84.13 $     & $ 90.45 $     & $ 51.22 $     & $ 41.13 $      & $ 44.45 $ 
                    & $ 89.00 $     & $ \underline{91.58} $     & $ \underline{95.53} $     & $ 90.25 $     & $ 89.00 $      & $ 90.50 $     \\

 Thematic Similarity(max) 
                    & $ 79.27 $     & $ 83.71 $     & $ 89.97 $     & $ 51.73 $     & $ 43.23 $      & $ 47.21 $ 
                    & $ 87.35 $     & $ 90.21 $     & $ 94.42 $     & $ 88.83 $     & $ 87.87 $      & $ 89.25 $     \\

 % Law-Match          & $ $     & $ $     & $ $     & $ $     & $ $      & $ $ 
 %                    & $ $     & $ $     & $ $     & $ $     & $ $      & $ $     \\
 ColBERT 
                    & $ 79.89 $     & $ 84.25 $     & $ 90.38 $     & $ 53.90 $     & $ 42.68 $      & $ 46.77 $ 
                    & $ 87.49 $     & $ 90.52 $     & $ 94.76 $     & $ 89.38 $     & $ 89.12 $      & $ 89.75 $     \\

 ColBERT-X 
                    & $ \underline{80.41} $     & $ \underline{85.21} $     & $ \underline{90.65} $     & $ 53.51 $     & $ 44.00 $      & $ 47.17 $ 
                    & $ 87.51 $     & $ 90.63 $     & $ 94.65 $     & $ 90.19 $     & $ 89.13 $      & $ 90.75 $     \\
 
 \midrule

 \bf{LCM-LAI + CE}       
                & $ 79.93 $     & $ 83.69 $     & $ 89.87 $     & $ \underline {54.42^{\dagger}} $     & $ \underline {44.54} $      & $ \underline {48.13}^{\dagger} $ 
                & $ 89.01 $     & $ 91.20 $     & $ 95.17 $     & $ \underline{90.79}^{\dagger} $     & $ 89.38 $      & $ \bf{92.50}^{\dagger} $     \\

 \bf{LCM-LAI + CoSENT}       
                & $ \bf {81.33}^{\dagger} $     & $ \bf {85.72}^{\dagger} $     & $ \bf {91.18}^{\dagger} $     & $ \bf {56.69}^{\dagger} $     & $ \bf {45.00}^{\dagger} $      & $ \bf {49.38}^{\dagger} $ 
                & $ \bf{89.63}^{\dagger} $     & $ \bf{91.90}^{\dagger} $     & $ \bf{95.70} $     & $ \bf{91.27}^{\dagger} $     & $ \bf{90.38}^{\dagger} $      & $ \underline{91.75}^{\dagger} $     \\
 \bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Basic Performance Evaluation (RQ1)}
\subsubsection{The LCR Results.} 
We present the results of our experiments conducted on the LeCaRD and LeCaRDv2 datasets for the LCR task, as shown in Tab.~\ref{tab: Retrieval Performance}.
In summary, LCM-LAI demonstrates exceptional performance across all six metrics compared to baseline methods, showcasing its state-of-the-art capabilities.
On the LeCaRD dataset, LCM-LAI showcases significant enhancements in MAP, P@5, and P@10, with improvements of $2.79\%$, $1.88\%$ and $0.53\%$ respectively, surpassing the state-of-the-art baseline. Similarly, the LeCaRDv2 dataset also witnessed improvements of $1.02\%$, $1.75\%$, and $0.63\%$.

\begin{table*}[t]
\centering
\caption{
The matching performances on ELAM, LeCaRD, and eCAIL test sets.
The best results are in \textbf{bold} as the second ones are \underline{underlined}.
$\dagger$ denotes that LCM-LAI achieves significant improvements over all baselines in paired t-test with $p$-value $\textless 0.05$.
“$--$” denotes that the IOT-match fails to deal with the ELAM dataset due to a lack of necessary input or label.
Thus, we use the average F1 score of other methods on the LeCaRD dataset to compute its final Mean-F1 score.
}\label{tab: Match Performance}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccccccc}
\toprule
 Datasets              & \multicolumn{4}{c}{ELAM}                     & \multicolumn{4}{c}{LeCaRD}   & \multicolumn{4}{c}{eCAIL} & \multirow{2}{*}{Mean-F1}\\ 
 \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} 
 Metrics            & Acc.        & MP          & MR          & F1      
                    & Acc.        & MP          & MR          & F1      
                    & Acc.        & MP          & MR          & F1        \\
                    % &  (Rank\_sum/task\_num)\\
 \midrule

 BERT                       & $ 68.58 $     & $ 68.46 $     & $ 67.86 $     & $ 67.97 $     
                            & $ 62.35 $     & $ 64.49 $     & $ 61.19 $     & $ 62.11 $     
                            & $ 73.68 $     & $ 74.07 $     & $ 73.71 $     & $ 73.76 $     & $67.95$\\
                            %10 /  (26/3)
                            
 Sentence-BERT              & $ 72.47 $     & $ 71.50 $     & $ 71.47 $     & $ 71.29 $     
                            & $ 62.29 $     & $ 61.34 $     & $ 60.40 $     & $ 60.59 $     
                            & $ 77.87 $     & $ 78.18 $     & $ 77.92 $     & $ 77.99 $     & $69.96$\\
                            % 7 / (22/3)
 % BERT-PLI                   & $ 70.74 $     & $ 68.89 $     & $ 68.84 $     & $ 68.80 $
 %                            & $ 61.60 $     & $ 60.88 $     & $ 60.41 $     & $ 60.48 $  
 %                            & $ 73.86 $     & $ 73.27 $     & $ 73.74 $     & $ 73.45 $\\
                            
 BERT-PLI                   & $ 73.59 $     & $ 73.43 $     & $ 72.88 $     & $ 73.08 $     
                            & $ 66.44 $     & $ 68.80 $     & $ 65.21 $     & $ 66.57 $     
                            & $ 77.87 $     & $ 77.82 $     & $ 78.06 $     & $ 77.83 $     & $72.49$\\
                            % 4 / (12/3)
                    
 Lawformer                  & $ 73.47 $     & $ 73.30 $     & $ 72.55 $     & $ 72.70 $     
                            & $ 66.08 $     & $ 66.41 $     & $ 64.43 $     & $ 65.45 $     
                            & $ 77.98 $     & $ 78.16 $     & $ 78.03 $     & $ 78.04 $     & $72.06$ \\
                            % 9 / \hspace{0.1cm}(25/3) 

 Thematic Similarity(avg)   & $ 73.79 $     & $ 71.92 $     & $ 72.30 $     & $ 71.93 $     
                            & $ 62.26 $     & $ 64.61 $     & $ 60.74 $     & $ 61.82 $        
                            & $ 76.00 $     & $ 75.68 $     & $ 76.06 $     & $ 75.69 $     & $69.81$\\
                            % 8 / \hspace{0.1cm}(23/3) 

 Thematic Similarity(max)   & $ 73.71 $     & $ 72.80 $     & $ 72.81 $     & $ 72.57 $     
                            & $ 65.08 $     & $ 67.57 $     & $ 62.62 $     & $ 63.81 $     
                            & $ 77.44 $     & $ 77.34 $     & $ 77.50 $     & $ 77.09 $     & $71.16$\\
                            % 6 / \hspace{0.1cm}(16/3) 
 ColBERT   & $ 74.27 $     & $ 73.29 $     & $ 73.31 $     & $ \underline{73.19} $ 
                            & $ 67.12 $     & $ 68.26 $     & $ 66.34 $     & $ 66.85 $  
                            & $ 78.24 $     & $ 78.43 $     & $ 78.37 $     & $ 78.35 $  & $ 72.80 $  \\

 ColBERT-X & $ 74.39 $     & $ 72.91 $     & $ 72.71 $     & $ 72.50 $     
                            & $ 67.68 $     & $ 68.92 $     & $ 64.53 $     & $ 65.78 $    
                            & $ 78.61 $     & $ 78.43 $     & $ 78.76 $     & $ 78.66 $     & $ 72.31 $  \\

 IOT-Match                  & $ 73.87 $     & $ 73.02 $     & $ 72.41 $     & $ 72.55 $     
                            & $ -- $     & $ -- $     & $ -- $     & $ -- $     
                            & $ \bf 82.00 $     & $ \bf 82.10 $     & $ \bf 81.92 $     & $ \bf 81.90 $     & $73.09$\\
                            % 3/ \hspace{0.28cm}(6/2) 

 Law-Match                  & $ \underline{74.95} $     & $ 72.96 $     & $ 71.75 $     & $ 72.35 $     
                            & $ 65.63 $     & $ 66.07 $     & $ 63.75 $     & $ 64.41 $     
                            & $ 80.00 $     & $ 79.78 $     & $ 79.92 $     & $ 79.84 $     & $72.20$\\
                            % 5 / \hspace{0.1cm}(14/3)
                            
\midrule
 \bf{LCM-LAI + CE}          
            & $\bf{76.51}^{\dagger} $ & $\bf{75.88}^{\dagger} $ & $\bf{75.64}^{\dagger} $ & $\bf{75.57}^{\dagger} $    
            & $\bf{70.97}^{\dagger} $ & $\bf{71.95}^{\dagger} $ & $\bf{71.36}^{\dagger} $ & $\bf{71.24}^{\dagger} $    
            & $\underline{81.07} $ & $\underline{81.01} $ & $\underline{81.23} $ & $\underline{80.89}$      & $\bf 75.90^{\dagger}$\\
            % \textbf{1} / \hspace{0.28cm}(4/3) 
 \bf{LCM-LAI + CoSENT}      
 & $ 74.31 $ & $\underline{73.61}^{\dagger} $ & $\underline{73.32}^{\dagger} $ & $ 73.15 $    
 & $\underline{69.10}^{\dagger} $ & $\underline{71.37}^{\dagger} $ & $\underline{69.02}^{\dagger} $ & $\underline{69.47}$ 
 & $ 80.08 $ & $ 80.16 $ & $ 80.04 $ & $ 79.91 $ & $\underline{74.18}^{\dagger}$\\
 % \underline{2} / \hspace{0.25cm}(7/3) 

 \bottomrule
\end{tabular}%
}
\end{table*}
\subsubsection{The LCM Results.}
Tab.~\ref{tab: Match Performance} shows the experiment results of the LCM task on the ELAM, LeCaRD, and eCAIL datasets. 
Overall, our proposed LCM-LAI model exhibits the most competitive performance, which achieves the best Mean-F1 score of the three datasets with an improvement of $2.81\%$. 
Specifically, on the two datasets of all three, i.e., ELAM and LeCaRD, our LCM-LAI achieves state-of-the-art performance in all four evaluation metrics.
Compared with the state-of-the-art baseline, it improves the F1 scores by $2.49\%$ on dataset ELAM and $4.67\%$ on dataset LeCaRD.
As for the eCAIL dataset, LCM-LAI also gets the second-best performance, coming closest to the state-of-the-art IOT-Match model, with only a $1.01\%$ lower F1 score.

\subsubsection{Discussion of Vertical Association.} \label{sec: basic_performance}
We correlated the results of LCR \& LCM tasks longitudinally while comparing the baselines in each task horizontally.
Then we get some confirmatory observations and guesses as follows:
\begin{enumerate}
    \item {
    For the LCR \& LCM tasks, LCM-LAI outperforms the most related baseline Law-Match on all datasets across all metrics, even without the utilization of ground-truth applicable law articles as input.
    This is attributed to LCM-LAI's end-to-end dependent multi-task learning framework, which combines case legal-rational feature extraction with representation computation to learn more comprehensive representations and better performance.
    }
    \item {
    In addition, we focus on comparing our approach to the IOT-Match baseline, which heavily relies on numerous elaborate labels.
    These labels include legal-rational labels for each sentence and alignment labels across case sentences for effectively capturing interaction information through supervised learning in the LCM task.
    In the experiments on all four datasets, IOT-Match cannot be applied to LeCaRD and LeCaRDv2 datasets due to the lack of rationale annotations and sentence alignment annotations.
    Besides, it performs worse than our LCM-LAI on the ELAM dataset.
    This demonstrates the effectiveness and robustness (or generalization) of our LCM-LAI, as it performs well without elaborate expert annotation.
    It is noted that the performance of LCM-LAI on the e-CAIl dataset is inferior to IOT-Match.
    We argue it is because many legal definitions of civil cases are relatively more fine-grained and more complex, 
    in this situation, the additional careful annotation that IOT-Match used can provide more detailed domain knowledge than directly using legal text definitions.
    Despite bringing performance improvement, we must emphasize that these additional annotations also hinder the generalization ability of IOT-MATch under different legal systems.
    }
    \item {
    Comparing the performance between \textit{LCM-LAI + CL} and \textit{LCM-LAI + CoSENT}, as outlined in Tabs.~\ref{tab: Retrieval Performance} and~\ref{tab: Match Performance}, we observed that CoSENT loss outperforms CE loss in the LCR task, especially on the metrics of NDCG@$k$.
    This phenomenon can be attributed to the effective utilization of fine-grained match-level labels for learning ranking strategies, facilitated by the contrast term $\hat{S}(X_{i}, Y_{j})- \hat{S}(X_{m}, Y_{n})$ within the CoSENT loss.
    As a result, this mechanism aids in addressing the challenges associated with LCR tasks.
    Conversely, an opposite trend was observed in the LCM task, suggesting that a nonlinear classification surface is better suited for this classification-like task.
    }
    \item {
    Comparing multi-vector retrieval baselines with single-vector ones, we could find multi-vector based baselines are more advantageous. For example, even though ColBERT-X has not been pre-trained on legal data, it still achieves better overall performance than the single-vector ones like BERT, Sentence-BERT, BERT-PLI, Lawformer and Thematic Similarity (avg/max).
    However, their performance is still inferior to our LCM-LAI, further reflecting the importance and necessity of considering legal relevance between cases, which we emphasize in this paper.
    }
\end{enumerate}

\subsection{Ablation Experiments (RQ2)}
\subsubsection{\textbf{Ablation of the Framework}}
To further emphasize the importance of incorporating both semantic and jurisprudential relevance in legal cases, we conducted a series of ablation experiments on the fifth data division of the LeCaRD dataset on the LCM task to illustrate this point.

In this experiment, the specific ablation variations include:
\begin{itemize} 
    \item {
    LCM-LAI (w/o AIA): To show the importance of our article-intervened attention (\textbf{AIA}) mechanism, the first variation model we build is the LCM-LAI without AIA, i.e., using only 
    $\mathbf{X}_f = \mathbf{X}^{(S)} \oplus \mathbf{X}^{(L)}$ and $\mathbf{Y}_f = \mathbf{Y}^{(S)} \oplus \mathbf{Y}^{(L)}$ to predict the matching level labels.
    }
    \item {
    LCM-LAI (w/o LIM): To evaluate the effectiveness of our dependent multi-task learning framework, especially the article prediction sub-task, we build the second variation that LCM-LAI without the legal interaction module (LIM), records as LCM-LAI (w/o LIM).
    This variation only uses the semantic interaction representations, i.e., $\mathbf{X}_f = \mathbf{X}^{(S)}$ and $\mathbf{Y}_f = \mathbf{Y}^{(S)}$ to predict the matching level labels, without optimizing the law article sub-task during training.
    }

    \item {
    LCM-LAI (w/o BIM): To compare whether semantic or legal-rational interactions are more critical in LCM \& LCR tasks, we also design a third variant that LCM-LAI without basic interaction module (BIM), records as LCM-LAI (w/o BIM) or LCM-LAI (only LIM).
    This variation uses only the legal-rational interaction representations, i.e., $\mathbf{X}_f = \mathbf{X}^{(L)} \oplus \mathbf{X}^{(A)}$ and $\mathbf{Y}_f = \mathbf{Y}^{(L)} \oplus \mathbf{Y}^{(A)}$ to predict the matching level labels.
    }
    \item{
    LCM-LAI (LIM without AIA): To further explore which is more effective in LIM between the legal interaction representation (i.e.,$\mathbf{X}^{(L)}$ and $\mathbf{Y}^{(L)}$) and the article-intervened representation (i.e., $\mathbf{X}^{(A)}$ and $\mathbf{Y}^{(A)}$), we construct another variant that LCM-LAI (LIM without AIA), which only uses the legal-rational representation without law intervention to make predictions, i.e., $\mathbf{X}_f = \mathbf{X}^{(L)}$ and $\mathbf{Y}_f = \mathbf{Y}^{(L)}$.
    }
    \item {
    LCM-LAI (only AIA): To further demonstrate that AIA complements the information of legal-rational interaction, we propose the final variation that LCM-LAI (only AIA), which uses the article-intervened legal-rational representation for prediction, i.e., $\mathbf{X}_f = \mathbf{X}^{(A)}$ and $\mathbf{Y}_f = \mathbf{Y}^{(A)}$.
    }
\end{itemize}

\begin{table}[t]
\centering
\caption{Ablation study of LCM-LAI on the LeCaRD dataset.}\label{tab: Ablation Study}
\begin{tabular}{lcccccc}
\toprule
 % \multicolumn{1}{l}{\multirow{2}{*}{Methods}}              & \multicolumn{2}{c}{ELAM}                     & \multicolumn{2}{c}{LeCaRd} & \multicolumn{2}{c}{e-CAIL}\\ 
 % \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 Methods            & Accuracy      & Macro-Precision       & Macro-Recall      & F1         \\
 \midrule
 LCM-LAI            & $\textbf{73.99}$       & $\textbf{73.60}$       & $\textbf{76.50}$       & $\textbf{74.28}$     \\

 LCM-LAI (w/o AIA)  & $73.07$       & $73.16$       & $75.60$       & $73.70$     \\

 LCM-LAI (w/o LIM)  & $62.54$       & $64.09$       & $63.25$       & $63.16$     \\

 LCM-LAI (w/o BIM)  & $72.45$       & $73.06$       & $74.03$       & $72.43$     \\
 
 LCM-LAI (only AIA) & $71.52$       & $71.74$       & $72.11$       & $71.87$     \\

 LCM-LAI (LIM without AIA) & $ 71.21 $       & $ 72.81 $       & $ 71.13 $       & $ 71.82 $     \\
% \midrule
% \rowcolor{red!30} LCM-LAI (unit) & $ 72.14 $       & $ 72.33 $       & $ 73.75 $       & $ 72.92 $     \\
% \rowcolor{red!30} LCM-LAI (random) & $ 71.21 $       & $ 71.88 $       & $ 73.64 $       & $ 72.13 $     \\
% \rowcolor{red!30} LCM-LAI (embedding distance) & $ 71.52 $       & $ 74.67 $       & $ 71.73 $       & $ 72.98 $     \\
\bottomrule
\end{tabular}%
\end{table}

Tab.~\ref{tab: Ablation Study} shows the experiment results. It demonstrates that all AIA, LIM, and BIM are necessary for our LCM-LAI to improve performance.
We summarize some other specific observations:
\begin{enumerate}
    \item {
    Compared to other variations, the LCM-LAI (w/o LIM) variation exhibits a considerable decrease in performance, indicating that the law prediction subtask is crucial in enhancing the performance of similar case matching. 
    This further highlights the superior effectiveness of its dependent multi-task learning framework.
    }
    \item {
    Compared to the LCM-LAI (w/o LIM), the improved performances of LCM-LAI (only AIA) and LCM-LAI (LIM without AIA) provide further evidence supporting the higher effectiveness of legal-rational interaction representations over pure semantic interaction representation in the context of similar case matching. 
    This finding again reinforces that the definition of similarity within the task of similar case matching is domain-specific.
    }
    \item {
    In contrast to the LCM-LAI (w/o BIM), LCM-LAI (w/o AIA) shows better performance,  particularly with a notable improvement of $1.57\%$ on the Macro-Recall metric and $1.27\%$ on the F1 score.
    This outcome underscores the capability of semantic interaction representations, which effectively complements the legal-rational interaction.
    }
\end{enumerate}

\subsubsection{\textbf{Ablation of the Legal Correlation Matrix}}
In addition, we try to validate further the efficacy of the legal correlation matrix proposed within the LCM-LAI framework from two distinct perspectives: (1) The successful acquisition of effective alignment relationships among case sentences; (2) The superiority of measuring legal correlation from a legal distribution perspective as opposed to the direct computation of sentence embedding correlations. 
Thus, we retain the integrity of the overall framework, only replace the legal correlation matrix, and generate the following variants:

\begin{itemize}
    \item {
    LCM-LAI (unit) and LCM-LAI (random): To ascertain the efficacy of the proposed legal correlation matrix $\mathbf{C}^{(L)}$ in learning meaningful sentence alignments between cases, we have developed two variant models by substituting $\mathbf{C}^{(L)}$ with a random matrix and a unit matrix, referred to as LCM-LAI (unit) and LCM-LAI (random), respectively.
    }
    \item {
    LCM-LAI (embedding distance): To further substantiate the superiority of assessing legal correlation from the perspective of law distribution, we construct another variant that directly computes cosine similarity between sentences' value vectors as legal-rational relevance between sentences, denoted as LCM-LAI (embedding distance).
    }
\end{itemize}
The results\footnote{All experimental results obtained experimentally on the fifth data division in our 5-fold setting.} are presented in Fig.~\ref{fig: Legal_matrix}. 
In a word, LCM-LAI (embedding distance) performs better than LCM-LAI (unit) and LCM-LAI (unit) performs better than LCM-LAI (random).
LCM-LAI performs best.
By analyzing the results, we can draw the following conclusions:

\begin{figure*}[t]
\centering
  \subfigure[\label{fig: Legal_matrix_acc}]{
    \adjustbox{valign=b}{
      \includegraphics[width=0.5\textwidth]{fig/Legal_matrix_acc.pdf}
    }}
  \hspace{-8mm}
  \subfigure[\label{fig: Legal_matrix_f1}]{
    \adjustbox{valign=b}{
      \includegraphics[width=0.5\textwidth]{fig/Legal_matrix_f1.pdf}
    }}
\caption{The performance of LCM-LAI w.r.t. different mode of legal correlation matrix on LCM task.
}
\label{fig: Legal_matrix}
\end{figure*}

\begin{enumerate}
    \item{
    The enhanced performance of the original model LCM-LAI and the variant LCM-LAI (embedding distance) in comparison to variants LCM-LAI (unit) and LCM-LAI (random) validate that the legal correlation matrix effectively captures meaningful sentence alignments between cases.
    More fine-grained experiments can be found in Sec \ref{sec:interpretability}.
    }
    \item{
    The superior performance of the original model LCM-LAI relative to the variant LCM-LAI (embedding distance) confirms that assessing the legal-rational correlation between across-case sentences from the perspective of law distribution is more effective than directly computing the similarity between their embedding representations.
    }
    \item{
    The variant LCM-LAI (unit) performs better than the variant LCM-LAI (random).
    We conjecture this is because the standardization of legal document writing causes key circumstances, constitutive elements of crime, and focus of dispute to have specific positional distributions, which makes the sentence alignment relationship matrix between similar cases closer to the unit matrix compared to a random matrix.
    }
\end{enumerate}

\subsection{Inference Efficiency Evaluation (RQ3)}
In the practical retrieval system, inference efficiency is a critical indicator, as it directly impacts the query delay, consequently affecting their overall product experience.
Thus, we examine the inference efficiency by re-ranking the top-k result given by a BOW retrieval model (e.g., BM25), the most typical setting for testing and deploying dense retrieval models.
We conduct this experiment on the LeCaRD dataset.
Regarding our metrics, we report two commonly used efficiency indicators: the re-ranking latency of the top 100 candidates and the FLOPs per query.
Besides, all benchmarks are measured on a single $32$ GB Tesla V$100$ GPU.
For a fair evaluation, we exclude CPU-based text pre-processing and present the primary GPU computation latency since it is a more time-consuming aspect.
To simulate real-world application scenarios, all methods aim to precompute as much of the processing (e.g., the encoding of candidate cases) as possible offline and parallelize the remaining online computations. 
As a result, only a negligible cost is incurred.
As for the FLOPs per query, we estimate each model with the torchprofile\footnote{\url{htps://github.com/mit-han-lab/torchprofle}} library.

Fig.~\ref{fig: Time Efficiency} show the experiment results.
As we can see, LCM-LAI maintains a reasonable range of re-ranking latency and FLOPs per query.
Specifically, we get some summing-up as follows:

\begin{figure*}[t]
\centering
  \subfigure[\label{fig: Latency}]{
    \adjustbox{valign=b}{
      \includegraphics[width=0.465\textwidth]{fig/Latency_new.pdf}
    }}
  \hspace{5mm}
  \subfigure[\label{fig: FLOPs}]{
    \adjustbox{valign=b}{
      \includegraphics[width=0.43\textwidth]{fig/FLOPs_new.pdf}
    }}
\caption{The performance of efficiency evaluation.
(a) Time latency under the top-100 re-ranking setting.
(b) FLOPs (in millions) per query of the re-ranking depth $k$.
To make an even distribution, we use logarithmic coordinates for sub-figure (b) abscissa.
}
\label{fig: Time Efficiency}
\end{figure*}

\begin{enumerate}
    \item {
    Referring to Fig.~\ref{fig: Latency}, under the top-100 re-ranking setting, LCM-LAI's inference latency of $76.52ms$ effectively meets the response time criteria set by the online retrieval system ($<<500ms$). Meanwhile, it's considerably lower than other all-to-all interaction baselines (i.e., BERT, BERT-PLI, and Lawformer) that model the interactions between words within as well as across query and candidates at the same time.
    }
    \item {
    Analyzing Fig.~\ref{fig: FLOPs}, we note that the FLOPs of all-to-all interaction baselines (i.e., Lawformer, BERT, and BERT-PLI) grow linearly with the number of candidate sets.
    This is because their interaction patterns must be recalculated for every case pair, which is impossible to precompute offline.
    On the contrary, the FLOPs of LCM-LAI do not change as the number of candidate sets grows.
    This is attributed to the late interaction framework of LCM-LAI, which allows us offline to precompute the computationally expensive Sentence Embedding module and only online compute the cheap interaction modules (i.e., LIM and BIM).
    Specifically, taking the legal case $Y$ as a candidate, the majority of computations, including $\mathbf{y}_i$, $\mathbf{L}_k^Y$, $\Lambda^{Y}$, and $\mathbf{v}_j^Y$, can be conducted offline.
    Hence, when the candidate set is expanded, the computational overhead of LCM-LAI can remain consistent, similar to representation-focused rankers such as Sentence-Bert and Thematic.
    }
\end{enumerate}

\subsection{The Interpretability of Matching (RQ4)}\label{sec:interpretability}
Interpretability assumes a crucial role by fulfilling two fundamental objectives in the legal domain. 
Firstly, it effectively disseminates accurate knowledge about the law to the public through reasonable explanations. 
Secondly, it assists judges in their decision-making process by offering professional argument logic.
Since our LCM-LAI uses considerable interactive operations and attention mechanisms, we expect the model to have some interpretability.
We separately visualize the semantic correlation matrix of BIM, the legal correlation matrix of LIM, and the attention matrix of AIA for an example legal case pair from the ELAM test set.
To evaluate the overall quality, we also compare our visualization with the ground truth obtained from the alignment labels  (cf. Fig.~\ref{fig: HOT_truth}).


Fig.~\ref{fig: Interpretability} shows the results, where the darker block is, the higher the relation score the corresponding sentence pair gets. 
Overall, for all three gold alignment labels, i.e., ($S_{11}^{1}$, $S_{6}^{2}$), ($S_{8}^{1}$, $S_{8}^{2}$), and ($S_{11}^{1}$, $S_{11}^{2}$), all interaction matrices and attention matrices can sparsely cover two of them, i.e., ($S_{8}^{1}$, $S_{8}^{2}$) and ($S_{11}^{1}$, $S_{11}^{2}$), which proves that LCM-LAI has some interpretability.
Then, referring to specific sentence descriptions with high scores (cf. Fig.~\ref{fig: explain_sentences}), we find the following interesting observations:

\begin{figure*}[t]
\centering
 \subfigure[\label{fig: HOT_truth}]{\includegraphics[width=0.245\linewidth]{fig/HOT_truth.pdf}}
 \subfigure[\label{fig: HOT_BIM}]{\includegraphics[width=0.245\linewidth]{fig/HOT_Semantic.pdf}}
 \subfigure[\label{fig: HOT_LIM}]{\includegraphics[width=0.245\linewidth]{fig/HOT_Legal.pdf}}
 \subfigure[\label{fig: HOT_AIA}]{\includegraphics[width=0.245\linewidth]{fig/HOT_Law.pdf}}
 \subfigure[\label{fig: explain_sentences}]{\includegraphics[width=\linewidth]{fig/explain_sentences_TOIS.pdf}}
\caption{The visualization of sentence alignment for an example in the ELAM test set.
(a) The human-labeled alignments;
(b), (c), and (d) The attention score visualization for semantic correlation matrix (cf. Sec~\ref{subsubsec: semantic correlation matrix}), legal correlation matrix (cf. Sec~\ref{subsubsec: law correlation matrix}), and legal correlation matrix combined with AIA (cf. Sec~\ref{subsubsec: law interaction encoder}); 
(e) The sentence pairs with ground-truth alignment labels or the highest interaction scores.
}
\label{fig: Interpretability}
\end{figure*}

\begin{enumerate}
    \item {
    We notice that the legal correlation matrix captures some sentence pairs far away from the gold alignment labels, i.e., ($S_{1}^{1}$, $S_{1}^{2}$) or ($S_{1}^{1}$, $S_{2}^{2}$) when other semantic correlation matrix and AIA centrally capture the matching relations around the aligned labels.
    From Fig.~\ref{fig: explain_sentences}, we find that these sentence pairs capture the interaction information about the key circumstance of fact description instead of the constitutive elements of the crime (represented by($S_{8}^{1}$, $S_{8}^{2}$) and ($S_{11}^{1}$, $S_{11}^{2}$)), or the focus of disputes (represented by ($S_{11}^{1}$, $S_{6}^{2}$)).
    This notable observation offers a conceivable rationale for the remarkable efficacy demonstrated by the LCM-LAI model on the LeCaRD dataset. 
    In the pre-processing phase, a conscious decision is made to retain solely the factual components of the case, eliminating court summary text that includes inferences and points of dispute. Consequently, the models can predict matching labels solely by capturing the interactive information in the case facts, which aligns perfectly with the strengths of the LCM-LAI model.
    }
    
    \item {
    We also notice that all three interaction matrices can hardly capture the labeled sentence pair ($S_{11}^{1}$, $S_{6}^{2}$), which reflects that LCM-LAI is still difficult to detect the controversial focus of a legal case (i.e., the underlined part of Fig.~\ref{fig: explain_sentences}).
    The focus of the dispute usually consists of a defense statement and a court decision, with the former often pleading for reduced charges or penalties.
    However, these statements are commonly deemed as noise or irrelevant to the law prediction subtask, which makes it difficult for the LCM-LAI model to pay attention to the focus of the dispute. 
    This is also the shortcoming of the LCM-LAI and inspires future enhancements to it.
    }
\end{enumerate}

\begin{table*}[t]
\centering
% \color{blue}
\small
\caption{
The matching performance on the LeCaRD dataset with different backbones. The results of our LCM-LAI are in \textbf{bold}. $\dagger$ denotes LCM-LAI achieves significant improvements over all existing baselines in paired t-test with $p$-value $\textless 0.05$.
}\label{tab: Different-backbones}
\begin{tabular}{lcccc}
\toprule
Methods   & Accuracy   & Macro-Precision   & Macro-Recall    & F1\\
 \midrule

BERT          
                    & $ 65.02 $     & $ 65.23 $     & $ 63.50 $     & $ 64.26 $     \\

$\textbf{LCM-LAI}_{BERT}$    
                    & $ \textbf{73.99}^{\dagger} $ & $ \textbf{73.60}^{\dagger} $ 
                    & $ \textbf{76.50}^{\dagger} $ & $ \textbf{74.28}^{\dagger} $\\

\midrule
Lawformer          
                    & $ 65.63 $     & $ 64.81 $     & $ 65.73 $     & $ 65.17 $\\
$\textbf{LCM-LAI}_{Lawformer}$    
                    & $ \textbf{73.37}^{\dagger} $ & $ \textbf{75.51}^{\dagger} $ 
                    & $ \textbf{73.25}^{\dagger} $ & $ \textbf{73.82}^{\dagger} $\\
\midrule
Legal-RoBERTa         
                    & $ 66.87 $     & $ 65.01 $  & $ 65.60 $     & $ 65.28 $\\
$\textbf{LCM-LAI}_{Legal-RoBERTa}$   
                    & $ \textbf{74.30}^{\dagger} $ & $ \textbf{76.67}^{\dagger} $ 
                    & $ \textbf{73.90}^{\dagger} $ & $ \textbf{74.66}^{\dagger} $\\
\midrule
XLNet         
                    & $ 64.09 $     & $ 63.93 $   & $ 63.16 $     & $ 63.41 $\\
$\textbf{LCM-LAI}_{XLNet}$   
                    & $ \textbf{71.83}^{\dagger} $ & $ \textbf{72.46}^{\dagger} $ 
                    & $ \textbf{72.33}^{\dagger} $ & $ \textbf{72.32}^{\dagger} $\\

\midrule
BGE         
                    & $ 64.71 $     & $ 64.15 $   & $ 64.84 $     & $ 64.43 $\\
$\textbf{LCM-LAI}_{BGE}$   
                    & $ \textbf{72.76}^{\dagger} $ & $ \textbf{73.97}^{\dagger} $ 
                    & $ \textbf{72.61}^{\dagger} $ & $ \textbf{73.19}^{\dagger} $\\
\bottomrule
\end{tabular}%
\end{table*}

\subsection{Robustness about Different Backbones}

As the proposed framework of LCM-LAI is backbone-independent, it is easy to transfer across different backbone models.
In this section, to demonstrate that the LCM-LAI framework can improve different backbone models, we conducted a comparative experiment on the fifth iteration of the LeCaRD dataset.
In addition to the BERT and Lawformer used in the previous experiments, we also compare the following two popular PLMs:

\begin{itemize}
\item \textbf{Legal-RoBERTa}~\cite{xiao2021Lawformer}\footnote{\url{https://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-330M-Similarity}}: the RoBERTa~\cite{liu2019roberta} model fine-tuned by Chinese legal corpus based on the RoBERTa-wwm-ext~\cite{cui2021pre} checkpoint. 
This model follows the whole word masking strategy, in which the tokens that
belong to the same word will be masked simultaneously.

\item \textbf{XLNet}~\cite{NEURIPS@XL_Net}\footnote{\url{https://huggingface.co/hfl/chinese-xlnet-base}}:
a PLM model that employs a generalized autoregressive pertaining strategy. 
It utilizes a different architecture and extra tokens compared to BERT.
By integrating the segment recurrence mechanism and relative encoding scheme of Transformer-XL into its perturbation process, XLNet demonstrates superior performance on tasks involving longer text sequences compared to BERT.

\item \textbf{BGE}~\cite{xiao2023BGE}\footnote{\url{https://huggingface.co/BAAI/bge-large-zh-v1.5}}: 
a general embedding model that is pre-trained using RetroMAE~\cite{xiao2022retromae} and trained on large-scale pair data using contrastive learning.
Compared with the general PLM, this model has proved to be more suitable for document retrieval tasks.


\end{itemize}
Note that for all original backbone models compared in this experiment, we all take the case document pair as input and utilize the fully interactive architecture, as it performs better than the Sentence-BERT-like architecture.
When the different bases are coupled with our LCM-LAI architecture, the experimental setup used is also completely consistent with that described in Sec.~\ref{sec: hyperparametter_setting}.

The experiment results are presented in Tab. \ref{tab: Different-backbones}. 
From the observations, we can draw the following conclusions:

\begin{enumerate}
    \item{
    For arbitrary backbone models, our LCM-LAI framework achieves significant improvement ranging from $7.43\%$ to $13.00\%$, further demonstrating the robustness of the proposed model.
    }
    \item{
    We observe that BERT, Lawformer, and Legal-RoBERTa perform better than XLNet and BGE for the original backbone models.
    We contend that this is because the former models have been pre-trained with a professional corpus in the judicial domain,  making them more adept at comprehending legal terminology.
    }
\end{enumerate}


% \subsubsection{Effects of pre-trained AP sub-task.}

\section{Discussion 1: Whether LCM-LAI benefits from fine-grained legal-rational and alignment labels?}

As mentioned in Sec.~\ref{sec: basic_performance}, we attribute the performance advantage of IOT-match over our LCM-LAI on the eCAIL dataset to the additional annotations they used.
In this section, we conduct additional experiments to discuss whether our LCM-LAI model is similarly able to benefit from these additional annotations.
IOT-match uses three kinds of additional annotations, including rationale labels for each sentence, the binary alignment matrix for pairwise cases, and explanatory sentences that describe matching relationships between cases.
Since the last additional annotation is employed to train a generative model to generate interpretable statements and cannot be naturally combined with our LCM-LAI,
only the former two are chosen in this experiment to enhance our LCM-LAI model.
Here we give the definitions of these two additional annotations according to the original IOT-Match~\cite{yu2022Explainable}.
At first, we denote the rationale labels of case $X$ and $Y$ as $\mathbf{r}^{X}$ and $\mathbf{r}^{Y}$.
The rationale labels are associated with the sentence embeddings, 
i.e., $\mathbf{r}^{X}=\{r_{x_i}\}_{i=1}^{n_x}$ and $\mathbf{r}^{Y}=\{r_{y_j}\}_{j=1}^{n_y}$, where the rationale label of a sentence $x_i$ is designed following:
$$
r_{x_{i}} =
\begin{cases}
0 & x_{i} \text{ is not a rationale}, \\
1 & x_{i} \text{ is a key circumstance}, \\
2 & x_{i} \text{ is a constitutive element of crime}, \\
3 & x_{i} \text{ is a focus of disputes}. \\
\end{cases}
$$
Then, we denote the binary alignment matrix between case $X$ and $Y$ as $\mathbf{A}=[a_{i,j}]\in \{0, 1\}^{n_x \times n_y}$, in which each element $a_{i,j}$ is annotated manually by the following formula,
$$
a_{i, j} =
\begin{cases}
0 & r_{x_i} \neq r_{y_j}, \\
1 & r_{x_i} = r_{y_j} \ \& \ x_i \cong y_j. \\
\end{cases}
$$
where $x_i \cong y_j$ means the sentences corresponding to $x_i$ and $y_j$
are semantically-similar. $a_{i,j}= 1$ means aligned rationales while
$a_{i,j}=0$ means misaligned rationales.

Utilizing additional annotations, we focus on improving the LIM of LCM-LAI, as this module primarily extracts legal-rational information and the legal-rational interaction information between cases, which aligns precisely with the focus of the additional annotations.
On the one hand, for the rationale label, taking case $X$ as an example, we use the value vector $\mathbf{v}_{i}^{X}$ (c.f., Eq.\ref{eq: sub_attention_sum} and Eq.\ref{eq: legal_inter_rep}) of each sentence $x_i$ to predict the corresponding rationale label.
Because LCM-LAI directly weighted sums value vectors of all sentences to predict applicable law labels, i.e., 
value vectors directly reflect the legal-rational information extracted by LCM-LAI.
As for the specific implementation, we use a simple linear classifier to predict the rationale labels, whose formula is shown as follows:
\[
\{ P(r=t | x_i) \}_{t=0}^{3} = \text{softmax}(\mathbf{W}_{r}\mathbf{v}_i^X + \mathbf{b}_{r} )
\]
where the softmax converts a 4-dimensional vector to a distribution over four rationale classes, $\mathbf{W}_{r}$ and $\mathbf{b}_{r}$ are trainable parameters.
We select the rationale class with the max probability as our prediction, i.e., 
$\hat{r}_{x_i} = \arg \max_{t\in \{0, \cdots, 3\}}P\left ( r=t| x_i\right)$.
Thus, following the practice of IOT-Match~\cite{yu2022Explainable}, for the case pair containing $X$ and $Y$, we also select the cross-entropy loss between the ground-truth rationale labels of each sentence and the corresponding predictions as the rationale identification loss $\mathscr{L}_{rationale}$.
The specific formula is as follows,
\[
    \mathscr{L}_{rationale} = \sum_{t=0}^{3}\left( 
    \sum_{i=1}^{n_x} \delta(r_{x_i}, t) \log \left(P (\hat{r}_{x_i}=t | x_{i}) \right)
    +  \sum_{j=1}^{n_y} \delta(r_{y_j}, t) \log \left(P(\hat{r}_{y_j}=t | y_{j})\right)
    \right),
\]
where $\delta(r, t) = 1$ if $r = t$ else 0. 
On the other hand, as the legal correlation matrix $\mathbf{C}^{(L)}$ is designed to capture the legal interaction information between cases, we use the label of the alignment matrix $\mathbf{A}$ to constrain it.
Following the IOT-Match, we also treat the KL divergence between $\mathbf{A}$ and $\mathbf{C}^{(L)}$ as the objective function.
Its formula is as follows,
\[
    \mathscr{L}_{align} = \text{KL}( \mathbf{A} || \mathbf{C}^{(L)}) = \sum_{i, j} a_{i,j} \log \frac{c_{i, j}^{(L)}}{a_{i,j}}.
\]
Thus, by adding different enhancing objective functions, we get the following variants:
\begin{itemize}
\item \textbf{LCM-LAI(+ rationale)}: the variant enhanced by using rationale labels, of which the overall loss function is $\mathscr{L} = \mathscr{L}_{a} + \mathscr{L}_{m} + \mathscr{L}_{rationale}$.

\item \textbf{LCM-LAI(+ align)}: the variant enhanced by using alignment matrix labels, of which the overall loss function is  $\mathscr{L} = \mathscr{L}_{a} + \mathscr{L}_{m} + \mathscr{L}_{align}$.

\item \textbf{LCM-LAI(+ both)}: the variant enhanced by using both alignment matrix and rationale labels, of which the overall loss function is  $\mathscr{L} = \mathscr{L}_{a} + \mathscr{L}_{m} + \mathscr{L}_{rationale} + \mathscr{L}_{align}$.
\end{itemize}

According to the experimental results shown in Tab.~\ref{tab: additional annotation},
we can get the following conclusions:

\begin{enumerate}
    \item{
    With the help of the additional labels, LCM-LAI(+ both) achieves better performance than IOT-Match, with Accuracy, Macro-Precision, Macro-Recall, and F1 exceeding $1.24\%$, $1.07\%$, $1.66\%$ and $1.54\%$ respectively.
    }
    \item{
    Both the rationale labels and the align matrix labels indeed improve the performance of our LCM-LAI.
    }
\end{enumerate}


\begin{table*}[t]
\centering
% \color{blue}
\small
\caption{
The matching performance of LCM-LAI on the eCAIL dataset with additional annotations.
}\label{tab: additional annotation}
\begin{tabular}{lcccc}
\toprule
Methods   & Accuracy   & Macro-Precision   & Macro-Recall    & F1\\
 \midrule

IOT-Match          
                    & $ 82.00 $     & $ 82.10 $     & $ 81.92 $     & $ 81.90 $     \\

$\textbf{LCM-LAI}$    
                    & $ 81.07 $ & $ 81.01 $ 
                    & $ 81.23 $ & $ 80.89 $\\
 
$\textbf{LCM-LAI(+ rationale)}$    
                    & $ 82.05 $ & $ 82.03 $ 
                    & $ 82.41 $ & $ 82.29 $\\

$\textbf{LCM-LAI(+ align)}$    
                    & $ 82.40 $ & $ 81.91 $ 
                    & $ 82.18 $ & $ 81.95 $\\

$\textbf{LCM-LAI(+ both)}$    
                    & $ \textbf{83.24} $ & $ \textbf{83.07} $ 
                    & $ \textbf{83.58} $ & $ \textbf{83.44} $\\
\bottomrule
\end{tabular}%
\end{table*}
