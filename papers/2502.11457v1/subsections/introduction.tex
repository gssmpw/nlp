\section{Introduction}
Controlled text simplification considers audience-targeted attributes when generating simplified texts, so that the generated texts do not only meet the criteria of simplicity, but also preserve desired attributes for the targeted audiences. 
Recent studies on controlled text simplification aimed to help reading comprehension for language learners and employed school grade levels annotated in the training corpus as the simplification target \cite{scarton-specia-2018-learning, sheang-saggion-2021-controllable,agrawal-carpuat-2023-controlling} or text features (number of words, character-level Levenshtein similarity etc.) between source and target sentences \cite{nishihara-etal-2019-controllable, martin-etal-2020-controllable}. 

Different from these studies, we aim to aid language learning and education for English as a Second Language (ESL) learners by simplifying sentences while preserving desirable attributes for language acquisition. 
We use the Common European Framework of Reference for Languages (CEFR), the world standard definition of language proficiency. 
Our method is motivated by two classic L2 learning theories: the input hypothesis \cite{krashen1981second} and frequency effect \cite{ellis2002frequency}. 
The input hypothesis stated that in order for the language acquisition to happen, the textual input which is either too simple or too complex for learner comprehension will not be useful for acquisition. If a learner’s current competence is $i$, then comprehensible input should contain both $i$ and $(i + 1)$ content \cite{mitchell2019second}. Frequency theory holds that the frequency of the words and phrases in the input is a key determinant of acquisition, and words with higher frequency in the usage tend to be easier to acquire \cite{ellis2009construction}. 
The key challenge here is the lack of a parallel corpus for training that should provide complex-simple sentence pairs labelled their levels. 
Parallel sentences of this kind are naturally scarce, and worse, annotation of difficulty levels, in particular, CEFR, is non-trivial and requires language education experts~\cite{arase-etal-2022-cefr}.  


To achieve sentence simplification for aiding language learning without a parallel corpus, we propose reinforcement learning on a pre-trained large language model (LLM). 
% method to control the simplification outputs according to the learners’ language ability without the need for a parallel corpus. 
Based on the aforementioned L2 learning theories, the proposed method simplifies the complex sentences to the one corresponding to the learner's proficiency level or one level higher ($i$ and $i+1$ levels) and increases the coverage (frequency and diversity) of the corresponding level’s vocabulary in the generated simplifications. 
Specifically, we reformulate the controlled simplification task as a lookahead search problem: in the decoding step $t$, the model searches for the token that satisfies the target vocabulary constraint while also ensuring that future tokens 
%in $t+1$ to $t_{EOS}$ 
increase the target vocabulary coverage as much as possible, and the final hypothesis falls into the desired CEFR level. 
We combine a simple word-match-based heuristic with the supervised sentence-level signal to guide decoding and train the model iteratively using gradient policy optimization to memorize the search strategy that maximizes the overall reward. 
Remarkably, we eliminate the need for a parallel corpus by utilizing LLMs' language generation capacity for simplification via reinforcement learning. 
% In this process, only target-audience-related attributes are required and the need for complex-simple parallel sentences with CEFR annotation is eliminated. 
Experimental results show that the method significantly increases the coverage and diversity of the target vocabulary in the outputs by up to $20\%$ compared to the baselines, while maintaining high simplification quality.

Our primary contributions are twofold. 
First, we propose the sentence simplification method that aligns generated simplifications with ESL learners’ proficiency level on word, phrase and sentence levels and preserves attributes effective for facilitating language learning. 
Second, our method is easy to deploy and does not require a parallel corpus that is often expensive to create. 

