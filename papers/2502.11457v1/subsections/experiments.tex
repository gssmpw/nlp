\section{Experiment Settings}
%
We aim to evaluate the effectiveness of the proposed model in generating high-quality simplifications that align with the target vocabulary and sentence-based CEFR level. 
This section provides details of the experiment settings.
%We also provide descriptions of the resources used for training and evaluation, the implementation details, and the evaluation metrics.

\subsection {Resource and Implemetation}\label{sec:resource}

 \begin{table}[t!] \centering 
 \begin{adjustbox}{width=0.9\linewidth}
\begin{tabular}{lrrrrrr} 
\toprule & A1 & A2 & B1 & B2 & C1 & C2 \\ 
\midrule Train & 248 & 1284 & 2479 & 2226 & 889 & 52 \\
Val & 79 & 276 & 485 & 336 & 149 & 40 \\ 
Test & 71 & 289 & 540 & 369 & 150 & 39 \\ 
\bottomrule 
\end{tabular} 
\end{adjustbox}
\caption{Statistics on CEFR-SP w/o Newsela} 
\label{tab:cefrsp}
 \end{table}

 \begin{table*}[t!]
    \small
    \centering
   \begin{adjustbox}{width=0.9\textwidth}
    \begin{tabular}{l L L L L L L L} \hline
     \textbf{CEFR-SP} &  \text{A-Frequency} & \text{A-Diversity} & \text{B-Frequency}  & \text{B-Diversity} & \text{C-Frequency}  & \text{C-Diversity}     \\ \hline
     Reference & 0.292 & 0.527 & 0.283 & 0.465  & 0.080 & 0.102  \\ 
      phi3-3b-vanilla &0.252 & 0.665 & 0.215 &0.435  &0.041 & 0.172  \\ \hline
       T5+grade-A & 0.194 & 0.438 & 0.269 & 0.271  &0.072 & 0.114  \\
       FUDGE-A & 0.257 & 0.215 & 0.207 & 0.069  &0.043 & 0.018 \\
       \textbf{phi3-A} & \textbf{0.299}  & \textbf{0.684} & 0.196 & 0.403  & 0.038 & 0.141 \\ \hline
       
       T5+grade-B & 0.204 & 0.447 & \textbf{0.275} & 0.266  &0.069 & 0.110  \\
       FUDGE-B & 0.223 & 0.226 & 0.231 & 0.084  &0.049 & 0.027 \\
        \textbf{phi3-B} & 0.151 & 0.677 & 0.262 & \textbf{0.538}  & 0.064 & 0.251 \\ \hline
        
      T5+grade-C & 0.203 & 0.441 &  0.276  &0.271 & 0.074 & 0.114 \\
       FUDGE-C  & 0.239 & 0.217 & 0.220 & 0.077  &0.052 & 0.025 \\
        \textbf{phi3-C} & 0.171 & 0.658 & 0.263 & 0.275  & \textbf{0.189}  & \textbf{0.365} \\ 
         \hline
    \end{tabular}
   \end{adjustbox}
    \caption{Results on target attribute controllability on CEFR-SP-Test. For ``Reference'', the frequency and diversity metrics were calculated using a subset of each grade level to show distributions in sentences of specific levels. }
    \label{tab:cefr1}
\end{table*}

\paragraph{Sentence CEFR Level} To train the sentence-level reward model,
we used CEFR-SP \cite{arase-etal-2022-cefr}, which provides labels of six CEFR levels for a total of $17k$ sentences annotated by experts.  
We only used the publicly available subset from the dataset (excluding data based on Newsela \cite{xu-etal-2015-problems}), which resulted in $10k$ sentences with labels.
The statistics of the dataset are described in Table\ref{tab:cefrsp}. %Appendix \ref{sec:appendix-training}. 
We fine-tuned the  GPT-2 \cite{radford2019language} using the annotated CEFR levels. 


\paragraph{Vocabulary List}
For the lexical constraint reward model, we need vocabulary lists per CEFR level. 
We downloaded the English Vocabulary Profile (EVP) data\footnote{https://www.englishprofile.org/wordlists/evp} and used it as a dictionary of words and phrases annotated with their corresponding CEFR levels\footnote{EVP assigns CEFR levels to each word sense, so the same word can appear at different levels depending on its meaning. For simplicity, we did not conduct word sense disambiguation and assigned the lowest level.}.
% To calculate the match counts for lexical constraint reward, we remove basic stop words from the sentence, and lemmatize both words in the sentence and in the list. 
Since our goal is to generate the simplifications in $i$ and $i+1$ levels, we always aggregate the vocabulary lists in two levels. For clarity, we consider A1$+$A2, B1$+$B2, and C1$+$C2 levels. In total, we got $1076$ words for A level, $3823$ words for B level, $3612$ words for C level. 

\paragraph{Complex Sentence Collection}
We trained the policy model to iteratively learn to search for a hypothesis that maximizes rewards based on its own generations. 
The only requirement for our training corpus is a supply of complex sentences that warrant simplification, because sufficiently simple sentences without the need for simplification may disturb the learning. 
\citet{cegin-etal-2023-chatgpt} showed that large language models are highly capable of paraphrasing. 
Following this study, we used GPT-4\footnote{https://openai.com/index/gpt-4/} to synthesize complex sentences from the CEFR-SP training set to create our training corpus. 
We manually prepared prompts to ensure that the outputs are always at least as complex as the highest C2 level. 
More details are in Appendix \ref{sec:appendix-complex}.

We trained separate models for A, B and C levels since different levels require different rewards (see Section~\ref{sec:policy_model}). 
For computational efficiency, we adopted a relatively small Phi-3-mini-3b model \cite{abdin2024phi}. 
% The complex sentences are generated from the CEFR-SP dataset using GPT-4 \footnote{https://openai.com/index/gpt-4/}.
% Before training, a GPT-2 model \cite{radford2019language} was trained on the CEFR-SP sentence labels as the sentence level reward model; during training, only the complex sentences are used as inputs.  
More implementation details can be found in Appendix \ref{sec:appendix-training}.


\subsection{Evaluation Datasets}
To evaluate the simplification outputs, we need parallel corpora of complex and reference simple paraphrases. Below describes the resources we used for the evaluation.

\paragraph{CEFR-SP-Test}
As the formal evaluation dataset, we used CEFR-SP. 
We expanded its test set to be parallel because CEFR-SP is a non-parallel corpus. 
Specifically, we generated complex sentences for each sentence in the CEFR-SP test set using the same method described in Section~\ref{sec:resource}. 
These complex sentences were input to models, and outputs were evaluated by comparing them to the original CEFR-SP sentences as references.


\paragraph{TurkCorpus} To assess the applicability of the proposed method for a general simplification task, we also evaluated models on another widely-used dataset, TurkCorpus \cite{xu-etal-2016-optimizing}. We used the test set of the corpus, including $359$ complex sentences, each has $8$ human-written simplified sentences as references. 
It should be noted that TurkCorpus does not provide any level annotations. 
% The evaluation on the TurkCorpus is zero-shot: the models trained on CEFR-SP complex sentences are directly evaluated using TurkCorpus test set.



\subsection{Evaluation Metrics}
We evaluated simplification outputs from two perspectives: \textbf{simplification quality} and \textbf{target audience attributes} by both automatic and human assessments. 
Simplification quality was assessed across three dimensions: \textbf{Simplicity; Fluency; and Adequacy}.         
As automatic metrics for simplicity, we employed LENS \cite{maddela-etal-2023-lens} and SALSA \cite{heineman-etal-2023-dancing}, which are two recently proposed model-based evaluation methods. 
For fluency and adequacy, we employed an instruction-tuned language model as an off-the-shelf evaluation model, which was shown to be effective in automatic translation quality evaluations \cite{kocmi-federmann-2023-large}. 
Target audience attributes were measured in terms of \textbf{target vocabulary coverage} and \textbf{sentence CEFR level},
% Vocabulary coverage includes both frequency and diversity of target vocabulary. 
in which vocabulary coverage includes both frequency and diversity of target vocabulary. 
% For determining sentence CEFR-level, we reused our reward model. 
For the evaluation of sentence CEFR-level, we used human evaluation. 
% For sentence level, we measure the absolute level using a classification model trained on the CEFR-SP dataset, and report the binary accuracy as the sentence level score. 
% Nonetheless, human evaluation is crucial to assess the quality of simplification. 
For more details on evaluation metrics, please refer to Appendix \ref{sec:appendix-metrics}.




\subsection{Baselines}
Overall, we choose two lines of work as the baselines for comparison.  

\paragraph{Controlled Simplification}
There are limited variants in controlled simplification methods which mostly employ control tokens with supervised learning. 
Based on previous literature, we implemented two baselines for controlling the target level of the simplified texts: a supervised baseline of T5+grade \cite{scarton-specia-2018-learning} that attaches CEFR levels as control tokens and an unsupervised baseline of FUDGE that uses a discriminator at decoding time \cite{yang-klein-2021-fudge}.


% \textbf{Direct simplification}
\paragraph{Non-controlled Simplification}
The Turk corpus was used to evaluate the effectiveness of the proposed method in general simplification. 
As opposed to controlled simplification, this task does not consider controlling attributes, such as grade levels, during the simplification.
% For this line of models, we choose various methods presented in the EASSE package \cite{alva-manchego-etal-2019-easse} to compare with the proposed method on TurkCorpus to demonstrate the method's robustness.
For this line of models, we choose the following methods: DRESS \cite{zhang-lapata-2017-sentence}, DMASS \cite{zhao-etal-2018-integrating}, EditNTS \cite{dong-etal-2019-editnts}, ACCESS \cite{martin-etal-2020-controllable}, IterativEdit \cite{kumar-etal-2020-iterative}. 
We used outputs of these models shared in the EASSE package \cite{alva-manchego-etal-2019-easse}.   
In addition, we also compare the vanilla phi3-3b instruction-tuned model as a baseline, under zero-shot setting without fine-tuning on simplification. 






\begin{table*}[t!]
    \small
    \centering
   \begin{adjustbox}{width=0.9\textwidth}
    \begin{tabular}{l L L L L L L L} \hline
     \textbf{TURK} & \text{A-Frequency} & \text{A-Diversity} & \text{B-Frequency}  & \text{B-Diversity} & \text{C-Frequency}  & \text{C-Diversity}    \\ \hline
     Reference & 0.176 & 0.229 & 0.227 & 0.132  & 0.056 & 0.046  \\ 
     phi3-3b-vanilla &0.166 & 0.180 & 0.177 &0.083  &0.034 & 0.023 \\\hline
       T5+grade-A & 0.187 & 0.180 & 0.217 & 0.088  &0.051 & 0.028 \\
         
        FUDGE-A & 0.175 & 0.177 & 0.175 & 0.069  &0.034 & 0.018 \\
       
       \textbf{phi3-A} & \textbf{0.216} & \textbf{0.208} & 0.153 & 0.063  & 0.031 & 0.018 \\ \hline
       
       T5+grade-B & 0.201 & 0.190  & 0.217  &0.085 & 0.052 & 0.028 \\
    
    FUDGE-B & 0.163 & 0.177 & 0.178 & 0.077  &0.039 & 0.022 \\
       
        \textbf{phi3-B} & 0.126 & 0.201 & \textbf{0.330} & \textbf{0.112}  & 0.066 & 0.035 \\ \hline
        
      T5+grade-C & 0.187 & 0.194  & 0.225  &0.090 & 0.050 & 0.026 \\
       
       FUDGE-C  & 0.171 & 0.174 & 0.181 & 0.076  &0.037 & 0.019 \\        
       
        \textbf{phi3-C} & 0.151 & 0.178 & 0.193 & 0.092  & \textbf{0.091}  & \textbf{0.041} \\  \hline
        
    \end{tabular}
   \end{adjustbox}
    \caption{Results on target attribute controllability on TurkCorpus }
    \label{tab:turk1}
\end{table*}

\begin{table*}[t!]
    \centering
    \begin{tabular}{p{0.97\textwidth}}\hline
    \textbf{Complex Sentence}~~The considerable distance, compounded by Jamie's current condition of pregnancy, which inexorably engenders a state of increased fatigue, renders the prospect of ambulation to said location prohibitively challenging for her.\\\hline
\textbf{Ref. (level B)}~~It is too far for Jamie to walk to, especially because she is pregnant and easily exhausted.\\\hline
\textbf{Simplifications}  \\
Level A: Jamie is \textcolor{blue}{\textit{too}} \textcolor{blue}{\textit{tired}} \textcolor{blue}{\textit{to}} walk far because she is pregnant.\\

Level B: Jamie's pregnancy makes it very hard for her to walk to the location \textcolor{blue}{\textit{due to}} the long distance.\\

Level C: Jamie's pregnancy leads to \textcolor{blue}{\textit{fatigue}}, making it hard for her to walk to the distant place.\\\hline
    \end{tabular}
    \caption{A randomly selected example from the simplification result of the proposed method. The target vocabulary of the corresponding level is marked in \textcolor{blue}{\textit{italic}} font.}
     \label{tab:result demo}
\end{table*}


\section{Experiment Results}
This section analyses experiment results of automatic and human evaluations, and ablation study.  
\subsection{Automatic Evaluation Results}



% \subsection{Result Analyses}
\paragraph{Target Attributes }
Tables \ref{tab:cefr1} and \ref{tab:turk1} show the evaluation results for the target vocabulary coverage. 
These results demonstrate that 
compared to the baseline models, the proposed model significantly increases the frequency of target vocabulary in simplified sentences while also improving vocabulary diversity. 
Notably, the proposed method successfully increases the frequency and diversity of A and C-level vocabulary, which should be harder than B-level due to the scarcity of level A and C samples~\cite{arase-etal-2022-cefr}. 

\paragraph{Simplification Quality }
Tables \ref{tab:cefr2} and \ref{tab:turk2} show the evaluation results for the simplification quality. 
Overall, these results indicate that our models can produce high-quality simplifications, greatly outperforming the baseline models. 
Remind that our model does not have reward to encourage the model to follow the adequacy requirement.
% We attribute this to the benefits of using KL constraints and updating the policy model under the reference model's constraints.
We attribute these improvements to the benefits of using entropy regularization imposed by the reference model that allows the preservation of the high paraphrasing capability of LLMs.
% which ensure that the generation model do not deviate too much from a baseline generation.  
Table~\ref{tab:result demo} shows a randomly picked example of simplification by our method; Appendix~\ref{sec:appendix-results} provides more. 





\subsection{Human Evaluation Results} \label{sec:human}
We perform a human evaluation to assess the simplification quality from human perspectives. 
We recruited three graduate-level students majoring in linguistics to perform the evaluation. 
The evaluators were first trained with the background knowledge and then given a guideline to evaluate the following aspects of the samples: fluency, simplicity, adequacy, and CEFR sentence level. 

We asked annotators to make binary judgements for fluency, simplicity, and adequacy. 
For sentence level, because CEFR-level judgements require expertise in language education, we simplified the task to collect reliable decisions. % to assess the model's ability to control simplification among different CEFR levels, 
We asked the evaluators to judge if a simplified sentence matches the desired sentence level (denoted as ``Level''). 
We showed a reference with its CEFR level and requested the evaluators to judge if the model output matches the reference's simplicity. 
% The results are shown in the "Level" column.
In addition, we asked them if a simplification output is preferable in terms of its CEFR level compared to the one generated by a model targeting a different level (denoted as ``Prefer''). 
For example, an evaluator judges if an output of the A-level model is preferable to that of the C-level compared to the A-level reference.\footnote{Preference score for the reference was judged by comparison with a sentence randomly chosen from another level} 
%, supposing that the task is an A-level simplification.
%newly added description here%%%%%%
% The results are shown in the "Prefer" column.
%%%%%%%%%%%
For each CEFR level, $30$ simplifications of the CEFR-SP-Test were randomly sampled and annotated ``Level'' and ``prefer'' judgements. 
We report the ratios of positive judgements as evaluation scores. 
% Evaluators were asked to choose ``yes'' or ``no" for each evaluation, and the resulting binary accuracy for the baseline and proposed models are presented in Table \ref{tab:human}. 
The details of the annotation guideline and interface are presented in Appendix \ref{sec:appendix-humaneval}. 

% \begin{table}[h]
%     \small
%     \centering
%    \begin{adjustbox}{width=1\linewidth}
%     \begin{tabular}{l c c c c c} \hline
%     Model & Simpicity  & Adequacy  & Fluency  & Prefer & Level \\ \hline
%      phi3-A & 1.0 &  0.76 & 0.90 & 0.67 &0.83 \\ 
%        phi3-B & 1.0 & 0.83 & 0.90 &0.70 &0.63 \\
%        phi3-C & 0.96 & 0.80 & 1.0 &0.80 &0.6 \\ \hline
%     \end{tabular}
%    \end{adjustbox}
%     \caption{Average human evaluation results.  Level denotes the match of level between reference sentence.}
%     \label{tab:human}
% \end{table}

\begin{table}[t!]
    \small
    \centering
   \begin{adjustbox}{width=1\linewidth}
    \begin{tabular}{l L L L L} \hline
    \textbf{CEFR-SP} & \text{LENS}  &  \text{SALSA}  &  \text{Fluency}  &  \text{Adequacy}    \\ \hline
     Reference & 43.57 &  59.54 & 0.829 & 0.624 \\ 
      phi3-3b-vanilla  & 63.37 & 74.18 & 0.897 & 0.538 \\ \hline
      
       T5+grade-A & 41.37 & 58.98 & 0.547 &0.291 \\
       
    FUDGE-A & 60.84 & 70.16 & 0.780 & 0.447 \\
    
    \textbf{phi3-A} & \textbf{67.29} & \textbf{76.23} & \textbf{0.827} & \textbf{0.604} \\ \hline
    
       T5+grade-B & 40.15 & 58.43 & 0.535 &0.290 \\
    FUDGE-B & 53.33 & 68.69 & 0.823 & 0.540 \\
    \textbf{phi3-B} & \textbf{64.61} & \textbf{72.21} & \textbf{0.871} & \textbf{0.768} \\ \hline

    
      T5+grade-C & 41.67 & 59.12 & 0.538 &0.277 \\
      
       FUDGE-C & \textbf{60.50} & 70.48 & 0.830 & 0.473 \\
       
        \textbf{phi3-C} & 57.06 & \textbf{70.93} & \textbf{0.913} &\textbf{0.615} \\
         \hline
    \end{tabular}
   \end{adjustbox}
    \caption{Simplification quality on CEFR-SP-Test per levels; T5-grade, FUDGE and proposed method were evaluated using subsets of specific levels (A, B and C level references, respectively).}
    \label{tab:cefr2}
\end{table}


\begin{table}[t!]
    \small
    \centering
   \begin{adjustbox}{width=1\linewidth}
    \begin{tabular}{l L L L L} \hline
     \textbf{TURK} & \text{LENS}  &  \text{SALSA}  &  \text{Fluency}  &  \text{Adequacy}     \\ \hline
     Reference & 35.20 &  64.96 & 0.732 & 0.901 \\ \hline
       ACCESS  & 49.90 & 62.68 & 0.576 & 0.780 \\
       DMASS   & 46.52 & 58.97 & 0.515 & 0.665 \\
      DRESS  & 59.76 & 62.63& 0.807 & 0.615 \\
      DRESS-LS   &60.56 & 62.92 & 0.838 & 0.657 \\
        EditNTS   &57.71 & 64.86 & 0.752 & 0.710 \\
       IterativEdit   & 37.35 & 49.74 & 0.409 & 0.607 \\
      
     \hline
        phi3-3b-vanilla  & 65.08 & 71.93 & 0.830 & 0.807 \\
        \textbf{phi3-A} & 64.92 & \textbf{73.68} & 0.720 & 0.708 \\
        \textbf{phi3-B} & \textbf{70.25} & 69.05 & 0.855 & \textbf{0.952} \\
        \textbf{phi3-C} & 62.24 & 70.43 & \textbf{0.869} &0.872 \\
         \hline
    \end{tabular}
   \end{adjustbox}
    \caption{Simplification quality on TurkCorpus; all models evaluated on the entire sentences as TurkCorpus does not annotate levels.  }
    \label{tab:turk2}
\end{table}


Table~\ref{tab:human} shows the results; the simplicity score is generally high, close to $1$, across models. 
This is expected as the source sentences were generated to be highly complex. %the model simplifies complicated sentences which are designed to be generated as highly complex. 
The adequacy measurement results are consistent with automatic evaluation; identifying our proposed models as the most adequate. 
Furthermore, the proposed method achieves the best controllability on sentence levels compared to the baselines as indicated by significantly higher ``Level'' and ``Prefer'' scores. 



\subsection{Ablation Study}
In this section, we show how each part of the proposed rewards contributes to the final performance. We compare the following models: vanilla phi3 model, reward using only target vocabulary counts, reward using dynamically adjusted vocabulary coverage rates, and reward using both dynamic vocabulary coverage rate and sentence levels (proposed method). 
The frequency and diversity evaluation results for A and B level models are presented in Fig.~\ref{fig:ablation}. Complete results can be found in Appendix \ref{sec:appendix-results}.
It can be seen that changing the simple match count reward to a dynamically adjusted reward indeed encourages the model to increase the entropy inside the target vocabulary and largely improve the vocabulary diversity. 

\begin{table}[t!]
    \small
    \centering
   \begin{adjustbox}{width=1\linewidth}
    \begin{tabular}{l L L L L L} \hline
    Model & \text{Simplicity}  & \text{Adequacy}  & \text{Fluency}  & \text{Prefer} & \text{Level} \\ \hline
     Reference & 1.00 & 0.89 & 0.99 & 0.87 & $--$ \\
     T5+grade-A & 0.83 & 0.16 & 0.47 &0.40 &0.10 \\
     T5+grade-B & 0.90 & 0.13 & 0.50 &0.43  &0.17\\
     T5+grade-C & 0.80 &0.16& 0.60
     &0.40 &0.17 \\
     FUDGE-A & 1.00 & 0.50 & 0.80 & 0.50 &0.43\\
     FUDGE-B & 0.96 & 0.43 & 0.83 &0.57&0.47\\
     FUDGE-C & 1.00 & 0.47 & 0.83&0.57&0.33\\
     \hline
     Ours phi3-A & 1.00 &  0.76 & 0.90 & 0.67 &0.83 \\ 
     Ours phi3-B & 1.00 & 0.83 & 0.90 &0.70 &0.63 \\
     Ours phi3-C & 0.96 & 0.80 & 1.00 &0.80 &0.60 \\ \hline
    \end{tabular}
   \end{adjustbox}
    \caption{Human evaluation results}
    
    % "Level" denotes if the sentence level of the generated simplification, which was guided by the sentence-level reward model during training, matches the level of the gold reference.}
    \label{tab:human}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{latex/figs/ablations_A-B.pdf}
    \caption{
    Reward effects on target vocabulary coverage
    % Vocabulary targeting w/ different rewards
    }
    \label{fig:ablation}
\end{figure}




