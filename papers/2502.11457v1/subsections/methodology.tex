\section{Problem Definition}\label{sec:method}
% We aim to facilitate language learning by generating text simplifications targeted at English learners with constrained knowledge of the language. In reality, different individuals have varied levels of knowledge for the language. 

We aim to facilitate language learning by simplification targeted at ESL learners. 
% In reality, different individuals have varied levels of knowledge for the language,
In this study, we use CEFR levels as a representative measure for the learners' proficiency and model the target level based on the vocabulary\footnote{https://www.englishprofile.org/wordlists/evp}  (words, phrases, idioms) 
and sentence 
% usage (as indicated in CEFR\footnote{https://www.coe.int/en/web/common-european-framework-reference-languages/table-1-cefr-3.3-common-reference-levels-global-scale}). 
CEFR levels\footnote{https://www.coe.int/en/web/common-european-framework-reference-languages/table-1-cefr-3.3-common-reference-levels-global-scale}. 


Our problem is thus defined as follows. 
We assume that learners know their own CEFR level $i$. 
Given a sentence above the learner's level $i$, we generate its simplified version that (a) contains as much vocabulary of the level $i$ and $i+1$ as possible, and (b) corresponds to the target (learner's) level $i$ at the sentence level.  %matches the level of the generated simplifications to the target levels. 
% based on the judgment determined by expert English teachers. 

\subsection {Constraint Formalization} \label{sec:task}
Generating simplified texts subject to vocabulary constraints can be approached as a lexical-constrained text generation task \cite{zetsu-etal-2022-lexically}. 
Traditionally, lexical constraints in text generation involve a \emph{short} list of required words, which \citet{lu-etal-2021-neurologic} expressed as a Conjunctive Normal Form (CNF), such as $\underbrace{(D_1 \lor D_2 \lor \cdots )}_{C_1} \land \cdots \land \underbrace{(D_{m-1}  \lor D_m)}_{C_m}$ in which $D_m$ stands for a single constraint, and all clauses must be satisfied, imposing hard constraints on the generation process.

In our setting, however, this formulation is no longer applicable because the vocabulary constraint is \emph{as large as the size of the vocabulary of a specific level}. 
In addition, we aim to satisfy \emph{as many clauses as possible}. 
Therefore, we formalize constraints as Disjunctive Normal Form (DNF), indicating words and phrases suitable for the target proficiency level: $D = \underbrace{(D_1)}_{C_1} \lor \underbrace{(D_2 \land D_3 \land \cdots )}_{C_2} \lor \cdots \lor \underbrace{(D_m)}_{C_m}$, where the form stands for the word list of the target language level, a single $D_m$ represents word and the conjunctive clauses represent several words, namely phrases. 
Notably, this form of constraints allows for the control of discontinuous phrases, which is difficult in previous methods. 

\subsection{Optimization Function}
Based on the DNF constraints, our task imposes soft constraints that aim to include as many clauses as possible. 
% For a collection of complex sentences, 
Given the simplification hypotheses $\{ \text{seq}_1, \text{seq}_2, \ldots,\text{ seq}_n \}$, the goal is to maximize:
\begin{equation}\label{eq:goal}
    \sum_{j=1}^{m} \sum_{k=1}^{n} \text{count}(C_j, \text{seq}_k),
\end{equation}
where $\text{count}(C_j,  \text{seq}_k)$ indicates the number of clauses $C_j$ satisfied by $ \text{seq}_k$. 
Consequently, the target during the generation process is to search for the next token that: 

\begin{itemize}[noitemsep,topsep=1pt,parsep=0pt,partopsep=0pt,]
 
    \item simplifies the original text;
    \item is contained in $\exists C_i \in D$; 
    \item leads to future tokens that satisfy $C_i$; 
    \item leads to complete phrases or phrases with slots (discontinuity) that satisfy $C_i$.
\end{itemize}


\section{Proposed Method}
To search for a hypothesis that better satisfies predetermined constraints, some previous methods use rollout in decoding that generates partial future sequences \cite{chaffin-etal-2022-ppl, lu-etal-2022-neurologic}. 
% This may work for smaller models, but they become infeasible for large models. 
These methods become infeasible for large models due to the inefficiency of sampling in decoding time and handling the large vocabulary constraints in our task. %since they apply the lexical constraints in CNF which only considers a small list of lexical terms. 
To effectively and efficiently search for the tokens that satisfy our constraints, we instead consider sampling in the training time and formulate the lookahead search problem using RL search~\cite{fickinger2021scalable} (see Fig.~\ref{fig:framework}). 

\subsection{RL Search}
Consider the text generation process as a Markov Decision Process (MDP), at each timestep $t$ of the generation, the language model observes a state $s_t$, which is the generated partial sequence $ \text{seq}_{t-1}$, and takes an action $a_t$ to choose the token from its vocabulary. When the EOS token is reached, a reward $R$ for the generated sequence is calculated and used to update the model. %given by the heuristic to measure the number of clauses in DNF that the sequence satisfies. 
In this setting, the language model is the policy function that 
searches for a token $v_i \in \mathcal{V}$ where $\mathcal{V}$ is the vocabulary,
% that gives the distribution $\pi(a|s)$ over actions $a$, 
and we can use any policy gradient algorithm to guide the language model to search for the generations that maximize the constraint satisfaction. 
Algorithm~\ref{alg:training} indicates our training procedure. 
% Combining the sentence level reward and the target vocabulary constraints, the process of the proposed method is then as follows:
\begin{algorithm}[t!]
\caption{Training Procedure}
\label{alg:training}
\begin{algorithmic}[1]
% [noitemsep,topsep=1pt,parsep=0pt,partopsep=0pt,]
    \Require Complex sentences;
    \State Generate simplified texts from the complex sentences using current policy (rollout);
     \State Evaluate the current policy and produce rewards to guide the search;
    \State Optimize the policy model using the rewards
    \State Iteratively perform steps 1-3 till converge.
\end{algorithmic}
\end{algorithm}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{latex/figs/architecture.pdf}
    \caption{(better viewed in color) The overall framework of the proposed method: the simplification model is initialized from a pretrained large language model which is also used as a frozen (\includegraphics[height=\fontcharht\font`X]{latex/figs/ac_unit.png}) reference model to provide entropy regularization (part 0.); top-k sampling is adopted in the decoding process to sample varied simplifications for the complex sentence (part 1.a.); the generated simplifications are evaluated based on the language proficiency level (vocabulary level and sentence level) of the target audience, which is used as rewards to update the simplification model (part 1.b.) to adopt better decoding strategy. % to generate simplifications that suit the target users. 
    }
    \label{fig:framework}
\end{figure*}



\subsection{Policy Model}\label{sec:policy_model}
The policy model generates a simplified sentence $\text{seq}$ given a complex counterpart as a prompt $\text{pmt}$. 
The policy model is initialized from an instruction tuned language model, which unsupervisedly provides robust text simplifications \cite{kew-etal-2023-bless}.

By design, the rewards for the policy model across different proficiency levels are varied. For instance, given the same model response, a positive reward for C level could correspond to a negative reward for A level. Therefore, using the original language model as the backbone, we train separate copies of the policy model for A, B and C levels by adding and updating distinct LoRA parameters to the backbone parameters \cite{hu2022lora}, while keeping the backbone frozen. 


\subsection{Reward Models}
Inspired by the L2 learning theories, we design two types of rewards at lexical and sentence levels.

 \subsubsection{Lexical Constraint Reward}
We use a simple heuristic to guide the search for generations that satisfy the lexical constraints:
\begin{equation} \label{eq:cnt}
    H(\text{seq}) = \sum_{C_j \in D} r (\text{count}(C_j, \text{seq})),
\end{equation}
where $C$ is a clause from  $D$, 
% the DNF, 
$r$ denotes the reward according to the number of satisfactions of $C$ in $\text{seq}$, and $H$ denotes the reward score for the generated sequence $\text{seq}$ in the current decoding step.
To calculate the match counts, we remove basic stop words from the sentence after lemmatization. 


% For each count, the reward $r_H$ is set to be 1. 
As a simple baseline, we define $r$ as a constant value $1$ for word and $1.5$ for phrase
% Additionally, $r$ is set to be a higher constant value when C is a phrase, 
to encourage the model to generate more phrases and idioms. 
However, we found that this simple baseline is easily hacked by the model after a few steps of training, i.e., the model only generates a limited set of frequent words that were learnt to produce rewards.
To encourage the model to explore more diverse words and improve the overall coverage of target-level words in the generations, the reward should intuitively encourage maximizing the entropy for the clauses in $D$, 
so that all the clauses are evenly distributed. Accordingly, we adjust the reward $r$ for the count of $C_j$ as a \textbf{dynamic reward}:
\begin{equation}\label{reward}
     r = \begin{cases} 1 & \text{if } 0 \le p_j < \frac{1}{m}, \\ e^{-\alpha p_j} & \text{if } \frac{1}{m} \le p_j \le 1, \end{cases} 
\end{equation}
where $p_j$ is:
\begin{equation}\label{cnt_c}
    p_j = \frac{\sum_{k=1}^{n} \text{count}(C_j, \text{seq}_k)}{\sum_{k=1}^{n} \sum_{j=1}^{m} \text{count}(C_j, \text{seq}_k)},
\end{equation}
to discourage the model from exploiting the same clause. 
Here, $m$ denotes the total number of clauses in $D$ and $\alpha$ is a constant to adjust the penalty degree for too frequent clauses. 
Eq. \ref{cnt_c} is calculated after each epoch and the reward is adjusted accordingly.
If matched clauses are above the target level, we give a constant negative score of $-1$.

    


\subsubsection{Sentence Level Reward}    
To go beyond words and guide the simplification model’s search for a sentence of the target level, we incorporate a sentence-level reward model by simulating human experts’ judgment for the sentence’s CEFR level. 
We use pairwise ranking loss to train the reward model,
since the class distribution for the CEFR-SP data is imbalanced \cite{arase-etal-2022-cefr}. %, training a classification model using CE would 
% amplify the problem. 
%bias the model towards the majority class.
The ranking loss has been shown to be able to encourage the model to only pay attention to the focused class \cite{henning-etal-2023-survey}, thus may mitigate the class imbalance problem. 

Consequently, we construct sentence pairs prioritizing the level we focus on generating: for a collection of sentences $\mathcal{S} = \{s_1, s_2, \ldots, s_n\}$, each sentence $s_i$ is evaluated by human experts and annotated with a language level $l$. Given the level we want to generate, we select the sentences with the target level $\mathcal{S}_{\text{tgt}} = \{ s_i \in \mathcal{S} \mid l_i = \text{level}_{\text{tgt}} \}$, and randomly sample sentences from other levels to construct a negative set $\mathcal{S}_{\text{non-tgt}} = \{ s_j \in \mathcal{S} \mid l_j \neq \text{level}_{\text{tgt}} \}$. Then, we construct sentence pairs $\mathcal{P} = \{ (s_i, s_j) \mid s_i \in \mathcal{S}_{\text{tgt}}, s_j \in \mathcal{S}_{\text{non-tgt}} \}$ by randomly selecting from $\mathcal{S}_{\text{tgt}}$ and $\mathcal{S}_{\text{non-tgt}}$. 

Notably, we do not require the pair to be parallel; they just need to be at different levels. %, that is, to be different simplified versions of the same sentence. 
By this design, we disentangle the adequacy requirement for the simplification from the target-level search process. 
The former is handled by the underlying LLM, and the latter is dealt with by the reward model by level judgment.

With the constructed sentence pairs, we train a sentence-level reward model $r_\theta$. The training objective is to minimize loss: 
\begin{equation}
    \mathcal{L}(\theta) = - \sum_{(s_i, s_j) \in \mathcal{P}} \log \sigma(r_\theta(s_i) - r_\theta(s_j))
\end{equation}
where $\sigma$ is the sigmoid function. 
After training the reward model, for a generated sentence $\text{seq}$, we take $r_l = \sigma(r_\theta(s))$ as the reward, and use a linear combination of the lexical reward and the sentence-level reward as the overall reward: 
\begin{equation}
    R = \lambda r + \gamma r_l
\end{equation}

% $r = ar+br_l$. 

\subsection{Stabilized RL Training}
The original instruct-tuned model is used as a frozen reference model, providing an entropy regularization for the updated policy model to ensure training stability during the search process. 
Specifically, the simplification $\text{seq}^\prime$ produced by the 
% vanilla model 
frozen backbone model
$f^\prime$ is added as an entropy regularization to the overall reward:
\begin{equation}
    R^\prime = R-\log(p_{(f(\text{seq}|pmt)}/p_{(f^\prime(\text{seq}^\prime|pmt))}).
\end{equation}
By doing so, we may keep the LLM's strong paraphrasing ability while letting it acquire controllability in CEFR levels.   

The policy model $f$, namely the simplification model is then updated to search for the generations that maximize the reward. In this study, we adopt Proximal Policy Optimization \cite{schulman2017proximal} to update the policy model, which achieves stable training and faster convergence.


