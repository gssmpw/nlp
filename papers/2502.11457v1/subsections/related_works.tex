\section{Related Work}
We briefly summarize two lines of simplification methods, controlled simplification and reinforcement learning based simplification. %, in the following part.


% \textbf{Control token/Soft prompting} 
\paragraph{Controlled Simplification} attaches tokens or prompts to the input %to represent attributes. Consequently, the tokens can be used 
to control the simplification-related attributes during generation \cite{yang-etal-2023-tailor,agrawal-carpuat-2023-controlling, sheang-saggion-2021-controllable,martin-etal-2020-controllable, martin-etal-2022-muss, scarton-specia-2018-learning, chi-etal-2023-learning}. 
While these methods learn to control levels of simplified sentences using a parallel corpus with annotated difficulties, our method controls attributes useful for language learning without a parallel corpus. 
As opposed to the training-time controlling, \citet{kew-ebling-2022-target} adopted FUDGE \cite{yang-klein-2021-fudge}, which adjusts the logits of the text generation model during decoding using a classifier, to directly control the attribute of the simplification in the decoding time. 


\paragraph{Reinforcement Learning based Simplification} has explored
% unsupervised 
controllability by defining rewards based on simplicity-related criteria %and dynamically determining the trade-offs for these objectives during generation using reinforcement learning (RL) 
\cite{zhang-lapata-2017-sentence,guo-etal-2018-dynamic,nakamachi-etal-2020-text,laban-etal-2021-keep}. The rewards for the objectives are constructed using supervised or unsupervised evaluation metrics for simplicity, adequacy and fluency. 
In contrast, we aim to control attributes useful for language learning and education. 
Furthermore, while RL tends to suffer from unstable training and sensitivity to the choice of hyperparameters, our method achieves training stability by adopting entropy regularization in the model optimization process and introducing a dynamic reward that adjusts based on the data distribution.   

