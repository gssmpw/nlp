% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
% \PassOptionsToPackage{hyphens,spaces,obeyspaces}{url}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{enumitem}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\usepackage{array}   % for \newcolumntype macro
\newcolumntype{L}{>{$}c<{$}} % math-mode version of "c" column type
% \usepackage[table]{xcolor}
\usepackage{float}
% \usepackage{dblfnote} 
\usepackage{tablefootnote}



% \usepackage{environ}         % provides \BODY
% \usepackage{etoolbox}        % provides \ifdimcomp


% \newlength{\myl}
% \let\origequation=\equation
% \let\origendequation=\endequation

% \RenewEnviron{equation}{
%   \settowidth{\myl}{$\BODY$}                       % calculate width and save as \myl
%   \origequation
%   \ifdimcomp{\the\linewidth}{>}{\the\myl}
%   {\ensuremath{\BODY}}                             % True
%   {\resizebox{\linewidth}{!}{\ensuremath{\BODY}}}  % False
%   \origendequation
% }






% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Controlling Text Simplification with Language Learner's Proficiency Level}
\title{Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Guanlin Li\textsuperscript{1,2}},
 \textbf{Yuki Arase\textsuperscript{2}},
 \textbf{NoÃ«l Crespi\textsuperscript{1}}\\
 \textsuperscript{1}Samovar, Telecom SudParis,
 Institut Polytechnique de Paris, France\\
 \textsuperscript{2}School of Computing, Institute of Science Tokyo, Japan\\
 \texttt{ \{guanlin\_li, noel.crespi\}@telecom-sudparis.eu, arase@c.titech.ac.jp}
}

 % \textsuperscript{1}Samovar, Telecom SudParis,
 
 % Institut Polytechnique de Paris \\ France \\
 % \textsuperscript{2}School of Computing \\
 % Institute of Science Tokyo \\ Japan

% \author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
% \\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
% }

\begin{document}
\maketitle
\begin{abstract}


Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. 
This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. 
Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. 
We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. 
Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. 
Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than $20\%$ compared to baseline models, while maintaining high simplification quality. 
\footnote{Codes available at \url{https://github.com/JumpyPizza/align-sentence-simplification-with-ESL-learner}}


\end{abstract}
\input{subsections/introduction}
\input{subsections/related_works}
\input{subsections/methodology}
\input{subsections/experiments}
\input{subsections/conclusions}

\section*{Limitations}
This work assumes the target vocabulary for the learner is accessible, which in reality may not be the case as the target vocabulary varies with learner individuals and has to be first estimated. 
Although it is out of the scope of this paper, this direction constitutes our future work. 
Besides, currently, we do not control the frequency to a specific number, such as $95\%$ $i$ level and $5\%$ $i+1$ level, which is an important aspect to consider according to the L2 learning theory. 

The control for target vocabulary and sentence is implemented individually for different levels rather than using one model altogether, causing heavier computational loads. 
In future, we seek to improve the design of reward model to integrate rewards for different proficiency levels into one model, and explore for a finer control over the frequency of the generated vocabulary.


% \bibliography{anthology, latex/custom}
\input{acl_latex.bbl}


\appendix

\section{Evaluation Metrics}\label{sec:appendix-metrics}
In this section, the evaluation metrics are explained in details: fluency, adequacy, target vocabulary frequency, diversity and target sentence level.

Previous studies have introduced various metrics for evaluating simplicity, which we summarize in Table \ref{tab:metrics}. Among these metrics, SARI \cite{xu-etal-2016-optimizing} is the most commonly employed in the literature. However, recent studies show that SARI may not be an optimal measure for assessing the quality of simplicity \cite{alva-manchego-etal-2021-un, maddela-etal-2023-lens, stodden-etal-2023-deplain}. 
Thus, we chose to use LENS \cite{maddela-etal-2023-lens} and SALSA \cite{heineman-etal-2023-dancing}, two recently proposed metrics, to measure simplicity. 

\begin{table}[h]
    \centering
    \begin{adjustbox}{width=1\linewidth}
    \begin{tabular}{c|c|c} \hline
        Metric & Scope & Reference \\ \hline
        BLEU \cite{papineni-etal-2002-bleu}& semantic similarity & Y \\
        FKGL \cite{kincaid1975derivation} & readability & N \\
        FKBLEU \cite{xu-etal-2016-optimizing} & readability, similarity & Y\\
        SARI \cite{xu-etal-2016-optimizing} & keep, add, delete & Y \\
        D-SARI \cite{sun-etal-2021-document} & keep, add, delete & Y \\
        SAMSA \cite{sulem-etal-2018-semantic}& semantic structural similarity & N \\
        BERTScore \cite{Zhang2020BERTScore} & semantic similarity & Y \\
        SLE \cite{cripwell-etal-2023-simplicity}& human rating + FKGL & N \\
        LENS \cite{maddela-etal-2023-lens} & human rating & Y \\
        SALSA \cite{heineman-etal-2023-dancing}& human rating & N \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Metrics used in recent literature. Scope denotes the aspect that the metric aims to evaluate, and reference indicates whether the metric is computed based on references or not.}
    \label{tab:metrics}
\end{table}


For adequacy and fluency, the ideal approach is human evaluation; however, this is impractical due to the large dataset size. Instead, we employed large language models to assess these two aspects. With a capable language model $f$, the generated simplification sentence $s$ is evaluated as:
\begin{equation}
    score(s) = \sum_{v \in V_y }f(v \mid (pmt, s)
\end{equation}
where $pmt$ is a prompt designed for the model to output "yes" if the model evaluate $s$ to be adequate or fluent, and $V_y$ is a vocabulary subset for "yes" with $ V_y = \{\text{YES}, \text{Yes}, \text{yes}\}$. We use Llama-3-8b-instruct\footnote{https://llama.meta.com/llama3/} model as the evaluation model in our experiment.

To measure target vocabulary frequency, we took the ratio between the total count of matched target words and the total generated words.
% \math
\begin{equation}
      \sum_{j=1}^{m} \sum_{k=1}^{n} \text{count}(C_j,  \text{seq}_k) / \sum_{k=1}^{n}\text{count}(\text{token},  \text{seq}_k)
\end{equation}
To measure vocabulary diversity, we took the ratio between the number of matched words and number of words in the word list.
% \math
\begin{equation}
    \sum_{j \in D} \mathbf{1} \left( \bigvee_{k=1}^{n} \mathbf{1}_{C_j}( \text{seq}_k) \right)/|D|
\end{equation}


\section{Complex Sentence Generations}\label{sec:appendix-complex}
To generate complex sentences, we prompted the GPT-4 model\footnote{https://openai.com/index/gpt-4/} to rephrase sentences of varied levels into highly complex sentences. To ensure the diversity of the generated complex data, we initially created a variety of seed prompts manually and instructed GPT-4 to generate additional prompts based on these seed prompts. GPT-4 was then prompted to generate complex sentences based on these diversified prompts.
The 5 manually written seed prompts and 10 model generated prompts are presented in Table \ref{tab:prompts}. In total 15 prompts were used to generate complex sentences, for each generation, one of the prompts was randomly selected. We present samples of the generated complex sentences together with simplifications in Table \ref{tab:simplification1} and Table \ref{tab:simplification2}.

\section{Training Details}\label{sec:appendix-training}



\paragraph{Implementation Details of Baselines}
We implemented the baseline models using the transformers library\footnote{https://huggingface.co/docs/transformers/en/index}. T5-s2s models require parallel corpus of complex-simple sentences, for which we used the pseudo-parallel sentences of generated complex sentences and their original sentences, and prepended level tokens for level controlling during training and evaluation. We implemented the FUDGE simplification control model with a Llama-3-8b-instruct model as the generation model, and its logits during the inference were adjusted using the CEFR level classification model released by \cite{arase-etal-2022-cefr}.

\paragraph{Implementation Details of Proposed Method}
The PPO training algorithm was implemented using the TRL library\footnote{https://huggingface.co/docs/trl/main/en/index} with a learning rate of $3e-5$. 
For the dynamic reward model used in the training, we set the $\alpha$ to be $1.2$, as we found a value slightly bigger than $1$ was shown to have better performance empirically; the reward for phrases is always set to be $1.5$ times more than words to reward more on the phrase generation.  For the overall reward, $\lambda$ was set to be $1.5$ to compensate for the vocabulary reward penalty, and $\gamma$  was set to 1. During training, we used the following prompt for the model to generate simplifications: ``Given a complex sentence \{\}, generate a simplified version for it:''.

\paragraph{Training Performance}
The performance of the sentence level reward model is shown in Fig.~\ref{fig:reward-performance}. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/figs/reward-performance.png}
    \caption{Sentence level reward model evaluation accuracy}
    \label{fig:reward-performance}
\end{figure}
% Fig. \ref{fig:vocab_all} shows the vocabulary targeting results with different rewards on all three levels of models.
Fig.\ref{fig:reward} shows mean reward and KL change over the training steps with and without the dynamic reward. Objective/KL indicates the deviation of the simplification model from the reference model, and an absurdly high KL indicates model collapse; a burst in mean reward indicates model collapses and only produces a limited set of vocabulary. It can be observed that using the dynamic reward helps stabilize the training, while using the match count alone causes the model to be over-optimized and collapse to a limited vocabulary subset.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{latex/figs/reward_steps.pdf}
    % \caption{KL and Reward Change During Training}
    \caption{Training stability w/wo dynamic reward}
    \label{fig:reward}
\end{figure}


\section{Human Evaluation Details}\label{sec:appendix-humaneval}
In this section, the annotation guidelines that evaluators used to evaluate the generated simplifications as well as the annotation interface are presented. 
The annotation guidelines contain the definition of the aspect to be evaluated, the criteria for the evaluation and indications for the annotation interface. The annotation interface is designed to be a binary-choice form, for each aspect to be evaluated, the evaluators chose to tick to indicate that the simplification contains the aspect to be evaluated, and does not satisfy the aspect otherwise. The evaluation results are then used to calculate the binary accuracy of the aspects to be evaluated. 
The evaluation guidelines are shown in Fig. \ref{fig:guide1} and the evaluation interface is shown in Fig. \ref{fig:interface}.



\begin{figure}[ht]
    \subfigure[Screenshot of annotation guidelines shown to the evaluators]{
    \includegraphics[width=\linewidth]{latex/figs/guideline1.PNG} \label{fig:guide1}}

    
    \subfigure[Screenshot of annotation interface shown to the evaluators]{
    \includegraphics[width=\linewidth]{latex/figs/interface.PNG}
    \label{fig:interface}}
    \caption{Screenshot of annotation guidelines}
\end{figure}


\section{More Evaluation Results}\label{sec:appendix-results}
Fig. \ref{fig:vocab_all} shows the ablation study results on all three levels of models.

Tables \ref{tab:simplification1} and \ref{tab:simplification2} present example outputs: the complex sentences, reference sentences, and simplified sentences. 
For each complex sentences, there are three versions of the simplified sentences, corresponding to A, B and C levels generated by different models targeting the corresponding level, respectively. 
% We also show the original labeled sentence level for the reference sentence.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{latex/figs/ablations-A-B-C.pdf}
    \caption{Vocabulary targeting w/ different rewards, all levels}
    \label{fig:vocab_all}
\end{figure}

        

\begin{table*}[]
    \centering
    \begin{tabular}{{|p{0.9\textwidth}|}} \hline 
     Manual Composed Prompts  \\ \hline
     You are an expert in academic writing, renowned for your ability to compose intricate and sophisticated sentences. Please rephrase the following sentence,so that it's a complex, hard to follow sentence that would usually appear in a journal article, without loss of original meaning: {} \\\hline
     You are an experienced English teacher. Please rephrase the following sentence,to make it a complicated, very hard sentence to read that a English learner may encounter in daily reading, without loss of original meaning: {}\\\hline
    You are a successful postmodernism theater and book critic. You used varied writing styles in your articles. Please rephrase the following sentence,to make it a complex and very difficult to understand sentence,without loss of original meaning: {}\\\hline
    You are a philosopher and literature professor. You usually make intricate perception and sharp insight in your writing.  Please rephrase the following sentence,to make it a short but complex and very hard to follow,without loss of original meaning: {}\\\hline
    You are an editor of social and financial news and journals. Please rephrase the following sentence,so that the sentence has complex compositions and advanced words, that normal readers cannot understand, without loss of original meaning: {}\\ \hline
    Model Generated Prompts  \\ \hline
    You are a legal scholar with extensive experience in drafting complex legal documents. Please rephrase the following sentence,to make it a complex and legally intricate sentence,without loss of original meaning: {}\\\hline
    You are a renowned scientist known for writing dense and comprehensive research papers. Please rephrase the following sentence,to make it a complex and highly technical sentence,without loss of original meaning: {}\\\hline
    You are a seasoned journalist known for crafting elaborate and detailed investigative reports. Please rephrase the following sentence,to make it a complex and deeply investigative sentence,without loss of original meaning: {}\\\hline
    You are a literary critic who writes for a prestigious literary journal, known for your sophisticated language. Please rephrase the following sentence,to make it a complex and highly sophisticated sentence,without loss of original meaning: {}\\\hline
    You are a historian known for your detailed and intricate historical analyses. Please rephrase the following sentence,to make it a complex and historically detailed sentence,without loss of original meaning: {}\\\hline
    You are an expert in technical writing, specializing in creating elaborate and detailed user manuals. Please rephrase the following sentence,to make it a complex and technically detailed sentence,without loss of original meaning: {}\\\hline
    You are a linguist with expertise in creating intricate and multifaceted linguistic analyses. Please rephrase the following sentence,to make it a complex and linguistically intricate sentence,without loss of original meaning: {}\\\hline
    You are a political theorist known for your dense and intricate political analyses. Please rephrase the following sentence,to make it a complex and politically intricate sentence,without loss of original meaning: {}\\\hline
    You are an economist renowned for your detailed and complex economic analyses. Please rephrase the following sentence,to make it a complex and economically detailed sentence,without loss of original meaning: {}\\\hline
    You are a theologian known for your intricate and deeply philosophical theological writings. Please rephrase the following sentence,to make it a complex and theologically intricate sentence,without loss of original meaning: {}\\\hline

\end{tabular}
    \caption{Prompts used to generate complex sentences}
    \label{tab:prompts}
\end{table*}


\begin{table*}[]
    \centering
    \begin{tabular}{|p{0.9\textwidth}|}\hline
       
        Complex Sentence \\
        Let us endeavor to delve into the intricacies and nuances of the text, striving to comprehend the underlying themes and implications inherent within, as we embark on this journey of intellectual exploration.\\\hline
        Reference, level: A \\   
        Let 's try to read .\\\hline
        Simplifications  \\
        Let's try to understand the text well. \\
        Let's try to understand the text's main ideas and meanings as we read.\\
        Let's understand the text's themes and meanings.\\ \hline
        Complex Sentence \\
        Are you experiencing any concerns or apprehensions regarding the composition, quality, or safety of the food you consume?\\\hline
        Reference, level: A \\   
        Do you worry about what you eat ?\\\hline
        Simplifications  \\
        Do you have worries about the food you eat? \\
        Do you have worries about the food you eat's quality or safety? \\
        Do you have concerns about your food's composition, quality, or safety?\\\hline
    %    Complex Sentence \\
    %     In a commendable display of relentless dedication and tireless effort, my husband engages in his professional endeavors with an exceptional degree of industriousness, thereby exemplifying a work ethic that is both remarkable and inspiring.\\\hline
    %     Reference, level: A \\   
    %     My husband works hard .\\\hline
    %     Simplifications  \\
    %     My husband works very hard at his job.\\
    %     My husband works hard and does a great job at his job.\\
    %     My husband consistently shows outstanding work dedication.\\\hline
    %     Complex Sentence \\
    % Within the confines of the structural barriers, an incessant cacophony of scraping sounds revealed the furtive activities of diminutive rodents, whose movements left an indelible mark upon the silent fabric of the edifice.\\\hline
    % Reference, level: A \\   
    % There were mice scratching in the walls .\\\hline
    % Simplifications  \\
    % In a building, small rodents make scratching noises and leave marks.\\
    % Inside the building, small rodents made scratching noises and left marks.\\
    % Structural barriers echoed with rodent noises, leaving noticeable trails.\\\hline
    Complex Sentence \\
The considerable distance, compounded by Jamie's current condition of pregnancy, which inexorably engenders a state of increased fatigue, renders the prospect of ambulation to said location prohibitively challenging for her.\\\hline
Reference, level: B \\   
It is too far for Jamie to walk to , especially because she is pregnant and easily exhausted .\\\hline
Simplifications  \\
Jamie is too tired to walk far because she is pregnant.\\

Jamie's pregnancy makes it very hard for her to walk to the location due to the long distance.\\
Jamie's pregnancy leads to fatigue, making it hard for her to walk to the distant place.\\\hline

Complex Sentence \\
With meticulous care, the young girl positioned the doll on her bed, her actions imbued with a quiet reverence, before softly pressing her lips to its cheek, an act infused with such tenderness that one might easily imagine she believed the doll to be a living boy.\\\hline
Reference, level: B \\   
The little girl placed the doll carefully on her bed and kissed its cheek as gently as if it were a real boy .\\\hline
Simplifications  \\
The girl put the doll on her bed and kissed it gently.\\
The girl carefully placed the doll on her bed, kissed its cheek gently, and seemed to love it like a boy.\\
The girl gently kissed the doll out of affection.\\\hline

    \end{tabular}
    \caption{Complex sentences, references and model simplified sentences in A, B, C levels, respectively}
    \label{tab:simplification1}
\end{table*}


\begin{table*}[]
    \centering
    \begin{tabular}{|p{0.9\textwidth}|}\hline
       
    
        Complex Sentence \\
        Numerous physical characteristics of alkenes and alkanes exhibit remarkable congruity; manifesting as chromatically imperceptible, possessing nonpolarity, and exhibiting combustibility.\\\hline
        Reference, level: C \\   
        Many of the physical properties of alkenes and alkanes are similar : they are colorless , nonpolar , and combustable .\\\hline
        Simplifications  \\
        Alkenes and alkanes have similar physical traits. They are hard to see color differences, not attracted to water, and burn easily.\\
        Alkenes and alkanes have similar physical traits. They are colorless, nonpolar, and burn easily.\\
        Alkenes and alkanes share traits like colorlessness, nonpolarity, and flammability.\\\hline

        By the month of May in the year 2008, the phenomenon known as Rickrolling had ascended to the status of an internet sensation, infiltrating the digital landscape with such pervasive enthusiasm that it swiftly garnered the attention of mainstream media outlets, which began to extensively cover this viral trend.\\\hline
    Reference, level: C \\   
    By May 2008 , Rickrolling had become an internet hit or phenomenon and very soon rickrolling was being covered on mainstream media .\\\hline
    Simplifications  \\
    In 2008, Rickrolling became very popular on the internet and in the news.\\
    In 2008, Rickrolling became a big internet trend.\\
    In 2008, Rickrolling became an internet sensation, leading to widespread media coverage.\\\hline
    
  
    \end{tabular}
    \caption{Complex sentences, references and model simplified sentences in A, B, C levels, respectively}
    \label{tab:simplification2}
\end{table*}




\end{document}
