\section{Related Work}
Most interpretability approaches hold a salient agreement on ``what is interpretable'' and ``what to interpret''. ``What is interpretable'' is influenced by models in physics and mathematics, where operations and derivations are framed around the manipulation of a small set of well-defined symbols. Hence, interpretable concepts are confined to word-level or token-level description, and approaches try to learn a mapping between the neural activities and the target interpretable concept to understand **Bengio et al., "A General Framework for Adding Unsupervised Objectives to Neural Networks"**. %Din et al., 2024; Langedijk et al., 2023; Belinkov,, 2024.

The current approaches on ``what to interpret'' to understand the computations inside a neural network is heavily influenced by neuroscience: either on the level of neurons as a computation unit or in a low-dimensional neural activity descriptions. The earliest interpretability approaches, inspired by neuroscience discoveries such as ``grandmother cells'' and ``Jennifer Aniston neurons'', focused on understanding the semantic meanings that drive the activity of individual neurons. Similarly, studies in artificial neural networks, from BERT to GPT, have identified specific neurons and attention heads whose activations correlate with semantic meanings in the data **Li et al., "Visualizing and Understanding Convolutional Neural Networks"**. The sparse autoencoders (SAEs) approach can be seen as an intermediate step that encourage the hidden neurons to be more monosemantic **Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders"**. Thereby, one trains an autoencoder to map neural activities of a hidden unit layer to a much larger number of intermediate hidden units while encouraging a sparse number of them to be active. In this way, the target hidden layer activity can be represented by a superposition of several individual neurons inside the SAE.

Other approaches reduces and interprets neural population activities in lower dimensions: representation engineering  captures the distinct neural activity corresponding to the target concept or function, such as bias or truthfulness **Kurakin et al., "Adversarial Examples in the Physical World"**. Then, it uses a linear model to identify the neural activity direction that predicts the concept under question or for interference with the network behavior.

The current interpretability approach that studies language-based descriptions as conceptual entities and their implications for individual/low-dimensional neurons suffers from limitations on both ends: meanings are finite, and individual neurons are limited in their expressiveness and may not map nicely to these predefined conceptual meanings. 

Just like physics models lose their failure to have a closed-form description of motion beyond two interacting bodies **Newton, "Philosophiæ Naturalis Principia Mathematica"**, confined, symbolic definitions of interpretation have inherent limitations in precision. This cognitive constraint—our reliance on well-defined symbolic entities for understanding—has made deciphering the complexity of billions of neural activities an especially daunting task. It underscores a fundamental trade-off between the expressiveness of a model and its interpretability **Bengio et al., "A General Framework for Adding Unsupervised Objectives to Neural Networks"**.

Focusing solely on individual neurons is also is insufficient to capture the broader mechanisms underlying neural activity across a network. ``monosemantic'' neurons, which respond to a single concept, make up only a small fraction of the overall neural population **Li et al., "Visualizing and Understanding Convolutional Neural Networks"**. Empirically, especially for transformer models **Vaswani et al., "Attention Is All You Need"**, neurons are often observed to be ``polysemantic'', i.e., associated with multiple, unrelated concepts **Kurakin et al., "Adversarial Examples in the Physical World"**, which complicates the task of understanding how neural population activity evolves across layers. This highlights the need for more holistic approaches that account for the complex, distributed nature of neural representations.

The challenges involved in interpreting artificial neural network’s computation are oftentimes similar to the challenges involved in interpreting biological neurons and how their computation gives rise to the behavior that animals exhibit. Common approaches to interpreting neural networks study neural activity and their associated meaning on both the single neuron and the population level **Bengio et al., "A General Framework for Adding Unsupervised Objectives to Neural Networks"**.

The earliest interpretability approaches, inspired by neuroscience discoveries such as ``grandmother cells'' and ``Jennifer Aniston neurons'', focused on understanding the semantic meanings that drive the activity of individual neurons.  Similarly, studies in artificial neural networks, from BERT to GPT, have identified specific neurons and attention heads whose activations correlate with semantic meanings in the data **Li et al., "Visualizing and Understanding Convolutional Neural Networks"**. However, these ``monosemantic'' neurons, which respond to a single concept, make up only a small fraction of the overall neural population **Kurakin et al., "Adversarial Examples in the Physical World"**. The majority of neurons are ``polysemantic'', responding to multiple concepts in the data, which complicates the task of understanding how neural population activity evolves across layers **Vaswani et al., "Attention Is All You Need"**. As a result, focusing solely on individual neurons is insufficient to capture the broader mechanisms underlying neural activity across a network. This highlights the need for more holistic approaches that account for the complex, distributed nature of neural representations.

The sparse autoencoders (SAEs) approach can be seen as an intermediate step that encourage the hidden neurons to be more monosemantic **Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders"**. Thereby, one trains an autoencoder to map neural activities of a hidden unit layer to a much larger number of intermediate hidden units while encouraging a sparse number of them to be active. In this way, the target hidden layer activity can be represented by a superposition of several individual neurons inside the SAE.

Other approaches have hypothesized that semantic meaning is encoded on the level of neural populations. The representation engineering approach captures the distinct neural activity corresponding to the target concept or function, such as bias or truthfulness **Kurakin et al., "Adversarial Examples in the Physical World"**. Then, it uses a linear model to identify the neural activity direction that predicts the concept under question or for interference with the network behavior. However, this assumes that the target concept is known a priori.