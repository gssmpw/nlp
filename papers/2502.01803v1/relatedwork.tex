\section{Related Work}
Most interpretability approaches hold a salient agreement on ``what is interpretable'' and ``what to interpret''. ``What is interpretable'' is influenced by models in physics and mathematics, where operations and derivations are framed around the manipulation of a small set of well-defined symbols. Hence, interpretable concepts are confined to word-level or token-level description, and approaches try to learn a mapping between the neural activities and the target interpretable concept to understand \cite{geva_transformer_2022, zou_representation_2023, belinkov-2022-probing,belrose2023tunedlens,pal2023futurelens,yomdin2023jump}. %Din et al., 2024; Langedijk et al., 2023; Belinkov,, 2024.

The current approaches on ``what to interpret'' to understand the computations inside a neural network is heavily influenced by neuroscience: either on the level of neurons as a computation unit or in a low-dimensional neural activity descriptions. The earliest interpretability approaches, inspired by neuroscience discoveries such as ``grandmother cells'' and ``Jennifer Aniston neurons'', focused on understanding the semantic meanings that drive the activity of individual neurons. Similarly, studies in artificial neural networks, from BERT to GPT, have identified specific neurons and attention heads whose activations correlate with semantic meanings in the data \cite{olah2020neuron, elhage2022toy, wang2022interpretabilitywildcircuitindirect, marks2024sparsefeaturecircuitsdiscovering, bau2020concept, goh2021multimodal, nguyen2016synthesizing, mu2021compositionalexplanationsneurons, radford2017learninggeneratereviewsdiscovering}. The sparse autoencoders (SAEs) approach can be seen as an intermediate step that encourage the hidden neurons to be more monosemantic \cite{bricken2023interpretable,braun2024identifyingfunctionallyimportantfeatures, cunningham2023sparseautoencodershighlyinterpretable,chaudhary2024evaluatingopensourcesparseautoencoders,karvonen2024evaluatingsparseautoencoderstargeted}. Thereby, one trains an autoencoder to map neural activities of a hidden unit layer to a much larger number of intermediate hidden units while encouraging a sparse number of them to be active. In this way, the target hidden layer activity can be represented by a superposition of several individual neurons inside the SAE. 
Other approaches reduces and interprets neural population activities in lower dimensions: representation engineering  captures the distinct neural activity corresponding to the target concept or function, such as bias or truthfulness \cite{zou_representation_2023}. Then, it uses a linear model to identify the neural activity direction that predicts the concept under question or for interference with the network behavior. 

% their limitations
The current interpretability approach that studies language-based descriptions as conceptual entities and their implications for individual/low-dimensional neurons suffers from limitations on both ends: meanings are finite, and individual neurons are limited in their expressiveness and may not map nicely to these predefined conceptual meanings. 

Just like physics models lose their failure to have a closed-form description of motion beyond two interacting bodies \cite{tao_e_2012}, confined, symbolic definitions of interpretation have inherent limitations in precision. This cognitive constraint—our reliance on well-defined symbolic entities for understanding—has made deciphering the complexity of billions of neural activities an especially daunting task. It underscores a fundamental trade-off between the expressiveness of a model and its interpretability \cite{wang2024largelanguagemodelsinterpretable}.

Focusing solely on individual neurons is also is insufficient to capture the broader mechanisms underlying neural activity across a network. ``monosemantic'' neurons, which respond to a single concept, make up only a small fraction of the overall neural population \cite{radford2017learninggeneratereviewsdiscovering, elhage2022toy, wang2022interpretabilitywildcircuitindirect, dai2022knowledge, voita2023neuron,miller2023neuron}. Empirically, especially for transformer models \cite{elhage2022toy}, neurons are often observed to be ``polysemantic'',, i.e., associated with multiple, unrelated concepts \cite{mu2021compositionalexplanationsneurons, elhage2022toy, olah2020circuits}, which complicates the task of understanding how neural population activity evolves across layers \cite{elhage2022toy, gurnee2023findingneuronshaystackcase}. This highlights the need for more holistic approaches that account for the complex, distributed nature of neural representations.

% The challenges involved in interpreting artificial neural network’s computation are oftentimes similar to the challenges involved in interpreting biological neurons and how their computation gives rise to the behavior that animals exhibit. Common approaches to interpreting neural networks study neural activity and their associated meaning on both the single neuron and the population level \cite{gurnee2023findingneuronshaystackcase}.

% % interpreting single neuron: legacy from neuroscience

% The earliest interpretability approaches, inspired by neuroscience discoveries such as ``grandmother cells' and ``Jennifer Aniston neurons'', focused on understanding the semantic meanings that drive the activity of individual neurons.  Similarly, studies in artificial neural networks, from BERT to GPT, have identified specific neurons and attention heads whose activations correlate with semantic meanings in the data \cite{olah2020neuron, elhage2022toy, wang2022interpretabilitywildcircuitindirect, marks2024sparsefeaturecircuitsdiscovering, bau2020concept, goh2021multimodal, nguyen2016synthesizing, mu2021compositionalexplanationsneurons, radford2017learninggeneratereviewsdiscovering}. However, these ``monosemantic'' neurons, which respond to a single concept, make up only a small fraction of the overall neural population \cite{radford2017learninggeneratereviewsdiscovering, elhage2022toy, wang2022interpretabilitywildcircuitindirect, dai2022knowledge, voita2023neuron,miller2023neuron}. The majority of neurons are ``polysemantic'', responding to multiple concepts in the data, which complicates the task of understanding how neural population activity evolves across layers \cite{elhage2022toy, gurnee2023findingneuronshaystackcase}. As a result, focusing solely on individual neurons is insufficient to capture the broader mechanisms underlying neural activity across a network. This highlights the need for more holistic approaches that account for the complex, distributed nature of neural representations.

% The sparse autoencoders (SAEs) approach can be seen as an intermediate step that encourage the hidden neurons to be more monosemantic \cite{bricken2023interpretable}. Thereby, one trains an autoencoder to map neural activities of a hidden unit layer to a much larger number of intermediate hidden units while encouraging a sparse number of them to be active. In this way, the target hidden layer activity can be represented by a superposition of several individual neurons inside the SAE. 

% Other approaches have hypothesized that semantic meaning is encoded on the level of neural populations. The representation engineering approach captures the distinct neural activity corresponding to the target concept or function, such as bias or truthfulness. Then, it uses a linear model to identify the neural activity direction that predicts the concept under question or for interference with the network behavior. However, this assumes that the target concept is known a priori.