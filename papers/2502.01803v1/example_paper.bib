@article{li2016convergent,
  title={Convergent Learning: Do different neural networks learn the same representations?},
  author={Li, Y. and Yosinski, J. and Clune, J. and Lipson, H. and Hopcroft, J.},
  year={2016}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to AI transparency},
  author={Zou, A. and Phan, L. and Chen, S. and Campbell, J. and Guo, P. and Ren, R. and Pan, A. and Yin, X. and Mazeika, M. and Dombrowski, A.-K. and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@unknown{wu2024,
author = {Wu, Shuchen and Thalmann, Mirko and Dayan, Peter and Akata, Zeynep and Schulz, Eric},
year = {2024},
month = {10},
pages = {},
title = {Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences},
doi = {10.48550/arXiv.2410.21332}
}

@article{wang2018understanding,
  title={Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation},
  author={Wang, L. and Hu, L. and Gu, J. and Wu, Y. and Hu, Z. and He, K. and Hopcroft, J.},
  year={2018}
}

@inproceedings{xin2019understanding,
  title={What part of the neural network does this? understanding LSTMs by measuring and dissecting neurons},
  author={Xin, J. and Lin, J. and Yu, Y.},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5823--5830},
  year={2019}
}


@article{wang2022interpretability,
  title={Interpretability in the wild: a circuit for indirect object identification in GPT-2 small},
  author={Wang, K. and Variengien, A. and Conmy, A. and Shlegeris, B. and Steinhardt, J.},
  journal={arXiv preprint arXiv:2211.00593},
  year={2022}
}

@misc{nostalgebraist2020logit,
  title={Interpreting GPT: The logit lens},
  author={Nostalgebraist},
  year={2020},
  note={\url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}}
}

@misc{olah2021interpretability,
  title={Interpretability vs neuroscience},
  author={Olah, C.},
  year={2021},
  note={\url{https://colah.github.io/notes/interp-v-neuro/}}
}

@article{olah2020circuits,
  title={An overview of early vision in InceptionV1},
  author={Olah, C. and Cammarata, N. and Schubert, L. and Goh, G. and Petrov, M. and Carter, S.},
  journal={Distill},
  year={2020},
  note={\url{https://distill.pub/2020/circuits/early-vision}}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, C. and Cammarata, N. and Schubert, L. and Goh, G. and Petrov, M. and Carter, S.},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}



@article{tao_e_2012,
    author = {Tao, Terence},
    title = {E pluribus unum: From Complexity, Universality},
    journal = {Daedalus},
    volume = {141},
    number = {3},
    pages = {23-34},
    year = {2012},
    month = {07},
    abstract = {In this brief survey, I discuss some examples of the fascinating phenomenon of universality in complex systems, in which universal macroscopic laws of nature emerge from a variety of different microscopic dynamics. This phenomenon is widely observed empirically, but the rigorous mathematical foundation for universality is not yet satisfactory in all cases.},
    issn = {0011-5266},
    doi = {10.1162/DAED_a_00158},
    url = {https://doi.org/10.1162/DAED\_a\_00158},
    eprint = {https://direct.mit.edu/daed/article-pdf/141/3/23/1830458/daed\_a\_00158.pdf},
}

@article{Miller1956TheInformation,
    title = {{The magical number seven, plus or minus two: some limits on our capacity for processing information}},
    year = {1956},
    journal = {Psychological Review},
    author = {Miller, George A.},
    doi = {10.1037/h0043158},
    issn = {0033295X},
    pmid = {13310704},
    keywords = {INFORMATION THEORY, HUMAN LIMITS LANGUAGE {\&} COMMUNICATION}
}


@inproceedings{laird1984towards,
    title={Towards Chunking as a General Learning Mechanism.},
    author={Laird, John E and Rosenbloom, Paul S and Newell, Allen},
    booktitle={AAAI},
    pages={188--192},
    year={1984}
}

@article{graybiel1998basal,
    title={The basal ganglia and chunking of action repertoires},
    author={Graybiel, Ann M},
    journal={Neurobiology of learning and memory},
    volume={70},
    number={1-2},
    pages={119--136},
    year={1998},
    publisher={Elsevier}
}


@article{servan1990learning,
    title={Learning artificial grammars with competitive chunking.},
    author={Servan-Schreiber, Emile and Anderson, John R},
    journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
    volume={16},
    number={4},
    pages={592},
    year={1990},
    publisher={American Psychological Association}
}

@article{hendrycks2023risks,
  title={An overview of catastrophic AI risks},
  author={Hendrycks, D. and Mazeika, M. and Woodside, T.},
  journal={arXiv preprint arXiv:2306.12001},
  year={2023}
}

@inproceedings{Graybiel1998TheRepertoires,
    title = {{The basal ganglia and chunking of action repertoires}},
    year = {1998},
    booktitle = {Neurobiology of Learning and Memory},
    author = {Graybiel, Ann M.},
    pages = {1-2},
    volume = {70},
    doi = {10.1006/nlme.1998.3843},
    issn = {10747427}
}

@article{Ellis1996SequencingOrder,
    title = {{Sequencing in SLA: Phonological memory, chunking, and points of order}},
    year = {1996},
    journal = {Studies in Second Language Acquisition},
    author = {Ellis, Nick C.},
    number = {1},
    volume = {18},
    doi = {10.1017/S0272263100014698},
    issn = {14701545}
}



@article{elhage2022toy,
  author    = {Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
  title     = {Toy Models of Superposition},
  journal   = {Transformer Circuits Thread},
  year      = {2022},
  url       = {https://transformercircuits.pub/2022/toy_model/index.html}
}


@misc{marks2024sparsefeaturecircuitsdiscovering,
      title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models}, 
      author={Samuel Marks and Can Rager and Eric J. Michaud and Yonatan Belinkov and David Bau and Aaron Mueller},
      year={2024},
      eprint={2403.19647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.19647}, 
}


@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@article{Chase&Simon1973,
    title = {{Perception in chess}},
    year = {1973},
    journal = {Cognitive Psychology},
    author = {Chase, W. G. and Simon, H. A.},
    number = {55-81},
    volume = {4(1)},
    doi = {10.1016/0010-0285(73)90004-2}}

@article{Gobet&Simon1998,
    title = {{Expert chess memory: Revisiting the chunking hypothesis}},
    year = {1998},
    journal = {Memory},
    author = {Gobet, F. and Simon, H. A.},
    number = {255-255},
    volume = {6},
    doi = {10.1080/741942359}}


@article{Gobet2001ChunkingLearning,
    title = {{Chunking mechanisms in human learning}},
    year = {2001},
    journal = {Trends in Cognitive Sciences},
    author = {Gobet, Fernand and Lane, Peter C.R. and Croker, Steve and Cheng, Peter C.H. and Jones, Gary and Oliver, Iain and Pine, Julian M.},
    number = {6},
    volume = {5},
    doi = {10.1016/S1364-6613(00)01662-4},
    issn = {13646613}
}

@article{Egan1979ChunkingDrawings,
    title = {{Chunking in recall of symbolic drawings}},
    year = {1979},
    journal = {Memory {\&} Cognition},
    author = {Egan, Dennis E. and Schwartz, Barry J.},
    number = {2},
    volume = {7},
    doi = {10.3758/BF03197595},
    issn = {0090502X}
}


@misc{bills2023language,
  title={Language models can explain neurons in language models},
  author={Bills, S. and Cammarata, N. and Mossing, D. and Tillman, H. and Gao, L. and Goh, G. and Sutskever, I. and Leike, J. and Wu, J. and Saunders, W.},
  year={2023},
  note={\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
}

@article{bau2018neurons,
  title={Identifying and controlling important neurons in neural machine translation},
  author={Bau, A. and Belinkov, Y. and Sajjad, H. and Durrani, N. and Dalvi, F. and Glass, J.},
  journal={arXiv preprint arXiv:1811.01157},
  year={2018}
}

@article{bau2020units,
  title={Understanding the role of individual units in a deep neural network},
  author={Bau, D. and Zhu, J.-Y. and Strobelt, H. and Lapedriza, A. and Zhou, B. and Torralba, A.},
  journal={Proceedings of the National Academy of Sciences},
  year={2020}
}

@inproceedings{casper2021frivolous,
  title={Frivolous units: Wider networks are not really that wide},
  author={Casper, S. and Boix, X. and D’Amario, V. and Guo, L. and Schrimpf, M. and Vinken, K. and Kreiman, G.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={6921--6929},
  year={2021}
}

@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, H. and Ewart, A. and Riggs, L. and Huben, R. and Sharkey, L.},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}

@article{dar2022transformers,
  title={Analyzing transformers in embedding space},
  author={Dar, G. and Geva, M. and Gupta, A. and Berant, J.},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@inproceedings{donnelly2019sentiment,
  title={On interpretability and feature representations: an analysis of the sentiment neuron},
  author={Donnelly, J. and Roegiest, A.},
  booktitle={Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part I 41},
  pages={795--802},
  year={2019},
  publisher={Springer}
}

@article{doshi2017interpretable,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, F. and Kim, B.},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

@article{geva2022concepts,
  title={Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space},
  author={Geva, M. and Caciularu, A. and Wang, K. R. and Goldberg, Y.},
  journal={arXiv preprint arXiv:2203.14680},
  year={2022}
}

@article{geva2020memory,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, M. and Schuster, R. and Berant, J. and Levy, O.},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@article{lange2022clustering,
  title={Clustering units in neural networks: upstream vs downstream information},
  author={Lange, R. D. and Rolnick, D. S. and Kording, K. P.},
  year={2022}
}

@article{quirke2023ngrams,
  title={Training dynamics of contextual n-grams in language models},
  author={Quirke, L. and Heindrich, L. and Gurnee, W. and Nanda, N.},
  journal={arXiv preprint arXiv:2311.00863},
  year={2023}
}


@inproceedings{voita2023neurons,
author = {Voita, Elena and Ferrando, Javier and Nalmpantis, Christoforos},
year = {2024},
month = {01},
pages = {1288-1301},
title = {Neurons in Large Language Models: Dead, N-gram, Positional},
doi = {10.18653/v1/2024.findings-acl.75}
}



@inproceedings{hardt2016equality,
  title={Equality of Opportunity in Supervised Learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  volume={29},
  pages={3315--3323},
  year={2016},
  url={https://arxiv.org/abs/1610.02413}
}

@inproceedings{dwork2012fairness,
  title={Fairness Through Awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
  pages={214--226},
  year={2012},
  url={https://dl.acm.org/doi/10.1145/2090236.2090255}
}


@inproceedings{bricken2023interpretable,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author       = {Trenton Bricken and Adly Templeton and Joshua Batson and Brian Chen and Adam Jermyn and Tom Conerly and Nicholas L. Turner and Cem Anil and Carson Denison and Amanda Askell and Robert Lasenby and Yifan Wu and Shauna Kravec and Nicholas Schiefer and Tim Maxwell and Nicholas Joseph and Alex Tamkin and Karina Nguyen and Brayden McLean and Josiah E. Burke and Tristan Hume and Shan Carter and Tom Henighan and Chris Olah},
  booktitle={Transformer Circuits Thread},
  year={2023},
  url={}
}

@article{olah2020neuron,
  title={Zoom In: An Interpretability Method for Deep Neural Networks},
  author={Olah, Chris and Satyanarayan, Arvind and Carter, Shan and others},
  journal={Distill},
  year={2020},
  url={https://distill.pub/2020/circuits/}
}

@article{belrose2023tunedlens,
  author = {Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
  title = {Eliciting Latent Predictions from Transformers with the Tuned Lens},
  journal = {arXiv preprint},
  volume = {arXiv:2303.08112},
  year = {2023}
}

@article{pal2023futurelens,
  author = {Koyena Pal and Jiuding Sun and Andrew Yuan and Byron C Wallace and David Bau},
  title = {Future Lens: Anticipating Subsequent Tokens from a Single Hidden State},
  journal = {arXiv preprint},
  volume = {arXiv:2311.04897},
  year = {2023}
}


@article{yomdin2023jump,
  author = {Alexander Yom Din and Taelin Karidi and Leshem Choshen and Mor Geva},
  title = {Jump to Conclusions: Short-Cutting Transformers with Linear Transformations},
  journal = {arXiv preprint},
  volume = {arXiv:2303.09435},
  year = {2023}
}



@inproceedings{bau2020concept,
  title={Understanding the Role of Individual Neurons in Deep Learning},
  author={Bau, David and Zhu, Jun-Yan and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  url={https://arxiv.org/abs/2005.13194}
}

@misc{braun2024identifyingfunctionallyimportantfeatures,
      title={Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning}, 
      author={Dan Braun and Jordan Taylor and Nicholas Goldowsky-Dill and Lee Sharkey},
      year={2024},
      eprint={2405.12241},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12241}, 
}


@misc{chaudhary2024evaluatingopensourcesparseautoencoders,
      title={Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small}, 
      author={Maheep Chaudhary and Atticus Geiger},
      year={2024},
      eprint={2409.04478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.04478}, 
}



@misc{dravid2023rosettaneuronsminingcommon,
      title={Rosetta Neurons: Mining the Common Units in a Model Zoo}, 
      author={Amil Dravid and Yossi Gandelsman and Alexei A. Efros and Assaf Shocher},
      year={2023},
      eprint={2306.09346},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.09346}, 
}

@misc{karvonen2024evaluatingsparseautoencoderstargeted,
      title={Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks}, 
      author={Adam Karvonen and Can Rager and Samuel Marks and Neel Nanda},
      year={2024},
      eprint={2411.18895},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.18895}, 
}

@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}
@article{goh2021multimodal,
  title={Multimodal Neurons in Artificial Neural Networks},
  author={Goh, Gabriel and others},
  journal={Distill},
  year={2021},
  url={https://distill.pub/2021/multimodal-neurons/}
}

@article{nguyen2016synthesizing,
  title={Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
  author={Nguyen, Anh and Dosovitskiy, Alexey and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2016},
  url={https://arxiv.org/abs/1605.09304}
}

@misc{mu2021compositionalexplanationsneurons,
      title={Compositional Explanations of Neurons}, 
      author={Jesse Mu and Jacob Andreas},
      year={2021},
      eprint={2006.14032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.14032}, 
}


@misc{radford2017learninggeneratereviewsdiscovering,
      title={Learning to Generate Reviews and Discovering Sentiment}, 
      author={Alec Radford and Rafal Jozefowicz and Ilya Sutskever},
      year={2017},
      eprint={1704.01444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1704.01444}, 
}


@misc{nostalgebraist2020logitlens,
  author={Nostalgebraist},
  title={Interpreting {GPT}: The logit lens},
  year={2020},
  howpublished={\textit{LessWrong}},
  url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}




@misc{marks_sparse_2024,
	title = {Sparse {Feature} {Circuits}: {Discovering} and {Editing} {Interpretable} {Causal} {Graphs} in {Language} {Models}},
	shorttitle = {Sparse {Feature} {Circuits}},
	url = {http://arxiv.org/abs/2403.19647},
	doi = {10.48550/arXiv.2403.19647},
	abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
	language = {en},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
	month = mar,
	year = {2024},
	note = {arXiv:2403.19647 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Code and data at https://github.com/saprmarks/feature-circuits. Demonstration at https://feature-circuits.xyz},
	file = {Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf:/Users/swu/Zotero/storage/9HF4FHML/Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf:application/pdf},
}





@misc{geva_transformer_2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {http://arxiv.org/abs/2012.14913},
	doi = {10.48550/arXiv.2012.14913},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	language = {en},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	month = sep,
	year = {2021},
	note = {arXiv:2012.14913 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2021},
	file = {Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf:/Users/swu/Zotero/storage/WMW2GJPC/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf:application/pdf},
}






@article{wu2025motifs,
  title = {Two types of motifs enhance human recall and generalization of long sequences},
  author = {Wu, Shuchen and Thalmann, Mirko and Schulz, Eric},
  journal = {Communications Psychology},
  volume = {3},
  number = {1},
  pages = {3},
  year = {2025},
  publisher = {Nature Publishing Group UK},
  url = {https://www.nature.com/articles/some-url-placeholder},
  abstract = {Whether it is listening to a piece of music, learning a new language, or solving a mathematical equation, people often acquire abstract notions in the sense of motifs and variables—manifested in musical themes, grammatical categories, or mathematical symbols. How do we create abstract representations of sequences? Are these abstract representations useful for memory recall? In addition to learning transition probabilities, chunking, and tracking ordinal positions, we propose that humans also use abstractions to arrive at efficient representations of sequences. We propose and study two abstraction categories: projectional motifs and variable motifs. Projectional motifs find a common theme underlying distinct sequence instances. Variable motifs contain symbols representing sequence entities that can change. In two sequence recall experiments, we train participants to remember sequences with projectional and ...},
}

@misc{miller2018explanationartificialintelligenceinsights,
      title={Explanation in Artificial Intelligence: Insights from the Social Sciences}, 
      author={Tim Miller},
      year={2018},
      eprint={1706.07269},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1706.07269}, 
}


@misc{wang2024largelanguagemodelsinterpretable,
      title={Large Language Models are Interpretable Learners}, 
      author={Ruochen Wang and Si Si and Felix Yu and Dorothea Wiesmann and Cho-Jui Hsieh and Inderjit Dhillon},
      year={2024},
      eprint={2406.17224},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.17224}, 
}


@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{miller2023neuron,
  title={We Found An Neuron in GPT-2},
  author={Miller, Joseph and Neo, Clement},
  year={2023},
  month={February},
  day={11},
  howpublished={AI Alignment Forum},
  note={Linkpost from \url{https://clementneo.com}}
}


@article{Churchland2012,
  author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Foster, Justin D. and Nuyujukian, Paul and Ryu, Stephen I. and Shenoy, Krishna V.},
  title = {Neural population dynamics during reaching},
  journal = {Nature},
  year = {2012},
  volume = {487},
  number = {7405},
  pages = {51--56},
  doi = {10.1038/nature11129},
  pmid = {22722855},
  pmcid = {PMC3393826},
  month = {Jul},
  day = {5},
}

@article{hart1971projectgutenberg,
  author  = {Michael S. Hart},
  title   = {Project Gutenberg},
  year    = {1971},
  journal = {projectgutenberg.org},
  url     = {https://www.gutenberg.org/}
}

@book{bird2009nltk,
  author    = {Steven Bird and Ewan Klein and Edward Loper},
  title     = {Natural Language Processing with Python},
  publisher = {O’Reilly Media},
  year      = {2009},
  url       = {https://www.nltk.org/}
}


@article{marcus1993ptb,
  author = {Mitchell P. Marcus and Beatrice Santorini and Mary Ann Marcinkiewicz},
  title = {Building a Large Annotated Corpus of English: The Penn Treebank},
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  year = {1993},
  pages = {313--330}
}


@inproceedings{tenney2019bert,
  author    = {Ian Tenney and Dipanjan Das and Ellie Pavlick},
  title     = {BERT Rediscovers the Classical NLP Pipeline},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year      = {2019},
  pages     = {4593--4601},
  doi       = {10.18653/v1/P19-1452},
  url       = {https://aclanthology.org/P19-1452}
}

@inproceedings{jawahar2019bert,
  author    = {Ganesh Jawahar and Benoît Sagot and Djamé Seddah},
  title     = {What Does {BERT} Learn about the Structure of Language?},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year      = {2019},
  pages     = {3651--3657},
  doi       = {10.18653/v1/P19-1356},
  url       = {https://aclanthology.org/P19-1356}
}


@article{Engel2001,
  author = {Engel, Andreas K. and Fries, Pascal and Singer, Wolf},
  title = {Dynamic predictions: Oscillations and synchrony in top-down processing},
  journal = {Nature Reviews Neuroscience},
  year = {2001},
  volume = {2},
  number = {10},
  pages = {704--716},
  doi = {10.1038/35094565},
}



@article{Cohen2011,
  author = {Cohen, Marlene R. and Kohn, Adam},
  title = {Measuring and interpreting neuronal correlations},
  journal = {Nature Neuroscience},
  year = {2011},
  volume = {14},
  number = {7},
  pages = {811--819},
  doi = {10.1038/nn.2842},
  pmid = {21709677},
  pmcid = {PMC3586814},
  month = {Jun},
  day = {27},
}



@article{belinkov-2022-probing,
    title = "Probing Classifiers: Promises, Shortcomings, and Advances",
    author = "Belinkov, Yonatan",
    journal = "Computational Linguistics",
    volume = "48",
    number = "1",
    month = mar,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-1.7/",
    doi = "10.1162/coli_a_00422",
    pages = "207--219",
    abstract = "Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple{---}a classifier is trained to predict some linguistic property from a model`s representations{---}and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances."
}

@ARTICLE{bengio2013,
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Representation Learning: A Review and New Perspectives}, 
  year={2013},
  volume={35},
  number={8},
  pages={1798-1828},
  keywords={Learning systems;Machine learning;Abstracts;Feature extraction;Manifolds;Neural networks;Speech recognition;Deep learning;representation learning;feature learning;unsupervised learning;Boltzmann machine;autoencoder;neural nets},
  doi={10.1109/TPAMI.2013.50}}


@misc{singh2024rethinkinginterpretabilityeralarge,
      title={Rethinking Interpretability in the Era of Large Language Models}, 
      author={Chandan Singh and Jeevana Priya Inala and Michel Galley and Rich Caruana and Jianfeng Gao},
      year={2024},
      eprint={2402.01761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01761}, 
}

@ARTICLE{adadiberrada,
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access}, 
  title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}, 
  year={2018},
  volume={6},
  number={},
  pages={52138-52160},
  keywords={Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
  doi={10.1109/ACCESS.2018.2870052}}


@misc{wu_thalmann_schulz_2024,
      title={Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences}, 
      author={Shuchen Wu and Mirko Thalmann and Peter Dayan and Zeynep Akata and Eric Schulz},
      year={2024},
      eprint={2410.21332},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.21332}, 
}



@inproceedings{wu_yoerueten_2024,
  author    = {Wu, S. and Yoerueten, M. and Wichmann, F. A. and Schulz, E.},
  title     = {Normalized Cuts Characterize Visual Recognition Difficulty of Amorphous Image Sub-parts},
  booktitle = {Computational and Systems Neuroscience (COSYNE)},
  year      = {2024},
  address   = {Lisbon, Portugal},
}

@inproceedings{schreiber2023biologicallyplausible,
title={Biologically-plausible hierarchical chunking on mixed-signal neuromorphic hardware},
author={Atilla Schreiber and Shuchen Wu and Chenxi Wu and Giacomo Indiveri and Eric Schulz},
booktitle={Machine Learning with New Compute Paradigms},
year={2023},
url={https://openreview.net/forum?id=IuN2WXtFSY}
}


@article{wu_chunking_2023,
	title = {Chunking as a rational solution to the speed–accuracy trade-off in a serial reaction time task},
	volume = {13},
	issn = {2045-2322},
	url = {https://doi.org/10.1038/s41598-023-31500-3},
	doi = {10.1038/s41598-023-31500-3},
	abstract = {When exposed to perceptual and motor sequences, people are able to gradually identify patterns within and form a compact internal description of the sequence. One proposal of how sequences can be compressed is people’s ability to form chunks. We study people’s chunking behavior in a serial reaction time task. We relate chunk representation with sequence statistics and task demands, and propose a rational model of chunking that rearranges and concatenates its representation to jointly optimize for accuracy and speed. Our model predicts that participants should chunk more if chunks are indeed part of the generative model underlying a task and should, on average, learn longer chunks when optimizing for speed than optimizing for accuracy. We test these predictions in two experiments. In the first experiment, participants learn sequences with underlying chunks. In the second experiment, participants were instructed to act either as fast or as accurately as possible. The results of both experiments confirmed our model’s predictions. Taken together, these results shed new light on the benefits of chunking and pave the way for future studies on step-wise representation learning in structured domains.},
	number = {1},
	journal = {Scientific Reports},
	author = {Wu, Shuchen and Éltető, Noémi and Dasgupta, Ishita and Schulz, Eric},
	month = may,
	year = {2023},
	pages = {7680},
}

@inproceedings{wu_learning_2022,
	title = {Learning {Structure} from the {Ground} up—{Hierarchical} {Representation} {Learning} by {Chunking}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ee5bb72130c332c3d4bf8d231e617506-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Shuchen and Elteto, Noemi and Dasgupta, Ishita and Schulz, Eric},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {36706--36721},
}





@article{gage1994new,
  author = {Gage, Philip},
  title = {A New Algorithm for Data Compression},
  journal = {C Users Journal},
  year = {1994}
}


@inproceedings{agrawal1995mining,
  author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
  title = {Mining Sequential Patterns},
  booktitle = {Proceedings of the Eleventh International Conference on Data Engineering},
  pages = {3--14},
  year = {1995},
  organization = {IEEE},
  doi = {10.1109/ICDE.1995.380415}
}

@inproceedings{zaki2000sequence,
  author = {Zaki, Mohammed J.},
  title = {Sequence Mining in Categorical Domains: Incorporating Constraints},
  booktitle = {Proceedings of the Ninth International Conference on Information and Knowledge Management},
  pages = {422--429},
  year = {2000},
  organization = {ACM},
  doi = {10.1145/354756.354807}
}




@inproceedings{templeton-2021-word,
    title = "Word Equations: Inherently Interpretable Sparse Word Embeddings through Sparse Coding",
    author = "Templeton, Adly",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Dupoux, Emmanuel  and
      Giulianelli, Mario  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.12/",
    doi = "10.18653/v1/2021.blackboxnlp-1.12",
    pages = "177--191",
    abstract = "Word embeddings are a powerful natural language processing technique, but they are extremely difficult to interpret. To enable interpretable NLP models, we create vectors where each dimension is inherently interpretable. By inherently interpretable, we mean a system where each dimension is associated with some human-understandable hint that can describe the meaning of that dimension. In order to create more interpretable word embeddings, we transform pretrained dense word embeddings into sparse embeddings. These new embeddings are inherently interpretable: each of their dimensions is created from and represents a natural language word or specific grammatical concept. We construct these embeddings through sparse coding, where each vector in the basis set is itself a word embedding. Therefore, each dimension of our sparse vectors corresponds to a natural language word. We also show that models trained using these sparse embeddings can achieve good performance and are more interpretable in practice, including through human evaluations."
}

@article{chang2024survey,
  title={A Survey on Evaluation of Large Language Models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024}
}


@article{templeton2020,
  author       = {Adly Templeton},
  title        = {Inherently Interpretable Sparse Word Embeddings through Sparse Coding},
  journal      = {CoRR},
  volume       = {abs/2004.13847},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.13847},
  eprinttype    = {arXiv},
  eprint       = {2004.13847},
  timestamp    = {Sat, 02 May 2020 19:17:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-13847.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% ==========================================


@inproceedings{dai2022knowledge,
  author    = {Dai, D. and Dong, L. and Hao, Y. and Sui, Z. and Chang, B. and Wei, F.},
  title     = {Knowledge Neurons in Pretrained Transformers},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2022},
  pages     = {8493--8502}
}


@misc{voita2023neuron,
      title={Neurons in Large Language Models: Dead, N-gram, Positional}, 
      author={Elena Voita and Javier Ferrando and Christoforos Nalmpantis},
      year={2023},
      eprint={2309.04827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.04827}, 
}


@misc{gurnee2023findingneuronshaystackcase,
      title={Finding Neurons in a Haystack: Case Studies with Sparse Probing}, 
      author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
      year={2023},
      eprint={2305.01610},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.01610}, 
}

@misc{huh2024platonicrepresentationhypothesis,
      title={The Platonic Representation Hypothesis}, 
      author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
      year={2024},
      eprint={2405.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07987}, 
}


@article{gurnee2024universal,
  title={Universal neurons in gpt2 language models},
  author={Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2401.12181},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}



@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019},
  publisher={Association for Computational Linguistics},
  address={Minneapolis, Minnesota},
  doi={10.18653/v1/N19-1423},
  url={https://aclanthology.org/N19-1423}
}



@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023},
  url={https://arxiv.org/abs/2307.09288}
}



@article{achiam2023gpt4,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023},
  url={https://arxiv.org/abs/2303.08774}
}


@article{gemini2023,
  title={Gemini: A family of highly capable multimodal models},
  author={{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023},
  url={https://arxiv.org/abs/2312.11805}
}


@misc{gurnee_finding_2023,
	title = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
	shorttitle = {Finding {Neurons} in a {Haystack}},
	url = {http://arxiv.org/abs/2305.01610},
	abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train k-sparse linear classiﬁers (probes) on these internal activations to predict the presence of features in the input; by varying the value of k we study the sparsity of learned representations and how this varies with model scale. With k = 1, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
	language = {en},
	urldate = {2023-05-16},
	publisher = {arXiv},
	author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
	month = may,
	year = {2023},
	note = {arXiv:2305.01610 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf:/Users/swu/Zotero/storage/L5GBKUJ7/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf:application/pdf},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	abstract = {We identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. Code is available at github.com/andyzoujm/representation-engineering.},
	language = {en},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	annote = {Comment: Code is available at https://github.com/andyzoujm/representation-engineering},
	file = {Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf:/Users/swu/Zotero/storage/C7IMTJ7Q/Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf:application/pdf},
}

@misc{liu_visual_2023,
	title = {Visual {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2304.08485},
	abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for generalpurpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	month = dec,
	year = {2023},
	note = {arXiv:2304.08485 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2023 Oral; project page: https://llava-vl.github.io/},
	file = {Liu et al. - 2023 - Visual Instruction Tuning.pdf:/Users/swu/Zotero/storage/MAZI69UP/Liu et al. - 2023 - Visual Instruction Tuning.pdf:application/pdf},
}

@article{alaniz_workshop_nodate,
	title = {Workshop on {Understanding} {Vision}-{Language} {Models}},
	language = {en},
	author = {Alaniz, Stephan and Georgescu, Iuliana and Wu, Shuchen and Bader, Jessica and Poesina, Eduard and Ionescu, Radu-Tudor and Schmid, Cordelia and Darrell, Trevor and Akata, Zeynep},
	file = {Alaniz et al. - Workshop on Understanding Vision-Language Models.pdf:/Users/swu/Zotero/storage/TSVLZV4F/Alaniz et al. - Workshop on Understanding Vision-Language Models.pdf:application/pdf},
}

@misc{liu_improved_2024,
	title = {Improved {Baselines} with {Visual} {Instruction} {Tuning}},
	url = {http://arxiv.org/abs/2310.03744},
	abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	month = may,
	year = {2024},
	note = {arXiv:2310.03744 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Camera ready, CVPR 2024 (highlight). LLaVA project page: https://llava-vl.github.io},
	file = {Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:/Users/swu/Zotero/storage/WRWA6CLB/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf:application/pdf},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciﬁc training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:/Users/swu/Zotero/storage/QNBWZI6Y/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@misc{yuksekgonul_when_2023,
	title = {When and why vision-language models behave like bags-of-words, and what to do about it?},
	url = {http://arxiv.org/abs/2210.01936},
	abstract = {Despite the use of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of Visual Genome Attribution, to test the understanding of objects’ properties; Visual Genome Relation, to test for relational understanding; and COCO-Order \& Flickr30k-Order, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings in which state-of-the-art VLMs behave like bagsof-words—i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deﬁciency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the training and evaluation procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This ﬁnding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modiﬁcation of contrastive learning signiﬁcantly improves the performance on tasks requiring an understanding of order and compositionality.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},
	month = mar,
	year = {2023},
	note = {arXiv:2210.01936 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: ICLR 2023 Oral (notable-top-5\%)},
	file = {Yuksekgonul et al. - 2023 - When and why vision-language models behave like ba.pdf:/Users/swu/Zotero/storage/2EKEKE8Y/Yuksekgonul et al. - 2023 - When and why vision-language models behave like ba.pdf:application/pdf},
}

@misc{goetschalckx_ganalyze_2019,
	title = {{GANalyze}: {Toward} {Visual} {Definitions} of {Cognitive} {Image} {Properties}},
	shorttitle = {{GANalyze}},
	url = {http://arxiv.org/abs/1906.10112},
	abstract = {We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability, aesthetics, and emotional valence. These attributes are of interest because we do not have a concrete visual deﬁnition of what they entail. What does it look like for a dog to be more or less memorable? GANs allow us to generate a manifold of natural-looking images with ﬁne-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more or less memorable. The resulting “visual deﬁnitions" surface image properties (like “object size") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. Visit the GANalyze website at http://ganalyze.csail.mit.edu/.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Goetschalckx, Lore and Andonian, Alex and Oliva, Aude and Isola, Phillip},
	month = jun,
	year = {2019},
	note = {arXiv:1906.10112 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 17 pages, 15 figures},
	file = {Goetschalckx et al. - 2019 - GANalyze Toward Visual Definitions of Cognitive I.pdf:/Users/swu/Zotero/storage/PMURKPKJ/Goetschalckx et al. - 2019 - GANalyze Toward Visual Definitions of Cognitive I.pdf:application/pdf},
}

@article{koh_concept_nodate,
	title = {Concept {Bottleneck} {Models}},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like “the existence of bone spurs”, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of ﬁrst predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the ﬁnal prediction. On x-ray grading and bird identiﬁcation, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (“bone spurs”) or bird attributes (“wing color”). These models also allow for richer human-model interaction: accuracy improves signiﬁcantly if we can correct model mistakes on concepts at test time.},
	language = {en},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	file = {Koh et al. - Concept Bottleneck Models.pdf:/Users/swu/Zotero/storage/QLT5IKLV/Koh et al. - Concept Bottleneck Models.pdf:application/pdf},
}

@inproceedings{tenney_bert_2019,
	address = {Florence, Italy},
	title = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
	url = {https://www.aclweb.org/anthology/P19-1452},
	doi = {10.18653/v1/P19-1452},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We ﬁnd that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.},
	language = {en},
	urldate = {2024-12-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	year = {2019},
	pages = {4593--4601},
	file = {Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:/Users/swu/Zotero/storage/7KTYZ7P8/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:application/pdf},
}

@misc{sariyildiz_unic_2024,
	title = {{UNIC}: {Universal} {Classification} {Models} via {Multi}-teacher {Distillation}},
	shorttitle = {{UNIC}},
	url = {http://arxiv.org/abs/2408.05088},
	doi = {10.48550/arXiv.2408.05088},
	abstract = {Pretrained models have become a commodity and offer strong results on a broad range of tasks. In this work, we focus on classification and seek to learn a unique encoder able to take from several complementary pretrained models. We aim at even stronger generalization across a variety of classification tasks. We propose to learn such an encoder via multi-teacher distillation. We first thoroughly analyze standard distillation when driven by multiple strong teachers with complementary strengths. Guided by this analysis, we gradually propose improvements to the basic distillation setup. Among those, we enrich the architecture of the encoder with a ladder of expendable projectors, which increases the impact of intermediate features during distillation, and we introduce teacher dropping, a regularization mechanism that better balances the teachers’ influence. Our final distillation strategy leads to student models of the same capacity as any of the teachers, while retaining or improving upon the performance of the best teacher for each task.},
	language = {en},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Sariyildiz, Mert Bulent and Weinzaepfel, Philippe and Lucas, Thomas and Larlus, Diane and Kalantidis, Yannis},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05088 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To be presented at ECCV 2024},
	file = {Sariyildiz et al. - 2024 - UNIC Universal Classification Models via Multi-te.pdf:/Users/swu/Zotero/storage/U8YJM3UZ/Sariyildiz et al. - 2024 - UNIC Universal Classification Models via Multi-te.pdf:application/pdf},
}

@article{miller_introduction_nodate,
	title = {Introduction to {WordNet}: {An} {On}-line {Lexical} {Database}},
	language = {en},
	author = {Miller, George A and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine},
	file = {Miller et al. - Introduction to WordNet An On-line Lexical Databa.pdf:/Users/swu/Zotero/storage/3XD5B5IE/Miller et al. - Introduction to WordNet An On-line Lexical Databa.pdf:application/pdf},
}

@misc{esser_taming_2021,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2012.09841},
	doi = {10.48550/arXiv.2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efﬁciently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the ﬁrst results on semanticallyguided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://git.io/JnyvK.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09841 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Changelog can be found in the supplementary},
	file = {Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:/Users/swu/Zotero/storage/QLYWQT94/Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:application/pdf},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks’ internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, humanunderstandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task (Wang et al., 2022) to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 20 pages, 18 figures, 2 tables},
	file = {Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:/Users/swu/Zotero/storage/BYP7XEYF/Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf},
}

@article{zhang_naturalistic_2021,
	title = {Naturalistic stimuli: {A} paradigm for multiscale functional characterization of the human brain},
	volume = {19},
	issn = {24684511},
	shorttitle = {Naturalistic stimuli},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468451121000386},
	doi = {10.1016/j.cobme.2021.100298},
	abstract = {Movies, audio stories, and virtual reality are increasingly used as stimuli for functional brain imaging. Such naturalistic paradigms are in sharp contrast to the tradition of experimental reductionism in neuroscience research. Being complex, dynamic, and diverse, naturalistic stimuli set up a more ecologically relevant condition and induce highly reproducible brain responses across a wide range of spatiotemporal scales. Here, we review recent technical advances and scientific findings on imaging the brain under naturalistic stimuli. Then we elaborate on the premise of using naturalistic paradigms for multi-scale, multi-modal, and high-throughput functional characterization of the human brain. We further highlight the growing potential of using deep learning models to infer neural information processing from brain responses to naturalistic stimuli. Lastly, we advocate large-scale collaborations to combine brain imaging and recording data across experiments, subjects, and labs that use the same set of naturalistic stimuli.},
	language = {en},
	urldate = {2024-12-30},
	journal = {Current Opinion in Biomedical Engineering},
	author = {Zhang, Yizhen and Kim, Jung-Hoon and Brang, David and Liu, Zhongming},
	month = sep,
	year = {2021},
	pages = {100298},
	file = {Zhang et al. - 2021 - Naturalistic stimuli A paradigm for multiscale fu.pdf:/Users/swu/Zotero/storage/DIA485RF/Zhang et al. - 2021 - Naturalistic stimuli A paradigm for multiscale fu.pdf:application/pdf},
}

@article{riveland_natural_2024,
	title = {Natural language instructions induce compositional generalization in networks of neurons},
	volume = {27},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-024-01607-5},
	doi = {10.1038/s41593-024-01607-5},
	abstract = {Abstract
            A fundamental human cognitive feat is to interpret linguistic instructions in order to perform novel tasks without explicit task experience. Yet, the neural computations that might be used to accomplish this remain poorly understood. We use advances in natural language processing to create a neural model of generalization based on linguistic instructions. Models are trained on a set of common psychophysical tasks, and receive instructions embedded by a pretrained language model. Our best models can perform a previously unseen task with an average performance of 83\% correct based solely on linguistic instructions (that is, zero-shot learning). We found that language scaffolds sensorimotor representations such that activity for interrelated tasks shares a common geometry with the semantic representations of instructions, allowing language to cue the proper composition of practiced skills in unseen settings. We show how this model generates a linguistic description of a novel task it has identified using only motor feedback, which can subsequently guide a partner model to perform the task. Our models offer several experimentally testable predictions outlining how linguistic information must be represented to facilitate flexible and general cognition in the human brain.},
	language = {en},
	number = {5},
	urldate = {2024-12-30},
	journal = {Nature Neuroscience},
	author = {Riveland, Reidar and Pouget, Alexandre},
	month = may,
	year = {2024},
	pages = {988--999},
	file = {Riveland and Pouget - 2024 - Natural language instructions induce compositional.pdf:/Users/swu/Zotero/storage/SVXTX7T2/Riveland and Pouget - 2024 - Natural language instructions induce compositional.pdf:application/pdf},
}

@misc{geva_transformer_2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {http://arxiv.org/abs/2203.14680},
	doi = {10.48550/arXiv.2203.14680},
	abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	month = oct,
	year = {2022},
	note = {arXiv:2203.14680 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2022},
	file = {Geva et al. - 2022 - Transformer Feed-Forward Layers Build Predictions .pdf:/Users/swu/Zotero/storage/ACKBXX66/Geva et al. - 2022 - Transformer Feed-Forward Layers Build Predictions .pdf:application/pdf},
}

@misc{geva_transformer_2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {http://arxiv.org/abs/2012.14913},
	doi = {10.48550/arXiv.2012.14913},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	month = sep,
	year = {2021},
	note = {arXiv:2012.14913 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2021},
	file = {Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf:/Users/swu/Zotero/storage/ZXAMGQJP/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf:application/pdf},
}

@misc{li_convergent_2016,
	title = {Convergent {Learning}: {Do} different neural networks learn the same representations?},
	shorttitle = {Convergent {Learning}},
	url = {http://arxiv.org/abs/1511.07543},
	doi = {10.48550/arXiv.1511.07543},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	month = feb,
	year = {2016},
	note = {arXiv:1511.07543 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016},
	file = {Li et al. - 2016 - Convergent Learning Do different neural networks .pdf:/Users/swu/Zotero/storage/AZETMGAQ/Li et al. - 2016 - Convergent Learning Do different neural networks .pdf:application/pdf},
}

@inproceedings{wang_finding_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Finding {Skill} {Neurons} in {Pre}-trained {Transformer}-based {Language} {Models}},
	url = {https://aclanthology.org/2022.emnlp-main.765},
	doi = {10.18653/v1/2022.emnlp-main.765},
	abstract = {Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers1 are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than finetuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https: //github.com/THU-KEG/Skill-Neuron.},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xiaozhi and Wen, Kaiyue and Zhang, Zhengyan and Hou, Lei and Liu, Zhiyuan and Li, Juanzi},
	year = {2022},
	pages = {11132--11152},
	file = {Wang et al. - 2022 - Finding Skill Neurons in Pre-trained Transformer-b.pdf:/Users/swu/Zotero/storage/FQ5G946W/Wang et al. - 2022 - Finding Skill Neurons in Pre-trained Transformer-b.pdf:application/pdf},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning (ML) models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identiﬁcation (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior “in the wild” in a language model. We evaluate the reliability of our explanation using three quantitative criteria–faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, pointing toward opportunities to scale our understanding to both larger models and more complex tasks. Code for all experiments is available at https://github.com/redwoodresearch/Easy-Transformer.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:/Users/swu/Zotero/storage/NBNQIRH2/Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indire.pdf:application/pdf},
}

@article{zhao_explainability_2024,
	title = {Explainability for {Large} {Language} {Models}: {A} {Survey}},
	volume = {15},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Explainability for {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3639372},
	doi = {10.1145/3639372},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
	language = {en},
	number = {2},
	urldate = {2024-12-30},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
	month = apr,
	year = {2024},
	pages = {1--38},
	file = {Zhao et al. - 2024 - Explainability for Large Language Models A Survey.pdf:/Users/swu/Zotero/storage/SALB58ED/Zhao et al. - 2024 - Explainability for Large Language Models A Survey.pdf:application/pdf},
}

@misc{singh_rethinking_2024,
	title = {Rethinking {Interpretability} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.01761},
	doi = {10.48550/arXiv.2402.01761},
	abstract = {Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
	month = jan,
	year = {2024},
	note = {arXiv:2402.01761 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 7 pages},
	file = {Singh et al. - 2024 - Rethinking Interpretability in the Era of Large La.pdf:/Users/swu/Zotero/storage/9MJGB6HW/Singh et al. - 2024 - Rethinking Interpretability in the Era of Large La.pdf:application/pdf},
}

@inproceedings{steck_is_2024,
	title = {Is {Cosine}-{Similarity} of {Embeddings} {Really} {About} {Similarity}?},
	url = {http://arxiv.org/abs/2403.05440},
	doi = {10.1145/3589335.3651526},
	abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless ‘similarities.’ For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosinesimilarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {Companion {Proceedings} of the {ACM} {Web} {Conference} 2024},
	author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
	month = may,
	year = {2024},
	note = {arXiv:2403.05440 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {887--890},
	annote = {Comment: 9 pages},
	file = {Steck et al. - 2024 - Is Cosine-Similarity of Embeddings Really About Si.pdf:/Users/swu/Zotero/storage/AN9HHVVH/Steck et al. - 2024 - Is Cosine-Similarity of Embeddings Really About Si.pdf:application/pdf},
}

@article{ke_learning_2024,
	title = {{LEARNING} {HIERARCHICAL} {IMAGE} {SEGMENTATION} {FOR} {RECOGNITION} {AND} {BY} {RECOGNITION}},
	abstract = {Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections. Our key observation is that, while an image can be recognized in multiple ways, each has a consistent part-and-whole visual organization. Segmentation thus should be treated not as an end task to be mastered through supervised learning, but as an internal process that evolves with and supports the ultimate goal of recognition.},
	language = {en},
	author = {Ke, Tsung-Wei and Mo, Sangwoo and Yu, Stella X},
	year = {2024},
	file = {Ke et al. - 2024 - LEARNING HIERARCHICAL IMAGE SEGMENTATION FOR RECOG.pdf:/Users/swu/Zotero/storage/J486TD9L/Ke et al. - 2024 - LEARNING HIERARCHICAL IMAGE SEGMENTATION FOR RECOG.pdf:application/pdf},
}

@misc{el-nouby_scalable_2024,
	title = {Scalable {Pre}-training of {Large} {Autoregressive} {Image} {Models}},
	url = {http://arxiv.org/abs/2401.08541},
	doi = {10.48550/arXiv.2401.08541},
	abstract = {This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illustrate the practical implication of these findings by pre-training a 7 billion parameter AIM on 2 billion images that achieves 84.0\% on ImageNet1k with a frozen trunk. Interestingly, even at this scale, we observe no sign of saturation in performance, suggesting that AIM potentially represents a new frontier for training large-scale vision models. The pre-training of AIM is similar to the pre-training of LLMs, and does not require any image-specific strategy to stabilize the training at scale.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {El-Nouby, Alaaeldin and Klein, Michal and Zhai, Shuangfei and Bautista, Miguel Angel and Toshev, Alexander and Shankar, Vaishaal and Susskind, Joshua M. and Joulin, Armand},
	month = jan,
	year = {2024},
	note = {arXiv:2401.08541 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: https://github.com/apple/ml-aim},
	file = {El-Nouby et al. - 2024 - Scalable Pre-training of Large Autoregressive Imag.pdf:/Users/swu/Zotero/storage/QXXWY7IZ/El-Nouby et al. - 2024 - Scalable Pre-training of Large Autoregressive Imag.pdf:application/pdf},
}

@article{ismail_concept_2024,
	title = {{CONCEPT} {BOTTLENECK} {GENERATIVE} {MODELS}},
	abstract = {We introduce a generative model with an intrinsically interpretable layer—a concept bottleneck layer† —that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines—in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.},
	language = {en},
	author = {Ismail, Aya Abdelsalam and Adebayo, Julius and Bravo, Héctor Corrada and Ra, Stephen and Cho, Kyunghyun},
	year = {2024},
	file = {Ismail et al. - 2024 - CONCEPT BOTTLENECK GENERATIVE MODELS.pdf:/Users/swu/Zotero/storage/HTURPM33/Ismail et al. - 2024 - CONCEPT BOTTLENECK GENERATIVE MODELS.pdf:application/pdf},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Users/swu/Zotero/storage/VIV39ZWU/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@misc{serrano_is_2019,
	title = {Is {Attention} {Interpretable}?},
	url = {http://arxiv.org/abs/1906.03731},
	doi = {10.48550/arXiv.1906.03731},
	abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Serrano, Sofia and Smith, Noah A.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.03731 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear at ACL 2019},
	file = {Serrano and Smith - 2019 - Is Attention Interpretable.pdf:/Users/swu/Zotero/storage/AYP7G5GZ/Serrano and Smith - 2019 - Is Attention Interpretable.pdf:application/pdf},
}

@article{pandey_interpretability_nodate,
	title = {On the {Interpretability} of {Attention} {Networks}},
	abstract = {Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: “The output depends only on a small (but unknown) segment of the input.” In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the ‘reasoning’ of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective definition of interpretability for SDC tasks to evaluate a few attention model learning algorithms designed to encourage sparsity and demonstrate that these algorithms help improve interpretability.},
	language = {en},
	author = {Pandey, Lakshmi Narayan and Vashisht, Rahul and Ramaswamy, Harish G},
	file = {Pandey et al. - On the Interpretability of Attention Networks.pdf:/Users/swu/Zotero/storage/KHDW9DLE/Pandey et al. - On the Interpretability of Attention Networks.pdf:application/pdf},
}

@inproceedings{huang_diffusion-based_2023,
	address = {Vancouver, BC, Canada},
	title = {Diffusion-based {Generation}, {Optimization}, and {Planning} in {3D} {Scenes}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10205207/},
	doi = {10.1109/CVPR52729.2023.01607},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Siyuan and Wang, Zan and Li, Puhao and Jia, Baoxiong and Liu, Tengyu and Zhu, Yixin and Liang, Wei and Zhu, Song-Chun},
	month = jun,
	year = {2023},
	pages = {16750--16761},
	file = {Huang et al. - 2023 - Diffusion-based Generation, Optimization, and Plan.pdf:/Users/swu/Zotero/storage/AS7QKYSH/Huang et al. - 2023 - Diffusion-based Generation, Optimization, and Plan.pdf:application/pdf},
}

@article{zhang_visual_2018,
	title = {Visual interpretability for deep learning: a survey},
	volume = {19},
	issn = {2095-9184, 2095-9230},
	shorttitle = {Visual interpretability for deep learning},
	url = {http://link.springer.com/10.1631/FITEE.1700808},
	doi = {10.1631/FITEE.1700808},
	abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artiﬁcial intelligence.},
	language = {en},
	number = {1},
	urldate = {2024-12-30},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Zhang, Quan-shi and Zhu, Song-chun},
	month = jan,
	year = {2018},
	pages = {27--39},
	file = {Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf:/Users/swu/Zotero/storage/HXL6N89Y/Zhang and Zhu - 2018 - Visual interpretability for deep learning a surve.pdf:application/pdf},
}

@article{swets_underspecification_2008,
	title = {Underspecification of syntactic ambiguities: {Evidence} from self-paced reading},
	volume = {36},
	copyright = {http://www.springer.com/tdm},
	issn = {0090-502X, 1532-5946},
	shorttitle = {Underspecification of syntactic ambiguities},
	url = {http://link.springer.com/10.3758/MC.36.1.201},
	doi = {10.3758/MC.36.1.201},
	language = {en},
	number = {1},
	urldate = {2024-12-30},
	journal = {Memory \& Cognition},
	author = {Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
	month = jan,
	year = {2008},
	pages = {201--216},
	file = {Swets et al. - 2008 - Underspecification of syntactic ambiguities Evide.pdf:/Users/swu/Zotero/storage/3QGC45RD/Swets et al. - 2008 - Underspecification of syntactic ambiguities Evide.pdf:application/pdf},
}

@article{miller_introduction_nodate-1,
	title = {Introduction to {WordNet}: {An} {On}-line {Lexical} {Database}},
	language = {en},
	author = {Miller, George A and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine},
	file = {Miller et al. - Introduction to WordNet An On-line Lexical Databa.pdf:/Users/swu/Zotero/storage/8TEGVVKQ/Miller et al. - Introduction to WordNet An On-line Lexical Databa.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2024-12-30},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	pages = {336--359},
	annote = {Comment: This version was published in International Journal of Computer Vision (IJCV) in 2019; A previous version of the paper was published at International Conference on Computer Vision (ICCV'17)},
	file = {Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/Users/swu/Zotero/storage/49Z2R36N/Selvaraju et al. - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf},
}

@misc{xie_memorization_2024,
	title = {On {Memorization} of {Large} {Language} {Models} in {Logical} {Reasoning}},
	url = {http://arxiv.org/abs/2410.23123},
	doi = {10.48550/arXiv.2410.23123},
	abstract = {Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs’ reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K\&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K\&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis based on a per-sample memorization score sheds light on how LLMs switch between reasoning and memorization when solving logical puzzles. Our code and data are available at https://memkklogic.github.io/.},
	language = {en},
	urldate = {2024-12-30},
	publisher = {arXiv},
	author = {Xie, Chulin and Huang, Yangsibo and Zhang, Chiyuan and Yu, Da and Chen, Xinyun and Lin, Bill Yuchen and Li, Bo and Ghazi, Badih and Kumar, Ravi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23123 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Xie et al. - 2024 - On Memorization of Large Language Models in Logica.pdf:/Users/swu/Zotero/storage/JXTAFVR5/Xie et al. - 2024 - On Memorization of Large Language Models in Logica.pdf:application/pdf},
}

@misc{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	language = {en},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:/Users/swu/Zotero/storage/KBU6HKVK/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf},
}

@misc{petsiuk_rise_2018,
	title = {{RISE}: {Randomized} {Input} {Sampling} for {Explanation} of {Black}-box {Models}},
	shorttitle = {{RISE}},
	url = {http://arxiv.org/abs/1806.07421},
	doi = {10.48550/arXiv.1806.07421},
	abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difﬁcult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model’s prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on blackbox models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.},
	language = {en},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
	month = sep,
	year = {2018},
	note = {arXiv:1806.07421 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Petsiuk et al. - 2018 - RISE Randomized Input Sampling for Explanation of.pdf:/Users/swu/Zotero/storage/QLHHRVDS/Petsiuk et al. - 2018 - RISE Randomized Input Sampling for Explanation of.pdf:application/pdf},
}


@misc{lipton2017mythosmodelinterpretability,
      title={The Mythos of Model Interpretability}, 
      author={Zachary C. Lipton},
      year={2017},
      eprint={1606.03490},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.03490}, 
}

@inproceedings{wu2022,
 author = {Wu, Shuchen and Elteto, Noemi and Dasgupta, Ishita and Schulz, Eric},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {36706--36721},
 publisher = {Curran Associates, Inc.},
 title = {Learning Structure from the Ground up---Hierarchical Representation Learning by Chunking},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ee5bb72130c332c3d4bf8d231e617506-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{Bansal21,
 author = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {225--236},
 publisher = {Curran Associates, Inc.},
 title = {Revisiting Model Stitching to Compare Neural Representations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/01ded4259d101feb739b06c399e9cd9c-Paper.pdf},
 volume = {34},
 year = {2021}
}


@InProceedings{balestriero18b,
  title = 	 {A Spline Theory of Deep Learning},
  author =       {Balestriero, Randall and richard baraniuk},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {374--383},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/balestriero18b.html},
  abstract = 	 {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of <em>max-affine spline operators</em> (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.}
}



@InProceedings{kornblith19a,
  title = 	 {Similarity of Neural Network Representations Revisited},
  author =       {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3519--3529},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kornblith19a.html},
  abstract = 	 {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.}
}




@InProceedings{roeder21a,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9030--9039},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/roeder21a.html},
  abstract = 	 {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are over-parameterized by design. In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.}
}



@misc{lenc2015understandingimagerepresentationsmeasuring,
      title={Understanding image representations by measuring their equivariance and equivalence}, 
      author={Karel Lenc and Andrea Vedaldi},
      year={2015},
      eprint={1411.5908},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1411.5908}, 
}


@misc{moschella2023relativerepresentationsenablezeroshot,
      title={Relative representations enable zero-shot latent space communication}, 
      author={Luca Moschella and Valentino Maiorca and Marco Fumero and Antonio Norelli and Francesco Locatello and Emanuele Rodolà},
      year={2023},
      eprint={2209.15430},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.15430}, 
}

@article{miller56,
  added-at = {2009-07-30T10:59:16.000+0200},
  author = {Miller, George A.},
  biburl = {https://www.bibsonomy.org/bibtex/2a5acd9332fe6e530ee7b01a986666645/trude},
  description = {dret'd bibliography},
  index = {usability},
  interhash = {14394c5c7a8fb2eace6d9d638774f6c1},
  intrahash = {a5acd9332fe6e530ee7b01a986666645},
  journal = {The Psychological Review},
  keywords = {1956 _skimmed brain cognitiveload knowledge learning},
  month = {March},
  number = 2,
  pages = {81-97},
  timestamp = {2009-07-30T12:01:36.000+0200},
  title = {The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information},
  uri = {http://www.well.com/user/smalin/miller.html},
  url = {http://www.musanim.com/miller1956/},
  volume = 63,
  year = 1956
}

@inproceedings{weidinger2022taxonomy,
  title={Taxonomy of Risks Posed by Language Models},
  author={Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={214--229},
  year={2022}
}


@article{yao2024llmsecurity,
  title={A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024}
}



@misc{ribeiro_why_2016-1,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Users/swu/Zotero/storage/KRTVAWRD/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@misc{radford_learning_2021-1,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciﬁc training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:/Users/swu/Zotero/storage/JFJH2CD6/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}





@article{servan-schrelber_learning_nodate,
	title = {{LEARNING} {ARTIFICIAL} {GRAMMARS} {WITH} {COMPETITIVE} {CHUNKING}},
	language = {en},
	author = {Servan-Schrelber, Emile and Anderson, John R},
	pages = {38},
	file = {Servan-Schreiber and Anderson - Learning Artificial Grammars With Competitive Chun.pdf:/Users/swu/Zotero/storage/832KUD37/Servan-Schreiber and Anderson - Learning Artificial Grammars With Competitive Chun.pdf:application/pdf;Servan-Schreiber and Anderson - Learning Artificial Grammars With Competitive Chun.pdf:/Users/swu/Zotero/storage/HR3EC9L8/Servan-Schreiber and Anderson - Learning Artificial Grammars With Competitive Chun.pdf:application/pdf;Servan-Schrelber and Anderson - LEARNING ARTIFICIAL GRAMMARS WITH COMPETITIVE CHUN.pdf:/Users/swu/Zotero/storage/MWFKFPDH/Servan-Schrelber and Anderson - LEARNING ARTIFICIAL GRAMMARS WITH COMPETITIVE CHUN.pdf:application/pdf},
}

@article{jimenez_taking_2008,
	title = {Taking patterns for chunks: is there any evidence of chunk learning in continuous serial reaction-time tasks?},
	volume = {72},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Taking patterns for chunks},
	url = {http://link.springer.com/10.1007/s00426-007-0121-7},
	doi = {10.1007/s00426-007-0121-7},
	abstract = {When exposed to a regular sequence, people learn to exploit its predictable structure. There have been two major ways of thinking about learning under these conditions: either as the acquisition of general statistical information about the transition probabilities displayed by the sequence or as a process of memorizing and using separate chunks that can later become progressively composed with extended practice. Even though chunk learning has been adopted by some theories of skill acquisition as their main building block, the evidence for chunk formation is scarce in some areas, and is especially so in the continuous serial reaction-time (SRT) task, which has become a major research tool in the study of implicit learning. This article presents a reappraisal, replication and extension of an experiment that stands so far as one of the few alleged demonstrations of chunk learning in the SRT task (Koch and HoVmann, Psychological Res., 63:22–35, 2000). It shows that the eVects which were taken as evidence for chunk learning can indeed be obtained before any systematic training and thus surely reXect a preexistent tendency rather than a learned outcome. Further analyses of the eVects after extended practice conWrm that this tendency remains essentially unchanged over continuous training unlike what could be expected from a chunk-based account of sequence learning.},
	language = {en},
	number = {4},
	urldate = {2022-08-13},
	journal = {Psychological Research},
	author = {Jiménez, Luis},
	month = jul,
	year = {2008},
	pages = {387--396},
	file = {Jiménez - 2008 - Taking patterns for chunks is there any evidence .pdf:/Users/swu/Zotero/storage/U38PWS4V/Jiménez - 2008 - Taking patterns for chunks is there any evidence .pdf:application/pdf},
}

@article{french_tracx_2011,
	title = {{TRACX}: {A} recognition-based connectionist framework for sequence segmentation and chunk extraction.},
	volume = {118},
	issn = {1939-1471, 0033-295X},
	shorttitle = {{TRACX}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0025255},
	doi = {10.1037/a0025255},
	abstract = {Individuals of all ages extract structure from the sequences of patterns they encounter in their environment, an ability that is at the very heart of cognition. Exactly what underlies this ability has been the subject of much debate over the years. A novel mechanism, implicit chunk recognition (ICR), is proposed for sequence segmentation and chunk extraction. The mechanism relies on the recognition of previously encountered subsequences (chunks) in the input rather than on the prediction of upcoming items in the input sequence. A connectionist autoassociator model of ICR, truncated recursive autoassociative chunk extractor (TRACX), is presented in which chunks are extracted by means of truncated recursion. The performance and robustness of the model is demonstrated in a series of 9 simulations of empirical data, covering a wide range of phenomena from the infant statistical learning and adult implicit learning literatures, as well as 2 simulations demonstrating the model’s ability to generalize to new input and to develop internal representations whose structure reflects that of the items in the input sequence. TRACX outperforms PARSER (Perruchet \& Vintner, 1998) and the simple recurrent network (SRN, Cleeremans \& McClelland, 1991) in matching human sequence segmentation on existing data. A new study is presented exploring 8-month-olds’ use of backward transitional probabilities to segment auditory sequences.},
	language = {en},
	number = {4},
	urldate = {2022-08-13},
	journal = {Psychological Review},
	author = {French, Robert M. and Addyman, Caspar and Mareschal, Denis},
	month = oct,
	year = {2011},
	pages = {614--636},
	annote = {TRACX
recognition based connectionist model.

three layer feedbforward backpropagation autoassociator.
“Based on the difference between its prediction and the item that really occurs, it changes its weights to bring prediction closer to reality. TRACX relies on a fundamentally different approach to learning; that is, recognizing what it has previously encountered.” (French et al., 2011, p. 620)


“Autoassociators are neural networks that gradually learn to produce output that is identical to their input. Items that they have encountered frequently will be reproduced on output; items that have never been encountered before or that have been encountered only infrequently will produce output that does not resemble the input.” (French et al., 2011, p. 618)
},
	file = {French et al. - 2011 - TRACX A recognition-based connectionist framework.pdf:/Users/swu/Zotero/storage/KV4UZVDJ/French et al. - 2011 - TRACX A recognition-based connectionist framework.pdf:application/pdf},
}

@article{wickelgren_speed-accuracy_1977,
	title = {Speed-accuracy tradeoff and information processing dynamics},
	volume = {41},
	issn = {0001-6918},
	url = {https://www.sciencedirect.com/science/article/pii/0001691877900129},
	doi = {10.1016/0001-6918(77)90012-9},
	abstract = {For a long time, it has been known that one can tradeoff accuracy for speed in (presumably) any task. The range over which one can obtain substantial speed-accuracy tradeoff varies from 150 msec in some very simple perceptual tasks to 1,000 msec in some recognition memory tasks and presumably even longer in more complex cognitive tasks. Obtaining an entire speed-accuracy tradeoff function provides much greater knowledge concerning information processing dynamics than is obtained by a reaction- time experiment, which yields the equivalent of a single point on this function. For this and other reasons, speed-accuracy tradeoff studies are often preferable to reaction-time studies of the dynamics of perceptual, memory, and cognitive processes. Methods of obtaining speed-accuracy tradeoff functions include: instructions, payoffs, deadlines, bands, response signals (with blocked and mixed designs), and partitioning of reaction time. A combination of the mixed-design signal method supplemented by partitioning of reaction times appears to be the optimal method.},
	language = {en},
	number = {1},
	urldate = {2022-08-17},
	journal = {Acta Psychologica},
	author = {Wickelgren, Wayne A.},
	month = feb,
	year = {1977},
	keywords = {speed-accuracy trade-off},
	pages = {67--85},
	file = {ScienceDirect Snapshot:/Users/swu/Zotero/storage/T2C3F9UW/0001691877900129.html:text/html},
}

@article{meyniel_human_2016,
	title = {Human {Inferences} about {Sequences}: {A} {Minimal} {Transition} {Probability} {Model}},
	volume = {12},
	issn = {1553-7358},
	shorttitle = {Human {Inferences} about {Sequences}},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005260},
	doi = {10.1371/journal.pcbi.1005260},
	language = {en},
	number = {12},
	urldate = {2022-08-17},
	journal = {PLOS Computational Biology},
	author = {Meyniel, Florent and Maheu, Maxime and Dehaene, Stanislas},
	editor = {Gershman, Samuel J.},
	month = dec,
	year = {2016},
	keywords = {Forecasting, Learning, Reaction time, Brain electrophysiology, Electrophysiology, Entropy, Functional magnetic resonance imaging, Probability distribution},
	pages = {e1005260},
	file = {Meyniel et al. - 2016 - Human Inferences about Sequences A Minimal Transi.pdf:/Users/swu/Zotero/storage/6HIJYZRQ/Meyniel et al. - 2016 - Human Inferences about Sequences A Minimal Transi.pdf:application/pdf;Meyniel et al. - 2016 - Human Inferences about Sequences A Minimal Transi.pdf:/Users/swu/Zotero/storage/5DAGFA4L/Meyniel et al. - 2016 - Human Inferences about Sequences A Minimal Transi.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/MV24AV5L/article.html:text/html},
}

@article{maheu_brain_2019,
	title = {Brain signatures of a multiscale process of sequence learning in humans},
	volume = {8},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/41541},
	doi = {10.7554/eLife.41541},
	abstract = {Extracting the temporal structure of sequences of events is crucial for perception, decision-making, and language processing. Here, we investigate the mechanisms by which the brain acquires knowledge of sequences and the possibility that successive brain responses reflect the progressive extraction of sequence statistics at different timescales. We measured brain activity using magnetoencephalography in humans exposed to auditory sequences with various statistical regularities, and we modeled this activity as theoretical surprise levels using several learning models. Successive brain waves related to different types of statistical inferences. Early post-stimulus brain waves denoted a sensitivity to a simple statistic, the frequency of items estimated over a long timescale (habituation). Mid-latency and late brain waves conformed qualitatively and quantitatively to the computational properties of a more complex inference: the learning of recent transition probabilities. Our findings thus support the existence of multiple computational systems for sequence processing involving statistical inferences at multiple scales.},
	language = {en},
	urldate = {2022-08-17},
	journal = {eLife},
	author = {Maheu, Maxime and Dehaene, Stanislas and Meyniel, Florent},
	month = feb,
	year = {2019},
	pages = {e41541},
	file = {Maheu et al. - 2019 - Brain signatures of a multiscale process of sequen.pdf:/Users/swu/Zotero/storage/5QTPYJI2/Maheu et al. - 2019 - Brain signatures of a multiscale process of sequen.pdf:application/pdf},
}

@techreport{planton_mental_2020,
	title = {Mental compression of binary sequences in a language of thought},
	url = {https://psyarxiv.com/aez4w/},
	abstract = {The capacity to store information in working memory strongly depends upon the ability to recode the information in a compressed form. Here, we tested the theory that human adults encode binary sequences of stimuli in memory using a recursive compression algorithm akin to a “language of thought”, and capable of capturing nested patterns of repetitions and alternations. In five experiments, we probed memory for auditory or visual sequences using both subjective and objective measures. We used a sequence violation paradigm in which participants detected occasional violations in an otherwise fixed sequence. Both subjective ratings of complexity and objective sequence violation detection rates were well predicted by complexity, as measured by minimal description length (also known as Kolmogorov complexity) in the binary version of the “language of geometry”, a formal language previously found to account for the human encoding of complex spatial sequences in the proposed language. We contrasted the language model with a model based solely on surprise given the stimulus transition probabilities. While both models accounted for variance in the data, the language model dominated over the transition probability model for long sequences (with a number of elements far exceeding the limits of working memory). We use model comparison to show that the minimal description length in a recursive language provides a better fit than a variety of previous encoding models for sequences. The data support the hypothesis that, beyond the extraction of statistical knowledge, human sequence coding relies on an internal compression using language-like nested structures.},
	language = {en-us},
	urldate = {2022-08-17},
	institution = {PsyArXiv},
	author = {Planton, Samuel and Kerkoerle, Timo van and Abbih, Leïla and Maheu, Maxime and Meyniel, Florent and Sigman, Mariano and Wang, Liping and Figueira, Santiago and Romano, Sergio and Dehaene, Stanislas},
	month = jan,
	year = {2020},
	doi = {10.31234/osf.io/aez4w},
	note = {type: article},
	keywords = {Behavioral Neuroscience, Cognitive Psychology, complexity, language of thought, Neuroscience, novelty detection, sequence processing, Social and Behavioral Sciences, statistical learning},
	file = {Full Text PDF:/Users/swu/Zotero/storage/2Z4KPZHL/Planton et al. - 2020 - Mental compression of binary sequences in a langua.pdf:application/pdf},
}

@article{graybiel_striatum_2015,
	title = {The {Striatum}: {Where} {Skills} and {Habits} {Meet}},
	volume = {7},
	issn = {, 1943-0264},
	shorttitle = {The {Striatum}},
	url = {http://cshperspectives.cshlp.org/content/7/8/a021691},
	doi = {10.1101/cshperspect.a021691},
	abstract = {After more than a century of work concentrating on the motor functions of the basal ganglia, new ideas have emerged, suggesting that the basal ganglia also have major functions in relation to learning habits and acquiring motor skills. We review the evidence supporting the role of the striatum in optimizing behavior by refining action selection and in shaping habits and skills as a modulator of motor repertoires. These findings challenge the notion that striatal learning processes are limited to the motor domain. The learning mechanisms supported by striatal circuitry generalize to other domains, including cognitive skills and emotion-related patterns of action.},
	language = {en},
	number = {8},
	urldate = {2022-08-17},
	journal = {Cold Spring Harbor Perspectives in Biology},
	author = {Graybiel, Ann M. and Grafton, Scott T.},
	month = aug,
	year = {2015},
	pmid = {26238359},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	pages = {a021691},
	file = {Full Text PDF:/Users/swu/Zotero/storage/STNRG6ND/Graybiel and Grafton - 2015 - The Striatum Where Skills and Habits Meet.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/5LNPASQ4/a021691.html:text/html},
}

@misc{addyman_tracx-python_2022,
	title = {{TRACX}-{Python}},
	url = {https://github.com/InfantLab/TRACX-Python/blob/8ede0e83fd13575f3d2c9e236a342d1a663e9954/tracx.py},
	abstract = {Neural network model of sequence learning in adults and infants, French, Addyman \& Mareschal (2011)},
	urldate = {2022-08-17},
	author = {Addyman, Caspar},
	month = feb,
	year = {2022},
	note = {original-date: 2016-02-21T12:20:02Z},
}

@article{marcus_oculomotor_2006,
	title = {Oculomotor evidence of sequence learning on the serial reaction time task},
	volume = {34},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03193419},
	doi = {10.3758/BF03193419},
	language = {en},
	number = {2},
	urldate = {2022-08-17},
	journal = {Memory \& Cognition},
	author = {Marcus, David J. and Karatekin, Canan and Markiewicz, Steven},
	month = mar,
	year = {2006},
	pages = {420--432},
	file = {Marcus et al. - 2006 - Oculomotor evidence of sequence learning on the se.pdf:/Users/swu/Zotero/storage/B74RT2R2/Marcus et al. - 2006 - Oculomotor evidence of sequence learning on the se.pdf:application/pdf},
}

@article{robertson_awareness_2004,
	title = {Awareness {Modifies} the {Skill}-{Learning} {Benefits} of {Sleep}},
	volume = {14},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982204000399},
	doi = {10.1016/j.cub.2004.01.027},
	abstract = {Behind every skilled movement lies months of practice. However, practice alone is not responsible for the acquisition of all skill; performance can improve between, not just within, practice sessions. An important principle shaping these offline improvements may be an individual's awareness of learning a new skill. New skills, such as a sequence of finger movements, can be learned unintentionally (with little awareness for the sequence, implicit learning) or intentionally (explicit learning). We measured skill in an implicit and explicit sequence-learning task before and after a 12 hr interval. This interval either did (8 p.m. to 8 a.m.) or did not (8 a.m. to 8 p.m.) include a period of sleep. Following explicit sequence learning, offline skill improvements were only observed when the 12 hr interval included sleep. This overnight improvement was correlated with the amount of NREM sleep. The same improvement could also be observed in the evening (with an interval from 8 p.m. to 8 p.m.), so it was not coupled to retesting at a particular time of day and cannot therefore be attributed to circadian factors. In contrast, in the implicit learning task, offline learning was observed regardless of whether the 12 hr interval did or did not contain a period of sleep. However, these improvements were not observed with only a 15 min interval between sessions. Therefore, the practice available within each session cannot account for these skill improvements. Instead, sufficient time is necessary for offline learning to occur. These results show a behavioral dissociation, based upon an individual's awareness for having learned a sequence of finger movements. Offline learning is sleep dependent for explicit skills but time dependent for implicit skills.},
	language = {en},
	number = {3},
	urldate = {2022-08-17},
	journal = {Current Biology},
	author = {Robertson, Edwin M. and Pascual-Leone, Alvaro and Press, Daniel Z.},
	month = feb,
	year = {2004},
	pages = {208--212},
	file = {ScienceDirect Full Text PDF:/Users/swu/Zotero/storage/48AXKZDU/Robertson et al. - 2004 - Awareness Modifies the Skill-Learning Benefits of .pdf:application/pdf;ScienceDirect Snapshot:/Users/swu/Zotero/storage/7XZ596FZ/S0960982204000399.html:text/html},
}

@misc{noauthor_doi101016jcub200401027_nodate,
	title = {doi:10.1016/j.cub.2004.01.027 {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {doi},
	url = {https://reader.elsevier.com/reader/sd/pii/S0960982204000399?token=4742ECD66099BC615942991600B8AC4867264C60F7B46682AB1AE778BF7E357919AB49F871470A89C3E0AA0AD9F3F937&originRegion=eu-west-1&originCreation=20220817093910},
	language = {en},
	urldate = {2022-08-17},
	doi = {10.1016/j.cub.2004.01.027},
	file = {Full Text PDF:/Users/swu/Zotero/storage/6YTK9HCC/doi10.1016j.cub.2004.01.027  Elsevier Enhanced .pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/2F78KE57/S0960982204000399.html:text/html},
}

@article{willingham_direct_2002,
	title = {Direct {Comparison} of {Neural} {Systems} {Mediating} {Conscious} and {Unconscious} {Skill} {Learning}},
	volume = {88},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.2002.88.3.1451},
	doi = {10.1152/jn.2002.88.3.1451},
	abstract = {Procedural learning, such as perceptual-motor sequence learning, has been suggested to be an obligatory consequence of practiced performance and to reflect adaptive plasticity in the neural systems mediating performance. Prior neuroimaging studies, however, have found that sequence learning accompanied with awareness (declarative learning) of the sequence activates entirely different brain regions than learning without awareness of the sequence (procedural learning). Functional neuroimaging was used to assess whether declarative sequence learning prevents procedural learning in the brain. Awareness of the sequence was controlled by changing the color of the stimuli to match or differ from the color used for random sequences. This allowed direct comparison of brain activation associated with procedural and declarative memory for an identical sequence. Activation occurred in a common neural network whether initial learning had occurred with or without awareness of the sequence, and whether subjects were aware or not aware of the sequence during performance. There was widespread additional activation associated with awareness of the sequence. This supports the view that some types of unconscious procedural learning occurs in the brain whether or not it is accompanied by conscious declarative knowledge.},
	language = {en},
	number = {3},
	urldate = {2022-08-17},
	journal = {Journal of Neurophysiology},
	author = {Willingham, Daniel B. and Salidis, Joanna and Gabrieli, John D.E.},
	month = sep,
	year = {2002},
	pages = {1451--1460},
	file = {Willingham et al. - 2002 - Direct Comparison of Neural Systems Mediating Cons.pdf:/Users/swu/Zotero/storage/ITMTF3QH/Willingham et al. - 2002 - Direct Comparison of Neural Systems Mediating Cons.pdf:application/pdf},
}

@article{keele_cognitive_2003,
	title = {The cognitive and neural architecture of sequence representation.},
	volume = {110},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.110.2.316},
	doi = {10.1037/0033-295X.110.2.316},
	language = {en},
	number = {2},
	urldate = {2022-08-17},
	journal = {Psychological Review},
	author = {Keele, Steven W. and Ivry, Richard and Mayr, Ulrich and Hazeltine, Eliot and Heuer, Herbert},
	year = {2003},
	pages = {316--339},
	file = {Keele et al. - 2003 - The cognitive and neural architecture of sequence .pdf:/Users/swu/Zotero/storage/LMJACIG6/Keele et al. - 2003 - The cognitive and neural architecture of sequence .pdf:application/pdf},
}

@article{romano_one-year_2010,
	title = {One-year retention of general and sequence-specific skills in a probabilistic, serial reaction time task},
	volume = {18},
	issn = {0965-8211, 1464-0686},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09658211003742680},
	doi = {10.1080/09658211003742680},
	language = {en},
	number = {4},
	urldate = {2022-08-17},
	journal = {Memory},
	author = {Romano, Jennifer C. and Howard, James H. and Howard, Darlene V.},
	month = may,
	year = {2010},
	pages = {427--441},
	file = {Romano et al. - 2010 - One-year retention of general and sequence-specifi.pdf:/Users/swu/Zotero/storage/AS8E6KBB/Romano et al. - 2010 - One-year retention of general and sequence-specifi.pdf:application/pdf},
}

@article{robertson_serial_2007,
	title = {The {Serial} {Reaction} {Time} {Task}: {Implicit} {Motor} {Skill} {Learning}?},
	volume = {27},
	copyright = {Copyright © 2007 Society for Neuroscience 0270-6474/07/2710073-03\$15.00/0},
	issn = {0270-6474, 1529-2401},
	shorttitle = {The {Serial} {Reaction} {Time} {Task}},
	url = {https://www.jneurosci.org/content/27/38/10073},
	doi = {10.1523/JNEUROSCI.2747-07.2007},
	abstract = {Since its development 20 years ago, the serial reaction time task (SRTT) has gone from being a tool used by psychologists ([Nissen and Bullemer, 1987][1]) to one that, in the last few years, has been embraced by a wider community ([Fig. 1][2]). Embedded within this task is a sequence, a connected},
	language = {en},
	number = {38},
	urldate = {2022-08-17},
	journal = {Journal of Neuroscience},
	author = {Robertson, Edwin M.},
	month = sep,
	year = {2007},
	pmid = {17881512},
	note = {Publisher: Society for Neuroscience
Section: Toolbox},
	pages = {10073--10075},
	file = {Full Text PDF:/Users/swu/Zotero/storage/AVSMKJ7V/Robertson - 2007 - The Serial Reaction Time Task Implicit Motor Skil.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/EPVYTRJH/tab-figures-data.html:text/html},
}

@article{bornstein_cortical_2013,
	title = {Cortical and {Hippocampal} {Correlates} of {Deliberation} during {Model}-{Based} {Decisions} for {Rewards} in {Humans}},
	volume = {9},
	doi = {10.1371/journal.pcbi.1003387},
	abstract = {How do we use our memories of the past to guide decisions we've never had to make before? Although extensive work describes how the brain learns to repeat rewarded actions, decisions can also be influenced by associations between stimuli or events not directly involving reward - such as when planning routes using a cognitive map or chess moves using predicted countermoves - and these sorts of associations are critical when deciding among novel options. This process is known as model-based decision making. While the learning of environmental relations that might support model-based decisions is well studied, and separately this sort of information has been inferred to impact decisions, there is little evidence concerning the full cycle by which such associations are acquired and drive choices. Of particular interest is whether decisions are directly supported by the same mnemonic systems characterized for relational learning more generally, or instead rely on other, specialized representations. Here, building on our previous work, which isolated dual representations underlying sequential predictive learning, we directly demonstrate that one such representation, encoded by the hippocampal memory system and adjacent cortical structures, supports goal-directed decisions. Using interleaved learning and decision tasks, we monitor predictive learning directly and also trace its influence on decisions for reward. We quantitatively compare the learning processes underlying multiple behavioral and fMRI observables using computational model fits. Across both tasks, a quantitatively consistent learning process explains reaction times, choices, and both expectation- and surprise-related neural activity. The same hippocampal and ventral stream regions engaged in anticipating stimuli during learning are also engaged in proportion to the difficulty of decisions. These results support a role for predictive associations learned by the hippocampal memory system to be recalled during choice formation.},
	journal = {PLoS computational biology},
	author = {Bornstein, Aaron and Daw, Nathaniel},
	month = dec,
	year = {2013},
	pages = {e1003387},
	file = {Full Text:/Users/swu/Zotero/storage/5WFF8DY9/Bornstein and Daw - 2013 - Cortical and Hippocampal Correlates of Deliberatio.pdf:application/pdf},
}

@article{schvaneveldt_attention_1998,
	title = {Attention and probabilistic sequence learning},
	volume = {61},
	issn = {1430-2772},
	url = {https://doi.org/10.1007/s004260050023},
	doi = {10.1007/s004260050023},
	abstract = {Limitations of using fixed sequences of events in studies of learning in the sequential reaction-time task led us to develop a probabilistic version of the task. When sequences occur probabilistically, transitions usually follow a sequence, but with some small probability, events occur out of sequence. This variation on the paradigm provides new evidence associated with manipulations of attentional load. Most notably, single-task learning leads to particularly high error rates on improbable transitions, suggesting anticipation of the sequence. Dual-task learning shows sensitivity to the sequence (by reaction-time differences to probable and improbable transitions), but without inflated errors on improbable transitions. Sensitivity to the sequence and anticipatory errors disappeared when participants transferred from single-task learning to dual-task conditions, suggesting that what is learned with single-task practice cannot be applied under conditions of limited attention. When learners transferred from dual- to single-task conditions, sensitivity of RT to the sequence increased but anticipation errors remained the same, suggesting that attentional load limits performance, but not learning. Qualitative differences in performance result from variations in attentional resources, which may reflect different learning processes.},
	language = {en},
	number = {3},
	urldate = {2022-08-17},
	journal = {Psychological Research},
	author = {Schvaneveldt, Roger W. and Gomez, Rebecca L.},
	month = aug,
	year = {1998},
	keywords = {Attentional Resource, High Error Rate, Limited Attention, Probabilistic Sequence, Sequence Learning},
	pages = {175--190},
	file = {Full Text PDF:/Users/swu/Zotero/storage/P59XFUF8/Schvaneveldt and Gomez - 1998 - Attention and probabilistic sequence learning.pdf:application/pdf},
}

@article{sense_probabilistic_2018,
	title = {Probabilistic motor sequence learning in a virtual reality serial reaction time task},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0198759},
	doi = {10.1371/journal.pone.0198759},
	abstract = {The serial reaction time task is widely used to study learning and memory. The task is traditionally administered by showing target positions on a computer screen and collecting responses using a button box or keyboard. By comparing response times to random or sequenced items or by using different transition probabilities, various forms of learning can be studied. However, this traditional laboratory setting limits the number of possible experimental manipulations. Here, we present a virtual reality version of the serial reaction time task and show that learning effects emerge as expected despite the novel way in which responses are collected. We also show that response times are distributed as expected. The current experiment was conducted in a blank virtual reality room to verify these basic principles. For future applications, the technology can be used to modify the virtual reality environment in any conceivable way, permitting a wide range of previously impossible experimental manipulations.},
	language = {en},
	number = {6},
	urldate = {2022-08-17},
	journal = {PLOS ONE},
	author = {Sense, Florian and Rijn, Hedderik van},
	month = jun,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Human learning, Learning, Learning and memory, Motor reactions, Neuropsychological testing, Reaction time, Virtual reality},
	pages = {e0198759},
	file = {Full Text PDF:/Users/swu/Zotero/storage/JG8E9JAJ/Sense and Rijn - 2018 - Probabilistic motor sequence learning in a virtual.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/CC8SRDYW/article.html:text/html},
}

@article{aslin_statistical_nodate,
	title = {Statistical {Learning}},
	abstract = {Statistical learning is a rapid and robust mechanism that enables adults and infants to extract patterns embedded in both language and visual domains. Statistical learning operates implicitly, without instruction, through mere exposure to a set of input stimuli. However, much of what learners must acquire about a structured domain consists of principles or rules that can be applied to novel inputs. It has been claimed that statistical learning and rule learning are separate mechanisms; in this article, however, we review evidence and provide a unifying perspective that argues for a single statistical-learning mechanism that accounts for both the learning of input stimuli and the generalization of learned patterns to novel instances. The balance between instance-learning and generalization is based on two factors: the strength of perceptual and cognitive biases that highlight structural regularities, and the consistency of elements’ contexts (unique vs. overlapping) in the input.},
	language = {en},
	author = {Aslin, Richard N and Newport, Elissa L},
	pages = {7},
	file = {Aslin and Newport - Statistical Learning.pdf:/Users/swu/Zotero/storage/SME847V2/Aslin and Newport - Statistical Learning.pdf:application/pdf},
}

@article{jin_basal_2014,
	title = {Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences},
	volume = {17},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn.3632},
	doi = {10.1038/nn.3632},
	abstract = {Chunking allows the brain to efficiently organize memories and actions. Although basal ganglia circuits have been implicated in action chunking, little is known about how individual elements are concatenated into a behavioral sequence at the neural level. Using a task where mice learn rapid action sequences, we uncovered neuronal activity encoding entire sequences as single actions in basal ganglia circuits. Besides start/stop activity signaling sequence parsing, we found neurons displaying inhibited or sustained activity throughout the execution of an entire sequence. This sustained activity covaried with the rate of execution of individual sequence elements, consistent with motor concatenation. Direct and indirect pathways of basal ganglia were concomitantly active during sequence initiation, but behaved differently during sequence performance, revealing a more complex functional organization of these circuits than previously postulated. These results have important implications for understanding the functional organization of basal ganglia during the learning and execution of action sequences.},
	language = {en},
	number = {3},
	urldate = {2022-08-18},
	journal = {Nature Neuroscience},
	author = {Jin, Xin and Tecuapetla, Fatuel and Costa, Rui M},
	month = mar,
	year = {2014},
	pages = {423--430},
	file = {Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:/Users/swu/Zotero/storage/33I8X2WX/Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:application/pdf;Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:/Users/swu/Zotero/storage/TTFZSSHJ/Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:application/pdf;Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:/Users/swu/Zotero/storage/3RA9T7BN/Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf:application/pdf},
}

@article{laird_towards_nodate,
	title = {Towards {Chunking} as a {General} {Learning} {Mechanism}},
	abstract = {Chunks have long been proposed as a basic organizational unit for human memory. More recently chunks have been used to model human learning on simple perceptual-motor skills. In this paper we describe recent progress in extending chunking to be a general learning mechanism by implementing it within a general problem solver. Using the Soar problem-solving architecture, we take significant steps toward a general problem solver that can learn about all aspects of its behavior. We demonstrate chunking in Soar on three tasks: the Eight Puzzle, Tic-Tat-Toe, and a part of the RI computer-configuration task. Not only is there improvement with practice, but chunking also produces significant transfer of learned behavior, and strategy acquisition.},
	language = {en},
	author = {Laird, John E and Rosenbloom, Paul S and Newell, Allen},
	pages = {5},
	file = {Laird et al. - Towards Chunking as a General Learning Mechanism.pdf:/Users/swu/Zotero/storage/4GB9NHR4/Laird et al. - Towards Chunking as a General Learning Mechanism.pdf:application/pdf},
}

@article{du_new_2017,
	title = {New insights into statistical learning and chunk learning in implicit sequence acquisition},
	volume = {24},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-016-1193-4},
	doi = {10.3758/s13423-016-1193-4},
	language = {en},
	number = {4},
	urldate = {2022-08-18},
	journal = {Psychonomic Bulletin \& Review},
	author = {Du, Yue and Clark, Jane E.},
	month = aug,
	year = {2017},
	pages = {1225--1233},
	file = {Du and Clark - 2017 - New insights into statistical learning and chunk l.pdf:/Users/swu/Zotero/storage/AWC5M95F/Du and Clark - 2017 - New insights into statistical learning and chunk l.pdf:application/pdf},
}

@article{saffran_statistical_1996,
	title = {Statistical {Learning} by 8-{Month}-{Old} {Infants}},
	volume = {274},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.274.5294.1926},
	doi = {10.1126/science.274.5294.1926},
	language = {en},
	number = {5294},
	urldate = {2022-08-18},
	journal = {Science},
	author = {Saffran, Jenny R. and Aslin, Richard N. and Newport, Elissa L.},
	month = dec,
	year = {1996},
	pages = {1926--1928},
	file = {Saffran et al. - 1996 - Statistical Learning by 8-Month-Old Infants.pdf:/Users/swu/Zotero/storage/P9AZC67V/Saffran et al. - 1996 - Statistical Learning by 8-Month-Old Infants.pdf:application/pdf},
}

@article{jones_long-term_2018,
	title = {Long-term associative learning predicts verbal short-term memory performance},
	volume = {46},
	issn = {1532-5946},
	url = {https://doi.org/10.3758/s13421-017-0759-3},
	doi = {10.3758/s13421-017-0759-3},
	abstract = {Studies using tests such as digit span and nonword repetition have implicated short-term memory across a range of developmental domains. Such tests ostensibly assess specialized processes for the short-term manipulation and maintenance of information that are often argued to enable long-term learning. However, there is considerable evidence for an influence of long-term linguistic learning on performance in short-term memory tasks that brings into question the role of a specialized short-term memory system separate from long-term knowledge. Using natural language corpora, we show experimentally and computationally that performance on three widely used measures of short-term memory (digit span, nonword repetition, and sentence recall) can be predicted from simple associative learning operating on the linguistic environment to which a typical child may have been exposed. The findings support the broad view that short-term verbal memory performance reflects the application of long-term language knowledge to the experimental setting.},
	language = {en},
	number = {2},
	urldate = {2022-08-18},
	journal = {Memory \& Cognition},
	author = {Jones, Gary and Macken, Bill},
	month = feb,
	year = {2018},
	keywords = {Associative learning, CLASSIC, Digit span, Nonword repetition, Short-term memory},
	pages = {216--229},
	file = {Full Text PDF:/Users/swu/Zotero/storage/L45VXENM/Jones and Macken - 2018 - Long-term associative learning predicts verbal sho.pdf:application/pdf},
}

@article{verwey_distinct_2012,
	title = {Distinct modes of executing movement sequences: {Reacting}, associating, and chunking},
	volume = {140},
	issn = {0001-6918},
	shorttitle = {Distinct modes of executing movement sequences},
	url = {https://www.sciencedirect.com/science/article/pii/S0001691812000789},
	doi = {10.1016/j.actpsy.2012.05.007},
	abstract = {Responding to individual key-specific stimuli in entirely unfamiliar keying sequences is said to involve a reaction mode. With practice, short keying sequences can be executed in the chunking mode. This is indicated by the first stimulus sufficing for rapid execution of the entire sequence. The present study explored whether an associative mode develops also in participants who practice short keying sequences. This associative mode would involve priming by earlier events of responses to external stimuli, and is believed to be responsible for skill in the Serial Reaction Time task. In the present study participants practiced two discrete 6-key sequences. In the ensuing test phase, participants were prevented from using the chunking mode by including two deviant stimuli in most sequences. The results from the remaining – unchanged – familiar sequences confirmed that participants no longer used the chunking mode, but as predicted by associative learning these sequences were executed faster than unfamiliar sequences.},
	language = {en},
	number = {3},
	urldate = {2022-08-18},
	journal = {Acta Psychologica},
	author = {Verwey, Willem B. and Abrahamse, Elger L.},
	month = jul,
	year = {2012},
	keywords = {Associative sequence learning, Discrete Sequence Production task, Keying sequences, Processing modes, Serial RT task},
	pages = {274--282},
	file = {ScienceDirect Snapshot:/Users/swu/Zotero/storage/YCSFVK2U/S0001691812000789.html:text/html},
}

@article{fu_implicit_2018,
	title = {Implicit sequence learning of chunking and abstract structures},
	volume = {62},
	issn = {1053-8100},
	url = {https://www.sciencedirect.com/science/article/pii/S1053810017303951},
	doi = {10.1016/j.concog.2018.04.010},
	abstract = {The current study investigated whether people can simultaneously acquire knowledge about concrete chunks and abstract structures in implicit sequence learning; and whether the degree of abstraction determines the conscious status of the acquired knowledge. We adopted three types of stimuli in a serial reaction time task in three experiments. The RT results indicated that people could simultaneously acquire knowledge about concrete chunks and abstract structures of the temporal sequence. Generation performance revealed that ability to control was mainly based on abstract structures rather than concrete chunks. Moreover, ability to control was not generally accompanied with awareness of knowing or knowledge, as measured by confidence ratings and attribution tests, confirming that people could control the use of unconscious knowledge of abstract structures. The results present a challenge to computational models and theories of implicit learning.},
	language = {en},
	urldate = {2022-08-18},
	journal = {Consciousness and Cognition},
	author = {Fu, Qiufang and Sun, Huiming and Dienes, Zoltán and Fu, Xiaolan},
	month = jul,
	year = {2018},
	keywords = {Abstract structures, Concrete chunks, Implicit learning, Sequence learning},
	pages = {42--56},
	file = {Accepted Version:/Users/swu/Zotero/storage/6NYTPKSJ/Fu et al. - 2018 - Implicit sequence learning of chunking and abstrac.pdf:application/pdf;ScienceDirect Snapshot:/Users/swu/Zotero/storage/8AV2W2LC/S1053810017303951.html:text/html},
}

@article{ioannucci_conscious_2021,
	title = {Conscious awareness of motor fluidity improves performance and decreases cognitive effort in sequence learning},
	volume = {95},
	issn = {1053-8100},
	url = {https://www.sciencedirect.com/science/article/pii/S105381002100146X},
	doi = {10.1016/j.concog.2021.103220},
	abstract = {Motor skill learning is improved when participants are instructed to judge after each trial whether their performed movements have reached maximal fluidity. Consequently, the conscious awareness of this maximal fluidity can be classified as a genuine learning factor for motor sequences. However, it is unknown whether this effect of conscious awareness on motor learning could be mediated by the increased cognitive effort that may accompany such judgment making. The main aim of this study was to test this hypothesis in comparing two groups with, and without, the conscious awareness of the maximal fluidity. To assess the possible involvement of cognitive effort, we have recorded the pupillary dilation to the task, which is well-known to increase in proportion to cognitive effort. Results confirmed that conscious awareness indeed improved motor sequence learning of the trained sequence specifically. Pupil dilation was smaller during trained than during novel sequence performance, indicating that sequence learning decreased the cognitive cost of sequence execution. However, we found that in the group that had to judge on their maximal fluidity, pupil dilation during sequence production was smaller than in the control group, indicating that the motor improvement induced by the fluidity judgment does not involve additional cognitive effort. We discuss these results in the context of motor learning and cognitive effort theories.},
	language = {en},
	urldate = {2022-08-24},
	journal = {Consciousness and Cognition},
	author = {Ioannucci, Stefano and Boutin, Arnaud and Michelet, Thomas and Zenon, Alexandre and Badets, Arnaud},
	month = oct,
	year = {2021},
	keywords = {Motor learning, Conscious awareness, Motor sequence, Pupil dilation},
	pages = {103220},
	file = {ScienceDirect Snapshot:/Users/swu/Zotero/storage/QR5B4J5R/S105381002100146X.html:text/html},
}

@article{song_impact_2014,
	title = {Impact of conscious intent on chunking during motor learning},
	volume = {21},
	issn = {1072-0502, 1549-5485},
	url = {http://learnmem.cshlp.org/content/21/9/449},
	doi = {10.1101/lm.035824.114},
	abstract = {Humans and other mammals learn sequences of movements by splitting them into smaller “chunks.” Such chunks are defined by the faster speed of performance of groups of movements. The purpose of this report is to determine how conscious intent to learn impacts chunking, an issue that remains unknown. Here, we studied 80 subjects who either with or without conscious intent learned a motor sequence. Performance was tested before and up to 1-wk post-training. Chunk formation, carryover of chunks, and concatenation of chunks into longer chunks, all measures of motor chunking success, were determined at each time-point. We found that formation, carryover, and concatenation of chunks were comparable across groups and did not improve over the training session and subsequent testing times. Thus, motor learning progressed in the absence of improvements in chunking irrespective of conscious intent. These data suggest that mechanisms other than chunking contribute to successful motor learning with and without conscious intent.},
	language = {en},
	number = {9},
	urldate = {2022-08-24},
	journal = {Learning \& Memory},
	author = {Song, Sunbin and Cohen, Leonardo},
	month = sep,
	year = {2014},
	pmid = {25128535},
	note = {Company: Cold Spring Harbor Laboratory Press
Distributor: Cold Spring Harbor Laboratory Press
Institution: Cold Spring Harbor Laboratory Press
Label: Cold Spring Harbor Laboratory Press
Publisher: Cold Spring Harbor Lab},
	pages = {449--451},
	file = {Full Text PDF:/Users/swu/Zotero/storage/46C23T59/Song and Cohen - 2014 - Impact of conscious intent on chunking during moto.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/AVXHRZKA/449.full.html:text/html;Song and Cohen - 2014 - Impact of conscious intent on chunking during moto.pdf:/Users/swu/Zotero/storage/WJ9BA8L7/Song and Cohen - 2014 - Impact of conscious intent on chunking during moto.pdf:application/pdf;Song and Cohen - 2014 - Impact of conscious intent on chunking during moto.pdf:/Users/swu/Zotero/storage/IH9UZ6CY/Song and Cohen - 2014 - Impact of conscious intent on chunking during moto.pdf:application/pdf},
}

@article{prashad_sequence_2021,
	title = {Sequence {Structure} {Has} a {Differential} {Effect} on {Underlying} {Motor} {Learning} {Processes}},
	volume = {9},
	issn = {2325-3193, 2325-3215},
	url = {https://journals.humankinetics.com/view/journals/jmld/9/1/article-p38.xml},
	doi = {10.1123/jmld.2020-0031},
	abstract = {Current methods to understand implicit motor sequence learning inadequately assess motor skill acquisition in daily life. Using fixed sequences in the serial reaction time task is not ideal as participants may become aware of the sequence, thereby changing the learning from implicit to explicit. Probabilistic sequences, in which stimuli are linked by statistical, rather than deterministic, associations can ensure that learning remains implicit. Additionally, the processes underlying the learning of motor sequences may differ based on sequence structure. Here, the authors compared the learning of fixed and probabilistic sequences to randomly ordered stimuli using a modified serial reaction time task. Both the fixed and probabilistic sequence groups exhibited learning as indicated by decreased response time and variability. In the initial stage of learning, fixed sequences exhibited both online and offline gains in response time; however, only the offline gain was observed during the learning of probabilistic sequences. These results indicated that probabilistic structures may be learned differently from fixed structures and have important implications for our current understanding of motor learning. Probabilistic sequences more accurately reflect motor skill acquisition in daily life, offer ecological validity to the serial reaction time framework, and advance our understanding of motor learning.},
	language = {en},
	number = {1},
	urldate = {2022-08-24},
	journal = {Journal of Motor Learning and Development},
	author = {Prashad, Shikha and Du, Yue and Clark, Jane E.},
	month = jan,
	year = {2021},
	note = {Publisher: Human Kinetics
Section: Journal of Motor Learning and Development},
	pages = {38--57},
	file = {Snapshot:/Users/swu/Zotero/storage/XKSRR3TH/article-p38.html:text/html},
}

@article{perlman_task-relevant_2010,
	title = {Task-relevant chunking in sequence learning.},
	volume = {36},
	issn = {1939-1277, 0096-1523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0017178},
	doi = {10.1037/a0017178},
	abstract = {In the present study, we investigated possible influences on the unitization of responses. In Experiments 1, 2, 3, and 6, we found that when the same small fragment (i.e., a few consecutive responses in a sequence) was presented as part of two larger sequences, participants responded to it faster when it was part of the sequence that was presented more often. This indicates that chunking can be driven by task-relevant considerations, as opposed to co-occurrence. The results are discussed in the context of chunking theories and the relevant motor learning literature.},
	language = {en},
	number = {3},
	urldate = {2022-08-24},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Perlman, Amotz and Pothos, Emmanuel M. and Edwards, Darren J. and Tzelgov, Joseph},
	month = jun,
	year = {2010},
	pages = {649--661},
	file = {Perlman et al. - 2010 - Task-relevant chunking in sequence learning..pdf:/Users/swu/Zotero/storage/FIBF46E4/Perlman et al. - 2010 - Task-relevant chunking in sequence learning..pdf:application/pdf},
}

@article{schmitt_empirical_1977,
	title = {Empirical approaches to information processing: {Speed}-accuracy tradeoff functions or reaction time},
	volume = {41},
	issn = {0001-6918},
	shorttitle = {Empirical approaches to information processing},
	url = {https://www.sciencedirect.com/science/article/pii/0001691877900257},
	doi = {10.1016/0001-6918(77)90025-7},
	abstract = {Recently Wickelgren has advocated the abandonment of reaction time research and its replacement with speed-accuracy methodology. This paper takes issue with the speed-accuracy approach and argues that the proposed methodology has little advantage over reaction time studies. Furthermore it is argued that speed-accuracy studies may have severe flaws of their own.},
	language = {en},
	number = {4},
	urldate = {2022-08-24},
	journal = {Acta Psychologica},
	author = {Schmitt, John C. and Scheirer, C. James},
	month = jun,
	year = {1977},
	pages = {321--325},
	file = {ScienceDirect Snapshot:/Users/swu/Zotero/storage/P6YJKFRD/0001691877900257.html:text/html},
}

@article{fitts_cognitive_1966,
	title = {Cognitive aspects of information processing: {III}. {Set} for speed versus accuracy},
	volume = {71},
	issn = {0022-1015},
	shorttitle = {Cognitive aspects of information processing},
	doi = {10.1037/h0023232},
	abstract = {Examines the capacity of Os to adapt to changes in the relative emphasis on speed vs. accuracy of responses. 3 matched groups of 6 Os each were trained for 3 days in a choice reaction-time (RT) task, with feedback indicating both speed and accuracy. Emphasis on speed decreased mean RT but increased errors. A control group, working without an exact payoff or immediate feedback, showed somewhat greater within- and between-S variability than did either the speed or accuracy groups and was at an intermediate level on all performance measures. Similar distributions of RTs were found for correct responses and for errors as was predicted by a sequential sampling and decision model of choice RT. RT distributions for all Os were approximately normal under a set for speed, but under accuracy instructions some Os gave highly skewed distributions. (21 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Journal of Experimental Psychology},
	author = {Fitts, Paul M.},
	year = {1966},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Automated Information Processing, Cognitive Processes, Reaction Time},
	pages = {849--857},
	file = {Snapshot:/Users/swu/Zotero/storage/6LCMMJBY/1966-07366-001.html:text/html},
}

@misc{noauthor_applying_nodate,
	title = {Applying an exemplar model to the serial reaction-time task: {Anticipating} from experience - {Randall} {K}. {Jamieson}, {D}. {J}. {K}. {Mewhort}, 2009},
	url = {https://journals.sagepub.com/doi/abs/10.1080/17470210802557637},
	urldate = {2022-08-24},
	file = {Applying an exemplar model to the serial reaction-time task\: Anticipating from experience - Randall K. Jamieson, D. J. K. Mewhort, 2009:/Users/swu/Zotero/storage/3BYP8Q5Q/17470210802557637.html:text/html;Visual Behavior Neuropixels - brain-map.org:/Users/swu/Zotero/storage/QVZVY6SV/visual-behavior-neuropixels.html:text/html},
}

@article{song_perceptual_2008,
	title = {Perceptual sequence learning in a serial reaction time task},
	volume = {189},
	issn = {1432-1106},
	url = {https://doi.org/10.1007/s00221-008-1411-z},
	doi = {10.1007/s00221-008-1411-z},
	abstract = {In the serial reaction time task (SRTT), a sequence of visuo-spatial cues instructs subjects to perform a sequence of movements which follow a repeating pattern. Though motor responses are known to support implicit sequence learning in this task, the goal of the present experiments is to determine whether observation of the sequence of cues alone can also yield evidence of implicit sequence learning. This question has been difficult to answer because in previous research, performance improvements which appeared to be due to implicit perceptual sequence learning could also be due to spontaneous increases in explicit knowledge of the sequence. The present experiments use probabilistic sequences to prevent the spontaneous development of explicit awareness. They include a training phase, during which half of the subjects observe and the other half respond, followed by a transfer phase in which everyone responds. Results show that observation alone can support sequence learning, which translates at transfer into equivalent performance as that of a group who made motor responses during training. However, perceptual learning or its expression is sensitive to changes in target colors, and its expression is impaired by concurrent explicit search. Motor-response based learning is not affected by these manipulations. Thus, observation alone can support implicit sequence learning, even of higher order probabilistic sequences. However, perceptual learning can be prevented or concealed by variations of stimuli or task demands.},
	language = {en},
	number = {2},
	urldate = {2022-08-24},
	journal = {Experimental Brain Research},
	author = {Song, Sunbin and Howard, James H. and Howard, Darlene V.},
	month = aug,
	year = {2008},
	keywords = {Sequence learning, Explicit, Implicit, Motor learning, Perceptual learning},
	pages = {145--158},
	file = {Accepted Version:/Users/swu/Zotero/storage/4EABZFDU/Song et al. - 2008 - Perceptual sequence learning in a serial reaction .pdf:application/pdf},
}

@article{cohen_off-line_2005,
	title = {Off-line learning of motor skill memory: {A} double dissociation of goal and movement},
	volume = {102},
	copyright = {Copyright © 2005, The National Academy of Sciences},
	issn = {1633-0773},
	shorttitle = {Off-line learning of motor skill memory},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0506072102},
	doi = {10.1073/pnas.0506072102},
	abstract = {Acquiring a new skill requires learning multiple aspects of a task simultaneously.
For example, learning a piano sonata requires learning the music...},
	language = {EN},
	number = {50},
	urldate = {2022-08-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cohen, Daniel A. and Pascual-Leone, Alvaro and Press, Daniel Z. and Robertson, Edwin M.},
	month = dec,
	year = {2005},
	note = {Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences
Publisher: Proceedings of the National Academy of Sciences},
	pages = {18237--18241},
	file = {Full Text PDF:/Users/swu/Zotero/storage/IFWWSMWI/Cohen et al. - 2005 - Off-line learning of motor skill memory A double .pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/E7XVXFCH/pnas.html:text/html},
}

@article{torriero_interference_2004,
	title = {Interference of {Left} and {Right} {Cerebellar} {rTMS} with {Procedural} {Learning}},
	volume = {16},
	issn = {0898-929X},
	url = {https://doi.org/10.1162/0898929042568488},
	doi = {10.1162/0898929042568488},
	abstract = {Increasing evidence suggests cerebellar involvement in procedural learning. To further analyze its role and to assess whether it has a lateralized influence, in the present study we used a repetitive transcranial magnetic stimulation interference approach in a group of normal subjects performing a serial reaction time task.We studied 36 normal volunteers: 13 subjects underwent repetitive transcranial magnetic stimulation on the left cerebellum and performed the task with the right (6 subjects) or left (7 subjects) hand; 10 subjects underwent repetitive transcranial magnetic stimulation on the right cerebellum and performed the task with the hand ipsilateral (5 subjects) or contralateral (5 subjects) to the stimulation; another 13 subjects served as controls and were not submitted to repetitive transcranial magnetic stimulation; 7 of them performed the task with the right hand and 6 with the left hand. The main results show that interference with the activity of the lateral cerebellum induces a significant decrease of procedural learning: Interference with the right cerebellar hemisphere activity induces a significant decrease in procedural learning regardless of the hand used to perform the serial reaction time task, whereas left cerebellar hemisphere activity seems more linked with procedural learning through the ipsilateral hand.In conclusion, the present study shows for the first time that a transient interference with the functions of the cerebellar cortex results in an impairment of procedural learning in normal subjects and it provides new evidences for interhemispheric differences in the lateral cerebellum.},
	number = {9},
	urldate = {2022-08-24},
	journal = {Journal of Cognitive Neuroscience},
	author = {Torriero, Sara and Oliveri, Massimiliano and Koch, Giacomo and Caltagirone, Carlo and Petrosini, Laura},
	month = nov,
	year = {2004},
	pages = {1605--1611},
	file = {Snapshot:/Users/swu/Zotero/storage/WW45P7T5/Interference-of-Left-and-Right-Cerebellar-rTMS.html:text/html;Submitted Version:/Users/swu/Zotero/storage/H7LRL6IX/Torriero et al. - 2004 - Interference of Left and Right Cerebellar rTMS wit.pdf:application/pdf},
}

@article{robertson_role_2001,
	title = {The {Role} of the {Dorsolateral} {Prefrontal} {Cortex} during {Sequence} {Learning} is {Specific} for {Spatial} {Information}},
	volume = {11},
	issn = {1047-3211},
	url = {https://doi.org/10.1093/cercor/11.7.628},
	doi = {10.1093/cercor/11.7.628},
	abstract = {Many studies have implicated the dorsolateral prefrontal cortex in the acquisition of skill, including procedural sequence learning. However, the specific role it performs in sequence learning has remained uncertain. This type of skill has been intensively studied using the serial reaction time task. We used three versions of this task: a standard task where the position of the stimulus cued the response; a non-standard task where the color of the stimulus was related to the correct response; and a combined task where both the color and position simultaneously cued the response. We refer to each of these tasks based upon the cues available for guiding learning as position, color and combined tasks. The combined task usually shows an enhancement of skill acquisition, a result of being driven by two simultaneous and congruent cues. Prior to the performance of each of these tasks the function of the dorsolateral prefrontal cortex was disrupted using repetitive transcranial magnetic stimulation. This completely prevented learning within the position task, while sequence learning occurred to a similar extent in both the color and combined tasks. So, following prefrontal stimulation the expected learning enhancement in the combined task was lost, consistent with only a color cue being available to guide sequence learning in the combined task. Neither of these effects was observed following stimulation at the parietal cortex. Hence the critical role played by the dorsolateral prefrontal cortex in sequence learning is related exclusively to spatial cues. We suggest that the dorsolateral prefrontal cortex operates over the short term to retain and manipulate spatial information to allow cortical and subcortical structures to learn a predictable sequence of actions. Such functions may emerge from the broader role the dorsolateral prefrontal cortex has in spatial working memory. These results argue against the dorsolateral prefrontal cortex constituting part of the neuronal substrate responsible for general aspects of implicit or explicit sequence learning.},
	number = {7},
	urldate = {2022-08-24},
	journal = {Cerebral Cortex},
	author = {Robertson, E.M. and Tormos, J.M. and Maeda, F. and Pascual-Leone, A.},
	month = jul,
	year = {2001},
	pages = {628--635},
	file = {Full Text PDF:/Users/swu/Zotero/storage/TSS5RSCB/Robertson et al. - 2001 - The Role of the Dorsolateral Prefrontal Cortex dur.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/ILDQ569A/317510.html:text/html},
}

@misc{noauthor_procedural_nodate,
	title = {Procedural learning in {Parkinson}'s disease and cerebellar degeneration - {Pascual}‐{Leone} - 1993 - {Annals} of {Neurology} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ana.410340414?casa_token=rpFW4PjuZEQAAAAA%3AKXHIDE_FLzqXW_LviTvSGCTwNRZZKJk9f3aJ4ZTo1z0uXmIgjs-qmpTXDj20eJkM7GBKYNZZ6Ggc4EA},
	urldate = {2022-08-24},
	file = {Procedural learning in Parkinson's disease and cerebellar degeneration - Pascual‐Leone - 1993 - Annals of Neurology - Wiley Online Library:/Users/swu/Zotero/storage/9A3MREDY/ana.html:text/html},
}

@article{provyn_associative_2013,
	title = {Associative {Processes} in {Statistical} {Learning}: {Paradoxical} {Predictions} of the {Past}},
	volume = {179},
	language = {en},
	journal = {Psychology-Dissertations},
	author = {Provyn, Jennifer Patricia},
	month = may,
	year = {2013},
	keywords = {SRTT},
	pages = {78},
	file = {Provyn - Associative Processes in Statistical Learning Par.pdf:/Users/swu/Zotero/storage/WU4JRMJ4/Provyn - Associative Processes in Statistical Learning Par.pdf:application/pdf},
}

@article{du_probabilistic_2016,
	title = {Probabilistic {Motor} {Sequence} {Yields} {Greater} {Offline} and {Less} {Online} {Learning} than {Fixed} {Sequence}},
	volume = {10},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2016.00087},
	abstract = {It is well acknowledged that motor sequences can be learned quickly through online learning. Subsequently, the initial acquisition of a motor sequence is boosted or consolidated by offline learning. However, little is known whether offline learning can drive the fast learning of motor sequences (i.e., initial sequence learning in the first training session). To examine offline learning in the fast learning stage, we asked four groups of young adults to perform the serial reaction time (SRT) task with either a fixed or probabilistic sequence and with or without preliminary knowledge (PK) of the presence of a sequence. The sequence and PK were manipulated to emphasize either procedural (probabilistic sequence; no preliminary knowledge (NPK)) or declarative (fixed sequence; with PK) memory that were found to either facilitate or inhibit offline learning. In the SRT task, there were six learning blocks with a 2 min break between each consecutive block. Throughout the session, stimuli followed the same fixed or probabilistic pattern except in Block 5, in which stimuli appeared in a random order. We found that PK facilitated the learning of a fixed sequence, but not a probabilistic sequence. In addition to overall learning measured by the mean reaction time (RT), we examined the progressive changes in RT within and between blocks (i.e., online and offline learning, respectively). It was found that the two groups who performed the fixed sequence, regardless of PK, showed greater online learning than the other two groups who performed the probabilistic sequence. The groups who performed the probabilistic sequence, regardless of PK, did not display online learning, as indicated by a decline in performance within the learning blocks. However, they did demonstrate remarkably greater offline improvement in RT, which suggests that they are learning the probabilistic sequence offline. These results suggest that in the SRT task, the fast acquisition of a motor sequence is driven by concurrent online and offline learning. In addition, as the acquisition of a probabilistic sequence requires greater procedural memory compared to the acquisition of a fixed sequence, our results suggest that offline learning is more likely to take place in a procedural sequence learning task.},
	urldate = {2022-08-24},
	journal = {Frontiers in Human Neuroscience},
	author = {Du, Yue and Prashad, Shikha and Schoenbrun, Ilana and Clark, Jane E.},
	year = {2016},
	file = {Full Text PDF:/Users/swu/Zotero/storage/7W8TXVSH/Du et al. - 2016 - Probabilistic Motor Sequence Yields Greater Offlin.pdf:application/pdf},
}

@article{jimenez_chunking_2011,
	title = {Chunking by colors: assessing discrete learning in a continuous serial reaction-time task},
	volume = {137},
	issn = {1873-6297},
	shorttitle = {Chunking by colors},
	doi = {10.1016/j.actpsy.2011.03.013},
	abstract = {Chunk learning (the process by which a sequence is learned and retrieved from memory in smaller, decomposed units of information) has been postulated as the main learning mechanism underlying sequence learning (Perruchet \& Pacton, 2006). However, the evidence for chunk formation has been elusive in the continuous serial reaction-time task, whereas other continuous, statistical processes of learning account well for the results observed in this task. This article proposes a new index to capture segmentation in learning, based on the variance of responding to different parts of a sequence. We assess the validity of this measure by comparing performance in a control group with that of another group in which color codes were used to induce a uniform segmentation. Results showed that evidence of chunking was obtained when the color codes were consistently coupled to responses, but that chunking was not maintained after the colors were removed.},
	language = {eng},
	number = {3},
	journal = {Acta Psychologica},
	author = {Jiménez, Luis and Méndez, Amavia and Pasquali, Antoine and Abrahamse, Elger and Verwey, Willem},
	month = jul,
	year = {2011},
	pmid = {21514547},
	keywords = {Implicit learning, Sequence learning, Chunk learning, Statistical learning, Reaction Time, Adult, Attention, Color Perception, Humans, Neuropsychological Tests, Serial Learning, Transfer, Psychology},
	pages = {318--329},
	file = {ScienceDirect Snapshot:/Users/swu/Zotero/storage/NPJME5KW/S0001691811000606.html:text/html},
}

@article{salthouse_framework_1999,
	title = {A {Framework} for {Analyzing} and {Interpreting} {Differential} {Aging} {Patterns}: {Application} to {Three} {Measures} of {Implicit} {Learning}},
	volume = {6},
	issn = {1382-5585, 1744-4128},
	shorttitle = {A {Framework} for {Analyzing} and {Interpreting} {Differential} {Aging} {Patterns}},
	url = {https://www.tandfonline.com/doi/full/10.1076/anec.6.1.1.789},
	doi = {10.1076/anec.6.1.1.789},
	abstract = {At least four distinct explanations can be proposed to account for patterns of spared and impaired performance in which the age-related effects on some variables are weaker than those on other variables. These are: (a) some variables lack sufﬁcient reliability to exhibit relations with other variables; (b) the variables differ in their dependence on what many of the variables have in common; (c) variables with little or no age differences reﬂect a qualitatively different form of cognition than variables with moderate to large age differences; and (d) variables with little or no age differences have independent positive age-related inﬂuences that offset the negative age-related effects shared with other cognitive variables. These interpretations were examined with three different variables hypothesized to reﬂect implicit learning obtained from a sample of 183 adults ranging from 18 to 87 years of age. Only an implicit learning measure derived from a sequential reaction time task had acceptable reliability at the level of individual participants, and it was negatively related to age and positively related to variables reﬂecting ﬂuid cognition. These results therefore suggest that typical measures of implicit learning, when they can be reliably assessed, do not reﬂect a qualitatively distinct type of cognitive processing nor do they seem to exhibit additional compensatory age-related inﬂuences.},
	language = {en},
	number = {1},
	urldate = {2022-08-28},
	journal = {Aging, Neuropsychology, and Cognition},
	author = {Salthouse, Timothy A. and McGuthry, Katheryn E. and Hambrick, David Z.},
	month = mar,
	year = {1999},
	pages = {1--18},
	file = {Salthouse et al. - 1999 - A Framework for Analyzing and Interpreting Differe.pdf:/Users/swu/Zotero/storage/V9IPCHFK/Salthouse et al. - 1999 - A Framework for Analyzing and Interpreting Differe.pdf:application/pdf},
}

@misc{noauthor_chunking_nodate,
	title = {Chunking by colors: {Assessing} discrete learning in a continuous serial reaction-time task {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Chunking by colors},
	url = {https://reader.elsevier.com/reader/sd/pii/S0001691811000606?token=5D7138E67422A8169D4DFA775EDF260139DF0F34B09CC874741FCFD68091B01A9DDBEE24E847C319AB8F7D6ED4B6ACC6&originRegion=eu-west-1&originCreation=20220830072418},
	language = {en},
	urldate = {2022-08-30},
	doi = {10.1016/j.actpsy.2011.03.013},
	file = {Full Text PDF:/Users/swu/Zotero/storage/NRXAAF4Q/Chunking by colors Assessing discrete learning in.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/HT8VJYA5/S0001691811000606.html:text/html},
}

@article{wu_galileo_nodate,
	title = {Galileo: {Perceiving} {Physical} {Object} {Properties} by {Integrating} a {Physics} {Engine} with {Deep} {Learning}},
	abstract = {Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to ﬁt key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efﬁcient inference.},
	language = {en},
	author = {Wu, Jiajun and Yildirim, Ilker and Lim, Joseph J and Freeman, William T and Tenenbaum, Joshua B},
	pages = {9},
	file = {Wu et al. - Galileo Perceiving Physical Object Properties by .pdf:/Users/swu/Zotero/storage/BLXJ8ELD/Wu et al. - Galileo Perceiving Physical Object Properties by .pdf:application/pdf},
}

@article{wu_learning_nodate,
	title = {Learning to {See} {Physics} via {Visual} {De}-animation},
	abstract = {We introduce a paradigm for understanding physical scenes without human annotations. At the core of our system is a physical world representation that is ﬁrst recovered by a perception module and then utilized by physics and graphics engines. During training, the perception module and the generative models learn by visual de-animation — interpreting and reconstructing the visual information stream. During testing, the system ﬁrst recovers the physical world state, and then uses the generative models for reasoning and future prediction.},
	language = {en},
	author = {Wu, Jiajun and Lu, Erika and Kohli, Pushmeet and Freeman, William T and Tenenbaum, Joshua B},
	pages = {12},
	file = {Wu et al. - Learning to See Physics via Visual De-animation.pdf:/Users/swu/Zotero/storage/YEYSDCIZ/Wu et al. - Learning to See Physics via Visual De-animation.pdf:application/pdf},
}

@article{wu_marrnet_nodate,
	title = {{MarrNet}: {3D} {Shape} {Reconstruction} via 2.{5D} {Sketches}},
	language = {en},
	author = {Wu, Jiajun and Wang, Yifan and Xue, Tianfan and Sun, Xingyuan and Freeman, William T and Tenenbaum, Joshua B},
	pages = {11},
	file = {Wu et al. - MarrNet 3D Shape Reconstruction via 2.5D Sketches.pdf:/Users/swu/Zotero/storage/TMAY8QGF/Wu et al. - MarrNet 3D Shape Reconstruction via 2.5D Sketches.pdf:application/pdf},
}

@article{wu_learning_nodate-1,
	title = {Learning a {Probabilistic} {Latent} {Space} of {Object} {Shapes} via {3D} {Generative}-{Adversarial} {Modeling}},
	abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The beneﬁts of our model are three-fold: ﬁrst, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
	language = {en},
	author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T and Tenenbaum, Joshua B},
	pages = {11},
	file = {Wu et al. - Learning a Probabilistic Latent Space of Object Sh.pdf:/Users/swu/Zotero/storage/EXZIYQTZ/Wu et al. - Learning a Probabilistic Latent Space of Object Sh.pdf:application/pdf},
}

@article{bogacz_humans_2010,
	title = {Do humans produce the speed–accuracy trade-off that maximizes reward rate?},
	volume = {63},
	issn = {1747-0218},
	url = {https://doi.org/10.1080/17470210903091643},
	doi = {10.1080/17470210903091643},
	abstract = {In this paper we investigate trade-offs between speed and accuracy that are produced by humans when confronted with a sequence of choices between two alternatives. We assume that the choice process is described by the drift diffusion model, in which the speed?accuracy trade-off is primarily controlled by the value of the decision threshold. We test the hypothesis that participants choose the decision threshold that maximizes reward rate, defined as an average number of rewards per unit of time. In particular, we test four predictions derived on the basis of this hypothesis in two behavioural experiments. The data from all participants of our experiments provide support only for some of the predictions, and on average the participants are slower and more accurate than predicted by reward rate maximization. However, when we limit our analysis to subgroups of 30?50\% of participants who earned the highest overall rewards, all the predictions are satisfied by the data. This suggests that a substantial subset of participants do select decision thresholds that maximize reward rate. We also discuss possible reasons why the remaining participants select thresholds higher than optimal, including the possibility that participants optimize a combination of reward rate and accuracy or that they compensate for the influence of timing uncertainty, or both.},
	number = {5},
	urldate = {2022-09-06},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Bogacz, Rafal and Hu, Peter T. and Holmes, Philip J. and Cohen, Jonathan D.},
	month = may,
	year = {2010},
	note = {Publisher: SAGE Publications},
	keywords = {sat},
	pages = {863--891},
	file = {SAGE PDF Full Text:/Users/swu/Zotero/storage/2UWLZXNZ/Bogacz et al. - 2010 - Do humans produce the speed–accuracy trade-off tha.pdf:application/pdf},
}

@article{mackay_problems_1982,
	title = {The {Problems} of {Flexibility}, {Fluency}, and {Speed}-{Accuracy} {Trade}-{Off} in {Skilled} {Behavior}},
	volume = {89},
	language = {en},
	number = {5},
	journal = {Psychological Review},
	author = {MacKay, Donald G},
	year = {1982},
	pages = {483--506},
	file = {MacKay - The Problems of Flexibility, Fluency, and Speed-Ac.pdf:/Users/swu/Zotero/storage/UG2QP5YM/MacKay - The Problems of Flexibility, Fluency, and Speed-Ac.pdf:application/pdf},
}

@article{howard_age_1997,
	title = {Age differences in implicit learning of higher order dependencies in serial patterns},
	volume = {12},
	issn = {0882-7974},
	doi = {10.1037//0882-7974.12.4.634},
	abstract = {3 experiments examined serial pattern learning in younger and older adults. Unlike the usual repeating pattern, the sequences alternated between events from a repeating pattern and those determined randomly. The results indicated that no one was able to describe the regularity, but with practice every individual in all 3 age groups (including old old) became faster, more accurate, or both, on pattern trials than on random trials. Although this indicates that adults of all ages are able to learn second-order statistical dependencies in a sequence, age-related deficits were obtained in the magnitude of pattern learning. There were also age differences in what was learned, with only younger people revealing sensitivity to higher order statistical dependencies in the sequence. In addition, whereas younger people revealed evidence of their pattern learning in a subsequent conceptually driven production test, young-old and old-old people did not.},
	language = {eng},
	number = {4},
	journal = {Psychology and Aging},
	author = {Howard, J. H. and Howard, D. V.},
	month = dec,
	year = {1997},
	pmid = {9416632},
	keywords = {Adult, Humans, Serial Learning, Age Factors, Aged, Aged, 80 and over, Analysis of Variance, Concept Formation, Cross-Sectional Studies, Female, Male, Mathematics, Probability Learning, Stochastic Processes, Unconscious, Psychology},
	pages = {634--656},
}

@article{cleeremans_learning_1991,
	title = {Learning the {Structure} of {Event} {Sequences}},
	volume = {120},
	language = {en},
	number = {3},
	journal = {Journal of Experimental Psychology: General},
	author = {Cleeremans, Axel and McClelland, James L},
	year = {1991},
	pages = {235--253},
	file = {Cleeremans and McClelland - Learning the Structure of Event Sequences.pdf:/Users/swu/Zotero/storage/7QBDKUFC/Cleeremans and McClelland - Learning the Structure of Event Sequences.pdf:application/pdf},
}

@article{cleeremans_finite_1989,
	title = {Finite state automata and simple recurrent networks},
	volume = {1},
	issn = {1530-888X},
	doi = {10.1162/neco.1989.1.3.372},
	abstract = {Explores a network architecture introduced by J. L. Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t – 1, together with element t, to predict element t + 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar. Conditions under which the network can carry information about distant sequential contingencies across intervening elements are also examined. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Neural Computation},
	author = {Cleeremans, Axel and Servan-Schreiber, David and McClelland, James L.},
	year = {1989},
	note = {Place: US
Publisher: MIT Press},
	keywords = {Neural Networks},
	pages = {372--381},
	file = {Snapshot:/Users/swu/Zotero/storage/UY458TR4/1991-03351-001.html:text/html},
}

@article{minier_temporal_2016,
	title = {The {Temporal} {Dynamics} of {Regularity} {Extraction} in {Non}-{Human} {Primates}},
	volume = {40},
	issn = {1551-6709},
	doi = {10.1111/cogs.12279},
	abstract = {Extracting the regularities of our environment is one of our core cognitive abilities. To study the fine-grained dynamics of the extraction of embedded regularities, a method combining the advantages of the artificial language paradigm (Saffran, Aslin, \& Newport, ) and the serial response time task (Nissen \& Bullemer, ) was used with a group of Guinea baboons (Papio papio) in a new automatic experimental device (Fagot \& Bonté, ). After a series of random trials, monkeys were exposed to language-like patterns. We found that the extraction of embedded patterns positioned at the end of larger patterns was faster than the extraction of initial embedded patterns. This result suggests that there is a learning advantage for the final element of a sequence that benefits from the contextual information provided by previous elements.},
	language = {eng},
	number = {4},
	journal = {Cognitive Science},
	author = {Minier, Laure and Fagot, Joël and Rey, Arnaud},
	month = may,
	year = {2016},
	pmid = {26303229},
	keywords = {Language, Learning, Reaction Time, Female, Male, Cognition, Animals, Artificial language, Behavior, Animal, Embedded patterns, Papio papio, Regularity extraction, Serial response time, Time Factors},
	pages = {1019--1030},
}

@article{vekony_speed_2020,
	title = {Speed or {Accuracy} {Instructions} {During} {Skill} {Learning} do not {Affect} the {Acquired} {Knowledge}},
	volume = {1},
	issn = {2632-7376},
	url = {https://doi.org/10.1093/texcom/tgaa041},
	doi = {10.1093/texcom/tgaa041},
	abstract = {A crucial question in skill learning research is how instruction affects the performance or the underlying representations. Little is known about the effects of instructions on one critical aspect of skill learning, namely, picking-up statistical regularities. More specifically, the present study tests how prelearning speed or accuracy instructions affect the acquisition of non-adjacent second-order dependencies. We trained 2 groups of participants on an implicit probabilistic sequence learning task: one group focused on being fast and the other on being accurate. As expected, we detected a strong instruction effect: accuracy instruction resulted in a nearly errorless performance, and speed instruction caused short reaction times (RTs). Despite the differences in the average RTs and accuracy scores, we found a similar level of statistical learning performance in the training phase. After the training phase, we tested the 2 groups under the same instruction (focusing on both speed and accuracy), and they showed comparable performance, suggesting a similar level of underlying statistical representations. Our findings support that skill learning can result in robust representations, and they highlight that this form of knowledge may appear with almost errorless performance. Moreover, multiple sessions with different instructions enabled the separation of competence from performance.},
	number = {1},
	urldate = {2022-11-07},
	journal = {Cerebral Cortex Communications},
	author = {Vékony, Teodóra and Marossy, Hanna and Must, Anita and Vécsei, László and Janacsek, Karolina and Nemeth, Dezso},
	month = jan,
	year = {2020},
	keywords = {sat},
	pages = {tgaa041},
	file = {Full Text PDF:/Users/swu/Zotero/storage/NTDVFEYI/Vékony et al. - 2020 - Speed or Accuracy Instructions During Skill Learni.pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/9VNQC2ME/5889933.html:text/html},
}

@misc{ellis_dreamcoder_2020,
	title = {{DreamCoder}: {Growing} generalizable, interpretable knowledge with wake-sleep {Bayesian} program learning},
	shorttitle = {{DreamCoder}},
	url = {http://arxiv.org/abs/2006.08381},
	abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
	month = jun,
	year = {2020},
	note = {arXiv:2006.08381 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/swu/Zotero/storage/3JG5DIS5/2006.html:text/html;Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:/Users/swu/Zotero/storage/IH689JSG/Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:application/pdf;Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:/Users/swu/Zotero/storage/UAKKIQMW/Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf:application/pdf},
}

@article{panagiotaropoulos_hierarchical_2020,
	title = {Hierarchical architecture of conscious processing and subjective experience},
	volume = {37},
	issn = {0264-3294, 1464-0627},
	url = {https://www.tandfonline.com/doi/full/10.1080/02643294.2020.1760811},
	doi = {10.1080/02643294.2020.1760811},
	language = {en},
	number = {3-4},
	urldate = {2022-11-23},
	journal = {Cognitive Neuropsychology},
	author = {Panagiotaropoulos, Theofanis I. and Wang, Liping and Dehaene, Stanislas},
	month = may,
	year = {2020},
	keywords = {Artificial language, Embedded patterns, Regularity extraction, Serial response time, Attention-schema theory, Conscious perception, Global Neuronal Workspace, internal models, prefrontal cortex, qualia},
	pages = {180--183},
	file = {Full Text PDF:/Users/swu/Zotero/storage/D5CUVMI4/Minier et al. - 2016 - The Temporal Dynamics of Regularity Extraction in .pdf:application/pdf;Müssgens and Ullén - 2015 - Transfer in Motor Sequence Learning Effects of Pr.pdf:/Users/swu/Zotero/storage/S6Q3TXI3/Müssgens and Ullén - 2015 - Transfer in Motor Sequence Learning Effects of Pr.pdf:application/pdf;O’Donnell et al. - Fragment Grammars Exploring Computation and Reuse.pdf:/Users/swu/Zotero/storage/X8XJWZ57/O’Donnell et al. - Fragment Grammars Exploring Computation and Reuse.pdf:application/pdf;Panagiotaropoulos et al. - 2020 - Hierarchical architecture of conscious processing .pdf:/Users/swu/Zotero/storage/CEAWSZDF/Panagiotaropoulos et al. - 2020 - Hierarchical architecture of conscious processing .pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/ABTBE6MC/cogs.html:text/html},
}

@article{planton_theory_2021,
	title = {A theory of memory for binary sequences: {Evidence} for a mental compression algorithm in humans},
	volume = {17},
	issn = {1553-7358},
	shorttitle = {A theory of memory for binary sequences},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008598},
	doi = {10.1371/journal.pcbi.1008598},
	abstract = {Working memory capacity can be improved by recoding the memorized information in a condensed form. Here, we tested the theory that human adults encode binary sequences of stimuli in memory using an abstract internal language and a recursive compression algorithm. The theory predicts that the psychological complexity of a given sequence should be proportional to the length of its shortest description in the proposed language, which can capture any nested pattern of repetitions and alternations using a limited number of instructions. Five experiments examine the capacity of the theory to predict human adults’ memory for a variety of auditory and visual sequences. We probed memory using a sequence violation paradigm in which participants attempted to detect occasional violations in an otherwise fixed sequence. Both subjective complexity ratings and objective violation detection performance were well predicted by our theoretical measure of complexity, which simply reflects a weighted sum of the number of elementary instructions and digits in the shortest formula that captures the sequence in our language. While a simpler transition probability model, when tested as a single predictor in the statistical analyses, accounted for significant variance in the data, the goodness-of-fit with the data significantly improved when the language-based complexity measure was included in the statistical model, while the variance explained by the transition probability model largely decreased. Model comparison also showed that shortest description length in a recursive language provides a better fit than six alternative previously proposed models of sequence encoding. The data support the hypothesis that, beyond the extraction of statistical knowledge, human sequence coding relies on an internal compression using language-like nested structures.},
	language = {en},
	number = {1},
	urldate = {2022-12-07},
	journal = {PLOS Computational Biology},
	author = {Planton, Samuel and Kerkoerle, Timo van and Abbih, Leïla and Maheu, Maxime and Meyniel, Florent and Sigman, Mariano and Wang, Liping and Figueira, Santiago and Romano, Sergio and Dehaene, Stanislas},
	month = jan,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Forecasting, Human learning, Kolmogorov complexity, Language, Learning, Vision, Working memory},
	pages = {e1008598},
	annote = {“ces. This isin line with the assumption that complexity istightly linked with the idea of compressibility in memory, and suggests that such acompression strategy, whether itissimple chunking or involves ahierarchical representation, ismore likely to be involved when the number of items to store in working memory exceeds the typical working memory span [16,88].” (Planton et al., 2021, p. 16)
“Yildirim and Jacobs [92], who showed cross-modal transfer of sequence knowledge: learning to categorize visual sequences facilitated the categorization of auditory sequences and vice versa.” (Planton et al., 2021, p. 18)
“. In fact, acrucial feature of our theory lies in going beyond asimple concatenation of chunks and forming recursively embedded or nested representations, that is the ability to represent “chunks of chunks” or “repetitions of repetitions”. Indeed, the construction of recursively nested structured has been proposed as acore human ability, which sets us apart from other primates [4,6,7,111]. Our results support the idea that the inclusion of such afeature isessential to explain human behavior when working memory capacity is exceeded and compression ismost beneficial.” (Planton et al., 2021, p. 27)
},
	file = {Full Text PDF:/Users/swu/Zotero/storage/VLK5F5ZQ/Planton et al. - 2021 - A theory of memory for binary sequences Evidence .pdf:application/pdf;Snapshot:/Users/swu/Zotero/storage/I33YXDZZ/article.html:text/html;Snapshot:/Users/swu/Zotero/storage/JZGVQVB9/article.html:text/html},
}

@article{alamia_non-parametric_2016,
	title = {Non-parametric {Algorithm} to {Isolate} {Chunks} in {Response} {Sequences}},
	volume = {10},
	issn = {1662-5153},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2016.00177},
	abstract = {Chunking consists in grouping items of a sequence into small clusters, named chunks, with the assumed goal of lessening working memory load. Despite extensive research, the current methods used to detect chunks, and to identify different chunking strategies, remain discordant and difficult to implement. Here, we propose a simple and reliable method to identify chunks in a sequence and to determine their stability across blocks. This algorithm is based on a ranking method and its major novelty is that it provides concomitantly both the features of individual chunk in a given sequence, and an overall index that quantifies the chunking pattern consistency across sequences. The analysis of simulated data confirmed the validity of our method in different conditions of noise, chunk lengths and chunk numbers; moreover, we found that this algorithm was particularly efficient in the noise range observed in real data, provided that at least 4 sequence repetitions were included in each experimental block. Furthermore, we applied this algorithm to actual reaction time series gathered from 3 published experiments and were able to confirm the findings obtained in the original reports. In conclusion, this novel algorithm is easy to implement, is robust to outliers and provides concurrent and reliable estimation of chunk position and chunking dynamics, making it useful to study both sequence-specific and general chunking effects. The algorithm is available at: https://github.com/artipago/Non-parametric-algorithm-to-isolate-chunks-in-response-sequences.},
	urldate = {2022-12-19},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Alamia, Andrea and Solopchuk, Oleg and Olivier, Etienne and Zenon, Alexandre},
	year = {2016},
	file = {Full Text PDF:/Users/swu/Zotero/storage/FHGDWHV4/Alamia et al. - 2016 - Non-parametric Algorithm to Isolate Chunks in Resp.pdf:application/pdf},
}

@article{solopchuk_chunking_2016,
	title = {Chunking improves symbolic sequence processing and relies on working memory gating mechanisms},
	volume = {23},
	issn = {1072-0502},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4755266/},
	doi = {10.1101/lm.041277.115},
	abstract = {Chunking, namely the grouping of sequence elements in clusters, is ubiquitous during sequence processing, but its impact on performance remains debated. Here, we found that participants who adopted a consistent chunking strategy during symbolic sequence learning showed a greater improvement of their performance and a larger decrease in cognitive workload over time. Stronger reliance on chunking was also associated with higher scores in a WM updating task, suggesting the contribution of WM gating mechanisms to sequence chunking. Altogether, these results indicate that chunking is a cost-saving strategy that enhances effectiveness of symbolic sequence learning.},
	number = {3},
	urldate = {2022-12-19},
	journal = {Learning \& Memory},
	author = {Solopchuk, Oleg and Alamia, Andrea and Olivier, Etienne and Zénon, Alexandre},
	month = mar,
	year = {2016},
	pmid = {26884228},
	pmcid = {PMC4755266},
	pages = {108--112},
	file = {Solopchuk et al. - 2016 - Chunking improves symbolic sequence processing and.pdf:/Users/swu/Zotero/storage/BT6Y87QZ/Solopchuk et al. - 2016 - Chunking improves symbolic sequence processing and.pdf:application/pdf;Solopchuk et al. - 2016 - Chunking improves symbolic sequence processing and.pdf:/Users/swu/Zotero/storage/JPCRYJPT/Solopchuk et al. - 2016 - Chunking improves symbolic sequence processing and.pdf:application/pdf},
}

@article{lengyel_statistically_2021,
	title = {Statistically defined visual chunks engage object-based attention},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-20589-z},
	doi = {10.1038/s41467-020-20589-z},
	abstract = {Abstract
            Although objects are the fundamental units of our representation interpreting the environment around us, it is still not clear how we handle and organize the incoming sensory information to form object representations. By utilizing previously well-documented advantages of within-object over across-object information processing, here we test whether learning involuntarily consistent visual statistical properties of stimuli that are free of any traditional segmentation cues might be sufficient to create object-like behavioral effects. Using a visual statistical learning paradigm and measuring efficiency of 3-AFC search and object-based attention, we find that statistically defined and implicitly learned visual chunks bias observers’ behavior in subsequent search tasks the same way as objects defined by visual boundaries do. These results suggest that learning consistent statistical contingencies based on the sensory input contributes to the emergence of object representations.},
	language = {en},
	number = {1},
	urldate = {2023-01-03},
	journal = {Nature Communications},
	author = {Lengyel, Gábor and Nagy, Márton and Fiser, József},
	month = jan,
	year = {2021},
	pages = {272},
	file = {Lengyel et al. - 2021 - Statistically defined visual chunks engage object-.pdf:/Users/swu/Zotero/storage/TBEUU9JE/Lengyel et al. - 2021 - Statistically defined visual chunks engage object-.pdf:application/pdf},
}

@article{lipkind_songbirds_2017,
	title = {Songbirds work around computational complexity by learning song vocabulary independently of sequence},
	volume = {8},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-01436-0},
	doi = {10.1038/s41467-017-01436-0},
	abstract = {Abstract
            While acquiring motor skills, animals transform their plastic motor sequences to match desired targets. However, because both the structure and temporal position of individual gestures are adjustable, the number of possible motor transformations increases exponentially with sequence length. Identifying the optimal transformation towards a given target is therefore a computationally intractable problem. Here we show an evolutionary workaround for reducing the computational complexity of song learning in zebra finches. We prompt juveniles to modify syllable phonology and sequence in a learned song to match a newly introduced target song. Surprisingly, juveniles match each syllable to the most spectrally similar sound in the target, regardless of its temporal position, resulting in unnecessary sequence errors, that they later try to correct. Thus, zebra finches prioritize efficient learning of syllable vocabulary, at the cost of inefficient syntax learning. This strategy provides a non-optimal but computationally manageable solution to the task of vocal sequence learning.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Nature Communications},
	author = {Lipkind, Dina and Zai, Anja T. and Hanuschkin, Alexander and Marcus, Gary F. and Tchernichovski, Ofer and Hahnloser, Richard H. R.},
	month = nov,
	year = {2017},
	pages = {1247},
	file = {Lipkind et al. - 2017 - Songbirds work around computational complexity by .pdf:/Users/swu/Zotero/storage/UJM923EC/Lipkind et al. - 2017 - Songbirds work around computational complexity by .pdf:application/pdf},
}

@article{mark_transferring_2020,
	title = {Transferring structural knowledge across cognitive maps in humans and models},
	volume = {11},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-18254-6},
	doi = {10.1038/s41467-020-18254-6},
	abstract = {Abstract
            Relations between task elements often follow hidden underlying structural forms such as periodicities or hierarchies, whose inferences fosters performance. However, transferring structural knowledge to novel environments requires flexible representations that are generalizable over particularities of the current environment, such as its stimuli and size. We suggest that humans represent structural forms as abstract basis sets and that in novel tasks, the structural form is inferred and the relevant basis set is transferred. Using a computational model, we show that such representation allows inference of the underlying structural form, important task states, effective behavioural policies and the existence of unobserved state-trajectories. In two experiments, participants learned three abstract graphs during two successive days. We tested how structural knowledge acquired on Day-1 affected Day-2 performance. In line with our model, participants who had a correct structural prior were able to infer the existence of unobserved state-trajectories and appropriate behavioural policies.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Nature Communications},
	author = {Mark, Shirley and Moran, Rani and Parr, Thomas and Kennerley, Steve W. and Behrens, Timothy E. J.},
	month = sep,
	year = {2020},
	pages = {4783},
	file = {Mark et al. - 2020 - Transferring structural knowledge across cognitive.pdf:/Users/swu/Zotero/storage/WQS5MXIN/Mark et al. - 2020 - Transferring structural knowledge across cognitive.pdf:application/pdf;Mark et al. - 2020 - Transferring structural knowledge across cognitive.pdf:/Users/swu/Zotero/storage/XSE5TXCV/Mark et al. - 2020 - Transferring structural knowledge across cognitive.pdf:application/pdf},
}

@article{mahadevan_proto-value_nodate,
	title = {Proto-value {Functions}: {A} {Laplacian} {Framework} for {Learning} {Representation} and {Control} in {Markov} {Decision} {Processes}},
	abstract = {This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystro¨m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end.},
	language = {en},
	author = {Mahadevan, Sridhar and Maggioni, Mauro},
	file = {Mahadevan and Maggioni - Proto-value Functions A Laplacian Framework for L.pdf:/Users/swu/Zotero/storage/UCXBZLU5/Mahadevan and Maggioni - Proto-value Functions A Laplacian Framework for L.pdf:application/pdf},
}

@article{wang_efficient_2015,
	title = {Efficient {Test} and {Visualization} of {Multi}-{Set} {Intersections}},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep16923},
	doi = {10.1038/srep16923},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Scientific Reports},
	author = {Wang, Minghui and Zhao, Yongzhong and Zhang, Bin},
	month = dec,
	year = {2015},
	pages = {16923},
	file = {Wang et al. - 2015 - Efficient Test and Visualization of Multi-Set Inte.pdf:/Users/swu/Zotero/storage/93NWX9JH/Wang et al. - 2015 - Efficient Test and Visualization of Multi-Set Inte.pdf:application/pdf;Wang et al. - 2015 - Efficient Test and Visualization of Multi-Set Inte.pdf:/Users/swu/Zotero/storage/M5XRYY8U/Wang et al. - 2015 - Efficient Test and Visualization of Multi-Set Inte.pdf:application/pdf},
}

@article{franklin_structured_nodate,
	title = {Structured {Event} {Memory}: {A} {Neuro}-{Symbolic} {Model} of {Event} {Cognition}},
	abstract = {Humans spontaneously organize a continuous experience into discrete events and use the learned structure of these events to generalize and organize memory. We introduce the Structured Event Memory (SEM) model of event cognition, which accounts for human abilities in event segmentation, memory, and generalization. SEM is derived from a probabilistic generative model of event dynamics defined over structured symbolic scenes. By embedding symbolic scene representations in a vector space and parametrizing the scene dynamics in this continuous space, SEM combines the advantages of structured and neural network approaches to high-level cognition. Using probabilistic reasoning over this generative model, SEM can infer event boundaries, learn event schemata, and use event knowledge to reconstruct past experience. We show that SEM can scale up to highdimensional input spaces, producing human-like event segmentation for naturalistic video data, and accounts for a wide array of memory phenomena.},
	language = {en},
	author = {Franklin, Nicholas T and Norman, Kenneth A and Ranganath, Charan and Zacks, Jeffrey M and Gershman, Samuel J},
	file = {Franklin et al. - Structured Event Memory A Neuro-Symbolic Model of.pdf:/Users/swu/Zotero/storage/I6H87FV5/Franklin et al. - Structured Event Memory A Neuro-Symbolic Model of.pdf:application/pdf;Franklin et al. - Structured Event Memory A Neuro-Symbolic Model of.pdf:/Users/swu/Zotero/storage/EU832GJN/Franklin et al. - Structured Event Memory A Neuro-Symbolic Model of.pdf:application/pdf},
}

@article{agresti_introduction_nodate,
	title = {Introduction to {Categorical} {Data} {Analysis}},
	language = {en},
	author = {Agresti, Alan},
	file = {Agresti - Introduction to Categorical Data Analysis.pdf:/Users/swu/Zotero/storage/VY4FX283/Agresti - Introduction to Categorical Data Analysis.pdf:application/pdf;Agresti - Introduction to Categorical Data Analysis.pdf:/Users/swu/Zotero/storage/BL2FURCR/Agresti - Introduction to Categorical Data Analysis.pdf:application/pdf},
}

@article{lehnert_reward-predictive_2020,
	title = {Reward-predictive representations generalize across tasks in reinforcement learning},
	volume = {16},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1008317},
	doi = {10.1371/journal.pcbi.1008317},
	abstract = {In computer science, reinforcement learning is a powerful framework with which artificial agents can learn to maximize their performance for any given Markov decision process (MDP). Advances over the last decade, in combination with deep neural networks, have enjoyed performance advantages over humans in many difficult task settings. However, such frameworks perform far less favorably when evaluated in their ability to generalize or transfer representations across different tasks. Existing algorithms that facilitate transfer typically are limited to cases in which the transition function or the optimal policy is portable to new contexts, but achieving “deep transfer” characteristic of human behavior has been elusive. Such transfer typically requires discovery of abstractions that permit analogical reuse of previously learned representations to superficially distinct tasks. Here, we demonstrate that abstractions that minimize error in predictions of reward outcomes generalize across tasks with different transition and reward functions. Such reward-predictive representations compress the state space of a task into a lower dimensional representation by combining states that are equivalent in terms of both the transition and reward functions.},
	language = {en},
	number = {10},
	urldate = {2023-01-04},
	journal = {PLOS Computational Biology},
	author = {Lehnert, Lucas and Littman, Michael L. and Frank, Michael J.},
	editor = {Gershman, Samuel J.},
	month = oct,
	year = {2020},
	pages = {e1008317},
	file = {Lehnert et al. - 2020 - Reward-predictive representations generalize acros.pdf:/Users/swu/Zotero/storage/JEM494BZ/Lehnert et al. - 2020 - Reward-predictive representations generalize acros.pdf:application/pdf;Lehnert et al. - 2020 - Reward-predictive representations generalize acros.pdf:/Users/swu/Zotero/storage/GD2M5NM5/Lehnert et al. - 2020 - Reward-predictive representations generalize acros.pdf:application/pdf},
}

@incollection{spieler_introduction_2019,
	edition = {1},
	title = {An {Introduction} to {Mixed} {Models} for {Experimental} {Psychology}},
	isbn = {978-0-429-31840-5},
	url = {https://www.taylorfrancis.com/books/9781000617467/chapters/10.4324/9780429318405-2},
	language = {en},
	urldate = {2023-01-04},
	booktitle = {New {Methods} in {Cognitive} {Psychology}},
	publisher = {Routledge},
	author = {Singmann, Henrik and Kellen, David},
	editor = {Spieler, Daniel and Schumacher, Eric},
	month = oct,
	year = {2019},
	doi = {10.4324/9780429318405-2},
	pages = {4--31},
	file = {Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf:/Users/swu/Zotero/storage/VN39MWJF/Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf:application/pdf;Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf:/Users/swu/Zotero/storage/24TGIP32/Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf:application/pdf},
}

@article{whittington_tolman-eichenbaum_2020,
	title = {The {Tolman}-{Eichenbaum} {Machine}: {Unifying} {Space} and {Relational} {Memory} through {Generalization} in the {Hippocampal} {Formation}},
	volume = {183},
	issn = {00928674},
	shorttitle = {The {Tolman}-{Eichenbaum} {Machine}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009286742031388X},
	doi = {10.1016/j.cell.2020.10.024},
	abstract = {The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine (TEM). After learning, TEM entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. TEM hippocampal cells include place and landmark cells that remap between environments. Crucially, TEM also aligns with empirically recorded representations in complex nonspatial tasks. TEM also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We conﬁrm this structural transfer over remapping in simultaneously recorded place and grid cells.},
	language = {en},
	number = {5},
	urldate = {2023-01-04},
	journal = {Cell},
	author = {Whittington, James C.R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E.J.},
	month = nov,
	year = {2020},
	pages = {1249--1263.e23},
	file = {Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf:/Users/swu/Zotero/storage/V2AEKXNC/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf:application/pdf;Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf:/Users/swu/Zotero/storage/WSWXLANT/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and .pdf:application/pdf},
}

@article{egan_chunking_1979,
	title = {Chunking in recall of symbolic drawings},
	volume = {7},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03197595},
	doi = {10.3758/BF03197595},
	language = {en},
	number = {2},
	urldate = {2023-01-04},
	journal = {Memory \& Cognition},
	author = {Egan, Dennis E. and Schwartz, Barry J.},
	month = mar,
	year = {1979},
	pages = {149--158},
	file = {Egan and Schwartz - 1979 - Chunking in recall of symbolic drawings.pdf:/Users/swu/Zotero/storage/MKG7T9TK/Egan and Schwartz - 1979 - Chunking in recall of symbolic drawings.pdf:application/pdf},
}

@article{boyer_processing_2005,
	title = {Processing abstract sequence structure: learning without knowing, or knowing without learning?},
	volume = {69},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Processing abstract sequence structure},
	url = {http://link.springer.com/10.1007/s00426-004-0207-4},
	doi = {10.1007/s00426-004-0207-4},
	language = {en},
	number = {5-6},
	urldate = {2023-01-04},
	journal = {Psychological Research Psychologische Forschung},
	author = {Boyer, Maud and Destrebecqz, Arnaud and Cleeremans, Axel},
	month = jun,
	year = {2005},
	pages = {383--398},
	file = {Boyer et al. - 2005 - Processing abstract sequence structure learning w.pdf:/Users/swu/Zotero/storage/4LE3WU4J/Boyer et al. - 2005 - Processing abstract sequence structure learning w.pdf:application/pdf},
}

@article{bower_perceptual_1972,
	title = {Perceptual groups as coding units in immediate memory},
	volume = {27},
	issn = {0033-3131},
	url = {http://link.springer.com/10.3758/BF03328942},
	doi = {10.3758/BF03328942},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Psychonomic Science},
	author = {Bower, Gordon H.},
	month = oct,
	year = {1972},
	pages = {217--219},
	file = {Bower - 1972 - Perceptual groups as coding units in immediate mem.pdf:/Users/swu/Zotero/storage/V77FMUML/Bower - 1972 - Perceptual groups as coding units in immediate mem.pdf:application/pdf},
}

@article{schacter_insights_1999,
	title = {Insights {From} {Psychology} and {Cognitive} {Neuroscience}},
	language = {en},
	journal = {American Psychologist},
	author = {Schacter, Daniel L},
	year = {1999},
	file = {Schacter - 1999 - Insights From Psychology and Cognitive Neuroscienc.pdf:/Users/swu/Zotero/storage/MJRSHMIC/Schacter - 1999 - Insights From Psychology and Cognitive Neuroscienc.pdf:application/pdf;Schacter - 1999 - Insights From Psychology and Cognitive Neuroscienc.pdf:/Users/swu/Zotero/storage/L2AFL726/Schacter - 1999 - Insights From Psychology and Cognitive Neuroscienc.pdf:application/pdf},
}

@article{donkin_getting_2009,
	title = {Getting more from accuracy and response time data: {Methods} for fitting the linear ballistic accumulator},
	volume = {41},
	issn = {1554-351X, 1554-3528},
	shorttitle = {Getting more from accuracy and response time data},
	url = {http://link.springer.com/10.3758/BRM.41.4.1095},
	doi = {10.3758/BRM.41.4.1095},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Behavior Research Methods},
	author = {Donkin, Chris and Averell, Lee and Brown, Scott and Heathcote, Andrew},
	month = nov,
	year = {2009},
	pages = {1095--1110},
	file = {Donkin et al. - 2009 - Getting more from accuracy and response time data.pdf:/Users/swu/Zotero/storage/2TKQ9GVC/Donkin et al. - 2009 - Getting more from accuracy and response time data.pdf:application/pdf;Donkin et al. - 2009 - Getting more from accuracy and response time data.pdf:/Users/swu/Zotero/storage/IUTTGHJ5/Donkin et al. - 2009 - Getting more from accuracy and response time data.pdf:application/pdf},
}

@article{brown_integrated_2008,
	title = {An integrated model of choices and response times in absolute identification.},
	volume = {115},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.115.2.396},
	doi = {10.1037/0033-295X.115.2.396},
	abstract = {Recent theoretical developments in the field of absolute identification have stressed differences between relative and absolute processes, that is, whether stimulus magnitudes are judged relative to a shorter term context provided by recently presented stimuli or a longer term context provided by the entire set of stimuli. The authors developed a model (SAMBA: selective attention, mapping, and ballistic accumulation) that integrates shorter and longer term memory processes and accounts for both the choices made and the associated response time distributions, including sequential effects in each. The model’s predictions arise as a consequence of its architecture and require estimation of only a few parameters with values that are consistent across numerous data sets. The authors show that SAMBA provides a quantitative account of benchmark choice phenomena in classical absolute identification experiments and in contemporary data involving both choice and response time.},
	language = {en},
	number = {2},
	urldate = {2023-01-04},
	journal = {Psychological Review},
	author = {Brown, Scott D. and Marley, A. A. J. and Donkin, Christopher and Heathcote, Andrew},
	month = apr,
	year = {2008},
	pages = {396--425},
	file = {Brown et al. - 2008 - An integrated model of choices and response times .pdf:/Users/swu/Zotero/storage/FRIPJ8IP/Brown et al. - 2008 - An integrated model of choices and response times .pdf:application/pdf;Brown et al. - 2008 - An integrated model of choices and response times .pdf:/Users/swu/Zotero/storage/KMGQC7FK/Brown et al. - 2008 - An integrated model of choices and response times .pdf:application/pdf},
}

@article{terrace_chunking_1987,
	title = {Chunking by a pigeon in a serial learning task},
	volume = {325},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/325149a0},
	doi = {10.1038/325149a0},
	language = {en},
	number = {6100},
	urldate = {2023-01-04},
	journal = {Nature},
	author = {Terrace, H. S.},
	month = jan,
	year = {1987},
	pages = {149--151},
	file = {Terrace - 1987 - Chunking by a pigeon in a serial learning task.pdf:/Users/swu/Zotero/storage/RBXQ3BVU/Terrace - 1987 - Chunking by a pigeon in a serial learning task.pdf:application/pdf;Terrace - 1987 - Chunking by a pigeon in a serial learning task.pdf:/Users/swu/Zotero/storage/IKFUCDF7/Terrace - 1987 - Chunking by a pigeon in a serial learning task.pdf:application/pdf},
}

@article{ratcliff_diffusion_2016,
	title = {Diffusion {Decision} {Model}: {Current} {Issues} and {History}},
	volume = {20},
	issn = {13646613},
	shorttitle = {Diffusion {Decision} {Model}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316000255},
	doi = {10.1016/j.tics.2016.01.007},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Trends in Cognitive Sciences},
	author = {Ratcliff, Roger and Smith, Philip L. and Brown, Scott D. and McKoon, Gail},
	month = apr,
	year = {2016},
	pages = {260--281},
	file = {Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and Histo.pdf:/Users/swu/Zotero/storage/8X76SS96/Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and Histo.pdf:application/pdf},
}

@article{hoffmann_stimulus-response_1997,
	title = {Stimulus-response compatibility and sequential learning in the serial reaction time task},
	volume = {60},
	issn = {0340-0727, 1430-2772},
	url = {http://link.springer.com/10.1007/BF00419682},
	doi = {10.1007/BF00419682},
	abstract = {The serial reaction time (SRT) task is a commonly used paradigm to investigate implicit learning. In most studies the settings originally introduced by Nissen and Bullemer are replicated, i.e., subjects respond to a visuo-spatial sequence of stimulus locations by pressing spatially compatible arranged keys. The present experiment was designed to explore to what degree the sequential learning observed under these conditions depends on the use of locational sequences. Under otherwise identical conditions, first the S-R compatibility was reduced by using symbols instead of locations as stimuli, and second, the "connectibility," i.e, the ease of connecting successive stimuli into coherent pattern, was varied. Effects on reaction times (RT) in the SRT task and on explicit memory in a generation task were evaluated. The results indicate that the connectibility of the stimuli has no effect at all and that S-R compatibility influences only the general RT level but does not seem to modify the learning process itself. Thus, the data are more consistent with the notion that learning is based primarily on the sequence of responses rather than on the sequence of stimuli. Moreover, a post hoc classification of subjects with regard to the amount of explicit sequence knowledge they have acquired reveals a striking modification of the general result: The RT difference between responses to locations and symbols vanishes in the course of learning for the complete explicit knowledge group. In order to account for this effect, we presume that the response control of these subjects shifts from stimuli to motor programs, so that RTs become increasingly independent of the stimuli used.},
	language = {en},
	number = {1-2},
	urldate = {2023-01-04},
	journal = {Psychological Research},
	author = {Hoffmann, Joachim and Koch, Iring},
	month = jun,
	year = {1997},
	pages = {87--97},
	file = {Hoffmann and Koch - 1997 - Stimulus-response compatibility and sequential lea.pdf:/Users/swu/Zotero/storage/QE43NF5T/Hoffmann and Koch - 1997 - Stimulus-response compatibility and sequential lea.pdf:application/pdf},
}

@article{herbranson_artificial_2008,
	title = {Artificial grammar learning in pigeons},
	volume = {36},
	issn = {1543-4494, 1543-4508},
	url = {http://link.springer.com/10.3758/LB.36.2.116},
	doi = {10.3758/LB.36.2.116},
	language = {en},
	number = {2},
	urldate = {2023-01-04},
	journal = {Learning \& Behavior},
	author = {Herbranson, W. T. and Shimp, C. P.},
	month = may,
	year = {2008},
	pages = {116--137},
	file = {Herbranson and Shimp - 2008 - Artificial grammar learning in pigeons.pdf:/Users/swu/Zotero/storage/RGKH3KIV/Herbranson and Shimp - 2008 - Artificial grammar learning in pigeons.pdf:application/pdf},
}

@techreport{schramm_are_2019,
	type = {preprint},
	title = {Are {Reaction} {Time} {Transformations} {Really} {Beneficial}?},
	url = {https://osf.io/9ksa6},
	abstract = {We investigate whether or not the common practice of transforming response times prior to conventional analyses of central tendency yields any notable beneﬁts. We generate data from a realistic single-bound drift diﬀusion model with parameters informed by several diﬀerent typical experiments in cognition. We then examine the eﬀects of log and reciprocal transformation on expected eﬀect size, statistical power, and Type I error rates for conventional two-sample t-tests. One of the key elements of our setup is that RTs have a lower bound, called the shift, which is well above 0. We closely examine the eﬀect that diﬀerent shifts have for the analyses. We conclude that logarithm and reciprocal transformation oﬀer no gain in power or Type I error control. In some typical cases, reciprocal transformations are detrimental as they lead to a lowering of power.},
	language = {en},
	urldate = {2023-01-04},
	institution = {PsyArXiv},
	author = {Schramm, Pele and Rouder, Jeffrey},
	month = mar,
	year = {2019},
	doi = {10.31234/osf.io/9ksa6},
	file = {Schramm and Rouder - 2019 - Are Reaction Time Transformations Really Beneficia.pdf:/Users/swu/Zotero/storage/5YVL6C4Z/Schramm and Rouder - 2019 - Are Reaction Time Transformations Really Beneficia.pdf:application/pdf},
}

@article{leite_modeling_2010,
	title = {Modeling reaction time and accuracy of multiple-alternative decisions},
	volume = {72},
	issn = {1943-3921, 1943-393X},
	url = {http://link.springer.com/10.3758/APP.72.1.246},
	doi = {10.3758/APP.72.1.246},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Leite, Fábio P. and Ratcliff, Roger},
	month = jan,
	year = {2010},
	pages = {246--273},
	file = {Leite and Ratcliff - 2010 - Modeling reaction time and accuracy of multiple-al.pdf:/Users/swu/Zotero/storage/7CQLSSWG/Leite and Ratcliff - 2010 - Modeling reaction time and accuracy of multiple-al.pdf:application/pdf},
}

@article{orbanz_lecture_nodate,
	title = {Lecture {Notes} on {Bayesian} {Nonparametrics}},
	language = {en},
	author = {Orbanz, Peter},
	file = {Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:/Users/swu/Zotero/storage/ZLLARNDT/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:application/pdf;Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:/Users/swu/Zotero/storage/E56A649T/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:application/pdf},
}

@article{houlsby_cognitive_2013,
	title = {Cognitive {Tomography} {Reveals} {Complex}, {Task}-{Independent} {Mental} {Representations}},
	volume = {23},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982213011287},
	doi = {10.1016/j.cub.2013.09.012},
	abstract = {Humans develop rich mental representations that guide their behavior in a variety of everyday tasks. However, it is unknown whether these representations, often formalized as priors in Bayesian inference, are speciﬁc for each task or subserve multiple tasks. Current approaches cannot distinguish between these two possibilities because they cannot extract comparable representations across different tasks [1–10]. Here, we develop a novel method, termed cognitive tomography, that can extract complex, multidimensional priors across tasks. We apply this method to human judgments in two qualitatively different tasks, ‘‘familiarity’’ and ‘‘odd one out,’’ involving an ecologically relevant set of stimuli, human faces. We show that priors over faces are structurally complex and vary dramatically across subjects, but are invariant across the tasks within each subject. The priors we extract from each task allow us to predict with high precision the behavior of subjects for novel stimuli both in the same task as well as in the other task. Our results provide the ﬁrst evidence for a single high-dimensional structured representation of a naturalistic stimulus set that guides behavior in multiple tasks. Moreover, the representations estimated by cognitive tomography can provide independent, behavior-based regressors for elucidating the neural correlates of complex naturalistic priors.},
	language = {en},
	number = {21},
	urldate = {2023-01-04},
	journal = {Current Biology},
	author = {Houlsby, Neil M.T. and Huszár, Ferenc and Ghassemi, Mohammad M. and Orbán, Gergő and Wolpert, Daniel M. and Lengyel, Máté},
	month = nov,
	year = {2013},
	pages = {2169--2175},
	file = {Houlsby et al. - 2013 - Cognitive Tomography Reveals Complex, Task-Indepen.pdf:/Users/swu/Zotero/storage/8AQN47SG/Houlsby et al. - 2013 - Cognitive Tomography Reveals Complex, Task-Indepen.pdf:application/pdf;Houlsby et al. - 2013 - Cognitive Tomography Reveals Complex, Task-Indepen.pdf:/Users/swu/Zotero/storage/XX5LP9YD/Houlsby et al. - 2013 - Cognitive Tomography Reveals Complex, Task-Indepen.pdf:application/pdf},
}

@article{gershman_computational_2015,
	title = {Computational rationality: {A} converging paradigm for intelligence in brains, minds, and machines},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Computational rationality},
	url = {https://www.science.org/doi/10.1126/science.aac6076},
	doi = {10.1126/science.aac6076},
	abstract = {After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.},
	language = {en},
	number = {6245},
	urldate = {2023-01-04},
	journal = {Science},
	author = {Gershman, Samuel J. and Horvitz, Eric J. and Tenenbaum, Joshua B.},
	month = jul,
	year = {2015},
	pages = {273--278},
	file = {Gershman et al. - 2015 - Computational rationality A converging paradigm f.pdf:/Users/swu/Zotero/storage/RL5IVSCD/Gershman et al. - 2015 - Computational rationality A converging paradigm f.pdf:application/pdf;Gershman et al. - 2015 - Computational rationality A converging paradigm f.pdf:/Users/swu/Zotero/storage/EQTNM4PP/Gershman et al. - 2015 - Computational rationality A converging paradigm f.pdf:application/pdf},
}

@article{vul_one_2014,
	title = {One and {Done}? {Optimal} {Decisions} {From} {Very} {Few} {Samples}},
	volume = {38},
	issn = {0364-0213, 1551-6709},
	shorttitle = {One and {Done}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12101},
	doi = {10.1111/cogs.12101},
	abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of ﬁndings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufﬁcient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples—but as samples are costly—how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We ﬁnd that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Cognitive Science},
	author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
	month = may,
	year = {2014},
	pages = {599--637},
	file = {Vul et al. - 2014 - One and Done Optimal Decisions From Very Few Samp.pdf:/Users/swu/Zotero/storage/UN2FBJ68/Vul et al. - 2014 - One and Done Optimal Decisions From Very Few Samp.pdf:application/pdf;Vul et al. - 2014 - One and Done Optimal Decisions From Very Few Samp.pdf:/Users/swu/Zotero/storage/R964EUHY/Vul et al. - 2014 - One and Done Optimal Decisions From Very Few Samp.pdf:application/pdf},
}

@article{bogacz_physics_2006,
	title = {The physics of optimal decision making: {A} formal analysis of models of performance in two-alternative forced-choice tasks.},
	volume = {113},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The physics of optimal decision making},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.4.700},
	doi = {10.1037/0033-295X.113.4.700},
	abstract = {In this article, the authors consider optimal decision making in two-alternative forced-choice (TAFC) tasks. They begin by analyzing 6 models of TAFC decision making and show that all but one can be reduced to the drift diffusion model, implementing the statistically optimal algorithm (most accurate for a given speed or fastest for a given accuracy). They prove further that there is always an optimal trade-off between speed and accuracy that maximizes various reward functions, including reward rate (percentage of correct responses per unit time), as well as several other objective functions, including ones weighted for accuracy. They use these findings to address empirical data and make novel predictions about performance under optimality.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Psychological Review},
	author = {Bogacz, Rafal and Brown, Eric and Moehlis, Jeff and Holmes, Philip and Cohen, Jonathan D.},
	year = {2006},
	pages = {700--765},
	file = {Bogacz et al. - 2006 - The physics of optimal decision making A formal a.pdf:/Users/swu/Zotero/storage/RM9LGBIX/Bogacz et al. - 2006 - The physics of optimal decision making A formal a.pdf:application/pdf;Bogacz et al. - 2006 - The physics of optimal decision making A formal a.pdf:/Users/swu/Zotero/storage/DX4EI3QU/Bogacz et al. - 2006 - The physics of optimal decision making A formal a.pdf:application/pdf},
}

@article{wiltschko_mapping_2015,
	title = {Mapping {Sub}-{Second} {Structure} in {Mouse} {Behavior}},
	volume = {88},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315010375},
	doi = {10.1016/j.neuron.2015.11.031},
	abstract = {Complex animal behaviors are likely built from simpler modules, but their systematic identiﬁcation in mammals remains a signiﬁcant challenge. Here we use depth imaging to show that 3D mouse pose dynamics are structured at the sub-second timescale. Computational modeling of these fast dynamics effectively describes mouse behavior as a series of reused and stereotyped modules with deﬁned transition probabilities. We demonstrate this combined 3D imaging and machine learning method can be used to unmask potential strategies employed by the brain to adapt to the environment, to capture both predicted and previously hidden phenotypes caused by genetic or neural manipulations, and to systematically expose the global structure of behavior within an experiment. This work reveals that mouse body language is built from identiﬁable components and is organized in a predictable fashion; deciphering this language establishes an objective framework for characterizing the inﬂuence of environmental cues, genes and neural activity on behavior.},
	language = {en},
	number = {6},
	urldate = {2023-01-04},
	journal = {Neuron},
	author = {Wiltschko, Alexander B. and Johnson, Matthew J. and Iurilli, Giuliano and Peterson, Ralph E. and Katon, Jesse M. and Pashkovski, Stan L. and Abraira, Victoria E. and Adams, Ryan P. and Datta, Sandeep Robert},
	month = dec,
	year = {2015},
	pages = {1121--1135},
	file = {Wiltschko et al. - 2015 - Mapping Sub-Second Structure in Mouse Behavior.pdf:/Users/swu/Zotero/storage/RUK36DKT/Wiltschko et al. - 2015 - Mapping Sub-Second Structure in Mouse Behavior.pdf:application/pdf},
}

@article{bramley_formalizing_2017,
	title = {Formalizing {Neurath}'s {Ship}: {Approximate} {Algorithms} for {Online} {Causal} {Learning}},
	volume = {124},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Formalizing {Neurath}'s {Ship}},
	url = {http://arxiv.org/abs/1609.04212},
	doi = {10.1037/rev0000061},
	abstract = {Higher-level cognition depends on the ability to learn models of the world. We can characterize this at the computational level as a structure-learning problem with the goal of best identifying the prevailing causal relationships among a set of relata. However, the computational cost of performing exact Bayesian inference over causal models grows rapidly as the number of relata increases. This implies that the cognitive processes underlying causal learning must be substantially approximate. A powerful class of approximations that focuses on the sequential absorption of successive inputs is captured by the Neurath’s ship metaphor in philosophy of science, where theory change is cast as a stochastic and gradual process shaped as much by people’s limited willingness to abandon their current theory when considering alternatives as by the ground truth they hope to approach. Inspired by this metaphor and by algorithms for approximating Bayesian inference in machine learning, we propose an algorithmic-level model of causal structure learning under which learners represent only a single global hypothesis that they update locally as they gather evidence. We propose a related scheme for understanding how, under these limitations, learners choose informative interventions that manipulate the causal system to help elucidate its workings. We ﬁnd support for our approach in the analysis of three experiments.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Psychological Review},
	author = {Bramley, Neil R. and Dayan, Peter and Griffiths, Thomas L. and Lagnado, David A.},
	month = apr,
	year = {2017},
	note = {arXiv:1609.04212 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {301--338},
	file = {Bramley et al. - 2017 - Formalizing Neurath's Ship Approximate Algorithms.pdf:/Users/swu/Zotero/storage/UH3K6X4N/Bramley et al. - 2017 - Formalizing Neurath's Ship Approximate Algorithms.pdf:application/pdf;Bramley et al. - 2017 - Formalizing Neurath's Ship Approximate Algorithms.pdf:/Users/swu/Zotero/storage/RSY85FKU/Bramley et al. - 2017 - Formalizing Neurath's Ship Approximate Algorithms.pdf:application/pdf},
}

@article{bronstein_geometric_2017,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	volume = {34},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Geometric deep learning},
	url = {http://arxiv.org/abs/1611.08097},
	doi = {10.1109/MSP.2017.2693418},
	abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	month = jul,
	year = {2017},
	note = {arXiv:1611.08097 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {18--42},
	file = {Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf:/Users/swu/Zotero/storage/YSL5NJH7/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf:application/pdf;Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf:/Users/swu/Zotero/storage/Q9W955KA/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf:application/pdf},
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.},
	language = {en},
	number = {476},
	urldate = {2023-01-04},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:/Users/swu/Zotero/storage/X3EU63JB/Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf;Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:/Users/swu/Zotero/storage/W9A9PXSD/Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf;Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:/Users/swu/Zotero/storage/IVG4DXLD/Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf;Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:/Users/swu/Zotero/storage/7YG8LAEA/Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf},
}

@article{schuck_medial_2015,
	title = {Medial {Prefrontal} {Cortex} {Predicts} {Internally} {Driven} {Strategy} {Shifts}},
	volume = {86},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315002111},
	doi = {10.1016/j.neuron.2015.03.015},
	abstract = {Many daily behaviors require us to actively focus on the current task and ignore all other distractions. Yet, ignoring everything else might hinder the ability to discover new ways to achieve the same goal. Here, we studied the neural mechanisms that support the spontaneous change to better strategies while an established strategy is executed. Multivariate neuroimaging analyses showed that before the spontaneous change to an alternative strategy, medial prefrontal cortex (MPFC) encoded information that was irrelevant for the current strategy but necessary for the later strategy. Importantly, this neural effect was related to future behavioral changes: information encoding in MPFC was changed only in participants who eventually switched their strategy and started before the actual strategy change. This allowed us to predict spontaneous strategy shifts ahead of time. These ﬁndings suggest that MPFC might internally simulate alternative strategies and shed new light on the organization of PFC.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Neuron},
	author = {Schuck, Nicolas W. and Gaschler, Robert and Wenke, Dorit and Heinzle, Jakob and Frensch, Peter A. and Haynes, John-Dylan and Reverberi, Carlo},
	month = apr,
	year = {2015},
	pages = {331--340},
	file = {Schuck et al. - 2015 - Medial Prefrontal Cortex Predicts Internally Drive.pdf:/Users/swu/Zotero/storage/KJZ7VBM7/Schuck et al. - 2015 - Medial Prefrontal Cortex Predicts Internally Drive.pdf:application/pdf},
}

@article{ma_changing_2014,
	title = {Changing concepts of working memory},
	volume = {17},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn.3655},
	doi = {10.1038/nn.3655},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Nature Neuroscience},
	author = {Ma, Wei Ji and Husain, Masud and Bays, Paul M},
	month = mar,
	year = {2014},
	pages = {347--356},
	file = {Ma et al. - 2014 - Changing concepts of working memory.pdf:/Users/swu/Zotero/storage/DDQ9F4JL/Ma et al. - 2014 - Changing concepts of working memory.pdf:application/pdf},
}

@article{tomov_neural_2018,
	title = {Neural {Computations} {Underlying} {Causal} {Structure} {Learning}},
	volume = {38},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3336-17.2018},
	doi = {10.1523/JNEUROSCI.3336-17.2018},
	language = {en},
	number = {32},
	urldate = {2023-01-04},
	journal = {The Journal of Neuroscience},
	author = {Tomov, Momchil S. and Dorfman, Hayley M. and Gershman, Samuel J.},
	month = aug,
	year = {2018},
	pages = {7143--7157},
	file = {Tomov et al. - 2018 - Neural Computations Underlying Causal Structure Le.pdf:/Users/swu/Zotero/storage/WHADBUSJ/Tomov et al. - 2018 - Neural Computations Underlying Causal Structure Le.pdf:application/pdf},
}

@article{lashley_problem_nodate,
	title = {The {Problem} of {Serial} {Order} in {Behavior}},
	language = {en},
	author = {Lashley, K S},
	file = {Lashley - The Problem of Serial Order in Behavior.pdf:/Users/swu/Zotero/storage/HK8DUVZU/Lashley - The Problem of Serial Order in Behavior.pdf:application/pdf},
}

@article{kollmorgen_nearest_2020,
	title = {Nearest neighbours reveal fast and slow components of motor learning},
	volume = {577},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1892-x},
	doi = {10.1038/s41586-019-1892-x},
	language = {en},
	number = {7791},
	urldate = {2023-01-04},
	journal = {Nature},
	author = {Kollmorgen, Sepp and Hahnloser, Richard H. R. and Mante, Valerio},
	month = jan,
	year = {2020},
	pages = {526--530},
	file = {Kollmorgen et al. - 2020 - Nearest neighbours reveal fast and slow components.pdf:/Users/swu/Zotero/storage/GITIQCUQ/Kollmorgen et al. - 2020 - Nearest neighbours reveal fast and slow components.pdf:application/pdf},
}

@article{gobet_whats_2016,
	title = {What's in a {Name}? {The} {Multiple} {Meanings} of “{Chunk}” and “{Chunking}”},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {What's in a {Name}?},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.00102/abstract},
	doi = {10.3389/fpsyg.2016.00102},
	language = {en},
	urldate = {2023-01-04},
	journal = {Frontiers in Psychology},
	author = {Gobet, Fernand and Lloyd-Kelly, Martyn and Lane, Peter C. R.},
	month = feb,
	year = {2016},
	file = {Gobet et al. - 2016 - What's in a Name The Multiple Meanings of “Chunk”.pdf:/Users/swu/Zotero/storage/F6YC9BX7/Gobet et al. - 2016 - What's in a Name The Multiple Meanings of “Chunk”.pdf:application/pdf;Gobet et al. - 2016 - What's in a Name The Multiple Meanings of “Chunk”.pdf:/Users/swu/Zotero/storage/89P9KB6T/Gobet et al. - 2016 - What's in a Name The Multiple Meanings of “Chunk”.pdf:application/pdf},
}

@article{rosenbaum_hierarchical_1983,
	title = {Hierarchical control of rapid movement sequences.},
	volume = {9},
	issn = {1939-1277, 0096-1523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.9.1.86},
	doi = {10.1037/0096-1523.9.1.86},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Rosenbaum, David A. and Kenny, Sandra B. and Derr, Marcia A.},
	year = {1983},
	pages = {86--102},
	file = {Rosenbaum et al. - 1983 - Hierarchical control of rapid movement sequences..pdf:/Users/swu/Zotero/storage/7MGIPQRQ/Rosenbaum et al. - 1983 - Hierarchical control of rapid movement sequences..pdf:application/pdf;Rosenbaum et al. - 1983 - Hierarchical control of rapid movement sequences..pdf:/Users/swu/Zotero/storage/SEJKJN4L/Rosenbaum et al. - 1983 - Hierarchical control of rapid movement sequences..pdf:application/pdf},
}

@article{nassar_chunking_2018,
	title = {Chunking as a rational strategy for lossy data compression in visual working memory.},
	volume = {125},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000101},
	doi = {10.1037/rev0000101},
	abstract = {The nature of capacity limits for visual working memory has been the subject of an intense debate that has relied on models that assume items are encoded independently. Here we propose that instead, similar features are jointly encoded through a “chunking” process to optimize performance on visual working memory tasks. We show that such chunking can: 1) facilitate performance improvements for abstract capacity-limited systems, 2) be optimized through reinforcement, 3) be implemented by center-surround dynamics, and 4) increase effective storage capacity at the expense of recall precision. Human performance on a variant of a canonical working memory task demonstrated performance advantages, precision detriments, inter-item dependencies, and trial-to-trial behavioral adjustments diagnostic of performance optimization through center-surround chunking. Models incorporating center-surround chunking provided a better quantitative description of human performance in our study as well as in a meta-analytic dataset, and apparent differences in working memory capacity across individuals were attributable to individual differences in the implementation of chunking. Our results reveal a normative rationale for center-surround connectivity in working memory circuitry, call for re-evaluation of memory performance differences that have previously been attributed to differences in capacity, and support a more nuanced view of visual working memory capacity limitations: strategic tradeoff between storage capacity and memory precision through chunking contribute to flexible capacity limitations that include both discrete and continuous aspects.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {Psychological Review},
	author = {Nassar, Matthew R. and Helmers, Julie C. and Frank, Michael J.},
	month = jul,
	year = {2018},
	pages = {486--511},
	file = {Nassar et al. - 2018 - Chunking as a rational strategy for lossy data com.pdf:/Users/swu/Zotero/storage/W7P6M4WV/Nassar et al. - 2018 - Chunking as a rational strategy for lossy data com.pdf:application/pdf;Nassar et al. - 2018 - Chunking as a rational strategy for lossy data com.pdf:/Users/swu/Zotero/storage/ZGQRIM52/Nassar et al. - 2018 - Chunking as a rational strategy for lossy data com.pdf:application/pdf},
}

@article{mathy_whats_2012,
	title = {What’s magic about magic numbers? {Chunking} and data compression in short-term memory},
	volume = {122},
	issn = {00100277},
	shorttitle = {What’s magic about magic numbers?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027711002733},
	doi = {10.1016/j.cognition.2011.11.003},
	abstract = {Short term memory is famously limited in capacity to Miller’s (1956) magic number 7 ± 2—or, in many more recent studies, about 4 ± 1 ‘‘chunks’’ of information. But the deﬁnition of ‘‘chunk’’ in this context has never been clear, referring only to a set of items that are treated collectively as a single unit. We propose a new more quantitatively precise conception of chunk derived from the notion of Kolmogorov complexity and compressibility: a chunk is a unit in a maximally compressed code. We present a series of experiments in which we manipulated the compressibility of stimulus sequences by introducing sequential patterns of variable length. Our subjects’ measured digit span (raw short term memory capacity) consistently depended on the length of the pattern after compression, that is, the number of distinct sequences it contained. The true limit appears to be about 3 or 4 distinct chunks, consistent with many modern studies, but also equivalent to about 7 uncompressed items of typical compressibility, consistent with Miller’s famous magical number.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Cognition},
	author = {Mathy, Fabien and Feldman, Jacob},
	month = mar,
	year = {2012},
	pages = {346--362},
	file = {Mathy and Feldman - 2012 - What’s magic about magic numbers Chunking and dat.pdf:/Users/swu/Zotero/storage/QHXAGIVD/Mathy and Feldman - 2012 - What’s magic about magic numbers Chunking and dat.pdf:application/pdf;Mathy and Feldman - 2012 - What’s magic about magic numbers Chunking and dat.pdf:/Users/swu/Zotero/storage/SBY25ITH/Mathy and Feldman - 2012 - What’s magic about magic numbers Chunking and dat.pdf:application/pdf},
}

@article{bialek_predictability_2001,
	title = {Predictability, {Complexity}, and {Learning}},
	volume = {13},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/13/11/2409-2463/6472},
	doi = {10.1162/089976601753195969},
	abstract = {We define predictive information I
              pred
              (T) as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times T: I
              pred
              (T) can remain finite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a finite number of parameters, then I
              pred
              (T) grows logarithmically with a coefficient that counts the dimensionality of the model space. In contrast, power-law growth is associated, for example, with the learning of infinite parameter (or non-parametric) models such as continuous functions with smoothness constraints. There are connections between the predictive information and measures of complexity that have been defined both in learning theory and the analysis of physical systems through statistical mechanics and dynamical systems theory. Furthermore, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of I
              pred
              (T) provides the unique measure for the complexity of dynamics underlying a time series. Finally, we discuss how these ideas may be useful in problems in physics, statistics, and biology.},
	language = {en},
	number = {11},
	urldate = {2023-01-04},
	journal = {Neural Computation},
	author = {Bialek, William and Nemenman, Ilya and Tishby, Naftali},
	month = nov,
	year = {2001},
	pages = {2409--2463},
	file = {Bialek et al. - 2001 - Predictability, Complexity, and Learning.pdf:/Users/swu/Zotero/storage/SQIVMP66/Bialek et al. - 2001 - Predictability, Complexity, and Learning.pdf:application/pdf;Bialek et al. - 2001 - Predictability, Complexity, and Learning.pdf:/Users/swu/Zotero/storage/UURPHVG4/Bialek et al. - 2001 - Predictability, Complexity, and Learning.pdf:application/pdf},
}

@article{kidd_goldilocks_2012,
	title = {The {Goldilocks} {Effect}: {Human} {Infants} {Allocate} {Attention} to {Visual} {Sequences} {That} {Are} {Neither} {Too} {Simple} {Nor} {Too} {Complex}},
	volume = {7},
	issn = {1932-6203},
	shorttitle = {The {Goldilocks} {Effect}},
	url = {https://dx.plos.org/10.1371/journal.pone.0036399},
	doi = {10.1371/journal.pone.0036399},
	abstract = {Human infants, like immature members of any species, must be highly selective in sampling information from their environment to learn efficiently. Failure to be selective would waste precious computational resources on material that is already known (too simple) or unknowable (too complex). In two experiments with 7- and 8-month-olds, we measure infants’ visual attention to sequences of events varying in complexity, as determined by an ideal learner model. Infants’ probability of looking away was greatest on stimulus items whose complexity (negative log probability) according to the model was either very low or very high. These results suggest a principle of infant attention that may have broad applicability: infants implicitly seek to maintain intermediate rates of information absorption and avoid wasting cognitive resources on overly simple or overly complex events.},
	language = {en},
	number = {5},
	urldate = {2023-01-04},
	journal = {PLoS ONE},
	author = {Kidd, Celeste and Piantadosi, Steven T. and Aslin, Richard N.},
	editor = {Rodriguez-Fornells, Antoni},
	month = may,
	year = {2012},
	pages = {e36399},
	file = {Kidd et al. - 2012 - The Goldilocks Effect Human Infants Allocate Atte.PDF:/Users/swu/Zotero/storage/N55FCW4R/Kidd et al. - 2012 - The Goldilocks Effect Human Infants Allocate Atte.PDF:application/pdf;Kidd et al. - 2012 - The Goldilocks Effect Human Infants Allocate Atte.PDF:/Users/swu/Zotero/storage/PUH8FZSE/Kidd et al. - 2012 - The Goldilocks Effect Human Infants Allocate Atte.PDF:application/pdf},
}

@article{buhmann_vector_1993,
	title = {Vector quantization with complexity costs},
	volume = {39},
	issn = {00189448},
	url = {http://ieeexplore.ieee.org/document/243432/},
	doi = {10.1109/18.243432},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {IEEE Transactions on Information Theory},
	author = {Buhmann, J. and Kuhnel, H.},
	month = jul,
	year = {1993},
	pages = {1133--1145},
	file = {Buhmann and Kuhnel - 1993 - Vector quantization with complexity costs.pdf:/Users/swu/Zotero/storage/6I6QB4T3/Buhmann and Kuhnel - 1993 - Vector quantization with complexity costs.pdf:application/pdf;Buhmann and Kuhnel - 1993 - Vector quantization with complexity costs.pdf:/Users/swu/Zotero/storage/STUSKKEE/Buhmann and Kuhnel - 1993 - Vector quantization with complexity costs.pdf:application/pdf},
}

@book{cover_elements_1991,
	address = {New York},
	series = {Wiley series in telecommunications},
	title = {Elements of information theory},
	isbn = {978-0-471-06259-2},
	language = {en},
	publisher = {Wiley},
	author = {Cover, T. M. and Thomas, Joy A.},
	year = {1991},
	keywords = {Information theory},
	file = {Cover and Thomas - 1991 - Elements of information theory.pdf:/Users/swu/Zotero/storage/RCJX5IEM/Cover and Thomas - 1991 - Elements of information theory.pdf:application/pdf;Cover and Thomas - 1991 - Elements of information theory.pdf:/Users/swu/Zotero/storage/JP3W6N3H/Cover and Thomas - 1991 - Elements of information theory.pdf:application/pdf},
}

@article{tishby_information_nodate,
	title = {The {Information} {Bottleneck} {Method}},
	abstract = {We deﬁne the relevant information in a signal x ∈ X as being the information that this signal provides about another signal y ∈ Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of ﬁnding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a ‘bottleneck’ formed by a limited set of codewords X˜ . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x˜) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X → X˜ and X˜ → Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	language = {en},
	author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
	file = {Tishby et al. - The Information Bottleneck Method.pdf:/Users/swu/Zotero/storage/D94SGQ9P/Tishby et al. - The Information Bottleneck Method.pdf:application/pdf;Tishby et al. - The Information Bottleneck Method.pdf:/Users/swu/Zotero/storage/WI2GN7US/Tishby et al. - The Information Bottleneck Method.pdf:application/pdf},
}

@article{abend_bootstrapping_2017,
	title = {Bootstrapping language acquisition},
	volume = {164},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027717300495},
	doi = {10.1016/j.cognition.2017.02.009},
	abstract = {The semantic bootstrapping hypothesis proposes that children acquire their native language through exposure to sentences of the language paired with structured representations of their meaning, whose component substructures can be associated with words and syntactic structures used to express these concepts. The child’s task is then to learn a language-speciﬁc grammar and lexicon based on (probably contextually ambiguous, possibly somewhat noisy) pairs of sentences and their meaning representations (logical forms).},
	language = {en},
	urldate = {2023-01-04},
	journal = {Cognition},
	author = {Abend, Omri and Kwiatkowski, Tom and Smith, Nathaniel J. and Goldwater, Sharon and Steedman, Mark},
	month = jul,
	year = {2017},
	pages = {116--143},
	file = {Abend et al. - 2017 - Bootstrapping language acquisition.pdf:/Users/swu/Zotero/storage/TELVFSGW/Abend et al. - 2017 - Bootstrapping language acquisition.pdf:application/pdf;Abend et al. - 2017 - Bootstrapping language acquisition.pdf:/Users/swu/Zotero/storage/6XKHSBXM/Abend et al. - 2017 - Bootstrapping language acquisition.pdf:application/pdf},
}

@article{beal_nite_nodate,
	title = {The {Inﬁnite} {Hidden} {Markov} {Model}},
	abstract = {We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite—consider, for example, symbols being possible words appearing in English text.},
	language = {en},
	author = {Beal, Matthew J and Ghahramani, Zoubin and Rasmussen, Carl Edward},
	file = {Beal et al. - The Inﬁnite Hidden Markov Model.pdf:/Users/swu/Zotero/storage/8FIT5GSM/Beal et al. - The Inﬁnite Hidden Markov Model.pdf:application/pdf;Beal et al. - The Inﬁnite Hidden Markov Model.pdf:/Users/swu/Zotero/storage/RBSUMVPU/Beal et al. - The Inﬁnite Hidden Markov Model.pdf:application/pdf},
}

@article{largeron-leteno_prediction_2003,
	title = {Prediction suffix trees for supervised classification of sequences},
	volume = {24},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016786550300182X},
	doi = {10.1016/j.patrec.2003.08.002},
	abstract = {This article presents a statistical test and algorithms for patterns extraction and supervised classification of sequential data. First it defines the notion of Prediction Suffix Tree PST. This type of tree can be used to efficiently describe Variable order chain. It performs better than the Markov chain of order L and at a lower storage cost. We propose an improvement of this model, based on a statistical test. This test enables us to control the risk of encountering different patterns in the model of the sequence to classify and in the model of its class. Applications to biological sequences are presented to illustrate this procedure. We compare the results obtained with different models (Markov chain of order L, Variable order model and the statistical test, with or without smoothing). We set out to show how the choice of the parameters of the models influences performance in these applications. Obviously these algorithms can be used in other fields in which the data are naturally ordered.},
	language = {en},
	number = {16},
	urldate = {2023-01-04},
	journal = {Pattern Recognition Letters},
	author = {Largeron-Leténo, Christine},
	month = dec,
	year = {2003},
	pages = {3153--3164},
	file = {Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:/Users/swu/Zotero/storage/WKZGD4YJ/Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:application/pdf;Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:/Users/swu/Zotero/storage/J5WU2FCE/Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:application/pdf;Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:/Users/swu/Zotero/storage/5JSHPD67/Largeron-Leténo - 2003 - Prediction suffix trees for supervised classificat.pdf:application/pdf},
}

@article{todorov_efficient_2009,
	title = {Efficient computation of optimal actions},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0710743106},
	doi = {10.1073/pnas.0710743106},
	abstract = {Optimal choice of actions is a fundamental problem relevant to fields as diverse as neuroscience, psychology, economics, computer science, and control engineering. Despite this broad relevance the abstract setting is similar: we have an agent choosing actions over time, an uncertain dynamical system whose state is affected by those actions, and a performance criterion that the agent seeks to optimize. Solving problems of this kind remains hard, in part, because of overly generic formulations. Here, we propose a more structured formulation that greatly simplifies the construction of optimal control laws in both discrete and continuous domains. An exhaustive search over actions is avoided and the problem becomes linear. This yields algorithms that outperform Dynamic Programming and Reinforcement Learning, and thereby solve traditional problems more efficiently. Our framework also enables computations that were not possible before: composing optimal control laws by mixing primitives, applying deterministic methods to stochastic systems, quantifying the benefits of error tolerance, and inferring goals from behavioral data via convex optimization. Development of a general class of easily solvable problems tends to accelerate progress—as linear systems theory has done, for example. Our framework may have similar impact in fields where optimal choice of actions is relevant.},
	language = {en},
	number = {28},
	urldate = {2023-01-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Todorov, Emanuel},
	month = jul,
	year = {2009},
	pages = {11478--11483},
	file = {Todorov - 2009 - Efficient computation of optimal actions.pdf:/Users/swu/Zotero/storage/XMUTSZKQ/Todorov - 2009 - Efficient computation of optimal actions.pdf:application/pdf;Todorov - 2009 - Efficient computation of optimal actions.pdf:/Users/swu/Zotero/storage/462M86ND/Todorov - 2009 - Efficient computation of optimal actions.pdf:application/pdf},
}

@article{tomov_discovery_2020,
	title = {Discovery of hierarchical representations for efficient planning},
	volume = {16},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1007594},
	doi = {10.1371/journal.pcbi.1007594},
	abstract = {We propose that humans spontaneously organize environments into clusters of states that support hierarchical planning, enabling them to tackle challenging problems by breaking them down into sub-problems at various levels of abstraction. People constantly rely on such hierarchical presentations to accomplish tasks big and small—from planning one’s day, to organizing a wedding, to getting a PhD—often succeeding on the very first attempt. We formalize a Bayesian model of hierarchy discovery that explains how humans discover such useful abstractions. Building on principles developed in structure learning and robotics, the model predicts that hierarchy discovery should be sensitive to the topological structure, reward distribution, and distribution of tasks in the environment. In five simulations, we show that the model accounts for previously reported effects of environment structure on planning behavior, such as detection of bottleneck states and transitions. We then test the novel predictions of the model in eight behavioral experiments, demonstrating how the distribution of tasks and rewards can influence planning behavior via the discovered hierarchy, sometimes facilitating and sometimes hindering performance. We find evidence that the hierarchy discovery process unfolds incrementally across trials. Finally, we propose how hierarchy discovery and hierarchical planning might be implemented in the brain. Together, these findings present an important advance in our understanding of how the brain might use Bayesian inference to discover and exploit the hidden hierarchical structure of the environment.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {PLOS Computational Biology},
	author = {Tomov, Momchil S. and Yagati, Samyukta and Kumar, Agni and Yang, Wanqian and Gershman, Samuel J.},
	editor = {Pascucci, David},
	month = apr,
	year = {2020},
	pages = {e1007594},
	file = {Tomov et al. - 2020 - Discovery of hierarchical representations for effi.pdf:/Users/swu/Zotero/storage/IAJY3KAE/Tomov et al. - 2020 - Discovery of hierarchical representations for effi.pdf:application/pdf;Tomov et al. - 2020 - Discovery of hierarchical representations for effi.pdf:/Users/swu/Zotero/storage/VH7VHR7U/Tomov et al. - 2020 - Discovery of hierarchical representations for effi.pdf:application/pdf},
}

@article{stolcke_hidden_nodate,
	title = {Hidden {Markov} {Model}\vphantom{\{}\} {Induction} by {Bayesian} {Model} {Merging}},
	abstract = {This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge.},
	language = {en},
	author = {Stolcke, Andreas and Omohundro, Stephen M},
	file = {Stolcke and Omohundro - Hidden Markov Model Induction by Bayesian Model M.pdf:/Users/swu/Zotero/storage/5K2AK5BI/Stolcke and Omohundro - Hidden Markov Model Induction by Bayesian Model M.pdf:application/pdf;Stolcke and Omohundro - Hidden Markov Model Induction by Bayesian Model M.pdf:/Users/swu/Zotero/storage/3FA5JGSC/Stolcke and Omohundro - Hidden Markov Model Induction by Bayesian Model M.pdf:application/pdf},
}

@article{russek_predictive_2017,
	title = {Predictive representations can link model-based reinforcement learning to model-free mechanisms},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005768},
	doi = {10.1371/journal.pcbi.1005768},
	language = {en},
	number = {9},
	urldate = {2023-01-04},
	journal = {PLOS Computational Biology},
	author = {Russek, Evan M. and Momennejad, Ida and Botvinick, Matthew M. and Gershman, Samuel J. and Daw, Nathaniel D.},
	editor = {Daunizeau, Jean},
	month = sep,
	year = {2017},
	pages = {e1005768},
	file = {Russek et al. - 2017 - Predictive representations can link model-based re.pdf:/Users/swu/Zotero/storage/B4MTRH3N/Russek et al. - 2017 - Predictive representations can link model-based re.pdf:application/pdf;Russek et al. - 2017 - Predictive representations can link model-based re.pdf:/Users/swu/Zotero/storage/DXW4YZ2B/Russek et al. - 2017 - Predictive representations can link model-based re.pdf:application/pdf},
}

@article{goldwater_bayesian_2009,
	title = {A {Bayesian} framework for word segmentation: {Exploring} the effects of context},
	volume = {112},
	issn = {00100277},
	shorttitle = {A {Bayesian} framework for word segmentation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027709000675},
	doi = {10.1016/j.cognition.2009.03.008},
	abstract = {Since the experiments of Saffran et al. [Saffran, J., Aslin, R., \& Newport, E. (1996). Statistical learning in 8-month-old infants. Science, 274, 1926–1928], there has been a great deal of interest in the question of how statistical regularities in the speech stream might be used by infants to begin to identify individual words. In this work, we use computational modeling to explore the effects of different assumptions the learner might make regarding the nature of words – in particular, how these assumptions affect the kinds of words that are segmented from a corpus of transcribed child-directed speech. We develop several models within a Bayesian ideal observer framework, and use them to examine the consequences of assuming either that words are independent units, or units that help to predict other units. We show through empirical and theoretical results that the assumption of independence causes the learner to undersegment the corpus, with many two- and three-word sequences (e.g. what’s that, do you, in the house) misidentiﬁed as individual words. In contrast, when the learner assumes that words are predictive, the resulting segmentation is far more accurate. These results indicate that taking context into account is important for a statistical word segmentation strategy to be successful, and raise the possibility that even young infants may be able to exploit more subtle statistical patterns than have usually been considered.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Cognition},
	author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark},
	month = jul,
	year = {2009},
	pages = {21--54},
	file = {Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:/Users/swu/Zotero/storage/TXCGNVLD/Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:application/pdf;Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:/Users/swu/Zotero/storage/8NDFS3SU/Goldwater et al. - 2009 - A Bayesian framework for word segmentation Explor.pdf:application/pdf},
}

@article{perruchet_parser_1998,
	title = {{PARSER}: {A} {Model} for {Word} {Segmentation}},
	volume = {39},
	issn = {0749596X},
	shorttitle = {{PARSER}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X98925761},
	doi = {10.1006/jmla.1998.2576},
	language = {en},
	number = {2},
	urldate = {2023-01-04},
	journal = {Journal of Memory and Language},
	author = {Perruchet, Pierre and Vinter, Annie},
	month = aug,
	year = {1998},
	pages = {246--263},
	file = {Perruchet and Vinter - 1998 - PARSER A Model for Word Segmentation.pdf:/Users/swu/Zotero/storage/XJYN8PA3/Perruchet and Vinter - 1998 - PARSER A Model for Word Segmentation.pdf:application/pdf;Perruchet and Vinter - 1998 - PARSER A Model for Word Segmentation.pdf:/Users/swu/Zotero/storage/MT6EKZUR/Perruchet and Vinter - 1998 - PARSER A Model for Word Segmentation.pdf:application/pdf},
}

@article{gershman_rational_nodate,
	title = {The rational analysis of memory},
	abstract = {This chapter surveys rational models of memory, which posit that memory is optimized to store information that will be needed in the future, subject to the constraint that information can only be stored with a limited amount of precision. This optimization problem can be formalized using the framework of rate-distortion theory. The design principles that emerge from this framework shed light on numerous regularities of memory, as well as how cognitive and environmental factors shape these regularities.},
	language = {en},
	author = {Gershman, Samuel J},
	file = {Gershman - The rational analysis of memory.pdf:/Users/swu/Zotero/storage/IDBLTVHK/Gershman - The rational analysis of memory.pdf:application/pdf;Gershman - The rational analysis of memory.pdf:/Users/swu/Zotero/storage/TMSYH3IV/Gershman - The rational analysis of memory.pdf:application/pdf},
}

@article{haefner_perceptual_2016,
	title = {Perceptual {Decision}-{Making} as {Probabilistic} {Inference} by {Neural} {Sampling}},
	volume = {90},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316300113},
	doi = {10.1016/j.neuron.2016.03.020},
	abstract = {We address two main challenges facing systems neuroscience today: understanding the nature and function of cortical feedback between sensory areas and of correlated variability. Starting from the old idea of perception as probabilistic inference, we show how to use knowledge of the psychophysical task to make testable predictions for the inﬂuence of feedback signals on early sensory representations. Applying our framework to a two-alternative forced choice task paradigm, we can explain multiple empirical ﬁndings that have been hard to account for by the traditional feedforward model of sensory processing, including the task dependence of neural response correlations and the diverging time courses of choice probabilities and psychophysical kernels. Our model makes new predictions and characterizes a component of correlated variability that represents task-related information rather than performance-degrading noise. It demonstrates a normative way to integrate sensory and cognitive components into physiologically testable models of perceptual decision-making.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Neuron},
	author = {Haefner, Ralf M. and Berkes, Pietro and Fiser, József},
	month = may,
	year = {2016},
	pages = {649--660},
	file = {Haefner et al. - 2016 - Perceptual Decision-Making as Probabilistic Infere.pdf:/Users/swu/Zotero/storage/IL42PQX4/Haefner et al. - 2016 - Perceptual Decision-Making as Probabilistic Infere.pdf:application/pdf;Haefner et al. - 2016 - Perceptual Decision-Making as Probabilistic Infere.pdf:/Users/swu/Zotero/storage/QR66UN84/Haefner et al. - 2016 - Perceptual Decision-Making as Probabilistic Infere.pdf:application/pdf},
}

@incollection{scholkopf_adaptor_2007,
	title = {Adaptor {Grammars}:{A} {Framework} for {Specifying} {Compositional} {Nonparametric} {Bayesian} {Models}},
	isbn = {978-0-262-25691-9},
	shorttitle = {Adaptor {Grammars}},
	url = {https://direct.mit.edu/books/book/3168/chapter/87467/adaptor-grammars-a-framework-for-specifying},
	abstract = {This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs). Adaptor grammars augment the probabilistic rules of PCFGs with “adaptors” that can induce dependencies among successive uses. With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars. We present a general-purpose inference algorithm for adaptor grammars, making it easy to deﬁne and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.},
	language = {en},
	urldate = {2023-01-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {The MIT Press},
	editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
	year = {2007},
	doi = {10.7551/mitpress/7503.003.0085},
	file = {Schölkopf et al. - 2007 - Adaptor GrammarsA Framework for Specifying Compos.pdf:/Users/swu/Zotero/storage/4XJB88HL/Schölkopf et al. - 2007 - Adaptor GrammarsA Framework for Specifying Compos.pdf:application/pdf},
}

@article{tempel_directed_2016,
	title = {Directed forgetting benefits motor sequence encoding},
	volume = {44},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/s13421-015-0565-8},
	doi = {10.3758/s13421-015-0565-8},
	abstract = {Two experiments investigated directed forgetting of newly learned motor sequences. Concurrently with the list method of directed forgetting, participants successively learned two lists of motor sequences. Each sequence consisted of four consecutive finger movements. After a short distractor task, a recall test was given. Both experiments compared a forget group that was instructed to forget list-1 items with a remember group not receiving a forget instruction. We found that the instruction to forget list 1 enhanced recall of subsequently learned motor sequences. This benefit of directed forgetting occurred independently of costs for list 1. A mediation analysis showed that the encoding accuracy of list 2 was a mediator of the recall benefit, that is, the more accurate execution of motor sequences of list 2 after receiving a forget instruction for list 1 accounted for better recall of list 2. Thus, the adaptation of the list method to motor action provided more direct evidence on the effect of directed forgetting on subsequent learning. The results corroborate the assumption of a reset of encoding as a consequence of directed forgetting.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Memory \& Cognition},
	author = {Tempel, Tobias and Frings, Christian},
	month = apr,
	year = {2016},
	pages = {413--419},
	file = {Tempel and Frings - 2016 - Directed forgetting benefits motor sequence encodi.pdf:/Users/swu/Zotero/storage/PGASIW5S/Tempel and Frings - 2016 - Directed forgetting benefits motor sequence encodi.pdf:application/pdf},
}

@article{flesch_comparing_2018,
	title = {Comparing continual task learning in minds and machines},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1800755115},
	doi = {10.1073/pnas.1800755115},
	abstract = {Significance
            Humans learn to perform many different tasks over the lifespan, such as speaking both French and Spanish. The brain has to represent task information without mutual interference. In machine learning, this “continual learning” is a major unsolved challenge. Here, we studied the patterns of errors made by humans and state-of-the-art neural networks while they learned new tasks from scratch and without instruction. Humans, but not machines, seem to benefit from training regimes that blocked one task at a time, especially when they had a prior bias to represent stimuli in a way that encouraged task separation. Machines trained to exhibit the same prior bias suffered less interference between tasks, suggesting new avenues for solving continual learning in artificial systems.
          , 
            Humans can learn to perform multiple tasks in succession over the lifespan (“continual” learning), whereas current machine learning systems fail. Here, we investigated the cognitive mechanisms that permit successful continual learning in humans and harnessed our behavioral findings for neural network design. Humans categorized naturalistic images of trees according to one of two orthogonal task rules that were learned by trial and error. Training regimes that focused on individual rules for prolonged periods (blocked training) improved human performance on a later test involving randomly interleaved rules, compared with control regimes that trained in an interleaved fashion. Analysis of human error patterns suggested that blocked training encouraged humans to form “factorized” representation that optimally segregated the tasks, especially for those individuals with a strong prior bias to represent the stimulus space in a well-structured way. By contrast, standard supervised deep neural networks trained on the same tasks suffered catastrophic forgetting under blocked training, due to representational interference in the deeper layers. However, augmenting deep networks with an unsupervised generative model that allowed it to first learn a good embedding of the stimulus space (similar to that observed in humans) reduced catastrophic forgetting under blocked training. Building artificial agents that first learn a model of the world may be one promising route to solving continual task performance in artificial intelligence research.},
	language = {en},
	number = {44},
	urldate = {2023-01-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Flesch, Timo and Balaguer, Jan and Dekker, Ronald and Nili, Hamed and Summerfield, Christopher},
	month = oct,
	year = {2018},
	file = {Flesch et al. - 2018 - Comparing continual task learning in minds and mac.pdf:/Users/swu/Zotero/storage/HUFEL4V3/Flesch et al. - 2018 - Comparing continual task learning in minds and mac.pdf:application/pdf},
}

@article{wang_model_2017,
	title = {A model of human motor sequence learning explains facilitation and interference effects based on spike-timing dependent plasticity},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005632},
	doi = {10.1371/journal.pcbi.1005632},
	language = {en},
	number = {8},
	urldate = {2023-01-04},
	journal = {PLOS Computational Biology},
	author = {Wang, Quan and Rothkopf, Constantin A. and Triesch, Jochen},
	editor = {Marinazzo, Daniele},
	month = aug,
	year = {2017},
	pages = {e1005632},
	file = {Wang et al. - 2017 - A model of human motor sequence learning explains .pdf:/Users/swu/Zotero/storage/SH4HM8MB/Wang et al. - 2017 - A model of human motor sequence learning explains .pdf:application/pdf},
}

@article{odonnell_fragment_nodate,
	title = {Fragment {Grammars}: {Exploring} {Computation} and {Reuse} in {Language}},
	language = {en},
	author = {O’Donnell, Timothy J and Goodman, Noah D and Tenenbaum, Joshua B},
	file = {O’Donnell et al. - Fragment Grammars Exploring Computation and Reuse.pdf:/Users/swu/Zotero/storage/C2GUJM7F/O’Donnell et al. - Fragment Grammars Exploring Computation and Reuse.pdf:application/pdf},
}

@misc{lake_compositional_2019,
	title = {Compositional generalization through meta sequence-to-sequence learning},
	url = {http://arxiv.org/abs/1906.05381},
	abstract = {People can learn a new concept and use it compositionally, understanding how to “blicket twice” after learning how to “blicket.” In contrast, powerful sequence-tosequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.},
	language = {en},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Lake, Brenden M.},
	month = oct,
	year = {2019},
	note = {arXiv:1906.05381 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: This paper appears in the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
	annote = {Comment: This paper appears in the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
	file = {Lake - 2019 - Compositional generalization through meta sequence.pdf:/Users/swu/Zotero/storage/K9GTL6A7/Lake - 2019 - Compositional generalization through meta sequence.pdf:application/pdf},
}

@article{mussgens_transfer_2015,
	title = {Transfer in {Motor} {Sequence} {Learning}: {Effects} of {Practice} {Schedule} and {Sequence} {Context}},
	volume = {9},
	issn = {1662-5161},
	shorttitle = {Transfer in {Motor} {Sequence} {Learning}},
	url = {http://journal.frontiersin.org/Article/10.3389/fnhum.2015.00642/abstract},
	doi = {10.3389/fnhum.2015.00642},
	abstract = {Transfer (i.e., the application of a learned skill in a novel context) is an important and desirable outcome of motor skill learning. While much research has been devoted to understanding transfer of explicit skills the mechanisms of skill transfer after incidental learning remain poorly understood. The aim of this study was to (1) examine the effect of practice schedule on transfer and (2) investigate whether sequence-speciﬁc knowledge can transfer to an unfamiliar sequence context. We trained two groups of participants on an implicit serial response time task under a Constant (one sequence for 10 blocks) or Variable (alternating between two sequences for a total of 10 blocks) practice schedule. We evaluated response times for three types of transfer: task-general transfer to a structurally non-overlapping sequence, inter-manual transfer to a perceptually identical sequence, and sequence-speciﬁc transfer to a partially overlapping (three shared triplets) sequence. Results showed partial skill transfer to all three sequences and an advantage of Variable practice only for task-general transfer. Further, we found expression of sequence-speciﬁc knowledge for familiar sub-sequences in the overlapping sequence. These ﬁndings suggest that (1) constant practice may create interference for task-general transfer and (2) sequence-speciﬁc knowledge can transfer to a new sequential context.},
	language = {en},
	urldate = {2023-01-04},
	journal = {Frontiers in Human Neuroscience},
	author = {Müssgens, Diana M. and Ullén, Fredrik},
	month = nov,
	year = {2015},
	file = {Müssgens and Ullén - 2015 - Transfer in Motor Sequence Learning Effects of Pr.pdf:/Users/swu/Zotero/storage/2SGBP2S6/Müssgens and Ullén - 2015 - Transfer in Motor Sequence Learning Effects of Pr.pdf:application/pdf},
}

@article{zemel_probabilistic_nodate,
	title = {Probabilistic {Interpretation} of {Population} {Codes}},
	abstract = {We present a general encoding-decoding framework for interpreting the activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population. In casting it in the encoding-decoding framework, we ﬁnd that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area. Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity. We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method.},
	language = {en},
	author = {Zemel, Richard S and Dayan, Peter and Pouget, Alexandre},
	file = {Zemel et al. - Probabilistic Interpretation of Population Codes.pdf:/Users/swu/Zotero/storage/HC2GPAAG/Zemel et al. - Probabilistic Interpretation of Population Codes.pdf:application/pdf;Zemel et al. - Probabilistic Interpretation of Population Codes.pdf:/Users/swu/Zotero/storage/LTRX6ZK6/Zemel et al. - Probabilistic Interpretation of Population Codes.pdf:application/pdf},
}

@article{perez_efficient_2016,
	title = {Efficient approximation of probability distributions with k-order decomposable models},
	volume = {74},
	issn = {0888613X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0888613X16300329},
	doi = {10.1016/j.ijar.2016.03.005},
	language = {en},
	urldate = {2023-01-04},
	journal = {International Journal of Approximate Reasoning},
	author = {Pérez, Aritz and Inza, Iñaki and Lozano, Jose A.},
	month = jul,
	year = {2016},
	pages = {58--87},
	file = {Pérez et al. - 2016 - Efficient approximation of probability distributio.pdf:/Users/swu/Zotero/storage/N8XXAZYT/Pérez et al. - 2016 - Efficient approximation of probability distributio.pdf:application/pdf},
}

@article{ku_approximating_1969,
	title = {Approximating discrete probability distributions},
	volume = {15},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1054336/},
	doi = {10.1109/TIT.1969.1054336},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {IEEE Transactions on Information Theory},
	author = {Ku, H. and Kullback, S.},
	month = jul,
	year = {1969},
	pages = {444--447},
	file = {Ku and Kullback - 1969 - Approximating discrete probability distributions.pdf:/Users/swu/Zotero/storage/ZG4RPDKL/Ku and Kullback - 1969 - Approximating discrete probability distributions.pdf:application/pdf},
}

@article{chow_approximating_1968,
	title = {Approximating discrete probability distributions with dependence trees},
	volume = {14},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1054142/},
	doi = {10.1109/TIT.1968.1054142},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {IEEE Transactions on Information Theory},
	author = {Chow, C. and Liu, C.},
	month = may,
	year = {1968},
	pages = {462--467},
	file = {Chow and Liu - 1968 - Approximating discrete probability distributions w.pdf:/Users/swu/Zotero/storage/9FIZYTHJ/Chow and Liu - 1968 - Approximating discrete probability distributions w.pdf:application/pdf},
}

@techreport{maheu_rational_2020,
	type = {preprint},
	title = {Rational arbitration between statistics and rules in human sequence processing},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.02.06.937706},
	abstract = {Abstract
          Detecting and learning temporal regularities is essential to accurately predict the future. A long-standing debate in cognitive science concerns the existence of a dissociation, in humans, between two systems, one for handling statistical regularities governing the probabilities of individual items and their transitions, and another for handling deterministic rules. Here, to address this issue, we used finger tracking to continuously monitor the online build-up of evidence, confidence, false alarms and changes-of-mind during sequence processing. All these aspects of behaviour conformed tightly to a hierarchical Bayesian inference model with distinct hypothesis spaces for statistics and rules, yet linked by a single probabilistic currency. Alternative models based either on a single statistical mechanism or on two non-commensurable systems were rejected. Our results indicate that a hierarchical Bayesian inference mechanism, capable of operating over distinct hypothesis spaces for statistics and rules, underlies the human capability for sequence processing.},
	language = {en},
	urldate = {2023-01-04},
	institution = {Neuroscience},
	author = {Maheu, Maxime and Meyniel, Florent and Dehaene, Stanislas},
	month = feb,
	year = {2020},
	doi = {10.1101/2020.02.06.937706},
	file = {Maheu et al. - 2020 - Rational arbitration between statistics and rules .pdf:/Users/swu/Zotero/storage/KGAH7GRZ/Maheu et al. - 2020 - Rational arbitration between statistics and rules .pdf:application/pdf},
}

@article{elteto_developmental_nodate,
	title = {Developmental differences in the underlying mechanism of statistical learning},
	language = {en},
	author = {Éltető, Noémi},
	file = {Éltető - Developmental differences in the underlying mechan.pdf:/Users/swu/Zotero/storage/YXMY2RWK/Éltető - Developmental differences in the underlying mechan.pdf:application/pdf},
}

@article{szegedi-hallgato_explicit_2017,
	title = {Explicit instructions and consolidation promote rewiring of automatic behaviors in the human mind},
	volume = {7},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-04500-3},
	doi = {10.1038/s41598-017-04500-3},
	abstract = {Abstract
            One major challenge in human behavior and brain sciences is to understand how we can rewire already existing perceptual, motor, cognitive, and social skills or habits. Here we aimed to characterize one aspect of rewiring, namely, how we can update our knowledge of sequential/statistical regularities when they change. The dynamics of rewiring was explored from learning to consolidation using a unique experimental design which is suitable to capture the effect of implicit and explicit processing and the proactive and retroactive interference. Our results indicate that humans can rewire their knowledge of such regularities incidentally, and consolidation has a critical role in this process. Moreover, old and new knowledge can coexist, leading to effective adaptivity of the human mind in the changing environment, although the execution of the recently acquired knowledge may be more fluent than the execution of the previously learned one. These findings can contribute to a better understanding of the cognitive processes underlying behavior change, and can provide insights into how we can boost behavior change in various contexts, such as sports, educational settings or psychotherapy.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Scientific Reports},
	author = {Szegedi-Hallgató, Emese and Janacsek, Karolina and Vékony, Teodóra and Tasi, Lia Andrea and Kerepes, Leila and Hompoth, Emőke Adrienn and Bálint, Anna and Németh, Dezső},
	month = jun,
	year = {2017},
	pages = {4365},
	file = {Szegedi-Hallgató et al. - 2017 - Explicit instructions and consolidation promote re.pdf:/Users/swu/Zotero/storage/235B5USZ/Szegedi-Hallgató et al. - 2017 - Explicit instructions and consolidation promote re.pdf:application/pdf},
}

@article{kobor_statistical_2017,
	title = {Statistical learning leads to persistent memory: {Evidence} for one-year consolidation},
	volume = {7},
	issn = {2045-2322},
	shorttitle = {Statistical learning leads to persistent memory},
	url = {https://www.nature.com/articles/s41598-017-00807-3},
	doi = {10.1038/s41598-017-00807-3},
	abstract = {Abstract
            Statistical learning is a robust mechanism of the brain that enables the extraction of environmental patterns, which is crucial in perceptual and cognitive domains. However, the dynamical change of processes underlying long-term statistical memory formation has not been tested in an appropriately controlled design. Here we show that a memory trace acquired by statistical learning is resistant to inference as well as to forgetting after one year. Participants performed a statistical learning task and were retested one year later without further practice. The acquired statistical knowledge was resistant to interference, since after one year, participants showed similar memory performance on the previously practiced statistical structure after being tested with a new statistical structure. These results could be key to understand the stability of long-term statistical knowledge.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Scientific Reports},
	author = {Kóbor, Andrea and Janacsek, Karolina and Takács, Ádám and Nemeth, Dezso},
	month = apr,
	year = {2017},
	pages = {760},
	file = {Kóbor et al. - 2017 - Statistical learning leads to persistent memory E.pdf:/Users/swu/Zotero/storage/NQ4MNSIR/Kóbor et al. - 2017 - Statistical learning leads to persistent memory E.pdf:application/pdf;Kóbor et al. - 2017 - Statistical learning leads to persistent memory E.pdf:/Users/swu/Zotero/storage/GC4FMZH5/Kóbor et al. - 2017 - Statistical learning leads to persistent memory E.pdf:application/pdf},
}

@article{aji_generalized_2000,
	title = {The generalized distributive law},
	volume = {46},
	issn = {00189448},
	url = {http://ieeexplore.ieee.org/document/825794/},
	doi = {10.1109/18.825794},
	abstract = {In this semitutorial paper we discuss a general message passing algorithm, which we call the generalized distributive law (GDL). The GDL is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. It includes as special cases the Baum–Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gallager–Tanner–Wiberg decoding algorithm, Viterbi’s algorithm, the BCJR algorithm, Pearl’s “belief propagation” algorithm, the Shafer–Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the “junction tree” condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to.},
	language = {en},
	number = {2},
	urldate = {2023-01-04},
	journal = {IEEE Transactions on Information Theory},
	author = {Aji, S.M. and McEliece, R.J.},
	month = mar,
	year = {2000},
	pages = {325--343},
	file = {Aji and McEliece - 2000 - The generalized distributive law.pdf:/Users/swu/Zotero/storage/APF9YI5T/Aji and McEliece - 2000 - The generalized distributive law.pdf:application/pdf},
}

@article{gershman_discovering_2016,
	title = {Discovering hierarchical motion structure},
	volume = {126},
	issn = {00426989},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0042698915000814},
	doi = {10.1016/j.visres.2015.03.004},
	abstract = {Scenes ﬁlled with moving objects are often hierarchically organized: the motion of a migrating goose is nested within the ﬂight pattern of its ﬂock, the motion of a car is nested within the trafﬁc pattern of other cars on the road, the motion of body parts are nested in the motion of the body. Humans perceive hierarchical structure even in stimuli with two or three moving dots. An inﬂuential theory of hierarchical motion perception holds that the visual system performs a ‘‘vector analysis’’ of moving objects, decomposing them into common and relative motions. However, this theory does not specify how to resolve ambiguity when a scene admits more than one vector analysis. We describe a Bayesian theory of vector analysis and show that it can account for classic results from dot motion experiments, as well as new experimental data. Our theory takes a step towards understanding how moving scenes are parsed into objects.},
	language = {en},
	urldate = {2023-01-04},
	journal = {Vision Research},
	author = {Gershman, Samuel J. and Tenenbaum, Joshua B. and Jäkel, Frank},
	month = sep,
	year = {2016},
	pages = {232--241},
	file = {Gershman et al. - 2016 - Discovering hierarchical motion structure.pdf:/Users/swu/Zotero/storage/TLEA8QCS/Gershman et al. - 2016 - Discovering hierarchical motion structure.pdf:application/pdf;Gershman et al. - 2016 - Discovering hierarchical motion structure.pdf:/Users/swu/Zotero/storage/EURKTSAE/Gershman et al. - 2016 - Discovering hierarchical motion structure.pdf:application/pdf},
}

@article{verwey_buffer_nodate,
	title = {Buffer {Loading} and {Chunking} in {Sequential} {Keypressing}},
	language = {en},
	author = {Verwey, Willem B},
	file = {Verwey - Buffer Loading and Chunking in Sequential Keypress.pdf:/Users/swu/Zotero/storage/52R634GG/Verwey - Buffer Loading and Chunking in Sequential Keypress.pdf:application/pdf},
}

@article{aslin_statistical_2012,
	title = {Statistical {Learning}: {From} {Acquiring} {Specific} {Items} to {Forming} {General} {Rules}},
	volume = {21},
	issn = {0963-7214, 1467-8721},
	shorttitle = {Statistical {Learning}},
	url = {http://journals.sagepub.com/doi/10.1177/0963721412436806},
	doi = {10.1177/0963721412436806},
	abstract = {Statistical learning is a rapid and robust mechanism that enables adults and infants to extract patterns embedded in both language and visual domains. Statistical learning operates implicitly, without instruction, through mere exposure to a set of input stimuli. However, much of what learners must acquire about a structured domain consists of principles or rules that can be applied to novel inputs. It has been claimed that statistical learning and rule learning are separate mechanisms; in this article, however, we review evidence and provide a unifying perspective that argues for a single statistical-learning mechanism that accounts for both the learning of input stimuli and the generalization of learned patterns to novel instances. The balance between instance-learning and generalization is based on two factors: the strength of perceptual and cognitive biases that highlight structural regularities, and the consistency of elements’ contexts (unique vs. overlapping) in the input.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Current Directions in Psychological Science},
	author = {Aslin, Richard N. and Newport, Elissa L.},
	month = jun,
	year = {2012},
	pages = {170--176},
	file = {Aslin and Newport - 2012 - Statistical Learning From Acquiring Specific Item.pdf:/Users/swu/Zotero/storage/Y9ZYRTF5/Aslin and Newport - 2012 - Statistical Learning From Acquiring Specific Item.pdf:application/pdf},
}

@article{koch_patterns_2000,
	title = {Patterns, chunks, and hierarchies in serial reaction-time tasks},
	volume = {63},
	issn = {0340-0727},
	url = {http://link.springer.com/10.1007/PL00008165},
	doi = {10.1007/PL00008165},
	abstract = {The impact of relational structures (i.e., the systematicity of relations between successive items) on incidental sequence learning was investigated in a serial reaction-time (SRT) task while keeping constant the statistical structure. In order to assess the in¯uence of relational structures in stimulus and response sequences separately, the strength of relational patterns in sequences of digits as stimuli and of keystrokes as responses was orthogonally varied. In Exps. 1 and 2, the variation of relational patterns was mainly eective in the keystroke sequence. In Exp. 2, in addition to the variation of relational patterns, the presentation of stimuli was delayed at serial positions that were incongruent with the relational structure. The results show that these incongruent pauses reduced the learning of strongly structured sequences of keystrokes but improved the learning of weakly structured sequences. Experiment 3 suggests that even higher-order relations between elementary patterns are utilized to accelerate responses. The data are interpreted as evidence for the impact of relational patterns, in addition to statistical redundancies, on the formation of chunks. Reasons are discussed for the ®nding that relational chunking was more pronounced in the keystroke than in the digit sequences.},
	language = {en},
	number = {1},
	urldate = {2023-01-04},
	journal = {Psychological Research Psychologische Forschung},
	author = {Koch, Iring and Hoffmann, Joachim},
	month = mar,
	year = {2000},
	pages = {22--35},
	file = {Koch and Hoffmann - 2000 - Patterns, chunks, and hierarchies in serial reacti.pdf:/Users/swu/Zotero/storage/XLX3R4GK/Koch and Hoffmann - 2000 - Patterns, chunks, and hierarchies in serial reacti.pdf:application/pdf},
}

@article{brown_simplest_2008,
	title = {The simplest complete model of choice response time: {Linear} ballistic accumulation},
	volume = {57},
	issn = {00100285},
	shorttitle = {The simplest complete model of choice response time},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028507000722},
	doi = {10.1016/j.cogpsych.2007.12.002},
	abstract = {We propose a linear ballistic accumulator (LBA) model of decision making and reaction time. The LBA is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from ﬁve previously published experiments, we demonstrate that the LBA model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven diﬃcult for other theoretical accounts. Our results are encouraging in a ﬁeld beset by the tradeoﬀ between complexity and completeness.},
	language = {en},
	number = {3},
	urldate = {2023-01-04},
	journal = {Cognitive Psychology},
	author = {Brown, Scott D. and Heathcote, Andrew},
	month = nov,
	year = {2008},
	pages = {153--178},
	file = {Brown and Heathcote - 2008 - The simplest complete model of choice response tim.pdf:/Users/swu/Zotero/storage/9RVXWWM5/Brown and Heathcote - 2008 - The simplest complete model of choice response tim.pdf:application/pdf},
}

@article{orban_bayesian_2008,
	title = {Bayesian learning of visual chunks by human observers},
	volume = {105},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0708424105},
	doi = {10.1073/pnas.0708424105},
	abstract = {Efficient and versatile processing of any hierarchically structured information requires a learning mechanism that combines lower-level features into higher-level chunks. We investigated this chunking mechanism in humans with a visual pattern-learning paradigm. We developed an ideal learner based on Bayesian model comparison that extracts and stores only those chunks of information that are minimally sufficient to encode a set of visual scenes. Our ideal Bayesian chunk learner not only reproduced the results of a large set of previous empirical findings in the domain of human pattern learning but also made a key prediction that we confirmed experimentally. In accordance with Bayesian learning but contrary to associative learning, human performance was well above chance when pair-wise statistics in the exemplars contained no relevant information. Thus, humans extract chunks from complex visual patterns by generating accurate yet economical representations and not by encoding the full correlational structure of the input.},
	language = {en},
	number = {7},
	urldate = {2023-01-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Orbán, Gergő and Fiser, József and Aslin, Richard N. and Lengyel, Máté},
	month = feb,
	year = {2008},
	pages = {2745--2750},
	file = {Orbán et al. - 2008 - Bayesian learning of visual chunks by human observ.pdf:/Users/swu/Zotero/storage/KIJW47PX/Orbán et al. - 2008 - Bayesian learning of visual chunks by human observ.pdf:application/pdf},
}

@article{bill_visual_2022,
	title = {Visual motion perception as online hierarchical inference},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-34805-5},
	doi = {10.1038/s41467-022-34805-5},
	abstract = {Identifying the structure of motion relations in the environment is critical for navigation, tracking, prediction, and pursuit. Yet, little is known about the mental and neural computations that allow the visual system to infer this structure online from a volatile stream of visual information. We propose online hierarchical Bayesian inference as a principled solution for how the brain might solve this complex perceptual task. We derive an online Expectation-Maximization algorithm that explains human percepts qualitatively and quantitatively for a diverse set of stimuli, covering classical psychophysics experiments, ambiguous motion scenes, and illusory motion displays. We thereby identify normative explanations for the origin of human motion structure perception and make testable predictions for future psychophysics experiments. The proposed online hierarchical inference model furthermore affords a neural network implementation which shares properties with motion-sensitive cortical areas and motivates targeted experiments to reveal the neural representations of latent structure.},
	language = {en},
	number = {1},
	urldate = {2023-01-06},
	journal = {Nature Communications},
	author = {Bill, Johannes and Gershman, Samuel J. and Drugowitsch, Jan},
	month = dec,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Perception, Computer science, Motion detection, Network models},
	pages = {7403},
	file = {Full Text PDF:/Users/swu/Zotero/storage/E6M2T3L4/Bill et al. - 2022 - Visual motion perception as online hierarchical in.pdf:application/pdf},
}

@article{liberti_unstable_2016,
	title = {Unstable neurons underlie a stable learned behavior},
	volume = {19},
	issn = {1097-6256},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5127780/},
	doi = {10.1038/nn.4405},
	abstract = {Motor skills can be maintained for decades, but the biological basis of this memory persistence remains largely unknown. The zebra finch, for example, sings a highly stereotyped song that is stable for years, but it is not known whether the precise neural patterns underlying song are stable or shift from day to day. Here, we demonstrate that the population of projection neurons coding for song in the pre-motor nucleus HVC change from day to day. The most dramatic shifts occur over intervals of sleep. In contrast to the transient participation of excitatory neurons, ensemble measurements dominated by inhibition persist unchanged even after damage to downstream motor nerves. These observations offer a principle of motor stability: spatio-temporal patterns of inhibition can maintain a stable scaffold for motor dynamics while the population of principle neurons that directly drive behavior shift from one day to the next.},
	number = {12},
	urldate = {2023-01-07},
	journal = {Nature neuroscience},
	author = {Liberti, William A. and Markowitz, Jeffrey E. and Perkins, L. Nathan and Liberti, Derek C. and Leman, Daniel P. and Guitchounts, Grigori and Velho, Tarciso and Kotton, Darrell N. and Lois, Carlos and Gardner, Timothy J.},
	month = dec,
	year = {2016},
	pmid = {27723744},
	pmcid = {PMC5127780},
	pages = {1665--1671},
	file = {PubMed Central Full Text PDF:/Users/swu/Zotero/storage/CCTQCJ5S/Liberti et al. - 2016 - Unstable neurons underlie a stable learned behavio.pdf:application/pdf},
}

@misc{noauthor_low-frequency_nodate,
	title = {Low-frequency neural activity reflects rule-based chunking during speech listening {\textbar} {eLife}},
	url = {https://elifesciences.org/articles/55613},
	urldate = {2023-01-09},
	file = {Low-frequency neural activity reflects rule-based chunking during speech listening | eLife:/Users/swu/Zotero/storage/2Q59BY5J/55613.html:text/html},
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} {How} should we measure chunks? a continuing issue in chunking research and a way forward},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01456/full},
	urldate = {2023-01-09},
	file = {Frontiers | How should we measure chunks? a continuing issue in chunking research and a way forward:/Users/swu/Zotero/storage/8D5YD4SA/full.html:text/html},
}

@article{kim_spatial_2012,
	title = {Spatial {Information} {Outflow} from the {Hippocampal} {Circuit}: {Distributed} {Spatial} {Coding} and {Phase} {Precession} in the {Subiculum}},
	volume = {32},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Spatial {Information} {Outflow} from the {Hippocampal} {Circuit}},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.5942-11.2012},
	doi = {10.1523/JNEUROSCI.5942-11.2012},
	abstract = {Hippocampal place cells convey spatial information through a combination of spatially selective firing and theta phase precession. The way in which this information influences regions like the subiculum that receive input from the hippocampus remains unclear. The subiculum receives direct inputs from area CA1 of the hippocampus and sends divergent output projections to many other parts of the brain, so we examined the firing patterns of rat subicular neurons. We found a substantial transformation in the subicular code for space from sparse to dense firing rate representations along a proximal-distal anatomical gradient: neurons in the proximal subiculum are more similar to canonical, sparsely firing hippocampal place cells, whereas neurons in the distal subiculum have higher firing rates and more distributed spatial firing patterns. Using information theory, we found that the more distributed spatial representation in the subiculum carries, on average, more information about spatial location and context than the sparse spatial representation in CA1. Remarkably, despite the disparate firing rate properties of subicular neurons, we found that neurons at all proximal-distal locations exhibit robust theta phase precession, with similar spiking oscillation frequencies as neurons in area CA1. Our findings suggest that the subiculum is specialized to compress sparse hippocampal spatial codes into highly informative distributed codes suitable for efficient communication to other brain regions. Moreover, despite this substantial compression, the subiculum maintains finer scale temporal properties that may allow it to participate in oscillatory phase coding and spike timing-dependent plasticity in coordination with other regions of the hippocampal circuit.},
	language = {en},
	number = {34},
	urldate = {2023-01-09},
	journal = {Journal of Neuroscience},
	author = {Kim, S. M. and Ganguli, S. and Frank, L. M.},
	month = aug,
	year = {2012},
	pages = {11539--11558},
	annote = {[TLDR] The findings suggest that the subiculum is specialized to compress sparse hippocampal spatial codes into highly informative distributed codes suitable for efficient communication to other brain regions.},
	file = {Full Text:/Users/swu/Zotero/storage/95SPZ32K/Kim et al. - 2012 - Spatial Information Outflow from the Hippocampal C.pdf:application/pdf},
}

@misc{noauthor_event-related_nodate,
	title = {Event-related responses reflect chunk boundaries in natural speech - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922003275#fig0001},
	urldate = {2023-01-09},
}

@article{knowlton_artificial_nodate,
	title = {Artificial {Grammar} {Learning} {Depends} on {Implicit} {Acquisition} of {Both} {Abstract} and {Exemplar}-{Specific} {Information}},
	language = {en},
	author = {Knowlton, Barbara J and Squire, Larry R},
	file = {Knowlton and Squire - Artificial Grammar Learning Depends on Implicit Ac.pdf:/Users/swu/Zotero/storage/C5SY8Q47/Knowlton and Squire - Artificial Grammar Learning Depends on Implicit Ac.pdf:application/pdf},
}

@misc{noauthor_frontiers_nodate-1,
	title = {Frontiers {\textbar} {Decomposing} a {Chunk} into {Its} {Elements} and {Reorganizing} {Them} {As} a {New} {Chunk}: {The} {Two} {Different} {Sub}-processes {Underlying} {Insightful} {Chunk} {Decomposition}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02001/full},
	urldate = {2023-01-09},
	file = {Frontiers | Decomposing a Chunk into Its Elements and Reorganizing Them As a New Chunk\: The Two Different Sub-processes Underlying Insightful Chunk Decomposition:/Users/swu/Zotero/storage/G26ACHRR/full.html:text/html},
}

@misc{noauthor_reaction_nodate,
	title = {Reaction {Time} {Analysis} of {Two} {Types} of {Motor} {Preparation} for {Speech} {Articulation}: {Action} as a {Sequence} of {Chunks}: {Journal} of {Motor} {Behavior}: {Vol} 35, {No} 2},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00222890309602129},
	urldate = {2023-01-09},
}

@article{akam_simple_2015,
	title = {Simple {Plans} or {Sophisticated} {Habits}? {State}, {Transition} and {Learning} {Interactions} in the {Two}-{Step} {Task}},
	volume = {11},
	issn = {1553-7358},
	shorttitle = {Simple {Plans} or {Sophisticated} {Habits}?},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004648},
	doi = {10.1371/journal.pcbi.1004648},
	abstract = {The recently developed ‘two-step’ behavioural task promises to differentiate model-based from model-free reinforcement learning, while generating neurophysiologically-friendly decision datasets with parametric variation of decision variables. These desirable features have prompted its widespread adoption. Here, we analyse the interactions between a range of different strategies and the structure of transitions and outcomes in order to examine constraints on what can be learned from behavioural performance. The task involves a trade-off between the need for stochasticity, to allow strategies to be discriminated, and a need for determinism, so that it is worth subjects’ investment of effort to exploit the contingencies optimally. We show through simulation that under certain conditions model-free strategies can masquerade as being model-based. We first show that seemingly innocuous modifications to the task structure can induce correlations between action values at the start of the trial and the subsequent trial events in such a way that analysis based on comparing successive trials can lead to erroneous conclusions. We confirm the power of a suggested correction to the analysis that can alleviate this problem. We then consider model-free reinforcement learning strategies that exploit correlations between where rewards are obtained and which actions have high expected value. These generate behaviour that appears model-based under these, and also more sophisticated, analyses. Exploiting the full potential of the two-step task as a tool for behavioural neuroscience requires an understanding of these issues.},
	language = {en},
	number = {12},
	urldate = {2023-01-16},
	journal = {PLOS Computational Biology},
	author = {Akam, Thomas and Costa, Rui and Dayan, Peter},
	month = dec,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Learning, Agent-based modeling, Decision making, Habits, Optimization, Random walk, Regression analysis},
	pages = {e1004648},
	file = {Full Text PDF:/Users/swu/Zotero/storage/L3WFAS7S/Akam et al. - 2015 - Simple Plans or Sophisticated Habits State, Trans.pdf:application/pdf},
}

@article{gruber_dopamine_2006,
	title = {Dopamine modulation in the basal ganglia locks the gate to working memory},
	volume = {20},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-005-5705-x},
	doi = {10.1007/s10827-005-5705-x},
	abstract = {The prefrontal cortex and basal ganglia are deeply implicated in working memory. Both structures are subject to dopaminergic neuromodulation in a way that exerts a critical influence on the proper operation of working memory. We present a novel network model to elucidate the role of phasic dopamine in the interaction of these two structures in initiating and maintaining mnemonic activity. We argue that neuromodulation plays a critical role in protecting memories against both internal and external sources of noise. Increases in cortical gain engendered by prefrontal dopamine release help make memories robust against external distraction, but do not offer protection against internal noise accompanying recurrent cortical activity. Rather, the output of the basal ganglia provides the gating function of stabilization against noise and distraction by enhancing select memories through targeted disinhibition of cortex. Dopamine in the basal ganglia effectively locks this gate by influencing the stability of up and down states in the striatum. Dopamine’s involvement in affective processing endows this gating with specificity to motivational salience. We model a spatial working memory task and show that these combined effects of dopamine lead to superior performance.},
	language = {en},
	number = {2},
	urldate = {2023-01-16},
	journal = {Journal of Computational Neuroscience},
	author = {Gruber, Aaron J. and Dayan, Peter and Gutkin, Boris S. and Solla, Sara A.},
	month = apr,
	year = {2006},
	keywords = {Attention, Phasic release, Salience, Spiny neuron},
	pages = {153},
}

@article{nieh_geometry_2021,
	title = {Geometry of abstract learned knowledge in the hippocampus},
	volume = {595},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03652-7},
	doi = {10.1038/s41586-021-03652-7},
	abstract = {Hippocampal neurons encode physical variables1–7 such as space1 or auditory frequency6 in cognitive maps8. In addition, functional magnetic resonance imaging studies in humans have shown that the hippocampus can also encode more abstract, learned variables9–11. However, their integration into existing neural representations of physical variables12,13 is unknown. Here, using two-photon calcium imaging, we show that individual neurons in the dorsal hippocampus jointly encode accumulated evidence with spatial position in mice performing a decision-making task in virtual reality14–16. Nonlinear dimensionality reduction13 showed that population activity was well-described by approximately four to six latent variables, which suggests that neural activity is constrained to a low-dimensional manifold. Within this low-dimensional space, both physical and abstract variables were jointly mapped in an orderly manner, creating a geometric representation that we show is similar across mice. The existence of conjoined cognitive maps suggests that the hippocampus performs a general computation—the creation of task-specific low-dimensional manifolds that contain a geometric representation of learned knowledge.},
	language = {en},
	number = {7865},
	urldate = {2023-01-16},
	journal = {Nature},
	author = {Nieh, Edward H. and Schottdorf, Manuel and Freeman, Nicolas W. and Low, Ryan J. and Lewallen, Sam and Koay, Sue Ann and Pinto, Lucas and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W.},
	month = jul,
	year = {2021},
	note = {Number: 7865
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Neural circuits},
	pages = {80--84},
	file = {Accepted Version:/Users/swu/Zotero/storage/8KILJW5M/Nieh et al. - 2021 - Geometry of abstract learned knowledge in the hipp.pdf:application/pdf},
}

@article{higham_dissociations_nodate,
	title = {Dissociations of {Grammaticality} and {Specific} {Similarity} {Effects} in {Artificial} {Grammar} {Learning}},
	language = {en},
	author = {Higham, Philip A},
	file = {Higham - Dissociations of Grammaticality and Specific Simil.pdf:/Users/swu/Zotero/storage/NQS4SFA3/Higham - Dissociations of Grammaticality and Specific Simil.pdf:application/pdf},
}

