\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
    \centering
    \begin{tikzpicture}[
        box/.style={draw, rounded corners=15pt, minimum width=2cm, minimum height=1.25cm, 
            align=center, font=\scriptsize, inner sep=5pt},
        violetbox/.style={box, fill=violet!10, draw=violet!30},
        rightbox/.style={box, fill=orange!5, draw=orange!20},
        redbox/.style={box, fill=red!15, draw=red!50},
        arrow/.style={->, >=latex, thick, draw=gray!40, shorten >=2pt, shorten <=2pt}
    ]
        \begin{scope}[scale=0.8]
        
        \node[violetbox] (b) at (0,2) {
        $12$ NLP topics (Table~\ref{tab:research-topics})\\$\times$ $3$ = $36$ research proposals\\ generated from \citet{si2024can} \\
        + \\
        $4$ research proposals showcased\\in \citet{si2024can} \\
        + \\
        $10$ research papers showcased in\\ \citet{lu2024ai}   
        };
        
        \node[violetbox] (o1) at (5,2) {Total $50$\\ research documents};
        \node[rightbox] (o2) at (9.75,2) {Expert assessment: identify\\source papers and assign score\\ ($1$-$5$) (scores defined in Table~\ref{tab:similarity-scores})};
        \node[redbox] (o3) at (15.25,2) {Verification by source\\paper's author.\\24.0\% verified plagiarism\\ (similarity score $4+$),\\36.0\% with unverified claims};

        \node[inner sep=0] (search) at (7,4) {\includegraphics[width=1cm]{images/search_2717575.png}};
        
        \node[text width=3cm, font=\tiny, align=center] (plag-text) at (7,3) {$13$ experts looking\\for plagiarism};
        
        \draw[arrow] (b.east) -- (o1.west);
        
        \draw[arrow] (o1.east) -- (o2.west);
        \draw[arrow] (o2.east) -- (o3.west);
        
        \end{scope}
    \end{tikzpicture}
    \caption{Overview of our expert-led evaluation for detecting plagiarism in LLM-generated research proposals. Unlike prior work, participants in our study are instructed to actively search for potential sources of plagiarism.}
    \label{fig:expert-evaluation-setup}
\end{figure*}

Automating research and discovering new knowledge 
has been a longstanding aspiration. 
The first step of 
scientific research is 
coming up with a bold hypothesis or a conjecture \citep{popper2014conjectures}. 
Automating this step is a crucial component of automating scientific research.
Recent research presents a positive case of
LLMs' ability to 
generate novel scientific contributions---be they hypotheses, or proposals or papers \citep{li2024chain,lu2024ai, baek2024researchagent, li2024mlr, wang2023scimon, yang2023large, li2024learning, weng2024cycleresearcher}. 

Understandably, 
evaluating the novelty LLM-generated ideas 
is challenging, especially 
given the subjective nature of scientific innovation. 
Previous studies 
evaluate novelty either through automated LLM-based judges \citep{lu2024ai}, or rely on small set of 
experts \citep{li2024chain, baek2024researchagent, li2024mlr, wang2023scimon, yang2023large, li2024learning, weng2024cycleresearcher}. 
Notably, the most 
rigorous evaluation to date 
engaged  experts %
to evaluate $81$ LLM-generated research proposals, 
implementing strict controls for confounding factors \citep{si2024can}. 
Their study leads to an  
important finding: 
human experts find LLM-generated research 
proposals 
to be \emph{more novel} than human-written ones.
The study 
also publicly releases 
four 
exemplar LLM-generated proposals,
holding back others to be used for future work. 


In this work, we conduct an expert-led 
evaluation 
where participants 
are instead  
instructed to presume plagiarism and 
actively search for it in LLM-generated research documents. 
This situational logic \citep{popper2013poverty, hoover2016situational} contrasts 
with prior assessment of LLM generated ideas,
where experts evaluate a shuffled set 
of LLM and human-generated documents, 
presuming no deliberate plagiarism 
and scoring them on novelty and other factors, such as excitement and feasibility. 
In total, 
experts in our study evaluate 
$50$ LLM-generated research documents, 
including $10$ 
exemplar papers 
generated by the ``The AI Scientist''~\citep{lu2024ai}, 
four 
public research proposals from \citet{si2024can}, 
and $36$ fresh proposals.


We request 
the experts in our study 
to identify a topic of their expertise, 
and 
share with
them $5$ 
research proposals 
related to that topic. 
We 
generate these proposals through 
the code made available by \citet{si2024can}. 
The experts are then requested 
to score any $3$ of the 
$5$ research proposals 
on a  
scale of $1$-$5$, 
where $5$ indicates 
direct 
copying---that is, there exists 
a one-to-one mapping 
between the LLM proposed methodology
and existing methods 
in one or two closely related prior papers.
A score of $4$ 
denotes that 
a significant portion of the LLM proposed method
was borrowed from $2$-$3$ prior works without credit.
On the other extreme, 
a score of $1$ is reserved 
for cases when 
experts 
find the proposal to be completely original
(see Table~\ref{tab:similarity-scores}
for the rubric).



Our expert-led analysis reveals that $14.0\%$ of LLM-generated research documents 
receive a score of $5$ using the rubric in Table~\ref{tab:similarity-scores}, and another $10.0\%$ 
documents receive a score of $4$. 
We consider both scores $4$ and $5$ as instances of plagiarism, 
totaling $24.0\%$ of proposals with noticeable plagiarism. 
These scores are verified by emailing the authors of referenced papers. 
When including claims where source paper authors are unreachable, 
these numbers increase: $18.0\%$ of proposals receive a score of $5$ and $18.0\%$ receive a score of $4$, 
amounting to $36.0\%$ of the examined proposals.
Notably, several
previously 
showcased exemplars of LLM generated research \citep{si2024can,lu2024ai}
are found to be plagiarized.
Our experimental setup 
and key findings 
are illustrated in Figure~\ref{fig:expert-evaluation-setup}.


It is important to note 
that the examined documents
do not 
acknowledge the original sources
and the high degree of 
similarity with past 
work 
is not caught by 
inbuilt plagiarism detection systems.
Typically,  
these inbuilt plagiarism detectors 
use large language models,  
which have access 
to the Semantic Scholar API 
to retrieve similar papers
\citep{si2024can, lu2024ai, li2024chain}.\footnote{We refer to this approach as Semantic Scholar Augmented Generation (SSAG) throughout the paper.}
To systematically evaluate automated detection methods, 
we create a synthetic dataset of 
research proposals intentionally plagiarized from existing papers.
Using this controlled dataset, 
we evaluate common automated detection methods, 
including SSAG, OpenScholar (embedding-based search) \citep{asai2024openscholar}, and a
commercial service \citep{turnitin}, and find them inadequate for detecting plagiarism in LLM-generated research proposals.




\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{@{}cl@{}}
        \toprule
        \textbf{Score} & \textbf{Description} \\
        \midrule \vspace{1.5mm}
        5 & \begin{tabular}[c]{@{}l@{}}\textbf{Direct Copy:} One-to-one mapping between the LLM proposed methodology \\ and existing methods in one or two closely related prior papers. \end{tabular} \\ \vspace{1.5mm}
        4 & \begin{tabular}[c]{@{}l@{}}\textbf{Combined Borrowing:} A significant portion of LLM proposed method is a \\ mix-and-match  from two-to-three prior works. \end{tabular} \\ \vspace{1.5mm}
        3 & \begin{tabular}[c]{@{}l@{}}\textbf{Partial Overlap:} The LLM proposed method bears decent similarity with some existing \\ methods, but there's no exact correspondence with a limited set of papers. \end{tabular} \\ \vspace{1.5mm}
        2 & \begin{tabular}[c]{@{}l@{}}\textbf{Minor Similarity:} The LLM proposal bears very slight resemblance  with some existing \\ papers. Mostly novel. \end{tabular} \\ \vspace{1.5mm}
        1 & \begin{tabular}[c]{@{}l@{}}\textbf{Original:} The LLM proposal is completely novel. \end{tabular} \\
        \bottomrule
    \end{tabular}
    \caption{Scoring rubric shared with experts to evaluate similarity of LLM-generated research proposals with prior work.}
    \label{tab:similarity-scores}
\end{table*}

Our analysis reveal a concerning pattern
wherein a significant portion of LLM-generated research ideas
appear novel on the surface
but are actually skillfully plagiarized
in ways that make their originality difficult to verify.
The case study presented in \S\ref{subsec:case-study} supports this thesis, examining a research proposal (an exemplar in \citep{si2024can}) that appears to be skillfully plagiarized from an existing paper.
Our analysis in \S\ref{sec:discussion} also 
finds that LLM-generated research content 
is less diverse 
and 
follows more predictable patterns. 
Our preliminary investigation suggests these patterns might be detectable through basic classification methods, potentially helpful in flagging content for additional review, though more research is needed to develop robust detection approaches.


While we do not recommend wholesale dismissal of LLM-generated research,
our findings suggest that they  
may not be as novel as 
previously thought, and additional scrutiny is warranted.
The sophisticated nature of the plagiarism
we uncover suggests 
that widespread adoption of these tools 
could significantly impact the peer review process, 
requiring (already overwhelmed) reviewers 
to spend additional time searching 
for potential content misappropriation. 



