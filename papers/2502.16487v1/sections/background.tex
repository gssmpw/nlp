\section{Background: Generating Proposals}
\label{sec:background}


Here, we elaborate on the methodology
used to generate research proposals \citep{si2024can},
with particular attention to the plagiarism detection module, 
as understanding it is central to our discussion.

The research proposal generation process consists of six sequential steps,
with Claude $3.5$ Sonnet \citep{claude35sonnet} being the backbone LLM. 
For a given topic, the first 
step uses a retrieval-augmented generation (RAG) system 
to retrieve and rank 
relevant papers 
using the Semantic Scholar API. %
The second step generates initial seed ideas 
using these retrieved papers, 
while the third step 
involves 
deduplication using text embeddings, retaining about $5\%$ of the original ideas. 
The fourth step expands these seed ideas into detailed project proposals, 
and the fifth step implements a Swiss tournament system 
where proposals compete in pairwise comparisons over five rounds 
to identify the strongest candidates. 
We refer readers to \cite{si2024can} for additional details on these steps.

The final step, 
most relevant to our work, 
attempts 
to detect potential plagiarism through Semantic Scholar Augmented Generation (SSAG). 
First, an LLM iteratively 
generates queries 
for the Semantic Scholar API 
to find papers similar to the proposal's content,
using results from previous iterations as context to inform each new query.
This search process continues 
either until they collect $100$ papers 
or reach $10$ iterations, 
whichever comes first. 
The retrieved papers are then scored by an LLM 
based on their relevance, 
narrowing down to $10$ most similar papers. 
Further, Claude $3.5$ Sonnet 
performs pairwise comparisons 
between the proposal and each of these top $10$ papers. 
The LLM is prompted to 
determine if the research proposal and the retrieved paper are substantially similar, 
discarding the proposal if they are. 
This process removes \emph{only} about $1\%$ of the generated proposals.

Similar 
detection approaches
have been implemented in other research agents \citep{li2024chain,lu2024ai}, 
with minor variations in prompting strategies 
and parameters---\citet{li2024chain} and \citet{lu2024ai} query Semantic Scholar $3$ and $10$ times to search for similar papers, respectively. 

For our study's implementation, \citet{si2024can}'s method is slightly modified to optimize computational costs while maintaining effectiveness. 
Instead of generating $4000$ ideas per research topic, 
we generate $500$ ideas per topic. 
This reduces our initial pool from around $200$ unique ideas per topic (as in the original paper \citep{si2024can}) 
to $138$, 
resulting in significant computational cost savings 
while only reducing unique ideas by $31\%$. 
All other components of the pipeline, 
including the plagiarism detection and ranking systems, 
are exactly the same as \citet{si2024can}'s original implementation.

The choice to use this specific methodology \citep{si2024can}
 for generating research proposals is motivated by several factors. 
 First, it is representative of fundamental prompt engineering techniques 
used across various research idea generation systems \citep{li2024chain, lu2024ai, baek2024researchagent, li2024mlr, wang2023scimon, yang2023large, weng2024cycleresearcher}. 
Second, as discussed in \S\ref{sec:related}, 
this study conducts the most comprehensive evaluation to date, 
and finds that human participants judge LLM-generated research 
more 
novel than those 
by humans. 
Third, this method uses minimal prompt engineering. Other approaches like \citet{li2024chain} use sophisticated prompt engineering that function as ``natural language programs,'' potentially incorporating human creativity. By keeping prompts simple, we can better assess the raw capabilities of LLMs themselves.
