\section{Experimental Design}
\label{sec:experiments}

We present our expert evaluation design for detecting plagiarism in LLM-generated research documents and our evaluation methodology for automated plagiarism detection tools.

\subsection{Expert Evaluation Design}
\label{subsec:expert-evaluation-design}

Our experimental setup involves generating 
five research proposals for each of the twelve topics listed in Table~\ref{tab:research-topics}. 
These NLP topics are determined 
by asking participants 
to describe their areas of expertise, 
ensuring evaluators have  
in-depth familiarity with 
the current literature for detecting potential plagiarism. 
Each participant evaluates three out of five proposals that aligns with their domain knowledge, resulting in $3\times12=36$ total proposals.

In addition to these $36$ proposals, 
we include fourteen previously-showcased exemplars---four research proposals from \citet{si2024can} 
and ten research papers from \citet{lu2024ai}, 
bringing the total to $50$ research documents. 
These fourteen exemplars were evaluated by two experts. 
We use convenience sampling 
to recruit participants who are actively conducting research in NLP 
through our professional networks. 
Our participant pool comprises experts from $5$ universities
and $2$ industrial labs,
with $69\%$ being Ph.D. students or recent graduates
and $31\%$ being associate researchers in industrial or academic labs.\footnote{Our experts are affiliated with University of Washington, Carnegie Mellon University, Harvard University, University of Southern California, Indian Institute of Science, Allen Institute for Artificial Intelligence (Ai2)
and Adobe, Inc.}
Unlike prior studies 
where participants assess LLM-generated research documents 
without suspecting plagiarism, 
our participants are instructed to actively search 
for potential overlaps 
with existing work.

Participants are asked to only 
consider papers available until April $2024$, 
corresponding to Claude $3.5$ Sonnet's training cutoff date.
Using a consistent evaluation rubric (see Table~\ref{tab:similarity-scores}), participants assign similarity scores from $1$-$5$ to each proposal. We focus on documents scoring $4$ or $5$, as these represent clear cases of content misappropriation---score $5$ indicates direct copying with one-to-one mapping to existing methods, while score $4$ indicates significant borrowing from prior work without attribution.
 These scores reflect methodological overlap considered plagiarism in academic publishing, 
 as they represent either wholesale adoption of existing methods (score $5$) 
 or substantial uncredited incorporation of others' technical contributions (score $4$). 
 Lower scores represent more ambiguous cases of potential similarity. 
 For all documents with scores $4$ and $5$, 
 we email source paper authors for verification 
 and adjust scores based on their feedback. 
 Since some authors were unreachable, we report both verified claims and total claims separately.

 While conventional human studies 
 aim for objectivity 
 through unbiased instructions 
 and experimental design, 
 our expert evaluation design 
 derives objectivity primarily 
 from author verifications and 
 the inherently verifiable nature 
 of plagiarism claims---readers 
 can independently examine both the source 
 and generated works through our open-sourced results. 
 The instructions shared with participants are shown in Table~\ref{tab:expert-instructions}. We discuss some limitations of our expert evaluation design in \S\ref{para:lim-expert-evaluation}.




\subsection{Evaluating Plagiarism Detectors}
\label{subsec:automated-plag-detection-exp}

To systematically evaluate automated plagiarism detection tools, 
we require a dataset of plagiarized research documents paired with their original source papers. 
While our expert evaluation study uncovers instances of plagiarism, 
the sample size is too small to comprehensively test automated tools. 
We therefore create a synthetic test set by 
generating plagiarized research proposals from papers 
retrieved during the literature review step of the baseline method \citep{si2024can}. 

For the twelve research topics listed in Table~\ref{tab:research-topics}, 
we select $40$ papers per topic, 
creating a test set of $480$ papers.
We then use GPT-4o~\citep{openai2024gpt4ocard} 
to generate plagiarized versions of these papers 
by skillfully paraphrasing the paper's details
to avoid detection.
We choose GPT-4o for this task because Claude $3.5$ Sonnet abstains from plagiarism requests. 
The specific prompt used for this process is detailed in Table~\ref{tab:generation-prompt}.

This synthetic testing 
approach poses a significantly easier challenge 
than detecting plagiarism in proposals 
generated by sophisticated research agents. 
Since we explicitly instruct GPT-4o 
to plagiarize from a single paper,
novel elements in these deliberately plagiarized proposals 
are likely more limited 
than those produced by systems designed to generate novel research. 
Therefore, the performance of automated detection systems on our test set 
likely overestimates their ability to detect more subtle forms of plagiarism 
in LLM systems designed to generate novel research content.

Successful plagiarism detection 
requires two steps: 
first retrieving potentially plagiarized source papers, 
and then determining whether the retrieved papers 
are substantially similar to the proposal in question. 
We design experiments to evaluate these 
components both separately and together across three approaches. 
The first approach uses two LLMs (GPT-4o and Claude 3.5 Sonnet) 
in three distinct scenarios: 
(a) oracle access, wherein we provide an LLM with 
both the proposal and its source paper
and evaluate its ability to detect plagiarism,
(b) parametric knowledge testing, 
which examines LLMs' ability to both retrieve and determine 
similarity using only their training data without external tools, 
and (c) Semantic Scholar Augmented Generation (SSAG), described in Section~\ref{sec:background}, 
which explicitly 
separates these steps by first retrieving papers through Semantic Scholar 
and then determining similarity.\footnote{For SSAG, we use the following hyperparameters: maximum $50$ papers and $5$ iterations.}


The other two approaches, described in \S\ref{para:rel-automated-plag-tools}, are OpenScholar \citep{asai2024openscholar} (prompt in Table~\ref{tab:openscholar-prompt}), an academic search system with a database of $45$ million papers and sophisticated retrieval mechanisms, and Turnitin \cite{turnitin}, a widely-used commercial plagiarism detection service. Due to the manual effort required for submission and analysis, we limit our testing with these tools to $3$ papers per research topic, totalling $36$.
