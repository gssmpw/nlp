\section{Limitations} 
\label{sec:limitations}

\paragraph{Automation Challenges.}
A key limitation identified in our study stems 
from the challenge of automating the 
detection of original papers that may have been plagiarized. 
Currently, no reliable automated systems exist for this task, 
necessitating reliance on human expertise. 
This manual process is time-intensive, making it a critical bottleneck.


\paragraph{Constraints Related to Expert Evaluation.} \label{para:lim-expert-evaluation}
Our expert evaluation design has some limitations. 
First, we ask our human participants to presume adversarial plagiarism and actively search for it. 
This may introduce confirmation bias, 
leading them to give higher similarity scores in order to complete their task. 
We indeed find $4$ instances 
where 
authors of source-papers over-turned the score downwards 
with difference of $2$ or more. 
However, our (author-verified) scores already account for these adjustments, 
and our findings remain independently verifiable 
through our open-sourced results. 
Second, we provide our human-participants with $5$ research proposals and ask them to choose any $3$. 
While this gives flexibility to our participants, 
since even in a single topic there can be sub-topics that participants may not be familiar with, 
it may introduce sampling bias. 

\paragraph{Computational Parameter Reductions.} 
We reduce the quantity of certain hyperparameters to optimize computational costs. 
First, as detailed and motivated in \S\ref{sec:background}, 
we decrease the number of candidate proposals generated per topic, 
resulting in approximately $31\%$ fewer unique ideas compared to the baseline method \citep{si2024can}. 
Although this reduction might marginally affect the quality of research proposals relative to \citet{si2024can}, 
the validity of our findings remains, 
particularly given that we identify plagiarism even in the LLM-generated proposals showcased in their original work. 
Second, in our automated plagiarism detection experiments (\S\ref{subsec:automated-plag-detection-exp}), 
we limit Semantic Scholar queries to a maximum of $5$ iterations, 
fewer than the $10$ iterations employed in \citet{si2024can,lu2024ai}, but more than the $3$ iterations in \citet{li2024chain}. 
However, as elaborated in \S\ref{subsec:automated-plag-detection-exp}, 
our synthetic dataset presents a simpler challenge than detecting plagiarism in actual research proposal generation pipelines
(note that for generating proposals in our expert evaluation study, we maintain the same number of Semantic Scholar queries as \citet{si2024can} in the plagiarism filtering step).

\paragraph{Title-only Analysis.} 
Our clustering 
and classification experiments (\S\ref{para:titles-clustering}) 
rely solely on paper titles as proxies for broad research directions and topics, 
rather than using full text.
While this approach may seem limited, 
we deliberately avoid using complete documents because LLM-generated proposals 
and human-written papers follow different organizational structures. 
These structural differences would confound any full-text analysis, 
making titles the most comparable unit for exploring patterns in research directions. 
These two experiments should be viewed as preliminary explorations rather than methodological recommendations.

Despite these limitations, 
we believe that our study highlights a critical concern, 
and adds nuance to the 
ongoing discourse about the role of LLMs in scientific research.
