\section{Discussion}
\label{sec:discussion}

\paragraph{Implications for Academic Publishing.} 
Presence of 
considerable plagiarism 
in LLM-generated 
research documents suggests 
that widespread adoption of these 
tools could lead to an increase in 
publications 
with improper citations or inadvertent plagiarism. 
Furthermore, 
if researchers execute LLM-generated proposals 
and submit their work to conferences and journals, 
the sophisticated nature of this plagiarism 
would require conference and journal reviewers 
to spend considerably more time searching for potential content misappropriation, 
adding to an already heavy reviewer workload.

\paragraph{Citing Relevant Work Is Challenging.} 
One might argue that 
requesting LLMs 
to provide citations while generating research documents could mitigate plagiarism concerns. 
In our preliminary experiments, 
we manually examine several cases 
where we ask LLMs to generate proposals with citations. 
However, 
the LLMs typically reference a few well-known papers, 
raising concerns about citation accuracy. 
A comprehensive evaluation of 
citation quality 
would require expert verification of each citation and, 
more importantly, 
identification of relevant citations that were omitted, 
making large-scale testing expensive. 
This limitation aligns with broader findings in the field---\citet{asai2024openscholar} 
find that $78$-$90\%$ of papers cited by non-retrieval augmented LLMs are hallucinated. 
While recent works \citep{asai2024openscholar, Gao2023EnablingLL, Qian2024OnTC} attempt to improve citation generation, 
their findings suggest that LLMs struggle with accurate citations, 
leading researchers to rely on RAG 
and other external tools for citation support.

\begin{figure}
    \centering
    \includegraphics[width=0.475\textwidth]{images/ai_assisted_proof_pca_scatter_plot.pdf}
    \caption{PCA projection of title embeddings for human-written papers and LLM-generated proposals on the topic ``Novel AI-assisted formal proof generation methods''. LLM-generated proposals occupy a more confined region, indicating less diversity in outputs.}
    \label{fig:scatter}
\end{figure}

\paragraph{Limited Diversity in Proposals.}
\label{para:titles-clustering} 
Prior research notes limited diversity in LLM-generated research proposals \citep{si2024can}, 
which our analysis confirms. 
For each research topic in Table~\ref{tab:research-topics}, 
we analyze titles of $100$ human-written papers and $100$ LLM-generated proposals.\footnote{Due to deduplication and filtering in \citep{si2024can}, some topics had $< 100$ proposals, resulting in $2370$ samples rather than the maximum of $2400$ ($200$ samples $\times$ $12$ topics).\label{footnote:less-samples}}\ 
Using the all-MiniLM-L6-v2 sentence transformer \citep{reimers2020making}, 
we generate embeddings for these titles.
Calculating cluster spread via mean squared distance from centroids, 
we find that despite aggressive deduplication strategies \citep{si2024can}, 
LLM-generated titles are more tightly clustered---the ratio of LLM to human cluster 
spreads averages $0.76$ across topics. 
Figure~\ref{fig:scatter} illustrates this pattern, 
showing that LLM proposals occupy a more confined region in the embedding space. 
While this analysis is limited since titles only reflect broad research directions rather than specific ideas,
the clear clustering pattern suggests current LLM systems explore a narrower range of research directions compared to humans, 
undermining their utility for scientific innovation.


\paragraph{Proposal Titles Are Easily Distinguishable.}
\label{para:titles-classification} 
Beyond being tightly clustered, 
LLM-generated proposals also appear to follow distinctive patterns in their research directions. 
In a preliminary analysis using $100$ human-written papers
and $100$ LLM-generated proposals per topic from Table~\ref{tab:research-topics}, 
a simple logistic regression model with bag-of-words features on a $60$:$40$ train-test split 
achieves $94.2\%$ accuracy in classifying titles.\footref{footnote:less-samples}\ 
While this basic classification experiment has limitations as titles only capture broad research directions, 
it provides initial evidence that LLM-generated research follows predictable patterns. 
However, developing reliable detection methods will require significant further research 
and is beyond the scope of this work.

\paragraph{Future Research Directions.} 
Developing ways 
to identify 
candidate source papers 
is one of the most pressing future research direction, 
as current automated detection methods are inadequate 
and manual evaluation by domain experts is both time-consuming and laborious.
Future work could also explore post-training strategies that could help reduce plagiarism in LLM-generated research content.
Lastly, 
future studies could examine 
whether LLM-generated 
content 
directly copy, or significantly borrow,
content from copyrighted materials, possibly through the lens of fair-use doctrine \citep{patterson1992understanding, balaji2024fair}.
