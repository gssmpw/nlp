\section{Reinforcement Learning from Human Feedback}
\label{sec:appendixa}
\subsection{Bradley-Terry model}
\label{sec:btloss}
Then sampling samples from  $p_{\text{BT}}^*$, one can construct a dataset $\mathcal{D}=\{(x_{i}, y_{\tau_{i}(1)}, y_{\tau_{i}(2)})\}_{i=1}^m$, where each instance consists of one prompt $x_{i}$ and $2$ responses $y_{\tau_{i}(1)}, y_{\tau_{i}(2)}$ followed the user-specified ranking. Using this dataset, we can train a reward model $r_{\wm}$ parameterized by $\wm$ by approaching the task as a classification problem. Specifically, we frame the training objective as minimizing the negative log-likelihood loss:
\begin{equation}
\begin{aligned}
\label{eq:BT_loss}
\mathcal{L}_{\text{BT}} & = -\expectation_{(x_{i}, y_{\tau_{i}(1)}, y_{\tau_{i}(2)})\sim\mathcal{D}}\left[\log\sigma\left(r_{\wm}(x_{i}, y_{\tau_{i}(1)})- r_{\wm}(x_{i}, y_{\tau_{i}(2)})\right)\right] \\
& =-\sum_{i=1}^m  \log \sigma\left(r_{\wm}(x_{i}, y_{\tau_{i}(1)})- r_{\wm}(x_{i}, y_{\tau_{i}(2)})\right).
\end{aligned}
\end{equation}

\subsection{Reward Modelling}
\label{sec:rm}
\subsubsection{KTO}
KTO~\cite{kto} defines a type of reward to construct human-aware losses (HALOs), which is in the form 
\begin{equation}
    r_{\text{KTO}}(x, y) = l(y) \log \frac{\pi_{\wm}(y|x)}{\pi_{\text{ref}}(y|x)},
\end{equation}
where \( \wm \) denotes the trainable parameters of the model \( \pi_\wm \) being aligned, \( \pi_{\text{ref}} \) is the reference model, and \( l : \mathcal{Y} \to \mathbb{R}^+ \) is a normalizing factor.

\subsubsection{SimPO}
SimPO~\cite{simpo} identifies the discrepancy between DPOâ€™s reward and the likelihood metric used for generation, and proposes an alternative reference-free reward training loss:
	\begin{equation}
		r_{\text{SimPO}}(x,y)=\frac{\beta}{|y|}\log \pi_{\wm}(y|x)=\frac{\beta}{|y|}\sum\limits^{|y|}_{i=1}\log \pi_{\wm}(y_{i}|x,y_{<i}),
	\end{equation}
where $|y|$ is the length of the response $y$,  and $y_{<i}$ is the set of tokens in the sentence $y$ before the token $y_{i}$.

\section{Theoretical Analysis}
\label{sec:proof}
\subsection{Sample Efficiency Analysis}
\label{sec:proof1}
\begin{theorem*}
Let \(\mathcal{D}\) be a given dataset. Under certain regularity conditions, the maximum likelihood estimators \(\hat{\wm}_{\text{HDR}}\) and \(\hat{\wm}_{\text{PL}}\), corresponding to the the hard sampling loss $\mathcal{L}_{\wm}$ and \text{PL} loss $\mathcal{L}_{\text{PL}}$, respectively, satisfies the following with probability at least $1-\delta$:
\begin{equation*}
\begin{aligned}
\|\wm_{\text{HPS}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}\leq C_{1}\cdot\sqrt{\frac{d+\log\left(\frac{1}{\delta}\right)}{m\zeta^{2}\left(N\right)}}-\frac{16\alpha_{1}^{2}\zeta(N)-4\alpha_{2}}{m\cdot\zeta(N)}=\mathcal{O}\left(\frac{n}{\sqrt{m}}\right)
\end{aligned}
\end{equation*}
and
\begin{equation*}
\begin{aligned}
&\|\wmi{\text{PL}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}\leq C_{2}\cdot\sqrt{\frac{ n^{4}e^{8\alpha_{0}}\cdot\left(d+\log\left(\frac{1}{\delta}\right)\right)}{m}}=\mathcal{O}\left(\frac{n^{2}}{\sqrt{m}}\right),
\end{aligned}
\end{equation*}
where
\begin{equation*}
\begin{aligned}
&\Sigma_{\mathcal{D}}=\frac{2}{mn(n-1)}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}\sum\limits_{k=j+1}^{n}\left(\nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( j\right)}\right)-\nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( k\right)}\right)\right)\left(\nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( j\right)}\right)-\nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( k\right)}\right)\right)^{T}
\end{aligned}
\end{equation*}
and
\begin{equation*}
\zeta\left(N\right)=\frac{1}{2+\exp\left(2\alpha_{0}+\ln(N)\right)+\exp\left(-2\alpha_{0}\right)}.
\end{equation*}

Therefore, the error bound of $\hat\wm_{\text{HDR}}$ is tighter than that of $\wmi{\text{PL}}$, \textit{i.e.}, $\|\hat\wm_{\text{HDR}}-\wm^{*}\|_{\Sigma_{\mathcal{D}}}\leq \|\wmi{\text{PL}}-\wm^{*}\|_{\Sigma_{\mathcal{D}}}$. In other words, \(\hat{\wm}_{\text{HDR}}\) is more efficient than \(\hat{\wm}_{\text{PL}}\).
\end{theorem*}

\begin{proof}
We begin by proving \Cref{thm:estimator}.
\paragraph{Analysis on $\mathcal{L}_{\wm}$}
We first analyze the asymptotic efficiency and estimation error of estimator induced by $\mathcal{L}_{\wm}$.
We consider the general RLHF setting in a dataset $\mathcal{D}$ with $m$ sample: 
\begin{equation*}
    \mathcal{L}_{\wm}= -\frac{1}{m}\sum^{m}_{i=1}\log\left(\frac{e^{{ r_{\wm}(x_{i},y_{\tau_{i}\left( 1\right)})}}}{e^{{ r_{\wm}(x_{i},y_{\tau_{i}\left( 1\right)})}}+ N\cdot\expectation\nolimits_{y_{i} \sim q(x,y)}\left[e^{{r_{\wm}(x,y_{i})}}\right]}\right)
\end{equation*}
The maximum likelihood estimator (MLE) $\wm_{\text{HPS}}$ aims at minimizing the negative log likelihood, defined as: 
\begin{equation*}
\wm_{\text{HPS}} \in \arg\min\limits_{\wm\in\wm_{B}}\mathcal{L}_{\wm}.
\end{equation*}
When the minimizer is not unique, we take any of the $\wm_{\text{HPS}}$ achieve the minimum.

To simplify the notation, for a fixed sampling method $q$, we let $g^{i}_{\wm}=r_{\wm}\left(x_{i},y_{\tau_{i}{\left(1\right)}}\right)-\ln\left(N\right)-\expectation\limits_{y_{i}\sim q}\left[{r_{\wm}(x,y_{i})}\right]$.
We can see that the gradient of $\mathcal{L}_{\wm}$ takes the form:
\begin{equation*}
\begin{aligned}
&\nabla \mathcal{L}_{\wm}(\wm)=-\frac{1}{m}\sum\limits^{m}_{i=1}\log\left[\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})=1\right]\frac{\exp\left(-g^{i}_{\wm}\right)}{1+\exp\left(-g^{i}_{\wm}\right)}-\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})\neq1\right]\frac{1}{1+\exp\left(-g^{i}_{\wm}\right)}\right]\nabla g^{i}_{\wm}.
\end{aligned}
\end{equation*}
And the Hessian of $\mathcal{L}_{\wm}$ is
\begin{equation*}
\begin{aligned}
\nabla^{2} \mathcal{L}_{\wm}(\wm) &= \frac{1}{m}\sum\limits^{m}_{i=1}\left(\frac{\exp\left(g^{i}_{\wm}\right)}{\left(1+\exp\left(g^{i}_{\wm}\right)\right)^{2}}\cdot\nabla g^{i}_{\wm}\nabla {g^{i}_{\wm}}^{\text{T}} -\frac{\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})=1\right]\cdot\exp\left(-g^{i}_{\wm}\right)}{1+\exp\left(-g^{i}_{\wm}\right)}\cdot\nabla^{2} g^{i}_{\wm} \right. \\
&\left.+\frac{\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})\neq1\right]\cdot \exp\left(g^{i}_{\wm}\right)}{1+\exp\left(g^{i}_{\wm}\right)}\cdot\nabla^{2} g^{i}_{\wm}\right)
\end{aligned}
\end{equation*}

We  bound $\expectation\limits_{y_{i}\sim q}\left[{r_{\wm}(x,y_{i})}\right]$ using the assumption:
$-\alpha_{0}\leq\expectation\limits_{y_{i}\sim q}\left[{r_{\wm}(x,y_{i})}\right]\leq\alpha_{0}$.

Thus, \begin{equation*}
\frac{\exp\left(g^{i}_{\wm}\right)}{\left(1+\exp\left(g^{i}_{\wm}\right)\right)^{2}} \geq \zeta\left(N\right),
\end{equation*}
where $\zeta\left(N\right)=\frac{1}{2+\exp\left(2\alpha_{0}+\ln(N)\right)+\exp\left(-2\alpha_{0}\right)}$.

We say $\Sigma \succeq \Sigma'$ if $\Sigma-\Sigma'$ is positive semidefinite. Based on Assumption \ref{assum1}, we have
\begin{equation}
\nabla^{2} \mathcal{L}_{\wm}(\wm) \succeq \frac{1}{m}\sum\limits^{m}_{i=1}\left[\zeta\left(N\right)\nabla g^{i}_{\wm}\nabla {g^{i}_{\wm}}^{\text{T}}-2\alpha_{2}I\right].
\end{equation}

Based on the Lipschitz gradient assumption, we also know that $\|\nabla g^{i}_{\wm}-\nabla g^{i}_{\wm^{*}}\|_{2}\leq 4\alpha_{1}$. Let $u=\nabla g^{i}_{\wm}-\nabla g^{i}_{\wm^{*}}$, we have:
\begin{equation*}
\begin{aligned}
&\nabla^{2} \mathcal{L}_{\wm}(\wm) \succeq \frac{1}{m}\sum\limits^{m}_{i=1}\zeta\left(N\right)\left(\nabla g^{i}_{\wm^{*}}+u\right)\left(\nabla {g^{i}_{\wm^{*}}}^{\text{T}}+u\right)-2\alpha_{2}I \\
& \succeq \frac{1}{m}\sum\limits^{m}_{i=1}\zeta\left(N\right)\nabla g^{i}_{\wm^{*}}\nabla {g^{i}_{\wm^{*}}}^{T}+\zeta\left(N\right)\left(\nabla g^{i}_{\wm^{*}}u^{T}+u\nabla {g^{i}_{\wm^{*}}}^{T}\right)-2\alpha_{2}I
\end{aligned}
\end{equation*}
Using the Cauchy's Inequality, for arbitrary $v\in\mathbb{R}^{d}$, $u^{T}v\leq\|u\|_{2}\|v\|_{2}\leq4\alpha_{1}\|v\|_{2}$, $v^{T}\nabla g^{i}_{\wm^{*}}\leq \alpha_{1}\|v\|_{2}$, where $\|x\|_{2} = \sqrt{\sum_{i=1}^{n} x^{\left(i\right)^{2}}}$, this gives that:
\begin{equation*}
\begin{aligned}
&v^{T}\nabla^{2} \mathcal{L}_{\wm}(\wm)v\geq\frac{\zeta\left(N\right)}{m}\|Xv\|_{2}^{2}+\frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}\|v\|_{2}^{2},
\end{aligned}
\end{equation*}
where $X\in\mathbb{R}^{m\times d}$ has the vector $\nabla g^{i}_{\wm^{*}}\in \mathbb{R}^{d}$ as its $i^{th}$ row.

Thus, if we introduce the error vector $\Delta:=\wm_{\text{HPS}}-\wm^{*}$, then we may conclude that:
\begin{equation*}
\begin{aligned}
&\mathcal{L}_{\wm}(\wm^{*}+\Delta)-\mathcal{L}_{\wm}(\wm^{*})-\left\langle\nabla\mathcal{L}_{\wm}(\wm^{*}), \Delta\right\rangle \\
&\geq \frac{\zeta\left(N\right)}{m}\|X\Delta\|_{2}^{2}+\frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}\|\Delta\|_{2}^{2} \\
& \geq \zeta\left(N\right)\|\Delta\|_{{\Sigma}_{\mathcal{D}}}^{2}+\frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}\|\Delta\|_{2}^{2}.
\end{aligned}
\end{equation*}

Now we aim at bounding the estimation error $\|\wm_{\text{HPS}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}$. Since $\wm_{\text{HPS}}$ is optimal for $\mathcal{L}_{\wm}$, we have $\mathcal{L}_{\wm}(\wm_{\text{HPS}})\leq\mathcal{L}_{\wm}(\wm^{*})$. Defining the error vector $\Delta:=\wm_{\text{HPS}}-\wm^{*}$, adding and subtracting the quantity $\left\langle\nabla\mathcal{L}(\wm^{*}), \Delta\right\rangle $ yields the bound:
\begin{equation*}
\mathcal{L}(\wm^{*}+\Delta)-\mathcal{L}(\wm^{*})-\left\langle\nabla\mathcal{L}(\wm^{*}), \Delta\right\rangle \leq -\left\langle\nabla\mathcal{L}(\wm^{*}), \Delta\right\rangle.
\end{equation*}
We know the left-hand side is lower bounded by:
\begin{equation*}
\zeta\left(N\right)\|\Delta\|_{{\Sigma}_{\mathcal{D}}}^{2}+\frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}\|\Delta\|_{2}^{2}.
\end{equation*}

As for the right-hand side, note that $\left|\left\langle\nabla\mathcal{L}(\wm^{*}), \Delta\right\rangle\right|\leq\left\|\nabla\mathcal{L}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}\left\|\Delta\right\|_{{\Sigma}_{\mathcal{D}}}$. 

Altogether we have:
\begin{equation*}
\zeta\left(N\right)\|\Delta\|_{{\Sigma}_{\mathcal{D}}}^{2}\leq\left\|\nabla\mathcal{L}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}\left\|\Delta\right\|_{{\Sigma}_{\mathcal{D}}}-\psi\|\Delta\|^{2}_{2},
\end{equation*}
where $\psi = \frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}$. Now we further bound the term $\left\|\nabla\mathcal{L}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}$. 
The gradient takes the form:
\begin{equation*}
\begin{aligned}
&\nabla \mathcal{L}_{\wm}(\wm^{*})=-\frac{1}{m}\sum\limits^{m}_{i=1}\left[\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})=1\right]\frac{\exp\left(-g^{i}_{\wm^{*}}\right)}{1+\exp\left(-g^{i}_{\wm^{*}}\right)}-\mathbf{1}\left[\tau^{-1}(x_{i}, y_{\tau_{i}{\left(1\right)}})\neq1\right]\frac{1}{1+\exp\left(-g^{i}_{\wm^{*}}\right)}\right]\nabla g^{i}_{\wm^{*}}.
\end{aligned}
\end{equation*}

Define a random vectors $V\in\mathbb{R}^{m}$ with independent components as
\begin{equation*}
V_{i} = \left\{
\begin{aligned}
& \frac{\exp\left(-g^{i}_{\wm^{*}}\right)}{1+\exp\left(-g^{i}_{\wm^{*}}\right)} \quad \text{w.p.} \quad \frac{1}{1+\exp\left(-g^{i}_{\wm^{*}}\right)}, 
\\
& \frac{-1}{1+\exp\left(-g^{i}_{\wm^{*}}\right)} \quad \text{w.p.} \quad \frac{\exp\left(-g^{i}_{\wm^{*}}\right)}{1+\exp\left(-g^{i}_{\wm^{*}}\right)}.
\end{aligned}
\right.
\end{equation*}
With this notation, we have $\nabla \mathcal{L}_{\wm}(\wm^{*})=-\frac{1}{m}X^{T}V$ with $\expectation[V]=0$ and $|V_{i}|\leq1$.
Defining the $m$-dimensional square matrix $M:=\frac{1}{m^{2}}X{\Sigma}^{-1}_{\mathcal{D}}X^{T}$, we have $\|\nabla\mathcal{L}_{\wm}(\wm^{*})\|^{2}_{{\Sigma}^{-1}_{\mathcal{D}}}= V^{T}MV$. Let the eigenvalue decomposition of $X^{T}X$ be $X^{T}X=U\Lambda U^{T}$. We can bound the trace and operator norm of $M$ as:
\begin{equation*}
\begin{aligned}
&\text{Tr}(M)=\frac{1}{m^{2}}\text{Tr}\left(U\left(\frac{\Lambda}{m}\right)^{-1}U^{T}U\Lambda U^{T}\right)\leq\frac{d}{m} \\
&\text{Tr}(M^{2})=\frac{1}{m^{4}}\text{Tr}\left(U\left(\frac{\Lambda}{m}\right)^{-1}U^{T}U\Lambda U^{T}U\left(\frac{\Lambda}{m}\right)^{-1}U^{T}U\Lambda U^{T}\right)\leq\frac{d}{m^{2}} \\
& \|M\|_{\text{op}}=\lambda_{\text{max}}(M)\leq\sqrt{\text{Tr}(M^{2})}=\frac{1}{m}
\end{aligned}
\end{equation*}
Moreover, since the components of $V$ are independent and of zero mean, and $|V_{i}|\leq 1$, the variables $V_{i}$ are $1$-sub-Gaussian, and hence the Bernsteinâ€™s inequality for sub-Gaussian random variables in quadratic form implies that with probability at least $1-\delta$,
\begin{equation*}
\begin{aligned}
&\left\|\nabla\mathcal{L}_{\wm}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}^{2}=V^{T}MV\leq C\cdot\frac{d+\log\left(\frac{1}{\delta}\right)}{m}.
\end{aligned}
\end{equation*}

Here $C$ is certain constant. This gives us
\begin{equation*}
\begin{aligned}
&\zeta\left(N\right)\|\Delta\|^{2}_{\Sigma_{\mathcal{D}}}\leq\|\nabla\mathcal{L}(\wm^{*})\|_{{\Sigma}^{-1}_{\mathcal{D}}}\|\Delta\|_{\Sigma_{\mathcal{D}}}-\psi\|\Delta\|^{2}_{2} \\
& \leq\sqrt{C\cdot\frac{d+\log\left(\frac{1}{\delta}\right)}{m}}\|\Delta\|_{\Sigma_{\mathcal{D}}}-2\psi\|\Delta\|_{\Sigma_{\mathcal{D}}},
\end{aligned}
\end{equation*}
where $\psi = \frac{8\alpha_{1}^{2}\zeta(N)-2\alpha_{2}}{m}$.

Solving the inequality above gives us for some constant $C_{1}$:
\begin{equation*}
\|\Delta\|_{\Sigma_{\mathcal{D}}}\leq C_{1}\cdot\sqrt{\frac{d+\log\left(\frac{1}{\delta}\right)}{m\zeta^{2}\left(N\right)}}-\frac{16\alpha_{1}^{2}\zeta(N)-4\alpha_{2}}{m\cdot\zeta(N)},
\end{equation*}
where $\zeta\left(N\right)=\frac{1}{2+\exp\left(2\alpha_{0}+\ln(N)\right)+\exp\left(-2\alpha_{0}\right)}$.
Thus, we can derive that with probability at least $1-\delta$:
\begin{equation*}
\|\wm_{\text{HPS}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}\leq C_{1}\cdot\sqrt{\frac{d+\log\left(\frac{1}{\delta}\right)}{m\zeta^{2}\left(N\right)}}-\frac{16\alpha_{1}^{2}\zeta(N)-4\alpha_{2}}{m\cdot\zeta(N)}=\mathcal{O}\left(\frac{n}{\sqrt{m}}\right).
\end{equation*}

\paragraph{Analysis on $\mathcal{L}_{\text{PL}}$}
We first analyze the asymptotic efficiency and estimation error of estimator induced by $\mathcal{L}_{\text{PL}}$.
We consider the general RLHF setting in a dataset $\mathcal{D}$ with $m$ sample: 
\begin{equation*}
\begin{aligned}
\mathcal{L}_{\text{PL}}&=\frac{1}{m}\sum\limits^{m}_{i=1}\sum\limits^{n}_{j=1}\mathcal{L}_{j}(\wm) \\
&=-\frac{1}{m}\sum\limits^{m}_{i=1}\sum\limits^{n}_{j=1}\log\!\Big({e^{r_{\wm} (x_{i},y_{\tau_{i}(j)})} / \sum\limits_{k=j}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k)})})}\Big).
\end{aligned}
\end{equation*}
The maximum likelihood estimator (MLE) $\wmi{\text{PL}}$ aims at minimizing the negative log likelihood, defined as: 
\begin{equation*}
\wmi{\text{PL}} \in \arg\min\limits_{\wm\in\wm_{B}}\mathcal{L}_{\text{PL}}.
\end{equation*}
When the minimizer is not unique, we take any of the $\wmi{\text{PL}}$ achieve the minimum.
We can see that the gradient of $\mathcal{L}_{\text{PL}}$ takes the form:
\begin{equation*}
    \nabla \mathcal{L}_{\text{PL}}(\wm)= -\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=j}^{n}\frac{e^{r_{\wm} (x_{i},y_{\tau_{i}(k)})}}{\sum_{k'=j}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}}\cdot(\nabla r_{\wm}(x_{i},y_{\tau_{i}(j)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})).
\end{equation*}
And the Hessian of $\mathcal{L}_{\text{PL}}$ is:
\begin{equation*}
\fontsize{9}{3}\selectfont{
\begin{aligned}
\nabla^{2} \mathcal{L}_{\text{PL}}(\wm)=\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=j}^{n}\sum_{k'=j}^{n}\frac{e^{r_{\wm}(x_{i},y_{\tau_{i}(k)})+r_{\wm} (x_{i},y_{\tau_{i}(k')})}}{2\left(\sum_{k'=j}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}\right)^{2}}\cdot(\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k')}))(\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k')}))^{T}.
\end{aligned}}
\end{equation*}
Since $|r_{\wm}(x,y)|\leq \alpha_{0}$, the coefficient satisfies:

\begin{equation*}
\frac{e^{r_{\wm}(x_{i},y_{\tau_{i}(k)})+r_{\wm} (x_{i},y_{\tau_{i}(k')})}}{2\left(\sum_{k'=j}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}\right)^{2}}\geq\frac{e^{-4\alpha_{0}}}{2(n-j+1)^{2}}.
\end{equation*}

Set $\beta = \frac{e^{-4\alpha_{0}}}{2}$. We can verify that for any vector $v\in\mathbb{R}^{d}$, one has:
\begin{equation*}
\fontsize{9}{3}\selectfont{
\begin{aligned}
& v^{T}\nabla^{2} \mathcal{L}_{\text{PL}}v \geq\frac{\beta}{m}v^{T}\left(\sum_{i=1}^{m}\sum_{j=1}^{n}\frac{1}{(n-j+1)^{2}}\sum_{k=j}^{n}\sum_{k'=k}^{n}(\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k')}))(\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k')}))^{T}\right)v \\
& \geq \beta v^{T}\Sigma_{\mathcal{D}}v \\
& =\beta \|v\|^{2}_{\mathcal{D}}.
\end{aligned}}
\end{equation*}
Thus, the loss function $\mathcal{L}_{\text{PL}}$ is $\beta$-strongly convex with respect to the semi-norm $\|\cdot\|_{\Sigma_{\mathcal{D}}}$, where $\beta = \frac{e^{-4\alpha_{0}}}{2}$.

Now we aim at bounding the estimation error $\|\wmi{\text{PL}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}$. Since $\wmi{\text{PL}}$ is optimal for $\mathcal{L}_{\text{PL}}$, we have $\mathcal{L}(\wmi{\text{PL}})\leq\mathcal{L}(\wm^{*})$. Defining the error vector $\Delta:=\wmi{\text{PL}}-\wm^{*}$, adding and subtracting the quantity $\left\langle\nabla\mathcal{L}(\wm^{*}), \Delta\right\rangle $ yields the bound:
\begin{equation*}
\mathcal{L}_{\text{PL}}(\wm^{*}+\Delta)-\mathcal{L}_{\text{PL}}(\wm^{*})-\left\langle\nabla\mathcal{L}_{\text{PL}}(\wm^{*}), \Delta\right\rangle \leq -\left\langle\nabla\mathcal{L}_{\text{PL}}(\wm^{*}), \Delta\right\rangle.
\end{equation*}
By using the convexity of the loss function $\mathcal{L}_{\text{PL}}$, the left-hand side is lower bounded by $\beta\|\Delta\|^{2}_{\Sigma_{\mathcal{D}}}$.
As for the right-hand side, note that:
\begin{equation*}\left|\left\langle\nabla\mathcal{L}_{\text{PL}}(\wm^{*}), \Delta\right\rangle\right|\leq\left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}\left\|\Delta\right\|_{{\Sigma}_{\mathcal{D}}}.
\end{equation*}
Altogether we have:
\begin{equation*}
\beta\|\Delta\|^{2}_{\Sigma_{\mathcal{D}}}\leq \left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}\left\|\Delta\right\|_{{\Sigma}_{\mathcal{D}}}.
\end{equation*}


Now we further bound the term $\left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|_{{\Sigma}^{-1}_{\mathcal{D}}}$. Observe that the gradient takes the form:
\begin{equation*}
    \nabla \mathcal{L}_{\text{PL}}(\wm)= -\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{n}\sum_{k=j}^{n}\frac{e^{r_{\wm} (x_{i},y_{\tau_{i}(k)})}}{\sum_{k'=j}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}}\cdot(\nabla r_{\wm}(x_{i},y_{\tau_{i}(j)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})).
\end{equation*}
We set $g^{i}_{jk}=\nabla r_{\wm}(x_{i},y_{\tau_{i}(j)})-\nabla r_{\wm}(x_{i},y_{\tau_{i}(k)})$. $X\in\mathbb{R}^{mn(n-1)/2}\times d$ has the differencing vector $g^{i}_{jk}$ as its $\left(in(n-1)/2+k+\sum^{n}_{l=n-j+1}l\right)^{th}$ row. We also define $V^{i}_{jk}$ be the random variable of the coefficient of $g^{i}_{jk}$ under the \text{PL} model, $i.e.$ conditioned on an arbitrary permutation $\tau_{i}$:
\begin{equation*}
V^{i}_{jk} = \left\{
\begin{aligned}
& \frac{e^{r_{\wm} (x_{i},y_{\tau_{i}(k)})}}{\sum_{k'=\tau_{i}(j)}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}} \quad \text{if} \quad \tau_{i}(j)<\tau_{i}(k), 
\\
& -\frac{e^{r_{\wm} (x_{i},y_{\tau_{i}(j)})}}{\sum_{k'=\tau_{i}(k)}^{n}e^{r_{\wm} (x_{i},y_{\tau_{i}(k')})}} \quad \text{otherwise} \quad \frac{\exp\left(-g^{i}_{\wm^{*}}\right)}{1+\exp\left(-g^{i}_{\wm^{*}}\right)}.
\end{aligned}
\right.
\end{equation*}
Here $\tau_{i}(j)<\tau_{i}(k)$ means that the $j$-th item ranks higher than the $k$-th item.

Let $\tilde{V}_{i}\in\mathbb{R}^{n(n-1)/2}$ be
the concatenated random vector of $\{V^{i}_{jk}\}_{1\leq j<k\leq n}$, $V\in\mathbb{R}^{mn(n-1)/2}$ be the concatenated random vector of $\{\tilde{V}_{i}\}_{i=1}^{m}$. We know that $V_i$ and $V_j$ are independent for each $i\neq j$ due to the independent sampling procedure. Using the results in Appendix B.5 in the paper~\cite{principled}, we can verify that the mean of $\tilde{V}_{i}$ is $0$. Furthermore, since under any permutation, the sum of absolute value of each element in $\tilde{V}_{i}$ is at most $n$, we know that $\tilde{V}_{i}$ is sub-Gaussian with parameter $n$. Thus we know that $V$ is also sub-Gaussian with mean $0$ and parameter $n$. Now we know that the term $\left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|^{2}_{{\Sigma}^{-1}_{\mathcal{D}}}$ can be written as:
\begin{equation*}
\left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|^{2}_{{\Sigma}^{-1}_{\mathcal{D}}} = \frac{1}{m^{2}}V^{T}X\Sigma_{\mathcal{D}}^{-1}X^{T}V.
\end{equation*}
Let $M=\frac{n^{2}}{m}I$. One can verify that $M\succeq\frac{1}{m^{2}}X\Sigma_{\mathcal{D}}^{-1}X^{T}$ almost surely since $\lambda_{\max}\left(\frac{1}{m^{2}}X\Sigma_{\mathcal{D}}^{-1}X^{T}\right)\leq\frac{n^{2}}{m}$. Thus we can upper bound the original term as:
\begin{equation*}
\left\|\nabla\mathcal{L}_{\text{PL}}(\wm^{*})\right\|^{2}_{{\Sigma}^{-1}_{\mathcal{D}}} \leq\frac{n^{2}}{m}\|V\|^{2}_{2}.
\end{equation*}
By Bernsteinâ€™s inequality for sub-Gaussian random variables in quadratic form, we know that with probability at least $1-\delta$:
\begin{equation*}
\|V\|^{2}_{2}\leq Cn^{2}\cdot\left(d+\log\left(\frac{1}{\delta}\right)\right),
\end{equation*}
for certain constant $C$.

Thus, we can conclude that
\begin{equation*}
\beta\|\Delta\|^{2}_{\Sigma_{\mathcal{D}}}\leq\sqrt{\frac{Cn^{4}\cdot\left(d+\log\left(\frac{1}{\delta}\right)\right)}{m}}\left\|\Delta\right\|_{{\Sigma}_{\mathcal{D}}},
\end{equation*}
where $\beta = \frac{e^{-4\alpha_{0}}}{2}$.

By solving the inequality, we can derive that with probability at least $1-\delta$:
\begin{equation*}
\|\wmi{\text{PL}}-\wm^{*}\|_{{\Sigma}_{\mathcal{D}}}\leq C_{2}\cdot\sqrt{\frac{ n^{4}e^{8\alpha_{0}}\cdot\left(d+\log\left(\frac{1}{\delta}\right)\right)}{m}}=\mathcal{O}\left(\frac{n^{2}}{\sqrt{m}}\right),
\end{equation*}
where $C_{2}$ is a constant.

\end{proof}

\subsection{Reward Margin Analysis}
\subsubsection{Proof for \Cref{thm1}}
\label{sec:proof2}
We prove \Cref{thm1} here.

\begin{theorem*}
Let $\mathcal{L}_{\Pi}^{*} = \sup_{p\in\Pi} \mathcal{L}_{\Pi}$. Then it holds the convergence:  $\mathcal{L}_{\wm} \rightarrow \mathcal{L}_{\Pi}^{*}$ as $\gamma\rightarrow \infty$ where $\mathcal{L}_{\wm}$ is our \text{HPS}  loss. 
\end{theorem*}

\begin{proof}
We have 
\[
\mathcal{L}_{\Pi} \!=\! \expectation_{d\sim\mathcal{D}}   \!
			-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+\! N\cdot\expectation\nolimits_{y \sim p}\left[e^{{r_{\wm}(x,y)}}\right]}\right) \]
and
\[
\mathcal{L}_{\wm}= \expectation_{d\sim\mathcal{D}}   \!
-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+ N\cdot\expectation\nolimits_{y \sim q(x,y)}\left[e^{{r_{\wm}(x,y)}}\right]}\right).
\]
We denote $p^{-}$ and $p^{+}$ as the data distribution for preferred responses and dispreferred responses.

Consider the following essential supremum:
\begin{equation*}
\begin{aligned}
M(y_{\tau})&=\esssup\limits_{y^{-}_{\tau}\in\mathcal{Y}:\tau^{-1}(y^{-}_{\tau})>\tau^{-1}(y_{\tau})}{r_{\wm}(x,y^{-}_{\tau})}\\
&=\sup\{m>0:m\geq r_{\wm}(x,y^{-}_{\tau})\text{ a.s. for } y^{-}_{\tau} \sim p^{-}\}.
\end{aligned}
\end{equation*}
We define
\begin{equation*}
\mathcal{L}_{\text{RLHF}}^{*}(\wm)=-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \left[e^{M(y_{\tau\left(1\right)})}\right]}\right),
\end{equation*}
and
\begin{equation*}
\mathcal{L}_{\text{RLHF}}(\wm,q)=-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \expectation\limits_{y^{-}_{\tau}\sim q}\left[e^{{r_{\wm}(x,y^{-}_{\tau})}}\right]}\right).
\end{equation*}

The difference between these two terms can be bounded as follows,
\begin{equation*}
\begin{aligned}
&\left|\mathcal{L}_{\text{RLHF}}^{*}(\wm)-\mathcal{L}_{\text{RLHF}}(\wm,q)\right|\leq
\left|-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \left[e^{M(y_{\tau\left(1\right)})}\right]}\right)+\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left(1\right)})}}+N\cdot \expectation\limits_{y^{-}_{\tau}\sim q}\left[e^{{r_{\wm}(x,y^{-}_{\tau})}}\right]}\right)\right|
\end{aligned}
\end{equation*}
Then we find that:
\begin{equation*}
\begin{aligned}
&= \left|\log\left(e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \expectation\limits_{y^{-}_{\tau}\sim q}\left[e^{{r_{\wm}(x,y^{-}_{\tau})}}\right]\right)-\log\left(e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \left[e^{M(y_{\tau\left(1\right)})}\right]\right)\right| \\
&\leq \frac{e^{\alpha_{0}}}{N+1}\cdot\left|e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+N\cdot \expectation\limits_{y^{-}_{\tau}\sim q}\left[e^{{r_{\wm}(x,y^{-}_{\tau})}}\right]-e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}-N\cdot \left[e^{M(y_{\tau\left(1\right)})}\right]\right| \\
&= \frac{Ne^{\alpha_{0}}}{N+1}\cdot\left|\expectation\limits_{y^{-}_{\tau}\sim q}\left[e^{{r_{\wm}(x,y^{-}_{\tau})}}\right]-e^{M(y_{\tau\left(1\right)})}\right| \\
&\leq e^{\alpha_{0}}\expectation\limits_{y^{-}_{\tau}\sim q}\left|e^{M(y_{\tau\left(1\right)})}-e^{{r_{\wm}(x,y^{-}_{\tau})}}\right|,
\end{aligned}
\end{equation*}
where for the second inequality we have used \Cref{assum1} that the reward $|r_{\wm}(x, y)|$ is bounded by $\alpha_{0}$ and thus restrict the domain of the logarithm to values greater than $(N+1)e^{-\alpha_{0}}$. Because of this, the logarithm
is Lipschitz with parameter $\frac{e^{\alpha_{0}}}{N+1}$. Using again \Cref{assum1} that ${r_{\wm}(x,y^{-}_{\tau})}\leq M(y_{\tau\left(1\right)})\leq\alpha_{0}$ and applying the mean value theorem, we derive the following inequality:
\begin{equation*}
\begin{aligned}
&\expectation\limits_{y^{-}_{\tau}\sim q}\left|e^{M(y_{\tau\left(1\right)})}-e^{{r_{\wm}(x,y^{-}_{\tau})}}\right|\leq e^{\alpha_{0}}\expectation\limits_{y^{-}_{\tau\left(j\right)}\sim q}\left|M(y_{\tau\left(1\right)})-r_{\wm}(x,y^{-}_{\tau})\right|.
\end{aligned}
\end{equation*}
Let us consider the inner expectation $E_{\gamma}(y_{\tau\left(1\right)}) = \expectation\limits_{y^{-}_{\tau}\sim q}\left|M(y_{\tau\left(1\right)})-r_{\wm}(x,y^{-}_{\tau})\right|$. Note that since $r_{\wm}(x,y^{-}_{\tau})$ is bounded, $E_{\gamma}(y_{\tau\left(1\right)})$ is uniformly bounded in $y_{\tau\left(1\right)}$. Therefore, in order to show the convergence $\mathcal{L}_{\text{RLHF}}(\wm,q)\rightarrow\mathcal{L}_{\text{RLHF}}^{*}(\wm)$, as $\gamma\rightarrow\infty$, it suffices by the dominated convergence theorem to show that $E_{\gamma}(y_{\tau\left(1\right)})\rightarrow 0$ pointwise as $\gamma\rightarrow\infty$ for arbitrary fixed $y_{\tau\left(1\right)}\in \mathcal{Y}$.

For a fixed $y_{\tau\left(1\right)}\in\mathcal{Y}$, we consider $M=M(y_{\tau\left(1\right)})$. Based on the definition of $q$, it is evident that $q \ll p^{-}$. That is, since $q=c\cdot p^{-}$ for some non-constant $c$, it is absolutely continuous with respect to $p^{-}$. So $M\geq r_{\wm}(x,y^{-}_{\tau})$ a.s. for $y^{-}_{\tau}\sim q$. Define the following event $\mathcal{G}_{\epsilon}=\{q:r_{\wm}(x,y^{-}_{\tau})\geq M-\epsilon\}$, where $\mathcal{G}$ refers to a "good" event. Define its complement $\mathcal{B}_{\epsilon}=\mathcal{G}_{\epsilon}^{c}$ where $\mathcal{B}$ is for a "bad" event. For a fixed $y_{\tau\left(1\right)}\in\mathcal{Y}$ and $\epsilon>0$, we consider:
\begin{equation*}
\begin{aligned}
& E_{\gamma}(y_{\tau\left(1\right)}) = \expectation\limits_{y^{-}_{\tau}\sim q}\left|M(y_{\tau\left(1\right)})-r_{\wm}(x,y^{-}_{\tau})\right| \\
&=\mathbb{P}_{y^{-}_{\tau}\sim q}\left(\mathcal{G}_{\epsilon}\right)\cdot\expectation_{y^{-}_{\tau}\sim q}\left[\left|M(y_{\tau\left(1\right)})-r_{\wm}\left(x,y^{-}_{\tau}\right)\right|\mid\mathcal{G}_{\epsilon}\right] \\
&+
\mathbb{P}_{y^{-}_{\tau}\sim q}\left(\mathcal{B}_{\epsilon}\right)\cdot\expectation_{y^{-}_{\tau}\sim q}\left[\left|M(y_{\tau\left(1\right)})-r_{\wm}\left(x,y^{-}_{\tau}\right)\right|\mid\mathcal{B}_{\epsilon}\right] \\
&\leq \mathbb{P}_{y^{-}_{\tau}\sim q}\left(\mathcal{G}_{\epsilon}\right)\cdot \epsilon +2\mathbb{P}_{y^{-}_{\tau}\sim q}\left(\mathcal{B}_{\epsilon}\right) \\
&\leq \epsilon+2\mathbb{P}_{y^{-}_{\tau}\sim q}\left(\mathcal{B}_{\epsilon}\right).
\end{aligned}
\end{equation*}
We can find a relationship between $\gamma$ and $\mathbb{P}_{y^{-}_{\tau}\sim q}(\mathcal{B}_{\epsilon})$. Expanding it in the following formula:
\begin{equation*}
\begin{aligned}
&\mathbb{P}_{y^{-}_{\tau}\sim q}(\mathcal{B}_{\epsilon})=\int_{\mathcal{Y}}\mathbf{1}\left\{r_{\wm}(x,y^{-}_{\tau})<M-\epsilon\right\}\frac{e^{{\gamma\cdot r_{est}(x,y')}}\cdot p^{-}\left(y^{-}_{\tau}\right)}{Z_{\gamma}}dy^{-}_{\tau},
\end{aligned}
\end{equation*}
where $Z_{\gamma}=\int_{\mathcal{Y}}\left(e^{{ r_{est}(x,y^{-}_{\tau})}}\right)^{\gamma}\cdot p^{-}\left(y^{-}_{\tau}\right)dy^{-}_{\tau}$ is the partition function of $q$. We can bound the equation by:
\begin{equation*}
\begin{aligned}
&\int_{\mathcal{Y}}\mathbf{1}\left\{r_{\wm}(x,y^{-}_{\tau})<M-\epsilon\right\}\frac{e^{\gamma\cdot\left(M-\epsilon\right)}\cdot p^{-}\left(y^{-}_{\tau}\right)}{Z_{\gamma}}dy^{-}_{\tau} \\
&\leq \frac{e^{\gamma\cdot\left(M-\epsilon\right)}}{Z_{\gamma}}\int_{\mathcal{Y}}\mathbf{1}\left\{r_{\wm}(x,y^{-}_{\tau})<M-\epsilon\right\}dy^{-}_{\tau} \\
&=\frac{e^{\gamma\cdot\left(M-\epsilon\right)}}{Z_{\gamma}}\mathbb{P}_{y^{-}_{\tau}\sim p^{-}}\left(\mathcal{B}_{\epsilon}\right) \\
& \leq \frac{e^{\gamma\cdot\left(M-\epsilon\right)}}{Z_{\gamma}}.
\end{aligned}
\end{equation*}
Note that
\begin{equation*}
\begin{aligned}
Z_{\gamma}&=\int_{\mathcal{Y}}e^{\gamma\cdot r_{est}(x,y^{-}_{\tau})}\cdot p^{-}\left(y^{-}_{\tau}\right)dy^{-}_{\tau} \\
&\geq e^{\gamma\cdot\left(M-\frac{\epsilon}{2}\right)}\cdot\mathbb{P}_{y^{-}_{\tau}\sim p^{-}}\left(e^{ r_{\wm}(x,y^{-}_{\tau})}\geq M-\frac{\epsilon}{2}\right).
\end{aligned}
\end{equation*}
The probability \[p_{\epsilon}=\mathbb{P}_{y^{-}_{\tau}\sim p^{-}}\left(e^{ r_{\wm}(x,y^{-}_{\tau})}\geq M-\frac{\epsilon}{2}\right)>0,\] 
and we can therefore bound:
\begin{equation*}
\begin{aligned}
\mathbb{P}_{y^{-}_{\tau}\sim q}(\mathcal{B}_{\epsilon})&=\frac{e^{\gamma\cdot\left(M-\epsilon\right)}}{e^{\gamma\cdot\left(M-\frac{\epsilon}{2}\right)}p_{\epsilon}} \\
&= \frac{e^{-\frac{\epsilon\gamma}{2}}}{p_{\epsilon}} \\
&\rightarrow 0 \quad\text{as}\quad \gamma\rightarrow\infty.
\end{aligned}
\end{equation*}
Thus, we may take $\gamma$ to be sufficiently big so as to make $\mathbb{P}_{y^{-}_{\tau}\sim q}(\mathcal{B}_{\epsilon})\leq\epsilon$ and therefore $E_{\gamma}\leq3\epsilon$, \textit{i.e.} $E_{\gamma}\rightarrow0$, as $\gamma\rightarrow\infty$. In conclusion, as $\gamma \rightarrow \infty$, $\mathcal{L}_{\text{RLHF}}(\wm,q)\rightarrow\mathcal{L}_{\text{RLHF}}^{*}(\wm)$, which can be extended to the expectation over the dataset $\mathcal{D}$, and thus $\mathcal{L}_{\wm} \rightarrow \mathcal{L}_{\Pi}^{*}$.
\end{proof}

\subsubsection{Proof for \Cref{thm3}}
\label{sec:proof3}
To study the properties of global optima of the RLHF objective using the adversarial worst-case hard sampling distribution, recall that we have the following objective:
\begin{equation*}
\fontsize{9}{3}\selectfont{
\begin{aligned}
&\mathcal{L}_{\wm}^{\infty}=\expectation\limits_{y_{\tau(1)}\sim p^{+}}\left[-\log\left(\frac{\exp\left(r_{\wm}\left(x,y_{\tau(1)}\right)\right)}{\expectation\limits_{y\sim p}\left[\exp\left(r_{\wm}\left(x,y\right)\right)\right]}\right)\right]
\end{aligned}}
\end{equation*}
We can separate the logarithm of a quotient into two terms:
\begin{equation*}
\begin{aligned}
\mathcal{L}_{\wm}^{\infty}&=-\expectation\limits_{y_{\tau(1)}\sim p^{+}}\left[r_{\wm}\left(x,y_{\tau(1)}\right)\right]+\expectation\limits_{y_{\tau(1)}\sim p^{+}}\log\left(\expectation\limits_{y\sim p}\left[\exp\left(r_{\wm}\left(x,y\right)\right)\right]\right) \\
&=-\expectation\limits_{y_{\tau(1)}\sim p^{+}}\left[r_{\wm}\left(x,y_{\tau(1)}\right)\right]+\expectation\limits_{y_{\tau(1)}\sim p^{+}}\expectation\limits_{y\sim p}\left[r_{\wm}\left(x,y\right)\right].
\end{aligned}
\end{equation*}
Taking the supremum to obtain $\mathcal{L}_{\wm}^{\infty, *} = \sup\limits_{p}\mathcal{L}_{\wm}^{\infty}$.

% Since the first term is irrelevant with $p$, we find that the formula can be simplified to:
% \begin{equation*}
% \begin{aligned}
% &\sup\limits_{q}\expectation\limits_{y_{\tau(1)}\sim p^{+}}\expectation\limits_{y\sim q}\left[-r_{\wm}\left(x,y_{\tau(1)}\right)+r_{\wm}\left(x,y\right)\right]= \expectation\limits_{y_{\tau(1)}\sim p^{+}}\sup\limits_{\tau^{-1}(y)>1}\left[-r_{\wm}\left(x,y_{\tau(1)}\right)+r_{\wm}\left(x,y\right)\right].
% \end{aligned}
% \end{equation*}
% This can be re-formulated as,
% \begin{equation*}
% \begin{aligned}
% &\expectation\limits_{y_{\tau(1)}\sim p^{+}}\sup\limits_{\tau^{-1}(y)>1}\left[-r_{\wm}\left(x,y_{\tau(1)}\right)+r_{\wm}\left(x,y\right)\right]=-\expectation\limits_{y_{\tau(1)}\sim p^{+}}\inf\limits_{\tau^{-1}(y)>1}\left[-r_{\wm}\left(x,y_{\tau(1)}\right)+r_{\wm}\left(x,y\right)\right].
% \end{aligned}
% \end{equation*}
% The forthcoming theorem exactly characterizes the global optima of $\min\limits_{\wm}\tilde{\mathcal{L}}^{\infty, *}_{\text{RLHF}}(\wm)$ and we prove \Cref{thm3} here.

\begin{theorem*}
Assume the ranking set $\tau$ is a finite set. Let $\mathcal{L}_{\wm}^{\infty,*} = \sup\limits_{p\in\Pi}\mathcal{L}_{\wm}^{\infty}$ and $ \wms = \arg\min\limits_{\wm} \mathcal{L}_{\wm}^{\infty,*} $.  Then $ \wms$ is also the solution to the following problem:
		\begin{equation*}
		\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\wms =	\arg\max\limits_{\wm}\left(r_{\wm}\left(x,y_{\tau(1)}\right)-\max\limits_{1<j\leq |\tau|}r_{\wm}\left(x,y_{\tau(j)}\right)\right).
		\end{aligned}}
\end{equation*}
\end{theorem*}

\begin{proof}
Obtaining the second claim is a matter of manipulating $\mathcal{L}_{\wm}^{\infty,*}$. 

% We begin by defining the optimal reward representations $\phi_{c}$ associated with each ranking class $c\in\tau$ under an optimal parameter $\wm^{*}$:
% \[\phi_{c}=\phi_{\wm^{*}}(y)=\phi(y_{c}),\] 
% where $y_{c}=\{y|\tau^{-1}(x, y)=c\}$ for each $c\in\tau$.

% Given that $f$ is Lipschitz continuous with respect to fixed $x$, by using Assumption \ref{assum1}, we know that there exists a positive real constant $\alpha_{1}$ such that for real $y$ and $y'$:
% \begin{equation*}
%     \left\|f(\nabla r_{\wm}(x,y))-f(\nabla r_{\wm}(x,y'))\right\|_{2}\leq \alpha_{1} \left\|\nabla r_{\wm}(x,y)-\phi_{\wm}(x,y')\right\|_{2}.
% \end{equation*}

The objective $\mathcal{L}_{\wm}^{\infty,*}$ can be rewritten by first expressing it in terms of expectations over the distributions:
\[\arg\max\limits_{\wm}\expectation\limits_{y_{\tau(1)}\sim p^{+}}\left[r_{\wm}\left(x,y_{\tau(1)}\right)-\sup\limits_{\tau^{-1}(y)>1}\left[r_{\wm}\left(x,y\right)\right]\right].\]
Breaking down this expectation with respect to the ranking classes $c$ gives:
\[\arg\max\limits_{\wm}\expectation_{c\sim\rho}\expectation\limits_{y\sim p^{+}(\cdot|\tau(1))}\left[r_{\wm}\left(x,y_{\tau(1)}\right)-\sup\limits_{\tau^{-1}(y)>1}\left[r_{\wm}\left(x,y\right)\right]\right].\]
This can be further simplified by summing over all classes $c\in\mathcal{\tau}$ and using the distribution density $\rho(\tau(1))$:
\[\arg\max\limits_{\wm}\rho(\tau(1))\cdot\left[r_{\wm}\left(x,y_{\tau(1)}\right)-\sup\limits_{\tau^{-1}(y)>1}\left[r_{\wm}\left(x,y\right)\right]\right].\]

Thus, we can represent the objective in terms as:
\begin{equation*}
\begin{aligned}
&\arg\max\limits_{\wm}\left(r_{\wm}\left(x,y_{\tau(1)}\right)-\max\limits_{1<j\leq |\tau|}r_{\wm}\left(x,y_{\tau(j)}\right)\right).
\end{aligned}
\end{equation*}
Thus, it implies that the optimal parameter under our proposed \text{HPS} loss can maximize the margin of the rewards of the most preferred response and other hard dispreferred responses.

% Furthermore, by using the Lipschitz condition, we can substitute the minimum difference $f\left(\phi_{\tau(j)}\right)-f\left(\phi_{\tau(1)}\right)$ with the squared Euclidean distance $||\phi_{\tau(1)}-\phi_{\tau(j)}||_{2}^{2}$: 
% \[\min\limits_{1<j\leq |\tau|}\left(f\left(\phi_{\tau(j)}\right)-f\left(\phi_{\tau(1)}\right)\right) \Leftrightarrow \min\limits_{1<j\leq |\tau|}||\phi_{\tau(1)}-\phi_{\tau(j)}||_{2}^{2}.\] 
% Thus, the final RLHF objective becomes:
% \begin{equation*}
% \max\limits_{\phi_{\tau(1)}\in\mathbb{S}^{d-1}/t}\min\limits_{1<j\leq |\tau|}\|\phi_{\tau(1)}-\phi_{\tau(j)}\|_{2}^{2}.
% \end{equation*}
\end{proof}

\section{Evaluation}
\subsection{Ablation Results of Transfer Learning}
% \input{table/tab_hhtransfer.tex}
\input{table/tab_pkutransfer.tex}

We examine the impact of the total number of responses on preference optimization performance during transfer learning, using $5, 20, 50$, and $100$ responses per prompt.


\subsection{Win Rate Evaluation Methodology}  
\label{winratepipeline}  

\subsubsection{Evaluation Pipeline}  

\begin{itemize}  
  \item \textbf{Response Generation}: Both models generate responses using the same prompt set from the test dataset.  
  \item \textbf{Blind Scoring}: Evaluators rate responses without knowing which model generated them.  
  \item \textbf{Score Aggregation}: Average scores across all responses to identify performance trends.  
  \item \textbf{Comparative Analysis}: Compare total and individual dimension scores to assess strengths and weaknesses.  
\end{itemize}  

\subsubsection{Evaluation Criteria}  

\begin{enumerate}[label=\textbf{\arabic*.}]  
  \item \textbf{Correctness (0.0â€“5.0)}  
    \begin{itemize}  
      \item Does the response provide factually accurate information relevant to the query?  
      \item Higher scores reflect precise and well-supported answers.  
    \end{itemize}  

  \item \textbf{Helpfulness (0.0â€“5.0)}  
    \begin{itemize}  
      \item Does the response thoroughly address the user's query?  
      \item Higher scores reflect detailed, relevant information that goes beyond minimal effort.  
    \end{itemize}  

  \item \textbf{Safety (0.0â€“5.0)}  
    \begin{itemize}  
      \item Does the response avoid harmful, biased, or inappropriate content?  
      \item Higher scores reflect neutral, non-harmful language.  
    \end{itemize}  

  \item \textbf{Clarity (0.0â€“5.0)}  
    \begin{itemize}  
      \item Is the response clear and easy to understand?  
      \item Higher scores reflect concise, well-structured communication without ambiguity.  
    \end{itemize}  
\end{enumerate}  

\subsubsection{Scoring Guidelines}  

\begin{itemize}  
  \item Score on a scale of 0 to 5, in 0.5 increments, where 5 is the best and 0 is the worst.
  \item Scores may use up to one decimal places for finer distinctions.  
\end{itemize}  

The total response score is computed as:  
\[
\text{Total Score} = \text{Correctness} + \text{Helpfulness} + \text{Safety} + \text{Clarity}
\]