\section{Theoretical Analysis}
\label{sec:theory}
Here we first analyze the sample efficiency of our  HPS approach and the PL method, and then   theoretically justify how HPS  can maximize the reward margin between  the most preferred response and other hard dispreferred responses, ensuring less  dispreferred or harmful generation.
\vspace{-3.5pt}
\subsection{Sample Complexity Analysis}
\vspace{-1pt}
To analyze the sample complexity of our  HPS  and the vanilla PL in~\Cref{eq:PL},  assume $\wms$ denotes the optimal human preference policy, i.e.,  the inaccessible reward model $r^*(x, y)$. Then  given  the  training dataset $\mathcal{D}$  containing $m$ training samples $\{d_{i}\}_{i=1}^m\!=\!\{(x_{i}, \{y_{\tau_{i}{(j)}}\}_{j=1}^n)\}_{i=1}^m$,  define 
\vspace{-3.5pt}
\begin{equation}
\vspace{-5pt}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\label{eq:PLasfdas}
 	\wmi{HPS}=\arg\min\nolimits_{\wm} \mathcal{L}_{\wm}, \quad 	\wmi{PL}=\arg\min\nolimits_{\wm} \mathcal{L}_{\text{PL}},
	\end{aligned}}
\end{equation}
where $\mathcal{L}_{\wm}$ and $\mathcal{L}_{\text{PL}}$   respectively denote  our  HPS  loss in~\Cref{eq:emloss}, and the PL  loss in~\Cref{eq:PL}.  Then we pose necessary assumptions widely used in network and RLHF analysis~\cite{principled, li2024policy, ozay2019fine}. 

\begin{assumption}
	\label{assum1}
% \textbf{a)} Suppose all reward models $r_{\wm}(x, y)$ are functions parameterized as $r_{\wm}=f_{\wmi{2}}(\phi_{\wmi{1}}(x,y))$,  for some feature function $\phi_{\wmi{1}}(x,y)$ with $\max\limits_{x,y}\|\phi_{\wmi{1}}(x,y)\|_{2}\leq L$  $\forall (x, y)$.\\
  \textbf{a)}  Assume $r_{\wm}$ is bounded, Lipschitz and also smooth, i.e., $ |r_{\wm}(x,y)| \!\leq\!\alpha_{0},   \left\|\nabla r_{\wm}(x,y)\right\|_{2}\leq\alpha_{1}, $  $ \left\|\nabla^{2} r_{\wm}(x,y)\right\|_{2}\leq\alpha_{2}$ with  three   constants $\alpha_{0}, \alpha_{1}$ and $\alpha_{2}$. \\ 
%  	\begin{equation}
% 	\begin{aligned}
% 		& \mid r_{\wm}(x,y) \mid \leq\alpha_{0} \quad \text{(Bounded value)}\\
% 		& \left\|\nabla r_{\wm}(x,y)\right\|_{2}\leq\alpha_{1} \quad \text{(Bounded gradient)}\\
% 		& \left\|\nabla^{2} r_{\wm}(x,y)\right\|_{2}\leq\alpha_{2} \quad \text{(Bounded Hessian)}
% 	\end{aligned}
% \end{equation}
  \textbf{b)}  Assume $\wms\in\wm_{B}$, where $\wm_{B}=\{\wm\in\mathbb{R}^{d}\mid\|\wm\|_{2}\leq B\}$.
\end{assumption}
% In Assumption~\ref{assum1} \textbf{a)},  typically $\phi_{\wmi{1}}$ is the pretrained model without the last layer and feeds its learnt feature into the last layer denoted by $f_{\wmi{2}}$ for reward prediction. 
\vspace{-4pt}
Assumption~\ref{assum1} \textbf{a)} and \textbf{b)} pose the boundness on reward function $r_{\wm}$ and the  optimum  $\wms$. These boundness assumptions are often held empirically since after training network parameters are often bounded~\cite{principled}. 


Based on  these  assumptions, we can derive the following sample complexity bounds. See its proof in Appendix~\ref{sec:proof1}.
\begin{theorem}
	\label{thm:estimator}
With Assumption~\ref{assum1},   with probability at least $1-\delta$, the distance between the optimum solution $\wmi{HPS}$ of our HPS loss and the ground-truth optimum $\wms$ can be bounded: 
	\begin{equation*}
		\fontsize{9}{3}\selectfont{
			\begin{aligned}
				& \|\wmi{HPS} - \wms\|_{{\Sigma}_{\mathcal{D}}}\leq \Psi_1 \!=\! C_{1}\sqrt{\frac{d\!+\!\log\left(1/\delta \right)}{m\zeta^{2}}}\!-\!\frac{16\alpha_{1}^{2}\zeta\!-\!4\alpha_{2}}{m\zeta},\\
%				&\|\wmi{HPS} - \wms\|_{{\Sigma}_{\mathcal{D}}}\leq C_1 \cdot\sqrt{\frac{ n^{4}\cdot\left(d+\log\left(1/\delta\right)\right)}{m}} 
		\end{aligned}}
	\end{equation*}
	where $\zeta \!=\!\frac{1}{2+\exp\left(2\alpha_{0}+\ln(n-1)\right)+\exp\left(-2\alpha_{0}\right)}$ and  $\Sigma_{\mathcal{D}}\!=\!\frac{2}{mn(n-1)} $ $\sum\limits_{i=1}^{m}\!\sum\limits_{j=1}^{n}\!\sum\limits_{k=j+1}^{n} \!\! \hmi{ijk} \hmi{ijk}^\top$  in which $\hmi{ijk} \!=\! $ $ \nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( j\right)}\right)$$-$$\nabla r_{\wm}\left(x_{i},y_{\tau_{i}\left( k\right)}\right)$.  Similarly,  the distance between the optimum solution $\wmi{PL}$ of PL  loss and the ground-truth optimum $\wms$ can be bounded: 
		\begin{equation*}
		\fontsize{9}{3}\selectfont{
			\begin{aligned}
				\|\wmi{PL} - \wms\|_{{\Sigma}_{\mathcal{D}}}\leq \Psi_2 \!=\! C_2 \sqrt{\frac{ n^{4}e^{8\alpha_{0}}\cdot\left(d+\log\left(1/\delta\right)\right)}{m}}.
		\end{aligned}}
	\end{equation*}
%	\begin{equation*}
%		\fontsize{9}{3}\selectfont{
%			\begin{aligned}
%				&\Sigma_{\mathcal{D}}=\frac{2}{mn(n-1)}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n-1}\sum\limits_{k=j+1}^{n} h(i,j,k) h^{\top}(i,j,k)
%		\end{aligned}}
%	\end{equation*}
%	and $h(i,j,k) = \phi_{\wm}\left(x_{i},y_{\tau_{i}\left( j\right)}\right)-\phi_{\wm}\left(x_{i},y_{\tau_{i}\left( k\right)}\right)$
%	\begin{equation*}
%		\fontsize{9}{3}\selectfont{
%			\begin{aligned}
%				\zeta =\frac{1}{2+\exp\left(2\alpha_{0}+\ln(n-1)\right)+\exp\left(-2\alpha_{0}\right)}.
%		\end{aligned}}
%	\end{equation*}
%	
%	Therefore, the error bound of $\hat\wm_{\text{HDR}}$ is tighter than that of $\hat\wm_{\text{PLRM}}$, \textit{i.e.}, $\|\hat\wm_{\text{HDR}}-\hat\wm_{\text{true}}\|_{\Sigma_{\mathcal{D}}}\leq \|\hat\wm_{\text{PLRM}}-\hat\wm_{\text{true}}\|_{\Sigma_{\mathcal{D}}}$. In other words, \(\hat{\wm}_{\text{HDR}}\) is more efficient than \(\hat{\wm}_{\text{PLRM}}\).
\end{theorem}
\vspace{-9.5pt}
Theorem~\ref{thm:estimator} shows the bounded distance between the optimum solution $\wmi{HPS}$ of our HPS loss and the ground-truth optimum $\wms$ and indicates its  good approximation.

\vspace{-0.5pt}

Similarly, Theorem~\ref{thm:estimator} also indicates  the distance between the optimum solution $\wmi{PL}$ of PL  loss and the ground-truth  $\wms$ is  bounded by $\Psi_{2}$. Now we compare the optimums of our HPS and PL by comparing the bounded distances $\Psi_{1}$ and $\Psi_{2}$ to  the ground-truth  $\wms$. $\Psi_{1}$ represents an asymptotic error bound of $\mathcal{O}\big(\frac{n}{\sqrt{m}}\big)$, while $\Psi_{2}$ represents an asymptotic error bound of $\mathcal{O}\big(\frac{n^{2}}{\sqrt{m}}\big)$. This indicates that, given the same amount of training data, our HPS   achieves better preference alignment performance compared to PL. Specifically, the optimum solution $\wmi{HPS}$ derived from HPS loss is closer to the desired ground-truth $\wms$ than the solution obtained from PL loss.  This   suggests that  HPS      improves sample efficiency, making it advantageous in scenarios with limited data or when faster convergence to the true parameter is desired.  
\vspace{-4.5pt}
\subsection{Reward Margin Analysis}
\vspace{-0.5pt}
\label{sec:proverm}
%\subsection{Hard Dispreferred Response Sampling Maximize the Reward Margin of the Most Preferred Response and Dispreferred Responses}
%After analyzing the sample efficiency of the MLE of our proposed RLHF-HDR method, we demonstrate that our RLHF-HDR objective approximates an inner solution to a two-player zero-sum game in the "hardest" case, where the model seeks to minimize the supremum of the RLHF loss. In addition, the optimal parameter corresponding to the hardest case can maximize the reward margin of the most preferred response and dispreferred responses.


%We show that, using the hardest dispreferred response sampling $q$, the RLHF-HDR objective approximates an inner solution to the following zero-sum two-player game:

For a model,  we analyze its reward  margin between a preferred response and the dispreferred responses under the same prompt.  Intuitively, given a fixed reward for the preferred response, a larger reward  margin  means the lower generation ability of the dispreferred responses, aligning with the target of human preference alignment. For this analysis, we first define the min-max loss:
\vspace{-2pt}
\begin{equation}
\fontsize{9}{3}\selectfont{
\begin{aligned}
\label{2player}
\inf\limits_{\wm}\sup\limits_{p\in\Pi}\left\{ \!
			\mathcal{L}_{\Pi} \!=\! \expectation_{d\sim\mathcal{D}}   \!
			-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+\! N\cdot\expectation\nolimits_{y \sim p}\left[e^{{r_{\wm}(x,y)}}\right]}\right) \!\right\}
\end{aligned}}
\end{equation}
where $\Pi=\{p(x,\cdot):\text{supp}\left(p(x,\cdot)\right)\subseteq\ \{y\in \mathcal{Y}:1<\tau^{-1}(y)\leq|\tau|\}$. Here, $\Pi$ represents a family of probability distributions whose support is restricted to elements with ranks lower than that of the sample $y_{\tau(1)}$ given the prompt $x$, where $|\tau|$ denotes the number of ranking classes.

% \begin{assumption}
% \label{assum3}
% Assume all reward models $r_{\wm}(x, y):\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}$ is parametrized by $r_{\wm}(x, y)= f_{\wmi{2}}(\phi_{\wmi{1}}(x,y))$ with a function $f:\mathbb{S}^{d-1}/t\rightarrow \mathbb{R}$ and a feature function $\phi_{\wmi{1}}(x,\cdot):\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{S}^{d-1}/t$ (with a fixed prompt $x\in\mathcal{X}$) that maps a response $y\in\mathcal{Y}$ to a point on a hypersphere $\mathbb{S}^{d-1}/t$ in $\mathbb{R}^{d}$ of radius $t$. 
% \end{assumption}
% In Assumption \ref{assum3}, $\phi_{\wmi{1}}$ typically represents the feature learnt by the pretrained model without the last layer, which is then passed to the last layer, denoted as $f_{\wmi{2}}$, for reward and token prediction. Assumption \ref{assum3} needs the token embeddings to be normalized so that they are located on a hypersphere  whose radius is  $t$.  This is to conveniently measure the similarity  among responses  in embedding feature, and   is very standard in analyzing token embeddings. Now we are ready to state our main results whose proof can be found in Appendix~\ref{sec:proof2}. 
 

\begin{theorem}
\label{thm1}
Let $\mathcal{L}_{\Pi}^{*} = \sup_{p\in\Pi} \mathcal{L}_{\Pi}$. Then it holds the convergence:  $\mathcal{L}_{\wm} \!\rightarrow \!\mathcal{L}_{\Pi}^{*}$ as $\gamma\!\rightarrow \!\infty$ where $\mathcal{L}_{\wm}$ is our HPS  loss. 
\end{theorem}
\vspace{-3pt}
Theorem \ref{thm1} establishes that when $\gamma\rightarrow \infty$, then our HPS training loss  $\mathcal{L}_{\wm} $ in~\eqref{eq:PLsfdas} would converge to  $\mathcal{L}_{\Pi}^{*}$  which is the loss under the  hardest  dispreferred  distribution. Since $\mathcal{L}_{\Pi}^{*}$ samples the hardest dispreferred   responses, optimizing $\mathcal{L}_{\Pi}^{*}$ encourages the model to identify the preferred response and   hardest dispreferred   responses, which is the desired training. This is because as discussed before,   the works in supervised, metric, and contrastive learning~\cite{schroff2015facenet, oh2016deep, contrastivehard} demonstrate that ``hard” examples—those closely resembling the correct output but still incorrect—are particularly useful for  learning. In our context, training the model to distinguish the preferred response from the hardest dispreferred response   enables it to reject less preferred responses  more effectively.

Furthermore, to examine the global minimizer of our HPS training loss $\mathcal{L}_{\wm}$, we analyze the optima of  the training loss $\mathcal{L}_{\Pi}^*$  in~\Cref{2player}, since we have proved their good approximation in  Theorem \ref{thm1}. Without loss of generality, when the number of dispreferred response samples $N=n-1\rightarrow\infty$,  we can remove the   $\log{N}$ from the HPS training loss $\mathcal{L}_{\wm}$ as it does not change the minimizers and the geometry of the loss surface, and obtain  a limiting objective:
%\begin{equation}
%\fontsize{9}{3}\selectfont{
%\begin{aligned}
%\label{eq:limit2player}
%&\tilde{\mathcal{L}}^{\infty}_{\text{RLHF}}\left(\wm, q'\right)=\expectation\limits_{y_{\tau(1)}\sim p^{+}}\left[-\log\left(\frac{e^{r_{\wm}\left(x,y_{\tau(1)}\right)}}{\expectation\limits_{y^{-}\sim q'}\left[e^{r_{\wm}\left(x,y\right)}\right]}\right)\right],
%\end{aligned}}
%\end{equation}
\begin{equation}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\label{eq:limit2player}
			\mathcal{L}_{\wm}^{\infty}=\expectation\limits_{d \sim \mathcal{D}}\left[-\log\left(\frac{e^{r_{\wm}\left(x,y_{\tau(1)}\right)}}{\expectation\nolimits_{y \sim p}\left[e^{r_{\wm}\left(x,y\right)}\right]}\right)\right].
	\end{aligned}}
\end{equation}
Now we are ready to give our results in Theorem~\ref{thm3} whose proof is in Appendix~\ref{sec:proof3}. 
\vspace{-1pt}
\begin{theorem}
	\label{thm3}
	Assume the ranking set $\tau$ is a finite set. Let $\mathcal{L}_{\wm}^{\infty,*} = \sup_{p\in\Pi}\mathcal{L}_{\wm}^{\infty}$ and $ \wms = \arg\min_{\wm} \mathcal{L}_{\wm}^{\infty,*} $.  Then $ \wms$ is also the solution to the following problem:
    \vspace{-5pt}
    \begin{equation}\label{afsafds}
		\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\wms =	\arg\max\nolimits_{\wm}\left(r_{\wm}\left(x,y_{\tau(1)}\right)-\max\nolimits_{1<j\leq |\tau|}r_{\wm}\left(x,y_{\tau(j)}\right)\right).
		\end{aligned}}
\end{equation}
\end{theorem}
\vspace{-10pt}
Theorem~\ref{thm3} implies that the minimizer $ \wms = \arg\min_{\wm} \mathcal{L}_{\wm}^{\infty,} $ is equivalent to  the one that maximizes the margin between the reward of the most preferred response, represented by $r_{\wm}\left(x,y_{\tau(1)}\right)$, and the reward of the hardest dispreferred responses, represented by $\max_{1<j\leq |\tau|}r_{\wm}\left(x,y_{\tau(j)}\right)$. So  the optimum  $\wms$ of our HPS loss aims to maximize the reward margin between the preferred  response and its closest dispreferred response.  This guarantees that the model learns a robust distinction between preferred and dispreferred responses, and enjoys  a better alignment performance with much less dispreferred or harmful generation.

%\begin{theorem}
%\label{thm3}
%Assume the ranking set $\tau$ is a finite set and let $\tilde{\mathcal{L}}^{\infty, *}_{\text{RLHF}}(\wm) = \sup\limits_{q\in\Pi}\tilde{\mathcal{L}}^{\infty}_{\text{RLHF}}\left(\wm, q\right)$. Under the relative mild condition that $f$ is Lipschitz continuous under fixed $x$, if the infimum $\inf\limits_{\wm\text{:}\phi_{\wm}\text{ is measurable}}\tilde{\mathcal{L}}^{\infty, *}_{\text{RLHF}}(\wm)$ is attained by $\wm^{*}$, letting $\Phi_{\tau(1)}=\phi_{\wm^{*}}(x,y_{\tau(1)})$, $\phi_{\wm^{*}}$ is characterized as being any solution to the following problem under an optimal
%parameter $\wm^{*}$,
%
%\begin{equation*}
%\fontsize{9}{3}\selectfont{
%\begin{aligned}
%\max\limits_{\Phi_{\tau(1)}\in\mathbb{S}^{d-1}/t}\min\limits_{1<j\leq |\tau|}\left(f\left(\Phi_{\tau(j)}\right)-f\left(\Phi_{\tau(1)}\right)\right),
%\end{aligned}}
%\end{equation*}
%\end{theorem}
%
%\begin{proof}
%The proof is given in the \Cref{sec:proof3}.
%\end{proof}


%\begin{paragraph}{Interpretation:}
%The theorem implies that the solution to the problem: $\min\limits_{\wm}\mathcal{L}_{\text{RLHF}}^{*}(\wm)$ aims at maximizing the margin between the rewards of the most preferred response, denoted by $\Phi_{\tau(1)}$, and the less preferred responses, denoted by $\Phi_{\tau(j)}$ for $j > 1$. The optimal parameter $\wm^{*}$ can achieve the maximum margin of the rewards of the most preferred response and other hard dispreferred responses. This ensures a robust separation in reward values, reinforcing the distinction between preferred and hard dispreferred responses to optimize the alignment.
%\end{paragraph}