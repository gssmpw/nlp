\section{Related Work}
\label{sec:rel}
%\subsection{Language Model Alignment}

Fine-tuning large language models (LLMs) to align with human preferences is a critical research challenge~\cite{stiennon2020learning,  ouyang2022training}. This task requires models to learn from contexts and corresponding responses scored by human annotators to replicate human  preferences.  

 
%Fine-tuning large language models (LLMs) to align with human preferences has emerged as a critical research problem~\cite{stiennon2020learning, christiano2017deep, ouyang2022training}. This task involves providing a context and corresponding responses scored by human annotators, requiring the model to learn and replicate human-like preferences.

%Reinforcement Learning (RL) is a common approach to achieve this alignment. In this framework, known as Reinforcement Learning from Human Feedback (RLHF), an agent receives supervision signals from reward models that act as human proxies. The agent is then iteratively refined through numerous trials~\cite{rlhfpipeline, ouyang2022training, dai2023safe, christiano2017deep, stiennon2020learning, principled, lee2021pebble, nakano2021webgpt, snell2022offline}. This cyclic process often leads to continuous performance improvements. Notably, LLMs like ChatGPT~\cite{gpt4, llama3} have successfully leveraged this approach.

Reinforcement Learning from Human Feedback (RLHF) is a common approach, where an agent iteratively refines itself using supervision signals from reward models acting as human proxies~\cite{rlhfpipeline, ouyang2022training, dai2023safe, christiano2017deep, stiennon2020learning, principled, lee2021pebble, nakano2021webgpt, snell2022offline}. This cyclic process has led to continuous performance improvements, enabling LLMs like ChatGPT~\cite{gpt4, llama3} to excel.  

 

However, RLHF’s on-policy nature introduces challenges. It requires learning a reward model from data as a preliminary step, leading to a complex two-stage optimization process. Recent advancements in preference alignment techniques have sought to simplify this process by enabling direct alignment through a single loss function~\cite{dpo, rdpo, ipo, sppo, simpo, kto, exo, nca, pro}. While these techniques streamline optimization, they face limitations such as poor handling of harmful content, inefficient utilization of dispreferred responses, and high computational costs.

%To overcome these limitations, we propose HPS, a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while explicitly rejecting all dispreferred and harmful ones. By dynamically emphasizing “hard” dispreferred responses-those closely resembling preferred ones—HPS enhances the model’s rejection capabilities. Additionally, the use of a single-sample Monte Carlo strategy reduces computational overhead while maintaining high alignment quality.

To address these limitations, we propose HPS, a novel framework for robust and efficient human preference alignment. HPS prioritizes the most preferred response while explicitly rejecting dispreferred and harmful ones. By emphasizing ``hard” dispreferred responses—those closely resembling preferred ones—it improves rejection capabilities. Additionally, a single-sample Monte Carlo strategy reduces computational overhead while maintaining strong alignment quality.  
