\section{Preliminaries}
\label{sec:preliminaries}

%Here we review the three phases in human preference alignment methods. 
Alignment methods typically contain three phases below. 

\noindent{\textbf{Supervised Fine-Tuning (SFT).}}
This phase fine-tunes a pretrained LLM on a labeled dataset, producing $\pi_{\text{SFT}}$, a model that achieves a strong baseline.% performance by leveraging labeled data effectively.

%\noindent{\textbf{Supervised Fine-Tuning (SFT).}} 
%This  phase involves fine-tuning a pretrained LLM using a labeled dataset under a supervised learning framework. The result is a model, denoted as $\pi_{\text{SFT}}$, that achieves robust baseline performance by effectively leveraging the labeled data.

%\noindent{\textbf{Preference Modeling (PM).}}  
%This phase focuses on constructing a model capable of evaluating text sequences and returning a scalar reward that numerically represents human preference. The process begins by inputting a given prompt $x$ into the supervised fine-tuned model $\pi_{\text{SFT}}$, which generates $n$ candidate responses $\{y_{i}\}_{i=1}^n$.

\noindent{\textbf{Preference Modeling (PM).}}  
This phase builds a model to evaluate text sequences and assign scalar rewards reflecting human preference. Given a prompt \( x \), the supervised fine-tuned model \( \pi_{\text{SFT}} \) generates \( n \) candidate responses \( \{y_{i}\}_{i=1}^n \). 
A common approach involves human labelers ranking responses to produce an ordering \( \tau \):  
\begin{equation}\label{ranking}
	y_{\tau(1)} \succ y_{\tau(2)} \succ \cdots \succ y_{\tau(n)},  
\end{equation}
where \( y \succ y' \) indicates \( y \) is preferred over \( y' \). But ranking becomes challenging as \( n \) increases~\cite{lambert2022illustrating}.


%To approximate human preferences, one common approach is to have human labelers manually assign scalar scores and rank the responses to produce a preference ordering $\tau$, represented as: 
%\begin{equation}\label{ranking}
%y_{\tau(1)} \succ y_{\tau(2)} \succ \cdots \succ y_{\tau(n)},  
%\end{equation}
%where $y \succ y'$ indicates that response $y$ is preferred over $y'$. However, this ranking task becomes challenging as the number of responses increases in practice~\cite{lambert2022illustrating}.

% Another approach involves modeling the preference ranking using probabilistic models. While the optimal human preference policy is an inaccessible reward model $r^*(x, y)$, it is often approximated by established ranking models like the Bradley-Terry (BT) model~\cite{BT} or the more general Plackett-Luce (PL) model~\cite{PL1, PL2}. Specifically, the human preference distribution $p^*$ is modeled under the PL framework:  
% \begin{equation}
% 	\label{eq:PL_distribution}
% 	\fontsize{9}{3}\selectfont{
% 		\begin{aligned}
% 			p_{\text{PL}}^*(y_{\tau(1)} \! \succ\!  y_{\tau(2)} \! \succ\!  \ldots \! \succ\!  y_{\tau(n)}  | x )\!=\! \prod\limits_{j=1}^{n}\!\frac{e^{r^*(x,y_{\tau(j)})}}{\sum\nolimits_{k=j}^{n}\!e^{r^{*}\left(x,y_{\tau(k)}\right)}}.
% 	\end{aligned}}
% \end{equation}


 This  preference ranking can be modeled probabilistically. While the ideal reward function \( r^*(x, y) \) is inaccessible, it is often estimated  by models like Bradley-Terry (BT)~\cite{BT} or Plackett-Luce (PL)~\cite{PL1, PL2}. Under PL, the preference distribution is:  
\begin{equation}
	\label{eq:PL_distribution}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			p_{\text{PL}}^*(y_{\tau(1)} \! \succ\!  \dots \! \succ\!  y_{\tau(n)}  | x )\!=\! \prod\limits_{j=1}^{n}\!\frac{e^{r^*(x,y_{\tau(j)})}}{\sum\nolimits_{k=j}^{n}\!e^{r^{*}\left(x,y_{\tau(k)}\right)}}.
	\end{aligned}}
\end{equation}
When $n = 2$, \Cref{eq:PL_distribution} degenerates  to the BT model. 

%\begin{equation}
%    \label{eq:BT_distribution}
%    \fontsize{9}{3}\selectfont{
%    \begin{aligned}
%        p_{\text{BT}}^*(y_{\tau(1)} \succ  y_{\tau(2)} \ |\ x )
%        &=\sigma\left(r^*(x,y_{\tau(1)})-r^*(x,y_{\tau(2)})\right),
%    \end{aligned}}
%\end{equation}
%where $\sigma(z)=\frac{1}{1+e^{-z}}$ is a logistic function. 

Finally, by sampling  from  the preference model, one can construct a prompt-response dataset $\mathcal{D}=\{d_i\}_{i=1}^m$, where each instance $d_{i}=(x_{i}, y_{\tau_{i}{(1)}}, y_{\tau_{i}{(2)}}, \cdots, $ $y_{\tau_{i}{(n)}})$ contains one prompt $x_{i}$ and the   ranked responses $\{y_{\tau_i(k)}\}_{k=1}^n$. 
 
 
 
 
%\noindent{\textbf{Preference Fine-Tuning (PFT).}}  
%The final phase focuses on further aligning the language model with human preferences using the prompt-response dataset $\mathcal{D}$. This can be achieved using both explicit and implicit reward  methods. For explicit  reward  methods, reinforcement Learning from Human Feedback (RLHF) is widely used. RLHF explicitly trains  a reward model $r_{\mathbf{\wm}}$ parameterized by $\mathbf{\wm}$ to learn the response ranking  in dataset $\mathcal{D}$.  Then it uses policy-gradient algorithms like PPO~\cite{ppo} to fine-tune LLM $\pi_{\text{SFT}}$ for generating responses with higher preference. See more details in \zp{xx work.} 


\noindent{\textbf{Preference Fine-Tuning (PFT).}}  
This phase further aligns the language model with human preferences using the dataset \( \mathcal{D} \), employing explicit or implicit reward methods.  

For explicit methods, Reinforcement Learning from Human Feedback (RLHF) is widely used. RLHF trains a reward model \( r_{\mathbf{\wm}} \) to learn response rankings in \( \mathcal{D} \), then fine-tunes  LLM \( \pi_{\text{SFT}} \) using policy-gradient algorithms like PPO~\cite{ppo} to generate higher-preference responses. Refer to previous works~\cite{rlhfpipeline, ouyang2022training} for further details.

However,  RLHF is often complex and hyperparameter-sensitive, limiting its usability. Implicit reward methods like  DPO~\cite{dpo}  offer a simpler alternative by directly parameterizing the reward function:
\begin{equation}  
	\fontsize{9}{3}\selectfont{
		\label{dporm}
		\begin{aligned}
			r_{\mathbf{\wm}}(x, y) = \beta \log \frac{\pi_\wm(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x), 
	\end{aligned}}
\end{equation}  
where \( \pi_\wm \) is the policy model, \( \pi_{\text{ref}} \) is the reference policy, \( \beta \) is a scaling factor, and \( Z(x) \) is the partition function. Additional implicit reward parametrizations are discussed in \Cref{sec:appendixa}, including KTO~\cite{kto} and SimPo~\cite{simpo}. The KTO reward is given by: $r_{\text{KTO}}(x,y)$$=$$l(y) \log \frac{\pi_{\wm}(y|x)}{\pi_{\text{ref}}(y|x)}$, where \( l(y)\in\mathbb{R}^+ \) is a normalizing factor, and SimPo reward is defined as: $r_{\text{SimPO}}(x,y)=\frac{\beta}{|y|}\log \pi_{\wm}(y|x)=\frac{\beta}{|y|}\sum\limits^{|y|}_{i=1}\log \pi_{\wm}(y_{i}|x,y_{<i}),$ where $|y|$ is the length of the response $y$ and $y_{<i}$ is the set of tokens in the sentence $y$ before the token $y_{i}$. By incorporating the reward into the PL model, one can derive the corresponding training  loss:
\begin{equation}
\vspace{-1.5pt}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\label{eq:dpoPL}
			\mathcal{L}_{\text{PL}}=\expectation_{d\sim\mathcal{D}}  \sum\nolimits_{j=1}^{n} \mathcal{L}_{j}(d),
	\end{aligned}}
\end{equation}
where
\begin{equation} 
\vspace{-0.5pt}
	\fontsize{9}{3}\selectfont{
		\label{eq:PL}
		\begin{aligned}
			\mathcal{L}_{j}(d) \!= \!-\log\!\Big({e^{r_{\wm} (x,y_{\tau(j)}} / \sum\nolimits_{k=j}^{n}e^{r_{\wm} (x,y_{\tau(k)}})}\Big).
	\end{aligned}}
\end{equation}
%Here $\mathcal{L}_{j}(d)$ encourages the model to predict the preferred response $y_{\tau\left( j\right)}$ over a set of dispreferred responses $\{y_{\tau\left(k\right)}\}_{k=j+1}^{n}$. 
%When $n=2$, \Cref{eq:PL}   degenerates to the loss of the  BT model. Moreover, for multiple dispreferred responses,  BT  selects the most preferred and most  dispreferred as pair to  construct the loss. See  details in \Cref{sec:btloss}. 
 Here, \( \mathcal{L}_{j}(d) \) encourages predicting the preferred response \( y_{\tau(j)} \) over more  dispreferred ones $\{y_{\tau\left(k\right)}\}_{k=j+1}^{n}$. For \( n=2 \), \Cref{eq:PL} reduces to the BT loss. Moreover, when multiple dispreferred responses exist, BT selects the most and least preferred to construct loss. See \Cref{sec:btloss} for details.
 
%
%\noindent{\textbf{Preference Fine-Tuning (PFT).}}  
%This phase further aligns the language model with human preferences using the dataset \( \mathcal{D} \), employing explicit or implicit reward methods.  
%
%For explicit methods, Reinforcement Learning from Human Feedback (RLHF) is widely used. RLHF trains a reward model \( r_{\mathbf{\wm}} \) to learn response rankings in \( \mathcal{D} \), then fine-tunes the LLM \( \pi_{\text{SFT}} \) using policy-gradient algorithms like PPO~\cite{ppo} to generate higher-preference responses. See \zp{xx work} for details.  
%
%However, RLHF is complex and hyperparameter-sensitive, limiting its usability. Implicit reward methods, such as Direct Preference Optimization (DPO)~\cite{dpo}, offer a simpler alternative by directly parameterizing the reward function:
%\begin{equation}  
%	\fontsize{9}{3}\selectfont{
%		\label{dporm}
%		\begin{aligned}
%			r_{\mathbf{\wm}}(x, y) = \beta \log \frac{\pi_\wm(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x), 
%	\end{aligned}}
%\end{equation}  
%where \( \pi_\wm \) is the policy model, \( \pi_{\text{ref}} \) is the reference policy, \( \beta \) is a scaling factor, and \( Z(x) \) is the partition function. By incorporating this reward into the BT or PL ranking framework, DPO replaces reinforcement learning with a supervised objective:
%\begin{equation}
%	\fontsize{9}{3}\selectfont{
%		\begin{aligned}
%			\label{eq:dpoPL}
%			\mathcal{L}_{\text{DPO-PL}}=\expectation_{d\sim\mathcal{D}}  \sum\nolimits_{j=1}^{n} \mathcal{L}_{j}(d),
%	\end{aligned}}
%\end{equation}
%where
%\begin{equation} 
%	\fontsize{9}{3}\selectfont{
%		\label{eq:PL}
%		\begin{aligned}
%			\mathcal{L}_{j}(d) \!= \!-\log\!\Big({e^{r_{\wm} (x,y_{\tau(j)})} / \sum\limits_{k=j}^{n}e^{r_{\wm} (x,y_{\tau(k)})}}\Big).
%	\end{aligned}}
%\end{equation}
%Here, \( \mathcal{L}_{j}(d) \) encourages predicting the preferred response \( y_{\tau(j)} \) over less preferred ones. For \( n=2 \), \Cref{eq:PL} reduces to the BT loss. When multiple dispreferred responses exist, BT selects the most and least preferred for loss computation (see \Cref{sec:btloss} for details).

 

%
%
%\noindent{\textbf{Preference Fine-Tuning (PFT).}}  
%The final phase focuses on further aligning the language model with human preferences using the prompt-response dataset $\mathcal{D}$. This can be achieved using both explicit and implicit reward-based methods.
%
%1) \noindent{\textbf{Explicit Reward Methods.}} Reinforcement Learning from Human Feedback (RLHF) is a common technique to optimize $\pi_{\text{SFT}}$ by leveraging supervision from a reward model and a reference policy, typically implemented as $\pi_{\text{SFT}}$.
%
%To begin, a reward model $r_{\mathbf{\wm}}$ parameterized by $\mathbf{\wm}$ can be trained as a classifier to learn the ranking of responses. The objective function minimizes the cross-entropy loss~\cite{dpo, pro, preferencecollapse}:  
%\begin{equation}
%	\fontsize{9}{3}\selectfont{
%		\begin{aligned}
%			\label{eq:PL}
%			\mathcal{L}_{\text{PLRM}}=\expectation_{d\sim\mathcal{D}}  \sum\nolimits_{j=1}^{n} \mathcal{L}_{j}(d),
%	\end{aligned}}
%\end{equation}
%where
%\begin{equation} 
%	\fontsize{9}{3}\selectfont{
%		\label{eq:PL2}
%		\begin{aligned}
%			\mathcal{L}_{j}(d) \!= \!-\log\!\Big({e^{r_{\wm} (x,y_{\tau(j)}} / \sum\limits_{k=j}^{n}e^{r_{\wm} (x,y_{\tau(k)}})}\Big).
%	\end{aligned}}
%\end{equation}
%Here $\mathcal{L}_{j}(d)$ encourages the model to predict the preferred response $y_{\tau\left( j\right)}$ over a set of dispreferred responses $\{y_{\tau\left(k\right)}\}_{k=j+1}^{n}$. 
%When $n=2$, \Cref{eq:PL}   degenerates to the loss of the  BT model. Moreover, for multiple dispreferred responses,  BT  selects the most preferred and most  dispreferred as pair to  construct the loss. See  details in \Cref{sec:btloss}. 
%
%Finally, using policy-gradient algorithms such as Proximal Policy Optimization (PPO)~\cite{ppo}, the language model $\pi_{\text{SFT}}$ is optimized to maximize rewards provided by the learned reward model in the reinforcement learning framework.
%
%2) \noindent{\textbf{Implicit Eeward Methods.}} 
%RLHF has been criticized for its complexity and sensitivity to hyperparameters, making it less practical in certain scenarios. Implicit reward-based approaches offer a simpler alternative by directly parameterizing the reward function rather than relying on a separate reward model.
%
%A notable example is Direct Preference Optimization (DPO)~\cite{dpo}, which reformulates the reward function $r_{\mathbf{\wm}}$ in closed form:: 
%\begin{equation}  
%	\fontsize{9}{3}\selectfont{
%		\label{dporm}
%		\begin{aligned}
%			r_{\text{DPO}}(x, y) = \beta \log \frac{\pi_\wm(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x), 
%	\end{aligned}}
%\end{equation}  
%where $\pi_\wm$ is the policy model, $\pi_{\text{ref}}$ is the reference policy, $\beta$ is a scaling hyperparameter, and $Z(x)$ is the partition function. By integrating this reward expression into the BT or PL ranking framework, DPO replaces reinforcement learning with a supervised training objective for the language model $\pi_{\text{SFT}}$:
%\begin{equation}
%	\fontsize{9}{3}\selectfont{
%		\begin{aligned}
%			\label{eq:dpoPL}
%			\mathcal{L}_{\text{DPO-PL}}=\expectation_{d\sim\mathcal{D}}  \sum\nolimits_{j=1}^{n} \mathcal{L}_{j}(d),
%	\end{aligned}}
%\end{equation}
%where
%\begin{equation} 
%	\fontsize{9}{3}\selectfont{
%		\label{eq:dpoPL2}
%		\begin{aligned}
%			\mathcal{L}_{j}(d) \!= \!-\log\!\Big({e^{\beta \log \frac{\pi_\wm(y_{\tau(j)} \mid x)}{\pi_{\text{ref}}(y_{\tau(j)} \mid x)}} / \sum\limits_{k=j}^{n}e^{\beta \log \frac{\pi_\wm(y_{\tau(k)} \mid x)}{\pi_{\text{ref}}(y_{\tau(k)} \mid x)}})}\Big).
%	\end{aligned}}
%\end{equation}

% \zp{ I have polished this section, and simplified a lot of notations. So please check carefully all notations to change them in the following sections and appendix. }

% \section{Preliminaries}
% \label{sec:preliminaries}

% %Here we first introduce the notations used through the whole paper, and then review the RLHF pipeline for human preference alignment.  

% % \noindent{\textbf{Notations.}} %  \label{sec:notation} 
% % We use $[n]$ to denote the set   $\{1, 2, \cdots, n\}$, and adopt $|\mathcal{S}|$ to denote the  cardinality of a set $\mathcal{S}$. Moreover,  we respectively define  $\|x\|_{\Sigma} = \sqrt{x^{T}\Sigma x}$ and $\|x\|_{2} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ as a semi-norm and $\ell_2$-norm of $x$.   We say $\Sigma \succeq \Sigma'$ if $\Sigma-\Sigma'$ is positive semidefinite. \zp{For this  work, the notation is simple, and it is not necessary to summarize the notations here. You can define it when your first use it.  }


% % \zp{please replace $\Sigma$ in $\|x\|_{\Sigma}$ with other notations like $\Phi$ due to the high similarity  $\Sigma$ and the sum $\Sigma$. Please remember to revise all notations. }
% % \zp{We use $\mathcal{O}$ and $\mathcal{L}$ to denote the computational complexity and the loss function, respectively.  The notation $\mathcal{O}$  seems to be not used and  $\mathcal{L}$  is explicitly defined. Thus both are not needed to define. }
 
% %
% %\subsection{Notations}
% %\label{sec:notation}
% %We use $\mathcal{O}$ and $\mathcal{L}$ to denote the computational complexity and the loss function, respectively. Given a set $\mathcal{A}$, we write $|\mathcal{A}|$ to represent the cardinality of $\mathcal{A}$. We use $[n]$ to denote the set of integers from $1$ to $n$. $\|x\|_{2} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ represents the L-$2$ norm. We write $\|x\|_{\Sigma}$ as a semi-norm of $x$ when $\Sigma$ is some positive-semidefinite matrix. We write $\Sigma \succeq \Sigma'$ if $\Sigma-\Sigma'$ is positive semidefinite.

% Here  we  review  RLHF for human preference alignment which includes the following three phases. 
% %, including 1) supervised fine-tuning (SFT), 2) reward model training (RMT), and 3) reinforcement learning fine-tuning.   

% \noindent{\textbf{Supervised fine-tuning (SFT).}}  It fine-tunes a pretrained LLM on a labeld dataset in a supervised manner, and obtains a model $\pi_{\text{SFT}}$. 

%  \noindent{\textbf{Reward model training (RMT).}}  In this phase, one first uses a  prompt  $x$ to let $\pi_{\text{SFT}}$ generate $n$ responses $y_{1}, y_{2}, \cdots, y_{n}$. Then, human labeler will manually rank these $n$ responses according to their preference ranking $\tau$, denoted by $y_{\tau(1)} \succ  y_{\tau(2)} \succ \cdots \succ y_{\tau(n)}$  where $y_{\tau(1)}$ and $y_{\tau(2)}, \cdots, y_{\tau(n)}$ respectively denote the most preferred response and the set of dispreferred responses. For brevity, we assume the optimal human preference policy is a inaccessible reward model $r(x,y)$ which can be modeled by many frameworks, like the popular Bradley-Terry (BT) model~\cite{BT} and the more general Plackett-Luce (PL) ranking models~\cite{PL1, PL2}. In practice, to predict these preferences, previous works employ the PL model which defines the human preference distribution $p^*$ as:
% \begin{small}
% \begin{equation}
% \label{eq:PL_distribution}
% \begin{aligned}
% p_{\text{PL}}^*(y_{\tau(1)} \succ  y_{\tau(2)} \succ \cdots \succ y_{\tau(n)} \ |\ x )&= \prod\limits_{j=1}^{n-1}\frac{\exp\left(r^*(x,y_{\tau(j)})\right)}{\sum\limits_{k=1}^{n}\exp\left(r^{*}\left(x,y_{\tau(k)}\right)\right)}.
% \end{aligned}
% \end{equation}
% \end{small}
% Notice that when $n = 2$, \Cref{eq:PL_distribution} reduces to the Bradley-Terry model:
% \begin{small}
% \begin{equation}
% \label{eq:BT_distribution}
% \begin{aligned}
% p_{\text{BT}}^*(y_{\tau(1)} \succ  y_{\tau(2)} \ |\ x )&= \frac{\exp\left(r^*(x,y_{\tau(1)})\right)}{\exp\left(r^*(x,y_{\tau(1)})\right)+\exp\left(r^*(x,y_{\tau(2)})\right)} \\
% &=\sigma\left(r^*(x,y_{\tau(1)})-r^*(x,y_{\tau(2)})\right),
% \end{aligned}
% \end{equation}
% \end{small}
% where $\sigma(z)=\frac{1}{1+\exp(-z)}$ is a logistic function. 
 
% Then sampling samples from  $p_{\text{PL}}^*$, one can construct a dataset $\mathcal{S}=\{(x^{i}, y^{i}_{\tau^{i}{(1)}}, y^{i}_{\tau^{i}{(2)}}, \cdots, y^{i}_{\tau^{i}{(n)}})\}_{i=1}^m$, where each instance $s^{i}$ consists of one prompt $x^{i}$ and $n$ responses $y^{i}_{\tau^{i}{(1)}}, y^{i}_{\tau^{i}{(2)}}, \cdots, y^{i}_{\tau^{i}{(n)}}$ followed the user-specified ranking. Using this dataset, we can train a reward model $r_{\wm}$ parameterized by $\wm$ by approaching the task as a classification problem. Specifically, we frame the training objective as minimizing the negative log-likelihood loss:
% \begin{small}
% \begin{equation}
% \begin{aligned}
% \label{eq:PL}
% \mathcal{L}_{\text{PLRM}}&= -\expectation_{s^{i}\sim\mathcal{S}}\left[\log\left(\prod\limits_{j=1}^{n-1}\frac{\exp\left(r_{\wm}(x,y_{\tau^{i}(j)})\right)}{\sum\limits_{k=1}^{n}\exp\left(r_{\wm}\left(x,y_{\tau^{i}(k)}\right)\right)}\right)\right] \\
% &=-\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n-1}\log\left(\frac{\exp\left(r_{\wm}\left(x^{i},y^{i}_{\tau^{i}(j)}\right)\right)}{\sum\limits_{k=1}^{n}\exp\left(r_{\wm}\left(x^{i},y^{i}_{\tau^{i}(k)}\right)\right)}\right),
% \end{aligned}
% \end{equation}
% \end{small}
% where $s^{i}=(x^{i}, y^{i}_{\tau^{i}(1)}, y^{i}_{\tau^{i}(2)}, \cdots, y^{i}_{\tau^{i}(n)})$.
% The loss function for BT model can be found in \Cref{sec:btloss}.
 
% \noindent{\textbf{Reinforcement learning fine-tuning (RLFT).}} In this phase,  $\pi_{\text{SFT}}$ is further optimized in a trial-and-error process containing repetitive sampling from linguistic space and corresponding feedback simultaneously from the reward model and the reference policy.

% Instead of learning an explicit reward model with unstable reinforcement learning, DPO~\cite{dpo} parameterizes the reward function $r_{\wm}$ in \Cref{eq:PL} using a closed-form:
% \begin{equation}
%     r_{\text{DPO}}(x,y) = \beta \log\frac{\pi_{\wm}(y|x)}{\pi_{\text{ref}}(y|x)}+\beta\log Z(x),
% \end{equation}
% where $\pi_{\wm}$ is the policy model, and $\pi_{\text{ref}}$ is the reference policy, typically the SFT model, $\beta$ is a scale hyperparameter, and $Z(x)$ is the partition function.

% % \subsection{RLHF}
% % \label{sec:rlhf}
% % We first review the RLHF pipeline~\cite{rlhfpipeline}. It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization. 

% % \begin{itemize}
% %     \item Supervised Fine-tuning (SFT) where labelers furnish the desired behavior’s response with tokens, denoted as $y$, for a given prompt, denoted as $x$. Subsequently, The policy LLM goes through direct fine-tuning (maximum likelihood) on this data, resulting in a model denoted as $\pi_{\text{SFT}}$.
% %     \item Reward Model (RM) Training where $\pi_{\text{SFT}}$ is utilized by prompts $x$ to generate pairs of responses, which are then denoted by human labelers as a more favored answer $y_{\tau(1)}$ against the other one $y_{\tau(2)}$, \ie $y_{\tau(1)} \succ y_{\tau(2)}|x$. The preferences are assumed to be generated by some latent reward model $r(x,y)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) model~\cite{BT} being a popular choice (although more general Plackett-Luce (PL) ranking models~\cite{PL1, PL2} are also compatible with the framework if we have access to several ranked answers). To predict these preferences, previous works often employ the BT RM, which essentially constructs a pairwise contrast:
% %     \begin{small}
% %     \begin{equation}
% %         \label{eq:BT}
% %         \mathcal{L}_{\text{RM}}=-\log\frac{\exp\left(r\left(x,y_{\tau(1)}\right)\right)}{\exp\left(r\left(x,y_{\tau(1)}\right)\right)+\exp\left(r\left(x,y_{\tau(2)}\right)\right)}
% %     \end{equation}
% %     \end{small}
% %     \item Reinforcement Learning (RL) stage where $\pi_{\text{SFT}}$ is further optimized in a trial-and-error process containing repetitive sampling from linguistic space and corresponding feedback simultaneously from the RM and reference policy.
% % \end{itemize}

% % We re-examine the objective of the Bradley-Terry RM, which helps LLMs understand $y_{\tau(1)} \succ y_{\tau(2)}$ through score contrast. The RM is trained in supervised settings and is expected to capture human preference. For a given prompt $x$ and two responses $y_{\tau(1)} \succ y_{\tau(2)}$, the RM should prefer $y_{\tau(1)}$. To directly optimize the policy model, \ie the LLM, we can similarly transfer the pair-wise contrast to it~\cite{dpo, simpo}. In this way, the LLM $\pi_{\wm}$ is considered as both RM and policy network, denoted as $r_{\wm}$ :

% % \begin{small}
% %     \begin{equation}
% %         \label{eq:BT2}
% %         \mathcal{L}_{\text{BTRM}}=-\log\frac{\exp\left(r_{\wm}\left(x,y_{\tau(1)}\right)\right)}{\exp\left(r_{\wm}\left(x,y_{\tau(1)}\right)\right)+\exp\left(r_{\wm}\left(x,y_{\tau(2)}\right)\right)}
% %     \end{equation}
% % \end{small}

% % \textit{As $n$ approaches infinity, the recipient LLM is exposed to more and more samples with scores, and should continuously become perfectly aligned with human preferences}~\cite{pro}. Consider conditioning on a fixed prompt $x \in \mathcal{X}$, we have a set of $n$ responses $\{y_{j}\}_{j=1}^{n}$. In the recursive contrasts can be imposed to exploit multi-positional patterns, which start with the first response, treat the remaining responses as negatives, drop the current response, and move to the next. This procedure repeats until there are no candidates left. Consequently, the further extension to \Cref{eq:BT2} is the PL RM as follows:
% % \begin{small}
% %     \begin{equation}
% %         \label{eq:PL}
% %         \mathcal{L}_{\text{PLRM}}=-\sum\limits_{j=1}^{n-1}\log\left(\frac{\exp\left(r_{\wm}\left(x,y_{\tau(j)}\right)\right)}{\sum\limits_{k=1}^{n}\exp\left(r_{\wm}\left(x,y_{\tau(k)}\right)\right)}\right)
% %     \end{equation}
% % \end{small}