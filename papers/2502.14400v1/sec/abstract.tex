\begin{abstract}
Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While Plackett-Luce (PL)-based methods have shown promise, they struggle with poor handling of harmful content, inefficient use of dispreferred responses, and high computational costs. To address these issues, we propose Dynamic Preference Sampling (DPS), a novel framework for robust and efficient human preference alignment.  DPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It dynamically emphasizes “hard” dispreferred responses—those closely resembling preferred ones—to enhance the model’s rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, DPS reduces computational overhead while maintaining alignment quality.  Theoretically, DPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate DPS’s effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.	
% Large language models (LLMs) frequently produce misleading content, highlighting the need to align them with human values for safer and more reliable AI systems. Reinforcement learning from human feedback (RLHF) has been used to achieve this alignment by guiding LLMs to prefer the positive sample over negative samples. However, existing RLHF methods often have the following limitations: (1) They typically rely on pairwise comparisons, which, despite extensive trial and error, lack broader context and contrastive learning from a macro perspective. (2) They lack comprehensive analysis of the negative/dispreferred samples used in RLHF, resulting in significant insights remaining underexplored. To address these limitations, we propose a novel RLHF framework: Reinforcement learning from human feedback with hard negatives (RLHF-HN), enabling LLMs to select hard dispreferred samples with adjustable difficulty levels, controlled by the user. 
% We demonstrate that our approach theoretically improves the sample efficiency by deriving a maximum likelihood estimator (MLE), $\hat\theta_{\text{HN}}$, based on our proposed RLHF-HN method, comparing to the conventional MLE, $\hat\theta_{\text{PLRM}}$, from the Plackett-Luce (PL) loss.
% In addition, we make the connection of between RLHF-HN and contrastive learning. 
% In a limiting scenario, our RLHF-HN objective approaches an inner solution to a zero-sum two player game, where the model aims to minimize the supremum of the RLHF loss.
% Furthermore, we illustrate that the optimal embedding solution aligns with a ball-packing problem, where samples from different ranking classes are arranged within a hypersphere to maximize separation between them.
% Finally, our experimental results show that RLHF-HN significantly outperforms standard RLHF baselines, with minimal code changes and no added computational cost.
\end{abstract}