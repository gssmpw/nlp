\vspace{-6pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-3pt}
Ensuring LLMs align with human preferences is crucial for building safe and controllable AI systems. We introduce Hard Preference Sampling (HPS), a novel framework that improves preference alignment by prioritizing the most preferred responses while effectively rejecting harmful and dispreferred ones. HPS enhances rejection capabilities by emphasizing ``hard” dispreferred responses and employs a single-sample Monte Carlo strategy to reduce computational costs. Theoretically, it improves sample efficiency and maximizes reward margins, ensuring clearer distinctions between preferred and dispreferred responses. Experiments on HH-RLHF and PKU-Safety datasets demonstrate HPS’s effectiveness, achieving strong BLEU and reward scores while significantly reducing harmful content generation.

\noindent{\textbf{Limitations.}} 
%Due to our limited burget, in the experiments, we are only affordable to use open-source LLMs like to estimate the win rate. More powerful instruct LLMs, such as GPT-4~\cite{gpt4} and Claude 3~\cite{claude}, may offer a more accurate and robust evaluation. 
Due to budget constraints, our experiments rely on open-source LLMs to estimate the win rate. More powerful instruct LLMs, such as GPT-4~\cite{gpt4} and Claude 3~\cite{claude}, may offer more accurate and robust evaluations and will be considered when additional resources become available.

\section*{Impact Statement}
The data used in this research may include sensitive or potentially offensive content, intended solely for academic and scientific purposes. The opinions expressed within this data do not represent the views of the authors. We remain committed to fostering the development of AI technologies which align with ethical standards and reflect societal values.
 
 %
%In this work, we introduce DPS, a novel and efficient framework designed to address key limitations in the human preference alignment problem. DPS introduces a training loss that prioritizes the most preferred responses while actively rejecting dispreferred and harmful ones. It dynamically focuses on "hard" dispreferred responses--those that are close to preferred ones--to enhance the model’s ability to reject undesirable outputs. By leveraging a single-sample Monte Carlo sampling strategy, DPS reduces computational overhead while preserving alignment quality.
%Moreover, we provide theoretical guarantees that DPS can improve sample complexity, reducing the amount of data required to achieve optimal preference alignment, and simultaneously maximizing the reward margin--the gap between the most preferred and dispreferred responses.
%Our experimental results demonstrate the effectiveness of DPS, consistently outperforming state-of-the-art methods in both fine-tuning and transfer learning scenarios. Specifically, DPS maintains competitive BLEU and reward scores, while it yields significant improvements in reward margins, reducing malicious content generation.