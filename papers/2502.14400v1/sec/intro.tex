\section{Introduction}
\label{sec:intro}
 Large Language Models (LLMs)~\cite{gpt4,llama,palm,chatglm} have demonstrated exceptional capabilities across diverse user applications by leveraging the extensive global knowledge and behavioral patterns embedded in their massive pretraining corpora. However, the presence of misleading, toxic, and harmful content in these corpora poses significant risks, as LLMs can inadvertently propagate undesirable information~\cite{bai2022constitutional, yao2024survey}. Consequently, selecting and aligning the model's responses and behaviors with desired human values is crucial to developing safe, effective, and controllable AI systems~\cite{christiano2017deep, stiennon2020learning, ouyang2022training, dai2023safe}.

To achieve this alignment, several human preference alignment methods have been proposed. For example, Reinforcement Learning from Human Feedback (RLHF)~\cite{ppo, christiano2017deep} optimizes LLMs by training a reward model on human preference rankings and maximizing the reward of generated outputs. Recognizing the complexity and sensitivity of RLHF, recent works, e.g., Direct Preference Optimization (DPO)~\cite{dpo}, Identity Preference Optimization (IPO)~\cite{ipo} and Self-Play Preference Optimization (SPPO)~\cite{sppo}, bypass the reward model by directly optimizing preferences, and have shown promising performance.
 
Despite their successes, existing methods for preference alignment often rely on underlying ranking models, such as the Plackett-Luce (PL) model~\cite{PL1, PL2} or its simplified counterpart, the Bradley-Terry (BT) model~\cite{BT}. The PL model ranks multiple responses to a prompt to align with human preferences, while the BT model focuses on pairwise comparisons. These models enable the derivation of training losses for alignment tasks. However, both PL- and BT-induced losses exhibit critical shortcomings when handling harmful responses.



Firstly, both PL- and BT-based losses fail to handle harmful responses effectively. The PL loss (e.g., DPO~\cite{dpo} and PRO~\cite{pro}) encourages ranking less harmful responses above more malicious ones, inadvertently treating harmful outputs as ``preferred" alternatives. This compromises the model's ability to robustly reject inappropriate or offensive content—essential in tasks requiring strict safeguards. The BT loss (e.g., DPO~\cite{dpo}, R-DPO~\cite{rdpo}, Online DPO~\cite{onlinerlhf}, and KTO~\cite{kto}) focuses only on rejecting the most dispreferred response in a pair, leaving other problematic responses unaddressed. 
Secondly, these losses overlook nuanced differences among dispreferred responses. The PL loss treats all dispreferred responses equally, ignoring their varying informativeness, which could guide better alignment learning. Similarly, the BT loss reduces rankings to pairwise comparisons, discarding macro-level distinctions that are crucial for capturing nuanced preferences~\cite{sun2024rethinking, pro}.  Finally, computational inefficiency poses a significant challenge. Training with the PL loss requires processing and backpropagating through all responses in a ranked set, leading to substantial memory and computational overhead—especially for long prompts or responses~\cite{oosterhuis2021computationally, maystre2015fast, sakhi2023fast}. While the BT loss is more efficient, its simplifications sacrifice critical preference information. These limitations underscore the need for an improved preference alignment framework—one that robustly rejects harmful content, captures nuanced preferences, leverages the varying informativeness of responses, and achieves computational efficiency without compromising alignment quality.


\noindent{\textbf{Contributions.}} We address these limitations by introducing a provably effective and efficient   Hard Preference Sampling framework(HPS)  for human preference alignment.  Our key contributions are highlighted below. 

 
Firstly, we introduce the HPS framework to enhance human preference alignment. Specifically, we first propose a training loss that fine-tunes LLMs to robustly prefer the most desired response while rejecting all dispreferred and potentially harmful ones. Moreover, HPS leverages insights from supervised, metric, and contrastive learning~\cite{schroff2015facenet, oh2016deep, contrastivehard}, emphasizing the importance of ``hard” examples—dispreferred responses closely resembling the preferred ones. Accordingly, HPS develops a hard preference sampling strategy to prioritize such hard examples, enabling the model to distinguish between preferred and highly similar dispreferred responses more effectively. To ensure efficiency, HPS is then reformulated into a sampling  approach, using a single Monte Carlo sampling to select a single dispreferred response per training iteration. This innovation significantly reduces computational overhead compared to PL which requires all dispreferred responses for each prompt.
 

Secondly, HPS provably improves sample complexity over the vanilla PL loss. For a dataset \(\mathcal{D}\) with \(m\) prompts and \(n\) responses per prompt, the distance between the optimum  of the PL loss and the optimal human preference policy  is bounded by  $\mathcal{O}\big(\frac{n^{2}}{\sqrt{m}}\big)$ which is  improved to $\mathcal{O}\big(\frac{n}{\sqrt{m}}\big)$ by  using our HPS loss. This improvement ensures better preference alignment with fewer training samples, making HPS particularly advantageous in data-limited scenarios or when faster convergence is required.


Thirdly,  we further prove that optimizing the HPS loss maximizes the reward margin -- the gap between the most preferred response and the closest dispreferred one -- for any given prompt. A high reward margin means less  dispreferred  or unethical generation.  So this maximization ensures the LLM learns a robust distinction between preferred and dispreferred responses, leading to superior alignment with human preferences.
 
 

Finally, experimental results demonstrate that HPS outperforms state-of-the-arts (SoTAs) in both fine-tuning and transfer learning settings. On the HH-RLHF dataset~\cite{hhdata}, HPS achieves comparable BLEU and reward performance but improves the average reward margin by $89\%$ over DPO, IPO and other preference alignment methods. A higher reward margin reflects fewer dispreferred or harmful generations. When transferring fine-tuned LLMs on HH-RLHF to the PKU-Safety dataset~\cite{pkusafe}, HPS maintains comparable BLEU and reward scores while achieving an average reward margin improvement of $83\%$ over SoTAs, further highlighting its robustness and generalizability.
