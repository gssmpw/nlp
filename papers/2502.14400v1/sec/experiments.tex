\input{table/rewardmargin}
% Compared with reward models which may have a distortion in capturing human preferences, human annotation is considered the most accurate evaluation method, and recently, the powerful large language model has emerged as a scalable approach for rapidly assessing human preference.
\input{table/finetune}

\input{table/tab_win}
\input{table/transfer}
\input{table/tab_ablation} 
\vspace{-7pt}
\section{Experiments}
\vspace{-2pt}
\label{sec:experiments}
%In this section, we present main results of our experiments, empirically highlighting the superior performance of HPS on various benchmarks and ablation studies.
%
%\subsection{Models and Evaluation Setup}
%For the backbone model, we select the widely-used LLaMA3-8B~\cite{llama3}. All preference alignment experiments are conducted using a checkpoint of the supervised fine-tuned LLaMA3-8B model~\cite{onlinerlhf}, which serves as both the baseline SFT model and the reference model. We begin by examining the SFT model, denoted as SFT Model. In addition, we utilize several popular preference optimization methods, including DPO~\cite{dpo} with BT and PL loss, EXO~\cite{exo}, IPO~\cite{ipo}, SPPO~\cite{sppo}, and NCA~\cite{nca}. For each baseline method, we apply our proposed hard preference sampling method, denoting the results as DPS-HPS, EXO-HPS, IPO-HPS, SPPO-HPS, and NCA-HPS, respectively.


\noindent{\textbf{Baselines.}} In experiments, we use a supervised fine-tuned LLaMA3-8B checkpoint~\cite{onlinerlhf,llama3} as both  baseline SFT and reference model. We test the SFT model and integrate our HPS into several preference optimization methods, including DPO~\cite{dpo} (BT\&PL), EXO~\cite{exo} (BT\&PL), IPO~\cite{ipo}, SPPO~\cite{sppo}, and NCA~\cite{nca}. 
%Each baseline is tested with our hard preference sampling-HPS method, denoted as DPS-HPS, EXO-HPS, IPO-HPS, SPPO-HPS, and NCA-HPS.  

\vspace{-1pt}

\noindent{\textbf{Datasets.}} We use two popular datasets, HH-RLHF~\cite{hhdata} and PKU-SafeRLHF~\cite{pkusafe},  focusing on helpfulness and safety~\cite{RewardBench, llmbench}. HH-RLHF is multi-turn, while PKU-SafeRLHF contains single question-answer pairs. Each prompt in datasets includes two responses with human-rated preferences. To test different methods, for each prompt, we adopt LLaMA3~\cite{onlinerlhf} to generate 100  responses, and generate their rankings and rewards  using a top-10 safety-ranked reward model~\cite{liu2024skywork} from the RewardBench benchmark~\cite{RewardBench}.

%We primarily assess our preference optimization methods using two of the most popular safety and helpfulness prompt-response datasets: HH-RLHF~\cite{hhdata} and PKU-SafeRLHF~\cite{pkusafe}. These datasets evaluate the models' versatile conversational abilities across a diverse set of queries, specifically focusing on helpfulness and safety, and have been widely adopted by the community~\cite{RewardBench, llmbench}. Specifically, each entry in those datasets includes two responses to a question, accompanied by preferences rated by human annotators for both responses based on their helpfulness and harmlessness. It is important to note that HH-RLHF is a multi-turn conversation dataset, whereas PKU-SafeRLHF consists of single question-answer pairs. To evaluate the performance of different preference optimization methods, we augment each sample with new candidates from a trained Llama3 LLM~\cite{onlinerlhf, llama3}, increasing the number of ranked responses to $100$. We then utilize a strong reward model~\cite{liu2024skywork}, $\text{R}_\text{train}$, which ranks among the top-$10$ in safety within the RewardBench benchmark, ~\cite{RewardBench}, to assign rankings and rewards to the 100 responses for each prompt.

%\subsection{Evaluation Metrics}
%We use automatic and model-based metrics to evaluate the quality of generated responses to a given prompt, as well as the ability to reject harmful content, across all preference optimization methods. To avoid unfairness and follow the evaluation pipeline in the previous work~\cite{pro}, we select another strong reward model~\cite{ArmoRM} for test evaluation, which we denote as $\text{R}_\text{eval}$. Specifically, we utilize BLEU~\cite{bleu} to assess the text quality and reward derived from $\text{R}_\text{eval}$ to measure the level of human preference gained. In addition, we employ several reward margins (RMs) induced from different implicit reward modeling methods to quantify the difference in rewards between the most preferred and dispreferred responses in the test dataset. A higher RM score indicates that the model is better at generating responses that are aligned with the intended preference, without producing harmful, biased, or otherwise undesirable outputs. This enables us to explicitly evaluate the probability margin between generating preferred responses and those containing harmful content, transitioning from the policy space to the reward space. \Cref{tab:rm} displays the specific function forms for various reward margins.
%
%
%Furthermore, human evaluation is widely regarded as the gold standard for assessing human preferences. In this process, an annotator judge is presented with a question and two responses, tasked with determining the better option or declaring a tie.  Recently, the open-source LLM backbones Qwen $2.5$~\cite{qwen2.5} and Granite $3.1$~\cite{granite} demonstrated their powerful capabilities in effectively evaluating chat assistant responses, showing a strong alignment with human preferences. This alignment is reflected in their performance on various benchmarks, including the Open LLM Leaderboard~\cite{llmbench}, the chatbot-arena-leaderboard~\cite{chatarena} and the reward benchmark~\cite{RewardBench}. Given these capabilities, we utilize Qwen-2.5-7B-Instruct and Granite-3.1-8B-Instruct to select the better response between the two options, thereby assessing the win rate across different methods to evaluate their performance.
%
%To improve the robustness of our evaluation, we employ a Best of $N=5$ approach, where $N$ responses are sampled from each method, and the highest-scoring response is reported for each metric.
\vspace{-0pt}

\noindent{\textbf{Evaluation Metrics.}} We evaluate response quality and harmful content rejection. We use BLEU~\cite{bleu} to assess the text quality by comparing responses to ground-truth preferred answers and adopt a strong reward model~\cite{ArmoRM} to measure the level of human preference gained. We also compute reward margins (RMs) from various implicit reward models (\Cref{tab:rm}) to quantify the gap between preferred and dispreferred responses, where higher RM scores indicate better preference alignment without harmful or biased outputs.

Human evaluation remains the gold standard for assessing response quality, where annotators compare two responses per question to select the better one or declare a tie. Recent LLMs like Qwen $2.5$~\cite{qwen2.5} closely align with human preferences, validated by benchmarks like the Open LLM Leaderboard~\cite{llmbench}, chatbot-arena-leaderboard~\cite{chatarena}, and RewardBench~\cite{RewardBench}. We use Qwen-2.5-Instruct to assess response quality and win rates. For evaluation robustness, we sample $N=5$ responses per method and report the highest-scoring one for each metric.

%We test response quality and harmful content rejection. First, we follow~\cite{ArmoRM}, and use  BLEU~\cite{bleu} to measure \zp{the generation quality by comparing generated response with the ground-truth preferred response.}   
%Then, we compute reward margins (RMs) from various implicit reward models summarized in \Cref{tab:rm} to assess the difference between preferred and dispreferred responses. Higher RM scores indicate stronger preference alignment without harmful or biased outputs.  
%
%Human evaluation remains the gold standard for assessing preferences. Annotators compare two responses per question to determine the better one or declare a tie. Recent LLMs like Qwen $2.5$~\cite{qwen2.5} and Granite $3.1$~\cite{granite} exhibit strong alignment with human preferences, validated by benchmarks like  the Open LLM Leaderboard~\cite{llmbench}, chatbot-arena-leaderboard~\cite{chatarena}, and RewardBench~\cite{RewardBench}. We use Qwen-2.5-7B-Instruct and Granite-3.1-8B-Instruct to determine the better response and assess win rates. For evaluation robustness, we adopt a Best of $N=5$ strategy, sampling $N$ responses per method and reporting the highest-scoring response for each metric.

%\subsection{Implementation Details}
%We conduct our experiments using the open-source packages PyTorch~\cite{pytorch}, TRL~\cite{trl}, Transformers~\cite{transformers}, Accelerate~\cite{accelerate}, and vLLM~\cite{vllm}. Constrained by computational resources, we employ low-rank adaptation (LoRA)~\cite{lora} to efficiently fine-tune the model. The LoRA configuration includes a rank of $8$, a scaling factor $\alpha = 16$, and a dropout rate of $0.01$. LoRA is applied to the query and value projection layers without bias handling. We set the KL penalty strength weight $\beta$ to $0.1$, following the DPO approach. The learning rate scheduler is a cosine scheduler~\cite{cosine}, with AdamW~\cite{adamw} as the optimizer. The sequence length, including the prompt, is $2048$ tokens, with training conducted over $2$ epochs at a learning rate of $5.0 \times 10^{-7}$. During inference, the model generates a maximum of $2048$ new tokens, with a sampling temperature of $0.9$.

\noindent{\textbf{Implementation.}}  Due to computational constraints, we apply LoRA~\cite{lora} for efficient fine-tuning with a rank of $8$ and scaling factor $\alpha = 16$. %, and dropout rate of $0.01$.  %, targeting query and value projection layers without bias handling.
 The KL penalty strength $\beta$ is set to $0.1$, following DPO.  
 %We use AdamW~\cite{adamw} with a learning rate of $5.0 \times 10^{-7}$ over $2$ epochs and with cosine scheduler. 
We use a sequence length of $2,048$ tokens for both training and inference, with a sampling temperature of $0.9$ during inference. We set the number of responses $n\!=\!5$ for all PL-based methods. The GPU fine-tuning time for the PL-based, BT-based, and HPS-based methods is $168.4 \!\pm\! 1.9$ hours, $62.8 \!\pm\! 1.1$ hours, and $64.4 \!\pm\! 0.8$ hours, respectively. This demonstrates that our proposed HPS method can significantly improve efficiency compared to the PL-based method, achieving a $61.76\%$ reduction in fine-tuning time, and validates \Cref{thm:estimator}.
\vspace{-8pt}
\subsection{Fine-Tuning Setting}\label{finetune}
\vspace{-4pt}
We integrate HPS into various alignment approaches and fine-tune LLMs on the HH-RLHF and PKU-SafeRLHF datasets. \Cref{tab:finetune} reports BLEU, Reward, and Reward Margin, revealing two key findings: \textbf{1)} HPS achieves comparable performance on BLEU and Reward metrics. For instance, on HH-RLHF, HPS-based methods achieve BLEU scores around $0.231$, similar to BT-based methods.  This shows that HPS does not affect the quality of the generation of preferred content. \textbf{2)}  HPS significantly improves Reward Margin, reducing harmful or unhelpful responses. Traditional methods like DPO-PL and SPPO exhibit negative $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$ values, indicating a higher likelihood of harmful outputs (e.g., DPO-PL: $\text{RM}_{\text{DPO}}$ of $-0.795$, $\text{RM}_{\text{R-DPO}}$ of $-1.448$). In contrast, DPO-HPS shows $\text{RM}_{\text{DPO}}$ of $2.723$ and $\text{RM}_{\text{R-DPO}}$ of $2.040$, reflecting improvements of $442.51\%$ and $240.88\%$. This validates \Cref{thm3}, confirming HPS leads to stronger rejection of harmful responses. 
 
\noindent\textbf{Win Rate Evaluation.}
 Unlike reward models that may distort human preferences, recent advances in instruction-tuned LLMs offer a scalable and reliable alternative for evaluating human preferences. Thus, we use Qwen-2.5-Instruct to assess response quality and win rates, where Qwen $2.5$ closely aligns with human preferences, validated by benchmarks like the Open LLM Leaderboard~\cite{llmbench}, chatbot-arena-leaderboard~\cite{chatarena}, and RewardBench~\cite{RewardBench}. Details of the evaluation methodology are provided in \Cref{winratepipeline}.
 
 As shown in \Cref{tab:winrate}, DPO-HPS consistently outperforms the baselines---SFT, DPO-BT, and DPO-PL---across both the HH-RLHF and PKU-Safety datasets, achieving an impressive win rate of approximately 60\%. This result underscores HPS's superior alignment with human preferences and is consistent with the reward model evaluation.

%\begin{itemize}[noitemsep, topsep=-5pt, leftmargin=*]
%    \item \textit{Our proposed hard preference sampling achieve the comparable performance on the generation metrics of BLEU and Reward.} As shown in BLEU and Reward metrics, HPS achieve competitive performance to those baseline methods. In the HH-RLHF dataset, methods like EXO-HPS and DPO-HPS achieve BLEU scores around $0.231-0.232$, which is on par with their baselines like EXO ($0.231$) and DPO ($0.230$).
%    \item \textit{On the Reward Margin metric, our proposed hard preference sampling demonstrates superior performance, suggesting that an LLM trained using the HPS approach is less likely to generate harmful or unhelpful responses compared to an LLM trained with traditional RLHF methods.} Traditional methods like DPO-PL and SPPO show negative $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$ values, indicating a greater likelihood of generating harmful or unhelpful responses. For instance, DPO-PL has $\text{RM}_{\text{DPO}}$ of $-0.795$ and $\text{RM}_{\text{R-DPO}}$ of $-1.448$ on the HH-RLHF dataset, suggesting the presence of less desirable outputs. In contrast, HPS methods, such as DPO-HPS and EXO-HPS, yield positive RM values (e.g., DPO-HPS has $\text{RM}_{\text{DPO}}$ of $2.723$ and $\text{RM}_{\text{R-DPO}}$ of $2.040$, increasing of $442.51\%$ and $240.88\%$), indicating that they are more effective at avoiding harmful content. This improvement in $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$ shows that HPS-based methods outperform traditional preference alignment approaches in terms of Reward Margin, validating \Cref{thm3}, which claims that hard preference sampling results in more robust rejection of harmful or unhelpful responses.
%\end{itemize}

% \input{table/tab_mainhh1}
% \input{table/tab_mainpku1}

\vspace{-8pt}
\subsection{Transfer Learning Setting}
\vspace{-3pt}
After fine-tuning LLMs on HH-RLHF (PKU-SafeRLHF), we evaluate their transferability on PKU-SafeRLHF (HH-RLHF) to assess generation quality and harmfulness rejection in a transfer learning setting.
\vspace{-2pt}

\Cref{tab:transfer} presents the results, leading to two key conclusions. First, HPS achieves comparable BLEU and Reward scores, demonstrating strong transferability. Despite dataset differences, HPS-based methods perform on par with baselines. For example, DPO-HPS achieves BLEU scores of $0.219$ (HH-RLHF) and $0.310$ (PKU-Safety), similar to DPO-BT ($0.219$ and $0.308$). This consistency suggests that HPS effectively transfers learned preferences and linguistic structures.   

\vspace{-1.5pt}

Moreover, HPS improves harmfulness rejection robustness, as reflected in Reward Margin. HPS consistently outperforms baselines in terms of  $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$, showing better generalization of safety properties. Notably, DPO-HPS achieves $\text{RM}_{\text{DPO}}$ of $5.725$ on PKU-Safety, compared to DPO-BTâ€™s $\text{RM}_{\text{DPO}}$ of $1.346$. Additionally, HPS excels in transfer tasks, with EXP-HPS achieving $\text{RM}_{\text{DPO}}$ of $2.903$ on PKU-Safety, significantly surpassing its fine-tuned counterparts which has $\text{RM}_{\text{DPO}}$ of $-5.495$, demonstrating its potential for safer and more effective cross-domain transfer.


%
%\begin{itemize}[noitemsep, topsep=-5pt, leftmargin=*]
%    \item \textit{Our proposed hard preference sampling achieves the comparable performance on the generation metrics of BLEU and Reward, reflecting strong transfer learning generalization.} Despite the differences in the datasets, the BLEU and Reward scores for HPS-based methods remain comparable to traditional baseline methods. For example, DPO-HPS achieves BLEU scores of $0.219$ (HH-RLHF) and $0.310$ (PKU-Safety), which are competitive with DPO-BT ($0.219$ and $0.308$). This consistency across datasets suggests that HPS enhances the model's ability to transfer learned preferences and linguistic structures effectively, maintaining high-quality text generation and alignment with objectives.
%    \item \textit{HPS enhances the robustness of harmfulness rejection, as reflected in Reward Margin, highlighting improved transfer of safety properties.} The results of $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$clearly show that HPS-based methods consistently outperform baseline methods in both datasets. This improvement indicates that HPS leads to better generalization of harmfulness rejection behaviors across tasks. For instance, DPO-HPS achieves a significantly higher $\text{RM}_{\text{DPO}}$ of $5.725$ on PKU-Safety, compared to the $\text{RM}_{\text{DPO}}$ ($1.346$) observed for DPO-BT on HH-RLHF. In addition, it is particularly evident in the transfer learning task on the PKU-Safety dataset, where HPS-based methods achieve superior $\text{RM}_{\text{DPO}}$ values (e.g., DPO-HPS: $5.725$, IPO-HPS: $5.386$) compared to the fine-tuning task on the same dataset (e.g., DPO-HPS: $-5.359$, IPO-HPS: $-21.607$). Thus, HPS demonstrates its potential to facilitate safer and more effective cross-domain transfer of language model capabilities.
%\end{itemize}




%Compared to reward models, which may distort the capture of human preferences, recent advancements in powerful instruct LLMs have emerged as a scalable approach for rapidly assessing human preferences.
%
%\Cref{tab:winrate} shows that DPO-HPS consistently outperforms DPO-BT and DPO-PL in both the HH-RLHF and PKU-Safety datasets, achieving around a $60\%$ win rate, which highlights its superior ability to capture human preferences.

\vspace{-8pt}
\subsection{Ablation Study}
\vspace{-3pt}
We examine the impact of the total number of responses on preference optimization performance during fine-tuning, using $5, 20, 50$, and $100$ responses per prompt. From \Cref{tab:ablation}, one can observe that while BLEU and Reward scores remain stable across response sizes for both DPO-BT and DPO-HPS, notable differences appear in $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$. As response size increases, $\text{RM}_{\text{R-DPO}}$ shows a pronounced improvement, particularly for DPO-HPS, which achieves a remarkable $\text{RM}_{\text{R-DPO}}$ of $2.040$ at $100$ responses, far surpassing DPO-BT's $-0.455$. This suggests that DPO-HPS benefits more from larger response sets, enhancing preference alignment. Additionally, the consistent increase in $\text{RM}_{\text{DPO}}$ for DPO-HPS suggests a cumulative learning effect, indicating that DPO-HPS scales better and achieves superior preference optimization with larger response sizes.

% As response size increases, $\text{RM}_{\text{R-DPO}}$ improves significantly, especially for DPO-HPS, which achieves an $\text{RM}_{\text{DPO}}$ of $2.723$ at $100$ responses, compared to DPO-BT's $0.349$. This suggests that DPO-HPS benefits more from larger response sets, enhancing preference alignment. Additionally, $\text{RM}_{\text{DPO}}$ shows that DPO-BT shifts from negative to positive values, while DPO-HPS steadily improves, reaching parity at $100$ responses. These results indicate that DPO-HPS scales better and achieves superior preference optimization with larger response sizes.  



%In this part, we investigate the influence of the total number of response on the performance of the perference optimization methods in the fine-tuning task. \Cref{tab:ablation} presents the results.
%\input{table/tab_ablation}
%The results in \Cref{tab:ablation} demonstrate the influence of response size on the performance of preference optimization methods. While BLEU and Reward scores remain stable across all response sizes for both DPO-BT and DPO-HPS, significant differences emerge in $\text{RM}_{\text{DPO}}$ and $\text{RM}_{\text{R-DPO}}$. As the number of responses increases, RM improves substantially, particularly for DPO-HPS, which achieves a notable $\text{RM}_{\text{DPO}}$ of $2.723$ at $100$ responses compared to DPO-BT's $0.349$. This indicates that DPO-HPS benefits more from a larger response sample size, effectively enhancing preference alignment. Additionally, RM2 reveals that DPO-BT transitions from negative to positive values as the response size increases, while DPO-HPS shows steady improvement, reaching parity at $100$ responses. These findings imply that DPO-HPS exhibits superior scalability and performance in preference optimization when a larger response sample size are used.