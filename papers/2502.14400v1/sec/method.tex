\vspace{-4pt}
\section{Methodology}
\vspace{-0.5pt}
\label{sec:method}
To begin with, we define the task of interest in this work. 

%\noindent{\textbf{Task Definition.}}  This work addresses a critical challenge in AI system development: ensuring that models generate helpful and harmless responses while strictly avoiding harmful or dispreferred outputs. Formally, for a given prompt \( x \) from the training dataset \( \mathcal{D} \), as shown in Fig.~\ref{img:responses}, there exists a most preferred response \( y_{\tau(1)}\) which is both harmless and highly desirable. Meanwhile, the prompt may also elicit a set of dispreferred responses \( \{y_{\tau(i)}\}_{i=2}^n \), such as \( y_{\tau(2)} \) and \( y_{\tau(3)} \) in Fig.~\ref{img:responses}, some of which exhibit varying degrees of harmfulness. That is, any \( y_{\tau(i)}\  (i \geq 2) \)  may contain malicious content. The target of this task is to align the model’s behavior to consistently generate harmless and preferred responses like  \( y_{\tau(1)} \) while strictly avoiding any dispreferred or potentially malicious responses like \( y_{\tau(i)}\  (i \geq 2) \).  

\vspace{-0.5pt}

\noindent{\textbf{Task Definition.}} This work tackles a critical challenge in AI development: ensuring models generate helpful and harmless responses while strictly avoiding harmful or dispreferred outputs. Formally, for a given prompt \( x \) from the training dataset \( \mathcal{D} \), as illustrated in Fig.~\ref{img:responses}, there exists a most preferred response \( y_{\tau(1)} \), which is both harmless and highly desirable. The prompt may also elicit a set of dispreferred responses \( \{y_{\tau(i)}\}_{i=2}^n \), such as \( y_{\tau(2)} \) and \( y_{\tau(3)} \), some of which may contain varying degrees of harmful content. The goal is to align the model to consistently generate harmless and preferred responses like \( y_{\tau(1)} \) while strictly avoiding dispreferred or potentially malicious ones like \( y_{\tau(i)}\  (i \geq 2) \).  

\vspace{-0.5pt}

This task is critical for applications requiring high-quality and safe content generation. For example, in healthcare or e-commerce, LLMs handle complex queries where harmful outputs, such as biased or offensive language, can lead to dissatisfaction, reputational harm, or legal liability. Similarly, in educational platforms, harmful responses referencing violence, drugs, or other inappropriate topics could mislead students or expose them to dangerous ideas. In such scenarios, increasing the rejection rate of unethical or dispreferred responses while maintaining acceptance of helpful ones is essential for safety, reliability, and user trust.  

%This task is highly significant because of  its widespread  applications, where maintaining the quality and safety of generated content is paramount. For instance, in healthcare or e-commerce, LLMs are often deployed to handle complex customer queries. Generating harmful or offensive content, such as biased or inappropriate language, could lead to customer dissatisfaction, reputational damage, or even legal liability. Similarly, in educational platforms, LLMs are increasingly used to provide explanations, solve problems, and guide learners. Harmful responses like those referencing violence, drugs, or other inappropriate topics could mislead students, normalize offensive behaviors, or expose them to dangerous ideas.  In these scenarios, it is crucial to increase the rejection rate of unethical and less-preferred responses without compromising the acceptance rate for helpful and  preferred ones. By addressing this dual requirement, the task ensures the safety, reliability, and user trust necessary for deploying AI systems in sensitive and impactful applications. 

\vspace{-1pt}

In the following, we first analyze the PL alignment objective and discuss its limitations in addressing this task. Then we elaborate on our proposed novel and effective approach. 

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{pic/responses.jpg}
\vspace{-22pt}
\caption{Example for harmless and preferred response $y_{\tau(1)}$ and  harmful and dispreferred response $y_{\tau(2)}$ and $y_{\tau(3)}$. $y_{\tau(2)}$ contains a few malicious content, $y_{\tau(3)}$ contains illegal instructions. Harmful content is highlighted with \underline{underlining}.}
\vspace{-13pt}
\label{img:responses}
\end{figure}
\subsection{Motivation: Analysis of PL \& BT Training Losses}
\vspace{0pt}
\label{sec:generalrlhf}
The PL  training loss \(\mathcal{L}_{\text{PL}}\)  in \Cref{eq:dpoPL}  consists of \(n\) sub-losses \(\{\mathcal{L}_{j}(d)\}_{j=1}^{n}\) defined in \eqref{eq:PL}. Each sub-loss \(\mathcal{L}_{j}(d)\) encourages the model to rank the \(j\)-th preferred response \(y_{\tau(j)}\) above a set of less preferred responses \(\{y_{\tau(k)}\}_{k=j+1}^n\), following the order \(y_{\tau(j)} \succ y_{\tau(j+1)} \succ \cdots \succ y_{\tau(n)}\) for all \(1 \leq j \leq n-1\). While this recursive ranking objective explores relative preferences among dispreferred responses, it falls short in  helping the LLM reject harmful  dispreferred samples while suffering from high training costs.
\vspace{-1pt}

\noindent{\textbf{Inadequacy for Rejecting Harmful Responses.}}
 Given a prompt \(x\) and its ranked responses \(\{y_{\tau(j)}\}_{j=1}^n\), the first response \(y_{\tau(1)}\) is always the preferred and helpful output, while subsequent responses \(\{y_{\tau(j)}\}_{j=2}^n\) are potentially harmful or purely dispreferred. Ideally, the training loss should prioritize producing response \(y_{\tau(1)}\) and strictly avoid generating any harmful outputs. However, the recursive nature of \(\mathcal{L}_{j}(d)\) inadvertently encourages the model to rank potentially harmful responses \(y_{\tau(j)}\) as ``preferred" compared to even less preferred alternatives. This misalignment limits the model's ability to robustly reject  potentially harmful content like  \(y_{\tau(j)} \ (j\geq 2)\), making the PL objective insufficient for addressing tasks where the strict rejection of inappropriate outputs is paramount.   The BT loss  focuses only on rejecting the most dispreferred response in a pair, leaving other problematic responses unaddressed.  Accordingly, the PL and BT  losses inadequately address the real-world need to prohibit harmful and  dispreferred responses, which is critical in many high-stakes applications as discussed earlier.  
 
\noindent{\textbf{Indiscriminate Handling of Dispreferred Responses.}}
 Given a prompt \(x\) and a set of response responses \(\{y_{\tau(j)}\}_{j=1}^n\), the PL loss treats all dispreferred responses \(\{y_{\tau(j)}\}_{j=2}^n\) equally as shown  in the denominator in \Cref{eq:PL} when training the model to prioritize the most preferred response without considering the inter-ranking relationship among dispreferred responses.  This overlooks the varying degrees of informativeness among dispreferred responses, which could otherwise guide more effective alignment learning.  The BT loss reduces rankings to pairwise comparisons, directly  discarding other dispreferred responses let alone their macro-level distinctions that are crucial for capturing nuanced preferences~\cite{sun2024rethinking, pro}.
 
 
 

 
 
 
\noindent{\textbf{Training Inefficiency.}}   For each prompt \(x\), the PL loss  \(\mathcal{L}_{\text{PL}}\) requires forwarding all \(n\) candidate responses \(\{y_{\tau(i)}\}_{i=1}^n\)  through the model to compute their rewards, followed by constructing \(n\) sub-losses \(\{\mathcal{L}_{j}(d)\}_{j=1}^{n}\) for back-propagation. Considering the big size of LLM, this  leads to high GPU memory and computational costs, especially  when dealing with long prompts or long responses. Indeed,  training costs  even scale linearly with the number of response candidates \(n\), further severe  large-scale training scenarios where computational resources and efficiency are critical considerations.  While the BT loss is more efficient, its simplifications sacrifice critical preference information. 
 

 
 
Given the limitations of the PL and BT objective in rejecting harmful responses and its high training cost, it is imperative to explore alternative strategies for alignment:  robustly preventing  harmful content generation while reducing training  overhead.  Below, we offer  a more practical and   effective  method for aligning LLMs with real-world requirements.  
  


% In contrast, DPO~\zp{citations} addresses these inefficiencies by focusing only on the most preferred (\(y_{\tau(1)}\)) and the most dispreferred (\(y_{\tau(n)}\)) responses. This paired approach requires fewer forward and backward passes, significantly reducing both memory and computational overhead. As a result, the training cost of PL is substantially higher than that of DPO, making PL less practical for large-scale training scenarios where computational resources are a critical concern.  


%\subsubsection{Misalignment}
%The PL alignment objective is based on the recursive preference relationship $y_{\tau(j)} \succ \cdots \succ y_{\tau(n)}, \forall 1\leq j\leq n-1$, where the model is trained to rank responses according to multiple preference orders. Although the PL alignment objective enables the LLM to learn the recursive preference relationship of responses, it falls short in helping the LLM reject hard negative samples. Specifically, while multiple contrastive losses based on these preference comparisons contribute to the overall PL alignment objective, only the top-ranked response $y_{\tau(1)}$ corresponds to a true positive. Furthermore, although the contrast $y_{\tau(j)} \succ \cdots \succ y_{\tau(n)}, j \neq 1$ offers an overview of the relative ranking among response candidates, it is insufficient for training the LLM to robustly reject harmful or inappropriate outputs, such as $y_{\tau(j)}$.
%
%In many real-world applications, it is crucial to increase the rejection rate for less preferred responses without compromising the acceptance rate for preferred ones. For example, if an LLM generates even a single word that is harmful, insulting, or violates ethical guidelines, it could result in a significant issue, even if the rest of the output is acceptable. As a result, the multi-dimensional contrastive approach does not adequately address the need for prioritizing the rejection of harmful or inappropriate responses. In other words, it does not maximize the margin between all dispreferred and the most preferred response.

%\subsubsection{Training Cost}
%Although the PL objective leverages all informative preference relationships among all response candidates, the training cost associated with the PL alignment objective is substantial. For instance, given a fixed prompt $x$, the training complexity for Direct Preference Optimization (DPO) is $\mathcal{O}(1)$ , while for the PL model, it increases to $\mathcal{O}(n)$, where $n$ is the number of response candidates.
%
%As the number of response candidates grows, the number of contrastive comparisons increases exponentially in the worst case. Consequently, the GPU training time for aligning large language models (LLMs) also grows exponentially, leading to substantial overhead. This issue becomes particularly pronounced in large-scale training scenarios, where computational resources and efficiency are critical.
%
%Given these challenges with the RLHF alignment objective, it is crucial to ask: is there a more efficient way to select negative samples for the alignment process? Exploring alternative strategies could significantly reduce training costs while maintaining or improving alignment performance.
\vspace{-1pt}
\subsection{Hard  Preference Sampling for   Alignment}
\vspace{0pt}
To solve the task of interests, we propose a hard  preference sampling framework (HPS). The target of the task is to train the model to reject all dispreferred and potentially harmful responses \(\{y_{\tau(i)}\}_{i=2}^n\), ensuring it generates only the most preferred response \(y_{\tau(1)}\) for a given prompt \(x\).   To this end, for a training sample $d=(x, y_{\tau{(1)}}, y_{\tau{(2)}}, \cdots, $ $y_{\tau{(n)}})\sim \mathcal{D}$, HPS can use the training loss 
\begin{equation}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\label{eq:PLsfdas}
			\mathcal{L}_{\wm} =\expectation_{d\sim\mathcal{D}}   \!-\log\!\Big({e^{r_{\wm} (x,y_{\tau(1)})} / \sum\nolimits_{i=1}^{n}e^{r_{\wm} (x,y_{\tau(i)})}}\Big).
	\end{aligned}}
\end{equation}
where the model is encouraged to rank \(y_{\tau(1)}\) above all dispreferred and  potentially harmful  responses $\{y_{\tau(i)}\}_{i=2}^n$. We use the DPO implicit reward as mentioned in \Cref{dporm} here.

However, this loss treats all dispreferred responses \(\{y_{\tau(i)}\}_{i=2}^n\) equally, ignoring their varying levels of informativeness. Previous works in supervised, metric, and contrastive learning~\cite{schroff2015facenet, oh2016deep, contrastivehard} demonstrate that “hard” examples—those closely resembling the correct output but still incorrect—are particularly useful for  learning. In our context, hard dispreferred responses are those that are highly similar to \(y_{\tau(1)}\) yet dispreferred or harmful. Training the model to distinguish \(y_{\tau(1)}\) from the hardest dispreferred response \(y_{\tau(2)}\) enables it to reject less preferred responses \(\{y_{\tau(i)}\}_{i=3}^n\) more effectively. Thus, harder dispreferred responses should be penalized more heavily during training.  

%hard samples refer to those 
%informative dispreferred examples that are   very similar to the preferred response  but should be rejected due to their dis-preferring and potential harmfulness. This can be intuitively understood:  if the model can well distinguish the hardest potentially harmful response $y_{\tau(2)}$ and the most preferred response $y_{\tau(1)}$, then the model should easily reject the remaining less preferred response  $\{y_{\tau(i)}\}_{i=3}^n$. Thus, during training, the more dispreferred response $y_{\tau(i)}\ (i\geq 2)$ should considered more or be penalized more than less dispreferred response $y_{\tau(j)}\ (i<j\leq n)$.  


 \textbf{Hard Preference Sampling Framework (HPS).}  Our HPS builds a distribution over the  dispreferred responses  as  
\begin{equation}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			q(x, y) =  e^{{r^{*}(x,y)}}\cdot p(y) / Z, 
	\end{aligned}}
\end{equation}
where $r^*(x, y)$ is the  inaccessible optimal  reward model defined in Sec.~\ref{sec:preliminaries}  and can provide  the ground-truth rewards,  $p(y)$ is the probability density of dispreferred response $y$, and  $Z$ is the partition function for normalization.  For each ranked response \(y_{\tau(i)}\), we can either directly access its reward $r_{\text{est}}$ if available in the dataset $\mathcal{D}$ or estimate it using a pretrained human preference-aligned reward model, $r_{\text{est}}(x,y_{\tau(i)})\approx r^{*}(x,y_{\tau(i)})$. Without loss of generality, we first formulate the Eqn.~\eqref{eq:PLsfdas} in the expectation form:
\begin{equation}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
        \label{eq:exhps}
			& \mathcal{L}_{\wm}\!=\! \expectation_{d\sim\mathcal{D}}   \!
			\!-\!\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+ N\cdot\expectation\nolimits_{y \sim q(x,y)}\left[e^{{r_{\wm}(x,y)}}\right]}\right),
\end{aligned}}
\end{equation}
where $N=n-1$.  Using the Monte Carlo importance sampling technique,  Eqn.~\eqref{eq:exhps} becomes:
\begin{equation*}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
   %          &\!=\! \expectation_{d\sim\mathcal{D}}   \!
			% \!-\!\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}\!+\! N\cdot\expectation\nolimits_{y \sim p(x,y)}\left[\frac{e^{{r_{\wm}(x,y)}}q(x,y)}{p}\right]}\right) \\
   %           &
\mathcal{L}_{\wm}\!=\!    \expectation_{d\sim\mathcal{D}}   \!
			\!-\!\log\!\left(\!\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}\!+\! N\cdot\expectation\nolimits_{y \sim p(y)}\left[e^{{r_{\wm}(x,y)}}e^{{ r^{*}(x,y)}}/{Z}\right]}\!\right)\!.
	\end{aligned}}
\end{equation*}
% We use the training sample $d$ to approximate $Z$ empirically as: $\hat Z\!=\!\sum\nolimits_{i=2}^{n}e^{r_{\text{est}} (x,y_{\tau(i)})}$.
Next, we can empirically estimate the distribution $ q(x, y) $:
\begin{equation*}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
			\label{eq:asfdsf}
		 q(x, y) = {e^{ \gamma \cdot r_{\text{est}} (x,y)}  }/{ \sum\nolimits_{i=2}^{n}e^{\gamma\cdot  r_{\text{est}} (x,y)}}.
	\end{aligned}}
\end{equation*}
Here for flexibility, we introduce a hyperparameter $\gamma>1$ to control penalty strength in $q(x,y)$.  
 Thus, the empirical  training loss function becomes: 
\begin{equation}
	\fontsize{9}{3}\selectfont{
		\begin{aligned}
        \label{eq:emloss}
		\mathcal{L}_{\wm}\!=\! \expectation_{d\sim\mathcal{D}}   \!
			\!-\!\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}\!+\! N\cdot\expectation\nolimits_{y \sim p(y)}\left[e^{{r_{\wm}(x,y)}} q(x,y)\right]}\right).
	\end{aligned}}
\end{equation}
Here, harder dispreferred responses whose hardness is reflected by their bigger rewards $r_{\text{est}} (x,y)$ contribute more to the loss due to their higher weights \( q(x, y)\). For instance, larger \(\gamma\) sharpens the distribution, emphasizing harder dispreferred responses and enabling the model to better distinguish closely-ranked preferred and dispreferred responses.

\noindent{\textbf{Reducing Training Costs with Flexible Sampling.}} Although this approach improves alignment, computing rewards for all \(n\) responses and backpropagating through them can be computationally expensive. To address this, we propose to sample only a single importance-weighted dispreferred response \(y \sim  q(y)\) and incorporate it into the loss function Eqn.~\eqref{eq:emloss} for each prompt $x$ in practice, which works very well as shown in Sec.~\ref{sec:experiments} and also significantly reduces computational and memory overhead. By focusing more on the hard dispreferred responses, our method retains robust alignment while greatly improving training efficiency. 
% we rewrite the loss using an expectation formulation:  
% \begin{equation}
% 	\label{eq:generalhardloss2}
% 	\fontsize{9}{3}\selectfont{
% 		\begin{aligned}
% 			\mathcal{L}_{\wm}= \expectation_{d\sim\mathcal{D}}   \!
% 			-\log\left(\frac{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}}{e^{{ r_{\wm}(x,y_{\tau\left( 1\right)})}}+ N\cdot\expectation\nolimits_{y \sim q(x,y)}\left[e^{{r_{\wm}(x,y)}}\right]}\right), 
% 	\end{aligned}}
% \end{equation}
% where $N=n-1$ when $n$ is finite. This enables flexible sampling during training.  
 
