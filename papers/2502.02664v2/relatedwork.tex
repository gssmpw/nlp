\section{Related Work}
Robot navigation in indoor environments has been studied over the past three decades~\cite{desouza2002vision, cox1991blanche}, with most approaches falling under classical, imitation-learning (IL), and reinforcement-learning (RL) paradigms.
Classical methods mostly assume a known map, and solve navigation using traditional robot motion planning methods \cite{burgard1999experiences, thrun1999minerva, karaman2011sampling}.
Other classical approaches that scale to unknown maps rely on Simultaneous Localization and Mapping (SLAM) in the loop \cite{jones2011visual, chaplot2020learning, zhan2022activermap}.
However, classical approaches usually suffer from large computational times, making them less lucrative for real-time applications.
Advancements in machine learning approaches resulted in the emergence of IL and RL-based methods. 
IL methods require an expert in the loop or its demonstration data for learning the navigation policies \cite{gupta2017cognitive, finn2017one, stepputtis2020language}.
In contrast, RL-based techniques learn by trial-and-error through interaction with the environment \cite{zhu2017target, mirowski2016learning, ye2021auxiliary}.
However, both IL and RL methods face challenges in translation to practical scenarios due to the need for massive amounts of diverse data and a lack of interpretability.
Instead, we present a principled, data-efficient approach based on implicit neural representations to model the environment for collision avoidance.

The representation framework used to model obstacles in the environment is an important factor in devising efficient motion planning algorithms.
In this regard, a number of different environment representation methods have been used~\cite{oleynikova2016signed, mildenhall2021nerf, nfomp2022, ni2022ntfields}, the signed distance field (SDF) being the most prominent and widely used in robotics research.
Recent advancements have led to faster ways of computing SDF via truncation and sparse voxelization \cite{pan2022voxfield, oleynikova2017voxblox, han2019fiesta}, leading to various approaches that leverage them for robot navigation in static environments.
Since computing the SDF of the full environment is not necessary for trajectory optimization, EGO-Planner \cite{zhou2020ego} formulates an approximation of the SDF, limiting the amount of environment information being considered.
However, its experimental setup involves simple geometries such as thin cylinders, which do not provide significant occlusion.
On the other hand, iPlanner~\cite{yang2023iplanner} directly infers trajectories from depth images by implicitly learning the SDF-guided trajectory optimization procedure.
A different line of work considers modeling the environment through Neural Radiance Fields (NeRFs) wherein the density estimates provide a proxy for workspace occupancy~\cite{adamkiewicz2022vision}.
However, NeRFs are optimized for novel view synthesis and do not accurately capture object surface information used to delineate formidable and free workspace regions and derive smooth gradients.

Recent approaches also consider navigating dynamic environments using SDF-based obstacle representations.
For instance, \cite{finean2021predicted} uses SDFs for trajectory optimization to find a path for a robot manipulator to maneuver around a moving obstacle.
However, the approach considers simple obstacle geometry with trackable linear motion, whereas daily-life objects have intricate shapes and geometries and are often not trackable using only onboard sensors due to occlusion.
In a similar vein, \cite{liu2022regularized} considers the task of human-in-the-loop robot motion planning under full observability, with SDF representing the human body.
In contrast to considering a single entity such as a human, our framework considers a variety of complex objects encountered in an indoor scenario, and performs dynamic collision avoidance.
Furthermore, our framework operates under partial observability with a single RGB-D sensor, tackling a pragmatic limited field-of-view setting.