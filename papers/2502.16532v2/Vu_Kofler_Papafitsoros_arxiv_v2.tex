

\documentclass[11pt]{amsart}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{subcaption}
\usepackage{float}
\usepackage{color}
\usepackage{xurl}
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor= Brown,
  linkcolor= link,
  urlcolor= link
}
\usepackage{enumitem}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[export]{adjustbox}
\usetikzlibrary{spy}

%%%%%%%%%%%%%%% Create the colorbars
\pgfplotsset{compat=1.18}

% Define custom grayscale colormap (black to white)
\pgfplotsset{
    colormap={bw}{
        rgb255(0cm)=(0,0,0);  % Black
        rgb255(1cm)=(255,255,255)  % White
    }
}

% Load the custom colormap from magma_colormap.tikz
\input{magma_colormap.tikz}

% Define a custom command for the colorbar
\newcommand{\customcolorbar}[6]{
    \pgfplotscolorbardrawstandalone[
        colormap name=#1, % Pass the colormap name
        colorbar horizontal,
        point meta min=#2, % Specify the minimum value
        point meta max=#3, % Specify the maximum value
        colorbar style={
            width=#4, % Set the width for the colorbar
            height=0.2cm,
            xtick={(#2 + #3)/2}, % Set fewer major ticks (min, middle, max)
            xticklabel style={
                /pgf/number format/fixed, % Fixed point numbers
                font=\scriptsize, % Font size for tick labels
                scaled ticks=false % Prevent scientific notation unless necessary
            },
            xtick pos=bottom, % Ensure ticks are at the bottom
            % % Add extra tick labels to overwrite only the first tick
            % extra x ticks={#2, #3}, % This adds the first tick as an extra tick
            % extra x tick labels={$\leq \quad \quad$, $\quad \quad \quad \geq$ #3} % This sets the custom label for the first tick
            extra x ticks={#2, #3}, % Add first and last ticks
            extra x tick labels={#5#2, #6#3} % Custom labels for the first and last ticks
        }
    ]
}
%%%%%%%%%%%%%%%

\definecolor{link}{rgb}{0.18,0.25,0.63}
\definecolor{myred}{rgb}{0.7,0.25,0.2}
\definecolor{mygray}{rgb}{0.8,0.8,0.8}
\usepackage[top=3.3cm,bottom=3.3cm,left=3.5cm,right=3.5cm]{geometry}
\renewcommand{\labelenumi}{(\roman{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}
\pagestyle{plain}
\numberwithin{equation}{section}

\newcommand{\bv}{\mathrm{BV}}
\newcommand{\tv}{\mathrm{TV}}
\newcommand{\tgv}{\mathrm{TGV}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\om}{\Omega}
\newcommand{\LL}{\mathrm{\Lambda}}

\definecolor{myred}{rgb}{0.8,0.25,0.2}
\definecolor{mygreen}{RGB}{0, 153, 0}
\definecolor{myblue}{RGB}{0, 153, 255}
\definecolor{myorange}{RGB}{255, 153, 0}
\newcommand{\kp}[1]{\textcolor{myred}{(KP: #1)}}
\newcommand{\ttv}[1]{\textcolor{mygreen}{(TTV: #1)}}
\newcommand{\ak}[1]{\textcolor{myorange}{(AK: #1)}}

\newcommand{\balpha}{\boldsymbol{\alpha}}

\newcommand\com[1]{\textcolor{myred}{\textbf{#1}}}
\newcommand\gray[1]{\textcolor{mygray}{#1}}

\newcommand{\cmmnt}[1]{}

\makeatletter
\g@addto@macro{\endabstract}{\@setabstract}
\newcommand{\authorfootnotes}{\renewcommand\thefootnote{\@fnsymbol\c@footnote}}%
\makeatother



\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}






\begin{document}
\definecolor{link}{rgb}{0,0,0}
\definecolor{mygrey}{rgb}{0.34,0.34,0.34}
\def\blue #1{{\color{blue}#1}}

 \begin{center}
 \large

   \textbf{Deep unrolling for learning optimal spatially varying regularisation parameters  for Total Generalised Variation} \par \bigskip \bigskip
   \normalsize
  \textsc{Thanh Trung Vu}\textsuperscript{$\,1,$$2$}, \textsc{Andreas Kofler}\textsuperscript{$\,3$},  \textsc{Kostas Papafitsoros}\textsuperscript{$\,2$}
\let\thefootnote\relax\footnote{
\textsuperscript{$1$}University of Cambridge, UK
}
\let\thefootnote\relax\footnote{
\textsuperscript{$2$}School of Mathematical Sciences, Queen Mary University of London, UK}

\let\thefootnote\relax\footnote{
\textsuperscript{$3$}Physikalisch-Technische Bundesanstalt (PTB), Braunschweig and Berlin, Germany}



\let\thefootnote\relax\footnote{
\hspace{3.2pt}Emails: \href{mailto:ttv22@cam.ac.uk}{\nolinkurl{ttv22@cam.ac.uk}},
 \href{mailto: andreas.kofler@ptb.de}{\nolinkurl{andreas.kofler@ptb.de}},
 \href{mailto: k.papafitsoros@qmul.ac.uk}{\nolinkurl{k.papafitsoros@qmul.ac.uk}}
}
\end{center}
\vspace{-0.8cm}

\begin{abstract}
We extend a recently introduced deep unrolling framework for learning spatially varying regularisation parameters in inverse imaging problems to the case of Total Generalised Variation (TGV). The framework combines a deep convolutional neural network (CNN) inferring the two spatially varying TGV  parameters with an unrolled algorithmic scheme that solves the corresponding variational problem. The two subnetworks are jointly trained end-to-end in a supervised fashion and as such the CNN learns to compute those parameters that drive the reconstructed images as close as possible to the ground truth. Numerical results in image denoising and MRI reconstruction show a significant qualitative and quantitative improvement compared to the best TGV scalar parameter case as well as to other approaches employing spatially varying parameters computed by unsupervised methods.
We also observe that the inferred spatially varying parameter maps have a consistent structure near the image edges, asking for further theoretical investigations. In particular, the parameter that weighs the first-order TGV term has a triple-edge structure with alternating high-low-high values whereas the one that weighs the second-order term attains small values in a large neighbourhood around the edges.\\

\noindent
\textbf{Keywords:} Spatially Varying Regularisation Parameters  $\cdot$ Inverse Problems $\cdot$  Total Generalised Variation $\cdot$  Denoising $\cdot$  Magnetic Resonance Imaging $\cdot$  Neural networks $\cdot$  Unrolling

\end{abstract}


%\tableofcontents


\definecolor{link}{rgb}{0.18,0.25,0.63}


\section{Introduction}

In inverse imaging problems, variational regularisation  problems of the type
\begin{equation}\label{intro:general_min}
\min_{u\in X} \; \mathcal{D}(Au,f)+ \mathcal{R}(u;\mathrm{\LL})
\end{equation}
are widely used to compute an estimation $u\in X$ of some ground truth imaging data $u_{\mathrm{true}}$, $X$ being a Banach space,  given data $f$ that satisfy the equation
\begin{equation}\label{intro:general_eq}
f=Au_{\mathrm{true}}+\eta.
\end{equation}
Here, $A$ denotes the forward operator of the, typically ill-posed, inverse problem and $\eta$ is a random noise component.
Solving \eqref{intro:general_min} using spatially varying regularisation parameters $\LL$, instead of scalar ones,
has been the subject of many  works, see \cite{Calatroni_bilevellearning,bilevel_handbook,Pragliola_SIAMreview} and the references therein.
The goal  is to compute and subsequently use a regularisation parameter  $\mathrm{\Lambda}: \Omega \to (\mathbb{R}_{+})^{\ell}$ that balances the data fidelity  $\mathcal{D}$ and the -- in general $\ell$ components -- of the regularisation term $\mathcal{R}$ with a different strength at every point $x\in \Omega \subset \mathbb{R}^{d}$ (every pixel in the pixel-domain $\Omega$, in the discrete setting).
%Here $\Omega \subset \mathbb{R}^{d}$ is typically a rectangle (array of pixels in the discrete setting).
In the case where $\mathcal{R}(u)=\mathrm{TV}(u)$, the Total Variation of the (grayscale) function $u: \Omega \to \mathbb{R}$ and assuming Gaussian noise, problem \eqref{intro:general_min} amounts to
\begin{equation}\label{intro:weighted_TV_min}
\min_{u\in X}\; \frac{1}{2}\|Au-f\|_{L^2(\Omega)}^{2} +\int_{\Omega} \LL(x) d|Du|.
\end{equation}
Small values of $\LL: \Omega \to \mathbb{R}_{+}$ impose little local regularity and are thus suitable for preserving detailed parts of the image and edges. On the other hand, high values of $\LL$  impose large regularity and are preferable for smooth, homogeneous areas.

For higher-order extensions of TV, especially those defined in an infimal convolution manner, the role and the interplay of spatially varying regularisation parameters on the resulting image quality and structure are not as straightforward. A prominent example is the Total Generalised Variation (TGV) \cite{TGV}
\begin{equation}\label{intro:weighted_TGV}
\tgv_{\LL_{0}, \LL_{1}}(u):=\min_{w\in \mathrm{BD}(\om)} \int_{\Omega} \LL_{1}(x) d|Du-w| + \int_{\Omega} \LL_{0}(x)d|\mathcal{E}w|,
\end{equation}
where $\LL: \Omega \to (\mathbb{R}_{+})^{2}$, with $\LL=(\LL_{0}, \LL_{1})$. Here, $\mathcal{E}$ denotes the measure that represents the distributional symmetrised gradient of $w\in \mathrm{BD}(\Omega)$, the space of functions of bounded deformations.
The combined values of $\LL_{0}$ and $\LL_{1}$ not only regulate the regularisation intensity but also control the staircasing effect, which is a characteristic limitation of  TV, with suitable values of these parameters promoting piecewise affine structures.

 In contrast to TV, where multiple works have considered computing spatially varying $\LL$ in \eqref{intro:weighted_TV_min}, see aforementioned reviews, there are limited works that focus on the computation of such spatially varying  $\LL_{0}, \LL_{1}$ for TGV.   In \cite{bilevelTGV}, a bilevel unsupervised scheme was used, employing a statistics-based upper level energy.
 The approach produced satisfactory results, albeit with a high computational cost and a need to impose continuity assumptions to $\LL_{0}, \LL_{1}$ for robustness reasons. The latter regularity for the parameter-maps limits their adaptability to the image structure, thus not fully exploiting the potential of spatial adaptivity.

Recently, there has been a series of works focusing on learning regularisation parameters for TV using  neural networks \cite{Afkham_2021,Nekhili_2022,Cuomo_2024}. There, a network is initially suitably trained in a supervised manner to infer regularisation parameters, which in a second phase are used in the corresponding variational problem that is solved as usual. Thus, one exploits the versatility of neural networks to inform a model-based variational regularisation scheme whose solution is interpretable since the regulariser remains handcrafted.

\subsection*{Contribution}
Here, we adapt the approach introduced in \cite{Kofler_2023} to compute spatially varying regularisation maps for TGV. It involves training a network that consists of two subnetworks in a supervised fashion. The first subnetwork is a deep convolutional neural network (CNN), 
that takes as an input the data $f$
and outputs the maps $\LL_{0}, \LL_{1}$. To tie these maps to the variational problem, they are fed into a second  appended  subnetwork, an unrolled PDHG algorithm \cite{chambolle2011first}, that solves the TGV minimisation problem considering the regularisation maps to be fixed. The entire network is trained end-to-end with pairs  $(f^{i}, u_{\mathrm{true}}^{i})$ and thus, the CNN is implicitly trained to output those maps that drive the approximate solution $u^{i}$ of the variational problem as close as possible to $u_{\mathrm{true}}^{i}$.
Since the CNN is expressive enough, given some new data $f^{\mathrm{test}}$, it can produce meaningful parameter-maps adapted to $f^{\mathrm{test}}$. We show that this approach produces far better results than similar approaches in image denoising and MR image reconstruction, significantly boosting the performance of TGV. We also show that in contrast to the TV, the structure of the resulting TGV parameter maps is non-trivial and asks for further theoretical investigation.

\noindent
\emph{Outline}:
In Section \ref{sec:framework}, we set some notation and recall some preliminaries about TV and TGV regularisation with scalar and spatially varying parameter maps. We proceed to describe the deep unrolled PDHG network for learning these maps. In Sections \ref{sec:denoising} and \ref{sec:mri}, we present our numerical results in denoising and MRI reconstruction, respectively, and discuss the structure of the resulting maps near the image edges. We conclude in Section \ref{sec:conclusion}.

\section{The framework}\label{sec:framework}
\subsection{Preliminaries}\label{sec:preliminaries}
We initially define the several notions in function space to recall a few properties that stem from their analysis. As usual, $\om\subset \RR^{d}$ is a bounded, Lipschitz domain and we consider grayscale images, i.e.\ $\RR$-valued.
For positive functions $\LL, \LL_{0}, \LL_{1}: \om\to \RR$ we define the spatially varying TV and TGV functionals as
\begin{align}
\tv_{\LL}(u)&= \int_{\om}\LL(x) d|Du|, \label{TVLLambda}\\
\tgv_{\LL_{0}, \LL_{1}}(u)&= \min_{w\in \mathrm{BD}(\om)}\ \int_{\om}\LL_{1}(x) d|Du-w| + \int_{\om} \LL_{0}(x) d|\mathcal{E}w|. \label{TGVLLambda01}
\end{align}
We  also denote by $\tv_{\lambda}$ and $\tgv_{\lambda_{0}, \lambda_{1}}$ the corresponding  functionals with scalar parameters. 
It holds that when the weights $\LL, \LL_{0}, \LL_{1}$ are bounded, lower semicontinuous functions, bounded away from zero,  the functionals \eqref{TVLLambda} and \eqref{TGVLLambda01} are well-defined for $u\in \bv(\om)$ 
\cite{davoli2023dyadic}. Moreover, the following proposition suggests that continuity of the weights is not essential for well-posedness.

\begin{proposition}\label{prop:existence}
Let $(\mathcal{H}, \|\cdot\|_{\mathcal{H}})$ be a Hilbert space, $f\in \mathcal{H}$ and  $\LL, \LL_{0}, \LL_{1}\in L^{\infty}(\om)$ be lower semicontinuous,  bounded away from zero. Let also $A\in \mathcal{L}(L^{p}(\om), \mathcal{H})$ with $p\in (1, d^{\ast}]$,  $d^\ast=d/(d-1)$ ($d^{\ast}=\infty$ if $d=1$). Then both problems
\begin{align}
&\min_{u\in \bv(\om)}\; \frac{1}{2}\|Au-f\|_{\mathcal{H}}^{2} +\tv_{\LL}(u), \label{weighted_TV_min}\\
&\min_{u\in \bv(\om)}\; \frac{1}{2}\|Au-f\|_{\mathcal{H}}^{2} +\tgv_{\LL_{0}, \LL_{1}}(u),  \label{weighted_TGV_min}
\end{align}
admit a solution in $\bv(\om)$.
\end{proposition}
 \begin{proof}
 The proof readily follows by combining \cite[Thm 3.2, Prop. 5.9]{davoli2023dyadic}, \cite[Prop. 5.1]{structuralTV} and \cite[Thm. 2.11 \& Prop. 5.17]{bredies2020higher} and we omit it here.
 \end{proof}

\noindent
We also recall the following proposition from \cite{Papafitsoros_Valkonen_2015}, which dictates that when the ratio $\lambda_{0}/\lambda_{1}$ is large enough, $\tgv_{\lambda_{0}, \lambda_{1}}$ essentially behaves like $\tv_{\lambda_{1}}$.
\begin{proposition}[from \cite{Papafitsoros_Valkonen_2015}]\label{prop:tgv_large_ratio}
There exists a constant $C>0$ depending only on the domain $\om$ such that if $\lambda_{0}, \lambda_{1}>0$ satisfy $\lambda_{0}/\lambda_{1}>C$ then
\begin{equation}\label{tgv_large_ratio}
\tgv_{\lambda_{0}, \lambda_{1}}(u)=\lambda_{1} |Du-m_{\mathcal{E}}(\nabla u)|(\Omega), \quad \text{for all }u\in \bv(\om),
\end{equation}
where for  $v\in L^{1}(\om,\RR^{d})$ we define
$m_{\mathcal{E}}:=\operatorname{argmin}\,\{\|v-w\|_{L^{1}(\om,\RR^{d})}:\, w\in Ker \mathcal{E}\}$.
Recall that $Ker \mathcal{E}=\{r(x)=Bx+c:\;c\in\mathbb{R}^{d},\,B\in \mathbb{R}^{d\times d} \text{ skew symmetric}\}$.
\end{proposition}

\subsection{U-Net plus unrolled PDHG for TGV}\label{sec:unet_pdhg}

The combined neural network-based algorithm unrolling technique for the estimation of spatially varying $\LL_{0}, \LL_{1}$  that we adopt here follows \cite{Kofler_2023}. Let $u_{n}=S^{n}(\tilde{u}_{0}, f, (\LL_{0}, \LL_{1}), A)$, $n\in\mathbb{N}$ denote the iterates of some algorithm  that solves \eqref{weighted_TGV_min}. That is, $u_{n}\to u^{\ast}$ as $n\to \infty$, where $u^{\ast}$ is a solution of \eqref{weighted_TGV_min}. Here $\tilde{u}_{0}$ denotes a suitable initialisation in the image space e.g.\ $\tilde{u}_{0}=A^{\ast} f$, with $A^{\ast}$ denoting the adjoint of $A$. Next we denote with $\mathrm{NET}_{\theta}:A^{\ast}f \mapsto (\LL_{0}, \LL_{1})$ a deep convolutional neural network with learnable parameters $\theta$ (a U-Net \cite{Ronneberger2015} in our implementations). Then,  for a \emph{fixed} $N\in \mathbb{N}$, we define the overall network
\begin{equation}\label{unrolled_network}
\mathcal{N}_{\theta}^{N}(f)= S^{N}(A^{\ast}(f), f, \mathrm{NET}_{\theta}(A^{\ast}f), A).
\end{equation}
The unrolled network \eqref{unrolled_network} can then be trained end-to-end in a supervised fashion using a dataset of data-ground truth pairs $(f^{i}, u_{\mathrm{true}}^{i})_{i=1}^{M}$, and an appropriate pairwise distance function $l$, 
\begin{equation}\label{supervised_learning}
\min_{\theta}\; \mathrm{Loss}(\theta):= \frac{1}{M} \sum_{i=1}^{M} l(\mathcal{N}_{\theta}^{N}(f^{i}), u_{\mathrm{true}}^{i}),
\end{equation}
see also Figure \ref{fig:workflow} for an illustration of the denoising case.
\begin{figure}[t]
\includegraphics[width=\textwidth]{workflow/workflow_editedbyTTV.pdf}
\caption{Visualisation of the network $\mathcal{N}_{\theta}^{N}: f\mapsto (\mathrm{\Lambda}_{0}, \mathrm{\Lambda}_{1})\mapsto u$ of \eqref{unrolled_network} (red arrows) and its training procedure \eqref{supervised_learning} (black arrows) for the denoising case.}\label{fig:workflow}
\end{figure}
Hence, the network $\mathrm{NET}_{\theta}$ implicitly learns to output  spatially varying regularisation parameters that force an approximate solution of \eqref{weighted_TGV_min} to be close to the ground truth. Because $\mathrm{NET}_{\theta}$ will be overparameterised and thus expressive enough, it is expected that when the trained $\mathcal{N}_{\theta}^{N}$ is applied to  new unseen data $f^{\mathrm{test}}$ it will still produce suitable data-adaptive parameters even though it does not have access to the  ground truth.
Henceforth, we denote the corresponding networks that follow the above approach by U-TV and U-TGV. We stress that, even though U-TV was introduced in \cite{Kofler_2023}, that work did not include 2D denoising and 2D MRI results, which we provide here for comparison with U-TGV. We finally note that here we used the anisotropic versions of  TV and TGV  with their standard discretisations.



\section{Numerical experiments in image denoising}\label{sec:denoising}

\subsection{Set-up}\label{sec:denoising-set-up}
 \begin{enumerate}[leftmargin=0pt]
 \item[] \emph{The training dataset:} To train the U-TV and U-TGV models for denoising we used the SeaTurtleID2022 dataset \cite{Adam_2024_WACV}, which is originally designed for re-identification tasks. The rationale  was to test among others the model performance when trained on a specific image distribution, here underwater images of sea turtles, and subsequently tested to images outside this distribution. We randomly selected 500 and 50 images for the training and the validation set respectively.
 All images were cropped and rescaled to $512\times512$, converted to grayscale and normalised to $[0,1]$.
 During training, the noisy input images were generated on the fly by corrupting the target images by zero-mean Gaussian noise with standard deviation randomly chosen in $[0, 0.2]$.
 \item[] \emph{The architecture of $\mathrm{NET}_{\theta}$:}
 We used a U-Net architecture \cite{Ronneberger2015} consisting of three encoding and three decoding blocks, and two final convolutional layers, resulting in 1-128-256-512-1024-512-256-128-2 structure (-1 for U-TV). All convolutional layers have a kernel size of $3 \times 3$ and a stride of 1.
 We used the Leaky ReLU as activation function with a negative slope of $0.01$.
 The map $0.1\times \mathrm{softplus(\cdot)}$  was applied to the final layer to guarantee positiveness for the parameters $\LL, \LL_{0}, \LL_{1}$. Overall, the number of trainable parameters $\theta$ was 28,712,577 and 28,712,706 for  U-TV and U-TGV  respectively.
\item[] \emph{The choice of solution algorithm:}
As an algorithm for TV minimisation, we used the standard PDHG algorithm \cite{chambolle2011first}, see also \cite[Alg. 5.1]{Kofler_2023}. Regarding step sizes, we chose $\sigma = \tau = \text{sigmoid}(10) / \sqrt{13}$.  We also used the PDHG algorithm for U-TGV as outlined in \cite{tgvcolour}, setting $\sigma = \tau = 0.29$. In both algorithms, we set the value of extrapolation parameter to $\text{sigmoid}(10)$,  where $\text{sigmoid}(y) = 1 / (1 + \exp(-y))$. We note that these values guarantee the convergence of the algorithms.
\item[] \emph{The choice of $N$:} The number of unrolled PDHG iterations is crucial since it should be large enough to approximate the solution of the variational problem. However, setting it too large potentially unnecessarily increases the computational cost (GPU-memory and time) during training. We set $N=256$ which, according to our observations, achieved a good balance.
\item[] \emph{The training procedure:} We used the Adam optimiser \cite{kingma2014adam}  with
a learning rate of $10^{-4}$, a batch size of 1, and the MSE loss.
The number of training epochs for  U-TV and U-TGV  was 200 and 100 respectively, beyond which, the performance of the models when applied to the validation data no longer improved. Training took approximately 10 and 20 hours for U-TV and U-TGV respectively, on an RTX 4090 (24GB VRAM).\\
The full code for all the experiments can be found in \url{https://github.com/trung-vt/LearningRegularizationParametersForTGV}. 
 \end{enumerate}


\subsection{Results}
\input{denoising/figures/orestis_test_case_noise_0_10_editedbyKP.tex}

\input{denoising/figures/parrot_test_case_noise_0_10_editedbyKP.tex}

Figures \ref{fig:turtle_test_case_noise_0_10} and \ref{fig:parrot_test_case_noise_0_10} show extensive results for the ``turtle'' and ``parrot'' images.
The ``turtle'' image belongs to the same distribution of images (the SeaTurtleID2022 dataset) the models were trained with, but was not part of the training set.
Apart from  U-TV and U-TGV, we also show the standard  TV and TGV results (best scalar parameter with respect to SSIM, found by a grid-search), as well the spatially varying bilevel TGV result of \cite{bilevelTGV} (unsupervised). We also report the spatially varying TV results (denoted by WTV) obtained by the algorithm proposed in \cite{Calatroni_2019} using a maximum-likelihood-type procedure (unsupervised).
We see that for both images U-TGV gives the best reconstruction, slightly better than U-TV, with both approaches significantly outperforming the other methods, both visually and quantitatively.
We also performed experiments with various noise levels in 50  natural images from
\cite{CCIA_dataset_UGR_dataset_2003}.
The results, shown in Table \ref{tab:summary_standard50_tests}, are aligned with the ones of the previous figures indicating that the models perform well in a wider distribution of images than the one trained on.




\subsection{Structure of the parameter-maps}\label{sec:triple_edge}

Further insights are gained by examining the parameter maps in Figures \ref{fig:turtle_test_case_noise_0_10} and \ref{fig:parrot_test_case_noise_0_10}. By inspecting the ratio $\mathrm{\Lambda}_{0}/\mathrm{\Lambda}_{1}$ we can see the areas of the image where  TGV locally behaves like TV, c.f.\ Proposition \ref{prop:tgv_large_ratio}. This ratio is indeed higher in flatter, almost constant, areas.
A surprising observation is that the structure of $\LL$ for  U-TV is significantly different than the one of $\LL_{1}$ for U-TGV. While $\LL$ takes small values at edges and detailed areas of the image,  the values of $\LL_{1}$ alternate between being high-low-high at the edges. We also observe that the values of $\LL_{0}$ are small at a wider neighbourhood around the edges. To obtain a better visualisation of this \emph{triple-edge} phenomenon,  in Figure \ref{fig:synthetic_square_triple_edge}, we show the denoising results of the ``square'' image and the corresponding $\LL_{0}, \LL_{1}$ maps. We observe that $\LL_{1}$ takes small values exactly at the edges, resulting in small penalisation of the gradient there,  while $\LL_{0}$ takes small values at a wider area. Both parameter maps take larger values further away from the edges.


\input{denoising/tables/summary_standard50_tests.tex}

\input{denoising/figures/synthetic_square_triple_edge.tex}



\section{Numerical experiments in MRI reconstruction}\label{sec:mri}
Here, we consider the case of accelerated MR image reconstruction, where the forward model in \eqref{intro:general_eq} is given by $A=P F$, where $F$ denotes the 2D Fourier transform and $P$ denotes a projection onto the set of the acquired measurements.\\[1em]
\subsection{Set-up}\label{sec:mri-set-up}
\begin{enumerate}[leftmargin=0pt]
 \item[] \emph{The training dataset:} The dataset consists of $M=3452$ pairs of retrospectively undersampled MR measurements and target images $(f^i,u_{\mathrm{true}}^{i})$ that are generated according to \eqref{intro:general_eq}, where the ground-truth images $u_{\mathrm{true}}^{i}$ are random samples extracted from 598 subjects from the fastMRI multi-coil brain dataset \cite{zbontar2018fastmri}. Since MR images are typically complex-valued and the fastMRI only provides root-sum-of-square (RSS) reconstructions as target images, we first estimated coil sensitivity maps from the fully sampled multi-coil measurements with \texttt{MRpro} \cite{zimmermann2025mrpro} using the method described in \cite{walsh2000adaptive}. Then, to obtain complex-valued images, we applied the adjoint of the multi-coil MR forward operator to the multi-coil measurement data and finally cropped the resulting images to a size of $320\times320$. We then retrospectively generated k-space data with random acceleration factor $R$ from $4, 5,\ldots, 8$ and adding zero-mean Gaussian noise with a random standard deviation in $[0, 0.2]$.  We used 3000, 150 and 302 images for training, validation and testing, respectively.



 \item[] \emph{The architecture of $\mathrm{NET}_{\theta}$:}
 The employed U-Net has a similar structure as before. 
 \item[] \emph{The choice of solution algorithm:}
As in the denoising case, 
we unrolled the PDHG algorithm as described in \cite{Knoll_2011, Kofler_2023}. This time we let the step sizes $\sigma, \tau$ to be trainable starting from $\sigma = \tau = 
1 / \sqrt{3}$, after which they took the values $\sigma=0.3414$, $\tau=0.3255$ for U-TV and $\sigma=0.1695$, $\tau=0.6553$ for U-TGV.
We set the extrapolation parameter $\theta=1$ for both models.
\item[] \emph{The choice of $N$:}
As before, we set $N=256$ for both models.
\item[] \emph{The training procedure:} We used the AdamW optimiser with a learning rate of $10^{-4}$, a weight decay  of $10^{-5}$ \cite{loshchilov2019decoupled}, a batch size of 1 and the MSE loss.
    Both models were trained for 100 epochs, and the epoch with the lowest validation error was chosen as the final model, $43^{\text{rd}}$  and $72^{\text{nd}}$ for U-TV and U-TGV respectively. Training took approximately 55 and 87 hours for U-TV and U-TGV respectively, done on the same machine as in denoising.
\end{enumerate}

\subsection{Results}
\input{results/sample_0-single_R_value_editedbyKP.tex}

Figure \ref{fig:mri_results}
compares reconstructions for a test image under $R=8$, and Gaussian noise of $sd=0.05$. The best result is that of U-TGV,  with a significant margin over U-TV, particularly with respect to SSIM, and more than 4dB higher PSNR than the scalar version. We  observe again a  triple-edge structure for the parameter maps, with $\mathrm{\Lambda}_{0}$ taking small values in larger areas than $\mathrm{\Lambda}_{1}$. In Table \ref{tab:summary_MRI_tests}, we summarise the results for all  test images for $R=4,8$ and various noise levels. Apart from the low noise regime, U-TGV  performs slightly better than U-TV with respect to PSNR, but significantly better with respect to SSIM  in all cases. Both models heavily outperform the scalar versions.





 \input{results/mean_and_std_editedbyKP.tex}

\section{Conclusion}\label{sec:conclusion}
We showed that using a CNN to infer spatially varying TGV regularisation parameters, which are highly adaptive to the data, significantly boosts its performance both in denoising and MRI reconstruction. The results enjoy high interpretability being solutions of variational problems with a handcrafted prior, as all the neural network-related ``black-boxness'' is transferred to the regularisation parameters rather than the reconstructed images themselves. Related to that, a further theoretical investigation of the structure of the parameter maps (triple-edge phenomenon) and their interplay in the regularisation process is of great interest.\\







\noindent
\textbf{Acknowledgments:}
We thank Luca Calatroni for producing the WTV results. 





\bibliographystyle{amsplain}
\bibliography{refs}
\end{document}
