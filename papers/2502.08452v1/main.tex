\documentclass[journal, twoside]{IEEEtran}
\let\labelindent\relax
% math related package
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
% table related package
\usepackage{tabularx}
\usepackage{ulem}
\usepackage{threeparttable}
\usepackage{booktabs}
% figure related package
\usepackage{graphicx}
\usepackage{wrapfig}
% algorithm related package
\usepackage[lined, ruled, linesnumbered, commentsnumbered, noend]{algorithm2e}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\renewcommand{\thefootnote}{\tiny~Note \arabic{footnote}}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
% unknown
\usepackage{subcaption}
\usepackage{eqlist}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{stfloats}
\usepackage[numbers]{natbib}
\normalem

\def\mathbi#1{\textbf{\em #1}}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{comment}
% \usepackage{flushend}
\newcommand{\harada}[1]{\textcolor{green}{#1}}
\newcommand{\recheck}[1]{\textcolor{olive}{#1}}
\usepackage{multirow}
\newcommand\dunderline[3][-2pt]{{%
  \setbox0=\hbox{#3}
  \ooalign{\copy0\cr\rule[\dimexpr#1-#2\relax]{\wd0}{#2}}}}
\newcommand{\rev}[1]{\textcolor{magenta}{#1}}
\hyphenation{}
\usepackage{tikz}
\usetikzlibrary{fit,shapes.misc}
\newcommand\marktopleft[1]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-a) at (0,1.5ex) {};%
}
\newcommand\markbottomright[1]{%
    \tikz[overlay,remember picture]
        \node (marker-#1-b) at (0,0) {};%
    \tikz[overlay,remember picture,thick,inner sep=3pt]
        \node[draw, rectangle, color=teal, fit=(marker-#1-a.center) (marker-#1-b.center)] {};%
}
\usepackage{CJKutf8}
\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}
\definecolor{awesome}{rgb}{1.0, 0.13, 0.32}
\newcommand{\wan}[1]{{\color{awesome}\begin{CJK*}{UTF8}{goth}\small#1\end{CJK*}}}
\newcommand{\wanold}[1]{{\color{gray}\begin{CJK*}{UTF8}{goth}\small#1\end{CJK*}}}
\newcommand{\wanextra}[1]{{\color{blue}\begin{CJK*}{UTF8}{goth}\small#1\end{CJK*}}}

\definecolor{darkgreen}{rgb}{0.0, 0.65, 0.0}
\definecolor{babyblue}{rgb}{0.29, 0.75, 0.93}
\newcommand{\yonemaru}[1]{{\color{darkgreen}\begin{CJK*}{UTF8}{goth}\small#1\end{CJK*}}}
\newcommand{\yonemaruextra}[1]{{\color{babyblue}\begin{CJK*}{UTF8}{goth}\small#1\end{CJK*}}}

\newcommand{\lqin}[1]{{\color{babyblue}\begin{CJK*}{UTF8}{gkai}\small#1\end{CJK*}}}

\begin{document}
\title{Learning to Group and Grasp Multiple Objects}
% two-dimensional manifolds
\author{Takahiro Yonemaru, Weiwei Wan$^{*}$, Tatsuki Nishimura, Kensuke Harada
\thanks{All authors are from the Graduate School of Engineering Science, Osaka University, Japan. \\Contact: Weiwei Wan, {\tt\small wan@sys.es.osaka-u.ac.jp}}}
\markboth{Under review by a robotics journal}
{Yonemaru \MakeLowercase{\textit{et al.}}: Learning to Group and Grasp Multiple Objects}
% \maketitle

% \author{Authors' names have been anonymized following the IEEE RAS Double-Anonymous Review Process Guidelines
% \thanks{Authors' affiliations and contact information have been anonymized following the IEEE RAS Double-Anonymous Review Process Guidelines.}}
% \markboth{IEEE Robotics and Automation Letters. Submission for Review, 2025.}
% {Authors' names anonymized: Learning to Group and Grasp Multiple Objects}
\maketitle

\bstctlcite{IEEEexample:BSTcontrol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Simultaneously grasping and transporting multiple objects can significantly enhance robotic work efficiency and has been a key research focus for decades. The primary challenge lies in determining how to push objects, group them, and execute simultaneous grasping for respective groups while considering object distribution and the hardware constraints of the robot. Traditional rule-based methods struggle to flexibly adapt to diverse scenarios. To address this challenge, this paper proposes an imitation learning-based approach. We collect a series of expert demonstrations through teleoperation and train a diffusion policy network, enabling the robot to dynamically generate action sequences for pushing, grouping, and grasping, thereby facilitating efficient multi-object grasping and transportation. We conducted experiments to evaluate the method under different training dataset sizes, varying object quantities, and real-world object scenarios. The results demonstrate that the proposed approach can effectively and adaptively generate multi-object grouping and grasping strategies. With the support of more training data, imitation learning is expected to be an effective approach for solving the multi-object grasping problem.
\end{abstract}

\begin{IEEEkeywords}
Multi-Object Grasping, Manipulation, Imitation Learning
\end{IEEEkeywords}

%==============================================================================================================================
\section{Introduction} % 1.5 page

\IEEEPARstart{I}{n} recent years, advances in AI and sensor technology have enabled the development of robots capable of performing picking tasks in environments where objects are randomly scattered. In such situations, robots need to transport objects efficiently to their target locations. However, a common limitation of robots is that they can typically handle only one object at a time, which leads to slower task execution. Different from robots, humans can grasp and carry multiple objects at once, and are thus more efficient. To bridge this gap, this study focuses on enabling robots to perform simultaneous multi-object grasping, thereby improving their work efficiency.

Previous studies on simultaneous multi-object grasping has identified several challenges. For example, Sakamoto et al. \cite{sakamoto2021efficient}  proposed a rule-based method to push objects closer together before grasping them simultaneously. However, the method cannot handle complex pushing actions, and the combinations of objects that can be grasped together are limited. Agboh et al. \cite{agboh2022multi} studied identifying near objects arranged on a plane and grasping multiple objects by pushing them together as the gripper closes. The method works under specific conditions that objects are not spaced farther apart than the gripper’s maximum opening width. Or else, it will degenerate to single-object grasping. These challenges highlight the need for better strategies to group objects through complex pushing actions and to select suitable combinations of objects for simultaneous grasping. Unfortunately, making such decisions based on fixed rules is difficult.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/teaser.jpg}
    \caption{(a) Collecting human expert data using teleoperation. (b) The collected dataset is used to train a diffusion policy network for inferring actions. (c) The policy network uses past observations to generate appropriate pushing or grasping actions.}
    \label{workflow}
\end{figure}  

In this context, we propose addressing these issues by using imitation learning and enabling robots to learn how humans decide which objects to group and grasp. The goal is to allow robots to perform simultaneous multi-object grasping in scattered environments. By acquiring grasping strategies through imitation learning, the proposed method enables robots to automatically choose suitable object combinations and perform the necessary pushing actions to group them. The method can overcome the limitations of existing methods and improve the efficiency and versatility of robotic picking tasks.

Specifically, the proposed framework comprises three main components: data collection, learning, and inference. Fig. \ref{workflow} illustrates these components and workflow. In the data collection phase shown in Fig. \ref{workflow}(a), a teleoperation system is developed by integrating a vision-based hand-detection model, allowing human operators to control the robot using natural hand movements. The setup enables collecting data like robot joint angles, gripper opening and closing states, and images from by the global and local cameras mounted in the environment and on the robot hand. In the learning phase shown in Fig. \ref{workflow}(b), the framework employs Diffusion Policy \cite{chi2023diffusion} to learn appropriate robot actions several steps ahead, based on the robot's state from previous steps. Finally, in the inference phase shown in Fig. \ref{workflow}(c), the framework uses the robot's joint angles, gripper states, and camera images observed from a few steps before to predict joint angles and gripper states for subsequent steps, thereby generating simultaneous grasping motions for multiple objects. The trained neural network learns policies from human demonstrations to perform object grouping and grasping, thus significantly improving robotic picking efficiency.

% we developed a vision-based teleoperation system to collect data and employed a diffusion model to train an object grouping neural network. 

% The proposed framework comprises three main components: data collection, learning, and inference. In the data collection phase, an intuitive teleoperation system is developed by integrating RGB-D cameras with a hand-detection model, allowing human operators to control the robot using natural hand movements. This setup enables the collection of data, including robot joint angles, gripper opening and closing states, and images captured by cameras mounted on the robot. In the learning phase, the framework employs Diffusion Policy \cite{chi2023diffusion} to generate appropriate robot actions several steps ahead, based on the robot's state from previous steps. Finally, in the inference phase, the framework uses the robot's joint angles, gripper states, and camera observations from a few steps before to predict joint angles and gripper states for subsequent steps, thereby generating simultaneous grasping motions for multiple objects.



% We carried out experiments to validate the performance of our proposed method. Experimental results show that ...

% Experiments were conducted to validate the proposed framework by performing inference with the trained model. The evaluation focused on verifying the grouping of objects through pushing actions and the selection of object groups suitable for simultaneous grasping in cluttered environments.
Two groups of experiments were conducted to validate the proposed framework. The first group focused on comparing and analyzing the influence of different training data volumes and object quantities on learning and inference. The second group conducted fine-tuning a model pre-trained on a dataset of simple objects to more practical scenarios and evaluated the generality of the proposed method to real-world objects. The experimental results validated the effectiveness of the proposed method and validated the adaptivity of fine-tuned models. Additionally, the results indicated that training a well-performing policy network requires a large amount of high-quality data, with a positive correlation between data volume and learning performance. Developing efficient methods for collecting such data could potentially improve both learning and inference performance.


\section{Related Work} % 2 page

Multi-object simultaneous grasping has been attracting significant attention for over two decades. As pioneering contributions, Aiyama et al. \cite{aiyama98} studied using two manipulators to cooperatively grasp and lift an array of box-shaped objects. Harada and Kaneko \cite{harada00neighbouring}\cite{harada98enveloping} studied the neighboring equilibrium of multiple cylindrical objects and their enveloping grasps. Yoshikawa et al. \cite{yoshikawa2001optimization} proposed the optimal power grasp condition for multiple objects from the viewpoint of reducing finger joint torques. Yamada et al. \cite{yamada2009grasp} presented a more general analysis of the grasp stability of multiple objects grasped by fingers considering objects' surface curvatures and the potential energy of the spring models in contact. The authors later extended the theories to multi-fingered hands with revolute joints \cite{yamada2012stability} and 3D objects \cite{yamada2015static}. 

More recent work by Chen et al. \cite{chen2021multiobjectgrasping} inserted the Barrett hand into a pile of objects and estimated the number of grasped items using a deep learning model. Shenoy et al. \cite{shenoy2022multi} further focused on efficiently transferring the grasped multiple objects to a separate container. Takahashi et al. \cite{takahashi2021uncertainty} applied multi-object grasping evaluation to grasping a given amount of granular foods. The authors also explored post-grasping methods for fine adjustments \cite{takahashi2021target}. Li et al. \cite{li2024graspmultiple} developed a framework for planning and learning to grasp (lift) multiple objects. Jiang et al. \cite{jiang24multiobject} developed a vacuum gripper equipped with multiple suction cups and also the grasp planning methods to simultaneously pick multiple objects from a tray. Nguyen et al. \cite{nguyen2023wiringclaw} proposed the Wiring-Claw Gripper, which took advantage of the soft interaction between wires in a claw and objects for simultaneous multi-object grasping.

The above approaches achieved stable grasping of multiple or unknown objects. However, they often assumed that the objects were initially positioned in proximity and thus are less applicable to environments where objects are scattered. To tackle such limitations, several studies have explored approaches that utilize pushing actions to rearrange objects into positions more conducive to simultaneous grasping. For example, Sakamoto et al. \cite{sakamoto2021efficient} proposed a method for grasping two distant objects by first pushing one object closer to the other and then grasping them together. The approach determined whether to grasp a single object, grasp two objects simultaneously, or push one object closer for simultaneous grasping based on the distance and friction between objects. Using dynamic programming, the authors optimized the grasping sequence and enabled simultaneous grasping of two rectangular objects by rotating them about the intersection of their long axes. Agboh et al. \cite{agboh2022multi}\cite{agboh2023learning} proposed the $\mu$-MOG framework to simultaneously grasp multiple rigid convex polygonal objects scattered on a plane. The method facilitated the simultaneous grasping of cluttered objects by aligning them using a gripper's opening jaw. Shrey et al. \cite{aeron2023push} partitioned clusters to ensure that the sum of the minimum graspable diameters within a cluster remained below a gripper’s width and leveraged linear pushing actions along lines perpendicular to the gripper fingers to bring object centroids closer, thus enabled simultaneous grasping of objects initially outside the gripper's opening width. Kishore et al. \cite{srinivas2023busboy} proposed a method for efficiently transporting cups, bowls, and utensils on tables by combining pushing and stacking actions for simultaneous grasping. The method especially employed inward-pushing actions to reduce positional uncertainty. 

The pushing-based methods have addressed the problem of scattered objects to some extent. However, they are predominantly rule-based, making it challenging to generate complex pushing trajectories required for handling intricate grouping and grasping tasks. In this paper, we propose employing imitation learning to generate complex pushing trajectories and solve the constraints on combinations of objects that can be grasped simultaneously. Previously, reinforcement learning methods have attracted much attention in robotic pushing and grasping as they can adapt to various object arrangements. For instance, Zeng et al. \cite{zeng2018learning} achieved efficient grasping by enabling the complementary relationship between pushing and grasping actions to be learned through self-supervised deep reinforcement learning. Chen et al. \cite{chen2020combining} proposed a grasping strategy that combines rule-based methods with reinforcement learning to integrate pushing and grasping actions. Wang et al. \cite{wang2024self} attained high success rates and robustness by leveraging self-supervised deep reinforcement learning to learn an integrated model of pushing and grasping actions. These studies demonstrate that reinforcement learning has the advantage of adapting to unknown environments. 

A key challenge in reinforcement learning lies in the design of rewards. This issue becomes particularly pronounced in the context of this study as we use pushing to achieve grouping, followed by grasping. Designing rewards that facilitate rapid convergence for the pushing-grouping-grasping routine is notably difficult. A learning method conceptually similar to reinforcement learning but not dependent on reward design is imitation learning. Recent studies have showcased the potential of imitation learning in robotic manipulation applications. For example, ACT \cite{zhao2023learning} has achieved high success rates in delicate tasks such as opening and closing Ziploc bags and inserting batteries into controllers. Diffusion policy \cite{chi2023diffusion} has enabled high success rates in tasks requiring diverse actions and significant flexibility, such as spreading sauce on pizza dough or relocating T-shape blocks and mugs placed at random positions on a plane to target positions and orientations. Given its advantages, this research adopts imitation learning as the primary methodology. By learning flexible pushing actions and grasping decisions from human demonstrations, our approach enables dynamic selection of object combinations, rearrangement through complex pushing trajectories, and adaptive grasping sequences that were previously difficult to attain with rule-based or reinforcement learning methods.

\section{Data Collection Using Visual Teleoperation}

Similar to AnyTeleop [26], we develop a teleoperation system for a robot by detecting the hand skeleton pose using an RGB-D camera. Specifically, we employ the WiLoR [27] model for detecting the hand skeleton pose. This model estimates the 3D positions of keypoints on the hand based on a single RGB image. The output is described in the $\Sigma_\textnormal{wilor}$ coordinate system shown in Fig. \ref{mapping}(a). The origin of the coordinate system is defined at the wrist position, with the $\textbf{z}$-axis aligned with the camera's line of sight, the $\textbf{x}$-axis corresponding to the width direction of the captured image, and the $\textbf{y}$-axis representing the height direction. The keypoints, including the positions on the wrist, index finger, and thumb used in this method, are illustrated in Fig. \ref{mapping}(b). After extracting the 3D hand skeleton data within the above coordinate system, we further integrate the data with the wrist depth value obtained from the depth frame of the same RGB-D camera and derive the 3D hand skeleton data in the coordinate system of the depth camera, $\Sigma_\textnormal{cam}$. After that, we transform the 3D data into commands for the robot, which includes the robot gripper's position, orientation, and open/close states, thereby enabling the teleoperation of the robot. The specific mapping between the coordinate system and the robot's control is defined as follows.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/mapping.jpg}
    \caption{Mapping relationship between hand poses detected by WiLoR and the end-effector of a robot. Teleoperation is achieved through the mapping to control the robot and collect training data.}
    \label{mapping}
\end{figure}  

\textit{Jaw bottom of the robot gripper:} 
The jaw bottom center of the robot's end-effector is defined as a position corresponding to the MCP joint of the index finger and is transformed to the world coordinate system using the following equation:
\begin{equation}
\prescript{\Sigma_\text{world}}{}{
\begin{bmatrix}
\boldsymbol{p}_{\text{eef}}\\
1
\end{bmatrix}}
= {}^{\Sigma_\text{world}}\textbf{T}_{\Sigma_\text{cam}}{}^{\Sigma_\text{cam}}\textbf{T}_{\Sigma_\text{wilor}}
\prescript{\Sigma_\text{wilor}}{}
{\begin{bmatrix}
\boldsymbol{p}_{5}\\
1
\end{bmatrix}}
\label{affine-pos}
\end{equation}

\textit{Rotation of the robot gripper:} 
The rotation of the end-effector, ${}^{\Sigma_\text{world}}\boldsymbol{R}_{\textnormal{eef}}$, is constrained such that its approaching direction is always vertically downward (the last columne or $\boldsymbol{z}$ axis of ${}^{\Sigma_\text{world}}\boldsymbol{R}_{\textnormal{eef}}$ is parallel with and inversely aligns with the $\boldsymbol{z}$ axis of $\Sigma_{\textnormal{world}}$ in Fig. \ref{mapping}(a)). The rotation around the approaching direction is defined using the position of the thumb tip (${}^{\Sigma_\text{wilor}}\boldsymbol{p}_4$) and the position of the index finger tip (${}^{\Sigma_\text{wilor}}\boldsymbol{p}_8$) following equation \eqref{ee_rot}.
\begin{equation}
{}^{\Sigma_\text{world}}\boldsymbol{R}_\text{eef}
=\begin{bmatrix}\hat{\boldsymbol{d}}_x & \hat{\textbf{d}}_y & 0 \\\hat{\boldsymbol{d}}_y &-\hat{\boldsymbol{d}}_x & 0 \\0 & 0 & 1 
\end{bmatrix},~
\hat{\boldsymbol{d}} = \frac{{}^{\Sigma_\text{wilor}}\boldsymbol{p}_8 - {}^{\Sigma_\text{wilor}}\boldsymbol{p}_4}{\|{}^{\Sigma_\text{wilor}}\boldsymbol{p}_8 - {}^{\Sigma_\text{wilor}}\boldsymbol{p}_4\|}
\label{ee_rot}
\end{equation}

\textit{Gripping state:} The robot gripper's open/close state, $g$, is defined based on the distance between the thumb and the index finger following.
\begin{equation}
\label{gripperstate}
g = 
\begin{cases}
0 & \text{if } d \geq \mu_\text{up} \\
1 & \text{if } d < \mu_\text{bot}
\end{cases},~
d =\frac{\|{}^{\Sigma_\text{wilor}}\boldsymbol{p}_8 - {}^{\Sigma_\text{wilor}}\boldsymbol{p}_4\| }{\max\|{}^{\Sigma_\text{wilor}}\boldsymbol{p}_8 - {}^{\Sigma_\text{wilor}}\boldsymbol{p}_4\|}
\end{equation}
Here, $g=0$ denotes the gripper in an open state, while $g=1$ indicates the gripper in a closed state. The threshold values $\mu_\text{up}$ and $\mu_\text{up}$ are chosen to be 0.8 and 0.1 for practical purpose. The distance between the thumb and the index finger is normalized by dividing its maximum distance.

Based on these mapping relationships, we define the observation $\boldsymbol{o}_{t}$ at a timestamp $t$ as
\begin{equation}
    \boldsymbol{o}_{t} = \left\{\boldsymbol{i}^{\text{gcam}}_t,\boldsymbol{i}^{\text{lcam}}_t,\boldsymbol{s}_{t}\right\},
\end{equation}
where $\boldsymbol{i}^{\text{gcam}}_t, \boldsymbol{i}^{\text{lcam}}_t$ are video frames (images) captured by the global and local cameras. $\boldsymbol{s}_t$ is the state of the robot and is defined as
\begin{equation}
    \boldsymbol{s}_t=\left\{{p}_x,{p}_y,{p}_z,{q}_w,{q}_x,{q}_y,{q}_z,{g}\right\},
\end{equation}
where $({p}_x,{p}_y,{p}_z)$ are the ${}^{\Sigma_\text{world}}\boldsymbol{p}_{\text{eef}}$ position of the robot at time $t$. $({q}_w,{q}_x,{q}_y,{q}_z)$ are the ${}^{\Sigma_\text{world}}\textbf{R}_{\text{eef}}$ rotation of the robot at time $t$. $g$ is the gripper open/close state at time $t$.

Based on this observation and the definition of the robot's state, the robot's action at timestamp $t$ is defined as:
\begin{equation}
    \boldsymbol{a}_t=\left\{{p}_x,{p}_y,{p}_z,{q}_w,{q}_x,{q}_y,{q}_z, {g}\right\}.
\end{equation}
It has the same mathematical representation as $\boldsymbol{s}_t$, except that the constituent values denotes the position, rotation, and gripper state that the robot is going to act to at the next times stamp.

\section{Network Selection and Learning}

We use the method proposed in \cite{chi2023diffusion} for imitation learning. The method enables the prediction of actions not just for a single step but for multiple future steps based on past observations. It can handle scenarios where multiple strategies exist and can achieve a goal with flexibility. Using the notations defined in the previous section, the input, prediction output, and usage of the model are as follows.

Based on the past $T_o$ time steps of observations at time $t$ $\boldsymbol{O}_{t}=\{\boldsymbol{o}_{t-T_o+1}$, $\boldsymbol{o}_{t-T_o+2}$, $...$, $\boldsymbol{o}_t\}$, the used method will infer actions for $T_{p}$ time steps, namely $\boldsymbol{A}_{t}=\{\boldsymbol{a}_{t-T_o+1}$, $\boldsymbol{a}_{t-T_o+2}$, $...$, $\boldsymbol{a}_t$, $\boldsymbol{a}_{t+1}$, $...$, $\boldsymbol{a}_{t+T_p-T_o}\}$. The inferred $\boldsymbol{A}_{t}$ includes the predicted past actions $\{\boldsymbol{a}_{t-T_o+1}$, $\boldsymbol{a}_{t-T_o+2}$, $...$, $\boldsymbol{a}_{t-1}\}$ and the predicted future actions $\{\boldsymbol{a}_{t}$, $\boldsymbol{a}_{t+1}$, $...$, $\boldsymbol{a}_{t+T_p-T_o}\}$. The predicted past actions correspond to the observations of the past $T_o$ time steps. The predicted future actions are the learned policies for the next $T_p-T_o$ time steps. The robot will execute $T_a$ steps of actions $\{\boldsymbol{a}_{t}, \boldsymbol{a}_{t+1}, ..., \boldsymbol{a}_{t+T_a-1}\}$ for collecting new observations and conduct new predictions. $T_a$ is selected to be a value smaller than $T_p-T_o$. Fig. \ref{network_io} illustrates the details of the time step division for observation, prediction, and execution.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/network_inputoutput.jpg}
    \caption{The imitation learning method employed in this study takes as input the observations from the past $T_o$ time steps. The output consists of the inferred likely actions for the past $T_o$ time steps and predicted actions for the future $T_p-T_o$ time steps. The first $T_a$ predicted future actions are executed for iterative observation-prediction looping.}
    \label{network_io}
\end{figure}  

The internal structure of the used method is illustrated in Fig. \ref{denoising}. There are in total $T_o$ observations. Each observation consists of two image frames respectively captured from the global and local cameras. They are processed separately using two ResNet-18 for feature extraction. Each image frame is compressed into a $N$-dimensional feature vector after feature extraction, and the total observations form two $N\times T_o$-dimensional vectors. Additionally, the observation includes the robot's state, which is an $8$-dimensional vector for each observation as discussed in the previous section. In total, the robot states form a $8\times T_o$-dimensional vector. The two $N\times T_o$-dimensional vectors together with the $8\times T_o$-dimensional robot state vector are concatenated to form a $(2N+8)\times T_o$-dimensional vector which serves as the observation embedding of a U-Net core used in the diffusion policy network. 

The U-Net core takes a noisy action sequence (represented as a $8\times T_p$-dimensional vector) as input and predicts the noise in the representation. The output of the U-Net is another $8\times T_p$-dimensional vector where each value corresponds to the estimated noise in the input sequence. This predicted noise is subsequently subtracted from the input sequence within the denoising box shown in Fig. \ref{denoising}. The denoising substraction is mathematically
\begin{equation}
\label{denoising}
\boldsymbol{A}_{t}^{k-1}=\alpha(\boldsymbol{A}_{t}^{k}-\gamma{\boldsymbol{\varepsilon_\theta}}(\boldsymbol{A}_{t}^{k}, \boldsymbol{O}_{t}, k)+\mathcal{N}(0, \sigma^2\boldsymbol{I})),
\end{equation}
where $k$ indicates the $k$th denoising step. $\boldsymbol{A}_{t}^{k}$ represents the noisy action sequence at the $k$th denoising step. $\boldsymbol{\varepsilon_\theta}()$ represents the noise prediction network (namely the U-Net core). $\theta$ indicates the parameters of the network. For each denoising iteration, the iteration step index $k$ is encoded using a $P$-dimensional sinusoidal positional embedding and is also incorporated into the U-Net core to provide temporal context. The positional embedding helps guide the network in estimating how much noise remains to be removed. $\alpha$, $\gamma$, $\sigma$ are parameters for controlling the learning stability\footnote{The default values from the LeRobot source code was adopted. Check the following webpage for details. https://github.com/huggingface/lerobot}. The noisy action sequence is initialized by sampling a Gaussian noise distribution. After undergoing $K$ iterative denoising steps, the diffusion policy network will output the denoised $\boldsymbol{A}_{t}^{0}$ as predicted action sequences.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/denoising.jpg}
    \caption{Internal mechanism of the diffusion policy network and its input-output processing. ResNet-18 is utilized to extract features from the input images. The policy network has a U-Net core, which accepts a noisy action sequence and predicts the noise within it. Image features and positional encoding are embedded into the U-Net core as conditional inputs. Through $K$ iterations, the network progressively removes the noise to acquire the action sequence most suitable for the observations.}
    \label{denoising}
\end{figure}  

To train the denoising network, we draw a sequence of $\boldsymbol{O}_{t}$ and a sequence of expert action $\boldsymbol{A}_{t}^0$ from the collected data set, selects a random diffusion step $k$ and add Gaussian noises $\boldsymbol{\varepsilon}^k$ to the expert actions. The parameters $\theta$ of the U-Net core are learned by minimizing 
\begin{equation}
    \mathcal{L}=||\boldsymbol{\varepsilon}^k-\boldsymbol{\varepsilon_\theta}(\boldsymbol{A}_{t}^0+\boldsymbol{\varepsilon}^k, \boldsymbol{O}_{t}, k)||^2.
\end{equation}

\section{Experiments and Analysis}

\subsection{Experimental environment and parameter settings}

In this section, we present the experiments used to evaluate the effectiveness of the proposed method. A picture of our experimental environment can be found in Fig. \ref{workflow}(a) shown in the introduction section. The experiments were conducted in a real-world environment using the Universal Robots UR3 manipulator, which is a 6 Degree-of-Freedoms (DoFs) robot arm and operates at a maximum control frequency of $125 \text{Hz}$. The end effector employed was the Robotiq 2F-85 gripper. It has a maximum opening width of $85$ mm. The target objects for grouping and grasping were placed within a yellow frame measuring $300$ mm $\times$ $400$ mm. They are expected to be transported to an adjacent tray. Two Intel RealSense D435 cameras are used as the global and local cameras. Only the RGB data from them are used in the experiments. The RGB image sizes are compressed to $240\times320$ pixels in height and width to avoid too much GPU memory consumption. A PC with Intel Core Ultra 7 285K, DDR5-5600 192GB memory, and Nvidia RTX A6000 GPU was used to train the diffusion policy. When collecting human expert data, another Intel RealSense D435 camera is used to track human hand poses and gripping states. Both RGB and Depth data are used during the tracking.

For evaluation, we used two major categories of objects. Both of them have elongated rectangular shape. The first category consists of uniformly red 3D-printed objects, each with a length, width, and height of $120$ mm, $20$ mm, and $20$ mm. These objects are easily distinguishable, making them suitable for quickly verifying the effectiveness of the algorithm. The second category comprises real-world commercial products with similar shapes They include chocolate snack bars and fructose snack bars. The dimensions of these commercial products will be presented later in subsections. Based on the model trained using data collected from the first category, we further fine-tuned the model with the second category to evaluate its transferability and the effectiveness of the proposed method on real-world products. 

For our particular diffusion policy implementation, we set both the ResNet-18 output dimension $N$ and the positional encoding dimension $P$ to be 128, set $T_o$, $T_a$, and $T_p$ to be $2$, $8$, and $16$, respectively. That is, the learned policy takes the consecutive observations $\boldsymbol{o}_{t-1}$ and $\boldsymbol{o}_{t}$ as input and predicts future actions $\boldsymbol{a}_{t}$, $\boldsymbol{a}_{t+1}$, $...$, $\boldsymbol{a}_{t+7}$ for the next 8 steps. By performing inference every 8 steps and executing the predicted actions in a closed-loop control manner, the system continuously adjusts its behavior. The hidden layers of the U-Net are set to $[512, 1024, 2024]$. The optimization method used is AdamW, with a learning rate of $10^{-4}$ and a weight decay of $10^{-6}$. The batch size is set to $64$. For the diffusion process, the number of denoising steps is set to $K=100$.

\subsection{Experiment 1: Randomly placed 3D printed objects}

In the first group of experiments, we focused on the influence by the number of randomly placed objects and number of training data. The evaluation is conducted on tasks where 3 to 6 randomly placed printed objects are grouped and transported using learned policies. The initial position of the UR3 end effector is set above the bin. The robot is expected to group the objects considering their distributions and then grasp and transport them to the bin.

During data collection, we conducted $200$ demonstrations, where the number of objects was randomly set between $3$ and $6$, and the target objects were randomly placed within the yellow frame. The collected training data were divided into two datasets of sizes $100$ and $200$, respectively, and a separate diffusion policy network was trained for each dataset to analyze the impact of data quantity on performance. Each diffusion policy network underwent $300000$ training steps.

During evaluation, $20$ trials were conducted for each number of placed objects. In each trial, the robot’s performance was assessed until it either successfully completed the task or failed. The following metrics were recorded for each trial.
\begin{itemize}[leftmargin=2em]
 \renewcommand{\labelitemi}{$\triangleright$}
    \item \uline{Number of grouping actions (\#P)}: The number of grouping motions performed to gather the objects. 
    \item Number of grasping actions (\#G): The total number of grasp attempts.
    \item \uline{Sqeuence of grasped objects (Seq.))}: The number of objects grasped by each grasping action, recorded in sequential order from the start of the task as a list.
    \item \uline{Task completion (C)}: Whether the all objects were successfully transported to the bin.
    \item \uline{Reason for task failure (R)}: If the task is not completed, the cause of failure is documented.
\end{itemize}
When the task fails to be completed, the failure causes can be attributed into the following three main reasons.
\begin{itemize}[leftmargin=2em]
 \renewcommand{\labelitemi}{$\triangleright$}
    \item Pushing failure (P): An object in contact during the pushing motion may deviate from the end effector and failed to be grouped.
    \item Grasping failure (G): The robot may fail to grasp the objects that were grouped together or drop the grasped object during transportation.
    \item Stagnation: The policy network may repeatedly suggest the same action and get into stagnation. If the robot remains stationary or repeated for more than 5 seconds, we consider it a failure. If stagnation occurs during grouping, it is classified as a pushing failure. Otherwise, it is classified as a grasping failure.
\end{itemize}
Table \ref{tab_exp} shows the detailed results for all trials and statistics at the last row of each dataset-number of objects combination. Due to the limited space on the article page, this table contains numerous abbreviations. The specific meanings can be referenced in the notes at the bottom of the table. Especially in the statistical section, we computed the following values:
\begin{itemize}[leftmargin=2em]
 \renewcommand{\labelitemi}{$\triangleright$}
\item Completion rate: The rate at which all objects were cleared over 20 trials.
\item Transport rate: The average number of successfully transported objects divided by the number of objects placed across 20 trials.
\item \# Objects per grasp: The average number of objects transported per successful grasp across 20 trials.
\end{itemize}
The maximum values of these statistics across both trained networks and all numbers of objects are highlighted in lime, while the minimum values are highlighted in pink.

\begin{table*}[!htbp]
% \renewcommand\arraystretch{1.2}
\caption{Results of randomly placed 3D printed objects}
\label{tab_exp}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{c|c|cclll|cclll|cclll|cclll}
\toprule
\multirow{2.5}{*}{Data} & \multirow{2.5}{*}{ID} & \multicolumn{5}{c|}{Number of Objects: 3} & \multicolumn{5}{c|}{Number of Objects: 4} & \multicolumn{5}{c|}{Number of Objects: 5} & \multicolumn{5}{c}{Number of Objects: 6}\\ \cmidrule{3-22}
                        &    & {\#P} & {\#G} & {Seq.} & C & R & {\#P} & {\#G} & {Seq.} & C & R & {\#P} & {\#G} & {Seq.} & C & R & {\#P} & {\#G} & {Seq.} & C & R\\ \midrule
\multirow{20}{*}{100}   & 1  & 2    &2    & {(1,1)}   & N & P &2    &2    & {(1,2)} & N & P & {1}    & {1}    & {(2)}       & N & P & {0}    & {0}    & {(0)}         & N & P\\
                        & 2  & 2    &2    & {(1,2)}   & Y & - & {3}    & {3}    & {(1,1,2)}   & Y & - & {4}    & {4}    & {(1,1,1,2)} & Y & -   & {0}    & {0}    & {(0)}         &N & P\\
                        & 3  & 2    &2    & {(2,1)}   & Y & - & 0    &0    & {(0)}       & N & P & {0}    & {0}    & {(0)}       &N & P  &  {0}    & {0}    & {(0)}         &N & P  \\
                        & 4  & 0    &0    & {(0)}     & N & P  &0    &0    & {(0)}       & N & P  & {3}    & {3}    & {(1,1,1)}   &N & P    & {0}    & {0}    & {(0)}         &N & P \\
                        & 5  & 2    &2    & {(2,1)}   & Y & - &1    &0    & {(0)}       & N & P & {1}    & {1}    & {(1)}       &N & P & {1}    & {1}    & {(1)}         &N & P  \\
                        & 6 & 0    &0    & {(0)}     & N & P &2    &2    & {(2,2)}     & Y & -  & {1}    & {1}    & {(1)}       &N & P & {0}    & {0}    & {(0)}         &N & P  \\
                       & 7 & 2    &2    & {(1,2)}   & Y & - & 2    &2    & {(2,2)}     & Y & - & {0}    & {0}    & {(0)}       &N & P & {2}    & {2}    & {(1,2)}       &N & P\\
                      & 8 & 0    &0    & {(0)}     & N & P   & 1    &1    & {(1)}       & N & P  & {0}    & {0}    & {(0)}       &N & P  & {2}    & {2}    & {(1,1)}       &N & P  \\
                      & 9 & 1    &1    & {(1)}     & N & P   & 2    &2    & {(2,2)}     & Y & - & {2}    & {2}    & {(3,2)}     & Y & -    & {1}    & {1}    & {(1)}         &N & P\\
                      & 10 & 2    &2    & {(1,2)}   & Y & -   & {3}    & {3}    & {(2,1,1)}   & Y & - & {2}    & {2}    & {(1,1)}     &N & P  & {1}    & {1}    & {(1)}         &N & P \\
                      & 11 & 1    &1    & {(1)}     & N & P    &0    &0    & {(0)}       & N & P & {1}    & {1}    & {(1)}       &N & P     & {0}    & {0}    & {(0)}         &N & P \\
                      & 12 & 1    &1    & {(1)}     & N & P    & {3}    & {3}    & {(1,2,1)}   & Y & - & {0}    & {0}    & {(0)}       &N & P   & {3}    & {3}    & {(3,2,1)}     & Y & -\\
                      & 13 & 1    &1    & {(3)}     & Y & -  &1    &1    & {(1)}       & N & P & {0}    & {0}    & {(0)}       &N & P   & {3}    & {3}    & {(1,3,2)}     & Y & -\\
                      & 14 & 1    &1    & {(3)}     & Y & -  &0    &0    & {(0)}       & N & P & {2}    & {1}    & {(3)}       &N & G    & {1}    & {1}    & {(3)}         &N & P\\
                      & 15 & {3}    & {3}    & {(1,1,1)} & Y & - & {3}    & {3}    & {(2,1,1)}   & Y & -  & {0}    & {0}    & {(0)}       &N & P & {2}    & {2}    & {(1,2)}       &N & P\\
                      & 16 & 2    &2    & {(2,1)}   & Y & - & {3}    & {3}    & {(1,2,1)}   & Y & - & {3}    & {3}    & {(1,3,1)}   & Y & -   & {4}    & {4}    & {(2,2,1,1)}   & Y & -\\
                      & 17 & 2    &2    & {(1,2)}   & Y & -  &2    &2    & {(2,2)}     & Y & - & {0}    & {0}    & {(0)}       &N & P & {0}    & {0}    & {(0)}         &N & P\\
                      & 18 & 2    &2    & {(2,1)}   & Y & -   &0    &0    & {(0)}       & N & P & {0}    & {0}    & {(0)}       &N & P  & {3}    & {3}    & {(2,2,1)}     &N & P \\
                      & 19 & 1    &1    & {(1)}     & N & P   & {4}    & {4}    & {(1,1,1,1)} & Y & - & {2}    & {1}    & {(1)}       &N & G  & {1}    & {1}    & {(1)}         &N & P\\
                      & 20 & {3}    & {3}    & {(1,1,1)} & Y & -   &1    &1    & {(1)}       & N & P   & {0}    & {0}    & {(0)}       &N & P  & {5}    & {5}    & {(1,1,1,1,2)} &  & - \\ \cmidrule{2-22}
& \multirow{4}{*}{St} & \multicolumn{5}{l|}{Completion rate: ~~~~~60\%} & \multicolumn{5}{l|}{Completion rate: ~~~~~50\%} & \multicolumn{5}{l|}{Completion rate: ~~~~~15\%} & \multicolumn{5}{l}{\colorbox{pink}{Completion rate: ~~~~~10\%}}\\
& & \multicolumn{5}{l|}{Transport rate: ~~~~~~~~70\%} & \multicolumn{5}{l|}{Transport rate: ~~~~~~~~58\%} & \multicolumn{5}{l|}{\colorbox{pink}{Transport rate: ~~~~~~~~29\%}} & \multicolumn{5}{l}{Transport rate: ~~~~~~~~37\%}\\
& & \multicolumn{5}{l|}{\colorbox{pink}{\# Objects per grasp: ~~1.4}} & \multicolumn{5}{l|}{\# Objects per grasp: ~~1.44} & \multicolumn{5}{l|}{\# Objects per grasp: ~~1.45} & \multicolumn{5}{l}{\# Objects per grasp: ~~1.52}\\
                      \midrule
\multirow{20}{*}{200} & 1 & 2    &2    & {(2,1)}   & Y & - &1    &1    & {(2)}       & N & P  & {3}    & {3}    & {(1,3,1)}   & Y & - & {3}    & {3}    & {(3,2,1)}     & Y & - \\
                      & 2  & 1    &0    & {(0)}     & N & G & 2    &2    & {(3,1)}     & Y & - & {4}    & {4}    & {(1,1,2,1)} & Y & - & {4}    & {4}    & {(1,2,2,1)}   & Y & - \\
                      & 3 & 2    &2    & {(2,1)}   & Y & - & {3}    & {3}    & {(2,1,1)}   & Y & - & {3}    & {3}    & {(2,2,1)}   & Y & - & {3}    & {3}    & {(2,3,1)}     & Y & - \\
                      & 4  & 1    &1    & {(3)}     & Y & - & {3}    & {3}    & {(2,1,1)}   & Y & - & {3}    & {3}    & {(2,2,1)}   & Y & -   & {3}    & {2}    & {(1,3)}       & N & \\
                      & 5 & 2    &2    & {(2,1)}   & Y & - & 2    &2    & {(2,2)}     & Y & -  & {2}    & {2}    & {(1,1)}     & N & P & {3}    & {3}    & {(3,2,1)}     & Y & -  \\
                      & 6 & 0    &0    & {(0)}     & N & P & {3}    & {3}    & {(1,2,1)}   & Y & - & {4}    & {4}    & {(1,1,1,2)} & Y & -  & {4}    & {4}    & {(1,2,1,2)}   & Y & - \\
                      & 7 & {3}    & {3}    & {(1,1,1)} & Y & - &1    &1    & {(3)}       & N & P & {2}    & {1}    & {(1)}       & N &  & {5}    & {5}    & {(2,1,1,1,1)} & Y & -  \\
                      & 8 & 2    &2    & {(1,2)}   & Y & - & {4}    & {4}    & {(1,1,1,1)} & Y & - & {2}    & {2}    & {(1,1)}     & N & P & {1}    & {1}    & {(3)}         & N & P \\
                      & 9 & {3}    & {3}    & {(1,1,1)} & Y & - &2    &2    & {(2,2)}     & Y & - & {2}    & {2}    & {(3,2)}     & Y & -  & {3}    & {3}    & {(3,2,1)}     & Y & - \\
                      & 10 & 1    &1    & {(3)}     & Y & - & {3}    & {3}    & {(1,2,1)}   & Y & - & {2}    & {1}    & {(1)}       & N &  & {4}    & {4}    & {(2,2,1,1)}   & Y & - \\
                      & 11 & {3}    & {3}    & {(1,1,1)} & Y & - & 0    &0    & {(0)}       & N & P  & {4}    & {4}    & {(2,1,1,1)} & Y & - & {0}    & {0}    & {(0)}         & N & P  \\
                      & 12 & 1    &1    & {(3)}     & Y & - & {3}    & {3}    & {(2,1,1)}   & Y & -  & {0}    & {0}    & {(0)}       & N & P  & {3}    & {2}    & {(2,1)}       & N &    \\
                      & 13 & 1    &0    & {(0)}     & N & G & 2    &2    & {(2,2)}     & Y & - & {0}    & {0}    & {(0)}       & N & P  & {1}    & {1}    & {(2)}         & N & P  \\
                      & 14 & 2    &2    & {(1,1)}   & N & P & 2    &1    & {(2,)}      & N & G & {3}    & {3}    & {(2,2,1)}   & Y & -  & {0}    & {0}    & {(0)}         & N & P  \\
                      & 15 & {3}    & {3}    & {(1,1,1)} & Y & - & {3}    & {3}    & {(2,1,1)}   & Y & - & {4}    & {4}    & {(1,1,2,1)} & Y & -   & {4}    & {4}    & {(2,2,1,1)}   & Y & -\\
                      & 16 & 2    &2    & {(2,1)}   & Y & - & 2    &2    & {(2,1)}     & N & P & {1}    & {1}    & {(2)}       & N & P   & {4}    & {4}    & {(1,2,2,1)}   & Y & -\\
                      & 17 & {3}    & {3}    & {(1,1,1)} & Y & -  & {4}    & {4}    & {(1,1,1,1)} & Y & - & {2}    & {1}    & {(2)}       & N &  & {1}    & {1}    & {(1)}         & N & P\\
                      & 18 & 2    &2    & {(2,1)}   & Y & - &2    &2    & {(1,3)}     & Y & - & {4}    & {4}    & {(1,1,1,2)} & Y & -  & {3}    & {3}    & {(2,1,3)}     & Y & - \\
                      & 19 & 1    &1    & {(3)}     & Y & - & 2    &2    & {(1,1)}     & N & P  & {0}    & {0}    & {(0)}       & N & P & {1}    & {0}    & {(0)}         & N &  \\
                      & 20 & 2    &2    & {(1,2)}   & Y & - &2    &2    & {(3,1)}     & Y & - & {4}    & {4}    & {(1,1,1,2)} & Y & - & {0}    & {0}    & {(0)}         & N & P \\  \cmidrule{2-22}
& \multirow{4 }{*}{St} & \multicolumn{5}{l|}{\colorbox{lime}{Completion rate: ~~~~~80\%}} & \multicolumn{5}{l|}{Completion rate: ~~~~~70\%} & \multicolumn{5}{l|}{Completion rate: ~~~~~55\%} & \multicolumn{5}{l}{Completion rate: ~~~~~55\%}\\
&  & \multicolumn{5}{l|}{ Transport rate: ~~~~~~~~83\%} & \multicolumn{5}{l|}{\colorbox{lime}{Transport rate: ~~~~~~~~85\%}} & \multicolumn{5}{l|}{Trans. Rate: 65\%} & \multicolumn{5}{l}{Transport rate: ~~~~~~~~66\%}\\
&  & \multicolumn{5}{l|}{\# Objects per grasp: ~~1.43} & \multicolumn{5}{l|}{\# Objects per grasp: ~~1.51}& \multicolumn{5}{l|}{\# Objects per grasp: ~~1.42} & \multicolumn{5}{l}{\colorbox{lime}{\# Objects per grasp: ~~1.68}}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \item{${}^{\text{Note}}$} Meanings of abbreviations: \#P -- Number of grouping actions; \#G -- Number of grasping actions; Seq. -- Sequence of grasped objects by each grouping and grasping; C -- Task completion (Y means Yes, all objects are cleared and transported. N means No, some objects remained due to failures); R -- Reason for task failure (P means a pushing failure. G means a grasping failure); St -- Statistical summary of the results above the row (Maximum values of the statistics are highlighted in lime, while minimum values are highlighted in pink).
\end{tablenotes}
\end{threeparttable}
}
\end{table*}

We can see from the results that as the number of randomly placed graspable objects increases, the task completion rates decrease. Failures frequently occur during pushing and grouping. The frequency of the failures increases with the number of initially randomly placed objects. The majority of pushing failures are caused by stagnation. This may be attributed to the increasing variety in the input images as the number of objects grows, making it more difficult for the neural network to determine the optimal pushing strategy based on limited information. We can also see from the results that the sequence of grouping and grasping is quite random. The network chose different policies according to object distributions. Furthermore, the results indicate that an increase in training data has a positive impact on all metrics. Notably, when the number of objects reaches $6$, the robot grasps an average of 1.68 objects per attempt. This demonstrates that the policy network can effectively support multi-object grasping and thus help a robot improve picking efficiency.

Fig. \ref{representative} presents two representative examples from the experimental results. As shown in Fig. \ref{representative}(a), the robot first pushed, grouped, and grasped three objects (a.i$\sim$iv), then grouped and grasped two objects (a.v$\sim$viii), and finally grasped a single object without pushing (a.ix). The robot selected appropriate policies based on the object arrangement to successfully complete the task. As shown in Fig. \ref{representative}(b), the policy network sometimes generates complex actions that are difficult to design manually, enabling flexible grouping of scattered but groupable objects. Therefore, despite the success rate being constrained by the amount of training data, the proposed method is confirmed to have high flexibility and promising potential.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/representative.jpg}
    \caption{Two examples from the trials for 3D printed objects. (a) The robot successfully completed the task by sequentially grouping and grasping 3 (a.i$\sim$iv), 2 (a.v$\sim$viii), and 1 (a.ix) objects. (b) The robot generated complex actions to group a challenging arrangement.}
    \label{representative}
\end{figure}  

\subsection{Experiment 2: Fine-tuning to snack bars}

We conducted fine-tuning on the pre-trained model using the dataset ($200$ demonstrations) from the previous experiments and evaluated the adaptivity of the proposed method to commercial products. The fine-tuning dataset consisted of $50$ demonstration samples using six objects. The demonstrations were carried out in the same way as the printed objects. The commercial products are 6 snack bars, including three Meiji chocolate bars (size: $125$ mm $\times$ $38$ mm $\times$ $15$ mm, available in two different appearances) and three HiChu fructose bars (size: $130$ mm $\times$ $28$ mm $\times$ $15$ mm, available in three different appearances). The object placement positions were randomly determined, and the fine-tuning process was conducted over $50000$ training steps.

Similar to the previous experiments, we carried out $20$ times of trials for evaluation. Table \ref{tab_exp2} shows the detailed results for these trials. We can see from the results that the task completion rate was 0\%, suggesting that additional learning using a small amount of data is insufficient for transferring policy knowledge effectively. An important reason for failing to clear all objects was that, during pushing, objects deviated in unexpected directions and became detached from the end effector. Additionally, the robot also encountered lots of stagnation or repeated the same policy. This suggests that the policy network struggles to adapt to new scenarios. Furthermore, grasping failures occurred frequently. The grasping failures were attributed to differences in the shape and surface texture of the snack bars compared to the 3D-printed objects used in pretraining. The fine-tuning struggles to adapt to the shape and texture variations.

On the other hand, the transport rate and \# objects per grasp are respectively 23\% and 1.47. They indicate that a certain level of multi-object grasping was achieved. The network can selectively group and grasp a few objects, as shown by the sequence column of the table and also the example in Fig. \ref{snackbar}. Overall, although the fine-tuned network did not help completely clear all objects, it still demonstrates a certain level of effectiveness. Learning from expert data requires a large volume of high-quality data. Developing efficient methods for collecting such data could potentially help improve learning and inference performance.

\begin{table}[!htbp]
\caption{Results of $6$ randomly placed snack bars using a model fine-tuned with 50 times of demosntrations}
\label{tab_exp2}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{c|cclll|c|cclll}
\toprule
ID &  {\#P} &  {\#G} &  {Seq.}      & C & R  & ID & {\#P} &  {\#G} &  {Seq.}      & C & R   \\ \midrule
 1 &  {1}    &  {1}    &  {(2)}       & N & P   & 11 &  {3}    &  {2}    &  {(2,1,1)}   & N & G\\  
 2 &  {1}    &  {1}    &  {(2)}       & N & P & 12                  &  {0}    &  {0}    &  {(0)}         & N & P \\  
 3 &  {1}    &  {0}    &  {(2)}       & N & G   & 13                  &  {2}    &  {2}    &  {(1,1)}     & N & P \\   
 4 &  {1}    &  {0}    &  {(1)}       & N & G   & 14                  &  {1}    &  {1}    &  {(2)}       & N & P \\   
 5 &  {3}    &  {3}    &  {(2,1,1)}   & N & P & 15                  &  {1}    &  {0}    &  {(1)}       & N & G   \\    
 6 &  {2}    &  {1}    &  {(1,1)}     & N & G   & 16                  &  {0}    &  {0}    &  {(0)}       & N & P \\   
 7 &  {0}    &  {0}    &  {(0)}       & N & P & 17                  &  {2}    &  {2}    &  {(2,1)}     & N & P \\ 
 8 &  {2}    &  {1}    &  {(2,2)}     & N & G   & 18                  &  {2}    &  {1}    &  {(2,1)}     & N & G   \\  
 9 &  {0}    &  {0}    &  {(0)}         & N & P   & 19                  &  {0}    &  {0}    &  {(0)}         & N & P \\  
 10 &  {4}    &  {3}    &  {(1,2,1,1)} & N & G   & 20                  &  {1}    &  {1}    &  {(1)}       & N & P \\ \midrule
St & \multicolumn{11}{l}{Completion rate: 0\%;~~ Transport rate: 23\%;~~ \# Objects per grasp: 1.47} \\ \bottomrule
\end{tabular}
\begin{tablenotes}
    \item{${}^{\text{Note}}$} See note of Table \ref{tab_exp} for meanings of abbreviations.
\end{tablenotes}
\end{threeparttable}
}
\end{table}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{imgs/snackbar.jpg}
    \caption{An example from the results of the snack bar trials. The robot initially pushed, grouped, and grasped two objects (i$\sim$vi). It then proceeded to grasp a single object (vii$\sim$viii). After that, the robot tried to push an object (ix) and noticed it could not be grouped (x). The robot changed its policy to grasp the object (xi) but failed (xii).}
    \label{snackbar}
\end{figure}  

\section{Conclusions and Future Work} % .25 page

In this study, we proposed a diffusion policy-based imitation learning method for grouping and grasping multiple objects. Experiments with 3D-printed objects show that a robot can use the proposed method to flexibly and efficiently push, group, and pick multiple objects irrespective of their arrangements. The pre-trained model using demonstrations with 3D-printed objects can be fine-tuned with practical snack bar objects for practical applications.

However, the proposed method has several drawbacks. It faces challenges as the number of objects increases. In practical applications, the diversity of textures and shapes may easily complicate adaptation. Future work will focus on improving training data collection and integrating conventional methods to ensure robustness.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\normalem
% \bibliographystyle{IEEEtran}
\bibliographystyle{IEEEtranN}
\bibliography{citations.bib}

\end{document}


% \begin{table*}[!htbp]
% % \renewcommand\arraystretch{1.2}
% \caption{$3\sim4$ Randomly placed 3D printed objects}
% \label{tab_34}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|cclll|cclll}
% \toprule
% \multirow{2.5}{*}{Dataset} & \multirow{2.5}{*}{Trial} & \multicolumn{5}{c|}{Number of Objects: 3} & \multicolumn{5}{c}{Number of Objects: 4}\\ \cmidrule{3-12}
%                          &    & {\# Group} & {\# Grasp} & {Sequence} & Compl. & Reason & {\# Group} & {\# Grasp} & {Sequence} & Compl. & Reason\\ \midrule
% \multirow{20}{*}{100}   & 1  & {2}    & {2}    & {(1,1)}   & Fail. & Push & {2}    & {2}    & {(1,2)} & Fail. & Push\\
%                         & 2                   & {2}    & {2}    & {(1,2)}   & Succ. & - & {3}    & {3}    & {(1,1,2)}   & Succ. & -\\
%                         & 3                   & {2}    & {2}    & {(2,1)}   & Succ. & - & {0}    & {0}    & {(0)}       & Fail. & Push \\
%                         & 4                   & {0}    & {0}    & {(0)}     & Fail. & Push  & {0}    & {0}    & {(0)}       & Fail. & Push \\
%                       & 5                   & {2}    & {2}    & {(2,1)}   & Succ. & - & {1}    & {0}    & {(0)}       & Fail. & Push\\
%                       & 6                   & {0}    & {0}    & {(0)}     & Fail. & Push & {2}    & {2}    & {(2,2)}     & Succ. & -\\
%                       & 7                   & {2}    & {2}    & {(1,2)}   & Succ. & - & {2}    & {2}    & {(2,2)}     & Succ. & -\\
%                       & 8                   & {0}    & {0}    & {(0)}     & Fail. & Push   & {1}    & {1}    & {(1)}       & Fail. & Push  \\
%                       & 9                   & {1}    & {1}    & {(1)}     & Fail. & Push   & {2}    & {2}    & {(2,2)}     & Succ. & -\\
%                       & 10                  & {2}    & {2}    & {(1,2)}   & Succ. & -   & {3}    & {3}    & {(2,1,1)}   & Succ. & -\\
%                       & 11                  & {1}    & {1}    & {(1)}     & Fail. & Push    & {0}    & {0}    & {(0)}       & Fail. & Push\\
%                       & 12                  & {1}    & {1}    & {(1)}     & Fail. & Push    & {3}    & {3}    & {(1,2,1)}   & Succ. & -\\
%                       & 13                  & {1}    & {1}    & {(3)}     & Succ. & -  & {1}    & {1}    & {(1)}       & Fail. & Push\\
%                       & 14                  & {1}    & {1}    & {(3)}     & Succ. & -  & {0}    & {0}    & {(0)}       & Fail. & Push\\
%                       & 15                  & {3}    & {3}    & {(1,1,1)} & Succ. & - & {3}    & {3}    & {(2,1,1)}   & Succ. & -\\
%                       & 16                  & {2}    & {2}    & {(2,1)}   & Succ. & - & {3}    & {3}    & {(1,2,1)}   & Succ. & - \\
%                       & 17                  & {2}    & {2}    & {(1,2)}   & Succ. & -  & {2}    & {2}    & {(2,2)}     & Succ. & -\\
%                       & 18                  & {2}    & {2}    & {(2,1)}   & Succ. & -   & {0}    & {0}    & {(0)}       & Fail. & Push\\
%                       & 19                  & {1}    & {1}    & {(1)}     & Fail. & Push   & {4}    & {4}    & {(1,1,1,1)} & Succ. & - \\
%                       & 20                  & {3}    & {3}    & {(1,1,1)} & Succ. & -   & {1}    & {1}    & {(1)}       & Fail. & Push  \\ \cmidrule{2-12}
% & Stats & \multicolumn{5}{c|}{Compl. Rate: 60\%; Trans. Rate: 70\%; \colorbox{pink}{\#Obj. per Grasp: 1.4}} & \multicolumn{5}{c}{Compl. Rate: 50\%; Trans. Rate: 58\%; \#Obj. per Grasp: 1.44}\\
%                       \midrule
% \multirow{20}{*}{200} & 1                   & {2}    & {2}    & {(2,1)}   & Succ. & - & {1}    & {1}    & {(2)}       & Fail. & Push  \\
%                       & 2                   & {1}    & {0}    & {(0)}     & Fail. & Grasp & {2}    & {2}    & {(3,1)}     & Succ. & - \\
%                       & 3                   & {2}    & {2}    & {(2,1)}   & Succ. & - & {3}    & {3}    & {(2,1,1)}   & Succ. & - \\
%                       & 4                   & {1}    & {1}    & {(3)}     & Succ. & - & {3}    & {3}    & {(2,1,1)}   & Succ. & - \\
%                       & 5                   & {2}    & {2}    & {(2,1)}   & Succ. & - & {2}    & {2}    & {(2,2)}     & Succ. & - \\
%                       & 6                   & {0}    & {0}    & {(0)}     & Fail. & Push & {3}    & {3}    & {(1,2,1)}   & Succ. & - \\
%                       & 7                   & {3}    & {3}    & {(1,1,1)} & Succ. & - & {1}    & {1}    & {(3)}       & Fail. & Push  \\
%                       & 8                   & {2}    & {2}    & {(1,2)}   & Succ. & - & {4}    & {4}    & {(1,1,1,1)} & Succ. & -  \\
%                       & 9                   & {3}    & {3}    & {(1,1,1)} & Succ. & - & {2}    & {2}    & {(2,2)}     & Succ. & -\\
%                       & 10                  & {1}    & {1}    & {(3)}     & Succ. & - & {3}    & {3}    & {(1,2,1)}   & Succ. & - \\
%                       & 11                  & {3}    & {3}    & {(1,1,1)} & Succ. & - & {0}    & {0}    & {(0)}       & Fail. & Push \\
%                       & 12                  & {1}    & {1}    & {(3)}     & Succ. & - & {3}    & {3}    & {(2,1,1)}   & Succ. & -  \\
%                       & 13                  & {1}    & {0}    & {(0)}     & Fail. & Grasp & {2}    & {2}    & {(2,2)}     & Succ. & - \\
%                       & 14                  & {2}    & {2}    & {(1,1)}   & Fail. & Push & {2}    & {1}    & {(2,)}      & Fail. & Grasp \\
%                       & 15                  & {3}    & {3}    & {(1,1,1)} & Succ. & - & {3}    & {3}    & {(2,1,1)}   & Succ. & -\\
%                       & 16                  & {2}    & {2}    & {(2,1)}   & Succ. & - & {2}    & {2}    & {(2,1)}     & Fail. & Push\\
%                       & 17                  & {3}    & {3}    & {(1,1,1)} & Succ. & -  & {4}    & {4}    & {(1,1,1,1)} & Succ. & - \\
%                       & 18                  & {2}    & {2}    & {(2,1)}   & Succ. & - & {2}    & {2}    & {(1,3)}     & Succ. & - \\
%                       & 19                  & {1}    & {1}    & {(3)}     & Succ. & - & {2}    & {2}    & {(1,1)}     & Fail. & Push  \\
%                       & 20                  & {2}    & {2}    & {(1,2)}   & Succ. & - & {2}    & {2}    & {(3,1)}     & Succ. & -\\  \cmidrule{2-12}
% & Stats & \multicolumn{5}{c|}{\colorbox{lime}{Compl. Rate: 80\%}; Trans. Rate: 83\%; \#Obj. per Grasp: 1.43} & \multicolumn{5}{c}{Compl. Rate: 70\%; \colorbox{lime}{Trans. Rate: 85\%}; \#Obj. per Grasp: 1.51}\\
% \bottomrule
% \end{tabular}
% }
% \end{table*}

% \begin{table*}[!htbp]
% % \renewcommand\arraystretch{1.2}
% \caption{$5\sim6$ Randomly placed 3D printed objects}
% \label{tab_56}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|cclll|cclll}
% \toprule
% \multirow{2.5}{*}{Dataset} & \multirow{2.5}{*}{Trial} & \multicolumn{5}{c|}{Number of Objects: 5} & \multicolumn{5}{c}{Number of Objects: 6}\\ \cmidrule{3-12}
%                          &    & {\# Group} & {\# Grasp} & {Sequence} & Compl. & Reason & {\# Group} & {\# Grasp} & {Sequence} & Compl. & Reason\\ \midrule
% \multirow{20}{*}{100}   & 1  & {1}    & {1}    & {(2)}       & Fail. & Push & {0}    & {0}    & {(0)}         & Fail. & Push\\
%                         & 2                    & {4}    & {4}    & {(1,1,1,2)} & Succ. & -   & {0}    & {0}    & {(0)}         & Fail. & Push\\
%                         & 3                  & {0}    & {0}    & {(0)}       & Fail. & Push  &  {0}    & {0}    & {(0)}         & Fail. & Push  \\
%                         & 4                   & {3}    & {3}    & {(1,1,1)}   & Fail. & Push    & {0}    & {0}    & {(0)}         & Fail. & Push \\
%                       & 5                   & {1}    & {1}    & {(1)}       & Fail. & Push & {1}    & {1}    & {(1)}         & Fail. & Push  \\
%                       & 6                   & {1}    & {1}    & {(1)}       & Fail. & Push & {0}    & {0}    & {(0)}         & Fail. & Push  \\
%                       & 7                 & {0}    & {0}    & {(0)}       & Fail. & Push & {2}    & {2}    & {(1,2)}       & Fail. & Push\\
%                       & 8                  & {0}    & {0}    & {(0)}       & Fail. & Push  & {2}    & {2}    & {(1,1)}       & Fail. & Push  \\
%                       & 9                   & {2}    & {2}    & {(3,2)}     & Succ. & -    & {1}    & {1}    & {(1)}         & Fail. & Push\\
%                       & 10                  & {2}    & {2}    & {(1,1)}     & Fail. & Push  & {1}    & {1}    & {(1)}         & Fail. & Push \\
%                       & 11                   & {1}    & {1}    & {(1)}       & Fail. & Push     & {0}    & {0}    & {(0)}         & Fail. & Push \\
%                       & 12                   & {0}    & {0}    & {(0)}       & Fail. & Push   & {3}    & {3}    & {(3,2,1)}     & Succ. & -\\
%                       & 13                 & {0}    & {0}    & {(0)}       & Fail. & Push   & {3}    & {3}    & {(1,3,2)}     & Succ. & -\\
%                       & 14                  & {2}    & {1}    & {(3)}       & Fail. & Grasp    & {1}    & {1}    & {(3)}         & Fail. & Push\\
%                       & 15                  & {0}    & {0}    & {(0)}       & Fail. & Push & {2}    & {2}    & {(1,2)}       & Fail. & Push\\
%                       & 16                   & {3}    & {3}    & {(1,3,1)}   & Succ. & -   & {4}    & {4}    & {(2,2,1,1)}   & Succ. & -\\
%                       & 17                 & {0}    & {0}    & {(0)}       & Fail. & Push & {0}    & {0}    & {(0)}         & Fail. & Push\\
%                       & 18                 & {0}    & {0}    & {(0)}       & Fail. & Push  & {3}    & {3}    & {(2,2,1)}     & Fail. & Push \\
%                       & 19                 & {2}    & {1}    & {(1)}       & Fail. & Grasp  & {1}    & {1}    & {(1)}         & Fail. & Push\\
%                       & 20                 & {0}    & {0}    & {(0)}       & Fail. & Push  & {5}    & {5}    & {(1,1,1,1,2)} & Succ. & - \\ \cmidrule{2-12}
% & Stats & \multicolumn{5}{c|}{Compl. Rate: 15\%; \colorbox{pink}{Trans. Rate: 29\%}; \#Obj. per Grasp: 1.45} & \multicolumn{5}{c}{\colorbox{pink}{Compl. Rate: 10\%}; Trans. Rate: 37\%; \#Obj. per Grasp: 1.52}\\
%                       \midrule
% \multirow{20}{*}{200} & 1                   & {3}    & {3}    & {(1,3,1)}   & Succ. & - & {3}    & {3}    & {(3,2,1)}     & Succ. & - \\
%                       & 2                    & {4}    & {4}    & {(1,1,2,1)} & Succ. & - & {4}    & {4}    & {(1,2,2,1)}   & Succ. & - \\
%                       & 3                   & {3}    & {3}    & {(2,2,1)}   & Succ. & - & {3}    & {3}    & {(2,3,1)}     & Succ. & - \\
%                       & 4                  & {3}    & {3}    & {(2,2,1)}   & Succ. & -   & {3}    & {2}    & {(1,3)}       & Fail. & Grasp\\
%                       & 5                 & {2}    & {2}    & {(1,1)}     & Fail. & Push & {3}    & {3}    & {(3,2,1)}     & Succ. & -  \\
%                       & 6                  & {4}    & {4}    & {(1,1,1,2)} & Succ. & -  & {4}    & {4}    & {(1,2,1,2)}   & Succ. & - \\
%                       & 7                   & {2}    & {1}    & {(1)}       & Fail. & Grasp & {5}    & {5}    & {(2,1,1,1,1)} & Succ. & -  \\
%                       & 8                   & {2}    & {2}    & {(1,1)}     & Fail. & Push & {1}    & {1}    & {(3)}         & Fail. & Push \\
%                       & 9                    & {2}    & {2}    & {(3,2)}     & Succ. & -  & {3}    & {3}    & {(3,2,1)}     & Succ. & - \\
%                       & 10                  & {2}    & {1}    & {(1)}       & Fail. & Grasp & {4}    & {4}    & {(2,2,1,1)}   & Succ. & - \\
%                       & 11                  & {4}    & {4}    & {(2,1,1,1)} & Succ. & - & {0}    & {0}    & {(0)}         & Fail. & Push  \\
%                       & 12                  & {0}    & {0}    & {(0)}       & Fail. & Push  & {3}    & {2}    & {(2,1)}       & Fail. & Grasp   \\
%                       & 13                  & {0}    & {0}    & {(0)}       & Fail. & Push  & {1}    & {1}    & {(2)}         & Fail. & Push  \\
%                       & 14                 & {3}    & {3}    & {(2,2,1)}   & Succ. & -  & {0}    & {0}    & {(0)}         & Fail. & Push  \\
%                       & 15                   & {4}    & {4}    & {(1,1,2,1)} & Succ. & -   & {4}    & {4}    & {(2,2,1,1)}   & Succ. & -\\
%                       & 16                    & {1}    & {1}    & {(2)}       & Fail. & Push   & {4}    & {4}    & {(1,2,2,1)}   & Succ. & -\\
%                       & 17                  & {2}    & {1}    & {(2)}       & Fail. & Grasp & {1}    & {1}    & {(1)}         & Fail. & Push\\
%                       & 18                  & {4}    & {4}    & {(1,1,1,2)} & Succ. & -  & {3}    & {3}    & {(2,1,3)}     & Succ. & - \\
%                       & 19                  & {0}    & {0}    & {(0)}       & Fail. & Push & {1}    & {0}    & {(0)}         & Fail. & Grasp \\
%                       & 20                    & {4}    & {4}    & {(1,1,1,2)} & Succ. & - & {0}    & {0}    & {(0)}         & Fail. & Push \\  \cmidrule{2-12}
% & Stats & \multicolumn{5}{c|}{Compl. Rate: 55\%; Trans. Rate: 65\%; \#Obj. per Grasp: 1.42} & \multicolumn{5}{c}{Compl. Rate: 55\%; Trans. Rate: 66\%; \colorbox{lime}{\#Obj. per Grasp: 1.68}}\\
% \bottomrule
% \end{tabular}
% }
% \end{table*}
