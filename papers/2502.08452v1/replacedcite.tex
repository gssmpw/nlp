\section{Related Work}
% 2 page

Multi-object simultaneous grasping has been attracting significant attention for over two decades. As pioneering contributions, Aiyama et al. ____ studied using two manipulators to cooperatively grasp and lift an array of box-shaped objects. Harada and Kaneko ________ studied the neighboring equilibrium of multiple cylindrical objects and their enveloping grasps. Yoshikawa et al. ____ proposed the optimal power grasp condition for multiple objects from the viewpoint of reducing finger joint torques. Yamada et al. ____ presented a more general analysis of the grasp stability of multiple objects grasped by fingers considering objects' surface curvatures and the potential energy of the spring models in contact. The authors later extended the theories to multi-fingered hands with revolute joints ____ and 3D objects ____. 

More recent work by Chen et al. ____ inserted the Barrett hand into a pile of objects and estimated the number of grasped items using a deep learning model. Shenoy et al. ____ further focused on efficiently transferring the grasped multiple objects to a separate container. Takahashi et al. ____ applied multi-object grasping evaluation to grasping a given amount of granular foods. The authors also explored post-grasping methods for fine adjustments ____. Li et al. ____ developed a framework for planning and learning to grasp (lift) multiple objects. Jiang et al. ____ developed a vacuum gripper equipped with multiple suction cups and also the grasp planning methods to simultaneously pick multiple objects from a tray. Nguyen et al. ____ proposed the Wiring-Claw Gripper, which took advantage of the soft interaction between wires in a claw and objects for simultaneous multi-object grasping.

The above approaches achieved stable grasping of multiple or unknown objects. However, they often assumed that the objects were initially positioned in proximity and thus are less applicable to environments where objects are scattered. To tackle such limitations, several studies have explored approaches that utilize pushing actions to rearrange objects into positions more conducive to simultaneous grasping. For example, Sakamoto et al. ____ proposed a method for grasping two distant objects by first pushing one object closer to the other and then grasping them together. The approach determined whether to grasp a single object, grasp two objects simultaneously, or push one object closer for simultaneous grasping based on the distance and friction between objects. Using dynamic programming, the authors optimized the grasping sequence and enabled simultaneous grasping of two rectangular objects by rotating them about the intersection of their long axes. Agboh et al. ________ proposed the $\mu$-MOG framework to simultaneously grasp multiple rigid convex polygonal objects scattered on a plane. The method facilitated the simultaneous grasping of cluttered objects by aligning them using a gripper's opening jaw. Shrey et al. ____ partitioned clusters to ensure that the sum of the minimum graspable diameters within a cluster remained below a gripperâ€™s width and leveraged linear pushing actions along lines perpendicular to the gripper fingers to bring object centroids closer, thus enabled simultaneous grasping of objects initially outside the gripper's opening width. Kishore et al. ____ proposed a method for efficiently transporting cups, bowls, and utensils on tables by combining pushing and stacking actions for simultaneous grasping. The method especially employed inward-pushing actions to reduce positional uncertainty. 

The pushing-based methods have addressed the problem of scattered objects to some extent. However, they are predominantly rule-based, making it challenging to generate complex pushing trajectories required for handling intricate grouping and grasping tasks. In this paper, we propose employing imitation learning to generate complex pushing trajectories and solve the constraints on combinations of objects that can be grasped simultaneously. Previously, reinforcement learning methods have attracted much attention in robotic pushing and grasping as they can adapt to various object arrangements. For instance, Zeng et al. ____ achieved efficient grasping by enabling the complementary relationship between pushing and grasping actions to be learned through self-supervised deep reinforcement learning. Chen et al. ____ proposed a grasping strategy that combines rule-based methods with reinforcement learning to integrate pushing and grasping actions. Wang et al. ____ attained high success rates and robustness by leveraging self-supervised deep reinforcement learning to learn an integrated model of pushing and grasping actions. These studies demonstrate that reinforcement learning has the advantage of adapting to unknown environments. 

A key challenge in reinforcement learning lies in the design of rewards. This issue becomes particularly pronounced in the context of this study as we use pushing to achieve grouping, followed by grasping. Designing rewards that facilitate rapid convergence for the pushing-grouping-grasping routine is notably difficult. A learning method conceptually similar to reinforcement learning but not dependent on reward design is imitation learning. Recent studies have showcased the potential of imitation learning in robotic manipulation applications. For example, ACT ____ has achieved high success rates in delicate tasks such as opening and closing Ziploc bags and inserting batteries into controllers. Diffusion policy ____ has enabled high success rates in tasks requiring diverse actions and significant flexibility, such as spreading sauce on pizza dough or relocating T-shape blocks and mugs placed at random positions on a plane to target positions and orientations. Given its advantages, this research adopts imitation learning as the primary methodology. By learning flexible pushing actions and grasping decisions from human demonstrations, our approach enables dynamic selection of object combinations, rearrangement through complex pushing trajectories, and adaptive grasping sequences that were previously difficult to attain with rule-based or reinforcement learning methods.