@misc{alzahrani2024benchmarkstargetsrevealingsensitivity,
      title={When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards}, 
      author={Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan},
      year={2024},
      eprint={2402.01781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01781}, 
}

@article{chen2023fortify,
  title={Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use},
  author={Chen, Yuhan and Lv, Ang and Lin, Ting-En and Chen, Changyu and Wu, Yuchuan and Huang, Fei and Li, Yongbin and Yan, Rui},
  journal={arXiv preprint arXiv:2312.04455},
  year={2023}
}

@article{chen2024premise,
  title={Premise Order Matters in Reasoning with Large Language Models},
  author={Chen, Xinyun and Chi, Ryan A and Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.08939},
  year={2024}
}

@misc{chhabra2024revisitingzeroshotabstractivesummarization,
      title={Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias}, 
      author={Anshuman Chhabra and Hadi Askari and Prasant Mohapatra},
      year={2024},
      eprint={2401.01989},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01989}, 
}

@misc{gupta2024changinganswerorderdecrease,
      title={Changing Answer Order Can Decrease MMLU Accuracy}, 
      author={Vipul Gupta and David Pantoja and Candace Ross and Adina Williams and Megan Ung},
      year={2024},
      eprint={2406.19470},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.19470}, 
}

@article{hao2022structured,
  title={Structured prompting: Scaling in-context learning to 1,000 examples},
  author={Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  journal={arXiv preprint arXiv:2212.06713},
  year={2022}
}

@inproceedings{haviv2022nopos,
  author       = {Adi Haviv and
                  Ori Ram and
                  Ofir Press and
                  Peter Izsak and
                  Omer Levy},
  title        = {Transformer Language Models without Positional Encodings Still Learn Positional Information},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP} 2022},
  year         = {2022},
}

@misc{haviv2022transformerlanguagemodelspositional,
      title={Transformer Language Models without Positional Encodings Still Learn Positional Information}, 
      author={Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},
      year={2022},
      eprint={2203.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.16634}, 
}

@article{hsieh2024found,
  title={Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization},
  author={Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long T and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others},
  journal={arXiv preprint arXiv:2406.16008},
  year={2024}
}

@article{hwang2007optimizing,
author = {Hwang, Seung-won and Chang, Kevin Chen-chuan},
title = {Optimizing top-k queries for middleware access: A unified cost-based approach},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/1206049.1206054},
doi = {10.1145/1206049.1206054},
abstract = {This article studies optimizing top-k queries in middlewares. While many assorted algorithms have been proposed, none is generally applicable to a wide range of possible scenarios. Existing algorithms lack both the “generality” to support a wide range of access scenarios and the systematic “adaptivity” to account for runtime specifics. To fulfill this critical lacking, we aim at taking a cost-based optimization approach: By runtime search over a space of algorithms, cost-based optimization is general across a wide range of access scenarios, yet adaptive to the specific access costs at runtime. While such optimization has been taken for granted for relational queries from early on, it has been clearly lacking for ranked queries. In this article, we thus identify and address the barriers of realizing such a unified framework. As the first barrier, we need to define a “comprehensive” space encompassing all possibly optimal algorithms to search over. As the second barrier and a conflicting goal, such a space should also be “focused” enough to enable efficient search. For SQL queries that are explicitly composed of relational operators, such a space, by definition, consists of schedules of relational operators (or “query plans”). In contrast, top-k queries do not have logical tasks, such as relational operators. We thus define the logical tasks of top-k queries as building blocks to identify a comprehensive and focused space for top-k queries. We then develop efficient search schemes over such space for identifying the optimal algorithm. Our study indicates that our framework not only unifies, but also outperforms existing algorithms specifically designed for their scenarios.},
journal = {ACM Trans. Database Syst.},
month = mar,
pages = {5–es},
numpages = {41},
keywords = {middlewares, Top-k query processing}
}

@article{junqing2023never,
  title={Never lost in the middle: Improving large language models via attention strengthening question answering},
  author={Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others},
  journal={arXiv preprint arXiv:2311.09198},
  year={2023}
}

@inproceedings{lee2019set,
  author       = {Juho Lee and
                  Yoonho Lee and
                  Jungtaek Kim and
                  Adam R. Kosiorek and
                  Seungjin Choi and
                  Yee Whye Teh},
  title        = {Set Transformer: {A} Framework for Attention-based Permutation-Invariant
                  Neural Networks},
  booktitle    = {ICML},
  year         = {2019},
}

@article{lee2024cord,
      title={CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation}, 
      author={Youngwon Lee and Seung-won Hwang and Daniel Campos and Filip Graliński and Zhewei Yao and Yuxiong He},
      year={2025},
      journal={NAACL},
}

@article{lee2024inference,
      title={Inference Scaling for Bridging Retrieval and Augmented Generation}, 
      author={Youngwon Lee and Seung-won Hwang and Daniel Campos and Filip Graliński and Zhewei Yao and Yuxiong He},
      year={2025},
      journal={NAACL},
}

@misc{leval,
      title={L-Eval: Instituting Standardized Evaluation for Long Context Language Models}, 
      author={Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu},
      year={2023},
      eprint={2307.11088},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.11088}, 
}

@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{longbench,
      title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding}, 
      author={Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2024},
      eprint={2308.14508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.14508}, 
}

@misc{marathon,
      title={Marathon: A Race Through the Realm of Long Context with Large Language Models}, 
      author={Lei Zhang and Yunshui Li and Ziqiang Liu and Jiaxi yang and Junhao Liu and Longze Chen and Run Luo and Min Yang},
      year={2024},
      eprint={2312.09542},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.09542}, 
}

@inproceedings{pcw,
    title = "Parallel Context Windows for Large Language Models",
    author = "Ratner, Nir  and
      Levine, Yoav  and
      Belinkov, Yonatan  and
      Ram, Ori  and
      Magar, Inbal  and
      Abend, Omri  and
      Karpas, Ehud  and
      Shashua, Amnon  and
      Leyton-Brown, Kevin  and
      Shoham, Yoav",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.352",
    doi = "10.18653/v1/2023.acl-long.352",
    pages = "6383--6402",
    abstract = "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks ({``}windows{''}), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at \url{https://github.com/ai21labs/parallel-context-windows}.",
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{peysakhovich2023attention,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{pezeshkpour2023large,
  title={Large language models sensitivity to the order of options in multiple-choice questions},
  author={Pezeshkpour, Pouya and Hruschka, Estevam},
  journal={arXiv preprint arXiv:2308.11483},
  year={2023}
}

@misc{pine,
      title={Eliminating Position Bias of Language Models: A Mechanistic Approach}, 
      author={Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji},
      year={2024},
      eprint={2407.01100},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01100}, 
}

@misc{setbasedprompting,
      title={Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem}, 
      author={Reid McIlroy-Young and Katrina Brown and Conlan Olson and Linjun Zhang and Cynthia Dwork},
      year={2024},
      eprint={2406.06581},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.06581}, 
}

@misc{tang2024middlepermutationselfconsistencyimproves,
      title={Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models}, 
      author={Raphael Tang and Xinyu Zhang and Xueguang Ma and Jimmy Lin and Ferhan Ture},
      year={2024},
      eprint={2310.07712},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.07712}, 
}

@misc{wei2024unveilingselectionbiasesexploring,
      title={Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models}, 
      author={Sheng-Lun Wei and Cheng-Kuang Wu and Hen-Hsen Huang and Hsin-Hsi Chen},
      year={2024},
      eprint={2406.03009},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.03009}, 
}

@article{xu2023retrieval,
  title={Retrieval meets long context large language models},
  author={Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2310.03025},
  year={2023}
}

@misc{yang2022tableformerrobusttransformermodeling,
      title={TableFormer: Robust Transformer Modeling for Table-Text Encoding}, 
      author={Jingfeng Yang and Aditya Gupta and Shyam Upadhyay and Luheng He and Rahul Goel and Shachi Paul},
      year={2022},
      eprint={2203.00274},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.00274}, 
}

@misc{yang2023revisitingparallelcontextwindows,
      title={Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration}, 
      author={Kejuan Yang and Xiao Liu and Kaiwen Men and Aohan Zeng and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2305.15262},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.15262}, 
}

@misc{yen2024longcontextlanguagemodelingparallel,
      title={Long-Context Language Modeling with Parallel Context Encoding}, 
      author={Howard Yen and Tianyu Gao and Danqi Chen},
      year={2024},
      eprint={2402.16617},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16617}, 
}

@article{yu2024mitigate,
  title={Mitigate Position Bias in Large Language Models via Scaling a Single Dimension},
  author={Yu, Yijiong and Jiang, Huiqiang and Luo, Xufang and Wu, Qianhui and Lin, Chin-Yew and Li, Dongsheng and Yang, Yuqing and Huang, Yongfeng and Qiu, Lili},
  journal={arXiv preprint arXiv:2406.02536},
  year={2024}
}

@article{zhang2024found,
  title={Found in the middle: How language models use long contexts better via plug-and-play positional encoding},
  author={Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2403.04797},
  year={2024}
}

@misc{zhao2023robut,
      title={RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations}, 
      author={Yilun Zhao and Chen Zhao and Linyong Nan and Zhenting Qi and Wenlin Zhang and Xiangru Tang and Boyu Mi and Dragomir Radev},
      year={2023},
      eprint={2306.14321},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.14321}, 
}

@misc{zhou2024frebtqa,
      title={FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering}, 
      author={Wei Zhou and Mohsen Mesgar and Heike Adel and Annemarie Friedrich},
      year={2024},
      eprint={2404.18585},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.18585}, 
}

@article{zhu2023judgelm,
  title={Judgelm: Fine-tuned large language models are scalable judges},
  author={Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong},
  journal={arXiv preprint arXiv:2310.17631},
  year={2023}
}

