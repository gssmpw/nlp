\section{Experiment setup}
\input{tables/litm_numbers}
\input{tables/kgqa}
\input{tables/mmlu}

\subsection{Baselines}
\label{sec:ablations}
Original causal LM with no modifications (Orig.) were compared, which processes text sequentially. Also, we compare \ours{} against previous zero-shot order-invariant LMs discussed in Sec.~\ref{subsec:baseline}, namely PCW~\cite{pcw}, PINE~\cite{pine}, and Set-Based Prompting~\cite{setbasedprompting}.
We use the LLaMA 3.1~\cite{llama3} 8B-Instruct\footnote{\texttt{meta-llama/Meta-Llama-3.1-8B-Instruct}}, Qwen1.5-4B-Chat and Qwen1.5-7B-Chat\footnote{\texttt{Qwen/Qwen1.5-4/7B-Chat}} as our backbone model for all of our experiments. As our method \textbf{doesn't need training}, a single A6000 GPU was sufficient to run all of the experiments except for the Llama-3.1-70B-Instruct model (Appendix, Tab.~\ref{table/70bresult}).
We also conduct experiments on a subset of benchmarks (LitM and MMLU) on the runtime latency, perplexity, and collision rate of PINE and \ours{}, to further validate our claims on Sec.~\ref{sec:method_ours}.

\subsection{Benchmarks with listwise inputs}\label{sec:benchmarks}

We experiment with three benchmarks involving real-world listwise input data.
% each representing fully invariant, fully- and semi-invariant, and semi-invariant inputs.
%encompassing diverse scenarios with different degree of order-sensitivity in inputs.
% We measure the effectiveness of the invariant model, or the MoV method.
Examples of exact inputs and outputs are provided in Appendix~\ref{appendix:inputexamples}.
All reported scores are rounded to the nearest tenth, except for the standard deviation (rounded to the second decimal place). 

\noindent \textbf{Knowledge Graph QA (KGQA)} In KGQA tasks, the model takes facts over knowledge graphs represented as (subject, relation, object), and answers the given question based on the given facts. We basically follow the KGQA dataset preprocessing and evaluation setup from ~\citet{baek2023knowledgeaugmentedlanguagemodelverification}, which uses %WebQSP~\cite{webqsp} modified from ~\citet{berant-etal-2013-semantic} and 
Mintaka~\cite{mintaka} with Wikidata for knowledge source, and use the Exact Match (EM), Accuracy (Acc), and F1 score metric for evaluation. We also use MPNet~\cite{song2020mpnetmaskedpermutedpretraining} as a dense retriever to retrieve top-k facts over each question, and experiment with segment size of 30 and 50. Replication details and example dataset format are at Appendix Sec.~\ref{appendix:dataset_details} and Fig.~\ref{Mintaka}. Along with measuring the performance of the initial input ordering, we report performance after we \textbf{shuffle} the order of segments with 3 different seeds to see shuffle robustness.


\noindent \textbf{Lost in the middle (LitM)}
We use the Lost in the Middle (LitM) benchmark~\cite{liu2024lost}, which draws from 2655 queries in the Natural Questions (NQ) dataset. It provides sets of (10, 20, 30) passages, placing the gold passage at predetermined positions (e.g., 0, 4, 9) and filling the remaining slots with irrelevant passages.
%, testing the modelâ€™s robustness to passage ordering. 
Following ~\citet{liu2024lost}, the \texttt{best\_subspan\_em} metric is used. Experiments on LitM found that eliminating the effect of index bias is another important detail for measuring true order robustness: (Appendix Sec.~\ref{appendix:index_bias}). Thus, we report experiments with index bias eliminated. The exact prompt and full results including index bias is reported at Appendix Fig.~\ref{lostinthemiddlenoindexing} and Sec.~\ref{appendix:litm_detail}.

\noindent \textbf{MMLU}
The Massive Multitask Language Understanding (MMLU) benchmark~\cite{hendrycks2021measuringmassivemultitasklanguage} (prompts at Appendix Fig.~\ref{mmlu_prompt}) consists of 57 diverse sub-tasks with a total of 14,015 queries to measure general performance of LMs about the knowledge of the world. Despite its popularity, many works report that performance fluctuates heavily depending on the order of choices~\cite{gupta2024changinganswerorderdecrease, pezeshkpour2023large, wei2024unveilingselectionbiasesexploring, alzahrani2024benchmarkstargetsrevealingsensitivity, zheng2024large} and is widely investigated to measure the positional bias of the model. We notice that a lot of proportions consist of ordering-sensitive inputs, which showed the effectiveness of adaptively applying \sr{}. %In addition to measuring the performance with the original ordering,
We additionally report the average performance for all possible (4!-1) re-orderings.
