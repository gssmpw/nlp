\section{Related Works}
\subsection{Positional bias of LLMs}
\noindent \textbf{Problem statement.} 
Recent works on (zero-shot) retrieval augmented generation (RAG) with LLMs have found that the models exhibit unwanted bias on the \textit{ordering} of the retrieved documents~\cite{chhabra2024revisitingzeroshotabstractivesummarization}.
% Retrieval augmented generation (RAG) is effective and efficient for LLMs to utilize external information. However, on RAG, recent works find out that models exhibit unwanted bias on the \textit{order} of the retrieved documents, and is usually investigated mostly in zero-shot setup~\cite{chhabra2024revisitingzeroshotabstractivesummarization}.
Widely known as the lost-in-the-middle problem~\cite{liu2024lost}, many prior studies~\cite{chen2024premise, gupta2024changinganswerorderdecrease, pezeshkpour2023large, zhao2023robut, zhou2024frebtqa, wei2024unveilingselectionbiasesexploring, alzahrani2024benchmarkstargetsrevealingsensitivity, zheng2024large} also investigate the importance of positional bias, extending the domain to 
structured knowledge grounding (SKG) tasks~\cite{zhao2023robut, zhou2024frebtqa} and multiple-choice questions~\cite{gupta2024changinganswerorderdecrease} where changing the ordering of rows, schemas, or choices greatly degrades performance.
% On mitigating positional bias, \citet{wei2024unveilingselectionbiasesexploring} points out that \textit{token bias} should be distinguished from \textit{order bias}.

\noindent \textbf{Considerations for decoder-only LMs.} While successful approaches are presented to mitigate this issue for encoder-only~\cite{yang2022tableformerrobusttransformermodeling} and encoder-decoder~\cite{yen2024longcontextlanguagemodelingparallel, cai2023scaling} models, they leave decoder-only models, which account for most of the current LLMs, for more consideration. In contrast to transformer encoders that use bidirectional attention which is invariant by nature~\cite{lee2019set}, transformer decoders use causal attention to learn causal relation signals, which is not invariant by nature~\cite{haviv2022nopos}. Therefore, positional bias for decoder-only models is known to stem from \textit{both} positional encoding and causal attention mask~\cite{yu2024mitigate, pine} and is harder to mitigate.
% which is non-trivial to mitigate. 
% Since most of the current LLMs have decoder-only architecture,
% % we believe solutions to
% mitigating positional bias for decoder-only models
% % in a zero-shot manner
% is practically valuable.
%In contrast to the line of works that investigate the phenomenon, fewer works are conducted to mitigate them at the mechanical level and understanding and figuring effective factors about set invariance are often overlooked under the performance.

%There are many cases where we want the model to be invariant 
%, distinguishing set invariant inputs with respect to those that can benefit from the orderings are often overlooked. We find that there are actually cases in MMLU where the orderings are related to each other, for example cases where having (D) as none of the above or both (A) and (B).


% Since the lost-in-the-middle problem~\cite{liu2023lost} has shown ~

% 1. Position bias 및 현상 자체에 대한 논문, 벤치마크 등등
%  - LITM, TQA, MCQA, Multi-hop QA, RAG, etc
%  - 문제 발생 원인을 분석한 논문 등

% 2. Position bias 해결을 위한 방법론 등
%   - Mechanistic approach (PCW, PINE 등 causal mask와 position encoding을 조작)
%   - Calibration (confidence based, 혹은 middle document emphasis 등등)
%   - Window size extension (yarn 등등)
%   - CoT 등 prompting (단답 말고 text로 답변하기 등등)

\begin{comment}
Prior works that focus on extending the context window size of Langauge Models~\cite{peng2023yarn, zhang2024found} attempt to extend the context length of LMs by modifying positional encodings.
% to do: https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling add papers from here

However, these approaches do not address the issue of set invariance violation. In the context of long context handling, effectively dealing with "long-range" positional information is crucial. On the other hand, addressing order invariance sometimes requires "ignoring" positional encodings.
In evaluating these aspects, the key point for long context handling is whether model performance is retained as input length increases. Conversely, for set invariance, the focus is on whether model performance remains consistent when input order is shuffled.

Previous 
works that focused on mechanistically eliminating the positional bias on causal Language Models show limited scalability. Up to our understanding, the most recent approach, the main results of PINE~\cite{pine} were only tested on impractically small, fixed-size parallel contexts. Upon testing on various benchmarks with bigger and variable n (n up to 60), we find that they exhibit significant degradation of performance or show diminishing effectiveness compared with the original counterparts. %when we extend this to practical scenarios, when $n$ varies for each question or when n becomes larger than the maximum reported number of n on the paper(=20).

We speculate that this is due to its numerical error on ordering contexts by attention scores, and propose to use a more deterministic approach by assigning global ordering without any collisions and circular arrangement between contexts. We show that our method works parallel to the number of n, and the overall performance also retained much better with respect to the original model, while the previous model failed to do so.


\subsection{Parallel Context Processing}
%Several works have proposed solutions to the challenge,~\cite{pcw, cai2023scaling, hao2022structured, junqing2023never, zhu2023judgelm}, they do it in an ad-hoc \sw{unclear} way, not fundamentally solving the problem and failing in many cases.
Among several works that propose to modify model architectures, PCW~\cite{pcw} was the first(?todo: check) that propose parallel context window for decoder-only models but recent works point out that interaction between passages are limited that it works merely the same as weighted sum ensemble of giving individual rows and summing up responses~\cite{yang2023revisitingparallelcontextwindows}.
\citet{yen2024longcontextlanguagemodelingparallel} introduce an additional overhead to train the encoder model from scratch, which gets very dependent on the training data for the encoder and also doesn't \sw{does not} generalize to windows that exceed the context length (256 per encoder). Also, the model doesn't have space to process sequential text. (No prefix or suffix) \sw{this section is too casually written} %[introduce PINE]
\citet{pine} further goes on to eliminate the bias arising from causal attention by rearranging the positional bias to behave in an invariant way and further improves on parallel text processing, but they are unstable to similar contexts that leads to same KV attention similarity value and it still doesn't generalize well to tasks with larger window size. 
Of all the above previous works, no method has ever tackled the problem of enhancing the sequential AND parallel processing ability together. % explain that it's different then just ensembling, it's important for one model to do both well!! 

\citet{chen2023fortify} conducted parallel runs of LLMs with different RoPE angles, thereby mitigating the risk of overlooking crucial information through a weighted sum of the outputs. These approaches usually require additional memory or multiple inference runs, which can be expensive for LLMs.  


\end{comment}

\subsection{Zero-shot order-invariance for LLMs}
\label{related_work:orderinv}
\textbf{Long context modeling.} Zero-shot approaches for mitigating positional bias in LLMs were first raised in long-context tasks, with a goal to correctly handle relevant information located in the \textit{middle} of lengthy inputs\footnote{\href{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}{\texttt{github.com/gkamradt/LLMTest\_NeedleInAHaystack}}}. Nonetheless, these works focus primarily on understanding long texts without losing precision~\cite{li2023loogle, marathon, leval, longbench}, whereas positional bias is a more general problem that can occur even on multiple-choices questions with relatively short contexts~\cite{alzahrani2024benchmarkstargetsrevealingsensitivity}. Technically, this line of works modify the attention mechanism by altering the positional encoding to adapt an LLM to longer contexts~\cite{peng2023yarn, hsieh2024found, peysakhovich2023attention,chen2023fortify, junqing2023never,xu2023retrieval, yu2024mitigate, zhang2024found}. But since they do not modify the causal mask which also contributes to positional bias, order-invariance is not guaranteed in general~\cite{haviv2022transformerlanguagemodelspositional}.

\noindent \textbf{(Zero-shot) order-invariance.}
%cai2023scaling: position bias자체에 focus하지 않거나 long context에서만 test됨
Recent line of works focused on achieving order-invariance by mechanistically altering both positional encoding and causal masking.
While several works require training~\cite{junqing2023never, zhu2023judgelm}, we focus on zero-shot approaches for practicality, namely
% Several works have proposed solutions to the challenge, but some works require additional training~\cite{junqing2023never, zhu2023judgelm} which limits their practicality.
PCW, Set-Based Prompting~\cite{pcw, setbasedprompting}, and PINE~\cite{pine}, which we explain in detail at Sec.~\ref{subsec:baseline}. %completely remove attention between different contexts to achieve zero-shot invariance.
%\citet{yang2023revisitingparallelcontextwindows} has shown that this is equivalent to processing each context separately and aggregating the responses, which can negatively affect global understanding across contexts for PCW.
% T, working merely the same as weighted sum ensemble of giving individual rows and summing up responses~\cite{yang2023revisitingparallelcontextwindows}.
%On the other hand, PINE~\cite{pine} modifies positional information per-query, which allows bidirectional attention between contexts while achieving ordering-invariance. However, we find that effectiveness of PINE is diminished when $n$ is scaled to practical scenarios, due to increased complexity and excessive reliance on attention scores.
% \noindent \textbf{Self-consistency for invariance.}
Another line of works based on self-consistency try to mitigate positional bias simply by running inference multiple times with different orderings of contexts~\cite{zheng2024large}. However, in principle, this requires evaluating \(n!\) forward passes in total, enforcing Monte Carlo approximations~\citep{tang2024middlepermutationselfconsistencyimproves}. More recent work optimizes the number or passes~\citep{lee2024inference} with similar comprehensiveness \citep{hwang2007optimizing}, or replaces with contrastive training objective~\cite{lee2024cord}. In contrast, our method guarantee invariance with a \textit{single} forward pass, without requiring any approximations.

