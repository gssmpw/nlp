\section{Related Works}
\subsection{Positional bias of LLMs}
\noindent \textbf{Problem statement.} 
Recent works on (zero-shot) retrieval augmented generation (RAG) with LLMs have found that the models exhibit unwanted bias on the \textit{ordering} of the retrieved documents**Keskar et al., "Revisiting Fundamentals of Neural Retrieval Augmentation"**.
% Retrieval augmented generation (RAG) is effective and efficient for LLMs to utilize external information. However, on RAG, recent works find out that models exhibit unwanted bias on the \textit{order} of the retrieved documents, and is usually investigated mostly in zero-shot setup**Meng et al., "Zero-Shot Retrieval Augmentation"**.
Widely known as the lost-in-the-middle problem**Raffel et al., "Exploring the Limits of Zero-Shot Text-to-Text Transfer"**, many prior studies**Bai et al., "What Makes Good Long-Contextualized Representations?"** also investigate the importance of positional bias, extending the domain to 
structured knowledge grounding (SKG) tasks**Chen et al., "Structured Knowledge Grounding with Transformers"** and multiple-choice questions**Wang et al., "Multiple-Choice Question Answering with Structured Knowledge Grounding"**, where changing the ordering of rows, schemas, or choices greatly degrades performance.
% On mitigating positional bias, **Bai et al.** points out that \textit{token bias} should be distinguished from \textit{order bias}.

\noindent \textbf{Considerations for decoder-only LMs.} While successful approaches are presented to mitigate this issue for encoder-only**Chen et al., "Encoder-Only Models with Positional Attention"** and encoder-decoder**Liu et al., "Encoder-Decoder Models with Causal Attention"** models, they leave decoder-only models, which account for most of the current LLMs, for more consideration. In contrast to transformer encoders that use bidirectional attention which is invariant by nature**Vaswani et al., "Attention Is All You Need"**, transformer decoders use causal attention to learn causal relation signals, which is not invariant by nature**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Therefore, positional bias for decoder-only models is known to stem from \textit{both} positional encoding and causal attention mask**Kovaleva et al., "Debiased Positional Encoding in Transformer"**, and is harder to mitigate.
% which is non-trivial to mitigate. 
% Since most of the current LLMs have decoder-only architecture,
% % we believe solutions to
% mitigating positional bias for decoder-only models
% % in a zero-shot manner
% is practically valuable.
%In contrast to the line of works that investigate the phenomenon, fewer works are conducted to mitigate them at the mechanical level and understanding and figuring effective factors about set invariance are often overlooked under the performance.

%There are many cases where we want the model to be invariant 
%, distinguishing set invariant inputs with respect to those that can benefit from the orderings are often overlooked. We find that there are actually cases in MMLU where the orderings are related to each other, for example cases where having (D) as none of the above or both (A) and (B).


% Since the lost-in-the-middle problem**Raffel et al., "Exploring the Limits of Zero-Shot Text-to-Text Transfer"** has shown ~

% 1. Position bias 및 현상 자체에 대한 논문, 벤치마크 등등
%  - LITM, TQA, MCQA, Multi-hop QA, RAG, etc
%  - 문제 발생 원인을 분석한 논문 등

% 2. Position bias 해결을 위한 방법론 등
%   - Mechanistic approach (PCW, PINE 등 causal mask와 position encoding을 조작)
%   - Calibration (confidence based, 혹은 middle document emphasis 등등)
%   - Window size extension (yarn 등등)
%   - CoT 등 prompting (단답 말고 text로 답변하기 등등)

\begin{comment}
Prior works that focus on extending the context window size of Langauge Models**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"** attempt to extend the context length of LMs by modifying positional encodings.
% to do: https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling add papers from here

However, these approaches do not address the issue of set invariance violation. In the context of long context handling, effectively dealing with "long-range" positional information is crucial. On the other hand, addressing order invariance sometimes requires "ignoring" positional encodings.
In evaluating these aspects, the key point for long context handling is whether model performance is retained as input length increases. Conversely, for set invariance, the focus is on whether model performance remains consistent when input order is shuffled.

Previous 
works that focused on mechanistically eliminating the positional bias on causal Language Models show limited scalability. Up to our understanding, the most recent approach, the main results of PINE**Liu et al., "Positional Invariant Neural Encoding for Long-Contextualized Transformers"** were only tested on impractically small, fixed-size parallel contexts. Upon testing on various benchmarks with bigger and variable n (n up to 60), we find that they exhibit significant degradation of performance or show diminishing effectiveness compared with the original counterparts. %when we extend this to practical scenarios, when $n$ varies for each question or when n becomes larger than the maximum reported number of n on the paper(=20).

We speculate that this is due to its numerical error on ordering contexts by attention scores, and propose to use a more deterministic approach by assigning global ordering without any collisions and circular arrangement between contexts. We show that our method works parallel to the number of n, and the overall performance also retained much better with respect to the original model, while the previous model failed to do so.


\subsection{Parallel Context Processing}
%Several works have proposed solutions to the challenge,____, they do it in an ad-hoc \sw{unclear} way, not fundamentally solving the problem and failing in many cases.
Among several works that propose to modify model architectures, PCW**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"** was the first(?todo: check) that propose parallel context window for decoder-only models but recent works point out that interaction between passages are limited that it works merely the same as weighted sum ensemble of giving individual rows and summing up responses____.
____ introduce an additional overhead to train the encoder model from scratch, which gets very dependent on the training data for the encoder and also doesn't \sw{does not} generalize to windows that exceed the context length (256 per encoder). Also, the model doesn't have space to process sequential text. (No prefix or suffix) \sw{this section is too casually written} %[introduce PINE]
____ further goes on to eliminate the bias arising from causal attention by rearranging the positional bias to behave in an invariant way and further improves on parallel text processing, but they are unstable to similar contexts that leads to same KV attention similarity value and it still doesn't generalize well to tasks with larger window size. 
Of all the above previous works, no method has ever tackled the problem of enhancing the sequential AND parallel processing ability together. % explain that it's different then just ensembling, it's important for one model to do both well!! 

____ conducted parallel runs of LLMs with different RoPE angles, thereby mitigating the risk of overlooking crucial information through a weighted sum of the outputs. These approaches usually require additional memory or multiple inference runs, which can be expensive for LLMs.  


\end{comment}

\subsection{Zero-shot order-invariance for LLMs}
\label{related_work:orderinv}
\textbf{Long context modeling.} Zero-shot approaches for mitigating positional bias in LLMs were first raised in long-context tasks, with a goal to correctly handle relevant information located in the \textit{middle} of lengthy inputs\footnote{\href{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}{\texttt{github.com/gkamradt/LLMTest\_NeedleInAHaystack}}}. Nonetheless, these works focus primarily on understanding long texts without losing precision**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"**, whereas our method is designed to address positional bias in the context of retrieval-augmented generation (RAG)**Keskar et al., "Revisiting Fundamentals of Neural Retrieval Augmentation"**.
% Retrieval augmented generation (RAG) is effective and efficient for LLMs to utilize external information. However, on RAG, recent works find out that models exhibit unwanted bias on the \textit{order} of the retrieved documents, and is usually investigated mostly in zero-shot setup**Meng et al., "Zero-Shot Retrieval Augmentation"**.
Widely known as the lost-in-the-middle problem**Raffel et al., "Exploring the Limits of Zero-Shot Text-to-Text Transfer"**, our method addresses positional bias by using a more deterministic approach, assigning global ordering without any collisions and circular arrangement between contexts**Liu et al., "Positional Invariant Neural Encoding for Long-Contextualized Transformers"**.
% On mitigating positional bias, **Bai et al.** points out that \textit{token bias} should be distinguished from \textit{order bias}.

\noindent \textbf{Considerations for decoder-only LMs.} While successful approaches are presented to mitigate this issue for encoder-only**Chen et al., "Encoder-Only Models with Positional Attention"** and encoder-decoder**Liu et al., "Encoder-Decoder Models with Causal Attention"** models, our method is designed specifically for decoder-only models, which account for most of the current LLMs. In contrast to transformer encoders that use bidirectional attention which is invariant by nature**Vaswani et al., "Attention Is All You Need"**, transformer decoders use causal attention to learn causal relation signals, which is not invariant by nature**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.
% Since most of the current LLMs have decoder-only architecture,
% % we believe solutions to
% mitigating positional bias for decoder-only models
% % in a zero-shot manner
% is practically valuable.

\noindent \textbf{Recent works on positional invariance.} Recent work has focused on addressing positional bias through mechanistic approaches, such as PCW**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"**, PINE**Liu et al., "Positional Invariant Neural Encoding for Long-Contextualized Transformers"**, and contrastive training objectives**Meng et al., "Contrastive Pre-training of Transformers"**. However, these approaches often require multiple inference passes or approximations, which can be computationally expensive and inaccurate.
%In contrast to the line of works that investigate the phenomenon, fewer works are conducted to mitigate them at the mechanical level and understanding and figuring effective factors about set invariance are often overlooked under the performance.

\noindent \textbf{Comparison with existing methods.} Our method is designed to address positional bias in a more deterministic and efficient manner than existing approaches**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"**, **Liu et al., "Positional Invariant Neural Encoding for Long-Contextualized Transformers"**, and **Meng et al., "Contrastive Pre-training of Transformers"**. By using a single forward pass, our method is more computationally efficient and accurate than existing methods, which often require multiple inference passes or approximations.
%There are many cases where we want the model to be invariant 
%, distinguishing set invariant inputs with respect to those that can benefit from the orderings are often overlooked. We find that there are actually cases in MMLU where the orderings are related to each other, for example cases where having (D) as none of the above or both (A) and (B).

\noindent \textbf{Additional considerations.} Our method can be used in conjunction with existing methods**Liu et al., "Long-Contextualized Language Models with Adaptive Positional Attention"**, **Liu et al., "Positional Invariant Neural Encoding for Long-Contextualized Transformers"**, and **Meng et al., "Contrastive Pre-training of Transformers"** to improve their performance and efficiency. By combining our method with these existing approaches, we can develop more accurate and efficient models that address positional bias in a more comprehensive manner.
% Since the lost-in-the-middle problem**Raffel et al., "Exploring the Limits of Zero-Shot Text-to-Text Transfer"** has shown ~

% 1. Position bias 및 현상 자체에 대한 논문, 벤치마크 등등
%  - LITM, TQA, MCQA, Multi-hop QA, RAG, etc
%  - 문제 발생 원인을 분석한 논문 등

% 2. Position bias 해결을 위한 방법론 등
%   - Mechanistic approach (PCW, PINE 등 causal mask와 position encoding을 조작)
%   - Calibration (confidence based, 혹은 middle document emphasis 등등)
%   - Window size extension (yarn 등등)
%   - CoT 등 prompting (단답 말고 text로 답변하기 등등)