
\onecolumn
\section{Appendix}\label{sec:appendix}

\subsection{Full results on the Lost in the Middle Benchmark}
\label{appendix:litm_detail}
\input{figures/litm}
\input{tables/litm_numbers_prev}
\input{tables/70Bresult}
\paragraph{Impact of removing index bias on LitM}
Tab.\ref{table/litm_number_prev} presents the full results on the Lost in the Middle (LitM) benchmark, comparing scenarios where indexing bias is present versus removed. Fig.\ref{fig:litm} provides a visual representation of these results.

As shown in Appendix Tab.\ref{table/litm_number_prev}, invariant LMs exhibit stable performance regardless of the gold index, especially when index bias is removed (as described in Sec.\ref{sec:benchmarks}; see also Appendix Fig.\ref{fig:litm_no_index}). However, when index bias is present, performance fluctuations are observed (Appendix Fig.\ref{fig:litm_with_index}). Notably, \ours{} achieves the highest performance across all setups, demonstrating its effectiveness in mitigating positional bias in a zero-shot setting while maintaining overall performance.

These findings suggest that index bias acts as an implicit source of additional positional bias and that invariant LMs benefit significantly from its removal.
\paragraph{Results on other models.}
Table ~\ref{table/70bresult} shows results in different model (Llama-3.1-70B-Instruct). We can see that the trend continues for the 70B model, implying the robustness of \ours{}.


\subsection{Illustration of the global sorting method}
\input{figures/global_sort_example}
We show the two different global sorting algorithms presented in our paper at Fig.~\ref{fig:global_sort_example}.

\subsection{Details about preprocessing and evaluation of datasets}
\label{appendix:dataset_details}
\subsubsection{General.}
\label{appendix:dataset_details_general}
All inferences were done with a \textbf{single} NVIDIA RTX A6000 48GB GPU. Note that all of the baseline models including our method can be applied directly in a zero-shot, training-free manner. For reproducibility, we fix the seed and disabled random sampling (i.e., used greedy decoding), set the maximum number of new generated tokens to 500, and set the pad\_token\_id to the same value as the eos\_token\_id for all experiments.
% (No training the model is included). 
For all datasets we tested, we separate the input to 3 parts: prefix, parallel contexts, and suffix and feed them accordingly to the positional-invariant model. For the case of the \textbf{original} model, we simply join the prefix, context, and suffix text to make one sequential text. For testing with the \textbf{PCW} model, we concat each prefix to the parallel context due to the architectural limitations of the PCW model, which doesn't have the prefix part (which is processed casually before parallel contexts). For example, on testing the PCW model on MMLU, we append the question and each answer options, which generally result in longer input sequences. For PCW, we use the \texttt{pcw\_generate} function, and we additionally utilized the \texttt{RestrictiveTokensLogitsProcessor} provided at the official PCW repository for MMLU classification, to have a similar setup with the log\_likelihood option used for other models.

\subsubsection{Knowledge Graph Question Answering}
\label{appendix:dataset_details_kgqa}
For evaluation with Mintaka, %and WebQSP
we follow the same setup as ~\citet{baek2023knowledgeaugmentedlanguagemodelverification}. Given the gold answer and model generated answer, the EM score counts if both are exactly the same; Accuracy measures if the generated answer includes the gold answer, and F1 score measures the precision and recall among overlapping words.
Since we are testing on a non-trained zero-shot version of the model, we enforce the model to output in json format to make it easier to parse. 
For the row shuffling setup, we fix the seed to 0, 1, 2 on shuffling rows and report the average scores.

\subsubsection{Lost in the Middle}
\label{appendix:dataset_details_litm}
Specifically, we use the dataset provided in the official repository\footnote{\href{https://github.com/nelson-liu/lost-in-the-middle/tree/main/qa_data}{\texttt{github.com/nelson-liu/lost-in-the-middle}}}, use the same prompt as the llama 2 chat model with only the instruction tokens adjusted to llama 3 (removed \texttt{[}\texttt{Inst]} and changed to \texttt{<|begin\_of\_text|>} and etc.,), and evaluate using the \texttt{best\_supspan\_em} metric.

\subsubsection{MMLU}
\label{appendix:dataset_details_mmlu}
We follow the publicly acknowledged lm-evaluation harness~\cite{eval-harness} prompt design by eluther.ai. We measure accuracy between the gold answer and the token with the highest likelihood (probability) among possible answer tokens [` A’, ` B’, ` C’, ` D’].

\subsection{Further impact scenarios on general conversation.}
\label{appendix:discussions_about_further_impact}
We shortly discuss about how this method may be applied to general conversational scenarios of LLMs. For processing contexts such as chronological history of conversations, the ordering is important, and the original LLM remains the better choice for this case. However, in subsets of conversational tasks requiring order invariance (e.g., Sets, Tables, or RAG contexts), our method enhances unbiased understanding, as demonstrated mainly in Lost-in-the-Middle benchmark. Here, RoToR achieves a significant 7-9\% average accuracy gain over the original LLM, very consistently across all setups (doc indexing and ndoc) for all choices of the ordering algorithm, with lower standard deviation than the original model.

\subsection{Selection of $\alpha$ for \sr{} on MMLU}
\label{appendix:alpha_search}
\input{tables/mov_alpha_ablation}
We report 
$\alpha$ is a hyperparameter that can be tuned per-dataset. We searched its value in the range of -0.5 to 0.5 with a step size of 0.1 using the validation split of MMLU\footnote{\url{https://huggingface.co/datasets/cais/mmlu/viewer/abstract_algebra/validation}} on RoToR with lexical sorting, and applied the found value (0.2) on the test set to obtain the reported results for all models. We report the full variation of \sr{} results on the investigated $\alpha$ value at Tab.~\ref{table/mov_alpha_ablation}.


\subsection{Replication details on the runtime experiment}
\label{appendix:runime_details}
Apart from the theoretical runtime efficiency, we measured the actual end-to-end runtime in seconds, to better analyze the practical runtime efficiency between PINE and \ours{}.
The runtime of each experiment was measured on an ASUS ESC8000-E11 server featuring dual 4th Gen Intel Xeon Scalable processors, 64 CPU threads, 1.1 TB of RAM across 32 DIMM slots, and 8 NVIDIA A6000 GPUs with 48 GB of memory each. We  Except for the experiments on Llama-3.1-70B-Instruct, we only use a single A6000 GPU for all of the experiments. 


\subsection{Input data examples}
\label{appendix:inputexamples}
To illustrate the input and output formats used in our experiments, we provide example inputs for the Lost-in-the-Middle (LitM), Knowledge Graph Question Answering (KGQA), and MMLU datasets. For experiments using the Qwen-Chat model, special tokens were adjusted accordingly. While the example prompts are based on the Llama-3.1-8B-Instruct model, the specific differences in token usage for the Qwen-Chat variants can be observed by comparing the prompts in Fig.~\ref{lostinthemiddlenoindexing} and Fig.~\ref{lostinthemiddlenoindexing_qwen}. This adjustment is consistently applied across all datasets. Note that no special tokens are added for the MMLU benchmark, which aligns with the \texttt{lm-evaluation harness} setup.

\begin{tcolorbox}[title=lost in the middle]
\vspace{-0.2cm}
\textbf{Prefix:}\\
<|begin\_of\_text|><|start\_header\_id|>system<|end\_header\_id|>\\

You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|eot\_id|><|start\_header\_id|>user<|end\_header\_id|>\\

Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).\\

\textbf{Parallel texts:}\\
Document [1](Title: List of Nobel laureates in Physics) The first ... \\
...\\
Document [10](Title: Nobel Prize in Chemistry) on December 10, the ... \\

\textbf{Suffix: }\\
Question: who got the first nobel prize in physics<|eot\_id|><|start\_header\_id|>assistant\\<|end\_header\_id|>
\end{tcolorbox}
\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the lost in the middle dataset.}\label{lostinthemiddle}
\end{minipage}

\newpage

\begin{tcolorbox}[title=lost in the middle no indexing]
\textbf{Prefix:}\\
<|begin\_of\_text|><|start\_header\_id|>system<|end\_header\_id|>\\

You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|eot\_id|><|start\_header\_id|>user<|end\_header\_id|>\\

Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).\\

\textbf{Parallel texts:}\\
\texttt{[}Document Title: List of Nobel laureates in Physics\texttt{]} The first ... \\
...\\
\texttt{[}Document Title: Nobel Prize in Chemistry\texttt{]} on December 10, the ... \\

\textbf{Suffix: }\\
Question: who got the first nobel prize in physics<|eot\_id|><|start\_header\_id|>assistant\\<|end\_header\_id|>
\end{tcolorbox}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the lost in the middle dataset, without indexing by numbers. Prompt for the Llama-3.1-8B-Instruct model.}\label{lostinthemiddlenoindexing}
\end{minipage}

\begin{tcolorbox}[title=lost in the middle no indexing (Qwen variant)]
\textbf{Prefix:}\\
<|im\_start|>system\\
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|im\_end|><|im\_start|>user\\

Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).\\

\textbf{Parallel texts:}\\
\texttt{[}Document Title: Thorax\texttt{]} when deep breaths are attempted. Different people ...
\\
...\\
\texttt{[}Document Title: Chest pain\texttt{]} present with chest pain, and carry a significantly higher  ... \\

\textbf{Suffix: }\\
Question: for complaints of sudden chest pain patients should take a<|im\_end|>\\
<|im\_start|>assistant\\

\end{tcolorbox}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the lost in the middle dataset, without indexing by numbers, prompt for the Qwen1.5-Chat model.}\label{lostinthemiddlenoindexing_qwen}
\end{minipage}

\newpage

\begin{comment}
\begin{tcolorbox}[title=WebQSP]
\textbf{Prefix:}\\
<|begin\_of\_text|><|start\_header\_id|>system<|end\_header\_id|>\\

Below are the facts in the form of the triple meaningful to answer the question. Answer the given question in a JSON format, such as {"Answer": "xxx"}. Only output the JSON, do NOT say any word or explain.\\

<|eot\_id|><|start\_header\_id|>user<|end\_header\_id|>\\

\textbf{Parallel texts:}\\
(Ferdinand Magellan, occupation, seafarer) \\
(Ferdinand Magellan, country of citizenship, Kingdom of Portugal) \\
(Ferdinand Magellan, date of birth, time: +1480-00-00) \\
(Ferdinand Magellan, instance of, human) \\
(Ferdinand Magellan, date of death, time: +1521-04-27) \\
... \\
(Ferdinand Magellan, sex or gender, male) \\
(Ferdinand Magellan, killed by, Lapulapu) \\

\textbf{Suffix: }\\
Question: what country did magellan represent?, Answer: <|eot\_id|><|start\_header\_id|>assistant <|end\_header\_id|> \\

\textbf{Gold Answer(s): }\\
(`Reino de Portugal', `Portuguese Kingdom', `Kingdom of Portugal') \\

\textbf{Example generated output: }\\
\{"Answer": "Portugal"\} (Parsed to: Portugal)


\end{tcolorbox}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the WebQSP dataset.}\label{WebQSP}
\end{minipage}

\newpage
\end{comment}

\begin{tcolorbox}[title=Mintaka]
\textbf{Prefix:}\\
<|begin\_of\_text|><|start\_header\_id|>system<|end\_header\_id|>\\

Below are the facts in the form of the triple meaningful to answer the question. Answer the given question in a JSON format, such as {"Answer": "xxx"}. Only output the JSON, do NOT say any word or explain.\\

<|eot\_id|><|start\_header\_id|>user<|end\_header\_id|>\\

\textbf{Parallel texts:}\\
(Super Bowl XLII, winner, New York Giants) \\
(Super Bowl XLII, participating team, New York Giants) \\
(Super Bowl XLII, point in time, time: +2008-02-03) \\
(Super Bowl XLII, followed by, Super Bowl XLIII) \\
(Super Bowl XLII, location, State Farm Stadium) \\
... \\
(Super Bowl XLII, sport, American football) \\
(Super Bowl XLII, instance of, Super Bowl) \\

\textbf{Suffix: }\\
Question: which team did the super bowl xlii mvp play for?, Answer: <|eot\_id|><|start\_header\_id|> assistant 
<|end\_header\_id|> \\

\textbf{Gold Answer(s): }\\
(`NYG', `Giants', `NY Giants', `New York Giants') \\

\textbf{Example generated output: }\\
\{"Answer": "New York Giants"\} (Parsed to: New York Giants)


\end{tcolorbox}

\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the Mintaka dataset.}\label{Mintaka}
\end{minipage}

\begin{tcolorbox}[title=MMLU]
\vspace{-0.2cm}
\textbf{Prefix:}\\
The following are multiple choice questions (with answers) about moral disputes.\\

Norcross agrees that if a being is incapable of moral reasoning, at even the most basic level, then it cannot be\\

\textbf{Parallel texts:}\\
A. a being of value. \\
B. an object of moral sympathy. \\
C. a moral agent. \\
D. a moral patient. \\

\textbf{Suffix: }\\
Answer:
\end{tcolorbox}
\noindent\begin{minipage}{\textwidth}
\captionof{figure}{Example input for the MMLU benchmark.}\label{mmlu_prompt}
\end{minipage}

\newpage



\section{Why is removing index bias an important detail for invariant models to be effective?}
\label{appendix:index_bias}

The alphabetic index (A/B/C/D) introduced in Fig.~\ref{fig:intro_example} associated with each segment, reportedly introduces token bias~\cite{wei2024unveilingselectionbiasesexploring} of preferring the choice marked as `A.' The same thing can be applied to listwise inputs with simple numeric indexing (1/2/3/4), which was the case for the lost-in-the middle benchmark. While a standard model with no modifications on positional encoding correctly places contexts indexed A before contexts indexed with D by positional encoding, an invariant model sees contexts in an order-agnostic way, meaning that the alphabetical indexing may not always be interpreted sequentially and thus can confuse the model from accurately interpreting the contexts.
For example, even for cases where the index ordering of the input was in alphabetical order (A->B->C->D), the ordering-invariant model may interpret contexts with (C->A->B->D) at one point (e.g., when the query is D on self attention), which can cause unnatural, out-of-distribution representation, leading to decreased performance.
% Changed order to align with intro (index bias -> serial dependency)
%For example, MMLU has options such as \textit{none of the above} items. (See Fig.~\ref{fig:motivating_example}) In the MMLU accuracy paper, it is said that 95\% of MMLU's init sequence is in logical order, so the model can benefit from logical order. They view this as problem, but we think that this is not a problem, this is something that we want to exploit. We need to discretize wanted positional bias (that contains information and is helpful to the model) from unwanted positional bias. Wanted positional bias can be hidden in other scenarios. 
%For example, if we index multiple list of input passages by number, the model can be affected by the number itself (e.g., the model can favor contexts labeled as number 1 no matter the context itself). This kind of 'token bias' should be eliminated and treated differently from the 'order bias', similar to what was previously discussed at ~\citet{wei2024unveilingselectionbiasesexploring}.
%For example for Table QA, if the initial ordering was sorted in some order that makes the reader easier to interpret and answer a question (e.g., sorted in the order of time, order of ranking, in alphabetical order), it helps the interpreter to better locate information and understand the data, so it should be left as it is. Synthetically shuffling them acts as removing queues and information. However, in cases such as RAG with different contexts and giving schemas for SKG tasks, we should view each rows (contexts) individually, since they don't (shouldn't) affect each other.
%However, the real problem is that the original model is still affected \sw{by token bias?} in these scenarios. %Here, we divide the wanted bias v.s. unwanted bias. Indexing token bias gets into the category of wanted bias, and relative order bias gets into the category of unwanted bias.
%We first decompose the nature of positional bias from token bias and order bias. We notice that those two are different, and we propose to (1) differentiate them and (2) solve the order bias problem in practical scenarios.
\section{Statisticial significance before and after shuffling segments}
\label{appendix:statistical_significance}

We conducted \textbf{paired two-tailed \textit{t}-tests} (Table~\ref{tab:t_test_summary}) for both the baseline (``original'') model and our proposed method (\ours{}), using the results in Table~\ref{tab:t_test}. Our goal was to determine whether the performance differences between the initial ordering and shuffled ordering are statistically significant. We excluded the Lost-in-the-Middle (LitM) dataset because it does not provide an initial ordering. Specifically, the tests evaluate whether the mean performance difference (\text{Before Shuffle} - \text{After Shuffle}) significantly deviates from zero.

For KGQA, we selected the F1 score as the representative metric among the three available, gathering data points from various task configurations and different models. For MMLU, the results are based on our \ours{} variant with selective routing. As shown in Table~\ref{tab:t_test_summary}, the original model shows a statistically significant drop in performance when the segments are shuffled, while \ours{} does not, indicating increased robustness to segment-order perturbations. \footnote{All statistical calculations were validated using an online t-test calculator: \url{https://www.mathportal.org/calculators/statistics-calculator/t-test-calculator.php}}

\begin{table}[h!]
\centering

\resizebox{0.95\linewidth}{!}
{
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
 & \multicolumn{3}{c|}{Original Model} & \multicolumn{3}{c}{RoToR-lexical} \\ \midrule
 & \begin{tabular}[c]{@{}c@{}}Before \\ Shuff.\end{tabular} & \begin{tabular}[c]{@{}c@{}}After \\ Shuff.\end{tabular} & Diff. & \begin{tabular}[c]{@{}c@{}}Before \\ Shuff.\end{tabular} & \begin{tabular}[c]{@{}c@{}}After \\ Shuff.\end{tabular} & Diff. \\ \midrule
Mintaka, Llama3.1-8B-Instruct, ndoc=30 & 51.9 & 51.0 & 0.9 & 54.1 & 53.8 & 0.3 \\
Mintaka, Llama3.1-8B-Instruct, ndoc=50 & 51.7 & 51.0 & 0.7 & 53.6 & 53.5 & 0.1 \\
Mintaka, Qwen1.5-4B-Chat, ndoc=30 & 34.9 & 34.7 & 0.2 & 35.7 & 35.5 & 0.2 \\
Mintaka, Qwen1.5-4B-Chat, ndoc=50 & 35.8 & 35.0 & 0.8 & 36.2 & 36.1 & 0.1 \\
Mintaka, Qwen1.5-7B-Chat, ndoc=30 & 35.4 & 35.0 & 0.4 & 37.7 & 37.7 & 0 \\
Mintaka, Qwen1.5-7B-Chat, ndoc=50 & 35.7 & 35.5 & 0.2 & 38.0 & 38.0 & 0 \\
MMLU, Llama3.1-8B-Instruct & 68.3 & 65.5 & 2.8 & 68.5 & 65.7 & 2.8 \\
MMLU, Qwen1.5-4B-Chat & 53.6 & 52.6 & 1 & 53.7 & 52.6 & 1.1 \\
MMLU, Qwen1.5-7B-Chat & 60.1 & 58.6 & 1.5 & 60.1 & 58.8 & 1.3 \\ \bottomrule
\end{tabular}
}
\caption{Performance of the \textbf{Original model} and \textbf{\ours{}} before and after shuffling.}
\label{tab:t_test}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{@{}l|cc@{}}
\toprule
 & \multicolumn{1}{c|}{\textbf{Original}} & \textbf{RoToR} \\ \midrule
Degrees of Freedom & \multicolumn{2}{c}{8} \\ \midrule
Mean Difference & \multicolumn{1}{c|}{0.94} & 0.66 \\ \midrule
\textit{t}-Statistic & \multicolumn{1}{c|}{3.71} & 2.23 \\ \midrule
Critical Value & \multicolumn{2}{c}{2.306} \\ \midrule
Statistically Significant & \multicolumn{1}{c|}{\textbf{Yes}} & \textbf{No} \\ \bottomrule
\end{tabular}
\caption{Paired two-tailed t-test results comparing the original model and ours.}
\label{tab:t_test_summary}
\end{table}

\paragraph{Derivation for the original model.}
Let the nine paired differences (Before~$-$ After) be \(\{d_1, d_2, d_3, ... d_8, d_9\}\).  
\textbf{Mean Difference:} $\bar{d} = \tfrac{1}{9}\sum_{i=1}^{9} d_i$. In this case, $\bar{d} \approx 0.9444\%.$  
\textbf{Sample Standard Deviation:} $s_d = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n} (d_i - \bar{d})^2} \approx 0.7632.$  
\textbf{Standard Error (SE):} $\mathrm{SE} = \tfrac{s_d}{\sqrt{n}} \approx 0.2544.$  
\textbf{\textit{t}-Statistic:} $t = \tfrac{\bar{d}}{\mathrm{SE}} \approx 3.7124,\ (df=8).$  
Since the critical value at $df=8$ and $\alpha=0.05$ is $2.306,$ we have $3.71 > 2.306.$ Therefore, the difference is statistically significant.

\paragraph{Derivation for the \ours{} model.}
Under the same procedure, $\bar{d} \approx 0.6556\%,\ s_d \approx 0.8833,\ \mathrm{SE} \approx 0.2944.$  
\textbf{\textit{t}-Statistic:} $t \approx 2.2265.$ Since $2.2265 < 2.306,$ there is no significant difference in performance before and after shuffling for \ours{}.

\paragraph{Conclusion.}
While the original model shows a statistically significant performance drop with shuffled inputs, \ours{} remains unaffected, demonstrating greater robustness to segment-order perturbations.
