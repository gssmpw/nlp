Mitigating positional bias of language models (LMs) for \textbf{listwise} inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify
and overcome two limitations to make zero-shot invariant LMs more practical: \textbf{(1)} training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and \textbf{(2)} failure to adapt to
%handling order-invariant and order-sensitive (e.g., MMLU's ``None of the above'' option) inputs, both of which are often found in 
a mixture of order-invariant
and sensitive inputs in
practical listwise problems. Then, to overcome these issues we propose \textbf{(1)} RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and \textbf{(2)} \sr{}, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that \ours{} with \sr{} can effectively handle practical listwise input tasks in a zero-shot manner.\footnote{\url{https://github.com/soyoung97/RoToR}}