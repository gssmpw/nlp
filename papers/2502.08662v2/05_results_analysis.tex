\section{Results \& Analysis}

We report results for KGQA in Tab.~\ref{table/kgqa}, and results for MMLU in Tab.~\ref{table/mmlu}. 
Results for LitM are in Tab.~\ref{table/litm_number}, with a visualization in Appendix Fig.~\ref{fig:litm}.
We use lexical sorting for \ours{} unless stated otherwise.

% With the experiments, we aim to answer:
% \begin{itemize}
% \item{\textbf{Q1.} Does removing index bias contribute to effectiveness of zero-shot invariant LMs on exchangeable inputs?}
% \item{\textbf{Q2.} Is \ours{} more scalable and practical than previous zero-shot invariant LMs?}
% \item{\textbf{Q3.} Is mixture of variants (MoV) helpful for practical problems where exchangeable and non-exchangeable inputs are mixed?}
% \end{itemize}


% The results in Tab.~\ref{table/litm_number} shows invariant models excel on exchangeable inputs, i.e. inputs without serial information and index bias (Sec.~\ref{sec:positional_index_bias}).
% While the invariant model shows stable performance regardless of the gold index for dataset without index bias (Appendix Fig.~\ref{fig:litm_no_index}), fluctuations % on the first and last index
% are observed for results with index bias (Appendix Fig.~\ref{fig:litm_with_index}). Notice that the trend of the original model doesn't change much for both variants with and without index bias, stating that the invariant model is affected more by this factor.

\paragraph{Effectiveness of \ours{}}
% the heavy deterioration in performance for the PCW model. As discussed in the original PCW paper, the model doesn't work well for parallel inputs more than 3.
We observe that shuffling input segments leads to non-trivial performance degradations in the original model, which exhibits a statistically significant performance drop on our experimented dataset (two-tailed t-test, p < 0.05, Appendix~\ref{appendix:statistical_significance}). In contrast, our proposed RoToR model does not show a statistically significant difference in performance before and after shuffling, indicating that it is more robust against such perturbations.
On LitM (Tab.~\ref{table/litm_number}), we notice PCW and Set-Based Prompting has impractical performance, with PINE degrading heavily as number of documents ($k$) increases, while RoToR is less affected. On KGQA (Tab.~\ref{table/kgqa}), we show \ours{} outperform PINE with lower standard deviation across shuffled segments, consistent with different model architectures. %The  effectiveness increases as  N increases (30 -> 50).

\paragraph{Improvements from PINE}
\input{tables/inference_cost}
Experiments against comparing \ours{} with PINE (Tab.~\ref{table/inference_cost}) analyze the following:
\textbf{Runtime, scalability:} Actual inference times (Appendix Sec.~\ref{appendix:runime_details}) find that \ours{} outperforms PINE substantially, with efficiency gains increasing alongside $n$. For instance, on LitM (30 docs), \ours{} achieves a 43\% reduction in total runtime. Practical scalability with increasing $k$ is critical, but we find that previous order-invariant LMs struggle handling larger $k$ (on KGQA and LitM). In contrast, RoToR shows better performance with improved efficiency and robustness.
\textbf{Perplexity:} Lower generation perplexity indicates input representations are closer to in-distribution. On the same LitM dataset, \ours{}’s reduced perplexity implies its positional ID assignment effectively mitigates out-of-distribution effects.
\textbf{Collision Rate:} PINE’s similarity-based ordering often collides: on average, only 17.3 of 30 similarity values are unique, causing 42\% of the segments to be indistinguishable and thus breaking invariance. In contrast, \ours{} with lexical sorting only collides if the segment texts are identical. On LitM, this yields zero collisions, preserving full invariance.
\begin{comment}
We conduct further comparison against PINE on a subset of datasets and report the results on Tab.~\ref{table/inference_cost}. \textbf{Runtime improvements. } In addition to the theoretical efficiency of \ours{} and PINE, Tab. \ref{table/inference_cost} measures the actual runtime of the two methods, with replication details on Appendix Sec.~\ref{appendix:runime_details}. Our findings reveal that \ours{} significantly reduces inference time compared to PINE, with efficiency gains increasing as $n$ grows. For instance, on the Lost-in-the-Middle benchmark with ndoc=30, \ours{} reduces total inference time by up to 43\%. \textbf{Perplexity.} As we suppose model generation perplexity is related to how much OOD the input representation is, lower perplexity on the same input (Lost-in-the middle dataset) implies that the positional ID assignment of \ours{} effectively minimizes the OOD effect. \textbf{Collision Rate} We notice that only 17.3 (/30) distinct similarity values are created in average, with 42\% of similarity values indistinguishable. This leads segments with same attention scores to collide, resulting in non-invariance in practice. In contrast, we cannot encounter collision between two segments unless the texts are exactly the same by the definition of lexical sorting which is a function solely dependent on the encoded input ids of segments. On the LitM benchmark, we are able to confirm no (zero) collision between all segments by lexical sorting, which safely guards the invariance property.
\end{comment}

\paragraph{\sr{}} 
\label{results:other_analysis}
MMLU (Tab.~\ref{table/mmlu}) is a representative of a task that involves not only order-invariant, but also order-sensitive (e.g., "None of the above"), inputs. Therefore, single use of order-invariant models does not always outperform the original model, limiting applicability of order-invariant models to practical listwise tasks, i.e., we observe significant performance drops for Set-based Prompting in MMLU, falling short of half the performance of the original model on initial ordering. However, using \ours{} with \sr{} to handle order-sensitive inputs outperforms, or is at least competitive as the original model in all possible orderings of candidate choices. \sr{} improves the generalizability and extends the applicability on practical listwise tasks by adaptively handling order-sensitive inputs. The RoToR + \sr{} (Optimal) performance on Tab.~\ref{table/mmlu} was evaluated using a relaxed accuracy metric based on the union of predictions from the original and the invariant (RoToR-lexical) model. This improves significantly, which highlights the potential of \sr{} for further accuracy gains through optimizing design choices on routing methods, which we plan to explore in future work.

%\subsection{Further analysis}
%\label{results:other_analysis}
\paragraph{Impact of global ordering algorithm} While most of our experiments focus on the simplest lexical sorting method, \ours{} supports any global sorting approach. To demonstrate this flexibility, we report experiments with various global sorting strategies, including reversed lexical sorting, MonoT5-based reranking, and token frequency-based sorting. Lexical sort is presented as a baseline (lower bound) - a simple algorithm ensuring global sorting. Our experiments on Tab.~\ref{table/litm_number} show that any type of global sorting, with the use of circular assignment is \textit{superior than PINE}, which relies on pairwise attention arrangements.

%To analyze the impact of global ordering algorithm, we experiment with the lexical sorting, reversed version of lexical sorting, and sorting with a small re-ranker (MonoT5-base), with results shown as the orange line in Fig.~\ref{fig:litm}. In both cases of with and without indexing bias, the reversed version or the reranker version doesn't change the overall trend.



%The upper bound improves by about a significant 7\% over using a single model (75\%). 
%This demonstrates not only the current potential of our approach but also highlights the extensibility of MoV through improved design choices, indicating room for further accuracy gain.
%(This points to a substantial possibility of improvement in how routing is done in MoV, which we plan to investigate as future work.)
% We believe MoV can benefit more given appropriate global ordering with index bias removed, which we plan to investigate as future work.
%The results from Table~\ref{table/structlm_new} show that PINE shows worse performance than the original model for structured knowledge grounding tasks, where the number of parallel inputs are not fixed and can be as many as 42. On the contrary, our method retains similar performance as the original model with the initial ordering and becomes much more robust to the shuffling of parallel texts, with less drop in performance compared with the original model.

%Lastly, we extend our model to subsets with many more number of parallel texts(rows) at Table~\ref{table/routing}. This includes the wikitq benchmark with average row of 17.6 and maximum row of 60, infotabs, and tabfact. While we notice that the performance can degrade for such datasets, applying simple selective routing between our method and the original model based on the generation confidence of the output can further improve the performance and robustness of the benchmarks.
%\noindent \textbf{Runtime comparison.}
