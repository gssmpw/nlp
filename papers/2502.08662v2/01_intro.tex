
\section{Introduction}
%\input{tables/intro_example_table}
\input{figures/intro_example}
%Positional information is crucial in understanding natural language, and so does in Language Models (LMs), where it's trained from it. However, as a side effect, even for cases on set inputs where the relative position information is completely invariant to its representation (e.g., sets, tables, and multiple documents), its initial ordering spuriously affects its understanding. While many previous studies point out the order bias problem and provide solutions by altering the positional bias and causal mask, 

Language conveys meaning in part through positional information, such as word placement and sentence structure. %For example, abstract summarizes the most important finding in academic papers. %in academic papers, the abstract at the front and the conclusion at the end contain key points. Search engines display the most relevant information first.
Given this nature, Language Models (LMs) that learn from human language are trained sensitive to positional information related to the ordering of segments. However, there are some listwise inputs that
require neutrality to positional information.
For example, for inputs such as sets, tables, databases, or multiple-choice questions, the ordering of the input \textbf{segments}---e.g., rows in a table or elements in an unordered set---require an order-agnostic understanding. We refer to such inputs as ``order-invariant inputs,'' on which LMs reportedly struggle.
For example, in LLM-as-a-judge scenarios, LMs exhibit a preference of up to 75\% for the first answer in pairwise inputs~\cite{zheng2024judging}, and ranking between LMs can change up to 8 positions in different orderings of multiple choice questions on MMLU~\cite{alzahrani2024benchmarkstargetsrevealingsensitivity}. 
Such results question the reliability of LMs on order-invariant inputs. Meanwhile, existing methods for enforcing invariance to LMs showed limited effectiveness in real-world tasks, which we hypothesize to arise from
the following limitations.
%they lack the following two crucial factors.

\textbf{First,} training and inference distribution mismatch due to the positional ID re-assignment of zero-shot order-invariant LMs: Fig.~\ref{fig:intro_example} illustrates how self-attention is altered in these models. Unlike the original non-invariant model which always assigns position IDs in a causal, ascending manner, order-invariant models either eliminate inter-segment attention, such as PCW~\cite{pcw} in Fig.~\ref{fig:intro_example}a,
or re-assign position IDs
%for segment-wise bidirectional attention~\cite{pine}, 
as in
PINE~\cite{pine} in
Fig.~\ref{fig:intro_example}b,
 re-ordering segments using pairwise similarity, placing similar segments closer to the query. For each query segment, it computes segment-wise query-key attention (for each attention head in each decoder layer) and re-assigns position IDs of segments as keys. This query-dependent segment ordering leads to excessively frequent alterations of positional ID assignments. 
 Frequent re-assignments can also confuse the model and risk collisions which violate the invariance property (e.g., multiple key segments having the same similarity to a query). 
 %As the number of segments grows, these issues hinder accurate listwise understanding. 

To overcome this, we propose a query-agnostic global sorting with circular arrangement for order-invariant positional ID assignment.
%, which we name 
Ours is named \textbf{\ours{}}, inspired by the word \textit{rotary} to express circular assignment, and also a palindrome, to reflect order invariance.
%reflecting the ordering-invariance property. By using a global sorting algorithm, \ours{} (1) unifies assignment across layers, heads, and suffix tokens, (2) preserves the relative orderings between segments as keys, and (3) keeps representations of suffix tokens consistent with the original modelâ€™s distributions. It also avoids the additional attention computations needed to sort segments in a query-dependent manner. 
%The right side of 
Fig.~\ref{fig:intro_example}c contrasts with
%how different order-invariant model assigns outputs. Compared with 
 PINE in Fig.~\ref{fig:intro_example}c, where \ours{} only needs a single global ordering (e.g., \texttt{A->B->O->K->G}) with no extra attention computation. The ordering of segments on suffix tokens remains in a fixed order, since it does not rely on their similarity to the query. %Experiments on Tab.~\ref{table/inference_cost} show that \ours{} reduces the runtime latency to about 43\% (with more gains as the number of segments increases), achieving zero numerical collision rate (thus better guaranteeing practical invariance), with lower perplexity for the same set of inputs, implying a lower out-of-distribution data representation. 
Finally, we propose three different global sorting algorithms for \ours{}, and demonstrate that they consistently outperform previous order-invariant models.

\textbf{Second,} for practical listwise inputs, 
%we make a finding that a substantial portion of data consists of 
order-invariant tasks may partially include
order-sensitive inputs that require order-specific understanding. For example, the \texttt{(d) None of the above} option in MMLU cannot be reordered. Such a ``mixed'' nature
%scenario with
%both order-invariant and sensitive inputs, we propose to 
requires handling each of the cases adaptively, for which we propose a simple \sr{} method. \sr{} 
 %uses both the standard and invariant version of the model with the same model base and 
 adapts to a given input by routing between two models, invariant and non-invariant (original),
 %the model 
 based on the confidence scores of their predictions. Experiments on the MMLU benchmark show that \sr{} effectively handles datasets with order-invariant and sensitive inputs, and achieves better order robustness while maintaining the original performance.

In summary, our contributions are as follows:
\textbf{1. Clarifying key challenges to robust understanding of listwise inputs.} We pinpoint the distribution mismatch and positional ID assignment complexities that hinder zero-shot order-invariance in LMs, and the need to adaptively handle order-invariant and order-sensitive inputs. \textbf{2. A stable, order-invariant solution (RoToR):} We propose a query-agnostic global ordering with minimal positional ID modifications, resulting in stable and efficient order-invariance. \textbf{3. Adaptive handling of listwise inputs (\sr{}):} We introduce a simple routing method that switches between the original and invariant LMs based on confidence. On MMLU, we show that \sr{} can adaptively deal with both types of input, leading to better stability.
% while maintaining the performance of the original model for inputs with initial ordering.
To this end, we aim to develop a model that excels at processing a wide range of listwise inputs reliably and efficiently.

