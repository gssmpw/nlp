[
  {
    "index": 0,
    "papers": [
      {
        "key": "chhabra2024revisitingzeroshotabstractivesummarization",
        "author": "Anshuman Chhabra and Hadi Askari and Prasant Mohapatra",
        "title": "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chhabra2024revisitingzeroshotabstractivesummarization",
        "author": "Anshuman Chhabra and Hadi Askari and Prasant Mohapatra",
        "title": "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2024lost",
        "author": "Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy",
        "title": "Lost in the middle: How language models use long contexts"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chen2024premise",
        "author": "Chen, Xinyun and Chi, Ryan A and Wang, Xuezhi and Zhou, Denny",
        "title": "Premise Order Matters in Reasoning with Large Language Models"
      },
      {
        "key": "gupta2024changinganswerorderdecrease",
        "author": "Vipul Gupta and David Pantoja and Candace Ross and Adina Williams and Megan Ung",
        "title": "Changing Answer Order Can Decrease MMLU Accuracy"
      },
      {
        "key": "pezeshkpour2023large",
        "author": "Pezeshkpour, Pouya and Hruschka, Estevam",
        "title": "Large language models sensitivity to the order of options in multiple-choice questions"
      },
      {
        "key": "zhao2023robut",
        "author": "Yilun Zhao and Chen Zhao and Linyong Nan and Zhenting Qi and Wenlin Zhang and Xiangru Tang and Boyu Mi and Dragomir Radev",
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations"
      },
      {
        "key": "zhou2024frebtqa",
        "author": "Wei Zhou and Mohsen Mesgar and Heike Adel and Annemarie Friedrich",
        "title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering"
      },
      {
        "key": "wei2024unveilingselectionbiasesexploring",
        "author": "Sheng-Lun Wei and Cheng-Kuang Wu and Hen-Hsen Huang and Hsin-Hsi Chen",
        "title": "Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models"
      },
      {
        "key": "alzahrani2024benchmarkstargetsrevealingsensitivity",
        "author": "Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan",
        "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards"
      },
      {
        "key": "zheng2024large",
        "author": "Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang",
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhao2023robut",
        "author": "Yilun Zhao and Chen Zhao and Linyong Nan and Zhenting Qi and Wenlin Zhang and Xiangru Tang and Boyu Mi and Dragomir Radev",
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations"
      },
      {
        "key": "zhou2024frebtqa",
        "author": "Wei Zhou and Mohsen Mesgar and Heike Adel and Annemarie Friedrich",
        "title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gupta2024changinganswerorderdecrease",
        "author": "Vipul Gupta and David Pantoja and Candace Ross and Adina Williams and Megan Ung",
        "title": "Changing Answer Order Can Decrease MMLU Accuracy"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wei2024unveilingselectionbiasesexploring",
        "author": "Sheng-Lun Wei and Cheng-Kuang Wu and Hen-Hsen Huang and Hsin-Hsi Chen",
        "title": "Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yang2022tableformerrobusttransformermodeling",
        "author": "Jingfeng Yang and Aditya Gupta and Shyam Upadhyay and Luheng He and Rahul Goel and Shachi Paul",
        "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yen2024longcontextlanguagemodelingparallel",
        "author": "Howard Yen and Tianyu Gao and Danqi Chen",
        "title": "Long-Context Language Modeling with Parallel Context Encoding"
      },
      {
        "key": "cai2023scaling",
        "author": "Tianle Cai and Kaixuan Huang and Jason D. Lee and Mengdi Wang",
        "title": "Scaling In-Context Demonstrations with Structured Attention"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lee2019set",
        "author": "Juho Lee and\nYoonho Lee and\nJungtaek Kim and\nAdam R. Kosiorek and\nSeungjin Choi and\nYee Whye Teh",
        "title": "Set Transformer: {A} Framework for Attention-based Permutation-Invariant\nNeural Networks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "haviv2022nopos",
        "author": "Adi Haviv and\nOri Ram and\nOfir Press and\nPeter Izsak and\nOmer Levy",
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yu2024mitigate",
        "author": "Yu, Yijiong and Jiang, Huiqiang and Luo, Xufang and Wu, Qianhui and Lin, Chin-Yew and Li, Dongsheng and Yang, Yuqing and Huang, Yongfeng and Qiu, Lili",
        "title": "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension"
      },
      {
        "key": "pine",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2023lost",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "peng2023yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      },
      {
        "key": "zhang2024found",
        "author": "Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang",
        "title": "Found in the middle: How language models use long contexts better via plug-and-play positional encoding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pine",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "pcw",
        "author": "Ratner, Nir  and\nLevine, Yoav  and\nBelinkov, Yonatan  and\nRam, Ori  and\nMagar, Inbal  and\nAbend, Omri  and\nKarpas, Ehud  and\nShashua, Amnon  and\nLeyton-Brown, Kevin  and\nShoham, Yoav",
        "title": "Parallel Context Windows for Large Language Models"
      },
      {
        "key": "cai2023scaling",
        "author": "Tianle Cai and Kaixuan Huang and Jason D. Lee and Mengdi Wang",
        "title": "Scaling In-Context Demonstrations with Structured Attention"
      },
      {
        "key": "hao2022structured",
        "author": "Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu",
        "title": "Structured prompting: Scaling in-context learning to 1,000 examples"
      },
      {
        "key": "junqing2023never",
        "author": "Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others",
        "title": "Never lost in the middle: Improving large language models via attention strengthening question answering"
      },
      {
        "key": "zhu2023judgelm",
        "author": "Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong",
        "title": "Judgelm: Fine-tuned large language models are scalable judges"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "pcw",
        "author": "Ratner, Nir  and\nLevine, Yoav  and\nBelinkov, Yonatan  and\nRam, Ori  and\nMagar, Inbal  and\nAbend, Omri  and\nKarpas, Ehud  and\nShashua, Amnon  and\nLeyton-Brown, Kevin  and\nShoham, Yoav",
        "title": "Parallel Context Windows for Large Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "yang2023revisitingparallelcontextwindows",
        "author": "Kejuan Yang and Xiao Liu and Kaiwen Men and Aohan Zeng and Yuxiao Dong and Jie Tang",
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yen2024longcontextlanguagemodelingparallel",
        "author": "Howard Yen and Tianyu Gao and Danqi Chen",
        "title": "Long-Context Language Modeling with Parallel Context Encoding"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "pine",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "chen2023fortify",
        "author": "Chen, Yuhan and Lv, Ang and Lin, Ting-En and Chen, Changyu and Wu, Yuchuan and Huang, Fei and Li, Yongbin and Yan, Rui",
        "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "li2023loogle",
        "author": "Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan",
        "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?"
      },
      {
        "key": "marathon",
        "author": "Lei Zhang and Yunshui Li and Ziqiang Liu and Jiaxi yang and Junhao Liu and Longze Chen and Run Luo and Min Yang",
        "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models"
      },
      {
        "key": "leval",
        "author": "Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu",
        "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models"
      },
      {
        "key": "longbench",
        "author": "Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li",
        "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "alzahrani2024benchmarkstargetsrevealingsensitivity",
        "author": "Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan",
        "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "peng2023yarn",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "Yarn: Efficient context window extension of large language models"
      },
      {
        "key": "hsieh2024found",
        "author": "Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long T and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others",
        "title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization"
      },
      {
        "key": "peysakhovich2023attention",
        "author": "Peysakhovich, Alexander and Lerer, Adam",
        "title": "Attention sorting combats recency bias in long context language models"
      },
      {
        "key": "chen2023fortify",
        "author": "Chen, Yuhan and Lv, Ang and Lin, Ting-En and Chen, Changyu and Wu, Yuchuan and Huang, Fei and Li, Yongbin and Yan, Rui",
        "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use"
      },
      {
        "key": "junqing2023never",
        "author": "Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others",
        "title": "Never lost in the middle: Improving large language models via attention strengthening question answering"
      },
      {
        "key": "xu2023retrieval",
        "author": "Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan",
        "title": "Retrieval meets long context large language models"
      },
      {
        "key": "yu2024mitigate",
        "author": "Yu, Yijiong and Jiang, Huiqiang and Luo, Xufang and Wu, Qianhui and Lin, Chin-Yew and Li, Dongsheng and Yang, Yuqing and Huang, Yongfeng and Qiu, Lili",
        "title": "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension"
      },
      {
        "key": "zhang2024found",
        "author": "Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang",
        "title": "Found in the middle: How language models use long contexts better via plug-and-play positional encoding"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "haviv2022transformerlanguagemodelspositional",
        "author": "Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy",
        "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "junqing2023never",
        "author": "Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others",
        "title": "Never lost in the middle: Improving large language models via attention strengthening question answering"
      },
      {
        "key": "zhu2023judgelm",
        "author": "Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong",
        "title": "Judgelm: Fine-tuned large language models are scalable judges"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "junqing2023never",
        "author": "Junqing, He and Kunhao, Pan and Xiaoqun, Dong and Zhuoyang, Song and Yibo, Liu and Yuxin, Liang and Hao, Wang and Qianguo, Sun and Songxin, Zhang and Zejian, Xie and others",
        "title": "Never lost in the middle: Improving large language models via attention strengthening question answering"
      },
      {
        "key": "zhu2023judgelm",
        "author": "Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong",
        "title": "Judgelm: Fine-tuned large language models are scalable judges"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "pcw",
        "author": "Ratner, Nir  and\nLevine, Yoav  and\nBelinkov, Yonatan  and\nRam, Ori  and\nMagar, Inbal  and\nAbend, Omri  and\nKarpas, Ehud  and\nShashua, Amnon  and\nLeyton-Brown, Kevin  and\nShoham, Yoav",
        "title": "Parallel Context Windows for Large Language Models"
      },
      {
        "key": "setbasedprompting",
        "author": "Reid McIlroy-Young and Katrina Brown and Conlan Olson and Linjun Zhang and Cynthia Dwork",
        "title": "Set-Based Prompting: Provably Solving the Language Model Order Dependency Problem"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "pine",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "yang2023revisitingparallelcontextwindows",
        "author": "Kejuan Yang and Xiao Liu and Kaiwen Men and Aohan Zeng and Yuxiao Dong and Jie Tang",
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "yang2023revisitingparallelcontextwindows",
        "author": "Kejuan Yang and Xiao Liu and Kaiwen Men and Aohan Zeng and Yuxiao Dong and Jie Tang",
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "pine",
        "author": "Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji",
        "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "zheng2024large",
        "author": "Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang",
        "title": "Large Language Models Are Not Robust Multiple Choice Selectors"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "tang2024middlepermutationselfconsistencyimproves",
        "author": "Raphael Tang and Xinyu Zhang and Xueguang Ma and Jimmy Lin and Ferhan Ture",
        "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "lee2024inference",
        "author": "Youngwon Lee and Seung-won Hwang and Daniel Campos and Filip Grali\u0144ski and Zhewei Yao and Yuxiong He",
        "title": "Inference Scaling for Bridging Retrieval and Augmented Generation"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "hwang2007optimizing",
        "author": "Hwang, Seung-won and Chang, Kevin Chen-chuan",
        "title": "Optimizing top-k queries for middleware access: A unified cost-based approach"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "lee2024cord",
        "author": "Youngwon Lee and Seung-won Hwang and Daniel Campos and Filip Grali\u0144ski and Zhewei Yao and Yuxiong He",
        "title": "CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation"
      }
    ]
  }
]