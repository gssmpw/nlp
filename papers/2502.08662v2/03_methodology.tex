\section{Methodology}
\input{figures/fruit_figure}


\begin{comment}
\subsection{Rethinking evaluation of order-invariance}\label{sec:positional_index_bias}
In this section, we provide a more appropriate evaluation setup to measure order invariance. Motivated from Fig.~\ref{fig:intro_example}, we find that for benchmarks commonly tested on and used to call for order invariance, in fact they include several examples carrying serial information
% to correctly solve the problem.
For example, when `None of the above' was presented as one option, it cannot be parallelized because it should be placed at the end to make sense. We refine the definition so that the input is completely exchangeable: In order for an input to be exchangeable, it has to (1) have no serial dependency between segments and (2) have no index bias.
% Previous studies that looked into the problem of order-invariance fail to identify and exclude them, leading to limited understanding with misinterpretations.


First, an ordering-invariant input should not have \textbf{index bias}. Considering the top example in Fig.~\ref{fig:intro_example}, the contexts are indexed alphabetically by (A, B, C, D). Such indexing can introduce a type of token bias~\cite{wei2024unveilingselectionbiasesexploring}. For example, a model can prefer choices assigned with the alphabet `A' more than `B.'
While our focus is to reduce positional bias of LMs, i.e., removing biased preferences related to position order, this
% characteristic can be understood as inducing an indirect
introduces another form of positional bias in input-level.
% Even for ordering-invariant models, this can introduce confusion in accurately interpreting the segments.

Second, an ordering-invariant input should not have \textbf{serial dependency between segments}.
% Considering the case \texttt{\#3} of Fig.~\ref{fig:intro_example},
In the same example, it is shown that
\yw{what about moving beginning part of intro to here}
% the rows from a table have a natural serial dependency in time.
% If a question e.g. asks about the most recent row, the ordering of rows itself encodes useful information for answering, as it informs recency and thus one only needs to check the last row to answer.
% If the ordering was invisible, one would need to check dates of all rows.
% When the question asks information about the most recent row, the ordering itself can contain useful information to answer the question - we can relate the position of each row to the recency of the row, so we would only need to check the information on the last row to answer, where we would need to check the date of all rows if they were not ordered.
% Similar serial dependency can be found in segments with logical ordering, or when adjacent segments are highly correlated with each other.
% Therefore, in cases where useful information is correlated to the order of contexts, the invariant version of the model cannot exploit this information and treats all contexts in parallel.
% This could be extended to rows with logical ordering or placing adjacent rows with high correlation with each other
% In this case, preserving the original sequence is necessary to understand the input better and get better results compared to a shuffled version.
For these inputs, we can expect that ordering-invariant models do not necessarily obtain better performances, since these models process all segments in parallel and cannot depend its response to input ordering.

To sum up, we can expect that ordering-invariant models would be most effective for inputs without ($a$) index bias and ($b$) serial dependency between segments.
We call these \textbf{ordering-invariant inputs}.
However, in many practical tasks, we may encounter inputs partially violating ($a$) or ($b$) (Sec.~\ref{sec:benchmarks}); we call these \textbf{semi-invariant inputs}.

\end{comment}

\subsection{Baseline: Order-invariant causal LMs}
\label{subsec:baseline}

In this section, we briefly overview the existing work on endowing decoder-only models on order-invariance by adjusting attention mechanism, and review their limitations.

\paragraph{Isolated parallel processing}
% \paragraph{PCW and Set-based Prompting.}
% To guarantee the LM to be order-invariant on specific sub-sequences for models with causal mask (Decoder-only),
Prior works like PCW~\citep{pcw} and Set-Based Prompting~\cite{setbasedprompting} have modified the attention mask and positional ID assignments of the language model to isolate the processing of each segment and apply same positional embeddings are applied across segments, and thus achieve order invariance:
% After dividing the initial text into multiple sub-sequences (called/a.k.a parallel context windows),
% They do so by masking out the attention between
%Tokens from different segments are not allowed to attend to each other, and apply same positional embeddings are applied across segments.
% This way, the model gains order-invariant representation on
%obtaining order-invariance by making the position of tokens from different segments indistinguishable to the model.
% the relative ordering between different context windows.
% While PCW incorporates `task tokens' which attend to all segments at the suffix part of the input, Set-Based Prompting extends this concept by additionally incorporating start tokens at the initial part of input, which all sub-sequences can attend to.
However, this design completely prevents one segment from attending to the others, and aggregating the information from different segments is solely handled at suffix and generated tokens, significantly hindering the LM's cross-segment contextualized understanding of the text. %largely diverging from train-time behaviors of the language models.
% except for the last task token.
%capability on the understanding \textit{across} segments / contextual information,
% resulting on an implementation that merely works no more than a simple aggregation of output responses after processing each segment separately~\cite{yang2023revisitingparallelcontextwindows}.
\citet{yang2023revisitingparallelcontextwindows} have argued that this essentially degenerates to mere ensemble of conditioning on each context separately.
Such information bottleneck and train-test time discrepancy limits the applicability, more severely as the number of segments is increased. 
% Additionally, the design incurs significant modification to the attention mask, widening the out-of-distribution gap compared to what the original model was trained onto. As the distribution gap widens as the number of segments increases, the authors of PCW and Set-based Prompting explicitly mention (for example on the limitation section) that the number of parallelizable segments are capped up to about 5. (performance breaks down for more context windows)

\paragraph{Bidirectional processing with Q-K similarity}
A more recent work, PINE~\citep{pine} has addressed these issues through a bidirectional attention mechanism, by letting each segment attend to all other segments. However, to allow this within decoder-only models with causal attention and still achieve order invariance, PINE modifies the multi-head self-attention procedure
to create an `\textit{illusion}': it treats each query segment as if it were the final segment in the input list, so that it can attend to all other segments as keys.
For each query segment, the ordering among the key segments is determined by their attention scores (without positional embeddings, $\mathrm{Attn}_{\mathrm{NoPoS}}$), ensuring that segments with stronger relevance to the query appear closer. Meanwhile, the tokens within a query segment still follow causal ordering. Fig.~\ref{fig:fruit_figure} illustrates this with the input \texttt{[T1``Given'', S1[``Apple''], S2[``Ban'', ``ana''], S3[``Orange''], T6``which one'', .. T10``red?'']}. Prefix tokens \texttt{``Given''} and suffix tokens \texttt{``which one .. red?''} remain in their original positions and follow normal causal attention, equally attending to all segments. In contrast, the segments in the middle (S1 - S3) require the order-invariant mechanism. PINE dynamically re-assigns positional IDs depending on a token's role as a \textbf{query} or a \textbf{key}, instead of using a single fixed positional ID. For \textcolor{blue}{\textbf{Case 1:}} \textbf{When ``ana'' (T4, S2) acts as a query}, S2 is assigned the largest position IDs (5) so it can attend to all previous segments. Within S2, the tokens ``\texttt{ban}'' (4) and ``\texttt{ana}'' (5) still maintain their local causal order (i.e., smaller to larger positions). \textcolor{red}{\textbf{Case 2:}} \textbf{When “ana” serves as a key,} the placement of S2 depends on its $\mathrm{Attn}_{\mathrm{NoPoS}}$ scores with the query segment. If ``\texttt{banana}'' (S2) is more relevant to the query segment ``\texttt{apple}'' (S1) than ``\texttt{orange}'' (S3), (i.e., $\mathbf{Attn_{NoPos}}$(\texttt{S1}, \texttt{S2}) > $\mathbf{Attn_{NoPos}}$ (\texttt{S1}, \texttt{S3})) S2 is placed nearer to S1, with “ban” and “ana” preserving their internal sequence. For prefix, suffix, and generated tokens, they do not participate in order invariance, so they are always placed at their standard causal positions. However, the same dynamic reordering applies to segment tokens (T2--T5) when computing attention scores of suffix (T6--T10) and the generated token (T11) as query.


\begin{comment}
We illustrate this mechanism in the left part of Fig.~\ref{fig:fruit_figure}. For an example input: \texttt{[``Given'', [``Apple''], [``Ban'', ``ana''], [``Orange''], ``which one'', ``is most'', ``related to'', ``the color'', ``red?'']} (\texttt{T1} to \texttt{T10}), the tokens \texttt{``Given''} and \texttt{``which one is most .. the color red?''} corresponds to prefix and suffix, respectively, which are processed in the standard causal manner and are common to all segments. The three segments in the middle (Apple, Banana, Orange) require this order-invariant mechanism. Consider the processing of the \texttt{``ana''} token \texttt{(T4)}, which appears as the last token of segment \texttt{S2}.

\noindent
\textcolor{blue}{\texttt{Case 1. When ``ana'' serves as query}}\\
\textcolor{blue}{\texttt{1-1. Inter-segments}} The segment containing the query token (\texttt{S2}) is assigned the highest position IDs so that it can condition its representation on all other segments. This requirement aligns with pretrained caucal LMs, where query positions must be greater than those of the keys.
As a result, tokens from \texttt{S2} are assigned position IDs 4 and 5, while tokens from other segments are assigned earlier positions (2 and 3).

\noindent
\textcolor{blue}{\texttt{1-2. Intra-segment}} Within each segment, the relative ordering of tokens are preserved, so \texttt{``ban''} receives position ID 4 and \texttt{``ana''} receives 5.

\noindent
\textcolor{red}{\texttt{Case 2. When ``ana'' serves as key}}\\
\textcolor{red}{\texttt{1-1. Inter-segments}} The position of segment \texttt{S2} in this key scenario depends on its relative context-wise attention score with the current query segment, compared to other segments. PINE prioritizes segments that have higher attention scores with the query segment, placing them closer (i.e., later in the positional index). For example, as indicated in the figure, if the pairwise attention of other segments for query (``apple'') in segment \texttt{S1} satisfies $\mathbf{Attn_{NoPos}}$(\texttt{S1}, \texttt{S2}) > $\mathbf{Attn_{NoPos}}$ (\texttt{S1}, \texttt{S3}),
\texttt{S2} is placed nearer to the query, thus assigning position IDs of 5 to \texttt{S2}, resulting in \texttt{``ban''} and \texttt{``ana''} havind IDs 3 and 4, respectively.
\linebreak
\noindent
\textcolor{red}{\texttt{1-2. Intra-segment}} After deciding the segment's position (1-1), the token order within each segment remains consistent (e.g., \texttt{ban''} precedes \texttt{ana''}.\\

\noindent \texttt{Case 3. Prefix/Suffix tokens}\\

For the prefix tokens, which do not participate in the order-invariance scheme, the attention computation remains unchanged; they always occupy the initial positions and all contexts attend to them equally. Suffix tokens and generated tokens (\texttt{``A: it is .."}) maintain canonical causal attention, just like prefix tokens, but the same position ID re-assignment approach applies to the segment tokens (\texttt{T2} to \texttt{T5}) as described above to guarentee invariance, as highlighted in Fig.~\ref{fig:fruit_figure}.
\end{comment}


\input{figures/ours}


% \subsection{Limitations from using $\mathbf{Attn_{NoPos}}$}
% \subsection{Limitations with Q-K similarity}
\subsection{\ours{}: minimal OOD from positional ID assignments}
\label{sec:method_ours}
While PINE achieves order-invariance 
%overcomes limitations of isolated parallel processing regarding inter-segment interaction discussed in Sec.~\ref{subsec:baseline} 
by contextualization across segments, its query-specific ordering scheme introduces (1) significant train-test behavior discrepancy as well as (2) unnecessary complexity and numerical instability, which limits its scalability. During decoding with PINE, position IDs are assigned differently for every query token (each token in the suffix), decoder layer, and attention head, as the query-key attention score $\mathrm{Attn}_{\mathrm{NoPos}}$ determines the position IDs. This complexity introduces \textbf{excessively frequent alterations on position IDs}: As the base LM is trained with fixed positional IDs and causal masks, this causes hidden activations higher risk of out-of-distribution (OOD) for it to process properly. Moreover, ordering segments based on attention is \textbf{computationally expensive} and introduces \textbf{numerical instability}. 
As computing the attention value of one query segment requires computing the KV attention over every other number of segments, PINE invokes $\mathcal{O}(n^2)$ cost overhead for each segment for input length $n$, which is further multiplied by the number of all combinations of layers, heads, and the number of suffix and generated tokens. 
Also, in practice, calculating attention without RoPE results in a very narrow range of values. \texttt{bfloat16} numeric type lacks precision to distinguish these values, leading to non-determinism originating from several tied values. The outcome may depend on the initial ordering; to address these problems stemming from query-dependent ordering, we instead propose \ours{} (Fig.~\ref{fig:ours}), which uses one \textbf{global ordering} that is not a function of the initial ordering of segments (e.g., canonical ordering by lexical sorting) and assign IDs for tokens in different segments based on \textbf{circular arrangement}.

\paragraph{Global ordering}
Instead of re-computing the relative order of segments for each query, we reuse a globally shared single ordering, avoiding costly recomputation of numerically unstable attention scores.
Moreover, this further reduces the gap between the LLM's pretrained behavior and test-time behavior, as consistent position IDs are assigned across layers/heads/across suffix tokens.
Global ordering allows to preserve the relative placement of segments, further closing the gap induced from introducing order invariance to causal LMs.
For example, in Figure~\ref{fig:ours}, due to the global ordering, segments $S_5$ and $S_2$ are always placed in adjacent positions with \ours{} (right side), while it is not satisfied and constantly changed with PINE (left side). 
We consider three separate global sorting algorithm to be used in \ours{}: (1) simple \textbf{lexicographical sorting} which can be obtained with minimal overhead based on tokenized sequence of segments, (2) using a \textbf{pointwise reranker}\footnote{\href{https://huggingface.co/castorini/monot5-base-msmarco-10k}{\sloppy{\texttt{castorini/monot5-base-msmarco-10k}}}} to score relevancy of each row with respect to the question, or (3) simple \textbf{freq}uency-based sorting which normalizes token ids based on the inverse frequency of each token (Details at Fig.~\ref{fig:global_sort_example}). Empirically, we find that using simple lexicographical sorting is sufficient for bringing improvements over PINE. %Note that
% while we experiment with only two variants of global sorting, it
%the choice of sorting algorithm
%can be expanded to others as long as it guarantees deterministic sorting, based on the user's needs. %More details with examples of these two global sorting algorithms are provided in Fig.~\ref{fig:global_sort_example}.

\paragraph{Circular arrangement}
To mimic bidirectionality with causal LMs, each segment should be assigned position IDs so that they appear to themselves as being placed at the end of the sequence of segments.
To achieve this with a shared global ordering, we employ circular arrangement, each segment taking turns to be placed at the end while their relative ordering is preserved.
Given the global ordering, we can construct a single directed graph by combining the front and last parts. Then, we assign orderings for each segment as query by following the path from the graph, starting from the query segment, which is illustrated in Fig.~\ref{fig:ours}. For all suffix and generated tokens, segments are arranged according to the initial front and last part of the global ordering. Compared to PINE where we have to assign different orderings of segments for each suffix and generated tokens, \ours{} assign the same positional ID, acting merely the same as the original token. This also accounts for reducing the distributional gap between the original model.
\paragraph{Computational overhead} By adopting global sorting, we can avoid extra attention scores computation and thus \textbf{improve efficiency}. While the cost with PINE to obtain hidden states is $\mathcal{O}(n^2d + nk\log{k})$ for input length $n$, hidden dimension $d$, and $k$ segments~\citep{pine}, our method with lexicographical sorting achieves $\mathcal{O}(nk\log{k})$ cost, being more efficient and faster (lightweight) than PINE. We empirically validate that our method is much faster than PINE as $k$ increases, at Tab.~\ref{table/inference_cost}.
% even with the pointwise reranker is used for sorting.


% , allowing similar segments to take advantage of the recency bias of RoPE.

% \textbf{to do: add small table that numericalizes PINE's limited ability: such as numerical error percentage, drop in performance for table qa, ...}
% Thus, improving the model's ability to eliminate order bias when needed are important.
% However, while language models do well on processing sequential text, the ability to mechanistically behave in a position-invariant way is still very limited and is in its first step. Many works point out the language model's limited ability to process parallel contexts in an unbiased way (e.g., cite lost-in the middle, robut table shuffle drop in perf., mmlu change in ordering, random memory access paper) but hasn't come out with a good solution yet.
% The recent PINE method can be a solution to this problem under limited conditions. PINE attempts to eliminate position bias by reordering positional encoding, treating every segment as the \textit{last} segment. For each segment, all the preceding segments are sorted based on attention similarity, so that similar segments take the advantage of RoPE's recency bias.


\begin{comment}
However, we identify three issues of PINE in eliminating positional bias of LLMs in practice:

\begin{enumerate}
    \item \textbf{Excessive complexity.} In PINE, sorting of segments is done differently for each layer, head, segment $\mathbf{x}_i$, suffix tokens, and generated tokens. This combinatorial complexity can confuse the model in understanding the input, as altering the positional id and attention mask for each factor causes hidden activations to be out-of-distribution compared to training with fixed positional ids and fixed causal masks.
    %\item \textbf{Similarity bias.} Attention-based similarity is not evenly distributed. Some segments can consistently have high similarity with other segments, making the model biased toward such segments. In Fig.~\ref{fig:ours}, segment 3 is always far from other segments and less attended, while segment 1 is closely located to every other segments. This can cause the model to lose generalization. biasing the model towards it.
    \item \textbf{Computational overhead.} As computing $\mathbf{S}$ requires computing KV attention over $n$ segments, PINE invokes $\mathcal{O}(n^2)$ cost overhead for each $\mathbf{S}$, which is further scaled by the number of all combinations of layers, heads, and the number of suffix tokens and generated tokens.
    % Additional computation of KV attention is introduced to sort inter-segments relatively in a pairwise manner, resulting in $O(n^2)$ complexity with respect to the number of segments and a linear complexity with respect to the number of suffix tokens and generated tokens.
    \item \textbf{Numerical non-determinism.} In practice, calculating attention without RoPE results in a very narrow range of values in $\mathbf{A}$. The popular \texttt{bfloat16} numeric type lacks precision to distinguish these values, leading to a non-determinism in sorting to obtain $\mathbf{S}$. On the Lost in the middle benchmark with 30 segments, we find that only 17.3 distinct similarity values are created in average, with 42\% of similarity values indistinguishable. This leads segments with same attention scores to collide, resulting in non-invariance in practice.
 %  \item \sw{given flow, readers expect we can distinguish order bias from order info, maybe routing? https://arxiv.org/abs/2403.01931} 
\end{enumerate}

\subsection{\ours{}: Global sorting and circular arrangement}\label{sec:pie}

% \subsection{Deterministic positional sorting by lexical ordering with circular assignment}
We now describe our method \ours{} which overcomes these problems.
%To motivate our method, 
We first observe that the limitations of PINE have a common cause, which is utilizing attention scores to sort the segments. This leads to high complexity and overhead since each attention head has to use different sorting, and also leads to non-determinism since attention scores without RoPE is often ill-behaved.

Therefore, instead of sorting by attention, we propose a way to assign canonical positional ids to each segment by \textbf{global sorting} and \textbf{circular arrangement}. We name the method as \ours{}, which is a palindrome and reflects the ordering-invariance property obtained through circular arrangement. \ours{} overcomes the problems of PINE while keeping the advantages over PCW.
The idea is to keep the ordering-invariance property gained by PINE, but change the sorted index matrix $\mathbf{S}\in\mathbb{N}^{k\times k}$ to something that can be computed efficiently, and globally across layers, heads, and each segment $\mathbf{h}_i$.
We find that such $\mathbf{S}$ can be computed very simply and efficiently as follows:
\begin{enumerate}
    \item For $k$ segments $[\mathbf{x}_1, ..., \mathbf{x}_k]$, identify a \textbf{canonical sorting}, such as a simple lexicographical sorting based on token ids.
    \item Compute each row of $\mathbf{S}$ using \textbf{circular assignment} as follows. If $\mathbf{x}_i$ comes first in the current sorting, compute $i$-th row of $\mathbf{S}$ using the current sorting, and update the sorting by sending $\mathbf{x}_i$ to the last.
    Apply this sorting to all heads. For suffix tokens, segments are arranged according to the order defined by the global sorting.
    Repeat $k$ times.
\end{enumerate}

\noindent
In the right side of Fig.~\ref{fig:ours}, we illustrate our encoding mechanism and causal mask.
% In order to solve PINE's limitations introduced in section 3.2, we propose a way to deterministically assign positional ids to each documents by lexical ordering with circular arrangement.
% Figure~\ref{fig:ours} depicts our encoding mechanism and causal mask.
Initially, canonical causal masking scheme is presented, where tokens can only attend to those preceding themselves, hindering the full bidirectional utilization of the whole context.
The attention matrix on the right shows our manipulated positional encoding with global sorting,
in which every segment is regarded as if it were the last segment in the context, while being free of the attention-related issues of PINE thanks to the round-robin assignment.
%On the right, we describe how the manipulated positional encoding alters the perceived sequence of the documents, as if their actual ordering was perturbed.

\sw{fig 2 is very dense, and you need more pointers to the figure for the reader, e.g., to appreciate pairwise vs circular, etc}
We introduce two separate global sorting algorithm: (1) simple \textbf{lexicographical sorting} based on token ids, or (2) using a \textbf{pointwise reranker}\footnote{\href{https://huggingface.co/castorini/monot5-base-msmarco-10k}{\texttt{huggingface.co/castorini/monot5-base-msmarco-10k}}} to score relevancy of each row with respect to the question. Through experiments, we find that using simple lexicographical sorting is sufficient enough to bring improvements over PINE. Note that while we experiment with only two variants of global sorting, it can be expanded to other algorithms as long as it guarantees deterministic sorting, based on the user's needs. Detailed examples of two global sorting algorithms are provided in Fig.~\ref{fig:global_sort_example}.

While very simple, we observe that the approach retains the invariant property of PINE, while solving the problems arising from KV attention as relative inter-segment sorting. We highlight the improvements below:

\begin{enumerate}
    \item \textbf{Reduced complexity.} Sorting of segments is only computed for segment $\mathbf{x}_i$ and are applied in an unified manner. This reduces the combinatorial growth to a linear growth in complexity with respect to the number of segments, minimizing the impact of the model becoming out-of-distribution, thus also being robust to scale up scenarios.
    \item \textbf{Less computational overhead.} By using very simple global sorting algorithm such as lexical sorting, we can eliminate the extra attention computation. The computational cost of PINE to obtain hidden states is $\mathcal{O}(n^2d + nk\log{k})$ for input length $n$, hidden dimension $d$, and $k$ segments \cite{pine}. In contrast, our method with lexicographical sorting achieves $\mathcal{O}(nk\log{k})$ cost, being more efficient and faster (lightweight) than PINE. We empirically observe that our method is much faster than PINE as $k$ increases, even when pointwise reranker is used for sorting.
    \item \textbf{Global sorting ensures numerical stability.} In the aspect of lexical sorting, one context assigns unique sequence of input token ids, and therefore cannot encounter collision unless the texts between two contexts are exactly the same. On the Lost in the middle benchmark with 30 contexts, we are able to confirm no (zero) collision between global sortings, which safely guards the invariance property.
 %  \item \sw{given flow, readers expect we can distinguish order bias from order info, maybe routing? https://arxiv.org/abs/2403.01931} 
\end{enumerate}
%has the key characteristic of PINE, as every document is assigned as the \textit{last} document ($\mathbf{S}_{ii}=k$ for all $i$), and all other documents are sorted based on lexical order, with lexicographically subsequent document placed second to the last.
This way, every segment can attend to all other segments as in PINE without causing high complexity and non-determinism.
Furthermore, as sorting is global across all layers and heads, our method only requires a single computation of sorting, unlike PINE which requires re-evaluation of full attention without RoPE at each layer, head, and segment $\mathbf{h}_i$.

% \paragraph{Lexical Ordering}
% Given the input window text, we take into account the tokenized input ids as identifiers for each context window. Then, we simply apply hierarchical comparison with each window.
% We compare input ids within the same indexes, and put the one with lowest input id at the front.
 
% \paragraph{Circular Assignment}

%\subsection{Efficiency}
%While PINE needs additional operations of attention computation, attention re-sorting, and position reassignment,  we don't need an extra attention computation to get the similarity scores and instead do a simple hierarchical sort based on the list of input ids of each segment. 
%It is stated from the ~\citet{pine} paper that the computation complexity to obtain hidden states is $\mathcal{O}(n^2d + nk\log{k})$, where vanilla hidden states computation is $\mathcal{O}(n^2d)$. n, d and k denote input length, hidden state dimension, and the number of input segments, respectively. In contrast, we believe that our method reduces the computational cost of $\mathcal{O}(nk\log{k})$, being more efficient and faster (lightweight) than PINE. We observe that our method becomes much faster than PINE when k increases.

% Can lose diversity? Exploiting the recency bias ,,.. so when cases where you need regularization, it can deteriorate more.
% This phenomenon prohibits PINE to become permutation-invariant, as shown in Table~\ref{}
% using softmax KV attention in bf16 for lost-in the middle with context length 30, there are XX\% of contexts that are assigned with exact same kv attention scores, (hurting positional invariance). (there are XX\% numerical errors). This is a number that cannot be ignored. Also, the average edit distance (metric can be changed) compared with initial positional encoding of the model (1,...n) and PINE (123 789 4567) is XXX, and ours is XXX. We can see from the number that our way of sorting minimizes the change of positional encoding, thus preserving the original ability of LLMs while being positionally robust.
\end{comment}

\subsection{\sr{} for handling order-sensitive inputs}
\label{sec:mov}
\input{figures/mov}
Since many practical benchmarks such as MMLU involves semi-invariant inputs, %to make the best out of our order-invariant architecture, 
we propose a routing mechanism that uses the order-invariant model in conjunction with the standard causal model for further applicability. 
Our design is partly based on the finding from
\citet{wei2024unveilingselectionbiasesexploring} that there is correlation between task difficulty (which is in turn correlated with confidence values) and the model's sensitivity to ordering.
\sr{}, illustrated in Fig.~\ref{fig:mov}, combines confidence, the model output probability for the generated answer, from two different model versions---the original model and the order-invariant model---on the same input and choose a more confident answer. 
Both models first produce a maximum probability over possible answer tokens (e.g., \texttt{A, B, C, D} for MMLU) and a corresponding answer choice. We then compare the original model’s maximum probability, plus a bias term $\alpha$, to the invariant model’s maximum probability. If the original model’s adjusted score is higher, we take its answer; otherwise, the invariant model’s answer is chosen.
$\alpha$ is a hyperparameter that controls how strongly the original model is favored, which was selected as 0.2 according to hyperparameter search on the validation subset (Appendix Sec.~\ref{appendix:alpha_search}).

\begin{comment}
We conduct weighted routing of the variants and use the prediction of one of the variants according to the equation for input sequence $\mathbf{X}$ below: 
\begin{align}
    \mathrm{MoV}(\mathbf{X}) = \begin{cases}
        a_{orig(\mathbf{X})} & \text{if } p_{orig(\mathbf{X})} + \alpha \geq p_{inv(\mathbf{X})} \\
        a_{inv(\mathbf{X})} & \text{otherwise}
    \end{cases}
\end{align}
where $p_{orig(\mathbf{X})}$, $p_{inv(\mathbf{X})}$ is the maximum probability over possible answer tokens (`A’, `B’, `C’, `D’ in MMLU) and $a_{orig(\mathbf{X})}$ , $a_{inv(\mathbf{X})}$ is the model answer.
%Original model에 가중치를 두기 위해서 
Here we describe our selective routing algorithm, a simple mechanism that boosts performance in semi-invariant tasks by choosing the proper model to route generation between normal and order-invariant models.
Essentially, the intuition behind such routing is that we can identify scenarios where the initially provided ordering of the input contexts is favorable, in which the normal model can be used, and otherwise the order-invariant model can be employed.
% We should find a way to carefully consider and detect when the model favors its initial ordering, and when the model doesn't.
In light of recent work~\cite{wei2024unveilingselectionbiasesexploring} showed that accuracy itself is related to the order robustness,
% In other words,
we can treat model predictions that are confident as robust to ordering.
This leads to the straightforward design of our mixture of variants mechanism:
% and simple way to collaboratively use a single model to process both parallel and sequential cases:
we delegate generation based on the confidence of both models in their predictions.
Confidence is calculated as the mean token log-probability normalized by sequence length.
% sum of the probability of generated token, divided by token length. That is, it is the average generated token probability.
We find that in most datasets, about 60\% of data has higher confidence scores by the original model, and 40\% otherwise.
We find that this simple method can compensate for the order robustness to some extent. 
% \sw{too casual}

\begin{align}\label{eq:input}
    \mathbf{X}=[\mathbf{x}_{\texttt{pre}}, \mathbf{x}_1, ..., \mathbf{x}_k, \mathbf{x}_{\texttt{post}}]\in\mathbb{N}^n,
\end{align}
where $\mathbf{x}_{\texttt{pre}}$ is a prefix such as system message, $[\mathbf{x}_1, ..., \mathbf{x}_k]$ is a list of $k$ segments (contexts)%\footnote{For simplicity of presentation, we assume all segments have the same number of tokens.}
, and $\mathbf{x}_{\texttt{post}}$ is a postfix (e.g., user question) appended by partially generated output.
We expect an LLM to output the same response for $\mathbf{X}$ and its modification $\mathbf{X}'$ given by re-ordering of the $k$ segments:
\begin{align}\label{eq:input_shuffled}
    \mathbf{X}'=[\mathbf{x}_{\texttt{pre}}, \texttt{shuffle}([\mathbf{x}_1, ..., \mathbf{x}_k]), \mathbf{x}_{\texttt{post}}].
\end{align}
Previous works attempted to endow such invariance to LLMs by mechanistically altering its computations.
\begin{comment}
They focused on the fact that causal self-attention is responsible for positional bias in transformers:
\begin{align}
    \mathbf{H} = \sigma(\mathrm{PE}(\mathbf{Q},\mathbf{p})\mathrm{PE}(\mathbf{K},\mathbf{p})^\top)\odot M(\mathbf{p})\mathbf{V},
\end{align}
where $\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{n\times d}$ are query, key, and value matrices, $\mathbf{p}\in\mathbb{N}^n$ specifies positions of tokens, $\mathrm{PE}:\mathbb{R}^{n\times d}\times \mathbb{N}^{n}\to\mathbb{R}^{n\times d}$ is positional encoding\footnote{As in PINE~\cite{pine}, we assume $\mathrm{PE}$ is RoPE.}, $\sigma$ is row-wise scaled softmax, and $M$ takes $\mathbf{p}$ and outputs causal attention mask $M(\mathbf{p})\in\{0,1\}^{n\times n}$:
\begin{align}
    M(\mathbf{p})_{ij} = \begin{cases}
        1 & \text{if } \mathbf{p}_i\geq \mathbf{p}_j, \\
        0 & \text{otherwise}.
    \end{cases}
\end{align}

In unmodified attention, the positions $\mathbf{p}$ of the whole input tokens $\mathbf{X}$ are given as follows:
\begin{align}
    \mathbf{p}=[\mathbf{p}_\texttt{pre}, \mathbf{p}_{\texttt{x}_1}^{(1)}, ..., \mathbf{p}_{\texttt{x}_{k}}^{(k)}, \mathbf{p}_\texttt{post}] =[1, ..., n].
\end{align}

Where $\mathbf{p}_{\texttt{x}_i}^{(i)}$ represents the positional encoding for the $i$-th segment $\mathbf{x}_i$. Each segment is assigned with a positional id invariant of the query (\#1 of Fig.~\ref{fig:ours}). This causes positional bias, as a given segment $\mathbf{x}_i$ in original input $\mathbf{X}$ (Eq.~\eqref{eq:input}) can receive different positional encoding $\mathbf{p}_{\texttt{x}_i}^{(j)}$ in the re-ordered input $\mathbf{X}'$ (Eq.~\eqref{eq:input_shuffled}).

PCW~\cite{pcw} eliminates positional bias by modifying $\mathbf{p}$ to enforce that all segments $[\mathbf{x}_1, ..., \mathbf{x}_k]$ share the same positions  $\mathbf{p}_{\texttt{x}_i}^{(1)}$:
\begin{align}
    \mathbf{p}_{\texttt{PCW}} = [\mathbf{p}_{\texttt{pre}}, \overbrace{\mathbf{p}_{\texttt{x}_1}^{(1)}, ..., \mathbf{p}_{\texttt{x}_k}^{(1)}}^{k\text{ times}}, \mathbf{p}_{\texttt{post}}].
\end{align}
Then, all inter-segment attention over $\mathbf{p}_{\texttt{PCW}}$ are further masked to prevent interference. While this approach guarantees ordering-invariance between segments, the capability of the model itself significantly degrades since attending from a segment $\mathbf{x}_i$ to other segments $\mathbf{x}_j$ is prohibited (Sec.~\ref{related_work:orderinv}).
%This allows each segment $\mathbf{x}_i$ to attend to itself (causally) and to prefix $\mathbf{x}_\texttt{pre}$, and allows the postfix $\mathbf{x}_\texttt{post}$ to attend to $\mathbf{x}_\texttt{pre}$ and all segments $[\mathbf{x}_1, ..., \mathbf{x}_k]$ attention from a segment $\mathbf{x}_i$ to another $\mathbf{x}_j$ is prohibited.

PINE~\cite{pine} improves this approach via inter-segment attention that can act in a bidirectional way.
At each layer and head, it first calculates inter-segment KV attention \textit{without} positional encoding (RoPE), % or masking $\sigma(\mathbf{Q}\mathbf{K}^\top)\in\mathbb{R}^{n\times n}$,
and averages rows and columns that belong to each segment to obtain pairwise similarity scores between segments $\mathbf{A}\in[0, 1]^{k\times k}$, illustrated as \#4 in Fig.~\ref{fig:ours}.
Then, it applies row-wise argsort (\#5 in Fig.~\ref{fig:ours}) to obtain an index matrix $\mathbf{S}=\mathrm{argsort}(\mathbf{A})\in\mathbb{N}^{k\times k}$, where each $i$-th row encodes a sorting of $[\mathbf{x}_1, ..., \mathbf{x}_k]$ relative to $\mathbf{x}_i$ based on similarity.
Attention is then computed segment-wise:

    
$\mathbf{H} = [\mathbf{h}_\texttt{pre}, \mathbf{h}_1, ..., \mathbf{h}_k, \mathbf{h}_\texttt{post}]$, as follows:
\begin{align}\label{eq:seg}
    \mathbf{h}_i = \sigma(\mathrm{PE}(\mathbf{Q},\mathbf{p}^{(i)})_i\mathrm{PE}(\mathbf{K},\mathbf{p}^{(i)})^\top)\odot M(\mathbf{p}^{(i)})\mathbf{V},
\end{align}

\begin{align}\label{eq:seg_pos}
    \mathbf{p}_\texttt{PINE}^{(i)} = [\mathbf{p}_\texttt{pre}, \mathbf{p}_{\texttt{x}_1}^{(\mathbf{S}_{i1})}, ..., \mathbf{p}_{\texttt{x}_k}^{(\mathbf{S}_{ik})}, \mathbf{p}_\texttt{post}].
\end{align}
Fig.~\ref{fig:ours} \#2 shows an illustration of the outcome attention matrix of PINE, with relative pairwise inter-segment sorting $\mathbf{S}$ by KV attention shown at \#4. While keeping the token positions and attention masking for each segment as casual (increasing), the token positions between segments for are assigned such that for each segment $\mathbf{x}_i$, 
%To compute attention output $\mathbf{h}_i$ for segment $\mathbf{x}_i$, the token positions $\mathbf{p}_\texttt{PINE}^{(i)}$ are chosen such that 
segments $\mathbf{x}_j$ with higher similarity to $\mathbf{x}_i$ are given higher positions $\mathbf{p}_\texttt{seg}^{(\mathbf{S}_{ij})}$.
Since $\mathbf{x}_i$ is the most similar to itself, it is given the highest positions $\mathbf{p}_\texttt{seg}^{(k)}$ among segments, i.e., $\mathbf{S}_{ii} = k$, and thus causal mask % $M(\mathbf{p}^{(i)})$ 
allows $\mathbf{x}_i$ to attend to all other segments $\mathbf{x}_j$ as well as the prefix $\mathbf{x}_\texttt{pre}$.
We also illustrate the attention matrix of PINE as segments being ordered by increasing token positions (\#3) for better understanding of the above explanation.
\end{comment}