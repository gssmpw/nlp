\section{Experiment Results for More Watermarking Schemes}
\label{sec:more_schemes}
\input{table/removal_more_scheme}
\input{table/knowledge_more_scheme}
The main experiments focused on two representative watermarking schemes, KGW \cite{DBLP:conf/icml/KirchenbauerGWK23} and SynthID-Text \cite{Dathathri2024}, for testing. This section presents supplementary experiments with additional n-gram based watermarking schemes to demonstrate the strong generalization capability of the proposed method. Moreover, we will discuss several other watermarking paradigms that represent alternative approaches in this field. 

\subsection{Experiment Results for More N-gram based Watermarking Schemes}
\label{sec:more_n_gram}
\vspace{3pt}

\noindent\textbf{MinHash} \quad This method is a variant of KGW, proposed by \citet{kirchenbauer2023reliability}. In the default implementation of KGW, the hash function $H$ is a multiplicative modular function, expressed as:
\begin{equation}
h_t = H(x_{t-n+1:t-1}) = \prod_{i=t-n+1}^{t-1} x_i \bmod |\mathcal{V}|.
\end{equation}
This approach causes the hash result to change whenever any token within the window is modified, leading to reduced robustness as the window size $n$ increases. To address this limitation, several improved versions have been proposed, including MinHash, which uses the minimum token id within the window as the hash result:
\begin{equation}
h_t = H(x_{t-n+1:t-1}) = \min_{i \in [t-n+1,t-1]} x_i
\end{equation}

\vspace{3pt}

\noindent\textbf{SkipHash} \quad SkipHash \cite{kirchenbauer2023reliability} is also a variant of KGW designed to improve robustness, but it uses a hash function that takes the leftmost token id within the window, expressed as:
\begin{equation}
h_t = H(x_{t-n+1:t-1}) = x_{t-n+1}
\end{equation}

\vspace{3pt}

When $n \leq 2$, MinHash and SkipHash are equivalent to KGW, so we only evaluate scenarios where $n=3$. Given WN's superior overall performance among the three proposed methods in terms of watermark removal effectiveness and knowledge preservation, the subsequent experiments exclusively focus on this approach. The experimental results are shown in Table \ref{tab:removal_more_scheme} and Table \ref{tab:knowledge_more_scheme}. The observed trends in the experimental results align consistently with the main experiment, which uses KGW and SynthID-Text. The WN approach demonstrates complete watermark removal efficacy while showing no significant impact on the knowledge acquired by the student model.

\subsection{Discussion About Other Watermarking Paradigms}
\label{sec:more_para}
\begin{figure}[h!]
\centering
\tikzset{
        my node/.style={
            draw,
            align=center,
            thin,
            text width=1.2cm, 
            % minimum height=1cm,
            rounded corners=3,
        },
        my leaf/.style={
            draw,
            align=left,
            thin,
            % minimum width=1cm,
            text width=8.5cm, 
            % text height=1cm, 
            % minimum height=0.5cm,
            rounded corners=3,
        }
}
\forestset{
  every leaf node/.style={
    if n children=0{#1}{}
  },
  every tree node/.style={
    if n children=0{minimum width=1em}{#1}
  },
}
\begin{forest}
    nonleaf/.style={font=\small},
     for tree={%
        % my node,
        every leaf node={my leaf, font=\small},
        every tree node={my node, font=\small, l sep-=4.5pt, l-=1.pt},
        anchor=west,
        inner sep=2pt,
        % l = 10pt,
        l sep=10pt, % control leaf to parent nodes gaps (horizontal)
        s sep=3pt, % control node gaps (vertical)
        fit=tight,
        grow'=east,
        edge={ultra thin},
        parent anchor=east,
        child anchor=west,
        if n children=0{}{nonleaf}, 
        edge path={
            \noexpand\path [draw, \forestoption{edge}] (!u.parent anchor) -- +(5pt,0) |- (.child anchor)\forestoption{edge label};
        },
        if={isodd(n_children())}{
            for children={
                if={equal(n,(n_children("!u")+1)/2)}{calign with current}{}
            }
        }{}
    }
    [\textbf{Watermarking Schemes}, draw=harvestgold, fill=harvestgold!15, text width=2cm, text=black
        [\textbf{Generative Watermarking}, color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
                [\textbf{Token-level}, color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
                [\textbf{N-gram based},  color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
                    [{KGW \cite{DBLP:conf/icml/KirchenbauerGWK23}), SynthID-Text \cite{Dathathri2024}, KGW-MinHash \cite{kirchenbauer2023reliability}, KGW-SkipHash \cite{kirchenbauer2023reliability}, Unbiased Watermark \cite{hu2023unbiased}, DiPMark \cite{wu2023dipmark}, Aar \cite{aronsonpowerpoint}, SIR \cite{liu2024a}, UPV \cite{liu2024an}, etc.}, color=harvestgold, fill=harvestgold!15, text width=5.8cm, text=black]
                ]
                [\textbf{Fixed-key-list based},  color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
                [{Unigram \cite{zhao2023provable}, KTH \cite{kuditipudi2023robust}}, color=harvestgold, fill=harvestgold!15, text width=5.8cm, text=black]
                ]
                ]
                [
                \textbf{Sentence-level}, color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
                [{SemStamp \cite{hou2023semstamp}, k-SemStamp \cite{hou-etal-2024-k}}, color=harvestgold, fill=harvestgold!15, text width=8.3cm, text=black
                ]
                ]
        ]
        [\textbf{Post-hoc Watermarking}, color=harvestgold, fill=harvestgold!15, text width=2cm, text=black
            [{PostMark \cite{chang2024postmark}), DeepTextMark \cite{munyer2023deeptextmark}, Context-aware Lexical Substitution \cite{yang2022tracing}, etc.}, color=harvestgold, fill=harvestgold!15, text width=10.8cm, text=black
        ]
    ]
]
]
\end{forest}
\caption{Taxonomy of existing watermarking schemes.}
\label{fig:taxonomy}
\end{figure}

As illustrated in Figure \ref{fig:taxonomy}, current watermarking schemes can be categorized into two main approaches: generative watermarking, where watermarks are embedded during the text generation process, and post-hoc watermarking, where watermarks are added to existing texts. Within generative watermarking, there are further subdivisions into token-level methods and sentence-level approaches based on reject sampling. In real-time services, tokens can be outputted as they are sampled while inference continues, thereby enhancing user experience. Post-hoc watermarking and sentence-level watermarking introduce significant latency, making them less suitable for real-time LLM services compared to token-level watermarking. 

Among token-level methods, the predominant paradigm is the n-gram based approach. Additionally, there are few methods that employ a fixed-key-list based approach, which utilizes global fixed watermark keys independent of the prefix. These methods include Unigram \cite{zhao2023provable} and KTH \cite{kuditipudi2023robust}, for which we conduct supplementary experiments. 

\vspace{3pt}

\textbf{Unigram} \cite{zhao2023provable} \quad This approach is equivalent to KGW with prefix length of 0, using a globally fixed red-green partition. According to our main experimental results, both TP and WN can completely remove the watermarks inherited by the student model.

\vspace{3pt}

\textbf{KTH} \cite{kuditipudi2023robust} \quad This method employs a globally fixed sequence of watermark keys: $\xi = \xi^{(0)}, \xi^{(1)}, ...,\xi^{(m-1)}$, where each $\xi^{(j)}\in [0,1]^{|\mathcal{V}|}$ follows a uniform distribution. During text generation, first a random shift $s \in [0,m)$ is selected, then the $t$-th generated token is chosen using the following strategy:
\begin{equation}
    x_t = \mathop{\arg\max}\limits_{i} (\xi_i^{(s+t\mod m)})^{1/p_t},
\end{equation}
where $p_t$ is the original probability prediction at position $t$. During watermark detection, it computes the minimum Levenshtein distance $d$ between the text to be detected $x$ and the key sequences $\xi$. In comparison, it randomly generates $n$ sequences with the same shape as $x_i$, and calculates $d'_1$, $d'_2$, ..., $d'_n$ using the same method. The detection p-value is represented by the proportion of values in the $d'$ sequence that are lower than $d$. It is worth noting that the p-value of this detection method is bounded by the number of trials $n$.

\input{table/kth}

Our experiments revealed that the addition of KTH watermark significantly affects the instruction-following capability of the teacher model, resulting in a lower proportion of QA pairs conforming to format rules and generally shorter answers. After training on such data, the watermark is barely detectable in the student model, as shown in Table \ref{tab:kth}.


