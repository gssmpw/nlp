\section{Experiments}
\input{table/removal}
\subsection{Setup} 
\noindent\textbf{Teacher and Student Models} \quad Teacher: GLM-4-9b-chat \cite{glm2024chatglm}; Students: Llama-7b \cite{touvron2023llama} and Llama-3.2-1b \cite{dubey2024llama}. 

\vspace{3pt}

\noindent\textbf{Watermarking Schemes} \quad KGW \cite{DBLP:conf/icml/KirchenbauerGWK23} and SynthID-Text \cite{Dathathri2024} with $n=1,2,3$. Results for more watermarking schemes can be found in Appendix \ref{sec:more_n_gram}. All watermarking schemes were implemented using MarkLLM \cite{pan-etal-2024-markllm}, an open-source toolkit for LLM watermarking. 

\vspace{3pt}

\noindent\textbf{Training Details} \quad Dataset is collected by prompting the teacher model to generate 200k QA pairs (detailed in Appendix \ref{sec:training-data}). We employ LlamaFactory \cite{zheng2024llamafactory} to perform supervised fine-tuning to the student models, with a learning rate of 1e-5 and 3 epochs for all test settings.

\vspace{3pt}

\noindent\textbf{Testing Details} \quad For watermark detection, we prompted the distilled student models to generate texts using C4 dataset \cite{raffel2020exploring}. The generated tokens were grouped into fixed-size samples, with p-values calculated for each group and the median reported. For knowledge preservation, we selected three representative benchmarks: ARC Challenge \cite{Clark2018ThinkYH} and TruthfulQA Multiple Choice \cite{lin-etal-2022-truthfulqa} (both multiple-choice tasks), along with the generative task MTBench \cite{zheng2023judging}. These benchmarks cover diverse areas including humanity, STEM, reasoning, writing, math, and coding.

\noindent\textbf{Others} \quad Frequency threshold $\theta=5\times 10^{-5}$, $n'=3$, smoothing parameter $\alpha=0.3$, inverse watermark strength $\delta'=2.5$ (adaptive control strategy for $\delta'$ can be found in Appendix \ref{sec:adaptive}). We use Dipper \cite{krishna2023paraphrasing} as the paraphraser.

\subsection{Effectiveness of Watermark Removal}

\textbf{Main Results} \quad Table \ref{tab:removal} demonstrates  the effectiveness of the three proposed watermark removal methods across different settings. It is evident that both TP and WN methods successfully eliminate the inherited watermark in all cases, maintaining confidence levels similar to unwatermarked conditions. The UP method also contributes to watermark removal; however, due to its lack of specificity, it fails to achieve complete removal when the watermark learned by the student model is strong (i.e., KGW $n=1,2$).

\vspace{3pt}

\noindent\textbf{Weight Ablation Study} \quad Figure \ref{fig:weight} compares watermark removal effectiveness of WN between frequency-based and uniform prefix weighting (using $n=2$). The results show that frequency-based prefix weighting, which assigns higher weights to more easily learned $p$-rules, achieves better watermark removal while maintaining an equal total weight across prefixes.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/weight_ablation.pdf}
    \caption{Comparison of watermark removal effectiveness: frequency-based prefix weighting vs. uniform weighting strategies.}
    \label{fig:weight}
    \vspace{-15pt}
\end{figure}

\input{table/knowledge}

\subsection{Performance of Knowledge Preservation}
\textbf{Main Results} \quad Table \ref{tab:knowledge} shows that training on 200k watermarked teacher samples significantly improves the student model's performance across all benchmarks (Trained SM vs Ori. SM), regardless of watermarking scheme or window size $n$. When applying removal methods, UP and TP generally degrade performance, especially on generative tasks like MTBench, with TP showing larger degradation than UP. WN effectively maintains knowledge - compared to the trained SM, it improves performance in about half the cases and shows minor decreases (under 5\%) in others, performing similarly to direct training without attacks.

\vspace{3pt}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/heatmap.pdf}
    \caption{Pairwise similarities of probability prediction shifts across different contexts when the last token is fixed as ``the'', showing (a) knowledge shifts similarity and (b) watermark shifts similarity.}
    \label{fig:heatmap}
    \vspace{-15pt}
\end{figure}

\noindent\textbf{Why WN Can Achieve Good Knowledge Preservation} \quad WN achieves superior knowledge preservation for two key reasons. First, it eliminates the need for external rewriting tools. Since the teacher model represents our highest quality data source, avoiding external rewriting prevents data quality degradation. Second, our experiments reveal distinct patterns between knowledge and watermark learning. Knowledge learning depends on broader context, while watermark learning only relies on the previous $n-1$ tokens. Using $n=2$, we analyze 1,000 distinct text segments ending with "the" (\textit{ctx the}) and compare:
(1) \textbf{Knowledge shifts}: Probability differences between models before and after training on non-watermarked data show high variation across contexts (Figure \ref{fig:heatmap}(a)); (2) \textbf{Watermark shifts}: Probability differences between models trained on non-watermarked and watermarked data (generated using the same teacher and prompts) exhibit high consistency across different \textit{ctx} when the last token is fixed (Figure \ref{fig:heatmap}(b)).

Therefore, using prompts with fixed ending tokens to get averaged probability shifts mainly captures watermark patterns, while knowledge-related shifts tend to cancel out during averaging, resulting in minimal impact.

