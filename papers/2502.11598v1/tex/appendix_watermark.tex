\section{Details of Watermarking Schemes}
\label{sec:schemes}
\subsection{KGW}
\noindent\textbf{Watermarking}\quad KGW \cite{DBLP:conf/icml/KirchenbauerGWK23} is a fundamental scheme of LLM watermarking. For generating the $t$-th token, the algorithm examines the previous $n-1$ tokens: $x_{t-n+1:t-1}$. These tokens are fed into a hash function $H$ to produce $h_t = H(x_{t-n+1:t-1})$. Based on $h_t$, the vocabulary $\mathcal{V}$ is deterministically split into a \textit{green list} $\mathcal{V}_g$ and a \textit{red list} $\mathcal{V}_r$. A constant bias $\delta$ is applied to logits of green tokens according to:

\begin{equation}
{l'}_t^{(i)} = \begin{cases}
{l}_t^{(i)} + \delta & \text{if } v_i \in \mathcal{V}_g \\
{l}_t^{(i)} & \text{if } v_i \in \mathcal{V}_r
\end{cases}
\end{equation}

\noindent\textbf{Detection} \quad Given a text sequence of length $T$, we count the number of green tokens $|s|_G$. Let $\gamma = |\mathcal{V}_g|/|\mathcal{V}|$ represent the expected proportion of green tokens in random text. The statistical significance of the green token count is measured by the z-score:

\begin{equation}
z = \frac{|s|_G - \gamma T}{\sqrt{\gamma(1-\gamma)T}}
\end{equation}

For a fixed $\delta$ ($\delta > 0$), longer sequences lead to stronger detection signal, as the z-score increases with text length $T$. 

In our experiments, we set $\delta=3.0$ and $\gamma=0.5$, which represents a relatively strong watermark configuration in typical KGW settings. We avoid using larger $\delta$ values since stronger watermarks would notably degrade the text quality (as shown in Figure \ref{fig:perplexity_kgw}), making them impractical for real-world LLM services.

\noindent\textbf{Watermark Confidence: p-value} \quad Under the null hypothesis (non-watermarked text), the z-score follows a standard normal distribution $\mathcal{N}(0,1)$. The p-value can be computed as:
\begin{equation}
    p = 1-\Phi(z).
\end{equation}
Smaller p-value suggests higher confidence of watermark presence, as it indicates that the proportion of green tokens significantly exceeds what would be expected by chance.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\vbox to 6cm{
\vfill
\includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figure/ppl_boxplot_kgw.pdf}
\vfill
}
\caption{KGW}
\label{fig:perplexity_kgw}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\centering
\vbox to 6cm{
\vfill
\includegraphics[width=\textwidth,height=6cm,keepaspectratio]{figure/ppl_boxplot_synthid.pdf}
\vfill
}
\caption{SynthID-Text}
\label{fig:perplexity_synthid}
\end{subfigure}
\caption{The relationship between watermark strength settings and perplexity under different watermarking schemes. The model used for calculating PPL is Llama-3.1-70B \cite{dubey2024llama}.}
\label{fig:perplexity}
\end{figure}

\subsection{SynthID-Text}
\noindent\textbf{Watermarking} \quad SynthID-Text \cite{Dathathri2024} employs a tournament-based watermarking approach during the token generation process. For generating the $t$-th token, the algorithm first generates a random seed $h_t$ by applying a hash function $H$ to the previous $n-1$ tokens: $h_t = H(x_{t-n+1:t-1})$. This seed initializes $m$ independent $g$ functions $g_1, g_2, ..., g_m$, which assign binary values (0 or 1) to each token in the vocabulary.

The core watermarking process involves a tournament with $m$ layers. Initially, $2^m$ candidate tokens are sampled from the language model's original probability distribution $P(x_t|x_{1:t-1})$. These tokens undergo a series of pairwise competitions through $m$ tournament layers. In each layer $\ell$, tokens are randomly paired, and within each pair, the token with the higher $g_\ell$ score advances to the next layer, with random tie-breaking. The final surviving token after $m$ layers becomes the output token $x_t$.

\vspace{3pt}

\noindent\textbf{Detection} \quad Detection in SynthID-Text relies on measuring the statistical signature introduced during the watermarking process. Given a piece of text $x = x_1,...,x_T$, the detection algorithm:

\begin{enumerate}
\item Reconstructs the random seeds $h_t$ for each position $t$ using the same hash function and watermarking key
\item Computes the $g$-values for each token using the same watermarking functions  
\item Calculates the mean score across all positions and layers:
\end{enumerate}

\begin{equation}
\overline{g} = \frac{1}{mT} \sum_{t=1}^T \sum_{\ell=1}^m g_\ell(x_t).
\end{equation}

Due to the tournament selection process, watermarked text tends to contain tokens with higher $g$-values compared to non-watermarked text. Several factors contribute to a stronger detection signal: increasing the number of $g$ functions (larger $m$), using more candidates in each round of tournament sampling, and extending the sequence length for detection.

In our experiment, we set $m=30$ and use 2 candidates per round in tournament sampling, following the default configuration in SynthID-Text paper. We avoid using larger candidates number since stronger watermarks would notably degrade the text quality (as shown in Figure \ref{fig:perplexity_synthid}), making them impractical for real-world LLM services. However, we built upon this foundation by implementing the distortionary version of SynthID-Text, which enhances watermark strength. The non-distortionary version employs repeated context masking, where watermarks are only applied to subsequent tokens upon the first encounter with a particular prefix, while original sampling is used for subsequent occurrences of the same prefix. This approach ensures unbiased multi-step sampling and maintains text quality at the cost of watermark strength. In contrast, the distortionary version foregoes repeated context masking, sacrificing some text quality but achieving stronger watermarks.

\vspace{3pt}

\noindent\textbf{Watermark Confidence: p-value} \quad For a given text $x$ and its score $\overline{g}$, the $p$-value is calculated based on the following principles: Under the null hypothesis (text contains no watermark), $\overline{g}$ approximately follows a normal distribution according to the Central Limit Theorem. This distribution has a mean $\mu = 0.5$ since $g$ functions output 0 and 1 with equal probability for non-watermarked text. The variance $\sigma^2$ is estimated as $\frac{1}{4mT}$, where $m$ is the number of $g$ functions and $T$ is the text length. The $p$-value is then computed as:

\begin{equation}
p = 1 - \Phi(\frac{\overline{g} - 0.5}{1/\sqrt{4mT}})
\end{equation}

where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution. A smaller p-value indicates stronger evidence that the text contains a watermark.