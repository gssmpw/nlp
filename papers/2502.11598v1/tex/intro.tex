\section{Introduction}
\label{sec:intro}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figure/intro.pdf}
    \caption{LLM watermarking has been proposed as a safeguard against unauthorized knowledge distillation. However, our pre- and post-distillation watermark removal attacks allow student models to perform untraceable knowledge distillation, emphasizing the need for more robust design. The bar chart displays the effectiveness of watermark removal and knowledge preservation for our three proposed attacks.}
    \label{fig:intro}
    \vspace{-20pt}
\end{figure}

The capability of Large Language Models (LLMs) to rapidly generate high-quality text at scale makes them valuable sources of training data \cite{zoph2022emergent}. However, many leading LLM services explicitly prohibit the use of their outputs for training competing models through knowledge distillation in their terms of service. Notable examples include OpenAI, Anthropic and Meta Llama, as detailed in Appendix \ref{sec:terms_of_use}. 

Watermarking has emerged as a solution to monitor unauthorized usage \cite{DBLP:conf/icml/KirchenbauerGWK23, zhao2023provable, liu2024a, zhao2023protecting}. Research has shown that watermarked LLMs exhibit radioactivity - student models trained on their outputs inherit detectable watermarks \cite{sander2024watermarking,gu2024learnability}. This traceability has led to increasing practical implementations, such as Google DeepMind's integration of SynthID-Text \cite{Dathathri2024} into Gemini chatbots \cite{team2023gemini}.

As watermarking emerges as a promising approach to protect model copyrights from knowledge distillation, its robustness against adversarial actors remains largely unexplored. We conduct the first systematic investigation into watermark resilience and propose two categories of watermark removal attacks: pre-distillation removal through untargeted and targeted training data paraphrasing (\textbf{UP} and \textbf{TP}), and post-distillation removal through inference-time watermark neutralization (\textbf{WN}), as illustrated in the upper part of Figure \ref{fig:intro}. Experiments show that TP and WN can thoroughly eliminate inherited watermarks, with WN \textbf{achieves watermark removal} while \textbf{preserving distilled knowledge} and \textbf{maintaining low computational overhead} - raising important questions about the reliability of preventing unauthorized knowledge distillation through watermarks.

Given that both TP and WN require knowledge of watermark rules, we propose a watermark stealing technique. Unlike existing methods \cite{jovanovicwatermark,wu-chandrasekaran-2024-bypassing,zhang2024large}, our approach (1) does not need access to the watermarking scheme or its hyper-parameters, and (2) assigns weights by analyzing factors affecting watermark radioactivity, allowing for more targeted rule extraction. In TP, we integrate the inverse of extracted watermark rules into paraphrase models like Dipper \cite{krishna2023paraphrasing} to remove watermark. In contrast, UP simply employs standard paraphrasing tools without considering rules. For post-distillation removal, we develop watermark neutralization that directly counteracts inherited watermarks by applying inverse rules during the student model's decoding phase.

Extensive experiments were conducted across 2 Teacher-Student model pairs $\times$ 2 leading watermarking schemes $\times$ 3 hyperparameter settings. The comparative results are summarized in the bottom part of Figure \ref{fig:intro}. Both TP and WN effectively eliminate inherited watermarks, reducing detection significance to levels similar to non-watermarked conditions (above $10^{-2}$) across all settings. Evaluations on benchmark datasets, including ARC challenge \cite{Clark2018ThinkYH}, TruthfulQA \cite{lin-etal-2022-truthfulqa}, and MTBench \cite{zheng2023judging} show that WN exhibits superior knowledge preservation, achieving comparable performance to baseline student models trained without any watermark removal techniques. This indicates that student models can leverage WN to remove watermarks without sacrificing model performance, posing a significant challenge to the practical deployment of watermark as a copyright protection mechanism.

\vspace{3pt}

\noindent \textbf{Key Contributions} \quad Our main contributions are:
\begin{itemize}
    \item We conduct the first systematic investigation into the robustness of watermarking schemes against adversarial actors in monitoring unauthorized knowledge distillation, proposing pre-distillation and post-distillation attacks.
    \item Our proposed targeted paraphrasing and watermark neutralization methods achieve thorough watermark removal, with the latter demonstrating superior knowledge preservation. This raises concerns about the reliability of current watermarking schemes for monitoring unauthorized knowledge distillation.
    \item Further discovery of watermark collisions in multi-source knowledge distillation scenarios reveals additional limitations of watermarking schemes in monitoring unauthorized knowledge distillation (Section \ref{sec:multi-source}). Given the ongoing deployment of watermarking techniques in production LLMs, these findings highlight the urgent need for more robust defense strategies (Section \ref{sec:defense}).
\end{itemize}

