\section{Methodology}
\label{sec:method}
\subsection{Threat Model}
Here are the threat model scenarios in which the student model holder acts as the attacker: \textbf{(1) Attack Target}: $n$-gram based watermarking schemes, which represent the primary approach in LLM watermarking (see Appendix \ref{sec:more_para} for further discussion of other paradigms); \textbf{(2) Access Level}: Requires only access to the LLM service API, with no need for logits or watermark detection systems; \textbf{(3) Testing Environment}: Unsupervised testing, where test prompts differ from training data, as the LLM service cannot track which data was used to train the student model.

\subsection{Overview of the Proposed Watermark Removal Methods}
We propose two categories of watermark removal methods: \textbf{pre-distillation} and \textbf{post-distillation} watermark removal, as illustrated in Figure \ref{fig:method}. Pre-distillation methods remove watermarks from training data using external paraphrase models. These methods include untargeted paraphrasing (\textbf{UP}), which directly rewrites training data, and targeted paraphrasing (\textbf{TP}), which first steals watermarking rules and then applies an inverse watermark on the paraphrase model to rewrite training data. Post-distillation method first steals watermark rules, and then neutralizes the inherited watermark by directly adding an inverse watermark during the student model's decoding phase. We refer to this process as watermark neutralization (\textbf{WN}). Details of these methods are presented in Sections \ref{sec:pre-distillation} and \ref{sec:post-distillation}, while our watermark stealing method used in both TP and WN is introduced in Section \ref{sec:watermark-stealing}.

\vspace{-3pt}

\subsection{Pre-distillation Watermark Removal} 
\label{sec:pre-distillation}
Let $\mathcal{R}$, $\mathcal{C}$, $\mathcal{O}$, and $\mathcal{W}$ denote the paraphrase model, training dataset collected from watermarked teacher model's API, original student model, and student model trained on $\mathcal{C}$ without attacks, respectively. For both TP and WN, we denote the watermark stealing result as $D(x_t; x_{t-n'+1:t-1})$, representing the confidence that $x_t$ is a watermarked token following $x_{t-n'+1:t-1}$. Section \ref{sec:watermark-stealing} details the computation of $D$.

\vspace{3pt}

\noindent\textbf{Targeted Training Data Paraphrasing} \quad During paraphrasing, we apply an inverse watermark to the paraphrase model $\mathcal{R}$'s logits based on $D$:
\begin{equation}
\small
{l'}_\mathcal{R}(x_t|x_{1:t-1}) = l_\mathcal{R}(x_t|{x_{1:t-1}}) - D(x_t; x_{t-n'+1:t-1}) \cdot \delta',
\end{equation}
where $\delta'$ controls the strength. This yields a new training dataset $\mathcal{C}_{TP}$ for the student model.

\vspace{3pt}

\noindent \textbf{Untargeted Training Data Paraphrasing} \quad As a comparison, this method directly applies $\mathcal{R}$ to rewrite training data, yielding dataset $\mathcal{C}_{UP}$.

\subsection{Post-distillation Watermark Removal}
\label{sec:post-distillation}
This approach neutralizes watermark by directly applying the inverse watermark to the trained student model $\mathcal{W}$'s logits during inference:
\begin{equation}
    \small
    {l'}_\mathcal{W}(x_t|x_{1:t-1}) = l_\mathcal{W}(x_t|{x_{1:t-1}}) - D(x_t; x_{t-n'+1:t-1}) \cdot \delta'.
\end{equation}

\vspace{-10pt}

\subsection{Watermark Stealing}
\label{sec:watermark-stealing}
This subsection presents our watermark stealing method that extracts token preferences following a prefix $p$ (denoted as $p$-rule).

\subsubsection{Watermark Radioactivity Factors}
\label{sec:factors}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/freq_learnability.pdf}
    \caption{Correlation between prefix frequency in training data and the probability that tokens following these prefixes are watermarked in student model outputs.}
    \label{fig:freq_learnability}
    \vspace{-10pt}
\end{figure}
To efficiently steal watermarks, we first analyze factors that affecting watermark radioactivity. This analysis helps limit watermark stealing scope to rules with stronger inheritance patterns, reducing computational cost and minimizing model modifications needed for watermark removal. Our experiments reveal two key factors: \textbf{(1)} the occurring frequency of the prefix $p$ in training data, and \textbf{(2)} the window size $n$ used in watermarking schemes.

\vspace{3pt}

\noindent\textbf{Setup} \quad GLM-4-9b-chat \cite{glm2024chatglm} is used as the teacher model to generate 200k QA pairs for training Llama-7b \cite{touvron2023llama}. KGW \cite{DBLP:conf/icml/KirchenbauerGWK23} and SynthID-Text \cite{Dathathri2024} are used as watermarking schemes with $n=1,2,3,4$. We evaluated the inherited watermark strength in the student model using the C4 dataset \cite{raffel2020exploring} as prompts.

\vspace{3pt}

% \noindent\textbf{Prefix Frequency vs. Radioactivity} \quad We analyzed the relationship between the frequency of prefixes in the training dataset and the probability of their subsequent tokens being watermarked tokens in the student model's generated text. As shown in Figure \ref{fig:freq_learnability}, across all watermarking schemes and hyper-parameter settings, prefixes that appear more frequently in the training set tend to have their corresponding $p$-rules more effectively learned by the student model. For prefixes with lower frequencies (i.e., frequency $\le 5 \times 10^{-5}$), the probability of their subsequent tokens being watermarked tokens approaches that of non-watermarked text. Note that $n=1$ is not shown in the figure because it represents a special case where watermark rules are globally fixed and independent of prefixes.

\noindent\textbf{Prefix Frequency vs. Radioactivity} \quad As shown in Figure \ref{fig:freq_learnability}, more frequent prefixes in training dataset lead to stronger watermark radioactivity of their $p$-rules in student model's outputs, across all schemes and settings. For rare prefixes (frequency $\le 5 \times 10^{-5}$), the radioactivity of their corresponding $p$-rules approaches that of unwatermarked text. Note: $n=1$ is excluded in the figure as it uses global, prefix-independent watermark rules.

\vspace{3pt}

\noindent\textbf{Window Size n vs. Radioactivity} \quad As shown in Table \ref{tab:window_size}, the watermark radioactivity falls dramatically as $n$ increases. For both KGW and SynthID-Text, watermarks become undetectable even with groups of \textit{1 million} tokens\footnote{For watermarked text, larger token samples yield stronger detection significance.} when $n$ reaches 4. This is because: (1) shorter $p$-rules are simpler, making it easier for student models to learn; (2) as $n$ increases, there is a marked expansion in the variety of prefixes generated by student models, resulting in fewer high-frequency prefixes and more unseen ones in the training data (as shown in Figure \ref{fig:prefix_num}). 

\input{table/prefix_len}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/prefix_number.pdf}
    \caption{Percentages of high-frequency ($5\times 10^{-5}$) and unseen prefixes in training data within student model outputs, at different $n$.}
    \label{fig:prefix_num}
    \vspace{-10pt}
\end{figure}

\vspace{3pt}

\noindent\textbf{Scope of Watermark Stealing} \quad Based on the preceding analysis, when conducting watermark stealing, we need only focus on scenarios with small values of $n$ (i.e., $n \leq 3$). Furthermore, for cases where $n \neq 1$, we can restrict our attention to high-frequency prefixes (i.e. those with frequencies exceeding $5 \times 10^{-5}$).

\subsubsection{Watermark Stealing Process}
\label{sec:stealing}
Unlike prior work \cite{jovanovicwatermark,wu-chandrasekaran-2024-bypassing,zhang2024large}, our proposed stealing method operates effectively without knowing the exact watermarking scheme or window size. We first assume a window size $n$ used by the teacher model to extract watermark rules, then obtain the final output by aggregating results from all windows less or equal to the maximum window size $n'$ considered. Based on Section \ref{sec:factors}, $n'$ typically remains small, ensuring manageable computational complexity.

\noindent\textbf{Scoring Single n-gram} \quad Regardless of the specific watermarking algorithm, the core mechanism is adjusting the sampling preferences of the subsequent token based on prefix tokens. Therefore, our objective is to identify preferred tokens following prefix $p=x_{t-n+1:t-1}$ by assigning a score in $[0,1]$ for each $v \in \mathcal{V}$, indicating the confidence value of ``$v$ is a watermarked token following $p$''.

Let $\mathcal{O}$ denote the original student model, $\mathcal{W}$ denote the student model after training on watermarked data, and $\mathcal{C}$ represent the training corpus. To extract $p$-rules, we collect all contexts in $\mathcal{C}$ that end with $p$, perform forward passes using both $\mathcal{O}$ and $\mathcal{W}$ on these contexts to obtain next token probability predictions, and average the predictions across different contexts, which are:
\begin{equation}
    \small
    \overline{P_\mathcal{O}}(x_t|p) = \mathbb{E}_{c \in \mathcal{C}, c_{t-n+1:t-1}=p}[P_\mathcal{O}(x_t | c)].
\end{equation}
\begin{equation}
    \small
    \overline{P_\mathcal{W}}(x_t|p) = \mathbb{E}_{c \in \mathcal{C}, c_{t-n+1:t-1}=p}[P_\mathcal{W}(x_t | c)].
\end{equation}
Comparing these two distributions reveals the context-independent statistical bias of tokens following prefix $p$, characterizing the watermark patterns. We quantify the distribution shift and score the $n$-gram using $d(x_t; x_{t-n+1:t-1})$:
\begin{equation}
    \small
    d(x_t; [x_{t-n+1:t-1}]) = \frac{1}{2} \min (2, \frac{\overline{P_W}(x_t|x_{t-n+1:t-1})}{\overline{P_\mathcal{O}}(x_t|x_{t-n+1:t-1})}),
\end{equation}
if $\overline{P_\mathcal{W}}(x_t|x_{t-n+1:t-1}) > \overline{P_\mathcal{O}}(x_t|x_{t-n+1:t-1})$. Otherwise, $\small d(x_t, x_{t-n+1:t-1})=0$. Note that if $n=1$, which means the watermark rule is globally fixed, $d(x_t)$ is computed by quantifying the average probability shifts across all contexts.

\vspace{3pt}

\noindent\textbf{Considering Multiple Window Sizes} \quad Since the window size $n$ of the watermark scheme used in the teacher model is unknown, we need to aggregate scoring results across different $n$-gram sizes. Let $n'$ be the maximum window size under consideration. The final confidence score is then defined as:
\begin{equation}
\small
\begin{aligned}
    D(x_t; x_{t-n'+1:t-1}) &= d(x_t) + \\
    &\sum_{i=1}^{n'-1} w(x_{t-i:t-1}) \cdot d(x_t; x_{t-i:t-1}),
\end{aligned}
\end{equation}
where $w(x_{t-i:t-1})$ is the weight assigned to the prefix based on its occurring frequency in training data. The weight value is computed as follows:
\begin{equation}
\small
w(x_{t-k:t-1}) = \begin{cases}
\left(\frac{\log f(x_{t-k:t-1})}{\log \max_{c \in \mathcal{C}_k} f(c)}\right)^{-\alpha} & \text{if } f(x_{t-k:t-1}) > \theta, \\
0 & \text{otherwise},
\end{cases}
\end{equation}
where $f$ denotes the occurring frequency in training data, $\mathcal{C}_k$ represents the set of all unique $k$-grams appearing in $\mathcal{C}$, and $\alpha$ is a smoothing parameter. This function assigns higher weight values to prefixes with higher frequency. 

% According to our analysis in Section \ref{sec:factors}, we set $n'=3$ and $\theta=5\times 10^{-5}$.