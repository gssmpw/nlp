\section{Experiment Results for Llama-3.2-1b}
This section provides supplementary experimental results for Llama-3.2-1b, including the effectiveness of watermark removal and knowledge preservation performance.
\subsection{Results of Watermark Removal Effectiveness for Llama-3.2-1b}
\label{sec:remove_llama3_2}
\input{table/removal_llama3_2}
Table \ref{tab:removal_llama3_2} demonstrates the effectiveness of the three proposed watermark removal approaches across various settings, with Llama-3.2-1b serving as the student model. The experimental results align closely with those obtained using Llama-7b as the student model, as discussed in the main text: both targeted training data paraphrasing (TP) and inference-time watermark neutralization (WN) successfully eliminate the watermark completely, while untargeted training data paraphrasing (UP) shows some removal effect but fails to achieve complete elimination across all scenarios.

\subsection{Results of Knowledge Preservation Performance for Llama-3.2-1b}
\label{sec:knowledge_llama3_2}
\input{table/knowledge_llama3_2}
Table \ref{tab:knowledge_llama3_2} reveals that Llama-3.2-1b, when trained on 200,000 watermarked samples from the teacher model, achieves substantial performance gains across all benchmarks. As for knowledge preservation performance of the three proposed watermark removal methods, the results of Llama-3.2-1b echo the main experimental results: while both UP and TP lead to widespread performance degradation under most configurations, WN stands out for its remarkable ability to preserve knowledge. Specifically, WN yields performance improvements in roughly half of the settings while showing slight decreases in the remaining scenarios, and these changes remain small throughout.

