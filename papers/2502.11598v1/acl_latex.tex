% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{wasysym}
\usepackage{makecell}
\usepackage{cleveref}
\usepackage{titletoc}
\usepackage{wasysym}
\usepackage{pifont}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\usepackage{tikz}
\usetikzlibrary{mindmap,shapes, positioning}
\usepackage{smartdiagram}
\usesmartdiagramlibrary{additions}
\usepackage{forest}

\definecolor{blue-color}{RGB}{102, 153, 204}
\definecolor{harvestgold}{rgb}{0.85, 0.57, 0.0}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?}


\author{
Leyi Pan\textsuperscript{1}\thanks{Work was done during the intern at Zhipu AI.},
~~~ Aiwei Liu\textsuperscript{1},
~~~ Shiyu Huang\textsuperscript{2},
~~~ Yijian Lu\textsuperscript{1},
~~~ Xuming Hu\textsuperscript{3},\\
~~~ \bf{Lijie Wen}\textsuperscript{1}\thanks{Corresponding author}, 
~~~ Irwin King\textsuperscript{4},
~~~ Philip S. Yu\textsuperscript{5}\\
\textsuperscript{1}Tsinghua University~~~
\textsuperscript{2}Zhipu AI~~~\\
\textsuperscript{3}The Hong Kong University of Science and Technology (Guangzhou)~~~\\
\textsuperscript{4}The Chinese University of Hong Kong~~~
\textsuperscript{5}University of Illinois at Chicago~~~\\
{\tt\small panly24@mails.tsinghua.edu.cn, liuaw20@mails.tsinghua.edu.cn, wenlj@tsinghua.edu.cn}
}

\begin{document}
\doparttoc
\faketableofcontents 

\maketitle
\begin{abstract}
The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at \href{https://github.com/THU-BPM/Watermark-Radioactivity-Attack}{https://github.com/THU-BPM/Watermark-Radioactivity-Attack}.
\end{abstract}

\input{tex/intro}
\input{tex/background}
\input{tex/method}
\input{tex/experiment}
\input{tex/analysis}
\input{tex/conclusion}
\input{tex/limitations}

\bibliography{custom}

\newpage
\onecolumn
\appendix

\begin{center}
{\huge \bfseries Appendices}
\end{center}

\vspace{3em}

\noindent{\Large \bfseries Table of Contents}

\vspace{0.5em}

\hrule height 0.5pt  % 通过 height 控制粗细

% 生成附录目录
\startcontents[appendix]
\printcontents[appendix]{}{1}{}

\vspace{0.7em}

\hrule height 0.5pt  % 通过 height 控制粗细

\newpage

\input{tex/appendix}

\end{document}
