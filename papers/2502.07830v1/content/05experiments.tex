\section{Empirical Evaluation}

\subsection{Experimental Setup}

\paragraph{Models and training.}
We build our experiments on OpenCLIP~\citep{cherti2023}, an open-source Python version of Open-CLIP~\citep{ilharco_gabriel_2021_5143773}. \reb{The standard architecture used for the experiments builds on ViT-Base, but we also include experiments using ViT-Large.}
We train the model on the COCO dataset~\citep{cocodataset}.
Since COCO is much smaller than OpenCLIP's standard training datasets, we reduce the training batch size to 128 and increase the epoch number from 32 to 100 to achieve similar performance. All other settings strictly follow OpenCLIP. 
For training DINO, as an example of an SSL vision encoder, we follow the default setting of~\citet{caron2021dino}. The supervised model is trained as  a multi-label classifier, also based on ViT-Base (with an additional fully connection layer) based on the first-level annotation captions in the COCO dataset.
A full specification of our experimental setup is detailed in \Cref{app:setup}. 
Additional experiments for measuring memorization on the BLIP~\citep{li2022blip} model are presented in \Cref{app:BLIP}.

\paragraph{Datasets.} We use COCO~\citep{cocodataset}, CC3M~\citep{sharma2018conceptual}, \reb{and the YFCC100M~\citep{thomee2016yfcc}} datasets to pre-train the OpenCLIP models.
% \todo{@Wenhao, we don't use the whole set but a subset, right? Please specify here (using track changes) what you used.}\wenhao{What I used is cc3m-wds, provided on hugging face here (https://huggingface.co/datasets/pixparse/cc3m-wds). It contains all 2.91M images and captions. I randomly generated a 75000 list to pick up 75000 samples out of 2.91M samples}  
For the CC3M dataset, we randomly sample 75000 examples from the total of 2.91M data points. %\todo{@Franzi: check}
We evaluate the models by testing the linear probing accuracy on ImageNet~\citep{deng2009imagenet} with an added classification layer trained on top of the output representations. 
\reb{We use the YFCC100M dataset to simulate an infinite data regime, \ie using a single training run where no data point is repeated whereas we train iteratively using CC3M and COCO.}

\paragraph{Measuring memorization.} We follow~\citet{wang2024memorization} to approximate our \ours. Since training a separate pair of models for every data point whose memorization we aim to measure would be computationally intractable, we measure memorization of multiple data points at the same time. Therefore, we divide the original training set in four subsets: (1) $S_S$, data points that both model $f$ and $g$ were trained on, (2) $S_C$, data points used only for training $f$, (3) $S_I$, data points used only for training $g$, and (4) $S_E$, external "test" data points that none of the models was trained on.
Note that $|S_C|=|S_I|$, such that $f$ and $g$ have the same number of training data points in total. 
For our experiments, following a similar approach to~\citet{wang2024memorization}, we want to strike a balance when choosing the size of $S_C$. If the size is too large, then $f$ and $g$ might differ too much and not yield a strong memorization signal, but if it is too small, we would only have a memorization signal for too few data points.
Concretely, for COCO and CC3M, we set $|S_S|=65000$ and $|S_C|=|S_I|=|S_E|=5000$. Memorization is reported as an average over all data points in $S_C$ for model $f$, or per individual data point in $S_C$.
% we  80,000 samples are randomly extracted from the CC3M and COCO datasets respectively as the train data. We divide the training data set $S$ into four disjoint partitions and use 81.25\% of the train data, \ie 65000 samples as shared training samples $S_S$ between model $f$ and $g$. The next 6.25\% of samples, \ie 5000 are used as candidates $S_C$ to evaluate memorization. We add those to the training data of $f$ only, and another 6.25\%, \ie 5000 samples, are used as an independent set $S_I$, on which we only train $g$. We also use the remaining 6.25\%, \ie 5000 samples that neither $f$ nor $g$ are trained on as extra set $S_E$.
% We measure our \ours on the candidates $S_C$ and report their average memorization scores as an aggregate metric.

\paragraph{Generating captions and images.}
For generating additional captions for the training images, we rely on GPT-3.5-turbo. For each input image, we provide the representation produced by our trained OpenCLIP model and ask GPT to generate five new captions.
Generated sample captions are presented in \Cref{fig:image_text_sample}. 
To generate additional images for the COCO dataset, we use Stable Diffusion v1.5 to generate five new images, one corresponding to each of the five per-image captions in the COCO dataset. Sample generated images are presented in \Cref{fig:text_image_sample}.

\begin{figure}[t]
    \centering
        % Subfigure with the image
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{image/4_set_hist.pdf} % Replace with your image file
        \caption{Memorization scores across data subsets.}
        \label{fig:memorization_subsets}
    \end{subfigure}
    %\hfill
    % Subfigure with the table
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \tiny
        \begin{tabular}{cccc}
\toprule
 &  Clean $S_C$ & Poisoned $S_C$ \\
\midrule
Clean Model &  0.438 & N/A\\
Poisoned Model &  0.440 & 0.586\\
%Poisoned Model (Mis-captioned)   \\
\bottomrule
\end{tabular}    %         \begin{tabular}{cccc}
    % \toprule
    %  & Clean Model & Poisoned Model (Clean) & Poisoned Model (Mislabeled) \\
    % \midrule
    %   Top 10\% Most Memorized  & 0.678 & 0.671& 0.707\\
    %   $S_C$   & 0.438 & 0.440&0.586\\
    %   \bottomrule
    % \end{tabular}
    \caption{\vspace{3em}Average \ours scores.}
    \label{tab:memorization_poisoning}
    \end{subfigure}
    \caption{\textbf{Memorization with \ours}. We train a CLIP model on COCO using standard image cropping and no text augmentations. (a) We present memorization scores according to \ours per data subset. The significantly higher scores for $S_C$ compared to $S_S$ indicate that $f$ memorizes $S_C$. (b)~We also study how inserting training samples with imprecise or incorrect captions ("mis-captioned") affects memorization. We refer to the model trained with correct captions as \textbf{Clean Model}, and the model trained with $S_C$ containing 4500 standard canaries (\textbf{Clean}) and 500 mis-captioned (\textbf{Mis-captioned}) as \textbf{Poisoned Model}.    
    We report \ours over the different subsets of candidates. We observe that the mis-captioned samples experience a significantly higher memorization while the memorization of the clean data points is (almost) not affected.}
    % We first measure \ours scores for a ``clean'' model trained on correctly labeled data, reporting the average for the top 10\% most memorized samples and the overall average for all 5000 candidates in $S_C$. \textbf{(Clean Model)}. Next, we assign incorrect captions to 500 data points in $S_C$ and retrain the model. For this \textit{poisoned} model, we measure \ours scores for both the remaining 4500 clean (\ie correctly labeled) samples and the 500 mislabeled samples. We report the results for the top 10\% most memorized clean data, consisting of 450 samples in \textbf{Poisoned Model (Clean)}, and mislabeled data, consisting of 50 samples in \textbf{Poisoned Model (Mislabeled)}. Additionally, we provide the overall average for both clean and mislabeled data (4500 and 500 data points, respectively). \textcolor{red}{We observe that... @Wenhao, in this figure, we have to make Figure (a) and fill the values into Table (b), maybe some data points can already be obtained from your Figure 6 in the report (standard training, so no custom batching).} $S_C$, $S_I$, $S_E$, and $S_S$.}
    % \label{fig:memorization_insights}
\end{figure}
\subsection{Studying Memorization using \ours}
We first set out to analyze the general memorization in CLIP in order to identify which data points are memorized.
To do this, we quantify \ours over the different training subsets. Our results are presented in \Cref{fig:memorization_subsets}. 
%highlight that our \ours reports sensible memorization. \adam{you should not use words like "sensible"}
In particular, we observe that \ours for $S_C$, the data points only used to train model $f$, is significantly higher than for $S_S$, the data points shared between the two models. Memorization for $S_S$ is comparable to that for $S_E$, \ie the external data not seen during training, indicating that $f$ does not memorize these samples. The data in $S_I$ causes negative \ours scores, indicating that this data is memorized by $g$, not by $f$. This is the expected behavior according to the definition of our metric.
\reb{In \Cref{app:modelsize}, we additionally highlight that memorization increases with model size, \ie CLIP based on ViT-Large has a higher overall memorization with an average of $0.457$ while CLIP based on ViT-Base only reaches $0.438$ on average.}

Additionally, we analyze individual data points according to their reported CLIPMem.
We give examples of highly memorized data points in CLIP in \Cref{fig:examples} and more highly vs. little memorized samples in Figures~\ref{fig:examples_memorized_5_caption_least},\ref{fig:examples_memorized_5_caption_most},\ref{fig:examples_memorized_1_caption_least}, and \ref{fig:examples_memorized_1_caption_most}
% \Cref{fig:examples_memorized_5_caption_least}, \Cref{fig:examples_memorized_5_caption_most}, \Cref{fig:examples_memorized_1_caption_least}, and \Cref{fig:examples_memorized_1_caption_most} 
in \Cref{app:examples}. 
Overall, the samples with high \ours, \eg in \Cref{fig:examples} seem to be difficult examples and examples with imprecise or incorrect captions whereas the samples with low \ours are simpler and (potentially consequently) more precisely captioned.
\reb{In \Cref{app:onerun}, we show that these findings also hold when we operate in the \textit{infinite data regime}, \ie when we perform only a single training run where no data point is repeated.}

Motivated by this insight and by observations from supervised learning where it was shown that models can memorize random labels~\citep{zhang2016understanding} and where mislabeled data experiences highest memorization~\citep{feldman2020does}, we test if the same effect can also be observed in CLIP. 
Therefore, we "poison" our CLIP's training data by randomly shuffling the captions among 500 of the 5000 candidate data points in $S_C$.
Thereby, these 500 data points are "mis-captioned". We train a model based on this data and see that the mis-captioned examples experience significantly higher memorization (\ours of 0.586) compared to the "clean" data points  (\ours of 0.440). Even though CLIP trains using a contrastive training objective, the memorization of clean data points is not significantly affected by training the model with the mis-captioned examples, as we can see by their \ours that is 0.438 on the clean model and 0.440 on the poisoned model.




\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss.pdf}
        \caption{SSLMem (Img).}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss_text_1.pdf}
        \caption{SSLMem (Text).}
    \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/clipmem_sc_ss.pdf}
        \caption{\ours.}
    \end{subfigure}
    % \begin{subfigure}[b]{0.24\textwidth}
    %     \centering
    %    \includegraphics[width=\textwidth]{image/ssl_sslnaive_clip.pdf}
    %     \caption{All.}
    % \end{subfigure}
    \caption{\textbf{Measuring memorization on individual modalities is not able to extract a strong signal.} (a)--(b) We measure SSLMem~\citep{wang2024memorization} on the individual encoders of our CLIP model trained on COCO. (c) Our \ours extracts a stronger memorization signal by using both modalities in CLIP jointly. 
    %(d) We compare all scores, including the naive adaptation of SSLMem to CLIP where we sum up the SSLMem for each sample over both modalities. 
    %In neither case, strong memorization can be measured for $S_C$ with SSLMem. In contrast, our \ours reports a strong signal for memorization.
    }
    \label{fig:ssl_vs_clip}
\end{figure}
\subsection{Measuring Memorization in one Modality does not Yield a Strong Signal}
\label{sub:sslmem_not_for_clip}
To understand how important it is to take both modalities into account in our definition of \ours, we set out to evaluate whether existing practical methods to measure memorization over uni-modal encoders~\citep{wang2024memorization} yield a sufficiently strong memorization signal in CLIP.
Therefore, we apply their SSLMem to the individual encoder parts of CLIP. Since SSLMem relies on augmentations of the encoder input, we use image crops, like during CLIP training for the vision encoder, and the 5 COCO captions as augmentations for the text, like in~\citep{fan2023}. 
% We also combine, \ie sum up, the signal from the text and image encoder as a naive baseline on how one could implement a memorization metric in CLIP. 
Our results in \Cref{fig:ssl_vs_clip} 
highlight that SSLMem and its naive adaptation to CLIP fail to yield a strong signal for memorization. In particular, there is a high overlap in scores between the non-memorized samples from $S_S$, and candidate examples for memorization $S_C$. Additionally, the highest reported memorization scores for $S_C$ go up to around 0.65 (for SSLMem on the vision encoder) and 0.73 (for SSLMem on the text encoder). 
In contrast, our new \ours is able to get a distinct signal for the candidates $S_C$ with respect to $S_S$ and reports a much higher memorization of 0.91. Thereby, our \ours prevents under-reporting the actual memorization in CLIP.

% Furthermore, we present the overlap in the number of samples reported as memorized by SSLMem on the different modalities with the highest 500 memorized samples reported by CLIPMem, see for example \Cref{fig:examples}.
% In total SSLMem on the vision encoder identifies 105/500 samples that are highly memorized according to CLIPMem, whereas SSLMem identifies 143/500 samples when applied to the text encoder.
% The overlap between the 500 samples reported as highly memorized by SSLMem on the text and on the image encoder is 389/500. This again indicates that using only SSLMem on an individual modality is insufficient. 
% \adam{However, it even leads to different samples being selected by CLIPMem vs SSLMem while the overlap between the selected data points by SSLMem for vision and text encoder is quite substantial.}
% \textcolor{red}{This indicates that...}

\subsection{Memorization between Modalities}
Our results in \Cref{fig:ssl_vs_clip} indicate that memorization is higher in CLIP's text encoder than in the image encoder (the average SSLMem on $S_C$ in the text encoder is $0.209$ vs. $0.168$ in the image encoder).
To provide further insights into how memorization behaves between the modalities in CLIP, we first analyze the use of augmentations.
We compare five cases: (1) no additional augmentations beyond the baseline (image cropping), \reb{(2) generating one image using a diffusion model for a given original caption,} 
(3) generating five variations of each image using a diffusion model and randomly selecting one for each training iteration while keeping the caption fixed, (4) using the original image but randomly selecting one of the five COCO captions for each training iteration, and (5) randomly pairing each of the five generated images with one of the five COCO captions.

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-0.3cm}
\tiny
\addtolength{\tabcolsep}{0pt}
    \centering
        \caption{Impact of augmentations.}
        \vspace{-0.1cm}
   \scalebox{0.9}{\begin{tabular}{ccc}
    \toprule
    Case & \ours & Lin. Prob. Acc. (ImageNet)  \\
    \midrule
      1 Image, 1 Caption & 0.438 & 63.11\% $\pm$ 0.91\%\\
      %5 Images (generated), 1 Caption & 0.477& 59.92\% $\pm$ 1.04\%\\
      \reb{1 Image (generated), 1 Caption} & \reb{0.428} & \reb{63.97\% $\pm$ 0.79\%} \\
      \reb{5 Images (generated), 1 Caption} & \reb{0.424} & \reb{64.60\% $\pm$ 0.82\%} \\
      1 Image, 5 Captions & 0.423 & 64.88\% $\pm$ 0.83\%\\
      5 Images (generated), 5 Captions & 0.417& 64.79\% $\pm$ 0.99\%\\
      %\begin{tabular}[c]{@{}c@{}} 5 Images (generated), 1 Caption\\ (6000 mis-captioned sample removed)\end{tabular}&0.424&64.60\% $\pm$ 0.82\%\\
      \bottomrule
    \end{tabular}}
    \label{tab:augmentations}
\vspace{-0.4cm}
\addtolength{\tabcolsep}{0pt}
\end{wraptable}
As shown in \Cref{fig:examples_memorized_5_caption_most}, there is quite a variability in the COCO captions for the same sample. Hence, some images might not fit well with the chosen training caption. This imprecise captioning can cause an increase in memorization.
We observe that the effect is mitigated when using the 5 images with the 5 captions (5th case, see \Cref{tab:augmentations}). 
This phenomenon results most likely from the increased number of possible image-text pairs (25), such that individual incorrect or imprecise pairs are not seen so often during training. \reb{For the third case, \ie row three in \Cref{tab:augmentations}, we generate five images with a diffusion model based on all five captions per image from the COCO dataset. However, as we only use the first caption during training, this would introduce many mis-captioned images which significantly lowers performance and increases memorization. To avoid this problem, we removed 6000 mis-captioned samples.}

Our results in \Cref{tab:augmentations} highlight that augmenting text during training reduces memorization and increases performance more than augmenting images. However, applying augmentations of both text and images strikes the right balance between the reduction in memorization and the increase in performance. In fact, applying both augmentations reduces memorization most significantly. 
Overall, these results indicate that memorization in CLIP's is tightly coupled to the captions assigned to the training images with imprecise captions having a destructive effect on CLIP performance and memorization.

% In contrast, memorization increases when using 5 diffusion model-generated images with one caption. 
% This is quite surprising since in the latter case, the linear probing accuracy drops by more than 4\% and one would usually expect that with lower model performance, there should also be lower memorization~\citep{feldman2020does}.
% The drop in ImageNet linear probing accuracy stems most likely from a distribution shift introduced by using diffusion generated instead of natural images.
% Regarding the increase in memorization, we hypothesize that it is linked to image-caption mis-matches during training:
% while we generate the 5 images with the diffusion model based on all 5 COCO captions, we only use the first caption during training. 
% To avoid these problems, we implement extra experiments that remove 6000 mis-captioned samples from the images generated by the diffusion model. The results in the last 2 row of \Cref{tab:augmentations} align with the trend that augmenting either text or image during training reduces memorization significantly.
% \adam{The image is generated per caption, however, in the training of the CLIP model we only select a single caption.
% The fewer times we see the mis-match image-caption pair, the less we memorize it in the case 5 images and 5 captions. Also, in this setup, we train for the same number of iterations, but in the 5 images and 5 captions case, we train on more samples, which improves generalization and leads to higher linear probing accuracy.
% Potential change in the experiment for future: for the case 5 images 1 caption - take the embedding for the caption from CLIP and then noise it 5 times and give these 5 slightly different version to generate from Stable Diffusion.
% }
% We hypothesize that using multiple images for the same caption further enforces memorization due to CLIP's contrastive learning objective. 
% When the exact same caption is learned with multiple images, they will still be considered as negative pairs and pushed away, hence, creating a wider region in the embedding space for the particular sample, causing higher memorization. 
% The same does not happen in the \textit{5 caption, 1 image} case since the image is still augmented through the cropping, and hence not exactly the same with the same caption.\todo{@Adam, please review if the last sentences make sense. Actually, it's really hard to process them.}
%%%% THE FOLLOWING ARE INTERESING RESULTS. BUT I HAVE NO WAY TO EXPLAIN THEM.....
% We also analyze the overlap between the highest memorized samples between case 1 and case 2 and case 1 and case 3, by looking at what is the percentage of overlap within the data points that are in the 10\% highest memorized ones in the respective cases.
% The overlap in the 10\% highest memorized samples between case 1 and case 2 is 66 out of 500 (13.2\%), and the overlap between case 1 and case 3 is 107 out of 500 (21.4\%).
% \textcolor{red}{This indicates that...}
%We show further insights on memorization between modalities in \Cref{app:modality}.
%Finally, in \Cref{app:captions}, we show that captions generated by GPT3.5 have the same effect like the original COCO captions on memorization and linear probing accuracy (\Cref{tab:coco_cpt_train}), and that using each caption at an equal frequency during training improves downstream performance (\Cref{tab:multi_caption_training}).


 


\subsection{Relation to CLIP Memorization to (Self-)Supervised Memorization}\label{chpt:clip_ssl_sl}
We further provide insights on whether CLIP's memorization behavior is more alike to the one of supervised learning or SSL.
This question is highly interesting since the captions in CLIP can be considered as a form of labels, like in supervised learning, whereas the contrastive training objective on the dataset resembles more SSL.
We perform two experiments to gain a better understanding of the memorization behavior of CLIP with respect to supervised learning and SSL.

First, we compare an SSL vision encoder pair $f$ and $g$ with the same architecture as CLIP's vision encoder but trained from scratch on COCO using DINO, 
%\todo{@Wenhao, check if correct: in the appendix you talk about DINO, so I am wondering if this SSL encoder is not trained with DINO?}\wenhao{This is trained with DINO, I correct this},
\ie standard SSL training. We train $f$ and $g$ using the same candidates as the pair of CLIP models in our previous experiments.
Then, we use the SSLMem metric from~\citet{wang2024memorization} to quantify memorization in the CLIP vision encoder and the SSL encoder, respectively.
The CLIP vision encoder has a significantly lower SSLMem than the SSL encoder (0.209 vs. 0.279). 
%\todo{these below were the numbers first, but this would not fit with \Cref{fig:ssl_vs_clip}(a), hence I took the number from there!}
%using the same candidates as for the CLIP models. Then, we used the SSLMem metric~\cite{wang2024memorization} to detect the highest memorized samples in both encoders (the 0.168 and 0.279 for the CLIP and the SSL encoder respectively. 
Hence, CLIP vision encoders experience lower SSL memorization than SSL trained encoders.
%since this metric focuses primarily on atypical examples whereas the memorization in CLIP also stems from mislabeling (caused by, \eg incorrect or imprecise captions).\todo{@Franzi: check the last sentence.}
%\todo{@Adam, help me think of this finding. One might argue that CLIP is just not made for this metric... SSLMem might just be under-reporting on CLIP. Adam: here, we just have to state that with respect to the CLIP vision encoder and not the whole CLIP model.}
%This suggests that overall memorization is \textcolor{red}{higher/lower/same} for CLIP over SSL-trained models. Looking at the top 10\% memorized samples, we can report an average SSLMem of 0.421 and 0.798 for the CLIP and the SSL encoder respectively.
To further investigate the difference, we also report the overlap between the top 10\% memorized samples between the two models, measured according to SSLMem. With an overlap of only 47 out of 500 (9.4\%) samples, we find that CLIP memorizes significantly different samples than SSL encoders.
\citet{wang2024memorization} had performed a similar experiment on SSL vs. supervised learning and found that the two paradigms also lead to different samples being memorized. 
While this is, on the one hand, an effect of the different objective function, the difference between the memorized samples in CLIP and SSL is likely also closely connected to the additional captions that CLIP takes into account. While SSL-trained encoders can memorize atypical images, CLIP encoders can memorize typical images when they have an atypical, imprecise, or incorrect caption.

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.5cm}
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{image/unitmem_vit_clip_dino_supervised.pdf}}
\vspace{-0.2cm}
\caption{
\label{fig:unitmem}
\textbf{UnitMem metric: CLIP is between supervised and SSL models.
}
}
\end{center}
\vspace{-0.8cm}
\end{wrapfigure}
Additionally, we compare the memorization behavior of CLIP against supervised and SSL-trained models on the neuron-level.
Therefore, we train two additional ViT-Base models on COCO using supervised training and SSL training with DINO. Then, we apply the UnitMem metric~\citep{wang2024localizing} to measures how much individual neurons memorize individual samples from the training data. A high UnitMem suggests that neurons highly memorize individual data points instead of groups/classes of points. 
It had been shown that supervised learning causes neurons in lower layers to experience low UnitMem, \ie being responsible for learning joint groups of data points, while neurons in later layers highly memorize individual data points.
In contrast, for SSL, UnitMem was shown to remain relatively constant over layers with neurons in lower layers also being able to memorize individual data points. This difference was attributed to the different objective functions where supervised learning's cross entropy loss pulls together data points from the same class, whereas SSL's contrastive loss leads to individual data points being pushed away from each other~\citep{wang2024localizing}.
Our results in \Cref{fig:unitmem} highlight that CLIP, in terms of its memorization behavior, is between supervised learning and SSL. At the lower layers, it is much less selective than models trained with SSL, \ie it focuses on groups of data points rather than memorizing individual data points, similar to supervised learning.
%\todo{Write about the training of the SSL Model with COCO and the supervised model with COCO.}\wenhao{I am not sure why we need to write about the training of the SSL Model with COCO and the supervised model with COCO, this is already mentioned at beginning of chapter 4.1}
Yet, in later layers, CLIP becomes more selective than SSL, \ie it memorizes individual data points more in individual neurons, but still less than supervised learning which there has a very high average per-layer UnitMem.



% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \centering
%     \includegraphics[width=0.95\linewidth]{image/caption_num.pdf}
%     %\caption{\textbf{More captions lowers memorization and increases performance.}}
%     \caption{
%     Influence of number of captions.
%     }
%     \label{fig:num_captions}
%     \end{subfigure}
%         \hfill
%     \    \begin{subfigure}{0.49\columnwidth}
%     \centering
%     \includegraphics[width=0.95\linewidth]{image/sample_remove_random_clipmem.pdf}
%     %\caption{\textbf{Removing memorized samples increases performance.}}  
%     \caption{Removing memorized samples.}  
%     \label{fig:remove_samples}
%     \end{subfigure}
%     \caption{\textbf{Relationship between memorization and generalization.} (a) We train CLIP models based on the COCO dataset and analyze the relationship between linear probing accuracy and memorization, based on the number of captions used during training. With more captions, memorization decreases while accuracy increases. \textbf{(b)} We compare the effects of removing random samples versus removing the highest memorized ones according to \ours. Removing according to CLIPMem has a stronger influence on the linear probing accuracy than removing random data points.}
%     \label{fig:generalization}
% \end{figure}



\subsection{Mitigating Memorization while Maintaining Generalization}


%\textcolor{red}{@Franziska: Write here about the generalization, describe the peak in Figure 5(b) which results from us being able to remove first mis-labeled samples (improves utility) and then atypical samples (that are required for learning).}
The experiments from \Cref{tab:augmentations} suggest that using augmentations during training can improve generalization while also reducing memorization.
This is an unexpected synergy since for both supervised learning~\citep{feldman2020does} and SSL~\citep{wang2024memorization}, generalization was shown to decline when memorization decreases.
\begin{figure}[t]
\begin{subfigure}{0.48\columnwidth}
        \centering
    \includegraphics[width=0.95\linewidth]{image/caption_num.pdf}
    \caption{Different numbers of captions.}
    \label{fig:num_captions}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\columnwidth}
\centering
\includegraphics[width=0.9\textwidth]{image/noise_strength.pdf}
        \caption{Noising text embedding during training.}
    \label{fig:noising}
\end{subfigure}
    \caption{\textbf{Mitigating memorization in CLIP improves downstream generalization.} We train CLIP models with different "augmentations" in the textual domain. (a) We use multiple captions for the same image during training. (b) We directly noise the text embeddings during the training using Gaussian noise with a mean of 0 and different standard deviations \reb{(adding the Gaussian noise $\mathcal{N}(0,0.15)$ gives us the sweet spot with the smallest memorization and highest performance)}. Both strategies successfully reduce memorization while improving performance.}
    \label{fig:mitigations}
\end{figure}
% \begin{wrapfigure}{r}{0.4\textwidth}
% \vspace{-0.5cm}
% \begin{center}
% \centerline{\includegraphics[width=0.95\linewidth]{image/caption_num.pdf}}
% \vspace{-0.2cm}
% \caption{
% \label{fig:num_captions}
% \textbf{Memorization vs Generalization.}
% }
% \end{center}
% \vspace{-0.8cm}
% \end{wrapfigure}
To further study the impact of mitigating memorization in CLIP on downstream generalization, we explore two orthogonal strategies for "augmenting" the text modality during CLIP training, first in the input space and second directly in the embedding space. Additionally, we analyze the effect of removing memorized samples from training.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.48\columnwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{image/sample_remove_random_clipmem_new.pdf}
    %\caption{\textbf{Removing memorized samples increases performance.}}  
    \caption{CLIP trained on COCO.}  
    \label{fig:remove_samples_cooc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \centering
    \includegraphics[width=0.95\linewidth]{image/sample_remove_random_clipmem_cc3m_new.pdf}
    %\caption{\textbf{More captions lowers memorization and increases performance.}}
    \caption{CLIP trained on CC3M.}
    \label{fig:remove_samples_cc3m}
    \end{subfigure}
    \caption{
    \textbf{Removing memorized samples according to \ours has a stronger influence on the linear probing accuracy than removing random data points.} 
    Removing the mislabeled samples based on \ours improves the performance significantly, followed by a sharper drop when removing atypical samples.
    %We compare the effects of removing random samples versus removing the highest memorized ones according to \ours.
    \vspace{-0.5cm}
    }
    \label{fig:generalization}
\end{figure}


\textbf{Multiple captions.}
We vary the number of captions used during training and report fine-grained insights into the resulting memorization and downstream performance in \Cref{fig:num_captions}. 
Our results highlight the trend that the more captions are used during training, the lower memorization and the higher the linear probing accuracy.
%with more captions, memorization decreases while accuracy increases.
%the trend of utility improving while memorization drops is consistent: 
Our additional results in \Cref{tab:multi_caption_training} highlight also that choosing all captions equally often is beneficial for utility while keeping memorization roughly the same.
%This suggests that we should use multiple captions during CLIP training, similar to LaCLIP \citep{fan2023}, to benefit from higher utility and lower memorization. 
Since not in every dataset, multiple captions are available, we experiment with generating these captions with a language model. Our results in \Cref{tab:coco_gpt3_captions_train} where we train CLIP with captions generated by GPT3.5 show that the results both in terms of utility and memorization are extremely similar to the original captions, making this improved training strategy widely applicable. 
Our findings that modifying the text during training can reduce memorization align with the insights presented by~\citet{jayaraman2024}.
For datasets where only single captions are available, they proposed \textit{text randomization}, \ie masking out a fraction of tokens during training as a mitigation for their Déjà Vu memorization.
In contrast to our GPT3.5-generated captions, this masking, however, causes a drop in performance when mitigating memorization. We hypothesize that this is due to the higher distribution shift introduced by the masked tokens. %\todo{Here would be space for the noise experiment.}

\textbf{Noising the text embedding during training.} 
To overcome such shortcomings altogether and avoid any inherent distribution shifts, we propose to perform the "augmentations" directly in the embedding space.
More precisely, we experiment with an approach where, during training, before calculating the cosine similarity between text and image embeddings for the contrastive loss, we add small amounts of Gaussian noise to the text embeddings.
Our results in \Cref{fig:noising} \reb{and \Cref{tab:noising}} highlight that this strategy is highly effective in reducing memorization while improving downstream generalization. %, making it a highly practical mitigation strategy.

% \begin{table}[t]
%     \centering
%    \begin{tabular}{ccc}
%     \toprule
%     Noise & & (ImageNet) \\
%     \midrule
%       NULL & 0.438 & 63.11\% $\pm$ 0.91\%\\
%       0 / 0.01 & 0.435 & 63.36\% $\pm$ 0.88\%\\
%       0 / 0.05 & 0.428 & 64.02\% $\pm$ 1.12\%\\
%       0 / 0.10 & 0.421 & 64.95\% $\pm$ 0.96\%\\
%       \bottomrule
%     \end{tabular}
%     \caption{\textbf{Noising the text representation during training.} We add Gaussian noise to the text representation during training of CLIP on COCO and measure \our and ImageNet linear probing accuracy. Adding noise reduces memorization while improving generalization.}
%     \label{tab:text_augmentations}

% \end{table}


\textbf{Removing memorized samples.}
Finally, we investigate the effect of removing memorized samples to understand how it impacts downstream performance. We
perform an additional experiment where we first train a CLIP model, then identify the highest memorized training data points, remove them, and retrain on the remaining data points only. \reb{We compare this method to two baselines where we either randomly remove samples or filter out the samples with the lowest CLIP similarity between the training data points' two modalities.}
We showcase the effect on the downstream linear probing accuracy on ImageNet in \Cref{fig:generalization} with CLIP models trained on COCO and on the CCM3 dataset. For the COCO dataset,
when removing up to 100 most memorized data points, we first observe a sharp increase in downstream performance in comparison to removing random samples. 
Then, the downstream performance starts dropping significantly more when removing memorized instead of random samples, until between 400 and 800 removed samples, the cutoff point is reached where model performance is worse when removing according to highest memorization instead of randomly.
For the CC3M dataset, this cutoff occurs later, between 1600 and 3200 removed samples.
% \reb{Compared with the naive similarity used by CLIP model, \ours is more effective when noise samples are not fully removed.}
\reb{While the CLIP similarity also manages to increase performance through removal, it is not as effective as \ours, highlighting the value of considering memorization as a lens to identify noisy samples.}
This finding is significantly different than for supervised learning and SSL, where the removal of highly memorized samples \textit{constantly} harms performance more than the removal of random samples~\citep{feldman2020does,wang2024memorization}.
We hypothesize that the effect observed in CLIP might result from the distinction between "mis-captioned" and atypical samples, where the former harm generalization while the latter help the model learn from smaller sub-populations~\citep{feldman2020does}. We empirically support this hypothesis in \Cref{app:generalization}.
%, where we find that mis-captioned samples harm utility, in particular through the contrastive training, whereas atypical samples help the model learn from smaller sub-populations
% are required to learn smaller sub-populations and thereby support training
The finding that CLIP generalization can be improved by identifying inaccurately captioned data points using our \ours and removing them from training is of high practical impact, given that state-of-the-art CLIP models are usually trained on large, uncurated datasets sourced from the internet with no guarantees regarding the correctness of the text-image pairs.
%Our results highlight that this practice not only exposes imprecise or incorrect data pairs to more memorization, often recognized as a cause for increased privacy leakage~\citep{carlini2019secret, carlini2021extracting, carlini2022privacy,song2017machine,liu2021encodermi}, but that it also negatively affects model performance. 
% To test this hypothesis, we run an experiment where we train a supervised model (ViT-Base) on CIFAR10 but flip the label of 200 candidate data points in $S_C$ prior to training. Then, we use our setup to approximate memorization in supervised learning as defined by~\citet{feldman2020does} and report generalization while removing most memorized vs. random samples.
% Our results in \Cref{fig:sample_remove_poison_cifar_imagnet_slt} highlight a similar trend of a first increase in generalization when removing most memorized samples. The peak is at roughly 200, \ie the number of inserted mislabeled samples. Afterwards, the removal of memorized samples harms generalization more as we can see a sharper drop than when removing random samples.
% For more details on the experiment, see \Cref{app:generalization}.
Overall, our results suggest that \ours can help reduce memorization in CLIP while improving downstream generalization.
%\todo{@Adam, can you review this last sentence? Is it strong enough to end the empirical part of the paper on it?}
%\textcolor{red}{I still need to go through this in detail.}


% \begin{enumerate}
%     \item How CLIP memorization relates to SL and SSL memorization. 1
%     \item Which modality (text or vision) is more responsible for memorization? 2
%     \item Which samples are memorized and why? 3
%     \item How does the CLIP memorization impact generalization? 4
%     \item How can we prevent memorization and maybe, at the same time, get better CLIP models? 5
% \end{enumerate}














    

% \begin{figure}[h]
%     \centering
%    \includegraphics[width=0.5\columnwidth]{image/caption_num.pdf}
%     \caption{\textbf{More captions used per image increase performance while reducing memorization.}
%     We present results for linear probing accuracy for ImageNet vs the memorization score from our \ClipMem when using different numbers of captions during training. Each caption is used to calculate the \ClipMem, and the final result is obtained by averaging the scores.}
%    \label{fig:clipmem_acc_caption}
% \end{figure}

% \begin{figure}[h]
%     \centering
%    \includegraphics[width=1.0\columnwidth]{}
%     \caption{\textbf{} We remove the memorized samples computed based on the text and image embeddings. The CLIP model is retrained from scratch and we report the linear probing accuracy using ImageNet.\adam{remove the grouping.}
%     }
%    \label{fig:clipmem_sample_remove}
% \end{figure}