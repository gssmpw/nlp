\newpage
\section{Appendix}


%\todo{to be finished!}
\subsection{Extended Background}
\label{app:deja_vu_comparison}
\paragraph{Déjà Vu Memorization in CLIP.}
The Déjà Vu memorization framework \citep{jayaraman2024} is the only existing other work that attempts to quantify memorization in vision-language models. It uses the text embedding of a training image caption to retrieve relevant images from a public dataset of images. It then measures the fraction of ground-truth objects from the original image that are present in the retrieved images. If the training pair is memorized, retrieved images have a higher overlap in ground truth objects, beyond the simple correlation.
While valuable, several aspects warrant further consideration for broader applicability of the framework. First, its focus on object-level memorization ignores non-object information like spatial relationships or visual patterns that can also influence memorization~\citep{feldman2020does,wang2024memorization}.
To perform object retrieval, the framework also relies on object detection and annotation tools, which may introduce variability based on the accuracy and robustness of these tools.
Additionally, the assumption that public datasets with similar distributions to the training data are readily available may not always hold, necessitating alternative approaches. 
Moreover, the framework does not analyze why certain images are memorized limiting detailed analysis. 
%The framework uses a guess-and-check approach to measure memorization, which may not fully leverage CLIP's multimodal nature or its contrastive learning objective. A more direct measurement could take advantage of these features to enhance evaluation.
Finally, while Déjà Vu must address the challenge of distinguishing between memorization and spurious correlations, \ours avoids this by directly assessing memorization on the output representations of the model.
One notable difference between the results of our approach and Déjà Vu's is that their findings show that their mitigation strategies can reduce memorization, but at the cost of decreased model utility. \ours, in contrast, does not observe trade-offs between memorization and performance.

    % \item \textbf{Lack of layer-wise insights.} Does not provide insights into how memorization occurs across different model layers.


% \paragraph{Shortcomings of Déjà Vu Memorization}
% \begin{enumerate}
%     \item \textbf{Limited to object-level memorization.} Focuses on object retrieval, neglecting other forms of memorization that pertain to non-object information like spatial relationships or visual patterns.
%     \item \textbf{Dependence on external models.} Relies on (inaccurate) object annotation tools, constraining the evaluation by the accuracy and robustness of these tools.
%     \item \textbf{Assumption of public dataset.} Assumes the availability of public datasets with similar distributions to the training data, which may not always be accessible. 
%     \item \textbf{No identification of memorized samples.} Does not specify which images are memorized or why.
%     \item \textbf{Lack of layer-wise insights.} Does not provide insights into how memorization occurs across different model layers.
%     \item \textbf{Indirect measurement of memorization.} Relies on a guess-and-check approach for measuring memorization that does not account for CLIP's multimodal nature or contrastive learning objective.
%     \item \textbf{Challenge with correlation vs. memorization.} Needs to distinguish between true memorization and spurious correlations, complicating the evaluation.
%     \item \textbf{Performance trade-offs.} Implements mitigation strategies that reduce memorization, but at the cost of decreased model utility.
% \end{enumerate}



\subsection{Extended Experimental Setup}
\label{app:setup}

\paragraph{General Setup.} 
All the experiments in the paper are done on a server with 4 A100 (80 GB) GPUs and a work station with one RTX 4090 GPU(24 GB).
We detail the setup for our model training, both CLIP and SSL (relying on DINO) in \Cref{tab:settings}.

\addtolength{\tabcolsep}{-2.5 pt}
\begin{table}[h]
\caption{\textbf{Experimental Setup.} We provide details on our setup for encoder training and evaluation.}        
          \label{tab:settings}
    \centering
    \scriptsize 
\begin{tabular}{cccccccc}
\toprule
                       & \multicolumn{3}{c}{Model Training}                                       &  & \multicolumn{3}{c}{Linear Probing}                      \\ \cmidrule{2-4} \cmidrule{6-8} 
                           &  CLIP  & DINO & Supervised ViT          &   & CLIP    &DINO     & Supervised ViT           \\ \midrule
Training Epoch          &  100     & 300          & 100 &   &   45    & 45          &45       \\
Warm-up Epoch          & 5  & 30        & 5& & 5          & 5          &5     \\
Batch Size             &  128  & 1024          &128  &   & 4096    & 4096 &4096      \\
Optimizer              &     Adam              & AdamW             & Adam      &  & LARS         & LARS & LARS        \\
Learning rate          &     1.2e-3               & 2e-3                 & 1e-3     &   & 1.6          & 1.6  & 1.6        \\
Learning rate Schedule &         Cos. Decay        & Cos. Decay & Cos. Decay & & Cos. Decay & Cos. Decay & Cos. Decay\\ \bottomrule 
\end{tabular}
\end{table}
\addtolength{\tabcolsep}{2.5 pt}


\paragraph{Experimental Setup for SSLMem.}
To experimentally evaluate memorization using the SSLMem framework~\citep{wang2024memorization}, the training dataset $S$ is split into four sets: \textit{shared set} ($S_S$) used for training both encoders $f$ and $g$; \textit{candidate set} ($S_C$) used only for training encoder $f$; \textit{independent set} ($S_I$) data used only for training encoder $g$; and an additional \textit{extra set} ($S_I$) from the test set not used for training either $f$ or $g$. For training encoders, encoder $f$ is trained on $S_S \cup S_C$, while encoder $g$ is trained on $S_S \cup S_I$. The alignment losses $\lalign(f, x)$ and $\lalign(g, x)$ are computed for both encoders, and the memorization score $m(x)$ for each data point is derived as the difference between these alignment losses, normalized to a range between $-1$ and $1$. A score of $0$ indicates no memorization, $+1$ indicates the strongest memorization by $f$, and $-1$ indicates the strongest memorization by $g$.

\paragraph{Normalization on \ours.}
For improved interpretability, we normalize our \ours scores to a range of $[-1,1]$. A memorization score of $0$ indicates no memorization, $+1$ indicates the strongest memorization on CLIP model f, and $-1$ indicates the strongest memorization on CLIP model g. We find the normalized CLIPMem score for a dataset using the following process:
For each image-text pair $(I,T)$, we first calculate the CLIPMem score as the difference in alignment scores between two CLIP models $f$ and $g$.
Once CLIPMem scores are computed for all data points, we normalize them by dividing each score by the range, which is the difference between the maximum and minimum scores in the dataset.
Finally, we report the normalized \ours score for a dataset as the average of these normalized values.
% Therefore, we calculate the differences in alignment loss per data sample $(I,T)$ over both CLIP encoder pairs $f$ and $g$.
% Afterwards, we normalize these differences by dividing them by the range (largest minus smallest difference),

% Finally, we report the \ours score as the average of the normalized scores over all data points.
% in $S_C$. 

\subsection{Additional Experiments}




% \subsubsection{Studying Memorization per Modality}
% \label{app:modality}
% \todo{@Adam, I find it very hard to justify the experiment below. Can you please have a look? The reviewers might just be confused.}
% We also perform an experiment where we remove memorized samples, retrain the model, and report the resulting linear probing accuracy.
% The more the linear probing accuracy is affected by the removal of a memorized sample, the more important the samples.
% Rather than taking the overall highest memorized samples and removing them, once the model is trained, we first cluster the training data in the embedding space into 50 clusters, once according to the text and once according to the image embeddings.
% Then, we remove from each cluster the highest memorized data point.
% This semantically corresponds to finding the highest memorized data points per-class in supervised learning (with the difference that we have no class information, and therefore, have to revert to a default number of clusters, in our case 50).
% Our results in \Cref{fig:modality} highlight that removing the highest memorized samples according to the text embeddings' grouping and then re-training boosts downstream performance more than removing based on image embeddings.\todo{@Adam, here, for sure something is missing, an explanation or so. But I have none. In the worst case, we need to remove this paragraph.} 
% Hence, the overall observed trend indicates that memorization in CLIP depends more on the text than on the image.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.55\linewidth]{image/sample_remove_text_image.pdf}
%     \caption{\textbf{Modality-based removal.} We train a CLIP models with standard image cropping on the COCO dataset. 
%     After training, we cluster training samples according to their similarity based on CLIP's image or text embeddings into 50 clusters, remove the highest memorized sample from each cluster, and retrain with the remaining points. The linear probing accuracy on ImageNet suggests that removing based on text embeddings' clustering memorization has higher influence on model performance.}
%     \label{fig:modality}
% \end{figure}

\subsubsection{Memorization vs. Generalization in CLIP}
\label{app:generalization}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.55\linewidth]{image/sample_remove_random_clipmem_5_caption.pdf}
    \caption{\textbf{Removing memorized samples.} We show the effect on downstream performance in terms of ImageNet linear probing accuracy and \ours for a CLIP model trained on COCO using 5 text captions instead of 1, like done in \Cref{fig:generalization}. We observe the same trend, with the difference that the peak is at roughly 500 removed samples rather than 100. This is likely due to the increase in captions (by factor 5) that causes increase in mis-captioned samples.}
    \label{fig:image_text_sample5}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.55\linewidth]{image/sample_remove_poison_cifar_imagnet_sl.pdf}
    \caption{\textbf{Removing memorized samples in supervised learning.} We train a ViT-tiny on CIFAR10~\citep{krizhevsky2009learning} using supervised learning. We use our evaluation setup with $S_C$, $S_S$, $S_I$, and $S_E$ to approximate the memorization metric from~\citet{feldman2020does}. We use 5000 samples in $S_C$, but before training, we flip the labels of 200 samples. We calculate memorization over all samples in $S_C$ and test the linear probing accuracy with ImageNet resized to 32*32 on the representations output before the original classification layer.}
    \label{fig:sample_remove_poison_cifar_imagnet_slt}
\end{figure}
\textbf{Extending evaluation.}
In \Cref{fig:image_text_sample5}, we perform the same experiment as in \Cref{fig:generalization}, but on a CLIP model trained with 5 captions instead of 1.
We observe the same trend, with the difference that the peak is at roughly 500 removed samples rather than 100. This is likely due to the increase in captions (by factor 5) that causes increase in mis-captioned samples.

\textbf{Verifying the hypothesis on memorizing mis-captioned samples through supervised learning.}
We repeat the same experiment in the supervised learning setup to understand where the increase and then decrease in linear probing accuracy stems from. To test our hypothesis that it stems from "mis-captioned" samples, we "poison" our supervised model by flipping the labels of 200 data points before training. 
Then, we approximate the memorization metric from~\citet{feldman2020does} in our setup and remove highly memorized vs. random data points.
In the same vein as in \Cref{app:generalization}, we first observe an increase in linear probing accuracy when removing memorized samples (instead of random samples). The peak is at roughly 200 data points, \ie the number of deliberately mislabeled samples.
Until the cutoff point at roughly 3200 examples, linear probing accuracy is still higher when removing most memorized rather than random samples, which might suggest that there are other outliers or inherently mislabeled samples whose removal improves model performance. After the cutoff, we observe the behavior as observed in prior work~\citep{wang2024memorization, feldman2020does} that reducing memorization harms generalization more than reducing random data points from training.

%\textcolor{red}{TBD: refer to \Cref{tab:reduce_memorization}}

%\textcolor{red}{We need to describe \Cref{fig:image_text_sample5}}








\subsubsection{The Effect of Captions}
\label{app:captions}

\begin{table}[t]
    \centering
           \caption{\textbf{Using different/multiple captions during training.} 
           We evaluate \ours how memorization on different data subsets and linear probing accuracy on ImageNet differ when using 1 caption (baseline), 5 COCO captions, one chosen at random at every round (random), and 5 COCO captions, but all chosen equally often, \ie 20 out of 100 training epochs (balanced).
           We observe that increasing the number of captions reduces highest memorization. Yet, only when we balance the usage of caption, also model performance increases.}        
          \label{tab:multi_caption_training}
    \scriptsize
    \begin{tabular}{cccc}
    \toprule
&baseline & random & balanced \\
    \midrule
Avg. \ours (Top 10 samples)&0.792&0.788 &0.790 \\
Avg. \ours (Top 20\%) &0.552&0.531&0.540\\
%Avg. \ours ($S_C$) &0.438&0.409&0.423\\
Linear Probing Acc. &63.11\% $\pm$ 0.91\%&62.44\% $\pm$ 1.18\%& 64.88\% $\pm$ 0.83\%\\
      \bottomrule     
      \end{tabular}
\end{table}
\begin{table}[t]
          \caption{\textbf{The CLIPMem and linear probing accuracy of model trained with original coco captions and captions generated by GPT3.5.} For 'Single Caption', only one caption is used during training. For 'Five Caption', all five caption are used equally during training (every caption trained for 20 epoch out of 100). The linear probing accuracy is tested on ImageNet}        
          \label{tab:coco_cpt_train}
    \centering
    \scriptsize
    \begin{tabular}{cccccc}
    \toprule 
    & \multicolumn{2}{c}{COCO} && \multicolumn{2}{c}{GPT3.5}\\
    &Single Caption &Five Caption&&Single Caption &Five Caption\\
    \midrule
    CLIPMem &0.438&0.423& &0.430&0.411\\
    LP. Acc. &63.11\% $\pm$ 0.91\%&64.88\% $\pm$ 0.83\%&&63.09\% $\pm$ 1.12\%&64.47 $\pm$ 0.72\%\\
      \bottomrule     
      \end{tabular}
\end{table}
In \Cref{tab:multi_caption_training}, we show that using more captions during training reduces memorization and that by using each caption at the same frequency over the training epochs, we can additionally improve model performance.
Additionally, we show that captions generated by GPT3.5 have the same effect as the original COCO captions on memorization and linear probing accuracy in \Cref{tab:coco_cpt_train}.

\subsection{The Effect of Model Size}
\label{app:modelsize}
In \Cref{tab:model_size}, we present how the model size affects the memorization level of CLIP models. Both models are trained using the same dataset and settings. We observe that with more parameters (larger model size), encoders have higher memorization capacity. This aligns with findings from previous research~\citep{wang2024memorization, feldman2020does, meehan2023ssl}.



\begin{table}[t]
          \caption{\textbf{
           CLIPMem and linear probing accuracy of models with different sizes.} The models are trained using identical settings and the same subset of the COCO dataset. Linear probing accuracy is tested on the ImageNet dataset as the downstream task.}        
          % The CLIPMem and linear probing accuracy of model with different size} We train the models with same settings as well as same subset of COCO dataset. The linear probing accuracy is tested on ImageNet dataset as the downstream task.
          \label{tab:model_size}
    \centering
    \scriptsize
    \begin{tabular}{ccc}
    \toprule 
     Model& \ours&Lin. Prob. Acc. (ImageNet) \\
     \midrule
    ViT-base (Baseline in main paper) & 0.438 & 63.11\% $\pm$ 0.91\%\\
    ViT-large & 0.457 &  67.04\% $\pm$ 1.05\%\\
      \bottomrule     
      \end{tabular}
\end{table}

\subsection{Verification of infinite data regimes}
\label{app:onerun}
To evaluate CLIPMem over infinite data regimes (\ie using a single training run where no data point is repeated), we use a subset $D$ (containing 7050000 samples) of YFCC100M dataset~\citep{thomee2016yfcc100m} to train another pair of ViT-Base models for only 1 epoch. Following our definition of \ours, we further divide $D$ into $S_S$ with 6950000 samples, $S_C$ with 50000 samples, and $S_I$ with 50000 samples. The reason we use 7M (6950000+50000) samples to train either model $f$ or model $g$ is to make sure the newly trained model has the same number of training samples as the model trained with K-epoch runs (70000 samples/epoch * 100 epoch). The results in \Cref{tab:YFCC7M} show that the model trained with infinite data regimes has higher linear probing accuracy on ImageNet as a downstream task and lower memorization scores, as measured by \ours. This aligns with the fact that duplicated data points increase the memorization level and make the model over-fit, hence reducing the generalization~\citep{wang2024memorization, feldman2020does}. The results in \Cref{fig:most_YFCC} show that the most memorized samples according to \ours in the model trained with infinite data regimes are also samples with imprecise or incorrect captions. This aligns with our statements in \Cref{chpt:clip_ssl_sl}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/most_memorized.pdf}
    \caption{\textbf{Top 10 memorized samples according to \ours in the model trained under infinite data regimes on YFCC100M.} The model is trained for one epoch, \ie seeing each training data point exactly once. Even in this setup, the most memorized samples are still the ones with imprecise or incorrect captions.}
    \label{fig:most_YFCC}
\end{figure}

\begin{table}[t]
\centering
\scriptsize
\caption{\textbf{Evaluation of CLIPMem under infinite data regimes, \ie seeing every data point only once during training vs training with 100 epochs.} We observe that both setups reach comparable downstream accuracy and memorization.}
\label{tab:YFCC7M}
\begin{tabular}{ccc}
      \toprule 
      Model & \ours & Lin. Prob. Acc. (ImageNet) \\
      \midrule
     ViT-Base (YFCC 7M, 1 epoch) &0.425&64.83\% $\pm$ 1.04\%\\
     ViT-Base (COCO 70K, 100 epochs)&0.438&63.11\% $\pm$ 0.91\%\\
      \bottomrule
\end{tabular}
\end{table}


\subsection{Evaluation on BLIP}
\label{app:BLIP}
To verify the effectiveness of \ours over other similar multi-modal models, we train a BLIP model on COCO dataset following the same settings as the baseline model in the main paper. We present the results for \ours on BLIP over all four data subsets in \Cref{fig:4_set_hist_blip}, which is in agreement with the results of the BLIP model in \Cref{fig:memorization_subsets}. We also present the results for UnitMem in \Cref{fig:unitmem}, which is also very similar to the results of CLIP models

\begin{figure}[t]
    \centering
    \includegraphics[width=0.55\linewidth]{image/4_set_hist_blip.pdf}
    \caption{\textbf{Memorization scores across data subsets on BLIP models} We train a BLIP model on COCO standard image cropping and no text augmentation. We present the results for \ours over all 4 data subsets, which is in agreement with the results of the CLIP model in \Cref{fig:memorization_subsets}}
    \label{fig:4_set_hist_blip}
\end{figure}

\subsection{Memorization distribution During training}
We present the distributions of neurons with highest UnitMem during training in \Cref{fig:clipmem_neuron_train}. These results highly consistently indicate that in the early stages of training, neuronal memory occurs mainly in the lower layer of the clip model, while in the middle and later stages of training, neuronal memory is more concentrated in the later layer of the model.
\begin{figure*}
    \centering
    
    \begin{subfigure}[b]{0.7\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/clipmem_neuron_train_1.pdf}
        \caption[]%
        {\textbf{Top 1\% neurons}}    
        \label{fig:clipmem_neuron_train_1}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.7\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/clipmem_neuron_train_3.pdf}
        \caption[]%
        {\textbf{Top 3\% neurons}}    
        \label{fig:clipmem_neuron_train_3}
    \end{subfigure}
    
        \begin{subfigure}[b]{0.7\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/clipmem_neuron_train_5.pdf}
        \caption[]%
        {\textbf{Top 5\% neurons}}    
        \label{fig:clipmem_neuron_train_5}
        
    \end{subfigure}
    \caption{\textbf{Distribution of top 1\%, 3\%, and 5\% neurons with highest UnitMem during training.} We train a CLIP model on COCO standard image cropping and no text augmentation following the settings of baseline model in main paper. We record the neurons with top 1\%, 3\%, and 5\% of highest UnitMem during training (every epoch). } 
    \label{fig:clipmem_neuron_train}
\end{figure*}

\subsection{Human vs Machine Generated Captions}
\label{app:GPTcaptions}

For each image in the COCO dataset, we use GPT 3.5 (specifically, gpt-3.5-turbo) to generate 5 captions (from scratch). We use the following instruction in the OpenAI API: %\todo{@Wenhao, can you update it with your code of "prompt = f"Generate a detailed descri..."}
\begin{verbatim}
def generate_description_for_image(image_caption, clip_features):
    prompt = f"Here is an image with the caption: '{image_caption}'. "
    prompt += f"Based on this caption and the visual features
    represented by this embedding '{clip_features}', 
    please generate a new detailed description."
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful 
            assistant that generates captions for images."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']
\end{verbatim}
We present the obtained captions in \Cref{fig:image_text_sample}.
\begin{table}[t]
          \caption{\textbf{The machine generated captions provide similar performance to the original human-generated captions.} We report the \ours and linear probing accuracy of model trained with original COCO captions and captions generated by GPT 3.5. For the 'Single Caption', only a single caption is used during training. For 'Five Captions', all five captions are used equally during training (every caption trained for 20 epochs out of 100). The linear probing accuracy is tested on the ImageNet dataset as the downstream task.}        
          \label{tab:coco_gpt3_captions_train}
    \centering
    \scriptsize
    \begin{tabular}{cccccc}
    \toprule 
    & \multicolumn{2}{c}{COCO} && \multicolumn{2}{c}{GPT 3.5}\\
    &Single Caption &Five Captions&&Single Caption &Five Captions\\
    \midrule
    \ours &0.438&0.423& &0.430&0.411\\
    Linear Probing Accuracy (ImageNet) &63.11\% $\pm$ 0.91\%&64.88\% $\pm$ 0.83\%&&63.09\% $\pm$ 1.12\%&64.47 $\pm$ 0.72\%\\
      \bottomrule     
      \end{tabular}
\end{table}
\begin{figure}[t]
    \centering
\begin{subfigure}{0.45\columnwidth}
   \includegraphics[width=1.0\columnwidth]{image/coco_cosine_old}
    \caption{\textbf{COCO (Average: 0.798)}}
   \label{fig:coco_cosine}
\end{subfigure}
\begin{subfigure}{0.45\columnwidth}
     \includegraphics[width=1.0\columnwidth]{image/gpt_cosine_new.pdf}
   \caption{\textbf{GPT3.5 (Average: 0.851)}}
   \label{fig:GPT3.5_cosine}
\end{subfigure}
\caption{\textbf{Pairwise cosine similarity of 5 captions from COCO and generated by GPT3.5.}}
     \label{fig:cosine_single}
\end{figure}
In \Cref{fig:GPT3.5_cosine}, we analyze the pairwise cosine similarity in the original COCO and the GPT3.5 generated captions. We find that the GPT3.5 generated captions are slightly more uniform than the original COCO captions, reflecting in a higher pairwise cosine similarity.



\begin{table}[ht]
\scriptsize
\centering
   \begin{tabular}{ccc}
    \toprule
    Noise & \ours & Lin. Prob. Acc. (ImageNet) \\
    \midrule
      None & 0.438 & 63.11\% $\pm$ 0.91\%\\
      $\mathcal{N}(0.01)$  & 0.435 & 63.36\% $\pm$ 0.88\%\\
      $\mathcal{N}(0.05)$ & 0.428 & 64.02\% $\pm$ 1.12\%\\
      $\mathcal{N}(0.10)$ & 0.421 & 64.95\% $\pm$ 0.96\%\\
      \boldsymbol{$\mathcal{N}(0.15)$}  & \boldsymbol{$0.417$} &  \boldsymbol{$65.34\% \ \pm \  0.84\%$}\\
      $\mathcal{N}(0.20)$ & 0.422 & 64.83\% $\pm$ 0.92\%\\
      $\mathcal{N}(0.25)$ & 0.436 & 63.28\% $\pm$ 0.79\%\\
      $\mathcal{N}(0.30)$ & 0.447 & 61.50\% $\pm$ 0.86\%\\
      $\mathcal{N}(0.50)$ & 0.491 & 57.04\% $\pm$ 1.11\%\\
      $\mathcal{N}(0.75)$ & 0.501 & 52.28\% $\pm$ 0.98\%\\
      $\mathcal{N}(1.00)$ & 0.504 & 51.92\% $\pm$ 1.03\%\\
      \bottomrule
    \end{tabular}
        \caption{\textbf{Noising text embedding during training.} We present the impact of adding noise to the text embedding during training for the ViT-base trained on COCO.}
    \label{tab:noising}
\end{table}


\subsection{Examples for Memorized Samples}
\label{app:examples}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/10_least_5_caption.pdf}
    \caption{\textbf{The 10 samples with lowest \ours in the CLIP model trained with all 5 captions.} We can see that these samples contain clear concepts and precise captions.}
    \label{fig:examples_memorized_5_caption_least}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/10_most_5_caption.pdf}
    \caption{\textbf{The 10 samples with highest \ours in the CLIP model trained with all 5 captions.} We can see that these samples contain atypical, difficult samples with imprecise or incorrect captions.}
    \label{fig:examples_memorized_5_caption_most}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/10_least_1_caption.pdf}
    \caption{\textbf{The 10 samples with lowest \ours in the CLIP model trained with 1 caption.} We can see that these samples contain clear concepts and precise captions.}
    \label{fig:examples_memorized_1_caption_least}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/10_most_1_caption.pdf}
    \caption{\textbf{The 10 samples with highest \ours in the CLIP model trained with 1 caption.} We can see that these samples contain atypical, difficult samples with imprecise or incorrect captions.}
    \label{fig:examples_memorized_1_caption_most}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/text_image_sample.pdf}
    \caption{\textbf{Samples of images generated by Stable Diffusion.} We present the generated images based on the COCO captions.}
    \label{fig:text_image_sample}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/image_text_sample.pdf}
    \caption{\textbf{Sample captions generated by GPT3.5.} We present the generated captions and the original image and captions from COCO.}
    \label{fig:image_text_sample}
\end{figure}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss_old.pdf}
        \caption[]{{SSLMem Image Encoder (1 caption)}}
        \label{fig:sslmem_sc_ss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss_5.pdf}
        \caption[]%
        {{SSLMem Image Encoder (5 captions)}}    
        \label{fig:sslmem_sc_ss_5}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss_text_1_old.pdf}
        \caption[]%
        {{SSLMem Text Encoder (1 caption)}}    
        \label{fig:sslmem_sc_ss_text_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/sslmem_sc_ss_text_5.pdf}
        \caption[]%
        {{SSLMem Text Encoder (5 captions)}}    
        \label{fig:sslmem_sc_ss_text_5}
    \end{subfigure}
        \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/clipmem_sc_ss_old.pdf}
        \caption[]%
        {{CLIPMem}}    
        \label{fig:clipmem_sc_ss}
    \end{subfigure}
    \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/ssl_sslnaive_clip.pdf}
        \caption[]%
        {{SSLMem, Naive Sum of SSLMem and CLIPMem}}    
        \label{fig:ssl_sslnaive_clip}
        
    \end{subfigure}
    \caption{\textbf{Evaluation of SSLMem and \ours on a CLIP model trained on COCO.}
    Extended version of \Cref{fig:ssl_vs_clip} where we also include SSLMem calculated on encoders trained with 5 captions instead of 1.
    The trends in both cases are the same. SSLMem for the CLIP Models trained with the 5 captions is slightly higher since SSLMem uses the captions as augmentations for the calculation of the memorization. Overall, our \ours reports the strongest memorization signal for CLIP.
    %Results are obtained on ViT-Base, trained with COCO
    } 
    \label{fig:mean and std of nets}
\end{figure*}




% \paragraph{General Setup.} 
% All the experiments in the paper are done on a server with 4 A100 (80 GB) GPUs and a work station with one RTX 4090 GPU(24 GB).
% We detail the setup for our model training, both CLIP and SSL (relying on DINO) in \Cref{tab:settings}.
