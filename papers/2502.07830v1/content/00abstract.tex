\begin{abstract}
Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective.
To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIPâ€™s memorization behavior falls between the supervised and self-supervised paradigms, with "mis-captioned" samples exhibiting highest levels of memorization. 
Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. 
Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility---something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.
%, some of our proposed mitigations for CLIP can reduce memorization while improving downstream utility.\todo{This needs to be reworked: our CLIPMem has the practical application to identify "miscaptioned" samples, such that we can remove them from the training, and then get better results. This is particularly important given that CLIP is trained on large amounts of uncurated data from the internet, and one cannot review all these image pairs. With out metric, it becomes possible.
%aybe also include the risk of exposure for these data points that otherwise arise.
%}
% Multi-modal models, such as CLIP, exhibit strong performance in aligning visual and textual representations, thereby achieving remarkable performance in tasks like image retrieval and zero-shot classification. 
% While the models have a strong generalization ability, it is not fully understood how the models leverage their training data to achieve this.
% One factor that is often linked to a model's generalization ability is memorization. For uni-modal models, both in supervised and self-supervised, it has been shown that memorization is required for generalization.
% Yet, it is unclear how the findings will translate to CLIP because in CLIP captions as supervisory signals, somewhat akin to traditional labels, but also employs self-supervised contrastive learning. Hence, CLIP is in between both paradigms.
% To bridge this gap, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. 
% Our results show that CLIP's memorization behavior indeed falls between supervised and self-supervised paradigms. Notably, "mis-captioned" samples exhibit high levels of memorization.
% Additionally, we find that the the text encoder has a higher impact on memorization than the image encoder.
% Based on these findings, we find some effective mitigation strategies for memorization in CLIP that focus more on the text domain to maintain model performance while reducing memorization.
% Indeed, unlike in traditional supervised or self-supervised learning, where reducing memorization often reduces utility, we empirically find that some mitigations in CLIP not only reduce memorization but at the same time improve downstream utility.
\end{abstract}
