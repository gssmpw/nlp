\section{Introduction}

Multi-modal models, such as CLIP~\citep{radford2021}, have demonstrated strong performance in representation learning.
By aligning visual and textual representations, these models achieve state-of-the-art results in tasks like image retrieval~\citep{baldrati2022conditioned,baldrati2022effective}, visual question answering~\citep{pan2023retrieving,song2022clip}, and zero-shot classification~\citep{radford2021,ali2023clip,wang2023improving,zhang2022tip}. 
Despite these successes, the mechanisms by which multi-modal models leverage their training data to achieve good generalization remain underexplored. 

In uni-modal setups, both supervised~\citep{feldman2020does,feldman2020neural} and self-supervised~\citep{wang2024memorization}, machine learning models have shown that their ability to \textit{memorize} their training data is essential for generalization. 
It was indicated that, in supervised learning, memorization typically occurs for mislabeled samples, outliers~\citep{bartlett2020benign,feldman2020does,feldman2020neural}, or data points that were seen towards the end of training~\citep{jagielski2022measuring}, while in self-supervised learning, high memorization is experienced particularly for atypical data points~\citep{wang2024memorization}. 
However, it is unclear how these findings extend to models like CLIP which entail elements from both supervised learning (through captions as supervisory signals) and self-supervised learning (through contrastive loss functions).

Existing definitions of memorization offer limited applicability to CLIP and therefore cannot fully address the gap in understanding.
% can, hence, not close the gap in understanding:
The standard definition from supervised learning~\citep{feldman2020does} relies on one-dimensional labels and the model's ability to produce confidence scores for these labels, whereas CLIP outputs high-dimensional representations. While the SSLMem metric~\citep{wang2024memorization}, developed for self-supervised vision models, could, in principle, be applied to CLIP's vision encoder outputs, it neglects the text modality, which is a critical component of CLIP. Additionally, measuring memorization in only one modality, or treating the modalities separately, risks diluting the signal and under-reporting memorization. Our experimental results, as shown in \Cref{sub:sslmem_not_for_clip}, confirm this concern. Therefore, new definitions of memorization tailored to CLIP's multi-modal nature are necessary.
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{image/10_most_1_caption.pdf}
        \caption[]{{\small Most Memorized: CLIPMem $>$ 0.89}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{image/10_least_1_caption.pdf}
        \caption[]%
        {{\small Least Memorized: CLIPMem $\approx$ 0.0}}    
    \end{subfigure}
    % \vskip\baselineskip
    % \begin{subfigure}[b]{0.475\textwidth}   
    %     \centering 
    %     \includegraphics[width=\textwidth]{Example-Image}
    %     \caption[]%
    %     {{\small Network 3}}    
    %     \label{fig:mean and std of net34}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.475\textwidth}   
    %     \centering 
    %     \includegraphics[width=\textwidth]{Example-Image}
    %     \caption[]%
    %     {{\small Network 4}}    
    %     \label{fig:mean and std of net44}
    % \end{subfigure}
    \caption{\textbf{Examples of data with different levels of memorization.} Higher memorization scores indicate stronger memorization. 
    We observe that atypical or distorted images, as well as those with incorrect or imprecise captions, experience higher memorization compared to standard samples and easy-to-label images with accurate captions.
    % We observe that atypical or distorted images and images with incorrect or imprecise captions experience higher memorization compared to more standard samples and easy-to-label samples with precise captions. 
    Results are obtained on OpenCLIP~\citep{ilharco_gabriel_2021_5143773}, with encoders based on the ViT-Base architecture trained on the COCO dataset.} 
        \label{fig:examples}
        %\vspace{-0.8cm}
\end{figure}

The only existing empirical work on quantifying memorization in CLIP models~\citep{jayaraman2024} focuses on Déjà Vu memorization~\citep{meehan2023ssl}, a specific type of memorization.
The success of their method relies on the accuracy of the integrated object detection method and on the availability of an additional public dataset from the same distribution as CLIP's training data, limiting practical applicability.
To overcome this limitation, we propose \textit{\ours} that measures memorization directly on CLIP's output representations.
Specifically, it compares the alignment---\ie the similarity between representations---of a given image-text pair in a CLIP model trained with the pair, to the alignment in a CLIP model trained on the same data but without the pair.

% Additionally, we focus on \textit{understanding} memorization rather than quantifying it. 
% %which, in CLIP, can be measured only with respect to an additional public dataset from the same distribution as CLIP's training data and fine-grained object detection methods. Moreover, the work is limited to \textit{quantifying} memorization.
% %---limiting its practical applicability. Moreover, while the work \textit{quantifies} Déjà Vu memorization, it does not offer detailed insights into which specific data points are memorized, why they are memorized, and how this relates to generalization. 
% %In contrast to their work, our focus is on \textit{understanding} memorization in CLIP by 
% We use this to identify which properties of the data and the two modalities contribute to CLIP memorization and on leveraging these insights to achieve \textit{better model utility while mitigating memorization}. To this end, we propose \textit{\ours} that directly measures memorization on the representations produced by CLIP's vision and text encoders.
% Specifically, \ours measures memorization by comparing the alignment, \ie the similarity between representations, of a given image-text pair in a CLIP model trained with this pair to the alignment in a CLIP model trained without this pair but on the same data otherwise.


In our empirical study of memorization in CLIP using \ours, we uncover several key findings. First, examples with incorrect or imprecise captions ("mis-captioned" examples) exhibit the highest levels of memorization, followed by atypical examples, as illustrated in \Cref{fig:examples}.
Second, removing these samples from training yields significant improvements in CLIP's generalization abilities.
These findings are particularly noteworthy, given that state-of-the-art CLIP models are usually trained on large, uncurated datasets sourced from the internet with no guarantees regarding the correctness of the text-image pairs.
Our results highlight that this practice not only exposes imprecise or incorrect data pairs to more memorization, often recognized as a cause for increased privacy leakage~\citep{carlini2019secret, carlini2021extracting, carlini2022privacy,song2017machine,liu2021encodermi}, but that it also negatively affects model performance. 
%By identifying highly memorized samples, our \ours can, hence, support a more private and performant deployment of CLIP.\todo{@Adam, is that last sentence too strong?}
Furthermore, by disentangling CLIP's two modalities, we are able to dissect how memorization manifests within each.
Surprisingly, we find that memorization does not affect both modalities alike, with memorization occurring more in the text modality than in the vision modality.
% even though the training objective is symmetric.\todo{@Adam, is that correct?}
% In fact, our results highlight that memorization occurs more in the text modality than in the vision modality. 
Building on these insights, we propose several strategies to reduce memorization while simultaneously improving generalization---a result that has not been observed in traditional supervised or self-supervised learning, where any reduction of memorization causes decreases in performance.
% which, at the same time, improve generalization.
% Such a result has not been observed in traditional supervised or self-supervised learning, where any reduction of memorization causes decreases in performance. 
Finally, at a deeper level, our analysis of the model internals, following~\citet{wang2024localizing}, shows that CLIP's memorization behavior sits between that of supervised and self-supervised learning. Specifically, neurons in early layers are responsible for groups of data points (\eg classes), similar to models trained using supervised learning, while neurons in later layers memorize individual data points, as seen in self-supervised learning.%\todo{cite our localization paper.}
% Performing an empirical evaluatin of memorization in CLIP according to our \ours, we find that
% -  examples with incorrect ("mis-captioned") or imprecise captions experience highest memorization, and then atypical examples . We show this effect in \Cref{fig:examples}.
% - memorization happens more in the text than in the vision modality
% - by including more captions into training, when only a few of them are mislabeled and the rest is correct, we can reduce memorization and at the same time improve generalization, something that has not been possible for supervised or self-supervised learning.
% - looking at the model internals, we see that the memorization behavior of CLIP is exactly in between supervised and self-supervised learning: in particular, neurons in early layers are responsible for groups (classes) of data points, same like for supervised learning, while neurons in later layers are responsible for individual data points

% \franzi{@Adam, do you think, we need an additional paragraph here on the mitigations we have? We actually only have multi-caption, so far, so probably not so important?}\adam{We also can mitigate the memorization if we remove the most memorized (probably mislabeled) samples.}

In summary, we make the following contributions:
\begin{itemize}
    \item We propose \ours, a metric to measure memorization in multi-modal vision language models.
    \item Through extensive evaluation, we identify that "mis-captioned" and "atypical" data points experience the highest memorization, and that the text encoder is more responsible for memorization than the image encoder.
    \item Based on our insights, we propose and evaluate multiple strategies to mitigate memorization in CLIP. We show that in CLIP, contrary to traditional supervised and self-supervised learning, a reduction of memorization does not need to imply a decrease in performance.
\end{itemize}

