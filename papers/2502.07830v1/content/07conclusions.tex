\section{Conclusion}
We presented \ours, a formal measure to capture memorization in multi-modal models, such as CLIP.
By not only quantifying memorization but also identifying \textit{which} data points are memorized and \textit{why}, we provide deeper insights into the underlying mechanisms of CLIP.
Our findings highlight that memorization behavior of CLIP models falls between that of supervised and self-supervised models.
In particular, CLIP highly memorizes data points with incorrect and imprecise captions, much like supervised models memorize mislabeled samples, but it also memorizes atypical examples.
%Furthermore, the definition allows us to study which data points are memorized between the modalities and how the modalities contribute to this memorization.
Furthermore, we find that memorization in CLIP happens mainly within the text encoder, which motivates instantiating mitigation strategies there.
By doing so, we can not only \textit{reduce memorization} in CLIP but also \textit{improve} downstream generalization, a result that challenges the typical trade-offs seen in both supervised and self-supervised learning.

% By doing so, in contrast to supervised and self-supervised learning, we can \textit{reduce memorization} in CLIP while at the same time \textit{improving} downstream generalization.

%While our metric focuses on CLIP-like vision-language models, it can, in principle, be extended to more modalities. By doing so, identifying where memorization happens between more than two modalities, and how downstream generalization will be affected presents an interesting avenue for future work.

% -- can we say something about even more modalities? would CLIPMem transfer there?
% -- can we link to other downstream tasks like retrieval?
% -- can we show for more models?
% -- 