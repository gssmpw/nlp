\section{Defining Memorization over Multi-Modal Encoders}
\label{sec:clipmem}



\subsection{Problem Setup}

Consider a single image-text pair $(I, T)$ from a dataset $S$ and two CLIP models: a model $f$ and a reference model $g$, trained on dataset $S$ and $S' = S \setminus \{(I, T)\}$, respectively.
% Following the setup of SSLMem, $f$ is trained on a dataset $S_S \cup S_C$, where $S_S$ is a shared set of image-text pairs and $S_C$ is a candidate set seen only by $f$.
% Encoder $g$ is trained on $S_S \cup S_I$, where $S_I$ is an independent seen only by $g$.
We aim to quantify the memorization of $(I,T)$ in $f$, trained on this data point, by leveraging model $g$ not trained on the data point but otherwise on the same data, in a \textit{leave-one-out} style of defining memorization~\citep{feldman2020does}.
We denote the image encoder in CLIP as $f_{\text{img}}:\text{Image}\to \mathbb{R}^d$ and the text encoder as $f_{\text{txt}}:\text{Text}\to \mathbb{R}^d$. For the image-text pair ($I$, $T$), we denote with $f_{\text{img}}(I)$ the output representation of $f$'s image encoder on image $I$ and with $f_{\text{txt}}(T)$ the output representation of $f$'s text encoder on text $T$. To evaluate the \textit{alignment} between the image and text representations, \ie to quantify how similar the two representations are, we use cosine similarity $sim(f_{\text{img}}(I), f_{\text{txt}}(T))$, as defined in the original CLIP paper \citep{radford2021}.

% Next, calculate the cosine similarity between the image representation and its corresponding text representation for each encoder: $CS^{f} = sim(RI^{f}, RT^{f})$, $CS^{g} = sim(RI^{g}, RT^{g})$.
% These similarities represent how closely each encoder considers the correct image-text pairs.

\subsection{Alignment with Contrastive Objective}
During training, the contrastive objective in CLIP maximizes the cosine similarity for correct image-text pairs while minimizing the cosine similarity for all the other $N-1$ incorrect pairs in any given training mini-batch with $N$ training samples. This means that for a given image $I$ and text $T$, the training objective pulls $f_{\text{img}}(I)$ and $f_{\text{txt}}(T)$ closer together in the latent space, while pushing $f_{\text{img}}(I)$ away from the representations of all other $N-1$ unrelated texts, and $f_{\text{txt}}(T)$ away from all other images. Hence, the intuition is that the quality of alignment in $f$, unlike in uni-modal self-supervised learning~\citep{wang2024memorization}, depends not only on the model's ability to create well-aligned text and image representations for a given text-image pair, but also on its ability to create distant representations for the $N-1$ other representations.

%%% Potentially bring somewhere else: did not fit here since we are not yet talking about memorization.
% To account for this contrastive objective when defining and measuring memorization, in our framework, we introduce an unrelated text set for comparison.
% For each image-text pair, we prepare an unrelated text set by selecting $127$ unrelated texts from the dataset shared by both encoders $f$ and $g$.
% This number is chosen based on a batch size of $128$, where one image-text pair is used as the target (\ie true) text, while the remaining $127$ are used as unrelated texts. 
% These unrelated texts are merged into a fixed set $\widehat{T_{test}}$.
% % Use $\widehat{T_{test}}$ as input to text encoders $ET_f$ and $ET_g$ to obtain representation sets $RT_{1\sim127}^{f}$ and $RT_{1\sim127}^{g}$.
% % Next, calculate the cosine similarity between the image representation and each unrelated text representation for each encoder, which can be represented as a set of similarities: $\widehat{CS_{test}^{f}}$ between $RI^{f}$ and $RT_{1\sim127}^{f}$, $\widehat{CS_{test}^{g}}$ between $RI^{g}$ and $RT_{1\sim127}^{g}$.
% By using these sets of unrelated pairs, we can evaluate whether the model can differentiate the correct image-text pairs from unrelated ones.

To formalize this intuition into a metric that quantifies the alignment of $f$ on the image-text pair $(I, T)$, we define $\widehat{T_{test}}$ as a set of $N-1$ randomly chosen testing samples that were not used in training $f$ or $g$. %as $\widehat{T_{test}}$.
Furthermore, when applicable, we denote random augmentations of the training data---\eg text augmentations in versions like LaCLIP~\citep{fan2023}---as $T'  \sim \text{Aug}(T)$ for texts and $I'  \sim \text{Aug}(I)$ for images.
Then, we define the alignment score of $f$ on $(I, T)$ as 
%To quantify this differentiation, we define the alignment score $\mathcal{A}_{\text{align}}(f, I, T)$ that measures how well the model aligns the true image-text pairs compared to unrelated ones. For encoder $f$ on a given image-text pair $(I, T)$, the alignment score is given by
% \begin{equation}
% \mathcal{A}_{\text{align}}(f, I, T) = \mathbb{E}_{T'  \sim \text{Aug}(T)}[\text{sim}(f_{\text{img}}(I), f_{\text{txt}}(T'))] - \mathbb{E}_{t \in \widehat{T_{test}}} \left[ \text{sim}(f_{\text{img}}(I), f_{\text{txt}}(t)) \right]\text{,}
% \end{equation}\todo{@Wenhao, please verify if this is correct. Do you ever use augmentations?}
% \wenhao{I could understand that the equation (4) will works when augmentations are applied to both image and text. For image side, this could be reasonable when more complex augmentation set are used. Currently, I think no augmentation for image is needed if only cropping is used during training.}

\begin{equation}
\begin{split}
\mathcal{A}_{\text{align}}(f, I, T) = & \underset{(I',T')  \sim \text{Aug}(I,T)}{\mathbb{E}} \left[\text{sim}(f_{\text{img}}(I'), f_{\text{txt}}(T'))\right] \\
& - \underset{(\_,t) \in \widehat{T_{test}}}{\mathbb{E}} \left[ \text{sim}(f_{\text{img}}(I), f_{\text{txt}}(t)) \right] - \underset{(i,\_) \in \widehat{T_{test}}}{\mathbb{E}} \left[ \text{sim}(f_{\text{img}}(i), f_{\text{txt}}(T)) \right] \text{,}
\end{split}
\end{equation}

% \begin{equation}
% \begin{split}
% \mathcal{A}_{\text{align}}(f, I, T) =& \mathbb{E}_{(I',T')  \sim \text{Aug}(I,T)}[\text{sim}(f_{\text{img}}(I'), f_{\text{txt}}(T'))]\\&- \mathbb{E}_{(\_,t) \in \widehat{T_{test}}} \left[ \text{sim}(f_{\text{img}}(I), f_{\text{txt}}(t)) \right] - \mathbb{E}_{(i,\_) \in \widehat{T_{test}}} \left[ \text{sim}(f_{\text{img}}(i), f_{\text{txt}}(T)) \right] \text{,}
% \end{split}
% \end{equation}
%Note that we use alignment \textit{score} rather than alignment \textit{loss}, unlike SSLMem. This is because CLIP aims to maximize alignment between correct image-text pairs using a contrastive objective, whereas SSLMem minimizes alignment loss as part of the training objective.
where high scores indicate a better alignment of $f$ on $(I,T)$. In case no text augmentations are applied, as in standard CLIP training, the first term is calculated only over $T$.


\subsection{Defining Memorization in CLIP}

Given our definition of alignment scores, we can define our \ours in a similar vein to the definition of memorization in supervised learning~\citep{feldman2020does}, in the leave-one-out style. 
%We define memorization in CLIP by following the SSLMem framework \citep{wang2024memorization}, which is based on the \textit{leave-one-out} definition of memorization in SL, proposed by ~\citet{feldman2020does}. This definition measures the impact of removing a single data point from training on the model's prediction for that particular data point. SSLMem extends this definition by measuring the impact of removing a data point on the encoder's alignment of augmented views for that data point. We apply a similar approach to CLIP, adapting it to the multi-modal context to evaluate the impact of removing an image-text pair. 
Given the image-text pair $(I, T)$ from dataset $S$ and two CLIP models, $f$ and $g$, trained on dataset $S$ and $S' = S \setminus \{(I, T)\}$, respectively, we define \ours as 
\begin{equation}
\text{\ours}(I, T) = \mathcal{A}_{\text{align}}(f, I, T) - \mathcal{A}_{\text{align}}(g, I, T)\text{.}
\end{equation}
If a model $f$ has a significantly higher alignment score than model $g$ on $(I, T)$, this means that $f$ memorizes this data point.
Note that taking the difference between $f$ and $g$ is crucial to get a solid estimate of memorization.
This is because without "context", a high or low alignment score of $f$ does not express much information. 
The alignment of $f$ can be high without memorizing $(I, T)$, for example, if $(I,T)$ is a simple (but not memorized) training example. In this case, the reference model $g$ will also have a high score, such that the difference is again small.
Thanks to this design of our \ours, it will then correctly report low memorization. 

%A positive value of $m(I, T)$ indicates that encoder $f$, which has seen the pair $(I, T)$ during training, aligns the image and text more closely, suggesting stronger memorization compared to encoder $g$.

% Finally, to compute \ours $m_{\text{CLIPMem}}$ on the entire dataset $S$, we calculate the memorization score $m(I, T)$ for all data points in $S_C$, and then find the range of these scores as 
% Normalize each memorization score to derive the final \ours score: 

% \begin{equation}
% m_{\text{CLIPMem}} = \frac{\sum\limits_{(I, T) \in S} m(I, T)}{\max\limits_{(I, T) \in S} \ m(I, T) - \min\limits_{(I, T) \in S} \ m(I, T)}
% \end{equation}
