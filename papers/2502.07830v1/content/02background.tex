\section{Background and Related Work}



\textbf{CLIP}.
Contrastive Language-Image Pretraining (CLIP)~\citep{radford2021} trains multi-modal encoders to map text-image pairs into a shared latent space with semantically equal representations.
%(\citet{radford2021}).
The core of CLIP is a two-encoder architecture with an image encoder $\fimg$ and a text encoder $\ftxt$ that are trained to maximize the similarity between the image and text features for correct text-image pairs, while minimizing the similarity for incorrect pairs. This is achieved using a contrastive loss function $\mathcal{L}$ defined as:
\begin{align*}
\mathcal{L} = - \frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(\fimg(x_i), \ftxt(y_i)) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(\fimg(x_i), \ftxt(y_j)) / \tau)},
\end{align*}
where $\text{sim}(\cdot, \cdot)$ is the cosine similarity,  $\tau$  is the temperature parameter, and $N$ is the batch size.
This training makes CLIP versatile across various downstream tasks, including image classification, retrieval, captioning, and object recognition.
%It trains vision and text encoders by matching images and texts through its contrastive learning objective; it maximizes the cosine similarity between the correct image-text pairs while minimizing the similarity for incorrect pairs.
%The model projects both images and texts into a shared latent space, where related image-text pairs are pulled together and unrelated pairs are pulled apart.
%CLIP generalizes well to downstream tasks by training on vast amounts of labeled images scraped from the web, making it effective and cost-efficient for vision-language tasks.
There are different versions of CLIP.
%OpenCLIP is an open-source version that supports different architectures and datasets while still achieving performance competitive with the original CLIP model (\citet{cherti2023}).
The popular Language augmented CLIP (LaCLIP)~\citep{fan2023} augments original CLIP by introducing text augmentations during training in addition to the image augmentations (crops) performed in the original CLIP training to reduce overfitting.
We study the impact of this practice on memorization and find it to be a suitable mitigation method.

%builds upon CLIP by introducing text augmentations through language rewrites generated by large language models (\citet{fan2023}).
%This variation addresses the asymmetry in CLIP training, where only images are augmented but texts remain the same, improving model performance by mitigating text overfitting (\citet{fan2023}).
%In our experiments, we leverage both OpenCLIP and LaCLIP.

\textbf{Memorization.}
Memorization refers to a model's tendency to store specific details of individual training examples, rather than generalizing patterns across the dataset~\citep{zhang2016understanding,arpit2017closer,chatterjee2018learning,feldman2020does}. This becomes problematic when models memorize sensitive data, as it has been shown to increase privacy risks~\citep{carlini2019secret, carlini2021extracting, carlini2022privacy,song2017machine}. 
To date, memorization has been studied within \textit{single modalities} for supervised and self-supervised learning.
In \textbf{supervised learning}, it has been shown that models tend to memorize \textit{mislabeled}~\citep{feldman2020does}, \textit{difficult}, or \textit{atypical} examples~\citep{arpit2017closer,sadrtdinov2021memorization}, and that this memorization improves generalization, especially on long-tailed data~\citep{feldman2020does,feldman2020neural}.
Similar findings have been observed in \textbf{self-supervised learning} (SSL) in the vision domain~\citep{wang2024memorization}, where atypical samples experience high memorization, and a reduction of memorization in SSL encoders leads to decreased performance in
% a reduction of memorization in the SSL encoders reduces performance of
various downstream tasks, such as classification, depth-estimation, and segmentation.
A connection between memorization and generalization has also been observed in the language domain~\citep{antoniades2024generalization,tirumala2022memorization}.
In contrast to our work, these papers consider single-modality models. How those insights transfer to multi-modal models remains unclear.
%\franzi{Here, we can think about whether it is to our advantage or disadvantage to mention memorization in diffusion models.}\todo{@Adam, what do you think?}\adam{I'd say that above is enough.}
%\wenhao{What I am thinking is that the image generate by diffusion models are quite different with the original coco images. Although we use the same normalization parameters (from ImageNet) to normalize them, the distribution of dataset is still not proper enough to directly make comparison}
%\franzi{No, I mean work on mitigating memorization in diffusion models, like the one that I shared in the channel. They also do text masking. These are exactly things that are what we also do.}

\textbf{Memorization in self-supervised learning.}
Our \ours builds on concepts from the SSLMem metric introduced by \citet{wang2024memorization}. This metric measures the memorization of an individual data point $x$  by an SSL encoder, based on the alignment of representations from augmented views of $x$. Let $f:\mathbb{R}^n \to \mathbb{R}^d$ be an SSL encoder trained using an SSL algorithm $\mathcal{A}$ on an unlabeled dataset $S = \{x_i\}_{i=1}^{m}$. The data augmentations are represented as $\Aug(x) = \{a(x) | a \in \Aug\}$, 
% data augmentations can be represented as $\Aug(x) = \{a(x) | a \in \Aug\}$,
where $a$ is a transformation function applied to the data point $x$, mapping from $\mathbb{R}^n \to \mathbb{R}^n$.
The encoder's output representation for a given data point $x$ is denoted as $f(x)$. For a trained SSL encoder $f$, the alignment loss for a data point $x$ is defined as
\begin{equation}
    \lalign(f, x) = \alignexp{f}{x}\text{,}
    \label{eq:alignment_loss}
\end{equation}
where $x', x''$ are augmented views of $x$ and $d(\cdot, \cdot)$ is a distance metric, typically the $\ell_2$ distance.
SSLMem is then defined as
\begin{equation}\label{eq:memdef}
    \begin{split}
    \text{SSLMem}(x) =  \underset{g \sim \mathcal{A}(S \setminus x)}{\mathbb{E}} \ \lalign(g, x)
    - \underset{f \sim \mathcal{A} (S)}{\mathbb{E}} \ \lalign(f, x)  
    \end{split}
    \end{equation}
with $f$ being an SSL encoder trained with data point $x$, and $g$, an encoder trained without $x$ but otherwise on the same dataset.
While this framework measures memorization using alignment loss for single-modality encoders, this approach is unsuitable to leverage the signal over both modalities from multi-modal encoders like CLIP, as we also highlight empirically in \Cref{sub:sslmem_not_for_clip}. 
However, we can build on the main concepts from SSLMem to define a new metric that can evaluate memorization in CLIP, by considering both image and text representations, as we will detail in \Cref{sec:clipmem}.
%Memorization refers to a model's tendency to store specific details of individual training examples, rather than generalizing patterns across the dataset.
%In supervised learning (SL), memorization has been actively studied, with the \textit{leave-one-out} approach, where the model is retrained with a data point excluded from the training set (\citet{feldman2020}).
%If this results in a change in the model’s predictions, memorization of that specific data point can be directly measured.
%In self-supervised learning (SSL), memorization can be measured with alignment loss (\citet{wang2024}).
%Alignment measures how similar representations of different augmented views of the same data point are after training.
%A model that has memorized the training data will show a large difference in alignment loss compared to a model that hasn’t seen that data point.
%By calculating the alignment loss between an encoder trained with a data point and another encoder trained without it, the difference provides a measure of how much the model memorizes that specific data point.
%Additionally, in both SL and SSL, atypical (or outlier) data points that deviate from the general training data distribution were found to be more likely to be memorized. Memorization can impose data privacy risks as a model that memorizes can potentially leak sensitive information it was trained on.
%However, prior work suggests that some degree of memorization is necessary in enhancing model performance, and that atypical examples are harder to generalize from, therefore requiring memorization for better generalization and performance on downstream tasks.

\textbf{Memorization in CLIP}.
Even though CLIP is a widely used vision-language encoder, there has been limited work on measuring memorization in CLIP.
% work on measuring memorization in CLIP is still limited.
The only existing work~\citep{jayaraman2024} applies the empirical Déjà Vu memorization framework from~\citep{meehan2023ssl} to CLIP. It measures memorization by computing the overlap between unique objects in potentially memorized images and their nearest neighbors---identified in the CLIP embedding space---from a public dataset.
However, the reliance on external public data from the same distribution, along with the required accuracy of the object detection (which may not perform well for all samples, especially atypical ones~\citep{kumar2023normalizing,dhamija2020overlooked}, limits the applicability of this approach. We further expand on this in \Cref{app:deja_vu_comparison}.
% The reliance on external public data from the same distribution and the accuracy of the object detection, which might not work equally well for all samples, in particularly not atypical ones~\citep{kumar2023normalizing,dhamija2020overlooked} limit the applicability of this approach.
In contrast, our \ours operates directly on CLIP's output representations and returns a joint score over both modalities. 
% Additionally, it evaluates several memorization mitigation strategies, identifies early stopping and weight decay as effective, and additionally proposes text randomization, \ie using multiple differently masked captions for the same image during training as a defense to mitigate Déjà Vu memorization while preserving utility.
% Due to its reliance on an additional public dataset from the same distribution as the potentially memorized data, and due to the required fine-grained object detection, the CLIP Déjà Vu memorization framework~\citep{jayaraman2024} is not generally practical or applicable to all data distributions.
%  In contrast, our \ours metric does not make such strong assumptions. Finally, their work is focused on quantifying memorization in CLIP whereas we take the sample perspective and seek to understand which data points experience high memorization and why.\todo{Here, we could also highlight that their mitigation reduces performance whereas we can leverage CLIPMem to increase performance.} 
 % providing empirical insights into memorization in CLIP while we seek to understand the formal connection to generalization in the same way to~\citet{feldman2020does,wang2024memorization}.

% To date, the only work addressing this gap is the Déjà Vu framework (\citet{jayaraman2024}).
% In this framework, memorization in CLIP is measured by performing nearest neighbor searches to retrieve images from a public set of images, separate from the training set, using a target caption.
% If the model has memorized the image-text pair, the retrieved images will closely resemble the training image, beyond what would be expected from a simple correlation.
% This memorization is measured by counting the number of ground-truth objects from the original image that appear in the retrieved images, and comparing the results to a model that wasn’t trained on the same image-text pair.
% If the model retrieves more detailed objects than described in the caption, it indicates memorization, based on the belief that this behavior will not be observed in models that were trained without that specific image-text pair.
% However, the Déjà Vu framework is limited, as it relies on indirect measurements and is heavily dependent on the additional datasets selected for evaluation.
% Rather than directly evaluating whether a model reproduces specific information from its training set, it indirectly measures memorization by checking whether objects in the retrieved images align with those in the training images beyond what the captions describe.
% A significant issue with this approach is its dependence on external public datasets for nearest-neighbor searches.
% To determine whether a model has memorized certain training data, images are retrieved from a separate public dataset based on the model’s interpretation of the caption, and then compared to the original training images.
% This dependency introduces challenges, as the memorization test is then dependent on the quality, diversity, and size of the public dataset.
% For example, the dataset must be large and diverse enough to include images similar to the training data, which is not very practical and can undermine the reliability of the results.
%Our definition of memorization in CLIP builds upon SSLMem by leveraging the concept of alignment but extending it to handle multimodal data.
%Specifically, we measure the difference in alignment between representations of text-image pairs from encoders trained on these pairs and those that were not.
%By doing so, we can quantify how much the model memorizes specific text-image pairs across both modalities, reflecting CLIP’s contrastive learning objectives.
% Additionally, our approach uses canary data to directly measure elimination, eliminating the need for an external dataset.


% \section{Background and Setup}


% %In supervised learning (SL), memorization has been actively studied, with the \textit{leave-one-out} approach, where the model is retrained with a data point excluded from the training set (\citet{feldman2020}).
% %If this results in a change in the model’s predictions, memorization of that specific data point can be directly measured.
% %In self-supervised learning (SSL), memorization can be measured with alignment loss (\citet{wang2024}).
% %Alignment measures how similar representations of different augmented views of the same data point are after training.
% %A model that has memorized the training data will show a large difference in alignment loss compared to a model that hasn’t seen that data point.
% %By calculating the alignment loss between an encoder trained with a data point and another encoder trained without it, the difference provides a measure of how much the model memorizes that specific data point.

% We present concepts relevant to defining memorization in CLIP by leveraging main ideas from the SSLMem framework \citep{wang2024memorization}. This framework measures memorization of individual data points in SSL encoders, based on their augmentations and alignment. Let $f:\mathbb{R}^n \to \mathbb{R}^d$ be an encoder trained using an SSL algorithm $\mathcal{A}$ on an unlabeled dataset $S = \{x_i\}_{i=1}^{m}$. To evaluate memorization, data augmentations are used, represented as $\Aug(x) = \{a(x) | a \in \Aug\}$, where $a$ is a transformation function applied to the data point $x$, mapping from $\mathbb{R}^n \to \mathbb{R}^n$. The encoder's output representation for a given data point $x$ is represented as $f(x)$.

% \subsection{Alignment and Memorization in SSL}

% % Alignment Loss
% \textbf{Alignment Loss} quantifies the similarity between representations of different augmentations of a data point. For a trained SSL encoder $f$, the alignment loss for a data point $x$ is defined as:
% \begin{equation}
%     \lalign(f, x) = \alignexp{f}{x}\text{.}
%     \label{eq:alignment_loss}
% \end{equation}
% where $x', x''$ are augmented views of $x$ and $d(\cdot, \cdot)$ is a distance metric, typically the $\ell_2$ distance.

% % Memorization Score
% \textbf{Memorization Score} measures the degree to which an SSL model retains information about a particular data point, by comparing alignment losses between two encoders: $f$, trained with a data point $x$, and $g$, trained without it. The memorization score for $x$ is given by:
% \begin{equation}\label{eq:memdef}
%     \begin{split}
%     m(x) =  \underset{g \sim \mathcal{A}(S \setminus x)}{\mathbb{E}} \ \lalign(g, x)
%     - \underset{f \sim \mathcal{A} (S)}{\mathbb{E}} \ \lalign(f, x)  \text{.} 
%     \end{split}
% \end{equation}

% This score measures the extent to which the presence of $x$ in the training set affects the alignment. A positive score indicates that the encoder $f$ trained with $x$ shows a higher alignment compared to the encoder $g$, suggesting higher memorization.

% this is a very good description, I moved it to the appendix.
% \subsection{Experimental Setup}
% To experimentally evaluate memorization using the SSLMem framework, the training dataset $S$ is split into three sets: \textit{shared set} ($S_S$) used for training both encoders $f$ and $g$, \textit{candidate set} ($S_C$) used only for training encoder $f$, and \textit{independent set} ($S_I$) data used only for training encoder $g$. For training encoders, encoder $f$ is trained on $S_S \cup S_C$, while encoder $g$ is trained on $S_S \cup S_I$. The alignment losses $\lalign(f, x)$ and $\lalign(g, x)$ are computed for both encoders, and the memorization score $m(x)$ for each data point is derived as the difference between these alignment losses, normalized to a range between $-1$ and $1$. A score of $0$ indicates no memorization, $+1$ indicates the strongest memorization by $f$, and $-1$ indicates the strongest memorization by $g$.