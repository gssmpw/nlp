\section{Related work}
\label{sec:related_work}

Tournament selection is a commonly used selection method, where $n$ randomly chosen individuals compete in a tournament and the best individual is selected as a parent for the next generation \cite{Poli.2008}. One disadvantage of tournament selection is that the quality of individuals is measured in terms of an aggregated value, leading to a loss of information about the data structure~\cite{Krawiec.2014}. In contrast, lexicase selection~\cite{Spector.2012, Helmuth.2014} evaluates individuals on each training case separately. This is beneficial during search, as individuals that solve certain training cases particularly well are preserved~\cite{Helmuth.2019, Helmuth.2020b, Pantridge.2018}. Additionally, it has been found that lexicase selection is able to maintain a high population diversity in comparison to tournament selection~\cite{Helmuth.2016}. Lexicase selection has been successfully applied in many problem domains~\cite{Helmuth.2014, Helmuth.2015, Sobania.2023, sobania2021generalizabilitymeasure, Aenugu.2019, Moore.2017, Moore.2018, ding2021optimizing}. 

For symbolic regression problems with continuous-valued errors, $\epsilon$-lexicase selection~\cite{LaCava.2016, LaCava.2019} significantly improved the solution quality compared to tournament selection and standard lexicase selection. $\epsilon$-lexicase selection also considers individuals for selection that are near-elite on training cases by applying an $\epsilon$-threshold to standard lexicase selection.

Recently, the combination of lexicase selection with down-sampling improved the solution quality even more in the domain of program synthesis~\cite{Hernandez.2019, Ferguson.2020} and symbolic regression~\cite{geiger.2023,geiger2024comprehensive,geiger2024lexicase}. A na\"ive down-sampling strategy is random down-sampling~\cite{Goncalves.2012, gonccalves2011experiments}, where only a random subset of training cases is used in each generation to evaluate the quality of the individuals.
It has been found that random down-sampling in combination with tournament selection improves performance, reduces overfitting and controls bloat in the domain of symbolic regression~\cite{Goncalves.2012, martinez2017comparison}. However, reduced overfitting was not (or only insignificantly) observed when combining random down-sampling with lexicase selection for program synthesis problems~\cite{Helmuth.2021, Schweim.2022}.

One drawback of random down-sampling is that the random selection of a subset of training cases may lead to the exclusion of important training cases for several generations. Therefore, Boldi et al.~\cite{boldi2024informed} proposed informed down-sampling, which creates more diverse subsets of training cases by using population statistics. Due to the success of informed down-sampling in program synthesis~\cite{boldi2024informed} and symbolic regression~\cite{geiger2024lexicase} its influence has been further analyzed for program synthesis problems~\cite{boldi2024untangling, Boldi2023.static, boldi2023problem}.

Both random and informed down-sampling have been found to improve performance not only for lexicase selection but also (among others) for tournament selection \cite{geiger2024lexicase, boldi2024untangling}.
Therefore, it is unclear if we need lexicase selection or if tournament selection combined with down-sampling is sufficient considering that tournament selection is significantly faster than lexicase selection.