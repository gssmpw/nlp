\section{Implementation and Evaluation}    \label{sec:eval}

\subsection{Exampled Transaction Key-Value Store} \label{subsec:implementation}

We construct a shared record key-value store (KVS) based on the \name~architecture. Similar to recent works~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, fasst, drtmh, grappa_atc15, drtm}, we adopt a shared-record organization where transactions on any node may manipulate any records of the dataset. 
Nodes execute in a symmetric model~\cite{fasst, drtmh, drtm, farm_nsdi14}, where each node runs both client and server processes. 
Note that \name~is also compatible with other organizations, such as shared-nothing architectures~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12} and deterministic scheduling~\cite{caracal_sosp21, calvin_sigmod12}.
% The client process issues transactions to read, write, add-up and delete the records specified by the keys, and the server executes the transaction and returns the client with either a success or failed acknowledgement. 
% \begin{figure}[t]
%   \centering 
%   \includegraphics[width=0.43\textwidth]{my/figs/data-layout.pdf}
%   \caption{Layout of data structures in our hash table.}
%   \label{fig:data-layout}
% \end{figure}
% \noindent \textbf{Data Layout. }
% \subsubsection{Data Layout. }
We adopt the widely used framework DBx1000~\cite{abyss_vldb14, taurus_vldb2020} as our codebase. The KVS uses either a hash table or a tree to store the key-value items. 
Taking hash index as an example, each key maps to a bucket, and the bucket is organized as a linked chain. 
A bucket item contains a pointer to the tuple header, which further points to the value. 
% The header content depends on the concurrency control policies. For example, the SILO header~\cite{silo_sosp13} takes 128 bits, comprising a one-bit latch, a 63-bit version number, and a 64-bit pointer to the record contents. 
We bind only the records with \name~primitives, while other shared contents, including indexes and headers, remain with \vanilla~primitives.


% Each index item points to a tuple header storing the \textit{metadata field}.
% Figure~\ref{fig:data-layout} illustrates an exampled data layout of the hash table index. 



\ifx\undefined\stale
\subsubsection{Transaction. }
We denote a set of tuples read and written by a transaction by the read set (\textit{RS}) and the write set (\textit{WS}). We assume that a transaction first reads the tuple it writes so that $RS \subseteq WS$. We use the four phase OCC as an example to illustrate how a transaction benefits from \name~primitives. 

\noindent \textbf{(1) Execution Phase. }The transaction begins execution by reading the header and the keys from the dataset. For a key in \textit{WS}, the worker thread allocates a buffer in the local memory and redirect writes to it. \textit{The isolation nature of \name-primitive's loosely coherent model allows the \textit{record field} accesses in this phase free of cross-node coherence issues hence exhibit lower latency. }

\noindent \textbf{(2) Validation Phase. } After accessing all tuples, the worker thread tries to lock each tuples in the \textit{WS}. \textit{This is done by standard single-node atomic operations, e.g. compare-and-swap (CAS) and fetch-and-add (FA), thanks to the strict cache coherence model on this field. }
% toward the headers in with CXL-vanilla primitives. 
If any key is locked or the version has changed from the first phase, the transaction will abort.

\noindent \textbf{(3) Logging Phase. } The worker thread poses the \textit{WS}'s key-value items and their versions to the buffer of helper threads and returns. We implement a batched logging scheme of taurus~\cite{taurus_vldb2020}. \textit{Logs are flushed to local file systems so that avoid intervening the shared memory. }

\noindent \textbf{(4) Commitment Phase. } 
In addition to copy WS's records to the shared locations, the worker thread should also call the GSync on the records' addresses and waits for an completion from the EP. 
\textit{The system could batch GSync calling from different transactions on the same node~\cite{fasst}, or hide the latency with co-routines~\cite{drtmh}. } After receiving EP's responses, the worker thread then increases the tuples' versions and unlocks the keys. 
% If the transaction aborts, the worker thread cleans up the temporal buffers, and cancels written records (if has) using the Wd primitive. 
\textit{The withdrawn writes would not cause any cross-node coherence issues. }

% Note that Wd is not used in OCC since writes are buffered, but the Wd is required in the pessimistic concurrency control policies such as two phase locking to withdraw writes. 

\fi

% It first copies each tuples in WS from the buffer to the shared locations. This step advertises the changes to all local peer processors. To advertise other nodes, the transaction calls GSync on the the location and waits for an completion signal from the EP. After that, the worker thread increases the tuples' versions and unlocks the keys. 


%  and explicitly synchronizes the content to other nodes. 
% Each node adopts a helper thread to store logs in local file systems. The worker thread poses modified data to a buffer and returns, the helper thread checks the buffer and flush it at 1ms interval.  


% But in CXL-vanilla, a data field read may access remote caches if a transaction has modified it but not written it back. 
% If validation succeeds, the worker thread can log the modified content to non-volatile storage. 

\ifx\stale\undefined
\begin{itemize}
    \item \textbf{Execute. } The transaction begins execution by reading the header and the keys from the dataset. For a key in \textit{WS}, the worker thread allocates a buffer in the local memory and redirect writes to it. The isolation nature of \name-primitive allows the data field reads in this phase without cross-node coherence issues. But in CXL-vanilla, a data field read may access remote caches if a transaction has modified it but not written it back.

    \item \textbf{Validate. } After accessing all tuples, the worker thread tries to lock each tuples in the \textit{WS}. This is done by standard single-node atomic operations, e.g. compare-and-swap (CAS) and fetch-and-add (FA), toward the headers bounded with CXL-vanilla primitives. If any key is locked or the version has changed from the first phase, the transaction will abort.

    \item \textbf{Log. } If validation succeeds, the worker thread can log the modified content to non-volatile storage. The worker thread poses the \textit{WS}'s key-value items and their versions to the buffer of helper threads and returns. We implement a batched logging scheme resembling taurus~\cite{taurus_vldb2020}. Each node adopts a helper thread to store logs in local file systems. The worker thread poses modified data to a buffer and returns, the helper thread checks the buffer and flush it at 1ms interval.  

    \item \textbf{Commit. } If logging succeeds, the worker thread updates the records in WS and explicitly synchronizes the content to other nodes. It first copies each tuples in WS from the buffer to the shared locations. This step advertises the changes to all local peer processors. To advertise other nodes, the transaction calls GSync on the the location and waits for an completion signal from the EP. After that, the worker thread increases the tuples' versions and unlocks the keys. 

    \item \textbf{Abort. } If the transaction aborts, it cleans up the temporal buffers, then cancels the updates on shared records with Wd (if has). Note that Wd is not used in OCC since writes are buffered, but the Wd is required in the pessimistic concurrency control policies such as two phase locking to withdraw writes. 
\end{itemize}
\fi

\subsection{Setups}  \label{subsec:setup}

\noindent \textbf{Benchmarks.} 
Following previous in-memory transaction processing works~\cite{dbx1000_dist_vldb17, drtm, drtmh, polyjuice_osdi21}, we adopt two OLTP tasks: 

\noindent (a) TPC-C~\cite{tpc-c}, the current industry standard for OLTP evaluation. 
% The number of warehouses controls the transaction contention and data sharing across nodes. 
Similar to previous works~\cite{dbx1000_dist_vldb17}, we adopt two (Payment and NewOrder) out of five transactions in our simulation as a default mixture (DM) since they account for 88\% of the TPC-C workload, and also tests NewOrder-only cases (NO). 

\noindent (b) YCSB~\cite{ycsb}, representative of large-scale cloud services. In this paper, we use a 16GB YCSB database containing a single table with 16 million records. Each tuple has a single primary key and 10 values, each with 20 bytes of randomly generated string data. 
A transaction accesses 16 records with reads or writes controlled by write ratio $w$, and follows the Zipfian distribution controlled by the skew factor $\theta$.

% We adopt \textit{theta=0} to evaluate the case that tuples are accessed a the same frequency, and \textit{theta=0.7} for the case that over 60\% accesses locate at 10\% tuples.
% We also adopt read access ratio of 1.0 and 0.1 to represent read only and write intensive cases. 

% which benchmarks cloud key-value stores with flexible read-write configuration. The key selection in YCSB satisfies the Zipf distribution with varied skew parameter $\theta$ to evaluate different use cases.  

% spanning from variants of pessimistic two-phase locking (NO\_WAIT and WAIT\_DIE), time-stamp based optimistic algorithms (SILO, OCC, and TICTOC), and multi-version algorithms (MVCC). 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.47\textwidth]{my/figs/pool.pdf}
%   \caption{Roundtrip latency breakdowns on prototype. }
%   \label{fig:cxl-overhead}
% \end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.93\textwidth]{my/figs/tput_speedup.pdf}
  \caption{Overall Performance Comparison. }
  \label{fig:throughput}
\end{figure*}

\begin{table}[t]
% \begin{minipage}{0.45\textwidth}
% \caption{Benchmark Configurations}  \vspace{-5pt}
% \label{tab:benchmark}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}c|c|c|c@{}}
% \toprule
% Benchmark                  & Abbr.   & Benchmark               & Abbr. \\ \midrule
% TPC-C. 8WH Default Mix      & T-HC-DM & TPC-C. 8WH 100\% NewOrder      &  T-HC-NO     \\ \midrule
% TPC-C. 64WH Default Mix     & T-LC-DM & TPC-C. 64WH 100\% NewOrder     &  T-LC-NO     \\ \midrule
% YCSB. $\theta=0$ 100\% Read    & Y-U-RO        & YCSB. $\theta=0.8$ 100\% Read      &  Y-SK-RO     \\ \midrule
% YCSB. $\theta=0$ 30\% Read &  Y-U-RW       & YCSB. $\theta=0.8$ 30\% Read  &   Y-SK-RW    \\ \bottomrule
% \end{tabular}%
% }
% \end{minipage}
\begin{minipage}{0.45\textwidth}
\caption{System Parameters and Configurations}\vspace{-5pt}
\label{tab:system-config}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}|cl@{}}
\toprule
\multicolumn{2}{c}{\textbf{Host Processors}}                                                    \\ \midrule
\multicolumn{1}{c|}{Processor}     & 10-core @ 3.2GHz                                   \\ \midrule
\multicolumn{1}{c|}{L1I/L1D/L2/L3} & 32KB,4way / 32KB,8way / 2MB,8way / 1.875MB,16way                    \\ \midrule
\multicolumn{1}{c|}{Memory System} & 2x channel, 64GB, DDR4 @2666MHz                   \\ \midrule
\multicolumn{2}{c}{\textbf{CXL Configurations}}                                                      \\ \midrule
\multicolumn{1}{c|}{Connection}    & 32Gb/s/Lane, 8x Lanes (32GB/s) per node     \\ \midrule
\multicolumn{1}{c|}{Memory}        & \begin{tabular}[c]{@{}l@{}}DDR4 @2666MHz, 21.3GB/s/channel,\\ 8x channels (170.4GB/s in total)\end{tabular} \\ \midrule
\multicolumn{1}{c|}{Snoop Filter}  & 128K entries, 16 way               \\ \midrule
\multicolumn{2}{c}{\textbf{CxTnL Parameters}}                                                   \\ \midrule
\multicolumn{1}{c|}{VF/VBF}        & 512B per node,2 hash / 16KB per node,2 hash                            \\ \midrule
\multicolumn{1}{c|}{VAT} & \begin{tabular}[c]{@{}l@{}}1M entries/hash table, 2-way, 40 bit index, \\ resizing threshold=0.6/0.01, interval=10ms\end{tabular} \\ \bottomrule
\end{tabular}%
}
\end{minipage}
\end{table}

\noindent \textbf{Evaluation Methodology.}
% We collect the performance number of the CXL-vanilla and \name~primitive on a prototype system, which contains an Intel Sapphire-Rapids processor and an R-type Agilex FPGA board with CXL 1.1 IP. Note that the CXL 3.0 IP has not been supported in any commercial platforms, hence we make the emulation of \cxlbi's datapath on our prototype with \cxlcache. We implement an LRU SF and hacks IP's address translation process to enable memory sharing on CXL 1.1 IP, by folding a half of physical addresses to map to the same DRAM address of another half. But the deadlock incurred by \cxlcache~obstacle us from running full system evaluation. We instead build a pin-tool simulator based on Sniper~\cite{sniper}, to simulate \name~and \cxl-vanilla architectures.
We develop an FPGA-based CXL memory system, incorporating an Intel$^\circledR$ Sapphire-Rapids$^\texttt{TM}$ Xeon Gold 6430 CPU and an Intel$^\circledR$ Agilex$^\texttt{TM}$-7 I-Series FPGA configured as a CXL type-2 device (CXL 1.1). This configuration enables both \cxlmem~and \cxlcache~of the CXL IP~\cite{cxl_ip}. 
\rvs{
The CXL IP consists of a hardware component, implemented as an independent chiplet known as R-Tile~\cite{cxl_ip}, and a soft-encrypted wrapper located within the Programmable Logic (PL) domain, working at 400 MHz. The inter-chiplet communication between the hard and soft components of the CXL IP introduces a significant portion of the overall latency. 
The FPGA owns dual-channel DDR4-2666 memory, totaling 16GB in capacity. 
The memory system operates under Linux kernel v6.3. 

% The driver abstracts the CXL device to both a DAX device and a zero-NUMA node. 

With this prototype, we evaluate two key performance features that determine the CXL protocol latency: the host-to-device \cxlmem~roundtrip latency and the device-to-host \cxlbi~roundtrip latency. To evaluate the first phase, we employ the approach used in prior studies~\cite{cxl_demystify, neomem} that adopts the standard memory latency checker~\cite{mlc} to generate loads and stores from the host CPU to the device memory. To mimic the snoop-filter behaviors as mentioned Figure~\ref{fig:motivation-over-coherent}, we implement a direct-map SF at the FPGA PL side, locating at \cxlmem~critical path and do udpates at each memory accesses. This number is known as C2M latency in Table~\ref{tab:latency}. 
For the second phase, we utilize \cxlcache~to emulate the datapath of \cxlbi~since it is not yet available in CXL 1.1 devices. \cxlbi~have similar protocol roundtrip constitution with \cxlcache, and it is introduced in 3.0 to avoid the circular channel dependence between \cxlcache~and \cxlmem~\cite{cxl-shortdoc, cxl-doc, cxl-paper}. 
We construct a test core on the FPGA's PL side to generate memory access requests. These requests are routed to the CXL IP in host-bias mode via the AXI bus, initiating \cxlcache~requests from the device to the host~\cite{cxl_ip, cxl-doc, cxl-shortdoc}.
The test core waits for the accessed data to be returned and records the total latency. 
To get the pure CXL link latency, we subtract the two aforementioned numbers with the time that PL test core spends on accessing the device memory without intervening CXL IP. We add the two subtraction results to get C2C latency. 

% Additionally, we measure the latency of directly accessing device memory from the PL side test core without CXL intervention 

% This CXL IP disallows the full remote signaling process, so that we break it to two parts: the host-to-device \cxlmem~roundtrip and the device-to-host \cxlbi~roundtrip. 
% To evaluate the first part, we follow previous works~\cite{cxl_demystify, neomem} to adopt the standard memory latency checker~\cite{mlc} that generates loads and stores from host CPU toward the device memory. 
% To evaluate the second part, we utilize \cxlcache~to emulate the datapath since \cxlbi~is not yet supported in CXL 1.1 devices. We implement a test core at the FPGA PL side to continuously generate memory accessing requests to random addresses. These requests are sent to the CXL IP in host-bias mode via the AXI bus to trigger the \cxlcache~requests from device to host~\cite{cxl_ip, cxl-doc, cxl-shortdoc}. The test core waits until the accessed data is returned and accumulates the total latencies. We also evaluate the latency for directly accessing the device memory from the PL side without CXL intervention, so that we could calculate the pure time spent on CXL links. 

% We also implement a direct-map SF at the test core to mimic the \cxlbi~behavior. 
% hese artifacts can be attributed to two main factors:
% First, the CXL IP operates at a relatively low frequency (approximately 400 MHz), resulting in longer processing latency compared to an ideal ASIC implementation. Second, the chiplet design of the Agilex™ 7 FPGA I-Series contributes to considerable latency overhead. Specifically, its CXL IP comprises a hard encrypted part as an independent chiplet, while its soft wrapper and other functional modules are implemented on another chiplet. These two chiplets are connected via a slow on-package interconnection, leading to increased processing latency compared to a monolithic circuit design.

% We implement a direct-map SF and modify the address translation process of the CXL memory device to facilitate memory sharing on an CXL 1.1 device by folding half of the physical addresses to map to the same DRAM address as the other half. 

% its CXL IP comprises a hard encrypted part as an independent chiplet, while its soft wrapper and other functional modules are implemented on another chiplet. These two chiplets are connected via a slow on-package interconnection, leading to increased processing latency compared to a monolithic circuit design.

% where the CXL and memory-related IP cores are implemented on the chiplet~\cite{cxl_ip}. 
% The host CPU features 32GB $\times 2$ dual-channel DDR5-4800 memory. 

Due to the deadlock issue from \cxlcache's channel dependency~\cite{cxl-shortdoc, cxl-doc, cxl-paper}, we are unable to conduct full system evaluation based on the prototype FPGA. We thus develop a pin-tool simulator based on Sniper~\cite{sniper} with around 2.5K LoC changes. We build a memory node that simulates the \name~and CXL-vanilla architecture based on the performance characteristics of the prototype. We change a few lines of DBx1000 to adapt to CTLib. 
We also assess the ideal ASIC performance in the simulator based on reports from chip vendors and IP providers~\cite{samsung, directcxl, tpp_asplos23, hash-ip}, as detailed in Table~\ref{tab:latency}. 
For network-based systems, we deploy the distribution version of DBx1000~\cite{dbx1000_dist_vldb17, abyss_vldb14} on 8 r320 nodes in CloudLab~\cite{cloudlab}. 
}


% But the deadlock incurred by \cxlcache~obstacle us from running full system evaluation. We instead build a pin-tool simulator based on Sniper~\cite{sniper}, to simulate \name~and \cxl-vanilla architectures.
% Considering that Sniper cannot simulate the host OS behaviours, we collect OS introduced overheads including the TLB miss penalty and DAX overheads on our prototype system and model it in our simulator. 
% We also modify Sniper to distinguish memory access regions by inserting special instruction before the transaction accessing the CXL-backed memory. 
% We build up hardware performance model based on our FPGA prototype.
% We use the \cxlcache~channel roundtrip as the approximation of \cxlbi~and \cxlmem~roundtrip due to their protocol similarities.

% The simulator runs on a single host with an Intel Xeon Gold 5220 CPU and 512GB of main memory. 

\noindent \textbf{Simulation Setups. }
Table~\ref{tab:system-config} summarizes the default system configurations. Each node equips with an OoO CPU with the Intel Xeon Gold 6430 architecture (Sapphire Rapids series). For CXL architecture, we equip each node with an P2P CXL $\times$8 connection with EP. The EP manages shared memory in 8 channels with interleaved addresses in cacheline granularity. The EP manages an 16 way on-chip SF with 128K-entries. For \name~architecture, each CTHW implements VF and VBF via 512B and 16KB bloom filters with 2 hash functions by default. 
The view shim indicator takes 56 bits with 4 bit node id and 48 bit EMS address. The most significant 16 bits index the on-chip LUT, the following 40 bits calculates the cuckoo hashes. 

% The VAT has two hash functions with 0.6 expansion threshold and 0.01 shrink threshold. 
% We adopt the zero-load latency parameter from our prototype and simulate the per channel queuing effect. 
% This configuration requires the home agent to be as large as an IO-die of AMD Genoa~\cite{geona1, geona2}. 

% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.95\textwidth]{asplos25-templates/my/figs/scalability_test.pdf}
%   \caption{Sensitivity to architectures. }
%   \label{fig:throughput_cc}
% \end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.47\textwidth]{my/figs/dram_hist.pdf}
  \caption{Detailed Analysis of \name.}
  \label{fig:dram_hist}
\end{figure}

\subsection{Main Results}

Figure~\ref{fig:throughput}~(a) presents a performance comparison between our proposed \name~and baseline primitives, with throughput averaged across all transaction control policies in the benchmark to neutralize algorithmic effects. The results highlight \name's consistent performance gains over CXL-vanilla systems, achieving a 1.36x improvement in ideal ASIC setups and 2.08x in prototype setups. Specifically, \name~exhibits greater enhancements in the memory-insensitive YCSB benchmarks (93\% on average) compared to the TPCC benchmarks (77\% on average), due to TPCC's higher computation burden. Moreover, \name~outstrips traditional networking-based architectures by 6.47x, leveraging the superior performance of the CXL protocol while circumventing the inefficiencies of its coherence model. 

% It's worth to notice that \name~surpasses CXL-vanilla by a factor of 2.08x in the read-only YCSB benchmark attributed to the reduced coherence and SF back-invalidation traffic. These results underscore \name's effectiveness in CXL-based transaction processing systems. 


% In specific benchmarks, \name~demonstrates substantial improvements over the baselines. For instance, in FPGA setups, \name~surpasses CXL-vanilla by a factor of 2.08x in the low-contention, read-only Y-U-RO benchmark, with significant gains attributed to high cache miss rates as discussed in \cite{oltp_micro_architecture_vldb21}. By reducing back-invalidation costs on SF conflicts associated with processor cache misses and minimizing unnecessary cache coherence in read operations, \name~achieves speedups of 1.78x and 1.77x in the highly skewed T-HC-NO and Y-SK-RW at prototype setups.~\cite{oltp_micro_architecture_vldb21}

% Figure~\ref{fig:throughput} shows the performance comparison of our proposed \name~against the baseline primitives. The throughput numbers are averaged among all transaction controlling policies at the benchmark to avoid algorithm influence. As shown by the strips bars representing \name, our approach consistently achieve performance improvements over CXL-vanilla systems with 1.36x at ideal ASIC setup, and with 2.08x at prototype setup. Our approach achieves higher improvement (93\% on average) at the memory insensitive YCSB benchmarks than TPCC benchmarks (77\% on average) since a TPCC transaction traverses multiple tables thus has higher controlling overhead. When compared with traditional networking-based architectures, \name~achieves 6.47x speedup since we successfully utilize the superior performance of CXL protocol while avoids its coherence model's inefficiencies. These results demonstrate \name's efficiency in CXL-based transaction processing systems. 

% At certain benchmarks, \name~has surprising improvements compared with the baseline primitives. For example, at FPGA setups, \name~outperforms CXL-vanilla by the factors of 2.08x low contented read only Y-U-RO benchmark. The remarkable performance gain is attributed to the high cache miss rate as discussed in \cite{oltp_micro_architecture_vldb21}. \name~successfully reduce the back-invalidation cost on SF conflicts associated with processor cache miss, and avoid unnecessary cache coherence between the read operations on records. For the highly skewed T-HC-NO and Y-SK-RW benchmark, \name~also has 1.78x and 1.77x speed up over \name~vanilla at FPGA setup, respectively.

Figure~\ref{fig:throughput}~(b) shows the throughput with an increasing number of nodes, each dedicating 8 worker threads per node. It illustrates \name's good scalability with the number of sharing nodes.  
Figure~\ref{fig:throughput}~(c) shows the throughput-to-median latency relationship of \name~and \vanilla. We vary the load by increasing the number of worker threads per node from 1 to 8 while keeping 8 nodes. \name~is able to achieve 0.87M txn/sec with 59ms median latency in the highly contended YCSB workload, which is notably better than CXL vanilla with 0.59M txn/sec and 67ms median latency. On the less contended TPCC benchmarks, \name~achieves 0.92M txn/sec with stable median latency. 

Figure~\ref{fig:throughput}~(d) shows the throughput of running all new-order transactions on \name~and CXL-RPCs~\cite{hydrarpc_atc24, partial_sosp23} as we increase the number of cross-partition transactions. To make an apple-to-apple comparison, we adopt the distributed version of DBx1000~\cite{dbx1000_dist_vldb17} and store shared tuples at G-FAM, with each node exclusively managing 8 warehouses of TPC-C. We adopt SOTA CXL-shared memory RPC, i.e., HydraRPCs~\cite{hydrarpc_atc24}, which achieves 1.47us avg. latency by passing references instead of data. 
We test on 4 nodes, each with 8 worker threads. The CXL-RPC-based shared-nothing architecture is the optimal solution for perfectly partitionable workloads due to minimized cross-node coherence~\cite{hekaton_sigmod13}. However, when roughly 20\% of transactions touch multiple partitions, the throughput of the CXL-RPC architecture drops below \name.


% which is notably better than CXL vanilla with 0.59M txn/sec and 67ms medium latency. On the fewer contented workload (T-LC-NO), \name~exhibits much better scalability that achieves 0.92M txn/sec with stable medium latency, while the CXL vanilla saturate at 0.45M txn/sec with 142ms medium latency.

% \vspace{-10pt}

\subsection{Detailed analysis of \name}

\noindent \textbf{CXL Overhead Breakdown.} Figure~\ref{fig:overhead_breakdown} reports the latency breakdown on G-FAM accesses with the configuration of the SILO algorithm, no logging, and a hash index. We break down the time into DRAM accessing time, which includes the time spent on the CXL IP and DRAM controller, remote signaling time, and snoop filter querying and back invalidation time. 
The numbers are averaged across all loads and stores towards the G-FAM address segment, including accesses that hit local caches. 
\name~reduces the average remote signaling cost by 65.9\% in the prototype and 70.8\% at the ASIC. 
\name~also reduces back invalidation overheads by 87.1\% and 96.5\% at maximum, respectively, due to the reduced SF conflicts and queries on records. 
Despite the endpoint logic increasing by 34.6\% and 44.4\% due to \name-introduced overheads, the overall cost reduces by 40.2\% and 23.7\%, respectively. 

\noindent \textbf{Primitive Distribution.}
Figure~\ref{fig:dram_hist}~(a) shows that 73.8\% of G-FAM accesses are \name~primitives, while only 26.2\% are \vanilla~for synchronization. Note that the fraction of \name~primitives could be larger in real-world applications due to larger record sizes and fine-grained primitive binds. 

\noindent \textbf{L-Ld/L-St Overheads.}
We further test the overheads of the \name~architecture. 
Figure~\ref{fig:dram_hist}~(b) illustrates the distribution of un-core latency for accesses that actually touch the G-FAM node. It shows that more than 50\% of G-FAM accesses take less than 500ns, and 90\% of accesses are bounded within 750ns. Given the zero-load \cxlmem~latency is around 480ns, the overhead of the view translation flow is limited to 270ns, which is 56.2\% relative to zero-load \cxlmem~latency. The p99 latency reaches 1250ns since it takes the CXL-vanilla primitive and causes back-invalidation. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.90\textwidth]{my/figs/cxl_breakdown.pdf}
  \caption{CXL-related overhead breakdown. }
  \label{fig:overhead_breakdown}
\end{figure*}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.475\textwidth]{my/figs/record_sensitivity.pdf}
  \caption{Generality Tests}
  \label{fig:record_sensitivity}
\end{figure}

\noindent \textbf{View Type Distribution.}
Figure~\ref{fig:dram_hist}~(c) shows the usage of VMS. It indicates that almost all view accesses are served by processors' caches and less than 0.2\% of views overflow to the VMS, thanks to the recent trend in processor design, which enlarges available on-chip caches by increasing L2 caches. Moreover, the VMS filter efficiently sifts out 96.1\% of VAT queries that might involve additional overheads. 

\noindent \textbf{Coherence Traffic.}
Figure~\ref{fig:dram_hist}~(d) shows the usage of internal bandwidth between CTHWs. In a 16-node scenario, the cross-node view synchronization traffic only takes up 8.54 GB/s at p99 usage. The bus usage is always under 12\% for a 512-bit bus working at 1GHz. This shows that the essential cross-node coherence traffic is in fact low, and our loosely coherent \name~primitive successfully exploits this.

% To evaluate the overheads introduced by \name's modules, we conduct a detailed study on the time spent on the critical path. 


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.98\textwidth]{asplos25-templates/my/figs/tput_cc.pdf}
%   \caption{Throutput improvement over CXL-Vanilla on different concurrency control strategies. }
%   \label{fig:throughput_cc}
% \end{figure*}

\subsection{Generality to Software Policies}    \label{subsec:generality}

\noindent \textbf{Concurrency Control Sensitivity. } 
To demonstrate the wide compatibility to the wide range use cases, 
We first investigate the impact of transaction processing policies on system performance at Figure~\ref{fig:record_sensitivity}~(a). 
\name~exhibits broad performance improvement with average 1.90x on all algorithms. The OCC~\cite{occ_tbs81} and SILO~\cite{silo_sosp13} gains the highest speedup by the factor of 2.26x and 2.06x due to their speculative execution that always access the entire read/write set despite of aborts. 2PL-variant policies such as NO\_WAIT and WAIT\_DIE~\cite{2pl} gain lower speedups, i.e. 30.7\% and 20.3\% on average at TPC-C benchmarks, since they are bottlenecked by the lock managers at most cases~\cite{abyss_vldb14}.
% , which is guared by CXL vanilla primitives. We leave the work on optimizing locking services on CXL primitives to future works. 

% \noindent \textbf{Transaction System Configurations. }
% We test the various choices of transaction processing policies based on the DBx1000~\cite{dbx1000_dist_vldb17} codebase. We consider five representative concurrency control algorithms: 2PL with aborting-once-wait deadlock prevention (NO\_WAIT)~\cite{2pl}, 2PL with aborting younger competitor deadlock prevention (WAIT\_DIE)~\cite{2pl}, simple optimistic concurrency control (OCC)~\cite{occ_tbs81}, OCC with read operation optimization (SILO)~\cite{silo_sosp13}, lazy timestamp computation (TICTOC)~\cite{tictoc}. 
% For index, we test binary trees and hash tables~\cite{dbx1000_dist_vldb17}. We also implement the silo style batched data logging at 10ms interval~\cite{taurus_vldb2020, silo_sosp13} with four helper threads per node. 

\noindent \textbf{Record Size Sensitivity. } 
As Figure~\ref{fig:record_sensitivity}~(b) shows, \name~achieves higher speedup on the large record size, since it directly reduce the coherence maintenance cost on record memory accesses. This figure adopts a SILO concurrency control at the FPGA setup, with the fixed 8 byte metadata~\cite{silo_sosp13}. By increasing the YCSB record size from 100B to 1KB per tuple, the speedup increases from 1.22x to 5.81x accordingly. This result demonstrates \name's opportunity for the applications with large record size.

\noindent \textbf{Index Sensitivity. } 
Figure~\ref{fig:record_sensitivity}~(c) shows \name's generality to hash indexes and binary tree indexes~\cite{abyss_vldb14, masstree} in TPC-C benchmarks. Despite the current index design is bounded to original CXL vanilla primitives that could not directly benefit from \name~primitives, we still achieve 1.72x and 1.98x performance improvement on hash and binary trees respectively, since \name~avoid the SF eviction traffic from tracking large records. 
% thereby eliminating the SF conflicts on accesses with CXL vanilla primitives. 
% This exhibits \name's indirect benefits on other data structures in transaction systems beyond records. 

% About sweetpoint disucssion, a ref to that.

% \rvs{
% % Due to the lack accessibility of CXL switches, it's hard to estimate the link latency for CXL-switch enabled clusters. A CXL switch that bridge the nodes and the endpoints. 
% The remote signaling cost could be more severe when introducing CXL switches. 
% Involving CXL switch makes the remote signaling cost more servere than the point-to-point setup we have discussed.
% Because traversing CXL switches could introduce considerable latency overheads to CXL link roundtrips. According to aforementioned discussion, the remote signaling process takes two CXL roundtrips while accessing the G-FAM only takes one. 
% Despite it's hard to know the precise number due to the lack accessibility of CXL switches, we perform a range analysis of the link latency at Section. 
% }

% Number of retries, check CLIO for details. 
\begin{figure}[t]
  \centering
  \includegraphics[width=0.485\textwidth]{my/figs/latency_sweep.pdf}
  \caption{\rvs{Scalability Tests}}
  \label{fig:latency_sweep}
\end{figure}

\rvs{
\subsection{Scalability Tests}  \label{subsec:latency_sweep}
\noindent \textbf{VAT Scalability.}
% Figure~\ref{fig:latency_sweep}~(a) discusses the relationship between the transactions' total pending write capacity and the number of VAT retries required to successfully insert an element. To test it, we use a single-node micro benchmark that randomly generates L-St requests until the VAT meets the maximum retries, i.e. 64, on insertion. This benchmark adopts 32 threads of the node to issue 64-byte L-St randomly. Each core owns 3.85MB on-chip caches and randomly choose the evicted cacheline. A hash table occupies 4MB and a VAT has 1, 4, and 16 hash tables. We could tell from the result that for small number of pending writes (< 20MB), we either insert the element on the first attempt or require a few retries even for only one hash table. As the write capacity increases, insertion performance deteriorates, after 22MB, 60MB, and 140MB in 1, 4, and 16 cuckoo hash tables, respectively. This could cover typical OLTP transactions since they are short-lived and rarely writes. To enlarge the supported write set, the user could enlarge the cuckoo hash table or adopt the pessimistic concurrency control algorithms. 
Figure.~\ref{fig:latency_sweep}~(a) illustrates the relationship between the total pending write capacity of transactions and the number of VAT retries required for successful element insertion. We build a single-node synthesized-benchmark that generates random 64-byte L-St requests until the VAT reaches the maximum retry threshold (set as 64). The host CPU owns 32 cores and each has 3.85MB on-chip caches and adopts random eviction strategy on cache conflict. 
A cuckoo hash table occupies 4MB DRAM accommodating 512K entries. 
The results demonstrate that when the number of pending writes is relatively small (less than 20 MB), the insertion is often successful on the first attempt or requires only a few retries. As the write capacity increases, the insertion performance deteriorates. But we could keep the number of retries under 6 by  maintaining the VAT occupation under 0.6. Under our default setup, the system could support the application with the write set capacity of 27MB, 52MB, and 112MB per node with the configurations using 1, 4, and 16 cuckoo hash tables, respectively. 

Default VF and VBF can hold about 1.4K and 53K items at about 25\% false-positive rate. Note that the default configuration of the CTHW is based on the general OLTP workloads, to optimize for applications with larger pending write sets, users could enlarge the default size or adopt pessimistic concurrency controls such as 2PL variants~\cite{abyss_vldb14}. 

% Since OLTP transactions are typically short-lived and involve few write operations, \name~works well at most cases. 
% \red{
% As we seen from the working set. 
% Depends on the workload features, we select the configuration from typical working set}
% hows the average number of insertion attempts required to successfully insert an element as a function of the table occupancy. We see that, for low occupancy, we either insert the key on the ￿rst attempt or require a single displacement. As occupancy increases, insertion performance deteriorates — e.g., after 50% occupancy in the 2-ary cuckoo, and after ⇠70% in the 3, 4, and 8-ary cuckoo hash tables

% Previous works like Pond~\cite{pond} assume the switch and the retimer take 70ns and 20ns respectively, but there are no available hardware at present.
\noindent \textbf{Link Latency.}
The CXL specification 2.0 introduces the CXL switches to facilitate connectivity in large-scale clusters. However, incorporating CXL switches, along with necessary retimers, significantly increases the link latency. 
% To illustrate the effectiveness of \name~under various system scale, 
To study the \name~sensitivity on link latency, we conduct a what-if analysis by adjusting C2C and C2M latency from hundreds of nanoseconds to several microseconds. 
% adjusting the C2M latency from 100ns to 1ms, C2C from 200ns to 2ms. 
Figure.~\ref{fig:latency_sweep}~(b) depicts the throughput improvement of \name~over the CXL-vanilla baseline. We test SILO algorithm on the TPC-C benchmark with 8 nodes and 8 threads each. We could tell that CXL link latency positively related to performance improvement due to its direct impact on the C2C overheads. \name~can achieve a throughput improvement of up to 3.41x when the CXL link takes near 1ms. 
% In practice, a CXL memory pool may have correlated C2C and C2M latencies as they both depend on CXL link performance, but it follows this performance trend. 

% The CXL specification 2.0 introduces the CXL switch to connect a large scale cluster. However, the involvement of CXL switches as well as the essential retimers introduce considerable overheads to link latency. 
% To this end, we make a what-if analysis by varying the latency of C2C and C2M from an ideal number to around a microsecond. Figure~\ref{fig:latency_sweep}~(b) illustrates \name's the throughout improvement over the CXL-vanilla baseline. We adopt a SILO algorithm and test the system on the TPCC benchmark where each node owns 8 worker threads. We could tell that the \name's achieves up to 2.19x throughput improvement as the C2C and C2M latency reaches 1ms, and the C2C latency is the major effect since it directly determines coherence overheads. In real-world, the C2C and C2M latencies may have correlation since they are both influenced by the CXL link performance, but they are following the trend that the larger the cluster is, the more benefit is gained from \name. 

% This result suggests that the benefits of \name~increase with the size of the cluster. But we believe that the sweetpoint of \name~is the scale with C2C and C2M roundtrip latency under a micro-second, which is typically a rack with 8-16 nodes~\cite{pond}. Because within CPU, the datapath of CXL resembles main memory accessing. This datapath hides memory latency with many modules such as re-order buffer, store buffer, and miss-status handling registers. Over-large memory accessing latency could overwhelm these modules and harms main memory accessing. 


These findings indicate that the advantages of \name~grow as the cluster size increases. However, we suggest that the sweetpoint scale for \name~is when the C2C and C2M roundtrip latency is below one microsecond, which is typically a rack containing 8-16 nodes with direct connection to shared memory~\cite{pond}. Because within the host processors, CXL accessing datapath share many latency hiding modules, such as store buffers and miss-status handling registers, with main memory accessing datapath. Excessive latency in CXL memory access may overwhelm these modules and effect main memory accessing performance.
}
% and 4.75x lower latency compared with RDMA-based systems.

% 2. About decouple: How much do we reduce the coherence traffic ? cxl-overhead breakdown, coherence packet lines

% 4. About overhead: the space and time overhead of our hardware and software ? 
% 5. About scalability: Is our design scalable ? The limit ? n
