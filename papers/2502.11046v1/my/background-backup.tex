
% Data generation and query volumes are scaling beyond the capacity of single-node database management systems. As a result, studies are distributing data across multiple servers, where each partition contains only a subset of the database. Some transactions invariably need to access data on multiple partitions. However, multi-partition serializable concurrency control protocols lead to the well-known performance penalties, due to the high cost of cross-node networking. To eliminate its performance impact, community has proposed many multi-node architectures, we can classify them into two types:

% When a transaction accesses multiple servers over the network, any other transactions it contends with may have to wait for it to complete [6]a potentially disastrous proposition for system scalability. 

% However, inter-node communication incurs networking, which has multiple orders of magnitude higher latency than communication within a node. To eliminate the impact of networking, community have proposed many multi-node architectures, we can classify them into two types:

% This renders distributed transactions performance degradation so that becomes the major bottleneck of the application services. 




% For decades, the shared-nothing architecture has been regarded as the ``gold standard'' in distributed DBMSs due to they reducing network communication especially in supporting single-partition queries~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12}.
% In SN architecture, the dataset is divided into disjoint partitions. Each partition is assigned with a singe-node runtime engine, which has exclusive access to that partition. Each partition is independent from all others, thus nodes do not need to send messages to fetch records. The system is vertically scaled out, as shown in Fig.~\ref{fig:bg-scaling}~(a), the application transaction is split into two sub-process according to which node its touched records locate on: the sub-processes touching record A and record D/E are dispatched to the node-0 and node-1 respectively. 
% The SN architecture adopts deterministic scheduling that the transaction managers on each host make the consensus on the serialization order of a transaction before it is allowed to start. 
% However, to find out which partitions do the transaction will touch, most SN architectures assume the pre-knowledge of transactions’ read and write sets or otherwise calculate them on-line via an expensive ``reconnaissance'' step~\cite{dbx1000_dist_vldb17}.

% To make the consensus on serialization order before a transaction is allowed to start, 
% Some protocols dependent on deterministic execution, thus they require that transactions’ read and write sets be known in advance or otherwise calculated on-line via an expensive “reconnaissance” step. 


% With such a priori, transaction managers on each host can make the consensus on serialization order before a transaction is allowed to start. 
% For example, in H-Store~\cite{hstore_damon16}, the DBMS assigns each transaction a timestamp and then adds it to the wait queues for the locks of its target partitions, before the transaction starts running. 
% The execution engine for a partition removes a transaction from the queue and grants it access to that partition if the transaction has the oldest timestamp in the queue. This transaction is allowed to start only after it acquires all the partition-level locks that it needs, so that a started transaction will always runs to commit. 


% The lock acquisition queues
% When a transaction request arrives, the DBMS assigns it a timestamp and then adds it to the wait queues for the locks of its target partitions. 
% all of the lock acquisition queues for its target partitions.  


% converge on a common two-phase concurrency control protocol design. 
% To address the networking issues, recent innovations in SD architectures have 

% Contrast with SN architecture, the shared-data architecture allows every transaction can access and modify all data stored in any nodes~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, fasst, drtmh, grappa_atc15, mtcp_nsdi14, drtm}. 
% There is no exclusive data ownership, so that a multi-partition transaction can be executed and committed by a single worker thread. The system is horizontally scaled up. 
% Since networks do not inherently provide the coherence support, the worker thread should check whether its accessed remote data has been changed by other concurrent transactions after it accesses. As shown in Fig.~\ref{fig:bg-scaling}~(b), the worker thread locating on node-0 first fetches the remote record D and E to its local buffer and apply writes D locally. When the time it wants to commit, it should check if other transactions have changed E by validating its version at node-1's transaction manager. 
% Given that exchanging messages between servers lies on the critical path of transaction execution, SD architectures are much sensitive to network latency~\cite{dbx1000_dist_vldb17}. Thereby recent works explore the usage of advanced networking techniques, including but not limited to user-level network stacks bypassing OSs~\cite{mtcp_nsdi14}, offloading protocol processing to SmartNICs~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, remote direct memory access (RDMA)~\cite{farm, myth_vldb17}, and Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc}. They also exploit compiler improvements such as  co-routines~\cite{fasst, drtmh, grappa_atc15} for efficient context switching, as well as programming tricks such as shared connection structures~\cite{fasst} and packet batching~\cite{grappa_atc15, guideline_atc16} to optimize performance. 

% \noindent \textbf{Client-direct} technique~\cite{farm, compromise, drtm} assigns all operations to the transaction coordinator thread. Contemporary systems utilize one-sided RDMA verbs, bypassing remote CPUs for object access. Since writes to remote objects are not inherently coherent, 
% % a common way establishes separate concurrency control for local and remote transactions. To bridge the consistency gap between inter-/intra-host communication, a common way establishes separate concurrency control for local and remote transactions. For example, 
% DrTM\cite{drtm} uses one-sided ATOMIC operations for remote locks and hardware transactional memory (HTM) for local locks. The system harmonizes HTM and network I/O discrepancies through a pessimistic concurrency control strategy, which locks remote writesets and fetches them locally, followed by starting the HTM.




% it offers good computational expressivity, completing i. This has maded it a versatile solution for all transaction processing phases~\cite{farm, fasst, grappa_atc15, calvin, traditional_txn}. Nevertheless, it consumes significant remote CPU resources for tasks like message processing and thread scheduling.


% It also introduces software overheads associated with and potential cache pollution. 
% \wz{A quantitive analysis} 

% This approach simplifies programming since all data accesses occur locally, enabling potential conflicts to be addressed using cache-coherent shared-memory primitives, like CAS. Moreover, it reduces the network round-trips essential for intricate tasks such as index lookups. 

% \noindent \textbf{In-network Computing} technique~\cite{xenic_sosp21, concordia_fast21, netcahe_sosp17, spin_sc06, coredirect_ipdps11, percs_hoti10} offloads remote operation functions to programmable networking firmware encompassing devices like smart NICs and programmable switches. Integrated with computing units and DRAMs, these devices handle stateful operation processing independently, without necessitating host CPU intervention. By caching frequently accessed objects and metadata, these devices reduce the need for DMA to fetch from the main memory. This method offers the benefit of eliminating host CPU overheads. However, due to devices' limited computing capacity and on-chip memory, real-world systems~\cite{concordia_fast21, xenic_sosp21} only offload a limited parts of functions, such as version validation, to the networking devices. 





% However, a limitation lies in the devices' limited computing capacity and on-chip memory. It's suboptimal to fully shoehorn concurrency control logic into network fabrics. In real-world scenarios, most systems only offload functions to the network to a limited extent, retaining CPU involvement for all other works. 


% , exploiting PCIe-free low-latency communication, and enabling efficient packet handling. 

% These devices, equipped with computing units and DRAMs, support stateful operation processing without involving host CPUs. By caching frequently accessed objects and metadata within their memory, these devices reduce the need for PCIe roundtrips. In-network computing eliminates the host CPU overheads, exploits PCIe-free low latency communication, and make efficient packet handling. However, these devices only have limited computing capacity and on-chip memory, thus it's inefficient to shoehorn all concurrency control logic into network fabrics. Practically, existing systems only exploit the network function shipping to a limited extent, while using CPUs for all other works.

% At some designs, transactions with only local data accesses still need to communicate with devices to update their caches, incurring unnecessary synchronization overheads. 

% \noindent \textbf{Client-direct} technique~\cite{farm, compromise, drtm} let the transaction coordinator thread be responsible for all operations, by fetching remote object to local buffers. Modern systems exploit one-sided RDMA verbs to bypass remote CPUs when accessing remote objects. This approach has consistency issues as mentioned in Sec.~\ref{subsec:gap}. A typical solution builds separate concurrency control schemes for local and remote transactions. For instance, DrTM~\cite{drtm} employs one-sided ATOMIC operations for remote locking while resorting to hardware transactional memory (HTM) for local locking. The system manages the incompatibility between HTM and network I/O by adopting a pessimistic concurrency control strategy that locks remote writeset and fetches the remote readset at the outset, then start the local HTM. 




% When a transaction request arrives, the DBMS assigns it a timestamp and then adds it to all of the lock acquisition queues for its target partitions. 
% The execution engine for a partition removes a transaction from the queue and grants it access to that partition if the transaction has the oldest timestamp in the queue [38].

% mostly shared-nothing, or recently, shared-storage architectures. For decades, the shared-nothing architecture has been regarded as the "gold standard" in distributed DBMSs due to their high performance especially in supporting single-shard queries [55, 56]. Examples include MySQL Cluster, PostgreSQL Citus [20], Teradata, MemSQL [18], VoltDB [8], SQL Server PDW, and Greenplum.



% Shared Everything Architecture






% All memory is strictly coherent
% Memory Allocation: only k-v store, OS, runtime, etc. 


% Slowdown -- Record Size

% \cxlcache~adopts the MESI (Modified, Exclusive, Shared, Invalid) cache coherence protocol. To resolve coherency for both host and device-attached memory, the protocol uses a centralized home agent (HA), which monitors cache transactions, arbitrates cacheline ownerships, and responds to the data and ownership requests. The CXL link implements a host-centric asymmetric coherence model, in which hosts implement HAs while devices implement Device's Coherency Engines (DCOH), which serve as cache agents (CAs). Once the host records that it may cache a device's memory (known as Host Bias in the CXL specification~\cite{cxl-doc}), the device must send a request to the host to resolve coherency whenever it incurs any cache transactions, such as cache misses. Similarly, if the device caches the host's memory, the host will also send requests to the device if it has cache transactions on these addresses. The HA utilizes a snoop filter to monitor whether devices own cachelines of host memory.

% Example: 
% The device acts as a peer L2 cache, as illustrated by the red line in Figure.~\ref{fig:cxl-cache}, which depicts an example where a device experiences a cache miss while attempting to read local memory. In this example, a CXL device connects to a host CPU with two cores. When the device requests a cache read for an object X in its attached memory, a cache miss occurs. The DCOH at the device detects that the host may have one or more copies of X (\red{\oone}). Thus, it issues a cache snooping request to the host to check if any cores have made local updates (\red{\ttwo}). After receiving the cache snooping request, the home agent broadcasts a cache snooping message to all cores (\red{\tthree}). At this point, core-1 possesses a cacheline containing X, and its MESI protocol state is Exclusive. According to the MESI protocol, core-1 sends a snooping response to the home agent to declare its cacheline state and value. Meanwhile, the cacheline state of core-1 changes from Exclusive to Shared. After receiving the response, the home agent responds to the device with data snooped from core-1. Following this, the device can fill its cacheline with X and set its MESI protocol state to Shared.


% , including customized accelerators~\cite{huangfu/micro22}, memory expanders~\cite{samsung, directcxl}, smartNICs~\cite{netdam}, and other PCIe devices~\cite{cxl-doc}. CXL enables byte-addressable memory access in cache-line granularity, enabling connected devices to access host memory with cache coherency. 

% We term it as the ``reverse latency'' challenge. 

\subsection{}

% The mismatch between SIM and HCM poses the substantial challenge for the transactions running on the shared memory architecture. To resolve this, previous systems generally adopt either the \textit{eager} or \textit{lazy} version management on both software and hardware.

As depicted in Fig.~\ref{fig:motivation-over-coherent}~(d), the \textit{eager} version management performs in-place updates of transactions, avoiding the overhead to redirect updates to private buffers~\cite{dudetm_asplos17}. This scheme necessities a pessimistic concurrency control (PCC)~\cite{book} to abort other transactions reading the speculative writes. However, PCC leads to a well-known concurrency disadvantages compared with OCC~\cite{thousandcore_vldb14}, since it unnecessarily aborts the conflicts between \textbf{uncommitted} transactional reads and writes. 
On the contrary, the \textit{lazy} version management buffers all transactional memory updates in a local buffer first, as shown in Fig.~\ref{fig:motivation-over-coherent}~(e). These changes are then committed to the actual data structure when the transaction successfully commits. However, this buffering approach necessitates allocating a local buffer for every writes, and any reads of the modified data within the same transaction will be redirected to this local buffer. Such an update redirection incurs the OCC's fundamental performance drawbacks for operation remapping and buffer allocating and reclaiming~\cite{thousandcore_vldb14, mvcc_vldb17}. 


% If the transaction aborts, the issued writes will be withdrawn from making any effects.
Such a separation of write's issue time and write's effect time is the fundamental feature of transaction programming and is not restricted to optimistic concurrency control. 
In other protocols such as S2PL~\cite{mvcc_vldb17, s2pl_dataeng87, s2pl_csur81}, every transaction acquires the proper lock on the logical record before it is allowed to read or modify it, and the serialization point is reached only after all accessed objects have been visited. The writes' effect time is thus later than the writes' issued time. 


% In contrast, a cache coherence resolving within a socket only takes 49 ns, which is only 0.53x than accessing memory. 

% As a consequence, an SMP's memory access is bottlenecked by touching DRAM, but a CXL's memory access is bottlenecked by cache coherence message. 
% Fig.~\ref{fig:motivation-rdma-vs-cxl}~illustrates the baseline system architecture. The CXL-attached memory pool is plugged to the processor with PCIe slots. It maps its memory as a continuous section to the host's physical memory space, but all pages of this section are reserved in booting. We utilize linux device file and rewrite the \textit{mmap} function to explicitly allocate application data structure to the CXL-attached pool.


Where coherence happens ? 
Record, Transaction Manager



Their drawbacks: 
Coherent read/write: 
time consuming on primitive, take times [ time stack ]
multiple roundtrips

Atomics: ?

% We argument the influence of hardware primitives in designing a IMDB. 
% Traditionally, a heterogeneous, simple primitive hierarchy. 
% Check TCC, the case. 
% Fig.~\ref{fig:motivation-rdma-vs-cxl} illustrates a typical CXL-enabled architecture. Compared with networks, the CXL pool has two advantages.

\subsection{}


% \begin{figure*}[t]
%   \centering 
%   \includegraphics[width=\textwidth]{my/figs/motivation-over-coherent.pdf}
%   \caption{The conflict on consistency lies on the software isolation model and hardware coherency model. A correct returned value of a read differs. }
%   \label{fig:motivation-over-coherent}
% \end{figure*}



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.48\textwidth]{my/figs/hybrid-improve.pdf}
%   \caption{Relative throughput of hybrid consistent memory and all strongly consistent memory. }
%   \label{fig:hybrid-improve}
% \end{figure}




\subsection{Design Consideration and Motivation}

A first challenge is the mismatch between the memory coherency models desired by transactions and those provided by the primary use of CXL. This mismatch causes unnecessary causal dependencies among concurrent transactions and the software overheads to resolve them, representing a fundamental impediment to achieving high system performance.


% The first challenge is the mismatch of memory coherency model between transactions' isolation model and CXL's coherence model. Such a mismatch leads to unnecessary causal dependency between concurrent transactions, and causes the fundamental reason for transaction's notorious low performance. 

% Cache coherence protocol eagerly propagates modifications to all copies for data, making the new value globally visible to any reads succeeding the write. However, transactions maintains execution atomicity, where any modifications of a transaction should not be observed by others until the transaction successfully commits. 

For the sake of clarity, we only discuss the data consistency model of records. We eliminate the user-transparent indexes and metadata essential for executing concurrency control protocols, in order to illustrate the foundational guarantee that a transaction system seeks to offer. We leave the discussion of their design in Sec.~\ref{}.

% simplified transactional KV-store without , then we categorize the shared memory objects into two discrete parts: metadata and records. While metadata encompasses system-specific data structures essential for executing concurrency control protocols, records represent the actual stored data. Metadata typically consists of elements like versions, record locks, and logs essential for transaction durability in a standard backward OCC system. Before delving into the intricacies of metadata, it's crucial to first understand the consistency model for records. This is because the records' consistency model embodies the foundational guarantee that a transaction system seeks to offer.


% Metadata states for the system-specific data structures for conducting concurrency control protocols. In a typical backward OCC system, it includes versions, record locks and transaction's for durability. Records are what the system stores, referring to the ``value'' in a KV-Store. We first discuss the consistency model for records, since it stands for the fundamental objective that a transaction system aims to provide. 

% Judge correctness: 

% The correctness of an application is determined by its invariants, which are logical predicates that hold for all states of an application (i.e., the combined states of all application processes). The semantics for users are determined by the rate of anomalies, which are behaviors the user would not observe while accessing a single-threaded, monolithic application running on a local machine with no failures. 

\subsubsection{Software Isolation Model}

The software isolation model (SIM) establishes a contract between a service and the transactional key-value store, as shown in Fig.~\ref{fig:motivation-over-coherent}~(a). It regards when a write from a transaction can be observed by other reads.  In this work, we target a typical transaction processing system~\cite{farm, drtm, compromise, timestone_asplos20, calvin, cicadia_sigmod17, tm_book, rss_sosp21} that provides \textit{Atomicity} and \textit{Strict Serializability}. 

The principle of \textit{Atomicity} dictates that a transaction should either make all its writes atomically visible to other transactions or abort, rendering none of the writes visible. It prevents any transaction from observing an intermediate state with any speculative writes. 
Meanwhile, \textit{Strict Serializability} ensures that the system appears to execute concurrent transactions sequentially, in an order respect to the real serialization point. In the example at Fig.~\ref{fig:motivation-over-coherent}~(b), the serialization point locates within the ``Validation Phase'', and typically identifies the instant that the transaction acquires all essential locks.
% that a transaction acquires locks for the entire write set (as discussed in the ``Validation Phase'' in Sec.~\ref{subsec:ccp}), as shown in the transaction 1 at Fig.~\ref{fig:motivation-over-coherent}~(b). 
Once a transaction passes the serialization point, it prevents any concurrency reads towards its write set. The locks will be free at the end of the commit phase. 
% The transaction 1 in Fig.~\ref{fig:motivation-over-coherent}~(b) illustrates these milestones. 

% any access to its write set records should either trigger an abort or a temporary halt until the transaction installs its modified values at the commit phase. 


% The software isolation model is a contract between a service and the transactional key-value store, it regards the allowable return values for a read in a transaction. 
% In this work, we target the service with \textit{Atomicity} and \textit{Strict Serializability}~\cite{farm, drtm, compromise, timestone_asplos20, calvin, cicadia_sigmod17, tm_book, rss_sosp21}. \textit{Atomicity} means each transaction either makes all its writes visible by other transactions at once, or aborts to discard all its writes thus none of them can be seen. \textit{Strict Serializability} means transactions appears to execute concurrent transactions sequentially, in an order respect to the real serialization point. Typically, the serialization point states for when the transaction locks the entire write set (in ``Validation Phase'' mentioned in Sec.~\ref{dp}) in the backward OCC. 
% Once a transaction successfully passes the serialization point, subsequent reads or writes to its write set records should cause an abort or a stall, until the transaction broadcasts its updated values in the ``Write Phase''. 

\textbf{This transactional definition inherently renders writes as non-atomic}, so that a write is issued during the execution phase but only takes effect after the transaction passes the serialization point. As shown in Fig.~\ref{fig:motivation-over-coherent}~(b), a written value remains invisible from other transactions in this interval, and a concurrent read (R(X) from transaction-2) following SIM will retrieve the old value. If the transaction aborts, the issued writes will be withdrawn from making any effects.

Such separation of write's issue time and write's effect time is the fundamental feature of transaction programming and is not restricted to optimistic concurrency control. In other protocols such as S2PL~\cite{mvcc_vldb17, s2pl_dataeng87, s2pl_csur81}, every transaction acquires the proper lock on the logical record before it is allowed to read or modify it, and the serialization point is reached only after all accessed objects have been visited. The writes' effect time is thus later than the writes' issued time. 

% only if all read/write set is locked. 

% The definition of transactions makes writes to be non-atomic: a write is issued in the read phase but the write takes effects at the Write Phase. During the interval between them, the written value should remain invisible to other transactions thus concurrent reads will yield with the old value. 



% Moreover, an issued write is possible to be withdrawn if the transaction aborts. Consequently, an object will 


% conflicted written values to the same object will co-exist , until all of them reaches the 


% The execution time of a transaction states for the moment it locks the entire write set (in ``Validation Phase'' mentioned in Sec.~\ref{dp}), which is also termed as the ``serialization point''. It ensures a transaction observing updates from others with preceding serialization point. It can be summarized as the following memory orders: 
% It ensures any operations after a transaction passes its serialization point must observe the effects of the transaction. To this end, the transaction system should hold the two orders: 

% Another important feature is \textit{Atomicity}. A transaction states for a series of reads and writes. Atomicity guarantees each transaction either makes all its writes visible by other transactions at once, or aborts to discard all its writes thus none of them can be seen. This prevents a read from returning uncommitted writes. It thus hold the following orders: 


% In this work, we target a common model~\cite{farm, drtm, compromise, timestone_asplos20, calvin, cicadia_sigmod17, tm_book, rss_sosp21} with atomicity and strong serializability. A transaction states for a series of operations. \textit{Atomicity} guarantees each transaction either completes all of them at once, or aborts to discard all its updates; 

% ithin the system must be identical to a result in which these transactions executed serially, and the order respects to the real execution time.  

% \textit{Isolation model} specifies the legal ordering of conflicted reads and writes on the shared object. This work targets the strict serializablility~\cite{farm, drtm, compromise, timestone_asplos20, calvin, cicadia_sigmod17, tm_book, rss_sosp21}, which states that the result of executing concurrent transactions within the system must be identical to a result in which these transactions executed serially, and the order respects to the real execution time. 

% Forgets some variants
% Some systems maintain a more relaxed model that permit more conflicts between reads and writes to trade higher parallelisms~\cite{check-}. However, they offer application programmers with a harsh tradeoff. For example, snapshot isolation ensures non-abort read-only transactions, but it cannot be used for developing data structures without addressing the write skew anomaly; process-ordered serializability~\cite{dns} 
% tolerates long communication delays and network partitions, but a read from a thread will return its subsequent write results.


\subsubsection{Hardware Coherency Model}

The granular operations executed by transactions eventually translate into a series of loads and stores toward memory systems, as shown in Fig.~\ref{fig:motivation-over-coherent}~(a). The hardware coherency model (HCM) deals with the execution order of these individual memory operations that toward the same object. 
% It's typically designed for general applications, and it does not distinguish whether the memory operation is issued by a transaction. 

% , which might originate from a single transaction or span across multiple ones. 

% Operations made by transaction software are finally implemented with multiple loads and stores submitted to memory systems. Hardware coherency model deal with the order of every single memory operations, which may reside within a transaction or in different transactions. 

CXL adopts a \textit{strict coherence} model~\cite{munin_ppopp90, rtm_isca14} based on the MESI protocol. In such a coherence model, \textbf{a write atomically takes effect at the time it is issued to the memory system.} This implies that the precise moment a store request is written to the cache corresponds to the instantaneous processing of the store within a monolithic memory abstraction~\cite{rvweak_isca18, spandex_asplos18}. The outcome of a read operation with HCM always return the data from the most recent write to that specific object. We show the detailed procedure of this example in Fig.~\ref{fig:motivation-over-coherent}~(c). The read from transaction-2 returns with the value of the write from transaction-1. This feature is termed as the ``write atomicity'' when referring to a memory consistency model~\cite{rvweak_pact17, armcm_cmb12, powercm_pldi11, rvweak_isca18, in-memory-transactions}, and it's held in most SMP coherent systems, such as SC, TSO, RMO, Alpha, and ARMv8. 

% In such a cache hierarchy, a store is atomic and instantaneous, so that the moment a store request is written to the cache corresponds to processing the store instantaneously in the monolithic memory abstraction; and the value returned by a read operation is the value written by the most recent write operation to the same object. This model resembles most SMP coherent systems, e.g., SC, TSO, RMO, Alpha, and ARMv8, to tolerate generous applications. 

% CXL incorporates a \textit{strict coherence} model~\cite{munin_ppopp90}, rooted in the MESI protocol. Within such a cache structure, a store operation is both atomic and immediate. This implies that the precise moment a store request populates the cache equates to the instantaneous processing of the store within a unified memory abstraction. Additionally, the outcome of a read operation always mirrors the data from the most recent write to that specific object. This model mirrors the coherency found in many SMP systems, such as SC, TSO, RMO, Alpha, and ARMv8, designed to accommodate a broad range of applications. 

% and the moment a load request gets its value corresponds to the instantaneous processing of the load in the monolithic memory. 
% , meaning the issue and effects is atomic, and the value returned by a read operation is the value written by the most recent write operation to the same object



% In this model, the value returned by a read operation is the value written by the most recent write operation to the same object. To this end, writes in CXL are atomic and instantaneous, which will be advertised to all processors simultaneously. Most SMP systems adopt a strict coherence model, 


\subsubsection{Drawbacks of Existing Approaches}

% 这种mismatch会带来什么样的问题，主要考虑Software的实现，The tradeoff between overheads (optimistic, MVCC) and concurrency (pessimistic concurrency control)
% 我们应该重点关注保存speculative write的overhead
%   Overhead：Buffer Allocation/Reclaim，Buffer Hash index？   从什么角度来分析这个Overhead？
% Hardware的解决方法Review：过于激进 + 实现限制（我们是否能够解决）
%   修改Cache Coherence Protocol，Integrate Circuits to Hosts

The mismatch between SIM and HCM poses the substantial challenge for the transactions running on the shared memory architecture. To resolve this, previous systems generally adopt either the \textit{eager} or \textit{lazy} version management on both software and hardware.

As depicted in Fig.~\ref{fig:motivation-over-coherent}~(d), the \textit{eager} version management performs in-place updates of transactions, avoiding the overhead to redirect updates to private buffers~\cite{dudetm_asplos17}. This scheme necessities a pessimistic concurrency control (PCC)~\cite{book} to abort other transactions reading the speculative writes. However, PCC leads to a well-known concurrency disadvantages compared with OCC~\cite{thousandcore_vldb14}, since it unnecessarily aborts the conflicts between \textbf{uncommitted} transactional reads and writes. 
On the contrary, the \textit{lazy} version management buffers all transactional memory updates in a local buffer first, as shown in Fig.~\ref{fig:motivation-over-coherent}~(e). These changes are then committed to the actual data structure when the transaction successfully commits. However, this buffering approach necessitates allocating a local buffer for every writes, and any reads of the modified data within the same transaction will be redirected to this local buffer. Such an update redirection incurs the OCC's fundamental performance drawbacks for operation remapping and buffer allocating and reclaiming~\cite{thousandcore_vldb14, mvcc_vldb17}. 

% and is prone to livelock due to repeating mutual aborts. 

% unnecessary aborts due to conflicts between \textbf{uncommitted} transactional reads and writes. Moreover, it heightens the risk of livelocks, arising from repeated mutual abort scenarios.

% Due to the mismatch between SIM and HCM, existing single-host in-memory transaction processing system implements either \textit{eager} or \textit{lazy} version management schemes to keep the ACID feature of the record. As illustrated in the Fig.~\ref{fig:version-management}, eager version management enables transactions to perform in-place updates, avoiding the overhead of redirecting updates~\cite{dudetm_asplos17} to the private buffers. However, it limits a pessimistic concurrency control (PCC)~\cite{book}, which comes at the cost of unnecessary aborts on conflicts between \textbf{uncommitted} transactional reads and writes, and is prone to livelock due to repeating mutual aborts. 


% To mitigate this issue, MVS2PL 
% However, this solution requires \wz{MVCC overheads}. 
% However, the inherent strict coherence model of CXL poses challenges when juxtaposed against the stipulations of the software isolation models.
% TCC [11], [17] and Bulk [12], [21] overcome this problem by detecting conflicts lazily before transaction commit.



% How to resolve them ?
% Although the object based [3, 31, 42] or block-based [40, 54]  can alleviate the cost by increasing the granularity of redirection 



% SI-TM utilizes lazy conflict detection, performing validation at commit time in contrast to eager conflict detection schemes that check for conflicts on every transactional memory access. Lazy conflict detection is preferable as it guarantees forward progress and mitigates the impact of certain conflicts [10], whereas eager conflict detection is prone to livelock due to repeating mutual aborts. Previous works including TCC [25], Bulk [13] and FlexTM [54] have shown that lazy implementations are able to outperform eager systems. SI-TM further improves on existing lazy systems by introducing a new validation technique based on timestamps.

% For example, DudeTM [57] and Mnemosyene [85] scale poorly because of the underlying STM, which is known for its poor scalability [10]. They extend STM with an extra durability layer, which incurs a high write amplification (∼4-7×), as shown in Figure 1 and Table 1 in the course of guaranteeing crash consistency.
% timestone_asplos20

% HTM
% Multiversioned Page Overlays: Enabling Faster Serializable Hardware Transactional Memory
% Hardware Acceleration of Software Transactional Memory

% Implementing transactions in software, as a consequence, comes with the dilemma between the drawback of limited transaction concurrency and the cost of maintaining a private buffer per transaction. It motivates the usage of commercially hardware transactional memory (HTM)~\cite{drtm, dhtm_isca18, dudetm_asplos17} to delegate the process of both version management and concurrency control to hardware. Taking Intel's Restricted Transactional Memory (RTM)~\cite{rtm} as an example. RTM buffers the speculative state in private L1 cache, and detects read-write conflicts with the help of cache coherence substrate. Specifically, it tracks transactional reads and writes with additional bits in L1 cachelines, and trigger an abort once those cachelines is evicted. However, this approach eventually conducts PCC so that limits concurrency, as well as poses an limitation on transaction capacity to L1 cache size. The write set overflow will unconditionally abort the transaction. Although other HTMs such as LogTM~\cite{logtm_hpca06}, OverlayTM~\cite{overlaytm_pact19}, FlexTM~\cite{flextm_isca08}, TCC~\cite{tcc_isca04}, and VTM~\cite{vtm_isca05} proposes various approaches to mitigate these limitations, they change the cache designs in processors, and integrates considerable logics into circuits. These changes are too ambitious for current CPU architectures and thus lack feasibility in today's systems. 


Consequently, software-based transaction implementation confronts a trade-off: the limitations of transaction concurrency versus the overhead of redirection. This dilemma has motivated hardware transactional memory (HTM)\cite{drtm, dhtm_isca18, dudetm_asplos17} as a means to delegate both version management and concurrency control tasks to hardware. Taking Intel's Restricted Transactional Memory (RTM)~\cite{rtm} as an example. RTM stores the speculative state in a private L1 cache and leverages cache coherence mechanisms to detect read-write conflicts.  Specifically, RTM tracks transactional reads and writes with additional bits in L1 cachelines, and aborts the transaction once those cachelines are touched by other cores. This method, however, inherently limits the transaction capacity to the L1 cache size, since any cacheline eviction will abort the transaction. Besides, RTM resembles PCC, which prevents the concurrency between uncommitted reads and writes. Despite alternative HTM solutions like LogTM~\cite{logtm_hpca06}, OverlayTM~\cite{overlaytm_pact19}, FlexTM~\cite{flextm_isca08}, TCC~\cite{tcc_isca04}, and VTM~\cite{vtm_isca05} propose diverse strategies to address these constraints, they necessitate changes in processor cache designs and the integration of extensive logic into circuits. Such radical modifications are overly ambitious for recent CPU architectures, rendering them impractical in today's systems.  


% \subsection{Challenge-2: Latency Reverse Problem}

% \wz{TBD}

\begin{figure}[t]
  \centering 
  \includegraphics[width=0.48\textwidth]{my/figs/pool2.pdf}
  \caption{Latency characteristic of CXL-enabled shared memory pool.}
  \label{fig:motivation-cxl-latency}
\end{figure}

\section{Insight and Design Choices}

Caches inherently provide data buffering, but coherence protocols normally propagate modifications quickly to all copies. As in most hardware transactional memory proposals [1, 8, 23, 28], we allow a thread to delay this propagation while executing speculatively, and then to make an entire set of changes visible to other threads atomically. We use a level of cache close to the processor to hold the new copy of data, and rely on shared lower levels of the memory hierarchy to hold the old values of lines. Unlike most other hardware TM designers, however, we allow lines to be read and written transactionally even when they are also being written by some other, concurrent transaction.

% . They interface with hardware buffers to speed up logging [39, 80, 95] or delegate the process of ordering stores to hardware [22, 50, 51, 64], clearly demanding significant hardware changes and introducing new logging instructions. Some approaches in this class propose extending hardware transactional memory (like Intel RTM) for making atomic updates to NVMM [38, 38, 53]. The performance of these techniques are bound by the L1-L3 cache size and requires changes in the existing cache-coherence protocol [38].

% Hardware transactional memory (HTM) [23] provides a lowoverhead tool to detect conflicts. The latest commercial processors support a version of HTM called restricted transactional memory (RTM) [27]. HTM-based designs improve transaction processing speed [37, 59, 60, 61]. Although HTM is promising for both uncontended and contended workloads, we focus on general concurrency control schemes that are applicable to a broader range of systems.



% [15], [16], [17], [18], [19] which motivated the development of the second class of designs that either employ hardware support for atomic durability [15], [17], [20], [21], [22], [23] or leverage hardware support for ordering to guarantee atomic durability [16], [18], [24], [25].

% The first of these is the inherent concurrency available in a workload using
% transactions: what is the optimal way in which a TM system could schedule the transactions, while
% providing specified semantics (e.g., TSC)? This provides a bound on the possible behavior that a
% practical TM implementation could provide. Some workloads provide a high degree of inherent
% concurrency—e.g., if all of the transactions access different pieces of data

% This mismatch lies on where a write should take place ? 
% Software isolation model: At the serialization point
% Hardware Consistency Model: At when it takes place

% TSC, SLA
% They're slow: why? They're stick to the message passing, loosely coupled devices. The gap between sharing data across hosts, and share data within a host: software perspective (heavy driver, OS), hardware perspective (Queue). 
% How such difference effects transaction processing ?
% Slow, limited primitive capability. 
% (See ASPLOS-ver 3.1)


% CXL v.s. network: 
% software perspective: unified coherent mem space, with byte-addressable, guaranteed by HW, efficient atomics. 
% hardware perspective: tightly coupled with CPU coherence protocol, thus no queue-based data sharing. 
