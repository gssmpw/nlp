
\section{Motivations}   \label{sec:motivation}
% , so that they involve multiple machines and shard the dataset on them based on the some shared features. 
% Distributed KVS includes multiple nodes connected with networks that enable larger dataset and provide higher throughput. 
% The dataset is partitioned based on the primary key, and each node accomodates one partition. 

% it can support independent scaling of compute and storage, better elasticity, and fast crash recovery
% Despite the network communication especially in supporting single-partition queries.
% A transaction may manipulate records locating on any nodes, hence networking is required during the transaction process.  

% This work targets distributed KVS inside a rack where a single instance of the system can scale to 8-16 nodes. 

% In this work, we observe introducing CXL's cache coherence capability will bring unique advantages and challenges for the transaction processing systems. These challenges do not exist in any existing architecture thereby requiring novel solutions. 

% In this work, we discuss on the basic accessing primitives for CXL shared memory. Despite the variety of transaction systems differ in algorithms like concurrency control, logging, and indexing, they all depend on the underlying storing fabric to provide the basic accessing primitives, typically including loads, stores, and synchronization operations such as fence and atomics. However,
 
% the vanilla primitives provided by CXL memory sharing introduce an universal performance drop on various uses cases (Sec.~\ref{subsec:motivation_eval}). 
% We observe that the CXL-vanilla suffers from a universal performance drop (Sec.~\ref{subsec:motivation_eval}) on various use cases and system policies. With in-depth analysis on its coherence model (Sec.~\ref{subsec:vanilla_primitive}), we observe the large remote cache signaling cost (Sec.~\ref{subsec:remote_fetching}) and snoop filter scalability issues (Sec.~\ref{subsec:snoop_filter}). We further question the necessity of the cache coherence in transaction processing (Sec.~\ref{subsec:overkill}) and motivates our work. 

% By performing an in-depth analysis (Sec.~\ref{subsec:vanilla_primitive}), and found the CXL's vanilla primitives have two main performance drawbacks (Sec.~\ref{subsec:remote_fetching} and Sec.~\ref{subsec:snoop_filter}) for transaction processing applications. 

% incur the large coherence cost when we share the CXL memory pool among nodes. 


% The correctness and optimization strategies rely on the rules of how the storing fabric could order these primitives. 
% to keep the illusion of a single cache coherent memory. 

% the keeping the hardware cache coherence across nodes.
% lead to the significant throughput degredation 


% For the sake of simplicity, we call the primitives that CXL specification describes as ``CXL-vanilla'', which is the primitive that is introduced in the CXL specifications. We describe and evaluate the coherence model of CXL-vanilla (Sec.~\ref{subsec:vanilla_primitive} and Sec.~\ref{subsec:motivation_eval}), and make detailed analysis on its overheads (Sec.~\ref{subsec:remote_fetching} and Sec.~\ref{subsec:snoop_filter}). 



\subsection{Scaling Transactions with Coherent CXL} \label{subsec:vanilla_primitive}


To investigate how current transaction systems perform under the CXL's shared memory, we deploy the popular transaction processing framework DBx1000~\cite{abyss_vldb14} on the G-FAM. We defer the evaluation details to Section~\ref{subsec:setup}. Figure~\ref{fig:cxl_slowdown_breakdown} illustrates the performance of six commonly adopted concurrency control algorithms on the TPC-C benchmark~\cite{tpc-c}. The numbers are normalized to an \textit{oracle} architecture that operates on local DRAM and consolidates all worker threads within a single socket. We group the time components based on the memory modules accessed by the transactions. It is evident that CXL memory sharing slows down all algorithms by 2.51x on average, with 46.89\% of the time spent on accessing G-FAM.
Algorithms that maintain multiple tuple versions, such as \textit{MVCC}, experience the most significant performance loss, since they require more CXL memory accesses to create new tuple versions and reclaim the stale ones than those maintaining only a single tuple version. 


\ifx\undefined\stale
Due to the lack of CXL 3.0 IP, we enable memory sharing by hacking the IP's address translation to fold a half of physical addresses to map to the same DRAM address of another half. We implement an LRU SF and emulate the \cxlbi~with \cxlcache~since they have similar protocol constitutions. We leave the details to Sec.~\ref{subsec:setup}. We employ the popular transaction processing framework DBx1000~\cite{abyss_vldb14} and modify its memory allocation logic to locate the shared data structures such as indexes and tuples at the G-FAM, while preserve the thread-private data on the local DRAM. 
\fi

\subsection{CXL's Coherence Cost}  

CXL's memory sharing, along with most caching architectures~\cite{rvweak_pact17, armcm_cmb12, powercm_pldi11, rvweak_isca18, gam, txcache_osdi10}, adopts the \textit{strictly coherent} cache model~\cite{munin_ppopp90, rtm_isca14} to simplify software development. The basic design principle of this model is to ensure a single, global order for all memory locations. A memory store in the strictly coherent model is only completed once all peer nodes have observed it. This order is also known as the single-writer-multiple-reader invariant~\cite{book_cc}, which allows only a single node to execute a write primitive on a location or multiple nodes to execute read primitives at the same time.
In subsequent discussions, we refer to the primitives that adhere to this coherence model as ``CXL-vanilla.''


Maintaining such a coherence model requires complex hardware logic to send coherence requests and track the lists of sharing nodes. In CXL, this is achieved through an MESI-like ownership-based (write-back) coherence and directory-based communication~\cite{spandex_asplos18, book_cc, denovo_pact11, quantitative_approach}. 
Write-back coherence means that a cache write is not immediately reflected in the DRAM but instead marks the cacheline as dirty. Consequently, a cache read miss retrieves the content from a remote dirty cache rather than directly from the memory.
Directory-based architecture implies that the hardware maintains a directory to track which caches hold each cacheline and their states. A cache controller intending to issue a coherence request first sends it to the directory to determine which node is holding the peer caches. Such a directory is implemented as a Snoop Filter (SF) in CXL specifications.


% In CXL specifications, the directory is implemented as a snoop filter (SF) by Intel processors. 

% The order is also called the single-writer-multiple-reader invariant, resembling the read-write locking semantic in the software context. 
% , this invariant enables only a single node that may execute write primitive to the location or multiple nodes that may execute read primitives the location. 
% The model is pessimistic so that it addresses data races at the point they happened, by only allowing one version at a time.  



% The write-back coherence means that a cache write or an atomic operation are not immediately reflected on the DRAM. The cache marks the modified data as dirty and updates the memory only when the cacheline is evicted. 
% if the remote cache is dirty and has not been written back, and a cache write would trigger a read request that invalidates all peer caches first, then follows with a local write cache hit. 
% , and the directory looks up the state of the cacheline to determine what actions to take next, such as retrieving the remote cache contents. 

% a cache write would trigger a read request for the exclusive permission first, then follows with a write cache hit. This process incurs invalidating remote caches to ensure they will not get the stale data. 
% In this design, a cache write would trigger a read request for the exclusive permission first, then follows with a write cache hit. This process incurs invalidating remote caches to ensure they will not get the stale data. 

% The CXL \textit{strictly coherent} primitives provide great convenience for software programmers to write code as if on the same node. However, both the write-back protocol and the directory introduces non-negligible performance drawbacks that have not been comprehensively discussed. 

% \noindent\fbox{%
% % \centering
%   \parbox{0.47\textwidth}{%
%      \textbf{{Take-away\#1:}} The CXL-vanilla primitive requires remote cache signaling mechanism and snoop filter to ensure its strictly coherent semantic.
%   }%
% }

% may retrieve the remote dirty cache since memory contents could be stale while the cache are always up-to-date. 
% Writer-initiated invalidation means that every read miss triggers a request for read permission in the form of Shared state
% This permission is revoked only when a future write to the line sends an explicit invalidation to the reader. 
% RfO caching means that every cache miss for a write or a atomic operation triggers a request for exclusive write permission in the form of ownership. 
% The CXL implements the \textit{strictly coherent} primitives via a MSI-style protocol, and are conducted at the cacheline granularity. CXL implements the writer-initiated invalidation and read-for-ownership (RfO) caches~\cite{spandex_asplos18, book_cc}. Writer-initiated invalidation means that every read miss triggers a request for read permission in the form of Shared state. This permission is revoked only when a future write to the line sends an explicit invalidation to the reader. 
% RfO caching means that every cache miss for a write or a atomic operation triggers a request for exclusive write permission in the form of ownership. It causes invalidation on every core that may have read or write permissions for the target cache line. The write in CXL is silent on the local caches, so that a read miss should query the peer cacheline that has the ownership to check if it has updated the value. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.48\textwidth]{my/figs/rdma-vs-cxl.pdf}
%   \caption{(a) is baseline architecture with two nodes and one CXL EP. Each node directly connects to the EP with CXL x16 link. 
%   (b) shows the memory hierarchy of the baseline architecture. }
%   \label{fig:prototype}
% \end{figure}

% We adopt the linux with 6.6.0 kernel version. 
% The memory pool is mapped to a process virtual address space in a DAX mode ~\ref{subsubsec:system_integration}. 

% The hardware architecture supports atomic operations with the same cacheline locking scheme as within a single node: 
% the processor acquires an exclusive ownership of the cacheline that the operation writes, then
% check if the cacehline is still in exclusive status after the operation is done. 

% We store all KVS tuples and the metadata to the CXL device, in order to leverage CXL's memory capacity and fast hardware coherence. We store the temporary stacks, heaps, and local buffers in the local DRAM since they are thread-private data structures.  


% This system allows processors on different nodes to access the shared KV dataset on the remote memory pool as conventional DRAM, where writes are automatically synchronized across nodes in hardware, as shown in Figure~\ref{fig:prototype}~(b). Atomic operations, such as compare-and-swap (CAS) and fetch-then-add (FA), are implemented via the 
% \wz{Concurrency Control not equals to Distributed Transactions, we should defend ourselves. }
% It necessitates us to rethink the design choices of CXL-augmented concurrency control schemes. 
% However, its novel programming scheme and performance characteristics necessitate us to rethink the design choices of distributed concurrency control schemes. 
% This work focuses on the concurrency control schemes since it contributes to most transaction execution time, as well as mostly benefits from the CXL's cache coherence capability. 
% We evaluate various widely-used concurrency control approaches. 
% including the variants of pessimisitic two-phase locking~\cite{s2pl_csur81, abyss_vldb14}, optimistic timestamp ordering~\cite{silo_sosp13, hekaton_sigmod13, occ_tbs81, farm_nsdi14}, and modern optimization includes TicToC~\cite{tictoc} and Silo~\cite{silo_sosp13}. 
% We base our code on DBx1000~\cite{abyss_vldb14} and modify the memory allocator to leverage the CXL memory pool. 
% We adopt a Silo~\cite{silo_sosp13} style batched logging and batched garbage collection for all test cases, they work on individual threads to keep off the transaction critical path. 

% Note that we focus on the CXL impact on existing concurrency control, but not aim to figure out which algorithm performs best, since the best practice always depend on the application features, such as access skewness, read-write ratios, and record size~\cite{polyjuice, ic3, others}.


% We can also observe transactions with lower contention exhibiting more severe performance loss than those with higher contention, because aborts mostly work on local memory data structures. 

% and manipulating with lock manager or increasing the timestamp. 
% time spent on CXL pool includes copying the tuples to/from local buffer and manipulating with lock manager or increasing the timestamp. 
% We can tell in a broad test cases, accessing the CXL memory pool significantly leads to the performance loss, especially for those algorithms maintaining multiple versions of a tuple, such as \textit{MVCC}. 
% , especially when the access patterns have with low contention. 

% the un-contended transactions rarely abort, so that they spend most time in the execution phase that accesses remote memory, while the abort procedure works on local memory data structures, such as freeing store buffers. 



% ``Memory Layer'' refers to the time of accessing the data structure shared by all workers. 
% It includes copying the tuple to/from local buffer and manipulating with lock manager or timestamp manager. This part is directly effected by CXL's overheads. 
% ``Transaction Manager'' indicates the time that the transaction spends on local concurrency control tasks, including allocating and cleanup the buffer, rolling back abort transactions, and waiting for the locks. 
% Its corresponding data all locates at local DRAM, so that it excludes any time that actually touches the shared data structures. ``Useful Work'' indicates the time spent on traversing the index, filling the logging buffer, retrieving queries from the client, and the time that the transaction is actually executing application logic.  
% The DBMS neither partitions the dataset, nor co-locating the related transactions on the same node. 
% We detail our implementation at Sec.~\ref{sec:implementation}. \textbf{}



% \wz{The gap between our prototype and the real } 

% re-examine the distributed txn ideas on cxl-augmented: partitioned, shared-records, software-cached

% The worker threads thus use an additional local buffer to temporally store transaction's speculative writes. 

% CXL exhibits unique performance characteristics and programming models, hence directly employing existing multi-node transaction systems is hard to fully exploit CXL's benefits.

% especially within a rack where nodes are conventionally connected by networks in a TOR. It's promising 
% Distributed transactions in multi-node environments are notorious for performance drawbacks, primarily due to the cost associated with cross-node networking. 
% due to the foundamental difference on performance characteristics and programming model,

 

% \subsection{Choice of Architectures}



% \noindent \textbf{CXL-augmented Shared-Data Architectures. }
% The SDA overcomes the SNA's bottlenecks at the cost of higher communication cost. 
% % SDA can overcome the bottleneck of partition-level serialization by allowing a transaction to access tuples on any partitions and validates conflicts in the more fine-grained tuples. This flexible approach however increases the latency significantly since it requires network messages on the critical path. 
% Figure~\ref{fig:sna-sda}~(b) shows the latency stack of a network-based SDA. We adopt the widely used distributed OCC~\cite{farm, fasst, drtm, drtmh} with remote procedure calls (RPCs).
% % to traverse remote index and fetch the record in one network roundtrip. Updates are buffered locally, validated and committed with one RPC each. 
% % We base our code on deneva~\cite{dbx1000_dist_vldb17} to leverage its carefully optimized transaction runtime, details are provided in Sec~\ref{sec:implement}. 
% % with 82us network latency on average. 
% One can observe from the results that a transaction spend over 35\% time on queueing remote requests and transmission on networks, but the actual processing time only takes less than 1\% time on all benchmarks. 

% One can tell from the results that the baseline system is not yet efficient for cross-host transaction processing when compared with oracle. \wz{Results analysis}

% \noindent \textbf{CXL-augmented distributed transactions: }

% To avoid partition-level serializing bottleneck and performance slowdown of networks, this work advocates to communicate hosts with a shared memory pool managed by CXL. 


% \noindent \textbf{Low-Latency Data Sharing:} CXL enables data accesses for a 64-byte cache line with the roundtrip latency to sub-microseconds. Such a low latency, fine-grained data sharing fits distributed transactions, which make frequently byte-level synchronization and is dominated by the delay of networks. CXL benefits from two protocol advantages. First, CXL directly fetches data to the host LLC, eliminating the memory copy between NIC buffers and main memory~\cite{pond, directcxl}. Second, the processor waits for the result in a synchronous, instruction-level way that resume the processor's pipeline as soon as the instruction returns, while the communication between NICs and the processor is asynchronous, requiring either busy pooling or OS-intervened interrupts on retrieving the results. 

% \noindent \textbf{Software Transparent Cache Coherency:} CXL enables memory attached to be cacheable (referred to as ‘Host managed Device Memory’, HDM), similar to the host memory, resulting in a processor's unified physical memory space across HDM and host memory. Operating systems thus transparently exploit the HDM by reusing virtual memory mechanisms. A typical system exposes HDM as a zNUMA node~\cite{tpp_asplos23, pond, mem_tunnel}, user-level applications thus are able to map applications to HDM leveraging existing NUMA balancing mechanisms. The programmers are free to differentiate accesses to whether host memory or remote HDM in application codes. On the contrary, networks requires applications to distinguish remote data accesses with explicit API calls



% In order to keep the strict coherence model, the CXL-vanilla primitive introduce the remote cache signaling process to invalidate a remote cacheline for exclusive ownership requirements, or retrieve the contents for a read miss. 
% % , which could be an remote cacheline invalidation or an remote cacheline content retrieval. 
% The remote cache signaling occurs when two or more nodes access the same cacheline, with at least one of the accesses being a write or being a read cache miss. 

% how CXL keeps coherence for a read miss that has the ownership on another node. 
\ifx\undefined\stale
A read miss starts with a memory read request from node-2 to the EP. 
When the EP receives the read request, it checks its internal directory and finds out node-1 is holding the copy. 
In case the node-1 has modified X, EP needs to send a back invalidation message via \cxlbi~to node-1 to get its X content. 
Node-1 thus responses its modified X to the EP, and the data is further forwarded to node-2, thereby resolving the coherence across node-1 and node-2. 
\fi
% Extensive works contribute to mitigate this overhead by treating the CXL as a tiered memory and carefully swapping hot and code pages~\cite{tpp,}, or treating CXL as a flattened memory extension and allocating latency insensitive applications to it~\cite{pond}. 
% However, existing work does not consider if the CXL memory is shared among multiple nodes. 
% Memory sharing introduces another important overhead on the cache coherence. To be precise,
% CXL memory is naturally cached by the processors, but maintaining the inter-node coherence require the CXL device acting as the switch to fordward cache coherence transactions. 
% between hosts significantly enlarge the LLC cache miss penalty. 
% It's because CXL not only introduces latency overhead to convert protocols from memory to CXL flits~\cite{anns, directcxl}, but also exhibits unique bottlenecks on coherence messages. 
% CXL's SMP-like memory consistency model is maintained with considerable time cost. 
%  to the memory request

% \red{Name: ownership ? Coherence Traffic, Sharing (read/write) }
\subsubsection{Remote Cache Signaling Cost} \label{subsec:remote_fetching}

In order to maintain the strict coherence model, \vanilla~introduces a remote cache signaling process to invalidate a remote cacheline for exclusive ownership requirements or to retrieve the contents for a read miss. 
Figure~\ref{fig:motivation-over-coherent}~(a) illustrates an example where a cache read miss on node-2 retrieves a modified cacheline from node-1 via the CXL fabric.
This process involves two CXL request-response roundtrips between the host and the G-FAM node: one for node-2’s read request to the G-FAM, and another for the read request from G-FAM to node-1. Each CXL roundtrip is designed to take about 75ns with an ideal ASIC implementation~\cite{pond, tpp_asplos23, directcxl, cxl-shortdoc}, 
\rvs{
and the currently available FPGA-based platform exhibits an average latency of approximately 400ns~\cite{cxl-fpga-doc, cxl_demystify, hydrarpc_atc24}. We develop a real-world prototype system utilizing these off-the-shelf hardware. As illustrated in Table~\ref{tab:latency}, the remote cache signaling roundtrip takes approximately 850ns, which is 3.4 times greater than the latency of an ideal ASIC implementation. This gap is primarily due to the low operational frequency of FPGA's CXL IP and the intrinsic overhead associated with this FPGA's chiplet architecture. 
We will detail this prototype in Section~\ref{subsec:setup}.
% \rvs{and the currently available FPGA-based platform takes approximately 400ns~\cite{cxl-fpga-doc, cxl_demystify, hydrarpc_atc24}. We build a real-world prototyped system based on these off-the-shelf devices from Intel. Table~\ref{tab:latency} shows the performance. The remote cache signaling roundtrip takes 847ns on average, which is about 3.5$\times$ larger than an ideal ASIC implementation. This is mainly attributed to the low frequency of its CXL IP and the inherent overhead of this FPGA's chiplet design. We will detail them at Section~\ref{subsec:setup}. 
}




% The request from the host to EP insists the host RC converting the request to CXL flits and reverting them to the memory request at the EP. 
% The responses also requires this conversion the inverse order~\cite{cxl_anns_atc23, directcxl}. 

\begin{table}
\caption{Roundtrip Latency Comparison}\vspace{-5pt}
\label{tab:latency} 
\resizebox{0.47\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
             & Socket & NUMA  & CXL (Ideal) & CXL (Proto) \\ \hline
Core-to-Mem  & 92ns       & 145ns & 170ns                 & 456ns                 \\ \hline
Core-to-Core & 49ns       & 133ns & 246ns                 & 847ns                 \\ \hline
C2C/C2M    & 0.53x         & 0.91x    & 1.44x             & 1.85x                 \\ \hline
\end{tabular}%
}
\end{table}



\ifx\undefined\stale

\begin{figure}[t]
%   \centering
  \includegraphics[width=0.48\textwidth]{my/figs/cc2read.pdf}
  \caption{The stack indicates cycle-per-instruction on memory accessing. The line indicates the CXL packets per transaction. \red{G-FAM}}
  \label{fig:cc2read}
\end{figure}


To illustrate the performance effects of remote cache fetching, we employ a simpler benchmark YCSB~\cite{ycsb} that enables us to directly tune the read and write ratios. The memory accesses follow the zipf distribution controlled with a skew factor $theta$. A large $theta$ indicates a small portion of objects are being accessed by transactions, so that enlarges the cross-node memory sharing. Moreover, a larger write ratio increases the coherence traffic since a store would invalidate all peer caches. By increasing $theta$ from 0 (i.e. uniform accessing) to 0.9 (i.e. over 80\% accesses on 10\% tuples), we can see in Figure~\ref{fig:cc2read}, the time for an instruction to access memory increases 17\%, 27\%, and 66\% at 0.2, 0.4, 0.8 write ratios. 
We then observe the coherence packets received and sent by all nodes per transaction increases dramatically (55.9x) when the skew factor increases from 0 to 0.9 at the 0.8 write ratio. 
Even with fast IO connections, the remote cache signaling introduced by CXL-vanilla primitive still effects the system performance. 
\fi

% The number of coherence packets increases dramatically (xxx) with two reasons: the aborted transactions introduce addtional unused memory accesses, and writes would incur invalidation messages. 
% the CXL coherence protocol resolves coherence for aborted transactions

% Despite no deliverable CXL devices enabling the back invalidation process, the BI channel is believed to have similar latency with \cxlcache, since they have similarities on the underlying protocol constitution on both link and physical layers. 

% which implies the potential opportunities of addressing the high remote singaling cost with previous NUMA balancing solutions. 

In addition to the significant link latency, CXL introduces a unique coherence bottleneck when a read miss is served by the remote dirty cache. 
Previous works~\cite{ccnic_asplos24, pond} have adopted the non-uniform memory access (NUMA) architecture as a mimic of CXL memory. 
However, such a mimic is not adequate in the memory sharing since CXL exhibits relatively larger latency in remote cache signal time compared with conventional NUMA interconnects, such as the QuickPath Interconnect (QPI)~\cite{qpi-doc}. As shown in Table~\ref{tab:latency}, in most commercial NUMA architectures~\cite{directory_sp19, moesi_isca22}, reading a remote cacheline is not overly costly since the NUMA remote signal time is similar to accessing local memory. 
However, in CXL, the remote cache signal takes 44\% to 85\% more time than the latter, primarily because CXL requires two link roundtrips to inform other nodes, whereas NUMA only takes one. 
\rvs{
% As the cluster size increases, this latency gap would become larger due to the potential overheads from retimers and CXL switches~\cite{pond} involved by CXL link. As a consequence, the larger the roundtrip latency is, the more severe the remote cache signaling cost is. 
As the cluster size increases, the latency gap tends to expand, primarily due to the potential overhead introduced by retimers and CXL switches~\cite{pond} associated with the CXL link. An increase in roundtrip latency could exacerbate the overhead of remote cache signaling process.
}


% Consequently, the performance gap between C2C and C2M widens, as the C2C requires an additional CXL link roundtrip compared to C2M. }


% It's worth to notice that this performance issue will be more severe as the cluster become larger since it could involve retimers and CXL switches at the critical path~\cite{pond}. These modules introduce considerable overheads on the CXL link roundtrip. As a consequence, the performance gap between the C2C and C2M is enlarged since C2C takes an additional CXL link roundtrip than C2M. 


 % \name~employs PCIe Generation 5 x16 links to connect each host, providing bandwidth comparable to that of local DRAM. Connections are facilitated through on-board links or extensions from the motherboard's PCIe slots (e.g. MCIO). Ideally, the connection of each host adopts a direct and an independent physical link. 
 % in the critical path~\cite{pond}. These components significantly add to the overheads on the CXL link roundtrip.
% But that limits the scalability of numbers of the device, since each device could comsume a PCIe x16 slot of the IO die. 
% But that results in a network topology of N nodes and M devices utilizing NM links and occupying M PCIe x16 slots per processor. It limits the scalability of both number of nodes and devices for commercial clusters due to the limited number of PCIe slots available on the processors' I/O die. 
% To mitigate the scarcity of I/O slots, users may implement a CXL switch that connects each processor and device with one x16 link. This configuration incurs an approximate latency increase of 100 nanoseconds due to the switch traversal. 

% \red{TODO: } % CXL下有独特的overhead：read from remote远大于SMP

% Compared with conventional multi-socket cache coherence protocols (e.g. quick-path-interconnect (QPI)~\cite{qpi-doc}), 
% the C2C cacheline resolving is always done in parallel with accessing the DRAM when a core accesses memory beyond the socket, 
% This procedure adds up an RC-EP roundtrip and an SF lookup on the critical path. 
% As illustrated in Figure~\ref{fig:}, this procedure takes xx ns, much larger than DRAM access within EP. 
% It results an end-to-end latency of a coherent memory access will take more than 2x time than a non-coherent access.
% CXL-based memory share exhibits an unique reversed latency characteristic. 
% Compared with the time spent on directly accessing the memory, 


% CXL-style MESI: Any-Execlusive-Invalid
\hspace*{\fill}

\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
     \textbf{Take-away\#1:} Remote cache signaling makes considerable performance overheads on G-FAM accesses. 
  }%
}
% The remote fetching occurs when two or more processors access the same cacheline backed by the CXL remote memory, with at least one of the accesses being a write or being a read cache miss. 
% The remote fetching issue is frequent when transactions access are highly skewed. It's worth to note that not only cache coherence resolving happens between a read and a write, which is believed to be the ``true data racing'', but also it could happen between two reads despite there is no value changes. 
% % Because the EP does not know whether a node makes a silent write if it has an exclusive copy, thereby only sending speculative but unnecessary snooping request to that peer node. 
% As Figure~\ref{fig:}, the application with only read-only transaction still suffers from the high coherence latency, even there's no record changes. 

\ifx\nocmt\undefined
This renders an insight that storing frequently sharing objects in the CXL memory can serve reads and writes much faster than local caches, thereby we can benefit the performance by properly flushing the caches to DRAM. 
\fi
% in-coherent memory access can be much faster than a coherent memory access in CXL, which motivates our 

% It is relatively much longer than in the ccNUMA. 


% an cacheline upgrade or host cache miss

% It consequently bottlenecks the cache performance in CXL 

% and that is much severe in CXL than traditional ccNUMA architectures. 

% , and requires new CXL-native approaches mitigate this issue. 

% Unfortunately, despite previous work~\cite{pond, tpp} point out that CXL memory has the architecture similarities with cache-coherent non-direct memory access (ccNUMA) in accessing DRAM, the cache coherence in ccNUMA is always faster than accessing DRAM. Consequently, we must develop a new CXL-native approach to mitigate this issue. 

% Within standard single-socket or non-uniform memory access, the cache coherency datapath is much shorter than the memory access, but such a relationship in CXL is reversed. 
% As Tab.~\ref{tab:latency} summarizes, 
% % assume two cores locate on different nodes, the core-to-core roundtrip will take at least 76ns more than the core-to-mem roundtrip. 
% a coherent CXL memory access with coherence resolving takes 40\% to 85\% more time than a bare non-coherent memory access. 
% This performance characteristic is unique among existing shared-memory architecture. 


% This challenge is unique in CXL-based memory share since the traditional cache coherent architectures, i.e. SMP, always exhibit lower core-to-core latency compared with memory accesses. 
% This consequently lead to CXL's larger memory access latency~\cite{anns, directcxl}, as shown in 




\begin{figure}[t]
  \centering 
  \includegraphics[width=0.47\textwidth]{my/figs/motivation-over-coherent.pdf}
  \caption{(a) Remote cache signaling to node-1 from a node-2's read. (b) SF eviction process incurred by node-1's read. }
  \label{fig:motivation-over-coherent}
\end{figure}

\subsubsection{Snoop Filter Scalability Issues}    \label{subsec:snoop_filter}


To enable cross-host cache coherence, the CXL endpoint should maintain a snoop filter (SF) to track the ownership of cachelines associated with the address space of G-FAM~\cite{directory_sp19, moesi_isca22, skylake_sf_doc}. The SF should be inclusive of the G-FAM's cachelines in all processors' local caches. This means the SF needs to track which nodes hold the cacheline and in what states, if the cacheline is located in at least one node's cache. Every unique G-FAM load or store should occupy an SF entry.


% The second performance issue is caused by the centralized snoop filter at EP side. 
% The EP device is known as the home node that keeps all information of which nodes hold the cacheline in what states. 


% The SF is typically organized as a set-associative cache indexed by the DRAM address, with the value as a bitmap indicating the states of cacheline copies on each node. 
% to holds directory entries for a super-set of all blocks cached on the nodes. 

% A node that wants to issue a coherence request sends it directly to the EP via \cxlmem~channel, and the SF looks up the state of the block to determine what actions to take next. 

% Such an SF architecture could limit the total on-chip cached memory capacity since it should be inclusive on the HDM-backed cachelines referring to processors' L2 and LLC. 


% \red{Scalability Issues SF}
% \red{Centralized SF}
% \red{HCDS}

Compared with the SF in NUMA architectures, CXL SF faces unique scalability issues due to its centralized design. The CXL SF is located only at the G-FAM node and tracks all hosts' accesses, whereas NUMA architecture distributes SFs across every socket, with each SF tracking only remote accesses to its own socket.
We use the existing NUMA SF design as a mimic since a specific CXL SF design has not yet been proposed. To support a common rack-scale cluster with 16 nodes, the CXL endpoint is expected to maintain a 64K-set 11-way on-chip SF, which is 16x larger than a single NUMA SF in the current Intel Skylake architecture~\cite{directory_sp19, skylake_sf_doc}. In the worst case, where all processors' valid cachelines cache the G-FAM contents, the SF would require 1.8 billion entries and consume 14.4GB of memory. As the number of nodes increases, the CXL SF should expand correspondingly and could easily become a system bottleneck.

% NUMA SFs are organized in distributed way that each node keeps a private snoop filter and only tracks remote accesses to its owned DRAM. 
% However, the CXL SF requires a centralized design at EP to track memory accesses from all nodes to the shared memory. 

% \begin{figure}[t]
% \begin{minipage}{0.47\textwidth}
%   \centering
%   \includegraphics[width=\textwidth]{my/figs/pool.pdf}
%   \caption{Roundtrip latency breakdown for (a) a processor to CXL remote memory and (b) a processor to a remote processor. }
%   \label{fig:cxl-overhead}
% \end{minipage}
% \centering
% \begin{minipage}{0.45\textwidth}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|}
% \hline
%              & Socket & NUMA  & CXL (Ideal) & CXL (Proto) \\ \hline
% Core-to-Mem  & 92ns       & 145ns & 170ns                 & 456ns                 \\ \hline
% Core-to-Core & 49ns       & 133ns & 246ns                 & 847ns                 \\ \hline
% C2C/C2M    & 0.53x         & 0.91x    & 1.44x             & 1.85x                 \\ \hline
% \end{tabular}%
% }
% \caption{CXL Memory Access Latency For Direct Connection}
% \label{tab:latency}
% \end{minipage}
% \end{figure}


% a 11-way 4K-set on-chip SF (respecting with the NUMA SF in Skylake), the SF could only support 2.75MB caches in total, which is only 0.1\% of the processors' internal caches.
% As a consequence, the SF capacity could limit the total number of cached shared memory in all nodes.

% Considering a memory sharing field with a $S$ set and $W$ way global SF at the EP side. All nodes can only cache $SW*64$ shared memory. 

\ifx\undefined\stale
We track the 

In a general rack-scale cluster with 16 nodes, and a 11-way 4K-set on-chip SF (respecting with the NUMA SF in Skylake), the SF could only support 2.75MB caches in total, which is only 0.1\% of the processors' internal caches.
% The capacity is even less than the cache size of a single CPU core (2MB L2 plus 1.875MB LLC in Sapphire Rapids). 
Despite we can adopt DRAM as the backup of the SF to enlarge its available capacity, tracking the worst-case usage of on-chip caches are still inevitable. For example, tracking the 16 node cluster with 32 core in each node requires 1.8 billion SF entries, which will take 14.4 GB memory. 
\fi

% and a CXL coherence fabric (can be based on CXL switch or only CXL devices) with an 
% The number of on-processor CXL-memory cacheline is limited by $\frac{C}{addr + 2n}$ where each SF entry contains an $addr$-bit tag and a state vector with two bits for each node's cacheline state. 
% when we adopt the full directory and encode each MESI state with two bits. 
% \blue{Despite the cache usage model varies among the transaction access patterns, e.g. access skewness, but it requires over 100MB EP on-chip SRAM to get full usage of all processor's on-chip data caches.} 
% moreover, the working set could be smaller if the SF adopts a set-associative organization (which is the case of internal SF in Skylake) or the number of nodes sharing the memory increases. 

% Note that such capacity is far less than the requirement of modern transaction processing systems, giving current 

% since the real-world applications always exhibit large working set and poor data locality. 
% \blue{Cites}
% they always exhibit poor data locality and have much larger working set than a practical SF capacity can offer. 
% Moreover, such working set shrinks linearly in correlation with t
% of such a SF expands linearly in correlation with the number of nodes sharing the memory. 
% A miss in such an inclusive SF indicates that the block does not exist in any nodes' cache. As a consequence, the 
% However, the SF size with practical latency is limited in xx MB. The capacity insufficiency results in high possibility of SF conflicts. 


% he SF controller then tries to allocate an entry for X. It finds out Y and X are hashed to the same cache locations thus chooses Y as the victim. Since Y has been kept by node-2, the EP needs to invalidate the cacheline on node-2 and retrieves the value to DRAM if dirty. The EP should wait for the eviction flow with node-2 to finish before returns to the read request from node-1. 
% Even node-1 and node-2 has no cacheline sharing, the read request still invalidates the remote cacheline, and waits for three CXL channel roundtrips in total. 

Conflicts on the SF lead to an SF eviction flow that back-invalidates other cachelines~\cite{cxl-doc, cxl-shortdoc} to make room for SF allocation. Figure~\ref{fig:motivation-over-coherent}~(b) illustrates an example: a read on X, which is not currently recorded by the SF, incurs an SF eviction on Y since they map to the same SF slot. The EP must wait for the eviction flow to reach node-2 before responding to node-1's read request. Such a process involves three CXL roundtrips on the critical path, which take over 400ns more than the remote signaling process.


% and is 5x larger than accessing the HDM. stays on the critical path of memory accesses but 
% Although compressing the directory states (e.g. via coarse directories or limited pointers) can help the SF to accommodate more cacheline states, these approaches will incur additional CXL traffic for unnecessary coherence messages, since they may send snooping messages speculatively to nodes that is not tracked in the SF. 

% invalidated cacheline (i.e. Y) has been modified so that it needs to write dirty data back. 
% on the structure can cause inclusion victims across cores.
% an SF miss will lead to an eviction flow to make room for the newly allocated cacheline states, as Figure~\ref{fig:} illustrates. 

%  when an SF miss incurs an SF eviction, which back invalidates processors' cachelines to keep the SF inclusive. In this example, 

% On the one hand, the SF conflicts is hard so solve since any new cache miss will lead to an SF allocation, even if the cacheline is only owned by only one node and not actually being shared. On the other hand, the SF conflicts seriously harm the performance since the SF eviction flows locates on the critical path of memory access. 
% This problem is serious when the transactions' memory footprint are large and accesses distribute uniformly, since the large memory footprint will very likely lead to the SF capacity conflicts. 
% It not only incurs significant performance loss since it blocks the requesting H2D channel over 1ms, but also may incur complex cache interference and trashing. 
% , thereby hurting the performance more. 

% The scale of such a directory expands linearly in correlation with the number of nodes sharing the memory. Consequently, there arises a critical need to balance the shared memory scale, carefully weighing the trade-offs between performance enhancement and the hardware overheads. Further exploration and discussion on this topic are deferred to Section~\ref{sec:overview}. 


% The CXL 3.0 adds a bi-direction BI channel for each host-device connection, specifically for sending coherence messages from the device to the host and retrieving the responses (CXL does not re-use \cxlcache~channels to avoid the circular channel dependence that cound potentially lead to deadlocks). The memory device thus becomes the ``home node'' of its owned cachelines. It maintains a snoop filter (SF) to track the ownership of cache lines associated with the address space of HDM. 

% Figure~\ref{fig:cxl-memory-pool} shows how we can share the DRAM and keep coherence across hosts. 
% All inter-node coherence requests should check the SF to route to the specific nodes. 
% \wz{Virtual channels?} 



% Fortunately, it's possible in CXL-based memory sharing since the CXL specification only specifies the coherence protocol but leaves the freedom of cache design to developers. 

% a cache flush will block the processor pipeline significantly since modern processor needs to clear its write buffers before the flush to keep memory consistency. 
% explicit cache flushing instruction (e.g. CLFLUSH in Intel~\cite{intel-doc}) is issued.

% \noindent \textbf{Our approach. }


% ypically modify cache architectures to add conflict monitor bits~\cite{rtm_isca07}, redesign the cache coherence protocol to facilitate multiple writers~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94}, and change the address translation flow~\cite{vtm_isca05, overlaytm_pact19, flextm_isca08, logtm_hpca06}. 
% They introduce changes in processor cache designs and integrates extensive logic into circuits, thus are 
% \hspace*{\fill}

\noindent\fbox{%
% \centering
  \parbox{0.47\textwidth}{%
     \textbf{{Take-away\#2: }}  The centralized snoop filter causes scalability issues of CXL-vanilla primitives.
  }%
}

\subsection{Do transactions require strict coherence ?}       \label{subsec:overkill}

% To figure out the reason of such a broaden throughput drop of the various transaction system strategies, we suppose to study the fundamental accessing primitives to access the shared CXL memory, which are relied on all algorithms. 



% CXL does not distinguish the exclusive state with the modified state, so that serving a request for the ownership cacheline (whether clean or not) requires transferring ownership as well as providing up-to-date data. 


% On a write miss, both the missing cache and the LLC must transition to a transient blocking state, typically delaying subsequent requests to the target address while all remote owners or sharers are downgraded and up-to-date data is retrieved.
% Once read permission (Shared state) or write permission (Modified or Exclusive state) is obtained, subsequent reads or writes to the same cache line may hit in the cache until a conflicting access from a remote core causes a downgrade or until the cache line is evicted. 



% On a write miss, both the missing cache and the LLC must transition to a transient blocking state, typically delaying subsequent requests to the target address while all remote owners or sharers are downgraded and up-to-date data is retrieved.
% MESI-based protocols are designed to exploit as much locality as possible by using writer-initiated invalidation, ownership-based (write-back) caches, and line granularity state and communication. Writer-initiated invalidation means that every read miss triggers a request for read permission in the form of Shared state – this permission is revoked only when a future write to the line sends an explicit invalidation to the reader. Ownership-based caching means that every cache miss for a write or atomic read-modify-write (RMW) access triggers a request for exclusive write permission in the form of ownership, or Modified state. Both read and write permissions are requested for the full line in MESI. Once read permission (Shared state) or write permission (Modified or Exclusive state) is obtained, subsequent reads or writes to the same cache line may hit in the cache until a conflicting access from a remote core causes a downgrade or until the cache line is evicted. This strategy can offer high cache efficiency by exploiting temporal and spatial locality, but it comes at a cost.



% The data structures of most transaction processing systems could be divided into two disjoint fields. 
% The \textit{record} field dictates the actual data that the system manages, which takes up the major memory footprint at most scenarios. Memory primitives toward this field include reads and writes, but all wrapped with the transactions. 
% The \textit{metadata} field contains the DBMS internal objects that enable concurrency control to guarantee mutual exclusive access on critical sections, such as locks, latches, and timestamps. This field only consumes a small memory space, but are heavily queried by concurrency controls. Memory primitives toward this field are almost atomic operations. 

% We concentrate our discussion how we manage the \textit{record} field, and leave the full story in Sec.~\ref{sec:}.

% RC is a widely adopted relaxed consistency model~\cite{threadmark, munin, txcache}. It distinguishes between two types of memory operations: ordinary and synchronization. RC introduces two synchronization operations: acquire and release. The acquire operation is performed before a process enters a critical section, ensuring that any updates made in other critical sections by other processes are visible. The release operation is performed when exiting a critical section, signaling that all updates made within the critical section are now visible to other processes. 
% RC follows two key ordering principles: 
% First, operations within a critical section can be reordered, but synchronization operations (acquire and release) enforce a partial ordering. Second, operations before a release cannot be reordered to after it, and operations after an acquire cannot be reordered to before it.

% The ordering of operations toward the \textit{record field} is governed by transactional consistency models, such as strict serializable consistency or snapshot isolation. These models allow a certain degree of flexibility in how transactions are processed, which in turn avoids the necessity for strict coherence protocols of CXL. Data across different transactions may exhibit temporary incoherency, especially if involving aborted or uncommitted transactions. It permits multiple transactions observing different values of the same data records, with coherence enforced only at transaction commitment. In contrast, the operations on the \textit{metadata field} highly rely on the cache coherency since they are mostly atomics operations build upon the cache locking mechanism. 



We observe that the coherence model adopted by the \vanilla~primitives is overkill for most transaction processing systems. We use the key-value store (KVS) as an example. A typical KVS comprises two data fields: the \textit{record field}, which stores the actual data (KVS's value) managed by the system, and the \textit{metadata field}, which contains the system's internal data structures to ensure transaction correctness, such as locks, timestamps, indexes, and latches.

% For the sake of simplicity, we take the transactional in-memory key-value store (KVS) as an example. The record field in KVS is the value, being the actual data that the system stores. The size and the structure of the value depend on the application but typically takes up the major fraction of memory usage. 
% Memory accesses toward the \textit{record field} are wrapped with transactions to ensure orders. On the other hand, the \textit{metadata field} is smaller in size. The \textit{metadata field} 

% Accesses toward the \textit{metadata field} require the strict coherence to ensure atomic operation orders, while the \textit{record field} accesses follow a looser order. 

Accesses to the \textit{record field} follow a much looser order than what CXL's strict coherence model provides.
Specifically, accesses to the \textit{record field} are all wrapped within transactions that are committed as all or none. These accesses thus adhere to specific transactional consistency models~\cite{rss_sosp21}, such as strict serializability~\cite{tcc_isca04} or snapshot isolation~\cite{sitm_asplos14}. These consistency models permit reads from two or more transactions to observe temporarily incoherent values. Figure~\ref{fig:txn_consistency} illustrates an example of a strict serializable consistency model, where the read from Txn-4 precedes Txn-2's write, despite this read occurring after the write in physical time. 


% the write happens before the read in logic. 

% Taking strict serializable consistency as an example, a transaction read should return with the mostly committed write instead of the most recent write to avoid dirty reads. 
% As Figure~\ref{fig:txn_consistency} shows, the transaction T1, T2, and T3 are trying to overwrite the value of A, and T4 is reading out A. 
% T4 would read out the old value committed by T2, despite a T3's write have overwritten A with 3 before T4's write in the physical time, since T3 commits after T4. 
% However, in the strict coherence model, the read from T2 could return with the write from T2. 

% concurrency control algorithms have already ordered operations toward the record field respecting to a transactional consistency model, which may be strict serializable, repeated read, or even more loose snapshot isolation. 
% An aborted or uncommitted store would not be observed by other transactions but should be seen by local reads to avoid unrepeated read. 
% it is allowed to read out the outdated value because a store takes effect at the transaction commit time instead of the issued time. 
% To be specific, a transaction processing system preserves the users with ``ACID'' features, which are short for atomic, consistency, isolation and durability. 

% This coherence model allows a certain degree of flexibility in how transactions are processed, which in turn avoids the necessity for strict coherence protocols of CXL. Data observed by different transactions might display temporary inconsistencies, when one transaction is aborted or is uncommitted. 
% to interact with the same data records in a speculative manner, with coherence enforced only at transaction commitment. 
% Inspired by the such coherence features, we propose a heterogeneous coherence primitive designed to apply differentiated protocols based on the type of data accessed. 
% For the \textit{metadata field}, we adhere to standard CXL-vanilla primitives to ensure that atomic operations are correctly serialized, preserving the integrity of metadata operations. Conversely, for the \textit{record field}, we introduce a loosely coherent model that decouples HDM accesses from remote cache management. This model encompasses three primary operations: Load (Ld), Store (St), and Synchronization (Sync).

Such counter-intuitive ordering is maintained by the concurrency control algorithms in transaction processing systems.
Therefore, there is no need for the hardware to maintain the strict cache coherence model, which incurs substantial performance overheads (Take-away \#1 and \#2). As an alternative, \textbf{the hardware could simply maintain the partial order required by transactional consistency, allowing data incoherence according to the transaction's execution status and synchronizing data only at the commitments of transactions~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94, rtm_isca07, overlaytm_pact19, lamport_tc79, newdefinition_isca98}.}


Prior works on single-node concurrency controls, such as SILO~\cite{silo_sosp13}, align with our findings but their solutions are orthogonal to us. Specifically, SILO aims to avoid unnecessary coherence traffic incurred by \textit{metadata} writes that are required for read-only records' accesses, but it can not mitigate the remote signal costs associated with \textit{record} accesses themselves, hence it can not address the remote singling cost on records accesses. 
Moreover, SILO-like concurrency controls only modify the software algorithms, which do not address SF issues without changes to the underlying hardware coherence mechanisms. 

% while our approach avoids unnecessary coherence overheads incurred by \textit{record} reads and writes from uncommitted or aborted transactions. 
% SILO-like approaches consequently could not address the high remote signal costs for \textit{records} such as reading 
% For approaches, SILO-like concurrency controls only change the software algorithms so that could not address SF scalability issues without changing underlying hardware coherence mechanisms. 

% Just maintain the coherence model that transactions require. 
% 支持Hybrid.
% \red{SILO}: % SILO 与我们的Insight一致，它做了什么我们做了什么，没法解决读引发的CC，SILO只减少了写引发的CC，目标；读引发的Coherence Overhead是比SMP里高. 


% Similarly, research in optimistic transaction memory (TM)~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94, rtm_isca07, overlaytm_pact19} aims to reduce coherence traffic. However, these approaches significantly alter processor cache architectures and incorporate complex logic into circuits, posing challenges for integration into modern complex processors due to their considerable overhead.

% Previous research in concurrency control algorithms, such as SILO~\cite{silo_sosp13}, supports our observations. These studies have reduced coherence traffic at the algorithmic level. But without modifying underlying hardware coherence mechanisms, they fail to resolve the SF scalability issues. 
% Optimistic transaction memory (TM) also has similar insights~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94, rtm_isca07, overlaytm_pact19} to reduce coherence traffic. However, their overhead is not negligible since they always introduce significant changes in processor cache designs and integrates extensive logic into circuits, which are too ambitious for recent complicated processors. 

% s of transaction systems have ordered the \textit{record field} operations with the help of atomic operations on the strict coherence on \textit{metadata field}.
% , local write buffers, or multiple versions managements. 

% Such an opportunity helps us to reduce the cache coherence cost, including the remote cache signaling time (\textbf{Take-away\#1}) and the desire for the centralized precise SF (\textbf{Take-away\#2}). 

% The memory accesses from the uncommitted and aborted transactions 

% To be specific, multiple object operations within a transaction are serialized all or none, and operations across transactions obey the strictly serializable order. 

\hspace*{\fill}

\noindent\fbox{%
% \centering
  \parbox{0.45\textwidth}{%
     \textbf{{Take-away\#3:}} Transaction processing systems allow the record field accesses to be temporal in-coherent. 
    %  temporal in-coherence on the record field thus do not necessitate the strict coherence semantic. 
    %  benefit from optimistic coherence nature of transaction processing systems.
  }%
}

\begin{figure}[t]
%   \centering 
  \includegraphics[width=0.45\textwidth]{my/figs/transaction_consistency.pdf}
  \caption{The strict serializable consistency. }
  \label{fig:txn_consistency}
\end{figure}



% However, they are overly ambitious since it integrates extensive logic into processor circuits and requires non-travial efforts for verification. 


\begin{figure*}[t]
\begin{minipage}{0.33\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
Proc. P1 @N1 & Proc. P2 @N2 \\
\hline
$I_1$: St/L-St $\alpha$ 1 & $I_4$: St/L-St  $\beta$ 1 \\
$I_2$: r1 = Ld/L-Ld $\alpha$ & $I_5$: r3 = Ld/L-Ld $\beta$ \\
$I_3$:r2=Ld/L-Ld($\beta$+r1-1) & $I_6$:r4=Ld/L-Ld($\alpha$+r3-1)\\
\hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}} CXL-vanilla forbids but \name~guarantees: \\  r1 = 1, r2 = 0, r3 = 1, r4 = 0 \end{tabular}}    \\
\hline
\end{tabular}% 
}
\caption{L-Ld and L-St are node-private}
\label{fig:litmus_1}
\end{minipage} %
\hfill
\centering
\begin{minipage}{0.33\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
Proc. P1 @N1 & Proc. P2 @N2 \\
\hline
$I_1$: St/L-St $\alpha$ 1 & $I_5$: St/L-St $\beta$ 1 \\
$I_2$: r1 = Ld/L-Ld $\alpha$ & $I_6$: r3 = Ld/L-Ld $\beta$ \\
\textbf{$I_3$: GSync $\alpha$}    & \textbf{$I_7$: GSync $\beta$} \\
$I_4$:r2=Ld/L-Ld($\beta$+r1-1) & $I_8$:r4=Ld/L-Ld($\alpha$+r3-1) \\
\hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}} \name~forbids: r1 = 1, r2 = 0, r3 = 1, r4 = 0 \end{tabular}}    \\
\hline
\end{tabular}% 
}
\caption{GSync broadcasts stores}
\label{fig:litmus_2}
\end{minipage} %
\hfill
\begin{minipage}{0.32\textwidth}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
Proc. P1 @N1 & Proc. P2 @N2 \\
\hline
$I_1$: St/L-St $\alpha$ 1 & $I_5$: St/L-St $\beta$ 1 \\
$I_2$: r1 = Ld/L-Ld $\alpha$ & $I_6$: r3 = Ld/L-Ld $\beta$ \\
\textbf{$I_3$: Wd $\alpha$}    & \textbf{$I_7$: GSync $\beta$} \\
$I_4$:r2=Ld/L-Ld($\beta$+r1-1) & $I_8$:r4=Ld/L-Ld($\alpha$+r3-1) \\
\hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}} \name~forbids: r3 = 1, r4 = 1 \end{tabular}}    \\
\hline
\end{tabular}% 
}
\caption{Wd withdraws stores. }
\label{fig:litmus_3}
\end{minipage} %
\end{figure*}


\section{\name~Overview}    \label{sec:primitive}


\ifx\stale\undefined
\subsection{The Over-Coherent Problem}
% In this section, we discuss the effect of CXL's coherence overhead on accessing these two fields. 
In a well-established transaction processing system, operations toward the \textit{records} are already well ordered referring to a \textit{strict serializable consistency} by concurrency control algorithms. 
These operations form only a small subset of possible interleaving orders.
To be specific, transaction processing exhibits an optimistic nature on coherence~\cite{farm_nsdi14, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13} so that it's legal for multiple transactions to hold the same record simultaneously in either an unchanged or a speculatively modified form. The different versions are serialized at the boundary of transactions, but no ordering limits between uncommitted operations. 
The ordering limits are further relaxed when one conflicted operation is from an aborted transaction, since it's unnecessary to serialize a memory operation that will be discarded. 
% Only a successful committed transaction that wins at data races could serialize, but the aborted transaction should discards all operations and restarts. 
It provides an opportunity to delay the coherence point only at the commit phase, and helps to reduce the coherence cost. 
% , where the CXL coherence model is an overkill. 

However, 
the CXL-based memory sharing, as well as most MESI-based caching architectures~\cite{rvweak_pact17, armcm_cmb12, powercm_pldi11, rvweak_isca18, gam, txcache_osdi10}, adopt a \textit{strict coherence} model~\cite{munin_ppopp90, rtm_isca14}. This model ensures accessing to every memory location following the single-writer-multiple-reader (SWMR) invariant, which enables only a single core that may write to the location or multiple cores that may read the location. 
It's a pessimistic ordering model that addresses data races at every point they happened, by only allowing one version at a time. 
All aforementioned performance drawbacks are the cost to maintain the SWMR invariant: the remote fetching is essential to address potential data races on ownership upgrades (Shared to Exclusive) and silent writes (Exclusive to Modified); the SF is essential in the directory-based protocol to route coherence requests to proper places. 

% To this end, all processors must observe the same ordering of write operations to the same location.
% can response to the reads at one time. 
% The order of versions exhibits the same when observed from any processors. 

% will broadcast transaction's modifications to notify all other transactions, 
% , while an abort will discard all transaction's changes. 
% an object A to simultaneously have multiple uncommitted versions that may differ, so that a read can return with the stale version that is lastly committed.
% Some of the uncommitted versions may be discarded due to transaction aborts, and others are committed with respect to the logical serialized time of their corresponding transactions. 

% The cache coherence protocol essentially specifies how the versions of memory objects are managed. Current hardware-based coherence protocol always addresses data races at the point they happened, i.e. only allows one version at a time. It resembles pessimistic approach in terms of concurrency control algorithms. Such pessimistic nature leads to the aforementioned performance drawbacks. 



From the software perspective, a transaction should prevent its uncommitted write from being observed by others. But the cache coherence conducts the broadcasts constantly. 
To this end, a transaction system achieves the write isolation via pessimistic locks~\cite{s2pl_csur81, abyss_vldb14}, or optimistic write buffers~\cite{silo_sosp13, hekaton_sigmod13, occ_tbs81, farm_nsdi14}. There's a wide debate on which approach performs better, and some works~\cite{tictoc, polyjuice_osdi21, cormcc_atc18} dynamically select both according to the application contention scenarios. However, few discussion is made on altering the underlying cache coherence model to make it fit for transaction processing, since it's overly ambitious to change cache designs and integrates extensive logic into processor circuits. Fortunately, it's possible in CXL-based memory sharing since the CXL specification only specifies the coherence protocol but leaves the freedom of cache design to developers. 
% in processor cache designs and the integration of extensive logic into circuits. Such radical modifications are overly ambitious for recent CPU architectures, rendering them impractical in today's systems.  
% , but an on-going transaction could observek the record in either a speculative state or a stale state~\cite{farm, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13}. 
% the strict serializable consistency violates the SWMR invariant that is essential for cache coherency, since it enables different transaction workers to observe the different version orders of the same memory object. 
% Compared with the SWMR invariants, strict serializable consistency not only allows parallel speculative writers to co-exist, but also enables different transaction workers observe the different version orders of the same memory object~\cite{farm, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13}. 
\fi

% \subsection{Benefits of The Incoherent System}
\ifx\stale\undefined
\subsection{Optimistic Cache Coherence Protocol}

% To this end, it's no need to keep the SWMR invariant for the \textit{records} for all time, since synchronization is only needed at the boundary of a transaction. 
% However, few discussion is made on altering the cache coherence model to fit transaction processing requirements, since it's


\name~leverages the loose coherence nature to eliminate the high coherence cost in CXL. \name~proposes a release consistency (RC)~\cite{rc} like memory consistency model that delays the version serialization to when the transaction commits. 

RC is a widely adopted relaxed consistency model~\cite{threadmark, munin, txcache}. It distinguishes between two types of memory operations: ordinary and synchronization. RC introduces two synchronization operations: acquire and release. The acquire operation is performed before a process enters a critical section, ensuring that any updates made in other critical sections by other processes are visible. The release operation is performed when exiting a critical section, signaling that all updates made within the critical section are now visible to other processes. 
RC follows two key ordering principles: 
First, operations within a critical section can be reordered, but synchronization operations (acquire and release) enforce a partial ordering. Second, operations before a release cannot be reordered to after it, and operations after an acquire cannot be reordered to before it.


This brings up the following three benefits: 

Challenges: 
Commercial cluster deployment
Supporting variety of concurrency control algorithms
Supporting essential components of IMDB

Changing the cache design to fit transaction processing is overly ambitious since it integrates extensive logic into processor circuits and requires non-travial efforts for verification. Fortunately, it's possible in CXL-based memory sharing since the CXL specification only specifies the coherence protocol but leaves the freedom of cache design to developers. 
\fi



\subsection{\name~Primitives}


To exploit the incoherence opportunity in the record field (\textbf{Take-away\#3}), we propose a hybrid primitive design that implements different protocols depending on the field accessed by the primitive. Users can employ the strictly coherent protocol for the \textit{metadata field} to ensure the correctness of atomic operations. For the \textit{record field}, \name~advocates for a new, loosely coherent protocol. In the following discussion, we use ``\name~primitive'' to indicate the \textit{record field} primitives for simplicity. The \name~primitive decouples costly cross-node coherence operations in CXL from normal memory accesses. It comprises four fundamental memory operations: Local-Load (L-Ld), Local-Store (L-St), Global Synchronization (GSync), and Withdraw (Wd).


L-Ld and L-St retrieve and put data from/to the shared memory, and their effects are limited to the requesting node, making no coherence demands. 
We utilize litmus tests~\cite{rvweak_pact17} to compare L-Ld and L-St with standard CXL Loads (Ld) and Stores (St). 
As depicted in Figure~\ref{fig:litmus_1}, P1 forwards the value from I1 to I2 and I3 locally without making I1 visible to P2 at N2, allowing r2 and r4 to both be 0 simultaneously. In contrast, with \vanilla~primitives, if both I2 and I5 return the value 1, it implies that stores I1 and I4 must have been globally observed, hence r2 and r4 cannot both be 0.


The GSync broadcasts a node-private value globally, while the Wd withdraws updates made by preceding L-Sts that have not yet been broadcast. 
The synchronization primitives should be specified for a particular address.
As depicted in Figure~\ref{fig:litmus_2}, following the GSync primitives, both P1 and P2 can observe the new value stored by I1 and I5. Consequently, I4 and I8 cannot both be 0, similar to the behavior observed with \vanilla~primitives.
As illustrated in Figure~\ref{fig:litmus_3}, Wd cancels the value stored by I2 so that it is invisible to I8.


% The synchronization primitives GSync and Wd are specified to an address. 
% any reads from neither local nodes and other nodes. 
% With the Wd primitive, the value stored by I2 could not be observed by I8 at any more since it is withdrawn by the I3. 

% Specifically, primitives originating from different nodes are invisible to each other, while primitives from the same node adhere to the strict coherence order in the typical x86 architectures. 
% GSync and Wd are pair of synchronization primitves. 


% We maintain the CXL-vanilla primitives for the \textit{metadata field} to ensure atomic operations are correctly serialized. For the \textit{record field}, however, we propose a loosely coherent model that separates HDM accesses from the remote cache management. It includes three primitives: Load (Ld), Store (St) and Synchronization (Sync). 



% \begin{figure}[h]
% \centering
% \begin{minipage}{0.40\textwidth}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|l|l|}
% \hline
% Proc. P1 @N1 & Proc. P2 @N2 \\
% \hline
% $I_1$: St/L-St $\alpha$ 1 & $I_4$: St/L-St  $\beta$ 1 \\
% $I_2$: r1 = Ld/L-Ld $\alpha$ & $I_5$: r3 = Ld/L-Ld $\beta$ \\
% $I_3$: r2 = Ld/L-Ld ($\beta$ + r1 - 1) & $I_6$: r4 = Ld/L-Ld ($\alpha$ + r3 - 1) \\
% \hline
% \multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}} CXL-vanilla forbids but \name~guarantees: \\  r1 = 1, r2 = 0, r3 = 1, r4 = 0 \end{tabular}}    \\
% \hline
% \end{tabular}% 
% }
% \caption{Loads and stores are node-private}
% \label{fig:litmus_1}
% \end{minipage} %

% % \hfill
% % \begin{minipage}{0.32\textwidth}
% % \centering
% % \resizebox{\textwidth}{!}{%
% % \begin{tabular}{|l|l|l|}
% % \hline
% % Proc. P1 @N1 & Proc. P2 @N1 & Proc. P3 @N2 \\
% % \hline
% % $I_1$: St $\alpha$ 2 & $I_2$: r1 = Ld $\alpha$ & $I_4$: r2 = Ld $\beta$ \\
% %             & $I_3$: St $\beta$ (r1 - 1) & $I_5$: FENCE \\
% %             &                     & $I_6$: r3 = Ld $\alpha$ \\
% % \hline
% % \multicolumn{3}{|l|}{\begin{tabular}[c]{@{}l@{}} Both CXL-vanilla and \name~allows: \\ r1 = 2, r2 = 1, r3 = 0 \end{tabular}} \\
% % \hline
% % \end{tabular}%
% % }
% % \caption{Test for non-atomic coherence}
% % \label{fig:litmus_3}
% % \end{minipage}
% % \hfill
% % \begin{minipage}{0.32\textwidth}
% % \centering
% % \resizebox{\textwidth}{!}{%
% % \begin{tabular}{|l|l|l|}
% % \hline
% % Proc. P1 @N1 & Proc. P2 @N1 & Proc. P3 @N2 \\
% % \hline
% % $I_1$: St $\alpha$ 2 & $I_3$: r1 = Ld $\alpha$ & $I_5$: r2 = Ld $\beta$ \\
% % \red{$I_2$: Sync $\alpha$} & $I_4$: St $\beta$ (r1 - 1) & $I_6$: FENCE \\
% %               &                     & $I_7$: r3 = Ld $\alpha$ \\
% % \hline
% % \multicolumn{3}{|l|}{\begin{tabular}[c]{@{}l@{}} Both CXL-vanilla and \name~forbid: \\ r1 = 2, r2 = 1, r3 = 0 \end{tabular}} \\
% % \hline
% % \end{tabular}%
% % }
% % \caption{Test for sync primitives}
% % \label{fig:litmus_3}
% % \end{minipage}
% \end{figure}



% L-Sts within our model are private to a node so that keep invisible to other nodes. 



% \begin{figure}[h]
% \centering
% \begin{minipage}{0.40\textwidth}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{|l|l|}
% \hline
% Proc. P1 @N1 & Proc. P2 @N2 \\
% \hline
% $I_1$: St/L-St $\alpha$ 1 & $I_5$: St/L-St $\beta$ 1 \\
% $I_2$: r1 = Ld/L-Ld $\alpha$ & $I_6$: r3 = Ld/L-Ld $\beta$ \\
% \textbf{$I_3$: Wd $\alpha$}    & \textbf{$I_7$: GSync $\beta$} \\
% $I_4$: r2 = Ld/L-Ld ($\beta$ + r1 - 1) & $I_8$: r4 = Ld/L-Ld ($\alpha$ + r3 - 1) \\
% \hline
% \multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}} \name~forbids: r3 = 1, r4 = 1 \end{tabular}}    \\
% \hline
% \end{tabular}% 
% }
% \caption{The Wd primitive rmoves local stores. }
% \label{fig:litmus_3}
% \end{minipage} %
% \end{figure}


% This primitive ensures that the subsequent operations, such as I4 and I8, cannot simultaneously return a zero value, mirroring the ordering guarantees provided by conventional CXL coherence primitives.
% As Figure~\ref{fig:litmus_1} shows, the primitives from the same processor follow the program order so that a load would return the most recent store from the same processor. 
% Stores in \name~are node-private and could only be observed from the processes at the same node. 
% In a shared memory pool with CXL-vanilla primitives, when both I2 and I5 have returned value 1, stores I1 and I4 must have been globally observed, thus r2 and r4 cannot both be 0. However, in \name, P1 is ensured to forward the value of I1 to I2 locally without advertising I1 to other nodes. 
% Synchronization is an address-specific primitive. It advertises the node-private write to all nodes atomically, as Figure~\ref{fig:litmus_2} shows. 
% With the sync primitive, both P1 and P2 make sure that each other observes the St, after I3 and I7 correspondingly. As a consequence, the I4 and I8 shares the same ordering with the CXL-vanilla, where they can not be both 0.
% This innovative approach contrasts with traditional CXL protocols by granting the software explicit control over coherence mechanisms. We leverage litmus tests, as documented in~\cite{rvweak_pact17}, to demonstrate the ordering principles of our primitive. 

% If I4 and I8 both returns with 0, the I4 precedes I7 and I8 precedes I3. Since the program order limits I3 precedes I4 and I7 precedes I8, there would be a circular dependence on I3, I4, I7, I8, I3. 
% in strictly coherence primitive order is globally serialized at any processors, so that if one observes r1 = 2 and r2 = 1 then r3 must be 2. However, the loosely coherence preserves coherence only within a node, so that P2 can see the value of I1 but P3 still see the old value. 

\ifx\undefined\stale
\noindent \textbf{Benefits for the primitive innovation. }
% There are two main benefits of our primitive design.
% First, 
The GSync and Wd grant the software control to selectively advertise a memory access. As we will discuss in later, L-Ld and L-St would not incur any coherence overheads on execution. By carefully invoking the GSync primitives referring to the system-adopted transaction consistency model (Sec.~\ref{subsec:implementation}), for example, only synchronizing writes at commits when using strict serializability model, the system developers could avoid unnecessary remote cache signaling penalty from those uncommitted or aborted operations. 
Moreover, the GSync and Wd primitive removes the SF checking from the critical path of G-FAM accessing. It thus tolerates higher latency that could be exploited to improve the spatial efficiency of SF. 
\fi
% to construct an approximate cache tracking approach that exhibits high spatial efficiency with the cost of falsely-tracking penalty. 

% Second, by exposing synchronization to software, the system developers could leverage latency hiding techniques such as overlapping and batching to remove the coherence time off the critical path (Sec.~\ref{subsec:implementation}). To this end, we build a distributed and approximate host cache tracking approach that exhibits much higher space efficiency than the precise SF. 
% scales up with the number of working nodes but may falsely send coherence requests. 
% and obliviously of the application semantic

% This enables the hardware SF to trade for higher scalability but tolerate for a slight accuracy drop. 
% all remote signaling traffic caused by coherence
% A typical usage depends the L-Ld and L-St for record accessing within transaction execution phase, and explicitly invokes the GSync for the write set on commitment or invokes Wd on transaction aborts. 
% Typical usage
% Reduce the remote signaling traffic
% Allow approximate snoop filters 
% rather than the hardware conducting the synchronization transparently and pessimistically. 

\ifx\define\stale
We define the coherence ordering rules resembling the definition of weakly memory consistency~\cite{newdefinition_isca98}: 
\begin{enumerate}
    \item Primitives are strongly coherent within a node so that processors at the same node observe only one primitive order. If primitives $p_1$ and $p_2$ belong to the same node, we say $p_1$ \textit{happens before} $p_2$ if $p_1$ precedes $p_2$ in the node's order. 
    
    \item \textit{Syncs} are strongly coherent across nodes so that processors at any nodes observe only one \textit{Sync} order. We say a the $Sync_1$ \textit{synchronizes before} $Sync_2$ if $Sync_1$ precedes $Sync_2$ in this order.
    
    \item If primitives $p_1$ and $p_2$ belongs to different nodes, we say $p_1$ \textit{happens before} $p_2$ iff. $p_1$ \textit{happens before} a $sync_1$ and a $sync_2$ \textit{happens before} $p_2$ and $sync_1$ \textit{synchronizes before} $sync_2$. 
    
    \item If 
\end{enumerate}
\fi

% In a machine with single-copy atomic stores (e.g. an SC machine), when both I2 and I5 have returned value 1, stores I1 and I4 must have been globally advertised. Thus r2 and r4 cannot both be 0. However, a machine with store buffers (e.g. a TSO machine) allows P1 to forward the value of I1 to I2 locally without advertising I1 to other processors, violating the single-copy atomicity of stores.


% HDM operations would private memory space that introduces no coherence issues, and data coherence is delayed to the transaction commitment phase. The decoupled optimistic coherence resolves aforementioned CXL's performance drawbacks in two aspects. 
% This model operates on a private memory space for each node, avoiding immediate coherence conflicts and deferring data synchronization to the transaction's commit phase. 

% This strategy aims to mitigate performance issues associated with remote data fetching and cache invalidation in CXL, optimizing data race handling and reducing the overhead associated with snoop filter (SF) demands through an alternative tracking mechanism.

\ifx\define\stale
In a transaction processing system, operations on the \textit{record field} are already well ordered referring to a \textit{strict serializable consistency} or more loose ordring models such as snapshot isolation by the software runtimes. 
These operations form a limited subset of possible interleaving orders than general parallel codes, thus it's no need to keep the strict pessimistic coherence as general cases. 
To be specific, operations would follow the two key principles on data coherency. 
First, the \textit{record} reads and writes within a transaction should be coherent, so that a read reflects its preceding writes on the same data object. 
Second, the \textit{record} reads and writes observed by different transactions could be incoherent if one belongs to an aborted or an uncommitted transaction. On the other words, data coherency is only preserved between committed operations. 


These rules indicate an optimistic coherence limit on the \textit{record field} so that it's legal for multiple transactions to hold the same record simultaneously in either an unchanged or a speculatively modified form, and these forms are only need to be serialized at the commit phases. On the other hand, the \textit{metadata field} requires a strict coherence guarantee that a write must grant the inclusive ownership of 
the target address and all nodes should observe the same global value order. 

To achieve the both worlds, we propose a heterogeneous coherency primitive that distinguishes memory operations by addresses (Sec.~\ref{sec:model}).
We adopt the strict coherence model for the \textit{metadata field}. But for the \textit{record field}, we propose a loosely coherent primitive that decouples the main memory read/write with remote cache invalidation and data fetching. Memory operations would hit the node private memory space that introduces no coherence issues, and data coherence is delayed to the transaction commitment phase. The decoupled optimistic coherence resolves aforementioned CXL's performance drawbacks in two aspects. 
First, reducing remote fetching cost traffic on the only true data races. 
Second, we remove the remote fetching from the performance-critical reads. So that we adopt a tracking data structure replacing SF to track the ownership of each node. It trades a false-positive rate by significantly reducing the SF's memory demands. 
\fi

% Minimizing cross-host communication by decoupling the write primitive.
% Because real-world OLTP applications exhibit skewed memory accesses with over 99\% operations are reads. From the hardware perspective, memory writebacks typically not locate on the critical path, miss status holding register


% It guarantees since limited on-chip resources, easy to verify correctness
% About virtual page to physical page address transmission
% Logging
% Leverage CXL's coherence property to expose information like gc, memory managing.
% Hash reclaiming, garbage collection. 


% We keep the original CXL's strict coherency for the metadata field, while loosen the record field's coherence protocol to enable host's caches to be stale. 
% To this end, we only preserve the ``true data races'' between committed RAW and WAW on the record field. 

% The protocol is built upon CXL's coherence primitives to minimize architecture changes. 
% The coherence manager exploits the preknowledge of the application to identify what are true data races with regarding to transaction processing systems. 
% Specifically, \name~only preserves the coherence between RAW and WAW for committed transactions.
% is making the CXL memory the centralized of coherence. 
% The memory always hold most up-to-date versions, while the cached versions may be stale or over-speculative. 
% Moreover, to make hardware simple and practical, \name~adopts a memory-centric coherence architecture that host processor only preserves coherence with the centralized and customized CXL memory manager, but could not communicate with each other. 
% Caches communicate only with memory using standard memory primitives to keep hardware simple, but caches could not talk to each other. 
% Stale cache invalidation is done asynchronously with common memory access, in a bulked way to minimize the cost. 


% , but no ordering limits between uncommitted operations. 
% The ordering limits are further relaxed when one conflicted operation is from an aborted transaction, since it's unnecessary to serialize a memory operation that will be discarded. 

% It distinguishes between two types of memory operations: ordinary and synchronization. RC introduces two synchronization operations: acquire and release. The acquire operation is performed before a process enters a critical section, ensuring that any updates made in other critical sections by other processes are visible. The release operation is performed when exiting a critical section, signaling that all updates made within the critical section are now visible to other processes. 
% RC follows two key ordering principles: 
% First, operations within a critical section can be reordered, but synchronization operations (acquire and release) enforce a partial ordering. Second, operations before a release cannot be reordered to after it, and operations after an acquire cannot be reordered to before it.




\ifx\undefined\stale
\noindent \textbf{G\#1: Hardware compatibility.} 
The design should not disturb neither host processor architectures nor CXL specifications.
Because recent processor has plenty of confidential and undocumented features~\cite{tsx, directory_sp19}. Alterations to them could introduce unforeseen effects in performance or even violate system correctness, posing significant challenges in system verification and debugging.
% Even the official cache innovations, i.e. Transactional Synchronization Extensions (TSX) from Intel, has also been suspected in CPU models post-Skylake, due to security vulnerabilities and functional anomalies with existing architectures.
% The primitives change the coherence protocol of existing CXL architectures. It may cause modifications to the CXL IP on both RC and EP sides. 
% However, the 

\noindent \textbf{G\#2: Universal support on transaction systems. } 
The design should ensure primitive compatibility with existing transaction processing systems. These systems are different in consistency models, data structure organization, and scheduling policies, and asks for the \name~system to provide a single-node like memory management methods, such as \textit{malloc} and \textit{free}. 
Keeping compatibility with the various software systems enables our proposal to be treated as an inexpensive feature that can be adopted without effecting other services. 
% The key is to support various software policies developed by database sectors to fit the diverse usage cases~\cite{abyss_vldb14, polyjuice_osdi21, mvcc_vldb17}. 

\noindent \textbf{G\#3: Low performance overhead. } 
It's also important for the design to introduce minimal overhead on the critical path, ensuring that the proposed primitive has performance advantages to CXL-vanilla primitives. It requires \name~implements memory loads and stores with the minimized hardware overheads. No software overheads, such as interrupts or context switching, are allowed since a primitive only takes hundreds of nanoseconds, but the software intervention will introduce several microseconds overheads. 
\fi

% Building up the data paths for \name~primitives pose three novel challenges.
% Recall that we aim the system to be hardware compatible (\textbf{G\#1}), system policies support (\textbf{G\#2}), and low performance overheads (\textbf{G\#3}). 

\begin{figure}[t]
  \centering
   \includegraphics[width=0.45\textwidth]{my/figs/overview.pdf}
  \caption{
    \name~overview.
    % (a) shows that the cluster with 4 nodes directly connects to the \name~device that manages the shared memory pool. (b) shows that the \name~device connects to the host processors with standard CXL protocol, but selects accessing primitves based on where the accesses locate.
  }
  \label{fig:overview}
\end{figure}

\subsection{Design Challenges}

% Implementing the L-Ld and L-St requires isolating accesses across nodes. Simply disabling \cxlbi~for G-FAM accesses does not work due to the cache eviction problem. The processor may evict the modified contents unpredictably. It is hard to intervene cache evictions by instructions since it is managed by uncore cache agents and being transparent to cores. 

% to the shared memory, so that the following loads from other nodes will observe the modified contents. 



% inserting instructions such as CLFLUSH or FENCE~\cite{intel-doc} could not address this issue. 

% so that it's impossible for software-only approach that manipulates cache eviction by inserting instructions such as CLFLUSH or FENCE~\cite{intel-doc} to codes, neither possible to change processor architectures (\textbf{G\#1}). 

% If a load is issued after such dirty eviction, the load would observe the previous memory store from another node. 
% It seems straightforward to implement L-Ld and L-St with normal \cxlmem~accesses without adopting \cxlbi~for coherence maintenance, since processor caches are naturally isolated across nodes. 
% , like previous single node NVM frameworks. 
% eviction could happen at any time due to cache conflicts, and the eviction

% effective use of granular memory synchronization techniques that were possible in earlier weakly consistent architectures.
% The CXL memory accessing sub-protocol \cxlmem~does not differentiate cacheline flushing and cacheline eviction. Both of them are translated to CXL writeback request. This means that GSync could not be implemented by the cacheline flushing as previous weakly consistent memory architectures did~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16}, since the EP could not distinguish the GSync flushing from normal cache evictions incurred by uncore cache agents. 
% In addition, current x86 architectures do not allow cache self-invalidating a cacheline without written it back, which is required by the Wd semantic. 



% Wd withdraws the content of a store from the requesting node. It involves self-invalidating the cacheline on processors to avoid future reads observing the store. However, current x86 architectures do not allow cache self-invalidation initialized by the processor instructions (\textbf{G\#1}). 


\noindent \textbf{Challenge\#1: Being Compatible with Processor Cache Design.} 
Implementing L-Ld and L-St requires isolating accesses across nodes. 
Merely disabling the \cxlbi~for shared memory accesses does not suffice to implement L-Ld and L-St due to cache eviction issues. Recall that CXL uses write-back protocols~\cite{cxl-shortdoc, cxl-doc}, meaning that stores always hit local caches. However, cache eviction can inadvertently leak modified contents from the private cache to the shared memory, allowing subsequent loads from other nodes to observe these modified contents. 
Worse still, cache eviction is managed by uncore cache agents, which operate transparently to cores.

\noindent \textbf{Challenge\#2: Being Compatible with CXL Protocols.} The CXL protocol does not differentiate between cacheline flushing and eviction; both are treated as \cxlmem~writeback requests. This conflation prevents the use of cacheline flushing as a synchronization point (to implement GSync), which was possible in earlier weakly consistent architectures~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16}. Moreover, current x86 architectures do not support the Wd semantic that merely self-invalidates a cacheline without writing it back.

\noindent \textbf{Challenge\#3: Being Compatible with Software Policies.} To fully exploit the advantages of the hybrid primitive, our system should allow application processes to choose which primitive to use for each data structure they allocate. Simply partitioning the address space to map to different primitives is too rigid to accommodate the diverse requirements of various use cases.


% It's non-travail in current architectures since CXL does not allow the RC to embed extra bits of primitive choosing within requests. Although we could pass primitive choosing via side channels, such as \cxlio~or~\cxlcache, on each memory accesses. This approach would incur unacceptable overheads due to additional RC-EP communication and essential data fences to ensure the order. 
% Optimally leveraging the capabilities of hybrid memory primitives requires runtime primitive binding on data allocations, which is a feature not currently supported by CXL. 

% Implementing this through sending primitive information along with each memory accesses with side channels, such as \cxlio~or~\cxlcache, introduces significant overhead due to increased communication between the processor and the G-FAM endpoint, as well as the necessity for strict data ordering. 

\ifx\undefined\stale
Simply discarding the hardware cache coherence for record field may work well at functionality. Such a strawman approach disables the back-invalidation channel by default and a transaction explicitly flushes what it writes when commitment. 
Once received the cache write back requests, the home node (CXL device in this example) invalidates all peer caches to force processors reading the up-to-date data from memory. 
This approach is widely adopted in weakly consistent memory, such as GPUs, IBM Power~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16}. 
Synchronization is maintained by the coherent metadata field which supports the atomic operations. 
% Note that the strawman architecture maintains coherence for metadata fields to support atomics operations, enabling the concurrency control to work correctly. 
% Coherence is maintained for metadata field to support atomics operations, so that concurrency control schemes can detect and resolve conflicts on accesses toward the record field. 



% \red{More Details.}
However, this strawman approach has two performance drawbacks. 
First, dirty writes could be flushed back to memory at any time before the transaction commits, due to either cacheline eviction or compiler-added memory fences. 
Without modifying the CXL protocol, the device is unable to distinguish the cacheline eviction from cacheline flush, since they share the same cacheline write-back flow in \cxlmem~\cite{intel_cxl_pub, cxl-paper, cxl-doc}. 
As a consequence, this approach incurs the unnecessary invalidation snooping request at every dirty cacheline eviction. It not only wastes CXL bandwidth, but also enlarges the average latency for an LLC cache miss.
% When an LLC cache miss selects an CXL-backed cacheline to evict, it introduces the aforementioned eviction flow in the critical path.
% This approach involves unnecessary CXL traffic due to the cacheline eviction. 
% It's always the case for the concurrency control algorithms that are performing in-situ updates, such as 2PL and TS-ordering. 
Second, the explicit cache flushing instruction (e.g. CLFLUSH in Intel~\cite{intel-doc} processors) is costly in modern processors, since they need to clear its write buffers to keep memory consistency. As pointed by previous work~\cite{dynamicamo_isca23}, the cacheline flush takes xx more time than a memory write. 
Moreover, this approach still requires the centralized SF to route invalidation snooping requests correctly. 



However, the cache eviction would break such isolation since it would leak the modified cachelines from the private cache to the shared memory. If a read from other nodes is issued right after the eviction of a written cacheline, it would observe the written data thereby violating primitive semantics. The cache eviction could happen at any time due to cache conflicts. In typical x86 architectures, the cache eviction is managed by the uncore cache agents which are transparent to the processor's pipeline. Hence it's impossible to address the eviction issue by inserting special instructions such as CLFLUSH or FENCE to codes, neither by changing the processor architectures (\textbf{P1}). 

\fi



\ifx\stale\undefined
\name~connects each hosts with PCIe Gen5 x16 links to provide similar memory bandwidth of local DRAM. The connection uses either on-board connections or the xxx links extended from the PCIe slot on mother board. Ideally, each host directly connects to each \name~with individual physical links. This results in a cluster with N nodes and M devices using NM links, and occupying M PCIe x16 slots on each processor. It's challenging for the commercial system to connect a large amount of \name~cards due to limited PCIe slots on the IO die. 
Users can adopt CXL switch to reduce the occupation of IO slots by connecting processors to the switch's upstream ports, and connecting devices to the downstream ports. This approach will lead to a higher latency by the estimated 100ns for traversing a CXL switch. 
% To achieve higher scalability, multiple hosts can share the physical device ports by first connecting to a CXL switch's up-streaming port, then share the same down-streaming port to connect the \name~device. 
% Moreover, since we limit the memory sharing within a rack, the wire latency is easy to guran
We limit the memory sharing field within a rack with up to 16 nodes since this scale keeps the sweet point between performance and actual use cases. 
\name~assigns an individual CXL driver IP to each host despite they may share the same physical link when adopting CXL switch. Each \name~device manages up to 1TB~\cite{anns, cxl_samsung} DRAM with device internal memory controllers. 
\fi
% When the system includes multiple \name~devices, they will work independently and share nothing, so that no inter-device communication is allowed in order to avoid deadlocks, we will detail the discussion in Sec.~\ref{sec:}. 


\ifx\stale\undefined
Commercial cluster deployment
Supporting variety of concurrency control algorithms
Supporting essential components of IMDB
\fi


% A MESI protocol—variants of which are used by modern Intel servers [42]—adds the Exclusive state as an optimization, where E encodes clean+writable. The extra E state avoids the need to obtain write permission after fetching private data (i.e., data only cached on a single core), reducing coherence traffic.

\ifx\stale\undefined

\subsection{The Metadata Field: Atomic Offloading}

\noindent \textbf{The write amplification problem. }

\noindent \textbf{Our approach. }

\fi

\ifx\stale\undefined

In such a model, \textbf{every write appears atomically} to the memory subsystem, in order to keep the single-writer-multiple-reader invariants for every object. A read must returns with the value of most recent write to that object. 
% This feature is termed as the ``write atomicity'' when referring to memory consistency models. 
This feature is held in both hardware cache architecture such as SC, TSO, RMO, Alpha, and ARMv8, and in most software distributed shared caches~\cite{gam, }. CXL achieves this via a Global-Observation (GO) based MESI protocol, as illustrated in Figure~\ref{fig:motivation-over-coherent}~(a). 
% To this end, when we consider the data field, the strict coherence model poses the ordering constraint between the uncommitted reads and writes. 
% To this end, a cache miss read toward an object that is already cached by other nodes, or writing a shared object will traverse the slow core-to-core coherence path. 

However, transactions preserves a more loose \textit{strict serializable consistency model}. \blue{briefly introduce other models, i.e. read-isolation} This model maintains an illusion of a single machine that executes transactions one at a time, in the order with respect to the real time~\cite{farm, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13}. Record operations are packed and ordered at the granularity of a transaction. The key of the strict serializable consistency is that the global order of the observed reads and writes~\cite{rss_sosp21, acid_79} should refer to the serialized time of their belonging transactions, but not the physical time they are executed. 
% , while the cache coherence orders these operations individually. 
We refer this phenomenon to the \textbf{non-atomic write} in transaction processing, where a write is logically executed at some point of the transaction, but it takes effect only after the transaction commits. 
A read is consequently able to return a stale value if the write is serialized after that read. 

\fi

\ifx\nocmt\undefined
Considering a read from transaction A returns the value written by a concurrent write from transaction B, the read imposes a global constraint on all A's reads that they all must return the new value, even if the write has not yet finished. 
\fi

% However, when we refer transaction processing, the ACID strict serializability indicates non-atomic writes that a transaction should commit all its writes after it ensures no conflicts with other transactions, as we discussed in Sec.~\ref{subsec:kvs_primary}. 

% CW, CR, UR, UW, strong coherency maintains what ( each relation requires coherence roundtrip ), transaction requires what

% This results in what ?
% Long latency read-Read False sharing: we do not need coherence
% Redirected write / bounded write. 


\subsection{Architecture Overviews}

%
% As shown in Figure~\ref{fig:overview}~(a), an \name~cluster contains multiple regular servers equipped with processors implementing the standardized CXL RC IPs, and a Top-of-Rack (ToR) EP device managing all shared memory. As Figure~\ref{fig:overview}~(b) shows, the \name~system contains three main components. 

\name~integrates multiple components to address these challenges. 
As Figure~\ref{fig:overview} shows, we employ a hardware agent (CTHW), which implements \name~primitives in the G-FAM to address the cache eviction problem for L-Ld and L-St (\textbf{Challenge\#1 and \#2}) and to enable synchronization primitives with a different mechanism (Sec.~\ref{subsec:vms},~\ref{subsec:vat}, and~\ref{subsec:vsf}). 
Additionally, a user-level library (CTLib) facilitates the selection of memory primitives for individual data structures (\textbf{Challenge\#3}), enhancing software integration with the hardware capabilities (Sec.~\ref{subsec:system_integration} and~\ref{subsec:memory_mapping}). 
Moreover, a helper thread (CTRt) adjusts the hardware configuration to maintain system efficiency (Sec.~\ref{subsec:vat_resizing}). 

\name~adopts a non-transparent interface for memory management calls such as \textit{cxl\_alloc} and \textit{cxl\_free}, but keeps accessing approaches the same as local memory. System developers can bind the primitive to each allocation with passed arguments and load or store data as they would with local memory. At the point of transaction commits, the developer should invoke the GSync primitive to synchronize record writes globally, and if the transaction aborts, they should call the Wd primitive to withdraw stores.


% Together, these components form a cohesive system that addresses the intrinsic limitations of existing architectures and optimizes the transaction processing capabilities of CXL-based systems.

% Each host connects to the EP with PCIe Generation 5 $\times$16 links which could be on-board links or extensions from the motherboard's PCIe slots (e.g. MCIO). 

% The CTHW module locates at the CXL device implemented as the ASIC logic by design. It works on the memory access path and implements the \name~primitives. The CTLib provides lightweight APIs for applications to invoke the primitives and choose primitives while allocating on the shared memory. The CTRt works aside the applications threads, and monitors the CTHW status and conducts reconfiguration in background. Details about these modules are introduced as follows: 
\ifx\undefined\stale
\noindent \textbf{Hardware Agent (CTHW). } The CTHW is a device-only hardware module that implements \name~primitives in ASIC by design. To avoid cache eviction from leaking the modified contents (\textbf{Challenge\#1}), CTHW adopts a view memory shim (Sec.~\ref{subsec:vms}) model to implement the copy-on-write semantics, and introduces the architectural supports (Sec.~\ref{subsec:vat}) that achieves both high efficiency and low hardware overhead. To overcome the limits of write-back caches and CXL protocols (\textbf{Challenge\#2}), CTHW introduces a queue-based approach (Sec.~\ref{subsec:vsf}) for the GSync and Wd primitives.


% We separate the data path working for the standard memory accessing primitives of L-Ld/L-St, and the synchronization primitives of GSync and Wd. 
% To keep the L-Ld and L-St semantic that addresses leaked contents (\textbf{Challenge\#1}), we propose a high-level design to redirect memory stores to a node private buffer called the view memory shim (Sec.~\ref{subsec:vms}). We further illustrates the architecture support named view address translation flow  and discusses implementation optimizations to minimize the redirection overheads. 
% To overcome the limitations on GSync and Wd (\textbf{Challenge\#2}), we propose a entirely different task offloading datapath, called view synchronization flow (Sec.~\ref{subsec:vsf}). It implements the distributed approximate SF, and adopts existing CXL subprotcols to achieve coherent with low overheads. 

% synchronization flow. All \name~hardware agents for different nodes are connected through an internal bus, and the synchronization flow would send invalidation messages with each other via this bus (if necessary). 
% An incoming request that arrives at the device first travels through the CXL IP to process CXL protocol stacks, the 
% , and forwarded to \name~HW agent's different data pathes based on the accessing types. 

% the device equips each node with a standard CXL IP provided by the vendors, an \name~HW agent accommodating all customized logic, and a common DRAM controller.

% As Figure~\ref{fig:overview}~(c) shows, the device equips each node with a standard CXL IP provided by the vendors, an \name~HW agent accommodating all customized logic, and a common DRAM controller. An incoming request arrives at the device and travels through the CXL IP to unpack and get the accessing type and the target addresses, and forwarded to \name~HW agent's different data pathes based on the accessing types. If the request is a common memory load or store, it is routed to the view address translation flow that performs the address translation to get the DRAM address. Afterward, the actual memory access is conducted by the memory controller, and the response is formed and sent out through the CXL EP IP stack. 


\noindent \textbf{User-Level Library (CTLib). } CTLib proposes a memory-mapped primitive choosing approach (Sec.~\ref{subsec:memory_mapping}) to allow the fine-grained primitive binding on data structures. Moreover, CTLib is carefully integrated into OS (Sec.~\ref{subsubsec:system_integration}) to avoid software overhands on memory loads and stores (\textbf{Challenge\#3}). 


% To allow the fine-grained primitive binding on data structures, we propose a memory-mapped primitive choosing approach (Sec.~\ref{subsec:memory_mapping}) that leverages the address invariant of the direct memory access file systems (DAX)~\cite{pmdk, dax} provided by the operating systems (Sec.~\ref{subsubsec:system_integration}). 
% , i.e. direct memory access file systems (DAX)~\cite{pmdk, dax}, 

% on common memory accesses. 
% To this end, every allocation on the shared memory could specify a primitive that following memory accesses will obey.
% Similar to recent distributed memory proposals~\cite{gam}, our current implementation adopts a non-transparent interface for memory management calls such as \textit{alloc} and \textit{free}. 
% The \name-managed shared memory occupies a continuous segment of each process's virtual address space. An application can perform a set of virtual memory operations to the shared memory segment, such as \textit{cxl\_alloc} and \textit{cxl\_free}. 

\noindent \textbf{Helper Thread (CTRt). }The performance of CTHW data path is highly effected by the application memory accessing characteristics, thus the CTRt monitors the hardware status and perform CTHW reconfiguration (Sec.~\ref{subsec:vat_resizing}) to avoid CTHW bottlenecks the system (\textbf{Challenge\#3}). 
% It leverages the CTLib's primitive mapping capability to conduct the reconfiguration. 
% The CTRt is a co-routine that is initialized at the start of application process, it relies on the CTLib to map the CTHW's internal data structures to a special memory space, and changes them with normal memory accesses. 

\fi
% As Figure~\ref{fig:overview}~(b) shows, transaction processing services run at worker nodes on top of our user-space library called \textit{CTLib}. It is in charge of memory management and primitive issuing. There is also a \name~helper runtime thread \textit{CTRt} invoked along with the application as a co-routine. The \textit{CTRt} is in charge of hardware monitoring and on-fly reconfiguration. 


% \name~employs PCIe Generation 5 x16 links to connect each host, providing bandwidth comparable to that of local DRAM. Connections are facilitated through on-board links or extensions from the motherboard's PCIe slots (e.g. MCIO). Ideally, the connection of each host adopts a direct and an independent physical link. 
% But that limits the scalability of numbers of the device, since each device could comsume a PCIe x16 slot of the IO die. 
% But that results in a network topology of N nodes and M devices utilizing NM links and occupying M PCIe x16 slots per processor. It limits the scalability of both number of nodes and devices for commercial clusters due to the limited number of PCIe slots available on the processors' I/O die. 
% To mitigate the scarcity of I/O slots, users may implement a CXL switch that connects each processor and device with one x16 link. This configuration incurs an approximate latency increase of 100 nanoseconds due to the switch traversal. 
% Users could involve a Top-of-Rack (ToR) CXL switch to pool CXL devices with one link for each node, but the switch incurs an approximate latency increase of 100 nanoseconds~\cite{pond, memtunnel, google_cxl}. 


% Caches only retrieve and write back data from/to the memory using standard memory primitives. 

% \name~keeps each node with an isolated view by tracking its eviction set from the processor. 
% No conventional, CXL's MESI style cache coherence protocol is used to resolve conflicts between different views, so that it is legal for multiple nodes to hold the same object in modified forms. 
% The remote DRAM keeps a consistent snapshot which only stores the committed writes. 
% Each transaction could only read and write the KVS that corresponds to its executing node.
% Coherence between different views and the snapshot is kept at the boundary of transactions, by publishing the writes in a batched form to reduce write cost.
% A VMS object is created when a transaction reads it from the consistent snapshot, and is installed to it once the transaction commits. 
% A VMS object is created when a transaction accesses the record, and is synchronized with the consist snapshot once the transaction commits. 


% , while only communicating with processor in standard CXL protocols. 

% \name~implements the L-Ld and L-St primitives with the standard \cxlmem~requests, and adopts \cxlcache~for the GSync. 

% First, ReadAny (RA), which aims to get the most recent value of the given address. Second, ReadExclusive (RE), which aims to get the value as well as requires for an ownership of the cacheline with given address. It's usually issued by a memory store, which follows with a silent write on the exclusively owned cacheline. 
% Third, WriteBack (WB), it writes the dirty cacheline which is evicted from the host L2 or LLC back to the HDM. A WB could be issued by a cache eviction or a cache flushing (e.g. CLFLUSH~\cite{intel-doc}), but unfortunately, the EP can not distinguish them. 

% 
% , if there's a potential updated cacheline on other nodes, i.e. in the Exclusive state, the EP should send a BI requests to get the value. Otherwise, the EP accesses the HDM and returns the value in DRAM.
% The EP would invalidates all peer caches and selects the value as ReadAny does. 

% However, this strawman approach has two performance drawbacks. 
% First, dirty writes could be flushed back to memory at any time before the transaction commits, due to either cacheline eviction or compiler-added memory fences. 
% Without modifying the CXL protocol, the device is unable to distinguish the cacheline eviction from cacheline flush, since they share the same cacheline write-back flow in \cxlmem~\cite{intel_cxl_pub, cxl-paper, cxl-doc}. 
% As a consequence, this approach incurs the unnecessary invalidation snooping request at every dirty cacheline eviction. It not only wastes CXL bandwidth, but also enlarges the average latency for an LLC cache miss.
% % When an LLC cache miss selects an CXL-backed cacheline to evict, it introduces the aforementioned eviction flow in the critical path.
% % This approach involves unnecessary CXL traffic due to the cacheline eviction. 
% % It's always the case for the concurrency control algorithms that are performing in-situ updates, such as 2PL and TS-ordering. 
% Second, the explicit cache flushing instruction (e.g. CLFLUSH in Intel~\cite{intel-doc} processors) is costly in modern processors, since they need to clear its write buffers to keep memory consistency. As pointed by previous work~\cite{dynamicamo_isca23}, the cacheline flush takes xx more time than a memory write. 
% Moreover, this approach still requires the centralized SF to route invalidation snooping requests correctly. 


% Despite the GSync share the similar semantics with cache flushing, the EP can not distinguish the cache flush (e.g. via CLFLUSH~\cite{intel-doc}) from the cache eviction, since they are both translated to a dirty cacheline write in CXL. M

% , so that a memory store is inherently invisible from other nodes since it would hit local caches and not change the shared HDM. 
% and enable the EP explicitly flushing modified cachelines. 

% We found the hardware coherence is an over-kill for maintaining transactional data isolation. 
% that a transaction should either make all its tuple writes atomically visible to other transactions or abort, rendering none of the writes visible. 
% It prevents any transaction from observing an intermediate state with any speculative writes. 
% \textbf{This transaction definition inherently renders writes as non-atomic}, 
% so that a write is issued during the execution phase but only takes effect after the transaction commits. 

% As a consequence, running bare transaction reads and writes without concurrency control, as we show in Figure~\ref{fig:motivation-over-coherent}~(b), will easily break the transaction correctness definition. In this example, transaction-2 reads the uncommitted write from transaction-1, thereby breaking the atomicity. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.40\textwidth]{my/figs/cxl_version_management.pdf}
%   \caption{
%   The time ratio spent on version management with regard to throughput degradation. We evaluate the concurrency control approaches on TPC-C with 1, 4, 8, 24, 48 warehouses. 
%   }
%   \label{fig:cxl_version_management}
% \end{figure}

\ifx\stale\undefined

The conflicts between CXL's atomic writes and transaction's non-atomic writes requires an overlayed memory layer to isolate uncommitted writes from being observed by others. It's typically provided by concurrency control, and is termed as ``version management''. 
Commonly adopted approaches include in-situ updates and write buffers. In in-situ updates, transactions directly update the tuples and record the values that have been overwritten. The recorded values are recovered in case of aborts~\cite{s2pl_csur81, abyss_vldb14}. This approach assumes the pessimistic execution and depends on the control fields, as we will discuss in Sec~\ref{}, to mutually exclusive. 
% The underlying strictly coherent memory ensures other transactions observe the new value immediately. 
On the other hand, the write buffer approach allocates temporal local buffers for each transaction, and redirects speculative writes to the buffers while keeps the shared tuple unchanged. It flushes the buffer contents when commits~\cite{silo_sosp13, hekaton_sigmod13, occ_tbs81, farm_nsdi14}. 
Modern concurrency controls interleave both of them within a transaction execution according to the application contention status~\cite{tictoc, polyjuice_osdi21, cormcc_atc18}, or allow multiple record versions co-exist~\cite{mvcc, mvcc_vldb17}. 

The version management is the well-known bottleneck, especially for the optimistic algorithms such as \textit{OCC}, \textit{TICTOC}, and \textit{SILO}~\cite{full-story, abyss, deneva}. The main overheads come from buffer allocation and reclamation and memory copying for each written tuple. Figure~\ref{fig:cxl_version_management} shows the version management bounded algorithms spend over half of the time on this procedure, which become the main reason of the up to 60\% overall performance loss. 

\fi


% We consequently build a loosed coherence model for such operations, while preserving 


% \subsection{Approach Overview}

% The main object of this work is to make a CXL-augmented KV store faster than an \textit{oracle}.
% % , which stores all KVS tuples within local-attached DRAM and accommodates workers on the same socket. 
% % The \textit{oracle} also 
% % \textit{oracle} that works on single socket and local-attached DRAM. 
% % To mitigate the CXL's performance degradation,
% Achieving this performance requires fast concurrency control with novel hardware supports to mitigate the performance overhead of CXL protocol, especially on coherence resolving latency. 
% To this end, we propose \name, a hardware-software co-designed concurrency control approach that avoid coherence messages between nodes. 

% \name~addresses the aforementioned challenges of CXL memory share by leveraging the record field's loose coherence requirements. 
% It keeps each node with a isolated virtual memory space (VMS). 
% No conventional, CXL's MESI style cache coherence protocol is used to prevent conflicted versions of the same record between VMS, so that it is legal for multiple nodes to hold the same record with conflicted modified forms. 
% All nodes also share a consistent snapshot of the KVS, but transactions can only work on the VMS. 
% A VMS object is created when a transaction accesses the record, and is synchronized with the consist snapshot once the transaction commits. 
% To this end, the VMS enables a record access with the similar latency of directly accessing the remote memory without cache coherence overheads. 



\ifx\stale\undefined

The cache copy is flushed to the CXL DRAM only when a transaction commits, and is discarded once aborts. 
To this end, we eliminate the need of software memory overlay and its related performance cost. 
We leverage existing CXL subprotocol requests to implement the decoupled store without disturbing host processors' architectures. 
However, many issues remain to ensure corretness, we leave the detailed discussion in Sec.~\ref{subsec:}. 


Different with all of the overlayed approaches, we resort to directly change the CXL hardware consistency model to make it match transaction processing. We 
% at the memory layer, so that a no-concurrency control transaction can execute correctly without software version management. 
To be precise, we decouple a store instruction to two memory references: store issue, and store commit. 
Stores that are issued but not committed are kept within the node's caches for the duration of the transaction in order to keep invisible from other nodes. 
No conventional, CXL's MESI style cache coherence protocol is used to maintain a global order of conflicted writes. As a consequence, it is legal for multiple nodes to hold the same record with conflicted modified forms. 
The cache copy is flushed to the CXL DRAM only when a transaction commits, and is discarded once aborts. 
To this end, we eliminate the need of software memory overlay and its related performance cost. 
We leverage existing CXL subprotocol requests to implement the decoupled store without disturbing host processors' architectures. 
However, many issues remain to ensure corretness, we leave the detailed discussion in Sec.~\ref{subsec:}. 

\fi

% , especially when the access is skewed. 
% Obliviously adopting single-node concurrency control approaches on CXL suffers from the performance loss. 





% This is also the case in CXL-based memory pool, 

% An undo-log holds the values that have been overwritten in an STM using eager version management, and a redo-log holds the tentative writes that will be applied during commit in a system using lazy version management.

% (\textit{WAIT\_DIE} and \textit{NO\_WAIT}) that performs in-situ updates and records the old version in case of aborts\cite{s2pl_csur81, abyss_vldb14}, as well as undo logs that 

% The formal approach use locks or monotonically increasing timestamp to specify the serialized order. 
% In variants of two-phase locking (2PL), transactions have to acquire a lock on a particular element before they are allowed to read or write that element. 
% % If fails, the transaction is forced to wait on the lock or abort. 
% % If a transaction is unable to acquire a lock for an element, then it is forced to wait until the lock becomes available or abort then restart. 
% % The in-place write protocols need to buffer the old value of writes to recover the state once the transaction aborts. 
% % TO specifies the serialization order of transactions before they are executed. 
% In timestamp ordering, each transaction is assigned with a timestamp in priori, and it is only permitted to accesses the record that is written with a smaller timestamp. 
% On the other hand, the latter approach temporarily store all of its write operations in a private buffer, then the DBMS validates the conflicts and 
% installs the buffer to database once commits. 
% % applies the writes from the transaction’s buffer into the database once commits. 


% We transaction atomicity is broken by the CXL's strict coherency thereby allowing transaction-2 reads the uncommitted write from transaction-1.
% A typical concurrency control approach is a software inter-layer, as shown in Figure~\ref{fig}, that isolates record reads and writes from directly taking effects on the memory. 
% This requires software scheduler, known as the concurrency control protocols, to re-order the reads and writes from when they are issued by the processors. 


% \textbf{a write atomically takes effect at the time it is issued to the memory system.} This implies that the precise moment a store request is written to the memory hierarchy corresponds to the instantaneous processing of the store within a monolithic memory abstraction~\cite{rvweak_isca18, spandex_asplos18}. The outcome of a read operation always return the data from the most recent write to that object. 
% CXL achieves this via a Global-Observation (GO) style cache coherence protocol, as illustrated in Figure~\ref{fig:motivation-over-coherent}~(a). 
% \wz{More details. Atomically. }
% The read from node-2 returns with the value of the write from node-1. 
% This feature is termed as the ``write atomicity'' when referring to a memory consistency model in hardware architecture. 

% However, these concurrency control algorithms introduce serious performance degradation. 
% We use the term ``version management'' to dictate the process of allocating/resuming thread-private buffers and copying data between them and the database.  
% To be precise, the pessimisitc writes should buffer old values for recovery, and the optimistic writes should buffer new values. 
% We quantify a wide range of concurrency control protocols and illustrate how much the version management contributes to execution time in Figure~\ref{fig:}. 



% \section{Approach Overview}



% access into two classes: conflict detection and version management. Conflict detection accesses relate to  with , including locks, latches, and timestamps. 

% the over coherent problem associated with data fields and the cacheline thrashing problem of the control fields. To be clear, the data field dictates the actual KV pairs that the DBMS stores, which is specified by the application and does not relate to the DBMS. The control field contains the DBMS internal objects that guarantee mutual exclusive access on critical sections, such as locks, latches, and timestamps. 

% that are responsible for concurrency controls to serialize transactions. For example, a typical lock-based approach assigns each row with a latch and a wait list, and in Silo~\cite{silo}, it's a word containing the most recent update transactions' ID. 
% We argue a over-coherent problem for the data field and the thrashing problem for the control field. 

% memory shim layer between the transaction algorithms and CXL memory pool. 
% \name~provides two memory primitives: isolated write, and conflict tracking reads, to push the critical operations of concurrency control down to the memory layer. 
% \name~is designed be independent to specific concurrency control protocols, since most DBMS inter-mix multiple algorithms within a transaction~\cite{polyjuice, ic3, others} to gain high performance. 
% We now describe the rationale behind the shim layer and why it outperforms the vanilla combination of CXL memory sharing and software-only concurrency control.

% We now describe the rationale behind our decision to build a new hardware primitive for the CXL data stores. 
% Almost all existing concurrency controls depend on the basic assumption of coherent read/write in hardware to ensure correct access order on records.




% We implement these memory references by 


% Despite hardware transactional memory~\cite{logtm_hpca06, overlaytm_pact19, flextm_isca08, vtm_isca05} have similar concepts with our approach, they necessitate changes in processor cache designs and the integration of extensive logic into CPU circuits. 
% Such radical modifications are overly ambitious for recent CPU architectures, rendering them impractical in today's systems. 



% Despite the implementation of these two fields varies in different scenarios, the typical data field will take up 90\% space of the tuple. 
% Interestingly, the relatively small control fields consumes considerable overheads as Figure~\ref{fig:} illustrates. The results follows the previous works' conclusions that accessing the control field takes most time at the high contention, while accessing data field dominates at low contention. 



% We first determine the performance slowdown


% Replacing networks with CXL-based share obviously brings performance gains due to its lower latency. 
% But it still suffers from performance degradation when compared with an oracle. 
% Accessing the CXL-based memory pool contributes to the most time. 
% despite the baseline outperforms network-based systems thanks to the CXL's performance advantage, it's not yet efficient for cross-host transaction processing when compared with oracle. \wz{Results analysis} 

% \subsubsection{Lock Thrashing Problem}





% We compare the baseline system with an \textit{oracle}, which stores KVS tuples in local DRAM and accommodates all worker threads on the same processor. 

% behind why \name~fits transactions better than CXL's vanilla primitives.  
% \name~is compatible with most concurrency control protocols, and incurs no need to change the processor's architecture and the CXL IP. 
% \subsection{An Oracle Concurrency Control}





% The key of this approach is to leverage the processor's cache hierarchy to buffer conflicted writes, and to use CXL's cacheline-level 


% , such as Silo~\cite{silo_sosp13}, Hekaton~\cite{hekaton_sigmod13}, and FaRM~\cite{farm_nsdi14}. 
% Some modern concurrency control algorithms~\cite{tictoc, polyjuice} also tend to interleave the 2PL and TO to adapt to the application contention status. 

% protocols adopt two-phase locking (2PL) or timestamp ~\cite{s2pl_csur81, abyss_vldb14} 
% The 2PL assigns the lock for a particular element in the database. Transactions have to acquire such a lock before they are allowed to read or write that element.
% Conflicts are resolved by 
% % The typical read-write lock allows concurrent reads and exclusive writes. 
% If a transaction is unable to acquire a lock for an element, then it is forced to wait until the lock becomes available (termed as \textit{deadlock-detection}), or abort then restart (termed as \textit{deadlock-prevention}). TO assigns each transaction with a unique and monotonically increasing timestamp before it starts.

% such a conflict between the transactional consistency model and the coherence model, the write toward X remains invisible from other transactions in this interval, and a concurrent read (R(X) from transaction-2) should return with the old value. 
% \wz{How the atomics are implemented in this strict coherence model ?.}


% \subsection{Software-Only Concurrency Control}
% % This fundamental mismatch between hardware coherence model and transaction ordering renders the system to re-order the reads and writes from when they are issued by the processors. 
% To provide the transaction's illusion that operations from a transaction are executed isolated and serializable, the DBMS should re-order the reads and writes from when they are issued by the processors. 
% Most existing work rely on additional metadata to schedule transactions with concurrency control algorithms. 
% %  to re-order the reads and writes from when they are issued by the processors. 
% \blue{This work follows the canonical classification that all concurrency control schemes are either a variant of two-phase locking (2PL) or timestamp ordering (TO) protocols~\cite{s2pl_csur81, abyss_vldb14}.} 
% 2PL prohibits the speculative writes from being observed via the use of locks. Transactions have to acquire the lock for a particular element in the database before they are allowed to read or write that element. 
% A typical system implements the read-write lock on tuples, where read locks can be held by multiple transactions, but write lock gains exclusive access to the tuple so that it conflicts with any other read or write locks. 
% If a transaction is unable to acquire a lock for an element, then it is forced to wait until the lock becomes available (termed as \textit{deadlock-detection}), or abort then restart (termed as \textit{deadlock-prevention}). 
% TO specifies the serialization order of transactions before they are executed. Each transaction is assigned with a unique and monotonically increasing timestamp in priori, and it uses this timestamp to process conflicting operations in the proper order. For example, in optimistic concurrency control (OCC)~\cite{silo_sosp13, hekaton_sigmod13, occ_tbs81, farm_nsdi14}, a transaction tracks its read/write sets and temporarily stores all of its write operations in a private buffer. \red{After all read and write operations, OCC reads its read set again to check any transactions with larger timestamps modify them.} If no change exists, then the DBMS applies the writes from the transaction’s buffer into the database and updates the timestamp of corresponding tuples. 
% Modern advancements of OCC improves performance of validation and timestamp allocation, such as Silo~\cite{silo_sosp13}, Hekaton~\cite{hekaton_sigmod13}, and FaRM~\cite{farm_nsdi14}. 
% Some modern concurrency control algorithms~\cite{tictoc, polyjuice} also tend to interleave the 2PL and TO to adapt to the application contention status. 

% Software concurrency control in fact contributes most memory accesses. Figure~\ref{fig:} breakdown the time/instruction count spent on a transaction touching the memory, we can tell. 


