
\section{Discussion}

\rvs{
\subsection{Scaling issues beyond a rack}
% The system scale is a critical factor that impacts many design choices of \name. 
As we discussed in last section, we believe that a rack with 8-16 nodes is the sweetpoint scale for \name, and 
% \red{latency, currenet CPU MSHR, write buffer, <1ms memory latency. }
beyond a rack, we anticipate the need for a heterogeneous interconnection approach, where intra-rack communication leverages CxTnL to enable efficient memory sharing, and inter-rack communication relies on traditional networks such as Ethernet or RDMA. The reasons are twofold. 

\noindent \textbf{Broadcasting limitations.} \name~adopts a broadcasting-based coherence design to avoid SF bottlenecks, but this scheme does not scale well with an increasing number of nodes due to the flooding snooping traffic. Within a rack, the broadcasting field with up to 16 nodes could be efficiently supported by an in-device hardware bus, while scaling beyond a rack requires multiple CxTnL devices working in a distributed way due to I/O port limitations~\cite{pond}. This introduces costly cross-device communication for broadcasting. 

\noindent \textbf{Node Failure Issues.} Fault tolerance is crucial in large-scale transaction processing systems. To maximize the efficiency of the critical path, \name~hardware does not implement failure recovery mechanisms. CxTnL will stops the world and raises a hardware error when probes a node failure, and relies on the transaction processing frameworks to ensure data consistency. Consequently, the scale of \name's becomes the blast radius of one node failure. By limiting \name~within a rack, a system can isolate blast regions using network-based fault-tolerance protocols such as 2PC.


% We believe that a rack with 8-16 nodes is the sweet point scale to use CXL for memory sharing, we thus confine our work within this scale. Under such a setting, the scalability of GSync broadcasting won’t be an issue. Despite the broadcasting protocol's complexity related to the size of the cluster, the broadcasting field with up to 16 nodes could be efficiently supported by an in-device hardware bus, and the bandwidth usage is only 12\% at p99 usage, as Figure 14 (d) shows. For the cluster beyond a rack-scale, we anticipate the need for a heterogeneous interconnection approach, where intra-rack communication leverages CxTnL to enable efficient memory sharing, and inter-rack communication falls back to traditional networks such as Ethernet or RDMA with rend/recv semantics. We would add a discussion section about this question in our paper. 


% Future works
% The key of this work is to propose a mechanism, implementation optimization


\subsection{Related works}

% \noindent \textbf{Weak Consistency Models.} Consistency model specifies the contract between hardware architecture and software programmers. It stipulates what data that a memory load is expected to return with. For developers, the most intuitive consistency model is the sequential consistency, which guarantees program order and write atomicity. To improve hardware performance, previous arts~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16, lrcdsm_isca92} have exploited a variety of weaker consistency models by loosing parts of sequential consistency's guarantees. \name~resembles the one called release consistency~\cite{threadmarks_tc94, munin_ppopp90, txcache_osdi10, lrcgpu_micro16}, which maintains a partial order for memory accesses. RC has been adopted by distributed shared memory and GPU coding. 

\noindent \textbf{Weak Consistency Models.} A consistency model defines the contract between hardware architecture and software developers, specifying the expected behavior of a memory load. For software developers, the most intuitive model is sequential consistency~\cite{lamport_tc79}, which maintains program order and write atomicity for parallel processes. To boost hardware performance, prior works~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16, lrcdsm_isca92} have explored a variety of weaker consistency models by relaxing certain guarantees of sequential consistency. \name~adopts the consistency model similar to release consistency~\cite{threadmarks_tc94, munin_ppopp90, txcache_osdi10, lrcgpu_micro16} which preserves a partial order of memory accesses. \name~details how this consistency model benefits CXL-based memory sharing and provides holistic hardware and software design. 

% RC is a widely adopted relaxed consistency model~\cite{threadmark, munin, txcache}. It distinguishes between two types of memory operations: ordinary and synchronization. RC introduces two synchronization operations: acquire and release. The acquire operation is performed before a process enters a critical section, ensuring that any updates made in other critical sections by other processes are visible. The release operation is performed when exiting a critical section, signaling that all updates made within the critical section are now visible to other processes. 
% RC follows two key ordering principles: 
% First, operations within a critical section can be reordered, but synchronization operations (acquire and release) enforce a partial ordering. Second, operations before a release cannot be reordered to after it, and operations after an acquire cannot be reordered to before it.


% \noindent \textbf{Hardware Transactional Memory (HTM).} HTM~\cite{timestone_asplos20, dudetm_asplos17, logtm_hpca06, dhtm_isca18, tcc_isca04, rtm_isca07, vtm_isca05, overlaytm_pact19} facilitates transactional semantics for memory accesses by adding bits to processor caches to denote a transaction’s read and write sets, and utilizes the cache coherence protocol to identify conflicts. Optimistic HTM architectures~\cite{tcc_isca04, munin_ppopp90, overlaytm_pact19, logtm_hpca06} relax strict cache coherence constraints, allowing multiple writers to maintain distinct values on the same cacheline. Our work is inspired by these works but proposes a distinct solution that avoids modifications to existing processor architectures and maintains compatibility with diverse concurrency control strategies from software transaction processing frameworks.
 
% This work is partially inspired by VTM~\cite{vtm_isca05} that adopts bloom fi
% However, their overhead is not negligible since they always introduce significant changes in processor cache designs and integrates extensive logic into circuits~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94, rtm_isca07, overlaytm_pact19}, which are too ambitious for recent complicated processors. 

% Distributed transaction processing frameworks could be classified into two categories: shared-nothing architecture, and shared-data architecture.
% For decades, the shared-nothing architecture has been the paradigm of choice in distributed database management systems, primarily due to its efficiency in reducing network communication~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12, dbx1000_dist_vldb17}. But its performance degrades significantly when the workload is not perfectly partitionable~\cite{hstore_damon16}. 
% Shared-data architecture offers a more integrated management that allows transactions to access and modify data across any node within the system~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, fasst, drtmh, grappa_atc15, mtcp_nsdi14, drtm}. 
% % Despite the dataset is also partitioned across nodes, this approach eliminates the notion of exclusive data ownership, enabling a single worker thread to execute and commit multi-partition transactions. 
% The shared-data architecture is particularly sensitive to network delays. This work typically benefits shared-data architectures with CXL's low accessing latency and optimized coherence primitives, but could also benefit shared-nothing architectures with optimizing network communication primitives. 

\noindent \textbf{Distributed Transaction Processing.}
Distributed transaction processing frameworks can be categorized into two main architectures: shared-nothing and shared-data. For decades, the shared-nothing architecture has dominated distributed database management systems, primarily due to its efficiency in minimizing network communication~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12, dbx1000_dist_vldb17}. 
% However, its performance deteriorates significantly when workloads are not perfectly partitionable~\cite{hstore_damon16}. 
The shared-data architecture allows transactions to access and modify data across any node within the system~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, fasst, drtmh, grappa_atc15, mtcp_nsdi14, drtm}. \name~design adheres to shared-data architectures but also offers potential benefits to shared-nothing architectures by optimizing conventional network communication primitives, such as HydraRPC~\cite{hydrarpc_atc24}. 
}

% In this architecture, each partition is exclusively assigned to a single-node transaction system. 
% If a transaction involves multiple partitions, it is split into multiple sub-transactions based on which nodes its touched records locate on. The participants vote for the results via consensus protocols such as two-phase commit. 

% hence recent works on this architecture mainly focus on reducing communication latency with caching~\cite{learned_cache, gam} and advanced networking techniques~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, ddio_ispass20, ddio-doc, drtm, drtmh, farm_nsdi14, fasst, compromise}. 


% Given the high communication costs, various networking enhancement techniques have emerged, including but not limited to user-level network stacks bypassing OSs~\cite{mtcp_nsdi14}, offloading protocol processing to SmartNICs~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, remote direct memory access (RDMA)~\cite{farm, myth_vldb17}, and Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc}. Compiler improvements also utilize co-routines~\cite{fasst, drtmh, grappa_atc15} for efficient context switching, and applications have adopted shared connection structures~\cite{fasst} and packet batching~\cite{grappa_atc15, guideline_atc16} to optimize performance.


% Achieving a high-performance distributed transaction system thus necessitates an intricate combination of these techniques. The optimal choice of RDMA primitives for transaction processing remains debated~\cite{farm, fasst, grappa_atc15, compromise, drtmh, drtm, storm_systor19, prism_sosp21, guideline_atc16}, with the best-practice depending on cluster scales, firmware versions, and data access patterns~\cite{drtmh, prism_sosp21, guideline_atc16}. Such discussions apply to other networking techniques as well~\cite{smartnic_osdi23, xenic_sosp21}. This lead to high system complexity and high hardware mitigation cost. Unfortunately, even these sophisticated methods can't fully compensate for the performance disadvantages of inter-host networks. 


% Lazy release consistency
% Distributed shared memory


\section{Conclusion}
The emergence of advanced interconnection techniques CXL enables us to rethink the distributed transactions. To overcome the performance drawbacks of CXL's vanilla coherence model, this work proposes \name, a software-hardware co-designed system that implements a novel hybrid coherence primitive tailored to the loosely coherent nature of transaction processing systems. Evaluations on OLTP benchmarks demonstrate the benefits of \name, which achieves 2.08x higher throughput than \vanilla~primitives and exhibits unified performance improvements on various transaction processing policies. 

% \hspace*{\fill}