\section{Introduction}  \label{sec:intro}



% such as index caching~\cite{learned_cache}, distributed shared memory inter-layer~\cite{gam}, and hardware task offloading~\cite{xenic_sosp21, clio_asplos22}, they all software approaches to keep 

% These primitives suffer from network stack overheads and coherence issues, which pose large gaps with local memory accesses. Although numerous studies reduce their overheads through algorithmic improvements~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} and hardware optimizations~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, these gaps still persist.

% If a transaction involves a remote tuple, it networking communication which inherently introduces
% large latency overheads and high programming complexities.~\cite{farm_nsdi14, compromise, drtm, drtmh, drtmr, rdma_txn1, calvin_sigmod12}. 
% Despite numerous studies have attempted to mitigate these network overheads through algorithm improvements~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} and hardware optimizations~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, the fundamental challenges posed by networks, such as PCIe memory copy, still not fully been addressed. 


% Despite various works attempt to reduce network overheads with either software optimization~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} or hardware efforts~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, they still can't fully compensate for the network's performance disadvantages due to the inherent memory copies and network stack overheads. 
% Modern data-centric applications, including e-commerce platforms, web services, OLTP systems, and in-memory databases, rely heavily on distributed transaction processing for their operations. Despite the convenience of programming distributed transactions, their performance is often hampered by the inherent inefficiencies of network communications. These inefficiencies include the unavoidable overheads associated with memory copies and the network stack, which negatively impact system performance. While numerous studies have sought to mitigate these network overheads through software and hardware optimizations, the fundamental challenges posed by network-induced latencies remain largely unaddressed.

% adopt an intricate combination of advanced networking techniques, such as 

% Achieving a high-performance distributed transaction system thus necessitates an intricate combination of optimization techniques spanning from hardware primitives~\cite{ddio_ispass20, ddio-doc, farm, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, operating systems, compilers~\cite{fasst, drtmh, grappa_atc15}, and applications~\cite{grappa_atc15, guideline_atc16}. This naturally lead to high system complexity and high hardware mitigation cost. However, even these sophisticated methods can't fully compensate for the performance disadvantages of networks. 


% It simplifies concurrency control with a powerful programming abstraction that guarantees write atomicity, data consistency, and isolated execution. 
% However, modern data center applications' data sizes are too large to fit into a single host's memory capacity, 
% so the transaction system scales by splitting the data set into multiple data shards and distributing them across different hosts~\cite{redis, other-share-everything-db}. Unfortunately, this approach yields cross-host transactions, which require cumbersome communications across multiple hosts and thereby leads to poor system performance~\cite{farm, compromise, drtm, drtmh, drtmr, rdma_txn1m, calvin}. 
% 

% \Chen{requires cumbersome communications across multiple hosts and thereby leads to poor system performance.} 
% \sout{requires cross-host transactions, which yield poor performance due to the high communication cost across hosts}.
% Coordinating across machines needs fine-grained and low latency message passing. 

% To reduce the overhead associated with cross-host transactions, both industry and academia have explored various approaches on networking techniques~\cite{rdma, programmable-switch, smartnic}, operating systems~\cite{kernel-bypassing,}, and application protocols~\cite{farm, compromise, drtm, drtmh, drtmr, rdma_txn1}.
% However, these aforementioned approaches all rely on message passing-based networks to fetch remote objects and serialize conflicts from different hosts.


% Furthermore, network communication introduces significantly higher latency compared to single-host communication~\cite{gam, calvin, farm, etc.}, resulting in a degradation of the overall per-host throughput by a factor of xx~\cite{check-sota}.

%  that are significantly distinct from those within a single host
% Unfortunately, they suffer from high system complexity and non-ideal performance due to the various network APIs and relatively large communication costs compared to single-host transactions. 
% The inefficiencies mainly come from the different paradigms and the performance characteristics of intra-host coherent memory load/store, and inter-host explicit networking stack calls.  

% These networks exhibit significantly distinct programming paradigms and performance characteristics compared to how data is shared within a host. Specifically, networks involve explicit software API calls while shared-memory cores use coherent memory loads and stores, and networks' latency is in the several microseconds range while intra-host core-to-core latency is in the tens of nanoseconds range. To bridge these gaps, transaction system developers not only need to distinguish where the data locates~\cite{farm, compromise}, but also carefully choose networking primitives~\cite{drtm-h}, otherwise the system will suffer from serious performance degradation. The resulting systems thus yield with great software complexity, as well as non-ideal performance. 

% Over the years, both industry and academia have explored various approaches to reduce the overhead of cross-host communication. One category of approach~\cite{farm, compromise, drtm, drtmh, drtmr, rdma_txn1} leverages fast networking primitives such as RDMA one-sided read/write to reduce the latency spent on remote data access. 

% \Chen{(what does the "average" mean? The conext reads not smooth.)} 
% The rest class of work develops efficient data structures~\cite{fasst, drtmh, learned_cache, sigmod/YoonCM18} or concurrency control strategies~\cite{calvin, txn_cc1, txn_cc2} to reduce the number of network round-trips for a transaction to commit. 

% and that shorten the commit delays. 
% DrTM leverages the strong consistency of RDMA and HTM to realize hardware-controlled serializability. 
% FaSST 
% Works on RDMA to reduce commit overheads: FaRM, IAASC

% such as combining multiple transactions' commit messages into a single network packet to reduce network round-trips, utilizing fast networking primitives (e.g. one-sided RDMA operations) to shorten the delay of one round-trip, and leveraging 

% Remote Direct Memory Access (RDMA) is the most widely adopted idea that is deployed in various commercial products, such as Infiniband, Mellanox, etc. Other 

% Compared with TCP/IP protocol, RDMA achieves an order of magnitude improvement in latency and throughput due to a bunch of optimization techniques, including kernel-bypassing, network stack offloading, and CPU-free memory access. 
% As a consequence, comprehensive works have been made on RDMA-enabled clusters. 
% Direct-CXL

% move coordination messages between participating machines over Remote Direct Memory Access (RDMA) or similar network protocols.
% \Chen{However, above approaches all rely on traditional networking fabrics, which is build on a protocol of unreliable datagrams(?).} 
% \sout{Although there are many variants, existing approaches rely on networking techniques, which use unreliable datagrams to fetch remote data, apply updates, and coordinate commitment information.} 
% \Chen{As a result} \sout{Thus}, 




% Consequently, most existing network-based systems suffer from both great programming complexity and mostly non-ideal system performance due to the non-travial combination of inter-host and intra-host thread communication. 
% However, the aforementioned approaches all depend on message passing-based, loosely coupled networks to oversee shared objects, implement updates, and coordinate access conflicts across threads on different hosts. Regrettably, these networks exhibit significantly distinct programming paradigms and performance characteristics compared to how threads share data within a host. Specifically, they involve explicit API calls versus memory loads and stores, core-to-core latencies in the tens of nanoseconds range versus host-to-host latencies in the several microseconds range. Consequently, developers of transactional systems must not only explicitly discern the host where the data resides~\cite{farm, compromise}, but also meticulously select networking primitives for various memory access types~\cite{drtm-h}. Failing to do so will result in severe performance degradation for the system. As a result, most existing network-based systems grapple with substantial programming complexity and suboptimal system performance due to the intricate interplay between inter-host and intra-host thread communication.


% As a result, they suffer from two common issues. First, due to redundant memory copies and heavy software stacks~\cite{directcxl, netdam}, traditional networks requires two orders of magnitude higher latency than accessing local memory.
% Second, traditional networks do not inherently provide essential primitives required by transaction processing, such as 
% efficient atomic operations and the capability to traverse index data structures. 
% Consequently, existing transaction systems resort to software methods, such as remote procedure calls~\cite{fasst, drtmh} or software locking services~\cite{sigmod/YoonCM18}, to enable these complex operations. 
% These software approaches introduce significant CPU overheads and often become the system's bottleneck~\cite{farm, drtmh}.

% adopt remote procedure calls to maintain serializability and atomicity, which can be significantly slower than invoking local functions.
% They develop memory expanders as zero-cpu numa nodes, and allocate memory transparently on the device. In addition, there are also some academic efforts on divers, OS~\cite{directcxl}, and cloud infrastructures~\cite{pond}. 



% ~\\
% \noindent \textbf{Baseline System. }
% This work advocates the recently standardize Compute Express Link (CXL~\cite{cxl-doc}) as the network replacement to manage the shared data. CXL is originally designed to achieve the excellency of heterogeneity management across different processor complexes, but both industry and academia anticipate its cross-node coherence capability to construct a shared memory pool. CXL 3.0 introduces the cross-node coherence support via a back-invalidation (BI) scheme that enables a processor's cached CXL memory to be invalidated by another processor locating on the different node. The CXL fabric acts as the home node and routes the snooping request to the proper nodes. 
% CXL is originally designed to achieve the excellency of heterogeneity management across different processor complexes, but both industry and academia anticipate its cross-node coherence capability to share memory across nodes. 

% Transaction provides high availability and strict serializability to simplify programming and reasoning about concurrency issues, but the single-node transaction systems are hard to scale beyond a node due to shard-memory dependent implementation. 
% As the current dataset exceeds a single host's memory, a typical distributed transaction system splits the dataset into multiple shards and distributes them across different nodes, and uses networking techniques, e.g. RDMA, to fetch remote data and harmonizes with other nodes. 
% However, such a scaling approach is always notorious for the poor performance, due to network latency and programming complexities~\cite{farm_nsdi14, compromise, drtmh, fasst, grappa_atc15, guideline_atc16}. Although various software~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} and hardware~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} optimizations have been proposed, they do not fully overcome network's performance disadvantages. 


% Transactions provide high availability and strict serializability, simplifying programming and reasoning about concurrency issues. 
% However, scaling single-node transaction systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} beyond a single node is challenging due to their dependency on shared-memory architectures. When datasets exceed a single host's memory capacity, a common solution is to employ the distributed transaction system that partitions the data into multiple shards and distributes them across different nodes. This approach adopts networking techniques, such as RDMA~\cite{farm_nsdi14, compromise, drtmh, fasst}, to fetch remote data and synchronize transactions across nodes. However, such distributed transactions are notorious for poor performance attributed to network stack overheads~\cite{farm_nsdi14, compromise, drtmh, fasst, grappa_atc15, guideline_atc16}. Despite numerous software~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} and hardware~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} optimizations proposed to mitigate these issues, they have not fully overcome the inherent performance disadvantages of networking techniques.  

Transactions provide high availability and strict serializability, simplifying programming and reasoning about concurrency issues. However, scaling single-node transaction systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} beyond a node is challenging due to their reliance on shared-memory architectures. When datasets exceed a single node’s memory capacity, a common solution is to employ a distributed transaction system that partitions the data into multiple shards and distributes them across different nodes. This approach utilizes networking techniques, such as RDMA~\cite{farm_nsdi14, compromise, drtmh, fasst}, to fetch remote data and synchronize transactions across nodes. However, distributed transactions are notorious for their poor performance, often attributed to network stack overheads~\cite{farm_nsdi14, compromise, drtmh, fasst, grappa_atc15, guideline_atc16}. Despite numerous optimizations in software~\cite{fasst, drtmh, grappa_atc15, guideline_atc16} and hardware~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} aimed at mitigating these issues, they have not fully overcome the inherent performance disadvantages of networking techniques.


%  they have not completely rectified the fundamental performance limitations associated with networking techniques.

% typical transaction processing systems scale out by splitting the dataset into multiple shards and distributing them across different hosts. This results a distributed transaction that leverages networking techniques, e.g. RDMA, to fetch data from remote nodes. 

% Efficient distributed transaction processing is the crux for modern data-center applications such e-commerce, web services, and OLTP systems~\cite{aurora, myth_vldb17}. 

% Various efforts have been made to reduce them by introducing a distributed shared memory (DSM) inter-layer over network infrastructures and partially caching data on local memory~\cite{xenic_sosp21, clio_asplos22, gam, learned_cache, legoos_atc19}. But they fail to gain wide usage due to high cost of networks and software overheads. 

% Despite some distributed shared memory (DSM) provides a unified memory space that adopts software to manage the caches, they fail to gain wide usage due to high cost in networks. 

% However, these solutions often introduce substantial overheads for software cache management and inter-node coherence traffic, leading to non-ideal system performance.

% The recently standardized Compute Express Link (CXL) technology~\cite{cxl-doc, cxl-shortdoc, cxl-paper} offers a promising alternative by enabling cross-node memory sharing with memory semantics and hardware-based cache coherence. CXL 3.0 architecturally defines a Global Fabric Attached Memory (G-FAM) node, allowing compute nodes to cache G-FAM data within their internal cache hierarchies during memory accesses, while the CXL back-invalidation (BI) scheme ensures cross-node coherence by monitoring and invalidating outdated caches across the CXL fabric using a MESI-like protocol. With simplified datapath, CXL achieves ultra-low x100ns link latency, and sub-microsecond end-to-end G-FAM accessing latency. 

% The introduction of Compute Express Link (CXL) technology offers a promising alternative~\cite{cxl-doc, cxl-shortdoc, cxl-paper} to achieve both scalability and efficiency. CXL 3.0 architecturally defines a Global Fabric Attached Memory (G-FAM) node, allowing compute nodes to cache G-FAM data within their internal cache hierarchies during memory accesses, while the CXL back-invalidation (BI) scheme ensures cross-node coherence by monitoring and invalidating outdated caches across the CXL fabric using a MESI-like protocol. With simplified datapath, CXL achieves ultra-low x100ns link latency, and sub-microsecond end-to-end G-FAM accessing latency. 


The introduction of Compute Express Link (CXL) technologies~\cite{cxl-doc, cxl-shortdoc, cxl-paper}, particularly the CXL 3.0 specification, provides a promising solution for achieving both scalability and efficiency in transaction systems. This technology defines a Global Fabric Attached Memory (G-FAM) node, which allows compute nodes to access G-FAM with memory semantics, and to cache G-FAM data within their processor cache hierarchies. The CXL back-invalidation (BI) scheme maintains cross-node coherence by monitoring and invalidating outdated caches at the CXL fabric using a MESI-like protocol. With the optimized datapath, CXL achieves ultra-low link latency and sub-microsecond G-FAM access latencies~\cite{pond, directcxl, tpp_asplos23, cxl_anns_atc23, cxl_demystify, neomem}. Despite the potential, it remains unknown whether the coherence link of CXL suits transaction processing.


% Despite the great potential, the discussions on CXL-based transaction processing systems have been limited. Numerous studies~\cite{pond, directcxl, tpp_asplos23, cxl_anns_atc23, cxl_demystify, neomem} have explored CXL as a memory expansion approach, primarily focusing on scenarios where nodes exclusively own an CXL memory endpoint, but overlook the its capabilities on memory sharing. With hardware-managed cache coherence, CXL enables an SMP-like abstraction that facilities to adapt existing single-node transaction systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} to the G-FAM architecture with only minor code modifications. This enables transaction systems seamlessly benefit from data locality, such as tree indexes~\cite{masstree, learned_cache, fasst} and locking services. 


% Numerous studies have explored CXL primarily as a memory expansion tool, focusing on scenarios where nodes own an exclusive CXL memory endpoint~\cite{pond, directcxl, tpp_asplos23, cxl_anns_atc23, cxl_demystify, neomem}, but they often overlook its memory sharing capabilities for transaction processing systems. 

% With hardware-managed cache coherence, CXL provides an SMP-like abstraction that facilitates the adaptation of existing single-node transaction systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} to the G-FAM architecture with minimal code modifications. This adaptation allows transaction systems to seamlessly benefit from CXL's low latency, and the exploit application's data locality~\cite{masstree, learned_cache, fasst}.


% On the CXL-based memory sharing, this shared memory organization offers distinct advantages over traditional shared nothing systems that assume non-coherent memory stores. First, it fully exploits CXL's cache coherency capability to benefit for accesses on the data structures with high locality, such as tree indexes~\cite{masstree, learned_cache, fasst} and locking services. 
% Second, accessing G-FAM with memory semantics, i.e. loads and stores, introduces theoretically minimized controlling overheads and benefits from the same optimized processor datapath used for local DRAM accesses~\cite{quantitative_approach, book_cc}. 

% and leverage CXL's zero-copy memory sharing capabilities and hardware coherence protocols, 
% achieving sub-microsecond tuple access times.
% Moreover, developing transaction processing systems on G-FAM is straightforward because CXL adopts a strictly coherent memory primitive similar to single-node symmetric multiprocessor (SMP) architectures. This ensures universal programming support for general applications~\cite{book_cc, cxl_hpc}.

% As a consequence, we could migrate existing single node transaction systems to CXL-based memory sharing with just a few code changes~\cite{cxl_hpc}. 

% , the memory loads and stores in CXL are observed by all nodes as soon as they are issued to the memory hierarchy
% The processor could cache the shared memory in its own cache hierarchy, and the CXL hardware fabrics guarantee the cache coherence across nodes via the a back-invalidation (BI) scheme. 


% Cache copies from different hosts remain coherent with a MESI protocol running at the CXL fabric, 
% resembling a shared-memory symmetric multiprocessor (SMP) semantic. Accesses toward GFAM has ultra-low (x100ns) end-to-end physical latency~\cite{pond, cxl-paper, directcxl, tpp_pub, intel_cxl_pub, samsung_2}, which is compatible with a NUMA access within a host. 

% Although no existing works leverage CXL for in-memory transaction systems, it's straightforward to build a valid baseline system 
% , which constructs a hardware-enabled distributed shared memory (DSM) leveraging CXL's byte-addressability and cache coherency. Transactions executing on different hosts access the DSM as if within a single host, thus they manage the records following standard 

% thus  accomodates in-memory transaction processing systems~\cite{STMs, nvms} without necessitating code modifications.
% significantly improves the cross-host communication latency with an order of magnitude than networks~\cite{smartnic, rdma, ethernet}. 

% CXL enables the cache coherent and low latency cross-host communication. 

% coherent shared memory pool, where threads running on different hosts can access that pool as if running in the coherent memory space. Compared with traditional networks, this approach has two advantages.  
% First, CXL enables memory accesses to be cached by CPU with hardware-guaranteed coherence across hosts, this enables cross-host data sharing to be implemented as common memory loads and stores, resembling sharing data within a host. 
% Second,


% build a unified memory pool that data can be coherently shared across all hosts within the rack. 
% We utilize Compute Express Link (CXL~\cite{cxl-doc}) to build a memory pool that data can be coherently shared across all hosts within the rack in a efficient way.

% \sout{all hosts can coherently share .} %Hosts can cache objects while accessing the memory pool, with cache coherency guaranteed transparently through hardware approaches. 

% Compared with traditional networking solutions, this approach has two advantages. First, thanks to the customized lightweight protocol and hardware acceleration, CXL is able to provide sub-microsecond end-to-end physical latency~\cite{pond, cxl-paper, directcxl, tpp_pub, intel_cxl_pub, samsung_2}, 
% which is two orders of magnitude lower than that of Ethernet or RDMA. Second, CXL enables typical memory primitives~\cite{cxl-doc, intel-doc}, such as atomic instructions, to potentially support efficient transaction primitives. 


%system modules, such as indexes, locks, and concurrency control protocols. 
% \Chen{Although some previous work have used CXL for data center, they are mainly focused on memory disaggregation, where memory is not shared across hosts. No preivous work has discussed using CXL for in-memory transaction systems.}

% \sout{To the best of our knowledge, there has been no discussion on building such a fabric for in-memory transaction processing, despite CXL exhibiting significant performance advantages over networks. Existing CXL-based memory pool solutions~\cite{pond, directcxl, tpp_pub} focus on memory disaggregation, where memory is not shared across hosts. }

% CPU to cache device memory and maintain coherence with the device at the same time. 
% for in-memory transactions . 
% to scale in-memory transactions beyond a machine. 
% , and there's no transaction working on data attached via CXL network.

% CXL offers low-latency and high speed communication between host processors, customized accelerators (e.g. GPUs)~\cite{xie-hpca22}, memory expanders~\cite{samsung, directcxl}, smartNICs~\cite{netdam}, and other PCIe devices. 

% on the one hand, RDMA relies on protocol stacks to convert raw data to a specific exchanging format, which introduces redundant memory copies and software intervention. On the other hand, RDMA does not hold the strong atomicity to implement locks, which wastes CPU cycles to implement additional software lock managers.

% has not been designed to provide fine-granularized or low-latency message passing. 
% there is a mismatch between transaction systems' requirements for low latency, fine granularity, memory consistency and RDMA, which 
% RDMA makes the latency of coordinating transactions on different machines longer than that of intra-machine coordination by multiple orders of magnitude. The performance drop comes from two folds: 

% From DirectCXL: In this work, we advocate compute express link (CXL [18]), which is a new concept of open industry standard interconnects offering high-performance connectivity among multiple host processors, hardware accelerators, and I/O devices [19]. CXL is originally designed to achieve the excellency of heterogeneity management across different processor complexes, but both industry and academia anticipate its cache coherence ability can help improve memory utilization and alleviate memory over-provisioning with low latency [20–22].

% With the development of implementation techniques, the end-to-end access latency is promised to be x100ns~\cite{pond, cxl-paper, directcxl, tpp_pub, intel_cxl_pub, samsung_2}. 

% \noindent \textbf{Baseline system and its disadvantages. }
% This study examines the foundational access primitives of a memory pool—namely, read, write, and atomic operations—within the context of the Compute Express Link (CXL) shared memory for transaction processing systems. 
% These primitives form the conversation between the hardware architectures and algorithms, thereby being pivotal for system optimization and algorithm tradeoffs. 

% However, we observe that the vanilla usage of CXL memory sharing primitives, including read, write, and atomic operations, could lead to significant and universal performance challenges in transaction processing applications. 
% The CXL-vanilla primitives adopt a strict cache coherence model as most shared memory architectures. 


% Compared with CXL's memory semantics, CXL PRC introduces inherent overheads on controlling and cache flushing, its performance thus fall behind direct CXL memory accesses with 3.22x longer latency.  

% directly accessing G-FAM with shared-memory semantics over 
% ~\cite{} propose to keep back-compatibility with current network-based systems via rebuilding remote procedure call (RPC) on CXL, their latency fall behind directly accessing G-FAM with shared-memory semantics over 53\%. 

% CXL's SMP-like abstraction allows existing single-node multi-threaded transaction systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} to be adapted on G-FAM with minor code changes. Compared with traditional network-based systems that assumes a non-coherent memory store, such a shared memory architecture introduces three advantages. First, the cache coherency benefits systems' data structures that has good locality, such as tree indexes~\cite{masstree, learned_cache, fasst} and locking services. Second, directly accessing G-FAM with memory loads and stores introduces theoretically minimized controlling overheads, since it shares the same datapath with local DRAM accesses. 
% Third, the G-FAM architecture naturally fits into a disaggregated architecture that separates compute and memory resources, allowing for more flexible and efficient resource utilization as well as better load balancing. 


% Third, G-FAM’s design is inherently suited to a disaggregated architecture, which decouples computing and memory resources, thereby enhancing resource flexibility and load balancing capabilities. 


% Moreover, developing transaction processing systems on G-FAM is straightforward because CXL adopts a strictly coherent memory primitive similar to single-node symmetric multiprocessor (SMP) architectures. This ensures universal programming support for general applications~\cite{book_cc, cxl_hpc}.

% \red{To evaluate CXL’s effectiveness in transaction processing systems, we adapt the widely-used DBx1000 framework~\cite{abyss_vldb14} to the G-FAM and compare its performance against state-of-the-art RDMA-based systems~\cite{drtmh}.} 

% \red{Remove Scalability Problems}
% \red{Naive usage: CXL RPC, however, CXL enables shared memory that could adopt single-node systems, we are the first.}
% \red{sub-optimal --> far worse than CXL-RPC}


We demonstrate that the vanilla adoption of CXL-based memory sharing in transaction processing systems achieves performance far worse than expected, primarily due to two reasons. First, CXL incurs significant overheads due to "remote cache signal" process which is necessary for coherence maintenance. Here, data loads or stores on G-FAM may notify other nodes' caches to either invalidate or update their cachelines. This process involves multiple cross-node communication roundtrips over the CXL fabric, with latencies exceeding 800ns in our FPGA prototypes, rendering coherent access 1.85$\times$ slower than direct G-FAM DRAM access. Second, to maintain coherence, CXL requires the implementation of an inclusive Snoop Filter (SF) on the G-FAM fabric to monitor the states of cachelines at each node. The SF presents significant scalability challenges, as its centralized design limits the capacity for memory sharing and the number of compute nodes.


% Baseline 两类：Shared Memory拓展，Network-based拓展（CXL Message Passing，CXL RPC，Data locality：Index Locality比较好, 定性）
% significantly increasing latency overhead and consuming CXL link bandwidth. In our prototype, remote cache signaling required 

% Second, to monitor cacheline states in sharing nodes, CXL necessitates the G-FAM fabric to implement an inclusive Snoop Filter (SF), which allocates a slot to track which node caches the G-FAM in what states for every accesses. This imposes prohibitive scalability bottlenecks due to the high parallelism and the large memory footprint of typical transaction applications. , especially on high parallelism and extensive memory footprint typical of transaction processing applications.

% nodes and the size of application's memory footprints. 

% spatial and temporal overheads and bottlenecks the system's scalability. 
% for transaction processing applications that features for large memory footprints. 
% , limiting system scalability for transaction processing applications which typically feature large memory footprints.

% While state compression or granularity coarsening of SF management may mitigate space requirements, these solutions introduce additional BI coherence messages, potentially leading to speculative snooping on untracked nodes. 
% The cost is serious once the number of unique host-cached memory pool is large, so that effects the overall performance when memory accesses are uniform. 
% Unfortunately, previous wisdom such as local cache bypassing, software-managed cache coherence, and hardware transactional memory encounter performance and compatibility challenges. 


% \noindent \textbf{Baseline system and its disadvantages. }
% This work focuses on the memory pool's basic accessing primitives including the read, write, and atomics. The primitive is the first-step choice for a transaction processing system to exploit the CXL shared memory, which further influences the following design tradeoffs. 
% We argue the vanilla CXL primitives fail on the efficiency of executing the modern distributed transaction processing applications due to two main drawbacks. 
% % found such a straightforward usage of CXL falls short of realizing the full performance potential that CXL can theoretically offer. 
% % Through quantitative analysis, we have identified that such a baseline system experiences a xx\% throughput loss for two primary reasons.
% First, the read miss and the write upgrade toward a CXL shared memory has a large latency overhead. 
% Because CXL adopts a snoop-based read-for-ownership coherence protocol for memory accesses, a read miss or a write could cause the BI snooping toward a remote node that owns the cacheline. 
% Remote node snooping incurs multiple CXL channel roundtrips and takes much longer time than accessing the DRAM, becoming main overhead of CXL memory accessing. 
% % accessing the CXL DRAM, as previous work assumes in the CXL memory disaggregation architectures. 
% % It thus significantly degrades the overall performance of the most memory-bounded transaction processing applications. 
% Second, to keep cross-node coherence, the CXL device is required to maintain a centralized Snoop Filter to track the lines cached in peer nodes. As the memory footprint of distributed transaction application is expected to be large, a precise snoop filter that works on the cacheline granularity is unacceptable in both space and time overhead. 
% Although compressing the SF states or coarsening the SF management granularity can reduce the SF space requirement, these approaches will incur additional BI coherence messages, since they may snoops speculatively to nodes that is not tracked in the SF. 
% Unfortunately, previous wisdom such as local cache bypassing, software-managed cache coherence, and hardware transactional memory meet the challenges in either performance drawbacks and compatibility issues. 

% , hence is not proper in a practical CXL based memory sharing pool. 


% ~\\
% \noindent \textbf{State-of-The-Art Approaches. }
% In-memory DBMS. Check vldb14



% With quantitatively analysis, we observe it's due to the unique performance characteristics of CXL-based memory: 

% Designing a shared memory pool using CXL is not trivial, as it presents two major challenges. 
% The first challenge is the lack of a practical solution for building the shared memory pool. Older versions of CXL (e.g., 1.1 and 2.0) adopts an asymmetric connection model, which restricts data sharing between hosts. The latest version of CXL (3.0) introduces the paradigm of data sharing, but there are no existing solutions to put this paradigm into practice.

% First, it lacks a practical solution to build the shared memory pool. The old versions of CXL (e.g. 1.1 and 2.0) originally adopt the asymmetric connection model which restricts data sharing between host. The newest \cxl~3.0 overviews the paradigm of data sharing, but there's no existing solutions make the paradigm into practice.

% Second, the cost of cache coherency may offset the benefits of CXL's low latency. CXL-attached devices act as the peer L2 cache, meaning that an L2 coherence request must go through the PCIe link when the device caches the object. This round-trip latency is an order of magnitude higher than the time it takes for coherence requests to travel between cores within a CPU socket, resulting in a significant drop in cache performance. 

% \cxl~enables atomic instruction via cache locking~\cite{cxl-doc}, which relies on cache coherence mechanism to serialize data races~\cite{intel-doc}.

% To be reasonable in hardware implementation, these approaches should be moderate in offloading functionalities to hardware and avoid over-designes. 
% To this end, the CXL's strict cache coherence model is much a overkill~\cite{rvweak_pact17, armcm_cmb12, powercm_pldi11, rvweak_isca18, in-memory-transactions} 


% transaction processing systems fundamentally diverge from the vanilla CXL protocol regarding their coherence model for managing the data records. 
% They allow the flexibility for nodes to hold incoherent values in scenarios involving uncommitted or aborted transactions. 

% Interestingly, the coherence model provided by CXL’s vanilla primitives is an overkill for most memory accesses in transaction processing systems. To be clear, their record loads and stores follow the well-established transaction consistency models~\cite{rss_sosp21, tcc_isca04, sitm_asplos14}. According to these models, a store is only finalized at the time of transaction commitment, allowing data to remain temporarily incoherent for transactions that are still pending. Moreover, data from transactions that are ultimately aborted do not require coherence, further illustrating that the CXL's strict coherence model is overly conservative. Previous works on concurrency control algorithms, e.g. SILO~\cite{silo_sosp13}, share the similar insights with us and they optimize coherence traffic at algorithm level. However, without interventing hardware coherence mechanisms, they could not address the SF scalability issues. 
% Interestingly, the coherence model implemented by CXL’s vanilla primitives is an overkill for the majority of memory accesses in transaction processing systems. 

\hspace*{\fill}

\noindent \textbf{Insights. }
Our key insight is that the strict coherence model of CXL is overkill for a large fraction of memory accesses in transaction processing systems. In these systems, memory accesses can be classified into record accesses and metadata accesses, based on the type of data they engage. While metadata accesses require strict coherence to ensure synchronization correctness, record accesses typically adhere to well-established transactional consistency models~\cite{rss_sosp21, tcc_isca04, sitm_asplos14}, where a record’s store operations are finalized only at the point of transaction commitment. This setup allows for a degree of temporary incoherence among transactions that are still in progress, and transactions that are eventually aborted do not require maintained coherence. However, hardware cannot distinguish between these types without application-provided information. Previous research in concurrency control algorithms, such as SILO~\cite{silo_sosp13}, supports our observations. Nonetheless, without modifications to underlying hardware mechanisms, these approaches fail to resolve the SF scalability issues. Other works on transactional memory~\cite{logtm_hpca06, overlaytm_pact19, flextm_isca08, vtm_isca05} share similar concepts with our findings but are limited to a single socket and are not practical as they necessitate significant changes to processor architectures. 

% \red{SILO problems: read coherence overhead. }
% \red{SILO: Software-only solution}
% \red{Network Overkill ?}

% espite differences in details, these models ensure that all operations within a transaction are observed atomically. Consequently, 
% Transaction systems have the totally different coherence model on their kept tuples with what the vanilla CXL protocol maintains. To be clear, most systems order their reads and writes to tuples with respect to a typical transaction consistency model, which ensures operations to appear atomically in the granularity of a transaction, or withdraw all effects once fail to serialize. This allows the synchronization of writes to be delayed until the transaction commits, while CXL's cache coherency ensures the synchronization happens at the point the write is issued. Transaction systems allow different nodes to observe incoherent values for the same address in the case of ongoing or aborted transactions, so that it's unnecessary for the hardware to keep such a strict, pessimistic coherence model. 

% We observe []the CXL's coherence model is an overkill for transaction processing systems. 
% To leverage the gap, we propose novel memory primitives tailored for transaction processing systems. 
% We relax the coherence limit for most data with the observation that memory accesses within transactional systems are already ordered according to transactional consistency models by concurrency control mechanisms. 
% These mechanisms ensure operation atomicity within transactions, allowing the synchronization of writes to be delayed until the transaction commits, and permitting different nodes to observe divergent values for the same address concurrently in the case of ongoing or aborted transactions. 
% two main functionalities of the memory accessing primitive: accessing the memory content, and achieving coherence on all cache copies. 

% from hardware to the software layer with explicit APIs.
% We propose \name-primitive, which transfers the right of cache coherence from hardware to the software layer with explicit APIs.
% This innovate primitive enables the software developer to decide whether and when to make coherence, so that it avoids unnecessary remote cache signaling overheads. 

% \noindent \textbf{\name-Primitive Innovations.}
% To leverage this insight, we propose a \textit{hybrid} primitive architecture that retains the CXL's strict coherence accesses for metadata, but for record accesses, we decouple coherence maintenance from the memory access procedure. We introduce the \name~primitive, which transfers the decision-making power regarding cache coherence from hardware to software, while implements the coherence call with CXL native hardware datapath. 
% By carefully invoking the coherence API in accordance with the transaction consistency model—specifically, only at transaction commitment—developers of transaction systems can avoid unnecessary cross-node cache coherence overheads. Furthermore, software developers can further hide cache coherence latency with techniques like batching and co-routines as traditional network-based systems. 

\noindent \textbf{\name-Primitive Innovations.}
To leverage this insight, we propose a \textit{hybrid} primitive architecture that maintains CXL’s strict coherence for metadata while decoupling coherence maintenance from the memory access process for record accesses. The \name~primitive transfers the decision of whether and when to achieve cache coherence from hardware to software, yet still utilizes CXL’s native hardware datapath for coherence operations. By strategically invoking the coherence API, specifically at the point of transaction commitment, transaction system developers can significantly reduce unnecessary cross-node cache coherence overheads. Additionally, software developers can further mitigate cache coherence latencies using techniques such as batching and co-routines, akin to traditional network-based systems.


% exposes cache coherence requests through explicit API calls, and allows memory loads and stores to remain temporarily invisible to peer nodes until the coherence API is invoked. This approach transfers the decision-making power regarding cache coherence from hardware to software, while relies on CXL native hardware datapath to ensure efficiency and correctness.

% The hardware maintains the write's invisibility to other nodes during this interim period. 
% This approach reduces the unnecessary coherence traffic and thus improves primitive performance with 43\% on average. Moreover, with proper implementation, we could also eliminates the necessity for the centralized snoop filter without disturbing neither coherence corretness nor performance. 

% . With proper implementation, these primitives have demonstrated a reduction in operation time 43\% on average compared to traditional CXL primitives, and 


% To this end, we introduce a heterogeneous primitive that distinguishes accesses to the serialization data structures such as locks and semaphores, and accesses to normal tuples. We keep the strictly coherent primitives for serialization data structures to enable atomic operations efficiently, but propose the novel loosely coherent primitive for tuples. The primitive decouples the accessing from a node to the memory pool from its coherence resolution with other nodes, and pass the right of when to synchronize a write to software. The synchronization time could be far delayed from the write, for example, at transaction commits, and the hardware keeps the write invisible from other nodes during this period. 
% % To be specific, writes from a node are temporally buffered at a node-specific memory space. Upon transaction commitment, an explicit coherence process is initiated. The transaction modified tuples are merged to the global observable memory space, and the stale caches at other nodes are invalidated. 
% This approach significantly reduce the unnecessary coherence traffic and improve the memory accesses performance. With proper implementations, the primitives only take 57\% time of the CXL vanilla primitives on average, and eliminates the need of the centralized snoop filter.





% This approach alleviates the need for maintaining cross-node cache line coherence for uncommitted and aborted transactions, thus reducing coherence traffic and eliminating the latency associated with remote coherence resolution.

% To address the aforementioned challenges, we propose the novel CXL memory primitives that is co-designed with the transaction processing systems. 
% We observe the fact that memory accesses from a transactional system have already been well ordered respect to the transactional consistency models by the concurrency controls. They keep an atomicity for operations within a transaction, so that transaction's writes are delayed to be observed until the transaction commits. No ordering limits between aborted or on-going transactions, so that it's legal for multiple nodes to observe different values of the same address at the same time. 
 
% To this end, we propose a loosely coherent primitive that decouples the process that a node interacts with the memory pool and the process that the node resolves coherence with other nodes. During the transaction execution phase, the nodes conduct memory reads and writes directly towarding the memory, without involving cross-node snooping requests. 
% Writes are temporally buffered in a node-private memory space. 
% At the commitment phase, the successfully validated transaction would process an explicit coherence process that writebacks its dirty cachelines and invalidates all other nodes' copies. This primitive removes the need of keeping the cross node coherence between cachelines if one belongs to an uncommitted transaction, hence reduces the cross-node coherence traffic as well as removes the costly remote node coherence from the critical path of memory accessing. 

% traffic since it only preserves the coherence for the ``true'' data dependencies between committed memory operations. 



% In order to specify the behaviors for all programs~\cite{rvweak_pact17, armcm_cmb12, powercm_pldi11, rvweak_isca18, in-memory-transactions}, CXL maintains a strict coherence model that guarantees write atomicity. In contrast, transactions require a consistency model with non-atomic write to prevent the visibility of speculative writes. CXL's overkill coherence protocol obstructs a running transaction to maintain its ACID features. We refer this mismatch as the \textit{over-coherent problem}. 
% Second, CXL exhibits a unique latency hierarchy compared to most SMP architectures. In CXL, the round-trip latency for cache-to-cache coherency is three times longer than that for cache-to-dram, which is much larger compared to the latency ratio in traditional SMP architectures. This reversed latency profile introduces novel trade-offs in designing concurrency control mechanisms, which have not been thoroughly investigated in prior research. We refer this latency characteristic as the \textit{latency reverse problem}. 

% This motivate our argument that a distributed transaction processing system over CXL must be a software-hardware co-design in which hardware accelerates simple primitives, and falls back to software for exploring complex high-level concurrency control strategies. 

% it only conducts the mechanisms for conflict detection and version management. 

% the system should support transaction's capabilities of atomicity, consistency, isolation, and durability. 


% We detail the discussion in Section.~\ref{sec:challenge}.

% a transaction system that connects multiple hosts via \cxl. We co-design the underlying network architecture and the transaction strategy. The network architecture adopts two fundamental techniques: 

% Specifically, we propose a design of a multi-port bridge acting as the switch to connect hosts. 
% The network exposes pooled memory in a global address space. 
% The bridge also has plugged memory. It pools host-attached and bridge-attached memory by reflecting \cxl~packets heard from one host to another. 


% Furthermore, this primitive eliminates the need for the precise centralized SF since 
% . The coherence is only induced by writes, which are typically not lying on the performance critical data path. It consequently enables approximate data structures to track the cacheline ownership to trade space with falsely positive snooping requests, without introducing heavy performance degradation. 
% , hence may getting the potential stale records or store over-speculative data. To ensure correctness, coherence is kept on the metadata for atomics so that concurrency control modules could work well. 


\ifx\undefined\stale
\noindent \textbf{Our Approach. }
We propose~\name, the first CXL-native in-memory transaction processing system. We base~\name~on a key insight: \textit{The transaction software clearly knows when to synchronize modifications with other hosts better than the application-oblivious hardware coherence protocols.} 
As a consequence, we construct an unique loosely coherent memory layer on hardware, rather than the most processors (so as CXL) adopted strong coherence model. 
The memory layer follows a software-defined data coherency where the updates toward a record referring to the partial order specified by the transaction programs~\cite{TSTM}.
Transactions make in-situ reads and writes on records optimistically, and trigger the memory layer to publish their updates when commits. 
Specifically, we employ host's LLCs to buffer transaction's speculative updates, rather than allocating additional software buffers and do costly access redirection. 
By carefully choosing CXL protocols, we make the LLC incoherent on different hosts. 
When a transaction commits, the processor installs all updates to memory with explicit flushes. 
% where the incoherent LLC on each host manages potentially conflicted record versions. 
% On hardware, we keep the LLC's uncommitted writes invisible from other hosts. 
% but not the when the processor issues the that write. 
% multiple incoherent copies of the same object can co-exist on different host. Transaction commitment becomes the ``safety net'' to synchronize 
% Uncommitted writes keep private to a host until the corresponding transaction commits. 
% % so that it allows multiple incoherent copies of the same object co-exist on different hosts. 
% % \textbf{unique loosely coherence}, which has 
% , referring to the partial order specified by the transaction programmes~\cite{TSTM}.
% We use host's LLCs to buffer transaction's speculative updates. 
% The host's LLCs that storing a transaction's updates are incoherent by default, and only synchronize when the transaction commits. 
By doing so, \name~avoids unnecessary cross-host coherence messages associated with accessing records and saves the software buffers. 
% enables multiple versions of shared objects coexists without the use of additional buffers. 
Furthermore, we introduce a two-phase \textbf{hardware-assisted validation} to alleviate the heavy coherence messages associated with retrieving and validating per-record metadata~\cite{stm, nvm, distributed-tm} when validating conflicts. Specifically, \name~maintains a bloom filter for each on-the-fly transaction as an abstract of their read and write sets. 
A validation process first compares the transaction's bloom filter with other concurrent transactions. If the bloom filter indicates potential conflicts, it fall backs to standard per-record validation at the second phase. 
The key of high performance is to keep such bloom filters in hardware, and utilize CXL's cache coherence to reduce the latency of data movement between hosts. 
% . We offload the validation to CXL fabrics to alleviate the heavy coherence overheads associated with retrieving and validating per-location metadata~\cite{stm, nvm, distributed-tm}.
We evaluate \name~against state-of-the-art distributed transaction processing systems~\cite{farm, drtm} and the aforementioned baseline systems using various benchmarks~\cite{}. Our experiments demonstrate that \name~can achieve a throughput that is much higher than previous network-based designs. 
\fi

% \noindent \textbf{Challenges and Solutions. }
% However, supporting such a primitive challenges the CXL's fundamental principle of strict cache coherency. It poses design challenges to a practical implementation with the following essential design goals: 

% \hspace*{\fill}

\noindent \textbf{Architectural Supports.}
We present holistic architectural support for our \name~primitive, designed with the following goals:
\noindent \textbf{G\#1: }
The architectural design should neither change host processor architectures nor CXL specifications.
\noindent \textbf{G\#2: }
The architectural design should ensure compatibility with various transaction processing systems' policies.
\noindent \textbf{G\#3: }
It is also important to limit implementation overheads to ensure the efficiency of the \name~primitive.

% has performance advantages to CXL-vanilla primitives. 


% In this work, we design, implement, and test an innovate CXL-based memory accessing primitive that optimizes for transaction processing. 
% a heterogeneous coherence primitive that implements differentiated protocols based on the data field being accessed. 
% For metadata accesses, we preserve a strictly coherent semantic to facilitate exclusive serializations on atomics. Conversely, for data tuple accesses, we introduce a novel, loosely coherent primitive. 
% Its key idea is to decouple the process of accessing memory from the process of achieving coherence across nodes. The primitive transfers the right of cache coherence from hardware to the software layer with explicit APIs. Consequently, the synchronization of a memory store could be delayed, for example, to the point of transaction commitment. 
% This innovate primitive enables the software developer to decide whether and when to make coherence, so that it avoids unnecessary remote cache signaling overheads. 
% The software could further mitigate the coherence overheads by explicitly utilizing latency hiding approaches such as batching and co-routines. 
% These software efforts further benefit the hardware design to trade the SF with higher scalability with a slight accuracy drop. 

To meet these goals, we contribute a hardware-software co-design with three modules:
First, we introduce an \name~hardware agent (CTHW) at the G-FAM side. The CTHW modifies the semantics of host processors' loads and stores by remapping them to a node-private address space, and exposing cache synchronization with explicit side-path API calls.
Second, on the host software side, we design a lightweight \name~user-level library (CTLib) that allows applications to select primitives for every G-FAM allocation according to application-specific requirements.
Third, we propose a user-level runtime thread (CTRt) to monitor CTHW states and reconfigure it according to the use case to maintain efficiency.

% . This enables the critical datapath always being optimized for the current use case. 
% manipulate data structures of G-FAM. CTLib allows the fine-grained primitive binding to virtual addresses on every memory allocation. 
% , where each memory allocation can specify a primitive that subsequent data accesses obey. 
% builds a datapath for explicit cache synchronization based on current CXL requests and responses. 
% CTHW adopts a high-level conceptual model named the view shim layer to transform primitive architectural support mission into an address mapping problem. CTHW 


% We overcome the design challenges mentioned above. 
% and fulfill various design objects. First, the hardware modifications are limited to the EP and are kept out of the CXL IPs to be compatible with existing hardware architectures. Second, \name~supports a traditional multi-threaded programming interface that allows primitive selection on each allocation to enable flexibility in system policies implementation. Third, we limit the implementation overhead via a clean and efficient hardware pipeline design and offloads non-critical but complex operations to the background user-level threads. Last, design feasibility and system integration constrains are well considered. 

\ifx\undefined\stale
We discuss how the \name~architecture meets the aforementioned design goals:
For \textbf{G\#1}, we limit our hardware changes to the device to be compatible with existing processor architectures and CXL protocols. 
The CTLib provides an allocation-specific primitive binding API, enabling current single-node systems to deploy \name~by only modifying memory allocation codes, meeting \textbf{G\#2}. 
To meet \textbf{G\#3}, we offload reconfiguration and interrupt handling to CTRt, keeping hardware logic deterministic and fast. Additionally, we keep CTLib off the datapath of memory loads and stores to avoid software overheads on common memory operations.
\fi
% For operations that are not critical and require complex logic, we propose to move it to the software slow path running on the host manager process. This allows the hardware to focus on core, performance-critical tasks, ensuring fast and predictable operation. The hardware expose its internal data structures as a special memory space, so that software runtime can read out the running status and modify the hardware configurations. 
% which can be accessed and modified by the host manager process via \cxlcache. 
% For example, data recovery (Sec.~\ref{Sec.xx}) is expected to be a rare operation. Handling data recovery, however, would involve dealing with complex historical log trees. 
% Separating these less urgent tasks from the latency-critical path allows for asynchronous processing on different clock fields, thus improving overall system efficiency. 

% To this end, the \name~hardware part is an the EP only architecture that stay behind the L2/LLC cache, and communicate with host processors with CXL standard protocols. 
% and raises potential compatibility issues necessitating substantial modifications to processor architectures and CXL protocols. 



% Moreover, the write redirection and peer cache invalidation in loosely coherent field are critical to performance so that they should be implemented at hardware. 
% Fortunately, the CXL's implementation of the DRAM controller at the device level provides a unique opportunity to intervene memory access transparently to processors and CXL protocols. We advocate for a software-hardware co-design approach, which intercepts and translates physical addresses in memory requests to appropriate DRAM locations. The hardware component functions only at the CXL device within the memory access pipeline, facilitating write redirection and cacheline invalidation, while the software component, operating outside the memory access path, optimizes hardware agent configurations based on application usage and handling other non-critical but complex procedures.  

% Implementing a hardware memory pool with such a heterogeneous coherence model represents novel challenges since it breaks the CXL's basic coherence semantics. 
% The challenges are primarily stem from hardware deployment constraints of modifying existing well-defined processor architectures, as well as the imperative to ensure compatibility with the intricate transaction processing systems with abundant software algorithms. Moreover, gaining performance benefit from these primitives also requires careful design to minimize the overheads. 
% We discuss three design principles adopted by \name~to address these issues. 




% Nodes' caches are naturally isolated without CXL BI, thereby the L-Ld and L-St being node-private when they hit the cache. 
% However, the cache eviction would break such isolation since it would leak the modified cachelines from the private cache to the shared memory. If a read from other nodes is issued right after the eviction of a written cacheline, it would observe the written data thereby violating primitive semantics. The cache eviction could happen at any time due to cache conflicts. In typical x86 architectures, the cache eviction is managed by the uncore cache agents which are transparent to the processor's pipeline. Hence it's impossible to address the eviction issue by inserting special instructions such as CLFLUSH or FENCE to codes, neither by changing the processor architectures (\textbf{P1}). 


% Adhereing to this principle not only simplifies hardware implementation but also confers performance benefits by enabling worker threads on the same node to exploit data locality through local cache hierarchies. 


% \noindent \textbf{Principle 1: The hardware design should not disturb neither the host processor architectures nor the CXL specifications.} Because commercial processor has plenty of confidential and undocumented features, hence introducing changes to the CPU pipelines and cache hierarchies could lead to unexpected performance loss or even correctness issues, leading to great challenges in verification and debugging. 
% For example, Intel disables its official hardware transactional memory extension on CPU generations from Skylakes due to the discovered security issues and functional bugs. In addition to implementation simplicity, this principle also brings performance advantages, for example, the worker threads on the same node could exploit the locality with local cache hierarchies. 



% To this end, \name~


% \noindent \textbf{Principle 2: The primitives should be compatible with existing softwares.} Because the database community has developed abundant software transaction scheduling strategies, such as concurrency control algorithms to serialize transactions, logging schemes for durability, and complex index keeping the key-value mapping. The new primitives should not pose any limits on the choice of these software algorithms, but should benefit the mainstream choices. Moreover, the proposed architecture should be compatible with existing operating systems, so that we could leverage the existing optimized address translation datapath for the virtual memory subsystem. 
% This enables our proposal to be treated as an inexpensive feature that can be turned on without effecting other services. 

% The resulting hardware consistency model should be compatible with universe design choices of existing in-memory transaction processing systems.
% This requires only a few application code changes to facilitate the remote memory. The database community has developed various concurrency control algorithms to fit different using cases, some of them even mix multiple algorithms during the execution of a single transaction. 
% To this end, the newly created consistency model should not pose any limits on the choice of software algorithms, especially on concurrency control. This enables existing system to treat this work as an inexpensive feature that can be turned on without effecting other services. 

% \noindent \textbf{Principle 3: Moving non-critical operations to software to make hardware fast path deterministic and fast. } If an operation is non-critical and requires complicated processing logics, we propose to move it to the software slow path running on the host manager process. The hardware expose its internal controlling metadata as a special memory space, which could be directly read out and modified by host process via \cxlcache. For example, data recovering~\ref{Sec.} is expected to be a rare operation. Handling data recovery, however, would involve dealing with complex historical log trees. Furthermore, in order to make the fast path performance deterministic, we decouple the slow path tasks, as well as cross-node communication from the performance critical path asynchronously run them on different clock fields. 



 
% This loosely coherent memory primitive challenges the basic limit, i.e. single write atomicity, of CXL specified memory sharing models. It consequently meets compatibility issues that potentially require significant modifications on both processor architectures and CXL protocols. 
% Fortunately, the CXL implements the DRAM controller at the device, which opens up an opportunity to intervene the memory accessing while being transparent to the processors and CXL protocols. 
% We propose a software-hardware co-designed approach named view memory shim, which intercepts host's memory accessing requests and redirects them to the proper DRAM locations. The hardware part is a pure CXL device agent that works on the path of memory accessing pipeline and performs the redirection. The software is a processor side runtime process working off the memory accessing path. The software monitors application using status, reconfigures the hardware agents to maximum its efficiency, and handles other non-critical but complex procedures such as view allocation and reclamation. 

% that differs with most shared memory architectures with respect to cache coherence models. We decouple the memory access with the coherence resolving process. During the transaction execution phase, the nodes conduct memory reads and writes without involving any snooping issues, as if it is the only node in the system. 
% Different with prior software-only approaches~\cite{farm, compromise, drtm, drtmh, drtmr, rdma_txn1}, we resort a CXL-native solution with a software-hardware co-design, in which hardware accelerates simple primitives, and falls back to software for exploring complex high-level concurrency control strategies. To this end, we do not disturb host processor architectures, CXL protocols, and OS mechanisms, thus keeping compatibility with existing systems. Meanwhile, the hardware has reasonable overheads to fit the computation capability of CXL devices. 
% To this end, this primitive reduces the cross node coherence traffic since it only preserves the coherence for the ``true'' data dependencies between committed memory operations. 


% By exposing internal control metadata through a special memory space, which can be accessed and modified by the host process via \texttt{CXLcache}, operations like data recovery, which are rare but complex, can be managed more effectively. Separating these less urgent tasks from the main performance pathway allows for asynchronous processing, improving overall system efficiency.
% It uses a bounded, known number of cycles to process a data unit. It avoids long-tail load/store on the critical path, since IMDB is seriously bounded by the memory performance. It requires fixed size of pipelines, 

% \noindent \textbf{Principle 1: Keeping separated memory coherence model for metadata and records. }
% We observe the metadata and records have distinct access features. A typical system induces heavy atomic operations on the metadata field to synchronize transactions, for example, it could atomically increases the timestamp with Fetch-and-Add~\cite{dbx1000}, or acquiring a lock with LL/SC or Compare-And-Swap, while the accesses toward record field are common reads and writes. To address \textbf{C2}, we maintain the traditional strict coherence model for metadata to efficiently support atomics, while adopting a new relaxed coherence model for the records. It is feasible since metadata only takes up only a negligible fraction of the total memory footprint. 

% it's costly to implement atomics on a non-coherent architecture since they rely on the coherence protocol to implement cacheline locking. Keeping the strict coherence for metadata is performant and feasible since it only takes up negligible memory footprint. 

 

% In this work, we propose \name, a novel CXL-native in-memory transaction processing system with software and hardware co-design. \name~is completely transparent to host CPU architectures and application software. To the best of our knowledge, \name~is the first work to use CXL for multi-host in-memory transaction processing systems. 
% We also customize the data layouts and co-design the logging scheme and garbage collection procedure. 
% Evaluate: feasibility with real-world devices, OS integration. 

% \name~is compatible to existing CXL-enabled processors, since it only requires development on CXL devices. 

% Note that it's challenging to implement them without disturbing hosts' architectures but keeping system efficiency. 


% First, to establish a coherent data path among hosts, we propose the \textit{protocol forwarding} technique, which extends CXL protocols beyond a single host. With this technique, multiple hosts can communicate with each other by reading/writing the shared memory. We provide detailed hardware architectures and protocol descriptions that enable this technique. 

% Second, to reduce the overhead of maintaining coherence, we further propose a hybrid consistency memory model that consists of two memory regions. In the \textit{strongly consistent} memory region, we guarantee a full protocol of memory coherence. In the \textit{weakly consistent} memory region, we allow a much looser consistency that does not preserve cache coherency. Within in this region, reads not always yield the most recent data, and explicit cache flushing operations are required for any data update to be observed globally. 

% Although some previous works~\cite{pond, directcxl, tpp_pub} have used CXL for data center, they are mainly focused on memory disaggregation, where memory is not shared across hosts. To the best of our knowledge, no previous work has discussed using CXL for in-memory transaction systems.  


% This model guarantees the coherence for a memory region, referred to as \textit{strongly consistent} memory region. 
% All other memory regions are \textit{weakly consistent}, and require explicit cache flushing for any data updates to be globally observed. Due to its looser consistency model, accessing the \textit{weakly consistent} memory region avoids impacting CPU's caches, thus incurs much lower latency than accessing the \textit{strongly consistent} region.

% This model allows different coherence coe
% (\Chen{hierarchical? multi-level?}) 
%  \Chen{This model allows a hierarchical coherence protocol across the memory region. In the \textit{strongly consistent} memory region, we allow the user to define a narrowed memory scope, where we guarantees a full protocol of memory coherence. In the \textit{weakly consistent} memory region, we allow a looser memory coherency at a much larger scope. Explicit cache flushing operations are required for any data update to be  observed globally. Thanks to the looser consistency design, data transactions in the \textit{weakly consistent} memory region has a much lower latency because it avoids impacting CPU's caches (?? polish, what does "impacting CPU cache" mean?).} \sout{This model guarantees the coherence only for a small memory region, referred to as \textit{strongly consistent} memory region. All other memory regions are \textit{weakly consistent}, and require explicit cache flushing for any data updates to be globally observed. Due to its looser consistency model, accessing the \textit{weakly consistent} memory region avoids impacting CPU's caches, thus incurs much lower latency than accessing the \textit{strongly consistent} region. }

% Furthermore, we construct a high-performance in-memory database leveraging both memory regions. We customize the data layouts and co-design the concurrency control protocol, logging scheme, and garbage collection procedure to  manipulate database tuples correctly and efficiently. Our experiments demonstrate that \name~can achieve a throughput that is much higher than previous network-based designs. \name~is compatible to existing CXL-enabled processors, since it only requires development on CXL devices. 

% Due to the asymmetric nature of the CXL protocol, implementing \name~requires no modification to the host CPUs, but only requires development on CXL devices. 
% \Chen{(What is this sentence intended for? How is it related to our work?)}

% \red{We prove that such a hybrid consistency model will not lead to faults due to the final consistency feature of transaction systems. By carefully designing the transaction. }

% Consequently, we co-design a transaction layer to leverage such heterogeneity. 

% The other parts are \textit{weakly consistent} that updates on that space need explicit cache flushing to be globally observed.
% The transaction layer implements transactional operations on the key-value store. 
% locks on the \textit{strongly consistent} memory region to exploit cache coherency, but stores data on the \textit{weakly consistent} memory region to get benefits from low access latency.

% we propose a co-designed transaction layer to provide transactional operations on. Specifically, we maintain serializability and atomicity with two-phase commit and two-phase locking, \red{and provide durability with write-ahead log.} 
\noindent \textbf{Contributions. }
In summary, this work makes the following contributions: 
% In this paper, we first discuss what CXL outperforms traditional networking techniques in transaction systems. Then we show the challenge to employ CXL as underlying interconnects, by analyzing primary design's inefficiencies. We designed \name~, a software-hardware co-designed transaction systems. \name~leverages coherency, byte-addressability, and low latency of CXL. Specifically, \name~employs MIC to carry data across machines and preserves fast locks. A software run-time. 
    % We discuss the inefficiencies of the vanilla CXL primitives that enable the memory sharing. We provide a quantitized analysis by evaluating on the off-the-shelf commercial platforms as well as simulating on the ideal hardware that has been claimed, and both of them shows significant performance loss. We point out the two unique performance issues of the vanilla primitives: the high remote fetching latency and the snoop filter conflicts. 

\begin{itemize}

    % attributable to two distinct issues: large latency in remote cache accessing and conflicts within the centralized snoop filters (Sec.~\ref{sec:motivation}).

    
    % We propose \name, a holistic solution that customizes accessing primitives for transaction processing systems, based on our observation of transaction's optimistic coherence model. We detail \name's primitives, hardware architectures, software runtime, and integration methods to existing systems. To the best of our knowledge, this is the first work to discuss the primitives of CXL shared memory pool. 
    
    % \red{Ranging from xx to xx}
    % This implementation includes various concurrency control mechanisms, logging strategies, and indexing methods. 
    
    % We showcase the usage of \name~with an exampled key-value store based on a widely adopted transaction processing framework. We implement multiple concurrency control algorithms, logging schemes, and index organizations based on \name~primitives. 
    % We evaluate \name~on a bunch of OLTP benchmarks and show a broadly performance benefits on most system choices. 
    
    \item We discuss the network-induced performance degradation in traditional distributed transaction processing systems and illustrate how CXL-based memory sharing offers a viable solution (Sec.~\ref{sec:background}).

    \item We critically examine the limitations inherent in standard CXL primitives for memory sharing. Through quantitative analysis, we demonstrate that the performance is far below expectations, due to two key performance issues (Sec.~\ref{sec:motivation}).

    \item We introduce a comprehensive solution with a software and hardware co-design approach. We detail a suite of custom primitives (Sec.~\ref{sec:primitive}), and describe the systematic and architectural supports of \name~(Sec.~\ref{sec:details}). To the best of our knowledge, this is the first work studying CXL-based memory sharing on transaction processing systems.

    \item Based on the \name~architecture, we implement an exemplary key-value store with a broad exploration of design choices in transaction systems, based on the popular framework DBx1000~\cite{abyss_vldb14}. In evaluations using typical OLTP benchmarks, \name~reduces over 95\% of cross-node coherence traffic and achieves up to 2.08x throughput improvements.


\end{itemize}


% The rest of the paper is organized as follows. Sec.~\ref{sec:background} and Sec.~\ref{sec:motivation} introduce the preliminary systems about transactions and discuss the motivation of \name. Sec.~\ref{sec:overview} highlights the three-layer design of \name, and Sec.~\ref{sec:hardware}, Sec.~\ref{sec:protocol}, and Sec.~\ref{sec:transaction} provide the details. We evaluate \name~in Sec.~\ref{sec:eval}, and discuss the limitations in Sec.~\ref{sec:limitation}. Sec.~\ref{sec:conclusion} concludes this paper. 


% introduces the preliminary information about transactions.  discusses the inefficiencies of traditional networks, describes why CXL is promising, and illustrates the challenges of \name.
% Sec.~\ref{sec:overview} provides the overview of \name, which is co-designed in three layers. 
% Sec.~\ref{sec:hardware} introduces the hardware layer, Sec.~\ref{sec:protocol} introduces the coherence protocol layer, and Sec.~\ref{sec:transaction} describes the transaction layer. Sec.~\ref{sec:eval} presents our experimental results, Sec.~\ref{sec:limitation} discusses the limitation of this work, and Sec.~\ref{sec:conclusion} concludes this paper. 