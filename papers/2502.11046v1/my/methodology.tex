\begin{figure}[t]
  \centering
  \includegraphics[width=0.47\textwidth]{my/figs/software.pdf}
  \caption{View memory shim. }
  \label{fig:view_model}
\end{figure}

\section{\name~Architectural Supports}     \label{sec:details}

\subsection{View Memory Shim}   \label{subsec:vms}


% As a consequence, we need a fundamentally new memory accessing data path to keep written back cacheline invisible from other nodes without changing processors' architectures (\textbf{Principle 1}). 


% \noindent \textbf{Approaches. }
% We introduce the view memory shim layer that redirects s
% : a view is a virtual table defined by a query containing database operators such as filtering and projection. 

% The processes in node-1 and node-2 thus see X in different values that node-1's X is 1 while the node-2's X is 0. 
% , but also matches the granularity of processors' cache allocating and reclaiming, thereby 
% , hence enables the view management to be seamless integrated to existing memory access pipeline (\textbf{Challenge 2}). 

In the following discussion, we first focus on the architectural support of the loosely coherent model and leave the integration of hybrid models to Sec.~\ref{subsec:memory_mapping}. 
In this section, we illustrate the core conceptual model of our design, named view shim layer. The view shim layer operates between the node's physical address space and the G-FAM DRAM address space, aiming to redirect memory accesses from nodes to the appropriate G-FAM DRAM locations according to L-Ld and L-St semantics. We adopt the concept of ``view'' from relational databases~\cite{mvc_micro23, pageoverlay_isca15, overlaytm_pact19}. 
As Figure~\ref{fig:view_model} shows, each physical address can be mapped to a DRAM location called a view shim, in addition to a regular DRAM location mapped by the CXL IP. At a high level, when a physical address has both an original DRAM mapping and a view shim mapping, the L-Ld and L-Sts are served by the view shim. Only addresses not present in the view shim are accessed from the regular DRAM location. 
Each node can own at most one view shim for a shared object, but views from different nodes are isolated. In this example, the physical object X is mapped to both the original DRAM address space and a view shim in node-1, while node-2 only has the DRAM mapping. Consequently, the process in node-1 observes X=1, while the process in node-2 observes X=0.


Logically, a view shim is created on a memory store and is merged with the original memory location upon a GSync call, resembling copy-on-write semantics. 
Physically, the view has two statuses: on-chip and overflowed. The on-chip view resides within the processor's cache hierarchy as a common dirty cacheline and transitions to the overflowed status once it is evicted from the processor. A view starts in the on-chip status since any store would first load the cacheline to the processor followed by a store hit. 

The on-chip view naturally fits the L-Ld and L-St semantics as it is only observed by local loads and stores. However, the G-FAM needs to ensure that overflowed views are not observed by other nodes. To achieve this, we allocate a DRAM space called the view memory store (VMS) to buffer the overflowed views, acting as an evicted cache. We term the DRAM space that stores original shared contents as the exposed memory store (EMS) and maintain the address mapping between EMS and VMS via the node-wise view address translation flow.


% We allocate a DRAM space named view memory store (VMS) to buffer the written back cachelines, and we name the DRAM space that stores original shared contents as the exposed memory store (EMS). We keep the address mapping between EMS and VMS via the node-wise view address translation flow. 
% The VMS is the exclusive eviction buffer of host processor's caches, so that an VMS read hit will turn the overflowed view to the on-chip status and remove the VMS content.

% \red{More Details. } % 与实现无关的目的，model说明白，整个流程，有一些数据在来了之后会被redirect到VMS，先讲目的，再讲具体实现。

% We call the DRAM segment that stores the view contents as view memory store (VMS) which is transparent to the worker nodes, and call the DRAM segment that is mapped to worker node's physical address space by CXL IP as Exposed Memory Store (EMS). Every dirty cacheline write-back request would create a view in the VMS for the requesting node. A view address table (VAT) keeps such a mapping between the physical address and the actual view address in VMS, while the CXL IP keeps the mapping between the physical addresses and EMS. 





% The VMS is private to a node, while the EMS is shared by all nodes.
% The overflowed view locates at the node's private VMS and the address mapping between the VMS and EMS is kept on a view address translation table (VAT). 
% The EP DRAM is split into two disjoint regions: exposed memory store (EMS) and the view memory store (VMS). 
% The EMS keeps regular memory contents shared by all nodes, and the VMS stores the node-private views. A view is created on an EMS write back, and 

% The view shim follows the copy-on-write semantic that a memory store toward the EMS would create a new view shim for the operation requesting node. 
% The on-chip view is shared by all processors within the requesting node through the intra-node cache coherence mechanism. 
% It observable for all processors within the node through intra-node cache coherence mechanism. 
% Each view would start with the on-chip status, since a store miss results in an HDM read with a later silent write cache hit. 
% To avoid the overflowed views being observed by other nodes, we allocate a buffer in the node's private VMS and redirect the overflowed view's write to it. 
% We keep the address mapping between the view address and the original EMS address via the view address translation table (VAT). 
% It is created by the processor's standard memory store flow that first issues a load for an exclusive ownership of the cacheline, and follows a cache store hit. 






% A view could only be either on-chip or overflowed at a time, so that a read hit on the VMS content will remove the VAT entry. A view shim is reclaimed and merged to the EMS with the GSync primitives, which invalidate all other nodes' views to force them observing the new value.
% In contrast, a Wd would delete the view shim awareness of its status. 
% we leverage the locality nature of L-Ld and L-St to successfully avoid remote fetching from the critical path, which make these primitives always fast. 
\ifx\undefined\stale
\noindent \textbf{Benefits for the view memory shim model. }
First, by redirecting written back caches at the EP side, we successfully keep the L-Ld and L-St private to node while being transparent to both processor architecture at hardware and the OS's virtual memory subsystems. 
Note that this approach is not allowed in traditional SMP architectures but is only enabled by the CXL, since CXL HDM allows hardware changes on the memory manager~\cite{neomem}. 
Second, The host cache is the major store of view shims, which are fast and well verified, but the device's VMS is only accessed when a memory access misses the entire processor's cache hierarchy. This brings two performance benefits. (1) The traffic related with view shim management is greatly reduced. (2) The amount of view shims that VMS manages is typically small. 
\fi
% by organizing the VMS as the victim of processors' caches, we greatly reduce data capacity that CTHW needs to manage, and eliminate VAT accesses if the host cache hits (which is the most case). 
% acts as the victim cache that is
% , which will copy the view shims to the EMS as well as all invalidates other nodes' views, no matter in on-chip or overflowed status, to force them to observe the updated content.
% the data since it is turned from the overflowed to on-chip. 
% HDM reads from the same node would return with the view value. 
% The 
% The VMS thus acts as the victim cache of the entire processor's cache hierarchy. 
% be returned with the view value but not the original location. 

% Similar to the on-chip view, the overflowed view shim serves the memory operations issued from the processors at the same node. 




% Any view shim starts with the on-chip status, since a store would read the data first then do the silent cacheline write. 

% Once a transaction commits, it merges its write set to the regular DRAM location, i.e. the main version, and invalidates all other node's stale views involved by its write set to force them observing the updates.  

% , then copies the new values from its private view to the regular DRAM space. Such a flow forces other nodes to observe the new value from this committed transaction.
% so that its value corresponds to the view shim. On the other hand, the cacheline X on node-2 only has the DRAM mapping, so that the X of node-2's physical address turns out to be 0. 

% Despite this primitive share some similarity with multi-version concurrency control, 
% This primitive is powerful enough to facilitate transaction processing on the shared memory pool. 
% All processes in the same node share the same view, and views of different nodes are isolated. 
% When a record is written by a process, whether the corresponding transaction is committed or not, a view shim is created for the node it is running, while other nodes could still observe the consistent value. 
% The consistent snapshot is not disturbed, so that the transaction on another node will still observe the consistent value. 
% Data races on the same node is addressed by single-node concurrency control shemes. 
% Once a transaction commits, it invalidates on all stale views involving its write set, then it copies the new values from its private view to the shared space. This forces reads on other nodes get the new value so that it keeps consistency.
% 应该是所有的stale copy，而不是所有copy。因为speculative copy可能在他后边serialize，这是write-after-write，属于可序列化的范畴，因此应该留下来

\ifx\stale\undefined
Keeping the loose coherence with read and write redirection allows three distinct advantages.
% First, it eliminates all hardware-introduced unnecessary coherence by redirecting potential data races to different addresses, since only a committed transaction could broadcast its stored data via the cache coherence mechanism. 
First, L-LD and L-St avoid incuring the remote fetching process, thus much faster than the CXL-vanilla loads and stores. 
% we leverage the locality nature of L-Ld and L-St to successfully avoid remote fetching from the critical path, which make these primitives always fast. 
Second, our approach is transparent to the virtual memory system of current OS by laying under the physical address. The current address translation component such as TLB and page tables would not observe the view shim (\textbf{P2}). 
Third, as we will describe the next section, our design fully exploit the write back nature of the typical x86 processors' cache designs (\textbf{P1}). The host cache is the major store of view shims, which are fast and well verified, but the device's view memory store only acts as the victim cache that is accessed when a memory access misses the entire processor's cache hierarchy. This incurs two performance benefits. (1) The traffic related with view shim management is greatly reduced. (2) The amount of view shims that view memory store manages is reduced. 
\fi
% This enables both operating systems and transaction processing software to work seamlessly with the hardware optimization. 
% This is very important as current software employs complex VM managing and optimization algorithms, so that changing the memory model may lead to unexpected issues on performance and correctness. 
% (3) Most importantly, the view management only incurs device modification so that incurs no changes to processors. This make \name~feasible in current commercial datacenter clusters. 

% Keep the mapping between physical address space and the DRAM memory could be costly. We offload the management to hardware with plenty of optimizations to reduce the cost. 

% our approach makes no changes to the way the existing VM framework maps virtual pages to physical pages. This is very important as the system can treat shims as an inexpensive feature that can be turned on only when the application benefits. 

% This is very important as it not only enables a low overhead implementation of our framework, but also enables the system to treat shims as an inexpensive feature that can be turned on or off depending on how much the specific system benefits from shims (backward-compatibility)

% Eliminate the need 
% Programming model
% Mapping







% Challenge: 
% Solved by the hardware: 
% 1. keep compatible with page table/virtual-indexed physically-tagged L1 cacheline
% 2. detect whether a cacheline belongs to the view
% 3. ensuring the consistency

% The hardware implementation of \name~is a device-only architecture being compatible with processors and current CXL protocols (3.1). 
\ifx\stale\undefined
While the view abstraction imposes simple memory access primitives, there are several key challenges to efficiently implement the proposed primitives. 
First, the hardware should check if a cacheline belongs to the view shim. 
Second, the hardware should ensure the consistency of committed writes. 
\fi
% physical address to view mapping
% invalidation other views



\subsection{View Address Translation Flow}      \label{subsec:vat}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{my/figs/vms.pdf}
  \caption{(a) The format of view shim indicator and (b) View address table (VAT) components. }
  \label{fig:vms}
\end{figure}

% As Figure~\ref{fig:overview}~(c) illustrates, we architecturally defines two data path working for the standard memory accessing primitives of L-Ld/L-St, and the view synchronization primitives of GSync and Wd. 
% We map the entire DRAM to a continuous OS's physical address segment for each node when the system boots up (detailed in Sec.~\ref{subsubsec:system_integration}). 
% The CXL IP then records the basic addresses for each node. When the IP receives a load or store request, it transforms the physical address by subtracting it with the basic address of the corresponding node. The resulting address is the EMS address of the object that it accesses. 

The view address translation flow controls whether the G-FAM returns a read request with the view in VMS or the original content in EMS, and it allocates space for overflowed views. It does not change the CXL IP but operates behind it. The CXL IP retrieves and translates the physical address to the EMS address~\cite{cxl-fpga-doc, cxl-doc}. 
Our flow takes this decoded information as input and outputs the target DRAM address according to the view shim model. We architecturally define three data structures: the view address table (VAT), which manages the mapping from the view shim indicator to the VMS address; the VMS filter (VF), which precedes the VAT to sift its query traffic; and the VMS Back Filter (VBF), which tracks the processor caches.


% The CTHW translates such a view shim indicator to the actual DRAM address of the VMS. 
% an overflowed view content with the concatenation of 
% The address translation flow takes a view shim indicator as the input, as outputs the actual DRAM address that may lie in EMS or VMS. 
% As Figure~\ref{fig:vms}~(a) shows, a view shim indicator is the concatenation of the view owner's node id and the EMS address of the accessed object. 
% With proper address mapping, the EMS address is directly calculated from the CXL IP (detailed in Sec.~\ref{subsubsec:system_integration}). 
% When the CXL IP receives a load or store request, it transforms the physical address by subtracting it with the basic address of the corresponding node.

% As Figure~\ref{fig:pipeline} shows, this flow architecturally defines three data structures: the view address table (VAT) that manages the mapping from the view shim indicator to the VMS address, the  VMS filter (VF) that stays before the VAT to sift its query traffic, and the VMS Back Filter (VBF) that tracks the processor caches. 

% Each memory accesses should check the VAT to find out if it has the view shim, the VAT thus lies on the critical paths and requires carefully design. 
% Similar to the traditional virtual memory subsystem in OS, each host is given an illusion that is working on a private, continuous section of shared memory. 
% In common memory accesses, the VMS is invisible to both applications' processes and operating systems. The creation, querying, and deletion of VATE is automatically managed by the \name~hardware. 
% Each VMS object is specified with an indicator concatenated by the view owner's node id and the EMS address of the original object, as Figure~\ref{fig:vms}~(a) shows.

\subsubsection{VAT Details. }

A VAT entry (VATE) holds the address mapping at cacheline granularity. The VAT resembles OS page tables in functionality, but traditional radix-tree-style page tables are not suitable for our scenario since address translation involves page table walks that potentially traverse all levels of the tree sequentially~\cite{cuckoo_pagetable_asplos20, clio_asplos22}. To address this, we adopt a flattened cuckoo hash table architecture that bounds the number of view address translations within a limited number of DRAM accesses. 
As Figure~\ref{fig:vms}~(b) shows, we organize the VAT as a variable list of cuckoo hash tables~\cite{cuckoo_hash, cuckoo_pagetable_asplos20}. 
The rationale behind choosing cuckoo hashing is to trade the low cost of L-Ld with the insertion overhead of L-St, 
since an L-Ld typically lies on the critical path, whereas an L-St does not, due to current processors' out-of-order execution and asynchronous cache eviction~\cite{book_cc, rvweak_isca18, rvweak_pact17, quantitative_approach}.


\ifx\undefined\stale
The cuckoo hash table maps one key to two possible hashing locations with different hash functions. The element is stored in at most one of these locations at a time, but it can move between its hashing locations. An L-Ld incurs a cuckoo hash query, which checks both hashing locations and succeeds if it is found in either of them. If no valid VATE is found, it returns with the original EMS address.
An L-St incurs a cuckoo hash insertion, which places an element in one of its two possible entries. If the selected entry is occupied, the algorithm kicks out the current occupant, and re-inserts to the other hashing location. The process keeps until no occupant is evicted, or meets the number of maximum tries. 
\fi

An on-chip lookup table (\oone) stores the hash table base addresses, and the hash tables are located in the VMS (\ttwo). We use the first k bits to calculate the table's base address and the following s bits to query the cuckoo hash tables. 
\rvs{
The cuckoo hash table maps one key to two possible hashing locations using different hash functions. An element is stored in only one of these locations at a time but can be relocated between them. An L-Ld operation introduces a cuckoo hash query, which checks both locations and returns success if the element is found in either. If no valid VATE is located, it reverts to the original EMS address.
An L-St operation initiates a cuckoo hash insertion, placing an element in one of the two possible entries. If the chosen entry is already occupied, the algorithm displaces the current occupant, attempting to re-insert it at the alternative hashing location. This displacement process continues until it succeeds without eviction or reaches the maximum number of allowed retries.

% There are two potential performance issues of this VAT design. 
% First, an VAT insertion could involve memory allocation for the cacheline content in addition to inserting an VATE to the hash map. To remove memory allocation from the critical path, we pre-allocate the buffers for cacheline contents related with a hash map when the CTRt creates it. Second, cuckoo hash tables have the possibility of insertion failure when meet maximum number of retries. When insertion failure happens, the CTHW instantly blocks the translation pipeline towards the corresponding hash map, set the resizing error bit, and calls CTRt to do VAT resizing and retries the insertion (Sec.~\ref{subsec:vat_resizing}). But the CTRt resizes the VAT in time to keep its occupancy moderately, so that the insertion failure is very rare (Sec.~\ref{subsec}). 

There are two potential performance inefficiencies in this VAT design. First, a VAT insertion may require memory allocation for the cacheline content, in addition to inserting the VATE to the hash table. To mitigate this, we pre-allocate buffers for cacheline contents associated with the hash table. Second, cuckoo hash insertion may fail after reaching the maximum number of retries. Upon such a failure, the CTHW blocks all accessing to the hash map, sets the resizing error bit, signals the CTRt to resize VAT and retries the insertion (Sec.\ref{subsec:vat_resizing}).
% However, such an insertion failure is in practice rare since the CTRt actively manages VAT occupancy to keep it under a moderate threshold. A typical threshold of 0.6 ensures that an insertion is completed under 6 retries, we would detail it at Sec.~\ref{subsec:latency_sweep}.  
Nonetheless, the occurrence of such an insertion failure is very infrequent in practice, as the CTRt actively oversees VAT occupancy to keep it within a moderate level. A threshold of 0.6 guarantees that an insertion is typically accomplished within 6 retries. We will discuss this at Sec.~\ref{subsec:latency_sweep}. 
}


% a hash table during CTRt's VAT expansion process, thus removing the non-deterministic allocation process from VMS insertion's critical path. Another key challenge of cuckoo hash tables is the possibility of insertion failure. 

% which we address with the help of CTRt (Sec.~\ref{subsec:vat_resizing}). 
% This hash table organization provides two benefits. First, it allows for parallel processing of the hash table base address checking and the hash table offset calculation. Second, it enables efficient scaling of the VAT size by adjusting the number of active tables. 

% By carefully selecting the occupancy threshold $r$, the insertion failure of VAT could be vary rare. A typical setup of $r = 0.6$ and the maximum number of cuckoo hash retries as $64$ could keep the almost zero failure. However, it still could occur. 



% The view shim indicator is divided to three segments. The most significant k bits calculate the table's base addresses, the following s bits query the cuckoo hashes to get the VATE, and the last c bit indicates corresponds to cacheline bias which is always be 0. 



% An insertion error incurs the VAT resizing (detailed in Sec.~\ref{subsec:vat_resizing}). 

%  indicating when the EMS has no view mapping for the requesting node, then the query would 
% To this end, a query involves a fixed two DRAM accesses, while a insertion requires undeterministic number of DRAM accesses. As occupancy increases, the insertion attempts could be large. Hence, it requires the VAT resizing, which is solved by the CTRt (Sec.~\ref{subsec:vat_resizing}). 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.49\textwidth]{my/figs/pipeline.pdf}
  \caption{Address translation process for L-Ld and L-St. }
  \label{fig:pipeline}
\end{figure}

% write buffer and non-blocking cache design
% Unlike normal virtual memory system that keeps address mapping at 4KB pages or larger, each view address table entry (VATE) manages the address mapping for a cacheline to align with the granularity of CXL requests. VATEs are allocated with the granularity of a large chunk. 
% % Such a design choice alleviates the well-known write pollution problem~\cite{rethinking} for pages level memory management.
% Accordingly, the view shim content's indicator is split into three segments, as Figure~\ref{fig:vms}~(b) shows. The most significant k bits calculate the chunk base address. The mapping are stored in a full sized on-chip lookup table with k-bit input and k-bit output. The following s bits are used to calculate the VATE chunk bias. The mapping is managed by a two-way cuckoo hash for each chunk. An address translation checks the LUT and hashes for the chunk bias in parallel, the results are joint to determine the VATE address. The translation process could fail due to either the chunk does not exist, or the cuckoo hash can not find a valid VATE, it indicates the PA has no view mapping, and thus the process returns the EMS address. 



\subsubsection{VF and VBF Details. }



Despite the VAT optimizing DRAM address translation, it still impacts performance since every memory access must check the VATE in DRAM. However, most VAT queries would miss for two reasons. First, stores are relatively rare in real-world transaction applications~\cite{snow_osdi16, port_osdi20}, so the VMS is typically sparse. Second, most views reside in host processor caches and are seldom written back to the VMS due to widely adopted LRU-based cache eviction policies.

To leverage this feature, we introduce an on-chip VMS Filter (\onee~in Figure~\ref{fig:pipeline}, VF for short) to sift out VAT queries. A read on the shared memory would query the VF first, accessing the VAT only if the filter outputs a positive hit. Otherwise, the read falls back to the original EMS address. 
CTHW also tracks the host processor's caches to invalidate stale cachelines at synchronization (detailed in Sec.~\ref{subsec:vsf}). Consequently, each CTHW maintains a VMS Back Filter (\twoo, VBF for short) that works in reverse to the VF: a DRAM load inserts into the VBF, and a dirty write-back removes from the VBF. 
Compared with the original CXL SF, the VBF has three advantages:
First, VBF is organized in a distributed way, so that each VBF only tracks the local processor's cache, avoiding a single VBF becoming the system's bottleneck. 
Second, VBF queries and inserts are parallel with VF accesses, thus introducing minimal overhead to the memory access critical path. 
Third, the VBF exhibits much higher spatial efficiency when implemented with approximate data structures, such as elastic counting bloom filters~\cite{elasticbf_tc22}. 


% thanks to CXL's enabled speculatively invalidation requests. 
% could be inaccurate thanks to CXL's enabled speculatively invalidation requests, hence it could adopt designs with higher space efficiency with the cost of slight accuracy drop. 

% In our current implementation, we adopt the counting bloom filter for both VF and VBF. It maps the key to multiple bits of the vector. By carefully trade-offing its design parameters, the VF works effectively that reduces 85\% VAT queries on average (Sec.~\ref{sec:evaluation}). 
% It may cause false-positive error that it may report positive for a non-exist object. But in our architecture, such an error will not cause correctness problem but only a slight performance issue since the VAT is accurate. 


% a fast and space efficient data structure. 
% The size and the number of hash functions of the bloom filter would effect the error ratio, which forms the trade-off with on-chip resource usage and performance. 
% The filter works effectively that reduces 85\% VAT queries on average (Sec.~\ref{sec:evaluation}). 

% It may cause false-positive error that it may report positive for a non-exist object. But in our architecture, such an error will not cause correctness problem but only a slight performance issue since the VAT is accurate. 

% \subsubsection{VMS Back Filter Architecture. }

% most tuples accesses are read-only in typical transaction applications
% The objects with no view shim does not necessitate VAT queries. 
%  avoids false-negative errors so that a miss on the VMS filter indicates the VAT must not hold the queried key. However, the filter may incur
% To this end, a query involves a fixed two DRAM accesses, while a insertion requires undeterministic number of DRAM accesses. As occupancy increases, the insertion attempts could be large. Hence, it requires the VAT resizing, which is solved by the CTRt (Sec.~\ref{subsec:vat_resizing}). 
% However, this issue is rare due to two reasons. First, a cacheline is only inserted to the view when it is both dirty and evicted from the entire host's cache hierarchy. It makes VAT insertion relatively more sparse than the memory access. 
% Second, a view content is removed when a read hits the view since the view would change to the on-chip status that no longer needs to be stored (Sec.~\ref{subsec:cold_path}). 
% As we evaluate in the real-world memory traces (Sec.~\ref{}), the number of chunks is always under xxx. 






% The view address translation introduces performance overheads since 


% Because most tuples are read-only in typical transaction applications that have no view shims~\cite{}. 




% The VAT querying could be expensive when the VAT is large, hence we introduce the ST as VAT abstract to minimize VAT visits.  
% The ST tracks two datasets: the evicted dirty cachelines kept in VAT and the cachelines resides at the host caches. 
% The ST is based on the customized bloom filter, which trades accuracy for query performance. A miss in the ST indicates a cacheline not exist in the queried status set, and a hit indicates that the cacheline probably exists. We maintain an acceptable false-positive rate of ST through dynamic sizing.


% \subsection{View Memory Store Address mapping}
% As Figure~\ref{fig:software} illustrates, the CXL internal memory is split into two disjoint regions: exposed memory store (EMS) and the view memory store (VMS). 
% The EMS keeps regular memory contents and is accessed when the requesting node does not has the corresponding view shim. 

\ifx\stale\undefined

\subsection{Memory Address Management}

Similar to conventional main memory mapping, the EMS's addresses are mapped one-to-one to the physical memory address space of each node based on a node-wise address bias.
The CXL IP maintains such bias at the device enumeration phase (Section.~\ref{Sec.}). 
When a host accesses HDM, it passes the physical address to EP via the CXL link, then CXL IP at EP side subtracts the physical address with the corresponding node's base address and gets the EMS address. 
The VMS stores two things: view shim contents in the overflowed status for every node, and a view address translation table (VAT) that indexes the view shim contents. 

As Figure~\ref{fig:vms}~(a) illustrates, each object in the VMS is specified with an indicator that is a bit vector concatenated by the view owner's node id and the EMS address of the original object. VAT resembles the current operation systems' page table that maps the view indicator to the DRAM address. However, the traditional radix-tree-style page table does not fit our scenario, since looking up a translation involves a page table walk potentially all the levels of the tree in a sequential way. It incurs a pointer-chasing process wiht multiple DRAM accesses. 

% to walk through the page tables. 
% Similar to traditional virtual memory systems, the VA requires an address translation process to map such view content identifiers to the DRAM address. However,

% As a consequence, the current architecture that stores entire mapping with the radix-tree-style or hash-based page tables does not fit our use case because of two reasons. 
% First, page table walking would introduce multiple DRAM accesses since the on-chip buffer is insufficient to store all page table entries, which would introduce significant latency overheads. 
% Second, page table allocation and walking could be a complex multi-step process, especially when we adopt the lookaside buffer to cache the translation results since it would introduce writeback issues. This violates the \textbf{P3} to keep the hardware simple and fast. 

We propose a flattened VAT that bounds the address translation to a fixed two DRAM accesses (\textbf{Principle 3}). Unlike normal virtual memory system that keeps address mapping at 4KB pages or larger, each view address table entry (VATE) manages the address mapping for a cacheline to align with the granularity of CXL requests. VATEs are allocated with the granularity of a large chunk. 
% Such a design choice alleviates the well-known write pollution problem~\cite{rethinking} for pages level memory management.
Accordingly, the view shim content's indicator is split into three segments, as Figure~\ref{fig:vms}~(b) shows. The most significant 12 bits are used to calculate the chunk base address, and we adopt the full sized on-chip lookup table to keep such a mapping. The following 43-6 bits are used to calculate the VATE chunk bias which are managed by a two-way cuckoo hash. An address translation checks the LUT and hashes for the chunk bias in parallel, the results are joint to determine the VATE address. The translation process could fail due to either the chunk does not exist, or the cuckoo hash can not find a valid VATE, it indicates the PA has no view mapping, and thus the process returns the EMS address. 


% \noindent \textbf{VAT Workflows. }
% We adopt an independent SRAM bank for each node to keep conflict-free parallel querying. 
% % The address segments that belong to one node could point to the chunk, but the segments belong to different nodes must be isolated. 

The key insight of this cuckoo hash table is to trade the low look-up latency with the cost of insertion, since queries are introduced by a processor's read, which typically lies on the critical path, but an insertion is issued by a dirty cacheline eviction, whose latency does not effect the performance much due to existing cache optimization methods such as write buffer and non-blocking cache design. 
As Figure~\ref{fig:vms}~(b) shows, the cuckoo hash table maps one key to multiple possible hashing locations with different hash functions. The element is stored in at most one of these locations at a time, but it can move between its hashing locations. A look-up in cuckoo hashing checks all the possible hashing locations of an element, and succeeds if it is found in either of them. Insertion in cuckoo hashing places an element in one of its two possible entries. If the selected entry is occupied, the algorithm kicks out the current occupant, and re-inserts to the other hashing location. The process keeps until no occupant is evicted, or meets an number of maximum tries. 


% \noindent \textbf{VAT Resizing. }
A well-known problem with the cuckoo hashing is the insertion failure. As occupancy increases, the insertion attempts could be large. Hence, it requires the VAT resizing, which requires costly and complex logic. As a consequence, we move it to the software runtime to keep hardware clean (Sec.~\ref{sec:implementation}). 
Fortunately, the VAT rehashing is quite rare because the view occupancy is typically low due to two reasons. First, a cacheline is only inserted to the view when it is both dirty and evicted from the entire host's cache hierarchy. Second, a view content is removed when a HDM read hits the view, or the writer transaction ends (Sec.~\ref{subsec:cold_path}). As the evaluation in the real-world memory traces (Sec.~\ref{}), the number of chunks is always under xxx. 


% But differently, there's no explicit view allocation or reclamation process, instead, they are both implicitly triggered by common HDM reads and writes. A view is created when the EP receives a dirty cacheline write-back request, and the veiw is removed when a memory read hits it. 
% Moreover, keeping the same granularity with memory access simplifies address translation pipeline (Sec.~\ref{asdf}). 
% The VMS keeps the view shims with node wise hash maps. The hash map is indexed with the EMS address, and the on-device hash function maps the EMS address to the VMS address. 


As we discussed in Sec.~\ref{}, the VMS hash map keeps the overflowed dirty views, so that each dirty cacheline written back would issue the VMS insertion. 
According to the view primitive, any memory reads should check the VMS to see if there exists a view mapping, in addition to directly returns with the EMS content. 
Querying the VMS could be expensive when hash conflicts occur, hence we architecturally define a view filter (VF) that stays before every VMS hash map to sift the traffic. The VF is an on-chip bloom filter that tracks the content of each VMS.
A read should query the VF first, if it returns the positive, then the read could query the VMS to get the view. Otherwise, the read request falls back to the normal EMS access. The VF may incur false-positive error that reports positive for a non-exist entry, but it will not result in correctness problem. If the VF hit but the VMS misses, the read still falls bath EMS accesses. 

The usage of hash map incurs the challenge of hash collisions that could overflow a bucket. A typical hash-based data structure relies on conflict chaining or open addressing to solve the overflows, but both of them require the query to make unbounded number of DRAM accesses or costly pointer chasing. In order to bound VMS query to a constant number (typically one) of DRAM accesses, we adopt the multi-copy hashing to resolve the collision at the insertion time. 

\noindent \textbf{VMS Management (Principle 2)}. 

including failed insertion and hash resizing. When an insertion cannot find the proper victim after a specific number of tries, 

% A miss in the ST indicates a cacheline not exist in the queried status set, and a hit indicates that the cacheline probably exists. We maintain an acceptable false-positive rate of ST through dynamic sizing.
% When a read misses all processor's cache hierarchy and is delivered to the memory pool, 


and 2) the view table (VT). Each view table entry (VTE) holds the mapping between a physical address from a specific node to an address of view shim frame. 
Similar to the traditional virtual memory abstraction, each host is given an impression that is working on a private, continuous section of HDM memory. But physically, the host-managed HDM could be redirected to a view shim, or be changed by other hosts at some data-race free points (e.g. transaction commitment). 
In common memory accesses, the VMS is invisible to both applications' processes and operating systems. The creation, querying, and deletion of VTE is automatically managed by the \name~hardware, as we detailed in the next section. 


\noindent \textbf{Memory access flow. } 
The memory access flow serves memory reads and writes with a private view for each node. \name~adopts a copy-on-write resource management approach by architecturally defining two data structures: view address transactor (VAT) and the state tracker (ST). 
Resembling the page tables in OS, VAT keeps the address mapping between the virtual view address space and the physical DRAM address space. 
VAT only tracks the cachelines that are written back dirtily to reduce the space cost. 
To be specific, when a RC incurs a dirty writeback, the VAT will redirect this write to the node's view space (Sec.~\cite{detail}) by allocating a free slot. 
Then it records the redirected address of the slot to serve future reads. Consistently, when a RC issues a cacheline read, the VAT will check whether any previous writeback on the same cacheline exists. If so, the VAT redirects the read to view space, otherwise the VAT returns with the shared object of the consistent snapshot. 
% The are caused by processor's explicit cache flush or the application oblivious cache eviction.
% It is managed by an hardware VAT walker, whose key operations includes inserting an entry on memory writes, querying the existence with an address, and deleting the entry when receiving an cache invalidation from the cache reclaimer. 

The VAT querying could be expensive when the VAT is large, hence we introduce the ST to keep an abstract to reduce VAT visits. 
The ST tracks two datasets: the evicted dirty cachelines kept in VAT and the cachelines resides at the host caches. We base ST on the customized bloom filters (Sec.~\ref{detail}), so that it trades the high query performance for potential false-positive errors. A miss in the ST indicates a cacheline not exist in the queried status set, and a hit indicates that the cacheline probably exists. We introduce a dynamic sizing approach to keep the false-positive ratio within an acceptable boundary. 
\fi

% The view memory service manages the address mapping between physical address and the view shim. 
% This flow translates a memory accesses with physical address to either a view shim access or a shared object access.
% As Figure~\ref{fig:overview}~(c) illustrates, we architecturally define two data structures, the View Address Table (VAT) and the View Filter (VF). 

% The memory access flow serves common memory operations, ensuring private memory views for each node through a copy-on-write mechanism. As Figure~\ref{fig:overview}~(c) shows, the memory access flow architecturally defines two data structures: the View Address Transactor (VAT) and the State Tracker (ST), analogous to operating system page tables. The VAT maps a physical address to either a view shim or a main memory location. And the ST abstracts the VAT to minimize its visits. The ST tracks two datasets: the evicted dirty cachelines kept in VAT and the cachelines resides at the host caches. 

% For each cache write-back, the VAT creates a view shim if the cacheline is dirty, and for each memory read, the VAT redirects the address to view shim if it exists. 
% Upon a dirty write-back, the VAT redirects this write to the node's view address space, and records the redirected address to serve subsequent reads. 





% , tracking only dirty written-back cachelines to reduce the space cost. 

% The commitment flow facilitates the global commit process offloading to hardware. It serializes a transaction's write set in a batched manner. \name~architecturally defines a queue pair to communicate with the host: a work queue (WQ) holds the write set addresses that committed by the transactions, and a completion queue (CQ) notifies the host processor of the completion of writes initialized in the WQ. The center data structure of the commitment flow is the device coherence manager (DCOH) that resolves the coherence for memory writes. The DCOH conducts two key operations: it generates and resolves invalidation requests on specific cachelines to/from other nodes' views, and writes back the new data from its owned view to the consistent snapshot. 


% \noindent \textbf{Commitment Flow. } 
% The view contents are synchronized with the GSync primitives to other nodes. Different with the L-Ld and L-St, \name~implements the GSync as an asynchronous task offloading off the normal memory accessing datapath. \name~provides the similar application-level semantics to the task offloading implemented by RDMA drivers. As Figure~\ref{fig:} illustrates, the EP maintains an unidirectional task offloading working queue (WQ) for each host.

\begin{figure}[t]
%   \centering
  \includegraphics[width=0.47\textwidth]{my/figs/commit-flow.pdf}
  \caption{Synchronization datapath for GSync and Wd.}
  \label{fig:cmt}
\end{figure}


\subsection{View Synchronization Flow}  \label{subsec:vsf}

% GSync and Wd require the new data path in addition to L-Ld and L-St. Because 

% In previous weak consistent memory architectures~\cite{GPU, Arm, xx}, the cores adopt cacheline flushing operation as the implementation of GSync. However, in CXL memory accessing sub-protocol (i.e. \cxlmem), the cacheline flushing and cacheline writeback are both transfromed to the dirty cacheline write back requests. As a consequence, the EP could not distinguish GSync from standard memory accesses. 
% In addtion, 
% An WQ element (WQE) contains the object address that should be synchronized to the WQ. 
% Upon executing the Gsync primitive, the host creates an WQ element (WQE) containing the physical addresses that should be synchronized and append the WQ. 

% This flow implements the GSync and Wd primitives. It manipulates the views created by L-St achieve coherence across all nodes. 

\subsubsection{Task Offloading Scheme. }

Different with the view address translation flow that works ``on the path'' of memory accesses, the GSync and Wd primitives are implemented in task offloading scheme. 
The endpoint maintains a queue pair for each host: a work queue and a completion queue. The host posts the addresses for GSync and Wd in the work queue element and polls the completion queue to synchronize for completion. The work queue is a circular buffer, while the completion queue is a bit vector where each bit maps to a work queue element.  
Both queues are located in the shared memory between hosts and devices and are accessed coherently utilizing the \cxlcache~sub-protocol. 
Inspired by the CC-NIC~\cite{ccnic_asplos24}, we apply an inline signal scheme to ensure only one coherent read is needed to retrieve the entire work queue element. Instead of signaling work queue changes with an explicit flag, we embed a valid bit into each work queue element. The endpoint polls the header element's valid bit and reads the payload once it observes that the valid bit is set. To further reduce coherence traffic, we limit the size of completion queue to a cacheline to enable one \cxlcache~read to get all pending completion queue elements. 

% Since the valid bit and the payload locates on the same cacheline, it takes only one cache coherent read to get both. 
% 

% 
% Similarly, For signaling task completion, the host polls on the CQE corresponding to its previously issued WQE, and gets the completion signal in one memory access. 
% This simple approach ensures only two \cxlcache~coherence roundtrips introduced by each primitive task offloading, and eliminates the need of memory copies as previous task offloading architectures~\cite{nic, rdma, gpu, etc.}. 

% in the WQE, implemented as a flag indicating whether the WQE is ready for consumption, having been finished, or free. For task offloading, the EP polls on the next WQE's valid bit of the queue tail in the ring, and retrieves the flag and the payload in one access. For signaling task completion, the host processor polls on the completion bit of its previously issued WQE. 
% This approach eliminates the need for software-based signaling via head and tail index registers. 
% The polling would not cause flooding of CXL coherence requests. Because the the EP keeps reading the WQE's payload while the processor is polling the completion bit, the cacheline 
% If we store them in the same cacheline, each EP's reads should send a 


% A CQ is an object which contains the completed work requests which were posted to the WQ. Every completion says that a specific WQE was completed so that the corresponding view contents are globally observed. 

\subsubsection{Achieving Coherence Across Nodes. }

Synchronizing a dirty view involves two phases: invalidating the cachelines on remote nodes and merging the dirty view with the EMS.  
As Figure~\ref{fig:cmt} shows, we refer to the CTHW executing the GSync as the \textit{requester} and other CTHWs as the \textit{peers}. 
During remote cache invalidation, the \textit{requester} broadcasts an invalidation message on the coherence snoop bus. 
Each \textit{peer} checks its own VF and VBF to determine if it holds the corresponding cacheline in either on-chip or overflowed status. If the view resides in the VMS, the \textit{peer} simply invalidates the VAT entry to remove it. If it resides on-chip, the \textit{peer} invalidates the host processor's cacheline by issuing a standard \cxlbi~invalidation request~\cite{cxl-doc, cxl-shortdoc}. The CTHW ignores the written-back value since it's outdated. 

The content merging phase encounters two scenarios: the modified cacheline either remains in the host's cache or has already overflowed to the VMS. The \textit{requester} checks its VF and VBF to determine the status. If the cacheline is on-chip, the \textit{requester} sends a \cxlbi~invalidation request to the host to force a cacheline eviction, then writes the evicted data to the EMS. Otherwise, the \textit{requester} performs a memory copy from the VMS to the EMS and invalidates the VATE.


% A filter hit indicates the view staying on-chip, so that the \textit{requester} sends \cxlbi~request to host to change the on-chip view to overflowed, then copies it from the VMS to EMS. Otherwise, the view is overflowed hence the \textit{requester} only does the memory copy. 


% Query the snoop filters. If hit on the VMS filter, means the cacheline is overflowed, so that it removes it from the VAT. 
% If hit on the VMS back-filter, it means the cacheline states in host processor's caches, so that the CTHW issues a \cxlbi~request to invalidate the cacheline. We adopt the Rd0 request to discard its content to reduce RC-EP traffic. 

% Content copy is done by the requesting CTHW. It queries the filter pair in parallel. If the view stays on the processor, the CTHW issues a \cxlbi~read-exclusive to retrieve the cacheline content and invalidate the cacheline simultaneously. If the view stays at the VMS, the CTHW will query the VAT for VMS address, and copy it to the EMS. 

% As Figure~\ref{fig:overview}~(c) shows, each node owns a hardware view agent at the EP side. 
% The view agents are connected with a bus pair that carries the cache controlling requests and responses respectively. The view agents work on a snoop-based invalidation protocol. As Figure~\ref{fig:cmt} illustrates, after a view agent gets a GSync WQE, it posts the addresses on the requesting bus. The view agents on other nodes would check its VMS filter and the VMS back filter to see if it holds the corresponding view in either the on-chip or overflowed status. 
% If the view is overflowed, the view agent simply clears the corresponding VATE's valid bit, but if it is on-chip, the view agent should invalidate the host processors' cacheline via a \cxlbi~request. 
% After finishing the invalidation, the peer view agents response to the requesting view agent. 

\ifx\stale\undefined
\noindent \textbf{Discussion and Optimization. }
% The EP implements each node with a hardware view agent to back invalidate views staying on the processor's cache. 
% The view agents exchanges invalidation requests and responses via an inner-EP bus to minimize the latency.
The synchronization flow works on a bus-based snooping protocol, which may incur scalability issues due to cores scrambling for the bus. However, this would rarely bottleneck our design due to two reasons. 
First, the bus only transmits the addresses to invalidate but avoid cacheline data. This approach greatly reduces the bus bandwidth requirements. 
Second, the invalidation requests are only introduced by the software-introduced GSync primitives, and avoids the unnecessary coherence traffics incurred by pessimistic coherence model (discussed in Sec.~\ref{subsec:overkill}). 
For a the larger cluster that achieves the higher system throughput, we could further improve the bus throughput by limiting the coherence scale, adopting multiple buses that serve different address fields, but we leave it to future works. 
\fi

% The coherence requests are thus minimal. It avoids the overly pessimistic coherence requests as we discussed in Sec.~\ref{sec:remote_fetching}, thus reduces the coherence traffic when compared with CXL-vanilla. 
% By carefully designing the hardware pipeline, the bus achieves xxx cacheline/s. This is far beyond the transaction system's requirements, which lie between xxx and xxx with a rack scale. 

% Restricting coherence scale may help to further reduce the coherence traffic, but it requires application to changes their algorithm thus violates our \textbf{principle 2}. 

% Our invalidation scheme share some similarities with conventional MESI snooping protocol on the architectural design. Despite exhibit great architectural simplicity, t
% Difference with Snooping Based MESI: Read and Write would not cause CC traffic, but only GSync. 


% The commitment flow offloads the global commit process to hardware, serializing transaction write sets in a batched manner. As Figure~\ref{fig:overview}~(c) shows, the commitment flow defines a pair of queues for host communication: the Work Queue (WQ) and the Completion Queue (CQ). 

% An WQ element holds the write set addresses that committed by a transaction, and an CQ element notifies the host processor of the completion of a transaction's writes initialized in the WQ. The Device Coherence Manager (DCOH) is central to this flow, managing memory write coherence by generating and resolving invalidation requests and updating data across nodes' views. 
% All DCOHs are connected with a coherence snooping bus to carry invalidation requests in order. The snooping bus only transmits the cache ownership signals but not data, since  
% Coherence is maintained through a snooping bus, which transmits cache ownership signals without data transfer.v

% It consists of a request channel and a response channel. 



% The commit queue is shared between the device and the host using \cxlcache~protocol in order to be coherent. 
% The memory access flow starts with the device receiving a memory access from the RC, after CXL IP unpacketizes the packet, it delivers the 

% The WT is the filter to track the host's memory writes, which works to reduce the visiting towards VAT. 

% It keeps an address map between the virtual memory address and the actual address of the valid copy. 
% The path includes the \cxlmem~port ip to implement the CXL protocol and communicate with RC, the device coherency agent (DCOH) to track the host working set, and the memory address transaction service to route the reads and writes to proper DRAM location. 
% Memory writes are reflected by the dirty
% CXL adopts the write-back cache management. 


% It creates an illusion for the node that it's the only node accessing on the memory pool. 
% This idea is similar to the OS's virtual memory system, but we face fundamentally different design objectives and implementation challenges. 


\ifx\stale\undefined

% CXL's cache coherence capability, despite significantly reduces programming complexity, may lead system throughput degradation when obliviously used. 
To address aforementioned problems, we propose \name, a hardware-software co-designed memory shim layer between CXL memory and user level services, as Figure~\ref{fig:overview} shows.
\name~takes care of the coherence issue as a replacement of hardware but preserving hardware-comparable performance. 
\name~reduces the costly cross-node coherence and SF evictions. 
% minimizes the cross-node cache coherence process from the transaction critical path, as well as shrinks the capacity of working set that requires cache coherence in order to reduce probability of the SF conflicts. 
\name~introduces no modifications to neither processor architectures nor CXL protocols to be practical, and it also keeps compatibility with common in-memory database services such as indexing, logging, and garbage collection. 

\fi




\begin{figure}[t]
  \centering
   \includegraphics[width=0.43\textwidth]{my/figs/address_mapping.pdf}
  \caption{The \name's memory mapping to OS. }
  \label{fig:runtime} 
\end{figure}



\ifx\stale\undefined
This work focuses on the transactional distributed key-value stores that partition the records with primary key, as Figure~\ref{fig:bg-kvstore} shows.
The KVS preserves the strict serializable consistency for user transactions. 
This work optimizes for the online transaction processing (OLTP) applications with the main concern on optimizing system throughput as well as processing latency. In practice, OLTP transactions are (1) short-lived, (2) access a small number of records at a time, and (3) are repeatedly executed with different input parameters~\cite{dbx1000_dist_vldb17}. 

CXL-based memory pool extends the shared-address system beyond a physical node. 
Despite the lack of practice, it is easy to facilitate a single-node concurrency control to execute on a cluster by allocating the tuples on the CXL shared memory. 
Nevertheless, simply deploying existing approaches is not yet an appropriate option due to CXL's high coherence latency and the drawbacks of centralized SF architecture. 

Specifically, during the PCIe enumeration, the host driver would query the size of EP's internal memory and allocate the physical address fragment for it. The base address of such fragment is then told to the EP via the \cxlio's base address register (BAR) mechanism. 

\fi

\subsection{Operating System Integration}       \label{subsec:system_integration}

\ifx\undefined\stale
The CXL-backed memory can be exposed as a NUMA memory node or as a direct-accessed file. Carefully selecting the memory mode is crucial for \name's functionality. Pond~\cite{pond}, TPP~\cite{tpp_asplos23}, and the Linux kernel expose the HDM as a zero-core virtual NUMA (zNUMA) node, i.e., a node with memory but no cores. This approach manages memory with existing NUMA-aware memory management provided by OS (numactl in Linux). It imposes no need to change the application codes, however, only able to control the memory allocation at the application granularity. The current OS only support specifying the NUMA balancing strategies, but can not control which memory node that a specific data structure locates on. 
\fi

% User threads can leverage the base address returned by \textit{mmap} and the relative bias to specify the address of CXL objects. 
% POSIX file APIs, such as \textit{mmap} and \textit{ioctl}. 
% Addresses are calculated by the bias 
% space using POSIX file APIs, such as \textit{mmap} and \textit{ioctl}. 

% with explicit \textit{mmap} to allocate memory on HDM. 

Resembling the Persistent Memory Development Toolkit (PMDK)~\cite{pmdk}, \name~adopts the Direct Access (DAX) mechanism to manage the shared memory~\cite{directcxl, cxl_anns_atc23, partial_sosp23}. We expose the device as a DAX file and allow the application to \textit{mmap} the G-FAM to its virtual address space. We do not adopt previous approaches of exposing the G-FAM as a zero-core virtual NUMA node~\cite{pond, tpp_asplos23}, i.e., a node with memory but no cores, since it is costly to use current NUMA control approaches to precisely specify the memory node at allocation. 

In this work, we set all G-FAM pages as ``reserved'' in the BIOS to prevent OS intervention, such as page swapping. This ensures that RTLib has full control to allocate the entire G-FAM to a contiguous virtual address segment for applications. 
The combination of DAX mode and page reservation ensures effective address translation among the virtual address space that CTLib operates on, the physical address space that the on-chip cache operates on, and the endpoint's DRAM address space that CTHW operates on, as it maintains a linear address mapping between these spaces. 

% It ensures the effective address of GSync and Wd primitive since they need CTHW to transform a virtual address passed from CTLib to the corresponding internal DRAM address, as well as the physical addresses on other nodes. 
% Being linearly mapped, such a transformation could be achieved by simply adding or subtracting the biases in a few cycles. 

% By keeping the linear mapping between these three spaces, the CTLib could transform the 
% working on the virtual address space, etween these three spaces can

% Note that the GSync and Wd index the data to synchronize with virtual addresses, but the hardware works at the internal DRAM address space. By keeping the linear mapping between the HDM and user-level virtual addresses, the bias of a virtual address relative to the segment's base address is the same with the bias of the data in DRAM address space. As a consequence, the EP could calculate the DRAM address from virtual address bias in GSync and Wd, disregarding different physical address frames that we reserve. 
% Moreover, the mapping between HDM and reserved physical address frame is also linear, so that the EP can recover the physical address from the HDM bias by adding it with physical base address of corresponding nodes. This enables the EP to correctly translate the virtual address to the physical addresses on other nodes. 

% we could use the bias of virtual address relative to the base address to specify teh 
% directly address an HDM location with the virtual address bias relative to the base address, disregarding different physical address frames that the OS allocates. Moreover, the bias for an object in the physical address space is also the same with the bias of HDM address space. As a consequence, the EP can recover the physical address from the HDM bias by adding it with the CXL IP managed node's physical base address. 
% The application hence indexes a shared memory objects with the bias and the base address pointer of the segment. 

\ifx\undefined\stale
The DAX mode takes full control of each allocation to the proper memory node. During the PCIe enumeration, the \name~driver queries the size of EP's internal memory and allocate a continuous physical address fragment to map it. The base address of such fragment is then told to the EP via the \cxlio's base address register (BAR) that resembles PCIe devices. 
We set all HDM pages as ``reserved'' at BIOS in order to prevent OS intervention such as page swapping out. To this end, \name~enables user-level runtime to take full control of the memory management by not changing the OS. 
The \name~library allocates entire HDM to a continuous virtual address segment at the application process initialization. The library addresses the shared memory objects with the bias and the base address pointer of the segment. 
\fi
% , and turn the right for allocation/deallocation entirely to the runtime. 
% We keep the bias of an EMS object's DRAM address to be the same with the bias between the corresponding virtual addresses. To keep invariant 2, the runtime allocate the entire HDM to a continuous space of virtual address space at the beginning, and turn the right for allocation/deallocation entirely to the runtime. 

% The payload contains the base addresses of the cacheines managed by the primitives. They are in the virtual address space so that it needs careful address mapping approach for the EP to find out the physical address on other nodes to invalidate (Sec.~\ref{subsubsec:system_integration}). 




\subsection{Memory-Mapped Primitive Selection}  \label{subsec:memory_mapping}

% Due to \textbf{P1}, we could not embed the primitive choice within the CXL request packet, since it could involve host RC modification. 
% \name~enables software to choose which primitive it wants to bind for each data structure dynamically at the runtime (\textbf{P2}). 

% implemented by \cxlcache~to enable efficient data exchanging between CTLib and CTHW.  
% manage the CTHW working data structures such as VAT, queue pairs, and VMS contents. 
% CTLib adopts an PMDK-based allocator to implement memory management calls, e.g. \textit{alloc} and \textit{free}.
% CTLib provides memory allocation and reclamation APIs, i.e. \textit{cxl\_alloc} and \textit{cxl\_free}, resembling traditional multi-threaded programming. 
% System developers bind the primitive at each allocation with API's arguments. 


To enable multi-primitive selection, we expand the concept of memory-mapped I/O. 
As Figure~\ref{fig:runtime} illustrates, the \name~driver exposes the G-FAM to a physical address region with three times larger capacity. Each segment maps to a different primitive, including the CXL-vanilla primitive with hardware-managed strict coherence, the \name~primitive with decoupled coherence, and 
\rvs{
a special hardware configuration primitive for internal data structures of \name, such as VAT's hash tables, VMS contents, and work/complete queues. 
Applications could specify the CXL-vanilla primitive or the \name~primitive via CTLib's APIs, while the hardware configuration segment is reserved for \name~internal usage so that it is transparent to users. 
To avoid overlap between the application data and \name~data, we adopt a virtual address management strategy where the application address segment grows from the bottom base address upward, while the hardware configuration segment expands from the top address downward. When two segments overlap, i.e. no unused memory exists, an out-of-memory error occurs. 
}
\rvs{
% We give the full freedom to users to manage application address segment since typical OLTP frameworks prefer customized data allocators.
% On the other hand, CTRts are required to manage the hardware configuration segments. To be specific, CTRt does memory allocation and reclamation with the granularity of fixed-sized chunks. CTRt stores the previously freed chunks in a doubly linked list. Memory allocation would first check this free chunk list using the first-fit strategy, then expands the segment if no sufficient chunks. Advanced memory management policies such as segregated free lists could improve the performance, but exploiting them is beyond the scope of this paper hence we leave them to future works.

We allow transaction processing systems to manage application address segments with customized allocators but allocation should be aligned with cacheline to match \cxl~protocol granularity. Conversely, CTRts are required to manage the hardware configuration segments. CTRt handles memory allocation and reclamation in fixed-sized chunks. Previously freed chunks are stored in a doubly linked list. Allocating a chunk in the hardware configuration segment first searches this list using a first-fit strategy. If no adequate chunks are available, CTRt expands the hardware configuration segment to satisfy this allocation. While advanced memory management strategies such as segregated free lists could enhance performance, delving into these methods is beyond the scope of this paper, thus we defer their exploration to future works.

% A CTRt would create or reclaim the chunks at VAT resizing, which may adjust the size of the configuration segment. 
% but a simple strategy suffices in our current use cases, where the VMS expansion is infrequent. 
% we do not allow cross-node data sharing in the hardware configuration space to avoid the \cxlcache~channel deadlock, so that we reserve disjoint hardware configuration address spaces for different nodes.

% Each chunk contains a 1MB hash table and a 4MB view buffer managed by the hash table. 
% This enables CTHW to easily know a load/store's primitive by checking which segment the address belongs to, without the need of  costly address mapping data structures like page tables. 
% Second, the CTHW could translate the virtual address specified by GSync and Wd to the physical addresses on other nodes, by first getting the relative virtual address bias then add it to the physical base addresses. This ensures the CTHW to back-invalidate on-chip view contents. 
% CTLib maintains the boundaries of these segments and does checking on every allocation.
}


% CTRt dynamically adjusts the size of the configuration segment as needed when VMS resizing occurs. 
% lies at the hardware configuration primitive is managed in 5MB chunks, with each chunk containing a 1MB hash table and a 4MB view buffer managed by the hash table. CTRt dynamically adjusts the size of the configuration segment as needed when VMS resizing occurs. Free chunks are stored in a doubly linked list and are matched for expansion using the first-fit strategy. Advanced policies such as segregated free lists could improve expansion performance, but a simple strategy suffices in our current use cases, where the VMS expansion is infrequent. 

% We co-locate the VAT's hash map and its managed corresponding VMS
% To avoid the complex ordering issues across primitives, CTLib ensures that each virtual address is bounded to at most one primitive. 
% In addition, we do not allow cross-node data sharing in the hardware configuration space to avoid the \cxlcache~channel deadlock, so that we reserve disjoint hardware configuration address spaces for different nodes. 


% since the configuration space is typically small. 
% CTLib do not address conflicts on application data structure allocation since it should be harmonized by the applications. By design, the CTLib on different nodes should communicate with each other to avoid conflicts on allocation, but our current implementation leave the duty to application developers. 



% Applications could only bound data structures to the CXL-vanilla primitive and \name~primitive, while the configuration primitive is reserved for \name~runtime to control the hardware. 
% The application data structures expands from the bottom address to up, while the hardware configuration space expands from the top address to down. The hardware configuration space accommodates the queue pairs for each node, the VAT, the VMS contents, and some device management data structures such as VAT occupancy monitor. The configuration primitive adopts the normal \cxlcache~sub-protocol to enable efficient host-device communication. We do not allow cross-node data sharing in this space to avoid the \cxlcache-introduced deadlock. 

\ifx\stale\undefined
The primitive selection is done at data allocations. 
CTLib provides memory allocation and reclamation APIs, i.e. \textit{cxl\_alloc} and \textit{cxl\_free}, being similar to traditional multi-threaded programming but require the primitive argument that following operations will obey. 
% Second, processes running on different nodes require explicit synchronization calls to get coherent if the shared data obeys \name~primitive. 
% An application process can perform a set of virtual memory operations to , including \textit{cxl\_alloc} and \textit{cxl\_free}, atomic operations (e.g. \textit{CAS, FA}), and normal memory loads and stores. Note that the effects of these memory operations depend on the targeting address' assigned primitives. For example, an atomic CAS serializes processes on all nodes on the lock allocated with CXL-vanilla primitive, while only serializes a single node processes on the lock allocated with \name~primitive. 
When the EP receives the HDM access, it chooses the primitive implementation based on which segment the physical address belongs to. 
In order to avoid the complex ordering issues across primitives, the CTLib allows each virtual address is bounded at most one primitive at a time for a process. 
By design, the CTLib on different nodes should communicate with each other to avoid conflict on hardware configuration space allocation. Our current implementation statically partition the segments for each node since the configuration space is typically small. CTLib do not address conflicts on application data structure allocation since it should be harmonized by the applications. 
\fi
% \noindent \textbf{Advantages. }


% \name~software allocates the object on a memory segment incurs following accesses to it obeying the corresponding primitive.  
% location is mapped every segments concurrently. Each segment is bounded to a primitive choice. Allocating the object on a memory segment incurs following accesses to it obeying the corresponding primitive.  
% As Figure~\ref{fig:runtime}, memory accesses to the upper address segment follow the vanilla-CXL primitives with strict coherence model, while the accesses to the lower address segment follows the \name~primitive with the loose coherence model. The EP instantiates the isolated datapaths for the two primitives, and chooses the path based on the address delivered from the CXL IP. 
% The application could specify the primitive by providing the API's argument. \name~does not preserve any ordering or coherence guarantees across segments, despite they may point to the same object. 

% It's consequently require the software runtime to harmonize accesses to preserve software coherence.

% The exposed memory store starts at the lowest DRAM address and grows up when the applications allocates memory with \textit{mmap}. The task offloading queues take up the largest 4KB address segment and the VMS starts from the following DRAM address and grows down. 

% \subsection{Software Interface}
% As Figure~\ref{fig:API} illustrates, applications should specify the requiring primitive through the argument during allocation, and the \name~library would redirect the data structure to the corresponding physical address segment. 

% The \name-managed shared memory occupies a continuous segment of each process's virtual address space. Programming in this segment is similar to traditional multi-threaded programming but has  two differences. First, each allocation on this segment should specify a primitive that following operations will obey. Second, processes running on different nodes require explicit synchronization calls to get coherent if the shared data obeys \name~primitive. Figure~\ref{fig:} illustrates the usage of \name~with a simple example. 

% An application process can perform a set of virtual memory operations to this segment, including \textit{cxl\_alloc} and \textit{cxl\_free}, atomic operations (e.g. \textit{CAS, FA}), and normal memory loads and stores. Note that the effects of these memory operations depend on the targeting address' assigned primitives. For example, an atomic CAS serializes processes on all nodes on the lock allocated with CXL-vanilla primitive, while only serializes a single node processes on the lock allocated with \name~primitive. 
% We restrict the allocation in cacheline granularity since the hardware accessing memory are aligned with the cacheline, while the accesses are in byte granularity.

%  are  servers equipped with a standardized CXL RC IP and connected to a 


\subsection{VAT Resizing} \label{subsec:vat_resizing}

% A key responsibility of \name~runtime involves managing the VAT resizing process. Two parameters control the process are the \textit{Resizing Threshold ($r$)} and the \textit{Expansion Factor ($k$)}, and the \textit{Monitor Interval $I$ms}. At each $I$, the runtime reads the VAT's status bits via \cxlcache. When the occupation of the a VAT cuckoo hash table $c_{old}$ reaches $r$, the runtime issues a hash table resizing to a new hash table with equivalent capacity that is $k$ times bigger than the $c_{old}$.We select the $r$ resulting in few cuckoo insertion collisions and negligible number of insertion failures. As shown in Figure~\ref{fig:cuckoo}, for the 2-ary table, a proper $r$ is 0.6 or less. We select $k$ that is neither too large since it may waste the memory nor too small that it may causes continuous resizing. An experienced value is $k > (r + 1)/r$, hence for $r=0.5$, $k > 3$ is good. However, for the best hardware simplicity, we choose $k=4$ which equals to a power of two. 


% There are two approaches to resize the cuckoo hash table $c_{old}$: scaling out the number of hash tables map the keys to $c_{old}$, and scaling up the size of  $c_{old}$. Recall that the chunk base LUT maps a set of address prefixes $s_1, s_2, ..., s_n$ to the $c_{old}$. Scaling out the hash table number involves allocating $k-1$ new cuckoo hash tables $c_{new_1}, c_{new_2}, ... c_{new_{k-1}}$, and map the addresses equally to them. 
% The runtime scans the $c_{old}$ and checks the VATEs' tags, then moves the VATE with prefix $s_i$ to $c_{new_{i\%k}}$. 
% We avoid rehashing the VATEs to the new hash table by sharing the hash functions among all hash tables, so that a key has the same location in $c_{old}$ and $c_{new}$. 
% The scaling out approach has an upper bound limit that each node owns up to $2^8$ different prefixes, enabling 256 hash tables. To gain larger VAT memory space, we need to scale up the hash table size, which requires allocating a $k$ larger cuckoo hash table to replacing $c_{old}$. It needs us to retrieve all VATEs from the $c_{old}$ and inserts them to the new hash table one-by-one by rehashing the key. 
% % The hash functions are fixed at hardware, which map the key to a 38 bit integer. The index is calculated by cutting off it. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.48\textwidth]{my/figs/vat-resizing.pdf}
%   \caption{VAT resizing approaches. }
%   \label{fig:vat-resizing}
% \end{figure}


The VAT resizing mechanism adapts the concept of page swapping deamon of virtual memory subsystems. To be specific, we introduce a helper thread CTRt to monitor the VAT's occupancy and conducting VAT resizing. At every $I$ milliseconds, the CTRt checks the occupation ratio of each hash table. The ratio is managed by the CTHW at hardware and is exposed through the hardware configuration address space. We introduce VAT expansion in this section, and VAT shrinking works in the reverse manner. If a table $t_{old}$ meets the occupancy threshold $r$, the runtime initiates a VAT expansion procedure to increase the $t_{old}$ to a table with $k\times$ equivalent capacity. The CTRt increases the number of hash tables to accommodate $t_{old}$'s VATEs. CTRt creates $k-1$ additional cuckoo hash tables with the same hash functions. CTRt scans the table base address LUT and moves the $t_{old}$'s VATEs with the prefix $s_i$ to the new hash table $t_{new_{i\%k}}$. The table bias of the VATE in $t_{new_{i\%k}}$ is the same to the $t_{old}$ thus avoiding the rehashing cost. After migration, CTRt updates the on-chip LUT to point the prefix to newly created hash tables. If the LUT is full that each $s_i$ points to an unique table, CTRt falls back to the rehashing. 

\rvs{To prevent data races between CTRt and worker threads, CTRt temporarily blocks any access to the hash table that is manipulated by the VAT resizing process. This involves negligible impact on overall performance, as VAT resizing occurs infrequently. The reasons are twofold. First, the VAT occupancy capacity is determined by the number of \textit{pending writes} from uncommitted transactions. Given that OLTP transactions are typically read-intensive and short-lived~\cite{snow_osdi16, port_osdi20}, the volume of pending writes is inherently limited in OLTP workloads. Second, the large on-chip caches is the primary location of pending writes, leaving VAT to manage only a minor fraction of the pending write set.
}


% Despite this process may introduce performance effects, VAT resizing frequency is very low during our evaluation (Section~\ref{subsec:scalability_study}). T
% A hash map could support about 5MB pending writes for a node. The 
% For OLTP applications, the temporal variation in read/write footprint is not severe. 
% To avoid data racing, we block the accesses to the VAT under resizing via hardware latches. 
% The resizing overhead is limited 
% Reason one: most accesses hit the cache, rather than spilled to the VMS, so the effects of changes are filtered by the cache
% Reason two: For OLTP applications, the temporal variation in read/write footprint is not severe. [We need supports] 
% A hash map could support about 5MB pending writes for a node. The algorithm that reduces the timing interval of pending writes, i.e. OCC and SILO, could more benefits from \name~compared with 2PL variants. We support it at Fig.~\ref{fig:record_sensitivity} and Fig.~\ref{fig:latency_sweep}. 
% since  very rare since VMS is sparse at most time and a moderate size for only one table is sufficient at most cases. 
% By maintaining a low $r = 0.6$ or lower, insertion failures are very rare. 
% When it occurs, the CTHW halts the corresponding node's pipeline, sets an error bit, and waits for CTRt to resolve it.

% In addition, algorithm that reduces the timing interval of pending writes, i.e. OCC and SILO, could more benefits from \name~compared with 2PL variants. We support it at Fig.~\ref{fig:record_sensitivity} and Fig.~\ref{fig:latency_sweep}. 



% $t_{new_1}, t_{new_2}, ..., t_{new_{k-1}}$. The runtime then scans $t_{old}$, checks each VATEs' prefix, and moves the VATE with the prefix $s_i$ to $t_{new_{i\%k}}$. After migration, CTRt updates the on-chip LUT to point to new hash tables. 
% Compared with the traditional cuckoo rehashng described Figure~\ref{fig:vat-resizing}~(c), our approach avoids the costly rehashing calculation. However, when each $s_i$ points to an unique table, CTRt falls back to the rehashing approach. 

% The VAT expansion has two approaches: (1) scaling out method that remaps its VATEs to additional $k-1$ hash tables, and (2) scaling up methods that allocates a new table with $k\times$ larger size. For sake of simplicity, we assume the VATEs mapped to the $t_{old}$ have prefixes $s_1, s_2, ..., s_n$. 
% $s_1, s_2, ..., s_n$
% , and distributing the VATE across these tables evenly, based on their indicator prefix. 
% As Figure~\ref{fig:vat-resizing}~(b) shows, the scaling out approach creates $k-1$ additional cuckoo hash tables $t_{new_1}, t_{new_2}, ..., t_{new_{k-1}}$. The runtime then scans $t_{old}$, checks each VATEs' prefix, and moves the VATE with the prefix $s_i$ to $t_{new_{i\%k}}$. After migration, CTRt updates the on-chip LUT to point to new hash tables. 
% As Figure~\ref{fig:vat-resizing}~(c) shows, the scaling up approach replaces the $t_{old}$ with a hash table $t_{new}$ that has $k\times$ size. This requires extracting all VATEs from $t_{old}$ and rehashing each key for insertion into $t_{new}$. Compared with the scaling out approach, the scaling up involves costly rehashing, so that it is only adopted when the LUT is full, i.e. each $s_i$ points to an unique table. 

% To mitigate potential channel dependencies between CXL channels, the runtime would pause the VMS translation pipeline and view invalidation pipeline. 

% However, it's very costly so that we increase the hash table number to enlarge VAT space by default, and only increase the table size when the prefixes are full, i.e. all prefixes point to a different chunks. 
% This method avoids emulating the hardware hash functions at software for new hash table insertion. Because all hash tables share the same hash functions so that a key mapped to a specific location in $c_{old}$ will map to the same location in $c_{new}$. The scalability of this approach is limited by the length of VMS indicator's indexing field. 



% We determine $r$ to minimize cuckoo insertion collisions and to almost eliminate insertion failures. An optimal $r$ for the 2-ary cuckoo hash table is 0.6 or lower. The factor $k$ should be large enough to prevent frequent resizing, but not so large as to waste memory. Empirically, a value of $k > (r + 1)/r$ is effective, thus for $r=0.5$, $k > 3$ is ideal. For simplicity in hardware design, we choose $k=4$ since it equals to a power of two.

% Note that the hashing functions is fixed at hardware, 
% Assuming the overwhelmed chunk $c_{old}$ 
% The resizing process first allocates a new chunk $c_{new}$ in VMS 1space, and randomly choose a segment $s_i$ to change its chunk mapping the new chunk. 


% \subsection{Memory Allocation and Reclaim}

% Memory allocation/deallocation: PMDK.
% Garbage Collection


% \subsection{Logging, Failure Recovery, Garbage Collection}


