
\section{Backgrounds}   \label{sec:background}

\ifx\undefined\stale
\subsection{Distributed Transaction Processing}    \label{subsec:kvs_primary}

% The most intuitive way to manage shared data is the relational databases management systems (RDBMS). These systems rely on the highly-structured data organized in fixed rows (records) and columns (attributes) to store the shared records.
% However, the  inherent rigidity of RDBMS data structures, coupled with SQL's limited operational flexibility, renders them suboptimal for the evolving requirements of dynamic datacenter applications. In contrast, the NoSQL systems transcends the traditional relational data scheme. These systems enable the storage of unstructured or semi-structured data, along with facilitating program-style user interaction. To meet the diverse store requirements, a variety of NoSQL systems have emerged, including key-value based, document-based, column-family based, and graph-based systems. Among these proposals, the key-value (KV) stores have garnered great adoption and community support, due to its simple data structure the superior performance for read/write operations. 


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.48\textwidth]{my/figs/bg-kvstore.pdf}
%   \caption{The basic architecture of a transactional key-value store. }
%   \label{fig:bg-kvstore}
% \end{figure}

% \noindent \textbf{Distributed Transaction Processing. } 

% Transaction is a widely adopted programming scheme to interact with highly paralleled systems, such as e-commerce, datacenter file systems, and web services. 
Transaction processing is the basic building block for modern datacenter applications, which provides a high-level abstraction that a single host executes transaction at a time and never fails. In current data center use cases, datasets are always partitioned into multiple shards, which are then distributed across multiple memory nodes. 

transactions typically span across nodes, so that they rely on networks to access records on remote nodes. 
% Transactions access remote tuples using 
% , since the dataset capacity exceed the limit of a single machine. 



\begin{figure}[t]
%   \centering
  \includegraphics[width=0.48\textwidth]{my/figs/motivation-sda-sna.pdf}
  \caption{
  (a) varies the number of partitions that each transaction accesses and draws the system overall throughput. (b) breakdowns the time ratio of the processor being idle. 
   }
  \label{fig:sna-sda}
\end{figure}


% \noindent \textbf{Low-Latency Data Sharing:} CXL enables data accesses for a 64-byte cache line with the roundtrip latency to sub-microseconds. Such a low latency, fine-grained data sharing fits distributed transactions, which make frequently byte-level synchronization and is dominated by the delay of networks. CXL benefits from two protocol advantages. First, CXL directly fetches data to the host LLC, eliminating the memory copy between NIC buffers and main memory~\cite{pond, directcxl}. Second, the processor waits for the result in a synchronous, instruction-level way that resume the processor's pipeline as soon as the instruction returns, while the communication between NICs and the processor is asynchronous, requiring either busy pooling or OS-intervened interrupts on retrieving the results. 

% \noindent \textbf{Software Transparent Cache Coherency:} CXL enables memory attached to be cacheable (referred to as ‘Host managed Device Memory’, HDM), similar to the host memory, resulting in a processor's unified physical memory space across HDM and host memory. Operating systems thus transparently exploit the HDM by reusing virtual memory mechanisms. A typical system exposes HDM as a zNUMA node~\cite{tpp_asplos23, pond, mem_tunnel}, user-level applications thus are able to map applications to HDM leveraging existing NUMA balancing mechanisms. The programmers are free to differentiate accesses to whether host memory or remote HDM in application codes. On the contrary, networks requires applications to distinguish remote data accesses with explicit API calls.


However, cross-node networks may easily bottleneck the system performance, regardless of the software policies that are adopted. 
We test deneva~\cite{dbx1000_dist_vldb17, abyss_vldb14}, a popular distributed transaction processing framework, on 8$\times$r320 instances in CloudLab~\cite{cloudlab}. 
Figure~\ref{fig:sna-sda}~(a) shows the commitment throughput on various concurrency control policies. As we vary the number of nodes that each transaction accesses from one to two, all protocols degrades over 8.4x due to the network overheads. We further breakdown the processor execution time and observe over 60\% are spent on waiting for the network to complete, as shown in Figure~\ref{fig:sna-sda}~(b). This is a common case despite a variety of works optimize the latency of networks, such as remote direct memory access~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} and smartnics~\cite{xenic_sosp21, clio_asplos22}. It's because the inherent overheads of PCIe copying, network card's asynchronous singling, and network stack processing~\cite{directcxl} required for every network transmission. 

% It's mainly because the networks significantly increase the running time of a transaction, so that the worker should wait for the older transactions to complete. 
% We chooses the 2 partition and 8 partition for simplicity. The objects are organized in a shared data manner and is manipulated by remote procedure calls (RPCs). We adopt the YCSB benchmarks with default write ratio 0.5. Each node manages one partition of the dataset.
% \fi

% Networks typically exhibit large transmission latency and high programming complexity when compared with single node systems. 
% It's because networking techniques are designed for the universal application, optimized for the throughput, and aims to support large field connection. 
% Despite a variety of works optimize the latency of networks, such as remote direct memory access~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} and smartnics~\cite{xenic_sosp21, clio_asplos22}, they can't fully compensate for the performance degradation due to the inherent overheads of PCIe copying and network stacking processing~\cite{directcxl}. 

% , co-routines~\cite{fasst, drtmh, grappa_atc15}, and batching requests~\cite{grappa_atc15, guideline_atc16}, 

% This work contributes to the concurrency control approaches in transactional key-value store (KVS) architecture, which is widely adopted in datacenter and is greatly explored by the research community, due to its simple data structure the superior performance on read/write operations. 
% % While there are various KV-store systems managing different record types, the basic storage structure is key-value pair. 
% % illustrates the basic modular components of a transactional KV-store.
% The basic KVS's data structure is the key-value pair, as shown in Figure~\ref{fig:bg-kvstore}~(a), the primary key is an unique record identifier, and the value stores all attributes of the record. 
% % The lookup relationship between a given key and the matching records is maintained by an index, typically implemented in trees or hash tables. 
% The dataset is partitioned based on the primary key, and each node is responsible for one partition. This work targets distributed KVS inside a rack where a single instance of the system can scale to 8-16 nodes. 
% The nodes execute in a symmetric model~\cite{fasst}, where each node executes both client and server processes. The client issues ACID transactions to read, write, add-up and delete the records by giving the corresponding keys, and the server execute the transaction and returns the client with a success or failed status. 
% % Transaction is the basic programming model that a KV-store expose to upper applications.
% % A transaction consists of an ordered set of reads, writes, allocation, and deletion with given keys. 
% % Applications use the key to checkout the index. 
% % Each worker thread contains program logic intermixed with transactions manipulating records in the database. 
% % Deneva provides a set of client and server processes that execute a range of transactional workloads using a variety of pre-defined but extensible protocols.
% % Typical transactions include online transaction processing (OLTP) and online analytical processing (OLAP). 
% This work optimizes for the online transaction processing (OLTP) applications with the main concern on system performance. In practice, OLTP transactions are (1) short-lived, (2) access a small number of records at a time, and (3) are repeatedly executed with different input parameters. 


% Decouple 
% OLTP v.s. OLAP: we focus on OLTP, can be extended to OLAP ? 

% OLTP workloads typically consist of short-running transactions that access few records. Every transaction requires a limited amount of remote accesses and utilizes low bandwidth. Thus, OLTP execution can be parallelized and scales with the number of PNs


% A key-value pair has limited operation sets: including read out, be written with new value, deleted and allocated. These operations are packed and issued with the granularity of transactions. A transaction consists of an ordered set of operations, which are expected to take effect with the programming order once it ends.
\fi

\ifx\nocmt\undefined


% \textbf{Limited expressivity of networking primitives.} Current networking primitives lack the expressivity required for transaction processing. On the one hand, data access operations require complex operations that involve multiple data accesses, such as traversing the index tree or acquiring remote locks. On the other hand, existing networks provide only two programming paradigms with poor performance in transaction processing. \textit{One-sided communication} enables direct access to remote memory without the involvement of remote CPUs, but it only provides simple primitives, such as remote reads, writes, and atomic instructions. To accomplish the complex operations required by transactions, a host requires multiple network round-trips. The overhead of network traffic offsets the CPU savings~\cite{fasst, learned_cache}. \textit{Datagram communication} implements unreliable packet sending/receiving, requiring the receiver's CPU to build remote procedure calls. However, remote procedure calls are slow~\cite{farm, drtmh, rpc_sigcomm14}, resulting in up to 2x lower throughput than one-sided data accesses due to software intervention.


\noindent \textbf{Strict Serializability. }

The aim of concurrency control is to provide a strict serializable consistency model of concurrent transactions. The model maintains an illusion of a single machine that executes transactions one at a time, in the order with respect to the real time~\cite{farm, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13}. The key of the strict serializable consistency is the global order of the observed reads and writes~\cite{rss_sosp21, acid_79}. For example, consider a read in a KVS that returns the value written by a concurrent write, the read imposes a global constraint on future reads that they all must return the new value, even if the write has not yet finished. 
% The ``future read'' dictates two cases: the read belongs to the same transaction, or the read belongs to another transaction that is serialized after the current one. 
A typical transactional system guarantees this feature by the commit-abort mechanism, so that a transaction can commit if all of its operations obey this ordering constraint, or it should abort and revert the changes. 
% Existing strictly serializable services guarantee this by blocking reads [22], incurring multiple round trips between clients and shards [94, 95], or aborting conflicting writes [94, 95].
% A transaction is sequence of tuple operations, interleaved with computation that are local to the user. 
% If the key-value store is weakly consistent, the read imposes no constraints on future reads. But if the key-value store is strictly serializable, the read imposes a global constraint on future reads they all must return the new value, even if the write has not yet finished. 
% The \textit{Strict Serializability} (the ``I'') ensures that the system appears to execute concurrent transactions sequentially, in an order respect to the commit time. 
% The \textit{atomicity} (the ``A'') dictates that a transaction should make all its writes atomically visible, which prevents any other transaction from observing an intermediate state with any speculative writes. 
% The \textit{durability} (the ``D'') ensures committed data is all logged to the persistent storage, while the \textit{Consistency} (the ``C'') is mostly specific to applications.
This renders an important feature of transactional writes: a write within a transaction is non-atomic, so that it is issued during execution but can only be observed once the transaction commits. 
Figure~\ref{fig:bg-kvstore}~(b) illustrates an example, where transaction T1, T2, and T3 are trying to overwrite the value of A, and T4 is reading out A. 
One can see that the value of A changes at the point of transaction commits. 
T4 commits before T3, so that T4 can only read out the old value despite T4's read happens before T3's write in physical time. 

\fi
% mimics the serial execution of the transaction in the order of T1, T2, T4, then T3. T4 is serialized before T3, so that T4 reads out the old value despite the actual time of T4's read is before T3's write. 

% the time a write is visible from other transactions is later than the transaction performs such write. 
% Figure~\ref{fig:bg-kvstore}~(b) illustrates how the ACID orders conflicted accesses. In this example, transaction T1 and T2, T3 and T4 are concurrent. T1, T2, and T3 are trying to overwrite the value of A, and T4 is reading out A. One can see that value of A changes in a way that mimics the serial execution of the transaction in the order of T1, T2, T4, then T3. T4 is serialized before T3, so that T4 reads out the old value despite the actual time of T4's read is before T3's write. This indicates an important transaction's ordering principle: the time a write is visible from other transactions is later than the transaction performs such write.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.49\textwidth]{my/figs/bg-scaling.pdf}
%   \caption{Existing distributed concurrency control schemes. }
%   \label{fig:bg-scaling}
% \end{figure}



% Specifically, a typical network card communicates asynchronously with CPU via direct-memory access (DMA), thereby requiring the receiver node to poll~\cite{fasst, myth_vldb17}, interrupt, or adopt circular buffer~\cite{farm_nsdi14} to get the message. 
% Moreover, DMA disables the corresponding CPU cache to keep coherency, hence a cold cache-fill process is needed for the CPU to work on the message contents after the DMA. 


% This naturally lead to high system complexity and high hardware mitigation cost. However, even these sophisticated methods
% \noindent \textbf{ACID transactions. } A server may execute multiple potentially conflicted transactions concurrently. To ease programming, a transaction system will hold multiple invariants that ensure data consistency. 
% The strictly serializable ACID transaction is the most general~\cite{farm, drtm, compromise, timestone_asplos20, calvin_sigmod12, cicadia_sigmod17, tm_book, rss_sosp21, hekaton_sigmod13} consistency model. The \textit{Strict Serializability} (the ``I'') ensures that the system appears to execute concurrent transactions sequentially, in an order respect to the commit time. The \textit{atomicity} (the ``A'') dictates that a transaction should make all its writes atomically visible, which prevents any other transaction from observing an intermediate state with any speculative writes. 
% The \textit{durability} (the ``D'') ensures committed data is all logged to the persistent storage, while the \textit{Consistency} (the ``C'') is mostly specific to applications.
% % Most in-memory KV-Stores are volatile, thus the \textit{durability} (the ``D'') ensures the committed data is not lost once crashed. \textit{Consistency} (the ``C'') is specific to applications and is achieved when aforementioned features are met. 
% Figure~\ref{fig:bg-kvstore}~(b) illustrates how the ACID orders conflicted accesses. In this example, transaction T1 and T2, T3 and T4 are concurrent. T1, T2, and T3 are trying to overwrite the value of A, and T4 is reading out A. One can see that value of A changes in a way that mimics the serial execution of the transaction in the order of T1, T2, T4, then T3. T4 is serialized before T3, so that T4 reads out the old value despite the actual time of T4's read is before T3's write. This indicates an important transaction's ordering principle: the time a write is visible from other transactions is later than the transaction performs such write.

% Making data durable across machine failures requires logging transactions to persistent storage, and quick recovery requires maintaining multiple replicas of the data store. Keeping persistent storage such as disk or SSDs on the critical path of transactions limits performance. Similar to recent work, FaSST assumes that the transaction processing nodes are equipped with battery-backed DRAM [12], though future NVRAM technologies, if fast enough, would also work. Finally, FaSST uses primary-backup replication to achieve fault tolerance. We assume that failures will be handled using a separate fault-tolerant configuration manager that is off of the critical path (the Vertical Paxos model [17]), similar to recent work on RDMA-based distributed transactions [12, 7]. We do not currently implement such a configuration manager.

% \textbf{This transaction definition inherently renders }, so that a write is issued during the execution phase but only takes effect after the transaction commits.



% The emerging Compute Express Link (CXL) presents a promising solution to bridge the performance and computing paradigm gaps between local and remote memory access.

% Recent developments in the IO protocols facilitate the opportunity to build a memory pool with hardware-enabled coherence. The pool is shared among multiple nodes, and user applications on each node can access it with the memory primitives, such as loads, stores, and atomic operations (e.g. Compare-And-Swap) as if local memory. 
% The applications thus can locate tuples on the memory pool to exploit its large capacity, while easily port existing single-node transaction processing systems without code changes. 


%  and the cross-node cache coherency is resolved by hardware. 

% In addition to the programming simplicity, sharing tuples in this way also gets performance benefits. 
% In CXL, the loads and stores toward the memory pool are synchronous with the processor pipeline and share the well optimized data path with local memory accesses. This enables the sub-microsecond end-to-end latency which is an order of magnitude lower than networks. Moreover, the memory is cacheable, similar to the host memory, enabling the application to exploit the spatial and temporal locality with the CPU cache hierarchy. It inherently outperforms the network-based software cache architectures~\cite{legoos_atc19, gam, txcache_osdi10, gam, learned_cache} that leverage the software runtime to track the ownership and issue coherence requests. 

% Thanks to the SMP-like interface, 
% , facilitating the seamless transition of existing single-node transaction processing systems to this new model without necessitating much modifications to the codebase. 
% The adoption of this model offers not only simplicity in programming but also significant performance benefits. 




% Second, CXL enables remote memory to be cacheable, similar to the host memory, resulting in a processor's unified physical memory space across HDM and host memory. The coherence protocol is enabled by the hardware and no explicit software protocols~\cite{gam, learned_cache} are needed. Applications thus can exploit the 
% it enables remote data accesses in a 64-byte cacheline granularity. 


% access CXL-attached memory with standard memory schematics, eliminating the network stacks' programming complexity and performance overheads. 

% It's known as a hardware distributed shared memory (DSM).


% This architectural innovation allows an external memory to be shared with standard memory semantics accessing, and hardware maintained cache coherence. 
% Transaction processing systems can leverage this additional capacity to store shared tuples. 

% Specifically, the Compute Express Link (CXL) protocol ensures that memory operations targeting the shared memory pool are synchronized with the processor pipeline, which allows these operations to utilize the same optimized datapath used for local memory accesses, thereby enhancing the efficiency of memory operations.
% For example, memory access latencies could be mitigated by the processor's out-of-order execution capabilities and write buffers. 
% These hardware modules allow the processor to better utilize cycles that would otherwise be wasted waiting for memory accesses to complete, reducing the impact of memory latency on overall system performance.
% % manage and reorder instructions to better utilize cycles that would otherwise be wasted waiting for memory accesses to complete, thus to a significant extent, reducing the impact of memory latency on overall system performance.
% This integration results in sub-microsecond end-to-end latency, significantly lower than that achievable through conventional network architectures by an order of magnitude. Additionally, the cacheability of the memory pool, similar to that of host memory, allows applications to exploit on spatial and temporal locality through the CPU's cache hierarchy. This feature distinctly outperforms network-based software cache architecture~\cite{legoos_atc19, gam, txcache_osdi10, gam, learned_cache} which rely on software runtimes to manage cacheline ownership and coordinate coherence requests.



\subsection{Network-Based Transaction Processing}

% \begin{figure}[t]
% %   \centering
%   \includegraphics[width=0.48\textwidth]{my/figs/motivation-sda-sna.pdf}
%   \caption{
%   (a) varies the number of partitions that each transaction accesses and draws the system overall throughput. (b) breakdowns the time ratio of the processor being idle. 
%   }
%   \label{fig:sna-sda}
% \end{figure}

Transaction processing is a fundamental component of modern datacenter applications, providing a high-level abstraction that simulates a single host executing a transaction at a time without failure~\cite{farm_nsdi14, compromise}. Current datasets exceed the memory capacity of a single node and are typically partitioned and distributed across multiple memory nodes. This setup necessitates cross-node networking communication for accessing remote records, which can involve great cost. Despite current systems~\cite{farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, grappa_atc15, guideline_atc16, fasst} employing various optimization techniques like co-routines and request batching to mitigate network overheads, their performance still greatly degrades if the workload inolves remote data accesses. We evaluate the state-of-the-art system DrTM-H under \rvs{an RDMA cluster with 8 CloudLab's x170 nodes~\cite{cloudlab}}. As the ratio of remote data access increases from 0\% to 100\%, its throughput degrades by over 53.3\%. 

We summarize two primary reasons for this degradation: First,\textbf{ network stacks' overhead}. Typical networking fabrics maintain a complex protocol stack to serve a wide range of hosts with robustness against packet loss, yet these stacks incur significant time costs. For example, \rvs{reading 64 bytes via RDMA can consume over 78\% of total latency in copying payloads between buffers within the network stack~\cite{directcxl}}. Second, \textbf{lack of cache coherence support}. Unlike single-node shared memory programming, networking primitives adopt a message-passing model that does not guarantee cross-node coherence at the primitive level. Consequently, it's common for distributed systems to avoid caching remote data~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} to eliminate software overheads associated with managing coherence. Thus, these systems fail to exploit data locality in applications and involve network communication for every remote data access~\cite{fasst, learned_cache}.

% Despite some distributed shared memory (DSM) provides a unified memory space that adopts software to manage the caches, they fail to gain wide usage due to high cost in networks. 


% network optimization techniques~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} demands substantial effort from developers to tune transaction systems for optimal performance.



% introduce an complex compbination of optimization techniques~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} to achieve system efficiency. 

% These stacks spend considerable cost on tasks like header generation, checksum calculations, error resolution, and congestion control. 
% introduce an complex compbination of optimization techniques~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} to achieve system efficiency. 


% do not guarantee data coherence across nodes. This lack of coherence makes it costly to exploit spatial and temporal locality, requiring software-level management of cross-node coherence~\cite{xenic_sosp21, gam}. Additionally, the great variety of network optimization techniques~\cite{ddio_ispass20, ddio-doc, farm_nsdi14, myth_vldb17, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} demands substantial effort from developers to tune transaction systems for optimal performance.


\subsection{Compute Express Link Basics}

% that mitigates the networking drawbacks in a relatively small cluster scale. 


Compute Express Link (CXL) is an open industry interconnect standard based on PCIe 5.0 physical links, contributed by a variety of vendors from firmware~\cite{samsung, samsung_2}, operating systems~\cite{tpp_asplos23}, and datacenter providers~\cite{tpp_asplos23, pond}. The recent advancements in the CXL protocol (specification 3.0) introduce a hardware memory sharing programming paradigm~\cite{cxl-doc, cxl-shortdoc, cxl-paper}, which allows external memory, i.e., Global Fabric Attached Memory (G-FAM), to be disaggregated from processing nodes. Multiple compute nodes can share G-FAM by accessing it with standard memory semantics.

% CXL enables the memory semantics of sharing the memory. 
% The CXL fabric maintain the cache coherence at hardware. 

The CXL's memory accessing sub-protocol, \cxlmem, enables the read/write memory access primitive at a 64-byte cacheline granularity. Loads and stores via \cxlmem~are managed with the well-optimized hardware datapath in processors~\cite{quantitative_approach, book_cc}, enabling the access latency of G-FAM to achieve sub-microseconds, significantly lower than network-based approaches by an order of magnitude~\cite{legoos_atc19, gam, txcache_osdi10, learned_cache}. Another key improvement of CXL is hardware-enabled cache coherence. CXL 3.0 introduces a back-invalidation channel (\cxlbi) that allows endpoint devices to invalidate host CPU's caches. The \cxlbi~enables the endpoint to retrieve host's un-flushed dirty cachelines or invalidate stale cachelines using a MESI-style protocol.


% \red{CXL with HW support, faster than Networks}
% synchronized with processor's pipelines, 
% so that they benefit from processors' optimized data path such as write buffer~\cite{quantitative_approach, book_cc} and re-order buffers (ROB)~\cite{quantitative_approach}. 


% This enables the traditional multi-thread programming on the CXL shared memory. 
% The cross-node communication backed with G-FAM follows the normal SMP-like shared memory programming. 
% Specifically, the coherence hardware implicitly transfers a node's written value to another node when it issues a read, rather than both of the nodes explicitly invoke send-receive pairs as in network programming. 
% , which allows these operations to utilize the same optimized datapath used for local memory accesses, thereby enhancing the efficiency of memory operations.
% Compared with cache-based architecture ? or direct-access architecture ?
% \blue{Compared with traditional networking-based memory disaggregation~\cite{disaggregation} and memory share~\cite{memory-share}, CXL-based memory share provides two advantages. }
% CXL has two performance advantages. 
% First, CXL enables remote data accesses in a 64-byte cacheline granularity. Its loads and stores are synchronous with the processor pipeline so that CXL exhibit an order of magnitude lower latency that networking, enabling sub-microsecond remote memory access. 
% Compared with networking-based memory access, CXL enables memory to be touched with much lower latency and finer granularity, thereby facilitating the more efficient distributed transaction processing.
% The resulting end-to-end latency is an order of magnitude lower than networking. 
% so that the processor can get the content without polling or interrupts. 

% In the following discussion, we adopt the same terminology with PCIe to identify CXL components. We term CXL devices as endpoints (EP), and term the host as the root complex (RC). 
\ifx\undefined\stale
CXL implements three sub-protocols between the Root Complex (RC) and the endpoints (EP). 
\cxlio~resembles traditional PCIe-like I/O primitives to be responsible for device controlling functionalities, such as device discovery, and interrupt handling. 
% , and power management. 
\cxlmem~enables the read/write memory primitive in a 64-byte cacheline granularity towards accessing device's attached memory. With \cxlmem, the host processor can treat device's internal memory as a host-managed device memory (HDM), which can be mapped to the physical address in the system memory just like local DRAM. 
\cxlmem~accesses are cachable and dirty cachelines are flushed back to the memory following standard cache eviction flow (typically writeback in modern x86 architectures~\cite{intel-doc, spandex_asplos18, book_cc}). 
% Memory accesses toward HDM are transformed to CXL requests by the RC and the requests are buffered at the processor cache hierarchies. Dirty data follows the CPU's cache protocol (typically write-back), and is flushed or is evicted to the HDM as memory writes. 
% The devices with only \cxlmem~and \cxlio~are functioned as memory expanders that could extend memory capacity and memory bandwidth. 
\cxlcache~allows the EP to get processors' un-flushed dirty cachelines with a MESI style protocol. It enables EP-side compute units to invalidate or to fetch dirty cachelines from the host CPU's cache hierarchies, and allows the host CPU accesses device's caches in the same way. 
The memory-intensive accelerators like smartNICs~\cite{ccnic_asplos24} could benefit from it since they incorporate internal memory and private cache hierarchies, and potentially share data between the EP and host CPUs. 
% The EP logically locates at the processor internal cache bus. 
\fi



% When an EP receives memory read or writeback requests from \cxlmem, it would send snooping requests via \cxlbi~to the hosts that may hold the cacheline copy, and responses to the \cxlmem~request with proper cacheline states. 
\ifx\undefined\stale
Instead of reusing the host-device coherence sub-protocol \cxlcache, \cxlbi~is introduced to avoid the circular channel dependence on \cxlmem~that could potentially lead to deadlocks~\cite{cxl-shortdoc, cxl-doc, cxl-paper}. 
\fi

%  To avoid unnecessary snooping traffic, the \cxlbi~implements a centralized cache directory that tracks nodes' cachelines belonging to HDM. 

% With BI, the EP acts as a centralized cache directory to track the ownership of HDM cachelines that hosts' caches hold. It adopts the directory-based MSI protocol similar to the QPI bus. The EP would send invalidation requests to the sharing hosts when one host require the exclusive ownership of the cacheline, or would speculatively probe the value of an exclusive copy when a host requests a read. 
% Similar to \cxlcache, the BI also allows the EP to invalidates an RC's cacheline.
% send coherence requests from the device to the host and retrieves the written back cachelines. 
% But differently, the BI module in RC works on DRAM controller level. 
% CXL does not re-use \cxlcache~since it may cause circular channel dependence that could potentially lead to deadlocks. 

% \begin{figure}[t]
% %   \centering
%   \includegraphics[width=0.48\textwidth]{asplos25-templates/my/figs/mem-access-lat.pdf}
%   \caption{
%   Memory Accessing Latency. 
%   }
%   \label{fig:mem-access-lat}
% \end{figure}

\begin{figure}[t]
%   \centering
  \includegraphics[width=0.47\textwidth]{my/figs/cxl_slowdown_breakdown.pdf}
  \caption{
  Relative slowdown than \textit{oracle}.}
  \label{fig:cxl_slowdown_breakdown}
\end{figure}


\subsection{CXL-Based Transaction Processing Systems}

One promising CXL-based approach for transaction processing rebuilds the remote procedure call (RPC) layer with a more efficient CXL implementation. This method adapts to the traditional networks' partitioned architecture, where each node exclusively owns a partition of Global Fabric Attached Memory (G-FAM) and uses RPCs to access partitions from other nodes. Hence, it enables existing network-based systems~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12} to migrate to CXL with almost zero code modifications. 
% The CXL RPC eliminates the need for cache coherence and can operate on CXL 1.1 without \cxlbi~channels~\cite{hydrarpc_atc24}. 
However, despite being carefully optimized, current SOTA CXL RPCs~\cite{hydrarpc_atc24, partial_sosp23} still suffer from performance overheads due to inherent software control overheads and cache flushing instructions. For example, the HydraRPC~\cite{hydrarpc_atc24} exhibits the averaged latency 3.22x longer than directly accessing G-FAM's DRAM, and consumes considerable CPU cycles on polling the RPC queue. Moreover, partitioned systems work great only if the workload is also perfectly partitionable. As the number of transaction-touched partitions increases, their performance degrade quickly~\cite{hekaton_sigmod13, silo_sosp13}. 
 
% , which degrade communication performance relatively higher than in traditional networks, due to the reduced time spent o n CXL link transmission. 

Another approach involves building a shared memory system leveraging CXL’s cache-coherent memory semantics. This approach employs single-node systems~\cite{silo_sosp13, abyss_vldb14, polyjuice_osdi21} that place shared data structures, including indexes, locks, and tuples, on the G-FAM, allowing worker nodes to run transactions in a multi-threaded manner. The \cxlbi~ensures hardware-level coherence across nodes. Compared to partitioned systems, this organization better aligns with CXL’s capabilities for cache coherence and exploits data locality, especially beneficial for indexes~\cite{masstree, learned_cache} and locking mechanisms. In this paper, we stick to this shared memory system where each transaction can access the entire dataset. Our evaluations in Section~\ref{sec:eval} shows that the \name-optimized shared memory system outperforms the CXL-RPC partitioned system at most 7.3x on poorly partitioned workloads. 

% In addition, the SE architecture is inherently matched with G-FAM architecture, which decouples computing and memory resources, thereby enhancing resource flexibility and load balancing capabilities. 
% it fully exploits CXL's cache coherency capability to benefit for accesses on the data structures with 
% Although there are efforts to enhance remote procedure calls on CXL for better efficiency~\cite{partial_sosp23, hydrarpc_atc24}, which could be leveraged to share slices across nodes, they largely assume an architecture that lacks full cache coherency, which undermines the potential performance benefits of CXL. The current CXL RPC introduces 1.71x larger latency than directly sharing memory with the store-load pairs, due to RPC's inherent overheads on controlling and cache flushing. 

% There some attempts~\cite{partial_sosp23, hydrarpc_atc24} on rebuilding remote procedure call (RPC) with higher efficiency on CXL, but they still stick to architecture without cache coherency. The RPC style programming, despite keeping back compatibility with network-based systems, fails to fully exploit CXL's performance advantages with 3.22x longer latency due to RPC's inherent overheads on controlling and cache flushing. 

% Figure.~\ref{fig:} tests a widely used index Masstree under 

% Unlike SN architectures, this model can fully exploit CXL’s cache coherence capabilities to exploit application data locality, especially beneficial for indexes~\cite{masstree, learned_cache} and locking mechanisms. Figure~\ref{fig:mem-access-lat}~(c) tests a commonly used index~\cite{masstree} on its address hotness under random queries. 


% We compare these setups in Figure~\ref{fig}. We could tell an CXL RPC takes xx times longer latency than directly adopting CXL load/store primitives with cache coherence. 
% However, sticking to this architecture and the message passing communication paradigm fail to fully exploit the CXL's memory semantics. Despite being extensively optimized, the RPCs introduce inherent software controlling overheads. As Figure~\ref{fig:} shows, an CXL RPC takes xx times longer latency than directly sharing data via load/store pairs with cache coherence. 


% The recent version of CXL (3.0) has introduced a back-invalidation (BI) mechanism, which is designed to facilitate the implementation of shared memory pool with hardware-enforced coherence as we discussed above. 
% CXL specifies three distinct device types, each reflecting unique usage models through diverse combinations of these sub-protocols. Type-1 devices, characterized as zero-memory accelerators, implement \cxlio~and \cxlcache. Type-2 devices, typically memory-intensive accelerators like GPUs, also incorporate internal memory and cache hierarchies and hence implement all three sub-protocols. In contrast, Type-3 devices, functioning as memory expanders, adopt only \cxlio~and \cxlmem~without facilitating direct interaction between the device-side compute unit and the CXL CPU’s RCs. 

% Writer-initiated invalidation means that every read miss triggers a request for read permission in the form of Shared state – this permission is revoked only when a future write to the line sends an explicit invalidation to the reader. Ownership-based caching means that every cache miss for a write or atomic read-modify-write (RMW) access triggers a request for exclusive write permission in the form of ownership, or Modified state. Both read and write permissions are requested for the full line in MESI. Once read permission (Shared state) or write permission (Modified or Exclusive state) is obtained, subsequent reads or writes to the same cache line may hit in the cache until a conflicting access from a remote core causes a downgrade or until the cache line is evicted. This strategy can offer high cache efficiency by exploiting temporal and spatial locality, but it comes at a cost.




% Different with them, this work investigates the advantage of CXL's cache coherent memory sharing as well as discusses its challenges and limitations. 


% To the best of our knowledge, this work is the first to contribute type-2 architectures, and is first to explore the implications of CXL's cache coherence on distributed transaction processing applications. 


% Type-2 devices are memory-abundant accelerators such as GPUs. These devices also accommodate cache system memory for processing. Type-2 devices thus implements all of three sub-protocols. Type-3 devices are memory expanders implementing \cxlio~and \cxlmem, which means they have no interface for a device-side compute unit to directly interact with CXL CPU’s RCs. Although various excellent works~\cite{pond, directcxl, tpp_pub, cxl_anns_atc23} have explored the benefits CXL-based data center architectures, they all stick to the type-3 devices to as the memory bandwidth and capacity expansion. It's partially due to the ambiguous usage of CXL's cache coherence and the its resulting performance advantages. This work explores it in the distributed transaction processing applications and propose our design based on Type-2 devices. 


% Previous works~\cite{pond, directcxl, tpp_pub} have used CXL for data center, they are mainly focused on memory disaggregation, where memory is not shared across hosts. To the best of our knowledge, no previous work has discussed using CXL for in-memory transaction systems. 

% By operating directly on LLC caches and avoiding access to main memory on the \cxlcache~data path, this protocol can provide a latency advantage when communicating over PCIe~\cite{cxl-doc}. 


% It provides global addressing of host-attached memory and device-attached memory through the sharing page table, 
% with the operating system treating the device-attached memory as a NUMA node~\cite{directcxl, pond}. In \cxl~3.0, a device can use \cxlmem~to change the host processors' cache, which is called the ``back-invalidation'' mechanism. 





% \noindent \textbf{System Integration. }

% as well as the DAX software introduces little overheads~\cite{cxl_hpc_profiling} compared with zNUMA mode. 

% since it avoids OS issues from unwanted page allocation on the HDM, thereby precisely  within an.  


% The host operating system uses PCIe enumeration to discover the \name~device during boot-up. Upon detection, 
% the device reports its memory size and strongly/weakly consistent memory regions to the host. The driver then allocates physical address segments to map these regions and marks the segments as reserved to prevent undesired page allocation/reclamation by kernel or user-level processes. Next, the driver writes the memory regions' base addresses to the Base Address Register (BAR) and creates a memory-mapped file, enabling user-level processes to access the device-backed memory. Once the boot-up process is complete, the device-backed memory becomes a NUMA-node without cores, and processes can only access this region by mmap the entry \name~device to their virtual memory space. When a process accesses the device-backed memory, the home agent takes control from the local memory manager and sends CXL requests following the principles mentioned in Sec.~\ref{subsec:detect} and Sec.~\ref{subsec:change}. 




% The cache coherence ability of CXL is believed as a significant contributor to augmenting the efficiency of cross-host communication. As a consequence, t

% For the effective activation of BI, it is imperative that memory devices sustain a directory to meticulously monitor the ownership of cache lines within the coherent shared memory. The scale of such a directory expands linearly in correlation with the number of nodes sharing the memory. Consequently, there arises a critical need to judiciously balance the shared memory scale, carefully weighing the trade-offs between performance enhancement and the accrual of hardware overheads. Further exploration and discussion on this topic are deferred to Section~\ref{sec:overview}.


% to help improving the performance of cross-host communication, as a substitution of traditional networks at a moderate scale. As a consequence, the newest CXL version (3.0) introduces a back-invalidation (BI) mechanism to enable the implementation of shared and hardware-enforced coherent memory across multiple independent hosts. The BI adds additional \cxlmem~channels to send coherence messages from the device to hosts, instead of re-using \cxlcache~channels, in order to avoid the circular channel dependence which may cause deadlocks. 

% To enable BI, the memory device should maintain a directory to track ownership of cache lines in the coherent shared memory. Such directory scales linearly with the number of sharing nodes. Consequently, the shared memory scale should be carefully limited with the tradeoff between performance and hardware overheads. We leave the discussion in Sec.~\ref{sec:overview}. 

% \wz{CXL.mem rather than CXL.cache}, check CXL SPECs, CXL pre, Linux Pre
% End-to-end stacks, DAX + Cache Flows

% \noindent \textbf{Scalability Issues}
% Limit it within a rack. Check pond, mem-tunnel, latency at different scales. 

% \noindent \textbf{Low-Latency Data Sharing:} CXL enables data accesses for a 64-byte cache line with the roundtrip latency to sub-microseconds. Such a low latency, fine-grained data sharing fits distributed transactions, which make frequently byte-level synchronization and is dominated by the delay of networks. CXL benefits from two protocol advantages. First, CXL directly fetches data to the host LLC, eliminating the memory copy between NIC buffers and main memory~\cite{pond, directcxl}. Second, the processor waits for the result in a synchronous, instruction-level way that resume the processor's pipeline as soon as the instruction returns, while the communication between NICs and the processor is asynchronous, requiring either busy pooling or OS-intervened interrupts on retrieving the results. 

% \noindent \textbf{Software Transparent Cache Coherency:} CXL enables memory attached to be cacheable (referred to as ‘Host managed Device Memory’, HDM), similar to the host memory, resulting in a processor's unified physical memory space across HDM and host memory. Operating systems thus transparently exploit the HDM by reusing virtual memory mechanisms. A typical system exposes HDM as a zNUMA node~\cite{tpp_asplos23, pond, mem_tunnel}, user-level applications thus are able to map applications to HDM leveraging existing NUMA balancing mechanisms. The programmers are free to differentiate accesses to whether host memory or remote HDM in application codes. On the contrary, networks requires applications to distinguish remote data accesses with explicit API calls.
