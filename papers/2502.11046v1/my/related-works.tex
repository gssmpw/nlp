\subsection{Coherence Protocol Layer}

CXL adopts an asymmetric approach to coherency and backwards-compatibility. CXL coherence is decoupled from host-specific coherence protocol details. As shown in Fig.~\ref{fig}, the host processor contains a Root Complex (RC), one per CXL link, acting as a home agent (HA) endpoint to enforce the MESI (Modified, Exclusive, Shared, Invalid) coherency protocol with external devices. A device implements a Device's Coherency Engine (DCOH) as cache agent (CA) accordingly. 

Other cores keep coherence with the HA via processor-specific protocols. A device implements a Device's Coherency Engine (DCOH) acting as the cache agent (CA) endpoint to pair with HAs. All cachelines are ``owned`` by the host by default (in Host Bias status), and their states is tracked 

The protocol between HA and CA is built on 3 channels in each direction. The direction of the channels are Host-to-Device (H2D) and Device-to-Host (D2H). Each direction has a Request, Response, and Data channel. The channels flow independently in each direction with one exception: a Snoop message from the host in H2D Request channel must push a prior Global Observation (GO) message in the H2D Response for the same cache line address. 


\noindent \textbf{Discussion. }
Similar to most CXL memory pool proposals~\cite{pond, partial, xxx}, we confine our work on a rack-scale since CXL achieves the sweet point between performance and cost at this scale. 
Expanding the cluster to a larger scale requires traditional networking techniques to communicate across racks. \name~could be easily extended to traditional distributed transaction frameworks by replacing the single-node runtime with an \name~rack-scale. \red{Performance Optimizations. }
% The IP facilitates \cxlcache, \cxlio, and \cxlmem~sub-protocols and registers as an standard CXL Type-2 device to the OS device hierarchy (Sec.~\ref{sec:cxl_bg}). 
% Each device within this architecture manages up to 1TB of DRAM, controlled by internal memory controllers, as documented in existing literature \cite{anns, cxl_samsung}.


\subsection{Potential Solutions and Drawbacks}

The architecture could leverage this loosely coherent nature to reduce the coherence drawbacks. Previous transaction systems have proposed approaches in three categories. 


\noindent \textbf{No caching, no coherence.}~\cite{farm_nsdi14, fasst, drtm, drtmh, thecase_vldb22, compromise} A transaction directly accesses remote memory without locally cache. Data is stored in the remote memory with a lock per data item, and a transaction use atomic operations (e.g. RDMA CAS) to acquire a lock before accessing data. The main drawback of this approach is low performance since every memory operations must touch the slow memory pool and no data locality is exploited. 

% \noindent \textbf{Caching, no coherence.} A weakly consistent model~\cite{rvweak_pact17, rvweak_isca18, spandex_asplos18, lrcgpu_micro16} allows 


\noindent \textbf{Transactional memory (TM).}~\cite{STM, HTM}. TM is previously adopted by the persistent memory to provide logging atomicity. 
HTM adds bits to the hardware cacheline to mark transaction's read and write set, and leverage cache coherence protocol to detect conflicts. Some optimistic HTM architectures loose the strict cache coherence limit and allows multiple writers own different values on the same cacheline. To this end, they reduces the the unnecessary cache coherence traffic that the pessimistic vanilla primitive has. However, their overhead is not negligible since they always introduce significant changes in processor cache designs and integrates extensive logic into circuits~\cite{munin_ppopp90, tcc_isca04, vtm_isca05, threadmarks_tc94, rtm_isca07, overlaytm_pact19}, which are too ambitious for recent complicated processors. 



\noindent \textbf{Shared-Nothing Architecture (SNA).} For decades, the SNA has been the paradigm of choice in distributed database management systems (DBMSs), primarily due to its efficiency in reducing network communication~\cite{hstore_damon16, citus_sigmod21, memsql_vldb16, voltdb, calvin_sigmod12}. 
% In SNA, data is divided into exclusive partitions, with each partitions assigned with a singe-node runtime engine. 
In SNA, each partition is exclusively assigned to a single-node transaction system. 
If a transaction involves multiple partitions, it is split into multiple sub-transactions based on which nodes its touched records locate on. The DBMS adds sub-transactions to the task queues of the corresponding nodes. Fig.~\ref{fig:bg-scaling}~(a) illustrates an example where a transaction is split into two sub-processes involving records A and D/E. The sub-processes are dispatched to node-0 and node-1 respectively. 
After execution, the participant nodes vote for the results via consensus protocols such as two-phase commit. 
% This setup ensures that each partition operates independently, negating the need for inter-node communication for retreiving remote records.
% To achieve high efficiency, most SNAs rely on deterministic scheduling~\cite{calvin_sigmod12, voltdb, hstore_damon16}, wherein the DBMS has the pre-knowledge of transactionsâ€™ read and write sets so that it knows the partition in advance~\cite{dbx1000_dist_vldb17}. 
% With such priori, transaction managers across nodes reach a consensus on the serialization order before the transaction actually starts.
% For example, in H-Store~\cite{hstore_damon16}, the DBMS assigns each transaction a timestamp and then adds it to the wait queues for the locks of its target partitions.
% The execution engine for a partition removes a transaction from the queue and grants it access to that partition if the transaction has the oldest timestamp in the queue. This transaction is allowed to start only after it acquires all the partition-level locks that it needs, so that a started transaction will always runs to finish. 
SNA works great when the workload is also partitionable. 
However, its performance drops quickly when the workload partitions poorly so that either the load between partitions is imbalanced, or the number of average touched partitions of a transaction is large. The reason is the increased overhead associated with dispatching and resumption of sub-transactions, as well as the two-phase commit for participant nodes to negotiate the commitment issues of sub-transactions. 
% However, its performance drops quickly when the workload partitions poorly so that a transaction on average touch many partitions. 

% The reason are three folds. First, a transaction that holds an increased number of partition-level locks impedes the progress of concurrent transactions within the same partition, even they have no direct conflicts over the accessed tuples. 
% Second, there is an increase in the overhead associated with the dispatching and subsequent resumption of sub-transactions. Third, the participant nodes of transaction necessitate a consensus process such as two-phase commit to negotiate the validation results, which is costly when the nodes 


 
\noindent \textbf{CXL-augmented Shared-Nothing Architectures. }

SNAs are purposely designed for slow networks by localizing transactions and query processing to eliminate slow remote record fetching. 
However, the rack-level CXL-based memory share exhibits monolithic access latency thereby benefiting not much from communication reduction. 
The main drawback of SNA, i.e. easily prone as the number of averaged touched partitions increases, is much worse on the CXL-augmented architecture. 
% The SNA's performance is not stable across applications since it is easily prone as the number of averaged touched partitions increases. 
Fig.~\ref{fig:sna-sda}~(a) analyzes the throughput of CXL-augmented SNA implementation and compares with traditional network-based distributed version. 
We detail the implementation and the testbeds at Sec.~\ref{sec:}. 
% that use the partition approach resembling HStore~\cite{hstore_damon16}, the corresponding network-based implementation is extended from Calvin~\cite{calvin_sigmod12}. See section~\ref{sec:} for more details.
% Each node consists of four worker threads. 
% The DBMS partitions the dataset by node and gives one node exclusive access to a partition, the worker threads on each node retrieve sub-transactions from a shared queue. 
% We also implement a distributed version by extending a distributed deterministic transaction manager resembling Calvin~\cite{calvin_sigmod12} for comparison. 
% We query the dataset with YCSB transactions, and assume the preknowledge of read/write sets. 
% With the exception of network's low performance, 
One can tell from the results that CXL-based memory share degrades more when the multi-partition ratio increases. The application with 1 mpr has only 10\% throughput than the single-partition setup with CXL, while the network-based cluster remains 37\%. 
\red{
The inadequation between SNA and shared-memory architecture is also discussed by previous works~\cite{hekaton}, 
% Similar with previous works' results~\cite{dbx1000_dist_vldb17, abyss_vldb14, thecase_vldb22}, 
% the throughput of network-based system degregates 21\% on 0.2 multi-partition ratio, and 63\% on 1 multi-partition ratio. 
% The performance drops 21\% when only 20\% transactions touches multiple partitions, and drops 63\% when all transactions access more than one partitions. 
% Unfortunately, the CXL-augmented system drops nearly 90\% at 1 mpr. 
it's due to shared-memory architecture enables a faster communication between workers, thereby lowering the relative overhead of dispatching and resuming sub-transactions. }
Consequently, we base our approach on SDA architectures. 
% It's due to CXL's shared memory abstraction brings more lightweight transaction manager, thereby having higher single-parition transaction performance. The overhead of dispatching and resuming sub-transactions is relatively much larger compared with actual works.


% Shared data implies that every database instance can access and modify all data stored in the database. There is no exclusive data ownership. A transaction can be executed and committed by a single instance. In contrast to partitioned databases, no workload knowledge is required to correctly partition data and minimize the number of distributed transactions. 

% First, shared data access requires synchronization [52]. We use lightweight load-link and store-conditional (LL/SC) primitives [25] as a foundation to implement efficient multiversion concurrency control (MVCC) [5] and provide latch free indexes [35]. Second, data access involves network communication. 

% As the cloud becomes prevalent, shared-storage DBMSs start to gain attention because they can best leverage the cloud infrastructure of storage disaggregation. Examples include Amazon Aurora [61], Google AlloyDB [2], Alibaba PolarDB [16], Alibaba AnalyticDB [68], Microsoft Socrates [11], Microsoft Polaris [10], Huawei Taurus [23], and Snowflake [21].



% not only the partition locks that a transaction holds increase, but also the overheads of sub-transaction dispatching scales linearly. The performance consequently drops quickly. 


% Such deterministic scheduling assumes the pre-knowledge of transactionsâ€™ read and write sets or calculates these sets in real time through a resource-intensive reconnaissance step.~\cite{dbx1000_dist_vldb17}

% One notable feature of SN architecture is its reliance on deterministic scheduling~\cite{calvin_sigmod12, voltdb, hstore_damon16}, wherein transaction managers across nodes should reach a consensus on the serialization order prior to transaction initiation. 



% \noindent \textbf{Shared-Data Architecture (SDA).} 


In contrast, SDA offers a more integrated approach, allowing transactions to access and modify data across any node within the system~\cite{mtcp_nsdi14, xenic_sosp21, ipipe_sigcomm19, linefs_sosp21, fasst, drtmh, grappa_atc15, mtcp_nsdi14, drtm}. Despite the dataset is also partitioned across nodes, this approach eliminates the notion of exclusive data ownership, enabling a single worker thread to execute and commit multi-partition transactions. As shown in Fig.~\ref{fig:bg-scaling}~(b), the worker thread locating on node-0 fetches the remote records D/E to its local buffer and apply writes D locally. 
After it finish all operations, the worker thread validates its accessed remote records, by querying the node-1's transaction manager to see if other concurrent transactions have changed E after its read. 

This approach replaces the SNA's partition-level serializer with a more fine grained record-level validation to allow more concurrency. However, SD architectures are particularly much sensitive to network delays since remote record access and validates take massive network roundtrips. 
% lies on the critical path. 
As a consequence, recent research systems~\cite{drtm, drtmh, farm, fasst, compromise} tend to reduce communication latency with caching~\cite{learned_cache, gam} and advanced networking techniques, including user-level network stacks~\cite{mtcp_nsdi14} that bypass operating systems, SmartNICs~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21} that offload protocol processing to network fabrics, and technologies like remote direct memory access (RDMA)~\cite{farm, myth_vldb17} and Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc} to reduce network latency. 
% Additionally, these architectures leverage compiler enhancements and programming techniques, such as co-routines~\cite{fasst, drtmh} for efficient context switching and door-bell batching~\cite{grappa_atc15, guideline_atc16} for fewer PCIe roundtrips. 


% its performance is highly bottlenecked by the partition-level serializer, but not the cross-node communication. 
% Moreover, the performance drops even worse in CXL-based systems since it has more lightweight transaction manager thereby exhibiting higher single-partition performance. The overhead of subtransaction dispatching and resuming is relatively much larger compared with actual works. 




Recent research systems~\cite{drtm, drtmh, farm, fasst, compromise} tend to converge on a common abstracted concurrency control protocol design that augments the traditional optimistic concurrency control (OCC). 
The transaction execution node acts as the coordinator, and the nodes owning remote access objects of the transaction act as the participant. 
(1) \textit{Execution Phase}: The coordinator thread fetches all remote read and write sets from the primary replica of participants. Writes are temporarily buffered in the coordinator. 
(2) \textit{Validation Phase}: The coordinator first acquires locks for the write set objects. Then revisits the primary replicas to fetch the read set once more. This is to make sure that the versions previously read by the transaction remain up to date. The transaction aborts if any versions have been changed, or if their corresponding locks are occupied. 
(3) \textit{Log Phase}: During this phase, the coordinator sends log messages to both primary and backup replicas to log the write set on durable storage. 
(4) \textit{Commit Phase}: Participant updates the write set objects on both primary and backup replicas, increments their respective versions, and releases the locks. After that, both the primary and backup replicas log the commit message. 

the transaction consequently execute with network communication to access the remote record D. Since networks are practically slower than local DRAM by orders of magnitude, these works proposes various approaches to reduce network roundtrips or to improve the network performance. 

\textit{Remote procedure call (RPC)} employs an agent thread on the remote servers to execute tasks on behalf of the request node~\cite{fasst, grappa_atc15, calvin, traditional_txn}. The RPC allows access to the remote store with a single network roundtrip, no matter how intricate the task is. For example, FaRM~\cite{farm, comprimise}, FassT~\cite{fasst} uses RPC for multi-lock acquisition, and DrTM-H~\cite{drtm-h} uses it for index traversal. 
% An RPC protocol can be implemented with common two-sided message passing primitives~\cite{fasst, guideline_atc16} or with efficient one-sided network verbs (e.g. RDMA) by remote server actively polling the message logs~\cite{farm, mit_distributed}. 
\textit{Protocol offloading} technique offloads remote tasks to computable network fabrics such as smart NICs and programmable switches~\cite{xenic_sosp21, concordia_fast21, netcahe_sosp17, spin_sc06, coredirect_ipdps11, percs_hoti10}. By caching frequently accessed records, these devices reduce the need for network roundtrips to fetch them from the hosts' memory.
% However, devices have only limited capacities on computing and storage, thus most implementations~\cite{concordia_fast21, xenic_sosp21} only offload selective functions, such as lock management, to the hardware, but leave the host processors for manipulating complex concurrent control protocols. 
\textit{Index caching} 


The distributed systems also benefit from advances in networking primitives, including but not limited to user-level network stacks bypassing OSs~\cite{mtcp_nsdi14}, offloading protocol processing to SmartNICs~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, remote direct memory access (RDMA)~\cite{farm, myth_vldb17}, and Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc}. Compiler improvements also utilize co-routines~\cite{fasst, drtmh, grappa_atc15} for efficient context switching, and applications have adopted shared connection structures~\cite{fasst} and packet batching~\cite{grappa_atc15, guideline_atc16} to optimize performance. However, despite some works exploit the best combination of these approaches~\cite{farm, fasst, grappa_atc15, compromise, drtmh, drtm, storm_systor19, prism_sosp21, guideline_atc16, smartnic_osdi23, xenic_sosp21}, they can't fully compensate for the performance disadvantages of networking. 

\subsection{Distributed Transactions}

% that govern the order in which transactional operations are observed by different concurrently executing threads. These rules 


% In this section, we briefly review the two important design choices for a distributed transactional system and discuss how they relate to network features. 
%  This section reviews the design space that preserves the features resembling \name. 

% thus develops various protocols to tradeoff performance under different system limitations as well as application features, e.g. conflict rates. This section describes the design 


% Concurrency control protocols detect conflicted operations either from the same host or from different hosts. 

We discuss the mechanics of a serializable distributed transaction operating on a key-value store. Conventionally, datasets are partitioned into multiple splits based on keys and are then distributed across different hosts. Each host performs both as a coordinator and as a participant. Transaction may access any address located on any of these host. 

\subsubsection{Concurrency Control Protocols}   \label{subsec:ccp}
Recent research systems~\cite{drtm, drtmh, farm, fasst, compromise} tend to converge on a common abstracted concurrency control protocol design that augments the traditional optimistic concurrency control (OCC). A transaction has a specific set of keys for reading and another set of key-value pairs for writing. The coordinator thread extracts the read/write set from applications and orchestrates the execution of each transaction through four phases:

% Recent research systems share a similar abstracted concurrency control protocol design that extends backward optimistic concurrency control (OCC) with primary-backup replication for fault tolerance. A transaction has a set of keys to read and a set of key-value pairs to write. The coordinator thread get the read / write set from applications, then conducts four phases for each transaction: 
\begin{enumerate}
    % \item In the read phase, the coordinator thread reads all read sets from primary replicas. Writes are buffered in a private space and the write set is locked. The transaction aborts if any lock is held. 
    
    \item \textbf{Execution Phase}: The coordinator thread fetches all read sets from primary replicas. Write operations are temporarily buffered in a thread-private space. 
    
    \item \textbf{Validation Phase}: The coordinator first acquires locks for the write set objects. Then revisits the primary replicas to fetch the read set once more. This is to make sure that the versions previously read by the transaction remain up to date. The transaction aborts if any versions have been changed, or if their corresponding locks are occupied. 
    
    % \item \textbf{Log Phase}: During this phase, the coordinator sends log messages to both primary and backup replicas to log the write set on durable storage. 
    
    \item \textbf{Commit Phase}: The primary node updates the write set objects, increments their respective versions, and releases the locks. After that, both the primary and backup replicas log the commit message. 
    
    
    % \item In the validation phase, the coordinator reads the readset from the primary replicas again to check if the versions read by the transaction are up to date. If any versions have been changed, or the corresponding locks are held, the transaction aborts. 
    
    % , the write set is simultaneously locked. If any of the locks are already engaged, the transaction is promptly aborted. 
    
    % \item In the log phase, the coordinator first send log messages to backup replicas to log write set, then log on the primary node. 
    
    % \item In the write phase, the primary node update the write set objects, increments their versions, and unlocks them, and both primary and backup replicas log the commit message.
\end{enumerate}



% The concentrated validation scheme makes OCC a favored design choice by most state-of-the-art cross-host in-memory transaction processing systems, where networking tolerates large latency and failure possibilities. 

% In pessimistic con currencycontrol (PCC), a transaction acquires the lock on a record prior to proceeding reads or writes. In a typical S2PL protocol, the lock is held until the transaction completes and blocks any concurrent transactions from accessing the record. The long-duration locking discipline introduces unnecessary aborts and stalls since it prevents concurrent un-commited reads and writes. 
% Optimistic concurrency control (OCC) allows more concurrency than PCC by delaying consistency checking until commits.

% In a typical three-phase OCC system, transactions are allowed to proceed without restrictions first (the ``read phase''), then the transaction perform a series of checks to ensure consistency (the ``validation phase''). If a conflict is detected, the transaction is rolled back and retried; otherwise, it makes its speculative updates globally visible (the ``write phase''). OCC exhibits high performance under low contention since un-committed writes never introduce aborts, as illustrated in Fig.~\ref{fig:over-coherent-b}. 
% They coarsen validation~\cite{} requests into a few RPCs to reduce network roundtrips, or adopt lock-free commits~\cite{} to avoid costly remote atomic operations. 



% However, OCC suffers from well-known expensive aborts problem that the lock-free execution risks many aborts by being too optimistically. 

% Regular Sequential Serializability and Regular Sequential Consistency

% every transaction should buffer an additional consistent local copy for transactionally accessed data, and introduces large roll-back cost when aborts are frequent. 

% refs from DrTM-H

% \subsubsection{Different scales}

% Transaction processing systems span from various scales from a socket to clusters. They work on different communication paradigms and network costs and face different tradeoffs on the choices of system components. 


% Both of them can be done in the hardware-only solutions or requiring software interventions. \textit{Software conflict detection} builds centralized or distributed \textit{lock service} on hosts, leveraging the only host CPUs to serialize conflicted accesses. This approach ease programming with an abstracted \textit{distributed lock service} that implements both network-based remote locks and local locks. However, it easily yields with non-ideal performance due to the substantial software overheads. In contrast, \textit{hardware conflict detection} directly adopts atomic operations provided by hardware to conduct synchronization without wasting CPU cycles. Works~\cite{rdma} leverage atomic RDMA single-sided operations to validate remote records' versions atomically, work~\cite{programmable-switches} build locking services within the programmable switches, and works~\cite{drtm} leverages HTM to xxx. This approach outperforms software approaches in both system throughput and commit latency, but they are limited with specific network and host architectures, and some of them even use non-commercial non-standard hardware. 



% OCC yields with higher abort cost since it rolls back the entire transaction modification, and degredates OCC's performance under high contention.  


% and they focus on using efficient RDMA verbs to reduce network costs and shipping functions to SmartNICs to reduce PCIe roundtrips. 

\subsubsection{Advanced Networking-assisted Distributed Transactions} 
Since remote operations take the most part of performance, recent systems employ a variety of advanced networking techniques to accelerate remote operations:

% , we propose a taxonomy based on their execution locations. 

% \noindent \textbf{Agency Thread} technique~\cite{fasst, grappa_atc15, calvin, traditional_txn} involves a thread on the participant performing remote operations as directed by coordinators. This thread can be invoked either using two-sided remote procedure calls (RPCs)~\cite{fasst, guideline_atc16} or through message logs written by coordinators using one-sided RDMA verbs, with participant CPUs actively polling these logs~\cite{farm}. This approach has good computing expressitivity that conducts complex operations with only one network roundtrip, so that it becomes the ``silver bullet'' for all phases~\cite{farm, fasst, grappa_atc15, calvin, traditional_txn}. However, a notable drawback is the significant consumption of remote CPU resources for message processing, argument marshalling, thread activation and scheduling. 




% Additionally, resolving local-remote conflicts becomes complex because the consistency models between in-network programmable cores (typically ARM-based) and host CPUs (typically x86) differ. 

% Requester waits: Requester threads directly access remote memory using RDMA single-sided verbs, locking with atomic CAS which leverages RDMA CC to work with local atomic operations. 


% Detecting conflicts between local memory operations, and detecting conflicts between local and remote memory operations
% local-local: atomic operations based on cache coherence, similar to single-host transactional memory, e.g. PL2, xxx
% local-remote: 
% - Agent cores: RPC, FaRM-like polling, NIC pushes the arguments of functionalities and hire an core to do it
% - Firmware-Direct: SmartNIC, RDMA one-sided ops, NIC directly access CPU's caches via DMA ? check how XeNIC does. 

% Intra-host: HTM, versions
% Inter-host: locks, versions, optimizations: hardware primitives (programmable switch, RDMA atomics), coarsen packets, multiple versions ( snapshot isolation )

% Version Management
% Intra-host: some HTMs, buffers
% Inter-host: natually fits isolation. 





% Despite the appealing cache coherency ability of CXL, existing works in industry and academia have advocated for memory disaggregation systems~\cite{pond, tpp_pub, directcxl, intel_cxl_pub, samsung_2} as a solution to alleviate memory over-provisioning in data centers. Hosts exclusively own parts of the HDMs, but still relies on network to share data. 

\subsection{Inefficiencies in Networking Techniques}

% Building a system in a network-capable distributed cluster is much more complex than a single host due to the extreme overhead of network IO. A typical network latency is an order of 10 us under load, which is much higher than the time spent by applications on local data store accesses. Network does not inherently keep coherence to remote load store as the invalidation-based coherence protocols in CPU cache does, thus requires the system to explicitly handle the conflicted accesses and synchronize updated contents. Some fast user-space network I/O even wastes considerable CPU cycles to poll on the task queue for response, just to prevent the large overhead of interrupts.


% Developing the transaction system on distributed clusters poses significantly greater complexity than a single-host setup, due to the difference between inter-host and intra-host communication schemes. Intra-host threads communicate with synchronized shared-memory primitives such as memory reads and writes, while inter-host communication requires asynchronized network I/O with explicit software calls. The two approaches exhibit substantial difference on both programming paradigms and performance.
% Under loads, typical network latency can reach multiple us~\cite{directcxl, gam, farm, fasst}, markedly exceeding the time consumed by application-level accesses to local data stores (x10 ns). Additionally, certain high-speed, user-space network I/O implementations~\cite{fasst, drtmh, herd_sigcomm14} consume substantial CPU resources while polling~\cite{pilaf_atc13, herd_sigcomm14} task queues to avoid the considerable overhead of interrupt handling. More importantly, network interfaces adopt the non-coherent producer-consumer semantics. This necessitates the application programmers~\cite{gam, concordia_fast21, grappa_atc15} to resolve inter-host conflicts and keep data consistency. 

Distributed clusters complicate transaction system development compared to single-host setups due to the contrasting inter-host and intra-host communication methods~\cite{directcxl, gam, farm, fasst}. Intra-host communication utilizes synchronized shared-memory primitives, while inter-host communication relies on asynchronized network I/O. These methods differ significantly in programming paradigms and performance. Network can achieve several microsecond roundtrip latency under load~\cite{directcxl, gam, farm, fasst}, far surpassing local data access times (x10 ns). Moreover, high-speed interrupt-free network I/O implementations consume notable CPU cycles while polling~\cite{fasst, drtmh, herd_sigcomm14, pilaf_atc13, herd_sigcomm14}, while local data access latency can be hidden with the use of out-of-order execution and Miss-status Handling Registers (MSHRs). More importantly, network interfaces employ non-coherent producer-consumer semantics, necessitating application programmers~\cite{gam, concordia_fast21, grappa_atc15} to handle inter-host conflicts and maintain data consistency, while intra-host maintains them transparently in hardware cache coherence. 

% , which in turn exacerbates the performance gap.

% Unlike CPU cache protocols, which rely on hardware invalidation-based coherence, network architectures do not inherently maintain coherence across remote load stores. 

% Faced with such a large network-induced communication cost, 
% the research and industrial community have devised a bunch of networking improvement techniques. For instance, firmware vendors have introduced user-level network stacks to bypass operating systems~\cite{mtcp_nsdi14}, offload protocol-processing logic into smart network interface cards (SmartNICs)~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, removed remote CPU cycles via single-sided Remote Direct Memory Access (RDMA)~\cite{farm, myth_vldb17}, and enabled direct access to the CPU's last-level cache through Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc}. Compiler advancements have also incorporated efficient context switching mechanisms via co-routines~\cite{fasst, drtmh, grappa_atc15} to tolerate the high latency associated with I/O operations. Applications also share connection data structures~\cite{fasst} and batch small packets~\cite{grappa_atc15, guideline_atc16} to further optimize performance. 


Given the high communication costs, various networking enhancement techniques have emerged, including but not limited to user-level network stacks bypassing OSs~\cite{mtcp_nsdi14}, offloading protocol processing to SmartNICs~\cite{xenic_sosp21, ipipe_sigcomm19, linefs_sosp21}, remote direct memory access (RDMA)~\cite{farm, myth_vldb17}, and Data Direct I/O (DDIO)~\cite{ddio_ispass20, ddio-doc}. Compiler improvements also utilize co-routines~\cite{fasst, drtmh, grappa_atc15} for efficient context switching, and applications have adopted shared connection structures~\cite{fasst} and packet batching~\cite{grappa_atc15, guideline_atc16} to optimize performance.


% A high-performance distributed transaction system thus necessitates an intricate combination of these techniques, in order to manage the gap between inter-host and intra-host communication. 
% For instance, scholarly discourse on the optimal choice of Remote Direct Memory Access (RDMA) primitives for transaction processing~\cite{farm, fasst, grappa_atc15, compromise, drtmh, drtm, storm_systor19, prism_sosp21, guideline_atc16}â€”specifically the trade-offs between one-sided and two-sided operationsâ€”has non-travail conclusions, and the best-practice always depends on many variables such as cluster scales, firmware generations, and application-specific data access patterns~\cite{drtmh, prism_sosp21, guideline_atc16}. Similar deliberations extend to other networking techniques as well~\cite{smartnic_osdi23, xenic_sosp21}. 
% This unfortunately lead to high software complexity and high hardware mitigation cost. Besides, even these advanced methods cannot fully amortize the intrinsic performance disadvantages that networks impose when compared to single-host architectures. 

Achieving a high-performance distributed transaction system thus necessitates an intricate combination of these techniques. The optimal choice of RDMA primitives for transaction processing remains debated~\cite{farm, fasst, grappa_atc15, compromise, drtmh, drtm, storm_systor19, prism_sosp21, guideline_atc16}, with the best-practice depending on cluster scales, firmware versions, and data access patterns~\cite{drtmh, prism_sosp21, guideline_atc16}. Such discussions apply to other networking techniques as well~\cite{smartnic_osdi23, xenic_sosp21}. This lead to high system complexity and high hardware mitigation cost. Unfortunately, even these sophisticated methods can't fully compensate for the performance disadvantages of inter-host networks. 


\begin{figure}[t]
  \centering 
  \includegraphics[width=0.48\textwidth]{my/figs/rdma-vs-cxl.pdf}
  \caption{The typical CXL-attached shared memory architecture.}
  \label{fig:motivation-rdma-vs-cxl}
\end{figure}