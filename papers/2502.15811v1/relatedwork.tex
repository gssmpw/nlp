\section{Related Work}
\subsection{Spiking Neural Networks and Transformers}


There are typically three ways to address the challenge of the non-differentiable spike function: (1) Spike-timing-dependent plasticity (STDP) schemes~\cite{bi1998synaptic}. (2) converting trained ANNs into equivalent SNNs using neuron equivalence, i.e., ANN-to-SNN conversion schemes~\cite{hu2023fast,wang2023masked}. (3) Training SNNs directly~\cite{guo2023rmp} using surrogate gradients. STDP is a biology-inspired method but is limited to small-scale datasets. 
Spiking neurons are the core components of SNNs, with common types including Integrate-and-Fire (IF)~\cite{bulsara1996cooperative} and Leaky Integrate-and-Fire (LIF)~\cite{gerstner2002spiking}. IF neurons can be seen as ideal integrators, maintaining a constant voltage in the absence of spike input. LIF neurons build on IF neurons by adding a voltage decay mechanism, which more closely approximates the dynamic behavior of biological neurons. In addition to IF and LIF neurons, Exponential Integrate-and-Fire (EIF)~\cite{brette2005adaptive} and Parametric Leaky Integrate-and-Fire (PLIF)~\cite{fang2021incorporating} neurons are also commonly used models. These neurons better simulate the dynamic characteristics of biological neurons. 

Various studies have explored Transformer-based SNNs that fully leverage the unique advantages of SNNs~\cite{kai2024evtexture}. Spikformer~\cite{zhouspikformer} firstly converts all components of ViT~\cite{dosovitskiy2020image} into spike-form. Spike-driven Transformer~\cite{yao2024spike} advances further by introducing the spike-driven paradigm into Transformers. Spikingformer~\cite{zhou2023spikingformer} proposes a hardware-friendly spike-driven residual learning architecture. In this work, we extend the Transformer-based SNNs from 2D images to 3D point clouds while employing efficient direct training methods.

\subsection{Deep Learning on Point Cloud}

% snn的点云算法

Deep neural network architectures for understanding point cloud data can be broadly classified into projection-based~\cite{lang2019pointpillars,chen2017multi}, voxel-based~\cite{song2017semantic}, and point-based methods~\cite{marethinking,zhao2019pointweb}. Projection-based methods project 3D point clouds onto 2D image planes, using a 2D CNN-based backbone for feature extraction. Voxel-based methods convert point clouds into voxel grids and apply 3D convolutions. Pioneering point-based methods like PointNet use max pooling for permutation invariance and global information extraction~\cite{qi2017pointnet}, while PointNet++ introduces hierarchical feature learning~\cite{qi2017pointnet++}. Recently, point-based methods have shifted towards Transformer-based architectures~\cite{zhao2021point,park2022fast,wu2022point,wu2024point}. The self-attention mechanism of the point transformer, insensitive to input order and size, is applied to each point's local neighborhood, crucial for processing point clouds.

Wu et al. construct a point-to-spike residual classification network by stacking 3D spiking residual blocks and combining spiking neurons with conventional point convolutions~\cite{wu2024pointsnn}. Spiking PointNet, the first SNN framework for point clouds, proposes a trained-less but learning-more paradigm based on PointNet~\cite{ren2024spiking}. It adopts direct encoding of point clouds, repeating over time steps, making it hard to train point clouds with large time steps. Due to these limitations, further accuracy improvement is challenging. To address this, we propose a transformer-based SNN framework and design Q-SDE, significantly saving computational costs, enabling training in multiple time steps, and achieving higher accuracy.



\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\linewidth]{SPTv1.1.0.jpg}
    \includegraphics[width=1.0\linewidth]{240810aaai_1.pdf}
    \caption{The overview of Spiking Point Transformer (SPT), which consists of Queue-Driven Sampling Direct Encoding (Q-
    SDE), MLP Module for adaptive learning, Spiking Point Encoder Module for feature interaction and Classification Head.}
    \label{fig:1}
\end{figure*}