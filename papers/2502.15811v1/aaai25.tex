%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[submission]{aaai25}
\pdfoutput=1
\usepackage{aaai25}
% DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}  % 用于数学符号
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage{color} 
\usepackage{booktabs}
\usepackage{graphicx}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Spiking Point Transformer for Point Cloud Classification}
\author {
    % Authors
    Peixi Wu\textsuperscript{\rm 1}\thanks{These authors contributed equally.},
    Bosong Chai\textsuperscript{\rm 3}\footnotemark[1], 
    Hebei Li\textsuperscript{\rm 1}, 
    Menghua Zheng\textsuperscript{\rm 4}, 
    Yansong Peng\textsuperscript{\rm 1}, 
    Zeyu Wang\textsuperscript{\rm 3}, 
    Xuan Nie\textsuperscript{\rm 5},
    Yueyi Zhang\textsuperscript{\rm 1}\thanks{Corresponding authors.}, 
    Xiaoyan Sun\textsuperscript{\rm 1,\rm 2}\footnotemark[2]
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China\\
    \textsuperscript{\rm 2}Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\\
    \textsuperscript{\rm 3}College of Computer Science and Technology, Zhejiang University~~ 
    \textsuperscript{\rm 4}Tsingmao Intelligence\\
    \textsuperscript{\rm 5}School of Software, Northwestern Polytechnical University\\
    
    % wupeixi@mail.ustc.edu.cn, chaibosong@mail.zju.edu.cn, thirdAuthor@affiliation1.com
    wupeixi@mail.ustc.edu.cn, \{zhyuey, sunxiaoyan\}@ustc.edu.cn\\
    % \small \texttt{\{chaibosong, wangzeyu2020\}@zju.edu.cn}\\
    
    }


%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Spiking Neural Networks (SNNs) offer an attractive and energy-efficient alternative to conventional  Artificial Neural Networks (ANNs) due to their sparse binary activation. When SNN meets Transformer, it shows great potential in 2D image processing. However, their application for 3D point cloud remains underexplored. 
To this end, we present Spiking Point Transformer (SPT), the first transformer-based SNN framework for point cloud classification. 
Specifically, we first design Queue-Driven Sampling Direct Encoding for point cloud to reduce computational costs while retaining the most effective support points at each time step. We introduce the Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF), designed to simulate selective neuron activation and reduce over-reliance on specific artificial neurons. SPT attains state-of-the-art results on three benchmark datasets that span both real-world and synthetic datasets in the SNN domain. Meanwhile, the theoretical energy consumption of SPT is at least 6.4$\times$ less than its ANN counterpart. 
% The code will be available at
% https://github.com/PeppaWu/Spiking-Point-Transformer. 
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.

\begin{links}
    \link{Code}{https://github.com/PeppaWu/SPT}
    % \link{Datasets}{https://aaai.org/example/datasets}
    % \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}

\section{Introduction}

Bio-inspired Spiking Neural Networks (SNNs) are regarded as the third generation of neural networks~\cite{maass1997networks}. 
In SNNs, spiking neurons transmit information through sparse binary spikes, where a binary value of 0 denotes neural quiescence and a binary value of 1 denotes a spiking event. Neurons communicate via sparse spike signals, with only a subset of spiking neurons being activated to perform sparse synaptic accumulation (AC), while the rest remain idle. 
Their high biological plausibility, sparse spike-driven communication~\cite{roy2019towards}, and low power consumption on neuromorphic hardware~\cite{pei2019towards} make them a promising alternative to traditional AI for achieving low-power, efficient computational intelligence~\cite{schuman2022opportunities}.  


Drawing on the success of Vision Transformers~\cite{dosovitskiy2020image}, researchers have combined SNNs with Transformers, achieving significant performance improvements on the ImageNet benchmark~\cite{shi2024spikingresformer,zhou2024spikformer,yao2024spike} and in various application scenarios~\cite{yu2024spikingvit,ouyang4706194spiking}. A question is naturally raised: can transformer-based SNNs be adapted to the 3D domain while maintaining their energy efficiency and fully leveraging the ability of transformers? To this end, we present Spiking Point Transformer (SPT), the first spiking neural network based on transformer architecture for deep learning on point cloud. 



The successful application of transformer-based traditional artificial neural networks (ANNs) in the 3D point cloud domain has been widely demonstrated~\cite{zhao2021point,park2022fast,wu2022point,wu2024point}. Since point clouds are collections embedded in 3D space, the core self-attention operator in Transformer networks is in essence a set operator which is invariant to the permutation and number of input elements, making it highly suitable for processing point cloud data. Considering the computational costs, point cloud transformers cannot perform global attention. The Point Transformer series~\cite{zhao2021point,wu2022point} calculates local self-attention within the k-nearest neighbors (KNN) neighborhood. In order to integrate this self-attention operation with SNNs, we follow the design of spiking self-attention~\cite{yao2024spike, li2024deep} and employ a spiking local self-attention mechanism to model sparse point cloud using spike Query, Key, and Value. By using AC operations instead of numerous multiply accumulate (MAC) operations, we significantly reduce the energy consumption of self-attention computations for 3D point cloud. 


Training point cloud networks requires more expensive memory and computational costs than images because point cloud data requires more dimensions to describe itself. Researchers have proposed various optimization strategies, including sparse convolutions~\cite{choy20194d}, optimization
during the data processing phase~\cite{hu2020randla}, and local feature extraction~\cite{marethinking}. If the existing direct encoding methods used by transformer-based SNNs~\cite{zhou2024spikformer,yao2024spike} for 2D static images or used by SNNs for 3D point clouds~\cite{ren2024spiking,wu2024pointsnn} are directly applied to the Transformer structure for point cloud, the training of SNNs with multiple time steps will result in a sharp increase in computational costs. Point cloud data is high-dimensional but has low information density. The current direct encoding methods for point clouds means we need to repeat T times along the temporal dimension. A clear approach is to consider whether we can split the point set across T time steps instead. To this end, we propose Queue-Driven Sampling Direct Encoding (Q-SDE), an improved direct encoding method for point cloud. Our method efficiently covers the original point cloud information through First-in, First-out (FIFO) sampling mechanism while maintaining certain key supporting points unchanged. 


Many studies~\cite{niiyama2023microglia,sakai2020synaptic} have shown that during brain development, neurons undergo a use it or lose it process, where neural circuits are remodeled to prune excessive or incorrect neurons. Inspired by this, we fuse different neural dynamic models to simulate neuronal pruning and selective activation of neurons in biological brains through divide-and-conquer and gating mechanisms, which is referred to as Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF) and placed in some critical position within the network. 
Our main contributions can be summarized as follows: 
\begin{itemize}
    \item We build a Spiking Point Transformer (SPT), which is the first transformer-based SNN framework for point cloud classification that significantly reduces energy consumption. 
    \item We design Queue-Driven Sampling Direct Encoding (Q-SDE), an improved SNN direct encoding method for point cloud that slightly enhances accuracy while significantly reducing memory usage. 
    \item We propose a Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF) to effectively integrate multiple neural dynamic mechanisms and simulate the selective activation of biological neurons. 
    \item The performance on two benchmark datasets ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting} demonstrates the effectiveness of our method and achieves a new state-of-the-art in the SNN domain. 
\end{itemize}

\section{Related Work}
\subsection{Spiking Neural Networks and Transformers}


There are typically three ways to address the challenge of the non-differentiable spike function: (1) Spike-timing-dependent plasticity (STDP) schemes~\cite{bi1998synaptic}. (2) converting trained ANNs into equivalent SNNs using neuron equivalence, i.e., ANN-to-SNN conversion schemes~\cite{hu2023fast,wang2023masked}. (3) Training SNNs directly~\cite{guo2023rmp} using surrogate gradients. STDP is a biology-inspired method but is limited to small-scale datasets. 
Spiking neurons are the core components of SNNs, with common types including Integrate-and-Fire (IF)~\cite{bulsara1996cooperative} and Leaky Integrate-and-Fire (LIF)~\cite{gerstner2002spiking}. IF neurons can be seen as ideal integrators, maintaining a constant voltage in the absence of spike input. LIF neurons build on IF neurons by adding a voltage decay mechanism, which more closely approximates the dynamic behavior of biological neurons. In addition to IF and LIF neurons, Exponential Integrate-and-Fire (EIF)~\cite{brette2005adaptive} and Parametric Leaky Integrate-and-Fire (PLIF)~\cite{fang2021incorporating} neurons are also commonly used models. These neurons better simulate the dynamic characteristics of biological neurons. 

Various studies have explored Transformer-based SNNs that fully leverage the unique advantages of SNNs~\cite{kai2024evtexture}. Spikformer~\cite{zhouspikformer} firstly converts all components of ViT~\cite{dosovitskiy2020image} into spike-form. Spike-driven Transformer~\cite{yao2024spike} advances further by introducing the spike-driven paradigm into Transformers. Spikingformer~\cite{zhou2023spikingformer} proposes a hardware-friendly spike-driven residual learning architecture. In this work, we extend the Transformer-based SNNs from 2D images to 3D point clouds while employing efficient direct training methods.

\subsection{Deep Learning on Point Cloud}

% snn的点云算法

Deep neural network architectures for understanding point cloud data can be broadly classified into projection-based~\cite{lang2019pointpillars,chen2017multi}, voxel-based~\cite{song2017semantic}, and point-based methods~\cite{marethinking,zhao2019pointweb}. Projection-based methods project 3D point clouds onto 2D image planes, using a 2D CNN-based backbone for feature extraction. Voxel-based methods convert point clouds into voxel grids and apply 3D convolutions. Pioneering point-based methods like PointNet use max pooling for permutation invariance and global information extraction~\cite{qi2017pointnet}, while PointNet++ introduces hierarchical feature learning~\cite{qi2017pointnet++}. Recently, point-based methods have shifted towards Transformer-based architectures~\cite{zhao2021point,park2022fast,wu2022point,wu2024point}. The self-attention mechanism of the point transformer, insensitive to input order and size, is applied to each point's local neighborhood, crucial for processing point clouds.

Wu et al. construct a point-to-spike residual classification network by stacking 3D spiking residual blocks and combining spiking neurons with conventional point convolutions~\cite{wu2024pointsnn}. Spiking PointNet, the first SNN framework for point clouds, proposes a trained-less but learning-more paradigm based on PointNet~\cite{ren2024spiking}. It adopts direct encoding of point clouds, repeating over time steps, making it hard to train point clouds with large time steps. Due to these limitations, further accuracy improvement is challenging. To address this, we propose a transformer-based SNN framework and design Q-SDE, significantly saving computational costs, enabling training in multiple time steps, and achieving higher accuracy.



\begin{figure*}[t]
    \centering
    % \includegraphics[width=0.9\linewidth]{SPTv1.1.0.jpg}
    \includegraphics[width=1.0\linewidth]{240810aaai_1.pdf}
    \caption{The overview of Spiking Point Transformer (SPT), which consists of Queue-Driven Sampling Direct Encoding (Q-
    SDE), MLP Module for adaptive learning, Spiking Point Encoder Module for feature interaction and Classification Head.}
    \label{fig:1}
\end{figure*}

\section{Method}

In this paper, we propose a Spiking Point Transformer (SPT) for 3D point cloud classification, integrating the spiking paradigm into Point Transformer.
First, we perform Queue-Driven Sampling Direct Encoding (Q-SDE) on the point cloud. Then, we preliminarily encode the membrane potential with an MLP Module and a Spiking Point Transformer Block (SPTB).
Next, further encoding is done through $L$ Spiking Point Encoder Modules, mainly including Spiking Transition Down Block (STDB) for downsampling and SPTB for feature interaction.
Finally, membrane potential is sent to Classification Head to output the prediction.

% We introduce the core components of the SPT one by one, which mainly include the Queue-Driven Sampling Direct Encoding (Q-SDE), Spiking Point Encoder Module, and Hybrid Dynamics Integrate-and-Fire Neuron (HD-IF). 


\subsection{Queue-Driven Sampling Direct Encoding}

Most of the high-performance SNN studies~\cite{zhou2024spikformer,yao2024spike,ren2024spiking} are based on direct encoding. Direct encoding is to repeat the input $T$ times along the time dimension, which incurs expensive computational costs. We design an encoding method suitable for point clouds, which is an improved direct encoding called Queue-Driven Sampling Direct Encoding (Q-SDE). Q-SDE uses a first-in, first-out queue-driven sampling method to retain the most effective support points of the original points at different time steps, while reducing computational costs. 

The original point queue \(P\) has a shape of \((N, C_0)\). We initialize the encoded multi-time-step point matrix \(P_e\) with a shape of \((T, N_s, C_0)\). \(T\) represents the number of time steps, \(N_s\) represents the number of sampled points per time step, and \(C_0\) represents the number of feature dimensions per point. 

As shown in Figure \ref{fig:1}, through furthest point sampling (FPS), \(N_s\) points are extracted from \(P\) and stored in the first time step of \(P_e\). The sampled points at first time step contain the object's key contours but lacks the \(N-N_s\) points which are unsampled, which are crucial for recognizing difficult objects. Subsequent time step sampling should efficiently cover the unsampled points.

\begin{algorithm}[t]
\caption{Queue-Driven Sampling Direct Encoding}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Point queue $P$, Sample number  $N_s$, Timestep  $T$
\STATE \textbf{Output:} Encoded point matrix $P_e$
\STATE \(  {N}_{p} = \left\lfloor  ( {N - {N}_{s} )/\left( {T - 1}\right) }\right\rfloor \) \hspace{0.03cm} $\vartriangleright$  Initialize $N_p$, points dequeued per timestep
\STATE \(  {P}_{e}[0] \gets \text{FPS}(P,N_s) \)  \hspace{0.03cm} $\vartriangleright$  
Set ${P}_{e}[0]$, denotes the first timestep point cloud

    \FOR{$i = 1, 2, 3, \ldots, T-1$}
        \STATE $\vartriangleright$  Remaining Point Check
        \IF{$P$ $\setminus$  ${P}_{e}[i-1]$ is empty}
            \STATE \( P_e[i] \gets P_e[i-1] \)  \hspace{2.17cm} $\vartriangleright$ Coverage
        \STATE \hspace{-0.45cm} $\vartriangleright$  Queue-driven Sample
        \ELSE
            \STATE \( S \gets \left\{ {P}_{e}[i-1][j] \mid j \geq N_p \right\} \) \hspace{0.56cm} $\vartriangleright$ Subset
            \STATE \( F \gets \text{FPS}(P \setminus {P}_{e}[i-1], N_p) \) \hspace{0.53cm} $\vartriangleright$ Sample
            \STATE \( {P}_{e}[i] \gets S \cup F \) \hspace{2.58cm} $\vartriangleright$ Merge
            \STATE \( P \leftarrow P \setminus \left\{ {P}_{e}[i-1][j] \mid j < N_p  \right\} \) $\vartriangleright$ Update
    \ENDIF
\ENDFOR
\end{algorithmic}
\label{alg_1}
\end{algorithm}

The specific approach is to dequeue the first \(N_p\) points referred to as discarded points from \(P\), then use FPS to select \(N_p\) points called sampling points from the unsampled points, and concatenate these points with the first \(N - N_p\) points of \(P\). The resulting point cloud data is stored in the next time step of \(P_e\). This process of dequeuing and concatenation is repeated $T-1$ times.


\(N_p\) represents the number of points to be dequeued at each time step. When \( T>1 \), to ensure that the number of remaining points in $P$ at the final time step is not less than \(N_s\), while minimizing the number of unused points, the following constraints must be satisfied:

\begin{equation}
\footnotesize
N_p = \left\lfloor \frac{N - N_s}{T - 1} \right\rfloor, \hspace{0.2cm} T>1
\end{equation}


When $T=1$, the first time step of $P_e$ is also the only time step that stores all points in $P$. Together, the main steps of Q-SDE are summaried in Algorithm 1.



\subsection{Spiking Point Encoder Module}
As shown in Figure~\ref{fig:1}, Spiking Point Encoder Module is the main component of the whole architecture, which contains the Spiking Transition Down Block (STDB) and Spiking Point Transformer Block (SPTB). 

\subsubsection{Spiking Transition Down Block.}
STDB is employed for spatial downsampling of point clouds to expand the spatial receptive field. Specifically, it involves obtaining a new spatial point cloud $P_l$ and its corresponding membrane potential features $U_l$ through FPS. 
We then utilize K-nearest neighbors (KNN) sampling to extract the features of the nearest points for each point in the new point cloud and project these features into a higher-dimensional space after spiking neuron firing.
Finally, by using LocalMaxPooling (LAP), we aggregate the local features $F$ from the neighborhood of spatial point cloud $P_l$ onto the membrane potential features $U_l'$. STDB can be expressed as: 
\begin{align}
&F_{l-1} = \{P_{l-1}, U_{l-1}\} \\
&F_l = {\rm{FPS}}(F_{l-1}, N_l) \\
&F \hspace{0.07cm}= {\rm{KNN}}(F_l, F_{l-1}, N_k) \\
&U_l' \hspace{-0.02cm}= {\rm{LAP}}({\rm{MLP}}(\mathcal{SN}(F))) 
\end{align}

where $N_l$ is the number of points in the $l$-th layer, $N_k$ is the number of sampled points in the neighborhood. $\mathcal{SN}(\cdot)$ represents the spiking neuron. KNN($\mathcal{A}$, $\mathcal{B}$, $N_k$) denotes sampling the $N_k$ nearest points from point set $\mathcal{B}$ to point set $\mathcal{A}$ through KNN. 

$P_l$, $U_l$ are features in ${\mathbb{R}}^{T \times N_l \times 3}$ and ${\mathbb{R}}^{T \times N_l \times C_l}$ respectively, representing the position information and membrane potential feature information of the point cloud in the $l$-th layer. $F$ represents the KNN neighborhood membrane potential feature of $F_l$. $F_l$ represents the union of $P_l$ and $U_l$, which belongs to ${\mathbb{R}}^{T \times N_l \times (3+C_l)}$. 


\subsubsection{Spiking Point Transformer Block.}

SPTB further encodes the membrane potential feature $U_l'$, and conducts extensive information interaction at a more advanced semantic level, so that the feature carried by each point can better represent the local points, thereby achieving better shape classification.


The specific implementation of SPTB, as shown in Figure~\ref{fig:1}, begins with the preliminary encoding of the spike signals $S_l'$ input by HD-IF. 
Then, by using KNN sampling, the $N_k$ point neighborhood features of $P_l$ are indexed, and these features are encoded to obtain spike Query and Value. 
Moreover, the input spike $S_l''$ is further encoded to obtain the spike Key. The learnable relative position encoding is performed on $P_l$ and its neighborhood. They are aggragated according to the methodology proposed by Point Transformer~\cite{zhao2021point}. Finally, output encoding is performed and membrane potential interaction is conducted through residual connection. SPTB can be written as follows: 
\begin{align}
& S_l'' = \mathcal{SN}({\rm{MLP}}(S_l’)) \\
& K \hspace{0.1cm}= \mathcal{SN}({\rm{MLP}}(S_l'')) \\
& Q, V = \mathcal{SN}({\rm{MLP}}({\rm{KNN}}(S_l'', N_k))) \\
& \delta \hspace{0.12cm}= \mathcal{SN}({\rm{MLP}}({\rm{KNN}}(P_l, N_k) - P_l)) \\
\small
& U_l'' = \mathop{\sum }\limits_{{\mathcal{X}}} \rho \left( \gamma \left( \beta \left(Q, K \right) + \delta \right) \right) \odot (V + \delta)  \label{eq:attn} \\
& U_l = {\rm{MLP}}(\mathcal{SN}(U_l'')) + U_l'
\end{align}

where $\delta$ represents relative position encoding. ${\mathcal{X}}$ represents the $N_k$ point neighborhood. \( \beta  \) is a relation function (e.g., subtraction), $\rho$ is a normalization function, and \( \gamma  \) is a mapping function (e.g., MLP with ${\mathcal{SN}}$) that produces attention vectors for feature aggregation. KNN($\mathcal{A}$, $N_k$) denotes sampling the $N_k$ nearest points from point set $\mathcal{A}$ to itself.

% $U_l^{\prime\prime}$ belongs to ${\mathbb{R}}^{T \times N_l \times C_l}$and represents the membrane potential features after local feature interaction. 
% $S_l^{\prime\prime}$ belongs to $\{0,1\}^{T \times N_l \times C_l}$ represents the spike after the preliminary encoding in the $l$-th layer.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.42\linewidth}
        \centering
        \includegraphics[width=\linewidth]{HD-IFa.pdf}
        % \caption{}
        \centerline{(a) HD-IF structure}
    \end{subfigure}
    \begin{subfigure}{0.57\linewidth}
        \centering
        \includegraphics[width=\linewidth]{HD-IFb.pdf}
        % \caption{}
        \centerline{(b) Neuronal membrane potential }
    \end{subfigure}
    \caption{(a) The main structure of HD-IF integrating neuronal membrane potential and firing. (b) The membrane potential of different neurons with 0.4 input and 0.5 threshold.}
    \label{fig:HDIF}
\end{figure}

\subsection{Hybrid Dynamics Integrate-and-Fire Neuron}
The spiking neuron model is simplified from the biological neuron model. In this paper, we uniformly adopt the LIF for ${\mathcal{SN}}$ function. Meanwhile, we design HD-IF which integrate different neuronal dynamic models, including 
LIF~\cite{gerstner2002spiking},
IF~\cite{bulsara1996cooperative}, EIF~\cite{brette2005adaptive}, and PLIF~\cite{fang2021incorporating} and place it before each SPTB. 

We begin by briefly revisiting their dynamic characteristics. Figure \ref{fig:HDIF}(b) shows that the IF neuron acts as an ideal integrator, with membrane potential changing through input accumulation. The LIF neuron is IF neuron with leakage, where the membrane potential gradually approaches the input with input and returns to the resting state without input. The EIF neuron is a nonlinear LIF model. It adds an exponential term to the LIF model to simulate the sudden jump in potential near the firing threshold.  The PLIF neuron adds a learnable membrane time constant $\tau$, dynamically adjusted by the parameter $w$ via $\text{Sigmoid}(w)$ function. The detailed equations for each neuron can be found in the Appendix.A. 

Then, we introduce a novel HD-IF neuron, which aims to promote competition among different neurons by selectively activating suitable neurons and fusing their dynamic characteristics to generate membrane potential spikes. This hybrid design effectively reduces over-reliance on specific artificial neurons and enhances the robustness of SNNs. 

The HD-IF neuron is embedded before each SPTB to optimize the dynamic behavior of the spiking neural network. Specifically, the HD-IF neuron processes the membrane potential $U_l’$ of STDB and outputs the spike $S_l'$, as shown in Figure~\ref{fig:HDIF}(a). First, the temporal dimension and feature dimension of the membrane potential $U_l'$ is combined to create an input feature with spatial and temporal dual features. Then, a gate network calculates weights for membrane potential generated by various neurons at different spatial points. During training, the model adjusts neuron responses through dense propagation and weighted summation. During inference, the Top-2 neural models are selected to reduce computational complexity and improve efficiency. Finally, the Heaviside function fires the mixed membrane potential to produce the spike sequence $S_l'$.

\section{Experiments}
\subsection{Experimental Settings}

\subsubsection{Datasets.}


We evaluate the performance of 3D point cloud classification on the synthetic dataset ModelNet40~\cite{wu20153d} 
and the real dataset ScanObjectNN (Uy et al. 2019).
ModelNet40 contains 40 different object categories, each of
which contains approximately 12,311 CAD models across 40 different categories. The training set contains 9,843 instances, and the testing set contains 2,468 instances. 
ModelNet10 is a subset of ModelNet40. The training set contains 3,991 instances, and the testing set contains 908 instances.
ScanObjectNN is constructed from real-world scans, characterized by varying degrees of data missing and noise contamination. The entire dataset consists of 3D objects from 15 categories, with 11,416 samples as a training set and 2,882 samples as a testing set.
% In terms of the evaluation metrics, we report the overall accuracy (OA) and class-average accuracy (mAcc) on the testing set. 
% We employ ins. Acc and cat. Acc as metrics to comprehensively evaluate the model's performance on the testing set. Specifically, ins. Acc denotes the overall accuracy, while cat. Acc represents the class-average accuracy.

\subsubsection{Implementation Details.}

We implement the Spiking Point Transformer in PyTorch 1.13~\cite{paszke2019pytorch} on 2 × RTX 3090Ti GPUs. SPT is developed using the SpikingJelly framework\footnote{https://github.com/fangwei123456/spikingjelly}~\cite{spikingjelly} based on PyTorch. We use the AdamW optimizer with momentum and weight decay set to 0.9 and 0.0001, respectively. The initial learning rate is set to 0.001 and is decreased by a factor of 0.3 every 50 epochs.The number of input point cloud points $N$ is set to 1024. For all our SNN models, we set $V_{th}$ as 0.5 for fair comparison with Spiking Pointnet~\cite{ren2024spiking}. The remaining hyperparameters are consistent with those used in the Point Transformer~\cite{zhao2021point}. We conducted iterative training on the entire dataset for 200 epochs. 



\begin{figure}[t]
    \centering
    % \includegraphics[width=1\linewidth]{.jpg}
    \includegraphics[width=1.0\linewidth]{Q-SDE.pdf}
    \caption{Visualization of support points and points at each time step. Support points repeated across most time steps capture the essence of the object shape. Blue points are  the enqueue points while red points are the dequeue points.}
    \label{fig:3}
\end{figure}


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{cccc}
        \toprule
        {Time} & {ModelNet10 } & {ModelNet40 } & {ScanObjectNN }\\
       Step & OA(\%) & OA(\%) & OA(\%)\\
        \midrule
        % \hline
        1 & 94.35 & 90.87 & 76.33\\
        2 & 94.29 & 91.13 & 77.03\\
        3 & 94.54 & 91.38 & 77.51\\
        \textbf{4} & \textbf{94.76} & \textbf{91.43} & \textbf{78.03}\\
        \bottomrule
    \end{tabular}
    \caption{Ablation study of time step on ModelNet10/40 and ScanObjectNN.}
    \label{tab:timestep}
\end{table}

\subsection{Experimental Results}

In this experiment, we evaluate our model's performance using two metrics: overall accuracy (OA) and  mean class accuracy (mAcc). 
These metrics provide a comprehensive assessment of our model on the test set.

\subsubsection{ModelNet10/40 Dataset.}
From Table~\ref{fig:main}, we can see that our SPT model shows superior performance on both ModelNet10 and ModelNet40 datasets. In the SNN domain, the SPT model achieves the highest accuracy, surpassing the SNN baselines. Specifically, on ModelNet40, SPT attains 91.43\% OA and 89.39\% mAcc, reflecting a 0.83\% and 0.19\% improvement over P2SResLNet-B respectively. On ModelNet10, SPT significantly outperforms Spiking Pointnet, with 94.76\% OA and 93.69\% mAcc, reflecting a 1.45\% improvement in OA. In the ANN domain, while the SPT model's accuracy on ModelNet40 is slightly lower than Point Transformer, it even surpasses the ANN baseline on ModelNet10, with 94.76\% OA and 93.69 \% mAcc, relecting 0.48\% improvement in OA.

\begin{table*}[t]
\small
\centering
% \begin{adjustbox}{max width=1.0\textwidth}
\label{tab_1}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule

\multirow{3}{*}{\vspace{1.2mm} Methods} & \multirow{3}{*} {\vspace{1.2mm} Type} & \multirow{2}{*}{\vspace{1.5mm} Time} & \multicolumn{2}{c}{ModelNet10} & \multicolumn{2}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN} \\ 

% \cline{4-9}\hspace{10pt}
\cmidrule(l{3pt}r{3pt}){4-5}\cmidrule(l{3pt}r{3pt}){6-7}\cmidrule(l{3pt}r{3pt}){8-9}
& & Step & OA(\%) & mAcc(\%) & OA(\%) & mAcc(\%) & OA(\%) & mAcc(\%) \\ 
% \midrule

% & & Step & (\%) & (\%) & (\%) & (\%) & (\%) & (\%) \\ [1.5pt]
\midrule
% \multicolumn{8}{c}{\hspace{26mm} \textit{ANN Domain}} \\ 
% \midrule
% \hline
PointNet & ANN & - & 92.98 & - & 89.20 & 86.00 & 68.20 & 63.40 \\ 
PointNet++ & ANN & - & - & - & 92.00 & 89.10 & 77.90 & 75.40 \\ 
Point Transformer$^*$ & ANN & - & 94.28 & 94.01 & 91.73 & 89.56 & 81.32 & 80.34 \\ 
PointMLP & ANN & - & - & - & 94.10 & 91.50 & \hspace{1mm} 85.40\raisebox{1ex}{\(\diamond\)} & \hspace{1mm} 83.90\raisebox{1ex}{\(\diamond\)}  \\ 
% \midrule
% \multicolumn{8}{c}{\hspace{26mm} \textit{SNN Domain}} \\ 
% \midrule
KPConv-SNN & ANN2SNN & 40 & - & - & 70.50 & 67.60 & 43.90 & 38.70 \\ 
Spiking Pointnet & SNN & 4 & 93.31 & - & 88.61 & - & \hspace{1mm} 64.04$^*$ & \hspace{1mm} 60.14$^*$ \\ 
P2SResLNet-B & SNN & 1 & - & - & 90.60 & 89.20 & 74.46$^*$/81.20\raisebox{1ex}{\(\diamond\)} & 72.58$^*$/79.40\raisebox{1ex}{\(\diamond\)}   \\ 

SPT(Q-SDE512) & SNN & 4 & 94.66 & 93.54 & \textbf{91.43} & \textbf{89.39} & \hspace{-1mm} 76.51 \hspace{0.01mm}/\hspace{0.05mm} 80.02\raisebox{1ex}{\(\diamond\)} & 74.53 \hspace{0.01mm}/\hspace{0.05mm} 78.12\raisebox{1ex}{\(\diamond\)} \\ 

SPT(Q-SDE768) & SNN & 4 & \textbf{94.76}  & \textbf{93.69} & 91.22 & 88.45 & \textbf{78.03} \hspace{0.01mm}/\hspace{0.05mm} \textbf{82.23}\raisebox{1ex}{\(\diamond\)} & \textbf{75.87} \hspace{0.01mm}/\hspace{0.05mm} \textbf{80.12}\raisebox{1ex}{\(\diamond\)} \\ 
\bottomrule
\end{tabular}
}
\caption{Performance comparison with the baseline methods. 
The best results in the SNN domain are presented in bold, with * indicating self-reproduced results and 
\(\diamond\) indicating results based on test voting.}
\label{fig:main}
\end{table*}

\subsubsection{ScanObjectNN Dataset.}
From Table~\ref{fig:main}, we can see that our SPT model still achieves the state-of-the-art performance in the SNN domain. Specifically, the SPT model attains 78.03\% OA without voting, reflecting a 3.57\% improvement over P2SResLNet-B, and 82.23\% OA  with voting, reflecting a 1.03\% improvement over P2SResLNet-B. In the ANN domain, the SPT model’s accuracy is slightly lower compared to Point Transformer without voting. Considering the theoretical energy consumption, our model provides a proper balance between classification accuracy and spike-based biological characteristics.



\subsection{Ablation Study}

\subsubsection{Ablation on Time Step.}
In our ablation study on time step, we observe a significant difference compared to previous models like Spiking PointNet and P2SResLNet-B. These models typically show a trend that longer time steps bring either reduced or stable accuracy. However, as illustrated in Table~\ref{tab:timestep}, our model basically improves accuracy with longer time steps, consistent with findings in 2D image classification~\cite{fang2021deep}.

Unlike 2D image, 3D point cloud is highly sparse. For direct encoding method, longer time steps may mean more redundancy rather than more useful information. As shown in Figure~\ref{fig:3}, our model improves this by modifying direct encoding so that each time step contains only a subset of the initial point cloud $P$.
The point cloud at each time step may look similar which maintains the repetitiveness of direct encoding, but there is a difference of $N_p$ points between them which exploits the dynamic characteristics of neurons to leverage longer time steps effectively.

However, excessively long time steps are impractical due to expensive memory and computational cost~\cite{wu2024pointsnn}. Therefore, we set the maximum time step to 4 in our ablation study. Table~\ref{tab_4} shows that the optimal accuracy at each time step. We can see that OA improves with longer time steps, reaching a peak of 91.43\% at 4 time steps on the ModelNet40 dataset and 78.03\% on the ScanObjectNN dataset.


\subsubsection{Ablation on Encoding Method.}
We first conduct ablation experiments on different input encoding methods on the ModelNet40 dataset, including direct encoding, Random-SDE (randomly sampling $\left\lfloor {N}/{T} \right\rfloor$ points per time step), and our proposed Q-SDE($N_s$). Here, $N_s$ represents the number of sampled points per time step, typically set to 256, 512, 768 or 1024. In our ablation study, these encoding methods are evaluated based on the performance and efficiency.


\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.05}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccc}
\toprule
 \multirow{3}{*}{\vspace{1.2mm} Methods}  & \multicolumn{2}{c}{$T$=2} & \multicolumn{2}{c}{$T$=4} \\ 
% \cline{2-5}
\cmidrule(l{3pt}r{3pt}){2-3}\cmidrule(l{3pt}r{3pt}){4-5}
  & OA(\%) & mAcc(\%) & OA(\%) & mAcc(\%) \\ 
  % & (\%) & (\%) & (\%) & (\%) \\ [1.5pt]
\midrule
% \hline
Direct Encoding  & 91.12 & 88.72 & 91.17 & 88.38 \\ 
Random-SDE   & 90.14 & 87.61 & 89.94 & 87.24 \\ 
Q-SDE1024   & 91.07 & 88.58 & 91.08 & 87.98 \\ 
Q-SDE768   & \textbf{91.13} & \textbf{88.93} & 91.22 & 88.45 \\ 
Q-SDE512   & 90.87 & 87.97 & \textbf{91.43} & \textbf{89.39} \\ 
Q-SDE256  & - & - & 90.89 & 88.35 \\ 
\bottomrule
\end{tabular}
}
\caption{Ablation study of encoding method performance on ModelNet40.}
\label{tab_4}
\end{table}

\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.05}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccc}
\toprule
Methods  & \multicolumn{2}{c}{Training} & \multicolumn{2}{c}{Inference} \\ 
% \cline{2-5}
\cmidrule(l{3pt}r{3pt}){2-3}\cmidrule(l{3pt}r{3pt}){4-5}
 ($T$=4) & Runtime & Memory & Runtime & Memory \\ 
\midrule
% \hline
Direct Encoding  & 478ms & 15.3G & 234ms & 9.3G \\ 
Q-SDE1024   & 431ms & 15.2G & 227ms & 9.5G \\ 
Q-SDE768   & 385ms & 12.5G & 201ms & 7.3G \\ 
Q-SDE512   & 326ms & 9.7G & 191ms & 5.2G \\ 
\textbf{Q-SDE256}  & \textbf{273ms} & \textbf{6.9G} & \textbf{164ms} & \textbf{3.0G} \\ 
\bottomrule
\end{tabular}
}
\caption{Ablation study of encoding method efficiency on ModelNet40.}
\label{tab:effi}
\end{table}

Moreover, too many support points increase encoding redundancy, failing to leverage the inherent sparsity of point clouds while introducing unnecessary points and even noise. This impacts the SNN model's performance over longer time steps, causing slightly lower accuracy for Q-SDE1024 than Q-SDE768 at 2 time steps and for both Q-SDE768 and Q-SDE1024 than Q-SDE512 at 4 time steps.

\noindent \textbf{Performance.} 
In our ablation study on different encoding methods, we compare the performance of the SPT model using common time steps of 2 and 4. From Table~\ref{tab_4}, we can see that at 2 time steps, Q-SDE768 and direct encoding exhibit comparable overall accuracy. However, at 4 time steps, Q-SDE512 surpasses direct encoding by 0.26\% in overall accuracy. In contrast, Random-SDE performs notably worse than direct encoding, further validating the effectiveness of Q-SDE.

Nevertheless, the overall accuracy of Q-SDE does not monotonically increase with fewer sampled points. Table~\ref{tab_4} shows that Q-SDE512 has lower accuracy than Q-SDE768 at 2 time steps, and Q-SDE256 has lower accuracy than Q-SDE512 at 4 time steps. This indicates that each time step should include a certain degree of repetition to ensure the core object shape is represented across most time steps. This core shape representation is called as support points. As shown in Figure \ref{fig:3}, highly sparse support points capture the essence of an object’s shape.

\noindent \textbf{Efficiency.}
We evaluate encoding method efficiency based on two metrics: runtime and memory consumption.The ablation experiments use a setting of 4 time steps and a batch size of 4. Efficiency metrics are measured on a single RTX 3090Ti, excluding the initial iteration to ensure steady-state measurements.

The results presented in Table \ref{tab:effi} clearly show that using fewer sampled points significantly reduces both runtime and memory consumption both during training and inference with the SPT model, which is consistent with our expectations. Compared to direct encoding, Q-SDE exhibits substantial advantages in optimizing runtime and memory consumption. During inference, encoding methods such as Q-SDE512 achieve a notable balance between model efficiency and inference accuracy, as corroborated by Table \ref{tab:timestep}. This further underscores that the Q-SDE encoding method effectively reduces redundancy and computational costs, making point cloud sampling at each time step more efficient and effective.

\subsubsection{Ablation on HD-IF.}
Table~\ref{tab:hdif} presents the results of the ablation study of HD-IF conducted on the ModelNet40 dataset. The experiment compares the overall accuracy of different encoding methods with various spiking neuron models at 4 time steps, aiming to demonstrate the universal superiority of HD-IF over other single neuron(e.g., IF, LIF, EIF, and PLIF).

From Table 6, we can see that incorporating HD-IF before each SPTB significantly enhances the overall accuracy across all encoding methods. Specifically, compared to replacing HD-IF with IF, for Q-SDE256, the accuracy increases from 90.53\% to 90.89\%. For Q-SDE512, the accuracy increases from 90.99\% to 91.43\%, and for Q-SDE768, the accuracy increases from 91.09\% to 91.22\%. Other single neurons replacing HD-IF also show various degrees of accuracy change, with some achieving minor improvements. However, HD-IF consistently attains the highest accuracy across all encoding methods, further demonstrating its effectiveness in enhancing model performance by leveraging the dynamic firing characteristics of different neurons. As shown in Figure 4, HD-IF can adapt to diverse data scenarios during inference by selectively activating different neurons to process information efficiently.


\subsection{Energy Efficiency}

In this section, we investigate  energy efficiency of our SPT model on the ModelNet40 dataset. In the ANN domain, the dot product operation, or MAC operation, involves both addition and multiplication operations. However, the SNN leverages the multiplication-addition transformation advantage, eliminating the need for multiplication operations in all layers except the first Conv+BN layer. According to the research~\cite{horowitz20141}, a 32-bit floating-point consumes 4.6pJ for a MAC operation and 0.9pJ for an AC operation.
\begin{figure}[t]
    \centering
    % \includegraphics[width=1\linewidth]{.jpg}
    \includegraphics[width=1.0\linewidth]{HD-IF-act_0.pdf}
    \caption{Visualization of  selectively
activated neurons on different datasets.The solid line shows the most frequently Top-1 activated neurons while the dashed line shows the most frequently Top-2 activated neurons.}
    \label{fig:HD-IFact}
\end{figure}
\begin{table}[t!]
    \small
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {TimeStep} & OA(\%) & AC(GB) & MAC(GB) & {Power(mJ)}  \\
        \midrule
        ANN & 91.73 & 0.0 & 18.42 & 84.7 \\
        \midrule
        1 & 90.87 & \textbf{3.10} & \textbf{0.044} & \textbf{3.0} \\
        4 & \textbf{91.43} & 13.85 & 0.179 & 13.3 \\
        \bottomrule
    \end{tabular}
    \caption{Power of ANN (Point Transformer) and SPT.}
    \label{tab:Power}
\end{table}
\begin{table}[t!]
    \small
    \centering
    \label{tab_10}
    \begin{tabular}{cccc}
        \toprule
        Neurons & Q-SDE256 & Q-SDE512 & Q-SDE768 \\
        ($T$=4) & OA(\%) & OA(\%) & OA(\%) \\ 
        \midrule
        % \hline
        IF & 90.53 & 90.99 & 91.09\\
        LIF & 90.34 & 91.08 & 91.07  \\
        EIF & 90.25 & 91.15 & 91.08\\
        PLIF & 90.78 & 91.28 & 91.13\\
        \textbf{HD-IF} & \textbf{90.89} & \textbf{91.43} & \textbf{91.22}\\
        \bottomrule
    \end{tabular}
    \caption{Ablation study of HD-IF on ModelNet40.}
    \label{tab:hdif}
\end{table}



Based on our SPT model, we calculate the energy consumption and present the results in Table \ref{tab:Power}. The specific method of energy consumption calculation is provided in Appendix.B.  Our SPT shows remarkable energy efficiency, requiring only 3.0mJ of energy per forward pass at 1 time step with a firing rate of 17.9\%, reflecting a 28.2-fold reduction compared to conventional ANNs. Furthermore, when we conduct inference at 4 time steps, the performance reaches 91.43\%, while the energy consumption
is merely about 6.4 times less than that of its ANN counterpart.

\section{Conclusion}
In this paper, we present the Spiking Point Transformer (SPT) which combines the low energy consumption of SNN and the excellent accuracy of Transformer for 3D point cloud classification. The results show that SPT achieves overall accuracies of 94.76\%, 91.43\%, and 78.03\% on the ModelNet10, ModelNet40, and ScanObjectNN datasets, respectively, making it the state-of-the-art in the SNN domain. We hope that our work can inspire the application of SNNs in other tasks, such as 3D semantic segmentation and object detection, and also promote the design of next-generation neuromorphic chips for point cloud processing. 
  
\section{Acknowledgments}
This work was in part supported by the National Natural Science Foundation of China under grants 62472399 and 62021001.

\bibliography{aaai25}

\end{document}
