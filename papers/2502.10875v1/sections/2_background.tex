\section{Task Formulation}
\subsection{Background}
Matrix completion is a fundamental problem in machine learning, and arises in a wide array of tasks, from recommender systems to image reconstruction. %
% Given a subset of entries from a given matrix, the goal is to infer the missing entries subject to some underlying assumption.
Formally, this problem is typically modeled as follows: Given a matrix $X \in \RR^{m \times n}$ where only a subset of the entries are observed, find a complete matrix $\hat X \in \RR^{m \times n}$ which closely approximates $X$ on the observed entries. 
For the task of recommendation, this involves predicting user interactions with items they have not previously interacted with, and a common assumption is that the preferences of users and characteristics of the items can be expressed by a small number of latent factors, with the alignment of these latent factors captured via dot-product. This justifies the search for a low-rank approximation $\hat X = BC$, where $B \in \RR^{m \times D}$ and $C \in \RR^{D \times n}$. In the case where the original matrix is binary, $X \in \{0,1\}^{m \times n}$, it is common to perform \emph{logistic matrix factorization}, where an elementwise sigmoid is applied after the dot-product of latent factors, which we denote (with slight abuse of notation) as $\hat X = \sigma(BC)$. \\
%Many recent advanced methods for this matrix completion task utilize sophisticated neural networks \citep{ncf, lightgcn, vae_rec_Sys}, that capture more expressive interactions among users and items. However, they are not particularly recognized for their set-theoretic compositionality, as previously described.

% \subsection{Notation}
% \label{sec:notation}
% % \mb{Remove whatever we don't need.}
% We will use the shorthand $\llbracket n\rrbracket \defeq \{1,\ldots,n\}$.
% Given any product of sets $X_1 \times \cdots \times X_n = \prod_{i=1}^n X_i$, we let $\pi_i\colon \prod_{i=1}^n X_n \to X$ denote projection to the $i$\textsuperscript{th} coordinate, i.e. $\pi_i(x_1, \ldots, x_n) = x_i$.
% % Given any function $f\colon X \to Y$, if $S \subseteq X$ then the \mathterm{restriction of $f$ to $S$} (function which takes the same values as $f$ but has domain $S$) is denoted $\restr{f}{S}$.
% The set of functions from $X$ to $Y$ can be written, with decreasing verbosity, as
% \[\{f \mid f\colon X \to Y\} = \{X \to Y\} = Y^X.\]

% Given some set $\U$, we denote the \mathterm{powerset} (set containing all subsets) of $\U$ as $\P(\U) = \{S \mid S \subseteq \U\}$.
% When considering some subset $S\subseteq \U$ we will often refer to $\U$ as the \mathterm{universe}. For a fixed universe $\U$, subsets $S \subseteq \U$ can be represented via their \mathterm{characteristic function},
% \begin{equation}
%     \label{eq:characteristic function of set}
% \CH_S\colon \U \to \{0,1\} \quad \text{where} \quad
%     \CH_S(x) = \begin{cases}
%         1 \quad &\text{if } x \in S,\\
%         0 \quad &\text{otherwise.}
%     \end{cases}
% \end{equation}
% Thus, we think of the powerset $\P(\U)$ as equivalent to the set of functions $\{0,1\}^\U$, commonly shortened to just $2^\U$.
% %from $\U$ to $\{0,1\}$, which we denote $\{0,1\}^\U$, or simply $2^\U$.

% The standard notations $\cap$ for intersection and $\cup$ for union will be used to denote their traditional na√Øve set-theoretic operations, and $\Delta$ denotes symmetric difference, i.e. $A \Delta B = (A \cup B) \setminus (A \cap B)$. While typically written using infix notation, we may also use prefix notation when emphasizing the interpretation of these operations as functions from $2^\U\times 2^\U$ to $2^\U$, eg. $\cap(A, B) = A \cap B$. In addition, given some universe $\U$ we will denote the set-theoretic complement of $A \subseteq \U$ as $A^c = \{x \in \U \mid x \notin A\}$, and use $\comp: 2^\U \to 2^\U$ when referring to the complement as a function, i.e. $\comp(A) = A^c$.
% We say a collection of sets $\F$ is \mathterm{closed under intersection} if, for all $A, B \in \F$, $A \cap B \in \F$, and similarly use the terms \mathterm{closed under union} and \mathterm{closed under complement}.


\subsection{Set-Theoretic Matrix Completion}

We will describe the task of set-theoretic matrix completion on the setting of movies, users, and attributes, though the formulation and our proposed model can be generalized to arbitrary domains.
% In our setting we are concerned not only with users and movies but also with attributes, and developing a model which can allow users to search for movies with various logical combinations of attributes.
We are given a set $\D_U \subseteq U \times M$ of user-movie interactions, and a set $\D_A \subseteq A \times M$ of attribute-movie pairs. We assume both of these sets are incomplete.
% and as such it is reasonable to model this as a matrix completion task for a matrix $X \in \{0,1\}^{(|U| + |A|) \times |M|}$.

Our goal is to eventually be able to recommend movies based on some query, for example "comedy and not romance". Such a query for a particular user can be represented as $u \wedge a_1 \wedge \neg a_2$, where $u$ is the user, $a_1 = \comedy$ and $a_2=\romance$. We let $Q$ be the set of all queries of interest, which depends on which queries we anticipate evaluating at inference time.
In this work, we will take $Q$ to be queries of the form $u$, $a_1$, $u \wedge a_1$, $u \wedge a_1 \wedge a_2$, and $u \wedge a_1 \wedge \neg a_2$, where $u \in U$ and $a_1, a_2 \in A$.

With this formulation, we can view our task as matrix completion for a matrix $X \in \{0,1\}^{|Q| \times |M|}$, where the rows are derived by applying bitwise operators on the rows of user and attribute data. While we could, in theory, proceed directly with logistic matrix factorization on this matrix, there are both practical and theoretical reasons to search for an alternative. First, the number of rows of this matrix is very large relative to the original data - in our case we have $|Q| = \O(|U||A|^2)$, but in general $|Q| = \O(3^{|U||A|})$. This poses practical issues, both at training time (as there are an exponential number of elements of $X$ to traverse) and inference time (storing the low-rank approximations requires $\O(|Q|)$ memory, which is much larger than $|U| + |A|$). There are also theoretical issues with the underlying assumption, as it is no longer reasonable to assume the rows of $\sigma^{-1}(X)$ are linear combinations of some latent factors.\mb{Also, there is no connection between related queries.}



% which includes not only $U$ and $A$ (since we may wish to query these single variables) but also queries such as $u\wedge a_1 \wedge \neg a_2$. 


