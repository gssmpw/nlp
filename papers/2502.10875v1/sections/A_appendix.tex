\input{sections/1_related_work}
\section{Experiment Details}
\subsection{Data Splits \& Query Generation}
\label{app:data_split}

\begin{algorithm}
\caption{\textsc{Personalised Simple Query} ($u \cap a$) generation algorithm $u \cap a$}
\begin{algorithmic}[1]
    \STATE Let the set of users, attributes, and movies be $\mathcal{U}, \mathcal{A}, \mathcal{M}$
    \STATE Marginal probability of an attribute $a$ in $A$, $P(a) = \sum_{m} A_{a, m} / \sum_{a'} \sum_{m} A_{a', m}$
    \STATE Marginal probability of an user $u$ in $U$, $P(u) = \sum_{m} U_{u, m} / \sum_{u'} \sum_{m} U_{u', m}$
    \STATE Marginal probability of an movie $m$ in $U$, $P(m) = \sum_{u} U_{u, m} / \sum_{u} \sum_{m'} U_{u, m'}$
    \STATE Let $U$ be the User $\times$ Item matrix and $A$ be the Attribute $\times$ Item matrix.
    \STATE $U^{Train} \leftarrow U$, $A^{Train} \leftarrow A$
    \STATE $U^{Eval} \leftarrow \mathbf{0}$, $A^{Eval} \leftarrow \mathbf{0}$
    \STATE Set of simple personalized queries, $Q_{U \cap A} \leftarrow \phi$
    \WHILE{$|Q_{U \cap A}|$ < \textsc{Max Sample Size}}
        \STATE Sample an attribute $a$ from $\mathcal{A}$ according to $P(a)$.
        \STATE Sample a movie $m$ from for the attribute $a$, i.e., Sample from $\{m' | A_{a, m'} = 1\}$, according to $P(m)$
        \STATE Sample a user $u$ from who has rated movie $m$, i.e., Sample from  $\{u' | U_{m, u'} = 1\}$, according to $P(u)$
        \STATE $U^{Train}_{u, m} = 0$, $A^{Train}_{a, m} = 0$, $U^{Eval}_{u, m} = 1$, $A^{Eval}_{a, m} = 1$
        \STATE $Q_{U \cap A}$.\textsc{insert}($(u, a, m)$)
    \ENDWHILE
\end{algorithmic}
\label{alg:joint_sampling}
\end{algorithm}

\begin{algorithm}
\caption{\textsc{Personalised Complex Query} Generation Algorithm}
\begin{algorithmic}[1]
    \STATE Compositional Query sets $Q_{U \cap A_1 \cap A_2}$, $Q_{U \cap A_1 \cap \neg A_2}$
    \STATE Non-Trivial attribute combination set $\mathcal{A}_{\circ}$
    \FOR{each user-movie tuple in Eval set, i.e., $(u, m) \in \{(u, m) | U^{Eval}_{u, m} = 1\}$}
        \FOR{each pair of attributes $(a_1, a_2) \in \{(a_1, a_2) | A^{Eval}_{a_1, m} = 1 \text{ and } A^{Eval}_{a_2, m} = 1\}$}
            \IF{the pair is viable and non-trivial, i.e., $(a_1, a_2) \in \mathcal{A}_{\cap}$}
                \STATE $Q_{U \cap A_1 \cap A_2}$.\textsc{insert}($(u, a_1, a_2, m)$)
            \ENDIF
        \ENDFOR
        \FOR{each pair of attributes $(a_1, a_2) \in \{(a_1, a_2) | A^{Eval}_{a_1, m} = 1 \text{ and } A_{a_2, m} = 0\}$}
            \IF{the pair is viable and non-trivial, i.e., $(a_1, a_2) \in \mathcal{A}_{\setminus}$}
                \STATE $Q_{U \cap A_1 \cap \neg A_2}$.\textsc{insert}($(u, a_1, a_2, m)$)
            \ENDIF
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\label{alg:complex_query}
\end{algorithm}

\subsection{Training Details}
\label{app:training_details}

\begin{table}[H]
\caption{Hyper Parameter range for all the dataset. We run 100 runs for both models and select the best model on User-Movie validation set NDCG metric}
\resizebox{\columnwidth}{!}{%

\begin{tabular}{ccccc}
\hline
Hyperparameters                         & \begin{tabular}[c]{@{}c@{}}Range\\ Box\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best Value\\ Box\end{tabular} & \begin{tabular}[c]{@{}c@{}}Range\\ Vector\end{tabular} & \begin{tabular}[c]{@{}c@{}}Best Value\\ Vector\end{tabular} \\ \hline
Embedding dim                            & 64                                                  & 64                                                       & 128                                                    & 128                                                         \\
Learning Rate                            & 1e-1, 1e-2, 1e-3, 1e-4, 1e-5                        & 0.001                                                    & 1e-1, 1e-2, 1e-3, 1e-4, 1e-5                           & 0.001                                                       \\
Batch Size                               & 64, 128, 256, 512, 1024                             & 128                                                      & 64, 128, 256, 512, 1024                                & 128                                                         \\
\# Negatives                             & 1, 5, 10, 20                                        & 20                                                       & 1, 5, 10, 20                                           & 5                                                           \\
\multicolumn{1}{l}{Intersection Temp}    & 10, 2, 1, 1e-1, 1e-2, 1e-3, 1e-5                    & 2.0                                                      & -                                                      & -                                                           \\
\multicolumn{1}{l}{Volume Temp}          & 10, 5, 1, 0.1, 0.01, 0.001                          & 0.01                                                     & -                                                      & -                                                           \\
\multicolumn{1}{l}{Attribute Loss const} & 0.1, 0.3, 0.5, 0.7, 0.9                             & 0.7                                                      & 0.1, 0.3, 0.5, 0.7, 0.9                                & 0.5                                                         \\ \hline
\end{tabular}
}
\label{tab:hyperparams}
\end{table}
Hyperparameters are reported in Table \ref{tab:hyperparams}. Best parameter values are reported for Box Embeddings and \textsc{MF} method. 
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{pictures/hparam_search.png} % Adjust the width as needed
    \caption{Parallel Co-ordinate plot for different hyperparameters vs model performance. Lighter the color, better the model's performance.}
    \label{fig:generalization-spectrum}
\end{figure*}

\subsection{Model Selection}
\begin{table}[t]
    \centering
    \caption{Test NDCG on $D_{U}^\eval$ for selected models.}
    \scalebox{0.9}{
    \begin{tabular}{lllll}
        \toprule
        Dataset & \textsc{MF}   & \textsc{NeuMF} & \textsc{Lgcn} & \textsc{Box}  \\ \hline
        \addlinespace
        Last-FM & 0.51 & 0.52 & 0.56 & 0.65 \\
        NYC-R   & 0.31 & 0.33 & 0.37 & 0.39 \\
        ML-1M   & 0.51 & 0.53 & 0.55 & 0.58 \\
        ML-20M  & 0.71 & 0.70 & 0.72 & 0.73 \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:model_selection}
\end{table}


\subsection{Set-Theoretic Generalization}
\begin{table}[H]
\caption{Hit Rate(\%)$\uparrow$ for Set-theoretic queries for dataset ML-20M. }
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllllllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c}{$U \cap A$} & \multicolumn{3}{c}{$U \cap A_1 \cap A_2$} & \multicolumn{3}{c}{$U \cap A_1 \cap \neg A_2$} \\ \cline{2-10} 
\multicolumn{1}{c}{}                         & h@10    & h@20    & h@50    & h@10        & h@20       & h@50       & h@10         & h@20          & h@50         \\ \hline
\addlinespace
\textsc{MF-Filter}         & 4.6      & 8.1      & 16.1     & 0.4          & 1.0         & 2.9         & 3.7             & 6.6              & 13.7             \\
\textsc{MF-Product}        & 4.1      & 7.5      & 15.6     & 3.3          & 6.6         & 16.4        & 2.7           & 5.1            & 11.4          \\
\textsc{MF-Geometric}      & 0.1      & 0.3      & 0.6      & 0.0          & 0.0         & 0.0         & 0.3           & 0.6            & 1.4           \\ \hdashline
\addlinespace
\textsc{NeuMF-Filter} & 4.6 & 8.2 &  16.1 & 1.1 & 5.6 & 6.4 & 4.9 & 7.3 & 13.9 \\
\textsc{NeuMF-product} & 4.6 & 8.2 & 16.1 & 4.1 & 8.5 & 22.1 & 4.3 & 6.9 & 12.0 \\ \hdashline
\addlinespace
\textsc{Box-Filter}         & 4.6      & 8.1      & 16.1     & 11.0         & 21.8        & 42.3        & 4.6           & 7.7            & 16.3           \\
\textsc{Box-Product}        & 4.5      & 8.2      & 16.1     & 11.1         & 21.8        & 42.5        & 4.3           & 7.1            & 15.1           \\
\textsc{Box-Geometric}      & 4.5      & 8.1      & 16.2     & 11.0         & 21.8        & 42.4        & \textbf{6.4}  & \textbf{12.8}  & \textbf{25.9} \\ \hline
\end{tabular}
}
\label{tab:set-theretic-results-ml20m}
\end{table}



\subsection{Spectrum of Weak Generalization}
\label{app:weak_generalization}

\begin{table}[H]
\caption{The spectrum of generalization for \textsc{Simple Personalized query} $U \cap A$. W: \textsc{Weakest Generalization}, W-U: \textsc{Weak Generalization-User}, W-A: \textsc{Weak Generalization-Attribute}, S: \textsc{Set Theoretic Generalization}}
\resizebox{\columnwidth}{!}{%

\begin{tabular}{llllllllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c}{Hit Rate @10}              & \multicolumn{3}{c}{Hit Rate @ 20}             & \multicolumn{3}{c}{Hit Rate @ 50}                      \\ \cline{2-10} 
\multicolumn{1}{c}{}                         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}                  \\ \hline
\textsc{MF-Filter}                         & \multicolumn{3}{l}{24.7 | 6.7 | 13.0 | 5.0}   & \multicolumn{3}{l}{36.3 | 13.3 | 20.7 | 10.2} & \multicolumn{3}{l}{54.2 | 30.1 | 33.3 | 22.3}          \\
\textsc{MF-Product}                        & \multicolumn{3}{l}{23.3 | 5.7 | 13.1 | 4.3}   & \multicolumn{3}{l}{35.0 | 10.8 | 21.4 | 8.5}  & \multicolumn{3}{l}{54.7 | 24.2 | 38.8 | 20.4}          \\
\textsc{MF-Geometric}                      & \multicolumn{3}{l}{4.9 | 0.9 | 1.8 | 0.4}     & \multicolumn{3}{l}{7.9 | 1.7 | 3.3 | 0.9}     & \multicolumn{3}{l}{15.1 | 4.5 | 7.4 | 3.0}             \\ \hline
\textsc{Box-Filter}                         & \multicolumn{3}{l}{24.1 | 13.0 | 16.4 | 11.7} & \multicolumn{3}{l}{34.5 | 22.3 | 24.6 | 19.1} & \multicolumn{3}{l}{50.5 | 40.5 | 37.6 | 32.3}          \\
\textsc{Box-Product}                        & \multicolumn{3}{l}{25.2 | 13.6 | 13.9 | 10.0} & \multicolumn{3}{l}{35.2 | 21.5 | 21.9 | 16.7} & \multicolumn{3}{l}{52.2 | 38.4 | 38.3 | 31.5}          \\
\textsc{Box-Geometric}                      & \multicolumn{3}{l}{25.4 | 14.7 | 14.8 | 11.0} & \multicolumn{3}{l}{35.6 | 23.3 | 23.5 | 18.3} & \multicolumn{3}{l}{\textbf{52.2 | 40.8 | 40.5 | 34.1}} \\ \hline
\end{tabular}
}
\label{tab:generalization-spectrum-simple-query}
\end{table}

\begin{table}[H]
\caption{The spectrum of generalization for \textsc{Complex Personalized query} $U \cap A_1 \cap \neg A_2$. W: \textsc{Weakest Generalization}, W-U: \textsc{Weak Generalization-User}, W-A: \textsc{Weak Generalization-Attribute}, S: \textsc{Set Theoretic Generalization}}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllllllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c}{Hit Rate @10}              & \multicolumn{3}{c}{Hit Rate @ 20}             & \multicolumn{3}{c}{Hit Rate @ 50}                      \\ \cline{2-10} 
\multicolumn{1}{c}{}                         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}                  \\ \hline
\textsc{MF-Filter}                        & \multicolumn{3}{l}{25.5 | 13.0 | 12.4 | 4.7}  & \multicolumn{3}{l}{34.9 | 14.1 | 19.5 | 9.8}  & \multicolumn{3}{l}{54.7 | 29.5 | 37.1 | 22.5}          \\
\textsc{MF-Product}                       & \multicolumn{3}{l}{23.5 | 7.0 | 10.4 | 3.4}   & \multicolumn{3}{l}{34.9 | 12.8 | 18.0 | 7.3}  & \multicolumn{3}{l}{54.5 | 27.5 | 35.0 | 19.3}          \\
\textsc{MF-Geometric}                     & \multicolumn{3}{l}{5.2 | 2.0 | 1.7 | 0.5}     & \multicolumn{3}{l}{8.8 | 3.5 | 1.9 | 1.0}     & \multicolumn{3}{l}{17.4 | 8.8 | 6.5 | 2.7}             \\ \hline
\textsc{Box-Filter}                        & \multicolumn{3}{l}{24.1 | 15.3 | 15.0 | 11.4} & \multicolumn{3}{l}{35.5| 22.7 | 21.1 | 19.5}  & \multicolumn{3}{l}{\textbf{54.1 | 39.2 | 37.3 | 34.0}} \\
\textsc{Box-Product}                       & \multicolumn{3}{l}{21.1 | 13.7 | 12.0 | 8.9}  & \multicolumn{3}{l}{30.5 | 21.7 | 19.3 | 15.2} & \multicolumn{3}{l}{47.4 | 38.0 | 35.0 | 29.4}          \\
\textsc{Box-Geometric}                     & \multicolumn{3}{l}{21.1 | 13.2 | 10.8 | 8.6}  & \multicolumn{3}{l}{30.4 | 20.8 | 17.7 | 15.1} & \multicolumn{3}{l}{\textbf{47.3 | 36.6 | 33.2 | 31.0}} \\ \hline
\end{tabular}
}
\label{tab:generalization-spectrum-difference-query}
\end{table}

\begin{table}[H]
\caption{The spectrum of generalization for \textsc{Complex Personalized query} $U \cap A_1 \cap A_2$. W: \textsc{Weakest Generalization}, W-U: \textsc{Weak Generalization-User}, W-A: \textsc{Weak Generalization-Attribute}, S: \textsc{Set Theoretic Generalization}}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llllllllll}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multicolumn{3}{c}{Hit Rate @10}              & \multicolumn{3}{c}{Hit Rate @ 20}             & \multicolumn{3}{c}{Hit Rate @ 50}                      \\ \cline{2-10} 
\multicolumn{1}{c}{}                         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}         & \multicolumn{3}{l}{W | W-U | W-A | S}                  \\ \hline
\textsc{MF-Filter}             & \multicolumn{3}{l}{35.3 | 17.6 | 16.9 | 11.4} & \multicolumn{3}{l}{45.0 | 27.3 | 23.3 | 17.9} & \multicolumn{3}{l}{55.2 | 41.9 | 30.5 | 27.5}          \\
\textsc{MF-Product}            & \multicolumn{3}{l}{34.0 | 11.0 | 11.6 | 5.1}  & \multicolumn{3}{l}{47.3 | 19.6 | 20.1 | 10.6} & \multicolumn{3}{l}{67.4 | 38.5 | 39.3 | 26.1}          \\
\textsc{MF-Geometric}          & \multicolumn{3}{l}{6.13 | 3.1 | 0.3 | 0.1}    & \multicolumn{3}{l}{9.90 | 5.8 | 0.6 | 0.2}    & \multicolumn{3}{l}{18.5 | 12.9 | 1.8 | 0.8}            \\ \hline
\textsc{Box-Filter}             & \multicolumn{3}{l}{30.8 | 21.5 | 17.3 | 14.5} & \multicolumn{3}{l}{41.1 | 31.2 | 23.3 | 20.5} & \multicolumn{3}{l}{52.7 | 44.5 | 30.3 | 28.5}          \\
\textsc{Box-Product}            & \multicolumn{3}{l}{35.4 | 23.8 | 13.4 | 10.6} & \multicolumn{3}{l}{47.0 | 34.5 | 21.7 | 17.8} & \multicolumn{3}{l}{64.6 | 52.8 | 39.0 | 34.2}          \\
\textsc{Box-Geometric}          & \multicolumn{3}{l}{34.6 | 25.2 | 20.0 | 16.8} & \multicolumn{3}{l}{45.7 | 35.7 | 30.5 | 26.6} & \multicolumn{3}{l}{\textbf{62.6 | 53.3 | 50.1 | 46.1}} \\ \hline
\end{tabular}
}
\label{tab:generalization-spectrum-intersection-query}
\end{table}

% \begin{table}[H]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{l|cccc|cccc|cccc}
% \hline
% \multicolumn{1}{c|}{\multirow{2}{*}{Methods}} & \multicolumn{4}{c|}{Hit Rate @10} & \multicolumn{4}{c|}{Hit Rate @20} & \multicolumn{4}{c}{Hit Rate @50} \\ \cline{2-13} 
% \multicolumn{1}{c|}{} & W & W-U & W-A & S & W & W-U & W-A & S & W & W-U & W-A & S \\ \hline
% \textsc{MF-Filter}    & 35.3 & 17.6 & 16.9 & 11.4 & 45.0 & 27.3 & 23.3 & 17.9 & 55.2 & 41.9 & 30.5 & 27.5 \\
% \textsc{MF-Product}   & 34.0 & 11.0 & 11.6 & 5.1  & 47.3 & 19.6 & 20.1 & 10.6 & 67.4 & 38.5 & 39.3 & 26.1 \\
% \textsc{MF-Geometric} & 6.13 & 3.1  & 0.3  & 0.1  & 9.90 & 5.8  & 0.6  & 0.2  & 18.5 & 12.9 & 1.8  & 0.8  \\ \hline
% \textsc{Box-Filter}    & 30.8 & 21.5 & 17.3 & 14.5 & 41.1 & 31.2 & 23.3 & 20.5 & 52.7 & 44.5 & 30.3 & 28.5 \\
% \textsc{Box-Product}   & 35.4 & 23.8 & 13.4 & 10.6 & 47.0 & 34.5 & 21.7 & 17.8 & 64.6 & 52.8 & 39.0 & 34.2 \\
% \textsc{Box-Geometric} & 34.6 & 25.2 & 20.0 & 16.8 & 45.7 & 35.7 & 30.5 & 26.6 & \textbf{62.6} & 53.3 & 50.1 & 46.1 \\ \hline
% \end{tabular}
% }
% \caption{Hit Rate Results}
% \label{tab:generalization-spectrum-intersection-query-2}
% \end{table}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\columnwidth]{pictures/weak_generaliztion.png}
  \caption{Weak Generalization Illustration}
  \label{fig:weak_generalization}
\end{figure}


The \textsc{Box-Geometric} achieves the best \textit{Generalization Spectrum Gap} for all types of queries.

\section{Error Compounding Analysis}
\label{app:error_compounding}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\textwidth]{pictures/all_success.png}
    \caption{Relationships of correct answers by the three box models on $u \wedge a_1 \wedge a_2$ queries.}
    \label{fig:first-figure}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\textwidth]{pictures/compund_error_solved.png}
    \caption{The Geometric method subsumes the benefit of the product in compounding error.}
    \label{fig:second-figure}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\textwidth]{pictures/not_compunding_error.png}
    \caption{The effect is less for the non-compounding error.}
    \label{fig:third-figure}
\end{figure}

We further perform more granular analysis amongst the \textsc{Box} based methods with complex query type $U \cap A_1 \cap A_2$. As claimed in our initial hypothesis, the \textsc{Filter} method suffers from error compounding. If the target movie $m$ is in the model's prediction list for $A_1$ but not for $A_2$ or the other way round, we denote this error as \textit{compounding error}. In figure \ref{fig:second-figure}, out of the compounding errors, $34 \%$ is solved by the \textsc{Box-Geometric} method and $26 \%$ by the \textsc{Box-Product} method. However, in figure \ref{fig:third-figure}, for the error that is not due to compounding (where the model gets both $A_1$ and $A_2$ prediction wrong), only $18 \%$ are corrected by the \textsc{Box-Geometric} method and a mere $10 \%$ of them are corrected by \textsc{Box-Product}. Refer to figure \ref{fig:first-figure} \ref{fig:second-figure} \ref{fig:third-figure} for details. This demonstrates that the \textsc{Box-Geometric} significantly contributes to the correction of error compounding.


\section{{Time Efficiency analysis}}

\begin{table}[ht]
\centering
\caption{Training time (\textit{mm:ss}) for a single epoch are measured for different batch sizes with 5 negative samples on Movielens-1M dataset. Experiments are conducted on Nvidia GTX 1080Ti gpus}
\begin{tabular}{lllll}
\hline
\begin{tabular}[c]{@{}l@{}}Batch Size\end{tabular} & \textsc{MF} & \textsc{NeuMF} & \textsc{LightGCN} & \textsc{Box} \\ \hline
64                                                   & 08:37                        & 17:00                           & 70:30                            & 19:32                         \\
128                                                  & 04:32                        & 09:46                           & 38:40                              & 11:40                         \\
256                                                  & 02:29                        & 04:40                           & 20:55                              & 05:28                         \\
512                                                  & 01:18                        & 02:23                           & 10:47                              & 02:54                         \\
1024                                                 & 00:40                        & 01:20                           & 05:24                              & 01:12                         \\ \hline
\end{tabular}
\label{tab:training_time}
\end{table}

{In \Cref{tab:training_time}, we observe that the \textsc{MF}, being the simplest approach with minimal computational requirements, is consistently the fastest across all batch sizes. At the largest batch size (1024), it achieves the shortest training time of just 00:40. The \textsc{Box}-based method exhibits training times comparable to \textsc{NeuMF}. However, it is significantly faster than \textsc{LightGCN}, which relies on graph convolutional computations. The iterative message-passing operations required by \textsc{LightGCN} result in considerably higher training times, particularly at smaller batch sizes (e.g., 70:30 at a batch size of 64). As the batch size increases, the training time for \textsc{Box} embeddings becomes almost as efficient as \textsc{MF}. For instance, at a batch size of 1024, \textsc{Box} achieves a training time of 01:12, compared to 00:40 for \textsc{MF}. This demonstrates that the computational complexity of box embeddings is of the same order as \textsc{MF}, making it a scalable and efficient choice.}

{Box embeddings are generally quite fast because the computation of box intersection volumes can be parallelized over dimensions. Note that the training times above use GumbleBox embeddings, which involve log-sum-exp calculations. However, this could be improved even further at inference time by replacing these soft min and max approximations with hard operators. If such an optimized approach is desired, then training can accommodate this by regularizing temperature. For deployment in industrial set-up, we could take additional steps with Box Embeddings as outlined in \cite{box_for_search}.}