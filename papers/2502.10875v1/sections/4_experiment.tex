\section{Experiments}
\vspace{-5pt}
\label{sec:experiments}
In our experiments, we evaluate all the models on item recommendation across three domains: movies, songs, and restaurants. (\ref{sec:dataset}). We systematically generate queries of varying complexity from these datasets to evaluate performance on set-theoretic tasks 
 (\ref{sec:simple_query}, \ref{sec:complex_query}). We train and select models based on the performance of the traditional personalized item prediction (\ref{sec:traning_details}). Finally, we demonstrate that our set-based representation method is better suited for handling set-theoretic constraints in recommendation tasks (\ref{sec:main_results}, \ref{sec:spectrum_generalization}).
% In this work, we focus on the task of movie recommendation to empirically demonstrate our research claims. We want to principally investigate if box embeddings provide better generalization than vectors obtained from matrix factorization for the recommendation task with set-theoretic queries.
\subsection{Dataset}
\label{sec:dataset}
\vspace{-5pt}
The datasets used in our study must contain two primary components: \textbf{Item-User interactions $\D_U$} and \textbf{Item-Attribute interactions $\D_A$} \shib{add a reference to the picture if added}. We select datasets that offer rich ground truth annotations for both components. We utilize the MovieLens 1M and 20M datasets for personalized movie recommendations \citep{harper:2015}. For the song domain, we employ a subset of the Last-FM dataset, which is the official song tag dataset of the Million Song Dataset \citep{lastfm}. In the restaurant domain, we use the NYC-R dataset introduced by \cite{nycr}.
%\paragraph{User $\times$ Movie Data $\D_U$:} MovieLens dataset \citep{harper:2015} is a popular benchmark for personalized movie-recommendation methods. In this work, we use Movielens-1M and Movielens-20M versions of the datasets. For both of them, we use the binarized implicit feedback data \cite{hu:ials}, \ie, $(u,m) \in \D_U$
% is a binary matrix where an entry $(u,m)$ is $1$ if user $u$ has rated movie $m$.
% \paragraph{Attribute $\times$ Movie Data $\D_A$:} We use the data curated by \citet{genere2movies} for constructing $\D_A$. \citet{genere2movies} uses Wikidata \citep{vrandevcic2014wikidata} to construct ground truth attribute labels for movies. While MovieLens-20M provides a far from complete  arbitrary tag list for each movie, \citet{genere2movies} only considers movie genres. However, the annotations that they create are $2.5$ times more dense than MovieLens tags. The high density and completeness of the ground truth are essential for the accurate assessment of models. The completeness of the annotation also helps us to construct set-theoretic queries out of them in a statistically reliable way.

We utilize the data curated by \citet{genere2movies} to construct $\D_A$ for the Movielens data. This dataset employs Wikidata \citep{vrandevcic2014wikidata} to generate ground truth attribute labels for movies\footnote{https://github.com/google-research-datasets/genre2movies}. For the Last-FM dataset, the authors use the Last.fm API ('getTopTags')\footnote{https://www.last.fm/} to create attribute tags. Likewise, the authors in \cite{nycr} crawl restaurant review data from TripAdvisor\footnote{https://www.tripadvisor.com} to curate tags and ratings for restaurants in NYC. The sparsity of $D_A$ and $D_U$ is comparable in the Movielens datasets. In contrast, the Last.fm and NYC-R datasets, designed with tag annotations in mind, exhibit much denser attribute-movie interaction. Thus, the selection of these three datasets not only encompasses diverse domains but also offers varying ground-truth distributions for our experiments.
% The sparsity of $D_A$ and $D_U$ are similar for movielens. However, the Last.fm and the NYC-R datasets are created keeping tag annotations in mind and hence we get doubly dense tag annotation in these two dataset.

We use the binarized implicit feedback data \cite{mf_hu:ials}, indicating whether the user or the attribute has been associated with the specific item. To ensure the quality of the data, we retain users/items with $5$ or more interactions and attributes with frequency $20$ or more in all the datasets. Refer to Table \ref{tab:dataset_table} for a detailed description of the dataset statistics.
\begin{table*}[]
\caption{\small Dataset Statistics, the Item-User interaction $\D_U$ \& the Item-Attribute interaction $\D_A$. \\The Train/Test split is created using algorithm \ref{alg:joint_sampling} to test set-theoretic generalization.}
\centering
\resizebox{0.8\textwidth}{!}{ % Scale to the text width
\begin{tabular}{lrrrrrrr}
\toprule
\multicolumn{1}{c}{Dataset} & \multicolumn{1}{c}{\#Users} & \multicolumn{1}{c}{\#Items} & \multicolumn{1}{c}{\#Attributes} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Train\\ $\D_U$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Eval\\ $\D_U$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Train\\ $\D_A$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Eval\\ $\D_A$\end{tabular}} \\ \hline 
\addlinespace % Adds space
Last-FM                     & 1,872                       & 2417                        & 490                              & 60,497                                                                    & 8,857                                                                    & 34,374                                                                    & 4,240                                                                    \\
NYC-R                       & 9,597                       & 3764                        & 579                              & 82,734                                                                    & 8,502                                                                    & 34,908                                                                    & 4,376                                                                    \\ 
MovieLens 1M                & 6,040                       & 3,705                       & 57                               & 963,554                                                                   & 36,655                                                                   & 10,273                                                                    & 1,545                                                                    \\
MovieLens-20M                & 138,493                     & 26,744                      & 95                               & 19,722,646                                                                & 277,617                                                                  & 80,178                                                                    & 1,734                                                                    \\\bottomrule
\end{tabular}
}
\label{tab:dataset_table}
\end{table*}
% \begin{table}[]
% 
% \resizebox{\columnwidth}{!}{%
% \centering

% \begin{tabular}{lrrrrrrr}
% \hline
% \multicolumn{1}{c}{Dataset} & \multicolumn{1}{c}{\#Users} & \multicolumn{1}{c}{\#Items} & \multicolumn{1}{c}{\#Attributes} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Train\\ $U$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Eval\\ $U$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Train\\ $A$\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\#Eval\\ $A$\end{tabular}} \\ \hline
% Movielens 1M                & 6,040                       & 3,705                       & 57                               & 963,554                                                                   & 36,655                                                                   & 10,273                                                                    & 1,545                                                                    \\
% Movielens-20                & 138,493                     & 26,744                      & 95                               & 19,722,646                                                                & 277,617                                                                  & 80,178                                                                    & 1,734                                                                    \\
% Last.fm                     & 1,872                       & 2417                        & 490                              & 60,497                                                                    & 8,857                                                                    & 34,374                                                                    & 4,240                                                                    \\
% NYC-R                       & 9,597                       & 3764                        & 579                              & 82,734                                                                    & 8,502                                                                    & 34,908                                                                    & 4,376                                                                    \\ \hline
% \end{tabular}

% \end{table}
\subsection{Dataset Splits  \& Query Generation}
To select models for each method, we train on a dataset split $D_U^\trn$ \& $D_A^\trn$ while evaluating on a held-out set $D_U^\eval$ \& $D_A^\eval$. However, we use these eval set pairs to construct compositional queries. Simple random sampling or leave-one-out data splits do not ensure a substantial number of these queries. Therefore, we devise a data splitting technique closely linked to query generation, which we discuss next.
 
 %We learn embeddings for the users, movies, and attributes by training the models on $\D = \D_U \cup \D_A$, which we split into train/eval sets $\D^\train=\D_U^\train \cup \D_A^\train$ and $\D^\eval=\D^\eval_U \cup \D^\eval_A$.
 % the user $\times$ movie matrix $U$, attribute $\times$ movie matrix $A$.
  % into Train/Eval sets for model selection.
 % Let us denote them as $U^{Train}$, $U^{Eval}$, $U^{Train}$, $U^{Eval}$ respectively.
 %Since trained models are tested on the compositional queries, one can not simply split these sets with usual mechanics such as random sampling or leave-one-out. Rather the split is deeply tied to the eventual queries in $Q$ we wish to evaluate. We discuss this composition query generation along with the data splitting strategy in the next section.
\subsubsection{Personalized Simple Query}
\label{sec:simple_query} This type of query corresponds to a single attribute for a particular user, \eg \textit{Bob wants to watch a comedy movie.} More formally, given a user $u$ and an attribute $a$, the query type would be - $u \cap a$.  Note that, these simple queries are set-theoretic combinations between the item sets corresponding to the users and the attributes. Let us denote the data corresponding to these queries as $Q_{U \cap A}$.

While constructing the $Q_{U \cap A}$ pairs we need to ensure that - if an item is held out for evaluation for a simple query, the individual user-item and attribute-item pair should belong to the evaluation set as well. More formally, $(u,a,i) \in  Q_{U \cap A} \iff  (u, i) \in \D_U^\eval \wedge (a,i) \in \D_A^\eval$.
% the target movies for each of these compositional queries are not present in the training split of the user or the training split of the attribute involved in the query.
% For example, for a test query $u \cap a$, if $m$ is a target movie for this query, then we must ensure that - $U^{Train}_{u, m} = 0$ and $A^{Train}_{a, m} = 0$.
To ensure this train/test isolation, we use the sampling algorithm \ref{alg:joint_sampling} that takes in $D_{U}$ and $D_{A}$ and outputs $Q_{U \cap A}$, $\D_U^\trn, \D_A^\trn, \D_U^\eval, \D_A^\eval$ (Refer to Appendix \ref{app:data_split} for more details). The detailed statistics for the splits are provided in Table \ref{tab:dataset_table}. Also, the statistics for the $Q_{U \cap A}$ are present in Table \ref{tab:set_queries} 
%The algorithm takes in the ground truth data $\D$ and provides us with a set of \textsc{Personalized Simple Queries}, $Q_{U \cap A}$, generating the dataset splits that are tied to the query generation.
%Table stats
\vspace{-3pt}
\subsubsection{Personalized Complex Query}
\vspace{-2pt}
\label{sec:complex_query}
The set-theoretic compositions that we consider here are the intersection and negation of attributes for a particular user. Given a user $u$ and attributes $a_1$ and $a_2$, we consider the query types- $u \cap a_1 \cap a_2$ and $u \cap a_1 \cap \neg a_2$, e.g, \textit{Bob want to watch an Action Comedy movie, Alice want to watch a Children but not Monster movie}. Creating meaningful attribute compositions requires careful consideration, as not all combinations make sense.
For instance, 'Sci-Fi' \& 'Documentary' might not be a meaningful combination, whereas 'Sci-Fi' \& 'Time-Travel' is. Similarly, 'Sci-Fi' 
$\neg$' Fiction' doesn't make sense, but 'Fiction' $\neg$ 'Sci-Fi' does. Sometimes, even if the intersection is valid, it could be trivial and non-interesting, e.g., 'Fiction' \& 'Sci-Fi'.\\
% \begin{wraptable}{r}{0.6\textwidth} % "r" for right, 0.5\textwidth for table width
%     \centering
%     \caption{Compositional Query Statistics}
%     \begin{tabular}{lccc}
%     \toprule
%     \multicolumn{1}{c}{\multirow{2}{*}{Dataset}} & \begin{tabular}[c]{@{}c@{}}Personalized\\ Simple Query\end{tabular} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Personalized\\ Complex Query\end{tabular}} \\
%     \multicolumn{1}{c}{}                         & $u \cap a$                                                          & $u \cap a_1 \cap a_2$                    & $u \cap a_1 \cap \neg a_2$                    \\ \hline
%     \addlinespace % Adds space
%     Last-FM                                      & 9,867                                                               & 45,142                                   & 10,814                                        \\
%     NYC-R                                        & 9,482                                                               & 7,460                                    & 2,369                                         \\ 
%     ML-1M                                        & 21,392                                                              & 51,299                                   & 37,769                                        \\
%     ML-20M                                       & 35,368                                                              & 42,355                                   & 47,374                                        \\\bottomrule
%     \end{tabular}
%     \label{tab:set_queries}
% \end{wraptable}
Intuitively, for two attributes $a_1$ \& $a_2$, their intersection is interesting if $|a_1 \cap a_2|$ is greater than combining any two random items set. Also, for their intersection to be non-trivial the size of the intersection $|a_1 \cap a_2|$ must be less than the individual sizes of the attributes i.e., $\alpha|a_1|$ and $\alpha|a_2|$. Here,$|.|$ denotes the size of the item set corresponding to the attributes. $\alpha \in [0,1]$ is a design parameter, dedicated after manual inspection of the quality of the item sets for the combinations \shib{Refer to an appendix here}. In case of difference queries such as $a_1 \cap \neg a_2$, we consider $\neg a_2$ to be the second attribute and carry out the same filtering strategy as done for the intersection queries. \\We denote the set of non-trivial and viable attribute pairs for the intersection to be $\mathcal{A}_{\cap} = \{(a_1,a_2)| |a_1 \cap a_2| > \epsilon, |a_1 \cap a_2| <  \alpha|a_1|, |a_1 \cap a_2| <  \alpha|a_2|\}$, and for the difference to be $\mathcal{A}_{\setminus} = \{(a_1,a_2)| |a_1 \cap \neg a_2| > \epsilon, |a_1 \cap \neg a_2| <  \alpha|a_1|, |a_1 \cap \neg a_2| <  \alpha|\neg a_2|\}$. Using the above formulation, we generate the test set for the personalized complex queries $Q_{U \cap A_1 \cap A_2}$ and $Q_{U \cap A_1 \cap \neg A_2}$ using algorithm \ref{alg:complex_query}. Please refer to Table \ref{tab:set_queries} for the detailed statistics.

\subsection{Training Details \& Evaluation Criteria}
\label{sec:traning_details}
We train all the methods on users and attributes jointly using $\D^\trn = \D_{U}^\trn \cup \D_{A}^\trn$.
% the User$\times$Movies matrix $U^{Train}$, Attribute$\times$Movies matrix $A^{Train}$.
% It is a joint training procedure, where movie representations are shared between both trainings. \shib{Refer to the method section for this.}
We use dimensions $d=128$ for vector-based models, and $d=64$ for box models so that the number of parameters per user, attribute, and movie is equal.\footnote{Recall that box embeddings are parameterized with two vectors, one for each min and max coordinate.}
We perform extensive hyperparameter tuning for the {learning rate, batch size, volume and intersection temperature of boxes, loss combination constant, etc. Please refer to the Appendix \ref{app:training_details} for details.}
% of the dataset $U^{Valid}$ and $A^{Valid}$
We follow the standard sampled evaluation procedure described in \citet{nc_vs_mf}, {only for model selection purpose}. For each user-item tuple $(u, m)$ in $\D_U^\eval$, the model ranks $m$ amongst a set of items consisting of the  $m$ together with $100$  other true negative items w.r.t the user. Then we report on two different evaluation metrics namely Hit Ratio@$k$ (HR@$k$) and NDCG. (a) HitRatio@$k$: If the rank of $m$  is less than or equals to $k$ then the value of HR@$k$ is $1$ or $0$ otherwise. (2) NDCG: if $r$ is the rank of $m$, then  $1/\log(r + 1)$ is the NDCG.

The model is selected based on the best-performing model on NDCG for the item prediction over the user-item validation 
set $\D_U^\eval$,  with the best-performing checkpoint saved for further evaluation on compositional queries. We follow the same evaluation protocol for the compositional queries as well, except, {we rank $m$ amongst all items in the vocabulary rather than a sampled subset.}
\vspace{-5pt}
\subsection{Baselines}
\vspace{-2pt}
The recommendation systems literature offers a wide range of methods that represent users, and items in $\mathbb{R}^d$. These methods then propose a compatibility score function between the user and item, $\phi: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$. A common and effective choice for $\phi$ is the dot product, which underpins matrix factorization \citep{nc_vs_mf, mf_Koren2015}. To capture more complex interactions among users, items, and attributes, \cite{ncf} extend matrix factorization by replacing the dot product with a neural network-based similarity function. This method, called Neural Matrix Factorization (\textsc{NeuMF}), combines the dot product with an MLP. Similarly, \cite{lightgcn} propose LightGCN (\textsc{LGCN}) to captures the user, items, and attribute interaction using Graph Convolution Network \cite{gcn_kipf} over a joint graph of user-item-attribute. We use \textsc{MF}, and, \textsc{NeuMF} \textsc{LGCN} as our baselines.

For a personalized query, be it simple or complex, we need to devise a method to combine the individual scores of the user and the attributes involved in the query. In this work, we compare three approaches to obtain an aggregated score:
%We compare the Box Embeddings method with Matrix-Factorization \cite{nc_vs_mf} \shib{find a better citation} for the compositional query. Both the embedding methods can provide scores for all the movies for a particular user or attribute. For a personalized query, be it simple or complex, we need to devise a method to combine the individual scores of the user and the attributes involved in the query. In this work, we compare three approaches to obtain an aggregated score:
\begin{enumerate}[leftmargin=*]
    \item \textsc{Filter}: In this approach, we retrieve a list of items corresponding to the attributes based on the scores provided by the embedding models. The list is generated by thresholding the scores, where the threshold is optimized by minimizing the F1 score between the training data and predicted scores. We refer to the methods using this aggregation technique as \textsc{Box-Filter} for box embeddings and \textsc{MF-Filter}, \textsc{NeuMF-Filter}, \textsc{LGCN-Filter}for vector-based methods.
    \item \textsc{Product}: In this method, the compositional score is computed by multiplying the scores for the individual queries. For vector-based embeddings, the scores for each movie related to a user or attribute are normalized using the \textit{sigmoid} function. For box embeddings, the energy function is normalized by conditioning on the movie box volume (see Section \ref{sec:inference}). The score for negation is calculated by subtracting the normalized score from 1. The three methods using this technique are referred to as \textsc{Box-Product}, \textsc{MF-Product}, \textsc{NeuMF-Product}, and \textsc{LGCN-Product}.
    \item \textsc{Geometric}: This approach leverages the geometry of the embedding space. For vector-based embeddings, learned through Matrix Factorization, addition, and subtraction are often used for query composition \citep{mikolov2013efficient}. Box embeddings, on the other hand, naturally represent intersection operations, allowing us to compute scores for any set-theoretic combination using box intersection and inclusion-exclusion principles. We refer to these methods as \textsc{Box-Geometric} and \textsc{MF-Geometric}.
\end{enumerate}
 