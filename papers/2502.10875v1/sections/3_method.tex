\section{Method}
Our proposed solution to address these issues starts by defining the sets of movies which comprise the queries of interest. Let, $\P(M)$ be the power set of movies $M$. Specifically, for each user $u$ we can define the set $M_u = \{m \mid (u,m) \in \D_U\}$, and for each attribute $a$ we can define the set $M_a = \{m \mid (a,m) \in \D_A\}$. If we let $\M \subseteq \P(M)$  be the collection of all such sets, then the set of movies corresponding to a given query $q$ are direct set-theoretic combinations of elements in $\M$. Hence, the reasonable underlying assumption, in this case, is to model the elements of $\M$ as sets via a map $f: \M \to R$ where $R$ is also a set of sets, and the map $f$ respects set-theoretic operations, \ie $f(S \cap T) = f(S) \cap f(T)$ and $f(S \setminus T) = f(S) \setminus f(T)$, etc. Such a map is referred to as a \emph{homomorphism of Boolean algebras}, and the problem of learning such a function was explored in general in \citep{boratko2022measure}. In our work, we propose box embeddings as the function $f$ which can be trained to obey the homomorphism constraints.  As a result, user-attribute-item representations based on box embeddings could serve as an optimal inductive bias for the proposed set-theoretic matrix completion task.
\subsection{Set-theoretic Representation Box Embeddings}
\label{sec:box_embeddings}
{As introduced in \citet{hard_box}, box embeddings represent entities by a hyperrectangle in $\mathbb{R}^D$, \ie a Cartesian product of intervals. Let the box embedding for user $u$ be: \[\Box(u) = \prod_{d=1}^D[u_d^\llcorner, u_d^\urcorner] = [u_1^\llcorner, u_1^\urcorner] \times \ldots \times [u_D^\llcorner, u_D^\urcorner] \subseteq \RR^D,\] where $[u_d^\llcorner, u_d^\urcorner]$ is the interval for $d$-th dimension, $u_d^\llcorner < u_d^\urcorner$ for $d \in \{1, \ldots, D\}$. \\
The volume of an interval is defined as the length of the interval $\operatorname{Vol}((u_d^\llcorner, u_d^\urcorner)) = \max(u_d^\urcorner-u_d^\llcorner, 0)$. \\
Let, $\Box(m) = \prod_{d=1}^D[m_d^\llcorner, m_d^\urcorner]$ be the box embeddings for a movie $m$. At dimension $d$, the volume of intersection between user $u$ and movie $m$ is defined as - 
\begin{align*}
    \operatorname{VolInt} &((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner)) \\ &= 
    \max \Big( \min(u_d^\urcorner, m_d^\urcorner)
    - \max(u_d^\llcorner, m_d^\llcorner), 0 \Big).
\end{align*}
}
When the movie interval $[m_d^\llcorner, m_d^\urcorner]$ is completely contained by user interval $[u_d^\llcorner, u_d^\urcorner]$, then $\frac{\operatorname{VolInt}((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner))}{\operatorname{Vol}((u_d^\llcorner, u_d^\urcorner))} = 1$. This objective creates a set-theoretic interpretation with box embeddings, where user $\Box(u)$ contains all the movie boxes related to $u$ (\cref{fig:box_depiction}). The score for containment for a single dimension $d$ is formulated as:
% \[
% F_{\Box}((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner)) \defeq \frac{\operatorname{VolInt}((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner))}{\operatorname{Vol}((u_d^\llcorner, u_d^\urcorner))}
% \defeq \frac{\max(\min(u_d^\urcorner, m_d^\urcorner) - \max(u_d^\llcorner, m_d^\llcorner), 0)}{\max(u_d^\urcorner-m_d^\llcorner, 0)}.
% \]
\begin{align}
F_{\Box}&((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner)) \notag \\
&\defeq \frac{\operatorname{VolInt}((u_d^\llcorner, u_d^\urcorner), 
(m_d^\llcorner, m_d^\urcorner))}{\operatorname{Vol}((u_d^\llcorner, u_d^\urcorner))} \notag \\
&\defeq \frac{\max(\min(u_d^\urcorner, m_d^\urcorner) - \max(u_d^\llcorner, m_d^\llcorner), 0)}
{\max(u_d^\urcorner - m_d^\llcorner, 0)}.
\end{align}
The overall containment score is the multiplication of $F_{\Box}$ for each dimension. The $\operatorname{log}$ of this score is referred to as the energy function as given:
\begin{equation}
\label{eq:energy}
\Energy_\Box(u,m) \defeq -\log \prod_{d=1}^D F_\Box((u_{d}^\llcorner, u_{d}^\urcorner), (m_d^\llcorner, m_d^\urcorner)).
\end{equation}
% Here $u$ and $v$ are entities and $\nodeparam$ is the box embedding function, 
% where the per-dimension parameters are endpoints of an interval, $\nodeparam(u)_d = [\nodeparam(u)_d^\llcorner, \nodeparam(u)_d^\urcorner]$, and $\nodeparam(u)$ is the box $\prod_{d=1}^D \nodeparam(u)_d$. The per-dimension score is defined as
% \[
% F_{\Box}((x^\llcorner, x^\urcorner), (y^\llcorner, y^\urcorner)) \defeq \frac{|[x^\llcorner, x^\urcorner] \cap [y^\llcorner, y^\urcorner]|}{|[y^\llcorner, y^\urcorner]|}
% = \frac{\max(\min(x^\urcorner, y^\urcorner) - \max(x^\llcorner, y^\llcorner), 0)}{\max(y^\urcorner-y^\llcorner, 0)}.
% \]




This energy function is minimized when the user  $\Box(u)$ contains the movie $\Box(m)$. Previous works have highlighted the difficulty of optimizing an objective including these hard $\min$ and $\max$ functions \citep{softbox, gumbel_box}. 
In our work, we use the latter solution, termed $\GumbelBox$, which treats the endpoints $x^\llcorner$ and $x^\urcorner$ as mean of $\GumbelMax$ and $\GumbelMin$ random variables, respectively. Given $1$-dimensional box parameters $\{[x_n^\llcorner, x_n^\urcorner]\}_{n=1}^N$, we define the associated $\GumbelMax$ random variables $X_n^\llcorner$ with mean $x_n^\llcorner$ and scale $\beta$, as well as the $\GumbelMin$ random variables $X_n^\urcorner$ with mean $x_n^\urcorner$ and scale $\beta$. \citet{gumbel_box} calculates that the expected volume of intersection of intervals $\{[X_n^\llcorner, X_n^\urcorner]\}$ can be approximated by
\begin{align*}
\EE&\Big[\max\big(\min_n X_n^\urcorner - \max_n X_n^\llcorner, 0\big)\Big] \notag \\
\approx & \LSE_\beta \big( \LSE_{-\beta} (x_1^\urcorner, \ldots, x_N^\urcorner) - \LSE_{\beta}(x_1^\llcorner, \ldots, x_N^\llcorner), 0\big).
\end{align*}
% &\eqdef \GumbelVolInt\left(\left[x_1^\llcorner, x_1^\urcorner\right], \ldots, \left[x_N^\llcorner, x_N^\urcorner\right]\right)
essentially replacing the hard $\min$ and $\max$ operators with a smooth approximation, $\LSE_t(\mathbf x) \defeq t \log(\sum_i e^{x_i/t})$. Expected intersection volume in higher dimensions is just a product of the preceding equation, as the random variables are independent.
% \begin{multline*}
% \prod_{i=1}^d m\left(-\beta\LSE{X^t \in T} \left(-\tfrac{\mu^{t,\wedge}_j}{\beta}\right) - \beta\LSE{X^t \in T} \left(\tfrac{\mu^{t,\vee}_j}{\beta}\right)\right)
% \end{multline*
{We use this $\GumbelBox$ (abbrev $GB$) formulation in our work changing the notations $F_{Box}, \operatorname{Vol}, \operatorname{VolInt}$  to $F_{GB}, \operatorname{Vol}_{GB}, \operatorname{VolInt}_{GB}$. We modify the per-dimension score function $F_\Box$ in \eqref{eq:energy} by replacing the ratio of hard volume calculations with the approximation to the expected volume,}
\begin{align}
F_\GB(&(u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner); (\tau, \nu)) \notag \\ &\defeq \frac{\LSE_\nu(\LSE_{-\tau}(u_d^\urcorner, m_d^\urcorner) - \LSE_\tau(u_d^\llcorner, m_d^\llcorner), 0)}{\LSE_\nu(m_d^\urcorner - m_d^\llcorner, 0)} \notag \\
&\eqdef \frac{\GumbelVolInt((u_d^\llcorner, u_d^\urcorner), (m_d^\llcorner, m_d^\urcorner); (\tau, \nu))}{\GumbelVol((m_d^\urcorner - m_d^\llcorner); \nu)}.
\end{align}
% This effectively replaces the hard $\min$ and $\max$ operators with a smooth approximation, and approximates $F_\Box$ for sufficiently small temperature hyperparameters $\tau, \nu > 0$.



% We treat these $\tau, \nu$ as additional hyperparameters of the model.
% For more details and alternative approaches see the related work in \Cref{sec:related work}.


\subsection{Training}
We model each user, attribute, and movie as a box in $\RR^D$, and denote the map from these entities to their associated box parameters as $\nodeparam$, i.e., the trainable box embedding for user $u$ is $\nodeparam(u) \defeq \Box(u)$. Our goal is to train these box representations to represent certain sets of movies which allow us to perform the sort of queries we are interested in. As motivated above, for a given user $u$, we train  $\Box(u)$ to approximate the set $M_u$ via a noise-contrastive estimation objective. Namely, for each $(u,m) \in \D_U$, we have a loss term
\begin{align*}
\ell_{(u,m)}(\nodeparam) \defeq & \Energy_\GB(u,m;\nodeparam) \notag \\
& - \EE_{\tilde m \sim M} \Big[\log\big(1 - \exp(-\Energy_\GB(u,\tilde m; \nodeparam))\big)\Big].
\end{align*}
{The first term is minimized when $\Box(u)$ contains $\Box(m)$. We approximate the second term via sampling, which encourages $\Box(u)$ to be disjoint from $\Box(\widetilde m)$ for a uniformly randomly sampled movie $\widetilde m$. We define an analogous loss function $\ell_{(a,m)}(\nodeparam)$ for attribute-movie interactions, which trains $\Box(a)$ to contain the box $\Box(m)$ for each $m$ such that $(u,m) \in \D_U$.}

The overall loss function is a convex combination of these loss terms:
\begin{align*}
\L(\nodeparam; \D_U,& \D_A ) \defeq \; w \ast \sum_{(u,m)\in \D_U}\ell_{(u,m)}(\nodeparam) \notag \\
& + (1-w) \ast \sum_{(a,m) \in \D_A}\ell_{(a,m)}(\nodeparam).
\end{align*}

for a hyperparameter $w \in [0,1]$. This optimization ensures that the movie boxes are contained within the corresponding user and attribute boxes, thereby establishing a set-theoretic inductive bias. {Both numbers of negative samples and $w$ are hyperparameters for training (Please Refer to \cref{sec:experiments}, \cref{app:training_details}) for further details.} 

% We train using a noise-contrastive estimation loss
% \[\L_{U}(\nodeparam, \D_U) = \sum_{(u,m) \in \D_U}\big[ E(u,m; \nodeparam) - \EE_{\tilde m \sim M} \left[\log(1 - \exp(-E(u, \tilde{m}; \nodeparam)))\right]\big].\]
% The first term of each summand is minimized when the box for a given user contains the box representing a particular movie that user has rated. We approximate the second term via sampling, which encourages the box for a user to be disjoint with the box for a movie sampled uniformly randomly.
% We define an analogous loss $\L_{A}$ for attributes, and take a convex combination of these $\L = w \L_U + (1-w) \L_A$ as our loss function.

\subsection{Inference}
\label{sec:inference}
During inference, given the trained embedding model $\nodeparam$ and a user $u$ we determine the user's preference for the movie $m$ by negating and exponentiating the energy function,
% \footnote{In practice, we use the $\log$ of the expected volume for numerical stability, as the volume involves a product over dimensions. As $\log$ is a monotonic transformation, this yields the same ranking of movies.}
\begin{align*}
\score(m,u; \nodeparam) &\defeq \exp\left(-\Energy_\GB(u,m;\nodeparam)\right) \nonumber \\
&= \prod_{d=1}^D F_\GB\left(\nodeparam(u)_d, \nodeparam(m)_d; (\tau, \nu)\right) \in \mathbb{R}_{\geq 0},
\end{align*}
% \GumbelVolInt\left(\nodeparam(m), \nodeparam(u)\right).\]
% \[
% \GumbelVol((x^\llcorner, x^\urcorner), (y^\llcorner, y^\urcorner); (\tau, \nu)) \defeq \LSE_\nu(\LSE_{-\tau}(x^\urcorner, y^\urcorner) - \LSE_\tau(x^\llcorner, y^\llcorner), 0).
% \]

{where $\nodeparam(x)_d = (x_d^\llcorner, x_d^\urcorner).$} Since the calculation is simply a product over dimensions, for notational clarity we will restrict our discussion for more complex queries to the one-dimensional case, and omit the explicit dependence on temperature hyperparameters, so
\[\score(m,u; \nodeparam) \defeq\frac{\GumbelVolInt\left(\nodeparam(m),\nodeparam(u)\right)} {\GumbelVol\left(\nodeparam(m))\right)}\]
which is the \emph{proportion of $\theta(m)$ which is contained within $\theta(u)$} (see \Cref{fig:box_depiction}). It achieves it's maximum at $1$ if $\nodeparam(u)$ contains $\nodeparam(m)$, and is minimized at $0$ when they are disjoint, corresponding to the motivation that $\nodeparam(u)$ represents the set of movies that user $u$ has interacted with.

Given a query with a conjunction between attributes (\eg "comedy and action") we denote the attributes involved $a_1$ and $a_2$.
\mb{This next line is where I was wondering if we had done membership functions or not.}
% Recall that the definition for $F_\GB$ was motivated by replacing the hard volume by an approximate expected volume.
Similarly to the score for a single user query, we define the score for these attributes as the proportion of the movie box $\theta(m)$ which is contained inside of the (soft) intersection of boxes $\theta(u)$, $\theta(a_1)$, and $\theta(a_2)$, \ie
% Thus, to calculate a score for a query involving these attributes, we include the intersection of the corresponding boxes in the numerator, \ie
\small
\[
\score(m,u\wedge a_1 \wedge a_2; \nodeparam) \defeq
\frac{\GumbelVolInt\left(\nodeparam(m),\nodeparam(u), \nodeparam(a_1), \nodeparam(a_2)\right)}
{\GumbelVol\left(\nodeparam(m)\right)}.
\]
\normalsize

Again, this score is maximized if $\nodeparam(m)$ is contained inside $\nodeparam(u), \nodeparam(a_1),$ and $\nodeparam(a_2)$, and minimized when it is disjoint.

In order to address queries with set differences, recall that, given two measurable sets $S$ and $T$, we can compute the volume of $S \setminus T$ as $\Vol(S \setminus T) = \Vol(S) - \Vol(S \cap T).$
Thus, if the query involves a negated attribute (\eg "comedy and not action"), we define
% \[\score(m,u \wedge a_1  \wedge \neg a_2; \nodeparam) \defeq \frac{\GumbelVolInt\left(\nodeparam(m),\nodeparam(u), \nodeparam(a_1)\right) - \GumbelVolInt\left(\nodeparam(m), \nodeparam(u), \nodeparam(a_1), \nodeparam(a_2)\right)}{\GumbelVol\left(\nodeparam(m))\right)}\]

\small
\begin{align*}
\score(m,u \wedge a_1  \wedge \neg a_2; & \nodeparam)  \defeq  \frac{\GumbelVolInt\left(\nodeparam(m),\nodeparam(u), \nodeparam(a_1)\right)}
{\GumbelVol\left(\nodeparam(m)\right)} \notag \\
& - \frac{\GumbelVolInt\left(\nodeparam(m), \nodeparam(u), \nodeparam(a_1), \nodeparam(a_2)\right)}{\GumbelVol\left(\nodeparam(m)\right)}
\end{align*}
\normalsize
This score is maximized when $\nodeparam(m)$ is contained inside $\nodeparam(u)$ and $\nodeparam(a_1)$ while being disjoint from $\nodeparam(a_2)$, and decreases when these conditions are not met.
