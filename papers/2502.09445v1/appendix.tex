% \clearpage
% \newpage
\section*{Appendix}


The organization of the appendix is as follows:

\begin{itemize}
    \item \textbf{Section \ref{appx:sec_A}} provides foundational material necessary for understanding proof methodology.
    \item \textbf{Section \ref{appx:sec_B}} presents the technical results that support the conclusions drawn in our work, particularly in relation to Theorem 4.
    \item \textbf{Section \ref{appx:sec_C}} gives further insights into the Toy Experiment 1.
    \item \textbf{Section \ref{appx:sec_D}} offers details regarding the feature selection datasets, which include both the synthetic dataset and the UCI datasets.
    \item \textbf{Section \ref{appx:sec_E}} contains information about the Waterbirds dataset utilized for feature learning.
    \item \textbf{Section \ref{appx:sec_F}} includes additional comments on the fairness experiments, which involve two UCI datasets and American Community Survey (ACS) data, made available through the Folktables package.
    \item \textbf{Section \ref{appx:sec_G}} elaborates on the experimental analyses and the configuration of hyperparameters.
    \item \textbf{Section \ref{appx:sec_H}} gives three concrete examples of the pseudocodes required for Alg. \ref{algo:general}.
    \item \textbf{Section \ref{sec:fairne_apx}} provides experimental details regarding experiments in Section \ref{sec:fairness}.
    \item \textbf{Section \ref{sec:parameter_beta_choice}} gives empirical evidence for the choice of fixing the parameter $\beta=0.2$ by analyzing the performance of difFOCI for different values of $\beta$.
    \item \textbf{Section \ref{appx:sec_K}} presents experimental evaluation of robustness to domain shift for feature selection using difFOCI \ref{dF1}.
\end{itemize}


\section{Additional Technical Background}
\label{appx:sec_A}
\subsection{Auxiliary results required for the Proof of Thm. \ref{thm:ours}}
\label{appx:aux_results}
In this section, we provide the required results for the proof of Theorem \ref{thm:ours}. We begin by providing the full forms of the estimator, including the case $p=0$.

If $p \geq 1$, the estimate of $T$ reads as follows:

\begin{align}
    T_n=T_n(Y, \mathbf{Z} \mid \mathbf{X}):=\frac{\sum_{i=1}^n\left(\min \left\{r_i, r_{M(i)}\right\}-\min \left\{r_i, r_{N(i)}\right\}\right)}{\sum_{i=1}^n\left(r_i-\min \left\{r_i, r_{N(i)}\right\}\right)}.
\end{align}


And if $p=0$,  we obtain:
\begin{align}
T_n=T_n(Y, \mathbf{Z}):=\frac{\sum_{i=1}^n\left(n \min \left\{r_i, r_{M(i)}\right\}-l_i^2\right)}{\sum_{i=1}^n l_i\left(n-l_i\right)},
\label{eqn:mona_estim_without_x}
\end{align}


with $M(i)$ denoting the index $j$ such that $\mathbf{Z}_j$ is the nearest neighbor of $\mathbf{Z}_i$, and $r_i$, $l_i$ as defined in Sec. \ref{sec:chaterjee}.

We then proceed by defining:
    \begin{enumerate}
        \item $P_n(Y, \bfX) := \frac{1}{n^2} \sum_{i=1}^n (r_i - \min\{r_i, r_{N(i)}\})$, and
        \item $Q_n(Y, \bfZ |\bfX) := \frac{1}{n^2} \sum_{i=1}^n (\min\{r_i, r_{M(i)}\} - \min\{r_i, r_{N(i)}\})$.
    \end{enumerate}

These two quantities are important for the following theorems:

\begin{theorem}\citep{azadkia2021simple}
\label{thm:91_mona}
    Suppose that $p \geq 1$. As $n \rightarrow \infty$, the statistics $Q_n(Y, \mathbf{Z} \mid \mathbf{X})$ and $P_n(Y, \mathbf{X})$ converge almost surely to deterministic limits. Call these limits $a$ and $b$, respectively. Then
    \begin{enumerate}
        \item $0 \leq a \leq b$.
        \item $Y$ is conditionally independent of $\mathbf{Z}$ given $\mathbf{X}$ if and only if $a=0$.
        \item $Y$ is conditionally a function of $\mathbf{Z}$ given $\mathbf{X}$ if and only if $a=b$.
        \item $Y$ is not a function of $\mathbf{X}$ if and only if $b>0$.
    \end{enumerate}

Explicitly, the values of $a$ and $b$ are given by
\begin{align*}
    a=\int \mathbb{E}(\operatorname{Var}(\mathbb{P}(Y \geq t \mid \mathbf{Z}, \mathbf{X}) \mid \mathbf{X})) d \mu(t)    
\end{align*}
and 


\begin{align*}
b & =\int \mathbb{E}\left(\operatorname{Var}\left(1_{\{Y \geq t\}} \mid \mathbf{X}\right)\right) d \mu(t) \\
& =\int \mathbb{E}(\mathbb{P}(Y \geq t \mid \mathbf{X})(1-\mathbb{P}(Y \geq t \mid \mathbf{X}))) d \mu(t) .
\end{align*}
\end{theorem}


Next, suppose that $p=0$. Define $Q_n(Y, \mathbf{Z}):=\frac{1}{n^2} \sum_{i=1}^n\left(\min \left\{r_i, r_{M(i)}\right\}-\frac{L_i^2}{n}\right)$ and $P_n(Y):=\frac{1}{n^3} \sum_{i=1}^n L_i\left(n-L_i\right)$, where $L_i$ is the number of $j$ such that $Y_j \geq Y_i$.. Then, one can show the following:

\begin{theorem}\citep{azadkia2021simple}
\label{thm:92_mona}
As $n \rightarrow \infty, Q_n(Y, \mathbf{Z})$ and $P_n(Y)$ converge almost surely to deterministic limits $c$ and d, satisfying the following properties:
\begin{enumerate}
	\item $0 \leq c \leq d$.
	\item $Y$ is independent of $\mathbf{Z}$ if and only if $c=0$.
	\item $Y$ is a function of $\mathbf{Z}$ if and only if $c=d$.
	\item $d>0$ if and only if $Y$ is not a constant.
\end{enumerate}

Explicitly,
\begin{align*}
c=\int \operatorname{Var}(\mathbb{P}(Y \geq t \mid \mathbf{Z})) d \mu(t),
\end{align*}
and

\begin{align*}
d  =\int \operatorname{Var}\left(1_{\{Y \geq t\}}\right) d \mu(t)  =\int \mathbb{P}(Y \geq t)(1-\mathbb{P}(Y \geq t)) d \mu(t).
\end{align*}
\end{theorem}

The two aforementioned theorems serve as the key ingredients to Theorems \ref{thm:21_mona} and \ref{thm:22_mona}, as well as the proof of Thm. \ref{thm:ours}, which is given in Sec. \ref{appx:sec_B}.

\subsection{Worst-group-accuracy (WGA) methods}
\label{appx:sec_A2}
Below, we mention the two most-popular methods appearing in the literature on WGA maximization:

\paragraph{ERM.} Empirical Risk Minimization (ERM), proposed by \citet{vapnik2006estimation}, chooses the predictor minimizing the empirical risk $\frac{1}{n} \sum_{i=1}^n \ell\left(f\left(x_i\right), y_i\right)$. ERM does not use attribute (group) labels.


\paragraph{Group DRO.} Group Distributionally Robust Optimization (gDRO) as proposed by \citet{sagawa2019distributionally} aims to minimize the maximum loss across different groups. The objective is formulated as: 

\begin{align*}
    \sup _{q \in \Delta_{|G|}} \sum_{g=1}^{|G|} \frac{q_g}{n_g} \sum_{i=1}^{n_g} \ell\left(f\left(x_i\right), y_i\right),
\end{align*}

where $G = Y \times A $ represents the set of all groups, $\Delta_{|G|}$ denotes the $|G|$-dimensional simplex, and $n_g$ is the number of examples belonging to group $g \in G$ within the dataset. As a result, gDRO incorporates attribute labels. Specifically, gDRO assigns a dynamic weight $q_g$ to the minimization of the empirical loss for each group, which is proportional to its current error rate.

\paragraph{Other methods.} The body of literature on robust, worst-group optimization is rapidly expanding, making it infeasible to compare all available methods thoroughly. Additional examples of robust learners that do not utilize attribute information (like ERM) include Learning from Failure \citep{nam2020learning}, the Too-Good-to-be-True prior \citep{dagaev2023too}, Spectral Decoupling \citep{pezeshki2021gradient}, Just-Train-Twice \citep{liu2021just}, and the George clustering algorithm \citep{sohoni2020no}. Conversely, methods that incorporate attribute information (like gDRO and difFOCI) include Conditional Value at Risk \citep{duchi2019distributionally}, Predict then Interpolate \citep{bao2021predict}, Invariant Risk Minimization \citep{arjovsky2019invariant}, and a wide range of domain-generalization algorithms \citep{gulrajani2020search}.


\section{Proof of Theorem 4.}
\label{appx:sec_B}

In this section, we re-state Theorem \ref{thm:ours} and prove it.

\paragraph{Theorem 4.}     Let $\beta\in\mathbb{R}^+$. Suppose that $Y$ is not almost surely equal to a measurable function of $\bfX$. Then, $\lim_{n\rightarrow\infty} \lim_{\beta\rightarrow\infty} T_{n,\beta}= T$ almost surely. 

\begin{proof}
    Let $Y$ be a random variable and $\mathbf{X}=\left(X_1, \ldots, X_p\right)$ and $\mathbf{Z}=\left(Z_1, \ldots, Z_q\right)$ be random vectors, defined on the same probability space. Here $q \geq 1$ and $p \geq 0$. The value $p=0$ means that $\mathbf{X}$ does not have any components. By $\mu$, we denote the law of $Y$. 
    
    Recall that we denote with $r_i$ the rank of $Y_{(i)}$, i.e., the number of $j$ for which $Y_{(j)} \leq Y_{(i)}$, and with $l_i$ the number of $j$ such that $Y_{(j)} \geq Y_{(i)}$. For each index $i$, $N(i)$ is the index $j$ where $\mathbf{X}_j$ is the closest to $\mathbf{X}_i$, and $M(i)$ is the index $j$ where the pair $(\mathbf{X}_j, \mathbf{Z}_j)$ is closest to $(\mathbf{X}_i, \mathbf{Z}_i)$ in $\mathbb{R}^{p+q}$ w.r.t. the Euclidean metric and resolving ties randomly.  
    
    The two quantities $Q_n(Y, \mathbf{Z} \mid \mathbf{X})$ and $P_n(Y, \mathbf{X})$ and their respective limits $a$ and $b$ (see Theorems \ref{thm:91_mona} and \ref{thm:92_mona}) are key to proving Theorems \ref{thm:21_mona} and \ref{thm:22_mona}. In order to prove that $T_{n,\beta}$ converges to the same limit as $T_n$, we have to introduce the following two quantities:

    \begin{enumerate}
        \item $P_{n,\beta}(Y, \bfX) := \frac{1}{n^2} \sum_{i=1}^n \left(r_i - \min\{r_i, r^\top{\bf{S}}_{{\beta}_{ i, \cdot}}\}\right)$, and 
        \item $Q_{n,\beta}(Y, \bfZ |\bfX) := \frac{1}{n^2} \sum_{i=1}^n \left(\min\{r_i, r^\top{\bf{U}}_{{\beta}_{i, \cdot}}\}\} - \min\{r_i, r^\top{\bf{S}}_{{\beta}_{i, \cdot}}\}\}\right)$, 
    \end{enumerate}
    with ${\bf{S}}_{\beta}=\sigma_\beta(-({\bf{M}}+\lambda\Idn))$, ${\bf{U}}_{\beta}=\sigma_\beta(-(\bf{\hat{M}}+\lambda\Idn))$, and $\hat{M}_{i,j}=\|(\bfX_i, \bfZ_i)-(\bfX_j, \bfZ_j)\|$, with $\sigma_\beta$ the softmax function as defined in Sec. \ref{sec:making_foci_dffble}.

Now, define $\gamma_i := |r_i-r_{M(i)}|$ and $\delta_i := |r_i-r_{N(i)}|$. Let $\epsilon = \min (\gamma_1, \dots, \gamma_n, \delta_1, \dots, \delta_n)$. Then, by the continuity properties of $\sigma_\beta(\cdot)$ and setting $\lambda=\max(1e^{10}, \max_{i,j}{\bf{M}}_{i,j}+\epsilon)$, we have $\lim_{\beta\rightarrow\infty} {\bf{S}}_{{\beta}_{i,\cdot}}= \lim_{\beta\rightarrow\infty} \sigma_\beta(-({\bf{M}}+\lambda\Idn))_{{i,\cdot}} = \mathbbm{1}\{ i=\argmax_{j\in[n]\setminus i} - \|{\bf{M}}_{i, \cdot}\|\} = \mathbbm{1}\{i=\argmin_{j\in[n]\setminus i} \|\bfX_i-\bfX_j\|\}=N(i)$. One can similarly show that $\lim_{\beta\rightarrow\infty}{{\bf{U}}_{\beta_i,\cdot}} = \lim_{\beta\rightarrow\infty} \sigma_\beta(-({\bf{\hat{M}}}+\lambda\Idn))_{{i,\cdot}} = M(i)$. Therefore, we can choose $n>N^*=\max(N_1, N_2)$, such that $\forall n>N_1, \max_i |r_{N(i)}-r^\top {\bf{S}}_{{\beta}_{i,\cdot}}|<\epsilon$, and $\forall n>N_2, \max_i |r_{M(i)}-r^\top {\bf{U}}_{{\beta}_{i,\cdot}}|<\epsilon$. Then, we can easily show that $Q_{n,\beta}(Y, \bfZ |\bfX)$ converges to the limit $c$, with $c=\lim_{n\rightarrow\infty}Q_n(Y, \bfZ |\bfX)$:

    \begin{align*}
        |Q_{n,\beta}(Y, \bfZ |\bfX)-c| & = |\frac{1}{n^2} \sum_{i=1}^n (\min\{r_i, r^\top{\bf{U}}_{{\beta}_{i, \cdot}}\}\} - \min\{r_i, r^\top{\bf{S}}_{{\beta}_{i, \cdot}}\}\})-c|  \\
        & \leq |\frac{1}{n^2} \sum_{i=1}^n (\min\{r_i, r_{M(i)}\}\} - \min\{r_i, r_{N(i)})-c| + \frac{2n\epsilon}{n^2} \\
        & \leq |Q_{n}(Y, \bfZ |\bfX)-c| + \frac{2n\epsilon}{n^2},
    \end{align*}
    
    where both terms go to zero as we take $n$ to infinity (for the first term, see Thm. \ref{thm:91_mona}). One can also straightforwardly show that $P_{n,\beta}(Y, \bfX)$ converges to the same limit $b$ as $P_n(Y, \bfX)$. 
    
    Finally, we can closely follow Sec. 10 in \citet{azadkia2021simple} to conclude; For case $p\geq1$, we recall the quantities $a$ and $b$ from the statement of Theorem \ref{thm:91_mona}, and notice that $T=a / b$.  By Theorem \ref{thm:91_mona}, $Q_n \rightarrow a$ and $S_n \rightarrow b$ in probability. Thus, $T_n \rightarrow a / b=T$ in probability. This proves Theorem \ref{thm:ours} when $p \geq 1$. Finally, for case $p=0$, here $T=c / d$, where $c$ and $d$ are the quantities from Theorem \ref{thm:92_mona}.  Note that $T_n=Q_n / S_n$, where $Q_n=Q_n(Y, \mathbf{Z})$ and $S_n=S_n(Y)$. By Theorem \ref{thm:92_mona}, $Q_n \rightarrow c$ and $S_n \rightarrow d$ in probability. Thus, $T_n \rightarrow c / d=T$ in probability. This proves Theorem \ref{thm:ours} when $p=0$.

\end{proof}


\section{Continuation of Toy Experiment 1.}
\label{appx:sec_C}
We present a plot for the two observations discussed in Section \ref{sec:experiments}, as well as Toy Example 1. The left plot shows that the differences between the three observations are all statistically significant, while the right plot highlights two key strengths of our method: it quickly stabilizes, and the parameter norms reflect the variability of the features.


\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth, trim= 0 0 0 0, clip]{images/0_errorbar.pdf}
    \caption{$\mu_i\pm\sigma_i$ for Obs.\hyperlink{obs_1}{1}- \hyperlink{obs_2}{2}}
    \label{plt:0_error_bar}
  \end{subfigure}%
  % \hfill
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth, trim= 0 0 0 0, clip]{images/1_param_evol.pdf}
    \caption{$\|\theta_{t_{i}}\|$ in Toy Exp. 1.}
    \label{plt:1_param_evol}
  \end{subfigure}
    \caption{\looseness=-1Left: Mean and std. across 50 random inits. All expressions yield values significantly greater than zero. Right: Development of the first five parameters in Toy Exp 1. }
  \label{fig:plots}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Environments}
\label{appx:sec_D}

In the following, we provide details on the environments used in our experiments in Section~\ref{sec:experiments}. We list the number of features, samples, and classes in each UCI environment in Table~\ref{tab:num_tasks_samples}.


%
\begin{table}[ht]
\centering
\begin{tabular}{l|ccccc}
 & Spambase  & Toxicity & QSAR & Breast Cancer & Religious\\ \toprule
 $n$ &  4601 & 171 & 8992 & 569 & 8265  \\ 
 $p$ & 57  & 1203 &  1024 & 30 & 590  \\ 
 \# classes & $2$ & $2$ & $2$ & $2$ & $8$ \\  \bottomrule

\end{tabular}
\caption{Feature Selection Dataset details}
\label{tab:num_tasks_samples}
\end{table}


\subsection{Synthetic Environment}
\label{sec:synthetic_env}
Here, we briefly describe how we generate the synthetic environment depicted in Fig. \ref{fig:wholefigure}, the synthetic dataset that is created using trigonometric transformations and permutations of parameters. 

Let $x$ be a linearly spaced vector defined as $x = \text{linspace}(-6, 6, 100)$. Define the parameters $a$, $b$, and $c$ as: $ a = \text{linspace}(0.1, 2, 4)$, $b = \text{linspace}(0.1, 2, 15)$, and $c = \text{linspace}(-1, 1, 4)$, where linspace(a, b, n) represents $n$ uniformly spaced points in the interval $[a,b]$.  Features are then generated using the formula: 
\begin{equation}
    f(x)_{a,b,c} = a \cdot \sin(b \cdot x + c)
\end{equation} 
where $a$, $b$, and $c$ are elements from the Cartesian product of the parameter sets $a$, $b$, and $c$. The features are stored in a matrix $X$ where each column represents a feature vector. For each feature vector, transformations are applied as follows: $X_{\text{new}} = (-1)^{i+1} \cdot X[:, i \cdot 15 + j]$
for $i \in \{0, 1\}$ and $j \in \{0, \ldots, 14\}$. Additional transformations are applied based on a permutation of parameters $c$, $a$, and $b$. The transformed features are: $X_{\text{final}} = -1 \cdot X_{\text{new}}[:, i \cdot 15 + j]$ for selected indices $i$ and all $j$. The final dataset $X$ is obtained by concatenating all transformed features and adding Gaussian noise: $X = X + \mathcal{N}(0, 0.1)$ four times, yielding $n=100$ and $p=4*15*4=240$. The predictor variable is calculated as $y=\sin(x)$ - we do not add further noise here as the features already contain noise.


\subsection{UCI Datasets - feature selection}

Below, we briefly describe the five UCI datasets \citep{uci2019} used in our feature selection comparison. 

\subsubsection{Spambase}

The "Spambase" dataset \citep{hopkins1999spambase} is designed for classifying emails as spam or non-spam. It consists of 4,601 email instances with 57 features, characterized by both integer and real values. The dataset is multivariate and is often used in computer science, with classification as the primary task.

The dataset includes diverse types of spam, such as product ads, money schemes and chain letters. The goal is to identify whether an email is spam, with some non-spam indicators like the word "george" or area code "650" reflecting personalized filters.

\subsubsection{Toxicity}


The "Toxicity" dataset \citep{gul2021toxicity} contains data on 171 small molecules designed for the functional domains of CRY1, a core clock protein involved in circadian rhythm regulation. Of these molecules, 56 are toxic, while the rest are non-toxic. The dataset is tabular, with 1,203 molecular descriptors per instance. The primary task is classifying molecules as toxic or non-toxic.

\subsubsection{QSAR}

The "QSAR Oral Toxicity" dataset \citep{qsar} consists of 8,992 chemical compounds represented by 1,024 binary molecular fingerprint attributes. These attributes are used to classify the chemicals into two categories: very toxic (positive) or not very toxic (negative). The dataset is multivariate and is often used in physics and chemistry, with classification as the main associated task.


\subsubsection{Breast Cancer}

The "Breast Cancer Wisconsin (Diagnostic)" dataset \citep{street1993breastcancer} is used for classifying breast cancer diagnoses based on data from fine needle aspirates (FNA) of breast masses. It consists of 569 instances with 30 real-valued features that describe characteristics of cell nuclei in digitized images. The dataset is multivariate and is often used in the field of health and medicine, with classification as the primary task. The features were created through an exhaustive search using the Multisurface Method-Tree and linear programming techniques to create a decision tree.


\subsubsection{Religious}

The dataset, "A Study of Asian Religious and Biblical Texts," \citep{sah2019biblical} primarily consists of texts sourced from Project Gutenberg. It includes a collection of key religious and philosophical texts, such as the Upanishads, Yoga Sutras, Buddha Sutras, Tao Te Ching, and selections from the Bible (Books of Wisdom, Proverbs, Ecclesiastes, and Ecclesiasticus). The dataset is multivariate and is analyzed in Social Science contexts, with associated tasks including classification and clustering.



\section{Waterbirds dataset - feature learning}
\label{appx:sec_E}


\begin{table}[htbp]
    \centering
    \begin{tabular}{l|lccc}
         &  & \multicolumn{2}{c}{Group Counts} & \\
        \cmidrule(lr){3-4} 
        Dataset & Target & Water & Land & Class Counts \\ \midrule
        \multirow{2}{*}{Waterbirds} & Land bird & 56 & 1057 & 1113 \\ 
        & Water bird & 3498 & 184 & 3682 \\ 
        \bottomrule
    \end{tabular}
    \caption{(Sub)group counts for the Waterbirds Dataset}
    \label{tab:waterbirds_counts}
\end{table}

The Waterbirds dataset consists of images of birds that have been digitally cut and pasted onto various backgrounds. The objective is to classify the specimens as either water birds or land birds. The group attribute indicates whether the bird is depicted in its natural habitat. The details of class counts are given in Tab. \ref{tab:waterbirds_counts}. While performing hyperaparameter search, each experiment is run on one Nvidia Tesla V100 GPU.


\subsection{Generating saliency maps}
Below, we briefly comment on how we obtained the saliency maps used for our experimentation. To generate a saliency map for a given input image, we use the trained ResNet-50 \citep{he2016deep}, pretrained on ImageNet \citep{imagenet} for which the results are reported in Tab. \ref{tab:waterb_table}. For further details regarding saliency maps, we refer the reader to \citet{simonyan2014visualising}.

\paragraph{Forward and backward pass.} Once the input image is prepared (properly resized and rescaled), we perform a forward pass through the model to obtain predictions. Then, the highest predicted score can be identified along with its corresponding class, after which, a backward pass is then executed to compute the gradient of this score with respect to the input image, highlighting which pixels in the image are most influential in determining the model's prediction.

\paragraph{Analyzing the gradients.} The resulting gradients can be analyzed to create a saliency map, which involves calculating the maximum gradient values across the color channels of the input image. This map is then normalized to [0, 1]. Finally, we plot the original image, and the corresponding saliency map can be plotted side by side to illustrate the regions of the image that significantly impacted the model's decision.

\looseness=-1Below (see Fig. \ref{fig:birds1}-\ref{fig:birds2}), we present saliency maps for ten randomly selected samples, demonstrating that difFOCI frequently assists the model (for both ERM and DRO) in relying less on the background, thereby reducing spurious correlations, and directing its attention toward the bird. It is important to note that we do not explicitly encourage the model to engage in any form of segmentation at any point.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_1.pdf}
  \end{subfigure}
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_2.pdf}
  \end{subfigure}
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_3.pdf}
  \end{subfigure}
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_4.pdf}
  \end{subfigure}
    \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_5.pdf}
  \end{subfigure}
  \caption{Five randomly selected samples along with their corresponding saliency maps. In some cases, ERM and gDRO do not rely on the background (as seen in the last row), but they do for others. In these instances, difFOCI reduces the reliance on the background, which can be observed clearly in rows 1, 2, and 3, and to a lesser extent in row 4.}
  \label{fig:birds1}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_6.pdf}
  \end{subfigure}
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_7.pdf}
  \end{subfigure}
  \begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_8.pdf}
  \end{subfigure}
\begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_9.pdf}
  \end{subfigure}
\begin{subfigure}{0.9\textwidth}
    \includegraphics[width=\textwidth]{images/saliency_appx/32619806_1956_10.pdf}
  \end{subfigure}
  \caption{Five randomly selected samples along with their corresponding saliency maps. It is evident that difFOCI has a more pronounced effect in reducing the reliance on the background for ERM compared to DRO. In most cases, the reliance is significantly reduced for ERM (e.g., rows 2, 3, 4, and 5). For DRO, the improvement is less pronounced, with potential minor improvements in rows 3 and 4.}
  \label{fig:birds2}
\end{figure}


\newpage

\section{Fairness experiments}
\label{appx:sec_F}


\begin{table}[t]
\small
\centering
\begin{tabular}{l|cccc}
 & Bank Marketing  & Student Performance & ACS Employment & ACS Income \\ \toprule
 $n$ &  41,188 & 395  & 3,236,107 & 1,664,500  \\ 
 $p$ & 20  & 30  & 17 & 10  \\ 
 \# classes & $2$ & $2$ & $2$ & $2$  \\
 \# Protected attributes & Marital Status & Sex & Race, Sex & Race, Sex \\\bottomrule
\end{tabular}
\caption{Number of samples, parameters, classes, and sensitives for each dataset}
\label{tab:num_tasks_samples_fairness}
\end{table}


\subsection{UCI Datasets - fairness}

\looseness=-1The UCI datasets \citep{uci2019} used in our fairness experiments are briefly described below. 

\subsubsection{Bank Marketing}

The "Bank Marketing" dataset \citep{moro2014bankmarketing} contains data from direct marketing campaigns (phone calls) conducted by a Portuguese bank. The goal is to classify whether a client will subscribe to a term deposit (variable 'y'). The dataset is multivariate, with 45,211 instances and 16 features that are either categorical or integer. The ratio of sensitive marital status is 60-30-10\% (married, single, divorced).

The marketing campaigns often involved multiple contacts with the same client to determine if they would subscribe to the term deposit. This dataset is used in business applications, with classification being the main associated task.

\subsubsection{Student Performance}

The "Student Performance" dataset \citep{cortez2008student} aims to predict the performance of secondary education (high school) students. It consists of 649 instances and 30 integer features, and the associated tasks include classification and regression.

The data collected from two Portuguese schools includes student grades, demographic, social, and school-related information. Two separate datasets cover performance in Mathematics and Portuguese language. The target variable, G3 (final grade, whether it is $\geq12$ or not), is strongly correlated with G1 and G2 (grades from earlier periods), making it more challenging but useful to predict G3 without using G1 and G2, as we do in our experiments. This dataset supports educational performance modeling in the Social Science domain. The ratio of sensitive sex is 50-50\%.


\subsection{Folktables dataset - fairness}

\looseness=-1The two datasets below are taken from the Folktables package \citep{ding2021retiringASCIfolktables}, designed to provide access to datasets derived from the US Census. It features a range of pre-defined prediction tasks across various domains, such as income, employment, health, transportation, and housing. Users can also create new prediction tasks within the US Census data ecosystem. Additionally, the package facilitates systematic studies on the impact of distribution shifts, allowing each prediction task to be applied to datasets covering multiple years and all states in the US. We use the Alabama data from 2018 with the 1 year horizon. 

\subsubsection{Income}

The task is to predict whether an individualâ€™s income exceeds \$50,000 based on a filtered sample of the ACS PUMS data. The sample includes individuals aged 16 and older who reported working at least 1 hour per week over the past year and earning a minimum income of \$100. The \$50,000 threshold was selected to make this dataset a potential replacement for the UCI Adult dataset \citep{kohavi1994data}, although the original paper provides additional datasets with different income thresholds, as detailed in their Appendix B. We use the California data from 2018 with the 1 year horizon. The ratio of sensitive for sex is 50-50\% and for race 62\%-17\%-5\% for White, Asian, Black (other minorities include American Indian,Hawaiian, etc.)


\subsubsection{Employment}

The objective of this task is to predict whether an individual is employed, using a filtered sample from the ACS PUMS data. This sample has been carefully curated to include only those individuals who are between the ages of 16 and 90.  The ratio of sensitive for sex is 50-50\% and for race 62\%-17\%-5\% for White, Asian, Black (other minorities include American Indian,Hawaiian, etc.)

Both tasks contain codes regarding the selected features in Sec. \ref{sec:fairness}. The codes are explained below.

\section*{Demographic variables}
\begin{enumerate}
  \item \textbf{OCCP}: 
  \begin{enumerate}
    \item Person's occupation
    \item approximately 500 categories (management, business, science, arts, service, sales, office, construction, maintenance, production, transportation, material moving, etc.)
  \end{enumerate}
  
  \item \textbf{COW}: 
    \begin{enumerate}
    \item Class of worker
    \item 10 categories (e.g., employee of a private for-profit company, local government employee, state government employee, federal government employee, self-employed, working without pay, etc.)
  \end{enumerate}
  
  \item \textbf{POBP}:
    \begin{enumerate}
    \item Place of birth
    \item approximately 300 categories (countries/states of birth), range of values includes most countries and individual U.S. states
  \end{enumerate}

    
  \item \textbf{SEX}:
    \begin{enumerate}
    \item range of values: Male and Female
  \end{enumerate}
  
  \item \textbf{MAR}: 
    \begin{enumerate}
    \item Person's marital status
    \item 5 categories (married, widowed, divorced, separated, never married, or under 15 years old)
  \end{enumerate}
  
  \item \textbf{ANC}: 
  \begin{enumerate}
    \item Ancestry
    \item 5 different categories (single, multiple, unclassified, not reported, suppressed information)
  \end{enumerate}

    \item \textbf{AGEP}: 
  \begin{enumerate}
    \item Age, Range of Values: 0-99
  \end{enumerate}

    \item \textbf{ESP}: 
  \begin{enumerate}
    \item Employment status of parents
    \item 9 different categories (Living with two parents - both in labor force, living with two parents - father only in labor force, living with father - father in labor force, living with father - father not in labor force, etc.)
    \end{enumerate}

  \end{enumerate}

\section*{Citizenship and Migration}
\begin{enumerate}
  \item \textbf{CIT}:
    \begin{enumerate}
    \item Citizenship status
    \item 5 categories (Born in the U.S., Born abroad of American parent(s), U.S. citizen by naturalization, Not a citizen of the U.S., Born in Puerto Rico, Guam, the U.S. Virgin Islands, or the Northern Marianas)
  \end{enumerate}
  
  \item \textbf{MIG}:
  \begin{enumerate}
    \item Mobility status (whether the person lived at the same location 1 year ago)
    \item 4 categories (N/A if less than 1 year old, Yes - same house, No - outside U.S. and Puerto Rico, No - different house in U.S. or Puerto Rico)
  \end{enumerate}
\end{enumerate}
\section*{Education}
\begin{enumerate}
  \item \textbf{SCHL}:
  \begin{enumerate}
    \item Amount of schooling completed
    \item 24 categories (No schooling completed, Nursery school/preschool, Kindergarten, Grade 1, Grade 2,..., Regular high school diploma, GED or alternative credential, Bachelor's degree, Master's degree, etc.)
  \end{enumerate}

\end{enumerate}
\section*{Race and Ethnicity}
\begin{enumerate}
  \item \textbf{NAT}:
\begin{enumerate}
    \item  Whether native or foreign born
    \item 2 categories (native or not)
  \end{enumerate}
  
  \item \textbf{RAC1P}:
  \begin{enumerate}
    \item Recorded detailed race code
    \item 9 categories (White alone, Black or African American alone, American Indian alone, Alaska Native alone, Asian alone, Some Other Race alone, etc.)
  \end{enumerate}
  
    \item \textbf{MIL}: 
  \begin{enumerate}
    \item Military service
    \item 5 Categories (Less than 17 years old, Now on active duty, On active duty in the past but not now, Only on active duty for training, Never served in the military)
  \end{enumerate}


\end{enumerate}

\section*{Disability and Sensory Impairments}
\begin{enumerate}
  \item \textbf{DIS}:
\begin{enumerate}
    \item  Disability recorded: With or without
  \end{enumerate}
  
  \item \textbf{DEAR}:
\begin{enumerate}
    \item  Hearing difficulty: Yes or No
  \end{enumerate}

  \item \textbf{DEYE}:
\begin{enumerate}
    \item  Vision difficulty: Yes or No
  \end{enumerate}

    \item \textbf{DREM}:
\begin{enumerate}
    \item  Cognitive difficulty: Yes or No
  \end{enumerate}
\end{enumerate}


For further explanation on the codes, we invite the reader to see Appendix B.1  and B.4 in the original paper \citep{ding2021retiringASCIfolktables}. Below, in Table \ref{tab:selected_attributes}, we can see that conditioning on multiple sensitive attributes removes additional features, highlighting the potential of the \textit{vec}-\ref{dF3} method to examine interactions between several sensitive attributes, as well as several features simultaneously.


\begin{table}[ht]
\centering
  \begin{tabular}{l|ccc}
    \toprule
    {\textbf{Data}} & \textbf{Race}  & \textbf{Sex} & \textbf{Both}  \\ \midrule
    Employment  & OCCP & COW & OCCP, COW, \textcolor{red}{POB} \\ \midrule
    \multirow{2}{*}{Income} & \multirow{2}{*}{MAR, ANC} & \multirow{2}{*}{MAR, ANC, CIT, MIG} &  MAR, ANC, CIT, \\
     &  &  & MIG, \textcolor{red}{SCHL}, \textcolor{red}{NAT} \\ \bottomrule
    \end{tabular}
    \caption{ACS dataset features which were not selected when conditioned on race, sex or both, represented in first, second and last column, respectively.}
  \label{tab:selected_attributes}
\end{table}

\section{Experimental configurations}
\label{appx:sec_G}





\begin{table}[ht]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        Dataset & Year & Size & N. Classes & Modality & Architecture \\
        \midrule
        MultiNLI & 2017 & 300k & 3 & Text & BERT \\
        CivilComments & 2019 & 250k & 2 & Text & BERT \\
        CelebA & 2015 & 200k & 2 & Image & ResNet-50 w. ImageNet \\
        NICO++ & 2022 & 90k & 60 & Image & ViT-B w. DINO \\
        MetaShift & 2022 & 3.5k & 2 & Image & VIT-B w. CLIP \\
        \bottomrule
    \end{tabular}
        \caption{Dataset Overview for experiments performed in Section 5.2.}
    \label{tab:dataset_overview}
\end{table}


\begin{table}[ht]
    \centering
    \label{tab:avg_acc}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccccccc}
        \toprule
        Dataset & difFOCI+ERM & difFOCI+DRO & ERM & DRO & JTT & Mixup & IRM \\
        \midrule
        MultiNLI & $\mathbf{81.9 \pm 0.2}$ & $\mathbf{81.8 \pm 0.5}$ & $81.4 \pm 0.1$ & $80.2 \pm 0.6$ & $81.2 \pm 0.4$ & $80.7 \pm 0.1$ & $77.7 \pm 0.3$ \\
        CivilComments & $\mathbf{86.3 \pm 0.1}$ & $81.9 \pm 0.3$ & $85.7 \pm 0.4$ & $82.3 \pm 0.4$ & $84.3 \pm 0.5$ & $84.9 \pm 0.3$ & $85.4 \pm 0.2$ \\
        CelebA & $94.4 \pm 1.1$ & $92.9 \pm 2.1$ & $94.9 \pm 0.2$ & $93.1 \pm 0.6$ & $92.4 \pm 1.6$ & $\mathbf{95.7 \pm 0.2}$ & $94.5 \pm 1.0$ \\
        NICO++ & $\mathbf{85.7 \pm 0.3}$ & $\mathbf{85.8 \pm 0.5}$ & $84.7 \pm 0.6$ & $83.0 \pm 0.1$ & $\mathbf{8\ref{sec:fairness} \pm 0.1}$ & $84.2 \pm 0.4$ & $84.7 \pm 0.5$ \\
        MetaShift & $\mathbf{92.1 \pm 0.2}$ & $\mathbf{91.8 \pm 0.3}$ & $91.3 \pm 0.5$ & $89.0 \pm 0.2$ & $90.7 \pm 0.2$ & $91.2 \pm 0.4$ & $\mathbf{91.5 \pm 0.6}$ \\
        CheXpert & $87.1 \pm 0.3$ & $81.9 \pm 0.5$ & $ 86.5 \pm 0.3$ & $77.9 \pm 0.4$ & $75.7 \pm 1.7$ & $82.2 \pm 5.1$ & $\mathbf{90.0 \pm 0.2}$ \\
        \bottomrule
    \end{tabular}}
    \caption{Average accuracy for benchmark methods on various datasets performed in Section 5.2. We can see that on almost all datasets, diFFOCI performs competitively.}

\end{table}



\begin{table}[ht]
    \centering
    \label{tab:worst_group_acc_appx}
    \setlength{\tabcolsep}{4pt}  % reduce column separation
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccc}
            \toprule
            Dataset & difFOCI+ERM & difFOCI+DRO & ERM & DRO & JTT & Mixup & IRM \\
            \midrule
            MultiNLI & $\mathbf{77.6 \pm 0.1}$ & $\mathbf{77.5 \pm 0.2}$ & $66.9 \pm 0.5$ & $77.0 \pm 0.1$ & $69.6 \pm 0.1$ & $69.5 \pm 0.4$ & $66.5 \pm 1.0$ \\
            CivilComments & $66.32 \pm 0.2$ & $\mathbf{70.3 \pm 0.2}$ & $64.1 \pm 1.1$ & $\mathbf{70.2 \pm 0.8}$ & $64.0 \pm 1.1$ & $65.1 \pm 0.9$ & $63.2 \pm 0.5$ \\
            CelebA & $\mathbf{89.32 \pm 0.4}$ & $\mathbf{89.8 \pm 0.9}$ & $65.0 \pm 2.5$ & $\mathbf{88.8 \pm 0.6}$ & $70.3 \pm 0.5$ & $57.6 \pm 0.5$ & $63.1 \pm 1.7$ \\
            NICO++ & $\mathbf{47.10 \pm 0.7}$ & $46.3 \pm 0.2$ & $39.3 \pm 2.0$ & $38.3 \pm 1.2$ & $40.0 \pm 0.0$ & $43.1 \pm 0.7$ & $40.0 \pm 0.0$ \\
            MetaShift & $83.1 \pm 0.5$ & $\mathbf{91.7 \pm 0.2}$ & $80.9 \pm 0.3$ & $86.2 \pm 0.6$ & $82.6 \pm 0.6$ & $80.9 \pm 0.8$ & $84.0 \pm 0.4$ \\
            \bottomrule
        \end{tabular}%
    }
    \caption{Worst group accuracy for benchmark methods on various datasets performed in Section 5.2. We can see that on almost all datasets, diFFOCI performs competitively.}

\end{table}




For the first two toy examples, we use a one-hidden-layer MLP with a configuration of 10-20-10 neurons. In contrast, the third example employs a two-hidden-layer MLP structured as 10-20-20-10 neurons. For all benchmarks using \textit{vec}-\ref{dF1} and \textit{vec}-\ref{dF3}, we initialize the parameter $\theta$ from a $\theta\sim \mathrm{N}(1, \sigma^2\Idp)$, with $\sigma^2=0.1$. In the case of \textit{NN}-\ref{dF1}, we design a one-layer MLP where the hidden dimension is double that of the input layer, and the output layer has the same number of neurons as the input layer. The ReLU function is used as the activation function. All input data is standardized, and across all benchmarks, we perform a (75-15-10)\% train-validation-test split. For the regression experiments (toy examples), we employ SVR ($C=1.0$, $\epsilon=0.2$) along with the Adam optimizer \citep{kingma2017adammethodstochasticoptimization}. For classification (UCI, Bank Marketing, Student and ACS Datasets), we employ Logistic Regression \citep{cox1958regression}. For the Waterbirds dataset, we train ResNet-50 models pre-trained on ImageNet \citep{imagenet} using the SGD optimizer with the PyTorch \citep{paszke2017automatic} implementation of \textit{BCEWithLogitsLoss}, which combines a Sigmoid layer and the BCELoss in one single class.


We adjust the learning rate and weight decay from the set $\left\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 5^{-4}, 5^{-3}, 5^{-2}\right\}$\footnote{We also experimented with $\ell1$ regularization, but it yielded poorer performance.}. The number of epochs is optimized within the range $\left\{10, 20, 50, 100\right\}$, and batch sizes are chosen from $\left\{8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096\right\}$. Notably, we train the Waterbirds dataset for 360 epochs, in line with previous research. As mentioned earlier, we keep the clipping parameter $\nu=0.1$ and the softmax temperature parameter $\beta=5$ consistent across all experiments. The value of $\eta$ for gDRO is set to 0.1. Each combination of hyperparameters is executed three times to compute the average and standard deviation of the chosen loss metric. We select the best models (in terms of hyperparameter combinations and epochs) based on the lowest MSE/logistic loss observed on the validation set, and for Waterbirds, we choose based on worst-group accuracy. We provide the hyperparameter configurations used to obtain our results in Table \ref{tab:exp_hyperparameters}. .

Finally, for the experiments on NICO++ \citep{zhang2023nico}, MultiNLI \citep{williams2017broad}, CivilComments \citep{borkan2019nuanced} and CelebA \citep{liang2022metashift} we follow the experimental configuration from \citet{yang2023change}, who provided a very useful codebase for benchmarking various methods, which we are thankful for. We briefly describe these datasets below, where the hyperparameter joint distribution is taken directly from their codebase. For each algorithm, we perform a thorough hyperparameter tuning process. This involves conducting 16 random searches over the entire range of hyperparameters. We then use the validation set to identify the optimal hyperparameters for each algorithm. With these hyperparameters fixed, we repeat the experiments three times with different random seeds and report the average results along with their standard deviations. This approach ensures a fair comparison between algorithms, where each is evaluated with its best possible hyperparameters, allowing for a reliable assessment of their performance. We provide brief dataset information below and in Table \ref{tab:dataset_overview}.

\textbf{CelebA} \citep{liang2022metashift}: A binary classification image dataset comprising over 200,000 celebrity face images. The task is to predict hair color (blond vs. non-blond), with gender serving as a spurious correlation. We employ standard dataset splits from prior work \citep{idrissi2022simple} and note that the dataset is licensed under the Creative Commons Attribution 4.0 International license. \\

\textbf{MetaShift} \citep{liang2022metashift}: A dataset creation method leveraging the Visual Genome Project \citep{krishna2017visual}. We use the pre-processed Cat vs. Dog dataset, where the goal is to distinguish between the two animals. The spurious attribute is the image background, with cats more likely to appear indoors and dogs outdoors. We utilize the "unmixed" version generated from the authors' codebase. \\

\textbf{CivilComments} \citep{borkan2019nuanced}: A binary classification text dataset aiming to predict whether an internet comment contains toxic language. The spurious attribute is the presence of references to eight demographic identities. We adopt the standard splits provided by the WILDS benchmark \citep{koh2021wilds}. \\

\textbf{MultiNLI} \citep{williams2017broad}: A text classification dataset with three classes, targeting natural language inference relationships between premises and hypotheses. The spurious attribute is the presence of negation in the text, which is highly correlated with the contradiction label. We use standard train/val/test splits from prior work \citep{idrissi2022simple}. \\

\textbf{NICO++} \citep{zhang2023nico}: a large-scale dataset for domain generalization. Specifically, we focus on Track 1, which involves common context generalization. Our analysis is based on the training dataset, comprising 60 classes and 6 shared attributes: autumn, dim, grass, outdoor, rock, and water. To adapt this dataset for attribute generalization, we identify all pairs of attributes and labels with fewer than 75 samples and exclude them from our training data, reserving them for validation and testing purposes. For each attribute-label pair, we allocate 25 samples for validation and 50 samples for testing, while using the remaining data as training examples.

\textbf{CheXpert} \citep{irvin2019chexpert}:   a collection of chest X-ray images from Stanford University Medical Center, consisting of over 200,000 images. In this study, we use "No Finding" as the label, where a positive label indicates that the patient does not have any illness. Following previous research \citep{seyyed2021underdiagnosis}, we use the intersection of race (White, Black, Other) and gender as attributes. The dataset is randomly divided into 85% training, 5% validation, and 10% testing sets.


\begin{table}[htbp]
  \centering
  \begin{NiceTabular}{cc|cccccc}
    \toprule
    Dataset & Method & Batch size & l.r. & N. epochs & w.d. & Val. loss & Test loss \\
    \midrule
    Synth. Dataset & \textit{vec}-\ref{dF1} & full & \num{5e-2}  & 2000 & \num{1e-1} & 0.02 $\pm$ 0.01 & 0.02 $\pm$ 0.02 \\ \midrule
    Toy Ex. 1 & \multirow{3}{*}{{\textit{vec}-\ref{dF1}}} & \multirow{3}{*}{full} & \num{5e-3}  & 1000 & \num{1e-4} & 0.01 $\pm$ 0.00 & 0.02 $\pm$ 0.00 \\
    Toy Ex. 2 &  &  & \num{10e-1} & 1000 & \num{1e-4} & 0.24 $\pm$ 0.00 & 0.25 $\pm$ 0.00 \\
    Toy Ex. 3 &  &  & \num{5e-2}  & 1000 & \num{5e-4}  & 0.23 $\pm$ 0.00 & 0.24 $\pm$ 0.00 \\
    \midrule
    Toy Ex. 1 & \multirow{3}{*}{\textit{NN}-\ref{dF1}} & \multirow{3}{*}{full} & \num{5e-3} & 1000 & \num{1e-2} & 0.08 $\pm$ 0.01 & 0.08 $\pm$ 0.01  \\
    Toy Ex. 2 &  &  & \num{5e-3} & 1000 & \num{1e-2} & 0.02 $\pm$ 0.01 & 0.02 $\pm$ 0.01 \\
    Toy Ex. 3 &  &  & \num{5e-4} & 1000 & \num{1e-2} & 0.18 $\pm$ 0.00 & 0.18 $\pm$ 0.01 \\
    \midrule
    Spambase  & \multirow{5}{*}{\textit{vec}-\ref{dF1}} & 2048 & \num{1e-2} & 100 & \num{1e-5} & 2.24 $\pm$ 0.14 & 2.56 $\pm$ 0.13 \\
    Toxicity  & & 4096 & \num{5e-2} & 50 & \num{1e-1} & 9.23 $\pm$ 1.96 & 11.61 $\pm$ 0.8 \\
    QSAR  & & 512 & \num{1e-2} & 10 & \num{5e-2} & 2.16 $\pm$ 0.14 & 2.54 $\pm$ 0.07 \\
    Breast Canc. &  & 2048 & \num{1e-3} & 50 & \num{1e-4} & 0.00 $\pm$ 0.00 & 0.00 $\pm$ 0.00 \\
    Biblical &  & 8 & \num{5e-4} & 50 & \num{1e-4} & 0.36 $\pm$ 0.02 & 0.48 $\pm$ 0.03 \\
    \midrule
    Spambase & \multirow{5}{*}{\textit{NN}-\ref{dF1}}  & 64 & \num{1e-4} & 10 & \num{1e-2} & 8.65 $\pm$ 0.91 & 9.61 $\pm$ 1.50 \\
    Toxicity & & 512 & \num{5e-4} & 50 & \num{5e-2} & 1.97 $\pm$ 0.13 & 2.11 $\pm$ 0.11 \\
    QSAR & & 512 & \num{1e-2} & 50 & \num{1e-1} & 2.52 $\pm$ 0.16 & 2.57 $\pm$ 0.19 \\
    Breast Canc. & & 2048 & \num{5e-3} & 0 & \num{1e-6} & 0.34 $\pm$ 0.32 & 0.00 $\pm$ 0.00 \\
    Biblical & & 128 & \num{1e-4} & 20 & \num{5e-4} & 0.69 $\pm$ 0.17 & 0.56 $\pm$ 0.04 \\
    \midrule
    \multirow{2}{*}{Student} & \textit{vec}-\ref{dF1} & 64 & \num{1e-1} & 100 & \num{5e-4} & 8.03 $\pm$ 1.07 & 8.41 $\pm$ 0.82 \\
     & \textit{vec}-\ref{dF3} & 256 & \num{5e-2} & 50 & \num{5e-3} & 7.65 $\pm$ 0.56 & 8.52 $\pm$ 0.89 \\
     \midrule
    \multirow{2}{*}{Bank} & \textit{vec}-\ref{dF1} & 2048 & \num{5e-3} & 50 & \num{1e-5} & 2.61 $\pm$ 0.02 & 2.68 $\pm$ 0.04 \\
    & \textit{vec}-\ref{dF3} & 256 & \num{5e-3} & 50 & \num{5e-4} & 2.59 $\pm$ 0.06 & 2.90 $\pm$ 0.07 \\
    \midrule
    \multirow{2}{*}{ACS Empl.} & \textit{vec}-\ref{dF1} & 64 & \num{5e-2} & 50 & \num{5e-3} & 7.65 $\pm$ 0.08 & 7.81 $\pm$ 0.03 \\
    & \textit{vec}-\ref{dF3} & 256 & \num{5e-3} & 50 & \num{1e-5} & 7.81 $\pm$ 0.01 & 7.97 $\pm$ 0.02 \\
    \midrule
    \multirow{2}{*}{ACS Inc.} & \textit{vec}-\ref{dF1} & 1024 & \num{1e-2} & 10 & \num{1e-4} & 7.65 $\pm$ 0.01 & 7.65 $\pm$ 0.01 \\
     & \textit{vec}-\ref{dF3} & 256 & \num{5e-2} & 100 & \num{5e-3} & 7.90 $\pm$ 0.01 & 7.92 $\pm$ 0.01 \\
    \bottomrule
  \end{NiceTabular}
  \caption{Hyperparameter configurations used throughout the experiments.}
  \label{tab:exp_hyperparameters}
\end{table}


\begin{table}[t]
  \centering
  \begin{NiceTabular}{c|cccccccc}
    \toprule
    Method  & Batch size & l.r. & reg. $\lambda$ & w.d. & Val. Acc. & Test Acc. & Val. WGA & Test WGA \\
    \midrule
    ERM  & 8 & \num{1e-5} & \num{1e-3} & \num{5e-2} & 91.2 & 93.7 & 84.2 & 85.7 \\
     gDRO & 32 & \num{1e-5} & \num{1e-1} & \num{1e-5} & 92.1 & 93.5 & 85.7 & 87.2 \\
    \bottomrule
  \end{NiceTabular}
  \caption{Hyperparameter configurations for Waterbirds experiment with \textit{NN}-\ref{dF2} method.}
  \label{tab:exp_hyperparameters_waterbirds}
\end{table}




\newpage


\section{Algorithmic examples}
\label{appx:sec_H}


In this section, we give three concrete examples of Alg. \ref{algo:general} used in Sections \ref{sec:real_world_data}-\ref{sec:fairness} for completeness: using the \textit{vec}-\ref{dF1}, \textit{NN}-\ref{dF2} and \textit{vec}-\ref{dF3} versions respectively.

\begin{algorithm}[ht]
\caption{difFOCI: version \textit{vec}-\ref{dF1}}
\begin{algorithmic}
\STATE \textbf{Input:} Standardized input $\mathbf{X}\in\mathbb{R}^{n,p}$, $Y\in\mathbb{R}^n$ 
\STATE \textbf{Input:} learning rate $\gamma$, weight decay parameter $\lambda$, batch size $b$, cutoff parameter $\upsilon$, softmax parameter $\beta$

\STATE init. $\theta\sim \mathrm{N}(1, \sigma^2\Idp)$, with $\sigma^2=0.1$ 
 \FOR{$t=1,...,n_{\text{iter}}$}
    \STATE $\mathcal{L} \gets -T_{n, \beta}(Y, \mathbf{\theta}_t\odot \mathbf{X})$ \hfill // Differentiable objective
    \STATE $\mathbf{\theta}_{t+1} \gets \theta_t - \gamma \text{Adam}_{\text{WD}_{\lambda, b}} (\mathcal{L})$   \hfill // Parameter update
\ENDFOR
\STATE $\theta_{\text{final}} = c(\theta_{n_{\text{iter}}}, \upsilon)$ \hfill // Parameter clipping
\STATE \textbf{Output:} parameter $\theta_{\text{final}}$
\end{algorithmic}
\label{algo:appx_ex_1}
\end{algorithm}

Alg. \ref{algo:appx_ex_1} is version of difFOCI used in Section \ref{sec:real_world_data} for feature learning and domain shift experiment.

\begin{algorithm}[ht]
\caption{difFOCI: version \textit{NN}-\ref{dF2}}
\begin{algorithmic}
\STATE \textbf{Input:} Standardized input $\mathbf{X}\in\mathbb{R}^{n,p}$, $Y\in\mathbb{R}^n$, group attribute $\bfX_G$
\STATE \textbf{Input:} learning rate $\gamma$, weight decay parameter $\lambda$, regularization strength $\eta$, batch size $b$, softmax parameter $\beta$, neural network $f_\theta(\cdot)=f_{LL_\theta}(f_{FE_\theta}(\cdot))$, where $f_{LL_\theta}$ and $f_{FE_\theta}$ denote the last layer and the feature extractor respectively, BCEWithLogits loss $\ell(\cdot, \cdot)$
\STATE init. NN parameters $\theta$ 
 \FOR{$t=1,...,n_{\text{iter}}$}
    \STATE $\mathcal{L}_1 \gets \ell(Y, f_{LL_{\theta_t}}(f_{FE_{\theta_t}}(\mathbf{X})))$ \hfill // Standard BCEWithLogits Loss
    \STATE $\mathcal{L}_2 \gets T_{n, \beta}(\bfX_G, f_{FE_{\theta_t}}(\mathbf{X}))$ \hfill // difFOCI regularizer
    \STATE $\mathcal{L} \gets \mathcal{L}_1 + \eta \mathcal{L}_2$ \hfill // Total loss calculation
    \STATE $\mathcal{L}^* \gets w_\text{gDRO} (\mathcal{L}) \text{ or } w_\text{ERM} (\mathcal{L}) $\hfill // Reweighting (in case of DRO)
    \STATE $\theta_{t+1} \gets \theta_t - \gamma \text{SGD}_{\text{WD}_{ \lambda, b}} (\mathcal{L}^*)$   \hfill // Parameter update
\ENDFOR
\STATE $\theta_{\text{final}} = c(\theta_{n_{\text{iter}}}, 0.1)$ \hfill // Final parameter clipping
\STATE \textbf{Output:} neural network parameters $\theta_{\text{final}}$ 
\end{algorithmic}
\label{algo:appx_ex_2}
\end{algorithm} 

Alg. \ref{algo:appx_ex_2} is version of difFOCI used in Section \ref{sec:domain_shift} for feature learning and domain shift experiment.

\begin{algorithm}[ht]
\caption{difFOCI: version \textit{vec}-\ref{dF3}}
\begin{algorithmic}
\STATE \textbf{Input:} Standardized input $\mathbf{X}\in\mathbb{R}^{n,p}$, $Y\in\mathbb{R}^n$, sensitive attribute(s) $\bfX_S$
\STATE \textbf{Input:} learning rate $\gamma$, weight decay parameter $\lambda$, batch size $b$, softmax parameter $\beta$
\STATE init. $\theta\sim \mathrm{N}(1, \sigma^2\Idp)$, with $\sigma^2=0.1$ 
 \FOR{$t=1,...,n_{\text{iter}}$}
    \STATE $\mathcal{L} \gets -T_{n, \beta}(Y, \mathbf{\theta}_t\odot \mathbf{X}\mid \bfX_S)$ \hfill // NN-based differentiable objective
    \STATE $\theta_{t+1} \gets \theta_t - \gamma \text{Adam}_{\text{WD}_{ \lambda, b}} (\mathcal{L})$   \hfill // Parameter update
\ENDFOR
\STATE $\theta_{\text{final}} = c(\theta_{n_{\text{iter}}}, 0.1)$ \hfill // Final parameter clipping
\STATE \textbf{Output:} parameter $\theta_{\text{final}}$
\end{algorithmic}
\label{algo:appx_ex_3}
\end{algorithm} 

Alg. \ref{algo:appx_ex_3} is version of difFOCI used in the fairness Section \ref{sec:fairness}.







\section{Fairness experiments}
\label{sec:fairne_apx}

\subsection{Experiment in Section \ref{sec:fairness}}

Here, we provide experimental details regarding the experiment in Section \ref{sec:fairness}. In this study, we employed a data splitting approach where the dataset was divided into training, validation, and testing sets in a ratio of 75%, 15%, and 10%, respectively. To ensure consistency across all experiments, we utilized the StandardScaler from scikit-learn to transform the data.
For our first network, we implemented a multi-layer perceptron (MLP) with three hidden layers, each featuring ReLU activations. We employed the BCEWithLogits loss function from PyTorch, along with the Adam optimizer as our optimization algorithm. The learning rate and weight decay were set as hyperparameters for the Adam optimizer.
To predict sensitive attributes, we leveraged the last layer of the MLP and trained an additional three-layer MLP on top of it, again utilizing ReLU activations and the BCEWithLogits loss function from PyTorch. The Adam optimizer was used with learning rate and weight decay parameters, and the hidden dimensions had sizes of 128.
When training using the (dF3)objective of diffoci, we employed a three-hidden-layer neural network, where all layers were of size 128. Throughout all experiments, the beta parameter was consistently set to 5.

\subsection{Another study on fairness}

In this section, we perform similar experiments to Section 5.1, however we use the \textit{vec}-\ref{dF3} rather than \ref{dF1}. We note that here we just experiment whether, by conditioning on $X_s$ we can still achieve good performance, which is affirmatively confirmed.

\begin{table}[t]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|ccccccccccc|c}
    \toprule
        & GUS   & S.Per. & FPR & FDR & FWE & K.B. & UMAP & LDA & PCA & FOCI & \textit{vec}-\ref{dF1}    & \textit{vec}-\ref{dF3}      \\ 
    \midrule
Student  & 13.30 & 14.14 & 11.36 & 11.64 & 11.36 & 10.53 & 8.87 &\textbf{8.31} & 8.59 & 10.25& \textbf{8.41 $\pm$ 0.82} &  \textbf{8.52 $\pm$ 0.89}                           \\
Bank  & 3.45  & 3.19   & 3.19  & 3.19   & 3.19   &  3.10 &  3.32 &  3.01 & 3.32 &  3.01       & \textbf{2.68 $\pm$ 0.04}  & 2.90  $\pm$ 0.07                   \\
% Coverage  & 11.63  & 11.63   & 11.41  & 11.41   & 11.41  &  11.41 &  11.41 &  13.06 & \textbf{11.36} &  11.49   & \textbf{11.32 $\pm$ 0.06}       & \textbf{11.29  $\pm$  0.14}                      \\
Income     & 10.39 & 10.39 & 7.62  & 7.62   & 7.62   &  7.71  &   \textbf{7.49} &  7.86  & 8.02 &  7.96 & 7.65 $\pm$ 0.01 & 7.92 $\pm$ 0.01 \\ 
Employment. & 12.09 & 11.23 & 8.31 & 8.31  & 8.31  & 8.41  &  14.60 &  8.67 & 9.01 & 8.43  & \textbf{7.81 $\pm$ 0.03} & 7.97 $\pm$ 0.02            \\ \bottomrule
    \end{tabular}}
    \vspace{-4pt}
    \caption{Fairness experiments and test log-loss. \textit{vec}-\ref{dF1} achieves best performance by not conditioning on sensitive attributes, though \textit{vec}-\ref{dF3} remains competitive even though it conditioning out the information regarding the sensitive data $\mathbf{X_S}$. \vspace{-10pt}}
  \label{tab:fairness_data}
\end{table}


\looseness=-1\paragraph{Environments.} As in Section \ref{sec:fairness}, we use Student dataset \citep{cortez2008student},  Bank Marketing dataset \citep{moro2014bankmarketing}; and two ACS datasets \citep{ding2021retiringASCIfolktables}, \emph{(iii)} Employment and \emph{(iv)} Income. Again, the performance is measured using Logistic Regression \citep{cox1958regression}.

\paragraph{Despite conditioning out sensitive information, difFOCI delivers solid performance.} From Table \ref{tab:fairness_data}, we see that \textit{vec}-\ref{dF3} demonstrates strong performance, regardless of whether we condition on the sensitive data or not. Both algorithms outperform other methods, and, expectedly, we observe a slight decrease in performance when conditioning on the sensitive attribute(s). For the Student dataset, conditioning on sex leads to the exclusion of seven additional features (a total of 11 out of 30), while for the Bank Marketing dataset, conditioning on marital status results in the exclusion of one additional feature (a total of 1 out of 20). 

\paragraph{difFOCI might be useful in intersectional fairness.} In both ACS datasets, conditioning on both sensitives led to the exclusion of previously included features (when conditioning on just one sensitive), as shown in Table \ref{tab:selected_attributes}, in Appendix \ref{appx:sec_E}. This reveals that the additional features, excluded only after considering both sensitives, might contain intertwined relationships with the two sensitives, providing an interesting avenue for intersectional fairness research \citep{gohar2023survey}. We leave this as future work.

\section{Choice for the regularization parameter}
\label{sec:parameter_beta_choice}


\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{4pt}  % reduce column separation
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccccc}
            \toprule
            & $\beta$: & $1e-5$ & $1e-3$ & $1.$ & $5$ & $100$ & $1e5$ & $1e7$ & Standard \\
            \midrule
            \multirow{2}{*}{(dF2) ERM} & Avg. Acc. & $97.1 \pm 0.4$ & $97.0 \pm 0.7$ & $94.2 \pm 0.3$ & $93.7 \pm 0.1$ & $94.6 \pm 0.1$ & $97.4 \pm 0.1$ & $97.4 \pm 0.2$ & $97.3 \pm 0.2$ \\
             & WGA & $61.0 \pm 0.2$ & $62.3 \pm 0.8$ & $84.8 \pm 0.8$ & $85.7 \pm 0.8$ & $85.7 \pm 0.5$ & $63.9 \pm 0.8$ & $61.2 \pm 1.0$ & $60.0 \pm 0.5$ \\
             \midrule
            \multirow{2}{*}{(dF2) DRO} & Avg. Acc. & $97.2 \pm 0.1$ & $97.5 \pm 0.7$ & $93.9 \pm 0.2$ & $93.5 \pm 0.5$ & $93.6 \pm 0.7$ & $97.5 \pm 0.3$ & $97.2 \pm 0.1$ & $97.4 \pm 0.4$ \\
             & WGA & $75.7 \pm 1.0$ & $97.2 \pm 0.3$ & $90.0 \pm 0.4$ & $87.2 \pm 0.6$ & $87.0 \pm 0.3$ & $77.1 \pm 0.6$ & $76.9 \pm 0.3$ & $76.9 \pm 0.8$ \\
            \bottomrule
        \end{tabular}%
    }
    \caption{Results for various $\beta$ on Waterbirds dataset. The results for reasonable values of $\beta$ yield similar performance, and very large or small values result the performance falling back to the standard ERM or DRO performance.}
        \label{tab:results_beta_waterbirds}

\end{table}

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{4pt}  % reduce column separation
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccccc}
            \toprule
            & $\beta$: & $1e-5$ & $1e-3$ & $1.$ & $5$ & $100$ & $1e5$ & $1e7$ & Standard \\
            \midrule
            \multirow{2}{*}{(dF2) ERM} & Avg. Acc. & $91.2 \pm 0.7$ & $91.5 \pm 0.9$ & $92.3 \pm 0.2$ & $92.1 \pm 0.2$ & $91.7 \pm 0.3$ & $91.4 \pm 0.2$ & $91.3 \pm 0.1$ & $91.3 \pm 0.5$ \\
             & WGA & $81.1 \pm 0.2$ & $81.2 \pm 0.1$ & $83.3 \pm 0.2$ & $83.1 \pm 0.5$ & $83.1 \pm 0.7$ & $80.6 \pm 0.1$ & $81.3 \pm 0.3$ & $80.9 \pm 0.3$ \\
             \midrule
            \multirow{2}{*}{(dF2) DRO} & Avg. Acc. & $88.8 \pm 0.2$ & $90.0 \pm 0.4$ & $91.9 \pm 0.3$ & $91.8 \pm 0.3$ & $91.8 \pm 0.1$ & $88.7 \pm 0.3$ & $88.9 \pm 0.2$ & $89.0 \pm 0.2$ \\
             & WGA & $86.1 \pm 0.3$ & $86.2 \pm 0.4$ & $91.5 \pm 0.3$ & $91.7 \pm 0.2$ & $91.9 \pm 0.3$ & $85.8 \pm 0.2$ & $85.9 \pm 0.6$ & $86.2 \pm 0.6$ \\
            \bottomrule
        \end{tabular}%
    }
        \caption{Results for various $\beta$ on MetaShift dataset. The results for reasonable values of $\beta$ yield similar performance, and very large or small values result the performance falling back to the standard ERM or DRO performance.}
    \label{tab:results_beta_metashift}

\end{table}



In this section, we provide empirical evidence of for our parameter $\beta$ choice (we fixed it to $5$), highlighting that although we might observe minor improvements by tuning the parameter, the performance is consistent. 

We observe that difFOCI exhibits robust performance across a range of values for the hyperparameter $\beta$. As long as $\beta$ is set within a reasonable range, avoiding extreme values that either zero out gradients or result in a uniform distribution from the softmax function, difFOCI consistently delivers robust results. This is evident in Tables \ref{tab:results_beta_metashift} and \ref{tab:results_beta_waterbirds}, which present results on the MetaShift and Waterbirds datasets, respectively. Our experiments show that tuning $\beta$ leads to only minor performance improvements, which are largely statistically insignificant. Furthermore, setting $\beta$ to extreme values causes the estimator $T(X_G, f_\theta(X))$ to degenerate to a constant, effectively reducing difFOCI to standard ERM performance.


\newpage

\section{difFOCI's robustness to domain shift}
\label{appx:sec_K}

This section presents experimental results on feature selection using difFOCI's objective \ref{dF1} on CIFAR10/10.1 \citep{recht2018cifar} and DomainNet datasets \citep{peng2019moment}, specifically examining the Real vs Sketch, Clipart vs Sketch, and Sketch vs Quickdraw domain shifts. The results, summarized in Tables \ref{tab:accuracy_difference_cifar10}-\ref{tab:accuracy_difference_combined} (for CIFAR10 vs CIFAR10.1 and DomainNet respectively) demonstrate that difFOCI maintains consistent performance across distribution shifts, with the selected features exhibiting similar performance differences as the full dataset. This consistency highlights difFOCI's ability to effectively handle distribution shifts.

\begin{table}[t]
    \centering
    \begin{tabular}{l|c|c|c|c}
        \hline
        Method & Train Accuracy & Test Accuracy & OOD Accuracy & Difference \\
        \hline
        Standard & $85.20 \pm 2.0$ & $82.75 \pm 1.9$ & $70.2 \pm 1.6$ & $12.55$ \\
        difFOCI with 75\% feats. & $82.66 \pm 1.2$ & $81.7 \pm 2.7$ & $68.95 \pm 0.8$ & $12.22$ \\
        difFOCI with 50\% feats. & $80.19 \pm 2.4$ & $79.4 \pm 1.0$ & $67.9 \pm 1.2$ & $11.5$ \\
        difFOCI with 25\% feats. & $79.55 \pm 2.1$ & $78.72 \pm 1.1$ & $65.40 \pm 2.8$ & $13.32$ \\
        \hline
    \end{tabular}
    \caption{Difference between standard predictive accuracy using ResNet-50 on CIFAR10 and CIFAR10.1}
    \label{tab:accuracy_difference_cifar10}
\end{table}

\begin{table}[t]
    \centering
    \caption{Difference between standard predictive accuracy using Resnet-50 on various DomainNet datasets}
    \label{tab:accuracy_difference_combined}
        \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|c|c|c|c}
        \hline
        Dataset & Method & Train Accuracy & Test Accuracy & OOD Accuracy & Difference \\
        \hline
        \multirow{2}{*}{Real vs. Sketch} & Standard & $88.72 \pm 1.7$ & $78.93 \pm 0.6$ & $29.34 \pm 2.1$ & $49.59$ \\
         & difFOCI, clip at $0.1$ & $85.58 \pm 1.3$ & $77.50 \pm 1.3$ & $27.58 \pm 1.5$ & $49.92$ \\
        \hline
        \multirow{2}{*}{Clipart vs. Sketch} & Standard & $89.98 \pm 2.1$ & $61.85 \pm 0.6$ & $39.43 \pm 2.1$ & $22.42$ \\
         & difFOCI, clip at $0.1$ & $88.95 \pm 1.6$ & $61.96 \pm 1.3$ & $40.34 \pm 1.8$ & $21.92$ \\
        \hline
        \multirow{2}{*}{Sketch vs. Quickdraw} & Standard & $66.51 \pm 1.0$ & $53.17 \pm 0.9$ & $7.01 \pm 1.9$ & $46.16$ \\
         & difFOCI, clip at $.1$ & $65.54 \pm 1.7$ & $52.97 \pm 0.8$ & $6.98 \pm 1.2$ & $45.99$ \\
        \hline
    \end{tabular}}
\end{table}
