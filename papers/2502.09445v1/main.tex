
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%%%%%%%%%%%%% added %%%%%%%%%%%%%
\usepackage{url}
\usepackage{longtable}
\usepackage{array}
\usepackage{soul}

\usepackage{nicematrix, booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{xargs}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{upgreek}

\usepackage{arydshln}

\usepackage{multirow}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage{outlines}
\usepackage{subcaption}  %
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{cleveref}


\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}




\declaretheorem[name=Theorem,numberwithin=section]{thm}


\newtheorem{asu}{Assumption}
\newtheorem{asuH}{Assumption}
\renewcommand\theasu{A\arabic{asu}}
\renewcommand\theasuH{H\arabic{asuH}}
\definecolor{apricot}{rgb}{0.98, 0.81, 0.69}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}

\newcommand{\ucmd}[1]{{\color{red}#1}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcounter{relctr} %% <- counter for relations
\everydisplay\expandafter{\the\everydisplay\setcounter{relctr}{0}} %% <- reset every eq
\renewcommand*\therelctr{\alph{relctr}} %% <- label format

\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand\labelrel[2]{%
  \begingroup
    \refstepcounter{relctr}%
    \stackrel{\textnormal{(\alph{relctr})}}{\mathstrut{#1}}%
    \originallabel{#2}%
  \endgroup
}
\AtBeginDocument{\let\originallabel\label}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newcommand{\LeventIssue}[1]{\textcolor{orange}{Levent: {#1}}}
\newcommand{\Kruno}[1]{\textcolor{blue}{Kruno: {#1}}}


\input{def}

%%%%%%%%%%%%% added end %%%%%%%%%%%%%

\title{A differentiable rank-based objective for better feature learning}

% A differentiable conditional dependence based regularizer for better feature learning
% A DIFFERENTIABLE RANK-BASED REGULARIZER FOR BETTER FEATURE LEARNING
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Krunoslav Lehman Pavasovic\thanks{Correspondence to krunolp@meta.com. \textsuperscript{\textdagger} Joint last author.}\\
Meta FAIR, Paris\\
 \\
\And
David Lopez-Paz\\
Meta FAIR, Paris\\
\\
\And
Giulio Biroli\textsuperscript{\textdagger}\\
ENS Paris\\
\\
\And
Levent Sagun\textsuperscript{\textdagger} \\
Meta FAIR, Paris \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% \input{content/0_abstract}
% \input{content/1_introduction}
% \input{content/2_preliminaries_and_technical_background}
% \input{content/3_main_results}
% \input{content/4_experiments}
% \input{content/5_real_experiments}
% \input{content/6_conclusion}


\begin{abstract}
In this paper, we leverage existing statistical methods to better understand feature learning from data.
We tackle this by modifying the model-free variable selection method, Feature Ordering by Conditional Independence (FOCI), which is introduced in \cite{azadkia2021simple}. 
While FOCI is based on a non-parametric coefficient of conditional dependence, we introduce its parametric, differentiable approximation. With this approximate coefficient of correlation, we present a new algorithm called difFOCI, which is applicable to a wider range of machine learning problems thanks to its differentiable nature and learnable parameters.
We present difFOCI in three contexts: (1) as a variable selection method with baseline comparisons to FOCI, 
(2) as a trainable model parametrized with a neural network, 
and (3) as a generic, widely applicable neural network regularizer, one that improves feature learning with better management of spurious correlations. We evaluate difFOCI on increasingly complex problems ranging from basic variable selection in toy examples to saliency map comparisons in convolutional networks. We then show how difFOCI can be incorporated in the context of fairness to facilitate classifications without relying on sensitive data.
\end{abstract}

\section{Introduction}

\looseness=-1Feature learning is crucial in machine learning (ML), enabling models to learn meaningful representations of the data. It can improve performance, reduce dimensionality, increase interpretability, and provide flexibility for adapting to new data distributions and tasks \citep{bengio2012unsupervised, bengio2013representation}. However, increasing model transparency \citep{arrieta2020explainable,rauker2023toward}, improving disentanglement and understanding architectural biases \citep{bouchacourt2021grounding, roth2022disentanglement}, as well as learning invariances to improve robustness \citep{arjovsky2019invariant} have proven to be challenging. 



\looseness=-1In this paper, we propose a new feature-learning approach that relies on ranks, notion seldom explored in the literature but thoroughly studied in statistics. The importance of ranks is evident, from independence tests \citep{bergsma2014consistent, blum1961distribution,  csorgHo1985testing, deb2023multivariate,drton2020high} and sensitivity analysis \citep{gamboa2018sensitivity}, to multivariate analysis \citep{sen1971nonparametric} and measuring deviation \citep{rosenblatt1975quadratic}. However, most of these methods are nonparametric and, therefore, not easily extendable to feature learning with neural networks (NNs). While there are a handful of feature learning works that rely on rank notions \citep{kuo2017feature, wojtas2020feature, fan2023few,li2023deep}, these works do so indirectly and through reliance on two NNs; one that optimizes for a non-rank-based feature-learning objective and another that learns how to rank those learned features according to some similarity measure. 


To fill this gap, we propose difFOCI, a parametric relaxation of the nonparametric, rank-based measures of correlation \citep{chatterjee2020original, azadkia2021simple}, which generalizes the measure proposed by \citet{dette2013copula}, roots of the idea can be related to the RÃ©nyi correlation \citep{bickel1993efficient, renyi1959measures}. To the best of our knowledge, difFOCI is the first parametric framework that directly optimizes a rank-based objective, making it directly applicable to numerous applications in machine learning, including end-to-end trainable neural networks. We demonstrate that difFOCI yields strong results in various areas, including \emph{(i)} feature selection, \emph{(ii)} domain shift and spurious correlation, and \emph{(iii)} fairness experiments.


\paragraph{Organization of the paper.} In Section 2, we introduce the notation and technical background. In Section 3, we outline the main results of this paper, explaining the proposed metric, and establishing its theoretical properties. We analyze it in toy examples that demonstrate solid performance. In Section 4, we extend difFOCI, showcasing its strengths in three examples. In Section 5, we highlight its wide applicability to real-world data, showing it achieves state-of-the-art performance on feature selection and dimensionality reduction, and competitive performance in domain shift and fairness literature. Finally, in Section 6, we conclude with a few remarks on the potential future applications.


\section{Preliminaries and technical background}
\label{sec:preliminaries_and_technical}

\subsection{Notation and preliminary definitions}

We let $\Idd$ denote the $d \times d$ identity matrix and $[n] = \{1, \dots, n \}$. We let $S(A)=\pi_1(A),...,\pi_{n!}(A)$ be the set of all permutations of a set $A$, with $|A|=n$. For a matrix $\bfX$, we denote the set of all permutations its columns by $S(\bfX)$ and by $\pi_i^j(\bfX)$, we represent the $i$-th element of the $j$-th permutation. We denote its $p$-th through $q$-th column as ${\bf{X}}_{p:q}$, with $p>q$, $p,q\in\mathbb{N}$.  We define the Hadamard product between a vector $\mathbf{\alpha}\in \mathbb{R}^p$ and a matrix $\mathbf{X}\in\mathbb{R}^{n,p}$ as $(\alpha\odot\mathbf{X})_{i,j}:=\alpha_i\mathbf{X}_{i,j}$. We represent the scaled Softmax function with $\sigma_\beta(x)$, where $\sigma_\beta(x)_{i}=e^{\beta x_i}/\sum_{j=1}^d e^{\beta x_j}$, for $x\in\mathbb{R}^d$, $\beta\in\mathbb{R}^+$. Finally, we use $c(x,p)$ to denote zeroing out any $x_i$ with $|x_i|\leq p$, for $x\in\mathbb{R}^d$ and $p\in\mathbb{R}$.

\subsection{Chaterjee's coefficient}
\label{sec:chaterjee}

We present the novel rank-based estimator developed by \citet{chatterjee2020original}, which is the first of two foundational works necessary for our approach. Consider a random vector $(X, Y)$ on a probability space $(\Omega ,\mathcal {F},\mathbb{P})$, with $Y$ being non-constant and governed by the law $\mu$. The estimator approximates the following rank-based measure \citep{dette2013copula}:


\begin{align}
        \xi(X, Y):=\frac{\int \operatorname{Var}\left(\mathbb{E}\left(\mathbbm{1}_{\{Y \geq t\}} \mid X\right)\right) d \mu(t)}{\int \operatorname{Var}\left(\mathbbm1_{\{Y \geq t\}}\right) d \mu(t)}.
        \label{eqn:chaterjee_measure}
\end{align}

\looseness=-1\citet{chatterjee2020original} establishes a straightforward estimator for (\ref{eqn:chaterjee_measure}) that has simple asymptotic theory, enjoys several consistency results and exhibits several natural properties; \emph{(i)} normalization: $\xi(X, Y)\in[0,1]$, \emph{(ii)} independence: $\xi(X, Y)=0 \iff Y \independent X$, \emph{(iii)} complete dependence: $\xi(X, Y)=1 \iff Y \text{ a measurable function of } X$ a.s., and \emph{(iv)} scale invariance: $\xi(aX, Y)=\xi(X, Y), a\in\mathbb{R}^*$.  To estimate $\xi$, consider \iid~pairs $\left(X_i, Y_i\right)_{i=1}^n\sim(X, Y)$, with $n \geq 2$. Rearrange the data as $\left(X_{(1)}, Y_{(1)}\right), \ldots,\left(X_{(n)}, Y_{(n)}\right)$, such that $X_{(1)} \leq \cdots \leq X_{(n)}$, breaking ties uniformly at random. Define $r_i$ as the rank of $Y_{(i)}$, i.e., the number of $j$ for which $Y_{(j)} \leq Y_{(i)}$, and $l_i$ as the number of $j$ such that $Y_{(j)} \geq Y_{(i)}$. The estimator is then defined as:

\begin{align}
    \xi_n(X, Y):=1-\frac{n \sum_{i=1}^{n-1}\left|r_{i+1}-r_i\right|}{2 \sum_{i=1}^n l_i\left(n-l_i\right)}.
    \label{eqn:chaterjee_estim}
\end{align}


Furthermore, \citet{chatterjee2020original} establishes the following consistency result for $\xi_n$:

\begin{theorem}\citep{chatterjee2020original}
    If $Y$ is not almost surely a constant, then as $n \rightarrow \infty$, $\xi_n(X, Y)$ converges almost surely to the deterministic limit $\xi(X, Y)$.
    \label{chaterjee_thm1}
\end{theorem}


\looseness=-1In simulations by \citet{chatterjee2020original}, this estimator demonstrates greater efficacy than most signal-detection tests. Its applications span diverse areas: approximate unlearning \citep{mehta2022deep}, topology \citep{deb2020measuring}, black carbon concentration estimation \citep{tang2023black}, sensitivity analysis \citep{gamboa2022global}, and causal discovery \citep{li2023nonlinear}. Extensive further research has been conducted: its limiting variance under independence \citep{han2022azadkiachatterjees}, permutation testing \citep{kim2022local}, bootstrapping \citep{lin2024failure}, rate efficiency \citep{lin2023boosting}, minimax optimality \citep{auddy2023exact} and kernel extension \citep{huang2022kernel}; \citet{bickel2022measures} analyzed it for independence testing, showing it might have no power or prove misleading\footnote{This does not impact us, however, as we do not utilize it for independence testing.}.

\subsection{Extending the coefficient for estimating conditional dependence}


In a subsequent study, \citet{azadkia2021simple} extend the coefficient (\ref{eqn:chaterjee_measure}) $\xi$ to a measure $T(Y, \bfZ \mid \bfX)$, capturing the strength of the conditional dependence between $Y$ and $\bfZ$, given $\bfX$. $T$ can be interpreted as a non-linear extension of the partial $R^2$ statistic \citep{draper1998applied}, and reads as follows: 

\begin{align*}
    T=T(Y, \mathbf{Z} \mid \mathbf{X}):=\frac{\int \mathbb{E}(\operatorname{Var}(\mathbb{P}(Y \geq t \mid \mathbf{Z}, \mathbf{X}) \mid \mathbf{X})) d \mu(t)}{\int \mathbb{E}\left(\operatorname{Var}\left(\mathbbm{1}_{\{Y \geq t\}} \mid \mathbf{X}\right)\right) d \mu(t)},
\end{align*}


where $Y$ denotes a random variable governed by $\mu$, and $\mathbf{X} = (X_1, \ldots, X_p)$ and $\mathbf{Z} = (Z_1, \ldots, Z_q)$ are random vectors, defined within the same probability space, with \iid~copies $\left(\bfX_i, \bfZ_i, Y_i\right)_{i=1}^n\sim(\bfX, \bfZ, Y), n\geq2$. Here, $q \geq 1$ and $p \geq 0$, with $p = 0$ indicating $\mathbf{X}$ has no components.



The statistic $T$ generalizes the univariate measure in (\ref{eqn:chaterjee_measure}). To construct its estimator, for each index $i$, define $N(i)$ as the index $j$ where $\mathbf{X}_j$ is the closest to $\mathbf{X}_i$, and $M(i)$ as the index $j$ where the pair $(\mathbf{X}_j, \mathbf{Z}_j)$ is closest to $(\mathbf{X}_i, \mathbf{Z}_i)$ in $\mathbb{R}^{p+q}$ w.r.t. the Euclidean metric and resolving ties randomly. The estimate of $T$ is given by:

\begin{align}
    T_n=T_n(Y, \mathbf{Z} \mid \mathbf{X}):=\frac{\sum_{i=1}^n\left(\min \left\{r_i, r_{M(i)}\right\}-\min \left\{r_i, r_{N(i)}\right\}\right)}{\sum_{i=1}^n\left(r_i-\min \left\{r_i, r_{N(i)}\right\}\right)}.
    \label{eqn:mona_estim_with_x}
\end{align}


with $M(i)$ denoting the index $j$ such that $\mathbf{Z}_j$ is the nearest neighbor of $\mathbf{Z}_i$, $p\geq1$ and $r_i$, $l_i$ as defined in Sec. \ref{sec:chaterjee}\footnote{The expression for $p=0$ is given in Appendix \ref{appx:aux_results}.}. The authors establish the same four natural properties for $T$ as for the estimator in (\ref{eqn:chaterjee_measure}) - normalization, independence, complete dependence, and scale invariance:


\begin{theorem}\citep{azadkia2021simple}
    Suppose that $Y$ is not almost surely equal to a measurable function of $\mathbf{X}$. Then $T$ is well-defined and $0 \leq T \leq 1$. Moreover, $T=0$ iff $Y$ and $\mathbf{Z}$ are conditionally independent given $\mathbf{X}$, and $T=1$ iff $Y$ is almost surely equal to a measurable function of $\mathbf{Z}$ given $\mathbf{X}$.
    \label{thm:21_mona}
\end{theorem} 

The authors further demonstrate that $T_n$ is indeed a consistent estimator of $T$:

\begin{theorem}\citep{azadkia2021simple}
    Suppose that $Y$ is not almost surely equal to a measurable function of $\mathbf{X}$. Then as $n \rightarrow \infty, T_n \rightarrow T$ almost surely.
    \label{thm:22_mona}
\end{theorem} 



\subsection{FOCI: A new paradigm for feature selection}

 \looseness=-1\citet{azadkia2021simple} utilize the estimator $T_n$ to propose a novel, model-independent, step-wise feature selection method. The method, termed FOCI: Feature Ordering by Conditional Independence, is free from tuning parameters and demonstrates provable consistency. FOCI is outlined in Alg. \ref{algo:FOCI}, where we observe its iterative nature: variables are chosen one by one until the estimator's value drops below zero.

\begin{algorithm}[htbp]
\caption{FOCI}
\begin{algorithmic}
\STATE \textbf{Input:} $n$ \iid~copies of $(Y, \bfX)$, with the set of predictors $\mathbf{X}=$ $\left(X_j\right)_{j\in[p]}$ and response $Y$
\STATE $j_1\gets\argmax_{j\in[p]} T_n(Y, X_j)$
\IF{$T_n(Y,X_{j_1})\leq0$}
    \STATE $\hat{S}=\emptyset$
\ELSE
    \WHILE{$T_n\left(Y, X_j \mid X_{j_1}, \ldots, X_{j_k}\right)>0$}
        \STATE $j_{k+1}\gets\argmax_{[p]\setminus\{j_1,...,j_k\}}$ $T_n\left(Y, X_j \mid X_{j_1}, \ldots, X_{j_k}\right)$ %\hfill // Choose index that maximizes $T_n$
    \ENDWHILE
    \STATE $\hat{S}=\{j_1,...,j_{k'}\}$
    
    \ENDIF
\STATE \textbf{Output:} Set $\hat{S}$ of chosen predictors' indices
\end{algorithmic}
\label{algo:FOCI}
\end{algorithm} 


FOCI performs well on both simulated and real-world datasets. In a toy example with $Y = X_1 X_2 + \sin(X_1 X_3)$, where $X_i\sim\mathrm{N}(0, \sigma^2 \Idp)$, $\sigma^2=1$, and $i\in[2000], p=100$, FOCI selects the correct subset 70 percent of the time. In contrast, popular scikit-learn feature selection algorithms \citep{pedregosa2011scikit}, explained in Sec. \ref{sec:experiments}, almost never identify the correct subset (difFOCI, proposed in the next section, consistently selects the correct subset while preserving the same relative feature importance as FOCI during its correct runs). When applied to real-world datasets, FOCI matches the performance of established methods while requiring up to four times fewer features. 

\subsection{Extending $T$ to machine and deep learning}

From a statistical point of view, both $\xi_n$ and $T_n$ exhibit several strengths: well-established theoretical properties, are non-parametric, have no tunable parameters nor any distributional assumptions. Furthermore, a simple application of $T_n$ results in a strong feature-selection baseline. However, the non-smooth nature of the objectives in (\ref{eqn:chaterjee_estim}) and (\ref{eqn:mona_estim_with_x}) renders them non-differentiable, and therefore not applicable to most ML applications\footnote{Even if applicable, FOCI is often not well-suited for deep learning applications, as shown in Sec. \ref{sec:domain_shift}.}.


In the following section, we make these objectives differentiable using straightforward, well-known tricks in the ML community. This allows us to extend them to various ML and deep learning applications (as showcased in Sec. \ref{sec:experiments}). Moreover, it also allows to account for interactions between all features simultaneously (rather than in a step-wise fashion as in FOCI). Although FOCI could account for this in principle, as can be seen from Alg. \ref{algo:FOCI}, this would increase FOCI's complexity from $O(p^2)$ to potentially $O(2^p)$ thus preventing its practical use.

\section{Main results}
\label{sec:main_results}
We now propose an alternative formulation to the estimator $T_n$ in (\ref{eqn:mona_estim_with_x}), the objective of FOCI. As we will show later, this variation allows for the retention of FOCI's strengths as well as the improvement of its shortcomings. 

\subsection{difFOCI: towards a differentiable version of FOCI}
\label{sec:making_foci_dffble}

The initial step involves making the objective $T_n(Y, \bfZ | \bfX)$ differentiable w.r.t inputs $\bfZ$. Implementing this can be accomplished using straightforward techniques. We employ the following approach:

\begin{enumerate}
\item Compute the pairwise distance matrix $\mathbf{M}\in\mathbb{R}^{n,n}$ where $M_{i,j}=\|\bfX_i-\bfX_j\|$.
\item Calculate ${\bf{S}}_{\beta} \in \mathbb{R}^{n,n}$ such that ${\bf{S}}_{\beta}=\sigma_\beta(-({\bf{M}}+\lambda\Idn))$\footnote{Throughout the experiments, we use $\lambda=\max(1e^{10}, \max_{i,j}{\bf{M}}_{i,j}+\epsilon)$.}.
\item Instead of indexing $r_{N(i)}=r[N(i)]$, utilize $r^\top\mathbf{S}_{\beta_{i,\cdot}}$.
\end{enumerate}


Similarly, for ${\bf{U}}_{\beta}:=\sigma_\beta(-(\bf{\hat{M}}+\lambda\Idn))$, and $\hat{M}_{i,j}=\|(\bfX_i, \bfZ_i)-(\bfX_j, \bfZ_j)\|$. This allows us to present difFOCI, a differentiable version of the estimator in (\ref{eqn:mona_estim_with_x}):

\begin{align}
    T_{n,\beta}=T_{n,\beta}(Y, \bfZ|\bfX):=\frac{\sum_{i=1}^n (\min\{r_i, r^\top{\bf{U}}_{{\beta}_{i, \cdot}}\}\} - \min\{r_i, r^\top{\bf{S}}_{{\beta}_{i, \cdot}}\}\})}{\sum_{i=1}^n (r_i -  \min\{r_i, r^\top{\bf{S}}_{{\beta}_{i, \cdot}}\}\})}.
    \label{eqn:new_obj}
\end{align}
 Using the following theorem, we establish that our new estimator (\ref{eqn:new_obj}) enjoys the same limiting theoretical properties as the estimator in (\ref{eqn:mona_estim_with_x}):

\begin{theorem}%{thm}{ours}
     Let $\beta\in\mathbb{R}^+$. Suppose that $Y$ is not almost surely equal to a measurable function of $\bfX$. Then, $\lim_{n\rightarrow\infty} \lim_{\beta\rightarrow\infty} T_{n,\beta}= T$ almost surely. 
    \label{thm:ours}
\end{theorem}

The proof's core argument (given in Appendix \ref{appx:sec_B}) is based on demonstrating that the quantities $r^\top{\bf{U}}_{{\beta}_{i, \cdot}}$ and $r^\top{\bf{S}}_{{\beta}_{i, \cdot}}$ converge to $r_{M(i)}$ and $r_{N(i)}$ respectively as the inverse temperature parameter $\beta$ approaches infinity. Once this convergence is established, the remainder of the proof follows easily from Theorems \ref{thm:91_mona} and \ref{thm:92_mona} in \citet{azadkia2021simple}, outlined in Appendix \ref{appx:sec_A}.


Making the estimator differentiable allows us to use $T_{n,\beta}$ in various ways. Considering the predictors $\bfX$, response variable $Y$ and potentially available sensitive attributes $\bfX_S$ or group affiliations $\bfX_G$, parameterization $f_{\mathbf{\theta}}$, we highlight three ways to use $T_{n, \beta}$:


\begin{enumerate}[label=\textbf{(dF\arabic*)}]
    \item $T_{n, \beta}(Y, f_{\mathbf{\theta}}(\bfX))$: as a maximization objective, learning features that preserve ranks in the same fashion as the response \label{dF1}
    \item $\ell(Y, \hat{Y}) + \lambda T_{n, \beta}(\mathbf{X_G}, f_{\mathbf{\theta}}(\bfX))$: as a regularizer, penalizing the outputs (or learned features) $f_{\mathbf{\theta}}(\bfX)$ for being dependent on the protected groups $\bfX_G$, where $\ell$ denotes the standard loss used in machine learning \label{dF2}
    \item $T_{n, \beta}(Y, f_{\mathbf{\theta}}(\bfX) | \bfX_S)$: as a conditioning objective, allowing to learn features that contain information about the response only after conditioning out the sensitive information $\bfX_S$ \label{dF3}
\end{enumerate}

For instance, \ref{dF1} can be utilized for feature selection or dimensionality reduction techniques. \ref{dF2} can be employed to prevent the network from relying on spurious correlations when group attributes are available. \ref{dF3} can be applied in fairness scenarios where we aim to avoid predictions based on certain personal information.

The remaining task is to select the parameterization $f_{\mathbf{\theta}}(\cdot)$. In the following sections, we will focus on two options: \emph{(i)} \textit{vec} - a dot product parameterization $f_{\mathbf{\theta}}(\bfX)=\theta \odot \bfX$, or \emph{(ii)} \textit{NN} - a neural network parameterization, $f_{\mathbf{\theta}}(\cdot)$\footnote{For example, with \textit{vec}-\ref{dF1} we denote using \ref{dF1} with vector parameterization.}$^{,}$\footnote{We also tried \textit{vec-NN} parameterization $f_{\mathbf{\theta}}(\bfX)=\theta_2 \odot f_{\mathbf{\theta}_1}(\bfX)$, with $\mathbf{\theta}=\{\mathbf\theta_1, \mathbf\theta_2\}$ but it did not show any improvements over the \textit{NN} parameterization.}. Algorithm \ref{algo:general} provides a general outline for using the $T_{n,\beta}$ with a chosen parameterization, and specific instances of the algorithm are given in Appendix \ref{appx:sec_G}.


\begin{algorithm}[ht]
\caption{Differentiable FOCI (difFOCI)}
\begin{algorithmic}
\STATE \textbf{Input:} predictor $\mathbf{Z}\in\mathbb{R}^{n,p}$, response $Y\in\mathbb{R}^n$, and optional $\bfX\in\{\emptyset, S, G\}$, for sensitive $S\in\mathbb{R}^{n, d}$ or group info. $G\in\mathbb{R}^{n,d}$, $d\geq1$
\STATE \textbf{Input:} parameterization $f_{\mathbf{\theta}}\in\{\textit{vec}, \textit{NN}\}$, objective choice $T_{n,\beta}\in \{\ref{dF1}, \ref{dF2}, \ref{dF3}\}$
\STATE Initialize ${\mathbf{\theta}}$ 
 \FOR{$t=1,...,n_{\text{iter}}$}
    \STATE $\mathcal{L} \gets T_{n, \beta}(Y, f_{{\mathbf{\theta}}_t}(\bfZ) | \bfX)$ \hfill // Applying difFOCI
    \STATE Update ${\mathbf{\theta}}_{t+1} \gets \text{Optim}(\mathcal{L}, {\mathbf{\theta}}_t)$    \hfill // Parameter update
\ENDFOR
\STATE \textbf{Output:} parameterization $f_{\mathbf{\theta}}$
\end{algorithmic}
\label{algo:general}
\end{algorithm} 



\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/2a_functions.pdf} % Adjust the path and filename
        \caption{Generating functions of functional process}
        \label{plt:2a_functions}
    \end{subfigure}
    \hfill % Optional: add some horizontal separation
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/2b_selected.pdf} % Adjust the path and filename
        \caption{First plot: norms of $\theta$. Remaining plots: features with 5 largest param. norms (only first 3 selected).}
        \label{plt:2b_selected}
    \end{subfigure}
    \caption{Synthetic dataset experiment, detailed in Sec. \ref{sec:preliminary_synthetic_study}. Out of 240 total features, our \textit{vec}-\ref{dF1} selects three informative, yet diverse features (corresponding to norms $0.27$, $0.23$, and $0.18$).}
    \label{fig:wholefigure}
\end{figure}



\begin{table}[b]
  \caption{Feature selection benchmark results in terms of test MSE. Our algorithms consistently yield the most accurate predictions while selecting one of the smallest feature subsets (as seen in (\ref{sim_data})). With $\hat\mu_y$, we denote predicting the overall mean and with \textit{Full}, regressing to the whole dataset.}
  \centering
  \begin{subtable}{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccccc}
      \toprule
          & GUS   & S.Per. & FPR & FDR & FWE & K.B. 2 & K.B. 50 & K.B. 75 & FOCI     & \textit{vec}-\ref{dF1}  & \textit{NN}-\ref{dF1}     \\ 
      \midrule
  \# Feat. Select. & 1     & 24    & 112   & 95    & 53    & 2*    & 50*   & 100*  & 6     & 2   & N/A   \\
  Test MSE              & 0.086 & 0.028 & 0.027 & 0.028 & 0.030 & 0.084 & 0.030 & 0.028 & 0.030 & 0.016 $\pm$ 0.02 & \textbf{0.012} $\pm$ \textbf{0.01}
       \\ \bottomrule
      \end{tabular}}
      \caption{Results from simulated data study, detailed in Sec. \ref{sec:preliminary_synthetic_study}. Both \ref{dF1} versions successfully inherit FOCI's strengths: they select a small number of features while exhibiting solid performance.}
      \label{sim_data}
  \end{subtable}
  
  \vspace{0.25cm}
  
  \begin{subtable}{\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccccccccc}
      \toprule
          & $\hat\mu_y$ & Full  & GUS   & S.Per. & FPR & FDR & FWE & K.B. & UMAP  & PCA & FOCI    & \textit{vec}-\ref{dF1}    & \textit{NN}-\ref{dF1}\\ 
      \midrule
  Exp 1. & 1.38 & 0.22 & 0.93 & 0.94 & 0.53 & 0.54 & 0.68 & 0.54  & 1.14  & 1.02 & 0.21  & \textbf{0.02 $\pm$ 0.00} & 0.08 $\pm$ 0.01 \\
  Exp 2. & 0.49 & 0.58 & 0.53 & 0.58 & 0.58 & 0.59 & 0.58 & 0.58 & 0.55 & 0.52 & 0.53 & 0.24 $\pm$ 0.00 & \textbf{0.02 $\pm$ 0.01} \\
  Exp 3. & 0.35 & 0.31 & 0.32 & 0.32 & 0.34 & 0.34 & 0.33 & 0.33 & 0.33 & 0.34 & 0.30  & 0.23 $\pm$ 0.00 & \textbf{0.18 $\pm$ 0.01}  
       \\ \bottomrule
      \end{tabular}}
      \caption{Results from three toy experiments, described in Sec. \ref{sec:toy_exps}, show that both versions of \ref{dF1} enhance FOCI's strengths. In Experiments 2 and 3, they are the only methods that outperform regressing to the mean \(\hat{\mu}_y\).}
      \label{tab:sinusoidal_experiments}
  \end{subtable}
\end{table}


We proceed by testing whether difFOCI performs well at FOCI's main application - feature selection. We begin with a simulated dataset, followed by three experiments with increasing complexity.


\subsection{Preliminary synthetic study}
\label{sec:preliminary_synthetic_study}

To evaluate the feature selection performance of difFOCI, we utilize \textit{vec}-\ref{dF1} to obtain the objective $T_{n, \beta}(Y, \theta \odot \bfX)$. Unlike FOCI, which returns a binary vector indicating whether a feature is selected or rejected, difFOCI's version \textit{vec}-\ref{dF1} yields a real-valued vector with components $(\theta_i)_{i\in[p]}$ representing the predictive informativeness of each corresponding feature (which allows taking into account feature variability). To perform feature selection, we need to choose a cutoff parameter $\upsilon$ and select the features with $|\theta_i|\geq\upsilon$.



Alg. \ref{algo:general} therefore requires the following hyperparameters: softmax temperature $\beta$, cutoff value $\upsilon$, and optimization parameters (e.g., learning rate $\gamma$, weight decay $\lambda$, minibatch size $b$, etc.). Our experimental analyses show that $\beta=5$ and $\upsilon=0.1$ yield consistently good performance, so we set these as fixed\footnote{A further discussion on this can be found in Appendix \ref{sec:parameter_beta_choice}}. As a result, our algorithm simplifies solely to the hyperparameters used in conventional optimization methods, which are in Appendix \ref{appx:sec_G} for all experiments.


\paragraph{Environment.} As an initial example, we consider a data-generation process ideal for FOCI: from a large pool of features, a handful is sufficient for strong performance with $n\sim p$. The functional process is illustrated in Fig. \ref{plt:2a_functions}, crafted to generate a diverse set of features: \emph{informative ones}, such as straight lines, sinusoids, or parabolas, and functions \emph{individually uninformative, yet informative in multidimensional contexts}, e.g., ellipses, rotated parabolas, and more involved curves. This process includes 60 functions, each noised four times, resulting in $p=240$ features with $n=100$ points. Ideally, a feature selection method should pinpoint a small but diverse set of features\footnote{The exact data-generating process is given in Appendix \ref{sec:synthetic_env}}.

\paragraph{Baselines.} For comparative analysis, we employ various feature selection techniques from the scikit-learn library \citep{pedregosa2011scikit}. These include: \textit{GenericUnivariateSelect} (GUS) for univariate feature selection, \textit{SelectPercentile} (S.Per.), retaining only the top user-specified percentile of features, and statistical test-based methods: \textit{SelectFpr} (FPR), \textit{SelectFdr} (FDR), and \textit{SelectFwe} (FWE) addressing false positive rate, false discovery rate, and family-wise error, respectively. Additionally, we employ \textit{SelectKBest} (K.B) to select the best 25\%, 50\%, or 75\% of features based on the ANOVA F-value test \citep{girden1992anova}. We also benchmark against dimensionality reduction techniques including Linear Discriminant Analysis (LDA, \citet{fisher1936lda}), Principal Component Analysis (PCA, \citet{wold1987principal}), and Uniform Manifold Approximation and Projection (UMAP, \citet{mcinnes2018umap}), retaining 25\%, 50\%, and 75\% of the features/principal components.

Throughout this and Sec. \ref{sec:toy_exps}, we measure the performance by looking at the test error using Support Vector Regression (SVR, $C=1.0$, $\epsilon=0.2$) \citep{svr2, svr3, svr1}. For SelectKBest, PCA, and UMAP, instead of reporting for $25, 50$, and $75\%$ of features/components separately, we only provide the results yielding the lowest mean-squared test error.

\looseness=-1\paragraph{Results.} Our approach selects a small, diverse, and informative set of features, resulting in good performance and showcasing successful inheritance of FOCI's main strengths (see Table \ref{sim_data}). The norms of the selection parameter $\theta$ are shown in Fig. \ref{plt:2b_selected}, demonstrating the evident relationship between the predictive informativeness of the features and the corresponding parameter norms. 


We have discussed the recent advances and methodologies necessary to introduce difFOCI, as well as provided experimental analysis on a synthetic examples. We now proceed to more challenging examples, and ultimately to real-world datasets.


\section{From feature selection to feature learning}
\label{sec:toy_exps}


\begin{table}
    \caption{Feat. selection and dim. reduction benchmarks in terms of logistic test loss. Reported are the mean and std. across five random seeds. Our algorithms yield competitive predictions.  \vspace{-10pt}}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|cccccccccccc}
    \toprule
        & GUS   & S.Per. & FPR & FDR & FWE & K.B. & UMAP & LDA & PCA & FOCI     & \textit{vec}-\ref{dF1}    & \textit{NN}-\ref{dF1}   \\ 
    \midrule
Spambase             & 10.70 & 6.05  & 2.92 & 2.92 & 2.92  & 3.39 & 2.97 & 3.20 & 3.12 & 3.04  &   \textbf{2.56 $\pm$ 0.13}  & \textbf{2.57 $\pm$ 0.19}  \\
Toxicity             & 14.41  & 12.98 & 17.30 & 12.98  & 18.02    & \textbf{10.09} &  12.98  & 15.86 & \textbf{10.00} & 16.30 & 11.61 $\pm$ 0.80  & \textbf{9.61 $\pm$ 1.50} \\
QSAR                 & 2.88  & 3.16 & 3.16 & 3.76 & 2.92 & 3.52 & 2.32 &  \textbf{2.16} & \textbf{2.16} & 3.44  & 2.54 $\pm$ 0.07  & \textbf{2.11} $\pm$ \textbf{0.11}  \\
Breast Cancer        & 4.66  & 1.69 & 0.42  & 0.42  & 0.42  & \textbf{0.00}  & 2.48 & 1.42 & 1.24 & 0.62 & \textbf{0.00 $\pm$ 0.00} &  \textbf{0.00  $\pm$ 0.00}    \\
Religious             & 0.84  & 0.56 & 0.65   & 0.57 & 0.56  & 0.48 & 6.63 & 1.61 & 0.60 & 0.53  & \textbf{0.48 $\pm$ 0.03}  & 0.56 $\pm$ 0.04 \\ \bottomrule
    \end{tabular}}
    \vspace{-4pt}
  \label{tab:real_data}
\end{table}


With a high-level understanding of difFOCI in place, we continue to assess its performance. We begin by highlighting two key observations we encountered during our preliminary experiments. We consider the following toy example:  $Y = \sin(X_1)+2\sin(X_2)+3\sin(X_3)+\epsilon$, where $\epsilon_i\sim \mathrm{N}(0,\sigma_\epsilon^2)$, $i\in[n]$, and $\bfX\sim N(0,\sigma^2_x\Idp)$, with $n=2000$, $p=10$, $\sigma_x=\sigma_\epsilon=0.1$.


\looseness=-1\textbf{Observation 1.}\hypertarget{obs_1}{($\dagger$)} The objectives (\ref{eqn:chaterjee_estim}) and (\ref{eqn:mona_estim_with_x}) consistently capture the correct feature functional forms. Specifically, the values \emph{(i)} $T_n\left(Y,\left[\sum_{i=1}^2\sin(\pi^j_i(\bfX_{1:3}), \sin(\pi^j_3(\bfX_{1:3}))\right]\right)$, $j\in[3]$, \emph{(ii)} $T_n(Y ,\sin(\bfX_{1:3}))$,  and \emph{(iii)} $T_n(Y ,\sum_{i=1}^3\sin(\bfX_i))$ are all significantly greater than \emph{(i)} $T_n\left(Y , \left[\sum_{i=1}^2\pi^j_i(\bfX_{1:3}),\pi^j_3(\bfX_{1:3})\right]\right)$, \emph{(ii)} $T_n(Y ,\bfX_{1:3})$, and \emph{(iii)} $T_n(Y ,\sum_{i=1}^3\bfX_i)$ (as illustrated in Figure \ref{plt:0_error_bar} in the Appendix). 
Therefore, a more complex parameterization (than $f_{\mathbf{\theta}}(\bfX)=\theta \odot \bfX$) might learn a nonlinear transformation of the features, maintaining ranks in a manner more consistent with the true functional forms.


\textbf{Observation 2.}\hypertarget{obs_2}{($\ddagger$)} The objectives (\ref{eqn:chaterjee_estim}) and (\ref{eqn:mona_estim_with_x}) consistently prefer correct, lower-dimensional bases of the features. Specifically, $T_n(Y , \sum_{i=1}^3\sin(\bfX_i))$ remains consistently greater than $T_n(Y , \sin(\bfX_{1:3}))$. Therefore, a more elaborate parameterization could learn an appropriate, possibly lower dimensional, basis transformation. 


\looseness=-1Motivated by these observations, we propose \textit{NN} parameterizations to further explore the capabilities of difFOCI. We begin with simple one-hidden-layer Multi-layer Perceptrons (MLPs) as $f_{\mathbf{\theta}}$ parameterizations. We set the output dimension to match the input, as this performed well across all experiments, though treating it as a hyperparameter might further enhance performance.

\subsection{Initial assessments of \ref{dF1}}

We now evaluate both \textit{vec}-\ref{dF1} and \textit{NN}-\ref{dF1} across three progressively challenging examples. We note that across all examples, FOCI selects the correct subset of the features more than 95 percent of the time. We set $p=10$ throughout the experiments, and both $\sigma_\epsilon=\sigma_x=0.1$. Full experimental details are given in Appendix \ref{appx:sec_F}.


\looseness=-1 \textbf{Toy example 1: difFOCI successfully accounts for feature variability.} Here, we test whether \textit{vec}-\ref{dF1} and \textit{NN}-\ref{dF1} on the following example, previously introduced in Sec. \ref{sec:main_results}: $Y = \sin(X_1)+2\sin(X_2)+3\sin(X_3)+\epsilon$, where $\epsilon_i\sim \mathrm{N}(0,\sigma_\epsilon^2)$, $i\in[n]$, and $\bfX\sim N(0,\sigma^2_x\Idp)$, with $n=2000$. In Table \ref{tab:sinusoidal_experiments}, we observe that \textit{vec}-\ref{dF1} and \textit{NN}-\ref{dF1} successfully pinpoint the correct feature subset and account for feature variability, resulting in improved performance to that of FOCI. We expand on this in Appendix, Fig. \ref{plt:1_param_evol} for \textit{vec}-\ref{dF1}, where we can observe the correct proportionality of the coefficients in the regression equation and the learned parameters $\theta_1, \theta_2$ and $\theta_3$.\footnote{Note that this is already an improvement to FOCI, as it cannot take into account feature variability.}. 


\textbf{Toy example 2: difFOCI can learn appropriate basis transformations.} The goal of this toy example is to examine whether \textit{NN}-\ref{dF1} effectively learns basis transformations. Data are generated as follows: $Y = \sin(X_1+2X_2+3X_3)+\epsilon$, where $\epsilon_i\sim \mathrm{N}(0,\sigma_\epsilon^2)$, $i\in[n]$, and $\bfX\sim N(0,\sigma^2_x\Idp)$, with $n=2000$. We affirmatively demonstrate its efficacy by examining the test loss after fitting the SVR - the substantially lower test error can be observed in Table \ref{tab:sinusoidal_experiments}.


\textbf{Toy example 3: difFOCI simultaneously addresses mutual interactions, basis, and nonlinear transformations.} Our final example seeks to explore the full capabilities of \ref{dF1} with NN parameterization, examining whether it can simultaneously discern complex, interrelated relationships as well as multiple transformations, encompassing both nonlinear and basis transformations. The data generation process is as follows: $Y = \sin((X_1X_2)^2+(X_2X_3)^2+(X_1X_3)^2)+\epsilon$, where $\epsilon_i\sim \mathrm{N}(0,\sigma_\epsilon^2)$, $i\in[n]$, and $\bfX\sim N(0,\sigma^2_x\Idp)$, with $n=5000$. As evidenced in Table  \ref{tab:sinusoidal_experiments} (using a two-hidden-layer MLP\footnote{For this example, we found one-hidden-layer MLP not to be expressive enough.
}), we successfully learn effective transformations that result in strong performance. 

\paragraph{Summary.} Throughout the experiments, both \textit{vec}-\ref{dF1} and \textit{NN}-\ref{dF1} yield strong performance, as seen in Table \ref{tab:sinusoidal_experiments}. The two penultimate examples emphasize the potential capabilities of difFOCI; not only can it correctly identify the relevant subsets, but it also learns useful transformation, yielding the only method that outperforms random guessing (see $\hat{\mu}_y$ column in Table \ref{tab:sinusoidal_experiments}).


\section{Experiments}
\label{sec:experiments}


Having examined \ref{dF1} on synthetic problems and toy datasets, we now proceed to real-world datasets. We attempt to demonstrate the flexibility of difFOCI and highlight the promising potential of all three objectives: \ref{dF1}-\ref{dF3}. Our aim in this section is not solely to outperform existing benchmarks, but rather to showcase difFOCI's broad applicability, inspire further investigation into these applications, and explorations of new areas where the method can be applied.


\subsection{Real-world data}
\label{sec:real_world_data}
In this section, we compare \textit{vec}-\ref{dF1} and \textit{NN}-\ref{dF1} to feature selection and dimensionality reduction methods using real-world datasets.

\looseness=-1\textbf{Environments.} We evaluate our methods on five UCI datasets \citep{uci2019}: Breast Cancer Wisconsin \citep{street1993breastcancer}, involving benign/malignant cancer prediction; Toxicity \citep{gul2021toxicity}, aimed at determining the toxicity of molecules affecting circadian rhythms; Spambase \citep{hopkins1999spambase}, classifying emails as spam or not; QSAR \citep{qsar}, a set containing molecular fingerprints used for chemical toxicity classification, and Religious \citep{sah2019biblical}, aimed at identifying the source of religious books texts.  We perform Logistic Regression \citep{cox1958regression} with default scikit-learn \citep{pedregosa2011scikit} parameters ($\text{tol}=$\num{1e-4}, $C=1.0$). Dataset information is provided in Appendix \ref{appx:sec_C}. 


\paragraph{difFOCI is competitive in feature selection and dimensionality reduction.} As seen in Table \ref{tab:real_data} difFOCI achieves solid performance in the experiments. For \textit{NN}-\ref{dF1}, we use two-hidden-layer MLPs. The findings, which employ logistic loss, demonstrate that taking into account feature variability and using parameterization are crucial for improved performance compared to FOCI.

\subsection{Domain shift/spurious correlations}
\label{sec:domain_shift}
Here, we investigate an application of difFOCI to deep learning in the form of \textit{NN}-\ref{dF2}. The data consists of triplets $(Y, \bfX, \bfX_G)$, denoting the predictor, response variables, and group attributes, respectively. In this context, difFOCI can be employed as a regularizer to enforce the learning of uncorrelated features with respect to spurious attributes, thereby mitigating relying on spurious correlations and shortcuts in the model \citep{kenney1982beware}.

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-.5cm}
\caption{Average and worst group accuracies for the Waterbirds dataset. We compare to the ERM and DRO, where e.s. stands for early-stopping and $l2$ for Ridge regularization. We can see that difFOCI performs comparably to state-of-the-art spurious correlation methods.} 
\centering
  \resizebox{0.5\textwidth}{!}{
  \begin{tabular}{l|cc|cc|}
    & \multicolumn{2}{c|}{Average acc.} & \multicolumn{2}{c|}{Worst group acc.} \\ 
    \toprule
    & Train & Test & Train & Test \\
    \midrule 
    ERM   &  100 & \textbf{97.3} &  100 &  60.0        \\
    ERM (e.s. $+$ strong $l2$)   &  97.6 & 95.7 &  35.7 &  21.3                  \\
    ERM + FOCI  &  99.9  & 77.8 & 1.1 & 0.0  \\
    ERM + \textit{NN}-\ref{dF2}    &  99.9 & 93.7 &  92.0 &  \textbf{85.7} \\ \midrule
    DRO  &  100.0 & \textbf{97.4} &  100.0 &  76.9                \\
    DRO (e.s. $+$ strong $l2$)  &  99.1 & 96.6 &  74.2 &  86.0                \\
    DRO + FOCI & 99.5  & 74.5 & 6.1 &  3.9                \\
    DRO + \textit{NN}-\ref{dF2} &  80.1 & 93.5 &  99.2 &  \textbf{87.2}                \\ \bottomrule
\end{tabular}}
  \label{tab:waterb_table}
  \vspace{-.5cm}
\end{wraptable}

\looseness=-1\paragraph{Environment.} We use Waterbirds dataset \citep{sagawa2019distributionally}, which combines bird photographs from the Caltech-UCSD Birds-200-2011 dataset \citep{wah2011caltech} with image backgrounds from the Places dataset \citep{zhou2017places}. The labels $Y = \{\text{waterbirds}, \text{landbirds}\}$ are placed against $G = \{\text{water}, \text{land}\}$ backgrounds, with waterbirds (landbirds) more frequently appearing against a water (land) background (exact details given in Table \ref{tab:waterbirds_counts}, Appx. \ref{appx:sec_E}). Due to this spurious correlation, \citep{sagawa2019distributionally, idrissi2022simple, bell2024reassessing} observed that NNs (i.e., ResNet-50 \citep{he2016deep}, pre-trained on ImageNet \citep{imagenet}) tend to rely on the background to infer the label, rather than solely focusing on birds. 

\paragraph{Preventing reliance on spurious correlations.} We investigate the potential benefits of employing \textit{NN}-\ref{dF2} as a regularization technique, which penalizes the reliance of extracted features $f_{FE_{\theta}}$ on the spurious attribute $\bfX_G$ (i.e., the background) via $T_{n, \beta}(\bfX_G, f_{FE_{\theta}}(\bfX)\mid \bfX_G)$. From Tables \ref{tab:waterb_table}-\ref{tab:worst_group_acc}, we can see that \textit{NN}-\ref{dF2} (applied to both ERM and DRO) compares competitively to state-of-the-art methods. The exact algorithm is given in \ref{algo:appx_ex_2}. Experimental details, reported average accuracy and further examples are in Appendix \ref{appx:sec_G}. 


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/3_saliency_plot.pdf}
    \caption{ResNet-50 \citep{he2016deep} saliency maps using the ERM \citep{vapnik2006estimation} loss, DRO \citep{sagawa2019distributionally} with standard regularization (early stopping and $\ell2$) or difFOCI. Without difFOCI, the models heavily rely on background (spurious features). difFOCI effectively resolves the problem (main focus is on relevant features: the bird). Further samples are shown in the Appendix \ref{appx:sec_E}.}
    \label{fig:birds}
\end{figure}


\looseness=-1\paragraph{difFOCI increases worst group accuracy while maintaining solid performance.} We can observe in Table \ref{tab:waterb_table} and Fig. \ref{fig:birds} that \textit{NN}-\ref{dF2} successfully prevents the network from relying on the spuriously correlated background while improving worst group accuracy for both ERM and DRO. Apart from Waterbirds dataset, we also tested difFOCI on 5 additional datasets: two text datasets: MultiNLI \citep{williams2017broad}, CivilComments \citep{borkan2019nuanced}, and four image datasets: NICO++ \citep{zhang2023nico}, CelebA \citep{liang2022metashift}, MetaShift \citep{liang2022metashift} and CheXpert \citep{irvin2019chexpert}. Full experimental details (including average accuracy performance) can be found in Appendix \ref{appx:sec_G}. We experimented with various architectures: in addition to the ResNet-50, we used BERT and ViT-B with pretraining strategies like DINO and CLIP. Furthermore, we compared to Just Train Twice \citep{liu2021just}, Mixup \citep{zhang2017mixup}, and Invariant Risk Minimization \citep{arjovsky2019invariant} as baselines. As shown in Table \ref{tab:fairness_experiments}, difFOCI demonstrates competitive performance in terms of both average and worst-group accuracy.




\begin{table}[t]
    \caption{Worst group accuracy across several datasets. difFOCI obtains competitive performance.}
    \centering
    \setlength{\tabcolsep}{4pt} 
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccccc}
            \toprule
            Dataset & difFOCI+ERM & difFOCI+DRO & ERM & DRO & JTT & Mixup & IRM \\
            \midrule
            MultiNLI & $\mathbf{77.6 \pm 0.1}$ & $\mathbf{77.5 \pm 0.2}$ & $66.9 \pm 0.5$ & $77.0 \pm 0.1$ & $69.6 \pm 0.1$ & $69.5 \pm 0.4$ & $66.5 \pm 1.0$ \\
            CivilComments & $66.32 \pm 0.2$ & $\mathbf{70.3 \pm 0.2}$ & $64.1 \pm 1.1$ & $\mathbf{70.2 \pm 0.8}$ & $64.0 \pm 1.1$ & $65.1 \pm 0.9$ & $63.2 \pm 0.5$ \\
            CelebA & $\mathbf{89.32 \pm 0.4}$ & $\mathbf{89.8 \pm 0.9}$ & $65.0 \pm 2.5$ & $\mathbf{88.8 \pm 0.6}$ & $70.3 \pm 0.5$ & $57.6 \pm 0.5$ & $63.1 \pm 1.7$ \\
            NICO++ & $\mathbf{47.10 \pm 0.7}$ & $46.3 \pm 0.2$ & $39.3 \pm 2.0$ & $38.3 \pm 1.2$ & $40.0 \pm 0.0$ & $43.1 \pm 0.7$ & $40.0 \pm 0.0$ \\
            MetaShift & $83.10 \pm 0.5$ & $\mathbf{91.7 \pm 0.2}$ & $80.9 \pm 0.3$ & $86.2 \pm 0.6$ & $82.6 \pm 0.6$ & $80.9 \pm 0.8$ & $84.0 \pm 0.4$ \\
            CheXpert & $54.42 \pm 3.2$ & $\mathbf{75.3 \pm 0.3}$ & $50.1 \pm 3.5$ & $73.9 \pm 0.4$ & $61.5 \pm 4.3$ & $40.2 \pm 4.1$ & $35.1 \pm 1.2$ \\
            \bottomrule
        \end{tabular}%
    }
    \label{tab:worst_group_acc}
\end{table}



\subsection{Fairness study}
\label{sec:fairness}


\begin{table}[t]
    \caption{\textit{NN}-\ref{dF3} allows preserving predictivity of $y$ while significantly reducing predictivity of $X_s$.}
    \centering
    \setlength{\tabcolsep}{4pt}  % reduce column separation
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccc}
            \toprule
            Dataset & Features & Train acc: $y$ & Val. Acc: $y$ & Test acc: $y$ & Train acc: $X_s$ & Val. Acc: $X_s$ & Test acc: $X_s$ \\
            \midrule
            \multirow{2}{*}{Bank marketing} & Stand. data & $91.32 \pm 2.3$ & $93.27 \pm 1.2$ & $90.05 \pm 2.0$ & $89.09 \pm 1.2$ & $72.26 \pm 1.5$ & $70.93 \pm 0.9$ \\
            & \ref{dF3} features & $90.81 \pm 1.8$ & $92.13 \pm 2.6$ & $89.35 \pm 1.1$ & $63.12 \pm 2.8$ & $62.24 \pm 0.7$ & $\mathbf{63.81 \pm 2.1}$ \\
                        \midrule

            \multirow{2}{*}{Student data} & Stand. data & $88.35 \pm 1.7$ & $79.63 \pm 0.9$ & $75.67 \pm 1.3$ & $95.68 \pm 2.1$ & $72.16 \pm 2.4$ & $71.21 \pm 1.5$ \\
            & \ref{dF3} features & $80.18 \pm 2.9$ & $72.16 \pm 1.6$ & $72.73 \pm 1.7$ & $59.47 \pm 1.1$ & $58.95 \pm 1.0$ & $\mathbf{48.89 \pm 1.1}$ \\
                        \midrule
            \multirow{2}{*}{ASCI Income} & Stand. data & $83.49 \pm 2.4$ & $85.10 \pm 2.1$ & $81.30 \pm 2.7$ & $68.97 \pm 1.6$ & $67.67 \pm 2.6$ & $66.00 \pm 0.7$ \\
            & \ref{dF3} features & $82.80 \pm 0.8$ & $81.99 \pm 1.5$ & $82.95 \pm 0.9$ & $56.58 \pm 1.2$ & $55.01 \pm 2.0$ & $\mathbf{52.73 \pm 2.0}$ \\
            \bottomrule
        \end{tabular}%
    }
    \label{tab:fairness_experiments}
\end{table}



Finally, we explore \textit{NN}-\ref{dF3}. This section, while not the primary focus of our contribution, offers a complementary illustration of the difFOCI objective's versatility through a heuristic example. We found that this form \ref{dF3} preserves the performance of the chosen parameterization while significantly reducing its predictivity of the sensitive attribute.


\looseness=-1\paragraph{Environments.} We utilize classification datasets with interpretable features and sensitive attributes: \emph{(i)} Student dataset \citep{cortez2008student}, aimed at predicting if a student's performance surpasses a specific threshold (sex as the sensitive); \emph{(ii)} Bank Marketing dataset \citep{moro2014bankmarketing} with predicting if a customer subscribes to a bank product (marital status as the sensitive); and two ACS datasets \citep{ding2021retiringASCIfolktables}, \emph{(iii)} Employment and \emph{(iv)} Income, for predicting individual's employment status and whether their income exceeds a threshold, with sex and race as sensitive attributes in both datasets. Exact experimental details are provided in Appendix \ref{sec:fairne_apx}.

\paragraph{Findings.} Leveraging the conditional dependence expression in (\ref{eqn:mona_estim_with_x}), our method flexibly incorporates sensitive features to facilitate fairer classification without exploiting sensitive data. Using NN-\ref{dF3}, we optimize $T_{n,\beta}(Y,  f_\theta(\bfX) \mid \bfX_s)$ to learn features that are informative about $Y$, offering an optimization that heuristically seems to favor solutions less predictive of $\bfX_s$. Specifically, we train two NNs to predict $y$: the first NN was trained on $X$ (without $X_s$), while the second NN was trained on features $f_\theta(X)$ obtained using \ref{dF3}. We then used the final layers of both NNs to predict the sensitive $X_s$. As can be observed from Table \ref{tab:fairness_experiments}, difFOCI \ref{dF3} significantly reduces the predictability of $X_s$ (sometimes to chance level) without significantly impacting accuracy on $y$ - in some cases even slightly improves it. 



\paragraph{Despite conditioning out sensitive information, difFOCI delivers solid performance.} From Table  \ref{tab:fairness_experiments}, we see that \textit{vec}-\ref{dF3} demonstrates strong performance by effectively debiasing the network (as it cannot predict the sensitive $X_s$ well), while keeping informativeness regarding $y$. In Appendix \ref{sec:fairne_apx}, we conduct another experiment with similar findings showcasing the promising potential of \ref{dF3}.


\section{Conclusion}
\label{sec:conclusion}


In this paper, we discussed two recent advancements in rank-based measures of correlation, critically examining the proposed estimators, including the FOCI algorithm and its barriers to adoption in machine learning. Leveraging these advancements, we introduced three enhanced and more adaptable versions of FOCI. We conducted several studies to showcase the retention of FOCI's strengths and the improvement of its weaknesses. We evaluated difFOCI's capabilities from toy examples, where our method was the sole one exceeding random guessing, to comprehensive real-world datasets involving feature selection and spurious correlations, where it demonstrated state-of-the-art performance. Finally, we proposed a direct application of our algorithm in fairness research, showcasing that difFOCI successfully debiases neural networks on several datasets.

\section*{Acknowledgements and funding}
This work has received funding from the French government, managed by the National Research Agency (ANR), under the France 2030 program with the reference ANR-23-IACL-0008. We extend our thanks to Samuel Bell, JoÃ£o Maria Janeiro, Badr Youbi Idrissi, Theo Moutakanni, StÃ©phane d'Ascoli and TimothÃ©e Darcet for feedback and support. Finally, we also thank Carolyn Krol for extensive consultation and support throughout this project.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\input{appendix}

\end{document}
