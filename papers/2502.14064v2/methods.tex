\section{Methods}

The following sections are structured are organized to provide a comprehensive overview of our methodologies and findings. Initially, we will introduce the data curation process for TriadMR-131K, as well as the protocol established for Triad pre-training. Following this, we will articulate the implementation strategies employed in three distinct experimental paradigms: 3D organ and tumor Segmentation, organ and cancer classification, and 3D medical image registration. 

\subsection{Pretraining}
\label{sec:data}

\noindent \textbf{Curation of TriadMR-131K dataset.} To ensure the quality and diversity of data for model pretraining, we curated a large-scale 3D MRI dataset of 131,170 3D volumes derived from 19,721 patients across 36 clinical datasets. TriadMR-131K comprises a diverse collection of 3D MRI data spanning three organs (brain, breast, and prostate), featuring modalities such as T1-w, T2-w, FLAIR, DWI-MRI, fMRI, DCE-MRI.
To standardize all sub-datasets, we used the same preprocessing protocol for all organs: we used the dicom2nifti\footnote{\url{https://github.com/icometrix/dicom2nifti}} package to convert all DICOM-format 2D slice collections into NIfTI-format 3D volumes. For 4D volume data, such as DCE-MRI, we took the $\frac{(t-1)}{2}$th or $\frac{(t+1)}{2}$th 3D slice to replace the original 4D data, where $t$ denotes the $t$th time step. All corrupted volumes were deleted during the conversion process.
Next, we reformatted all MRI scans so that the first axis points from left to right, the second from posterior to anterior, and the third from inferior to superior. We then resampled the images to a 1 mm resolution using bilinear interpolation. We also resized all images to (256,256,128) using trilinear interpolation.
To save storage space, we stored most of the 3D volume data types as UINT16 and the rest as Float32.
Finally, the brain MRI data involved 51,112 series from 37,436 examinations of 12,994 patients; the breast MRI data involved 46,116 series from 8,180 examinations of 3,834 patients; the prostate MRI data involved 33,942 series from 9,941 examinations of 4,639 patients. The statistical information of the 36 datasets is shown in Table \ref{table:t1}. Note that due to deletions during the conversion process, the numbers in the table are usually lower than the officially published figures.
In addition, we extracted the imaging description from the metadata of each 3D volume, which describes the imaging modality and related device parameters. Since it is organ-independent information, it helps to adjust the positional relationship of each modality in the semantic space, thereby improving the generalization ability of the model.
We tried to avoid any overlap between the datasets used in pre-training and all downstream evaluation sets to minimize the risk of data contamination.

\noindent \textbf{Protocol of Triad pre-training.} Triad uses nnUNet (31M)\cite{isensee2021nnu} and SwinTransformer\cite{hatamizadeh2021swin} for the image encoder. Furthermore, SwinTransformer is expanded into Swin-B (72.8M), Swin-L (290.4M), and Swin-H (11.6B) according to feature sizes of 48, 96, and 192 to study the parameter scaling law (Fig. 2b).
We use GTR-T5-Large~\cite{ni2021large} as the text encoder instead of CLIP~\cite{radford2021learning} because the text embedding of T5 can provide a more semantically nuanced distribution and is suitable as a supervisory signal to guide the alignment of visual modality distributions to the semantic space, rather than just as a representation of relative similarity in contrastive learning. This means that the parameters of the text encoder are frozen and do not participate in gradient updates.
In the pretraining stage, we use an upsampling 3D CNN as the image decoder for the self-supervised reconstruction task. In the downstream task, we only keep the parameters of the image encoder and replace the image decoder with the task adapter.
Triad's pretraining uses L1 loss as the reconstruction loss and log-ratio loss~\cite{kim2019deep} to align the distribution of the visual modality with that of the textual modality. To prevent the log-ratio loss from dominating the optimization process, its weight is set to 0.01.
The architectural hyperparameters of the models involved are shown in Table \ref{table:t3}.


\subsection{3D organ/tumor segmentation}
\label{sec:sts}

\subsubsection{Curation of segmentation datasets}

\textbf{Within-domain segmentation datasets}. The BraTS21 dataset~\citep{baid2021rsna}, released as part of the 2021 RSNA-ASNR-MICCAI Brain Tumor Segmentation Challenge, consists of multi-institutional, multi-modal MRI scans (T1-w, T1 postcontrast, T2-w, FLAIR) from patients with glioblastoma or lower-grade glioma. Each case includes expert annotations delineating tumor subregions (enhancing tumor, edema, and necrotic core). The core challenge portion provides 1,251 labeled scans for training.
The MSD–BrainTumour dataset~\citep{antonelli2022medical} (Task 01 in the Medical Segmentation Decathlon) includes 484 preoperative multi-modal MRI scans (T1-w, T1 postcontrast, T2-w, FLAIR) sourced primarily from earlier BraTS collections.
For BreastDM~\citep{zhao2023breastdm}, the original publication reports a new breast MRI dataset (with dynamic contrast-enhanced volumes) consisting of 232 scans.
The Prostate158 dataset~\citep{adams2022prostate158} offers 158 MRI scans (T2, ADC, DFI) with detailed prostate annotations.
Finally, the MSD–Prostate dataset~\citep{antonelli2022medical} (Task 05 in the MSD) contains 32 T2-w, ADC map, and DFI scans with corresponding prostate zonal annotations (central gland and peripheral zone).

\textbf{Out-of-domain segmentation datasets}. The MM-WHS–MRI subset~\citep{zhuang2018multivariate} from the Multimodality Whole Heart Segmentation challenge consists of around 20 annotated 3D MRI volumes of the heart.
ATLAS–MRI~\citep{quinton2023tumour} is a publicly available dataset of contrast-enhanced MRI for hepatocellular carcinoma (HCC), which consists of 60 scans.
The Abdomen-1K~\citep{ma2021abdomenct}, released as part of the MICCAI FLARE 2022 Playground subtask 1, includes a training set adapted from MSD Pancreas (281 cases) and NIH Pancreas (80 cases), where all 361 CT scans are from the portal phase.
Kipa22~\citep{he2021meta} comes from the Kidney PArsing Challenge 2022, and its goal is to segment 3D kidneys, renal tumors, arteries, and veins.  It  released 70 training sets with detailed annotations.
Lastly, the remaining MSD tasks, namely MSD-Pancreas (281 training scans), MSD-Liver (131 training scans), MSD-Heart (20 training scans), MSD-Hippocampus (260 training scans), MSD-Lung (63 training scans), MSD-HepaticVessel (303 training scans), MSD-Spleen (41 training scans), and MSD-Colon (126 training scans), do not provide official validation sets.
For all the above datasets, we keep the same split method as provided by VoCo~\citep{wu2024voco}.

\subsubsection{Fully supervised finetuning with nnUNet framework}

In our image segmentation experiments, we adopt nnUNetv2 as a unified framework to ensure consistent data preprocessing for fair comparisons across different models. Within this framework, we have implemented Swin-Transformer Base/Large/Huge networks, thereby aligning the training protocols.
For each publicly available dataset with detailed annotations, nnUNetv2's built-in code is used to perform a 5-fold cross-validation split. Note that, in order to make a fair comparison with VoCo, we report the 0th fold in the 5-fold cross-validation.
Throughout training, nnUNet models are trained for 300 epochs, while Swin-Transformer models are trained for 150 epochs, and we select the model with the highest validation performance for final evaluation.
We employ an SGD optimizer with an initial learning rate of 0.01, following nnUNet's default decay schedule. VoCo-SSL pretrained weights are sourced from the code library\footnote{\url{https://github.com/Luffy03/Large-Scale-Medical}}.
Because our experimental setup closely matches that of VoCo, some of the results reported here are derived from the extended version of the original publication~\citep{wu2024voco}.

\subsection{Organ/cancer classification}
\label{sec:occ}
\subsubsection{Curation of classification datasets}
\textbf{Within-domain classification datasets.} The ADNI dataset~\citep{jack2008alzheimer} (Alzheimer’s Disease Neuroimaging Initiative) is a longitudinal, multi-center, observational study that includes thousands of participants, from cognitively normal (CN) to those with mild cognitive impairment (MCI) or Alzheimer’s disease (AD).
In this study, we use a dataset consisting of participants who have screening, 6-month, 1-year, 18-month (MCI only), 2-year, and 3-year (normal and MCI only) scans, which is called ``ADNI1\_Complete 3Yr 1.5T,'' totaling 2,182 samples. Consistent with the literature~\cite{majee2024enhancing}, the training set, validation set, and test set contain 1,526, 326, and 330 samples, respectively.
We use NPPY~\cite{he2023neural} and its available pre-trained weights to convert raw MRI scans into uniformly sized skull-stripped, intensity-normalized brain volumes in standard coordinate space, and then reshape them into a smaller dimension of 96×96×96.
The BreastDM dataset~\citep{zhao2023breastdm} provides dynamic contrast-enhanced (DCE) breast MRI scans for lesion classification, containing 85 benign samples and 147 malignant samples. We adopt the same dataset split scheme as in the segmentation task.

\textbf{Out-of-domain classification datasets}. The OrganMNIST3D dataset~\citep{yang2023medmnist} is part of MedMNIST v2 and contains more than 1,700 3D CT volumes of 11 organs for classification. Its official distribution includes dedicated training sets (972 volumes), validation sets (161 volumes), and test sets (610 volumes). The samples in OrganMNIST3D are available in 28×28×28 and 64×64×64 versions, and we use the latter for evaluation.
The LUNA16 dataset~\citep{setio2017validation}, derived from the LIDC/IDRI collection, contains 888 thoracic CT scans for lung nodule analysis. LUNA16 includes a total of 551,065 candidate nodules, of which 1,120 nodules are detected as positive, represented by 1, and the rest are represented by 0. The full dataset is divided into 10 subsets, and we use subsets 0-5 as training sets, subset 6 as the validation set, and subsets 7-9 as test sets.
Finally, the LLD-MMRI dataset~\citep{lou2024sdrformer} contains 498 annotated multi-stage liver lesions from the same number of patients. The lesions are classified into seven categories: hepatocellular carcinoma (HCC), intrahepatic cholangiocarcinoma (ICC), hepatic metastasis (HM), hepatic cyst (HC), hepatic hemangioma (HH), focal nodular hyperplasia (FNH), and hepatic abscess (HA). The dataset has been pre-partitioned into a training set (316 lesions), a validation set (78 lesions), and a test set (104 lesions).

\subsubsection{Fully supervised finetuning with linear classifier}

As shown in Fig. \ref{fig:cls} (a), we use the parameters saved in the pretraining phase as the initial parameters of the encoder, perform an average pooling operation on the output of the last layer of the encoder, and then input it into a two-layer linear classifier to predict the probability distribution of the category.
In classification experiments, we set the ADNI dataset input size to 96×96×96, while all other datasets are resized to 64×64×64. Classifiers based on the Swin-Transformer are trained for 150 epochs, and those based on the 3D UNet are trained for 300 epochs. In each experiment, we report the best result.
We employ a learning rate of 1e-3, with the Adam optimizer, following a cosine decay schedule. Additionally, the first 5 epochs are used for warmup to stabilize training.

\subsection{3D medical image registration}
\subsubsection{Curation of registration datasets}

\textbf{Within-domain registration datasets}. The IXI dataset~\citep{Brain-Development_IXI_2019} consists of over 576 T1-weighted brain MRI scans from healthy volunteers collected at three different hospitals in London. Following the TransMorph~\citep{chen2022transmorph} protocol, we use 403 scans as the training set, 58 as the validation set, and 115 as the test set. The volumes are cropped to 160×192×224. Thirty annotated structures were used for evaluation.
The OASIS dataset~\citep{krentzel2023clem} (Open Access Series of Imaging Studies) includes 413 T1-weighted brain MRI scans from participants aged 18 to 96, with both healthy controls and patients exhibiting mild to moderate cognitive impairment. The original MR volumes are preprocessed using FreeSurfer~\cite{fischl2012freesurfer}, which includes spatial normalization, skull stripping, affine transformations, and automatic structural segmentation.
Following the TransMorph~\citep{chen2022transmorph} protocol, we use 394 scans as the training set and 19 scans as the validation set. Since there is no test set available, we employed the validation set for evaluation. The volumes are cropped to 160×192×224. 35 structures are used as ground truths to evaluate the performance.

\textbf{Out-of-domain registration datasets}. The ACDC dataset \citep{bernard2018deep} (Automated Cardiac Diagnosis Challenge) comprises 150 cardiac MRI scans in short-axis view, covering subjects with various heart conditions. The original challenge reserves 100 scans for training and 50 for testing. The volumes are cropped to 160×192×224. 

\subsubsection{Fine-tuning TransMorph/SwinUNETR for image registration}

For fine-tuning of TransMorph, we replaced the ``Transformer Encoder'' in the original framework with our own Swin-Transformer Encoder, loading the weights of Triad-L. The rest of the components, such as ``CNN Decoder,'' ``Affine Network,'' and ``Spatial Transform,'' are randomly initialized.
For fine-tuning of SwinUNETR, we load the weights of Triad-B into the encoder and randomly initialize the UNETR decoder. The pre-trained weights of SwinUNETR and SupreM are obtained from the code repository provided by VoCo.
Due to limited resources, we only fine-tune for 200 epochs on each set of experiments and select the best-performing results for reporting. The Adam optimizer is used for fine-tuning, and the batch size was 1. The learning rates for OASIS and IXI are 0.00005, while the learning rate for ACDC is 0.0001.
The remaining parameters, such as the type of loss function and weight factor, remain consistent with the default settings in the code provided by TransMorph.

\subsection{Computing hardware and software}
\label{sec:code}
We use pydicom 3.0.1 and dicom2nifti 2.5.0 for 2D slice sequences and 3D volume data preprocessing. We use Python 3.10.13 for all experiments and analyses in the study.
For the pretraining stage, we use the AdamW~\cite{loshchilov2017decoupled} optimizer with an initial learning rate of 1e-6, coupled with a cosine learning rate scheduler. The learning rate decays to zero over 200,000 steps, with a warm-up phase during the first 1,000 steps.
We use two 80-GB NVIDIA A100 GPUs configured for multi-GPU training using DistributedDataParallel (DDP) as implemented by the framework PyTorch (version 2.5.1, CUDA 12.4), with a batch size of 8. We do not divide the data for TriadMR-131K but use all the data to pretrain Triad and then save the model at 200,000 steps to serve as the initial parameters for downstream tasks.
For fine-tuning and validation of downstream tasks, we use the repository provided by VoCo v2~\cite{wu2024voco,wu2024large} (\url{https://github.com/Luffy03/Large-Scale-Medical}) and TransMorph~\cite{chen2022transmorph} (\url{https://github.com/junyuchen245/TransMorph_Transformer_for_Medical_Image_Registration}), respectively.

\subsection{Evaluation metrics}

We used several evaluation metrics to thoroughly assess the capabilities of our Triad model across different tasks. Accuracy is a primary metric used for evaluating the performance in medical-image classification, it is defined as the ratio of the number of correctly predicted samples to the total number of samples:

\begin{equation}
	\label{e:acc}
	\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
where TP (True Positives) denotes the number of samples correctly predicted as positive by the model; TN (True Negatives) denotes the number of samples correctly predicted as negative by the model; FP (False Positives) denotes the number of negative samples incorrectly predicted as positive by the model; FN (False Negatives) denotes the number of positive samples incorrectly predicted as negative by the model.

%Target Registration Error (TRE) is an important indicator of registration accuracy, which is used to measure the error between the landmarks in the reference image $I_r$ and the registered floating image $I_f$. Assuming $p_{i}^{ref}$ denotes the position of the $i$-th landmark point in the reference image $I_r$, and $p_i^{reg}$ denotes the registered position of the corresponding landmark point in the floating image $I_f$ after transformation $\mathcal{T}$, then TRE is defined as:
%
%\begin{equation}
%	\label{e:tre}
%	\text{TRE} = \frac{1}{N}\sum_{i=1}^{N}||p_i^{reg}-p_i^{ref}||_2
%\end{equation}
%where $N$ is the total number of landmark points; $p_i^{reg}=\mathcal{T}(p_i^{float})$, $p_i^{float}$ denotes the landmark point in the floating image; $\mathcal{T}$ is the transformation function in the registration process (such as rigid, affine or non-rigid transformation); $||\cdot||_2$ denotes the Euclidean distance.
%
%The Jacobian matrix is an important indicator to measure the smoothness and reversibility of the deformation field in non-rigid image registration. Standard Deviation of the Logarithm of the Jacobian Determinant (SDlogJ) reflects the consistency of the registration transformation on the local volume change and is defined as follows:
%\begin{equation}
%	\label{e:SDlogJ}
%	\text{SDlogJ} = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(\log J_i-\overline{\log J})^2}
%\end{equation}
%where $J_i = \det(\mathbf{J}_i)$ denotes the Jacobian determinant of the deformation field at the $i$-th voxel, $\mathbf{J}_i$ is the Jacobian matrix of the deformation field at the same voxel; $\log J_i$ is the logarithm of the Jacobian determinant; $\overline{\log J}$ is the mean of the logarithm of the Jacobian determinant across all voxels; $N$ is the total number of voxels in the image. SDlogJ quantifies the volume change of the deformation field at different locations by calculating the standard deviation of $\overline{\log J}$. Smaller SDlogJ values indicate taht the deformation field is smoother in space, while larger values may suggest that there is a large volume contraction or expansion in the local area.

Dice Similarity Coefficient (DSC) is used to measure the overlap between two sets, which is widely used in medical image segmentation tasks:

\begin{equation}
	\label{e:dsc}
	\text{DSC} = \frac{2|X \cap Y|}{|X|+|Y|}
\end{equation}
where $X$ is the pixel set of the predicted segmentation result; $Y$ is the pixel set of the ground truth segmentation; $|X \cap Y|$ denotes the number of pixels contained in the intersection of $X$ and $Y$; $|X|$ and $|Y|$ denote the number of pixels of $X$ and $Y$ respectively. Equivalently, the DSC can be calculated based on the pixel category, which is expressed in pixel-by-pixel binary form by the predicted label $P$ and the ground truth label $G$:

\begin{equation}
	\label{e:dsc2}
	\text{DSC} = \frac{2\sum_{i}P_iG_i}{\sum_{i}P_i+\sum_{i}G_i}
\end{equation}
where $P_i$ is the predicted label for the $i$-th pixel value; $G_i$ is the $i$-th pixel value of the ground truth label. The value of DSC ranges between 0 and 1, where 1 indicates perfect overlap and 0 indicates no overlap.

