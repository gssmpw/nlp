\section{Related Work}
\label{sec:RelatedWork}
This section provides an overview of the most relevant related work, divided into three distinct categories: Power Analysis techniques, Machine Learning on Embedded Systems, and Transfer Learning approaches.

\emph{Power Analysis:}
Power analysis techniques have been pivotal in reverse engineering of neural networks, where researchers employ timing, power, and electromagnetic (EM) data to reveal intricate details like activation functions and network structures ____. Wang et al. ____ explore how to deduce the structure of deep neural network (DNN) models deployed within in-memory computing (IMC) systems through power analysis. Investigations in ____ collect voltage and current data to infer model structures, and ____ demonstrate an attack on FPGA-based DNN accelerators using EM leakage. In____, the authors present a novel approach of utilizing cache timing side channels that offers insights into DNN structures. Our new power analysis technique complements the aforementioned techniques by generating an approximated weights matrix directly from power traces, which the existing techniques do not provide, as they focus mainly on uncovering the DNN model structure. In contrast, our focus is on transferring as much as possible knowledge, encapsulated in a DNN, via the approximated weights matrix. 

\emph{Machine Learning on Embedded Systems:}
ML models deployed on embedded systems find use in a wide range of fields, including medical imaging and autonomous driving. Innovative applications of ML models include GFUNet, which integrates Fourier Transform with U-Net architecture for medical image segmentation ____. Edge computing benefits from a long short-term memory (LSTM) model on TensorFlow Lite for gesture recognition in wearable devices as demonstrates in____. A lightweight combination of LSTM and a multi-layer perceptron (MLP) model has enabled real-time electrocardiogram (ECG) anomaly detection for internet of things (IoT) devices ____. Additionally, MLP models have been optimized for medical decision-making within medical IoT (MIoT) using boosting and dimension reduction for efficient predictions ____. Tiny convolutional neural networks (CNNs) on low-power devices have demonstrated the robustness of transfer learning in autonomous driving____, while MLP models have shown efficacy in early breast cancer detection____. In contrast, our work harnesses the power of \texttt{EDNN} models to perform analysis of power traces captured for SoCs, thereby enabling the generation of approximated weights matrices for transfer learning, which is a novel application of \texttt{EDNNs} in the transfer learning and embedded systems domain.

\emph{Transfer Learning:}
Transfer learning addresses the challenge of scarce or difficult-to-obtain training data for ML models by leveraging data from a related but different source domain, thereby improving models' performance even when traditional assumptions of identical feature spaces and data distributions do not hold ____. For instance, in text sentiment classification, transfer learning can enhance predictions when training data from one domain (e.g., digital camera reviews) are used for another domain (e.g., food reviews) ____. In image classification, models pre-trained on large datasets can be fine-tuned for specific tasks with limited data, such as adapting general image recognition models to medical image analysis ____. Human activity recognition benefits from transferring knowledge across different sets of activities, improving model accuracy when applied to new activities ____. In software defect detection, transfer learning allows models trained on data from one software project to be applied to another, even with different metrics ____. Additionally, multi-language text classification uses transfer learning to adapt models for different languages, enabling improved performance in scenarios with limited labeled data in the target language ____. Other applications include environmental monitoring, where models trained on data from one geographical region are adapted to another, and financial forecasting, where models utilize data from different market conditions to enhance predictive accuracy.

These transfer learning approaches typically require direct access to a fully functional ML model and its parameters and coefficients, which is not always feasible. In contrast to existing transfer learning approaches, our P2W transfer learning approach does not require direct access to such parameters and coefficients because it utilizes power traces to extract such information. Thus, P2W expands the applicability of transfer learning to scenarios where direct access to ML models is not feasible.