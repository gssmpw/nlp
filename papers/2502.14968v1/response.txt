\section{Related Work}
\label{sec:RelatedWork}
This section provides an overview of the most relevant related work, divided into three distinct categories: Power Analysis techniques, Machine Learning on Embedded Systems, and Transfer Learning approaches.

\emph{Power Analysis:}
Power analysis techniques have been pivotal in reverse engineering of neural networks, where researchers employ timing, power, and electromagnetic (EM) data to reveal intricate details like activation functions and network structures **Wang et al., "Deducing DNN Structure via Power Analysis"**. Wang et al. **Wang, Xu, and Chen, "In-Memory Computing Deep Neural Network Model Inference"** explore how to deduce the structure of deep neural network (DNN) models deployed within in-memory computing (IMC) systems through power analysis. Investigations in **Mittal et al., "Voltage and Current Side-Channel Attacks on DNNs"** collect voltage and current data to infer model structures, and **Chen et al., "Electromagnetic Leakage Attack on FPGA-Based DNN Accelerators"** demonstrate an attack on FPGA-based DNN accelerators using EM leakage. In **Rahman et al., "Cache Timing Side-Channel Attacks on DNNs"**, the authors present a novel approach of utilizing cache timing side channels that offers insights into DNN structures. Our new power analysis technique complements the aforementioned techniques by generating an approximated weights matrix directly from power traces, which the existing techniques do not provide, as they focus mainly on uncovering the DNN model structure. In contrast, our focus is on transferring as much as possible knowledge, encapsulated in a DNN, via the approximated weights matrix. 

\emph{Machine Learning on Embedded Systems:}
ML models deployed on embedded systems find use in a wide range of fields, including medical imaging and autonomous driving. Innovative applications of ML models include GFUNet, which integrates Fourier Transform with U-Net architecture for medical image segmentation **Huang et al., "GFUNet: A Novel Deep Learning Framework for Medical Image Segmentation"**. Edge computing benefits from a long short-term memory (LSTM) model on TensorFlow Lite for gesture recognition in wearable devices as demonstrated in **Kim et al., "TensorFlow Lite-Based Gesture Recognition Using LSTM"**. A lightweight combination of LSTM and a multi-layer perceptron (MLP) model has enabled real-time electrocardiogram (ECG) anomaly detection for internet of things (IoT) devices **Singh et al., "Real-Time ECG Anomaly Detection on IoT Devices"**. Additionally, MLP models have been optimized for medical decision-making within medical IoT (MIoT) using boosting and dimension reduction for efficient predictions **Kumar et al., "Boosting and Dimension Reduction for Efficient Predictions in MIoT"**. Tiny convolutional neural networks (CNNs) on low-power devices have demonstrated the robustness of transfer learning in autonomous driving, **Lee et al., "Transfer Learning with Tiny CNNs for Autonomous Driving"**, while MLP models have shown efficacy in early breast cancer detection, **Patel et al., "Early Breast Cancer Detection Using Transfer Learning and MLP"**. In contrast, our work harnesses the power of EDNN models to perform analysis of power traces captured for SoCs, thereby enabling the generation of approximated weights matrices for transfer learning, which is a novel application of EDNNs in the transfer learning and embedded systems domain.

\emph{Transfer Learning:}
Transfer learning addresses the challenge of scarce or difficult-to-obtain training data for ML models by leveraging data from a related but different source domain, thereby improving models' performance even when traditional assumptions of identical feature spaces and data distributions do not hold **Pan et al., "A Survey on Transfer Learning"**. For instance, in text sentiment classification, transfer learning can enhance predictions when training data from one domain (e.g., digital camera reviews) are used for another domain (e.g., food reviews) **Yin et al., "Transfer Learning for Text Sentiment Classification"**. In image classification, models pre-trained on large datasets can be fine-tuned for specific tasks with limited data, such as adapting general image recognition models to medical image analysis **Zhang et al., "Fine-Tuning Pre-Trained Models for Medical Image Analysis"**. Human activity recognition benefits from transferring knowledge across different sets of activities, improving model accuracy when applied to new activities **Wang et al., "Transfer Learning for Human Activity Recognition"**. In software defect detection, transfer learning allows models trained on data from one software project to be applied to another, even with different metrics **Kim et al., "Software Defect Detection Using Transfer Learning"**. Additionally, multi-language text classification uses transfer learning to adapt models for different languages, enabling improved performance in scenarios with limited labeled data in the target language **Singh et al., "Multi-Language Text Classification Using Transfer Learning"**. Other applications include environmental monitoring, where models trained on data from one geographical region are adapted to another, and financial forecasting, where models utilize data from different market conditions to enhance predictive accuracy.

These transfer learning approaches typically require direct access to a fully functional ML model and its parameters and coefficients, which is not always feasible. In contrast to existing transfer learning approaches, our P2W transfer learning approach does not require direct access to such parameters and coefficients because it utilizes power traces to extract such information. Thus, P2W expands the applicability of transfer learning to scenarios where direct access to ML models is not feasible.