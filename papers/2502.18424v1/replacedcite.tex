\section{Related Work}
\paragraph{Quantization.}

The objective of quantization is to represent weights (and optionally activations) using fewer bits. This reduction in precision reduces memory requirements, and typically enables inference speedups ____. Beyond their scale, contemporary LMs pose unique challenges for effective quantization, including the existence of high-magnitude outlier features ____. Recent directions include: holding outlier weights in higher precision ____, Hessian-based weight sensitivity ____, searching for optimal clipping thresholds ____, or combinations of these approaches ____.

\paragraph{Pruning.}

The aim of neural network pruning is to remove less important weights, therefore reducing the overall model size ____. Pruning can be performed at the level of individual weights (unstructured), within groups of weights (semi-structured), or entire dimensions (structured) ____. In particular, 2:4 semi-structured sparsity (i.e. pruning two weights in every block of four), enables enhanced inference performance on NVIDIA GPUs ____. However, the extensive size of LMs presents challenges in pruning them optimally ____. Recent work has instead decomposed LM pruning into a sequential layer-wise approach, demonstrating remarkable performance retention, even at high sparsity levels ____.

\paragraph{Domain-specific pruning.}

Early work focused on pruning deep neural networks for specific tasks ____. This trend continued ____ following the advent of BERT ____. However, the shift towards general-purpose LMs ____ has led to a focus on preserving general performance ____, i.e. language modeling and reasoning. Most recently, ____ proposed D-Pruner for domain-specific pruning, leveraging general weight importance to form a domain-specific training loss. However, this requires computationally expensive full-parameter fine-tuning, yet does not consistently outperform general-purpose pruning methods.

\paragraph{Calibration data.}

In a post-training setting, model compression typically relies upon calibration data ____. Calibration data consists of a small number of unlabeled examples, for the generation of layer activations ____. Typically, these examples are randomly sampled from web text or pre-training datasets (e.g. C4; ____). However, recent work has illustrated the influential role that calibration data can play, impacting the downstream performance of compressed models ____. Most recently, ____ suggest that pruning with calibration data based on downstream tasks does not necessarily benefit performance over generic data.