\section{Related Work}
\paragraph{Quantization.}

The objective of quantization is to represent weights (and optionally activations) using fewer bits. This reduction in precision reduces memory requirements, and typically enables inference speedups \citep{gholami-etal-2021-survey}. Beyond their scale, contemporary LMs pose unique challenges for effective quantization, including the existence of high-magnitude outlier features \citep{bondarenko-etal-2021-understanding, dettmers-etal-2022-gpt3}. Recent directions include: holding outlier weights in higher precision \citep{dettmers-etal-2022-gpt3}, Hessian-based weight sensitivity \citep{frantar-etal-2023-optq}, searching for optimal clipping thresholds \citep{wei-etal-2023-outlier}, or combinations of these approaches \citep{lin-etal-2024-awq, dettmers-etal-2024-spqr}.

\paragraph{Pruning.}

The aim of neural network pruning is to remove less important weights, therefore reducing the overall model size \citep{lecun-etal-1989-optimal}. Pruning can be performed at the level of individual weights (unstructured), within groups of weights (semi-structured), or entire dimensions (structured) \citep{han-etal-2015-learning, mishra-etal-2021-accelerating, ma-etal-2023-llm}. In particular, 2:4 semi-structured sparsity (i.e. pruning two weights in every block of four), enables enhanced inference performance on NVIDIA GPUs \citep{mishra-etal-2021-accelerating}. However, the extensive size of LMs presents challenges in pruning them optimally \citep{hassibi-etal-1993-optimal}. Recent work has instead decomposed LM pruning into a sequential layer-wise approach, demonstrating remarkable performance retention, even at high sparsity levels \cite{frantar-alistarh-2023-sparsegpt, sun-etal-2024-simple, yin-etal-2024-outlier}.

\paragraph{Domain-specific pruning.}

Early work focused on pruning deep neural networks for specific tasks \citep{han-etal-2015-learning, molchanov-etal-2017-pruning}. This trend continued \citep{sanh-etal-2020-movement, lagunas-etal-2021-block, kwon-etal-2022-fast} following the advent of BERT \citep{devlin-etal-2019-bert}. However, the shift towards general-purpose LMs \citep{brown-etal-2020-language, dredze-etal-2024-academics} has led to a focus on preserving general performance \citep{frantar-alistarh-2023-sparsegpt, ma-etal-2023-llm, sun-etal-2024-simple}, i.e. language modeling and reasoning. Most recently, \citet{zhang-etal-2024-pruning} proposed D-Pruner for domain-specific pruning, leveraging general weight importance to form a domain-specific training loss. However, this requires computationally expensive full-parameter fine-tuning, yet does not consistently outperform general-purpose pruning methods.

\paragraph{Calibration data.}

In a post-training setting, model compression typically relies upon calibration data \citep{wan-etal-2024-efficient}. Calibration data consists of a small number of unlabeled examples, for the generation of layer activations \citep{nagel-etal-2020-up, hubara-etal-2021-accurate}. Typically, these examples are randomly sampled from web text or pre-training datasets (e.g. C4; \citealp{raffel-etal-2020-exploring}). However, recent work has illustrated the influential role that calibration data can play, impacting the downstream performance of compressed models \citep{williams-aletras-2024-impact}. Most recently, \citet{bandari-etal-2024-c4} suggest that pruning with calibration data based on downstream tasks does not necessarily benefit performance over generic data.