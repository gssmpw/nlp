%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\newcommand{\HRule}{\rule{.98\linewidth}{.6pt}}
\usepackage{multirow,colortbl}
\definecolor{grey}{rgb}{0.9,0.9,0.9}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
% \usepackage{etoolbox}
\usepackage{hyperref}
% % Redefine the back-reference format to include "p." or "pp."
% \makeatletter
% \newcommand{\countelements}[1]{%
%     \def\tmp@count{0}%
%     \@for\@temp:=#1\do{\advance\@temp@count by 1}%
%     \the\@temp@count
% }
% \makeatother
% \renewcommand*{\backref}[1]{%
%     \ifx#1\relax
%     \else
%         \ifnumequal{\listlen{#1}}{1}
%             (p.~#1)%
%         \else
%             (pp.~#1)%
%         \fi
%     \fi
% }

% \makeatletter
% \renewcommand*{\backref}[1]{%
%     \newcounter{tempcount}%
%     \setcounter{tempcount}{0}%
%     \@for\@tempc:=#1\do{\stepcounter{tempcount}}%
%     \ifnum\value{tempcount}=1
%         (p.~#1)%
%     \else
%         (pp.~#1)%
%     \fi
% }
% \makeatother

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}


% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{upgreek}
\usepackage{makecell}
\usepackage{dsfont}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{marvosym}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{csquotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mixture of Expert Reconstructors}






\graphicspath{{figure/}{figures/}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sech}{sech}
\newcommand{\tabhead}[1]{{\bfseries#1}}
% \DeclareMathOperator{\md}{d}
\newcommand{\md}{\text{d}}
\DeclareMathOperator{\uniform}{\text{Unif}}
\DeclareMathOperator{\e}{e}
\DeclareMathOperator{\lbint}{[\![}
\DeclareMathOperator{\rbint}{]\!]}
\DeclareMathOperator{\trainset}{\mathcal{D}_{\text{tr}}}
\DeclareMathOperator{\trainsetenv}{\mathcal{D}_{\text{tr}}^e}
\DeclareMathOperator{\testset}{\mathcal{D}_{\text{te}}}
\DeclareMathOperator{\adaptset}{\mathcal{D}_{\text{ad}}}
\DeclareMathOperator{\trainenvs}{\upxi_{\text{tr}}}
\DeclareMathOperator{\adaptenvs}{\upxi_{\text{ad}}}




\usepackage{xfrac}
\usepackage{listings}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{algorithmicx,algpseudocode}
\usepackage{cuted}

\usepackage{svg}

\usepackage{tabularx,ragged2e}
\usepackage{balance}
\usepackage{longtable}

\algblockdefx{MRepeat}{EndRepeat}{\textbf{Repeat}}{}
\algnotext{EndRepeat}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{grey}{rgb}{0.9,0.9,0.9}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Dangerous, remove this if not ICML
\setlength{\tabcolsep}{2pt}






\begin{document}

\twocolumn[
\icmltitle{Towards Foundational Models for Dynamical System Reconstruction: Hierarchical Meta-Learning via Mixture of Experts}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Roussel Desmond Nzoyem}{rdn}
\icmlauthor{David A.W. Barton}{dab}
\icmlauthor{Tom Deakin}{td}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}



% \author{Roussel Desmond Nzoyem \\
% School of Computer Science\\
% University of Bristol\\
% Bristol, BS8 1QU, UK \\
% \texttt{rd.nzoyemngueguin@bristol.ac.uk} \\
% \And
% David A.W. Barton \\
% School of Engineering Mathematics and Technology \\
% University of Bristol\\
% Bristol, BS8 1QU, UK \\
% \texttt{David.Barton@bristol.ac.uk} \\
% \And
% Tom Deakin \\
% School of Computer Science\\
% University of Bristol\\
% Bristol, BS8 1QU, UK \\
% \texttt{tom.deakin@bristol.ac.uk} \\
% }


\icmlaffiliation{rdn}{School of Computer Science, University of Bristol, UK}
\icmlaffiliation{dab}{School of Engineering Mathematics and Technology, University of Bristol, UK}
\icmlaffiliation{td}{School of Computer Science, University of Bristol, UK}

\icmlcorrespondingauthor{Roussel Desmond Nzoyem}{rd.nzoyemngueguin@bristol.ac.uk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Scientific Machine Learning, Meta-Learning, Multi-Task Learning, Time Series}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}

As foundational models reshape scientific discovery, a bottleneck persists in dynamical system reconstruction (DSR): the ability to learn across system hierarchies. Many meta-learning approaches have been applied successfully to single systems, but falter when confronted with sparse, loosely related datasets requiring multiple hierarchies to be learned. Mixture of Experts (MoE) offers a natural paradigm to address these challenges. Despite their potential, we demonstrate that naive MoEs are inadequate for the nuanced demands of hierarchical DSR, largely due to their gradient descent-based gating update mechanism which leads to slow updates and conflicted routing during training. To overcome this limitation, we introduce MixER: \underline{Mix}ture of \underline{E}xpert \underline{R}econstructors, a novel sparse top-1 MoE layer employing a custom gating update algorithm based on $K$-means and least squares. Extensive experiments validate MixER's capabilities, demonstrating efficient training and scalability to systems of up to ten parametric ordinary differential equations. However, our layer underperforms state-of-the-art meta-learners in high-data regimes, particularly when each expert is constrained to process only a fraction of a dataset composed of highly related data points. Further analysis with synthetic and neuroscientific time series suggests that the quality of the contextual representations generated by MixER is closely linked to the presence of hierarchical structure in the data.



\end{abstract}















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{introduction}


% 1. The general area of the parametric PDEs, and one layer on top
The emergence of foundational models in language and vision has catalyzed an accelerated pursuit of analogous models for scientific discovery \cite{subramanian2024towards,herde2024poseidon}. Unlike traditional data modalities, scientific data presents unique challenges due to its inherent complexity and scarcity. This challenge has motivated the development of sophisticated dynamical system reconstruction (DSR) models capable of robust generalization across varying domains---with each variation constituting an \emph{environment}. However, the effectiveness of these systems in low-data scenarios hinges on substantial relatedness among environments, raising fundamental questions about learning across \textbf{families} of loosely connected environments (see \cref{fig:hierarchy_problem}).

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.95\columnwidth]{figures/2Families.pdf}
\captionof{figure}{Task-specific and hierarchical meta-learning. Each family comprises a set of environments defined by the same ordinary differential equation (ODE). Within a family, parameters of the underlying ODE are varied, producing dynamics that are similar but unique. Task-specific meta-training focuses on adaptation within a family, while hierarchical meta-learning enables simultaneous training across families, followed by adaptation to any of them.}
\label{fig:hierarchy_problem}
\end{center}
\end{figure}



% 2. Lit review (from parametric to foundational)
Usual approaches to data-driven generalizable DSR \cite{goring2024out} predominantly rely on Expected Risk Minimization (ERM) \cite{sagawa2020distributionally,brandstetter2022message}, assuming abundant environment-specific samples to capture the full spectrum of observable dynamics. This assumption proves problematic in data-scarce domains like clinical or neuroscience applications \cite{brenner2024learning}. While multitask learning \cite{yin2021leads} has emerged as a popular alternative, it lacks robust adaptation mechanisms for novel scenarios. Recent advances in \textbf{meta-learning} \cite{wang2021bridging,finn2017model} have demonstrated remarkable success by explicitly incorporating adaptation capabilities into the training process. \emph{Contextual} meta-learning \cite{nzoyem2024extending} achieves this through a strategic separation of parameters into environment-agnostic components and compact context vectors amenable to environment-specific fine-tuning via gradient descent. Current state-of-the-art approaches are categorized in two primary paradigms: \emph{hypernetwork}-based methods \cite{kirchmeyer2022generalizing,brenner2024learning,roeder2019efficient,koupai2024boosting} that condition environment-specific weights on context, and \emph{concatenation}-based alternatives \cite{nzoyem2025neural,zintgraf2019fast} that directly feed the context to the dynamics-generating model. Despite their strong potential, meta-learning approaches exhibit limitations when confronted with environments that have minimal or no similarities, which is the case when the time series are not governed nor driven by the same underlying differential equations.

% 3. Mixture of Experts
Drawing inspiration from recent breakthroughs in large language modeling \cite{liang2024mixture,jiang2024mixtral,dai2024deepseekmoe,abnar2025parameters}, we investigate the potential of augmenting existing meta-learners with sparse \textbf{mixture of experts} (MoEs) \cite{jacobs1991adaptive,shazeer2017outrageously} for generalizable DSR. Despite inherent routing challenges that constrain their applications to DSR, MoEs offer a natural framework for learning across families of arbitrarily related environments. We claim that strategic combination of contextual meta-learners enables simultaneous reconstruction across all families while preserving rapid adaptation capabilities, obviating the need for manual dataset partitioning prior to meta-learning on each subset.

Specifically, we introduce a novel routing mechanism for MoEs that facilitates learning across dynamical systems and potentially unrelated families thereof, while maintaining adaptability within individual environments. Our approach achieves this through two key innovations: ($i$) grounding the routing mechanism in contextual information rather than state input vectors, and ($ii$) implementing an explicit routing protocol combining $K$-means clustering with least squares optimization for precise expert-cluster pairing.


% 4. Structure of the paper
After establishing the formal problem structure and highlighting the imperative for hierarchical meta-learning in \cref{problem}, we present our MixER methodology and its core optimization components in \cref{method}. \cref{experiments} demonstrates our main findings in few-shot learning and time series classification, benchmarked against current state-of-the-art approaches. We summarize our contributions as follows:
\begin{enumerate}
    \item We identify a fundamental limitation of gradient-descent when routing contextual information to DSR models, which slows down expert specialization when training MoEs.
    \item We propose an effective unsupervised routing mechanism for MoEs to collectively learn dynamical systems with various degrees of relatedness.
    \item We provide experimental evidence of the breadth of applicability of our method on two and ten families of ordinary differential equations, several classical DSR benchmarks, and synthetic time-series data.
    \item We demonstrate the limited contextual representations resulting from MixER when the data is ambiguous and closely related, as is the case with neuroscientific epileptic data.
\end{enumerate}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Description \& Motivation}
\label{problem}

This work addresses the fundamental challenge of learning collections of dynamical systems from data with limited knowledge of their interrelationships. We demonstrate the limitations of conventional meta-learning approaches through two ordinary differential equations (ODEs), and we propose MixER as a solution to these limitations.

% \subsection{Conventions \& Notations}
% \label{notations}



\subsection{Hierarchical Dynamical System Reconstruction}
\label{dsr}

In this section, we present the setting of hierarchical DSR. For notational clarity, we employ consistent shorthand throughout the paper. The notation $[T]$ denotes the discrete set $\{1,\ldots,T\}$ for any positive integer $T$. Environment-related indices (e.g., $e$, $i$, and $f$ as seen below) are denoted by \emph{superscripts}, while other indices such as time $t$ and expert count $m$ use \emph{subscripts}.

The reconstruction of families of dynamical systems requires a novel framework for handling multi-level temporal data. In our framework, each datum consists of a (multivariate) time series $\{x_t\}_{t\in[T]} \in \mathbb{R}^{T \times d}$ of length $T>0$ and dimension $d\geq1$, representing either simulated trajectories or observed process measurements. These data points may present shared knowledge, such as repeated clinical measurements from a patient \cite{brenner2024learning} or varying parameters of the same physical system, referred to as ``environments''. The complete dataset comprises $E\geq 1$ environments $\{x_t^{e,i}\}^{e\in[E]}_{i\in[I]}$, where $I\geq1$ represents the distinct time series count in environment $e$. When environments exhibit higher-order relationships, the dataset extends to $F\geq1$ \emph{families}, denoted as $\{x_t^{f,e,i}\}^{f\in[F]}$ (see \cref{fig:hierarchy_problem}).

Importantly, we make no assumptions about inter-family relationships, which may range from loose to intricate connections. For this reason, the training data is presented as $\trainset \triangleq \{x_t^{e,i}\}^{e\in[E]}$, with unsupervised environment clustering into families occurring during learning. In cases without repeated measurements, each time series $i$ constitutes its own environment. This framework enables the development of foundational models capable of processing heterogeneous data while generalizing conventional dynamical system reconstruction approaches.

Learning on our datasets can be viewed in two levels: conventional (or flat), and hierarchical DSR models.

\paragraph{Flat DSR Models} The base level formulates dynamical system reconstruction as a supervised learning problem \cite{goring2024out,kramer2021reconstructing,yin2021leads}. The primary objective is learning a flow mapping $G_{\theta}$ that transforms latent representation $z_t$ across time steps:
\begin{align} \label{eq:seqtoseq}
    z_t = G_{\theta} (z_{t-1}, x_{t-1}),
\end{align}
where $x_{t-1}$ represents an optional ground truth teacher-forcing signal and $\theta$ denotes learnable parameters. We note, however, that $x_{t-1}$ is \emph{not} used during inference as the system is rolled out auto-regressively. This formulation describes a \emph{sequence-to-sequence} learning problem \cite{brenner2024learning,kidger2020neural,gu2023mamba}.

In scenarios without teacher forcing \cite{yin2021leads,kirchmeyer2022generalizing}, the problem transforms into a \emph{state-to-sequence} or \emph{initial value problem} (IVP):
\begin{align} \label{eq:statetoseq}
    \frac{\md z_t}{\md t} = G_{\theta}(z_t), \qquad \forall t \in \left[ 0,T \right],
\end{align}
or equivalently:
\begin{align} \label{eq:statetoseq_int}
    z_t = z_0 + \int_{0}^{t} G_{\theta}(z_{\tau}) \, \md \tau, \qquad \forall t \in \left[ 0,T \right].
\end{align}
This approach underlies Neural ODEs \cite{chen2018neural,rackauckas2020universal,haber2017stable,weinan2017proposal}, which have become invaluable in generative modeling \cite{lipman2022flow,liu2023flow} and engineering applications \cite{kochkov2024neural,shen2023differentiable}.

\paragraph{Hierarchical DSR Models} Environment-aware models introduce a context vector $\xi$ that modulates model behavior. We consider two conditioning approaches: \emph{hypernetwork}-based conditioning \cite{kirchmeyer2022generalizing,brenner2024learning}, where a secondary network $H_{\theta}$ generates environment-specific weights:
\begin{align} \label{eq:hier-model}
    z_t = G_{\theta^e} (z_{t-1}, x^e_{t-1}), \quad \text{with } \theta^e = H_{\theta}(\xi^e),
\end{align}
and \emph{concatenation}-based conditioning \cite{nzoyem2025neural,zintgraf2019fast}, where the context is directly fed to the flow map:
\begin{align} \label{eq:concat-model}
    z_t = G_{\theta} (z_{t-1}, x^e_{t-1}, \xi^e).
\end{align}
For convenience, both approaches can be denoted as
\begin{align} \label{eq:meta-learner}
    z_t = G_{\theta,\xi^e} (z_{t-1}, x^e_{t-1}).
\end{align}

Current hierarchical DSR models, such as \cref{eq:meta-learner}, struggle with complex data relationships (e.g. families of unrelated environments), raising the critical question:
\begin{displayquote}
\emph{What is the optimal way to cluster environments so that existing contextual meta-learning approaches can utilize them effectively?} 
\end{displayquote}

Before presenting our solution to this question, we underline the central limitation of contextual meta-learning with a simple example. We show that without appropriate clustering, these methods become ineffective when confronted with datasets of loosely related families.


\subsection{Motivating Example}
\label{motivation}

The limitations of task-specific meta-learners and naive MoE become evident in the initial value problem (IVP) setting of \cref{fig:hierarchy_problem}. Our test case involves simultaneous learning of two 2-dimensional ODEs proposed by \citet{d'ascoli2024odeformer}. The dataset comprises two families, with the goal of reconstructing $I=32$ test-time trajectories across $E=10$ total environments from both families (see ODEBench-2 in \cref{tab:ode-bench}). We evaluate three state-of-the-art meta-learners—Neural Context Flow (NCF) \cite{nzoyem2025neural}, CoDA \cite{kirchmeyer2022generalizing}, and GEPS \cite{koupai2024boosting}—within a top-1 MoE framework.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/mixer_barplots.pdf}
\includegraphics[width=0.9\columnwidth]{figures/mixer_losses.pdf}
\captionof{figure}{Limitation of task-specific meta-learning and vanilla MoE on the two families of ODEs from \cref{fig:hierarchy_problem}. Strategically increasing the capacity of the network with MixER and its special routing algorithm results in a successful model. (Top) Relative $L^2$ error on test set; (Bottom) Validation MSE losses during training.}
\label{fig:mixer_barplots}
\end{center}
\end{figure}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/MoE_vs_MixER.pdf}} 
% \includesvg[width=0.95\linewidth]{figures/MoE_vs_MixER.svg}
\captionof{figure}{Illustration of vanilla MoE and our proposed MixER layer. \textbf{(Left)} Vanilla MoE setting where a single input $x$ is passed through a gating network whose outputs enable the router to assign computation to a specific expert \cite{chen2022towards}. \textbf{(Right)} Our sparse MixER layer requires a context vector $\xi$ alongside the input $x$. The gating network computes expert affinities based on this context vector. Contrary to MoE, the MixER layer disregards the softmax-weighted output aggregation.}
\label{fig:moe_vs_mixer}
\end{center}
\end{figure*}

\cref{fig:mixer_barplots} demonstrates that single task-specific meta-learners cannot capture the inherent data complexity. Furthermore, a naive MoE implementation with gradient-based gating updates \cite{shazeer2017} routes all contexts to a single expert (\cref{fig:mixer_heatmap}), yielding suboptimal performance. The validation losses reveal that once suitable gating weights are found and family-expert pairings established, our proposed solution (in green) dramatically improves performance starting around training step 40 for CoDA or 150 for NCF.

Top-1 MoE's fundamental advantage lies in its reduced active parameter count which saves computation during inference \cite{jiang2024mixtral,fedus2022switch}. The following section details our improved routing mechanism which leverages this top-1 sparsity structure.













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mixture of Expert Reconstructors}
\label{method}




We present a novel MoE layer that leverages $K$-means clustering and least squares to minimize routing conflicts across DSR models in its layer.

The MixER layer, depicted in \cref{fig:moe_vs_mixer}, fundamentally differs from vanilla MoE layers in two aspects. First, MixER incorporates an environment-specific context vector $\xi$ as additional input for computing gating weights, addressing the limitation that pointwise state input $x_t$ alone cannot fully characterize temporal behavior. Second, MixER employs a top-1 MoE architecture and eliminates the need for softmax weighting of expert outputs. This design choice enables experts to function independently outside the layer, a critical feature for our gating network update methodology.



\subsection{Optimization Procedure}
\label{optimisation}

Our training pipeline optimizes both environment-specific parameters $\Xi \triangleq \{ \xi^e \}^{e\in[E]}$ and shared parameters $\Theta \triangleq \{ \theta_m \}_{m\in[M]}$, where $M$ denotes the number of experts. The optimization minimizes the aggregate MSE loss:

\begin{align}
\label{eq:loss}
% \resizebox{.84\hsize}{!}{\displaystyle
    \mathcal{L}(\Theta, \Xi, \trainset) \triangleq \frac{1}{E \times I \times T} \sum_{e=1}^E \sum_{i=1}^I \sum_{t=1}^T \Vert \hat x^{e,i}_t - x^{e,i}_t\Vert_2^2,
% }
\end{align}
where $\hat x$ represents the reconstructed trajectory. We implement \emph{proximal} alternating minimization \cite{li2019alternating}, chosen for its proven convergence to second-order optimal solutions \cite{li2019alternating,nzoyem2025neural}. Notably, our framework eliminates the need for importance or load-balancing terms in the loss function \cite{shazeer2017outrageously}.

The gating network $W$ updates occur independently of $\Theta$, as motivated in \cref{motivation}. Our implementation applies gating updates after each (or several) gradient update of either $\Theta$ or $\Xi$. During adaptation to novel environments, only context vectors undergo optimization via gradient descent, while $W$ and $\Theta$ remain fixed.

To address scale variations across trajectory families, we employ small batches of closely related environments (determined by $L^1$ norm between context vectors) for stochastic updates of $\Theta$ and $\Xi$. For validation and model selection in these scenarios, we utilize the relative $L^2$ loss defined in \cref{eq:relmse}.












\subsection{Gating Network Update}
\label{k-means}

The gating network transforms a context $\xi^e$ into $M$ logits $g^e \triangleq \{g^e_m\}_{m\in[M]}$, where the maximum value identifies the optimal expert for environment $e$. We implement a linear mapping\footnote{In practice, we note that $W$ contains a bias term omitted here for conciseness.}:
\begin{align}
g^e = \xi^e W, \qquad \forall \, e \in \{1,\ldots,E\}
\end{align}
optimized through least squares (see \cref{alg:gateupdate}), with labels $Y$ (proxies for $g^e$)  derived from $K$-means clustering using Lloyd's algorithm \cite{lloyd1982least} (see \cref{alg:kmeans}).


The update procedure in \cref{alg:gateupdate} comprises four key stages: \textbf{(1)} $K$-means clustering (line 6); \textbf{(2)} per-expert per-environment loss computation (lines 7 and 8); \textbf{(3)} expert-cluster pairings (lines 9 to 18); and \textbf{(4)} least-squares optimization (lines 19 to 24). These stages are visualized in \cref{fig:gating_update}.


\begin{algorithm}[H]
    \caption{Gating Network Update}
    \label{alg:gateupdate}
\begin{algorithmic}[1]
    \State {\bfseries Require:} $\Theta := \{ \theta_m \}_{m\in [M]} $ mixture of $M$ experts
    \State $\qquad \qquad \Xi := \{ \xi^e \}^{e\in [E\times F]} $ context vectors
    \State $\qquad \qquad  \bar \Xi := \{ \bar \xi_m \}_{m\in [M]}$ centroid initialization
    \State $\qquad \qquad \trainset \triangleq \{\mathcal{D}^e_{\text{tr}}\}^{e\in[E]}$ training data
    \State $\qquad \qquad \sigma>0$ noise standard deviation

    \State $C, \bar \Xi \leftarrow$ $K$-Means$(\Xi, \bar \Xi)$ \Comment{see \cref{alg:kmeans}}

    \;
    \State $\ell_{m,e} = \mathcal{L}(\theta_m, \xi^e, \mathcal{D}^e_{\text{tr}}) \quad \forall m \in [M], \forall e \in [E]$ 
    
    \State $\bar \ell_{\cdot,c} = \text{Median}\{\ell_{\cdot,e} : e \in C_c \} \quad \forall c \in [M]$ 

    \;

    \State Initialize $\mathcal{S} \gets \emptyset$ \Comment{Selected experts}
    \For{$c \in [M]$}
        \State $\text{SortedList} \gets \text{argsort}(\bar \ell_{\cdot,c})$
        \State $m \gets \text{SortedList}_1$
        \While{$m \in \mathcal{S}$}
            \State $\text{SortedList} \gets \text{SortedList}_{2:\text{length(SortedList)}}$
            \State $m \gets \text{SortedList}_1$
        \EndWhile
        \State $\mathcal{S} \gets \mathcal{S} \cup \{m\}$
    \EndFor

    \;

    \State $Y \gets \mathbf{0}_{E \times M}$    \Comment{Least squares proxy labels}
    \For{$c \in [M]$}
        \State $Y_{C_c} \gets \text{OneHotEncode}(\mathcal{S}_c, M)$
    \EndFor
    
    \State $X \gets \Xi + \mathcal{N}(0, \sigma)$ \Comment{Add noise to context}
    
    \State $W \gets \text{LeastSquares}(X, Y)$
    
    \;
    \State {\bfseries Return $W, \bar \Xi$}
    
\end{algorithmic}
\end{algorithm}


\begin{figure}[h]
\begin{center}
\includegraphics[width=\columnwidth]{figures/MixER-GatingUpdate.pdf}
\captionof{figure}{Illustration of the main stages of our context-based gating update algorithm, with all tensors in compatible shapes.}
\label{fig:gating_update}
\end{center}
\end{figure}

Our implementation incorporates two crucial optimizations. First, we mitigate $K$-means sensitivity to initial conditions by reusing centroids from previous gating updates (line 6), achieving convergence typically within two iterations. Second, we introduce controlled noise to $\Xi$ before least squares computation, enhancing robustness against suboptimal configurations and preventing instability during early training when context values cluster near their zero initialization.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{experiments}

We evaluate our approach through comprehensive experiments on both loosely and closely related dynamical systems, synthetic and real-world datasets. Our analysis encompasses datasets of varying complexity, baseline comparisons, and detailed performance assessments.

\subsection{One-Shot Learning on Loosely Related Families}

Meta-learning across families of dynamical systems demonstrates the potential of our approach. Using the ODEBench dataset \cite{d'ascoli2024odeformer}, we analyze 10 distinct ODE families, each containing multiple environments generated by parameter variations (\cref{tab:ode-bench}). The experimental setup consists of 4 meta-training trajectories per environment, with 32 additional trajectories reserved for evaluation. One-shot adaptation is evaluated by fine-tuning context vectors on a single trajectory, repeated across 4 adaptation environments per family. Further data generation details are available in \cref{app:datasets}.

\begin{table}[h!]
    \caption{Number of training families and environments extracted from the ODEBench dataset \cite{d'ascoli2024odeformer}.}
    \vspace*{0.1cm}
    \centering
    \small
    \begin{tabular}{l|c|c|c}
        \toprule
         & \tabhead{\# Families} & \tabhead{\# Env. Per Fam.} & \tabhead{\# Total Envs.} \\
        \midrule
         \tabhead{ODEBench-2} & 2 & 5 & 10 \\
         \tabhead{ODEBench-10A} & 10 & 5 & 50 \\
         \tabhead{ODEBench-10B} & 10 & 16 & 160 \\
         \bottomrule
    \end{tabular}
    \label{tab:ode-bench}
\end{table}

As in \cref{motivation}, we consider three leading adaptation rules: NCF \cite{nzoyem2025neural}, CoDA \cite{kirchmeyer2022generalizing}, and GEPS \cite{koupai2024boosting}. We wish to know whether our approach boosts the performance of these baselines on such loosely connected data. All adaptation rules utilize the same MLP \cite{haykin1994neural} as the root (or main) network. Our MixER implementations employ context vectors of dimension $d_{\xi}=40$, evenly distributed among expert meta-learners (implementation details in \cref{app:algorithms}).


\begin{table}[ht!]
\caption{Training and adaptation relative MSEs ($\downarrow$) on the ODEBench-10A dataset, across 3 runs with different seeds. MixER-$M$ means $M$ experts are present in the layer. The $\dagger$ indicates the naive MoE with the gate updated via gradient descent. The best along the columns is reported in \textbf{bold}.}
% \vspace*{-0.3cm}
\label{tab:odebench-10A}
\begin{center}
\tiny
\begin{sc}
\begin{tabularx}{0.48\textwidth}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{NCF}} & \multicolumn{2}{c}{\textbf{CoDA}} & \multicolumn{2}{c}{\textbf{GEPS}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Train  & Adapt & Train  & Adapt & Train & Adapt   \\
\midrule
MixER-1      & 2.05$\pm$0.12 & \textbf{1.80$\pm$0.28} & 0.98$\pm$0.08 & 6.91$\pm$1.25 & 2.61$\pm$0.1 & 2.20$\pm$0.21 \\
% MixER-5   & 0.38$\pm$0.02 & 0.54$\pm$0.02 & 0.12$\pm$0.04 & 0.38$\pm$0.02 & 0.17$\pm$0.03 & 0.92$\pm$0.10 \\
MixER-$10^\dagger$      & 1.53$\pm$0.34 & 5.28$\pm$1.04 & 0.76$\pm$0.07 & \textbf{4.25$\pm$0.15} & \textbf{0.58$\pm$0.04} & \textbf{1.16$\pm$0.09} \\
MixER-10   & \textbf{1.05$\pm$0.09} & 2.38$\pm$0.23 & \textbf{0.47$\pm$0.06} & 15.9$\pm$4.2 & 1.01$\pm$0.05 & 1.29$\pm$0.08 \\
\bottomrule
\end{tabularx}
\end{sc}
\end{center}
\end{table}

Results on ODEBench-10A (\cref{tab:odebench-10A}) reveal that MixER enhances performance across all contextual meta-learning backbones on the training evaluation sets. However, adaptation performance varies significantly, with GEPS maintaining consistency while NCF and CoDA's performance degrades. The limited environment count in ODEBench-10A constrains overall performance, motivating our evaluation on the more comprehensive ODEBench-10B dataset.


\begin{table}[ht!]
\caption{Training and adaptation metrics on ODEBench-10B. (Top) Relative MSE ($\downarrow$); (Bottom) Proportion of environments (\%) with relative MSE below the threshold of $\varepsilon=0.1$ ($\uparrow$) .}
% \vspace*{-0.3cm}
\label{tab:odebench-10B}
\begin{center}
\tiny
\begin{sc}
\begin{tabularx}{0.48\textwidth}{lcccccc}
\toprule
& \multicolumn{2}{c}{\textbf{NCF}} & \multicolumn{2}{c}{\textbf{CoDA}} & \multicolumn{2}{c}{\textbf{GEPS}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Train  & Adapt & Train  & Adapt & Train & Adapt   \\
\midrule
MixER-1      & \textbf{0.12$\pm$0.01} & 3.20$\pm$0.28 & \textbf{0.07$\pm$0.01} & \textbf{0.34$\pm$0.05} & 0.21$\pm$0.1 & 1.24$\pm$0.07 \\
MixER-$10^\dagger$      & 0.29$\pm$0.02 & 2.53$\pm$0.20 & 0.15$\pm$0.03 & 0.72$\pm$0.08 & 0.13$\pm$0.4 & \textbf{0.49$\pm$0.02} \\
MixER-10   & 0.22$\pm$0.60 & 1.43$\pm$0.23 & 0.10$\pm$0.02 & 14.8$\pm$4.2 & \textbf{0.06$\pm$0.01} & 1.43$\pm$0.02 \\
MixER-20   & 0.38$\pm$0.02 & \textbf{0.54$\pm$0.02} & 0.12$\pm$0.04 & 0.38$\pm$0.02 & 0.17$\pm$0.03 & 0.92$\pm$0.10 \\
\bottomrule
\end{tabularx}

\begin{tabularx}{0.445\textwidth}{lcccccc}
% \toprule
& \multicolumn{2}{c}{\textbf{NCF}} & \multicolumn{2}{c}{\textbf{CoDA}} & \multicolumn{2}{c}{\textbf{GEPS}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Train  & Adapt & Train  & Adapt & Train & Adapt   \\
\midrule
MixER-1      & \textbf{66.9$\pm$2.1} & \textbf{40.0$\pm$2.3} & \textbf{85.6$\pm$2.7} & \textbf{50.0$\pm$2.5} & 64.4$\pm$3.9 & 37.5$\pm$3.5 \\
MixER-$10^\dagger$  & 36.9$\pm$6.4 & 30.0$\pm$3.5 & 71.9$\pm$1.6 & 42.5$\pm$0.8 & 71.2$\pm$5.4 & \textbf{50.0$\pm$1.4} \\
MixER-10   & 55.6$\pm$4.6 & 2.5$\pm$2.8 & 80.0$\pm$2.5 & 25.0$\pm$2.8 & \textbf{85.6$\pm$5.4} & 32.5$\pm$3.0 \\
MixER-20   & 37.5$\pm$2.4 & 20.0$\pm$4.2 & 60.6$\pm$8.0 & 27.5$\pm$1.0 & 55.6$\pm$3.2 & 35.0$\pm$2.5 \\
\bottomrule
\end{tabularx}
\end{sc}
\end{center}
\end{table}


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/gating_values_10_fams.pdf}
\captionof{figure}{Gating weights on ODEBench-10B, at the end of training with MixER-10. (Top) Gating heatmap. (Bottom) Histogram across all 160 environments.}
\label{fig:gating_10_fams}
\end{center}
\end{figure}

Analysis of ODEBench-10B (\cref{tab:odebench-10B}) shows that MixER-10 underperforms compared to MixER-$10^\dagger$ (naive MoE) and MixER-1 (a single meta-learner). Performance analysis using relative $L^2$ thresholds (defined in \cref{eq:tprelmse}) globally indicates diminished benefits from using MixER-10. However, GEPS exhibits remarkable robustness, showing consistent improvement with increased expert count in both training and adaptation scenarios, even with gradient-based gating updates. As expected, visualization of gating values (\cref{fig:gating_10_fams}) reveals that enhanced performance correlates with improved environment-to-expert routing in groupings of 16 across all 160 environments.







\subsection{Generalization on Classical DSR Datasets}

Classical DSR benchmarks reveal the breadth of applicability of our approach. We evaluate three datasets of closely related environments: ($i$) Lotka-Volterra (\textbf{LV}), a 2-dimensional ODE modeling species evolution in closed ecosystems \cite{yin2021leads}; ($ii$) Glycolytic Oscillator (\textbf{GO}), a model of yeast glycolysis \cite{kirchmeyer2022generalizing}; and ($iii$) Sel'kov Model (\textbf{SM}), a more complex 2-dimensional ODE for glycolysis that exhibits a Hopf bifurcation \cite{nzoyem2025neural}.

We consider the same NCF, CoDA, and GEPS backbones as above. Additionally, we consider CAVIA\footnote{We did not augment CAVIA within our MixER layer due to its second-order optimization algorithm.} \cite{zintgraf2019fast}. MixER employs three experts across all experiments, with parameter counts matched to baselines for fair comparison. Context vector dimensions vary by backbone: NCF uses $d_{\xi}=512$, while CoDA and GEPS use $d_{\xi}=2$, reflecting underlying physical parameter variations. Additional hyperparameters are documented in \cref{app:hyperparams}.




\begin{table*}[ht!]
\caption{In-Domain (InD) and Out-of-Distribution (OoD) test MSEs ($\downarrow$) for the LV, GO, and SM problems. The star indicates runs using the reference implementations\footnotemark. Results for CAVIA, CoDA* and NCF* are reported from \cite{nzoyem2025neural}. The best is reported in \textbf{bold}. The best of the three MixERs is shaded in \colorbox{grey}{grey}. The \#\textsc{Params} columns indicate the active parameter counts.}
\label{tab:classical_results}
\begin{center}
% \footnotesize
\scriptsize
\begin{sc}
\begin{tabularx}{0.82\textwidth}{lccccccccc}
\toprule
& \multicolumn{3}{c}{\textbf{LV} ($\times 10^{-5}$)} & \multicolumn{3}{c}{\textbf{GO} ($\times 10^{-4}$)} & \multicolumn{3}{c}{\textbf{SM} ($\times 10^{-3}$)}\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
 & \#Params  & InD  & OoD &  \#Params  & InD & OoD  &  \#Params  & InD & OoD \\
\midrule
CAVIA      & 305246 & 91.0$\pm$63.6 & 120.1$\pm$28.3 & 130711 & 64.0$\pm$14.1 & 463.4$\pm$84.9 & 50486 & 979.1$\pm$141.2 & 859.1$\pm$70.7 \\
CoDA*      & 305793 & \textbf{1.40$\pm$0.13} & 2.19$\pm$0.78 & 135390 & 5.06$\pm$0.81 & 4.22$\pm$4.21 & 50547 & 156.0$\pm$40.52 &  8.28$\pm$0.29 \\
NCF*   & 308240 &  1.68$\pm$0.32 &  \textbf{1.99$\pm$0.31} & 131149 & \textbf{3.33$\pm$0.14}  & \textbf{2.83$\pm$0.23} & 50000 & \textbf{6.42$\pm$0.41}  & \textbf{2.03$\pm$0.12} \\
MixER-NCF   & 307245 & \cellcolor{grey} 3.70$\pm$0.4 &  \cellcolor{grey}  4.45$\pm$0.3 & 130535 & 73.5$\pm$21.1  &  141.5$\pm$82.8 & 50387 & \cellcolor{grey} 32.3$\pm$4.2 & 64.2$\pm$1.5\\
MixER-CoDA   & 307995 &  4.00$\pm$0.01 &   53.5$\pm$0.4 & 132137 &  42.0$\pm$18.9  & \cellcolor{grey} 49.3$\pm$25.1 & 51995 & 32.8$\pm$3.9 & 317.2$\pm$6.0\\
MixER-GEPS   & 305112 &  14.8$\pm$0.7 &   82.4$\pm$0.9 & 131747 & \cellcolor{grey} 22.3$\pm$23.2  &  259.7$\pm$45.0 & 51312 & 27.6$\pm$5.8 & \cellcolor{grey} 46.3$\pm$2.7\\
\bottomrule
\end{tabularx}
\end{sc}
\end{center}
\end{table*}

\footnotetext{For integration within the MixER layer, we performed custom reimplementation of the backbones as explained in \cref{app:baselines}.}

Results presented in \cref{tab:classical_results} demonstrate that while all methods successfully approximate the IVP vector fields, MixER underperforms relative to its baseline meta-learners. Clustering and routing analysis (\cref{fig:classical_heatmap}) shows that MixER logically partitions datasets into three subsets, but this partitioning limits each expert meta-learner's exposure to the full dataset, potentially explaining the performance degradation despite clear cross-environment commonalities.

\subsection{Feature Interpretability and Downstream Clustering}
\label{interpretability}


A major benefit of contextual meta-learning is in its by-product context features, which can be used for downstream tasks. To test the interpretability of these features, we consider two time series classification datasets. First, the Synthetic Control Chart Time Series (SCCTS) \cite{alcock1999synthetic} is a collection of 600 time series\footnote{Each time series constitutes its own environment, i.e., $I=1$.} across six classes: A. Normal, B. Cyclic, C. Increasing trend, D. Decreasing trend, E. Upward shift, and F. Downward shift. The traditional $K$-means typically struggles to separate these classes due to the similarities among the pairs A/B, C/D, and E/F. We expect the grouping (600 environments $\rightarrow$ 6 classes $\rightarrow$ 3 families) to be suitable for hierarchical models. As such, we train a MixER with 3 expert meta-learners in a completely \emph{unsupervised} manner.

Second, the Epilepsy2 dataset \cite{andrzejak2001indications} is a large collection of real-world neuroscientific EEG data with noisy labels indicating whether a subject is healthy (0) or experiencing a seizure (1). The 80 unshuffled training samples are labeled as follows [0-30): 1, [30-60): 0, [60-70): 1, and [70-80): 0. For this dataset, our families are the two underlying classes. We emphasize that SoTA methods do not face difficulties classifying this data, whereas naive $K$-means consistently struggles. For both SCCTS and Epilepsy2 datasets, the backbone meta-learner we use is the hier-shPLRNN \cite{brenner2024learning} based on the generalized teacher forcing approach \cite{hess2023generalized}. We fix its mixing coefficient $\alpha=0.5$, the hidden layer's width to $16$, and we use a linear hypernetwork to generate weights based on context vectors of size $d_{\xi}=10$.



\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/trends_clusters.pdf}
\captionof{figure}{PCA clusters formed when training a MixER on the SCCTS dataset. (Top) Coloring using the ground truth labels; (Bottom) Coloring using labels from a $K$-means algorithm, with its means initialized at the ground truth means.}
\label{fig:trends_clusters}
\end{center}
\end{figure}

SCCTS results (\cref{fig:trends_clusters}) demonstrate improved class separation with three experts, effectively grouping similar classes (A/B, C/D, E/F) and routing them to the same expert. This configuration unambiguously outperforms both single-expert and vanilla $K$-means approaches in qualitative and quantitative metrics.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.95\columnwidth]{figures/epilepsy_clusters.pdf}
\captionof{figure}{PCA clusters on the Epilepsy2 datasets, using the hier-shPLRNN meta-learning backbone. Accuracy scores are obtained on the testing contexts upon training a logistic regression classifier.}
\label{fig:epilepsy_clusters}
\end{center}
\end{figure}

Conversely, Epilepsy2 results (\cref{fig:epilepsy_clusters}) show degraded performance with MixER. While context routing roughly aligns with class boundaries, the clusters lack clear separation, with epileptic subjects split between experts while healthy subjects route exclusively to the second expert. This routing pattern persists in test data, challenging downstream classification via logistic regression \cite{cox1958regression}. The classification performance degradation likely stems from the dataset's inherent noise, as noted by \citet{brenner2024learning}. Indeed, such close proximity of time series prevents clean discrimination and routing during training. These results highlight MixER's limitations with ambiguous, highly related environments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{relatedwork}

We review the emerging field of dynamical system reconstruction (DSR) and its intersection with meta-learning for multi-environment generalization. We cover learning generalizable DSRs and their extension to foundational models.

\paragraph{Multi-Environment Learning} The challenge of multi-environment learning has received substantial attention in the machine learning community. Contemporary multi-domain training approaches extend the traditional Empirical Risk Minimization (ERM) framework through Invariant Risk Minimization (IRM) \cite{arjovsky2019invariant} and Distributionally Robust Optimization (DRO) \cite{ben2013robust,sagawa2020distributionally,krueger2021out}, which optimize models to minimize worst-case performance across potential test distributions. For optimal reconstruction of ODEs, PDEs, and differential equation-driven time series, several models incorporate physical parameters as model inputs \cite{brandstetter2022message,takamoto2023learning}. This approach assumes that exposure to training physical parameters enables models to learn the underlying parameter distribution and its relationship to system dynamics. However, these physical parameters are often sparse or unobservable, necessitating the learning of data-driven proxies through multitask learning (MTL) \cite{caruana1997multitask} and meta-learning \cite{hospedales2021meta} approaches for DSRs. While MTL methods typically adapt components of a generalist model across training environments \cite{yin2021leads}, they often lack the rapid adaptation capabilities of their meta-learning counterparts when confronted with out-of-distribution scenarios.

\paragraph{Generalization to New Environments} Meta-learning, embodied by \emph{adaptive conditioning} \cite{serrano2024zebra} in the DSR community, represents the primary framework for generalization. Rather than conducting complete model fine-tuning for each new environment \cite{subramanian2024towards,herde2024poseidon}, this approach implements training with rapid adaptation in mind. \emph{Contextual} meta-learning partitions learnable parameters into environment-agnostic and environment-specific components. These contexts serve diverse purposes: (1) Encoder-based methods \cite{garnelo2018conditional,wang2022meta} employ dedicated networks for context prediction, though they tend to overfit on training environments \cite{kirchmeyer2022generalizing}. (2) Hypernetwork-based approaches \cite{kirchmeyer2022generalizing,brenner2024learning,blanke2024interpretable} learn transformations from context to model parameters. GEPS \cite{koupai2024boosting}, through its LoRA-inspired adaptation rule \cite{hu2021lora}, enhances these methods for large-scale applications. (3) Concatenation-based conditioning strategies \cite{zintgraf2019fast,nzoyem2025neural} incorporate context as direct input to the model. While these frameworks demonstrate considerable efficacy, none directly addresses learning across families of arbitrarily related environments.

\paragraph{Learning in Families of Environments} Clustering before training, followed by task-specific meta-learning \cite{nzoyem2025neural,kirchmeyer2022generalizing,koupai2024boosting,brenner2024learning} would constrain the adaptability of our models. The challenge of simultaneous learning across arbitrarily related families remains largely unexplored, particularly in the context of Mixture of Experts (MoE) \cite{jacobs1991adaptive}. MoE is a powerful paradigm, as \citet{chen2022towards} demonstrate that certain tasks fundamentally require expert mixtures rather than single experts. Most relevant to our context is the variational inference approach of \cite{roeder2019efficient,davidian2003nonlinear} which infers Neural ODE \cite{chen2018neural} parameters across well-defined hierarchies. The language modeling community provides compelling demonstrations of MoE efficacy \cite{shazeer2017outrageously}. Sparse MoEs enable expert MLPs to encode domain-specific knowledge \cite{dai2024deepseekmoe,jiang2024mixtral,guo2025deepseek}, while some MoE variants address catastrophic forgetting \cite{he2024mixture}. Drawing inspiration from ``switch routing'' \cite{fedus2022switch}, our work dedicates single experts to individual families during training.

\paragraph{Foundational Scientific Models} Current foundational scientific models remain domain-specific, as exemplified in climate modeling \cite{nguyen2023climax,bodnar2024aurora} where abundant data sources maintain relative homogeneity. \citet{kochkov2024neural} achieves generalization through the hybridization of principled atmospheric models with neural networks. While PINNs \cite{cuomo2022scientific} underpin numerous powerful systems, they demand substantial data and domain expertise \cite{nzoyem2023comparison}. Our approach diverges by discovering physics from data without prior physical knowledge while maintaining adaptability. Although domain-agnostic models are emerging \cite{subramanian2024towards,herde2024poseidon}, they typically require resource-intensive pre-training and fine-tuning. To the best of our knowledge, our work represents the first DSR approach targeting such broad generalization through rapid adaptation of only a fraction of the training parameters.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion and Future Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations \& Conclusion}
\label{conclusion}

\paragraph{Limitations} 
Our experiments demonstrate that our framework successfully learns families of environments that share either minimal or extensive structure. However, several limitations warrant consideration: (1) MixER's interpretability performance on closely related datasets is inferior to single meta-learners, particularly in scenarios with abundant data availability; (2) the computational demands typically exceed those of individually trained meta-learners, as all experts must remain simultaneously loaded in memory.

\paragraph{Future Work} 
Our work establishes foundations for promising research directions beyond the aforementioned limitations. The cluster-expert associations which were observed to \emph{dynamically} shift during training suggest interesting potential for continual learning. Also, exploring the combination of meta-learners with \emph{different} nature or architecture could significantly broaden the usable datasets.

\paragraph{Conclusion} 
We integrated traditional ML techniques within deep learning to address the open problem of reconstructing families of dynamical systems with arbitrary relatedness. Through our analysis, we identified the inherent limitations of task-specific meta-learning and proposed as a solution MixER---a Mixture of Experts approach featuring a specialized routing mechanism. Our results demonstrated that while MixER excels when processing highly heterogeneous data with limited amounts of training examples, it conversely underperforms classical meta-learning baselines on datasets exhibiting high degrees of relatedness, with individual experts being exposed to only a fraction of the dataset. Nevertheless, by successfully extending meta-learning from multi-environment DSRs to hierarchies thereof, our findings establish a promising pathway toward domain-agnostic foundational models for scientific applications.




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% UNUMBERED SECTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}



\section*{Impact Statement}

This work advances scientific modeling by enabling AI systems to learn from diverse datasets simultaneously. The high computational requirements and complexity of the system could exacerbate research inequity between well-resourced and under-resourced institutions. To address this concern, we open-source our implementation at \url{https://anonymous.4open.science/r/MixER}, with pre-trained weights optimized for resource-constrained environments to follow.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{nzoyem2023comparison}

\bibliography{main}
\bibliographystyle{icml2025}
\balance

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\onecolumn

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX A
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Theoretical Analysis}

% Can we prove that the least squares problem ALWAYS admits a solution !?

% % Can we prove that the training/sampling strategy targets the biggest subset of environments that can be learned together ? 

% % If we only have one loss contributor, then we only optimise that one till the end. If we have more than one, we might just attend to all environments.


% Can we prove that the gradeitns of the gate are too small compared to the gradients of the experts, and that the optimizer cannot handle this !?

% We have $E$ experts and $P$ problems (ODE families). Assume that any experts can learn any of the ODEs up to epsilon training loss. If we have $E < P$, what can we say about the overal loss ?



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % EXT. RELATED WORK
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Extended Related Work}
% \label{app:extendedrelwork}

% We extend our review from \cref{relatedwork} to the growing SciML subfield of learning to reconstruct dynamical systems and generalization to new environments via meta-learning.

% \paragraph{Multi-Environment Learning.} The problem of multi-environment has extensively been studied in machine learning through the lens of OoD. It is common for multi-domain training approaches to extend the traditional ERM paradigm with Invariant Risk Minimization (IRM) \cite{arjovsky2019invariant} and Distributionally Robust Optimization (DRO) \cite{ben2013robust,sagawa2020distributionally,krueger2021out}, in which one model is trained to minimize worst-case behaviors over potential test distributions. To achieve maximal performance for reconstructing ODEs, PDEs, and time series driven by differential equations, some of these models embed the physical parameters as input to the surrogate model \cite{brandstetter2022message,takamoto2023learning}. Their hope is that the training physical parameters are enough for the model to learn the overall distribution of parameters, and how it related to the dynamics. These physical parameters, however, are commonly scarce if not unobserved, meaning proxies must be learned from the data, which has opened the field of MultiTask and Meta-Learning for DSRs. MTL methods aim to `blindly' adapt some pieces of existing models (heads, operands, etc.) to all environments in the training set \cite{yin2021leads}. Contrary to their Meta-Learning counterparts, they lack the ability to quickly adapt to OoD settings.

% \paragraph{Generalization to New Environments.}  The dominant paradigm for generalisation to new environments involves Meta-Learning, embodied by the umbrella term \emph{adaptive condition} in the DSR community. Here, rather than fully fine-tuning the model each time a new environment is encountered \cite{subramanian2024towards,herde2024poseidon}, surgical measures are taken to ensure the model can quickly adapt. Contextual Meta-Learning splits the learnable parameters into environment-agnostic (or group-level) and environment-specific (or subject-specific) environments. The nature and purpose of these contexts can be varied. (1) Encoder-based methods \cite{garnelo2018conditional,wang2022meta} train a specific network to predict the context for each environment, typically based on (partially) injected trajectories in that environment. Like MTL, they tend to overfit on the training environments. (2) Hierachical models \cite{kirchmeyer2022generalizing,brenner2024learning,blanke2024interpretable,koupai2024boosting} learn to transform group-level model weights and biases to predict subject-level copies. With its LoRA adaptation, GEPS \cite{koupai2024boosting} improves on these approaches by scaling to large data sizes and number of environments. (2) Concatenation-based conditioning (naturally additive-based cond as well) \cite{zintgraf2019fast,nzoyem2025neural} is another successful approach that simplifies the architectures and result in typically faster trainings. While expressive, none of these frameworks has addressed learning across families of arbitrarily related environments.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX A'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
\section{Algorithms \& Definitions}
\label{app:algorithms}

\subsection{Llyod's $K$-Means}
\label{app:kmeans}


\begin{algorithm}[H]
    \caption{Lloyd's K-Means}
    \label{alg:kmeans}
\begin{algorithmic}[1]
    \State {\bfseries Require:} $\Xi := \{ \xi^e \}^{e\in [E\times F]} $ context vectors
    \State $\qquad \qquad  \bar \Xi := \{ \bar \xi_m \}_{m\in [M]}$ centroid initialization
    \If{$\bar \Xi = \text{Null}$}
        \State $\bar \Xi \leftarrow \text{RandomUniformSample}(M, d_{\xi})$
    \EndIf
    % \Else
    \Repeat
        \State \scalebox{0.95}{$\displaystyle C_m \leftarrow \{\xi \in \Xi : m = \argmin_{j} \|\xi - \bar\xi_{j}\|_1\}, \quad \forall m \in [M] $}
        \If{$|C_m| = 0$}
        \State {\textbf{Return} $\{C_m\}_{m\in[M]}, \text{Null}$}
        \Else
        \State $\displaystyle \bar\xi_m \leftarrow \frac{1}{|C_m|}\sum_{\xi \in C_m} \xi, \quad \forall m \in [M]$
        \EndIf
    \Until{$\bar \Xi$ converges}
    \State {\bfseries Return $\{C_m\}_{m\in[M]}, \bar \Xi$}
\end{algorithmic}
\end{algorithm}







\subsection{Metric Definitions}

We define the relative MSE or relative $L^2$ loss used to perform model selection in several experiments.

\begin{align} \label{eq:relmse}
    \text{Rel. MSE} \triangleq \frac{1}{E \times I \times T} \sum_{e=1}^E \sum_{i=1}^I \sum_{t=1}^{T}  \frac{\Vert x^{e,i}_t - \hat x^{e,i}_t\Vert_2^2}{\Vert x^{e,i}_t\Vert_2^2}.
\end{align}
To avoid numerical instability in the metric computation, we only consider states $x^{e,1}_t$ with $L^2$ norm grater than $10^{-6}$. Additionally, we consider the TPRMSE (Thresholded Percentage Relative MSE) defined as the proportion of environments in which the Rel. MSE is below a specified threshold $\varepsilon$:

\begin{align} \label{eq:tprelmse}
\text{TPRMSE} \triangleq \frac{100}{E} \sum_{e=1}^{E} \mathds{1}_{\{\text{RelMSE}_e < \varepsilon\}},
\end{align}

where:
\begin{itemize}
    \item $\mathds{1}_{\{\cdot\}}$ is the indicator function,
    \item $E$ is the total number of environments available,
    \item $\text{RelMSE}_i$ is the aggregate relative MSE across trajectories in the $e$-th environment.
\end{itemize}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX A'
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Ablation Studies}

% \subsection{Removing the gating network upwork update}

% We do this on the two env dataset.

% \subsection{Removing split the contexts}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX C
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Datasets Details}
\label{app:datasets}

We describe the datasets used in this paper. We begin with the synthetic ODEBench datasets used both to illustrate the limitations of classical meta-learning and those of our MixER in high-data regimes. We follow with the classical synthetic DSR datasets, and finish with real-world EEG data. 


\subsection{ODEBench}

ODEBench \cite{d'ascoli2024odeformer} features a selection of ordinary differential equations primarily from \cite{strogatz2018nonlinear}, to which other iconic systems have been added. In total, it boasts 63 definitions of ODE families spanning various regimes: chaotic, periodic, etc., and dimensionality: 1D, 2D, 3D, and 4D. We study 10 of these families, all two-dimensional. First, we describe the data generation process for generating ODEBench-10A (introduced in \cref{tab:ode-bench}), whose training and adaptation trajectories is obtained by adapting the default ODEBench initial conditions and parameters as described below.

The initial conditions for each ODE are generated by interpolating between two reference initial conditions. For each dimension of the ODE, the initial conditions are sampled uniformly between the minimum and maximum values of the two reference conditions. This ensures a diverse set of starting points for the trajectories while maintaining consistency with the ODE's physical or mathematical constraints. 

The parameters of the ODEs (e.g., \(c_0\), \(c_1\)) are selected based on predefined reference values. For training and testing, these parameters are varied within a range of 90\% to 110\% of their reference values. This variation is achieved by creating a grid of parameter values, ensuring a systematic exploration of the parameter space. For adaptation tasks, the parameters are scaled linearly between 80\% and 120\% of their reference values to simulate environments outside the training domain.

The ODEs are solved using the \texttt{solve\_ivp} function from the \texttt{scipy.integrate} module, which the Runge-Kutta method of order 4(5) (RK45). This method is a widely used numerical integrator for solving initial value problems due to its balance between accuracy and computational efficiency. The evaluation time step for reporting \(\Delta t\) is determined by dividing the time horizon \(T\) by the number of steps (fixed to 100 across all families), ensuring a consistent resolution across all simulations. We focus on ODEs that display a \emph{periodic} behavior, and the time horizon is chosen so as to observe at least one full oscillation.

The dataset is divided into four distinct splits: train, test, adaptation train, and adaptation test. The number of environments and initial conditions for each split is summarized in the table below.

\begin{table}[h!]
\centering
\caption{Data splits and their characteristics for ODEBench-10A. Similar attributes apply to ODEBench-10B as per \cref{tab:ode-bench}.}
\vspace*{0.2cm}
\begin{tabular}{lcccc}
\toprule
\textbf{Split} & \textbf{Environments} & \textbf{Initial Conditions} & \textbf{Description} \\
\midrule
Train & 5 & 4 & Used for training models. \\
Test & 5 & 32 & Used for evaluating model performance. \\
Adaptation Train & 1 & 1 & Used for fine-tuning context vectors. \\
Adaptation Test & 1 & 32 & Used for evaluating fine-tuned contexts. \\
\bottomrule
\end{tabular}
\end{table}

The following table provides a detailed description of the ODE families used in the dataset. Each ODE is identified by an ID, and its analytical definition, time horizon, initial conditions, and parameters are listed.

\begin{longtable}{lccccc}
\caption*{\textit{Table 6.} ODE identifiers and definitions from \cite{d'ascoli2024odeformer}, along with custom parameters, initial conditions, and time horizon values. The custom values are used to generate ODEBench-10A and ODEBench-10B.} \\
\toprule
\textbf{ID} & \textbf{Family Name} & \textbf{Equation} & \textbf{Parameters} & \textbf{Initial Values} & \textbf{Time Horizon} \\
\midrule
24 & \makecell{Harmonic oscillator \\ without damping} & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = -c_0 x_0
\end{cases}\) & \(c_0 = 2.1\) & \makecell{\([0.4, -0.03]\)\\ \([0.0, 0.2]\)} & 10 \\
\midrule
25 & \makecell{Harmonic oscillator\\ with damping} & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = -c_0 x_0 - c_1 x_1
\end{cases}\) & \makecell{\(c_0 = 4.5\) \\ \(c_1 = 0.43\)} & \makecell{\([0.12, 0.043]\) \\ \([0.0, -0.3]\)} & 8 \\
\midrule
28 & Pendulum without friction & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = -c_0 \sin(x_0)
\end{cases}\) & \(c_0 = 0.9\) & \makecell{\([-1.9, 0.0]\) \\ \([0.3, 0.8]\)} & 15 \\
\midrule
32 & \makecell{Damped double \\ well oscillator} & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = -c_0 x_1 - x_0^3 + x_0
\end{cases}\) & \(c_0 = 0.18\) & \makecell{\([-1.8, -1.8]\) \\ \([-2.8, 1.0]\)} & 5 \\
\midrule
34 & \makecell{Frictionless bead \\ on a rotating hoop} & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = (-c_0 + \cos(x_0)) \sin(x_0)
\end{cases}\) & \(c_0 = 0.93\) & \makecell{\([2.1, 0.0]\) \\ \([-1.2, -0.2]\)} & 20 \\
\midrule
35 & \makecell{Rotational dynamics of \\ an object in a shear flow} & 
\(\begin{cases}
\dot{x}_0 = \frac{\cos(x_0)}{\tan(x_1)} \\
\dot{x}_1 = \sin(x_0) (c_0 \sin^2(x_1) + \cos^2(x_1))
\end{cases}\) & \(c_0 = 4.2\) & \makecell{\([1.13, -0.3]\) \\ \([0.7, -1.7]\)} & 5 \\
\midrule
37 & \makecell{Van der Pol oscillator \\ (standard form)} & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = -c_0 x_1 (x_0^2 - 1) - x_0
\end{cases}\) & \(c_0 = 0.43\) & \makecell{\([2.2, 0.0]\) \\ \([0.1, 3.2]\)} & 15 \\
\midrule
38 & \makecell{Van der Pol oscillator \\ (simplified form)} & 
\(\begin{cases}
\dot{x}_0 = c_0 \left(-\frac{x_0^3}{3} + x_0 + x_1\right) \\
\dot{x}_1 = -\frac{x_0}{c_0}
\end{cases}\) & \(c_0 = 3.37\) & \makecell{\([0.7, 0.0]\) \\ \([-1.1, -0.7]\)} & 15 \\
\midrule
39 & Glycolytic oscillator & 
\(\begin{cases}
\dot{x}_0 = c_0 x_1 + x_0^2 x_1 - x_0 \\
\dot{x}_1 = -c_0 x_0 + c_1 - x_0^2 x_1
\end{cases}\) & \makecell{\(c_0 = 2.4\) \\ \(c_1 = 0.07\)} & \makecell{\([0.4, 0.31]\) \\ \([0.2, -0.7]\)} & 4 \\
\midrule
40 & Duffing equation & 
\(\begin{cases}
\dot{x}_0 = x_1 \\
\dot{x}_1 = c_0 x_1 (1 - x_0^2) - x_0
\end{cases}\) & \(c_0 = 0.886\) & \makecell{\([0.63, -0.03]\) \\ \([0.2, 0.2]\)} & 10 \\
\bottomrule
\label{tab:ode_becnhdefinitions}
\end{longtable}


The ODEBench-2 dataset is a subset of the original ODEBench-10 datasets, focusing on two specific systems: the Harmonic Oscillator with Damping (ID 25) and the Rotational Dynamics of an Object in a Shear Flow (ID 35) from \cref{tab:ode_becnhdefinitions}. A few changes where made to emphasize the differences in dynamics behaviour between the two families. Those changes are summarized in \cref{tab:ode_becnhdefinitions_2}.

\begin{table}[H]
    \caption{Parameter, initial condition, and time horizon values for ODEBench-2.}
    \vspace*{0.2cm}
    \centering
    \begin{tabular}{cccc}
    \toprule
    \textbf{ID} & \textbf{Parameters} & \textbf{Initial Values} & \textbf{Time Horizon} \\
    \midrule
      25   & $c_0 = 0.4$ & \makecell{\([0.1, 0.1]\) \\ \([0.0, -0.3]\)} & 5 \\
      \midrule
       35  & $c_0 = 6.0$ & \makecell{\([1.13, -0.3]\) \\ \([0.7, -1.7]\)} & 5 \\
        \bottomrule
    \end{tabular}
    \label{tab:ode_becnhdefinitions_2}
\end{table}


\subsection{LV, GO, and SM}
The Lotka-Volterra (LV), Glycolytic Oscillator (GO), and Sel'kov Model (SM) have been the subject of extensive studies these past years. A complete description of each dataset along with the generation processes is provided in \cite{yin2021leads,kirchmeyer2022generalizing,nzoyem2025neural}. For our use case, we download the data from the \texttt{Gen-Dynamics} repository \cite{nzoyem2025neural}.
% at \href{github.com/ddrous/gen-dynamics}{\texttt{github.com/ddrous/gen-dynamics}}.



\subsection{Synthetic Control}

The Synthetic Control Chart Time Series (SCCTS) dataset is a collection of synthetically generated control charts, designed for time series clustering and classification tasks. The dataset contains 600 time series instances, each comprising 60 time steps, and is divided into six distinct classes: Normal, Cyclic, Increasing Trend, Decreasing Trend, Upward Shift, and Downward Shift. The dataset has been used in prior research to explore time series similarity queries and control chart pattern recognition. Key references include works by \cite{alcock1999synthetic} on feature-based time series similarity, and \cite{pham1998control} on neural network-based control chart recognition.

The primary task associated with this dataset is clustering, with a focus on evaluating the performance of time series clustering algorithms. The dataset is particularly useful for testing algorithms that go beyond the Euclidean distance, as certain class pairs are often misclassified using traditional distance measures. For instance, Derivative Dynamic Time Warping (DDTW) \cite{Keogh2001DerivativeDT} has been shown to achieve better clustering results compared to Euclidean distance. The raw dataset was downloaded from \url{https://www.timeseriesclassification.com/description.php?Dataset=SyntheticControl}.



\subsection{Epilepsy2} 

The Epilepsy2 dataset comprises single-channel electroencephalogram (EEG) measurements collected from 500 subjects \cite{andrzejak2001indications,zhang2022self}. For each subject, brain activity is recorded over a duration of 23.6 seconds, then partitioned and shuffled, resulting in 11,500 examples (80 for training, and 11,420 for testing), each spanning 1 second and sampled at 178 Hz.

The raw dataset downloaded from \url{https://www.timeseriesclassification.com/description.php?Dataset=Epilepsy2} includes five classification labels corresponding to different subject states or measurement locations: eyes open, eyes closed, EEG from a healthy brain region, EEG from a tumor-affected region, and seizure episodes. For binary classification as performed in \cref{interpretability}, the first four classes were merged into a single "no seizure" class, while the seizure episodes were retained as the "seizure" class. The training set is balanced, containing 40 seizure and 40 non-seizure samples, whereas the test set is imbalanced, with 19.79\% seizure and 80.21\% non-seizure samples.



% \subsection{Additional Results} 

% Task-specific DSR models are generally built to adapt to small parameter variations in the underlying dynamics systems that we learn. While generalisation to wider behaviors has been studied in \cite{nzoyem2025neural,koupai2024boosting}, they assume the same underlying ODE data-genreation process subject to bifurcations, and enough data to learn each regime. 

% We observe poor performance with 20 experts because our clustering algorithm wasn't able to fill all 20 clusters during training. As a result, the gate initialised with Xavier uniform persited throughout training. Visualisations of predicted trajectories are presented in the APPENDIX.

% For convenience of visualisation, we present the environments in a consecutive manner. As observed in \cref{fig:gating_10_fams}, GEPS demonstrates a cleaner separation of families. Some environments are never resolved, as a result, adaptation in those is weak, which skews the results. A LESS STRICT METRIC IS NEEDED FOR THIS TASK: HOW MANY FAMILIES/ENVIRONMENTS ARE RESOLVED WITHIN A CERTAIN THRESHOLD? (Make two more tables with this new metric).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX B
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Implementation Details}
\label{app:hyperparams}

We describe the implementation of our MixER framework through the lens of its routing and its hyperparameters. We also present the baselines and the changes we made to fit them within our framework.


\subsection{Context-Based Routing}
\label{app:routing}

In our framework, the only way the gating network influences the output is via the logits it produces for routing (see \cref{fig:moe_vs_mixer}). We effectively eliminate the final aggregations so that the expert can be used on its own outside the MoE layer. This has adverse consequences however, in that the gating doesn't impact the output enough to receive high gradients. While this is generally solved with our clustering mechanism, we find that two mechanisms improve the clustering when the relatedness of families is minimal:
\begin{enumerate}
    \item \textbf{Context Splitting.} The router splits the contexts $\xi$ into $m$ equal-length pieces $\{ \xi_m \}_{m\in[M]}$ before feeding them to the experts. This means each experts only ever sees a specific portion of the contexts. We apply this only on the IVPs tested in this paper.
    \item \textbf{Context Shifting.} Each expert is augmented with a single floating point offset, by which the inputted contexts are shifted before usage. Again, with shifts the overall mean of the contexts received by the experts, further facilitating clustering. We apply this to all experiments conducted in this paper.
\end{enumerate}



\subsection{Core Baseline Methods}
\label{app:baselines}

With the exception of CAVIA, we perform a custom implementation of several baselines and incorporate them within our MixER layer.

\paragraph{\textbullet{} CAVIA \cite{zintgraf2019fast}} is a concatenation-based meta-learning approach that improves on the seminal \cite{finn2017model} by optimizing parameter-specific context vectors in its inner loop. Within the model $G_{\theta}$, pre-processing of $\xi^e$, $z_{t-1}$, and $x^e_{t-1}$ may be performed before concatenation and processing within a main network.

\paragraph{\textbullet{} Neural Context Flows \cite{nzoyem2025neural} } use a first-order optimization procedure coupled with contextual self-modulation to share information between environments, thus encouraging the formation of clusters and improving generalization. We use 2nd order Taylor expansion resulting in NCF-$t_2$. Its model $G_{\theta}$ processes inputs like in CAVIA.

\paragraph{\textbullet{} CoDA \cite{kirchmeyer2022generalizing}} is aimed at initial value problems and leverages a linear hypernetwork to generate environment-scpefic weights of the root (main) network based on context vectors.

\paragraph{\textbullet{} GEPS \cite{koupai2024boosting}} improves on CoDA's scalability by performing low-rank adaptation on MLP and CNN weights, conditioned on context vectors. In our implementation, we use Xavier initialization \cite{glorot2010understanding} for the $A$ and $B$ matrices, and we initialize the contexts at 0.

\paragraph{\textbullet{} hier-shPLRNN \cite{brenner2024learning}} is a fast sequence-to-sequence shallow Recurrent Neural Network meta-learner. Similar to CoDA, subject-specific weights are generated with a linear hypernetwork. We set the width of its single hidden layer to 16. Our setting does not require any encoders to map $x$ to $z$, which live in the same space. We set the initial $z_0=0$.




\subsection{Main Hyperparameters}


\paragraph{Training} All the experts in the MixER are initialized with the same seed. Across our experiments, the batch size is the expected number of environments per expert, i.e. $E/M$. We use the AdaBelief optimizer \cite{zhuang2020adabelief} for both contexts and weights. Adaptation to new environments is performed on a sequential one by one basis, except on the Epilpesy2 dataset which considers batches of size 571.

\paragraph{Gating Update} In our proximal alternating minimization, we performed up to 500 outer iterations, and 12 inner iterations of both weights $\Theta$ and contexts $\Xi$, with the gate updated every time either are updated. We upper bound the number of iterations in $K$-means to 20 (see \cref{alg:kmeans}), and we set the convergence tolerance to $10^{-3}$ and the noise standard deviation to $10^{-4}$ (see \cref{alg:gateupdate}).

\paragraph{Architectures} On problems using ODEBench, we use a 3-layer MLP of width 64 as the main network. For NCF, we use shallow context and data networks or depth 1, each with outputs of size 32. We use the Swish activation \cite{ramachandran2017searching} throughout, except with the hier-shPLRNN where we use ReLU activations. On other IVP problems, we adjust the width of the main layer so that the active parameter count (equal to the number of parameters in \emph{one} expert \cite{jiang2024mixtral}) matches the baselines. 

\paragraph{Software} We use JAX \cite{jax2018github} and its differentiable programming ecosystem \cite{nzoyem2023comparison}. Specifically, we use \texttt{diffrax} and its \texttt{Tsit5} solver to integrate differential equations \cite{kidger2022neural}, with all neural networks implemented with \texttt{Equinox} \cite{kidger2021equinox}.

\paragraph{Hardware} Depending on the experiment, our model was trained on a workstation fitted with a NVIDIA 4080 GPU with 16GB VRAM memory, and a supercomputer containing four NVIDIA GH200 GPUs with 480GB total memory. We aimed for quick training times, with hier-shPLRNN being by far the faster to train in less than 5 minutes on both Epilepsy and SCCTS datasets. It took CoDA around 20 minutes, GEPS 30 minutes, and finally NCF 25 minutes to complete 500 outer steps on the largest ODEBench-10B dataset.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX D
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Qualitative Results}


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/gradient_descent_vs_least_squares.pdf}
\captionof{figure}{Visualisation of the clustering heatmap as the training progresses on ODEBench-2. The four columns correspond to outer training steps 0, 25, 125, and 250 respectively (from left to right). (Top) Naive mixture of two GEPS models with gating updates via vanilla gradient descent. (Bottom) MixER and least-squares-based gating update. }
\label{fig:mixer_heatmap}
\end{center}
\end{figure}






% For the 10 Families, we present all 160 envs. But we plot them in groups of 10 columns; for NCF, CoDA, and GEPS.


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.98\columnwidth]{figures/classical_heatmap.pdf}
\captionof{figure}{Heatmaps of the gating values of MixER with 3 experts on three classical meta-learning datasets: LV, GO, and SM.}
\label{fig:classical_heatmap}
\end{center}
\end{figure}




\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/visuals_10_odes_1.png}
\captionof{figure}{Visualization of a single testing trajectory and the phase space within the \emph{first} 5 families with 10 expert GEPS meta-learners on the large ODEBench-10B dataset.}
\label{fig:geps_visuals_1}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\columnwidth]{figures/visuals_10_odes_2.png}
\captionof{figure}{Visualization of a single testing trajectory and the phase space within the \emph{last} 5 families with 10 expert GEPS meta-learners on the large ODEBench-10B dataset.}
\label{fig:geps_visuals_2}
\end{center}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
