\section{Related Work}
\label{relatedwork}

We review the emerging field of dynamical system reconstruction (DSR) and its intersection with meta-learning for multi-environment generalization. We cover learning generalizable DSRs and their extension to foundational models.

\paragraph{Multi-Environment Learning} The challenge of multi-environment learning has received substantial attention in the machine learning community. Contemporary multi-domain training approaches extend the traditional Empirical Risk Minimization (ERM) framework through Invariant Risk Minimization (IRM) \cite{arjovsky2019invariant} and Distributionally Robust Optimization (DRO) \cite{ben2013robust,sagawa2020distributionally,krueger2021out}, which optimize models to minimize worst-case performance across potential test distributions. For optimal reconstruction of ODEs, PDEs, and differential equation-driven time series, several models incorporate physical parameters as model inputs \cite{brandstetter2022message,takamoto2023learning}. This approach assumes that exposure to training physical parameters enables models to learn the underlying parameter distribution and its relationship to system dynamics. However, these physical parameters are often sparse or unobservable, necessitating the learning of data-driven proxies through multitask learning (MTL) \cite{caruana1997multitask} and meta-learning \cite{hospedales2021meta} approaches for DSRs. While MTL methods typically adapt components of a generalist model across training environments \cite{yin2021leads}, they often lack the rapid adaptation capabilities of their meta-learning counterparts when confronted with out-of-distribution scenarios.

\paragraph{Generalization to New Environments} Meta-learning, embodied by \emph{adaptive conditioning} \cite{serrano2024zebra} in the DSR community, represents the primary framework for generalization. Rather than conducting complete model fine-tuning for each new environment \cite{subramanian2024towards,herde2024poseidon}, this approach implements training with rapid adaptation in mind. \emph{Contextual} meta-learning partitions learnable parameters into environment-agnostic and environment-specific components. These contexts serve diverse purposes: (1) Encoder-based methods \cite{garnelo2018conditional,wang2022meta} employ dedicated networks for context prediction, though they tend to overfit on training environments \cite{kirchmeyer2022generalizing}. (2) Hypernetwork-based approaches \cite{kirchmeyer2022generalizing,brenner2024learning,blanke2024interpretable} learn transformations from context to model parameters. GEPS \cite{koupai2024boosting}, through its LoRA-inspired adaptation rule \cite{hu2021lora}, enhances these methods for large-scale applications. (3) Concatenation-based conditioning strategies \cite{zintgraf2019fast,nzoyem2025neural} incorporate context as direct input to the model. While these frameworks demonstrate considerable efficacy, none directly addresses learning across families of arbitrarily related environments.

\paragraph{Learning in Families of Environments} Clustering before training, followed by task-specific meta-learning \cite{nzoyem2025neural,kirchmeyer2022generalizing,koupai2024boosting,brenner2024learning} would constrain the adaptability of our models. The challenge of simultaneous learning across arbitrarily related families remains largely unexplored, particularly in the context of Mixture of Experts (MoE) \cite{jacobs1991adaptive}. MoE is a powerful paradigm, as \citet{chen2022towards} demonstrate that certain tasks fundamentally require expert mixtures rather than single experts. Most relevant to our context is the variational inference approach of \cite{roeder2019efficient,davidian2003nonlinear} which infers Neural ODE \cite{chen2018neural} parameters across well-defined hierarchies. The language modeling community provides compelling demonstrations of MoE efficacy \cite{shazeer2017outrageously}. Sparse MoEs enable expert MLPs to encode domain-specific knowledge \cite{dai2024deepseekmoe,jiang2024mixtral,guo2025deepseek}, while some MoE variants address catastrophic forgetting \cite{he2024mixture}. Drawing inspiration from ``switch routing'' \cite{fedus2022switch}, our work dedicates single experts to individual families during training.

\paragraph{Foundational Scientific Models} Current foundational scientific models remain domain-specific, as exemplified in climate modeling \cite{nguyen2023climax,bodnar2024aurora} where abundant data sources maintain relative homogeneity. \citet{kochkov2024neural} achieves generalization through the hybridization of principled atmospheric models with neural networks. While PINNs \cite{cuomo2022scientific} underpin numerous powerful systems, they demand substantial data and domain expertise \cite{nzoyem2023comparison}. Our approach diverges by discovering physics from data without prior physical knowledge while maintaining adaptability. Although domain-agnostic models are emerging \cite{subramanian2024towards,herde2024poseidon}, they typically require resource-intensive pre-training and fine-tuning. To the best of our knowledge, our work represents the first DSR approach targeting such broad generalization through rapid adaptation of only a fraction of the training parameters.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion and Future Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%