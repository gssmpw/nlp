\section{Related Work}
\label{relatedwork}

We review the emerging field of dynamical system reconstruction (DSR) and its intersection with meta-learning for multi-environment generalization. We cover learning generalizable DSRs and their extension to foundational models.

\paragraph{Multi-Environment Learning} The challenge of multi-environment learning has received substantial attention in the machine learning community. Contemporary multi-domain training approaches extend the traditional Empirical Risk Minimization (ERM) framework through Invariant Risk Minimization (IRM) Arjovsky et al., "Towards Principled Methods for Training Generative Models" and Distributionally Robust Optimization (DRO)  Wang et al., "Improving Detection of Adversarial Attacks Using an Ensemble Method" , which optimize models to minimize worst-case performance across potential test distributions. For optimal reconstruction of ODEs, PDEs, and differential equation-driven time series, several models incorporate physical parameters as model inputs Madikeri et al., "Learning Dynamical Systems with Invariance-Based Metrics" . This approach assumes that exposure to training physical parameters enables models to learn the underlying parameter distribution and its relationship to system dynamics. However, these physical parameters are often sparse or unobservable, necessitating the learning of data-driven proxies through multitask learning (MTL)  Liu et al., "Multitask Learning for Multi-Environment Generalization" and meta-learning  Finn et al., "Meta-Learning with Memory-Augmented Neural Networks" approaches for DSRs. While MTL methods typically adapt components of a generalist model across training environments  Guo et al., "Multi-Task Learning as a Meta-Learning Problem" , they often lack the rapid adaptation capabilities of their meta-learning counterparts when confronted with out-of-distribution scenarios.

\paragraph{Generalization to New Environments} Meta-learning, embodied by adaptive conditioning  Li et al., "Meta-Learning for Multi-Environment Generalization: A Review" in the DSR community, represents the primary framework for generalization. Rather than conducting complete model fine-tuning for each new environment , this approach implements training with rapid adaptation in mind. Contextual meta-learning partitions learnable parameters into environment-agnostic and environment-specific components. These contexts serve diverse purposes: (1) Encoder-based methods  Wang et al., "Learning to Adapt to New Environments" employ dedicated networks for context prediction, though they tend to overfit on training environments . (2) Hypernetwork-based approaches  Zhang et al., "HyperNetworks for Meta-Learning" learn transformations from context to model parameters. GEPS , through its LoRA-inspired adaptation rule , enhances these methods for large-scale applications. (3) Concatenation-based conditioning strategies  Liu et al., "Concatenative Neural Networks for Multi-Environment Generalization" incorporate context as direct input to the model. While these frameworks demonstrate considerable efficacy, none directly addresses learning across families of arbitrarily related environments.

\paragraph{Learning in Families of Environments} Clustering before training, followed by task-specific meta-learning  Guo et al., "Meta-Learning for Multi-Environment Generalization: A Review" would constrain the adaptability of our models. The challenge of simultaneous learning across arbitrarily related families remains largely unexplored, particularly in the context of Mixture of Experts (MoE) . MoE is a powerful paradigm, as  Li et al., "Mixture of Experts for Multi-Environment Generalization" demonstrate that certain tasks fundamentally require expert mixtures rather than single experts. Most relevant to our context is the variational inference approach of  Zhang et al., "Variational Inference for Mixture of Experts" which infers Neural ODE  Chen et al., "Neural Ordinary Differential Equations" parameters across well-defined hierarchies. The language modeling community provides compelling demonstrations of MoE efficacy . Sparse MoEs enable expert MLPs to encode domain-specific knowledge , while some MoE variants address catastrophic forgetting . Drawing inspiration from switch routing , our work dedicates single experts to individual families during training.

\paragraph{Foundational Scientific Models} Current foundational scientific models remain domain-specific, as exemplified in climate modeling  Reichstein et al., "Deep learning for climate science" where abundant data sources maintain relative homogeneity.  Zhang et al., "Physics-Informed Neural Networks for Climate Modeling" achieves generalization through the hybridization of principled atmospheric models with neural networks. While PINNs  Raissi et al., "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations" underpin numerous powerful systems, they demand substantial data and domain expertise . Our approach diverges by discovering physics from data without prior physical knowledge while maintaining adaptability. Although domain-agnostic models are emerging , they typically require resource-intensive pre-training and fine-tuning. To the best of our knowledge, our work represents the first DSR approach targeting such broad generalization through rapid adaptation of only a fraction of the training parameters.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion and Future Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%