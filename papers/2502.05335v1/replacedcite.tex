\section{Related Work}
\label{relatedwork}

We review the emerging field of dynamical system reconstruction (DSR) and its intersection with meta-learning for multi-environment generalization. We cover learning generalizable DSRs and their extension to foundational models.

\paragraph{Multi-Environment Learning} The challenge of multi-environment learning has received substantial attention in the machine learning community. Contemporary multi-domain training approaches extend the traditional Empirical Risk Minimization (ERM) framework through Invariant Risk Minimization (IRM) ____ and Distributionally Robust Optimization (DRO) ____, which optimize models to minimize worst-case performance across potential test distributions. For optimal reconstruction of ODEs, PDEs, and differential equation-driven time series, several models incorporate physical parameters as model inputs ____. This approach assumes that exposure to training physical parameters enables models to learn the underlying parameter distribution and its relationship to system dynamics. However, these physical parameters are often sparse or unobservable, necessitating the learning of data-driven proxies through multitask learning (MTL) ____ and meta-learning ____ approaches for DSRs. While MTL methods typically adapt components of a generalist model across training environments ____, they often lack the rapid adaptation capabilities of their meta-learning counterparts when confronted with out-of-distribution scenarios.

\paragraph{Generalization to New Environments} Meta-learning, embodied by \emph{adaptive conditioning} ____ in the DSR community, represents the primary framework for generalization. Rather than conducting complete model fine-tuning for each new environment ____, this approach implements training with rapid adaptation in mind. \emph{Contextual} meta-learning partitions learnable parameters into environment-agnostic and environment-specific components. These contexts serve diverse purposes: (1) Encoder-based methods ____ employ dedicated networks for context prediction, though they tend to overfit on training environments ____. (2) Hypernetwork-based approaches ____ learn transformations from context to model parameters. GEPS ____, through its LoRA-inspired adaptation rule ____, enhances these methods for large-scale applications. (3) Concatenation-based conditioning strategies ____ incorporate context as direct input to the model. While these frameworks demonstrate considerable efficacy, none directly addresses learning across families of arbitrarily related environments.

\paragraph{Learning in Families of Environments} Clustering before training, followed by task-specific meta-learning ____ would constrain the adaptability of our models. The challenge of simultaneous learning across arbitrarily related families remains largely unexplored, particularly in the context of Mixture of Experts (MoE) ____. MoE is a powerful paradigm, as ____ demonstrate that certain tasks fundamentally require expert mixtures rather than single experts. Most relevant to our context is the variational inference approach of ____ which infers Neural ODE ____ parameters across well-defined hierarchies. The language modeling community provides compelling demonstrations of MoE efficacy ____. Sparse MoEs enable expert MLPs to encode domain-specific knowledge ____, while some MoE variants address catastrophic forgetting ____. Drawing inspiration from ``switch routing'' ____, our work dedicates single experts to individual families during training.

\paragraph{Foundational Scientific Models} Current foundational scientific models remain domain-specific, as exemplified in climate modeling ____ where abundant data sources maintain relative homogeneity. ____ achieves generalization through the hybridization of principled atmospheric models with neural networks. While PINNs ____ underpin numerous powerful systems, they demand substantial data and domain expertise ____. Our approach diverges by discovering physics from data without prior physical knowledge while maintaining adaptability. Although domain-agnostic models are emerging ____, they typically require resource-intensive pre-training and fine-tuning. To the best of our knowledge, our work represents the first DSR approach targeting such broad generalization through rapid adaptation of only a fraction of the training parameters.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion and Future Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%