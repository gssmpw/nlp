@article{poria2017review,
  title={A review of affective computing: From unimodal analysis to multimodal fusion},
  author={Poria, S. and Cambria, E. and Bajpai, R. and Hussain, A.},
  journal={Information Fusion},
  volume={37},
  pages={98--125},
  year={2017}
}

@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, R. and Chopra, S. and LeCun, Y.},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={1735--1742},
  year={2006},
  publisher={IEEE}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, T. and Kornblith, S. and Norouzi, M. and Hinton, G.},
  booktitle={International Conference on Machine Learning},
  pages={1597--1607},
  year={2020},
  publisher={PMLR}
}

@inproceedings{tran2018closer,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, D. and Wang, H. and Torresani, L. and Ray, J. and LeCun, Y. and Paluri, M.},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6450--6459},
  year={2018}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? A new model and the kinetics dataset},
  author={Carreira, J. and Zisserman, A.},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@inproceedings{zadeh2017tensor,
  title={Tensor fusion network for multimodal sentiment analysis},
  author={Zadeh, A. and Chen, M. and Poria, S. and Cambria, E. and Morency, L. P.},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={1103--1114},
  year={2017}
}

@inproceedings{liu2018efficient,
  title={Efficient low-rank multimodal fusion with modality-specific factors},
  author={Liu, Z. and Shen, Y. and Lakshminarasimhan, V. B. and Liang, P. P. and Zadeh, A. and Morency, L. P.},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  volume={1},
  pages={2247--2256},
  year={2018}
}

@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, C. and Bulut, M. and Lee, C. C. and Kazemzadeh, A. and Mower, E. and Kim, S. and Narayanan, S.},
  journal={Language Resources and Evaluation},
  volume={42},
  number={4},
  pages={335--359},
  year={2008}
}

@inproceedings{poria2019meld,
  title={MELD: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, S. and Hazarika, D. and Majumder, N. and Naik, G. and Cambria, E. and Mihalcea, R.},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={527--536},
  year={2019}
}

@article{abdullah2021multimodal,
  title={Multimodal emotion recognition using deep learning},
  author={Abdullah, S. M. S. A. and Ameen, S. Y. A. and Sadeeq, M. A. and Zeebaree, S.},
  journal={Journal of Applied Science and Technology Trends},
  volume={2},
  number={1},
  pages={73--79},
  year={2021}
}

@article{gupta2024visatronic,
  title={Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis},
  author={Gupta, A. and Likhomanenko, T. and Yang, K. D. and Bai, R. H. and Aldeneh, Z. and Jaitly, N.},
  journal={arXiv preprint arXiv:2411.17690},
  year={2024}
}

@article{mai2022hybrid,
  title={Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis},
  author={Mai, S. and Zeng, Y. and Zheng, S. and Hu, H.},
  journal={IEEE Transactions on Affective Computing},
  volume={14},
  number={3},
  pages={2276--2289},
  year={2022}
}

@article{kugate2024efficient,
  title={Efficient Key Frame Extraction from Videos Using Convolutional Neural Networks and Clustering Techniques},
  author={Kugate, A. H. and Balannanavar, B. Y. and Goudar, R. H. and Rathod, V. N. and Dhananjaya, G. M. and Kulkarni, A. and Kaliwal, R. B.},
  journal={EAI Endorsed Transactions on Context-aware Systems and Applications},
  volume={10},
  year={2024}
}

@inproceedings{fan2016video,
  title={Video-based emotion recognition using CNN-RNN and C3D hybrid networks},
  author={Fan, Y. and Lu, X. and Li, D. and Liu, Y.},
  booktitle={Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  pages={445--450},
  year={2016}
}

@article{pang2023caver,
  title={CAVER: Cross-modal view-mixed transformer for bi-modal salient object detection},
  author={Pang, Y. and Zhao, X. and Zhang, L. and Lu, H.},
  journal={IEEE Transactions on Image Processing},
  volume={32},
  pages={892--904},
  year={2023}
}

@article{hu2021mmgcn,
  title={MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation},
  author={Hu, J. and Liu, Y. and Zhao, J. and Jin, Q.},
  journal={arXiv preprint arXiv:2107.06779},
  year={2021}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, J. and Chang, M. W. and Lee, K. and Toutanova, K.},
  booktitle={Proceedings of NAACL-HLT 2019},
  pages={4171--4186},
  year={2019}
}

@inproceedings{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, A. and Zhou, Y. and Auli, M.},
  booktitle={Proceedings of NeurIPS 2020},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@inproceedings{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, S. and Zhan, X. and Cheng, W.},
  booktitle={Proceedings of NeurIPS 2018},
  volume={31},
  pages={1--11},
  year={2018}
}

@inproceedings{hazarika2018conversational,
  title={Conversational memory network for emotion recognition in dyadic dialogue videos},
  author={Hazarika, D. and Poria, S. and Zadeh, A. and Cambria, E. and Morency, L.-P. and Zimmermann, R.},
  booktitle={Proceedings of the conference on Association for Computational Linguistics},
  volume={2018},
  pages={2122},
  year={2018}
}

@inproceedings{poria2017context,
  title={Context-dependent sentiment analysis in user-generated videos},
  author={Poria, S. and Cambria, E. and Hazarika, D. and Majumder, N. and Zadeh, A. and Morency, L.-P.},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics},
  volume={1},
  pages={873--883},
  year={2017}
}

@inproceedings{liu2018efficientfusion,
  title={Efficient multimodal fusion with factorized bilinear pooling},
  author={Liu, Z. and Shen, Y. and Li, Y. and Zadeh, A. and Morency, L. P.},
  booktitle={Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3511--3520},
  year={2018}
}

@article{xing2020adapted,
  title={Adapted dynamic memory network for emotion recognition in conversation},
  author={Xing, S. and Mai, S. and Hu, H.},
  journal={IEEE Transactions on Affective Computing},
  volume={13},
  number={3},
  pages={1426--1439},
  year={2020}
}

@inproceedings{hazarika2018icon,
  title={ICON: Interactive conversational memory network for multimodal emotion detection},
  author={Hazarika, D. and Poria, S. and Mihalcea, R. and Cambria, E. and Zimmermann, R.},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2594--2604},
  year={2018}
}

@inproceedings{ghosal2019dialoguegcn,
  title={DialogueGCN: A graph convolutional neural network for emotion recognition in conversation},
  author={Ghosal, D. and Majumder, N. and Poria, S. and Chhaya, N. and Gelbukh, A.},
  booktitle={EMNLP-IJCNLP 2019: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing},
  year={2020}
}

@inproceedings{majumder2019dialoguernn,
  title={DialogueRNN: An attentive RNN for emotion detection in conversations},
  author={Majumder, N. and Poria, S. and Hazarika, D. and Mihalcea, R. and Gelbukh, A. and Cambria, E.},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={6818--6825},
  year={2019}
}

@inproceedings{ishiwatari2020relation,
  title={Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations},
  author={Ishiwatari, T. and Yasuda, Y. and Miyazaki, T. and Goto, J.},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7360--7370},
  year={2020}
}

@article{ren2021lr,
  title={LR-GCN: Latent relation-aware graph convolutional network for conversational emotion recognition},
  author={Ren, M. and Huang, X. and Li, W. and Song, D. and Nie, W.},
  journal={IEEE Transactions on Multimedia},
  volume={24},
  pages={4422--4432},
  year={2021}
}

@article{ai2023der,
  title={DER-GCN: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition},
  author={Ai, W. and Shou, Y. and Meng, T. and Li, K.},
  journal={arXiv preprint arXiv:2312.10579},
  year={2023}
}

@article{shou2024efficient,
  title={Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations},
  author={Shou, Y. and Ai, W. and Du, J. and Meng, T. and Liu, H. and Yin, N.},
  journal={arXiv preprint arXiv:2407.00119},
  year={2024}
}

@article{ma2023transformer,
  title={A transformer-based model with self-distillation for multimodal emotion recognition in conversations},
  author={Ma, H. and Wang, J. and Lin, H. and Zhang, B. and Zhang, Y. and Xu, B.},
  journal={IEEE Transactions on Multimedia},
  year={2023}
}

@article{meng2024multimodal,
  title={Revisiting Multimodal Emotion Recognition in Conversation from the Perspective of Graph Spectrum},
  author={Meng, T. and Zhang, F. and Shou, Y. and Ai, W. and Yin, N. and Li, K.},
  journal={arXiv preprint arXiv:2404.17862},
  year={2024}
}
