\section{Related Work}
\subsection{Multimodal Emotion Recognition}

Multimodal emotion recognition initially focused on emotion recognition from individual modalities. However, with advancements in technology, recent studies have increasingly integrated speech, text, and visual modalities to improve emotion recognition performance. Methods based on deep neural networks have been proposed to combine speech and text modalities, significantly enhancing emotion recognition accuracy ____. Additionally, frameworks that integrate speech, text, and visual features have further improved emotion prediction accuracy through joint training ____.

\subsection{Application of Contrastive Learning in Emotion Recognition}

Contrastive learning, which maximizes the similarity between similar samples and minimizes the distance between dissimilar samples, has achieved success across various domains, including vision, speech, and text. In recent years, contrastive learning-based multimodal emotion recognition frameworks have been introduced to enhance the feature fusion of speech and text modalities, significantly improving model performance ____.

\subsection{Visual Sequence Compression}

To address the redundant information in the visual modality, particularly in video data, methods for visual sequence compression have been proposed. These include CNN-based and LSTM-based compression methods, which effectively extract key frame information and improve emotion recognition accuracy by reducing the impact of redundant data ____.

\subsection{Cross-modal Fusion Methods}

One core challenge in multimodal emotion recognition is effectively fusing information from different modalities. Traditional fusion methods include early fusion, late fusion, and intermediate fusion. Recently, approaches based on self-attention mechanisms and graph convolution networks (GCNs) have emerged as new trends, allowing for effective interaction between modalities and consideration of their mutual influence during the fusion process ____.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=14cm]{figures/1.png}
	\caption{The overall architecture of DeepMSI-MER for multimodal emotion recognition. DeepMSI-MER consists of a high-level semantic feature module, an early feature fusion module, and a late feature fusion module. The high-level semantic feature module fuses the semantic features of text and audio to further extract contextual semantic features, which are ultimately used in VSC-Swin.}
	\label{fig:fig1}
\end{figure}