 

\section{Experiments}
% We have made our proposed framework publicly available to encourage widespread adoption and testing at our github account\footnote{link is anonymized for blind submission}.
 
\subsection{Experiment Setup}
Our work was conducted using a single NVIDIA RTX 6000 Ada Generation GPU, equipped with 18,176 CUDA cores and 48GB of dedicated memory. We selected the pre-trained BERT base model (cased), which contains 109 million parameters, to ensure that LLM loading and fine-tuning could be accommodated within the internal memory. The model is available at Hugging Face hub\footnote{https://huggingface.co/google-bert/bert-base-cased}.
\\
We utilized the Flower framework\footnote{https://flowerai.net/} to simulate a federated learning environment for fine-tuning LLMs, while the Transformers library was used for tasks such as training, tokenization, and evaluation. The GLUE dataset\footnote{https://huggingface.co/datasets/nyu-mll/glue}, accessed via Hugging Face, was used for data loading. In particular, we utilized four specific datasets from the GLUE benchmark, which are described below.

\begin{itemize}
\item 
QNLI: The Question-Answering Natural Language Inference (QNLI) dataset, sourced from Wikipedia, comprises 110,400 question-paragraph pairs. Each paragraph contains only one sentence that answers the associated question. The task for the language model is to identify whether a given sentence contains the correct answer to the question.
\item 
QQP \cite{wang2019superglue}: The Quora Question Pairs (QQP) dataset contains more than 400,000 pairs of questions, each labeled to show whether the questions are semantically equivalent, meaning they are paraphrases of each other. The task for the language model is to determine if one question is a paraphrase of the other.
\item 
SST2 \cite{socher2013recursive}: The Stanford Sentiment Treebank (SST2) dataset consists of 68,800 sentences from movie reviews, each annotated with its sentiment. The task for the language model is to classify the sentiment of a given sentence as either positive or negative.
\end{itemize}
 
 
We defined the training and testing splits for each dataset as follows: QNLI with 105,000 samples for training and 5,460 for testing; QQP with 364,000 samples for training and 391,000 for testing; and SST-2 with 67,000 samples for training and 1,820 for testing.

 
We simulate a federated learning environment with 4 distinct clients, each assigned its own training and testing dataset. The setup includes 5 training rounds, a learning rate of \( 2e-5 \), and a batch size of 550. We adopt the FedAvg algorithm of McMahan et al. \cite{mcmahan2017communication}
 
To replicate real-world data generation across decentralized devices, we distribute the training and testing data among clients using 4 distinct partitioning strategies: Iid, Linear, Square, and Exponential.\\
In the Iid policy, the partitioner creates partitions by randomly and uniformly sampling data from the dataset. \\In the Linear policy, partitions are created such that the size of each partition is linearly proportional to its ID. The amount of data assigned to each client increases linearly with the partition ID. For example, if the IDs range from 1 to \( k\), the client with ID 1 receives 1 unit of data, client 2 receives 2 units, and so on, until client \( k\), which receives \( k\) units.\\
In the Squared policy, the data assigned to each client is proportional to the square of the partition ID. For instance, if the IDs range from 1 to \( k\), the client with ID 1 receives 1 unit of data, client 2 receives 4 units, and so on, up to client \( k\), which receives \(k^2\) units.\\
In the Exponential policy, the data allocation is based on the exponential value of the partition ID. For example, if the IDs range from 1 to M, the client with ID 1 receives \(e^1\) units of data, client 2 receives \(e^2\) units, and so on, up to client \( k\), which receives \(e^k\) units.
 
We assessed and pre-computed the necessary noise for the specified security parameters and data partition size to be added during the learning process, utilizing two widely-used state-of-the-art differential privacy accountants: Renyi Differential Privacy (\rdp) \cite{abadi2016deep,mironov2017renyi} and \sys~Accountant \cite{birrell2024differentially}. 
We developed a function in the Flower framework that incorporates noise at the client side during training. The noise is added after each round, scaled by the standard deviation divided by the batch size (550 in our experiments).
For security parameters, we set \( \epsilon = 10,6 \) and \( \delta = 1e-6 \) for larger datasets (e.g., QNLI, and QQP, each containing several hundred thousand samples) and \( \delta = 1e-5 \) for the smaller dataset (e.g., SST-2, with tens of thousands of samples). Clipping norm is set to 3 for all experiments.
\begin{table}[t]
\caption{Max Accuracy Across Datasets and Partition Policies Using \nonoise, \rdp, and \sys~Accountant for \(\epsilon\) = 6 and \(\epsilon\) = 10}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|cc|cc|}
\hline
\multirow{2}{*}{\textbf{Dataset}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Partition \\ Policy\end{tabular}}} &
  \multirow{2}{*}{\textbf{\nonoise}} &
  \multicolumn{2}{c|}{\textbf{\(\epsilon\) = 10}} &
  \multicolumn{2}{c|}{\textbf{\(\epsilon\) = 6}} \\ \cline{4-7} 
                      &             &      & \multicolumn{1}{c|}{\textbf{\rdp}} & \textbf{\sys} & \multicolumn{1}{c|}{\textbf{\rdp}} & \textbf{\sys} \\ \hline
                      & Iid         & 88\% & \multicolumn{1}{c|}{87\%}         & 86\%           & \multicolumn{1}{c|}{87\%}         & 86\%           \\
\multirow{2}{*}{QQP}  & Linear      & 88\% & \multicolumn{1}{c|}{88\%}         & 86\%           & \multicolumn{1}{c|}{86\%}         & 85\%           \\
                      & Square      & 89\% & \multicolumn{1}{c|}{88\%}         & 85\%           & \multicolumn{1}{c|}{85\%}         & 84\%           \\
                      & Exponential & 89\% & \multicolumn{1}{c|}{87\%}         & 85\%           & \multicolumn{1}{c|}{88\%}         & 84\%           \\ \hline
                      & Iid         & 87\% & \multicolumn{1}{c|}{87\%}         & 86\%           & \multicolumn{1}{c|}{86\%}         & 85\%           \\
\multirow{2}{*}{QNLI} & Linear      & 88\% & \multicolumn{1}{c|}{88\%}         & 87\%           & \multicolumn{1}{c|}{87\%}         & 83\%           \\
                      & Square      & 88\% & \multicolumn{1}{c|}{88\%}         & 86\%           & \multicolumn{1}{c|}{86\%}         & 84\%           \\
                      & Exponential & 88\% & \multicolumn{1}{c|}{85\%}         & 84\%           & \multicolumn{1}{c|}{88\%}         & 83\%           \\ \hline
                      & Iid         & 91\% & \multicolumn{1}{c|}{90\%}         & 89\%           & \multicolumn{1}{c|}{90\%}         & 89\%           \\
\multirow{2}{*}{SST2} & Linear      & 90\% & \multicolumn{1}{c|}{90\%}         & 89\%           & \multicolumn{1}{c|}{89\%}         & 87\%           \\
                      & Square      & 92\% & \multicolumn{1}{c|}{90\%}         & 89\%           & \multicolumn{1}{c|}{89\%}         & 88\%           \\
                      & Exponential & 91\% & \multicolumn{1}{c|}{91\%}         & 90\%           & \multicolumn{1}{c|}{90\%}         & 89\%           \\ \hline
\end{tabular}%
}
\end{table}


\begin{table}[t]
\caption{Data Partition size based on Iid, Linear, Square, and Exponential partition Policies}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Dataset} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Partition \\ Policy\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Partition\\ 1\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Partition\\ 2\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Partition\\ 3\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Partition\\ 4\end{tabular}} \\ \hline
                      & Iid         & 90962 & 90962 & 90962  & 90962  \\
\multirow{2}{*}{QQP}  & Linear      & 36384 & 72769 & 109153 & 145540 \\
                      & Square      & 12128 & 48512 & 109153 & 194053 \\
                      & Exponential & 11664 & 31707 & 86188  & 234287 \\ \hline
                      & Iid         & 26186 & 26186 & 26186  & 26185  \\
\multirow{2}{*}{QNLI} & Linear      & 10474 & 20948 & 31422  & 41899  \\
                      & Square      & 3491  & 13965 & 31422  & 55865  \\
                      & Exponential & 3357  & 9127  & 24811  & 67448  \\ \hline
                      & Iid         & 16838 & 16837 & 16837  & 16837  \\
\multirow{2}{*}{SST2} & Linear      & 6734  & 13469 & 20204  & 26942  \\
                      & Square      & 2244  & 8979  & 20204  & 35922  \\
                      & Exponential & 2159  & 5869  & 15953  & 43368  \\ \hline
\end{tabular}%
}
\end{table}

\begin{table*}[t]
\caption{Required Noise per Data Partition to Achieve \(\epsilon\) = 6 and \(\epsilon\) = 10 for accountants \rdp~and \sys }
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|cccccccc|cccccccc|}
\hline
\multirow{3}{*}{\textbf{Dataset}} &
  \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Partition \\ Policy\end{tabular}}} &
  \multicolumn{8}{c|}{\textbf{\(\epsilon\) = 10}} &
  \multicolumn{8}{c|}{\textbf{\(\epsilon\) = 6}} \\ \cline{3-18} 
 &
   &
  \multicolumn{2}{c|}{\textbf{Partition 1}} &
  \multicolumn{2}{c|}{\textbf{Partition 2}} &
  \multicolumn{2}{c|}{\textbf{Partition 3}} &
  \multicolumn{2}{c|}{\textbf{Partition 4}} &
  \multicolumn{2}{c|}{\textbf{Partition 1}} &
  \multicolumn{2}{c|}{\textbf{Partition 2}} &
  \multicolumn{2}{c|}{\textbf{Partition 3}} &
  \multicolumn{2}{c|}{\textbf{Partition 4}} \\ \cline{3-18} 
 &
   &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \textbf{\sys} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \multicolumn{1}{c|}{\textbf{\sys}} &
  \multicolumn{1}{c|}{\textbf{\rdp}} &
  \textbf{\sys} \\ \hline
 &
  Iid &
  \multicolumn{1}{c|}{0.84} &
  \multicolumn{1}{c|}{1.66} &
  \multicolumn{1}{c|}{0.84} &
  \multicolumn{1}{c|}{1.66} &
  \multicolumn{1}{c|}{0.84} &
  \multicolumn{1}{c|}{1.66} &
  \multicolumn{1}{c|}{0.84} &
  1.66 &
  \multicolumn{1}{c|}{0.91} &
  \multicolumn{1}{c|}{1.79} &
  \multicolumn{1}{c|}{0.91} &
  \multicolumn{1}{c|}{1.79} &
  \multicolumn{1}{c|}{0.91} &
  \multicolumn{1}{c|}{1.79} &
  \multicolumn{1}{c|}{0.91} &
  1.79 \\
QQP &
  Linear &
  \multicolumn{1}{c|}{0.96} &
  \multicolumn{1}{c|}{1.88} &
  \multicolumn{1}{c|}{0.86} &
  \multicolumn{1}{c|}{1.71} &
  \multicolumn{1}{c|}{0.82} &
  \multicolumn{1}{c|}{1.63} &
  \multicolumn{1}{c|}{0.8} &
  1.59 &
  \multicolumn{1}{c|}{1.12} &
  \multicolumn{1}{c|}{2.09} &
  \multicolumn{1}{c|}{0.95} &
  \multicolumn{1}{c|}{1.85} &
  \multicolumn{1}{c|}{0.89} &
  \multicolumn{1}{c|}{1.75} &
  \multicolumn{1}{c|}{0.86} &
  1.69 \\
 &
  Square &
  \multicolumn{1}{c|}{1.28} &
  \multicolumn{1}{c|}{2.39} &
  \multicolumn{1}{c|}{0.91} &
  \multicolumn{1}{c|}{1.8} &
  \multicolumn{1}{c|}{0.82} &
  \multicolumn{1}{c|}{1.63} &
  \multicolumn{1}{c|}{0.77} &
  1.55 &
  \multicolumn{1}{c|}{1.65} &
  \multicolumn{1}{c|}{2.84} &
  \multicolumn{1}{c|}{1.03} &
  \multicolumn{1}{c|}{1.98} &
  \multicolumn{1}{c|}{0.89} &
  \multicolumn{1}{c|}{1.75} &
  \multicolumn{1}{c|}{0.83} &
  1.63 \\
 &
  Exponential &
  \multicolumn{1}{c|}{1.29} &
  \multicolumn{1}{c|}{2.42} &
  \multicolumn{1}{c|}{0.98} &
  \multicolumn{1}{c|}{1.92} &
  \multicolumn{1}{c|}{0.84} &
  \multicolumn{1}{c|}{1.67} &
  \multicolumn{1}{c|}{0.76} &
  1.52 &
  \multicolumn{1}{c|}{1.68} &
  \multicolumn{1}{c|}{2.88} &
  \multicolumn{1}{c|}{1.16} &
  \multicolumn{1}{c|}{2.16} &
  \multicolumn{1}{c|}{0.92} &
  \multicolumn{1}{c|}{1.81} &
  \multicolumn{1}{c|}{0.81} &
  1.6 \\ \hline
 &
  Iid &
  \multicolumn{1}{c|}{1.02} &
  \multicolumn{1}{c|}{1.99} &
  \multicolumn{1}{c|}{1.02} &
  \multicolumn{1}{c|}{1.99} &
  \multicolumn{1}{c|}{1.02} &
  \multicolumn{1}{c|}{1.99} &
  \multicolumn{1}{c|}{1.02} &
  1.99 &
  \multicolumn{1}{c|}{1.24} &
  \multicolumn{1}{c|}{2.26} &
  \multicolumn{1}{c|}{1.24} &
  \multicolumn{1}{c|}{2.26} &
  \multicolumn{1}{c|}{1.24} &
  \multicolumn{1}{c|}{2.26} &
  \multicolumn{1}{c|}{1.24} &
  2.26 \\
\multirow{2}{*}{QNLI} &
  Linear &
  \multicolumn{1}{c|}{1.34} &
  \multicolumn{1}{c|}{2.49} &
  \multicolumn{1}{c|}{1.08} &
  \multicolumn{1}{c|}{2.09} &
  \multicolumn{1}{c|}{0.98} &
  \multicolumn{1}{c|}{1.92} &
  \multicolumn{1}{c|}{0.93} &
  1.84 &
  \multicolumn{1}{c|}{1.76} &
  \multicolumn{1}{c|}{2.99} &
  \multicolumn{1}{c|}{1.34} &
  \multicolumn{1}{c|}{2.4} &
  \multicolumn{1}{c|}{1.17} &
  \multicolumn{1}{c|}{2.17} &
  \multicolumn{1}{c|}{1.07} &
  2.03 \\
 &
  Square &
  \multicolumn{1}{c|}{2.13} &
  \multicolumn{1}{c|}{3.77} &
  \multicolumn{1}{c|}{1.22} &
  \multicolumn{1}{c|}{2.3} &
  \multicolumn{1}{c|}{0.98} &
  \multicolumn{1}{c|}{1.92} &
  \multicolumn{1}{c|}{0.89} &
  1.76 &
  \multicolumn{1}{c|}{2.93} &
  \multicolumn{1}{c|}{4.8} &
  \multicolumn{1}{c|}{1.56} &
  \multicolumn{1}{c|}{2.71} &
  \multicolumn{1}{c|}{1.17} &
  \multicolumn{1}{c|}{2.17} &
  \multicolumn{1}{c|}{1} &
  1.93 \\
 &
  Exponential &
  \multicolumn{1}{c|}{2.17} &
  \multicolumn{1}{c|}{3.85} &
  \multicolumn{1}{c|}{1.41} &
  \multicolumn{1}{c|}{2.6} &
  \multicolumn{1}{c|}{1.04} &
  \multicolumn{1}{c|}{2.01} &
  \multicolumn{1}{c|}{0.87} &
  1.72 &
  \multicolumn{1}{c|}{2.99} &
  \multicolumn{1}{c|}{4.9} &
  \multicolumn{1}{c|}{1.86} &
  \multicolumn{1}{c|}{3.15} &
  \multicolumn{1}{c|}{1.26} &
  \multicolumn{1}{c|}{2.3} &
  \multicolumn{1}{c|}{0.96} &
  1.87 \\ \hline
 &
  Iid &
  \multicolumn{1}{c|}{1.15} &
  \multicolumn{1}{c|}{2.19} &
  \multicolumn{1}{c|}{1.15} &
  \multicolumn{1}{c|}{2.19} &
  \multicolumn{1}{c|}{1.15} &
  \multicolumn{1}{c|}{2.19} &
  \multicolumn{1}{c|}{1.15} &
  2.19 &
  \multicolumn{1}{c|}{1.45} &
  \multicolumn{1}{c|}{2.56} &
  \multicolumn{1}{c|}{1.45} &
  \multicolumn{1}{c|}{2.56} &
  \multicolumn{1}{c|}{1.45} &
  \multicolumn{1}{c|}{2.56} &
  \multicolumn{1}{c|}{1.45} &
  2.56 \\
\multirow{2}{*}{SST2} &
  Linear &
  \multicolumn{1}{c|}{1.58} &
  \multicolumn{1}{c|}{2.88} &
  \multicolumn{1}{c|}{1.23} &
  \multicolumn{1}{c|}{2.32} &
  \multicolumn{1}{c|}{1.09} &
  \multicolumn{1}{c|}{2.1} &
  \multicolumn{1}{c|}{1.02} &
  1.98 &
  \multicolumn{1}{c|}{2.13} &
  \multicolumn{1}{c|}{3.54} &
  \multicolumn{1}{c|}{1.58} &
  \multicolumn{1}{c|}{2.75} &
  \multicolumn{1}{c|}{1.36} &
  \multicolumn{1}{c|}{2.43} &
  \multicolumn{1}{c|}{1.23} &
  2.25 \\
 &
  Square &
  \multicolumn{1}{c|}{2.7} &
  \multicolumn{1}{c|}{4.79} &
  \multicolumn{1}{c|}{1.42} &
  \multicolumn{1}{c|}{2.61} &
  \multicolumn{1}{c|}{1.09} &
  \multicolumn{1}{c|}{2.1} &
  \multicolumn{1}{c|}{0.96} &
  1.88 &
  \multicolumn{1}{c|}{3.75} &
  \multicolumn{1}{c|}{6.27} &
  \multicolumn{1}{c|}{1.88} &
  \multicolumn{1}{c|}{3.17} &
  \multicolumn{1}{c|}{1.36} &
  \multicolumn{1}{c|}{2.43} &
  \multicolumn{1}{c|}{1.12} &
  2.1 \\
 &
  Exponential &
  \multicolumn{1}{c|}{2.77} &
  \multicolumn{1}{c|}{4.9} &
  \multicolumn{1}{c|}{1.68} &
  \multicolumn{1}{c|}{3.03} &
  \multicolumn{1}{c|}{1.17} &
  \multicolumn{1}{c|}{2.22} &
  \multicolumn{1}{c|}{0.93} &
  1.83 &
  \multicolumn{1}{c|}{3.84} &
  \multicolumn{1}{c|}{6.45} &
  \multicolumn{1}{c|}{2.27} &
  \multicolumn{1}{c|}{3.74} &
  \multicolumn{1}{c|}{1.48} &
  \multicolumn{1}{c|}{2.6} &
  \multicolumn{1}{c|}{1.06} &
  2.02 \\ \hline
\end{tabular}%
}
\end{table*}
\subsection{Experiments Results}
Figures 3, 4, and 5 depict the accuracy results for three noise addition methods—\nonoise, \sys, and \rdp—across various datasets and partitioning policies in 5 rounds of the federated learning process. We examined the noise levels needed for each accountant to achieve a target \( \epsilon \) and evaluated how different partitioning policies influence the maximum accuracy attained.
\subsubsection{Various Partition Policies Impact on Max Accuracy}Table I presents the maximum accuracy achieved across various datasets, partition policies, \( \epsilon\) values, and accountant methods.
For small datasets like SST2, different partitioning policies have minimal impact on accuracy for both \rdp~and \sys.  
For large datasets without noise, partitioning policies similarly show little effect on accuracy.  
When dealing with large datasets, high security (\( \epsilon\) = 6), and the \sys~accountant, the Iid policy delivers the best performance.  
In contrast, for the same security level and large datasets using the \rdp~accountant, the Exponential policy performs best.  
For large datasets requiring lower accuracy (\( \epsilon\) = 10) with both \sys~and \rdp~accountants, partitioning policies have a negligible impact on performance.
Table II describes data partition size based on various partition policies. Finally, Table III outlines the necessary noise standard deviation for \( \epsilon = 10 \) and \( \epsilon = 6 \), which must be added during the training phase of each client at the end of each training round.
\subsubsection{Accountant Type Impact on Noise for Target \( \epsilon\)}In reference to Table III, achieving a higher privacy level (lower \( \epsilon\)) necessitates the addition of more noise. Furthermore, the \sys~accountant requires greater noise levels compared to the \rdp~accountant to attain the same epsilon value  under the add-remove adjacency relation. We note, however, that under replace-one adjacency, another commonly used adjacency notion, \rdp~and \sys~require nearly identical noise levels to achieve the same $\epsilon$ value; see the discussion in Section 4 of \cite{birrell2024differentially}, including the comparison in Figure 4. More specifically, the noise levels for \rdp~in Table III would need to be approximately doubled in order to provide the same $\epsilon$ guarantee under both adjacency relations, while the  noises currently listed for \sys~in Table III guarantee the stated $\epsilon$ for both adjacency relations. Thus, when requiring the privacy guarantees to extend to replace-one adjacency, the benefits of fixed-size subsampling are even more apparent.  As the proper choice of adjacency relation is debatable, one might reasonably require guarantees that cover both, in which case the benefits of \sys~are even more apparent.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{QQP.pdf}
    \caption{QQP Accuracy Across 5 Rounds with \( \epsilon\) = 10 }
    \label{fig:enter-label}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{QNLI.pdf}
    \caption{QNLI Accuracy Across 5 Rounds with \( \epsilon\) = 10 }
    \label{fig:enter-label}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{SST2.pdf}
    \caption{SST2 Accuracy Across 5 Rounds with \( \epsilon\) = 10 }
    \label{fig:enter-label}
\end{figure}




