\section{Introduction}
Foundation models have achieved superior performance across various domains, including finance, healthcare, and cybersecurity. 
%
These models are trained on public data for general tasks and later fine-tuned by different industries for application-specific downstream tasks. 
%%
The performance of these models relies heavily on the quality and diversity of their training data.
%
This is especially important for sensitive applications where the performance and robustness of the model are crucial.
%
However, for these applications, access to distributed and diverse data is often restricted through internal privacy policies or regulations such as HIPAA. 
%
This limitation is widely acknowledged as a primary barrier to training and fine-tuning models in the medical field \cite{liu2024moe}. 
%
Federated learning (FL) mitigates this issue by maintaining the locality of the data and only requiring the participating clients to share their local model (trained on their data) with a central server. 
%

While maintaining data locality can contribute to data privacy, recent attacks (e.g., \cite{balle2022reconstructing,hayes2024bounding}), demonstrated that updates shared with the server can leak significant information about the user dataset. 
%
These attacks can be categorized as follows, sorted by their privacy implications.
%
1) Membership Inference \cite{nasr2018machine, yeom2018privacy}: The attacker's goal is to determine whether a specific data point was part of the dataset used to train the model. 
%
2) Model inversion \cite{carlini2019secret}: A more severe privacy attack, where the attacker adversary attempts to recover information about the training data (i.e., approximate reconstruction). 
%
3) Training Data Extraction \cite{carlini2021extracting}: This is the most potent privacy attack where the attacker attempts to recover the original training samples precisely.

Differential privacy (DP) has long been the gold standard for mitigating these attacks \cite{dwork2006calibrating}. 
%
This is achieved by injecting a measured noise into model gradients to minimize the influence of any single data point on the model parameters. 
%
The magnitude of the noise is determined based on the desired privacy guarantee, measured by the privacy cost $(\epsilon,\delta)$, which serves as a key parameter in DP algorithms. 

In the FL setting, DP can be applied in different stages and by different parties to protect against data privacy attacks.
%
Depending on the privacy goals, often dictated by privacy policies or regulation requirements (e.g., HIPAA), DP can be applied by the clients adding noise to their gradient before sending it to the central server; this is often referred to as local DP (LDP). Alternatively, in central DP (CDP), the central server injects noise into the aggregated global model at the end of each training iteration. 
%
In CDP, the server has access to gradients sent by the users and is, therefore, assumed to be trusted.
 %
 However, in applications that involve sensitive user data, relying on a trusted server can pose significant privacy risks and potentially violate privacy policies and regulations \cite{groce2011limits}.
%
 LDP can provide privacy at different stages of the model's life cycle (from model training to deployment) without assuming a trusted server. 

DP methods provide a worst-case privacy guarantee, which could result in utility loss due to the excessive added noise.  
%
%
In practice, the worst-case DP guarantees may not always be necessary for certain applications, potentially sacrificing performance by overprotecting the model.
%
For instance, in some applications, user participation might be public (e.g., social media), and only the user data should be protected. 
%
In such cases, the privacy goal is to defend against data reconstruction attacks to protect user data, which often require significantly less noise compared to the noise needed to defend against membership inference attacks \cite{balle2022reconstructing}.  
%

Another critical aspect of privacy-preserving FL is the selection of the DP method and the training parameters.
%
In highly distributed FL applications (e.g., \cite{yang2018applied}), the choice of DP method and parameters can significantly impact user participation. 
%
%
Certain DP methods tend to overestimate the necessary noise or impose a high computational overhead, making them impractical for low-end devices, particularly those from underrepresented groups. 
%
For example, as recently highlighted in \cite{birrell2024differentially}, most of the widely adopted DP methods, such as Renyi Differential Privacy (\rdp) \cite{mironov2017renyi}, rely on Poisson subsampling which results in producing variable size minibatches. 
%
This directly affects the machine's memory usage (Figure~\ref{fig:memory}).
%
While this might be manageable in traditional centralized settings, where the model training happens on powerful servers, in distributed FL applications with low-end devices, this can lead to an out-of-memory (OOM) error, potentially preventing certain devices from participating in the training process. 
%
This can directly affect the representation of different user groups in the training process, impacting the data diversity and potentially the model fairness \cite{fu2023client,xiao2021vehicle}. 
% 
Lastly, the selection of FL parameters and the AI models to be trained in the federated setting can significantly impact the performance and utility of the trained model and user participation rates. 
%
For example, while a larger batch size may reduce the noise required for differential privacy, it can also introduce additional performance overhead for users. 
%
These challenges highlight a key research gap.
%

To address this, in this work, we present a framework for Federated Learning Implementation with Privacy (\oursys) that integrates a privacy practitioner into the training process to guide privacy-aware decision-making.
%
The practitioner's role is to help guide the selection of these parameters based on application requirements, AI model, privacy objectives, system specifications, resource constraints, and user participation dynamics. 
%
This approach ensures a more informed, context-sensitive decision-making process, optimizing both privacy protection and overall system performance and leading to the training of robust AI models.
%
Figure~\ref{fig:flip} provides a high-level overview of \oursys.
%
% Our framework considers a privacy practitioner to aid in the parameter selection of FL and DP. 
%
% The crucial need to employ a privacy practitioner, and as it is demonstrated by our experiments,  in sensitive applications that require high model performance and are often regulated by multiple privacy policies and regulations (e.g., medical models) \birrell{sentence is a bit redundant: regulated ... regulations}.

\subsection{Our Contributions}
The contributions of our work as as follows. 
To our knowledge, our work is the first to empirically highlight the importance and effect of this selection process in the private FL training of AI models. 
%
Our contributions are as follows. 

\noindent$\bullet$ \textbf{Adoption of a Privacy Practitioner:} Our work is the first to empirically highlight the significance and impact of selecting DP and FL parameters in privacy-preserving federated learning (FL) for AI models. We introduce a novel framework where a privacy practitioner assists in tuning these parameters based on factors such as privacy requirements, performance trade-offs, computational constraints, and system specifications.

\noindent $\bullet$\textbf{Adopting a Fixed Mini-Batch DP Method:} \oursys~is the first privacy-preserving FL framework to: 1) highlight the side effects of variable mini-batch sizes in existing DP methods (e.g., \rdp) on FL model training, 2) adopt a fixed mini-batch approach (i.e., \sys) to ensure stable memory usage throughout training, and 3) empirically compare the impact of this choice on model accuracy.

\noindent $\bullet$ \textbf{Comprehensive Study on DP, FL Parameters, and Data Distribution:} To our knowledge, \oursys~is the first framework to study the impacts of various DP and FL parameters and the data distributions among the users in FL settings on model performance. We conduct our experiments by fine-tuning Large Language Models (LLMs) on four well-known natural language processing tasks from the GLUE dataset \cite{glue_2019}. Fine-tuning LLMs in the context of studying the impact of DP on model performance is a well-established approach \cite{behnia2022ew,yu2021differentially}. For example, simulating our framework in fine-tuneing BERT (with 109M parameters) \cite{devlin2018bert} with two different target privacy costs, $\epsilon = 6$ and $\epsilon = 10$, while adopting \sys~as our DP method, incurs up to a 5\% accuracy loss compared to the non-private model. However, this gap can be reduced to as low as 2\% when optimized parameters and data distribution are enforced by the practitioner. Our framework is open-source for public verification and testing using the following link:

\begin{center}
    {\href{https://github.com/KasraAhmadi/FL-Privacy-LLM}{https://github.com/KasraAhmadi/FL-Privacy-LLM}}
\end{center}
% \noindent $\bullet$ \textbf{Comprehensive Study on DP, FL Parameters, and Data Distribution:} \oursys~studies the impacts of various DP and FL parameters and the data distributions among the users in FL settings on model performance. We conduct our experiments by fine-tuning Large Language Models (LLMs) on four common natural language processing tasks from the GLUE dataset \cite{glue_2019}. Fine-tuning LLMs in the context of studying the impact of DP in model training is a well-established approach \cite{behnia2022ew,yu2021differentiall}. 
% For example, simulating our framework with two different target privacy costs, $\epsilon = 6$ and $\epsilon = 10$, while adopting \sys~as our DP method, incurs up to a $5\%$  accuracy loss compared to the non-private model. However, this gap can be reduced to as low as $2\%$ when optimized parameters and data distribution are enforced by the practitioner.

% \oursys~is the first attempt to 
% The privacy practitioner is provided with information on the participating devices (e.g., hardware specification), as well as performance and privacy requirements.
% %and suggest FL and privacy parameters to be used during the FL training or model. 
% %
% In the case of fine-tuning, the practitioner is also provided with information about the pre-trained model. 
% %

% \AAY{(i) The relationship between LLM and DP-FL seems little less emphasis, we have it in our title and likely in the abstract, but here we mention little. (ii) It is understood DP impacts parameter tuning more than plan FL, but would use of LLM, even without DP, could make things better? Basically, can we emphasis what LLM-supported parameter tuning is designed specifically for DP-FL versus FL? }

% \AAY{I agree with adding some numbers and list contributions like bullet items as we do.}

% \JLP{Echoing the above comment, as I read through the introduction it isn't clear what the contributions of this paper are.  By the end of the introduction the reader should have a clear picture of what we are contributing in this work, and I'm not sure it stands out.  We could do this with an itemized list, or revise the introduction to make these contributions more clear, or do both.}

% \AAY{Are there at least 2-3 papers published before using LLM for parameter tuning? If so, mentioning them here, and saying we do differently, for example by considering DP, or whatever technical delta we have. Basicaly, at least in Intro, some sort of minimal related work to leverage in our contrib side.}

% The goal of the privacy practitioner is to present a set of optimized FL and DP parameters that lead to high-performing models while meeting the privacy needs of certain applications. 
% %

% \oursys~utilizes a recent DP method with fixed minibatch size \cite{birrell2024differentially} in the context of FL for the first time. 
% %
% We present numerous experiments \kasra{I believe that rather than conducting numerous experiments, we can highlight the use of different partitioning policies to analyze the impact of data distribution on learning} to showcase how the selection of different parameter sets in FL settings with different data distributions among the clients can affect the privacy and performance of the model.
% %
% \kasra{I believe adding some numbers and results here is benefecial, some senstece like:  We showcase the impact of utilizing \sys \xspace over \rdp, achieving a manageable decrease in accuracy while preserving stable memory usage, unlike \rdp, which lacks unified memory management. We simulated our framework for two different target security values, \(\epsilon\) = 6 and \(\epsilon\) = 10. By using \sys \xspace accountant, while achieving constant memory usage, the results showed an average accuracy drop of 1.33\% for \(\epsilon\) = 10 and 1.9\% for \(\epsilon\) = 6, in comparison to the \rdp \xspace accountant.}


  
%

% Human practitioners acting as human entities in the HMI system are essential for managing the competing demands of privacy preservation, security, accuracy, and resource constraints. 
% %
% Their expertise is critical for the following reasons:
% \begin{itemize}
%     \item \textbf{Privacy Preservation}: The human practitioner oversees the application of privacy-preserving methods, such as differential privacy or secure multi-party computation, to protect client data during training and aggregation.
%     %
%     Their role is crucial in refining these techniques to achieve an optimal balance between data privacy and model performance.
%     \item \textbf{Resource Management}: In FL, clients often vary in computational power and memory availability.
%     %
%     Through collaboration with the privacy engine and clients, a practitioner can identify and deliver suitable FL parameters that influence privacy and accuracy, ensuring they are customized to align with each client's computational resources and memory constraints.
% \end{itemize}

% \subsection{Major Contribution}
% We introduced a framework that incorporates privacy practitioner, collaborating with clients involved in FL and the proposed privacy engine to meet privacy and learning process requirements.
% \begin{itemize}
%     \item \textbf{Privacy Engine}: The privacy engine in the proposed framework calculates the necessary noise to be added to the DP-SGD algorithm to ensure differential privacy, taking into account clients' accuracy, privacy, and resource requirements. It is memory-efficient and well-suited for embedded systems, utilizing the \sys \xspace accountant.
%     %
%     We showcase the impact of utilizing \sys \xspace over \rdp, achieving a manageable decrease in accuracy while preserving stable memory usage, unlike \rdp, which lacks unified memory management.
%     %
%     We simulated our framework for two different target security values, \(\epsilon\) = 6 and \(\epsilon\) = 10.
%     %
%     By using \sys \xspace accountant, while achieving constant memory usage, the results showed an average accuracy drop of 1.33\% for \(\epsilon\) = 10 and 1.9\% for \(\epsilon\) = 6, in comparison to the \rdp \xspace accountant.
%     %
%     \item \textbf{Various Partition Policies Impact on Accuracy}: To simulate real-world scenarios, we tested four partition policies—IID, Linear, Square, and Exponential distributions—on three different datasets (QQP, QNLI, and SST2) ordered from largest to smallest dataset size.
%     %
%     This allowed us to assess how the data size for each client impacts the final model accuracy in FL, when incorporating DP into the learning process.
% \end{itemize}

