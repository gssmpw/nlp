\section{Proposed Framework}
The goal of our framework, \oursys, is to optimize model performance given the required privacy requirements. 
In our proposed framework, the privacy practitioner (human expertise) is crucial in improving security and privacy through collaboration with clients and the privacy engine. The suggested data flow architecture, as depicted in Figure 2, includes four entities: Requirement, Privacy Practitioner, Clients, and Privacy Engine.




\subsection{Clients}
A client is a device, user, or entity involved in the decentralized training process, where it locally stores and processes its own private data. Clients contribute to training a shared machine learning model using their private data and periodically transmit model updates to the privacy engine. Since in \oursys~we aim to achieve full-stack privacy (during training and after deployment),  DP is implemented on the client side. 
%
Therefore, after receiving the FL and DP parameters from the practitioner, aside from model training, the client has to compute and inject the noise to its update. 
\subsection{Requirements}
In Federated Learning (FL), clients collaboratively train a centralized model, which may either be deployed for their own use or commercialized by the entity that organizes the federation. 
This entity facilitates client participation, often in exchange for a service or financial compensation.
Therefore, the selection of requirements is determined either by a coalition of clients or by the organizing entity overseeing the federation.
These requirements can be categorized into privacy and learning process requirements.
\subsubsection{Privacy Requirements}
\begin{itemize}
\item \textbf{Target privacy requirement:} This key parameter is central to calculating the privacy cost ($\epsilon$) and helps the practitioner determine the optimal trade-off between individual privacy and model accuracy. The selection of \(\epsilon\) varies by application, requiring a trade-off, as lower values of \(\epsilon\) enhance privacy but often come at the cost of reduced model accuracy. 

The interpretation of $\epsilon$ depends on many factors, including the number of training iterations (both local and global), data type, AI model, and other system specific parameters. However, this interpretation has proven to be very challenging \cite{pmlr-v119-triastcyn20a}, as $\epsilon$ does not directly translate to an intuitive measure of privacy risk across different settings . Instead, they can specify a privacy goal, such as mitigating the membership inference attack (MIA) or reconstruction attack, and a privacy practitioner can determine the suitable \(\epsilon\) value accordingly. 
Alternatively, if the DP is solely being implemented to meet certain requirements, enforced by regulations, the privacy cost could be computed by the privacy practitioner based on these requirements. 

MIA and reconstruction privacy goals are applied differently across various real-world scenarios. For example, in financial applications, MIA is generally not a primary concern. This is because certain information, such as the knowledge of which bank a person has a credit card with, is often considered public and not sensitive. However, more private details like one's Social Security Number (SSN) and account balance are crucial to protect. In such cases, the goal is not necessarily to prevent MIA but to guard against reconstruction attacks, where an adversary might attempt to reconstruct sensitive information about an individual from available data.
\end{itemize}
\begin{figure}[t]
    \centering
    \includegraphics[height=6cm, width=1\columnwidth]{framework.pdf}
    \caption{The data flow architecture for the proposed framework}
    \label{fig:flip}
\end{figure}
\subsubsection{Learning Process Requirements}
\begin{itemize}
    \item \textbf{Number of Clients:} The number of clients in the FL requires careful consideration. A larger client base increases diversity and robustness but necessitates efficient communication and aggregation strategies to address scalability issues. On the other hand, fewer clients make management simpler but may reduce the model's representativeness and resilience.
    \item \textbf{Data Distribution:}
   In some applications, no information about user data can be shared with the privacy practitioner. However, when feasible, insights into data distribution can significantly aid the practitioner in setting an effective privacy target and ultimately computing an appropriate $\epsilon$. As demonstrated in our experiments, when data is uniformly distributed across all users, the model generally achieves higher accuracy and can tolerate a lower $\epsilon$ while maintaining utility. Conversely, maintaining high accuracy under strict privacy constraints becomes more challenging in heterogeneous settings where data distributions vary significantly across clients. Certain data partitions may hold greater significance in the training process in such cases, requiring the privacy engine to incorporate adaptive strategies that account for these constraints, balancing privacy preservation with model performance.
   
   In addition to aiding in the selection of the appropriate $\epsilon$, knowledge of data distribution can also enable a tailored client selection during each training round. By carefully choosing clients with better uniform data distributions, the practitioner can improve overall data diversity across participants, model performance, and privacy. This tailored client selection approach can also help address other challenges posed by heterogeneous data distributions, ensuring a more balanced and effective training process in federated learning while maintaining privacy targets.
   
    \item \textbf{Client's Computation Constraints:} Computation constraints on the client side are another key concern in federated learning, as clients often consist of resource-limited devices such as smartphones, IoT devices, or edge devices. These limitations significantly impact the efficiency and feasibility of the learning process. In this context, we identify the benefit of the \sys~accountant \cite{birrell2024differentially}. Although not explicitly designed for resource-constrained devices, it is particularly valuable in our framework because it uses constant memory when calculating the noise for a given $\epsilon$, making it well-suited for these applications. Furthermore, the batch size, a parameter that directly influences both the training duration and the DP-SGD algorithm, is closely tied to the available memory on each client. It is the responsibility of the privacy practitioner to determine the batch size based on the memory capacity of individual clients (that can be provided in the requirements).  
\end{itemize}
\subsection{Privacy Practitioner}
\begin{itemize}
    \item \textbf{Security Parameters Calculation:}
   By utilizing its local privacy engine, the practitioner must determine the appropriate \(\epsilon\) value based on the target security constraints, which are influenced by the need to mitigate potential risks such as membership inference attacks (MIA) or reconstruction attacks, as well as to meet the privacy requirements set for the application. After selecting the suitable \(\epsilon\) value, the practitioner also chooses the appropriate differential privacy accountant. For memory-constrained applications, in our framework, \sys~ is selected, as it ensures constant memory usage but may result in slightly lower model accuracy. For cases where higher model accuracy is desired, \rdp~is chosen, which may involve non-constant memory usage. Once the \(\epsilon\) and \(\delta\) (where \(\delta = \frac{1}{|D_i|}\), representing the inverse of the dataset size) are determined, these parameters are provided clients. The client then utilizes their privacy engine, which calculates the necessary noise to be added to the client's gradient.
    \item \textbf{Set Batch size:}
    The batch size affects the efficiency and effectiveness of DP-SGD by determining how many samples are used in each model update. The ideal batch size strikes a balance between model security, training speed, memory usage, and overall performance. The privacy practitioner sets this parameter according to the available memory on the client side and the desired model performance.
\end{itemize}
\subsection{Privacy Engine}
\begin{itemize}
    \item \textbf{Noise Calculation:}
The privacy engine calculates the necessary noise using the specified accountant, \(\epsilon\), batch size, and \(\delta\) values. This noise is then injected into the client's gradient via the DP-SGD algorithm. 
\item \textbf{Requirement Adherence Tracking:}
In certain scenarios, the framework may fail to achieve the required accuracy due to factors such as the client's data partition, target security level, and target minimum accuracy. This limitation is primarily caused by the small size of partitioned data, where adding high noise to a gradient computed on a limited dataset can result in reduced accuracy. In such cases, the privacy engine generates a warning to indicate that the desired accuracy cannot be met. To address this, clients can either expand their data partitions or increase memory resources to adopt a different accountant, such as \rdp~ instead of \sys, to reduce the noise added during the DP-SGD algorithm.
% \item \textbf{Performing as the FL Server:}
% \kasra{This needs to be removed in the new architecture}
% The server serves as the central component responsible for overseeing and managing the distributed training process across clients. Its key functions involve collecting model updates, such as weights or gradients, from clients after their local training and merging them to refine the global model. Furthermore, the server initializes the global model and disseminates it to clients at the beginning of the training process and after each aggregation round. It also plays a vital role in facilitating communication among clients, ensuring the secure and efficient exchange of model updates and parameters.
\end{itemize}







