@inproceedings{2014SunDataSkipping,
author = {Sun, Liwen and Franklin, Michael J. and Krishnan, Sanjay and Xin, Reynold S.},
title = {Fine-grained partitioning for aggressive data skipping},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610515},
doi = {10.1145/2588555.2610515},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1115–1126},
numpages = {12},
keywords = {query processing, partitioning, data warehouse, algorithms},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{ANN-Hashing-5,
author = {Indyk, Piotr and Motwani, Rajeev},
title = {Approximate nearest neighbors: towards removing the curse of dimensionality},
year = {1998},
isbn = {0897919629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276698.276876},
doi = {10.1145/276698.276876},
booktitle = {Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing},
pages = {604–613},
numpages = {10},
location = {Dallas, Texas, USA},
series = {STOC '98}
}

@article{Aguerrebere2023,
author = {Aguerrebere, Cecilia and Bhati, Ishwar Singh and Hildebrand, Mark and Tepper, Mariano and Willke, Theodore},
title = {Similarity Search in the Blink of an Eye with Compressed Indices},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611537},
doi = {10.14778/3611479.3611537},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3433–3446},
numpages = {14}
}

@inproceedings{AndoniOptimal-Hashing-3,
author = {Andoni, Alexandr and Razenshteyn, Ilya},
title = {Optimal Data-Dependent Hashing for Approximate Near Neighbors},
year = {2015},
isbn = {9781450335362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746539.2746553},
doi = {10.1145/2746539.2746553},
abstract = {We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an n-point dataset in a d-dimensional space our data structure achieves query time O(d ⋅ nρ+o(1)) and space O(n1+ρ+o(1) + d ⋅ n), where ρ=1/(2c2-1) for the Euclidean space and approximation c>1. For the Hamming space, we obtain an exponent of ρ=1/(2c-1). Our result completes the direction set forth in (Andoni, Indyk, Nguyen, Razenshteyn 2014) who gave a proof-of-concept that data-dependent hashing can outperform classic Locality Sensitive Hashing (LSH). In contrast to (Andoni, Indyk, Nguyen, Razenshteyn 2014), the new bound is not only optimal, but in fact improves over the best (optimal) LSH data structures (Indyk, Motwani 1998) (Andoni, Indyk 2006) for all approximation factors c>1.From the technical perspective, we proceed by decomposing an arbitrary dataset into several subsets that are, in a certain sense, pseudo-random.},
booktitle = {Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing},
pages = {793–801},
numpages = {9},
keywords = {data structures, decision trees, high-dimensional geometry, similarity search, theory},
location = {Portland, Oregon, USA},
series = {STOC '15}
}

@online{DatastaxAstraDB,
  author = {Datastax},
  title = {Astra DB Serverless},
  url = {https://docs.datastax.com/en/astra-db-serverless/index.html},
  year = {2024}
}

@online{FaissMissingManual,
  author = {James Briggs},
  title = {Faiss: The Missing Manual},
  url = {https://www.pinecone.io/learn/series/faiss/},
  year = {2025}
}

@inproceedings{Ferhatosmanoglu2000VAPlus,
author = {Ferhatosmanoglu, Hakan and Tuncel, Ertem and Agrawal, Divyakant and El Abbadi, Amr},
title = {Vector approximation based indexing for non-uniform high dimensional data sets},
year = {2000},
isbn = {1581133200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/354756.354820},
doi = {10.1145/354756.354820},
booktitle = {Proceedings of the Ninth International Conference on Information and Knowledge Management},
pages = {202–209},
numpages = {8},
location = {McLean, Virginia, USA},
series = {CIKM '00}
}

@article{Ferhatosmanoglu2006VAPlusApprox,
title = {High dimensional nearest neighbor searching},
journal = {Information Systems},
volume = {31},
number = {6},
pages = {512-540},
year = {2006},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2005.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306437905000128},
author = {Hakan Ferhatosmanoglu and Ertem Tuncel and Divyakant Agrawal and Amr El Abbadi},
keywords = {High dimensional data, Nearest neighbor queries, Indexing, Similarity search, Approximate and progressive search, Non-uniform data, Scalability, Performance}
}

@article{FuFastApproximate,
author = {Fu, Cong and Xiang, Chao and Wang, Changxu and Cai, Deng},
title = {Fast approximate nearest neighbor search with the navigating spreading-out graph},
year = {2019},
issue_date = {January 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3303753.3303754},
doi = {10.14778/3303753.3303754},
abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial scenario of Taobao (Alibaba Group) and has been integrated into their billion-scale search engine.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {461–474},
numpages = {14}
}

@INPROCEEDINGS{Gillis2021,
  author={Yu, Minchen and Jiang, Zhifeng and Ng, Hok Chun and Wang, Wei and Chen, Ruichuan and Li, Bo},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Gillis: Serving Large Neural Networks in Serverless Functions with Automatic Model Partitioning}, 
  year={2021},
  volume={},
  number={},
  pages={138-148},
  doi={10.1109/ICDCS51616.2021.00022}}

@inproceedings{Gollapudi2023FilteredDiskANN,
author = {Gollapudi, Siddharth and Karia, Neel and Sivashankar, Varun and Krishnaswamy, Ravishankar and Begwani, Nikit and Raz, Swapnil and Lin, Yiyong and Zhang, Yin and Mahapatro, Neelam and Srinivasan, Premkumar and Singh, Amit and Simhadri, Harsha Vardhan},
title = {Filtered-DiskANN: Graph Algorithms for Approximate Nearest Neighbor Search with Filters},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583552},
doi = {10.1145/3543507.3583552},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3406–3416},
numpages = {11},
keywords = {Approximate nearest neighbor search, Dense retrieval, Filtered Search, Graph algorithms, Vector Search},
location = {Austin, TX, USA},
series = {WWW '23}
}

@ARTICLE{HouleRankBasedTree,
  author={Houle, Michael E. and Nett, Michael},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Rank-Based Similarity Search: Reducing the Dimensional Dependence}, 
  year={2015},
  volume={37},
  number={1},
  pages={136-150},
  keywords={Approximation methods;Measurement;Indexes;Navigation;Complexity theory;Data mining;Search problems;Nearest neighbor search;intrinsic dimensionality;rank-based search},
  doi={10.1109/TPAMI.2014.2343223}}

@article{IntelligentProbingLv-Hashing-8,
author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
title = {Intelligent probing for locality sensitive hashing: multi-probe LSH and beyond},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137836},
doi = {10.14778/3137765.3137836},
abstract = {The past decade has been marked by the (continued) explosion of diverse data content and the fast development of intelligent data analytics techniques. One problem we identified in the mid-2000s was similarity search of feature-rich data. The challenge here was achieving both high accuracy and high efficiency in high-dimensional spaces. Locality sensitive hashing (LSH), which uses certain random space partitions and hash table lookups to find approximate nearest neighbors, was a promising approach with theoretical guarantees. But LSH alone was insufficient since a large number of hash tables were required to achieve good search quality. Building on an idea of Panigrahy, our multi-probe LSH method introduced the idea of intelligent probing. Given a query object, we strategically probe its neighboring hash buckets (in a query-dependent fashion) by calculating the statistical probabilities of similar objects falling into each bucket. Such intelligent probing can significantly reduce the number of hash tables while achieving high quality. In this paper, we revisit the problem motivation, the challenges, the key design considerations of multi-probe LSH, as well as discuss recent developments in this space and some questions for further research.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2021–2024},
numpages = {4}
}

@misc{Jaiswal2022OODDiskANN,
  title={OOD-DiskANN: Efficient and Scalable Graph ANNS for Out-of-Distribution Queries}, 
  author={Shikhar Jaiswal and Ravishankar Krishnaswamy and Ankit Garg and Harsha Vardhan Simhadri and Sheshansh Agrawal},
  year={2022},
  eprint={2211.12850},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2211.12850}, 
}

@inproceedings{Jarachanthan2021AMPS,
author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
title = {AMPS-Inf: Automatic Model Partitioning for Serverless Inference with Cost Efficiency},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472501},
doi = {10.1145/3472456.3472501},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {14},
numpages = {12},
keywords = {machine learning inference, serverless computing, cost efficiency},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@ARTICLE{Jegou2011PQ,
  author={Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Product Quantization for Nearest Neighbor Search}, 
  year={2011},
  volume={33},
  number={1},
  pages={117-128},
  keywords={Quantization;Nearest neighbor searches;Indexing;Neural networks;Euclidean distance;File systems;Scalability;Image databases;Permission;Electronic mail;High-dimensional indexing;image indexing;very large databases;approximate search.},
  doi={10.1109/TPAMI.2010.57}}

@article{LuVHPHypersphere,
author = {Lu, Kejing and Wang, Hongya and Wang, Wei and Kudo, Mineichi},
title = {VHP: approximate nearest neighbor search via virtual hypersphere partitioning},
year = {2020},
issue_date = {May 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3397230.3397240},
doi = {10.14778/3397230.3397240},
abstract = {Locality sensitive hashing (LSH) is a widely practiced c-approximate nearest neighbor(c-ANN) search algorithm in high dimensional spaces. The state-of-the-art LSH based algorithm searches an unbounded and irregular space to identify candidates, which jeopardizes the efficiency. To address this issue, we introduce the concept of virtual hypersphere partitioning. The core idea is to impose a virtual hypersphere, centered at the query, in the original feature space and only examine points inside the hypersphere. The search space of a hypersphere is isotropic and bounded, and thus more efficient than the existing one. In practice, we use multiple physical hyperspheres with different radii in corresponding projection subspaces to emulate the single virtual hypersphere. We also developed a principled method to compute the hypersphere radii for given success probability.Based on virtual hypersphere partitioning, we propose a novel disk-based indexing and searching scheme VHP to answer c-ANN queries. In the indexing phase, VHP stores LSH projections with independent B+-trees. To process a query, VHP keeps increasing the radii of physical hyperspheres co-ordinately, which in effect amounts to enlarging the virtual hypersphere, to accommodate more candidates until the success probability is met. Rigorous theoretical analysis shows that the proposed algorithm supports c-ANN search for arbitrarily small c ≥ 1 with probability guarantee. Extensive experiments on a variety of datasets, including the billion-scale ones, demonstrate that VHP could achieve different tradeoffs between efficiency and accuracy, and achieves up to 2x speedup in running time over the state-of-the-art methods.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1443–1455},
numpages = {13}
}

@article{MALKOV201461,
title = {Approximate nearest neighbor algorithm based on navigable small world graphs},
journal = {Information Systems},
volume = {45},
pages = {61-68},
year = {2014},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437913001300},
author = {Yury Malkov and Alexander Ponomarenko and Andrey Logvinov and Vladimir Krylov},
keywords = {Similarity search, k-Nearest neighbor, Approximate nearest neighbor, Navigable small world, Distributed data structure},
abstract = {We propose a novel approach to solving the approximate k-nearest neighbor search problem in metric spaces. The search structure is based on a navigable small world graph with vertices corresponding to the stored elements, edges to links between them, and a variation of greedy algorithm for searching. The navigable small world is created simply by keeping old Delaunay graph approximation links produced at the start of construction. The approach is very universal, defined in terms of arbitrary metric spaces and at the same time it is very simple. The algorithm handles insertions in the same way as queries: by finding approximate neighbors for the inserted element and connecting it to them. Both search and insertion can be done in parallel requiring only local information from the structure. The structure can be made distributed. The accuracy of the probabilistic k-nearest neighbor queries can be adjusted without rebuilding the structure. The performed simulation for data in the Euclidean spaces shows that the structure built using the proposed algorithm has small world navigation properties with log2(n) insertion and search complexity at fixed accuracy, and performs well at high dimensionality. Simulation on a CoPHiR dataset revealed its high efficiency in case of large datasets (more than an order of magnitude less metric computations at fixed recall) compared to permutation indexes. Only 0.03% of the 10 million 208-dimensional vector dataset is needed to be evaluated to achieve 0.999 recall (virtually exact search). For recall 0.93 processing speed 2800queries/s can be achieved on a dual Intel X5675 Xenon server node with Java implementation.}
}
\bibliography{squash-refs}

@article{MARUKATAT20131101,
title = {Fast nearest neighbor retrieval using randomized binary codes and approximate Euclidean distance},
journal = {Pattern Recognition Letters},
volume = {34},
number = {9},
pages = {1101-1107},
year = {2013},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167865513000937},
author = {Sanparith Marukatat and Ithipan Methasate},
keywords = {Fast nearest neighbor search, Binary code indexing, Hamming distance, Approximate Euclidean distance},
abstract = {This paper investigates the use of binary codes in fast nearest neighbor retrieval for multi-dimensional dataset. The proposed method is based on a relation between the Euclidean distance and the Hamming distance between binary codes obtained from random projections of the two vectors. This relation allows approximating multi-dimensional Euclidean distance rapidly. The accuracy of the proposed approximation depends mainly on the length of the binary codes and not on the dimension of the input vector. Experimental results show that the proposed method yields an accurate approximation of the true distance. Fast search technique using the proposed distance is also presented. This technique is compared to other existing search methods. The experimental results are promising.}
}

@ARTICLE{Malkov2020HNSW,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  keywords={Routing;Complexity theory;Search problems;Data models;Approximation algorithms;Biological system modeling;Brain modeling;Graph and tree search strategies;artificial intelligence;information search and retrieval;information storage and retrieval;information technology and systems;search process;graphs and networks;data structures;nearest neighbor search;big data;approximate search;similarity search},
  doi={10.1109/TPAMI.2018.2889473}}

@Article{Martin2015,
author={Martin, Eric
and Cao, Eddie},
title={Euclidean chemical spaces from molecular fingerprints: Hamming distance and Hempel's ravens},
journal={Journal of Computer-Aided Molecular Design},
year={2015},
month={May},
day={01},
volume={29},
number={5},
pages={387-395},
abstract={Molecules are often characterized by sparse binary fingerprints, where 1s represent the presence of substructures and 0s represent their absence. Fingerprints are especially useful for similarity calculations, such as database searching or clustering, generally measuring similarity as the Tanimoto coefficient. In other cases, such as visualization, design of experiments, or latent variable regression, a low-dimensional Euclidian ``chemical space'' is more useful, where proximity between points reflects chemical similarity. A temptation is to apply principal components analysis (PCA) directly to these fingerprints to obtain a low dimensional continuous chemical space. However, Gower has shown that distances from PCA on bit vectors are proportional to the square root of Hamming distance. Unlike Tanimoto similarity, Hamming similarity (HS) gives equal weight to shared 0s as to shared 1s, that is, HS gives as much weight to substructures that neither molecule contains, as to substructures which both molecules contain. Illustrative examples show that proximity in the corresponding chemical space reflects mainly similar size and complexity rather than shared chemical substructures. These spaces are ill-suited for visualizing and optimizing coverage of chemical space, or as latent variables for regression. A more suitable alternative is shown to be Multi-dimensional scaling on the Tanimoto distance matrix, which produces a space where proximity does reflect structural similarity.},
issn={1573-4951},
doi={10.1007/s10822-014-9819-y},
url={https://doi.org/10.1007/s10822-014-9819-y}
}

@ARTICLE{Muja2014ScalableNN,
  author={Muja, Marius and Lowe, David G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Scalable Nearest Neighbor Algorithms for High Dimensional Data}, 
  year={2014},
  volume={36},
  number={11},
  pages={2227-2240},
  keywords={Approximation algorithms;Clustering algorithms;Vegetation;Partitioning algorithms;Approximation methods;Machine learning algorithms;Computer vision;Nearest neighbor search;big data;approximate search;algorithm configuration},
  doi={10.1109/TPAMI.2014.2321376}}

@inproceedings{Muller2020Lambada,
author = {M\"{u}ller, Ingo and Marroqu\'{\i}n, Renato and Alonso, Gustavo},
title = {Lambada: Interactive Data Analytics on Cold Data Using Serverless Cloud Infrastructure},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389758},
doi = {10.1145/3318464.3389758},
abstract = {Serverless computing has recently attracted a lot of attention from research and industry due to its promise of ultimate elasticity and operational simplicity. However, there is no consensus yet on whether or not the approach is suitable for data processing. In this paper, we present Lambada, a serverless distributed data processing framework designed to explore how to perform data analytics on serverless computing. In our analysis, supported with extensive experiments, we show in which scenarios serverless makes sense from an economic and performance perspective. We address several important technical questions that need to be solved to support data analytics and present examples from several domains where serverless offers a cost and performance advantage over existing solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {115–130},
numpages = {16},
keywords = {data lake, interactive analytics, serverless computing, serverless functions, cloud computing, elasticity},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{NEURIPS2019-DISKANN,
 author = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2023-Wang-NHQ,
 author = {Wang, Mengzhao and Lv, Lingwei and Xu, Xiaoliang and Wang, Yuxiang and Yue, Qiang and Ni, Jiongkang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {15738--15751},
 publisher = {Curran Associates, Inc.},
 title = {An Efficient and Robust Framework for Approximate Nearest Neighbor Search with Attribute Constraint},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/32e41d6b0a51a63a9a90697da19d235d-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@INPROCEEDINGS{Oakley2024FSDInference,
  author={Oakley, Joe and Ferhatosmanoglu, Hakan},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication}, 
  year={2024},
  volume={},
  number={},
  pages={2109-2122},
  keywords={Publish subscribe systems;Costs;Runtime;Computational modeling;Scalability;Serverless computing;Publish-subscribe;Serverless Computing;Machine Learning Inference;Deep Neural Network;Batch Inference;Serverless Communication;Distributed Inference;Function-as-a-Service;Publish-Subscribe;Message Queues;Object Storage;Hypergraph Partitioning;Cost Modelling},
  doi={10.1109/ICDE60146.2024.00168}}

@Article{Oakley2024ForesightPlus,
author={Oakley, Joe
and Conlan, Chris
and Demirci, Gunduz Vehbi
and Sfyridis, Alexandros
and Ferhatosmanoglu, Hakan},
title={Foresight plus: serverless spatio-temporal traffic forecasting},
journal={GeoInformatica},
year={2024},
month={Apr},
day={26},
issn={1573-7624},
doi={10.1007/s10707-024-00517-9},
url={https://doi.org/10.1007/s10707-024-00517-9}
}

@ARTICLE{OptimizedProductQuantization,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  keywords={Quantization (signal);Vectors;Artificial neural networks;Optimization;Encoding;Indexing;Linear programming;Vector quantization;nearest neighbor search;image retrieval;compact encoding;inverted indexing},
  doi={10.1109/TPAMI.2013.240}}

@article{PM-LSH-Hashing-11,
author = {Zheng, Bolong and Zhao, Xi and Weng, Lianggui and Hung, Nguyen Quoc Viet and Liu, Hang and Jensen, Christian S.},
title = {PM-LSH: A fast and accurate LSH framework for high-dimensional approximate NN search},
year = {2020},
issue_date = {January 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3377369.3377374},
doi = {10.14778/3377369.3377374},
abstract = {Nearest neighbor (NN) search in high-dimensional spaces is inherently computationally expensive due to the curse of dimensionality. As a well-known solution to approximate NN search, locality-sensitive hashing (LSH) is able to answer c-approximate NN (c-ANN) queries in sublinear time with constant probability. Existing LSH methods focus mainly on building hash bucket based indexing such that the candidate points can be retrieved quickly. However, existing coarse-grained structures fail to offer accurate distance estimation for candidate points, which translates into additional computational overhead when having to examine unnecessary points. This in turn reduces the performance of query processing. In contrast, we propose a fast and accurate LSH framework, called PM-LSH, that aims to compute the c-ANN query on large- scale, high-dimensional datasets. First, we adopt a simple yet effective PM-tree to index the data points. Second, we develop a tunable confidence interval to achieve accurate distance estimation and guarantee high result quality. Third, we propose an efficient algorithm on top of the PM-tree to improve the performance of computing c-ANN queries. Extensive experiments with real-world data offer evidence that PM-LSH is capable of outperforming existing proposals with respect to both efficiency and accuracy.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {643–655},
numpages = {13}
}

@article{PQCacheLocality2015,
author = {Andr\'{e}, Fabien and Kermarrec, Anne-Marie and Le Scouarnec, Nicolas},
title = {Cache locality is not enough: high-performance nearest neighbor search with product quantization fast scan},
year = {2015},
issue_date = {December 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/2856318.2856324},
doi = {10.14778/2856318.2856324},
abstract = {Nearest Neighbor (NN) search in high dimension is an important feature in many applications (e.g., image retrieval, multimedia databases). Product Quantization (PQ) is a widely used solution which offers high performance, i.e., low response time while preserving a high accuracy. PQ represents high-dimensional vectors (e.g., image descriptors) by compact codes. Hence, very large databases can be stored in memory, allowing NN queries without resorting to slow I/O operations. PQ computes distances to neighbors using cache-resident lookup tables, thus its performance remains limited by (i) the many cache accesses that the algorithm requires, and (ii) its inability to leverage SIMD instructions available on modern CPUs.In this paper, we advocate that cache locality is not sufficient for efficiency. To address these limitations, we design a novel algorithm, PQ Fast Scan, that transforms the cache-resident lookup tables into small tables, sized to fit SIMD registers. This transformation allows (i) in-register lookups in place of cache accesses and (ii) an efficient SIMD implementation. PQ Fast Scan has the exact same accuracy as PQ, while having 4 to 6 times lower response time (e.g., for 25 million vectors, scan time is reduced from 74ms to 13ms).},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {288–299},
numpages = {12}
}

@article{ParkNeighbor-Hashing-9,
author = {Park, Yongjoo and Cafarella, Michael and Mozafari, Barzan},
title = {Neighbor-sensitive hashing},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850589},
doi = {10.14778/2850583.2850589},
abstract = {Approximate kNN (k-nearest neighbor) techniques using binary hash functions are among the most commonly used approaches for overcoming the prohibitive cost of performing exact kNN queries. However, the success of these techniques largely depends on their hash functions' ability to distinguish kNN items; that is, the kNN items retrieved based on data items' hashcodes, should include as many true kNN items as possible. A widely-adopted principle for this process is to ensure that similar items are assigned to the same hashcode so that the items with the hashcodes similar to a query's hashcode are likely to be true neighbors.In this work, we abandon this heavily-utilized principle and pursue the opposite direction for generating more effective hash functions for kNN tasks. That is, we aim to increase the distance between similar items in the hashcode space, instead of reducing it. Our contribution begins by providing theoretical analysis on why this revolutionary and seemingly counter-intuitive approach leads to a more accurate identification of kNN items. Our analysis is followed by a proposal for a hashing algorithm that embeds this novel principle. Our empirical studies confirm that a hashing algorithm based on this counter-intuitive idea significantly improves the efficiency and accuracy of state-of-the-art techniques.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {144–155},
numpages = {12}
}

@article{Patel2024ACORN,
author = {Patel, Liana and Kraft, Peter and Guestrin, Carlos and Zaharia, Matei},
title = {ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654923},
doi = {10.1145/3654923},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {120},
numpages = {27},
keywords = {approximate nearest neighbor search, hybrid search, vector search}
}

@inproceedings{Perron2020,
author = {Perron, Matthew and Castro Fernandez, Raul and DeWitt, David and Madden, Samuel},
title = {Starling: A Scalable Query Engine on Cloud Functions},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380609},
doi = {10.1145/3318464.3380609},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {131–141},
numpages = {11},
keywords = {FAAS, OLAP, serverless, cloud},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@online{Pinecone-Serverless,
  author = {Pinecone},
  title = {Pinecone Serverless},
  url = {https://www.pinecone.io/product/},
  year = {2024}
}

@article{RabitQ2024,
author = {Gao, Jianyang and Long, Cheng},
title = {RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654970},
doi = {10.1145/3654970},
abstract = {Searching for approximate nearest neighbors (ANN) in the high-dimensional Euclidean space is a pivotal problem. Recently, with the help of fast SIMD-based implementations, Product Quantization (PQ) and its variants can often efficiently and accurately estimate the distances between the vectors and have achieved great success in the in-memory ANN search. Despite their empirical success, we note that these methods do not have a theoretical error bound and are observed to fail disastrously on some real-world datasets. Motivated by this, we propose a new randomized quantization method named RaBitQ, which quantizes D-dimensional vectors into D-bit strings. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. In addition, we introduce efficient implementations of RaBitQ, supporting to estimate the distances with bitwise operations or SIMD-based operations. Extensive experiments on real-world datasets confirm that (1) our method outperforms PQ and its variants in terms of accuracy-efficiency trade-off by a clear margin and (2) its empirical performance is well-aligned with our theoretical analysis.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {167},
numpages = {27},
keywords = {Johnson-Lindenstrauss transformation, approximate nearest neighbor search, quantization}
}

@INPROCEEDINGS{SilpaAnan2008OptimizedKDTree,
  author={Silpa-Anan, Chanop and Hartley, Richard},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Optimised KD-trees for fast image descriptor matching}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  keywords={Application software;Image recognition;Image databases;Indexing;Principal component analysis;Binary search trees;Binary trees;Computer vision;Image retrieval;Search methods},
  doi={10.1109/CVPR.2008.4587638}}

@article{Su2024Vexless,
author = {Su, Yongye and Sun, Yinqi and Zhang, Minjia and Wang, Jianguo},
title = {Vexless: A Serverless Vector Data Management System Using Cloud Functions},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654990},
doi = {10.1145/3654990},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {187},
numpages = {26},
keywords = {cloud functions, serverless computing, serverless databases, vector databases}
}

@inproceedings{TuncelFerhatosmanoglu2002VQIndex,
author = {Tuncel, Ertem and Ferhatosmanoglu, Hakan and Rose, Kenneth},
title = {VQ-index: an index structure for similarity searching in multimedia databases},
year = {2002},
isbn = {158113620X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/641007.641117},
doi = {10.1145/641007.641117},
booktitle = {Proceedings of the Tenth ACM International Conference on Multimedia},
pages = {543–552},
numpages = {10},
keywords = {approximate similarity searching, clustering, indexing, retrieved information reduction, retrieved set reduction, vector quantization},
location = {Juan-les-Pins, France},
series = {MULTIMEDIA '02}
}

@online{TurboPuffer,
  author = {TurboPuffer},
  title = {TurboPuffer},
  url = {https://turbopuffer.com},
  year = {2024}
}

@online{Upstash,
  author = {Upstash},
  title = {Upstash Vector: Serverless Vector Database for AI and LLMs},
  url = {https://upstash.com/blog/introducing-vector-database},
  year = {2024}
}

@INPROCEEDINGS{VarianceAwareQuantization,
  author={Paparrizos, John and Edian, Ikraduya and Liu, Chunwei and Elmore, Aaron J. and Franklin, Michael J.},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Fast Adaptive Similarity Search through Variance-Aware Quantization}, 
  year={2022},
  volume={},
  number={},
  pages={2969-2983},
  keywords={Dimensionality reduction;Quantization (signal);Dictionaries;Nearest neighbor methods;Data engineering;Robustness;Explosives;quantization;similarity search;proximity search},
  doi={10.1109/ICDE53745.2022.00268}}

@article{Wang2020DeltaPQ,
author = {Wang, Runhui and Deng, Dong},
title = {DeltaPQ: lossless product quantization code compression for high dimensional similarity search},
year = {2020},
issue_date = {September 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3424573.3424580},
doi = {10.14778/3424573.3424580},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3603–3616},
numpages = {14}
}

@inproceedings{Wang2021Milvus,
author = {Wang, Jianguo and Yi, Xiaomeng and Guo, Rentong and Jin, Hai and Xu, Peng and Li, Shengjun and Wang, Xiangyu and Guo, Xiangzhou and Li, Chengming and Xu, Xiaohai and Yu, Kun and Yuan, Yuxing and Zou, Yinghao and Long, Jiquan and Cai, Yudong and Li, Zhenxiang and Zhang, Zhifeng and Mo, Yihua and Gu, Jun and Jiang, Ruiyi and Wei, Yi and Xie, Charles},
title = {Milvus: A Purpose-Built Vector Data Management System},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457550},
doi = {10.1145/3448016.3457550},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2614–2627},
numpages = {14},
keywords = {data science, heterogeneous computing, high-dimensional similarity search, machine learning, vector database},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@online{Weaviate,
  author = {Weaviate},
  title = {Weaviate Serverless Database},
  url = {https://weaviate.io/deployment/serverless},
  year = {2024}
}

@article{Wei2020AnalyticDBV,
author = {Wei, Chuangxian and Wu, Bin and Wang, Sheng and Lou, Renjie and Zhan, Chaoqun and Li, Feifei and Cai, Yuanzhe},
title = {AnalyticDB-V: a hybrid analytical engine towards query fusion for structured and unstructured data},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415541},
doi = {10.14778/3415478.3415541},
abstract = {With the explosive growth of unstructured data (such as images, videos, and audios), unstructured data analytics is widespread in a rich vein of real-world applications. Many database systems start to incorporate unstructured data analysis to meet such demands. However, queries over unstructured and structured data are often treated as disjoint tasks in most systems, where hybrid queries (i.e., involving both data types) are not yet fully supported.In this paper, we present a hybrid analytic engine developed at Alibaba, named AnalyticDB-V (ADBV), to fulfill such emerging demands. ADBV offers an interface that enables users to express hybrid queries using SQL semantics by converting unstructured data to high dimensional vectors. ADBV adopts the lambda framework and leverages the merits of approximate nearest neighbor search (ANNS) techniques to support hybrid data analytics. Moreover, a novel ANNS algorithm is proposed to improve the accuracy on large-scale vectors representing massive unstructured data. All ANNS algorithms are implemented as physical operators in ADBV, meanwhile, accuracy-aware cost-based optimization techniques are proposed to identify effective execution plans. Experimental results on both public and in-house datasets show the superior performance achieved by ADBV and its effectiveness. ADBV has been successfully deployed on Alibaba Cloud to provide hybrid query processing services for various real-world applications.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3152–3165},
numpages = {14}
}

@INPROCEEDINGS{Zhao2020SONGGPU,
  author={Zhao, Weijie and Tan, Shulong and Li, Ping},
  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)}, 
  title={SONG: Approximate Nearest Neighbor Search on GPU}, 
  year={2020},
  volume={},
  number={},
  pages={1033-1044},
  keywords={Graphics processing units;Instruction sets;Memory management;Indexes;Data structures;Approximation algorithms},
  doi={10.1109/ICDE48307.2020.00094}}

@misc{aguerrebere2024locallyadaptivequantizationstreamingvector,
      title={Locally-Adaptive Quantization for Streaming Vector Search}, 
      author={Cecilia Aguerrebere and Mark Hildebrand and Ishwar Singh Bhati and Theodore Willke and Mariano Tepper},
      year={2024},
      eprint={2402.02044},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02044}, 
}

@article{db-lsh-hashing-2,
  title={DB-LSH 2.0: Locality-sensitive hashing with query-based dynamic bucketing},
  author={Tian, Yao and Zhao, Xi and Zhou, Xiaofang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@misc{douze2024faisslibraryivfsq8,
      title={The Faiss library}, 
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08281}, 
}

@inproceedings{ferhatosmanoglu2001approximate,
  title={Approximate nearest neighbor searching in multimedia databases},
  author={Ferhatosmanoglu, Hakan and Tuncel, Ertem and Agrawal, Divyakant and El Abbadi, Amr},
  booktitle={Proceedings 17th International Conference on Data Engineering},
  pages={503--511},
  year={2001},
  organization={IEEE}
}

@misc{singh2021freshdiskannfastaccurategraphbased,
      title={FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search}, 
      author={Aditi Singh and Suhas Jayaram Subramanya and Ravishankar Krishnaswamy and Harsha Vardhan Simhadri},
      year={2021},
      eprint={2105.09613},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2105.09613}, 
}

@misc{wang2022navigableproximitygraphdrivennative-NHQ-2,
      title={Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints}, 
      author={Mengzhao Wang and Lingwei Lv and Xiaoliang Xu and Yuxiang Wang and Qiang Yue and Jiongkang Ni},
      year={2022},
      eprint={2203.13601},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2203.13601}, 
}

@misc{zhao2022constrainedapproximatesimilaritysearchAIRSHIP,
      title={Constrained Approximate Similarity Search on Proximity Graph}, 
      author={Weijie Zhao and Shulong Tan and Ping Li},
      year={2022},
      eprint={2210.14958},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2210.14958}, 
}

