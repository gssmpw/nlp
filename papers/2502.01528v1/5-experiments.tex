\section{Experimental Analysis}
\label{s:5-squash-expts}

\subsection{Experimental Setup and Datasets}
We compare SQUASH against two state-of-the-art serverless vector search offerings (a commercial offering and Vexless \cite{Su2024Vexless}), plus two server-based baselines. We explore the performance characteristics at varying parallelism levels across multiple datasets. Finally, we evaluate the performance using our optional caching module targeted at sustained workloads. %, where caching is used to improve throughput.
We run experiments on high-dimensional vector data benchmarks: \textbf{SIFT1M}, \textbf{GIST1M}, \textbf{SIFT10M} and \textbf{DEEP10M} (see Table \ref{tab:datasets}). 
For Local Intrinsic Dimensionality (LID) \cite{fu2021high-LID-Higher}, a higher figure indicates a more challenging dataset. 
% While a large range of similar reference datasets now exist, 
We selected these datasets in line with Vexless \cite{Su2024Vexless}, and added an additional dataset to allow us to further assess the scalability of our approach. 
We use $recall@k = \frac{G \cap R}{k}$, where $G$ is the set of ground truth nearest neighbors (satisfying the filter predicate) and $R$ is the retrieved set.
For all datasets, we use segment size $S=8$, and select a bit budget $b$ = $4 \times d$. 
Our queries have an approximate (joint) attribute selectivity (i.e., proportion of vectors passing all filters) of $8\%$ in line with other works on hybrid search \cite{Patel2024ACORN}.
We achieve this by generating $A=4$ uniform attributes for each dataset.
To illustrate the scalability of SQUASH to large workloads, we consider the processing of \textbf{1000 queries} for all experiments. 
We report the average of 3 runs, and the same queries are used for all solutions. 

\input{tables/tab-datasets}

\subsection{Baselines}
\label{ss:7-baselines}

We first compare against a commercial serverless vector database, which we refer to as \textbf{System-X}.
System-X offers a fully managed, cloud-based solution for high-dimensional vector search, with pay-as-you-use pricing, although it does not run on FaaS. 
To evaluate System-X, we first 
locally transform the data into the required format (i.e., a dictionary for each record with keys: ID, values (vector data), metadata (attribute data)). 
We then `upsert' the data, constituting both the upload and indexing phases. 
We use Python's \textsc{ThreadPoolExecutor} on a large server (Intel Xeon W-2245 CPU, 503GB memory) to send parallel requests to System-X.
SQUASH is also evaluated against \textbf{Vexless}\cite{Su2024Vexless}, the only other serverless solution built using FaaS.
Vexless\footnote{https://github.com/Vexless/Vexless} leverages caching in conjunction with their workload generator, to model large query volumes over long test time periods (e.g., several thousand queries issued per second for an entire day). 
While this is a sensible approach if repeated queries are frequently submitted, the performance figures it enables may not be representative of individual test runs against previously unseen queries. 
Additionally, Vexless does not offer support for hybrid queries, an important feature of SQUASH which requires significant computation in the QueryAllocators (QAs). 
Therefore, we compare with Vexless separately 
in Section \ref{ss:7-caching}. 
In all other experiments, \textit{SQUASH does not use result caching}.
Finally, the cost and performance of SQUASH are compared against several \textbf{server-based baselines}. 
For all server experiments, the same codebase as SQUASH is used, modified to run on a single machine (i.e., spawning separate processes rather than invoking parallel Lambda functions).

% \input{plots/insert-cost-plot}

\subsection{FaaS and Server Setup}

We use AWS Lambda as the FaaS compute platform. Single Lambda applications are created for the \textit{QA} and \textit{CO} components, described in Section \ref{s:3-serverless}. 
For the \textit{QPs}, a Lambda application is created per partition, e.g., \textit{squash-processor-0, squash-processor-1}.  
We allocate $M_{CO}$ = 512MB to the CO and $M_{QA} = M_{QP} $ = 1770MB to the QA and all QPs; 1770MB has been reported as a cut-off point where an additional core is allocated to the function \cite{AWS-Lambda-1770MB, FN-AWS-Lambda-Memory-Compute}. 
% Boto3 (AWS Python SDK) is used to interact with all AWS services. 
We invoke concurrent query-parallel QAs $N_{QA}$ = 10 ($F=10, l_{max}=1$), 20 ($F=4, l_{max}=2$), 84 ($F=4, l_{max}=3$), 155 ($F=5, l_{max}=3$), 258 ($F=6, l_{max}=3$), 340 ($F=4, l_{max}=4$), each of which can invoke multiple processors. 
Python's \textsc{ThreadPoolExecutor} is employed to parallelize the invocation of child QA/QP functions in Lambda instances. 
SQUASH was built using Python 3.11, NumPy 2.0.0 and Bitarray 2.5.0. $P$ = 10 partitions are used for \textbf{SIFT1M} and \textbf{GIST1M} and $P$ = 20 for \textbf{SIFT10M} and \textbf{DEEP10M}, in accordance with the sizes of the datasets. 
For all experiments, our entire pipeline described in Section \ref{ss:2.4-search-pipeline} is run. 
All experiments are performed in the AWS eu-west-1 region. In all cases, SQUASH is calibrated to achieve the same 97\% recall as System-X, although our solution can achieve $>99\%$ recall if configured to do so.
The parameters used to achieve this are as follows: Binary Quantization Cut-off Percentage $H_{perc}=10$, Fine-Tuning Ratio $R=2$ for all datasets. 
For the Centroid Distance Threshold $T$, we use $\beta = 0.001$. SIFT1M: $T=1.15$, GIST1M: $T=1.2$, SIFT10M: $T=1.15$, DEEP10M: $T=1.13$.
For our server baselines, two instance sizes are selected, enabling us to compare performance/cost against relatively small and large instances. 
AWS EC2 is used for all server baselines. 
Our larger instance type is c7i.16xlarge (64 vCPU, 128GB memory). 
We also baseline against a c7i.4xlarge instance (16 vCPU, 32GB memory) to evaluate a more cost-effective option. 

\input{plots/insert-cost-plot}


\subsection{Cost and Performance Comparison}

First, the cost-effectiveness of SQUASH is evaluated against System-X and both server-based baselines in Figure \ref{fig:squash-daily-cost-plot}.
In this study, a sporadic vector search workload is modeled, where queries arrive at uniform intervals over a 24 hour period. 
These requests are evenly distributed over the SIFT1M, GIST1M, SIFT10M and DEEP10M datasets. 
For this experiment, a balanced SQUASH configuration is selected ($N_{QA} = 84$), which achieves an attractive cost-to-performance ratio (N.B., this is not the cheapest available, so costs can be reduced further if latency requirements are less strict). 
For System-X, costs are calculated based on the number of read pricing units consumed by each request. 
For the server baselines, it is assumed that two of the respective instances are provisioned, to accommodate periods of bursty traffic and to offer redundancy.
It is observed that \textbf{SQUASH is consistently cheaper per-request than System-X} (per-query cost reductions for each dataset - SIFT1M: 5x, GIST1M: 3.8x, SIFT10M: 3.6x, DEEP10M: 4.1x).
Significant cost savings are also achieved compared to all server-based solutions until very large daily query volumes are reached (\textbf{approx 1M / 3.5M queries per day}).
Next, we compare the queries per second (QPS) processed by each vector search solution in Figure \ref{fig:squash-performance-graph}. 
It is noted that SQUASH achieves significantly higher QPS than System-X for all datasets, with up to an $\sim$18x improvement for SIFT10M. System-X is most competitive on GIST1M, but the significant query parallelism of SQUASH leads to higher throughput. 
Both server baselines struggled with scalability. The nature of the SQUASH system requires high parallelism to function effectively; even on a large server, there is contention between QA and QP processes. In contrast, the ability of FaaS to offer thousands of independent parallel instances gives it a significant scalability and performance advantage.

\input{plots/insert-performance-plot}


\subsection{SQUASH Cost-to-Performance Trade-Off}

Next, in Figure \ref{fig:squash-latency-and-cost-2x2} we explore the scalability and cost-to-performance trade-off of our fully serverless solution, by varying the number of QueryAllocators ($N_{QA}$) invoked for each batch query execution. 
Recall that QAs are responsible for performing attribute filtering, as well as cluster ranking/selection; therefore, the total number of QPs invoked scales with $N_{QA}$, up to thousands of concurrent QPs for large $N_{QA}$. 
In Figure \ref{fig:squash-latency-and-cost-2x2}, the dotted horizontal lines correspond to the System-X latency/cost for each dataset; SQUASH achieves lower latency at all but the lowest parallelism levels, and lower costs at all parallelism levels. 
For GIST1M, where System-X offers the most competitive latency compared to SQUASH with $N_{QA}=84$, we see significant cost savings with SQUASH.
It is observed that invoking $N_{QA} = \{84, 155\}$ offers an attractive balance of cost and performance, particularly for SIFT1M and GIST1M. 
At these parallelism levels, each QA is responsible for managing $\sim$ 12/7 queries respectively in each batch. 
Using additional QAs for these datasets may yield slightly higher QPS, albeit it with a disproportionate cost increase.
It should be noted that GIST1M latencies are $\sim 2.3$x higher than those of SIFT1M, reflecting the challenge of increasing the dimensionality by a factor of almost 8.
For the larger SIFT10M and DEEP10M datasets, our results indicate that while $N_{QA}=84$ still offers a reasonable balance of cost and performance, additional throughput can be achieved at higher parallelism levels, up to $N_{QA}=258$. 
It is noted that $N_{QA}=340$ is generally not recommended for this workload, as the overhead of invoking the thousands of concurrent FaaS instances this produces (even accelerated by our tree-based invocation method and warm starts) dominates the time spent performing computation. A larger query set would be required for this to be beneficial.

\input{plots/insert-latency-and-cost-2x2-plot}

\input{tables/tab-caching-performance}
\subsection{Performance with Caching}
\label{ss:7-caching}

As discussed in Section \ref{ss:7-baselines}, Vexless \cite{Su2024Vexless} employs a caching solution to improve query performance, which is a valid approach for sustained workloads. 
It is also evaluated over long periods of high query traffic (e.g., 3k+ queries per second for an entire day, over 250M+ in total), repeating the same 1k/10k reference queries. % provided with the benchmark datasets. 
However, this induces significant query repetition and thus many cache hits, so the reported latencies may not be reflective of execution against previously unseen queries.
To allow us to evaluate SQUASH in a similar context, here we also leverage a lightweight caching solution. 
For this study, we seek to determine the `cache ratio' (i.e., the number of times we duplicate the same queries) required for SQUASH to achieve higher QPS than Vexless, on each common dataset. We use the same recall target as in our other experiments for both systems. These figures are shown in Table \ref{tab:caching-performance}. For GIST1M ($d = 960$), SQUASH is able to achieve higher throughput with no query duplication; this illustrates the impressive performance of our SQ-based approach.% 