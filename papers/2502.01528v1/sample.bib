
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{Weber1998VA,
author = {Weber, Roger and Schek, Hans-J\"{o}rg and Blott, Stephen},
title = {A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces},
year = {1998},
isbn = {1558605665},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 24rd International Conference on Very Large Data Bases},
pages = {194–205},
numpages = {12},
series = {VLDB '98}
}

@inproceedings{Ferhatosmanoglu2000VAPlus,
author = {Ferhatosmanoglu, Hakan and Tuncel, Ertem and Agrawal, Divyakant and El Abbadi, Amr},
title = {Vector approximation based indexing for non-uniform high dimensional data sets},
year = {2000},
isbn = {1581133200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/354756.354820},
doi = {10.1145/354756.354820},
booktitle = {Proceedings of the Ninth International Conference on Information and Knowledge Management},
pages = {202–209},
numpages = {8},
location = {McLean, Virginia, USA},
series = {CIKM '00}
}

@ARTICLE{Jegou2011PQ,
  author={Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Product Quantization for Nearest Neighbor Search}, 
  year={2011},
  volume={33},
  number={1},
  pages={117-128},
  keywords={Quantization;Nearest neighbor searches;Indexing;Neural networks;Euclidean distance;File systems;Scalability;Image databases;Permission;Electronic mail;High-dimensional indexing;image indexing;very large databases;approximate search.},
  doi={10.1109/TPAMI.2010.57}}

@inproceedings{TuncelFerhatosmanoglu2002VQIndex,
author = {Tuncel, Ertem and Ferhatosmanoglu, Hakan and Rose, Kenneth},
title = {VQ-index: an index structure for similarity searching in multimedia databases},
year = {2002},
isbn = {158113620X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/641007.641117},
doi = {10.1145/641007.641117},
booktitle = {Proceedings of the Tenth ACM International Conference on Multimedia},
pages = {543–552},
numpages = {10},
keywords = {approximate similarity searching, clustering, indexing, retrieved information reduction, retrieved set reduction, vector quantization},
location = {Juan-les-Pins, France},
series = {MULTIMEDIA '02}
}

@ARTICLE{AltiparmakFerhatosmanoglu2008PredQuant,
  author={Altiparmak, Fatih and Tuncel, Ertem and Ferhatosmanoglu, Hakan},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Incremental Maintenance of Online Summaries Over Multiple Streams}, 
  year={2008},
  volume={20},
  number={2},
  pages={216-229},
  keywords={Databases;Quantization;Stock markets;Query processing;Data analysis;Data security;Telecommunication network management;Intrusion detection;Data mining;Monitoring;multiple streams;Prediction;quantization;summarization;online update;multiple streams;Prediction;quantization;summarization;online update},
  doi={10.1109/TKDE.2007.190693}}

@Article{Guzun2020,
author={Guzun, Gheorghi
and Canahuate, Guadalupe},
title={High-dimensional similarity searches using query driven dynamic quantization and distributed indexing},
journal={Distributed and Parallel Databases},
year={2020},
month={Jun},
day={01},
volume={38},
number={2},
pages={255-286},
issn={1573-7578},
doi={10.1007/s10619-019-07266-x},
url={https://doi.org/10.1007/s10619-019-07266-x}
}

@INPROCEEDINGS{Berchtold2000IQTree,
  author={Berchtold, S. and Bohm, C. and Jagadish, H.V. and Kriegel, H.-P. and Sander, J.},
  booktitle={Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073)}, 
  title={Independent quantization: an index compression technique for high-dimensional data spaces}, 
  year={2000},
  volume={},
  number={},
  pages={577-588},
  keywords={Quantization;Cost function;US Department of Transportation;Indexing;Ear;Needles;Radio access networks;Encoding},
  doi={10.1109/ICDE.2000.839456}}

@article{Wang2020DeltaPQ,
author = {Wang, Runhui and Deng, Dong},
title = {DeltaPQ: lossless product quantization code compression for high dimensional similarity search},
year = {2020},
issue_date = {September 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3424573.3424580},
doi = {10.14778/3424573.3424580},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3603–3616},
numpages = {14}
}

@InProceedings{Baranchuk2023Dedrift,
    author    = {Baranchuk, Dmitry and Douze, Matthijs and Upadhyay, Yash and Yalniz, I. Zeki},
    title     = {DEDRIFT: Robust Similarity Search under Content Drift},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {11026-11035}
}

@article{Ferhatosmanoglu2006VAPlusApprox,
title = {High dimensional nearest neighbor searching},
journal = {Information Systems},
volume = {31},
number = {6},
pages = {512-540},
year = {2006},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2005.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0306437905000128},
author = {Hakan Ferhatosmanoglu and Ertem Tuncel and Divyakant Agrawal and Amr El Abbadi},
keywords = {High dimensional data, Nearest neighbor queries, Indexing, Similarity search, Approximate and progressive search, Non-uniform data, Scalability, Performance}
}

@INPROCEEDINGS{Ferhatosmanoglu2001ProgressiveSearch,
  author={Ferhatosmanoglu, H. and Tuncel, E. and Agrawal, D. and El Abbadi, A.},
  booktitle={Proceedings 17th International Conference on Data Engineering}, 
  title={Approximate nearest neighbor searching in multimedia databases}, 
  year={2001},
  volume={},
  number={},
  pages={503-511},
  keywords={Nearest neighbor searches;Multimedia databases;Humans;Information retrieval;Performance analysis;Large-scale systems;Image databases;Euclidean distance;Multimedia systems;Internet},
  doi={10.1109/ICDE.2001.914864}}

@INPROCEEDINGS{Paparrizos2022,
  author={Paparrizos, John and Edian, Ikraduya and Liu, Chunwei and Elmore, Aaron J. and Franklin, Michael J.},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Fast Adaptive Similarity Search through Variance-Aware Quantization}, 
  year={2022},
  volume={},
  number={},
  pages={2969-2983},
  keywords={Dimensionality reduction;Quantization (signal);Dictionaries;Nearest neighbor methods;Data engineering;Robustness;Explosives;quantization;similarity search;proximity search},
  doi={10.1109/ICDE53745.2022.00268}}

@article{Aguerrebere2023,
author = {Aguerrebere, Cecilia and Bhati, Ishwar Singh and Hildebrand, Mark and Tepper, Mariano and Willke, Theodore},
title = {Similarity Search in the Blink of an Eye with Compressed Indices},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611537},
doi = {10.14778/3611479.3611537},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3433–3446},
numpages = {14}
}

@article{Niu2023ResidualVectorProductQuantization,
title = {Residual Vector Product Quantization for approximate nearest neighbor search},
journal = {Expert Systems with Applications},
volume = {232},
pages = {120832},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120832},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423013349},
author = {Lushuai Niu and Zhi Xu and Longyang Zhao and Daojing He and Jianqiu Ji and Xiaoli Yuan and Mian Xue},
keywords = {Vector quantization, Residual structure, Index structure, Approximate nearest neighbor search},
}

@article{Gao2024RabitQ,
author = {Gao, Jianyang and Long, Cheng},
title = {RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654970},
doi = {10.1145/3654970},
abstract = {Searching for approximate nearest neighbors (ANN) in the high-dimensional Euclidean space is a pivotal problem. Recently, with the help of fast SIMD-based implementations, Product Quantization (PQ) and its variants can often efficiently and accurately estimate the distances between the vectors and have achieved great success in the in-memory ANN search. Despite their empirical success, we note that these methods do not have a theoretical error bound and are observed to fail disastrously on some real-world datasets. Motivated by this, we propose a new randomized quantization method named RaBitQ, which quantizes D-dimensional vectors into D-bit strings. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. In addition, we introduce efficient implementations of RaBitQ, supporting to estimate the distances with bitwise operations or SIMD-based operations. Extensive experiments on real-world datasets confirm that (1) our method outperforms PQ and its variants in terms of accuracy-efficiency trade-off by a clear margin and (2) its empirical performance is well-aligned with our theoretical analysis.},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {167},
numpages = {27},
keywords = {Johnson-Lindenstrauss transformation, approximate nearest neighbor search, quantization}
}

@inproceedings{Zhang2023,
 author = {Zhang, Hailin and Wang, Yujing and Chen, Qi and Chang, Ruiheng and Zhang, Ting and Miao, Ziming and Hou, Yingyan and Ding, Yang and Miao, Xupeng and Wang, Haonan and Pang, Bochen and Zhan, Yuefeng and Sun, Hao and Deng, Weiwei and Zhang, Qi and Yang, Fan and Xie, Xing and Yang, Mao and CUI, Bin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {54903--54917},
 publisher = {Curran Associates, Inc.},
 title = {Model-enhanced Vector Index},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ac112e8ffc4e5b9ece32070440a8ca43-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@ARTICLE{Malkov2020HNSW,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  keywords={Routing;Complexity theory;Search problems;Data models;Approximation algorithms;Biological system modeling;Brain modeling;Graph and tree search strategies;artificial intelligence;information search and retrieval;information storage and retrieval;information technology and systems;search process;graphs and networks;data structures;nearest neighbor search;big data;approximate search;similarity search},
  doi={10.1109/TPAMI.2018.2889473}}

@misc{Jaiswal2022OODDiskANN,
  title={OOD-DiskANN: Efficient and Scalable Graph ANNS for Out-of-Distribution Queries}, 
  author={Shikhar Jaiswal and Ravishankar Krishnaswamy and Ankit Garg and Harsha Vardhan Simhadri and Sheshansh Agrawal},
  year={2022},
  eprint={2211.12850},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2211.12850}, 
}

@article{Liu2024Nndes2+,
  title={Nndes2+: Enhancing K-Nearest Neighbor Graph Construction on High-Dimensional Data},
  year={2024},
  author={Liu, Yingfan and Yang, Shuo and Zhang, Hui and Dong, Longxiang and Song, Chaowei and Wang, Mingzhe and Peng, Yanguo and Li, Hui and Cui, Jiangtao},
  journal={SSRN 4698764}
}

@article{Patel2024ACORN,
author = {Patel, Liana and Kraft, Peter and Guestrin, Carlos and Zaharia, Matei},
title = {ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654923},
doi = {10.1145/3654923},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {120},
numpages = {27},
keywords = {approximate nearest neighbor search, hybrid search, vector search}
}

@article{Xu2020,
author = {Xu, Xiaoliang and Li, Chang and Wang, Yuxiang and Xia, Yixing},
title = {Multiattribute approximate nearest neighbor search based on navigable small world graph},
journal = {Concurrency and Computation: Practice and Experience},
volume = {32},
number = {24},
pages = {e5970},
keywords = {approximate nearest neighbor, K-nearest neighbor, multiattribute search, navigable small world graph},
doi = {https://doi.org/10.1002/cpe.5970},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5970},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5970},
year = {2020}
}

@article{Mohoney2023,
author = {Mohoney, Jason and Pacaci, Anil and Chowdhury, Shihabur Rahman and Mousavi, Ali and Ilyas, Ihab F. and Minhas, Umar Farooq and Pound, Jeffrey and Rekatsinas, Theodoros},
title = {High-Throughput Vector Similarity Search in Knowledge Graphs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589777},
doi = {10.1145/3589777},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {197},
numpages = {25},
keywords = {batch processing, hybrid vector similarity search, vector query processing}
}

@inproceedings {Zhang2023VBASE,
author = {Qianxi Zhang and Shuotao Xu and Qi Chen and Guoxin Sui and Jiadong Xie and Zhizhen Cai and Yaoqi Chen and Yinxuan He and Yuqing Yang and Fan Yang and Mao Yang and Lidong Zhou},
title = {{VBASE}: Unifying Online Vector Similarity Search and Relational Queries via Relaxed Monotonicity},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {377--395},
url = {https://www.usenix.org/conference/osdi23/presentation/zhang-qianxi},
publisher = {USENIX Association},
month = jul
}

@misc{Gupta2023CAPS,
  title={CAPS: A Practical Partition Index for Filtered Similarity Search}, 
  author={Gaurav Gupta and Jonah Yi and Benjamin Coleman and Chen Luo and Vihan Lakshman and Anshumali Shrivastava},
  year={2023},
  eprint={2308.15014},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/2308.15014}, 
}

@article{Wei2020AnalyticDBV,
author = {Wei, Chuangxian and Wu, Bin and Wang, Sheng and Lou, Renjie and Zhan, Chaoqun and Li, Feifei and Cai, Yuanzhe},
title = {AnalyticDB-V: a hybrid analytical engine towards query fusion for structured and unstructured data},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415541},
doi = {10.14778/3415478.3415541},
abstract = {With the explosive growth of unstructured data (such as images, videos, and audios), unstructured data analytics is widespread in a rich vein of real-world applications. Many database systems start to incorporate unstructured data analysis to meet such demands. However, queries over unstructured and structured data are often treated as disjoint tasks in most systems, where hybrid queries (i.e., involving both data types) are not yet fully supported.In this paper, we present a hybrid analytic engine developed at Alibaba, named AnalyticDB-V (ADBV), to fulfill such emerging demands. ADBV offers an interface that enables users to express hybrid queries using SQL semantics by converting unstructured data to high dimensional vectors. ADBV adopts the lambda framework and leverages the merits of approximate nearest neighbor search (ANNS) techniques to support hybrid data analytics. Moreover, a novel ANNS algorithm is proposed to improve the accuracy on large-scale vectors representing massive unstructured data. All ANNS algorithms are implemented as physical operators in ADBV, meanwhile, accuracy-aware cost-based optimization techniques are proposed to identify effective execution plans. Experimental results on both public and in-house datasets show the superior performance achieved by ADBV and its effectiveness. ADBV has been successfully deployed on Alibaba Cloud to provide hybrid query processing services for various real-world applications.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3152–3165},
numpages = {14}
}

@misc{Zhao2022,
  title={Constrained Approximate Similarity Search on Proximity Graph}, 
  author={Weijie Zhao and Shulong Tan and Ping Li},
  year={2022},
  eprint={2210.14958},
  archivePrefix={arXiv},
  primaryClass={cs.IR},
  url={https://arxiv.org/abs/2210.14958}, 
}

@InProceedings{Ferhatosmanoglu2001Constrained,
author="Ferhatosmanoglu, Hakan
and Stanoi, Ioanna
and Agrawal, Divyakant
and El Abbadi, Amr",
editor="Jensen, Christian S.
and Schneider, Markus
and Seeger, Bernhard
and Tsotras, Vassilis J.",
title="Constrained Nearest Neighbor Queries",
booktitle="Advances in Spatial and Temporal Databases",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="257--276",
isbn="978-3-540-47724-2"
}

@inproceedings{Gollapudi2023FilteredDiskANN,
author = {Gollapudi, Siddharth and Karia, Neel and Sivashankar, Varun and Krishnaswamy, Ravishankar and Begwani, Nikit and Raz, Swapnil and Lin, Yiyong and Zhang, Yin and Mahapatro, Neelam and Srinivasan, Premkumar and Singh, Amit and Simhadri, Harsha Vardhan},
title = {Filtered-DiskANN: Graph Algorithms for Approximate Nearest Neighbor Search with Filters},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583552},
doi = {10.1145/3543507.3583552},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3406–3416},
numpages = {11},
keywords = {Approximate nearest neighbor search, Dense retrieval, Filtered Search, Graph algorithms, Vector Search},
location = {Austin, TX, USA},
series = {WWW '23}
}

@misc{Wang2022,
  title={Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints}, 
  author={Mengzhao Wang and Lingwei Lv and Xiaoliang Xu and Yuxiang Wang and Qiang Yue and Jiongkang Ni},
  year={2022},
  eprint={2203.13601},
  archivePrefix={arXiv},
  primaryClass={cs.DB},
  url={https://arxiv.org/abs/2203.13601}, 
}

@inproceedings{Guu2020REALM,
author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
title = {REALM: retrieval-augmented language model pre-training},
year = {2020},
publisher = {JMLR.org},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {368},
numpages = {10},
series = {ICML'20}
}

@inproceedings{Lewis2020,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {793},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@InProceedings{Borgeaud2022RETRO,
  title = 	 {Improving Language Models by Retrieving from Trillions of Tokens},
  author =       {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2206--2240},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/borgeaud22a.html}
}

@misc{Li2022RAGSurvey,
  title={A Survey on Retrieval-Augmented Text Generation}, 
  author={Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
  year={2022},
  eprint={2202.01110},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2202.01110}, 
}

@inproceedings{Cai2022RAGTutorial,
author = {Cai, Deng and Wang, Yan and Liu, Lemao and Shi, Shuming},
title = {Recent Advances in Retrieval-Augmented Text Generation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532682},
doi = {10.1145/3477495.3532682},
abstract = {Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3417–3419},
numpages = {3},
keywords = {information retrieval, text generation},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{Zhao2024Survey,
author = {Zhao, Wayne Xin and Liu, Jing and Ren, Ruiyang and Wen, Ji-Rong},
title = {Dense Text Retrieval Based on Pretrained Language Models: A Survey},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3637870},
doi = {10.1145/3637870},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {89},
numpages = {60},
keywords = {Text retrieval, dense retrieval, pretrained language models}
}

@misc{Jiang2023,
  title={Active Retrieval Augmented Generation}, 
  author={Zhengbao Jiang and Frank F. Xu and Luyu Gao and Zhiqing Sun and Qian Liu and Jane Dwivedi-Yu and Yiming Yang and Jamie Callan and Graham Neubig},
  year={2023},
  eprint={2305.06983},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.06983}, 
}

@misc{Hardt2024,
  title={Test-Time Training on Nearest Neighbors for Large Language Models}, 
  author={Moritz Hardt and Yu Sun},
  year={2024},
  eprint={2305.18466},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.18466}, 
}

@article{Echihabi2018HydraSurvey,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
title = {The lernaean hydra of data series similarity search: an experimental evaluation of the state of the art},
year = {2018},
issue_date = {October 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3282495.3282498},
doi = {10.14778/3282495.3282498},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {112–127},
numpages = {16}
}


@Article{Ukey2023ExactSurvey,
AUTHOR = {Ukey, Nimish and Yang, Zhengyi and Li, Binghao and Zhang, Guangjian and Hu, Yiheng and Zhang, Wenjie},
TITLE = {Survey on Exact kNN Queries over High-Dimensional Data Space},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {2},
ARTICLE-NUMBER = {629},
URL = {https://www.mdpi.com/1424-8220/23/2/629},
PubMedID = {36679422},
ISSN = {1424-8220},
DOI = {10.3390/s23020629}
}

@article{Zhang2024ExperimentalAnalysis,
author = {Zhang, Hailin and Zhao, Penghao and Miao, Xupeng and Shao, Yingxia and Liu, Zirui and Yang, Tong and Cui, Bin},
title = {Experimental Analysis of Large-Scale Learnable Vector Storage Compression},
year = {2024},
issue_date = {December 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3636218.3636234},
doi = {10.14778/3636218.3636234},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {808–822},
numpages = {15}
}

@article{Matsui2018PQSurvey,
  title={[Invited Paper] A Survey of Product Quantization},
  author={Yusuke Matsui and Yusuke Uchida and Herv&eacute; J&eacute;gou and Shin'ichi Satoh},
  journal={ITE Transactions on Media Technology and Applications},
  volume={6},
  number={1},
  pages={2-10},
  year={2018},
  doi={10.3169/mta.6.2}
}

@misc{AlMamun2024LearnedIndexSurvey,
  title={A Survey of Learned Indexes for the Multi-dimensional Space}, 
  author={Abdullah Al-Mamun and Hao Wu and Qiyang He and Jianguo Wang and Walid G. Aref},
  year={2024},
  eprint={2403.06456},
  archivePrefix={arXiv},
  primaryClass={cs.DB},
  url={https://arxiv.org/abs/2403.06456}, 
}

@ARTICLE{Pan2023VectorDBMSSurvey,
   author = {{Jie Pan}, James and {Wang}, Jianguo and {Li}, Guoliang},
    title = "{Survey of Vector Database Management Systems}",
  journal = {arXiv e-prints},
 keywords = {Computer Science - Databases},
     year = 2023,
    month = oct,
      eid = {arXiv:2310.14021},
    pages = {arXiv:2310.14021},
      doi = {10.48550/arXiv.2310.14021},
archivePrefix = {arXiv},
   eprint = {2310.14021},
primaryClass = {cs.DB},
   adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv231014021J},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Li2022ServerlessSurvey,
author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, Bingsheng and Guo, Minyi},
title = {The Serverless Computing Survey: A Technical Primer for Design Architecture},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3508360},
doi = {10.1145/3508360},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {220},
numpages = {34},
keywords = {Serverless computing, architecture design, FaaS, Lambda paradigm}
}

@ARTICLE{Johnson2021GPUSearch,
  author={Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
  journal={IEEE Transactions on Big Data}, 
  title={Billion-Scale Similarity Search with GPUs}, 
  year={2021},
  volume={7},
  number={3},
  pages={535-547},
  keywords={Graphics processing units;Quantization (signal);Big Data;Indexing;Task analysis;Random access memory;Similarity search;multimedia databases;indexing methods;graphical processing units},
  doi={10.1109/TBDATA.2019.2921572}}

@inproceedings{Guo2020ScannPaper,
author = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
title = {Accelerating large-scale inference with anisotropic vector quantization},
year = {2020},
publisher = {JMLR.org},
abstract = {Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach, whose implementation is open-source, achieves state-of-the-art results on the public benchmarks available at ann-benchmarks.com.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {364},
numpages = {10},
series = {ICML'20}
}

@article{Wang2024Starling,
author = {Wang, Mengzhao and Xu, Weizhi and Yi, Xiaomeng and Wu, Songlin and Peng, Zhangyang and Ke, Xiangyu and Gao, Yunjun and Xu, Xiaoliang and Guo, Rentong and Xie, Charles},
title = {Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639269},
doi = {10.1145/3639269},
journal = {Proc. ACM Manag. Data},
month = {mar},
articleno = {14},
numpages = {27},
keywords = {approximate nearest neighbor search, block shuffling, disk-based graph index, high-dimensional vector, range search}
}

@inproceedings{Wang2021Milvus,
author = {Wang, Jianguo and Yi, Xiaomeng and Guo, Rentong and Jin, Hai and Xu, Peng and Li, Shengjun and Wang, Xiangyu and Guo, Xiangzhou and Li, Chengming and Xu, Xiaohai and Yu, Kun and Yuan, Yuxing and Zou, Yinghao and Long, Jiquan and Cai, Yudong and Li, Zhenxiang and Zhang, Zhifeng and Mo, Yihua and Gu, Jun and Jiang, Ruiyi and Wei, Yi and Xie, Charles},
title = {Milvus: A Purpose-Built Vector Data Management System},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457550},
doi = {10.1145/3448016.3457550},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2614–2627},
numpages = {14},
keywords = {data science, heterogeneous computing, high-dimensional similarity search, machine learning, vector database},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{Kraska2018,
author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
title = {The Case for Learned Index Structures},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196909},
doi = {10.1145/3183713.3196909},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {489–504},
numpages = {16},
keywords = {neural net, mixture of experts, linear regression, learned index structure, learned index, learned data structures, index structures, hash-map, cdf, bloom-filter, b-tree},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{Nathan2020,
author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
title = {Learning Multi-Dimensional Indexes},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380579},
doi = {10.1145/3318464.3380579},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {985–1000},
numpages = {16},
keywords = {primary index, multi-dimensional, indexing, in-memory, databases},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{Ding2020Tsunami,
author = {Ding, Jialin and Nathan, Vikram and Alizadeh, Mohammad and Kraska, Tim},
title = {Tsunami: a learned multi-dimensional index for correlated data and skewed workloads},
year = {2020},
issue_date = {October 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3425879.3425880},
doi = {10.14778/3425879.3425880},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {74–86},
numpages = {13}
}

@article{Kurmanji2023,
author = {Kurmanji, Meghdad and Triantafillou, Peter},
title = {Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588713},
doi = {10.1145/3588713},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {33},
numpages = {27},
keywords = {knowledge distillation, learned databases, out of distribution detection, transfer learning}
}

@INPROCEEDINGS{Madden2022,
year = {2022},
author = {Madden, Samuel and Ding, Jialin and Kraska, Tim and Sudhir, Sivaprasad and Cohen, David and Mattson, Timothy},
size = {8 p. accepted version},
language = {en},
title = {Self-Organizing Data Containers},
booktitle = {12th Annual Conference on Innovative Data Systems Research (CIDR 2022)},
url = {https://www.cidrdb.org/cidr2022/papers/p44-madden.pdf}
}

@misc{Stoian2021,
title={Towards Practical Learned Indexing}, 
author={Mihail Stoian and Andreas Kipf and Ryan Marcus and Tim Kraska},
year={2021},
eprint={2108.05117},
archivePrefix={arXiv},
primaryClass={cs.DB},
url={https://arxiv.org/abs/2108.05117}, 
}

@inproceedings{Kipf2022LSI,
author = {Kipf, Andreas and Horn, Dominik and Pfeil, Pascal and Marcus, Ryan and Kraska, Tim},
title = {LSI: a learned secondary index structure},
year = {2022},
isbn = {9781450393775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533702.3534912},
doi = {10.1145/3533702.3534912},
booktitle = {Proceedings of the Fifth International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
articleno = {4},
numpages = {5},
location = {Philadelphia, Pennsylvania},
series = {aiDM '22}
}

@article{Sabek2022,
author = {Sabek, Ibrahim and Vaidya, Kapil and Horn, Dominik and Kipf, Andreas and Mitzenmacher, Michael and Kraska, Tim},
title = {Can Learned Models Replace Hash Functions?},
year = {2022},
issue_date = {November 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3570690.3570702},
doi = {10.14778/3570690.3570702},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {532–545},
numpages = {14}
}

@INPROCEEDINGS{Liu2021LHist,
  author={Liu, Qiyu and Shen, Yanyan and Chen, Lei},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, 
  title={LHist: Towards Learning Multi-dimensional Histogram for Massive Spatial Data}, 
  year={2021},
  volume={},
  number={},
  pages={1188-1199},
  keywords={Histograms;Query processing;Estimation;Machine learning;Benchmark testing;Data structures;Spatial databases;Data Synopsis;Multi-dimensional Histogram;Learned Index},
  doi={10.1109/ICDE51399.2021.00107}}

@InProceedings{Wang2021,
author="Wang, Youyun
and Tang, Chuzhe
and Yao, Xujia",
editor="U, Leong Hou
and Spaniol, Marc
and Sakurai, Yasushi
and Chen, Junying",
title="A Distribution-Aware Training Scheme for Learned Indexes",
booktitle="Web and Big Data",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="143--157",
isbn="978-3-030-85899-5"
}

@article{Qi2020,
author = {Qi, Jianzhong and Liu, Guanli and Jensen, Christian S. and Kulik, Lars},
title = {Effectively learning spatial indices},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407829},
doi = {10.14778/3407790.3407829},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2341–2354},
numpages = {14}
}

@inproceedings{AlMamun2020,
author = {Al-Mamun, Abdullah and Wu, Hao and Aref, Walid G.},
title = {A Tutorial on Learned Multi-dimensional Indexes},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3426358},
doi = {10.1145/3397536.3426358},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {1–4},
numpages = {4},
keywords = {Learned Indexes, Multi-dimensional, Spatial},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{Kraska2019SageDB,
title	= {SageDB: A Learned Database System},
author	= {Tim Kraska and Mohammad Alizadeh and Alex Beutel and Ed H. Chi and Jialin Ding and Ani Kristo and Guillaume Leclerc and Samuel Madden and Hongzi Mao and Vikram Nathan},
booktitle = {9th Annual Conference on Innovative Data Systems Research (CIDR 2019)},
year	= {2019}, 
url= {https://www.cidrdb.org/cidr2019/papers/p117-kraska-cidr19.pdf}}

@inproceedings{Ding2021InstanceOptimized,
author = {Ding, Jialin and Minhas, Umar Farooq and Chandramouli, Badrish and Wang, Chi and Li, Yinan and Li, Ying and Kossmann, Donald and Gehrke, Johannes and Kraska, Tim},
title = {Instance-Optimized Data Layouts for Cloud Analytics Workloads},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457270},
doi = {10.1145/3448016.3457270},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {418–431},
numpages = {14},
keywords = {instance-optimized databases, cloud analytics},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{Su2024Vexless,
author = {Su, Yongye and Sun, Yinqi and Zhang, Minjia and Wang, Jianguo},
title = {Vexless: A Serverless Vector Data Management System Using Cloud Functions},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654990},
doi = {10.1145/3654990},
journal = {Proc. ACM Manag. Data},
month = {may},
articleno = {187},
numpages = {26},
keywords = {cloud functions, serverless computing, serverless databases, vector databases}
}

@INPROCEEDINGS{Kesavan2023Firestore,
  author={Kesavan, Ram and Gay, David and Thevessen, Daniel and Shah, Jimit and Mohan, C.},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Firestore: The NoSQL Serverless Database for the Application Developer}, 
  year={2023},
  volume={},
  number={},
  pages={3376-3388},
  keywords={Databases;Scalability;Ecosystems;Prototypes;Mobile communication;Real-time systems;User experience;cloud;database;mobile and web applications;continuous queries},
  doi={10.1109/ICDE55515.2023.00259}}

@INPROCEEDINGS{Gupta2023ReliableTransactions,
author = {Suyash Gupta and Sajjad Rahnama and Erik Linsenmayer and Faisal Nawab and Mohammad Sadoghi},
booktitle = {2023 IEEE 39th International Conference on Data Engineering (ICDE)},
title = {Reliable Transactions in Serverless-Edge Architecture},
year = {2023},
volume = {},
issn = {},
pages = {301-314},
keywords = {fault tolerance;protocols;philosophical considerations;databases;fault tolerant systems;collaboration;computer architecture},
doi = {10.1109/ICDE55515.2023.00030},
url = {https://doi.ieeecomputersociety.org/10.1109/ICDE55515.2023.00030},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {apr}
}

@inproceedings{Shrivastava2014ALSH,
author = {Shrivastava, Anshumali and Li, Ping},
title = {Asymmetric LSH (ALSH) for sublinear time Maximum Inner Product Search (MIPS)},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2321–2329},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@misc{Zhao2023LLMSurvey,
title={A Survey of Large Language Models}, 
author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
year={2023},
eprint={2303.18223},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2303.18223}, 
}

@inproceedings{Xiao2024Efficient,
title={Efficient Streaming Language Models with Attention Sinks},
author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NG7sS51zVF}
}

@article{Chang2024LLMEvaluationSurvey,
author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
title = {A Survey on Evaluation of Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3641289},
doi = {10.1145/3641289},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {mar},
articleno = {39},
numpages = {45},
keywords = {Large language models, evaluation, model assessment, benchmark}
}

@article{Gao2023DCOs,
author = {Gao, Jianyang and Long, Cheng},
title = {High-Dimensional Approximate Nearest Neighbor Search: with Reliable and Efficient Distance Comparison Operations},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589282},
doi = {10.1145/3589282},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {137},
numpages = {27},
keywords = {distance comparison operation, high-dimensional nearest neighbor search, random projection}
}

@article{Mic2024RelationalSimilarity,
title = {Filtering with relational similarity},
journal = {Information Systems},
volume = {122},
pages = {102345},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102345},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000036},
author = {Vladimir Mic and Pavel Zezula},
keywords = {Searching and sorting, Metric space searching, Similarity Search, Metric filtering, Data dependent filtering},
}

@article{Zeng2023ColumnarFormats,
author = {Zeng, Xinyu and Hui, Yulong and Shen, Jiahong and Pavlo, Andrew and McKinney, Wes and Zhang, Huanchen},
title = {An Empirical Evaluation of Columnar Storage Formats},
year = {2023},
issue_date = {October 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3626292.3626298},
doi = {10.14778/3626292.3626298},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {148–161},
numpages = {14}
}

@article{Afroozeh2023ALP,
author = {Afroozeh, Azim and Kuffo, Leonardo X. and Boncz, Peter},
title = {ALP: Adaptive Lossless floating-Point Compression},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626717},
doi = {10.1145/3626717},
journal = {Proc. ACM Manag. Data},
month = {dec},
articleno = {230},
numpages = {26},
keywords = {big data formats, columnar storage, floating point compression, lightweight compression, lossless compression, vectorized execution}
}

@article{Kuschewski2023BtrBlocks,
author = {Kuschewski, Maximilian and Sauerwein, David and Alhomssi, Adnan and Leis, Viktor},
title = {BtrBlocks: Efficient Columnar Compression for Data Lakes},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589263},
doi = {10.1145/3589263},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {118},
numpages = {26},
keywords = {columnar storage, compression, data lake, query processing}
}

@inproceedings{Chakrabarti2000,
author = {Chakrabarti, Kaushik and Mehrotra, Sharad},
title = {Local Dimensionality Reduction: A New Approach to Indexing High Dimensional Spaces},
year = {2000},
isbn = {1558607153},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 26th International Conference on Very Large Data Bases},
pages = {89–100},
numpages = {12},
series = {VLDB '00}
}

@inproceedings{Lian2008LpNorm,
author = {Lian, Xiang and Chen, Lei},
title = {Similarity Search in Arbitrary Subspaces Under Lp-Norm},
year = {2008},
isbn = {9781424418367},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDE.2008.4497440},
doi = {10.1109/ICDE.2008.4497440},
booktitle = {Proceedings of the 2008 IEEE 24th International Conference on Data Engineering},
pages = {317–326},
numpages = {10},
series = {ICDE '08}
}

@inproceedings{Lee2022RegularizedAutoencoders,
title={Regularized Autoencoders for Isometric Representation Learning},
author={Yonghyeon Lee and Sangwoong Yoon and MinJun Son and Frank C. Park},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=mQxt8l7JL04}
}

@inproceedings{Rayhan2023SIMD,
author = {Rayhan, Yeasir and Aref, Walid G.},
title = {SIMD-ified R-tree Query Processing and Optimization},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625610},
doi = {10.1145/3589132.3625610},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {37},
numpages = {10},
keywords = {query optimization, R-tree, spatial query processing, single instruction multiple data (SIMD)},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@ARTICLE{Zhang2023NIOT,
author={Zhang, Zining and Chen, Yao and He, Bingsheng and Zhang, Zhenjie},
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={NIOT: A Novel Inference Optimization of Transformers on Modern CPUs}, 
year={2023},
volume={34},
number={6},
pages={1982-1995},
keywords={Transformers;Optimization;Layout;Instruction sets;Computational modeling;Bit error rate;Resource management;Acceleration;CPU;neural network;transformers},
doi={10.1109/TPDS.2023.3269530}}

@Inbook{Guzun2014Slicing,
author="Guzun, Gheorghi
and Tosado, Joel
and Canahuate, Guadalupe",
editor="Hameurlain, Abdelkader
and K{\"u}ng, Josef
and Wagner, Roland",
title="Slicing the Dimensionality: Top-k Query Processing for High-Dimensional Spaces",
bookTitle="Transactions on Large-Scale Data- and Knowledge-Centered Systems XIV",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="26--50",
isbn="978-3-662-45714-6",
doi="10.1007/978-3-662-45714-6_2",
url="https://doi.org/10.1007/978-3-662-45714-6_2"
}

@InProceedings{Malik2009MLRIndex,
author="Malik, Rahul
and Kim, Sangkyum
and Jin, Xin
and Ramachandran, Chandrasekar
and Han, Jiawei
and Gupta, Indranil
and Nahrstedt, Klara",
editor="Winslett, Marianne",
title="MLR-Index: An Index Structure for Fast and Scalable Similarity Search in High Dimensions",
booktitle="Scientific and Statistical Database Management",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="167--184",
isbn="978-3-642-02279-1"
}

@InProceedings{Bernecker2010Subspace,
author="Bernecker, Thomas
and Emrich, Tobias
and Graf, Franz
and Kriegel, Hans-Peter
and Kr{\"o}ger, Peer
and Renz, Matthias
and Schubert, Erich
and Zimek, Arthur",
editor="Gertz, Michael
and Lud{\"a}scher, Bertram",
title="Subspace Similarity Search: Efficient k-NN Queries in Arbitrary Subspaces",
booktitle="Scientific and Statistical Database Management",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="555--564",
isbn="978-3-642-13818-8"
}

@inproceedings{Tung2006KNMatch,
author = {Tung, Anthony K. H. and Zhang, Rui and Koudas, Nick and Ooi, Beng Chin},
title = {Similarity search: a matching based approach},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {631–642},
numpages = {12},
location = {Seoul, Korea},
series = {VLDB '06}
}

@article{10.1145/276305.276318,
author = {Berchtold, Stefan and B\"{o}hm, Christian and Kriegal, Hans-Peter},
title = {The pyramid-technique: towards breaking the curse of dimensionality},
year = {1998},
issue_date = {June 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/276305.276318},
doi = {10.1145/276305.276318},
abstract = {In this paper, we propose the Pyramid-Technique, a new indexing method for high-dimensional data spaces. The Pyramid-Technique is highly adapted to range query processing using the maximum metric Lmax. In contrast to all other index structures, the performance of the Pyramid-Technique does not deteriorate when processing range queries on data of higher dimensionality. The Pyramid-Technique is based on a special partitioning strategy which is optimized for high-dimensional data. The basic idea is to divide the data space first into 2d pyramids sharing the center point of the space as a top. In a second step, the single pyramids are cut into slices parallel to the basis of the pyramid. These slices from the data pages. Furthermore, we show that this partition provides a mapping from the given d-dimensional space to a 1-dimensional space. Therefore, we are able to use a B+-tree to manage the transformed data. As an analytical evaluation of our technique for hypercube range queries and uniform data distribution shows, the Pyramid-Technique clearly outperforms index structures using other partitioning strategies. To demonstrate the practical relevance of our technique, we experimentally compared the Pyramid-Technique with the X-tree, the Hilbert R-tree, and the Linear Scan. The results of our experiments using both, synthetic and real data, demonstrate that the Pyramid-Technique outperforms the X-tree and the Hilbert R-tree by a factor of up to 14 (number of page accesses) and up to 2500 (total elapsed time) for range queries.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {142–153},
numpages = {12}
}

@inproceedings{Berchtold1998Pyramid,
author = {Berchtold, Stefan and B\"{o}hm, Christian and Kriegal, Hans-Peter},
title = {The pyramid-technique: towards breaking the curse of dimensionality},
year = {1998},
isbn = {0897919955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276304.276318},
doi = {10.1145/276304.276318},
booktitle = {Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data},
pages = {142–153},
numpages = {12},
location = {Seattle, Washington, USA},
series = {SIGMOD '98}
}

@InProceedings{Li2011MapReduce,
author="Li, Rui
and Ju, Li
and Peng, Zhuo
and Yu, Zhiwei
and Wang, Chaokun",
editor="Du, Xiaoyong
and Fan, Wenfei
and Wang, Jianmin
and Peng, Zhiyong
and Sharaf, Mohamed A.",
title="Batch Text Similarity Search with MapReduce",
booktitle="Web Technologies and Applications",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="412--423",
isbn="978-3-642-20291-9"
}

@article{Nahshan2021,
author = {Nahshan, Yury and Chmiel, Brian and Baskin, Chaim and Zheltonozhskii, Evgenii and Banner, Ron and Bronstein, Alex M. and Mendelson, Avi},
title = {Loss aware post-training quantization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {110},
number = {11–12},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-021-06053-z},
doi = {10.1007/s10994-021-06053-z},
journal = {Mach. Learn.},
month = {dec},
pages = {3245–3262},
numpages = {18},
keywords = {Post-training quantization, Convolutional neural networks}
}

@ARTICLE{Shao2008VideoSearch,
  author={Shao, Jie and Huang, Zi and Shen, Heng Tao and Zhou, Xiaofang and Lim, Ee-Peng and Li, Yijun},
  journal={IEEE Transactions on Multimedia}, 
  title={Batch Nearest Neighbor Search for Video Retrieval}, 
  year={2008},
  volume={10},
  number={3},
  pages={409-420},
  keywords={Nearest neighbor searches;Neural networks;Spatial databases;Query processing;Indexing;Feature extraction;Image segmentation;Information retrieval;Content based retrieval;Multimedia databases;Content-based retrieval;high-dimensional indexing;multimedia databases;query processing},
  doi={10.1109/TMM.2008.917339}}

@article{Zhang2022Parrot,
author = {Zhang, Liang and Alghamdi, Noura and Zhang, Huayi and Eltabakh, Mohamed Y. and Rundensteiner, Elke A.},
title = {PARROT: pattern-based correlation exploitation in big partitioned data series},
year = {2022},
issue_date = {May 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {3},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-022-00767-9},
doi = {10.1007/s00778-022-00767-9},
journal = {The VLDB Journal},
month = {oct},
pages = {665–688},
numpages = {24},
keywords = {Big data series, Correlation-aware indexing, Approximate similarity queries}
}

@inproceedings{Xu2023SPFresh,
author = {Xu, Yuming and Liang, Hengyu and Li, Jin and Xu, Shuotao and Chen, Qi and Zhang, Qianxi and Li, Cheng and Yang, Ziyue and Yang, Fan and Yang, Yuqing and Cheng, Peng and Yang, Mao},
title = {SPFresh: Incremental In-Place Update for Billion-Scale Vector Search},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613166},
doi = {10.1145/3600006.3613166},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {545–561},
numpages = {17},
keywords = {vector search, incremental update, billion-scale},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@article{Azizi2023ELPIS,
author = {Azizi, Ilias and Echihabi, Karima and Palpanas, Themis},
title = {ELPIS: Graph-Based Similarity Search for Scalable Data Science},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583166},
doi = {10.14778/3583140.3583166},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {1548–1559},
numpages = {12}
}

@article{Echihabi2022Hercules,
author = {Echihabi, Karima and Fatourou, Panagiota and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
title = {Hercules against data series similarity search},
year = {2022},
issue_date = {June 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3547305.3547308},
doi = {10.14778/3547305.3547308},
journal = {Proc. VLDB Endow.},
month = {jun},
pages = {2005–2018},
numpages = {14}
}

@article{Wang2020PPQ,
author = {Wang, Shuang and Ferhatosmanoglu, Hakan},
title = {PPQ-trajectory: spatio-temporal quantization for querying in large trajectory repositories},
year = {2020},
issue_date = {October 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3425879.3425891},
doi = {10.14778/3425879.3425891},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {215–227},
numpages = {13}
}

@inproceedings{Echihabi2020Scalable,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis},
title = {Scalable Machine Learning on High-Dimensional Vectors: From Data Series to Deep Network Embeddings},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405989},
doi = {10.1145/3405962.3405989},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {1–6},
numpages = {6},
keywords = {time series, similarity search, machine learning, high-dimensional vectors, embeddings, deep learning, data series},
location = {Biarritz, France},
series = {WIMS 2020}
}

@misc{Karpukhin2020DPR,
title={Dense Passage Retrieval for Open-Domain Question Answering}, 
author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
year={2020},
eprint={2004.04906},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2004.04906}, 
}

@inproceedings{Jain2019Lakehouse,
title	= {Analyzing and Comparing Lakehouse Storage Systems},
author = {Paras Jain and Peter Kraft and Conor Power and  Tathagata Das and Ion Stoica and Matei Zaharia},
booktitle = {13th Annual Conference on Innovative Data Systems Research (CIDR 2023)},
year	= {2019}, 
url= {https://www.cidrdb.org/cidr2023/papers/p92-jain.pdf}}

@article{Nievergelt1984GridFile,
author = {Nievergelt, J. and Hinterberger, Hans and Sevcik, Kenneth C.},
title = {The Grid File: An Adaptable, Symmetric Multikey File Structure},
year = {1984},
issue_date = {March 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/348.318586},
doi = {10.1145/348.318586},
journal = {ACM Trans. Database Syst.},
month = {mar},
pages = {38–71},
numpages = {34}
}

@inproceedings{Robinson1981KDBTree,
author = {Robinson, John T.},
title = {The K-D-B-tree: a search structure for large multidimensional dynamic indexes},
year = {1981},
isbn = {0897910400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/582318.582321},
doi = {10.1145/582318.582321},
booktitle = {Proceedings of the 1981 ACM SIGMOD International Conference on Management of Data},
pages = {10–18},
numpages = {9},
location = {Ann Arbor, Michigan},
series = {SIGMOD '81}
}

@Article{Finkel1974QuadTree,
author={Finkel, R. A.
and Bentley, J. L.},
title={Quad trees a data structure for retrieval on composite keys},
journal={Acta Informatica},
year={1974},
month={Mar},
day={01},
volume={4},
number={1},
pages={1-9},
issn={1432-0525},
doi={10.1007/BF00288933},
url={https://doi.org/10.1007/BF00288933}
}

@inproceedings{Guttman1984RTree, 
author = {Guttman, Antonin}, 
title = {R-trees: a dynamic index structure for spatial searching}, 
year = {1984}, 
isbn = {0897911288}, 
publisher = {Association for Computing Machinery}, 
address = {New York, NY, USA}, 
url = {https://doi.org/10.1145/602259.602266}, 
doi = {10.1145/602259.602266}, 
booktitle = {Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data}, 
pages = {47–57}, 
numpages = {11}, 
location = {Boston, Massachusetts}, 
series = {SIGMOD '84} }

@inproceedings{Sellis1987R+Tree,
author = {Sellis, Timos K. and Roussopoulos, Nick and Faloutsos, Christos},
title = {The R+-Tree: A Dynamic Index for Multi-Dimensional Objects},
year = {1987},
isbn = {093461346X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 13th International Conference on Very Large Data Bases},
pages = {507–518},
numpages = {12},
series = {VLDB '87}
}

@inproceedings{Beckmann1990R*Tree,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-tree: an efficient and robust access method for points and rectangles},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98741},
doi = {10.1145/93597.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {322–331},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@inproceedings{Berchtold1996XTree,
author = {Berchtold, Stefan and Keim, Daniel A. and Kriegel, Hans-Peter},
title = {The X-tree: An Index Structure for High-Dimensional Data},
year = {1996},
isbn = {1558603824},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 22th International Conference on Very Large Data Bases},
pages = {28–39},
numpages = {12},
series = {VLDB '96}
}

@inproceedings{Katayama1997SRTree, 
author = {Katayama, Norio and Satoh, Shin'ichi}, 
title = {The SR-tree: an index structure for high-dimensional nearest neighbor queries}, 
year = {1997}, 
isbn = {0897919114}, 
publisher = {Association for Computing Machinery}, 
address = {New York, NY, USA}, 
url = {https://doi.org/10.1145/253260.253347}, 
doi = {10.1145/253260.253347}, 
booktitle = {Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data}, pages = {369–380}, numpages = {12}, location = {Tucson, Arizona, USA}, series = {SIGMOD '97} }

@inproceedings{Ciaccia1997MTree,
author = {Ciaccia, Paolo and Patella, Marco and Zezula, Pavel},
title = {M-tree: An Efficient Access Method for Similarity Search in Metric Spaces},
year = {1997},
isbn = {1558604707},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 23rd International Conference on Very Large Data Bases},
pages = {426–435},
numpages = {10},
series = {VLDB '97}
}

@Article{Lin1994TVTree,
author={Lin, King-Ip
and Jagadish, H. V.
and Faloutsos, Christos},
title={The TV-tree: An index structure for high-dimensional data},
journal={The VLDB Journal},
year={1994},
month={Oct},
day={01},
volume={3},
number={4},
pages={517-542},
issn={0949-877X},
doi={10.1007/BF01231606},
url={https://doi.org/10.1007/BF01231606}
}

@article{Lomet1990HBTree,
author = {Lomet, David B. and Salzberg, Betty},
title = {The hB-tree: a multiattribute indexing method with good guaranteed performance},
year = {1990},
issue_date = {Dec. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/99935.99949},
doi = {10.1145/99935.99949},
journal = {ACM Trans. Database Syst.},
month = {dec},
pages = {625–658},
numpages = {34}
}

@misc{douze2024faisslibraryivfsq8,
      title={The Faiss library}, 
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08281}, 
}

@online{AWSLambdaGeneralOLD,
  author = {Amazon},
  title = {AWS Lambda},
  url = {https://aws.amazon.com/lambda/},
  note = {Accessed: 2023-11-30}
}

@online{AWSLambdaGeneral,
  author = {Amazon},
  title = {AWS Lambda},
  url = {https://aws.amazon.com/lambda/},
  year = {2025}
}

@online{GoogleCloudFunctionsGeneral,
  author = {Google},
  title = {Google Cloud Functions},
  url = {https://cloud.google.com/functions?hl=en},
  note = {Accessed: 2023-11-30}
}

@online{AzureFunctionsGeneral,
  author = {Microsoft},
  title = {Azure Functions Overview},
  url = {https://azure.microsoft.com/en-gb/products/functions},
  note = {Accessed: 2023-11-30}
}

@inproceedings{Muller2020Lambada,
author = {M\"{u}ller, Ingo and Marroqu\'{\i}n, Renato and Alonso, Gustavo},
title = {Lambada: Interactive Data Analytics on Cold Data Using Serverless Cloud Infrastructure},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389758},
doi = {10.1145/3318464.3389758},
abstract = {Serverless computing has recently attracted a lot of attention from research and industry due to its promise of ultimate elasticity and operational simplicity. However, there is no consensus yet on whether or not the approach is suitable for data processing. In this paper, we present Lambada, a serverless distributed data processing framework designed to explore how to perform data analytics on serverless computing. In our analysis, supported with extensive experiments, we show in which scenarios serverless makes sense from an economic and performance perspective. We address several important technical questions that need to be solved to support data analytics and present examples from several domains where serverless offers a cost and performance advantage over existing solutions.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {115–130},
numpages = {16},
keywords = {data lake, interactive analytics, serverless computing, serverless functions, cloud computing, elasticity},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{Perron2020,
author = {Perron, Matthew and Castro Fernandez, Raul and DeWitt, David and Madden, Samuel},
title = {Starling: A Scalable Query Engine on Cloud Functions},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380609},
doi = {10.1145/3318464.3380609},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {131–141},
numpages = {11},
keywords = {FAAS, OLAP, serverless, cloud},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@INPROCEEDINGS{Gillis2021,
  author={Yu, Minchen and Jiang, Zhifeng and Ng, Hok Chun and Wang, Wei and Chen, Ruichuan and Li, Bo},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)}, 
  title={Gillis: Serving Large Neural Networks in Serverless Functions with Automatic Model Partitioning}, 
  year={2021},
  volume={},
  number={},
  pages={138-148},
  doi={10.1109/ICDCS51616.2021.00022}}

@inproceedings{Jarachanthan2021AMPS,
author = {Jarachanthan, Jananie and Chen, Li and Xu, Fei and Li, Bo},
title = {AMPS-Inf: Automatic Model Partitioning for Serverless Inference with Cost Efficiency},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472501},
doi = {10.1145/3472456.3472501},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {14},
numpages = {12},
keywords = {machine learning inference, serverless computing, cost efficiency},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@INPROCEEDINGS{Oakley2024FSDInference,
  author={Oakley, Joe and Ferhatosmanoglu, Hakan},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication}, 
  year={2024},
  volume={},
  number={},
  pages={2109-2122},
  keywords={Publish subscribe systems;Costs;Runtime;Computational modeling;Scalability;Serverless computing;Publish-subscribe;Serverless Computing;Machine Learning Inference;Deep Neural Network;Batch Inference;Serverless Communication;Distributed Inference;Function-as-a-Service;Publish-Subscribe;Message Queues;Object Storage;Hypergraph Partitioning;Cost Modelling},
  doi={10.1109/ICDE60146.2024.00168}}


@inproceedings{weber2000trading,
  title={Trading quality for time with nearest-neighbor search},
  author={Weber, Roger and B{\"o}hm, Klemens},
  booktitle={International Conference on Extending Database Technology},
  pages={21--35},
  year={2000},
  organization={Springer}
}

@Article{Oakley2024ForesightPlus,
author={Oakley, Joe
and Conlan, Chris
and Demirci, Gunduz Vehbi
and Sfyridis, Alexandros
and Ferhatosmanoglu, Hakan},
title={Foresight plus: serverless spatio-temporal traffic forecasting},
journal={GeoInformatica},
year={2024},
month={Apr},
day={26},
issn={1573-7624},
doi={10.1007/s10707-024-00517-9},
url={https://doi.org/10.1007/s10707-024-00517-9}
}

@software{Levy-Kramer_k-means-constrained_2018,
  author = {Levy-Kramer, Josh},
  month = apr,
  title = {{k-means-constrained}},
  url = {https://github.com/joshlk/k-means-constrained},
  year = {2018}
}

@article{bradley2000constrained,
  title={Constrained k-means clustering},
  author={Bradley, Paul S and Bennett, Kristin P and Demiriz, Ayhan},
  journal={Microsoft Research, Redmond},
  volume={20},
  number={0},
  pages={0},
  year={2000}
}

@inproceedings{Weber1998Short,
author = {Weber, Roger and Schek, Hans-J\"{o}rg and Blott, Stephen},
title = {A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces},
year = {1998},
isbn = {1558605665},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 24rd International Conference on Very Large Data Bases},
pages = {194–205},
numpages = {12},
series = {VLDB '98}
}

@article{karhunen1947under,
  title={Under lineare methoden in der wahr scheinlichkeitsrechnung},
  author={Karhunen, Kari},
  journal={Annales Academiae Scientiarun Fennicae Series A1: Mathematia Physica},
  volume={47},
  year={1947}
}

@article{loeve1948functions,
  title={Functions aleatoires du second ordre},
  author={Loeve, Michel},
  journal={Processus stochastique et mouvement Brownien},
  pages={366--420},
  year={1948},
  publisher={Gauthier-Villars}
}

@article{dony2001karhunen,
  title={Karhunen-loeve transform},
  author={Dony, R and others},
  journal={The transform and data compression handbook},
  volume={1},
  number={1-34},
  pages={29},
  year={2001},
  publisher={CRC Press Boca Raton, London, New York, Washington, DC}
}

@book{gersho2012vector,
  title={Vector quantization and signal compression},
  author={Gersho, Allen and Gray, Robert M},
  volume={159},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{MARUKATAT20131101,
title = {Fast nearest neighbor retrieval using randomized binary codes and approximate Euclidean distance},
journal = {Pattern Recognition Letters},
volume = {34},
number = {9},
pages = {1101-1107},
year = {2013},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167865513000937},
author = {Sanparith Marukatat and Ithipan Methasate},
keywords = {Fast nearest neighbor search, Binary code indexing, Hamming distance, Approximate Euclidean distance},
abstract = {This paper investigates the use of binary codes in fast nearest neighbor retrieval for multi-dimensional dataset. The proposed method is based on a relation between the Euclidean distance and the Hamming distance between binary codes obtained from random projections of the two vectors. This relation allows approximating multi-dimensional Euclidean distance rapidly. The accuracy of the proposed approximation depends mainly on the length of the binary codes and not on the dimension of the input vector. Experimental results show that the proposed method yields an accurate approximation of the true distance. Fast search technique using the proposed distance is also presented. This technique is compared to other existing search methods. The experimental results are promising.}
}

@inproceedings{sanca2024efficient,
  title={Efficient Data Access Paths for Mixed Vector-Relational Search},
  author={Sanca, Viktor and Ailamaki, Anastasia},
  booktitle={Proceedings of the 20th International Workshop on Data Management on New Hardware},
  pages={1--9},
  year={2024}
}

@online{FN-AWS-Lambda-Pricing-OLD,
  author = {Amazon},
  title = {AWS Lambda Pricing},
  url = {https://aws.amazon.com/lambda/pricing},
  note = {Accessed: 2024-10-13}
}

@online{FN-AWS-Lambda-Memory-Compute-OLD,
  author = {Amazon},
  title = {AWS Lambda Memory and Computing Power},
  url = {https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html},
  note = {Accessed: 2024-10-13}
}


@online{FN-AWS-Lambda-Pricing,
  author = {Amazon},
  title = {AWS Lambda Pricing},
  url = {https://aws.amazon.com/lambda/pricing},
  year = {2024}
}

@online{FN-AWS-Lambda-Memory-Compute,
  author = {Amazon},
  title = {AWS Lambda Memory and Computing Power},
  url = {https://docs.aws.amazon.com/lambda/latest/operatorguide/computing-power.html},
  year = {2024}
}

@online{AWS-Lambda-1770MB,
  author = {Amazon},
  title = {What's New in Serverless, AWS re:Invent 2020},
  url = {https://www.youtube.com/watch?v=aW5EtKHTMuQ&t=339s},
  year = {2024}
}

@online{Pinecone-Serverless,
  author = {Pinecone},
  title = {Pinecone Serverless},
  url = {https://www.pinecone.io/product/},
  year = {2024}
}

@online{Pinecone-Serverless-2,
  author = {Pinecone},
  title = {Reimagining the vector database to enable knowledgeable AI},
  url = {https://www.pinecone.io/blog/serverless-architecture/},
  year = {2024}
}

@article{li2019approximateLID,
  title={Approximate nearest neighbor search on high dimensional data—experiments, analyses, and improvement},
  author={Li, Wen and Zhang, Ying and Sun, Yifang and Wang, Wei and Li, Mingjie and Zhang, Wenjie and Lin, Xuemin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={32},
  number={8},
  pages={1475--1488},
  year={2019},
  publisher={IEEE}
}


@article{fu2021high-LID-Higher,
  title={High dimensional similarity search with satellite system graph: Efficiency, scalability, and unindexed query compatibility},
  author={Fu, Cong and Wang, Changxu and Cai, Deng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={8},
  pages={4139--4150},
  year={2021},
  publisher={IEEE}
}

------- Hashing -------
@article{near-optimal-hashing-hashing-1,
author = {Andoni, Alexandr and Indyk, Piotr},
title = {Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327494},
doi = {10.1145/1327452.1327494},
abstract = {In this article, we give an overview of efficient algorithms for the approximate and exact nearest neighbor problem. The goal is to preprocess a dataset of objects (e.g., images) so that later, given a new query object, one can quickly return the dataset object that is most similar to the query. The problem is of significant interest in a wide variety of areas.},
journal = {Commun. ACM},
month = jan,
pages = {117–122},
numpages = {6}
}

@article{db-lsh-hashing-2,
  title={DB-LSH 2.0: Locality-sensitive hashing with query-based dynamic bucketing},
  author={Tian, Yao and Zhao, Xi and Zhou, Xiaofang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@inproceedings{AndoniOptimal-Hashing-3,
author = {Andoni, Alexandr and Razenshteyn, Ilya},
title = {Optimal Data-Dependent Hashing for Approximate Near Neighbors},
year = {2015},
isbn = {9781450335362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746539.2746553},
doi = {10.1145/2746539.2746553},
abstract = {We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an n-point dataset in a d-dimensional space our data structure achieves query time O(d ⋅ nρ+o(1)) and space O(n1+ρ+o(1) + d ⋅ n), where ρ=1/(2c2-1) for the Euclidean space and approximation c>1. For the Hamming space, we obtain an exponent of ρ=1/(2c-1). Our result completes the direction set forth in (Andoni, Indyk, Nguyen, Razenshteyn 2014) who gave a proof-of-concept that data-dependent hashing can outperform classic Locality Sensitive Hashing (LSH). In contrast to (Andoni, Indyk, Nguyen, Razenshteyn 2014), the new bound is not only optimal, but in fact improves over the best (optimal) LSH data structures (Indyk, Motwani 1998) (Andoni, Indyk 2006) for all approximation factors c>1.From the technical perspective, we proceed by decomposing an arbitrary dataset into several subsets that are, in a certain sense, pseudo-random.},
booktitle = {Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing},
pages = {793–801},
numpages = {9},
keywords = {data structures, decision trees, high-dimensional geometry, similarity search, theory},
location = {Portland, Oregon, USA},
series = {STOC '15}
}

@article{iDEC-Hashing-4,
author = {Gong, Long and Wang, Huayi and Ogihara, Mitsunori and Xu, Jun},
title = {iDEC: indexable distance estimating codes for approximate nearest neighbor search},
year = {2020},
issue_date = {May 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3397230.3397243},
doi = {10.14778/3397230.3397243},
abstract = {Approximate Nearest Neighbor (ANN) search is a fundamental algorithmic problem, with numerous applications in many areas of computer science. In this work, we propose indexable distance estimating codes (iDEC), a new solution framework to ANN that extends and improves the locality sensitive hashing (LSH) framework in a fundamental and systematic way. Empirically, an iDEC-based solution has a low index space complexity of O(n) and can achieve a low average query time complexity of approximately O(log n). We show that our iDEC-based solutions for ANN in Hamming and edit distances outperform the respective state-of-the-art LSH-based solutions for both in-memory and external-memory processing. We also show that our iDEC-based in-memory ANN-H solution is more scalable than all existing solutions. We also discover deep connections between Error-Estimating Codes (EEC), LSH, and iDEC.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1483–1497},
numpages = {15}
}

@inproceedings{ANN-Hashing-5,
author = {Indyk, Piotr and Motwani, Rajeev},
title = {Approximate nearest neighbors: towards removing the curse of dimensionality},
year = {1998},
isbn = {0897919629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276698.276876},
doi = {10.1145/276698.276876},
booktitle = {Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing},
pages = {604–613},
numpages = {10},
location = {Dallas, Texas, USA},
series = {STOC '98}
}

@InProceedings{mmLSH-Hashing-6,
author="Jafari, Omid
and Nagarkar, Parth
and Monta{\~{n}}o, Jonathan",
editor="Satoh, Shin'ichi
and Vadicamo, Lucia
and Zimek, Arthur
and Carrara, Fabio
and Bartolini, Ilaria
and Aum{\"u}ller, Martin
and J{\'o}nsson, Bj{\"o}rn Þ{\'o}r
and Pagh, Rasmus",
title="mmLSH: A Practical and Efficient Technique for Processing Approximate Nearest Neighbor Queries on Multimedia Data",
booktitle="Similarity Search and Applications",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="47--61",
abstract="Many large multimedia applications require efficient processing of nearest neighbor queries. Often, multimedia data are represented as a collection of important high-dimensional feature vectors. Existing Locality Sensitive Hashing (LSH) techniques require users to find top-k similar feature vectors for each of the feature vectors that represent the query object. This leads to wasted and redundant work due to two main reasons: 1) not all feature vectors may contribute equally in finding the top-k similar multimedia objects, and 2) feature vectors are treated independently during query processing. Additionally, there is no theoretical guarantee on the returned multimedia results. In this work, we propose a practical and efficient indexing approach for finding top-k approximate nearest neighbors for multimedia data using LSH called mmLSH, which can provide theoretical guarantees on the returned multimedia results. Additionally, we present a buffer-conscious strategy to speed up the query processing. Experimental evaluation shows significant gains in performance time and accuracy for different real multimedia datasets when compared against state-of-the-art LSH techniques.",
isbn="978-3-030-60936-8"
}

@Article{EI-LSH-Hashing-7,
author={Liu, Wanqi
and Wang, Hanchen
and Zhang, Ying
and Wang, Wei
and Qin, Lu
and Lin, Xuemin},
title={EI-LSH: An early-termination driven I/O efficient incremental c-approximate nearest neighbor search},
journal={The VLDB Journal},
year={2021},
month={Mar},
day={01},
volume={30},
number={2},
pages={215-235},
abstract={Nearest neighbor in high-dimensional space has been widely used in various fields such as databases, data mining and machine learning. The problem has been well solved in low-dimensional space. However, when it comes to high-dimensional space, due to the curse of dimensionality, the problem is challenging. As a trade-off between accuracy and efficiency, c-approximate nearest neighbor (c-ANN) is considered instead of an exact NN search in high-dimensional space. A variety of c-ANN algorithms have been proposed, one of the important schemes for the c-ANN problem is called Locality-sensitive hashing (LSH), which projects a high-dimensional dataset into a low-dimensional dataset and can return a c-ANN with a constant probability. In this paper, we propose a new aggressive early-termination (ET) condition which stops the algorithm with LSH scheme earlier under the same theoretical guarantee, leading to a smaller I/O cost and less running time. Unlike the ``conservative'' early termination conditions used in previous studies, we propose an ``aggressive'' early termination condition which can stop much earlier. Though it is not absolutely safe and may result in the probability of failure, we can still devise more efficient algorithms under the same theoretical guarantee by carefully considering the failure probabilities brought by LSH scheme and early termination. Furthermore, we also introduce an incremental searching strategy. Unlike the previous LSH methods, which expand the bucket width in an exponential way, we employ a more natural search strategy to incrementally access the hash values of the objects. We also provide a rigorous theoretical analysis to underpin our incremental search strategy and the new early termination technique. Our comprehensive experiment results show that, compared with the state-of-the-art I/O efficient c-ANN techniques, our proposed algorithm, namely EI-LSH, can achieve much better I/O efficiency under the same theoretical guarantee.},
issn={0949-877X},
doi={10.1007/s00778-020-00635-4},
url={https://doi.org/10.1007/s00778-020-00635-4}
}

@article{IntelligentProbingLv-Hashing-8,
author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
title = {Intelligent probing for locality sensitive hashing: multi-probe LSH and beyond},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137836},
doi = {10.14778/3137765.3137836},
abstract = {The past decade has been marked by the (continued) explosion of diverse data content and the fast development of intelligent data analytics techniques. One problem we identified in the mid-2000s was similarity search of feature-rich data. The challenge here was achieving both high accuracy and high efficiency in high-dimensional spaces. Locality sensitive hashing (LSH), which uses certain random space partitions and hash table lookups to find approximate nearest neighbors, was a promising approach with theoretical guarantees. But LSH alone was insufficient since a large number of hash tables were required to achieve good search quality. Building on an idea of Panigrahy, our multi-probe LSH method introduced the idea of intelligent probing. Given a query object, we strategically probe its neighboring hash buckets (in a query-dependent fashion) by calculating the statistical probabilities of similar objects falling into each bucket. Such intelligent probing can significantly reduce the number of hash tables while achieving high quality. In this paper, we revisit the problem motivation, the challenges, the key design considerations of multi-probe LSH, as well as discuss recent developments in this space and some questions for further research.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2021–2024},
numpages = {4}
}

@article{ParkNeighbor-Hashing-9,
author = {Park, Yongjoo and Cafarella, Michael and Mozafari, Barzan},
title = {Neighbor-sensitive hashing},
year = {2015},
issue_date = {November 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/2850583.2850589},
doi = {10.14778/2850583.2850589},
abstract = {Approximate kNN (k-nearest neighbor) techniques using binary hash functions are among the most commonly used approaches for overcoming the prohibitive cost of performing exact kNN queries. However, the success of these techniques largely depends on their hash functions' ability to distinguish kNN items; that is, the kNN items retrieved based on data items' hashcodes, should include as many true kNN items as possible. A widely-adopted principle for this process is to ensure that similar items are assigned to the same hashcode so that the items with the hashcodes similar to a query's hashcode are likely to be true neighbors.In this work, we abandon this heavily-utilized principle and pursue the opposite direction for generating more effective hash functions for kNN tasks. That is, we aim to increase the distance between similar items in the hashcode space, instead of reducing it. Our contribution begins by providing theoretical analysis on why this revolutionary and seemingly counter-intuitive approach leads to a more accurate identification of kNN items. Our analysis is followed by a proposal for a hashing algorithm that embeds this novel principle. Our empirical studies confirm that a hashing algorithm based on this counter-intuitive idea significantly improves the efficiency and accuracy of state-of-the-art techniques.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {144–155},
numpages = {12}
}

@article{StreamingSundaram-Hashing-10,
author = {Sundaram, Narayanan and Turmukhametova, Aizana and Satish, Nadathur and Mostak, Todd and Indyk, Piotr and Madden, Samuel and Dubey, Pradeep},
title = {Streaming similarity search over one billion tweets using parallel locality-sensitive hashing},
year = {2013},
issue_date = {September 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/2556549.2556574},
doi = {10.14778/2556549.2556574},
abstract = {Finding nearest neighbors has become an important operation on databases, with applications to text search, multimedia indexing, and many other areas. One popular algorithm for similarity search, especially for high dimensional data (where spatial indexes like kd-trees do not perform well) is Locality Sensitive Hashing (LSH), an approximation algorithm for finding similar objects.In this paper, we describe a new variant of LSH, called Parallel LSH (PLSH) designed to be extremely efficient, capable of scaling out on multiple nodes and multiple cores, and which supports high-throughput streaming of new data. Our approach employs several novel ideas, including: cache-conscious hash table layout, using a 2-level merge algorithm for hash table construction; an efficient algorithm for duplicate elimination during hash-table querying; an insert-optimized hash table structure and efficient data expiration algorithm for streaming data; and a performance model that accurately estimates performance of the algorithm and can be used to optimize parameter settings. We show that on a workload where we perform similarity search on a dataset of > 1 Billion tweets, with hundreds of millions of new tweets per day, we can achieve query times of 1-2.5 ms. We show that this is an order of magnitude faster than existing indexing schemes, such as inverted indexes. To the best of our knowledge, this is the fastest implementation of LSH, with table construction times up to 3.7\texttimes{} faster and query times that are 8.3\texttimes{} faster than a basic implementation.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1930–1941},
numpages = {12}
}

@article{PM-LSH-Hashing-11,
author = {Zheng, Bolong and Zhao, Xi and Weng, Lianggui and Hung, Nguyen Quoc Viet and Liu, Hang and Jensen, Christian S.},
title = {PM-LSH: A fast and accurate LSH framework for high-dimensional approximate NN search},
year = {2020},
issue_date = {January 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3377369.3377374},
doi = {10.14778/3377369.3377374},
abstract = {Nearest neighbor (NN) search in high-dimensional spaces is inherently computationally expensive due to the curse of dimensionality. As a well-known solution to approximate NN search, locality-sensitive hashing (LSH) is able to answer c-approximate NN (c-ANN) queries in sublinear time with constant probability. Existing LSH methods focus mainly on building hash bucket based indexing such that the candidate points can be retrieved quickly. However, existing coarse-grained structures fail to offer accurate distance estimation for candidate points, which translates into additional computational overhead when having to examine unnecessary points. This in turn reduces the performance of query processing. In contrast, we propose a fast and accurate LSH framework, called PM-LSH, that aims to compute the c-ANN query on large- scale, high-dimensional datasets. First, we adopt a simple yet effective PM-tree to index the data points. Second, we develop a tunable confidence interval to achieve accurate distance estimation and guarantee high result quality. Third, we propose an efficient algorithm on top of the PM-tree to improve the performance of computing c-ANN queries. Extensive experiments with real-world data offer evidence that PM-LSH is capable of outperforming existing proposals with respect to both efficiency and accuracy.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {643–655},
numpages = {13}
}

----- Trees -------
@article{MultidimBSTs,
author = {Bentley, Jon Louis},
title = {Multidimensional binary search trees used for associative searching},
year = {1975},
issue_date = {Sept. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/361002.361007},
doi = {10.1145/361002.361007},
abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.},
journal = {Commun. ACM},
month = sep,
pages = {509–517},
numpages = {9},
keywords = {partial match queries, nearest neighbor queries, key, intersection queries, information retrieval system, binary tree insertion, binary search trees, attribute, associative retrieval}
}

@inproceedings{BeygelzimerCoverTrees,
author = {Beygelzimer, Alina and Kakade, Sham and Langford, John},
title = {Cover trees for nearest neighbor},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143857},
doi = {10.1145/1143844.1143857},
abstract = {We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer \& Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger \& Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {97–104},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@ARTICLE{HouleRankBasedTree,
  author={Houle, Michael E. and Nett, Michael},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Rank-Based Similarity Search: Reducing the Dimensional Dependence}, 
  year={2015},
  volume={37},
  number={1},
  pages={136-150},
  keywords={Approximation methods;Measurement;Indexes;Navigation;Complexity theory;Data mining;Search problems;Nearest neighbor search;intrinsic dimensionality;rank-based search},
  doi={10.1109/TPAMI.2014.2343223}}

@article{LuVHPHypersphere,
author = {Lu, Kejing and Wang, Hongya and Wang, Wei and Kudo, Mineichi},
title = {VHP: approximate nearest neighbor search via virtual hypersphere partitioning},
year = {2020},
issue_date = {May 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3397230.3397240},
doi = {10.14778/3397230.3397240},
abstract = {Locality sensitive hashing (LSH) is a widely practiced c-approximate nearest neighbor(c-ANN) search algorithm in high dimensional spaces. The state-of-the-art LSH based algorithm searches an unbounded and irregular space to identify candidates, which jeopardizes the efficiency. To address this issue, we introduce the concept of virtual hypersphere partitioning. The core idea is to impose a virtual hypersphere, centered at the query, in the original feature space and only examine points inside the hypersphere. The search space of a hypersphere is isotropic and bounded, and thus more efficient than the existing one. In practice, we use multiple physical hyperspheres with different radii in corresponding projection subspaces to emulate the single virtual hypersphere. We also developed a principled method to compute the hypersphere radii for given success probability.Based on virtual hypersphere partitioning, we propose a novel disk-based indexing and searching scheme VHP to answer c-ANN queries. In the indexing phase, VHP stores LSH projections with independent B+-trees. To process a query, VHP keeps increasing the radii of physical hyperspheres co-ordinately, which in effect amounts to enlarging the virtual hypersphere, to accommodate more candidates until the success probability is met. Rigorous theoretical analysis shows that the proposed algorithm supports c-ANN search for arbitrarily small c ≥ 1 with probability guarantee. Extensive experiments on a variety of datasets, including the billion-scale ones, demonstrate that VHP could achieve different tradeoffs between efficiency and accuracy, and achieves up to 2x speedup in running time over the state-of-the-art methods.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1443–1455},
numpages = {13}
}

@ARTICLE{Muja2014ScalableNN,
  author={Muja, Marius and Lowe, David G.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Scalable Nearest Neighbor Algorithms for High Dimensional Data}, 
  year={2014},
  volume={36},
  number={11},
  pages={2227-2240},
  keywords={Approximation algorithms;Clustering algorithms;Vegetation;Partitioning algorithms;Approximation methods;Machine learning algorithms;Computer vision;Nearest neighbor search;big data;approximate search;algorithm configuration},
  doi={10.1109/TPAMI.2014.2321376}}

@INPROCEEDINGS{SilpaAnan2008OptimizedKDTree,
  author={Silpa-Anan, Chanop and Hartley, Richard},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Optimised KD-trees for fast image descriptor matching}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  keywords={Application software;Image recognition;Image databases;Indexing;Principal component analysis;Binary search trees;Binary trees;Computer vision;Image retrieval;Search methods},
  doi={10.1109/CVPR.2008.4587638}}

----- Quantization------

@inproceedings{ferhatosmanoglu2001approximate,
  title={Approximate nearest neighbor searching in multimedia databases},
  author={Ferhatosmanoglu, Hakan and Tuncel, Ertem and Agrawal, Divyakant and El Abbadi, Amr},
  booktitle={Proceedings 17th International Conference on Data Engineering},
  pages={503--511},
  year={2001},
  organization={IEEE}
}

VQ index
vaplusapprox
JegouPQ

@ARTICLE{OptimizedProductQuantization,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  keywords={Quantization (signal);Vectors;Artificial neural networks;Optimization;Encoding;Indexing;Linear programming;Vector quantization;nearest neighbor search;image retrieval;compact encoding;inverted indexing},
  doi={10.1109/TPAMI.2013.240}}

DeltaPQ

@INPROCEEDINGS{VarianceAwareQuantization,
  author={Paparrizos, John and Edian, Ikraduya and Liu, Chunwei and Elmore, Aaron J. and Franklin, Michael J.},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Fast Adaptive Similarity Search through Variance-Aware Quantization}, 
  year={2022},
  volume={},
  number={},
  pages={2969-2983},
  keywords={Dimensionality reduction;Quantization (signal);Dictionaries;Nearest neighbor methods;Data engineering;Robustness;Explosives;quantization;similarity search;proximity search},
  doi={10.1109/ICDE53745.2022.00268}}

blink of an eye

------- prox graphs

@article{FuFastApproximate,
author = {Fu, Cong and Xiang, Chao and Wang, Changxu and Cai, Deng},
title = {Fast approximate nearest neighbor search with the navigating spreading-out graph},
year = {2019},
issue_date = {January 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3303753.3303754},
doi = {10.14778/3303753.3303754},
abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial scenario of Taobao (Alibaba Group) and has been integrated into their billion-scale search engine.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {461–474},
numpages = {14}
}

filtered disk ann
also adding ood disk ann

@inproceedings{NEURIPS2019-DISKANN,
 author = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{MALKOV201461,
title = {Approximate nearest neighbor algorithm based on navigable small world graphs},
journal = {Information Systems},
volume = {45},
pages = {61-68},
year = {2014},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437913001300},
author = {Yury Malkov and Alexander Ponomarenko and Andrey Logvinov and Vladimir Krylov},
keywords = {Similarity search, k-Nearest neighbor, Approximate nearest neighbor, Navigable small world, Distributed data structure},
abstract = {We propose a novel approach to solving the approximate k-nearest neighbor search problem in metric spaces. The search structure is based on a navigable small world graph with vertices corresponding to the stored elements, edges to links between them, and a variation of greedy algorithm for searching. The navigable small world is created simply by keeping old Delaunay graph approximation links produced at the start of construction. The approach is very universal, defined in terms of arbitrary metric spaces and at the same time it is very simple. The algorithm handles insertions in the same way as queries: by finding approximate neighbors for the inserted element and connecting it to them. Both search and insertion can be done in parallel requiring only local information from the structure. The structure can be made distributed. The accuracy of the probabilistic k-nearest neighbor queries can be adjusted without rebuilding the structure. The performed simulation for data in the Euclidean spaces shows that the structure built using the proposed algorithm has small world navigation properties with log2(n) insertion and search complexity at fixed accuracy, and performs well at high dimensionality. Simulation on a CoPHiR dataset revealed its high efficiency in case of large datasets (more than an order of magnitude less metric computations at fixed recall) compared to permutation indexes. Only 0.03% of the 10 million 208-dimensional vector dataset is needed to be evaluated to achieve 0.999 recall (virtually exact search). For recall 0.93 processing speed 2800queries/s can be achieved on a dual Intel X5675 Xenon server node with Java implementation.}
}
\bibliography{squash-refs}
@misc{malkov2018efficientrobustapproximatenearest,
      title={Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs}, 
      author={Yu. A. Malkov and D. A. Yashunin},
      year={2018},
      eprint={1603.09320},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/1603.09320}, 
}

@misc{singh2021freshdiskannfastaccurategraphbased,
      title={FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search}, 
      author={Aditi Singh and Suhas Jayaram Subramanya and Ravishankar Krishnaswamy and Harsha Vardhan Simhadri},
      year={2021},
      eprint={2105.09613},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2105.09613}, 
}

@INPROCEEDINGS{Zhao2020SONGGPU,
  author={Zhao, Weijie and Tan, Shulong and Li, Ping},
  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)}, 
  title={SONG: Approximate Nearest Neighbor Search on GPU}, 
  year={2020},
  volume={},
  number={},
  pages={1033-1044},
  keywords={Graphics processing units;Instruction sets;Memory management;Indexes;Data structures;Approximation algorithms},
  doi={10.1109/ICDE48307.2020.00094}}


------pg based filtered anns

@inproceedings{NEURIPS2023-Wang-NHQ,
 author = {Wang, Mengzhao and Lv, Lingwei and Xu, Xiaoliang and Wang, Yuxiang and Yue, Qiang and Ni, Jiongkang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {15738--15751},
 publisher = {Curran Associates, Inc.},
 title = {An Efficient and Robust Framework for Approximate Nearest Neighbor Search with Attribute Constraint},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/32e41d6b0a51a63a9a90697da19d235d-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{wang2022navigableproximitygraphdrivennative-NHQ-2,
      title={Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints}, 
      author={Mengzhao Wang and Lingwei Lv and Xiaoliang Xu and Yuxiang Wang and Qiang Yue and Jiongkang Ni},
      year={2022},
      eprint={2203.13601},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2203.13601}, 
}

ACORN

@misc{zhao2022constrainedapproximatesimilaritysearchAIRSHIP,
      title={Constrained Approximate Similarity Search on Proximity Graph}, 
      author={Weijie Zhao and Shulong Tan and Ping Li},
      year={2022},
      eprint={2210.14958},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2210.14958}, 
}

FilteredDiskANN again

-----non PG filtered ANNS
CAPS

@inproceedings{huang2020embeddingFAISS-IVF,
  title={Embedding-based retrieval in facebook search},
  author={Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2553--2561},
  year={2020}
}

---- Hamming distance
Markatat

@Article{Martin2015,
author={Martin, Eric
and Cao, Eddie},
title={Euclidean chemical spaces from molecular fingerprints: Hamming distance and Hempel's ravens},
journal={Journal of Computer-Aided Molecular Design},
year={2015},
month={May},
day={01},
volume={29},
number={5},
pages={387-395},
abstract={Molecules are often characterized by sparse binary fingerprints, where 1s represent the presence of substructures and 0s represent their absence. Fingerprints are especially useful for similarity calculations, such as database searching or clustering, generally measuring similarity as the Tanimoto coefficient. In other cases, such as visualization, design of experiments, or latent variable regression, a low-dimensional Euclidian ``chemical space'' is more useful, where proximity between points reflects chemical similarity. A temptation is to apply principal components analysis (PCA) directly to these fingerprints to obtain a low dimensional continuous chemical space. However, Gower has shown that distances from PCA on bit vectors are proportional to the square root of Hamming distance. Unlike Tanimoto similarity, Hamming similarity (HS) gives equal weight to shared 0s as to shared 1s, that is, HS gives as much weight to substructures that neither molecule contains, as to substructures which both molecules contain. Illustrative examples show that proximity in the corresponding chemical space reflects mainly similar size and complexity rather than shared chemical substructures. These spaces are ill-suited for visualizing and optimizing coverage of chemical space, or as latent variables for regression. A more suitable alternative is shown to be Multi-dimensional scaling on the Tanimoto distance matrix, which produces a space where proximity does reflect structural similarity.},
issn={1573-4951},
doi={10.1007/s10822-014-9819-y},
url={https://doi.org/10.1007/s10822-014-9819-y}
}

----serv filtered anns
pinecone
Vexless

@article{echihabi2020return,
  title={Return of the lernaean hydra: Experimental evaluation of data series approximate similarity search},
  author={Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
  journal={arXiv preprint arXiv:2006.11459},
  year={2020}
}

@online{Weaviate,
  author = {Weaviate},
  title = {Weaviate Serverless Database},
  url = {https://weaviate.io/deployment/serverless},
  year = {2024}
}

@online{Upstash,
  author = {Upstash},
  title = {Upstash Vector: Serverless Vector Database for AI and LLMs},
  url = {https://upstash.com/blog/introducing-vector-database},
  year = {2024}
}

@online{DatastaxAstraDB,
  author = {Datastax},
  title = {Astra DB Serverless},
  url = {https://docs.datastax.com/en/astra-db-serverless/index.html},
  year = {2024}
}

@online{TurboPuffer,
  author = {TurboPuffer},
  title = {TurboPuffer},
  url = {https://turbopuffer.com},
  year = {2024}
}


@online{Rifai2021,
   author = {Moneer Rifai},
   title = {Serverless showdown: AWS Lambda vs Azure Functions vs Google Cloud Functions},
   url = {https://www.pluralsight.com/resources/blog/cloud/serverless-showdown-aws-lambda-vs-azure-functions-vs-google-cloud-functions},
    note = {Accessed: 2023-11-30}
}

@article{ServerlessSurvey2022,
author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, Bingsheng and Guo, Minyi},
title = {The Serverless Computing Survey: A Technical Primer for Design Architecture},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3508360},
doi = {10.1145/3508360},
abstract = {The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potential of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.},
journal = {ACM Computing Survey},
month = {sep},
articleno = {220},
numpages = {34},
keywords = {Serverless computing, FaaS, Lambda paradigm, architecture design}
}

@misc{aguerrebere2024locallyadaptivequantizationstreamingvector,
      title={Locally-Adaptive Quantization for Streaming Vector Search}, 
      author={Cecilia Aguerrebere and Mark Hildebrand and Ishwar Singh Bhati and Theodore Willke and Mariano Tepper},
      year={2024},
      eprint={2402.02044},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02044}, 
}

@article{PQCacheLocality2015,
author = {Andr\'{e}, Fabien and Kermarrec, Anne-Marie and Le Scouarnec, Nicolas},
title = {Cache locality is not enough: high-performance nearest neighbor search with product quantization fast scan},
year = {2015},
issue_date = {December 2015},
publisher = {VLDB Endowment},
volume = {9},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/2856318.2856324},
doi = {10.14778/2856318.2856324},
abstract = {Nearest Neighbor (NN) search in high dimension is an important feature in many applications (e.g., image retrieval, multimedia databases). Product Quantization (PQ) is a widely used solution which offers high performance, i.e., low response time while preserving a high accuracy. PQ represents high-dimensional vectors (e.g., image descriptors) by compact codes. Hence, very large databases can be stored in memory, allowing NN queries without resorting to slow I/O operations. PQ computes distances to neighbors using cache-resident lookup tables, thus its performance remains limited by (i) the many cache accesses that the algorithm requires, and (ii) its inability to leverage SIMD instructions available on modern CPUs.In this paper, we advocate that cache locality is not sufficient for efficiency. To address these limitations, we design a novel algorithm, PQ Fast Scan, that transforms the cache-resident lookup tables into small tables, sized to fit SIMD registers. This transformation allows (i) in-register lookups in place of cache accesses and (ii) an efficient SIMD implementation. PQ Fast Scan has the exact same accuracy as PQ, while having 4 to 6 times lower response time (e.g., for 25 million vectors, scan time is reduced from 74ms to 13ms).},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {288–299},
numpages = {12}
}

@article{RabitQ2024,
author = {Gao, Jianyang and Long, Cheng},
title = {RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3654970},
doi = {10.1145/3654970},
abstract = {Searching for approximate nearest neighbors (ANN) in the high-dimensional Euclidean space is a pivotal problem. Recently, with the help of fast SIMD-based implementations, Product Quantization (PQ) and its variants can often efficiently and accurately estimate the distances between the vectors and have achieved great success in the in-memory ANN search. Despite their empirical success, we note that these methods do not have a theoretical error bound and are observed to fail disastrously on some real-world datasets. Motivated by this, we propose a new randomized quantization method named RaBitQ, which quantizes D-dimensional vectors into D-bit strings. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. In addition, we introduce efficient implementations of RaBitQ, supporting to estimate the distances with bitwise operations or SIMD-based operations. Extensive experiments on real-world datasets confirm that (1) our method outperforms PQ and its variants in terms of accuracy-efficiency trade-off by a clear margin and (2) its empirical performance is well-aligned with our theoretical analysis.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {167},
numpages = {27},
keywords = {Johnson-Lindenstrauss transformation, approximate nearest neighbor search, quantization}
}

@article{SeRF2024,
author = {Zuo, Chaoji and Qiao, Miao and Zhou, Wenchao and Li, Feifei and Deng, Dong},
title = {SeRF: Segment Graph for Range-Filtering Approximate Nearest Neighbor Search},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3639324},
doi = {10.1145/3639324},
abstract = {Effective vector representation models, e.g., word2vec and node2vec, embed real-world objects such as images and documents in high dimensional vector space. In the meanwhile, the objects are often associated with attributes such as timestamps and prices. Many scenarios need to jointly query the vector representations of the objects together with their attributes. These queries can be formalized as range-filtering approximate nearest neighbor search (ANNS) queries. Specifically, given a collection of data vectors, each associated with an attribute value whose domain has a total order. The range-filtering ANNS consists of a query range and a query vector. It finds the approximate nearest neighbors of the query vector among all the data vectors whose attribute values fall in the query range. Existing approaches suffer from a rapidly degrading query performance when the query range width shifts. The query performance can be optimized by a solution that builds an ANNS index for every possible query range; however, the index time and index size become prohibitive -- the number of query ranges is quadratic to the number n of data vectors. To overcome these challenges, for the query range contains all attribute values smaller than a user-provided threshold, we design a structure called the segment graph whose index time and size are the same as a single ANNS index, yet can losslessly compress the n ANNS indexes, reducing the indexing cost by a factor of Ω(n). To handle general range queries, we propose a 2D segment graph with average-case index size O(n log n) to compress n segment graphs, breaking the quadratic barrier. Extensive experiments conducted on real-world datasets show that our proposed structures outperformed existing methods significantly; our index also exhibits superior scalability.},
journal = {Proc. ACM Manag. Data},
month = mar,
articleno = {69},
numpages = {26},
keywords = {approximate nearest neighbor search, filtered search, information retrieval, multimodel search}
}

@article{LloydsAlgorithm1982,
  title={Least squares quantization in PCM},
  author={Lloyd, Stuart},
  journal={IEEE transactions on information theory},
  volume={28},
  number={2},
  pages={129--137},
  year={1982},
  publisher={IEEE}
}


@online{NumpyAdvancedIndexing,
  author = {NumPy},
  title = {Indexing on ndarrays},
  url = {https://numpy.org/doc/stable/user/basics.indexing.html},
  year = {2025}
}

@online{NumpyVectorization,
  author = {Shivam Bhatele},
  title = {Vectorization in Python - An Alternative to Python Loops},
  url = {https://medium.com/pythoneers/vectorization-in-python-an-alternative-to-python-loops-2728d6d7cd3e},
  year = {2023}
}

@online{FaissMissingManual,
  author = {James Briggs},
  title = {Faiss: The Missing Manual},
  url = {https://www.pinecone.io/learn/series/faiss/},
  year = {2025}
}

@InProceedings{Noh_2021_ICCV_PQ_Recall,
    author    = {Noh, Haechan and Kim, Taeho and Heo, Jae-Pil},
    title     = {Product Quantizer Aware Inverted Index for Scalable Nearest Neighbor Search},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {12210-12218}
}

@inproceedings{2014SunDataSkipping,
author = {Sun, Liwen and Franklin, Michael J. and Krishnan, Sanjay and Xin, Reynold S.},
title = {Fine-grained partitioning for aggressive data skipping},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2610515},
doi = {10.1145/2588555.2610515},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {1115–1126},
numpages = {12},
keywords = {query processing, partitioning, data warehouse, algorithms},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

https://zilliz.com/learn/scalar-quantization-and-product-quantization

@online{ZillizSQ,
  author = {Zilliz},
  title = {Scalar Quantization and Product Quantization},
  url = {https://zilliz.com/learn/scalar-quantization-and-product-quantization},
  year = {2023}
}