%!TEX root = main.tex


\section{\crisp: Theory and Algorithm}
\label{sec:method}
% This section introduces the key components and overall framework of \scenario{CRISP}, followed by a rigorous convergence analysis.
Clearly, the contact-implicit planning problem~\eqref{eq:contact-implicit} can be transformed into a generic nonlinear programming (NLP) problem, which also aligns with our implemented C++ solver. Let $x = [v, \lambda]$, we denote the NLP formulation of~\eqref{eq:contact-implicit} as:
\begin{subequations}\label{eq:problem_general} 
    \begin{align}
        \min_x &\quad J(x) \\
        \subject &\quad c_i(x) = 0, \quad i \in \mathcal{E} \label{eq:general:eq}\\
        &\quad c_i(x) \ge 0. \quad i \in \mathcal{I} \label{eq:general:ineq}
    \end{align}
\end{subequations}

\textbf{Primal-only merit function.}
From a primal-only perspective, the core optimization challenge is to balance descent of the objective function with satisfaction of the constraints~\cite{nocedal1999springer-numerical-optimization}. 
% To evaluate the quality of each trial step, we require a metric that captures this trade-off. 
In \crisp, we adopt the following merit function with weighted $\ell_1$ penalty to evaluate the quality of a point $x$:
\begin{align}
    \label{eq:merit-func}
    \phi_1(x;\mu) \triangleq J(x) +  \sum_{i \in \mathcal{E}} \mu_i| c_i(x) | + \sum_{i \in \mathcal{I}} \mu_i [c_i(x)]^-,
\end{align}
where $\mu_i > 0$ is the penalty parameter for each constraint $i$. The notation $[c_i]^- \triangleq \max\{0, -c_i\}$ is used to properly penalize inequality constraint violations. 

\begin{example}[Merit Function for the Toy Problem]\label{ex:toy:merit}
    Continuing Example~\ref{example:toy}, we plot the merit function for the toy MPCC problem~\eqref{eq:toy} with different penalty parameters in Fig. \ref{fig:toy_example_merit_function}. Evidently, the stationary points of the merit function align with the global minimum of Example~\ref{example:toy} at the origin.
\end{example}

% left lower right upper

\begin{remark}[Merit Function]
    The $\ell_1$ penalty function is a class of nonsmooth penalty functions that has demonstrated remarkable success in practical applications. It is exact in the sense that, given an appropriate penalty parameter and under mild constraint qualifications, local optimizers of the original problem are also minimizers of the merit function~\cite{Fletcher1983}.

    We need to emphasize that, due to the lack of CQs in the MPCC problem, it is unclear whether minimization of the merit function~\eqref{eq:merit-func} has the same type of guarantees as in the case where CQs hold. However, the intuition that the merit function balances objective reduction and constraint satisfaction still holds, and according to Example~\ref{ex:toy:merit}, the minimization of the merit function leads to the optimal solution, at least for the toy problem. We leave a precise investigation of the relationship between the merit function and the original problem in the case of MPCC as future work.
    
    % Moreover, unlike the $\ell_2$ loss, the gradient of the $\ell_1$ penalty does not approach zero near the origin, which can be beneficial as the constraints approach the boundary.

    A difference of our merit function from the usual $\ell_1$ penalty is that we maintain a separate penalty parameter $\mu_i$ for each constraint. This individualized penalization strategy often leads to superior convergence in practice.

    % Besides, inspired by the role of Lagrange multipliers in primal-dual algorithms, we recognize that different constraints may have varying contributions to optimality and degrees of difficulty in satisfaction. Therefore, instead of using a single penalty variable for all constraints, we maintain separate penalty variables for each constraint. 
\end{remark}

\textbf{Convex subproblem.} 
To minimize the merit function~\prettyref{eq:merit-func}, we adopt a sequential optimization strategy that solves a series of subproblems by approximating a local model of the merit function in the vicinity of the current iteration. Formally, our subproblem seeks to minimize the following quadratic model:
\begin{align}\label{eq:subproblem-nonsmooth}
\min_{p_k} \, q_{\mu,k}(p_k) := & \quad J_k + \nabla J_k^\top p_k 
+ \frac{1}{2} p_k^\top \nabla^2_{xx} J_k \, p_k 
\notag\\
&\quad + \sum_{i \in \mathcal{E}}\mu_i \lvert c_i(x_k) + \nabla c_i(x_k)^\top p_k \rvert \notag\\ &\quad + \sum_{i \in \calI}\mu_i  \big[c_i(x_k) + \nabla c_i(x_k)^\top p_k\big]^-,
\end{align}
where $J_k$, $\nabla J_k$, and $\nabla_{xx}^2 J_k$ represent the objective function's value, gradient, and Hessian at the current iterate $x_k$, respectively. The constraints $c_i$ are linearized around $x_k$. In the subproblem~\eqref{eq:subproblem-nonsmooth}, ``$p_k$'' is known as the \emph{trial step}.

From~\eqref{eq:subproblem-nonsmooth}, the readers see that our method diverges from the subproblem formulation in the classical Sequential Quadratic Programming (SQP) methods~\cite{sqp-method, fletcher2000practical, jordana2023stagewise}, in the sense that~\eqref{eq:subproblem-nonsmooth} refrains from computing second-order derivatives of the constraints and the Lagrangian. Instead, we linearize the constraints while maintaining a quadratic model of the objective function only. Our approach aligns with the concept of sequential convex programming discussed in~\cite{scp-01,scp-02,scp-03}.

Leveraging convexity of the objective function $J$ and introducing the penalty term, the subproblem~\eqref{eq:subproblem-nonsmooth} is always convex and feasible. This offers several advantages. First, it reduces the computational burden associated with calculating second-order derivatives of constraints, which is particularly beneficial for problems with complex constraints (e.g., in robotics). Second, the convexity and guaranteed feasibility of the subproblem not only facilitate efficient solution of~\eqref{eq:subproblem-nonsmooth} using well-established convex optimization techniques but also enhance the robustness of the overall algorithm.

\begin{remark}[Elastic Mode]
    In conventional sequential optimization methods, such as SQP, constraints are typically linearized directly from the original problem. However, when applied to MPCC, this approach inevitably leads to infeasible subproblems~\cite{SIOPT-FLETCHER-2006,SIOPT-ANITESCU-2005}. To address this issue, some numerical solvers, like \snopt, implement an ``Elastic Mode'' where they add penalty variables to move constraints into the objective function upon detecting subproblem infeasibility.
    Our approach aligns with the motivation of the elastic mode in the sense that the merit function~\eqref{eq:merit-func} is ``elastic'' from the very beginning because the constraints are penalized in the objective function.
    % Our approach differs fundamentally by approximating the merit function in the subproblem formulation. 
    This strategy ensures our subproblems are always feasible by design.
\end{remark}


% enabling consistent progress even in challenging optimization landscapes, making it more tractable and amenable to theoretical convergence analysis. 

% \yl{I add the elastic mode staffs here, this is what it really do. We remark here that another significant numerical challenge presented by the contact-implicit problem~\eqref{eq:contact-implicit} is its inherent tendency to cause inconsistent linearization, inevitably leading to infeasible subproblems~\cite{SIOPT-FLETCHER-2006}. This is why some numerical solvers, like \texttt{SNOPT}, have the so-called ``Elastic Mode'' to ensure the feasibility of subproblems.}

\textbf{Globalization.}
The only issue with the subproblem~\eqref{eq:subproblem-nonsmooth} is that it may be unbounded from below (i.e., the optimal value is $-\infty$).
To avoid this issue, we introduce the trust-region approach following \cite{nocedal1999springer-numerical-optimization}. Specifically, we constrain the $\ell_{\infty}$ norm of the trial step $p_k$. Formally, the trust-region subproblem with $\ell_\infty$ norm constraint becomes:
\begin{subequations} \label{eq:subproblem-smooth}
    \begin{align}
        \min_{p_k, v, w, t} & \displaystyle \,\,\,J_k + \nabla J_k\tran p_k  + \frac{1}{2} p_k\tran \nabla_{xx}^2 J_k p_k \nonumber \\
        & + \mu \sum_{i \in \mathcal{E}} (v_i + w_i) + \mu \sum_{i \in \mathcal{I}} t_i  \\
        \text{subject to} &\,\, \nabla c_i(x_k)\tran p_k + c_i(x_k) = v_i - w_i, \ i \in \mathcal{E} \label{eq:subproblem-smooth-eq}\\
        & \,\, \nabla c_i(x_k)\tran p_k + c_i(x_k) \ge -t_i, \ i \in \mathcal{I} \label{eq:subproblem-smooth-ineq}\\
        & \,\, v, w, t \ge 0 \\
        & \,\, \|p_k\|_\infty \le \Delta_k, \label{eq:subproblem-trust-region}
    \end{align}
\end{subequations}
where~\eqref{eq:subproblem-trust-region} represents the $\ell_\infty$ norm constraint with trust-region radius $\Delta_k$. From~\eqref{eq:subproblem-nonsmooth} to~\eqref{eq:subproblem-smooth}, we have also introduced slack variables $v,w,t$ to reformulate the nonsmooth $\ell_1$ objective as smooth constraints. Evidently, the subproblem~\eqref{eq:subproblem-smooth} is a standard convex quadratic program (QP).

Essentially, the trust region constraint~\eqref{eq:subproblem-trust-region} acts as a filter on how much we trust the local model (quadratic objective and linearized constraints) to approximate the merit function. The $\ell_{\infty}$ norm only limits the maximum step length in the trial step, avoiding the issues related to ill-conditioning that can occur in line search methods.


To determine whether a trial step should be accepted and whether the trust-region radius should be adjusted, we monitor the ratio of actual decrease to predicted decrease:
\begin{align}
    \label{eq:reduction_ratio}
    \rho_k \triangleq \frac{\text{ared}_k}{\text{pred}_k} 
    = \frac{
        \phi_1(x_k; \mu) - \phi_1(x_k + p_k; \mu)
    }{
        q_{\mu,k}(0) - q_{\mu,k}(p_k) 
    }.
\end{align}
This ratio $\rho_k$ serves as a key indicator of the quality of the trial step. As $\rho_k$ approaches 1, it signifies that the trial step is increasingly satisfactory, indicating the local quadratic model at this step accurately describes the local behavior of the true merit function within the trust region radius. When the actual reduction is negative or the reduction ratio is very low, it indicates that the local model is highly inaccurate. In such cases, we typically shrink the trust region radius. To avoid continuous shrinking of the trust region and the Maratos effect, we introduce a second order correction step. The essence of this correction is to replace the linear approximation of $c(x_k+p)$ in subproblem~\prettyref{eq:subproblem-smooth} with a second-order model:
\begin{equation}
c(x_k+p) \approx c(x_k) + \nabla c(x_k)^T p + \frac{1}{2}p^T \nabla_{xx}^2c(x_k) p.
\end{equation}

Suppose we have calculated an unsatisfactory trial step $p_k$, and the correction step to be determined is not far from $p_k$. 
We approximate the second-order term $p^T \nabla_{xx}^2c(x_k) p$ utilizing the value $c(x_k + p_k)$ computed at the trial step:
\begin{align}\label{eq:second_order_correction}
    p^T \nabla_{xx}^2c(x_k) p \approx \,& p_k^T \nabla_{xx}^2c(x_k) p_k \notag\\ 
    \approx \,& c(x_k+p_k) - c(x_k) - \nabla c(x_k)^\top p_k.
\end{align}
From~\prettyref{eq:second_order_correction}, we use $c(x_k+p) = \nabla c(x_k)^{\top} p + (c(x_k+p_k) -  \nabla c(x_k)^\top p_k)$ to replace the first-order approximation in~\eqref{eq:subproblem-smooth-eq} and~\eqref{eq:subproblem-smooth-ineq} to calculate a correction step $\Bar{p}_k$.
This approach does not require explicit second-order information of the constraints, thereby maintaining computational efficiency. Furthermore, it preserves the convex QP structure of the subproblem. In practice, this second-order correction has proven highly effective in correcting inaccuracies of the linearized model, leading to more efficient reductions in the merit function.

\textbf{Convergence analysis.} 
We now provide convergence guarantees of \crisp to stationary points of the merit function~\eqref{eq:merit-func}. We first recall the definition of the directional derivative.
\begin{definition}[Directional Derivative]
    The directional derivative of a function $f$: $\mathbb{R}^n \rightarrow \mathbb{R}$ in the direction $p$ is:
        \begin{equation}
        D(f;p) = \lim_{t\rightarrow0} \frac{f(x + tp)-f(x)}{t}.
    \end{equation}
\end{definition}

The stationary point of the nonsmooth merit function is:
\begin{definition}[Stationary Point]
A point $\Bar{x}$ is a stationary point of the merit function $\phi_1(x,\mu)$ in~\eqref{eq:merit-func} if its directional derivatives are nonnegative along all directions $p$, i.e.,
    \begin{equation}
        D(\phi_1(\Bar{x};\mu);p)\geq 0.
    \end{equation}
\end{definition}

We then make mild assumptions about the motion planning problems under consideration.
\begin{assumption}[Convexity]\label{assump:convergence_assump}
In problem~\prettyref{eq:problem_general}, the objective function $J(x)$ is convex and continuous differentiable; $c_i$ is differentiable and $\nabla c_i$ is Lipschitz continuous for all $i\in\calE$ and $i\in\calI$.
\end{assumption}
We are ready to present the main result.
\begin{theorem}[Convergence]
\label{thm:local_convergence}
Under Assumption~\ref{assump:convergence_assump}, suppose:
\begin{enumerate}
    \item The sequence of convex subproblems~\eqref{eq:subproblem-smooth} converges, i.e., $x_k\rightarrow x^*$ for some point $x^*$
    \item The trust region radius remains bounded above zero, i.e., there exists $\Delta_{\min} > 0$ such that $\Delta_k \geq \Delta_{\min}$ for all $k$.
\end{enumerate}
Then, $x^*$ is a stationary point of the merit function~\eqref{eq:merit-func}.
\end{theorem}
\begin{proof}
    See~\prettyref{app:proof_main_thm}.
\end{proof}

\textbf{Final algorithm.}
Having introduced the key algorithmic components and the convergence analysis, we formally present \crisp in Algorithm~\ref{alg:crisp}. 
% The essence of \scenario{CRISP} is encapsulated in its recursive structure, comprising outer iterations and inner iterations.
The algorithm initiates with a common penalty variable $\mu_0$ for all constraints. Guided by the checkable convergence conditions outlined in~\prettyref{thm:local_convergence}, we solve a series of trust-region subproblems (convex QPs) in the inner iterations. This process continues until the conditions in Theorem~\ref{thm:local_convergence} are satisfied, indicating convergence to a stationary point of the current merit function. 
% At each step within the inner loop, we update the trust region based on the quality of the local model.
In the outer iterations, we examine the constraint violation against a threshold $\epsilon_c$. For constraint violations exceeding this threshold, we increase their corresponding penalty parameters.
The default values of all hyperparameters in Algorithm~\ref{alg:crisp} are summarized in~\prettyref{tab:hyperparameters}.




