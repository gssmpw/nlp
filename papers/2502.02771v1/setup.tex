\section{Experimental Setup}
\textbf{Dataset. } We use a de-identified CT dataset of 935 patients from The University of Texas MD Anderson Cancer Center.
All CT images were of patients who had received surgical mastectomy to the right side of the body, and radiotherapy to the post-mastectomy chest wall and/or axillary lymph nodes. We resample each scan to 1 $mm^3$, remove table artifacts and resize each slice to $128^2$ pixels. We split our dataset into 729 patients consisting of over 300k slices for training and 206 patients consisting of over 618 slices testing. We evaluate our models on test data only.

\noindent\textbf{Unconditional Diffusion Model Training. } We train a classic 2D U-Net diffusion model to reconstruct CT slices unconditionally. Our U-Net consists of four downsampling and upsampling modules with attention heads in the third and fourth layers. We use 1000 training timesteps and a squared cosine noise scheduler as outlined in \cite{ho2020denoising}. 
% We train with a batch size of 90 for 120 epochs on NVIDIA A100 GPUs each with 48 GB of memory. 
Model choice was determined based on loss convergence.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/fig2_updated.png}
    \caption{\textbf{Diffusion priors prevail with extremely few projections but may have wrong content and structure. } We plot evaluation metric versus number of projections for classical and diffusion priors. For 206 patients (3 slices/patient), we plot the metric and interquantile range (0.05 and 0.95 quantile, $IQR_{5,95}$) for PSNR, SSIM, fat content accuracy, and fat dice score.}
    % {\color{red} You say they prevail with few observations, but you're not pointing out that they can also be wrong with fat content, and structure,which is the main point of the whole paper.}
    \label{fig:trend}
\end{figure*}

\noindent\textbf{Inference. }
% Diffusion setup
We performed a grid search to determine the best optimization, regularization, and guidance parameters for $L_1$ ($\lambda=10$), $L_2$ ($\lambda=10$) and diffusion ($\lambda=0.1$).
These corresponded to the lowest mean squared error at 360 projections % Sophia, confirm if this is what we did. --> YES!
We used the \verb|tomosipo| package~\cite{hendriksen2021tomosipo} to compute the projection operator.
For $L_1$, we ran the total-variation regularized least squares for reconstruction for 1000 iterations. 
% For $L_2$, we ran the regularized least squares with the Nesterov accelerated gradient descent method for 100 iterations.\footnote{The algorithm implementations for $L_1$ and $L_2$ prior based reconstruction can be found in \url{https://github.com/ahendriksen/ts_algorithms}}
At each time step of our guided diffusion inference we take the projection of the predicted reconstruction using our projection operator and compute its mean squared error with the ground truth projections, varying the number of projections used. We then compute the gradients of this loss with respect to the predicted reconstruction, and guide the diffusion process by adding the gradients back to the prediction~\cite{dhariwal2021diffusion,graikos2022diffusion}. We iterate through this process over 50 timesteps.

\noindent\textbf{Metrics}
We investigate pixel (peak-signal-to-noise ratio or PSNR), structural (structural similarity index or SSIM), and downstream metrics (content and localization).
We use the percentage of correct pixels classified as fat (fat content accuracy) and the fat segmentation dice score (fat localization accuracy).
Fat has Hounsfield units between -150 and -50.