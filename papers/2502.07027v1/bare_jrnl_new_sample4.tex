\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{mhchem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{color}
\usepackage{verbatim}
\usepackage{wrapfig}
\usepackage{picinpar}
\usepackage{cutwin}
\usepackage{bbding}
\usepackage{colortbl}
\usepackage{bbding}
\usepackage{framed}
\usepackage[hang]{footmisc}
\usepackage{orcidlink}
\usepackage{amsthm}
% \usepackage[dvipsnames]{xcolor}
\definecolor{c1}{HTML}{95bddc}
\definecolor{c2}{HTML}{FF4500}
\definecolor{c3}{HTML}{92CDDC}
\definecolor{c4}{HTML}{336626}
\definecolor{c5}{HTML}{FFB400}
\definecolor{c6}{HTML}{55416A}
\definecolor{c7}{HTML}{CCC2D9}
\definecolor{c8}{HTML}{541254}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{hyperref}
\usepackage{orcidlink}
\newtheorem{theorem}{Theorem}[section] 
\newtheorem{lemma}{Proposition}[section]   
\newtheorem{definition}{Definition}[section] 

% 设置全局超链接样式
\hypersetup{
    colorlinks = true,
    linkcolor  = blue,  % 正文中的引用为蓝色
    urlcolor   = blue,  % 其他超链接为蓝色
    citecolor  = green,  % 引用链接为蓝色
}

% 在orcid引用中取消框和颜色
\newcommand{\orcidlinkcustom}[1]{\href{https://orcid.org/#1}{\textcolor{black}{\orcidicon}}}


% updated with editorial comments 8/9/2021

\begin{document}

\title{Representational Alignment with Chemical Induced Fit for Molecular Relational Learning}
% ReAlignFit: 
% Interpretable Causal Substructure-based Graph Information Bottleneck for Molecular Relational Learning
\author{Peiliang Zhang\orcidlink{0000-0002-1826-3349},~\IEEEmembership{Student Member,~IEEE},
% Xin Zhang\orcidlink{0000-0002-9647-7203},
Jingling Yuan\orcidlink{0000-0001-7924-8620},~\IEEEmembership{Senior Member,~IEEE},
Qing Xie\orcidlink{0000-0003-4530-588X},
Yongjun Zhu\orcidlink{0000-0003-4787-5122},\\
Lin Li\orcidlink{0000-0002-1826-3349},~\IEEEmembership{Senior Member,~IEEE}
        % <-this % stops a space
\thanks{This work was supported by the National Natural Science Foundation of China (No. 62472332 and No. 62276196), the National Key Research and Development Program of China (2022YFB2404300) and the China Scholarship Council. (\textit{Corresponding author: Jingling Yuan}.)}
% , the China Scholarship Council and the Special Project for High-Quality Development of Manufacturing Industry in Hubei Province (2206-420118-89-04-959008).}
\thanks{Peiliang Zhang is with the School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, 430070, China and also with the Department of Library and Information Science, Yonsei University, Seoul, 26493, Republic of Korea. (E-mail: cheungbl@ieee.org)}
\thanks{Jingling Yuan is with the School of Computer Science and Artificial Intelligence, Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology, Wuhan, 430070, China. (E-mail: yjl@whut.edu.cn)}
\thanks{Qing Xie and Lin Li are with the School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, 430070, China. (E-mail: \{felixxq, cathylilin\}@whut.edu.cn)}
\thanks{Yongjun Zhu is with the Department of Library and Information Science, Yonsei University, Seoul, 26493, Korea. (E-mail: zhu@yonsei.ac.kr)}
% \thanks{Corresponding author: Jingling Yuan (E-mail: yjl@whut.edu.cn).}
\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Molecular Relational Learning (MRL) is widely applied in natural sciences to predict relationships between molecular pairs by extracting structural features. 
The representational similarity between substructure pairs determines the functional compatibility of molecular binding sites. Nevertheless, aligning substructure representations by attention mechanisms lacks guidance from chemical knowledge, resulting in unstable model performance in chemical space (\textit{e.g.}, functional group, scaffold) shifted data.
% \textcolor{red}{in distribution-shifted data}.
% Representational alignment is essential for capturing key features (e.g., substructures). Nevertheless, alignment methods that introduce inductive bias by attention mechanisms lack the guidance of domain knowledge, resulting in unstable model performance in distribution-shifted data. 
With theoretical justification, we propose the \textbf{Re}presentational \textbf{Align}ment with Chemical Induced \textbf{Fit} (ReAlignFit) to enhance the stability of MRL. ReAlignFit dynamically aligns substructure representation in MRL by introducing chemical-Induced Fit-based inductive bias.
% ReAlignFit dynamically captures molecules' core substructure features by introducing inductive bias-guided molecular representation alignment with the Induced Fit theory in the chemical domain. 
In the induction process, we design the Bias Correction Function based on substructure edge reconstruction to align representations between substructure pairs by simulating chemical conformational changes (dynamic combination of substructures).
ReAlignFit further integrates the Subgraph Information Bottleneck during fit process to refine and optimize substructure pairs exhibiting high chemical functional compatibility, leveraging them to generate molecular embeddings. Experimental results on nine datasets demonstrate that ReAlignFit's outperforms state-of-the-art models in two tasks and significantly enhances model’s stability in both rule-shifted and scaffold-shifted data distributions.
\end{abstract}

\begin{IEEEkeywords}
Molecular Relational Learning, Representational Alignment, Chemical Induced Fit, Model Stability.
\end{IEEEkeywords}

\section{Introduction}
Molecular Relational Learning (MRL) predicts the interaction between molecular pairs by mining features and properties~\cite{fang2024moltcACL,lee2023shiftKDD}. MRL has garnered significant attention in natural science research due to its applications in new material design and drug discovery~\cite{tkdeGIBB,pei2024hagoAAAI,li2022oodtkde}. 
The molecular structure representation-based methods have advantages in MRL and demonstrate satisfactory performance in downstream tasks~\cite{gao2024drugclip,wang2024kdd,du2024mmgnnIJCAI}. Therefore, accurately representing molecular features becomes critical in MRL.


% The molecular structure representation-based methods have advantages in MRL~\cite{li2022oodtkde,gao2024drugclip}. However, these methods exhibit varying degrees of performance fluctuations in real-world scenarios~\cite{wang2024kdd,du2024mmgnnIJCAI}. Therefore, improving model stability is crucial for MRL.

The functional compatibility of binding sites is an essential determinant of molecular relationships~\cite{koshland1995key,mcgibbon2024intuition,xia2023understanding}.
Recent research primarily quantifies functional compatibility by calculating the similarity between substructure representations, focusing on molecular representational alignment by attention-based inductive bias~\cite{maboudi2024retuning,seo2024selfKDD,xia2023understanding}.
% Representation alignment is critical in capturing molecular core features by ensuring that the embedding representations accurately reflect molecular structures or functions~\cite{maboudi2024retuning,wang2024KDD2,seo2024selfKDD}. 
% Recent research has focused on molecular representation alignment through inductive bias~\cite{mcgibbon2024intuition,xia2023understanding}.
The molecular interaction is regarded as an essential source of inductive bias, which aligns the properties of paired molecules with the representations of the molecular core features by modeling the chemical reactions between the molecules. 
The core challenge lies in effectively computing inductive bias within molecular relationships. 
Existing methods have explored molecular-level~\cite{born2023regression}, substructure-level~\cite{lee2023cgibICML}, and hybrid strategy alignment~\cite{dsn}.
% 表示对齐通过确保嵌入表示能准确反应分子结构或功能，在捕捉分子核心特征中发挥着重要作用。通过归纳偏差实现分子表示对齐是最近的研究热点。分子间的交互行为被视为归纳偏差的来源，通过建模分子间的化学反应，将配对分子的特性与分子核心特征的表示对齐。这里的一个关键问题是有效的对齐分子表示，即如何计算分子关系中的归纳偏差。现有的方法已经探索了多种层级的对齐策略，包括分子层级、子结构层级以及两者混合对齐策略。
% 分子结合位点的功能兼容性是决定分子关系的重要因素。现有研究大多通过计算子结构间表示的相似性来量化这种兼容性。通过基于注意力的归纳偏差实现分子表示对齐是最近的研究热点。
% 子结构对间的表示相似性决定了分子结合位点的功能兼容性。但通过注意力机制对齐子结构间表示的方式缺乏领域知识的指导，导致模型在分布变化数据中性能不稳定。

Nevertheless, applying inductive bias-based representational alignment to MRL involves two key considerations: (1) \textbf{Guidance from Domain Knowledge}: Molecular feature representations are frequently influenced by domain knowledge in chemistry or biology~\cite{xia2023understanding,fang2022geometry,nc}.
Changes in adjacent atoms or scaffolds within the chemical space influence the chemical properties of molecular substructures~\cite{li2023reaction,yang2023molerecwww}. 
% Substructures within molecules are susceptible to influence by surrounding atoms and scaffolds, which may change some chemical properties~\cite{li2023reaction,yang2023molerecwww}. 
Most of the existing representational alignment methods~\cite{saddi,yang2024interaction,lee2023shiftKDD} predominantly compute inductive bias using attention mechanisms, resulting in the results reflecting statistical correlations and overlooking other active atoms that may affect the properties of these substructures. (2) \textbf{Dynamic Adaptability of Inductive Bias}: The substructures (functional groups within molecules) involved in chemical reactions are closely related to their paired substructures \cite{mak2024artificial,hu2017recentScaffold,bohm2004scaffold}, meaning that substructures involved in different chemical reactions within the same molecule may differ. 
Structural shifts in functional groups and scaffolds may induce dynamic changes in the importance patterns of substructures in chemical reactions.
Static inductive bias based on attention mechanisms~\cite{miracle,su2024dualAAAI} tends to concentrate weights on certain substructures with specific features, failing to account for dynamic changes such as the adaptive adjustment of substructures in MRL.
% 然而，将基于归纳偏执的表示对齐应用于MRL涉及两个关键考虑因素：（1）领域知识的指导：分子的特征表示往往受到化学或生物学领域知识的影响。分子中的子结构容易受到周围原子的影响，从而改变某些化学性质。现有的表示对齐方法大多基于注意力机制计算归纳偏差，导致结果反映的是统计相关性而忽略了可能影响这些子结构性质的其他活性原子。（2）归纳偏差的动态适应性：参与化学反应的亚结构与其配对的亚结构密切相关\cite{mak2024artificial}，这意味着同一分子内参与不同化学反应的亚结构可能不同。基于注意力机制的静态归纳偏差往往将权重集中在某些具有特定特征的子结构上，未充分考虑分子关系中子结构的自适应调整等动态变化。


To tackle these challenges, we first theoretically demonstrate that aligning information between core substructure pairs facilitates stable MRL. With theoretical justification, we propose the \textbf{Re}presentational \textbf{Align}ment with Chemical Induced \textbf{Fit} (ReAlignFit) to improve MRL stability. ReAlignFit generates substructure embedding through the GNN encoder. Inspired by Induced Fit theory~\cite{koshland1995key}, we design the Dynamic Representational Alignment Module (DRAM) with substructure edge reconstruction, whose core is the Bias Correction Function (BCF). BCF simulates dynamic conformational changes (i.e., dynamic adjustments among substructures) during Induced Fit by a self-supervised approach. Combined with the Subgraph Information Bottleneck (S-GIB), DRAM aligns core substructure pairs for potential compatibility in chemical function. ReAlignFit integrates domain knowledge while avoiding errors in identifying core substructures and aligning information caused by spurious attention. Ultimately, we train ReAlignFit by synergistically optimizing molecular representational alignment's confusion loss and task prediction loss.
% We design the loss function consistent with the S-GIB optimization objective, which synergistically optimizes the confusion loss in molecular representation alignment and task prediction loss.
Experimental results demonstrate that ReAlignFit achieves state-of-the-art predictive performance and significantly improves model stability in both rule-shifted and scaffold-shifted data distributions.
% 为了解决上述关键问题，我们首先从理论上证明了对齐核心子结构对之间的信息的分子表示策略有利于稳定的MRL。有了理论依据，我们提出了提升MRL稳定性的Representational Alignment with Induced Fit (ReAlignFit) 。ReAlignFit通过GNN-based的模型生成子结构的嵌入表示。在诱导契合理论的启发下，我们设计了基于子结构边重建的动态表示对齐模块，其核心为偏差校正函数。偏差校正函数通过自监督的方式模拟诱导契合过程中动态的构象变化（即子结构间的随机动态调整），同时结合子图信息瓶颈（S-GIB）以对齐核心子结构对在化学功能上的潜在兼容性。动态表示对齐模块在融合领域知识的同时避免因虚假注意力造成的核心子结构识别和信息对齐错误。最终，我们以协同优化分子表示对齐的混淆损失和任务预测损失的方式实现对ReAlignFit的训练。实验结果表明，ReAlignFit实现了最佳的预测性能，并显著提高了模型在分布变化场景中的稳定性。
The main contributions are listed below:
\begin{itemize}
    \item We propose ReAlignFit, which introduces domain knowledge into representational alignment for stable MRL. To the best of our literature review, it is the first work to explore the dynamic representational alignment of substructures in different chemical reactions.
    \item With theoretical justification, we provide a formal certification for the loss function design to align representations between substructures by minimizing differences between core substructures.
    % 通过理论证明，我们提供了分子表示对齐和损失函数设计的正式认证，以通过最小化核心子结构间的差异来对齐子结构之间的表征。
    \item Experiments on two tasks in nine real-world datasets and three different data distributions demonstrate that the predictive performance and stability of ReAlignFit outperforms eight state-of-the-art models.
\end{itemize}

\section{Preliminaries}
\label{Preliminaries}
In this section, we illustrate how the attention mechanism-based inductive bias affects the stability of MRL with specific examples (Section \ref{Motivating Example}). We formally describe stable MRL (Section \ref{Stable Molecular Relational Learning}) and conduct a theoretical analysis to reveal feasible solutions to improve MRL stability (Section \ref{Theoretical Analysis of Stable MRL}).
% 在本节中，我们将结合具体的示例来阐述基于注意力机制的归纳偏差如何影响MRL的稳定性。接下来我们将给出Stable MRL的形式化描述并结合理论分析揭示提高MRL稳定性的可行方案。
% Example
% Task Description+Stable MRLe
% Problem Analysise

\subsection{Motivating Example}
\label{Motivating Example}
The inherent biases and errors in the inductive bias due to attention mechanisms~\cite{cao2024towards} are critical factors affecting the stability of MRL. In this subsection, we explain in further detail how the error in inductive bias affects the representational alignment between molecular pairs and the stability of MRL with the example in Fig. \ref{fig1}.
%注意力机制导致的归纳偏差中固有的偏见和错误是影响MRL稳定的重要因素。在本节中，我们将以图1为例，进一步详细解释归纳偏差的偏误如何影响分子对间的表示对齐以及MRL的稳定性。


We will explain the impact of inductive bias errors on the representational alignment between substructure pairs by considering the dynamic adaptation of the induction bias and the guidance of chemical domain knowledge. 
(1) \textbf{The impact of dynamic adaptability of inductive bias on representational alignment}: The core substructure determines the reaction may change depending on the paired molecule. In Fig. \ref{fig1}\textcolor{blue}{(a)}, when molecule A reacts with B, the reactive substructure in A is \textcolor{c2}{\ce{-COOH}}, while with C, it is \textcolor{blue}{\ce{-NH2}}. The scaffold of molecule B is \ce{-OH}, while that of molecule C is \ce{C=O}. This variation in scaffolds results in different reactive substructures in molecule A. However, the \textcolor{c2}{\ce{-COOH}} is slightly more reactive than \textcolor{blue}{\ce{-NH2}} in chemical reactions. The data co-occurrence model of the attentional mechanism causes the inductive bias to overly focus on \textcolor{c2}{\ce{-COOH}} and ignore the \textcolor{blue}{\ce{-NH2}}, which are the true drivers of chemical reactions. This phenomenon indicates that attention-based inductive bias lacks dynamic adaptability to paired molecules in MRL, making it difficult to identify and align the representations of core substructures involved in chemical reactions.
(2) \textbf{The guidance of domain knowledge on representational alignment}: The properties of molecular substructures are susceptible to influence by surrounding atoms. As shown in Fig. \ref{fig1}\textcolor{blue}{(b)}, the strong electron-adsorption group \textcolor{c4}{\ce{-CI}} in molecule E enhances the reactivity of \textcolor{c5}{\ce{-CO}}. In the esterification reaction, the \ce{-COCI} in molecule E will replace \textcolor{c5}{\ce{-CO}} and react with the hydroxyl group in alcohols. Inductive bias lacking guidance from domain knowledge usually assigns higher weights to more frequently occurring \textcolor{c5}{\ce{-CO}}, hindering the alignment of representations for core substructures that drive chemical reactions. 
Therefore, a key challenge in enhancing the stability is how to dynamically align the representations of core substructures under the guidance of chemical domain knowledge.
% In summary, it is necessary to dynamically align the representations of core substructures with the guidance of chemical domain knowledge to enhance the stability of MRL.
% 因此,如何在化学领域知识的指导下让MRL模型动态对齐核心子结构间的表征是提升其稳定性的一个关键问题。

% 我们将通过考虑归纳偏差的动态适应性和化学领域知识对分子对齐的影响来解释归纳偏差的偏误对子结构对之间的表示对齐及MRL稳定性的影响。（1）归纳偏差的动态适应对表示对齐的影响：决定反应的核心亚结构可能会因配对分子的不同而发生变化。在图\ref{fig1}(c)中，当分子A与B反应时，A中的反应亚结构是 \textcolor{c2}{ce/{-COOH}}，而与C反应时，则是\textcolor{blue}{ce/{-NH2}}。在化学反应中，\textcolor{c2}{ce{-COOH}的反应活性略高于\textcolor{blue}{ce{-NH2}。因此，\textcolor{c2}{ce{-COOH}在大多数化学反应中起主导作用。注意力机制的数据共现模式会导致归纳偏差模型过多的关注\textcolor{c2}{ce{-COOH}，而忽略了真正推动化学反应的\textcolor{blue}{ce{-NH2}。这表明基于注意力机制的归纳偏差模型在MRL中缺乏对配对分子的动态适应，难以识别和对齐化学反应中的核心子结构之间的表示。
% （2）领域知识对表示对齐的指导：分子亚结构的性质容易受到周围原子的影响。如图\ref{fig1}(b)所示，分子E中的强\textcolor{c4}{ce{-CI}吸电子基团增强了\textcolor{c5}{ce{-CO}的反应活性。｝ 在酯化反应中，分子E中的\ce{-COCI}基团将取代\textcolor{c5}{ce{-CO}，并与醇中的羟基发生反应。然而，缺乏领域知识指导的归纳偏差通常会给出现频率较高的（textcolor{c5}{\ce{-CO}）子结构赋予较高的权重，因此很难对齐决定化学反应的核心子结构\textcolor{c5}{\ce{-COCI}和\ce{-OH}间的表示。 
%综上，我们需在化学领域知识的指导下动态的对齐核心子结构间的表示，以提升MRL的稳定性。
\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{Figure/TKDE-MOTIVATION.png}
\caption{The motivating example. (a) When molecule A reacts with molecules B and C, the core substructures are \textcolor{c2}{\ce{-COOH}} and \textcolor{blue}{\ce{-NH2}}, respectively. (b) The properties of \textcolor{c5}{\ce{-CO}} within molecular E are influenced by surrounding reactive atoms \textcolor{c4}{\ce{-CI}}, leading to changes in its behavior in chemical reactions.}
\vspace{-3mm}
% \caption{Functional groups exhibit versatile functionalities in different chemical reactions.}
\label{fig1}
\end{figure}

\subsection{Stable Molecular Relational Learning}
\label{Stable Molecular Relational Learning}
\subsubsection{Molecular Representation and MRL}
We first introduce the problem definitions of molecular representation and MRL in detail.
For any molecule $\mathcal{G}$, it can be represented as $\mathcal{G}=(V,\mathcal{E},\mathcal{X},A)$. 
Here, $V=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$ denotes the set of nodes. $\mathcal{E}\in N\times N$ represents the connections between atoms within the molecule, which is closely related to the adjacency matrix $A$. If $({{v}_{i}},{{v}_{j}})\in \mathcal{E}$, then ${{A}_{ij}}=1$; otherwise, ${{A}_{ij}}=0$. $\mathcal{X} \in {\mathbb{R}^{B\times N}}$ is the feature matrix, consisting of the atom feature representations. For a given molecular pair $({\mathcal{G}_{x}}, {\mathcal{Y}_{xy}}, {\mathcal{G}_{y}}) $, the objective of MRL is to construct a model $\mathcal{F}_{MRL}=(\mathcal{F}_{End},\mathcal{F}_{Pred})$ that generates molecular embedding representations $\mathcal{H}_x$ and $\mathcal{H}_y$ through the encoder $\mathcal{F}_{End}$, and predicts the relationship $\mathcal
{\hat{Y}}_{xy}$ between moleculars with the classifier $\mathcal{F}_{Pred}$.
\begin{equation}
    \mathcal{\hat{Y}}_{xy}\!=\!\mathcal{F}_{MRL}({\mathcal{G}_{x}}, {\mathcal{Y}_{xy}}, {\mathcal{G}_{y}})\!=\!\mathcal{F}_{Pred}(\mathcal{F}_{End}({\mathcal{G}_{x}},{\mathcal{G}_{y}}),{\mathcal{Y}_{xy}})
\end{equation}
% 对于给定的分子对$({\mathcal{G}_{x}}, {\mathcal{Y}_{xy}}, {\mathcal{G}_{y}}) $，MRL的目标是构建一个模型$f$,其通过编码器$f_{encoder}$生成分子嵌入表示$\mathcal{H}_x$和$\mathcal{H}_y$，并借助分类器$f_{pred}$预测分子间的关系$\hat{Y}$。

\subsubsection{Stable MRL}
We further give a detailed definition of Stable MRL based on MRL. Given the dataset $\mathcal{D}=\{\mathcal{D}_{tra},\mathcal{D}_{val},\mathcal{D}_{tes}\}$, $\mathcal{D}_{tra}, \mathcal{D}_{val}, \mathcal{D}_{tes}$ are the training set, validation set, and test set, respectively. In the case of data distribution bias, the data distribution in $\mathcal{D}_{tra}, \mathcal{D}_{val}, \mathcal{D}_{tes}$ are different from each other. Formally, $\mathcal{S}(\mathcal{D}_{tra}) \neq \mathcal{S}(\mathcal{D}_{val})$, $\mathcal{S}(\mathcal{D}_{tra}) \neq \mathcal{S}(\mathcal{D}_{tes})$, $\mathcal{S}(\mathcal{D}_{val}) \neq \mathcal{S}(\mathcal{D}_{tes})$. The specific setup and partitioning of $\mathcal{D}_{tra},\mathcal{D}_{val},\mathcal{D}_{tes}$ will be described in detail in Section \ref{Experimental Setup}. Stable MRL aims to train a model $\mathcal{F}_{MRL}^{S}=(\mathcal{F}_{End}^{tra},\mathcal{F}_{Pred}^{val},\mathcal{F}_{Pred}^{tes})$ using $\mathcal{D}_{tra}$ and $\mathcal{D}_{val}$ that generalizes well to $\mathcal{D}_{tes}$. The model $\mathcal{F}_{MRL}$ should maintain relatively stable predictive performance in different distribution shifts.
\begin{equation}
    \begin{aligned}
        \mathcal{F}_{MRL}^{S} \leftarrow &  \{\mathcal{F}_{Pred}^{val}({\mathcal{G}_{x}},{\mathcal{Y}_{xy}},{\mathcal{G}_{y}}) \backsimeq \mathcal{F}_{Pred}^{tes}({\mathcal{G}_{u}},{\mathcal{Y}_{uv}},{\mathcal{G}_{v}})\}\\
        & s.t. {\mathcal{G}_{x}},{\mathcal{G}_{y}} \in \mathcal{D}_{val},\quad{\mathcal{G}_{u}},{\mathcal{G}_{v}} \in \mathcal{D}_{tes}
    \end{aligned}
\end{equation}
% 我们在MRL的基础上进一步给出了Stable MRL的详细定义。对于给定的数据集$D={D_{tra},D_{val},D_{tes}}$,D_{tra},D_{val},D_{tes}分别为训练集，验证集和测试集。在数据分布偏移情况下，D_{tra},D_{val},D_{tes}三者中的数据分布两两之间存在差异,即$S(),S(),S(),$。D_{tra},D_{val},D_{tes}的设置和划分将在Section中详细介绍。Stable MRL的目标是在D_{tra}和D_{val}中训练一个在D_{tes}保持良好泛化性的模型f。f应在不同数据分布偏移中保持相对稳定的预测性能。

Therefore, ReAlignFit aims to learn stable molecular representations consisting of core substructures, thereby improving the model's predictive performance.

\subsection{Theoretical Analysis of Stable MRL}
\label{Theoretical Analysis of Stable MRL}
Considering that modeling molecular relationships solely from the data perspective easily leads to instability in the model, we introduce the Induced Fit theory in chemistry to identify key factors affecting the stability of MRL at the theoretical analysis stage. 
% 考虑到仅从数据层面建模分子关系易导致模型缺乏稳定性，我们在理论分析阶段引入了化学领域的诱导契合理论以分析影响MRL稳定性的关键因素。

\subsubsection{Theoretical Analysis}
The Induced Fit theory describes the dynamic mechanism of specific molecular binding. It emphasizes that matching binding sites (representation similarity between paired substructures) is critical for enhancing binding stability \cite{koshland1995key,stiller2022structure}. Inspired by the Induced Fit theory, we attempt to analyze the contribution of matching between substructure pairs to MRL performance stabilization on the theoretical level.
% 诱导契合理论描述了分子间特异性结合的动态机制，其强调化学反应中结合位点的匹配性（配对子结构间的表示相似性）是增强结合稳定性的关键。受诱导契合理论的启发，我们尝试从理论层面上来分析子结构间的匹配性对MRL性能稳定的贡献。
% Inspired by this theory, we can identify core substructures by aligning paired substructure embeddings to mitigate performance fluctuations in MRL models. We theoretically demonstrate the feasibility of this idea.
% 考虑到仅从数据层面建模分子关系易导致模型缺乏稳定性，我们在理论分析阶段引入了化学领域的诱导契合理论以分析影响MRL稳定性的关键因素。诱导契合理论描述了分子间特异性结合的动态机制，其强调化学反应中结合位点的匹配性（配对子结构间的表示相似性）是增强结合稳定性的关键。受诱导契合理论的启发，我们可以通过对齐配对子结构间的嵌入表示来筛选核心子结构，以缓解MRL模型的性能波动。
% 受诱导契合理论的启发，我们尝试从理论层面上来分析通过对齐子结构间嵌入表示来筛选高匹配性子结构对的方式MRL性能稳定的贡献。
\begin{theorem}
\label{theorem 1}
    \textit{Given the molecular pair $({\mathcal{G}_{x}},{\mathcal{G}_{y}})$ and the prediction target $\mathcal{Y}$, where the substructure ${\mathcal{G}^{s}}$ of $\mathcal{G}$ consists of core substructure ${\mathcal{G}^{c}}$ and confounding substructure ${\mathcal{G}^{n}}$. For $\forall$ ${\mathcal{G}_{x}},{\mathcal{G}_{y}}\in \mathcal{G}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. Furthermore, considering the correlation between ${\mathcal{G}^{c}}$ and ${\mathcal{G}^{n}}$, if there exists a minimal value $\varepsilon $ such that:}
% 给定分子对$({{G}_{x}},{{G}_{y}})$和目标$Y$，其中，$G$的子结构${{G}^{sub}}$由核心子结构${{G}^{c}}$和混淆子结构${{G}^{n}}$构成。对于$\forall {{G}_{x}},{{G}_{y}}\in G$，由条件互信息法则可知，$I({{G}_{x}},{{G}_{y}};Y)\le I({{G}^{c}};Y|{{G}^{n}})$。进一步，考虑到${{G}^{c}}$和${{G}^{n}}$的关联性，如果存在一个极小的数$\eta $，使得：
\begin{equation}
\label{eq:theorem}
\left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)\! \right|\!\le \!\varepsilon 
\end{equation}
\textit{where $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is the true probability between molecular pair and the prediction target. 
% $\mathcal{P}({{G}^{c}};Y|{{G}^{n}})$ denotes the conditional probability between ${{G}^{c}}$ and $Y$ given ${{G}^{n}}$. 
$\mathcal{P}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c};\mathcal{Y})$ is interaction probability between core substructure captured by model learning and prediction target called learning probability. $\mathcal{P}({\mathcal{G}^{c}_x};{\mathcal{G}^{n}_x})$ and $\mathcal{P}({\mathcal{G}^{c}_y};{\mathcal{G}^{n}_y})$ are the confounding probabilities between core and confounding substructures.}
\end{theorem}

% \textbf{Theorem 1.} \textit{Given the molecular pair $({\mathcal{G}_{x}},{\mathcal{G}_{y}})$ and the prediction target $\mathcal{Y}$, where the substructure ${\mathcal{G}^{s}}$ of $\mathcal{G}$ consists of core substructure ${\mathcal{G}^{c}}$ and confounding substructure ${\mathcal{G}^{n}}$. For $\forall$ ${\mathcal{G}_{x}},{\mathcal{G}_{y}}\in \mathcal{G}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. Furthermore, considering the correlation between ${\mathcal{G}^{c}}$ and ${\mathcal{G}^{n}}$, if there exists a minimal value $\varepsilon $ such that:}
% 给定分子对$({{G}_{x}},{{G}_{y}})$和目标$Y$，其中，$G$的子结构${{G}^{sub}}$由核心子结构${{G}^{c}}$和混淆子结构${{G}^{n}}$构成。对于$\forall {{G}_{x}},{{G}_{y}}\in G$，由条件互信息法则可知，$I({{G}_{x}},{{G}_{y}};Y)\le I({{G}^{c}};Y|{{G}^{n}})$。进一步，考虑到${{G}^{c}}$和${{G}^{n}}$的关联性，如果存在一个极小的数$\eta $，使得：
% \begin{equation}
% \label{eq:theorem}
% \left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)\! \right|\!\le \!\varepsilon 
% \end{equation}
% \textit{where $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is the true probability between molecular pair and the prediction target. 
% % $\mathcal{P}({{G}^{c}};Y|{{G}^{n}})$ denotes the conditional probability between ${{G}^{c}}$ and $Y$ given ${{G}^{n}}$. 
% $\mathcal{P}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c};\mathcal{Y})$ is interaction probability between core substructure captured by model learning and prediction target called learning probability. $\mathcal{P}({\mathcal{G}^{c}_x};{\mathcal{G}^{n}_x})$ and $\mathcal{P}({\mathcal{G}^{c}_y};{\mathcal{G}^{n}_y})$ are the confounding probabilities between core and confounding substructures.}
% 其中，$I({{G}_{x}},{{G}_{y}};Y)$ 是分子对与预测目标间的真实概率。$I({{G}^{c}};Y|{{G}^{n}})$ 是给定 ${{G}^{n}}$ 时，${{G}^{c}}$ 和 $Y$ 之间的条件概率。$I(G_{x}^{c},G_{y}^{c};Y)$是模型捕获的核心子结构与预测目标间的作用概率，可通过模型学习动态调整称为学习概率。$I({{G}^{c}_x};{{G}^{n}_x})$ 和$I({{G}^{c}_y};{{G}^{n}_y})$ 是核心子结构与混淆子结构间的混淆概率。

The $\varepsilon $ measures the similarity between true probability, learning probability, and confounding probability. The discrepancy between true and learned probabilities is derived from the task's prediction loss, whereas the degree of calibration between substructure representations quantifies the confusion probability. 
Theorem \ref{theorem 1} demonstrates that when $\eta $ is sufficiently small, meaning the prediction loss and confusion probability are small enough, the relationships between molecular pairs can be stably represented. Inspired by Theorem \ref{theorem 1}, we optimize model learning by combining prediction loss and confusion probability. This approach guides us in identifying core substructures by substructure representational alignment while reducing the impact of non-core substructures on molecular representations, thereby improving the stability of MRL.
% 定理1表明，当$\eta $足够小时，即预测损失与混淆概率足够小时，可以稳定的表示分子对之间的关系。受定理1的启发，我们利用预测损失与混淆概率相结合的方式来优化模型学习，即通过子结构表示对齐识别核心子结构并降低非核心子结构对分子表征的影响以提高MRL的稳定性。
\subsubsection{The Proof of Theorem \ref{theorem 1}}
% 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小。根据互信息法则，$I({{G}_{x}},{{G}_{y}};Y)$表示为：
% \geq I({{G}^{c}};Y|{{G}^{n}})
Since $\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)$ is non-negative and $\mathcal{G}_x^n$, $\mathcal{G}_y^n$ have little effect on $\mathcal{Y}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is expressed as:
\begin{equation}
\begin{aligned}
    \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) &= \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}_y^n;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,\mathcal{G}_y^c)\\
    &+\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,)\\
    &\geq \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)
\end{aligned}   
\end{equation}
% % 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小，因此：
% \begin{equation}
%         I({{G}_{x}},{{G}_{y}};Y) \geq I({G}^c;Y)
% \end{equation}
% $I({G}^c_y;Y)$是${G}^c_y$和$Y$之间的无条件互信息，其至少包含了条件互信息$I({{G}^{c}_y};Y|{{G}^{c}_c})$。我们可得到：
$\mathcal{P}(\mathcal{G}^c_y;\mathcal{Y})$ represents the probability between $\mathcal{G}^c_y$ and $\mathcal{Y}$, which contains at least the conditional probability $\mathcal{P}({\mathcal{G}^{c}_y};\mathcal{Y}|{\mathcal{G}^{c}_c})$. We can obtain $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) \geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. According to the properties of conditional probability:
\begin{equation}
    \begin{aligned}
        \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}}) &\approx \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)\\
        &:= \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y})=\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})
    \end{aligned}
    \label{eq2}
\end{equation}
% 根据Eq. (\ref{eq1})和Eq. (\ref{eq2})，同时考虑$I(G_x^c;G_x^n)$、$I(G_y^c;G_y^n)$是非负的，可以得到：

Based on Eq (\ref{eq2}), and considering that $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)$ and $\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ are non-negative, we have the equation as:
\begin{equation}
    \mathcal{P}({\mathcal{G}_{x}},\!{\mathcal{G}_{y}};\mathcal{Y})\!-\!\mathcal{P}(\mathcal{G}_x^c,\!\mathcal{G}^c_y;\!\mathcal{Y})\!+\!\mathcal{P}(\mathcal{G}_x^c;\!\mathcal{G}_x^n)\!+\!\mathcal{P}(\mathcal{G}_y^c;\!\mathcal{G}_y^n)\!\!\geq\!\! 0
    \label{eq3}
\end{equation}
% 因此，我们可以通过增大$I({G}_x^c,{G}^c_y;Y)$和减小$I(G_x^c;G_x^n)+I(G_y^c;G_y^n)$，使得存在一个极小的正数$\eta $,满足以下关系：

Therefore, by increasing $\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})$ and decreasing $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ (\textit{i.e.}, increasing the correlation between the core substructures captured by the model and prediction targets, and decreasing the impact of the confounding substructures on the core substructure representation), we can ensure that there exists a minimal positive number $\varepsilon $ that satisfies the following relationship:
\begin{equation}
\left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n) \!\right|\! \le \! \varepsilon 
\end{equation}





% \section{Theoretical Analysis}
% \label{Theoretical Analysis}
% \subsection{Stable MRL}
% \label{Stable MRL}
% % Any molecule can be represented as $\mathcal{G}=(V,\mathcal{E},\mathcal{X},A)$. 
% For any molecule $G$, it can be represented as $G=(V,E,X,A)$. 
% Here, $V=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$ denotes the set of nodes. $\mathcal{E}\in N\times N$ represents the connections between atoms within the molecule, which is closely related to the adjacency matrix $A$. If $({{v}_{i}},{{v}_{j}})\in \mathcal{E}$, then ${{A}_{ij}}=1$; otherwise, ${{A}_{ij}}=0$. $\mathcal{X} \in {\mathcal{R}^{B\times N}}$ is the feature matrix, consisting of the atom feature representations. For the given molecular pair $({\mathcal{G}_{x}}, {\mathcal{Y}_{xy}}, {\mathcal{G}_{y}})$, the objective of Stable MRL is to generate stable molecular embeddings $\mathcal{H}$ by core substructure representation alignment and utilize it for molecular interaction prediction in distribution-shifted data. 
% Formally, ${\mathcal{Y}_{xy}} = \mathcal{F}_s (\mathcal{H}_x, \mathcal{H}_y)$. 
% Therefore, ReAlignFit aims to learn stable molecular representations consisting of core substructures, thereby improving the model's predictive performance.
% % 对于给定的分子对 $({{G}_{x}},{Y}_{xy},{{G}_{y}})$，Stable MRL的目标是通过核心子结构表示对齐生成分子的稳定嵌入表示$Z(G^{st})$，并将其用于不同数据分布下的分子对相互作用预测。Formally, ${{Y}_{xy}}=F_s ({Z({G}_{x}^{st})},{Z({G}_{y}^{st}}))$.因此，ReAlignFit的目标是学习由核心子结构构成的分子稳定表征，提高模型的预测性能。

% \subsection{Theoretical Analysis of Stable MRL}
% \label{Theoretical Analysis of Stable MRL}
% Considering that modeling molecular relationships solely from the data perspective easily leads to instability in the model, we introduce the Induced Fit theory in chemistry to identify key factors affecting the stability of MRL at the theoretical analysis stage. The Induced Fit theory describes the dynamic mechanism of specific molecular binding. It emphasizes that matching binding sites (representation similarity between paired substructures) is critical for enhancing binding stability \cite{koshland1995key,stiller2022structure}. Inspired by this theory, we can identify core substructures by aligning paired substructure embeddings to mitigate performance fluctuations in MRL models. We theoretically demonstrate the feasibility of this idea.
% % 考虑到仅从数据层面建模分子关系易导致模型缺乏稳定性，我们在理论分析阶段引入了化学领域的诱导契合理论以分析影响MRL稳定性的关键因素。诱导契合理论描述了分子间特异性结合的动态机制，其强调化学反应中结合位点的匹配性（配对子结构间的表示相似性）是增强结合稳定性的关键。受诱导契合理论的启发，我们可以通过对齐配对子结构间的嵌入表示来筛选核心子结构，以缓解MRL模型的性能波动。

% \textbf{Theorem 1.} \textit{Given the molecular pair $({\mathcal{G}_{x}},{\mathcal{G}_{y}})$ and the prediction target $\mathcal{Y}$, where the substructure ${\mathcal{G}^{s}}$ of $\mathcal{G}$ consists of core substructure ${\mathcal{G}^{c}}$ and confounding substructure ${\mathcal{G}^{n}}$. For $\forall$ ${\mathcal{G}_{x}},{\mathcal{G}_{y}}\in \mathcal{G}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. Furthermore, considering the correlation between ${\mathcal{G}^{c}}$ and ${\mathcal{G}^{n}}$, if there exists a minimal value $\varepsilon $ such that:}
% % 给定分子对$({{G}_{x}},{{G}_{y}})$和目标$Y$，其中，$G$的子结构${{G}^{sub}}$由核心子结构${{G}^{c}}$和混淆子结构${{G}^{n}}$构成。对于$\forall {{G}_{x}},{{G}_{y}}\in G$，由条件互信息法则可知，$I({{G}_{x}},{{G}_{y}};Y)\le I({{G}^{c}};Y|{{G}^{n}})$。进一步，考虑到${{G}^{c}}$和${{G}^{n}}$的关联性，如果存在一个极小的数$\eta $，使得：
% \begin{equation}
% \label{eq:theorem}
% \left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)\! \right|\!\le \!\varepsilon 
% \end{equation}
% \textit{where $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is the true probability between molecular pair and the prediction target. 
% % $\mathcal{P}({{G}^{c}};Y|{{G}^{n}})$ denotes the conditional probability between ${{G}^{c}}$ and $Y$ given ${{G}^{n}}$. 
% $\mathcal{P}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c};\mathcal{Y})$ is interaction probability between core substructure captured by model learning and prediction target called learning probability. $\mathcal{P}({\mathcal{G}^{c}_x};{\mathcal{G}^{n}_x})$ and $\mathcal{P}({\mathcal{G}^{c}_y};{\mathcal{G}^{n}_y})$ are the confounding probabilities between core and confounding substructures.}
% % 其中，$I({{G}_{x}},{{G}_{y}};Y)$ 是分子对与预测目标间的真实概率。$I({{G}^{c}};Y|{{G}^{n}})$ 是给定 ${{G}^{n}}$ 时，${{G}^{c}}$ 和 $Y$ 之间的条件概率。$I(G_{x}^{c},G_{y}^{c};Y)$是模型捕获的核心子结构与预测目标间的作用概率，可通过模型学习动态调整称为学习概率。$I({{G}^{c}_x};{{G}^{n}_x})$ 和$I({{G}^{c}_y};{{G}^{n}_y})$ 是核心子结构与混淆子结构间的混淆概率。

% The $\varepsilon $ measures the similarity between true probability, learning probability, and confounding probability. The discrepancy between true and learned probabilities is derived from the model's prediction loss, whereas the degree of calibration between substructure representations determines the confusion probability. 
% Theorem 1 demonstrates that when $\eta $ is sufficiently small, meaning the prediction loss and confusion probability are small enough, the relationships between molecular pairs can be stably represented. Inspired by Theorem 1, we optimize model learning by combining prediction loss and confusion probability. This approach guides us in identifying core substructures by substructure representational alignment while reducing the impact of non-core substructures on molecular representations, thereby improving the stability of MRL. 
% % 定理1表明，当$\eta $足够小时，即预测损失与混淆概率足够小时，可以稳定的表示分子对之间的关系。受定理1的启发，我们利用预测损失与混淆概率相结合的方式来优化模型学习，即通过子结构表示对齐识别核心子结构并降低非核心子结构对分子表征的影响以提高MRL的稳定性。
% \subsection{The Proof of Theorem 1}. 
% % 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小。根据互信息法则，$I({{G}_{x}},{{G}_{y}};Y)$表示为：
% % \geq I({{G}^{c}};Y|{{G}^{n}})
% Since $\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)$ is non-negative and $\mathcal{G}_x^n$, $\mathcal{G}_y^n$ have little effect on $\mathcal{Y}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is expressed as:
% \begin{equation}
% \begin{aligned}
%     \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) &= \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}_y^n;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,\mathcal{G}_y^c)\\
%     &+\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,)\\
%     &\geq \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)
% \end{aligned}   
% \end{equation}
% % % 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小，因此：
% % \begin{equation}
% %         I({{G}_{x}},{{G}_{y}};Y) \geq I({G}^c;Y)
% % \end{equation}
% % $I({G}^c_y;Y)$是${G}^c_y$和$Y$之间的无条件互信息，其至少包含了条件互信息$I({{G}^{c}_y};Y|{{G}^{c}_c})$。我们可得到：
% $\mathcal{P}(\mathcal{G}^c_y;\mathcal{Y})$ represents the probability between $\mathcal{G}^c_y$ and $\mathcal{Y}$, which contains at least the conditional probability $\mathcal{P}({\mathcal{G}^{c}_y};\mathcal{Y}|{\mathcal{G}^{c}_c})$. We can obtain $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) \geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. According to the properties of conditional probability:
% \begin{equation}
%     \begin{aligned}
%         \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}}) &\approx \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)\\
%         &:= \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y})=\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})
%     \end{aligned}
%     \label{eq2}
% \end{equation}
% % 根据Eq. (\ref{eq1})和Eq. (\ref{eq2})，同时考虑$I(G_x^c;G_x^n)$、$I(G_y^c;G_y^n)$是非负的，可以得到：

% Based on Eq. (\ref{eq2}), and considering that $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)$ and $\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ are non-negative, we have the equation as:
% \begin{equation}
%     \mathcal{P}({\mathcal{G}_{x}},\!{\mathcal{G}_{y}};\mathcal{Y})\!-\!\mathcal{P}(\mathcal{G}_x^c,\!\mathcal{G}^c_y;\!\mathcal{Y})\!+\!\mathcal{P}(\mathcal{G}_x^c;\!\mathcal{G}_x^n)\!+\!\mathcal{P}(\mathcal{G}_y^c;\!\mathcal{G}_y^n)\!\!\geq\!\! 0
%     \label{eq3}
% \end{equation}
% % 因此，我们可以通过增大$I({G}_x^c,{G}^c_y;Y)$和减小$I(G_x^c;G_x^n)+I(G_y^c;G_y^n)$，使得存在一个极小的正数$\eta $,满足以下关系：

% Therefore, by increasing $\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})$ and decreasing $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ (\textit{i.e.}, increasing the correlation between the core substructures captured by the model and prediction targets, and decreasing the impact of the confounding substructures on the core substructure representation), we can ensure that there exists a minimal positive number $\varepsilon $ that satisfies the following relationship:
% \begin{equation}
% \left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n) \!\right|\! \le \! \varepsilon 
% \end{equation}

\begin{figure*}[htpb]
\centering
\includegraphics[width=\textwidth]{Figure/TKDE-Model.png} 
\caption{The model structure of ReAlignFit. (a)
SRIN generates substructure representations. (b) DRAM aligns and optimizes the core substructure representations to generate stable representations of molecules.}
\vspace{-4mm}
\label{model}
\end{figure*}

\section{Methodology}
\label{Methodology}
Inspired by Theorem \ref{theorem 1}, we propose ReAlignFit to enhance the stability of MRL by simulating the dynamic mechanism of molecular binding, as shown in Fig. \ref{model}. Based on Substructure Representation-based Interaction Network (SRIN) in Fig. \ref{model}\textcolor{blue}{(a)}, we design the Dynamic Representational Alignment Module (DRAM), as shown in Fig. \ref{model}\textcolor{blue}{(b)}, which employs BCF and S-GIB to align the core substructure representations (Section \ref{Representational Alignment with Induced Fit}). 
% Additionally, we refine the Graph Information Bottleneck (GIB) and integrate it into synergistic optimization of ReAlignFit (Section \ref{CS-GIB-based Synergistic Optimization for ReAlignFit}). Finally, we analyze the computational complexity of ReAlignFit (Section \ref{Model Analysis}).
Additionally, we detail the transformation and solution process of Subgraph Information Bottleneck (S-GIB) (Section \ref{Synergistic Optimization Loss Function for ReAlignFit}) and analyze the computational complexity of ReAlignFit (Section \ref{Model Analysis}).
% Additionally, we integrate ReAlignFit optimization with S-GIB solving (Sec \ref{Synergistic Optimization Loss Function for ReAlignFit}) and analyze the computational complexity of ReAlignFit (Sec \ref{Model Analysis}).
% 受定理1的启发，我们提出了ReAlignFit，通过模拟分子结合的动态机制来增强MRL的稳定性，如图ref{model}所示。我们设计了基于子结构表征的相互作用网络（SRIN）的动态表征对齐模块（DRAM），该模块采用 BCF 和 S-GIB 对齐核心子结构表征（Sec （ref{Representational Alignment with Induced Fit}））。此外，我们分析并给出了S-GIB的求解过程（Sec \ref{Synergistic Optimization Loss Function for ReAlignFit}），并分析了ReAlignFit的计算复杂性（Sec \ref{Model Analysis}）。

\subsection{Representational Alignment with Induced Fit}
\label{Representational Alignment with Induced Fit}
Induced Fit suggests that chemical reactions between molecules are realized by combining the pre-adaptation of substructures and dynamic adjustments during binding~\cite{koshland1995key}. Guided by it, we design SRIN to simulate the pre-adaptation process. In DRAM, BCF simulates the molecular induction process to align substructure representations, while S-GIB identifies and refines substructures with high chemical functional compatibility.

% We employ the BCF to facilitate the selection of core substructures and their dynamic representation alignment. Finally, we leverage the Subgraph Information Bottleneck (S-GIB) to refine substructure representations and identify core substructures for stable molecular representations.

% Finally, we design the loss function that synergizes prediction loss and calibration loss to optimize the training. The implementation is introduced as follows.
% 基于诱导契合的表示对齐，包含两部分，一部分模拟第一阶段，另一部分模拟第二阶段的动态调整。最后写模型的loss设计.
% 诱导契合指出，分子间的化学反应由子结构的预先适配和结合过程中的动态调整两部分来实现。在其引导下，我们首先设计了基于子结构表征的静态交互网络以模拟子结构的预选适配过程。然后，基于Bias Correction Function实现核心子结构的筛选和动态表示对齐。最后，我们结合子图信息瓶颈进一步优化了子结构的表征。
% 我们在DRAM中通过BCF模拟分子诱导过程以对齐子结构间的表示，并结合S-GIB筛选并优化具有高化学功能兼容性的子结构。
% 具体来说，在诱导阶段，在分子间交互行为和子结构表征的基础上，我们设计了子结构边重建的偏差校正函数，通过模拟化学构象变化（子结构动态组合）对齐子结构间的表示。在契合阶段，ReAlignFit 进一步整合了子图信息瓶颈（S-GIB）筛选并优化具有高化学功能兼容性的子结构对并用于生成分子的嵌入表示。

% 在理论1的启发下设计了预测损失和校准损失协同的损失函数以实现ReAlignFit的训练优化。

\subsubsection{Substructure Representation-based Interaction Network}
\label{Substructure Representation-based Interaction Network}
Considering the irregularity of molecular spatial topologies and the effectiveness of GNN in dealing with topological features~\cite{du2024mmgnnIJCAI,zhang2024heterogeneousIJCAI}, ReAlignFit utilizes the GNN encoder and an adjacency aggregation strategy to generate embeddings for irregular substructures. 
% 考虑到分子空间拓扑结构的不规则性和GNN处理拓扑特征的有效性，ReAlignFit利用GNN编码器和邻接聚合策略生成不规则子结构的嵌入表征。

Initially, molecule $\mathcal{G}$ represented by SMILES~\cite{smiles} are converted into GNN-compatible representations by RDKit~\cite{brown2015silico}. Subsequently, we use neighbourhood feature weighting consisting of adjacency matrix $\mathcal{E}$, node feature representation $\mathcal{Z}(v_n)$, and neighbourhood structural coefficient $\mathcal{C}$ as the input for irregular substructure generation. The node feature representation $\mathcal{Z}^{l+1}(v_n)$ in $(l+1)$th layer and neighborhood structural coefficient $\mathcal{C}$ are defined as follows:
% 首先，我们将由SMILES表示的分子G通过RDKIT转化为GNN可处理的表示形式。接下来，我们使用由邻接矩阵 $\mathcal{E}$、节点特征表示 $Z(v_n)$ 和邻接结构系数 $\mathcal{C}$ 组成的邻接特征加权作为不规则子结构生成的输入。我们定义第(l+1)层的节点特征表示$Z^{l+1}(v_n)$和邻域结构系数$mathcal{C}$如下：
\begin{equation}
\label{node}
\begin{aligned}
    \mathcal{Z}^{l+1}&(v_n)=\sum \nolimits_{v_u \in \mathcal{N}(v_n)}( W_u \mathcal{Z}^{l}(v_u) + W_{n} \mathcal{Z}^{l}(v_n))\\
    &\mathcal{C}=\frac{\sigma (\mathcal{Z}(v_n))\cdot \log \sigma (\mathcal{Z}(v_n))}{\sum \nolimits_{v_u \in \mathcal{N}(v_n)}\sigma (\mathcal{Z}(v_u))\cdot \log \sigma (\mathcal{Z}(v_u))}
\end{aligned}
\end{equation}
where $\mathcal{N}(v_n)$ denotes the set of neighbor nodes of node $v_n$. $W$ is the weight matrix and $\sigma$ is the activation function. $\mathcal{Z}(*)$ is the embedding representation obtained from the GNN encoder, which can be selected from GIN \cite{gin}, MPNN \cite{mpnn}, GAT \cite{gat}, or GCN \cite{gcn}. 
For molecule $\mathcal{G}$, its irregular substructure $\mathcal{Z}(\mathcal{G}^s)$ is derived by aggregating the central node $v_n$ and its $K$-hop neighbours $v_n^k$, weighted by $\mathcal{C}$:
% For molecule $G$, its irregular substructure $Z(G^s)$ is obtained by aggregating the central node $v_n$ and the $K$-hop adjacent nodes along with $\mathcal{C}$ weighted accordingly:
\begin{equation}
    \label{irr}
    \mathcal{Z}({\mathcal{G}^s}) = \sum \nolimits_{k=1}^K \sum \nolimits_{{{v}_{u}}\in v_{n}^{k}}(\mathcal{Z}(v_n)||{{\mathcal{C}}} \cdot \mathcal{Z}(v_u))
\end{equation}
% 中，$N(v_n)$ 表示节点 $v_n$ 的邻居节点集。$W$ 是权重矩阵，$\sigma$ 是激活函数。$Z(*)$ 是 GNN 编码器得到的嵌入表示，可以从 GIN \cite{gin}、MPNN \cite{mPNn}、GAT \cite{gat}或 GCN \cite{gcn}中选择。对于分子$\mathcal{G}_x$，其不规则子结构$Z({\mathcal{G}_x^s})$是通过聚合中心节点$v_n$和相邻的$K$跳节点以及相应加权的$\mathcal{C}$得到的：

In the subsequent step, we calculate the interaction probability between substructures by Eq (\ref{eq:interpro}) to simulate the pre-adaptation process of substructures. We further optimize the molecular substructure representations by noise elimination with Eq (\ref{eq:gen}) results. The substructure interaction probability for pre-adaptation is calculated as follows:
% 接下来，我们通过公式（5）计算子结构间的交互概率来模拟子结构的预选适配，并根据静态交互概率和噪声剔除的方式进一步的优化分子子结构的表征。预选适配的子结构交互概率计算如下：
\vspace{-1mm}
\begin{equation}
\label{eq:interpro}
\mathcal{R}_i\!=\!\!\!\!\!\!\sum \limits_{\mathcal{G}_{y}^{s_j}\in \mathcal{G}_{y}^{s}} \!\!\!\!\!{\sigma(\mathcal{Z}(\!\mathcal{G}_{x}^{s_i}\!),\mathcal{Z}(\!\mathcal{G}_{y}^{s_j}\!))\!+\!\frac{1}{J\!\!-\!\!1}\!\!\!\!\! \sum \limits_{\mathcal{G}_{y}^{s_k} \neq \mathcal{G}_{y}^{s_j}}\!\!\!\!\!\! {\sigma(\mathcal{Z}(\!\mathcal{G}_{x}^{s_i}\!),\mathcal{Z}(\!\mathcal{G}_{y}^{s_k}\!))}}
\end{equation}
where $\sigma ()$ is a probability function. $\mathcal{G}_{x}^{s_i}$ and $\mathcal{G}_{y}^{s_j}$ denote the substructures of the paired molecules $(\mathcal{G}_{x},\mathcal{G}_{y})$. $\mathcal{G}_{y}^{s}$ represents the substructure set of molecule $\mathcal{G}_{y}$, containing $J$ elements. ReAlignFit eliminates unimportant substructures (i.e., those with lower $\mathcal{R}_i$) via the $\mathcal{R}_i$ and the noise rejection function $\lambda$ to control the effect of confounding information on substructure representations while generating the optimized substructure representation $\mathcal{Z}({\mathcal{G}}'^s)$:
% $G_{x}^{s_i}$和$G_{y}^{s_j}$表示配对分子$(G_{x},G_{y})$的子结构。$G_{y}^{s}$为分子$G_{y}$的子结构集合，数量为J。ReAlignFit通过$\mathcal{R}_i$ 和噪声剔除函数$\lambda$剔除不重要的子结构（即$\mathcal{R}_i$较低的子结构)以控制混淆信息对子结构表征的影响，同时生成优化的分子表示$Z({\mathcal{G}}')$：
\begin{equation}
\label{eq:gen}
\begin{aligned}
  \mathcal{Z}({\mathcal{G}}'^s)&=h({\mathcal{G}}^s,\lambda_i,\eta)={{\lambda }_{i}}\mathcal{Z}({\mathcal{G}^{s_i}})+(1-{{\lambda }_{i}})\eta\\
  {{\lambda }_{i}} \sim &{\text{Bernoulli}}( \sigma ( {\mathcal{R}_i} ) ), \eta \sim ( {{\mu }_{\mathcal{Z}(\mathcal{G}_{{}}^{s})}},\sigma _{\mathcal{Z}(\mathcal{G}^{s})}^{2})
\end{aligned}
\end{equation}
where ${{\mu }_{\mathcal{Z}(\mathcal{G}_{{}}^{s})}}$ and $\sigma _{\mathcal{Z}(\mathcal{G}_{{}}^{s})}^{2}$ are mean and variance of $\mathcal{Z}(\mathcal{G}_{{}}^{s})$, respectively. The refined $\mathcal{Z}({\mathcal{G}}'^s_x)$ and $\mathcal{Z}({\mathcal{G}}'^s_y)$ are utilized to dynamic representational alignment.





\subsubsection{Dynamic Representational Alignment Module}
\label{Dynamic Representation Alignment with Bias Correction Function} 
% Representational alignment methods that introduce inductive bias through attention mechanisms rely on static molecular features, overlooking the dynamic adjustments in molecular reactions. This static alignment method fails to effectively capture the core substructures that truly determine chemical reactions, making it challenging to ensure the stability of MRL in distribution-shifted data.
Representational alignment methods introducing inductive bias via attention mechanisms rely on static combinations between substructures, ignoring dynamic adjustments in reactions. This limits their ability to capture core substructures driving different chemical reactions. 
The Induced Fit theory highlights conformational changes of paired substructures, \textit{i.e.}, the dynamic variations of substructure topologies and binding sites during molecular interactions. 
Therefore, we design the BCF to dynamically adjust substructure binding sites and representational alignment while integrating the S-GIB to identify highly functional compatible substructures (core substructures).
% The detailed implementation of DRAM and BCF is as follows.

% 通过注意力机制引入归纳偏差的表示对齐方法通常基于分子静态特征，而忽略了分子反应中的动态调整。这种静态的对齐方法无法有效捕捉真正决定化学反应的核心子结构，难以保证分布变化数据中MRL的稳定性。诱导契合理论强调子结构的构象变化，即在分子结合过程中子结构拓扑结构和结合位点的动态变化。因此，我们设计Bias Correction Function来动态的调整子结构结合位点和表示对齐，并结合S-GIB实现高功能兼容性子结构（核心子结构）的筛选和优化。

Considering that GNN-based molecular representation methods may ignore the connecting mechanisms of the chemical bonds, we employ a substructure edge reconstruction strategy in DRAM to represent substructure information as accurately as possible. 
Specifically, we remove the edges between all substructures $\mathcal{G}^s$ in $\mathcal{G}$ at the initial stage. DRAM computes the Bernoulli distribution $e_{ik}$ between substructures $\mathcal{G}^{s_i}$ and $\mathcal{G}^{s_k}$ by edge sampling, and reconstruct the edges with $e_{ik}=1$. $e_{ik} \sim \text{Bernoulli}(\theta_{ik})$, where $\theta_{ik}$ is computed using a Gaussian Kernel function. The new substructure obtained in reconstruction is denoted as $\mathcal{G}^{news}$.
%考虑到给予GNN的分子表示方法可能忽略了分子化学键的连接机制，因此，我们在DRAM中采用子结构边重建的方式尽可能准确的表征子结构信息。具体来说，初始阶段我们移除分子$G$中所有子结构$G^s$之间的边。我们通过边采样的方式计算子结构$G^{s_i}$和$G^{s_k}$间的伯努利分布$e'_{ik}$，重建$e'_{ik}=1$的边。$e'_{ik}~Bernoulli(\theta_{i,k})$，$\theta_{i,k}$由高斯核函数计算得到。重建后获得的新子结构表示为$G^{new}$。

After edge reconstruction, we design the Bias Correction Function $\gamma$ to quantify the degree of alignment between substructures before and after reconstruction. The computation of $\gamma$ is provided in Eq (\ref{pxc}). 
% 经过边重建后，我们设计了Bias Correction Function $\gamma$来表征重建前后子结构间的对齐程度。$\gamma$的计算方式如公式（5）所示。
\begin{equation}
\label{align}
    \gamma \!\!=\!\!\!\sum \limits_{j\leq J}\!\!\frac{2\left\| \sigma(\mathcal{Z}(\mathcal{G}_{x}^{news}),\mathcal{Z}(\mathcal{G}_{y}^{s_j})) \right\|_{2}^{2}}{J(\left\| \sigma(\!\mathcal{Z}(\!\mathcal{G}_{x}^{s_i}\!),\!\mathcal{Z}(\!\mathcal{G}_{y}^{s_j}\!)\!) \right\|_{2}^{2}\!+\!\left\| \sigma(\!\mathcal{Z}(\!\mathcal{G}_{x}^{s_k}\!),\!\mathcal{Z}(\!\mathcal{G}_{y}^{s_j}\!)\!) \right\|_{2}^{2})}
\end{equation}
where $\mathcal{G}_{x}^{news}$ is reconstructed from the substructures $\mathcal{G}_{x}^{s_i}$ and $\mathcal{G}_{x}^{s_k}$ of molecule $\mathcal{G}_x$. $\mathcal{G}_{y}^{s_j}$ represents the substructure of molecule $\mathcal{G}_y$ paired with $\mathcal{G}_x$. 

In the Induced Fit theory, conformational changes aim to achieve functional compatibility at binding sites, which can be approximately quantified through similarity in vector space~\cite{stiller2022structure}. Therefore, we employ cosine similarity to compute the components of $\gamma$, quantifying the chemical functional compatibility between substructures.
% 其中，$G_{x}^{news}$是由分子$G_x$中的子结构$G_{x}^{s_i}$和$G_{x}^{s_k}$重建得到。$G_{y}^{s_j}$是与$G_x$相配对的分子$G_y$的子结构。诱导契合理论中的构象变化是为了实现化学反应中结合位点的功能兼容性，而这种兼容性可以通过向量空间中的相似性来近似量化。因此，我们采用了余弦相似度来计算$\gamma$中的各项结果，以量化子结构间化学功能的兼容性。
% 种变化往往是为了实现结合位点间的化学或几何特性匹配。余弦相似度可以量化分子或子结构在特征空间中的相似性，反映结合位点在化学功能（如电子分布、疏水性等）上的潜在兼容性。

DRAM dynamically determines whether to perform edge reconstruction and align substructure representation.  based on $\gamma$. The substructure representation after dynamic adjustment is defined as:
% DRAM根据$\gamma$来动态的决定是否进行边重建和对齐子结构表征，如公式（6）所示。
\begin{equation}
    \mathcal{G}^{c}_x =
\begin{cases} 
  \text{if } \gamma \geq 1, & \mathcal{G}_{x}^{news} \\
  \text{if } \gamma < 1, & 
  \begin{cases}
      \text{if } \sigma_{ij} \geq \sigma_{kj}, & \mathcal{G}_{x}^{s_i}\\
      \text{if } \sigma_{ij} < \sigma_{kj}, & \mathcal{G}_{x}^{s_k}\\
  \end{cases} \\
\end{cases}
\label{if}
\end{equation}
where $\sigma_{*j}=\left\| \sigma(\mathcal{Z}(\mathcal{G}_{x}^{s_*}),\mathcal{Z}(\mathcal{G}_{y}^{s_j})) \right\|_{2}^{2}$. The substructure $\mathcal{G}^{c}_y$ after dynamic adjustment of $\mathcal{G}_y$ can be computed similarly.

Considering the advantages of the Graph Information Bottleneck (GIB) in graph optimization and the specific requirements of MRL tasks~\cite{hu2024GIBsurvey,li2022oodtkde,lee2023cgibICML}, we further extend the optimization objective of GIB to the subgraph level for core substructure selection and optimization. The refined aim is defined as follows:
% 考虑到图信息瓶颈在图优化中的先进性和MRL任务的具体需求，我们进一步将GIB的优化目标扩展到了子图层级并用于核心子结构的优化。优化后的目标定义如下：
\begin{definition}
    \label{Definition 1}
    (\textit{\textbf{S-GIB}}) \textit{Given a set of graphs and their interaction relationships $({\mathcal{G}_{x}}, \mathcal{Y}, {\mathcal{G}_{y}})$, $(\mathcal{G}^{c}_x, \mathcal{G}^{c}_y)$ and $(\mathcal{G}^{n}_x, \mathcal{G}^{n}_y)$ are the core subgraph pairs and confounding subgraph pairs of $({\mathcal{G}_{x}}, {\mathcal{G}_{y}})$ and $\mathcal{Y}$, respectively. The subgraph ${\mathcal{G}^{s}} = \{\mathcal{G}^{c}, \mathcal{G}^{n}\}$. According to the minimal sufficient principle of mutual information, the optimization objective is defined as:}
\begin{equation}
\label{eq:CSGIBnew}
\mathcal{G}^{co}\!\!=\!{\mathop{\arg\! \min }}(\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\!\mathcal{G}_{x}^{n})\!+\!\beta \mathcal{I}(\mathcal{G}_{y}^{c},\!\mathcal{G}_{y}^{n})\!-\!\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c}))
\end{equation}
\end{definition}
% \textbf{Definition 1.} (\textit{\textbf{S-GIB}}) \textit{Given a set of graphs and their interaction relationships $({\mathcal{G}_{x}}, \mathcal{Y}, {\mathcal{G}_{y}})$, $(\mathcal{G}^{c}_x, \mathcal{G}^{c}_y)$ and $(\mathcal{G}^{n}_x, \mathcal{G}^{n}_y)$ are the core subgraph pairs and confounding subgraph pairs of $({\mathcal{G}_{x}}, {\mathcal{G}_{y}})$ and $\mathcal{Y}$, respectively. The subgraph ${\mathcal{G}^{s}} = \{\mathcal{G}^{c}, \mathcal{G}^{n}\}$. According to the minimal sufficient principle of mutual information, the optimization objective is defined as:}
% \begin{equation}
% \label{eq:CSGIBnew}
% \mathcal{G}^{co}\!\!=\!{\mathop{\arg\! \min }}(\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\!\mathcal{G}_{x}^{n})\!+\!\beta \mathcal{I}(\mathcal{G}_{y}^{c},\!\mathcal{G}_{y}^{n})\!-\!\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c}))
% \end{equation}
$-\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ allows the model to fully learn the information of core subgraphs relevant to the prediction target. $\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})+\beta \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$ minimizes the influence of confounding subgraphs on core subgraphs by eliminating confounding information.
The detailed solution process for S-GIB will be presented in Section \ref{Synergistic Optimization Loss Function for ReAlignFit}.
% 我们将在section4.2中详细展示S-GIB的求解过程。

We regard core substructures as core subgraphs in S-GIB and non-core substructures as confounding subgraphs. During the optimization of S-GIB, core substructures representing stable molecular representations are identified, while the influence of other substructures on molecular representations is minimized.
% 我们将核心子结构视为S-GIB中的核心子图，非核心子结构视为混淆子图。在S-GIB优化的过程中实现对代表分子稳定表征的核心子结构的筛选，并降低其他子结构对分子表征的影响。
Through the synergy between Eqs (\ref{align}), (\ref{if}) and (\ref{eq:CSGIBnew}), ReAlignFit dynamically selects and aligns the embedded representations of core substructures determining chemical reactions with full consideration of paired molecules.
% $G_y$的核心子结构$G^{core}_y$可通过同样方式计算得到.基于公式6和7之间的协作，ReAlignFit在充分考虑配对分子的情况下，实现影响分子间化学反应的核心子结构的动态筛选和嵌入表示对齐。经过S-GIB优化的核心子结构为$G^{co}$。







% Through the synergy between Eqs (\ref{align}) and (\ref{if}), ReAlignFit dynamically selects and aligns the embedded representations of core substructures determining chemical reactions with full consideration of paired molecules.

The core substructure optimized by S-GIB is denoted as $\mathcal{G}^{co}$.
The final embedding representation $\mathcal{H}$ is generated by aggregating the core substructures of the molecules.
\begin{equation}
    \label{embedding}
    \mathcal{H}=\text{Readout}(\mathcal{Z}(\mathcal{G}^{co_1}) || \cdots || \mathcal{Z}(\mathcal{G}^{co_M})), M \ll  N
\end{equation}
% 我们通过聚合分子的核心子结构生成最终的嵌入表示$Z(G^{st})$。
where $M$ is the number of core substructures.

% \vspace{-3mm}
\subsection{Model Optimization and S-GIB Solution}
\label{Synergistic Optimization Loss Function for ReAlignFit}
We design the loss function $\mathcal{L}$ with Theorem \ref{theorem 1} and demonstrate the transformation and solution process of S-GIB.
%我们在理论1的启发下设计了模型损失函数$\mathcal{L}$，并展示了$\mathcal{L}$如何与S-GIB的优化相结合。

\subsubsection{Model Loss Function}
We design the model optimization objective $\mathcal{L}$ consistent with the derived conclusion in Theorem \ref{theorem 1}, which consists of the sum of prediction loss ${\mathcal{L}}_{pred}$ and calibration loss $\mathcal{L}_{KL}$.
\begin{equation}
\label{lossnum}
   \mathcal{L}= {\mathcal{L}}_{pred}+\alpha \mathcal{L}_{KL}^{\alpha }+\beta \mathcal{L}_{KL}^{\beta }
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters that consistent with the settings in Eq~(\ref{eq:CSGIBnew}). To reduce the complexity of mutual information calculation, we transform Eq (\ref{eq:CSGIBnew}) as follows. 

\subsubsection{The Upper Bound of $\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n}) + \beta \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$ in Eq (\ref{eq:CSGIBnew})}
% 优化求解
We utilize information-theoretic principles to derive the upper bounds of $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})$ and $\mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$, respectively.
% 我们通过信息论原理来分别求解$\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})$,$\beta \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$下界。

\begin{lemma}
    \label{Proposition 1}
    \textbf{(Upper bound of $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})$)}
\textit{Since $\mathcal{G}^{c}_x$, $\mathcal{G}^{n}_x$ are subgraphs of ${G}_x$, according to the information transferability of Markov chain, we have
\begin{equation}
\label{eq:op1}
\begin{aligned}
\mathcal{I}(\mathcal{G}_{x}^{c},\!\mathcal{G}_{x}^{n})\!&\le\! \min (\mathcal{I}({\mathcal{G}^{c}_x},\mathcal{G}_x),\mathcal{I}({\mathcal{G}^{n}_x},\mathcal{G}_x))\\
&=\min \! (\!\!\int \!\!\!\!\! \int\!\!p(\!\mathcal{G}^c_x|{G}_x,\!\gamma)\! \log(\!\frac{p(\!\mathcal{G}^c_x|\mathcal{G}_x,\!\gamma)}{p(\mathcal{G}^c_x)}\!) d\mathcal{G}^c_xd\mathcal{G}_x, \\
&\quad  \!\!\!\int \!\!\!\!\! \int\!\!p(\mathcal{G}^n_x|\mathcal{G}_x,\gamma) \log(\frac{p(\mathcal{G}^n_x|\mathcal{G}_x,\gamma)}{p(\mathcal{G}^n_x)}) d\mathcal{G}^n_xd\mathcal{G}_x )\\
&:=\min(KL(p(\mathcal{G}^c_x|\mathcal{G}_x,\gamma)||p(\mathcal{G}^c_x)),\\
&\quad \quad \quad KL(p(\mathcal{G}^n_x|\mathcal{G}_x,\gamma)||p(\mathcal{G}^n_x))))\\
&=\min(\mathcal{L}_{KL_{x}}^{c},\mathcal{L}_{KL_{x}}^{n})
\end{aligned}
\end{equation}
where $p()$ is a posterior distribution function.}
\end{lemma}


Therefore, minimizing $\mathcal{L}_{KL}^{\alpha }=\min (\mathcal{L}_{K{{L}_{x}}}^{c},\mathcal{L}_{K{{L}_{x}}}^{n})$ provides an upper bound for the minimization of $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})$. Similarly, the upper bound for minimizing $I(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$ can be obtained by minimizing $\mathcal{L}_{KL}^{\beta }=\min (\mathcal{L}_{K{{L}_{y}}}^{c},\mathcal{L}_{K{{L}_{y}}}^{n})$.


\textbf{The proof of Eq (\ref{eq:op1})}.
% % $I_c=I(G^c,G)$表示为积分形式：
% The $\mathcal{I}(\mathcal{G}^c,\mathcal{G})$ is expressed in integral form as:
% \begin{equation}
% \label{eq:op1-1}
% \begin{aligned}
% \mathcal{I}(\mathcal{G}^c,\mathcal{G})=\iint p(\mathcal{G}^c,\mathcal{G})\log(\frac{p(\mathcal{G}^c,\mathcal{G})}{p(\mathcal{G}^c)p(\mathcal{G})})d\mathcal{G}^cd\mathcal{G}
% % \\
% % &=-H(G)-H(G^c|G).
% \end{aligned}
% \end{equation}
% 我们利用公式\ref{eq:nos}中的概率函数$R$调整条件概率分布$p(G^c,G)$，且$G^c$、$G^n$条件独立于$G$。因此，可以得到：
The $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ is expressed in integral form as:
\begin{equation}
\label{eq:op1-1}
\begin{aligned}
\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)&=\int \!\!\!\! \int p(\mathcal{G}^c_x,\mathcal{G}_x)\log(\frac{p(\mathcal{G}^c_x,\mathcal{G}_x)}{p(\mathcal{G}^c_x)p(\mathcal{G}_x)})d\mathcal{G}^c_xd\mathcal{G}_x
% \\
% &=-H(G)-H(G^c|G).
\end{aligned}
\end{equation}

The probability function $\gamma$ in Eq (\ref{align}) is utilized to adjust the conditional probability distribution $p(\mathcal{G}^c_x,\mathcal{G}_x)$, and $\mathcal{G}^c_x$, $\mathcal{G}^n_x$ are conditionally independent of $\mathcal{G}_x$. We can obtain $p(\mathcal{G}^c_x|\mathcal{G}_x)=p(\mathcal{G}^c_x|\mathcal{G}_x,\gamma)$.
% 则公式\ref{eq:op1-1}进一步表示为：
Further, $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ is expressed as:
\begin{equation}
\begin{aligned}
\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)&\!\!=\!\!\int \!\!\!\! \int \!\! p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma) \log(\frac{p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma)}{p(\mathcal{G}^c_x)}) d\mathcal{G}_x^cd\mathcal{G}_x\\
&:= {KL}(p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma)||p(\mathcal{G}^c_x))\\
& = \mathcal{L}_{KL_x}^c
\end{aligned}
\label{eq:op1-2}
\end{equation}

The result of Eq (\ref{eq:op1-2}) indicates that $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ can be computed by the KL divergence between $\mathcal{G}^c_x$ and $\mathcal{G}_x$. Similarly, $\mathcal{I}(\mathcal{G}^n_x,\mathcal{G}_x)={KL}(p(\mathcal{G}^n_x|\mathcal{G}_x,\gamma)||p(\mathcal{G}^n_x))=\mathcal{L}_{KL_x}^n$.
% $I_{c}$可用$G^c$和$G$之间的KL散度计算得到。同理我们可以得到$I_{n}=I(G^n,G)=\mathcal{L}_{KL}^n(G^n,G)$.

Finally, we have the following equation as:
\begin{equation}
    \mathcal{I}(\mathcal{G}^c_x,\mathcal{G}^n_x)\le \min (\mathcal{L}_{KL_x}^c,\mathcal{L}_{KL_x}^n)
    % (\mathcal{L}_{KL}^c(\mathcal{G}^c_x,\mathcal{G}_x),\mathcal{L}_{KL}^n(\mathcal{G}^n_x,\mathcal{G}_x))
\end{equation}



\textbf{The solution to Eq (\ref{eq:op1})}. To reduce the difficulty of mutual information computation in Eq (\ref{eq:op1}), we decompose probability distributions $p(\mathcal{G}^c|\mathcal{G},\gamma)$ and $p(\mathcal{G}^c)$ into variational approximation and multivariate Bernoulli distribution, respectively.

We redefine $p(\mathcal{G}^c)$ using the variational approximation $\omega(\mathcal{G}^c)$, represented by $e_{ik}$. 
$\omega(\mathcal{G}^c) =e_{ik}^{|\mathcal{G}^{s_i}|}(1-e_{ik}^{|\mathcal{G}^{s_i}|/|\mathcal{G}^{s_k}|})$. 
Inspired by \cite{TGNN}, we parameterize $p(\mathcal{G}^c_x|\mathcal{G}_x,\gamma)$ as a multivariate Bernoulli distribution:
\begin{equation}
    \label{multbern}
    p(\mathcal{G}^c_x|\mathcal{G}_x,\gamma) = \prod_{\mathcal{G}_x^{s_{i}} \in \mathcal{G}^c_x} p_x^c \cdot \prod_{\mathcal{G}_x^{s_{i}} \notin \mathcal{G}^c_x} (1 - p_x^c)
\end{equation}
where $p_x^c$ is the probability distribution given $\mathcal{G}_x$ and $\gamma$, which can be computed via $p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)$.
\begin{equation}
\label{pxc}
     p_x^c \!= \! p(\mathcal{G}^c_x|\mathcal{G}^n_x,\!\mathcal{G}_x) \!\! = \! \sigma \! \left( \mathcal{Z}(\mathcal{G}^c_x),\!(\!\mathcal{Z}(\mathcal{G}^{n_1}_x)||\cdots||\mathcal{Z}(\mathcal{G}^{n_i}_x\!)\!) \right)
\end{equation}
where $\sigma(*)$ is a sigmoid function. 
Therefore, the $\mathcal{L}_{KL_{x}}^{c} $ in Eq (\ref{eq:op1}) is calculated as follows:
\begin{equation}
    \label{klxc}
    \begin{aligned}
 \mathcal{L}_{KL_{x}}^{c}&=KL[p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)||\omega(\mathcal{G}^c)]\\
        &=\mathbb{E}_{\sim \mathcal{G}^c_x,\mathcal{G}_x} \!\!\! \left[\!\sum \limits p_x^c \log\frac{p_x^c}{e}\! +\! (1\!-\!p_x^c)\log\frac{1\!\!-\!\!p_x^c}{1\!\!-\!\!e}\!\right]
    \end{aligned}
\end{equation}

Similarly, we can obtain the $\mathcal{L}_{KL_{x}}^{n}$, $\mathcal{L}_{KL_{y}}^{c}$ and $\mathcal{L}_{KL_{y}}^{n}$.

\subsubsection{The Lower Bound of $\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ in Eq (\ref{eq:CSGIBnew})}
% 优化求解
We compute the lower bound of $\mathcal{I}(\mathcal{ Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ using the given graph pair $(\mathcal{G}_{x},\mathcal{G}_{y})$ and the label $\mathcal{Y}$. 
%我们采用给定图$（\mathcal{G}_{x}，\mathcal{G}_{y}）$的核心子图$(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$和标签$\mathcal{Y}$计算$\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$的下界。

\begin{lemma}
    \label{Proposition 2}
    \textbf{(Lower bound of $\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$)}
\textit{Given a graph pair $(\mathcal{G}_{x},\mathcal{G}_{y})$, its label information $\mathcal{Y}$, and the learned core subgraphs $(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$, we have:
%给定一对图$（\mathcal{G}_{x}，\mathcal{G}_{y}）$，它的标签信息$\mathcal{Y}$，以及学习到核心子图的$(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$，我们有
\begin{equation}
\label{eq:op2}
\begin{aligned}
\mathcal{I}_{ca}\!&=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})\\
% =\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\! p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}) \log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{p(\mathcal{Y})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
&=H(\mathcal{Y})\!+\!\!\int\!\!{p(\mathcal{Y})\!\!\int\!\!\!\!\!\int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}\\
&\ge \int{p(\mathcal{Y})(\int\!\!\!\!\!\int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}}\\
& := \frac{1}{NM}\sum\nolimits_{n=1}^{N}{\sum\nolimits_{m=1}^{M}{\!q(\gamma (\mathcal{G}_{{{x}_{n}}}^{c},\mathcal{G}_{{{y}_{m}}}^{c}\!)|{{Y}}\!)}}\\
&=-{{\mathcal{L}}_{pred}}\\
\mathcal{F}&_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\!=\!q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\log(\frac{p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}{q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})})
% (q(\gamma (\mathcal{G}_{{{x}_{n}}}^{c},\mathcal{G}_{{{y}_{m}}}^{c})|{\mathcal{Y}})|\hat{\mathcal{Y}})
\end{aligned}
\end{equation}
where $q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$ is the variational approximation distribution used to approximate the posterior distribution $p()$. }
\end{lemma}


The Eq (\ref{eq:op2}) indicates that minimizing the prediction loss ${{\mathcal{L}}_{pred}}$ achieves the minimization of $-\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$. 



\textbf{The proof of Eq (\ref{eq:op2})}.
For the term $\mathcal{I}_{ca}=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$, by definition:
\begin{equation}
    \mathcal{I}_{ca} \!\!=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\!\! p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})\! \log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{p(\mathcal{Y})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}
\end{equation}

% For the term $\mathcal{I}_{ca}=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$, 
% , by definition:
% \begin{equation}
%     I_{ca} \!\!=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\!\! p(Y,G_{x}^{c},G_{y}^{c})\! \log (\frac{p(Y,G_{x}^{c},G_{y}^{c})}{p(Y)})dY\!dG_{x}^{c}dG_{y}^{c}
% \end{equation}
We introduce the variational approximation distribution $q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$ to approximate the conditional probability distribution $p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$. Then, the $\mathcal{I}_{ca}$ is expressed as:
% 引入变分近似分布A来近似条件概率分布B，我们可以得出以下等式：
\begin{equation}
\begin{aligned}
    \mathcal{I}_{ca}\!\!&
    = \!\!\!\!\int\!\!\!p(\mathcal{Y})\!\!\!\int\!\!\!\!\! \int\!\!\! q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
    &=\!\!\!\!\int\!\!\!p(\!\mathcal{Y}\!)\!\!\!\int\!\!\!\!\! \int\!\!\! q(\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c}|\mathcal{Y}\!)\!\log (\!\frac{p(\mathcal{Y})p(\mathcal{G}_{x}^{c}\!,\!\mathcal{G}_{y}^{c}|\mathcal{Y})}{q(\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c})}\!)d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
    &=\!\!\!\!\int\!\!p(\mathcal{Y})(\log p(\mathcal{Y})+\!\!\!\int\!\!\!\!\! \int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}\\
    &=\!\!\!\!\int\!\!p(\mathcal{Y})\log p(\mathcal{Y})d\mathcal{Y}\\
    &+\!\!\!\!\int\!\!p(\mathcal{Y})\!\!\!\int\!\!\!\!\! \int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}\\
    &=\!H(\mathcal{Y})\!\!+\!\!\!\int\!\!{p(\mathcal{Y})\!\!\iint\!\!{{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}
\end{aligned}
\end{equation}

$H(\mathcal{Y})$ is the entropy of $\mathcal{Y}$. Finally, we have the following equation as:
\begin{equation}
\mathcal{I}(\mathcal{Y};\!\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c})\!\ge\!\! \int\!\!{p(\mathcal{Y})\!\!\int\!\!\!\!\!\int\!\!{{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}
\end{equation}




\textbf{The solution to Eq (\ref{eq:op2})}.
The prediction term $q(\gamma (\mathcal{G}_{{{x}}}^{c},\mathcal{G}_{{{y}}}^{c})|{\mathcal{Y}})$ in Eq (\ref{eq:op2}) emphasizes leveraging the core substructure pair $(\mathcal{G}_{{x}}^{c},\mathcal{G}_{{y}}^{c})$ to predict the relationship between molecules $\mathcal{G}_{{x}}$ and $\mathcal{G}_{{y}}$.

According to Eq (\ref{embedding}), the embedding representations of molecules $\mathcal{G}_{{{x}}}$ and $\mathcal{G}_{{{y}}}$, composed of their core substructures, are expressed as $\mathcal{H}_x$ and $\mathcal{H}_y$, respectively. Finally, the prediction loss ${{\mathcal{L}}_{pred}}$ of ReAlignFit is denoted as:
\begin{equation}
    \label{loss}
    {{\mathcal{L}}_{pred}}=\frac{-1}{MN}\mathbb{E}_{(\mathcal{G}_x,\mathcal{G}_y)\sim \mathcal{Y}}[\log (\sigma(\mathcal{H}_x,\mathcal{H}_y))|\mathcal{Y}]
\end{equation}
% where $M$ and $N$ represent the number of molecules ${G}$ and $\mathcal{G}$.

In MRL, ${{\mathcal{L}}_{pred}}$ can be chosen as the cross-entropy loss for Drug-Drug Interaction (DDI) prediction or the mean absolute error loss for Molecular Interaction (MI) prediction.
% , depending on the nature of the task.

\begin{algorithm}[htpb]
\caption{ReAlignFit}\label{alg:ReAlignFit}
\textbf{Input}: Dataset $\mathcal{D}=(\mathcal{G}_x, \mathcal{Y}, \mathcal{G}_y)$, SRIN training epochs $T_1$, DRAM training epochs $T_2$, Model training iterations $T$\\
    % \textbf{Parameter}: Optional list of parameters\\
    \textbf{Output}: Prediction result $\hat{\mathcal{Y}}$
    \begin{algorithmic}[1]
        \STATE Convert SMILES sequences into graph structures;
        \STATE Initialize training parameters;
        \FOR{$t=1$ to $T$}{
        \FOR{$t_1=1$ to $T_1$}{
        \STATE Compute substructure embeddings $\mathcal{Z}(\mathcal{G}^s)$ by Eq (\ref{irr});
        \STATE Obtain interaction-optimized substructure representations $\mathcal{Z}(\mathcal{G}'^s)$ based on Eqs (\ref{eq:interpro}) and (\ref{eq:gen});
        }
        \ENDFOR
        \FOR{$t_2=1$ to $T_2$}{
        \STATE Generate reconstructed substructures $\mathcal{Z}(\mathcal{G}^{news})$ via $e_{ik}$;
        \STATE Obtain aligned core substructure pairs according to Eqs (\ref{align}) and (\ref{if});
        \STATE Optimize S-GIB and generate refined substructure pairs with Eqs (\ref{klxc}) and (\ref{loss});
        \IF{$\mathcal{L}$ in Eq (\ref{lossnum}) converges}
        \STATE \textbf{Return} the S-GIB-optimized representation $\mathcal{H}$;
        \ELSE
        \STATE $t_2 \leftarrow t_2+1$;
        \ENDIF
        }
        \ENDFOR
        }
        \ENDFOR
        \STATE Output the prediction result $\hat{\mathcal{Y}}$, computed from $\mathcal{H}$.
    \end{algorithmic}
\end{algorithm}

\subsection{Model Analysis}
\label{Model Analysis}
We train ReAlignFit by the iterative optimization between SRIN and DRAM. The pseudocode is provided in Algorithm \ref{alg:ReAlignFit}.
The computational complexity of ReAlignFit is mainly derived from iterative optimization. The complexity of SRIN is $\mathcal{O}(T_1\cdot L\cdot |N|^2)$, and that of DRAM is $\mathcal{O}(T_2 \cdot J^2)$. 
$T$ represents the number of iterations, $L$ is GNN layers. Consequently, the overall computational complexity of ReAlignFit is approximately $\mathcal{O}(T_1\cdot L\cdot |N|^2 +T_2 \cdot J^2)$. Owing to $J \ll N$, the added computational overhead is still manageable.
% 我们采用SRIN和DRAM之间的迭代优化的方式训练ReAlignFit。附录C中的算法1给出了伪代码。ReAlignFit的计算复杂度主要来源于模型的迭代优化。SRIN的计算复杂度为A，DRAM的计算复杂度为B。其中，$T$为迭代次数，L为GNN层数，E和H分别为分子的边的数量和子结构数量。因此，ReAlignFit的计算复杂度大约为D。由于F，增加的额外计算复杂度仍可以接受。



\section{Experiments}
\label{Experiments}
In this section, we validate the prediction performance of ReAlignFit in two types of tasks on nine datasets. By analyzing and summarizing the relevant experiments, we aim to answer the following research questions: 
\begin{itemize}
    \item \textbf{RQ1}: How does the performance of ReAlignFit in MRL, and whether it is susceptible to backbone? 
    \item \textbf{RQ2}: Can ReAlignFit improve the stability of MRL in distribution-shifted data?
    \item \textbf{RQ3}: What is the key to ReAlignFit's performance improvement?
    \item \textbf{RQ4}: How does confounding information affect the performance of ReAlignFit?
    \item \textbf{RQ5}: Can the results of ReAlignFit be visually supported?
\end{itemize}



\subsection{Experimental Setup}
\label{Experimental Setup}

\subsubsection{Datasets}
Following the related research \cite{lee2023cgibICML,lee2023shiftKDD,boulougouri2024molecularMNI,du2024mmgnnIJCAI}, we conduct extensive Molecular Interaction (MI) prediction and Drug-Drug Interaction (DDI) prediction experiments on nine datasets, as detailed in Table \ref{dataset}. Chromophore \cite{chromophore}, MNSol \cite{mnsol}, FreeSolv \cite{freesolv}, CompSol \cite{comsol}, Abraham \cite{abraham}, and CombiSolv \cite{combisolv} are chromophore datasets used to describe the free energy of solutes in solvents. These six datasets are widely used for MI prediction \cite{lee2023cgibICML,lee2023shiftKDD,du2024mmgnnIJCAI}. ZhangDDI \cite{zhangddi}, HetionetDDI \cite{hddi}, and DrugBankDDI (pay version) \cite{drugbankddi} are commonly utilized for DDI prediction \cite{zhang2024heterogeneousIJCAI}. Following the setup of related work in MRL \cite{lee2023cgibICML,lee2023shiftKDD}, we divide the datasets into training, validation, and test sets with the ratio of 6:2:2. For DDI datasets that contain only positive examples, we generated negative samples using rule matching and scaffold clustering methods, respectively. To better simulate the real-world data distribution, we set up in-distribution data (Original) with the same distribution as the original data and two different distribution-shifted data (P1 and P2) for DDI prediction, which were used for ReAlignFit learning.
\begin{table}[htpb]
    \centering
    \renewcommand{\arraystretch}{1.15}
    \caption{Data statistics for different datasets.}
    \vspace{-1mm}
    \setlength{\tabcolsep}{0.6mm}{
    \begin{tabular}{c|c|ccccccc}
    \hline
        \multicolumn{2}{c|}{Dataset} & \#$G_x$ & \#$G_y$ & \#Pairs & \#Tra & \#Val & \#Tes  & Task\\
        \hline
        \multirow{3}{*}{Chromophore} & Absorption & 6416 & 725 & 17276 & 10366 & 3455 & 3455 & MI \\	
        &  Emission & 6412 & 1021 & 18141 & 10885 & 3628 & 3628 & MI\\
        & Lifetime & 2755 & 247 & 6960 & 4176 & 1392 & 1392 & MI\\
        \hline
        \multicolumn{2}{c|}{MNSol} & 372 & 86 & 2275 & 1365 & 455 & 455 & MI\\
        \multicolumn{2}{c|}{FreeSolv} & 560 & 1 & 560 & 336 & 112 & 112 & MI\\
        \multicolumn{2}{c|}{CompSol} & 442 & 259 & 3548 & 2130 & 709 & 709 & MI\\
        \multicolumn{2}{c|}{Abraham} & 1038 & 122 & 6091 & 3655 & 1218 & 1218 & MI\\
        \multicolumn{2}{c|}{CombiSolv} & 1495 & 326 & 10145 & 6087 & 2029 & 2029 & MI\\
        \hline
        \multicolumn{2}{c|}{ZhangDDI} & 544 & 544 & 40255 & 48306 & 16102 & 16102 & DDI\\
        \multicolumn{2}{c|}{HetionteDDI} & 696 & 696 & 6410 & 7692 & 2564 & 2564 & DDI \\
        \multicolumn{2}{c|}{DrugBankDDI} & 2045 & 2045 & 291822 & 350186 & 116729 & 116729 & DDI \\
        \hline
    \end{tabular} 
    }
    \vspace{-5mm}
    \label{dataset}
\end{table}

\begin{table*}[htpb]
\caption{The performance of ReAlignFit and comparative methods in MI prediction, with the best results highlighted in \colorbox{c1!80}{\textbf{bold}} and the second results highlighted in \colorbox{c1!40}{text}.}
\vspace{-2mm}
\small
    \centering
    \renewcommand{\arraystretch}{1.05}
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{lcccccccc}
    \hline
        \multirow{2}{*}{Model} & \multicolumn{3}{c}{Chromophore} & \multirow{2}{*}{MNSol} & \multirow{2}{*}{FreeSolv} & \multirow{2}{*}{CompSol} & \multirow{2}{*}{Abraham} & \multirow{2}{*}{CombiSolv}\\
        \cline{2-4}
         & Absorption & Emission & Lifetime &  &  &  &  &  \\
          \hline
          \multicolumn{9}{c}{\textbf{No Representational Alignment}}\\
          \hline
        GCN(ICLR'17) & 25.75$\pm$1.48 & 31.87$\pm$1.70 & 0.866$\pm$0.015 & 0.675$\pm$0.021 & 1.192$\pm$0.042 & 0.389$\pm$0.009 & 0.738$\pm$0.041  & 0.672$\pm$0.022\\
        GAT(ICLR'18) & 26.19$\pm$1.44 & 30.90$\pm$1.01 & 0.859$\pm$0.016 & 0.731$\pm$0.007 & 1.280$\pm$0.049 & 0.387$\pm$0.010 & 0.798$\pm$0.038 & 0.662$\pm$0.021\\
        MPNN(ICML'17) & 24.43$\pm$1.55 & 30.17$\pm$0.99 & 0.802$\pm$0.024 & 0.682$\pm$0.017 & 1.159$\pm$0.032 & 0.359$\pm$0.011  & 0.601$\pm$0.035 & 0.568$\pm$0.005\\
       GIN(ICLR'19) & 24.92$\pm$1.67 & 32.31$\pm$0.26 & 0.829$\pm$0.027 & 0.669$\pm$0.017 & 1.015$\pm$0.041 & 0.331$\pm$0.016 & 0.648$\pm$0.024 & 0.595$\pm$0.014\\
       CIGIN(AAAI'20) & 19.32$\pm$0.35 & 25.09$\pm$0.32 & 0.804$\pm$0.010 & 0.607$\pm$0.024 & 0.905$\pm$0.014 & 0.308$\pm$0.018 & 0.411$\pm$0.008 & 0.451$\pm$0.009\\
        \hline
        \multicolumn{9}{c}{\textbf{Representational Alignment by Attention-based Inductive Bias}}\\
        \hline
       CMRL(KDD'23) & 17.93$\pm$0.31 & 24.30$\pm$0.22 & 0.776$\pm$0.007 & 0.551$\pm$0.017 & 0.815$\pm$0.046 & 0.255$\pm$0.011 & \cellcolor{c1!40}{0.374$\pm$0.011} & 0.421$\pm$0.008\\
       CGIB(ICML'23) & 18.11$\pm$0.20 & 23.90$\pm$0.35 & \cellcolor{c1!40}{0.771$\pm$0.005} & \cellcolor{c1!80}\textbf{0.538$\pm$0.007} & 0.852$\pm$0.022 & 0.276$\pm$0.017 & 0.390$\pm$0.006 & 0.422$\pm$0.005\\
       MMGNN(IJCAI'24)& 18.65$\pm$0.34 & 25.33$\pm$0.43 & 0.801$\pm$0.007 & 0.546$\pm$0.011 & 0.902$\pm$0.026 & 0.267$\pm$0.012 & 0.385$\pm$0.008 & \cellcolor{c1!80}\textbf{0.303$\pm$0.033} \\
        \hline
        ReAlignFit & \cellcolor{c1!80}\textbf{16.82$\pm$0.25} & \cellcolor{c1!80}\textbf{22.95$\pm$0.33} & \cellcolor{c1!80}\textbf{0.769$\pm$0.005} & 0.541$\pm$0.010 & \cellcolor{c1!40}{0.799$\pm$0.034} & 0.261$\pm$0.013 & \cellcolor{c1!80}\textbf{0.371$\pm$0.008} & 0.419$\pm$0.008\\
        ReAlignFit$_\text{GCN}$ & \cellcolor{c1!40}{17.23$\pm$0.27} & \cellcolor{c1!40}{23.35$\pm$0.29} & 0.771$\pm$0.007 & \cellcolor{c1!40}{0.539$\pm$0.012} & \cellcolor{c1!80}\textbf{0.796$\pm$0.035} & \cellcolor{c1!40}{0.257$\pm$0.016} & 0.375$\pm$0.012 & \cellcolor{c1!40} \textbf{0.316$\pm$0.006}\\
        ReAlignFit$_\text{GAT}$ & 17.55$\pm$0.23 & 23.98$\pm$0.36 & 0.776$\pm$0.007 & 0.543$\pm$0.021 & 0.806$\pm$0.036 & \cellcolor{c1!80}\textbf{0.254$\pm$0.012} & 0.381$\pm$0.013 & 0.421$\pm$0.009\\
        ReAlignFit$_\text{GIN}$ & 17.92$\pm$0.46 & 24.10$\pm$0.34 & 0.772$\pm$0.007 & 0.552$\pm$0.037 & 0.803$\pm$0.025 & 0.267$\pm$0.021 & 0.386$\pm$0.017 & 0.318$\pm$0.008\\
        \hline
    \end{tabular}
    } 
    \vspace{-2mm}
    % The results of comparison methods are from~\cite{lee2023cgibICML,lee2023shiftKDD}.}
    \label{tab:mi}
\end{table*}

\begin{table*}[t]
\caption{The performance of ReAlignFit and comparative methods in DDI prediction, with the best results highlighted in \colorbox{c1!80}{\textbf{bold}} and the second results highlighted in \colorbox{c1!40}{text}.} 
\vspace{-2mm}
    % The results of comparison methods in ZhangDDI are from~\cite{lee2023cgibICML,lee2023shiftKDD}.}
    \label{tab:ddi}
\small
    \centering
    \renewcommand{\arraystretch}{1.05}
    \setlength{\tabcolsep}{0.85mm}{
    \begin{tabular}{lccccc|ccccc|ccccc}
    \hline
        \multirow{2}{*}{Model} & \multicolumn{5}{c|}{ZhangDDI} & \multicolumn{5}{c|}{HetionteDDI} & \multicolumn{5}{c}{DrugBankDDI}\\
        \cline{2-16}
         & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR  \\ \hline
         \multicolumn{16}{c}{\textbf{No Representational Alignment}}\\
          \hline
        GCN(ICLR'17) & 83.31 & 91.64 & 82.91 & 77.67 & 91.33 & 87.62 & 92.91 & 90.07 & 84.66 & 91.26 & 84.02 & 90.96 & 82.87 & 80.04 & 88.36  \\
        GAT(ICLR'18) & 84.14 & 92.10 & 83.42 & 78.45 & 91.21 & 87.94 & 94.35 & 89.78 & 83.52 & 92.83 & 84.33 & 91.26 & 83.34 & 80.58 & 89.13  \\
        MPNN(ICML'17) & 84.56 & 92.34 & 83.32 & 78.15 & 91.46 & 88.73 & 95.13 & 89.52 & 83.66 & 93.21 & 87.30 & 95.90 & 88.53 & 80.73 & 95.08  \\
        GIN(ICLR'19) & 85.59 & 93.16 & 85.07 & 79.82 & 92.11 & 87.37 & 93.01 & 87.91 & 84.05 & 91.23 & 85.21 & 92.58 & 85.88 & 83.57 & 90.92  \\
        MIRACLE(WWW'21) & 84.90 & 93.05 & 83.94 & 77.81 & 91.25 & 87.01 & 92.88 & 87.34 & 83.91 & 90.88 & 84.98 & 92.17 & 85.43 & 83.22 & 90.28  \\
        CIGIN(AAAI'20) & 85.54 & 93.28 & 84.36 & 80.23 & 91.89 & 90.87 & 92.77 & 90.33 & 88.98 & 94.62 & 91.02 & 93.62 & 91.18 & 89.35 & 94.88  \\
        \hline
         \multicolumn{16}{c}{\textbf{Representational Alignment by Attention-based Inductive Bias}}\\
          \hline
        SSI-DDI(BIB'21) & 85.35 & 93.14 & 81.96 & 78.97 & 92.09 & 87.95 & 93.83 & 88.56 & 84.28 & 91.60 & 84.16 & 91.23 & 84.64 & 82.13 & 89.47  \\
        DSN-DDI(BIB'23) & 86.65 & 91.13 & 87.86 & 84.33 & 86.42 & 91.25 & 95.31 & 91.59 & 89.52 & 94.66 & 90.49 & 96.15 & 90.58 & 89.76 & 95.27  \\
        SA-DDI(CS'22) & 77.31 & 50.26 & 45.06 & 52.54 & 29.97 & 91.00 & \cellcolor{c1!40}{95.89} & 91.26 & 88.39 & 94.96 & 92.60 & 95.33 & 92.80 & 90.38 & 96.62  \\
        CMRL(KDD'23) & 86.32 & 93.73 & 87.68 & 84.01 & 91.56 & 91.25 & 93.18 & 91.53 & 91.23 & 95.24 & 92.26 & 95.05 & 92.43 & 90.30 & 95.96  \\
        CGIB(ICML'23) & 86.36 & 93.78 & 87.24 & 83.91 & 91.88 & 91.08 & 93.26 & 91.35 & 91.55 & 94.89 & 92.37 & 94.98 & 92.03 & 90.65 & 96.11  \\ 
        MMGNN(IJCAI'24) & 85.33 & 93.23 & 86.45 & 84.36 & 90.89 & 90.67 & 94.34 & 90.75 & 92.22 & \cellcolor{c1!40}{95.28} & 93.61 & 95.12 & 92.38 & 91.22 & 95.39\\
        \hline
        ReAlignFit & \cellcolor{c1!80}\textbf{89.43} & \cellcolor{c1!40}{95.68} & \cellcolor{c1!80}\textbf{90.88} & \cellcolor{c1!80}\textbf{87.68} & \cellcolor{c1!80}\textbf{93.36} & \cellcolor{c1!80}\textbf{92.39} & \cellcolor{c1!80}\textbf{97.17} & \cellcolor{c1!80}\textbf{92.62} & \cellcolor{c1!80}\textbf{95.48} & \cellcolor{c1!80}\textbf{96.05} & \cellcolor{c1!40}{95.53} & 96.31 & 94.59 & \cellcolor{c1!80}\textbf{94.38} & \cellcolor{c1!40}{97.05} \\ 
        ReAlignFit$_\text{GCN}$ & 87.62 & \cellcolor{c1!80}\textbf{95.69} & 90.34 & \cellcolor{c1!40}{87.54} & \cellcolor{c1!40}{92.81} & 91.16 & 94.22 & 91.55 & 92.29 & 95.21 & \cellcolor{c1!80}\textbf{95.62} & 97.95 & \cellcolor{c1!80}\textbf{95.62} & \cellcolor{c1!40}{94.37} & \cellcolor{c1!80}\textbf{97.09} \\
        ReAlignFit$_\text{GAT}$ & \cellcolor{c1!40}{88.35} & 94.22 & \cellcolor{c1!40}{90.41} & 86.92 & 91.76 & 91.66 & 94.56 & 91.76 & 91.97 & 95.03 & 95.35 & \cellcolor{c1!80}\textbf{98.42} & 93.87 & 93.67 & 96.26 \\
        ReAlignFit$_\text{GIN}$ & 85.23 & 94.35 & 89.34 & 86.89 & 90.67 & \cellcolor{c1!40}{92.12} & 92.95 & \cellcolor{c1!40}{91.91} & \cellcolor{c1!40}{93.04} & 95.23 & 93.33 & \cellcolor{c1!40}{98.09} & \cellcolor{c1!40}{95.01} & 91.58 & 95.82\\
        \hline
    \end{tabular}
    }
    \vspace{-2mm}
\end{table*}
\begin{itemize}
    \item \textbf{In-Distribution Data (Original)}: The original data generated negative samples based on rule matching and is randomly divided into training, validation, and test sets.
    \item \textbf{Rule-based Partitioning (P1)}: We generated negative samples for original data based on rule matching and partitioned them by ID to ensure that at least one drug does't repeat in training, validation, and test sets. P1 simulates application scenarios of discovering unknown interactions in existing molecules, such as drug repurposing.
    \item \textbf{Scaffold-based Partitioning (P2)}: We generated negative samples for the original data based on scaffold clustering. We used METIS \cite{karypis1998multilevelMETIS} to iteratively partition the drug interaction graph, ensuring that molecules in training, validation, and test sets are entirely distinct. P2 simulates scenarios of discovering interactions between previously unknown molecules, such as drug discovery.
\end{itemize}


\subsubsection{Evaluation Metrics}
In this study, we consider MI prediction and DDI prediction as regression and classification tasks, respectively. Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are commonly used metrics in regression prediction. Considering the high positive correlation between RMSE and MAE, we employ RMSE for performance evaluation in MI prediction. We assess model performance for DDI prediction based on classification evaluation metrics, including Area Under the Receiver Operating Characteristic Curve (AUROC), Accuracy (ACC), F1-score (F1), Precision (Pre), and Area Under the Precision-Recall Curve (AUPR).

% The Root Mean Square Error (RMSE) and  Mean Absolute Error (MAE) are commonly employed in molecular interaction prediction models, and the two evaluation metrics exhibit a high positive correlation. Due to space constraints, we use RMSE for performance evaluation in molecular interaction prediction and evaluate the performance of DDI prediction tasks in terms of  AUROC, ACC, F1, Precision, and AUPR.
% 在本文中，我们将MI预测和DDI预测分别视为回归任务和分类任务。均方根误差（RMSE）和平均绝对误差（MAE）是分子相互作用预测模型中常用的指标。考虑到RMSE和MAE之间的高度正相关性，我们在MI预测中仅采用RMSE进行性能评估。同时，我们根据分类任务评价指标AUROC、ACC、F1、Pre、AUPR来评估模型在DDI预测中的性能。
% 均方根误差（RMSE）和平均绝对误差（MAE）是分子相互作用预测模型中常用的指标，这两个评价指标呈现出高度的正相关性。由于篇幅限制，我们在分子相互作用预测中使用 RMSE 进行性能评估。我们根据AUROC、ACC、F1、Pre、AUPR评估药物-药物相互作用预测任务的性能。(Wang et al.， 2021)

\subsubsection{Training Details}
In all experiments, we employ a three-layer MPNN as the encoder to extract molecular feature representations and train ReAlignFit based on two NVIDIA GeForce RTX 4090 24G GPUs. We optimize the parameters using the Adam optimizer and train the model for 50 epochs. The batch size is 128, and the dropout rate is 0.1. For other hyperparameters, we choose from a specific range: learning rate $lr \in \{0.01,0.005,0.001,0.0005,0.0001\}$, both $\alpha$ and $\beta$ are selected from $\{0.5,0.3,0.1,0.01,0.001\}$, and the iterations is searched from $\{1,3,5,10,15\}$. The code of ReAlignFit will be publicly available after the paper is accepted.
% 在所有实验中，我们采用三层MPNN作为编码器提取分子特征表示并使用两块NVIDIA GeForce RTX 4090 24G计算显卡训练了ReAlignFit。我们采用Adam优化器进行了参数优化并对模型进行了50次训练。模型批量大小为 128，丢包率为 0.1。对于其他超参数，我们从一定的范围进行选择：学习率$lr \in \{0.01,0.005,0.001,\textbf{0.0005},0.0001\}$,$a$和$b$均从${1e−1,1e−2,1e−3,1e−4}$中选择，The iterations are searched from $\{1,2,3,5,\textbf{10},15,20\}$。
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{Figure/TKDE-STABLE.png} 
\caption{The performance and RPD of ReAlignFit, CGIB and CIGIN in different data distributions.}
\vspace{-4mm}
\label{stableresults}
\end{figure*}


\subsection{Overall Performance (\textbf{RQ1})}
We compare ReAlignFit with four backbone models four backbone models (GCN~\cite{gcn}, GAT~\cite{gat}, MPNN~\cite{mpnn}, and GIN~\cite{gin}), three molecular representation-based models (CIGIN~\cite{cigin}, MIRACLE~\cite{miracle}, and MMGNN~\cite{du2024mmgnnIJCAI}), and five substructure representation-based models (SSI-DDI~\cite{ssiddi}, SA-DDI~\cite{saddi}, DSN-DDI~\cite{dsn}, CMRL~\cite{lee2023shiftKDD}, and CGIB~\cite{lee2023cgibICML}) in MI and DDI tasks. 
% seven state-of-the-art models (SSI-DDI~\cite{ssiddi}, SA-DDI~\cite{saddi}, DSN-DDI~\cite{dsn}, CMRL~\cite{lee2023shiftKDD}, CGIB~\cite{lee2023cgibICML}, and MMGNN~\cite{du2024mmgnnIJCAI}) in MI and DDI tasks. 
% We focus on comparing the performance differences between CIGIN, CMRL, CGIB, and ReAlignFit for MI tasks. For DDI tasks, we select four state-of-the-art models (SSI-DDI, MIRACLE, SA-DDI, and DSN-DDI) in the DDI prediction and three general domain models (CIGIN, CMRL, and CGIB) to compare their performance with ReAlignFit. 
% The experimental results are shown in Table \ref{tab:mi} and Table \ref{tab:ddi}.
The MI prediction and DDI prediction results are presented in Table \ref{tab:mi} and Table \ref{tab:ddi}, respectively.



\textbf{Comprehensive Analysis}: ReAlignFit demonstrates the best overall performance in MI prediction and DDI prediction. Particularly, ReAlignFit achieves an average improvement of 2.7\% in ACC and 3.0\% in AUROC in three DDI datasets. Additionally, ReAlignFit achieves the best prediction performance in multiple MI datasets, reducing RMSE by 1.83 and 2.38 of Absorption and Emission attributes on the Chromophore dataset, respectively. This result suggests that incorporating domain knowledge into MRL and substructure representational alignment contributes to improving the model's predictive performance. 

\textbf{Importance Analysis of Representational Alignment}: The prediction performance of methods considering representational alignment (CGIB, MMGNN, ReAlignFit, etc.) is significantly better than those that ignore it. Whether it is DDI or MI prediction, the model performance improvement by representational alignment is noticeable. This result indicates representational alignment is a critical factor in molecular relationships. However, it is worth noting that representational alignment methods by attention-based inductive bias achieve an average improvement of less than 5\% on the Chromophore dataset, whereas ReAlignFit exceeds 8.5\%. This observation is consistent with the nature of the Induced Fit, emphasising dynamic conformational changes of substructures.

\textbf{ReAlignFit Compatibility Analysis}: By substituting different GNN models within ReAlignFit, we observe that its performance fluctuates slightly. ReAlignFit, combined with different GNN backbones, consistently achieved the best or second-best predictive performance in all evaluation metrics for FreeSolv, CompSol, ZhangDDI and DrugBankDDI datasets. This result indicates that ReAlignFit is less affected by the backbone, demonstrating flexibility in integration with general graph models. Furthermore, this highlights ReAlignFit's reliable applicability at the model level.

% \textbf{综合分析}： ReAlignFit 在 MI 和 DDI 预测中表现出最佳的整体性能。特别是在DDI预测中，ReAlignFit在三个数据集中的ACC平均提高了2.7%，AUROC平均提高了3.0%。ReAlignFit在多个MI数据集中取得了最佳的预测性能。其将Absorption和Emission数据集中的RSME分别降低了1.83和2.38。这表明将领域知识融入到分子关系建模和子结构表示对齐有助于提升模型的预测性能。\textbf{表示对齐的重要性分析}：考虑表示对齐的方法的预测性能明显优于忽略表示对齐的方法。不论是DDI预测还是MI预测，表示对齐对模型性能的提升都是明显的。这表明表示对齐是分子关系的重要影响因素。但不可忽视的是，基于静态归纳偏差的表示对齐方法在Chromophore数据集中的平均提升不足5%，而ReAlignFit则分别超过了8.5%。这一发现与诱导契合的本质，即子结构动态的构象变化，是一致的。 \textbf{ReAlignFit兼容性分析}： 通过在ReAlignFit中替换不同的GNN模型，我们发现其性能波动较小。在FreeSolv，CompSol，ZhangDDI和DrugBankDDI数据集的所有评价指标中，结合不同GNN模型的ReAlignFit均取得了最好和第二好的预测性能。这表明 ReAlignFit受骨架的影响较小，可以灵活地与一般图模型集成，同时也从侧面体现出ReAlignFit在模型层面具有良好的适用性。



\begin{table*}[t]
\caption{The DDI prediction performance of ReAlignFit and comparison models in distributional drift data, with the best results highlighted in \colorbox{c1!80}{\textbf{bold}} and the second results highlighted in \colorbox{c1!40}{text}.}
\vspace{-2mm}
\small
    \centering
    \renewcommand{\arraystretch}{1.05}
    \setlength{\tabcolsep}{1.4mm}{
    \begin{tabular}{lccccc|ccccc|ccccc}
    \hline
        \multirow{2}{*}{Model} & \multicolumn{5}{c|}{ZhangDDI} & \multicolumn{5}{c|}{HetionteDDI} & \multicolumn{5}{c}{DrugBankDDI}\\
        \cline{2-16}
         & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR  \\ \hline
         \multicolumn{16}{c}{\textbf{Rule-based Partitioning (P1)}}\\
          \hline
        GCN & 65.84 & 71.31 & 65.46 & 66.46 & 69.94 & 66.95 & 68.13 & 70.08 & 60.19 & 65.67 & 61.15 & 72.58 & 72.62 & 63.45 & 69.87  \\
        GAT & 68.38 & 70.79 & 67.75 & 66.89 & 65.94 & 67.88 & 68.98 & 73.61 & 64.05 & 66.37 & 62.18 & 73.04 & 74.11 & 61.96 & 67.56  \\
        MPNN & 68.34 & 67.64 & 67.82 & 64.96 & 62.27 & 68.03 & 69.57 & 72.57 & 65.32 & 65.39 & 66.51 & 70.43 & 74.02 & 60.47 & 64.02  \\
        GIN & 70.15 & 71.38 & 71.83 & 66.99 & 66.09 & 69.03 & 68.88 & 72.26 & 66.09 & 66.33 & 66.93 & 71.23 & 74.67 & 61.22 & 65.21  \\
        MIRACLE & 72.46 & 71.31 & 71.92 & 65.90 & 64.41 & 69.56 & 70.30 & 70.95 & 65.36 & 65.99 & 68.57 & 71.14 & 75.32 & 62.03 & 64.04  \\
        CIGIN & 73.49 & 76.20 & 72.44 & 72.28 & 80.94 & 69.06 & 69.11 & 70.23 & 73.37 & 70.01 & 70.82 & 75.69 & 74.41 & 80.03 & 83.67  \\
        SSI-DDI & 71.87 & 75.00 & 67.12 & 71.72 & 74.19 & 68.30 & 69.85 & 72.16 & 64.51 & 69.62 & 64.47 & 75.69 & 53.72 & 77.03 & 73.70  \\ 
        DSN-DDI & 72.62 & \cellcolor{c1!40}{79.83} & 69.00 & 72.40 & \cellcolor{c1!80}\textbf{81.66} & 69.92 & 68.10 & \cellcolor{c1!40}{74.98} & \cellcolor{c1!40}{74.90} & \cellcolor{c1!40}{70.04} & \cellcolor{c1!40}{77.22} & \cellcolor{c1!40}{84.01} & 72.73 & \cellcolor{c1!80}\textbf{90.59} & \cellcolor{c1!40}{87.90}  \\ 
        CMRL & \cellcolor{c1!40}{73.75} & 76.82 & 75.05 & 74.41 & \cellcolor{c1!40}{81.54} & 69.66 & 67.19 & 70.02 & 68.86 & 68.23 & 72.68 & 73.27 & 72.90 & 81.63 & 74.22  \\ 
        CGIB & 73.42 & 79.79 & \cellcolor{c1!40}{76.23} & \cellcolor{c1!80}\textbf{75.45} & 80.79 & \cellcolor{c1!40}{72.88} & 72.15 & 72.26 & 71.64 & 69.28 & 75.14 & 75.27 & \cellcolor{c1!80}\textbf{80.34} & 85.09 & 79.80\\
        MMGNN & 72.87 & 74.95 & 74.47 & 74.90 & 78.05 & 69.52 & \cellcolor{c1!40}{73.14} & 73.80 & 71.07 & 67.49 & 71.78 & 76.26 & 72.88 & 83.52 & 79.01\\
        \hline
        ReAlignFit & \cellcolor{c1!80}\textbf{78.82} & \cellcolor{c1!80}\textbf{83.15} & \cellcolor{c1!80}\textbf{79.46} & \cellcolor{c1!40}{75.27} & 81.37 & \cellcolor{c1!80}\textbf{76.55} & \cellcolor{c1!80}\textbf{79.19} & \cellcolor{c1!80}\textbf{81.98} & \cellcolor{c1!80}\textbf{78.15} & \cellcolor{c1!80}\textbf{75.47} & \cellcolor{c1!80}\textbf{82.41} & \cellcolor{c1!80}\textbf{85.88} & \cellcolor{c1!40}{78.97} & \cellcolor{c1!40}{88.84} & \cellcolor{c1!80}\textbf{89.47}\\ 
        \hline \hline
        \multicolumn{16}{c}{\textbf{Scaffold-based Partitioning (P2)}}\\
         \hline
        GCN & 47.04 & 44.36 & 51.58 & 51.56 & 54.44 & 58.22 & 53.82 & 53.06 & 51.32 & 62.46 & 43.15 & 41.18 & 46.20 & 51.29 & 50.17  \\
        GAT & 49.94 & 47.42 & 55.34 & 52.90 & 56.87 & 61.42 & 53.88 & 55.68 & 53.92 & 61.32 & 43.44 & 45.61 & 51.20 & 50.06 & 52.30  \\
        MPNN & 46.81 & 45.15 & 53.91 & 51.59 & 53.14 & 63.07 & 55.25 & 55.98 & 53.32 & 60.84 & 46.50 & 45.08 & 52.66 & 52.95 & 53.26  \\ 
        GIN & 52.25 & 44.35 & 58.52 & 52.17 & 51.14 & 63.13 & 54.83 & 52.38 & 54.14 & 60.43 & 45.13 & 41.85 & \cellcolor{c1!40}{54.95} & 52.27 & 51.38  \\ 
        MIRACLE & 45.47 & 48.34 & 59.09 & 51.31 & 48.91 & \cellcolor{c1!40}{64.14} & 54.06 & \cellcolor{c1!40}{57.66} & 54.19 & 59.68 & 42.57 & 47.33 & 53.12 & 53.77 & 53.49  \\ 
        CIGIN & 51.82 & 52.19 & 59.23 & 51.59 & 55.42 & 62.01 & 56.67 & 57.24 & 57.35 & 62.70 & 47.39 & 56.10 & 52.02 & 52.68 & 54.52  \\
        SSI-DDI & 49.32 & 49.92 & 62.92 & 48.83 & 55.16 & 61.64 & 56.61 & 58.44 & 54.04 & \cellcolor{c1!40}\textbf{67.70} & 45.75 & 54.77 & 53.96 & 54.62 & 54.59  \\ 
        DSN-DDI & 48.78 & 47.17 & 63.51 & 54.78 & 47.33 & 61.72 & 55.11 & 52.45 & 53.72 & 61.27 & 46.08 & \cellcolor{c1!40}{57.44} & 53.49 & 52.28 & 55.71  \\ 
        CMRL & 53.70 & 56.79 & \cellcolor{c1!40}{65.39} & \cellcolor{c1!40}{54.80} & \cellcolor{c1!40}{57.84} & 63.83 & 56.17 & 55.42 & 55.81 & 60.69 & 52.61 & 57.16 & 54.36 & 54.25 & 54.53  \\ 
        CGIB & \cellcolor{c1!40}{54.61} & \cellcolor{c1!40}{57.83} & 64.39 & 54.45 & 57.53 & 63.91 & 59.78 & 56.79 & \cellcolor{c1!40}{59.96} & 58.26 & 54.07 & 57.19 & 54.42 & \cellcolor{c1!40}{55.64} & \cellcolor{c1!40}{56.02}  \\ 
        MMGNN & 52.02 & 56.81 & 65.31 & 53.95 & 56.16 & 64.08 & \cellcolor{c1!40}{60.47} & 56.26 & 59.09 & 57.65 & \cellcolor{c1!40}{54.29} & 56.74 & 52.11 & 51.65 & 51.31\\
        \hline
        ReAlignFit & \cellcolor{c1!80}\textbf{60.85} & \cellcolor{c1!80}\textbf{59.49} & \cellcolor{c1!80}\textbf{68.28} & \cellcolor{c1!80}\textbf{57.76} & \cellcolor{c1!80}\textbf{61.78} & \cellcolor{c1!80}\textbf{69.75} & \cellcolor{c1!80}\textbf{68.64} & \cellcolor{c1!80}\textbf{62.62} & \cellcolor{c1!80}\textbf{65.91} & \cellcolor{c1!80}{67.73} & \cellcolor{c1!80}\textbf{62.58} & \cellcolor{c1!80}\textbf{66.20} & \cellcolor{c1!80}\textbf{56.87} & \cellcolor{c1!80}\textbf{60.95} & \cellcolor{c1!80}\textbf{61.55}\\  \hline
    \end{tabular}
    }
    \vspace{-2mm}
    \label{tab:stable}
\end{table*}



\vspace{-2mm}
\subsection{Stability Analysis (\textbf{RQ2})}
In this section, we analyze the stability of model prediction performance in distribution-shifted data. 
To validate the model's performance in different data distributions, we quantify model stability by calculating the Relative Performance Degradation (RPD) for AUROC, ACC, F1, Pre, and AUPR. RPD is defined as follows:
\begin{equation}
    RPD_{M}=\frac{|Eva_{M}^{P_i}-Eva_{M}^{Ori}|}{Eva_{M}^{Ori}}\times 100\%
\end{equation}
where $Eva$ represents the evaluation metrics. $Eva_{M}^{P}$ and $Eva_{M}^{Ori}$ denote the prediction performance in the original and drift distributions, respectively. $M \in {AUROC, ACC, F1, Pre, AUPR}$ and ${P_i}\in {P1,P2}$.

We conduct extensive experiments on the datasets generated by the three data distributions. The experimental results are shown in Fig. \ref{stableresults}. The \textbf{values} in Fig. \ref{stableresults} is RPD. We also present the DDI prediction performance of ReAlignFit and comparison models on distribution-shifted data in Table \ref{tab:stable}.
Among these, P1 have the most minor distributional differences from the original data, followed by P2. 

\textbf{Stability Analysis}: Experimental results show that data distribution variations significantly impact the predictive performance of models. 
ReAlignFit maintains the best stability in different data distributions compared to methods that do not consider representational alignment. Compared to CGIB, ReAlignFit improved ACC prediction stability by at least 4.5\% and 6\% in two data distribution scenarios. ReAlignFit exhibits the most minor performance degradation and achieves the highest stability in varying data distributions. ReAlignFit's performance degradation ranges from 6\% to 18\% in the P1 scenario, with certain evaluation metrics showing only around 25\% decline in P2. In contrast, CGIB and CIGIN experience performance fluctuations exceeding 35\% in most metrics on P2. These findings show that stability is the critical factor in the success of ReAlignFit, primarily attributed to DRAM, which identifies chemically functionally compatible core substructure pairs through representational alignment. Comprehensively, representational alignment improves model stability and maintains strong predictive performance in distribution-shifted datasets. This directly confirms the role of representational alignment in improving the stability of MRL.

\textbf{Predictive Performance Analysis:}
With increasing distribution differences, all models exhibit varying degrees of performance degradation, highlighting that data distribution shifts remain a significant challenge for MRL. 
Encouragingly, ReAlignFit consistently outperforms other comparison models in DDI prediction in the P2 data scenario. 
Moreover, ReAlignFit outperforms the second-best method in three datasets on the P1 data scenario by 5.07\%, 3.67\%, 5.19\% in ACC, and 3.32\%, 6.05\%, 1.87\% in AUROC. These results provide the possibility of applying ReAlignFit in molecular science. By comprehensively analyzing the experimental results of ReAlignFit in distribution drift scenarios, we can see that ReAlignFit exhibits better prediction performance and stability in distribution-shifted data. We attribute the results to two main reasons: substructure representational alignment identifies substructure pairs that contain stable causal information.
On the other hand, ReAlignFit mitigates the impact of confounding substructures on molecular representations by weakening the association of confounding substructures with causal substructures.
% This directly confirms the role of representation alignment in improving the stability of MRL.
% % 实验结果表明数据分布变化对模型预测性能有显著影响。稳定性分析：与不考虑表示对齐的方式相比，ReAlignFit 在不同的数据分布下都能保持最好的稳定性。与CGIB相比，ReAlignFit在两种分布情况下的ACC预测稳定性至少提高了4.5%和6%。在实验设置的不同数据分布中，ReAlignFit的性能下降幅度最小，稳定性最好。ReAlignFit在P1中的性能下降在6%至18%之间，个别评价指标在P2中仅下降了约25%。而CGIB和CIGIN在P2中性能波动绝大多数超过了35%。这些实验结果表明，稳定性是ReAlignFit成功的关键，这主要归功于ReAlignFit中的DRAM通过表示对齐识别了化学功能兼容的核心子结构对。
% 预测性能分析：随着分布差异的增加，所有模型都出现了不同程度的性能下降。这表明，数据分布差异对模型性能的影响仍然是 MRL 的一个重大挑战。令人鼓舞的是，ReAlignFit在P2数据场景中DDI预测性能全面领先于其他对比模型，同时在P1的三个数据集中的ACC和AUROC分别比第二好的方法高出5.07%，3.67%，5.19%和3.32%，6.05%，1.87%。这为ReAlignFit在分子科学领域的应用提供了可能性。综合分析ReAlignFit在分布漂移场景中的实验结果，我们认为ReAlignFit在数据分布变化情况下表现出了更好的预测性能和稳定性。这归因于两个方面：一方面，子结构表示对齐识别了包含稳定因果信息的子结构对。另一方面，ReAlignFit 通过削混淆杂子结构与因果子结构之间的关联，减轻了混杂子结构对分子表征的影响。
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figure/TKDE-ABA.png} 
\caption{The experimental results of ablation experiment.}
\vspace{-3mm}
\label{aba}
\end{figure}

\subsection{Ablation Experiment (\textbf{RQ3})}
To analyze the impact of representational alignment and S-GIB on the overall performance of the model, we compared the following model variants: NONE (no representational alignment and optimization), RLG (optimized substructure representation based on S-GIB), and RLS (optimized substructure representation based on representational alignment). The results are shown in Fig. \ref{aba}.
\begin{itemize}
    \item \textbf{NONE}: No representational alignment is applied. Relationship prediction relies solely on substructures extracted by SRIN as molecular feature representations.
    \item \textbf{RLG}: S-GIB is used for optimizing substructure representations, focusing on molecular-level alignment and assessing S-GIB's impact on MRL stability without representational alignment.
    \item \textbf{RLS}: Representational alignment is performed at the substructure level, considering dynamic interactions between substructures for MRL, excluding S-GIB's optimization. 
\end{itemize}

The experimental results in Fig. \ref{aba} demonstrate that incorporating S-GIB optimization and substructure representational alignment improves AUROC by 4\%, 8\%, 10\% and 8\%, 16\%, 13\%, respectively, for the three data distributions in the HetionetDDI dataset. Similar trends are observed in the ZhangDDI and DrugBankDDI datasets. Notably, the performance gains in AUROC achieved by substructure representational alignment are higher than those from S-GIB optimization. These experimental results suggest that representational alignment and S-GIB optimization are critical factors in improving ReAlignFit's predictive performance.

Comparing the distribution ranges of AUROC of the five ablation experiments in Fig. 4, we observe that the RLS variant exhibits tighter AUROC ranges on distribution-shifted data. This result indicates that substructure representational alignment effectively enhances model stability. In contrast, the contribution of S-GIB optimization to stability is negligible and even reduces the model stability in individual scenarios. These phenomena suggest that selecting substructure pairs with high functional compatibility through representational alignment is the key to model stability improvement.

Overall, representational alignment improves MRL stability by identifying highly compatible binding sites. S-GIB enhances predictive performance by mitigating the influence of confounding substructures on highly compatible substructures. The synergy between representational alignment and S-GIB optimization achieves a win-win situation regarding model predictive performance and stability.

% 预测性能：图4中的实验结果表明，在HetionteDDI数据集的三种数据分布中，引入分子表示对齐和子结构表示对齐分别提高了4%、8%、10% 和 8%、16%、13% 的 AUROC。在ZhangDDI和DrugBankDDI的性能提升也呈现类似的趋势。但不可忽视的是，子结构表示对齐所获的预测AUROC的性能增益远高于S-GIB优化。这些实验结果表明，表示对齐和S-GIB优化是ReAlignFit预测性能提升的重要因素。
% 稳定性：对比图4中5次消融实验的分布区间，我们发现RLS变体在不同的分布漂移数据中的AUROC结果范围更加紧密。子结构表示对齐有效的提升了模型的稳定性。而S-GIB优化对模型稳定性的贡献是微不足道，甚至在个别场景中反而降低了模型的稳定性。这些现象表明，通过表示对齐筛选的具有高功能兼容性的子结构对是模型稳定性提升的关键。
% S-GIB通过降低混淆子结构对高兼容性子结构的影响来提升模型的预测性能。表示对齐通过筛选具有高结合兼容性的位点来提升MRL的稳定性。总体而言，表示对齐和S-GIB优化的协同实现了模型预测性能和稳定性的双赢。

\subsection{ Confusion Analysis on $\alpha$ and $\beta$ (RQ4)}
To verify the impact of confounding information $\alpha I(G_{x}^{c},G_{x}^{n})+ \beta I(G_{y}^{c},G_{y}^{n})$ in Eq (\ref{eq:CSGIBnew}) on the stability of ReAlignFit's predictive performance, we set $\alpha$ and $\beta$ to $\{0.001,0.01,0.1,0.3,0.5\}$ and conduct relevant experiments in three different data distributions. The experimental results are shown in Fig. \ref{fig:confusion-result}. 
% 为了验证公式（3）中混淆信息x对ReAlignFit预测性能稳定性的影响，我们将A和B设置为C并在三种不同数据分布中进行了相关实验。实验结果如图5（a）-(c)所示。

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\columnwidth]{Figure/TKDE-CONFUSION.png}
    \caption{The experimental results of confusion analysis in HetionteDDI dataset.}
    \label{fig:confusion-result}
\end{figure}

In Fig. \ref{fig:confusion-result}, there exists an optimal value of $\alpha$ and $\beta$ (i.e., $\alpha \!\!=\!\! \beta \!\!= \!\!0.1$) that balances the model's predictive performance and the impact of confounding information on molecular representation. When $\alpha$ and $\beta$ are excessively large ($\alpha \!\!= \!\!\beta \!\!\ge \!\!0.3$), the model's predictive performance fluctuates significantly in different data distributions, and its stability decreases markedly. This occurs because high values of $\alpha$ and $\beta$  cause the model to learn excessive confounding information, making information difficult to use in MRL effectively. Conversely, when $\alpha$ and $\beta$ are smaller ($\alpha \!\!=\!\! \beta \!\!< \!\!0.1$), while the model's performance fluctuations are reduced, its predictive performance remains below optimal levels. In this case, the model struggles to adequately distinguish between confounding and core substructures, consequently affecting its performance and generalization. Therefore, appropriately adjusting $\alpha$ and $\beta$ enables the model to more effectively balance the impact of confounding and core information on molecular representation to improve the stability of the model's prediction performance.




\subsection{Visualization Analysis (RQ5)}
To validate ReAlignFit's ability to capture core substructures in MRL, we visualize the node features and the interactions between substructures, as shown in Fig. \ref{vis}.

In the node feature representations of drug molecules, the substructure information captured by ReAlignFit closely aligns with the actual electrostatic potential of the molecules and emphasizes the interactions between atoms within molecules. As shown in Figs. \ref{vis}\textcolor{blue}{(a), (c)} and Figs. \ref{vis}\textcolor{blue}{(d), (f)}, ReAlignFit demonstrates satisfactory recognition capabilities for multiple complex functional groups within molecules. 
The Mantel test~\cite{manteltest} results shown in the Fig. \ref{vis}\textcolor{blue}{(b)} and Fig. \ref{vis}\textcolor{blue}{(e)} display the interaction strength between different substructures. Darker and thicker lines represent stronger interactions between substructures.
The results indicate the high interaction strength between the symmetrical -COOH groups (green and orange areas) in DB00548 and Group1 (benzene ring) in DB01117. Group4 in DB00802 strongly interacts with the functional group (purple region) in DB00440. 
This is consistent with the domain knowledge in Induced Fit theory that chemical reactions occur between core substructures.
Additionally, the Group3 in DB01117 exhibits a weaker interaction with substructures in DB00548, as shown in Fig. \ref{vis}. This is due to the inertness of Group3's cyclic structure, which makes it difficult to react chemically with other substructures. In conclusion, ReAlignFit not only captures substructure information within molecules but also highlights the crucial role of this substructure information in determining the occurrence of chemical reactions. The experimental results provide an effective explanation for ReAlignFit to capture core substructure in MRL.

To further demonstrate the performance of ReAlignFit, we perform dimensionality reduction on the molecular pair features captured by ReAlignFit, as shown in Fig. \ref{visualization2}. ReAlignFit distinctly differentiates the interactions among molecular pairs in the three datasets. The visual presentation results confirm ReAlignFit's satisfactory performance in MRL.





% 在药物分子的节点特征表示中，ReAlignFit 所捕获的亚结构信息与分子实际的静电势密切吻合，并强调了分子内原子间的关联，如图5（a）和图5（c）所示。图5(b)和(e)中的曼特尔测试结果显示了不同亚结构之间的相互作用强度。较深和较粗的线条代表子结构之间较强的相互作用。结果表明 DB00548 中的对称 -COOH 基团（绿色和橙色区域）和 DB01117 中的（textcolor{red}{Group1}（苯环））之间的相互作用强度很高。这与诱导契合理论中化学反应发生在核心子结构之间的领域知识是一致的。总之，ReAlignFit 不仅能捕捉分子内的亚结构信息，还能强调这些亚结构信息在决定化学反应发生方面的关键作用。
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Figure/TKDE-MOLECULAR-INTERACTION.png}
\caption{The visualization of node features and interaction strengths between substructures.}
\vspace{-2mm}
\label{vis}
\end{figure*}
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Figure/tsne.png} 
\caption{The visualization of molecular pairs interaction prediction results in the DDI dataset. Class 1 indicates molecular pairs with an interaction, while Class 2 indicates molecular pairs without an interaction.}
\vspace{-5mm}
\label{visualization2}
\end{figure}
\section{Related Work}
\subsection{Molecular Relational Learning}
MRL is an essential task in molecular representation, encompassing various applications \cite{pei2024hagoAAAI,du2024mmgnnIJCAI,chen2024uncovering}. 
In this paper, we focus on MI and DDI predictions. 
In molecular interaction prediction, the model primarily predicts changes in chemical properties caused by chemical reactions. CIGIN \cite{cigin} employed a message-passing network and collaborative attention to encode intra-molecular atoms and predict solvation-free energy. 
% Their interpretability study highlights the importance of atomic interactions in molecular relationships. 
CMRL \cite{lee2023shiftKDD} combined molecular representation with GNNs and causal relationships, identifying substructures causally related to chemical reactions. MMGNN~\cite{du2024mmgnnIJCAI} explicitly models chemical bonds between atoms by adding connections within the molecule, ultimately predicting solvent-free energy by attention-based aggregation.
DDI prediction focuses on identifying the interaction relationships between drug molecules, which is critical for co-medication in clinical settings \cite{wu2024mkgaaai}. 
Substructure-based DDI prediction is currently a research hotspot. 
SA-DDI \cite{saddi}, SSI-DDI \cite{ssiddi}, and DSN-DDI \cite{dsn} extract substructure information based on GNNs from drugs in various ways to better represent drug molecules. Additionally, TIGER \cite{zhang2024heterogeneousIJCAI} uses a Transformer architecture with a relation-aware attention mechanism to construct semantic relationships between drugs. From the data perspective, solutions targeting sparse and imbalanced data in DDI tasks have been progressively proposed~\cite{ddidata,xiang2024molecularnc}. DDIPrompt~\cite{ddidata} combines hierarchical pre-training with prompt learning to encourage an understanding of the drug's molecular properties. Moreover, researchers have advanced the interpretability of DDI by leveraging extensive drug evaluation~\cite{zhong2024learningnmi} and hierarchical views~\cite{HTCL}. In summary, the current mainstream of MRL is to utilize graph-based molecular representations while incorporating attention mechanisms to capture interactions between molecular substructures.
% MeTDDI~\cite{zhong2024learningnmi} conducts an in-depth interpretation of 13,786 DDI interactions involving 73 drugs, aiming to validate its understanding of complex DDI mechanisms.
% 综上所述，目前MRL的主流是以Graph为分子表示基础，并结合注意力机制来捕获子结构间的交互关系。

% MMGNN通过在分子内添加连接的方式显式建模原子之间的化学键，最后结合注意力聚集实现溶剂自由能的预测。从数据层面来看，面向稀缺数据和不平衡数据的DDI解决方案被陆续提出。不少研究者通过广泛药物评估和分层视图等途径，更进一步的开展了DDI的可解释性工作。
% 预测通过最初在分子间原子之间形成不加区分的连接来显式模拟氢键等原子相互作用，然后使用基于注意力的聚集方法进行细化，针对特定的溶质-溶剂对进行定制。
% Although existing MRL methods have advantages in predictive performance, they do not adequately consider the difficulty in accurately identifying response sites due to the spuriousness of attention mechanisms, which significantly reduces the interpretability of the model.


% \textbf{Molecular Relational Learning}.
% MRL is an essential task in molecular representation~\cite{pei2024hagoAAAI,fang2024moltcACL}. 
% In this paper, we focus on MI and DDI prediction. 
% In MI prediction, CIGIN~\cite{cigin} employed a message-passing network to encode molecular atoms. CMRL~\cite{lee2023shiftKDD} combined molecular representation with causal relationships. 
% In DDI prediction, SA-DDI~\cite{saddi}, SSI-DDI~\cite{ssiddi}, and DSN-DDI~\cite{dsn} extracted substructure information in various ways to better represent drug molecules.

\subsection{Representational Alignment}
Representational alignment refines embedding representations to guarantee that input data is coherently and precisely captured within the representation space~\cite{zhou2024lima,kopf2024openassistant,ji2024beavertails}. Alignment enhances model performance and reliability in complex environments. In the era of large language models (LLMs), representational alignment has become a critical technology for building LLMs that meet specific human needs~\cite{cao2024towards,kirk2024benefits}.
Existing studies primarily introduce alignment through inductive bias, behavior imitation, environmental feedback, and model feedback~\cite{cao2024towards}. In molecular representations, researchers predominantly adopt attention mechanism-based inductive bias to realize representational alignment~\cite{hamamsy2024protein}.
Molecular representational alignment is a critical step in models that accurately capture molecules' structural and functional properties~\cite{wang2024KDD2,seo2024selfKDD}.
Graphormer~\cite{mu2024graphormer} achieves molecular-level representational alignment through message passing, while CGIB~\cite{lee2023cgibICML} focuses on aligning molecular representations at the substructure level. The Dual-Graph Framework~\cite{dualgraph} combines molecular-level and substructure-level alignments, integrating the strengths of both approaches.
% 分子表示对齐。分子表示对齐是准确反映分子结构和功能特性的模型中关键的一步，是反应建模、关系预测的基础。Graphormer基于消息传递实现分子层面表示对齐，而CGIB则从子结构层面对齐分子表示。Dual-Graph Framework将分子层级和亚结构层级对齐相结合。DDIPrompt将分层预训练与提示学习相结合来鼓励模型全面的理解药物分子的特性。MeTDDI通过对73种药物涉及的13786个DDI进行全面解释以验证其对复杂DDI机制的理解。
% 表示对齐是指通过优化模型嵌入表示使模型输入在表示空间中得到一致且准确的反映。对齐能够提升模型在复杂环境中的表现和可信度。尤其是在大语言模型时代，表示对齐已成为构建满足人类特定需求的大语言模型的关键技术。根据表示对齐信号的不同，现有的研究主要通过归纳偏差、行为模仿、环境反馈和模型反馈的方式引入对齐信号。在分子表示中，研究者大多采用基于注意力机制的归纳偏差实现表示对齐。


Existing molecular representational alignment methods rely on attention mechanisms to compute inductive bias, resulting in results that reflect statistical correlations and lack dynamic adaptability. Therefore, we incorporate chemical domain knowledge into ReAlignFit to dynamically align the representations between substructures based on simulated molecular conformational changes to identify substructure pairs with high functional compatibility.
% 现有的分子表示对齐方法大多基于注意力机制计算归纳偏差，导致结果反映的是统计相关性且缺乏动态适应性。因此，我们将领域知识融入ReAlignFit，在模拟分子构象变化的基础上动态的对齐子结构间的表示以识别具有高功能兼容性的子结构。


\section{Conclusion}
In this paper, we propose the the Representational Alignment with Chemical Induced Fit to improve the stability of MRL. ReAlignFit dynamically aligns representation by introducing Induced Fit-based inductive bias. ReAlignFit simulates dynamic conformational changes during Induced Fit by a self-supervised approach. Additionally, it integrates S-GIB to align core substructure pairs with potential compatibility in chemical functions. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the predictive stability in distribution-shifted data.

In future work, we will explore the deep integration of ReAlignFit with drug discovery and chemical synthesis. Simultaneously, we will focus on leveraging Large Language Model to integrate cross-modal experimental data and optimize computational resource efficiency to promote its deployment in real-world applications.


% For future work, we will explore extending ReAlignFit to MRL application scenarios with more pronounced distribution shifts. We will focus on evaluating its stability in different data distributions to reveal the key factors influencing the stability of molecular relational learning.
% 在未来的工作中，我们将探索ReAlignFit与药物研发、化学合成等领域深度融合。同时我们重点关注利用大语言模型技术整合跨模态的领域实验数据和模型计算资源效率优化，推动其在实际场景的应用部署。
% 考虑将ReAlignFit与药物研发、化学合成等领域应用相结合，并
% 将探索将 ReAlignFit 扩展到具有更明显分布偏移的 MRL 应用场景。我们将重点评估其在不同数据分布下的稳定性以详细揭示影响MRL稳定的关键因素。
% We will validate ReAlignFit on additional datasets and tasks to ensure broad applicability.


% In this paper, we propose the Causal Subgraph Information Bottleneck to improve the stability of molecular relational learning. ReAlignFit incorporates substructure causal information into the dual-layer optimization of GIB to generate stable molecular representations by minimizing the difference between confounding information and causal information. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model’s predictive stability in distribution-shifted data. In the future, we will consider validating ReAlignFit on additional datasets and tasks to ensure the broad applicability of the model.






























% \clearpage

% \section{Introduction}
% Molecular Relational Learning (MRL) predicts the interaction between molecular pairs by mining structures \cite{fang2024moltcACL,lee2023shiftKDD}. MRL has garnered significant attention in natural science due to its efficiency in revealing molecular interaction mechanisms and understanding chemical reaction pathways \cite{pei2024hagoAAAI,zhang2024heterogeneousIJCAI}. MRL effectively improves compound screening efficiency, accelerates new material design, and advances the development of drug discovery and precision medicine \cite{lee2023cgibICML,dasfaa,seo2024interpretableNeurIPS}.
% % {\color{black}\fontsize{0.1pt}{0.1pt}\selectfont 123444}

% % MRL has garnered significant attention in natural science research due to its applications in new material design and drug discovery \cite{pei2024hagoAAAI}. However, due to the inability to understand how substructure pairings affect interactions between molecular pairs, MRL methods are usually regarded as black-box models with limited transparency. Analyzing the pairing mode between substructures can aid in understanding molecular synergy and chemical reaction mechanisms, thereby providing reasonable interpretability for predictions. The interpretability of MRL is expected to enhance the reliability of the models in practical applications.
% % 分子关系学习(Molecular Relational Learning, MRL)通过挖掘分子的结构和特性来预测分子对之间的相互作用关系。MRL因其在揭示分子作用机制和理解化学反应机理方面的高效性而在自然科学研究中备受关注。MRL有效的提高了化合物的筛选效率、加速了新材料设计、推动了药物发现和精准医疗的发展。
% % 提供因其在新材料设计及药物发现等领域的广泛应用，MRL在自然科学研究中备受关注。然而，由于无法理解子结构的配对方式如何影响分子对间的作用关系，MRL方法通常被认为是透明度有限的黑盒模型。分析子结构间的配对方式有助于模型对分子协同作用的分析和化学反应机理的理解，并为预测提供合理的解释性。MRL的可解释性有望增强模型在实际场景中应用的可靠性。
% % The molecular structural information mining-based methods have advantages in MRL \cite{li2022oodtkde,gao2024drugclip}. However, these methods exhibit varying degrees of performance fluctuations in real-world scenarios. Therefore, improving model stability is crucial for MRL.


% % In recent years, researchers have developed a strong interest in exploring the interaction mechanisms between substructures, which is beneficial for enhancing the transparency and interpretability of MRL. These approaches can be categorized into feature importance-based methods and information-theoretic methods. Feature importance-based methods investigate the use of attention mechanisms or feature importance visualization techniques to identify which substructures in molecules are most critical for predicting molecular relationships. For instance, recent studies have utilized class activation functions and feature mapping techniques to calculate interaction scores between substructures, thereby assessing the likelihood of chemical reactions occurring between substructures. However, these methods are data-driven and overly focused on predictive accuracy, usually capturing statistical correlations rather than significance in the chemical sense. To alleviate the aforementioned problems, information-theoretic methods incorporate physical significance modeling into the interpretability of MRL. Graph Information Bottleneck (GIB), as one of the representative methods, optimizes information transfer through the global graph structure, retaining only the most relevant substructures for the paired molecules. GIB avoids the interpretative bias caused by high statistical correlations, thus providing more interpretable substructures for MRL models. However, current GIB studies primarily focus on the information interaction between substructures and paired molecules rather than modeling the interaction mechanisms strictly between substructures. Therefore, further research is needed to combine graph information bottleneck techniques with modeling the interaction behavior between substructures.
% % 近年来，研究者对探索子结构间的作用机理产生了浓厚的兴趣，这有利于提升MRL的透明度和可解释性。这些方法可以分为基于特征重要性方法和基于信息论的方法。基于特征重要性方法探讨利用注意力机制或特征重要性可视化方法来识别分子中哪些子结构对分子对关系预测更为关键。例如，最近的相关研究利用类激活函数和特征映射等方式计算子结构间的交互得分以衡量子结构间化学反应发生的可能性。然而，这些方法依赖于数据驱动且过度关注预测精度，其捕捉和计算的结果更多的倾向于统计相关性而并非化学意义中的重要性。为了缓解上述问题，基于信息论的方法将物理意义建模融入到MRL的解释性研究中。图信息瓶颈（Graph Information Bottleneck，GIB）作为代表方法之一，其通过全局的图结构来优化信息传递，仅保留与配对分子最相关的子结构。GIB避免了高统计相关性导致的解释偏差，从而为MRL模型提供了具有更好解释性的子结构。但目前的GIB研究主要聚焦于子结构与配对分子之间的信息交互，并非严格意义中子结构之间的作用机制建模。因此，如何结合图信息瓶颈建模子结构之间的交互行为仍需深入研究。

% % 研究者构建了很多的方法，其本质多为基于注意力机制的，可以分为子结构和分子层面两类。
% In recent years, researchers have predominantly explored MRL by modeling the interaction behaviors of molecules or substructures using attention mechanisms to improve predictive performance \cite{boulougouri2024molecularMNI,dsn,TGNN}. The molecule-based method models structures as independent individuals, focusing only on the intrinsic structure and properties of the molecule \cite{dasfaa,cigin,yoshikai2024nc}. 
% % However, this method struggles to reflect the synergistic interactions between molecules by neglecting interactions between substructures, resulting in the inability to capture the complex molecular reaction mechanisms. 
% Substructure-level modeling methods commonly use attention mechanisms to compute interaction scores between substructures, estimating the likelihood of chemical reactions occurring between them \cite{saddi,tkdeGIBB,li2022oodtkde}. Notably, while these methods improve the predictive performance of MRL, the spuriousness of attention mechanisms leads to the calculated results leaning toward statistical correlations rather than actual chemical reaction sites. This phenomenon overlooks the understanding of chemical reaction mechanisms, reducing MRL interpretability.

% % In recent years, researchers have explored MRL through both molecule-level and substructure-level modeling methods to improve prediction performance \cite{boulougouri2024molecularMNI,dsn,TGNN}. The former method models substructures as independent individuals, focusing only on the intrinsic structure and properties of the molecule \cite{dasfaa,cigin,yoshikai2024nc}. However, by neglecting interactions between substructures, this method struggles to reflect the synergistic interactions between molecules, resulting in the inability to capture the complex molecular reaction mechanisms. Substructure-level modeling methods commonly use attention mechanisms to compute interaction scores between substructures, estimating the likelihood of chemical reactions occurring between them \cite{saddi,tkdeGIBB,li2022oodtkde}. It is noteworthy that the spuriousness of attention mechanisms causes the calculated results to lean toward statistical correlations rather than actual chemical reaction sites. This overlooks the understanding of chemical reaction mechanisms, reducing MRL interpretability.

% % In recent years, related studies have increasingly focused on modeling MRL from the substructure level to understand the interactions between molecules better \cite{boulougouri2024molecularMNI,dsn,TGNN}. Based on whether intermolecular interactions are considered, these methods can be categorized into substructure-independent and substructure-interaction modeling methods. The former method models substructures as independent individuals, focusing only on the intrinsic structure and properties of the molecule \cite{dasfaa,cigin,yoshikai2024nc}. However, by neglecting interactions between substructures, this method struggles to reflect the synergistic interactions between molecules, resulting in the inability to capture the complex molecular reaction mechanisms. The substructure-interaction modeling method calculates the interaction scores between substructures through the attention mechanism to measure the likelihood of chemical reactions occurring between substructures \cite{saddi,tkdeGIBB,li2022oodtkde}. However, the results captured and computed by these methods tend to statistical correlation rather than significance in the chemical sense.

% % 近年来，为了更好的理解分子间的相互作用关系，相关研究更多的从子结构层面建模MRL。根据是否考虑分子间交互行为，这些方法可以分为子结构独立建模方法和子结构交互行为建模方法。前者将子结构作为独立个体进行建模，仅关注分子的内在结构和分子属性。然而，这种忽略子结构间交互行为的方法难以体现分子间的协同作用和相互作影响，导致无法捕捉复杂的分子反应机制。基于子结构交互行为的建模方法大多通过注意力机制计算子结构间的交互得分，以衡量子结构间化学反应发生的可能性。然而，这些方法捕捉和计算的结果更多的倾向于统计相关性而并非化学意义中的重要性。

% % 近年来，为了提升MRL的性能，研究者分别以分子建模和子结构建模的方式开展了相关研究。前者将分子视为独立个体进行建模，仅关注分子的内在结构和分子属性。然而，这种忽略子结构间交互行为的方法难以体现分子间的协同作用和相互作影响，导致无法捕捉复杂的分子反应机制。基于子结构交互行为的建模方法大多通过注意力机制计算子结构间的交互得分，以衡量子结构间化学反应发生的可能性。虽然这些方法有效的提升了MRL的性能，但忽略了对化学反应机制的理解和化学反应位点的捕获，降低了模型的可解释性，限制了MRL的应用推广。


% % MRL中的子结构对间复杂的交互和子结构特性的多重性进一步加剧了注意力机制的虚假性。（1）子结构对间交互的复杂性：参与化学反应的亚结构与其配对的亚结构密切相关\cite{mak2024artificial}，这意味着同一分子内不同化学反应所涉及的亚结构可能不同。注意力机制从数据分布的共现模式中建模分子对间的关系，导致其学习的结果仅为数据分布的统计相关性，而并未包含因果关联的真实化学反应位点。（2）子结构特性的多重性：分子内的子结构很容易受到周围原子的影响，从而改变它们的某些化学性质。基于注意力机制的模型会集中权重与某些特征的子结构，而忽略其他可能影响这些子结构特性的活跃原子。这最后会导致模型在决策中错误的认为子结构为决定化学反应的关键因素。总而言之，基于数据分布的注意力机制没有真正的模拟分子对间的因果关系，因而难以为MRL提供可解释性。因此，MRL方法需要充分考虑子结构间因果关系，以捕获真正决定分子关系的子结构。
% The complex interactions between substructure pairs and the multiplicity of substructure properties in MRL further exacerbate the spuriousness of attention mechanisms~\cite{segler2018naturersa,du2024NMIrsa,jin2018ICLRrsa}.
% \textbf{(1) The complex interactions between substructure pairs}: The substructures involved in chemical reactions are closely related to their paired substructures \cite{mak2024artificial}, meaning that substructures involved in different chemical reactions within the same molecule may differ. The attention mechanism models relationships between molecular pairs based on co-occurrence patterns in data distributions~\cite{lee2023cgibICML,seo2024interpretableNeurIPS}, leading to results that reflect statistical correlations rather than authentic causal relationships of chemical reaction sites.
% \textbf{(2) The multiplicity of substructure properties}: Substructures within a molecule are susceptible to influence by surrounding atoms, which may change some chemical properties \cite{li2023reaction}. The attention mechanisms-based methods often focus their weights on certain substructures with specific features while ignoring other active atoms that may affect these substructures' properties~\cite{hu2024GIBsurvey,WWW24}. This ultimately leads to the incorrect assumption in decision-making that substructures are the key determinants of chemical reactions. In summary, attention mechanisms based on data distributions fail to accurately model the causal relationships between molecules, making it challenging to provide interpretability for MRL. Therefore, MRL methods require establishing causal relationships between substructure pairs to capture the substructures that truly determine molecular relationships. 


% % 1. 解释性不足
% % 注意力机制的权重分布通常难以直接映射到实际的分子化学机制上，尤其是关键反应位点或子结构，导致模型缺乏明确的化学意义和解释性。
% % 权重分布的生成往往依赖数据驱动，可能无法准确反映分子间关系的本质因果特性。

% % 无法全面捕捉关键子结构
% % 注意力机制的归纳过程通常聚焦于局部特征，但在多尺度分子结构中，可能无法全面捕捉与分子关系强相关的核心子结构或复杂交互特性。

% % Recently, the approach of utilizing the Graph Information Bottleneck (GIB) \cite{wu2020GIB} to capture core subgraphs within graphs provides a novel pathway for identifying the core substructures that determine MRL.
% % The Graph Information Bottleneck (GIB) has provided a novel pathway for improving MRL interpretability due to its advantage of selectively retaining substructures closely related to molecular relationships \cite{hu2024GIBsurvey,BrainIBtnnls2024,li2022oodtkde}. However, the reaction site ambiguity problem in MRL introduces new challenges for GIB-based methods. Specifically, the problem of reaction site ambiguity in MRL mainly refers to the multiplicity of core substructures in molecules and the complexity of substructure synergy between molecules \cite{segler2018naturersa,du2024NMIrsa,jin2018ICLRrsa}. \textbf{(1) The Complexity of Synergy Between Substructures:} The recombination of molecular substructures leads to chemical reactions, and the substructures involved in chemical reactions are closely related to their paired substructures \cite{mak2024artificial}. The substructures involved in different chemical reactions within the same molecule may differ. However, the GIB-based methods consider paired molecules only at the molecular level and overlook the importance of paired substructures \cite{lee2023cgibICML,seo2024interpretableNeurIPS}. This not only reduces the interpretability of the model but also limits the performance of downstream tasks. \textbf{(2) The Multiplicity of Core Substructures:} Molecules typically contain multiple distinct substructures, and these substructures are susceptible to influence by surrounding atoms, which may change some of their chemical properties \cite{li2023reaction}. GIB-based methods struggle to capture these subtle distinctions, resulting in limitations for precise MRL modeling \cite{hu2024GIBsurvey,WWW24}. This is because they do not truly model causal relationships between substructures and molecular interactions, making it challenging to capture the nuanced changes in substructure properties. Therefore, MRL methods require establishing causal relationships between paired substructures and molecular interactions at the substructure level to capture substructures with higher interpretability of molecular relationships.
% % 最近，通过图信息瓶颈（Graph Information Bottleneck，GIB）捕获图中核心子图的思想为挖掘MRL中具有重要化学意义的子结构提供了一种全新的路径。尽管如此，MRL中的反应位点模糊化问题仍然对GIB提出了新的挑战。具体来说，在MRL中反应位点模糊化问题主要指分子中核心子结构的多重性和分子间子结构协同的复杂性。（1）子结构间协同的复杂性：分子子结构间的重新组合导致了化学反应的发生，而参与化学反应的子结构与配对子结构息息相关。在不同化学反应中，同一分子内参与化学反应的子结构可能是不同的。而GIB仅从分子层面考虑配对分子的方式忽略了配对子结构的重要性。这不仅降低了模型的可解释性，还限制了下游任务的性能。（2）核心子结构的多重性：分子内通常包含多个不同的子结构且子结构易受到周围原子影响而改变其部分化学性质。基于GIB的方法从数据层面建模化学反应难以捕获这些细微差别，导致无法对MRL进行精确建模。这是因为它们并未真正建模子结构与分子间作用关系的因果关联，导致难以捕获子结构中细微的性质变化。因此，MRL需要在子结构层面建立配对子结构对与分子作用关系之间的因果关系，以捕获对分子关系具有更高解释性的子结构。


% % 除此之外，MRL可解释性研究面临的核心挑战在于反应位点模糊化导致的可解释性子结构难以捕获。具体来说，在MRL中反应位点模糊化问题主要指分子中核心子结构的多重性和分子间子结构协同的复杂性。（1）子结构间协同的复杂性：分子子结构间的重新组合导致了化学反应的发生，而参与化学反应的子结构与配对子结构息息相关。在不同化学反应中，同一分子内参与化学反应的子结构可能是不同的。而GIB仅从分子层面考虑配对分子的方式忽略了配对子结构的重要性。这不仅降低了模型的可解释性，还限制了下游任务的性能。（2）核心子结构的多重性：分子内通常包含多个不同的子结构且子结构易受到周围原子影响而改变其部分化学性质。基于特征重要性和信息论的方法从数据层面建模化学反应难以捕获这些细微差别，导致无法对MRL进行精确建模。这是因为它们并未真正建模子结构与分子间作用关系的因果关联，导致难以捕获子结构中细微的性质变化。因此，MRL需要在子结构层面建立配对子结构对与分子作用关系之间的因果关系，在准确识别参与化学反应子结构的同时提供更高的可解释性。
% % 最近，已经提出了几种方法，通过基于结构因果模型 (SCM) [52] 发现各种图级任务的因果子结构，例如基本原理提取 [80] 和图分类 [13, 66]，从这种有偏差的图中学习。具体来说，DIR[80]通过最小化介入风险之间的差异来生成因果子结构，从而为gnn的预测提供不变的解释。此外，CAL[66] 学习因果关注具有不同偏差级别的合成图分类的核心子结构，而 DisC [13] 将因果子结构与各种有偏差的颜色背景分开。然而，尚未探索如何发现分子关系学习任务的因果子结构。
% % 从分子的核心子结构中学习的主要挑战是源于数据收集过程不可预测的性质的偏差[13，14，66]。也就是说，无意义的子结构（例如碳环）可能与标签信息（例如致突变性）虚假相关，激励模型捕获此类快捷方式来预测标签，而不会发现因果子结构（例如，二氧化氮 (NO2)）。然而，由于快捷方式可能会随着数据分布的变化而改变，因此学习和依赖这种快捷方式会严重恶化模型分布外 (OOD) 泛化能力 [33, 80]，这在分子科学中至关重要。例如，发现在属于完全不同的支架类时具有共同特性的分子(见图1(b))，即支架跳跃[22,61]。然而，由于不同的支架具有完全不同的分布，如图1(c)所示，这有助于开发新的化学类型[89]，然而不是微不足道的。在分子关系学习任务中，分布偏移更为普遍，这是本文的主要重点，其训练对仅限于实验数据，而测试对是候选分子的组合大宇宙。

% % Furthermore, the core challenge in MRL interpretability research lies in the difficulty of capturing interpretable substructures due to reaction site ambiguity. Specifically, the problem of reaction site ambiguity in MRL mainly refers to the multiplicity of core substructures in molecules and the complexity of substructure synergy between molecules. \textbf{(1) The Complexity of Synergy Between Substructures:} The recombination of molecular substructures leads to chemical reactions, and the substructures involved in chemical reactions are closely related to their paired substructures. The substructures involved in different chemical reactions within the same molecule may differ. However, the GIB-based methods consider paired molecules only at the molecular level and overlook the importance of paired substructures. This not only reduces the interpretability of the model but also limits the performance of downstream tasks. \textbf{(2) The Multiplicity of Core Substructures:} Molecules typically contain multiple distinct substructures, and these substructures are susceptible to influence by surrounding atoms, which may change some of their chemical properties. Feature importance-based and information-theoretic methods that model chemical reactions from the data perspective struggle to capture these subtle differences, making it challenging to model MRL accurately. This is because they do not truly model causal relationships between substructures and molecular interactions, making capturing the nuanced changes in substructure properties challenging. Therefore, MRL methods require the establishment of causal relationships between paired substructures and molecular interactions at the substructure level, which can provide higher interpretability and accurately identify substructures involved in chemical reactions.
% % 除此之外，MRL可解释性研究面临的核心挑战在于反应位点模糊化导致的可解释性子结构难以捕获。具体来说，在MRL中反应位点模糊化问题主要指分子中核心子结构的多重性和分子间子结构协同的复杂性。（1）子结构间协同的复杂性：分子子结构间的重新组合导致了化学反应的发生，而参与化学反应的子结构与配对子结构息息相关。在不同化学反应中，同一分子内参与化学反应的子结构可能是不同的。而GIB仅从分子层面考虑配对分子的方式忽略了配对子结构的重要性。这不仅降低了模型的可解释性，还限制了下游任务的性能。（2）核心子结构的多重性：分子内通常包含多个不同的子结构且子结构易受到周围原子影响而改变其部分化学性质。基于特征重要性和信息论的方法从数据层面建模化学反应难以捕获这些细微差别，导致无法对MRL进行精确建模。这是因为它们并未真正建模子结构与分子间作用关系的因果关联，导致难以捕获子结构中细微的性质变化。因此，MRL需要在子结构层面建立配对子结构对与分子作用关系之间的因果关系，在准确识别参与化学反应子结构的同时提供更高的可解释性。
% % 我们从理论上证明，分子对的相互作用可由核心子结构间的相互作用概率和子结构间的混淆信息共同表示。这种表示方式通过分析子结构间的潜在关联细化反应位点的定位，提升MRL模型的可解释性。

% In this paper, we first theoretically prove that the molecular pairs relationship can be represented by maximizing the interaction probability between core substructures and minimizing the confounding information among substructures. 
% This representation encourages the model to focus on the relationships between substructure pairs rather than superficial statistical correlations, consequently mitigating the impact of spurious attention on MRL interpretability.
% With theoretical justification, we propose the Subgraph Information Bottleneck to improve the interpretability of MRL (ReAlignFit). 
% ReAlignFit constructs a causal substructure model based on domain knowledge in chemistry to reveal causal relationships between substructures and the prediction target, thus modeling the complex relationships between substructures.
% Building on substructure sampling, we design  causal bias correction function based on substructure sampling to distinguish causal and confounding substructures, thereby avoiding misidentification of chemical reaction site identification caused by spurious attention. During model optimization, we employ the dual-layer optimization to optimize the causal substructure representation in different reactions. 
% The subgraph interaction layer minimizes confounding information to reduce the association between confounding substructures and prediction targets. The causal layer encourages the model to maximize mutual information between causal substructures and molecular relationships to strengthen their correlation.
% The difference in correlation ultimately provides interpretable substructures for predicting molecular relationships. 
% Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model's interpretation of the participating MRL substructures.
% % This representation refines the localization of reaction sites by analyzing the potential associations between substructures, thereby improving the interpretability of MRL models. 
% % we theoretically prove that the molecular pairs interaction can be represented by the probability of core substructure interaction and the confounding substructure information in different data distributions. 
% % With theoretical justification, we propose the Subgraph Information Bottleneck to improve the interpretability of MRL (ReAlignFit). 
% % ReAlignFit constructs a causal substructure model based on domain knowledge in chemistry to reveal causal relationships between substructures and the prediction target, thus modeling the complex cooperative interactions among substructures. Building on substructure sampling, we design the causal bias correction function based on substructure sampling to distinguish causal and confounding substructures and employ dual-layer optimization to identify core substructures in different reactions. The subgraph interaction layer minimizes confounding information to reduce the association between confounding substructures and prediction targets. The causal layer encourages the model to maximize mutual information between causal substructures and molecular relationships to strengthen their correlation.
% % % ReAlignFit constructs a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and prediction targets. We integrate causal information into the joint optimization of the causal layer and the subgraph interaction layer to identify substructures with interpretability for molecular relationships. Specifically, the subgraph interaction layer minimizes confounding information between substructure pairs by edge random reconstruction, and the causal layer encourages the model to learn causal mutual information between core substructures to maximize the action probability. 
% % Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model's interpretation of the participating MRL substructures.
% % % With theoretical justification, we propose the Causal Interpretable Subgraph Information Bottleneck (ReAlignFit) to improve the interpretability of MRL. ReAlignFit constructs a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and prediction targets. We incorporate causal information into the causal layer and subgraph interaction layer to molecular substructures with interpretability. Specifically, the subgraph interaction layer minimizes confounding information between substructure pairs by noise injection and edge random reconstruction, and the causal layer encourages the model to learn the maximum causal mutual information between causal substructures and molecular relationships. The substructure correlation for molecular relationship-relevant components increases during joint optimization of the subgraph interaction layer and causal layer, whereas the substructure correlation for molecular relationship-irrelevant components decreases. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model's interpretation of the participating MRL substructures.

% In summary, the main contributions are listed below:
% \begin{itemize}
%     \item We propose ReAlignFit to introduce subgraph causal information into GIB to improve the interpretability of MRL. To the best of our knowledge, this is the first work to address the problem of reaction site ambiguity in MRL at the subgraph level.
%     \item With theoretical justification, we provide a formal certification for the loss function design to improve the interpretability of captured substructures by minimizing the difference between confounding and causal information.
%     \item Experimental results on two tasks in nine real-world datasets demonstrate that ReAlignFit outperforms state-of-the-art models in predictive performance while providing satisfactory interpretability for the core substructures involved in MRL guided by differential testing.
% \end{itemize}


% % % MRL的核心挑战在于训练数据与真实世界场景数据之间的分布差异所导致的模型预测的不稳定性（cite{lee2023shiftKDD}）。应对这一挑战的有效方法是捕捉决定分子间反应的核心亚结构 \cite{fang2024moltcACL}。基于 GIB 的 MRL 可从子结构层和成对分子层识别核心子结构。具体来说，这些方法通过减少配对分子对子结构的影响和增加子结构与目标之间的关联来生成核心子结构。然而，这些方法也有以下局限性：（1）化学反应中的子结构因果信息缺失：} 化学反应的本质是动态的。化学反应的本质是子结构之间的动态重排\cite{hu2017recentScaffold, bohm2004scaffold}。 在不同的化学反应中，与预测目标有因果关系的同一配对分子的子结构可能是不同的 （cite{chemoselectivity}）。基于 GIB 的方法仅在分子水平上考虑配对分子，难以识别不同反应中的因果亚结构。(2) \textbf{Challenges in Capturing Nuanced Differences Between Substructures}： 子结构容易受到分子内活性原子的影响，在化学反应中改变其性质。如果仅从分子水平考虑 MRL，则很难捕捉子结构之间的这些细微差别。这些局限性限制了相关方法保持稳定关系学习的能力。MRL 需要在亚结构对和预测目标之间建立因果关系，以捕捉亚结构之间的细微差别。因此，我们专注于利用子结构间的因果关系来提高 MRL 的稳定性。
% % % 子结构的重新组合决定了化学反应的发生。当同一个分子同时包含两种以上子结构时，其参与化学反应的子结构会随着配对分子的变化而变化。如图1（a）所示，当分子A与分子B发生反应时，A中参与化学反应的子结构是-COOH(橙色部分)，而与C反应时则是-NH2(蓝色部分)。只有从子结构层面来分析图1（a）中的化学反应才可以发现分子A决定不同化学反应的核心子结构，而从分子层面则不能。分子子结构的性质不仅受子结构的影响，还与周围原子密切相关。如图1（b）所示,分子C和D都含有-CO(红色部分)，但他们在与分子E发生反应时的子结构却不同。这是因为分子D中的-CI(绿色部分）具有更强的电子吸附性，增强了-CO的反应活性，导致-COCI与分子E发生了化学反应。因此，MRL需更加细致捕获这些差别以区别不同化学反应中真正的核心子结构。

% % % 通过使用图信息瓶颈，可以在分子图中发现最相关的子结构，自动筛选与目标相关的图子集，从而为模型提供具有更好解释性的结构。这种方法可以克服注意力机制在长程依赖捕捉方面的不足，因为信息瓶颈可以通过全局的图结构来优化信息传递。
% % % 图信息瓶颈方法是信息瓶颈在图数据上的应用，主要用于压缩分子图中的信息，仅保留与任务相关的部分。例如，在MRL中，GIB可以用于生成一个简化的子图，其中只包含对预测重要的节点和边，帮助理解哪些子结构对于分子间关系至关重要。通过应用信息瓶颈，可以筛选出最小充分信息的子结构，而不是捕捉所有的可能特征。这有助于减少对冗余特征的依赖，避免高权重仅仅是由于统计相关性导致的解释偏差。

% % % 除此之外，MRL可解释性研究面临的核心挑战在于。。。


% % % 注意力机制和特征重要性分析：探讨利用注意力机制或特征重要性方法来识别分子中哪些子结构对关系预测最为关键。
% % % 子结构提取与解释：通过图神经网络或信息瓶颈方法来提取分子中与反应相关的核心子结构，并尝试解释这些子结构与反应间的关系。
% % % 可视化工具：一些研究会使用图形可视化的方法来直观展示模型内部信息，如用图形显示子结构对关系预测的贡献。

% % % 一般来说，可解释性可以从两个角度看待：1）通过为模型的预测提供解释来提高可解释性，以及 2）通过对模型训练过程的解释来提供模型预测背后的推理过程。提高 GNN 的可解释性涉及在推理阶段检测重要的子结构，这对于识别分子化学中的官能团（即重要的子结构）等任务很有用 [29, 33, 23, 12]。另一方面，为为什么模型预测以某种方式提供推理过程也很重要，这需要对训练阶段进行深入分析，以便以更基本的级别理解模型。通过这个推理过程，我们可以可视化和分析模型如何做出正确或不正确的决策，从而为提高其性能获得关键信息、
% % % 近年来，人们对探索推理过程以在深度学习模型中提供更大的透明度和可解释性越来越感兴趣。这些方法通常可以分为两大类：1）事后方法，以及 2）内置方法。事后方法侧重于通过测量神经元基于模型预测梯度的贡献来可视化神经元的激活程度来探索模型输出。例如，最近的工作[14,9,34]使用显著图和类激活映射等技术来可视化模型预测过程中的激活区域。然而，这些方法为每个训练模型需要单独的解释模型，因此需要一个新的解释模型进行额外的训练数据和不同的模型 [2, 15]。为了解决上述挑战，内置方法旨在将解释的生成集成到模型训练过程中。一种这样的方法是原型学习，它涉及学习表示输入数据的每个类的原型，然后将其与新实例进行比较进行预测。例如，ProtGNN [35] 测量输入图和每个原型的嵌入之间的相似性，通过相似度计算提供解释，并根据输入图与学习原型的相似性进行预测。更准确地说，ProtGNN 将每个学习到的原型投影到最接近的训练图上，使其能够为其预测提供主要结构的解释。
% % % 然而，由于 ProtGNN 将输入图的图级嵌入与学习的原型进行比较，该模型忽略了输入图中的关键子结构，同时可能包含无信息子结构。这不仅导致推理过程的可解释性下降，而且限制了下游任务的性能。图 1(a) 显示了 ProtGNN 检测到的输入分子（即 G）的训练集（即 Gp，表示为粗体边）中的原型图属于“突变”类。尽管 NO2 结构是将给定分子分类为“突变”的关键功能组，但 ProtGNN 检测到的 Gp 往往包含许多在整个输入图中常见的环形结构（即无信息子结构），并在学习的原型图中排除 NO2 结构（即关键子结构），这主要是由于输入图 G 在整个图级别被考虑。因此，识别输入图中的一个关键子图是至关重要的，该子图保存原型学习的基本信息，这反过来又增强了推理过程的解释和下游任务的性能。在检测重要子图的各种解决方案中，信息瓶颈 (IB) 已成为最有效的方法之一 [27, 31]，已经证明基于 IB 检测到的关键子图有助于各种任务（例如关系学习 [11] 和结构学习 [19]）的性能改进。我们的目标是从原型的角度接近IB原理，将重要的子结构信息传达给原型。

% % % 基于分子结构信息挖掘的方法在MRL中具有明显的优势，但在这些方法在数据分布不断变化的现实场景中表现出明显的性能波动。因此，提升模型在分布外数据中的稳定性对MRL至关重要。
% % % MRL的可解释性可以应用于分子协同作用分析、化学反应机理理解，增强模型在实际场景中应用的可靠性。
% % % In practical applications, the primary challenge of MRL arises from the scaffold hopping problem in distributional shift data \cite{li2022oodtkde,gao2024drugclip}, which leads to reduced model stability. Scaffold hopping \citep{bohm2004scaffold,hu2017recentScaffold} can be understood as the functional groups that determine the same type of chemical reaction, which may differ. For example, in Fig. \ref{fig1}(a), in the nucleophilic substitution reaction, the {\ce{Br^{-}} in haloalkane can be substituted by \ce{CH3CH2O^{-}} to produce nitriles and can also be substituted by \ce{HOCH2^{-}} to produce alcohols. Therefore, improving the stability of molecular relational prediction in out-of-distribution (OOD) data is crucial for MRL's \note{wide and effective application[??]}.
% % % 时间图神经网络(TGNN)具有捕获图结构数据随时间交互的能力，并在电子商务[15]中的用户-项目交互和社交网络中的朋友关系[6,21]等领域表现出很高的效用。TGNN 在他们的方法中结合了时间动态和图拓扑，并专注于学习与时间相关的节点表示来预测未来的演化 [3, 24, 40]。然而，由于无法辨别过去的事件如何影响结果，TGNN 模型被认为是透明度有限的黑匣子。基于 TGNN 中预测逻辑提供见解有助于改进对模型决策的理解，并为预测提供合理性。TGNN的可解释性可以应用于医疗保健预测[2,14]和欺诈检测[23,29]等高风险情况，以增强模型的可靠性，并有助于检查和缓解现实应用[5]中与隐私、公平性和安全性相关的问题。

% % % 分子关系学习(Molecular Relational Learning, MRL)通过挖掘分子的结构和特性来预测分子对之间的相互作用关系。因其在新材料设计及药物发现等领域的广泛应用，MRL在自然科学研究中备受关注。基于分子结构信息挖掘的方法在MRL中具有明显的优势，但在这些方法在数据分布不断变化的现实场景中表现出明显的性能波动。因此，提升模型在分布外数据中的稳定性对MRL至关重要。
% % % 在实际应用中，MRL的主要挑战来自于分布变化数据中支架偏移（scaffold hopping）问题导致模型稳定性性降低。支架偏移可以理解为决定同一类型化学反应的官能团可能是不相同的。例如，在图1（a）中，在亲核取代反应中，卤代烷中的-Br可以被OH-取代生成醇类物质，同时也可以被CN-取代生成氰类物质。因此，提升模型在分布外数据中的分子关系预测的稳定性是MRL广泛、有效应用的关键。

% % The Graph Information Bottleneck (GIB) theory \cite{wu2020GIB} has recently been applied to extracting and identifying core subgraphs. GIB generates interpretable subgraphs that represent the minimal sufficient information of the input graph through information compression \cite{WWW24,fan2022debiasingNeurIPS,seo2024selfKDD}. The theory provides a novel solution for MRL, with the key problem being to efficiently capture the core substructures that are most beneficial for MRL \cite{TAI,seo2024interpretableNeurIPS}. Related research addresses the above problem by maximizing the mutual information between substructures and prediction targets \cite{yang2023molerecwww,TAI}. Specifically, these methods use noise injection to control the information flow between molecules and substructures, consequently identifying core substructures that contain minimal sufficient information about molecules and prediction targets \cite{lee2023cgibICML}. The interpretability and generalizability of GIB-based MRL are improved by integrating the interaction information into molecular representations.

% % % Unlike Graph Neural Network (GNN)-based methods that solely extract molecular representations, GIB-based MRL integrates the relational information between molecule and prediction into molecular representations. This effectively improves the interpretability and generalizability of the MRL model.
% % % 图信息瓶颈（Graph Information Bottleneck， GIB）理论已经应用于重要子图的提取和识别。其通过信息压缩的方式来生成代表输入图的最小充分信息的可解释性子图。GIB为分子关系学习提供了一种全新的思路，其关键是如何捕获对MRL最有效的核心子结构。目前，相关研究主要通过最大化子结构与目标间的互信息来解决上述关键问题。具体来说，这些方法以噪声注入来控制分子和子结构之间的信息流，来识别出包含分子和预测目标最少充分信息的核心子结构。与基于图神经网络仅提取分子表征的方式不同，基于GIB的MRL将分子与目标间的关联信息融入分子表征。这增强了模型的可解释性和泛化性。
% % % Graph Neural Networks (GNNs) have achieved great success in molecular representation learning through the ability to model molecular structural information explicitly \citep{zhang2024heterogeneousIJCAI,su2024dualAAAI,yang2023molerecwww}. 
% % % However, due to the irregularity of molecular structures, GNNs have challenges in identifying the core substructures that represent the primary chemical properties. 
% % % The Graph Information Bottleneck (GIB) theory \citep{wu2020GIB} has recently been applied to extracting and identifying core subgraphs. GIB generates interpretable subgraphs that represent the minimal sufficient information of the input graph through information compression \cite{WWW24,fan2022debiasingNeurIPS,seo2024selfKDD}. 
% % % % This method provides a novel perspective for extracting interpretable molecular substructures and molecular relational learning. 
% % % The core idea of GIB-based MRL is to determine which information in the molecular representation should be used to represent core substructures and which information should be discarded \citep{seo2024interpretableNeurIPS}. Currently, related research focuses on maximizing the mutual information between substructures and targets to identify core and confusing substructures \citep{lee2023cgibICML}. \note{They[?]} control the information flow between molecules and substructures by injecting noise into their mutual information, thereby enhancing the mutual information between substructures and targets to identify the minimal sufficient substructures to the target\note{[LONG SENTENCE]}. By emphasizing the relationship between substructures and targets rather than merely extracting molecular representations, GIB is gradually becoming a research hotspot in MRL\note{[LOGIC UNCLEAR]}.
% % % Specifically, these methods control the information flow between molecules and substructures with noise injection to identify core substructures that contain the least sufficient information about molecules and predicted targets.
% % % 图神经网络（GNN）凭借明确建模分子的结构信息，在分子表示学习中取得了巨大成功。但是由于分子结构的不规则性和复杂性，GNN难以发现代表分子主要化学性质的核心子结构。(若篇幅过长，这句话可删掉)
% % % 最近，图信息瓶颈（Graph Information Bottleneck， GIB）理论已经应用于重要子图的提取和识别。其通过信息压缩的方式来生成代表输入图的最小充分信息的可解释性子图。
% % % GIB为提取可解释性的分子子结构和分子关系学习提供了全新的思路。(若篇幅过长，这句话可删掉)
% % % GIB为分子关系学习提供了一种全新的思路，其关键是如何捕获对MRL最有效的核心子结构。目前，相关研究主要通过最大化子结构与目标间的互信息来解决上述关键问题。具体来说，这些方法以噪声注入来控制分子和子结构之间的信息流，来识别出包含分子和预测目标最少充分信息的核心子结构。与基于图神经网络仅提取分子表征的方式不同，基于GIB的MRL将分子与目标间的关联信息融入分子表征。这增强了模型的可解释性和泛化性。
% % % 基于 GIB 的 MRL 的核心思想是确定分子表征中哪些信息应该用来表示核心亚结构，哪些信息应该舍弃。目前，相关研究主要集中在最大化子结构与目标之间的互信息，以识别核心和易混淆的子结构。他们通过在分子和子结构间的互信息中注入噪声来控制分子和子结构之间的信息流，从而增强子结构和目标之间的互信息，以识别出对目标来说最小的足够的子结构。通过强调子结构与目标之间的关系而不仅仅是提取分子表征，GIB 正逐渐成为 MRL 的研究热点。

% % The core challenge in MRL is the instability of model predictions caused by distribution differences between training data and real-world scenario data \cite{lee2023shiftKDD}. An effective way to deal with this challenge is to capture the core substructures that determine intermolecular reactions \cite{fang2024moltcACL}. GIB-based MRL identifies core substructures from both the substructure and paired molecule levels. 
% % % Specifically, these methods generate core substructures by reducing the influence of paired molecules on substructures and increasing the association between substructures and targets. 
% % Nevertheless, these methods have the following limitations: (1) \textbf{Missing Substructure Causal Information in Chemical Reactions:} The nature of chemical reactions is the dynamic rearrangement between substructures \cite{hu2017recentScaffold, bohm2004scaffold}.  The substructures of the same paired molecule that are causally related to prediction targets in different chemical reactions may be different \cite{chemoselectivity}. 
% % % The core substructures in the same paired molecule may differ in different reactions \citep{bohm2004scaffold}. 
% % GIB-based methods consider paired molecules only at the molecular level and struggle to recognize causal substructures in varying reactions. (2) \textbf{Challenges in Capturing Nuanced Differences Between Substructures}: Substructures are susceptibly impacted by active atoms within the molecule, altering their properties in chemical reactions \cite{su2024dualAAAI}. Considering MRL solely at the molecular level makes capturing these nuanced differences between substructures difficult. These limitations restrict the ability of relevant methods to maintain stable relational learning. MRL needs to establish causal relationships between substructure pairs and prediction targets to capture nuanced differences between substructures.
% % % (1) \textbf{Insufficient Core Substructure Capture}: The Scaffold hopping problem \citep{bohm2004scaffold,hu2017recentScaffold} (i.e., molecules with similar properties have different structures) is prevalent in MRL. This results in the possibility that the core substructures in the molecule's reaction with paired molecules of the same type may differ. GIB-based methods consider paired molecules only at the molecular level, leading to capturing the same core substructures when the paired molecule belongs to the same class, but the actual core substructures are different. (2) \textbf{Overlooking the Nature of Chemical Reactions}: The nature of chemical reactions is the recombination between substructures. Considering molecular relational learning solely at the molecular level lacks domain interpretability and generalizability \citep{su2024dualAAAI}. 
% % Therefore, we focus on improving the stability of MRL by utilizing inter-substructure causal relationships.
% % % 突出为什么要用因果关系
% % % MRL的核心挑战来自于训练数据和真实场景数据间的分布差异导致模型预测性能不稳定。解决这个挑战的有效方式是捕获决定分子间反应的核心子结构。基于GIB的MRL从子结构和配对分子层面来识别核心子结构。具体来说，这些方法通过降低配对分子对子结构间的影响并增大子结构与预测目标的关联以生成决定化学反应的核心子结构。这种方法存在以下两方面的局限：（1）忽略化学反应本质导致核心子结构难以识别：化学反应的本质是子结构间的重新组合。同一配对分子在不同化学反应中与prediction存在因果关系的子结构可能是不同的。这是因为子结构间的因果关系与配对子结构密切相关。基于GIB的方法仅从分子层面考虑配对分子导致难以识别在不同反应中的因果子结构。（2）难以捕获子结构间细微差别：子结构容易受到分子内活性原子的影响，导致在化学反应中子结构的性质发生改变。从分子层面考虑MRL难以捕获子结构间的这种细微差别。MRL需要构建子结构、配对子结构与prediction间的因果关联以捕获子结构间的细微差别。

% % % （1）scaffold hopping导致核心子结构不一致:scaffold hopping是指具有相近性质的分子其结构是不同的。这导致分子与同种类的配对分子发生反应时起关键作用的子结构可能是不同的。基于GIB的方法仅从分子层面考虑配对分子导致当配对分子类别相同时其捕获的核心子结构可能是相同的，但实际上核心子结构是不同的。（2）忽略了化学反应的本质：化学反应的本质是子结构间的重新组合。因此，从分子层面考虑分子关系学习缺乏领域可解释性和泛化性。这些局限限制了相关方法在分布外数据中保持稳定关系学习的能力。因此，本文的研究重点是利用子结构间因果关系来增强MRL的稳定性。

% % % （1）忽略化学反应本质导致核心子结构难以识别：化学反应的本质是子结构间的重新组合。同一配对分子在不同化学反应中起关键作用的子结构可能是不同的。基于GIB的方法仅从分子层面考虑配对分子导致难以识别配对分子在不同反应中的核心子结构。（2）难以捕获子结构间细微差别：子结构容易受到分子内活性原子的影响，导致在化学反应中子结构的性质发生改变。从分子层面考虑MRL难以捕获子结构间的这种细微差别。
% % % 从分子层面表征分子，会导致在不同化学反应中发挥作用的子结构难以被区分。
% % % （2）难以捕获子结构间细微差别：图1中，分子中的官能团会受到分子内其他分子的影响。（现在的example）

% % % The core challenge of MRL is the poor model stability due to data distribution variations and molecular scaffold hopping\note{[REF]}. However, GIB-based MRL still has limitations in solving the stability problem, i.e., GIB does not sufficiently consider the causal relationship between paired substructures and targets, which can lead to insufficient adaptation of the model to changes in data distribution. Specifically, in the context of scaffold hopping in MRL, the core substructures of the same molecule in different chemical reactions may be different \note{[LONG EXPLANATION]}. 
% % % As shown in Fig. \ref{fig1}(b), in the chemical reaction between 2-aminopropanoic acid and \ce{CH3OH}, the carboxyl group (\ce{-COOH}) is esterified, while in the reaction with \ce{CH3COCl}, the amino group (\ce{-NH2}) is acylated to form an amide.
% % % % the functional groups of molecule involved in different chemical reactions are different, specifically \colorbox{c3}{\ce{COOH^{-}}} and \colorbox{c3}{\ce{NH2^{-}}}.
% % % This indicates that chemical reactions are determined by both substructures and their paired substructures, meaning that both influence molecular relational learning. However, GIB-based methods only consider the molecule itself and overlook the impact of paired substructures on molecular representation, limiting their ability to maintain stable relational learning in the OOD data.
% % % % This indicates that understanding the causal relationships between molecules and targets can help the model learn a more comprehensive molecular representation in different chemical reaction types, improving MRL's stability. However, the GIB-based methods only consider the molecule and ignore the impact of paired substructures on molecular representation. This shortcoming limits their ability to ensure stable relational learning in changing data distributions. 
% % % Besides, the nature of chemical reactions is the recombination of different substructures within molecules. Focusing solely on the molecular level without considering substructure interactions also contributes to its instability. Therefore, this paper focuses on utilizing the information on the relationship between substructures to enhance the stability of MRL. \note{[NO RELATED WORK IN THIS SPECIFIC FIELD?]}
% % % MRL的核心挑战是数据分布差异和分子scaffold hopping导致的模型稳定性较差。（什么原因导致了存在挑战。GIB存在什么样的局限，导致不能解决这些挑战。 再写子结构的问题）
% % % 但是，基于GIB的MRL在解决稳定性问题中仍存在局限，即GIB未充分考虑配对子结构与目标间的因果关系，这会导致模型对数据分布变化的适应性不足。具体来说，由于MRL中的scaffold hopping问题，同一分子在不同化学反应中起主导作用的子结构可能是不同的。
% % % 2-氨基丙酸与甲醇（CH3OH）的化学反应中羧酸基团（-COOH）被酯化，而与乙酰氯（CH3COCl）反应时氨基基团 (-NH₂)被酰化形成酰胺。
% % % 这说明化学反应同时由子结构和配对子结构决定，即子结构与配对子结构同时影响分子关系学习。而基于GIB的方法仅考虑了分子本身、忽略了配对子结构对分子表征的影响,限制了它们在分布外数据中保持稳定关系学习的能力。另外，化学反应的本质是分子不同的子结构间的重新组合，仅从分子层面讨论MRL也是导致其不稳定的一个重要因素。因此，本文重点研究了如何利用子结构间作用信息提升MRL的稳定性。
% % % MEL的核心挑战来自于训练数据和真实场景数据间的分布差异导致模型预测性能不稳定。解决这个挑战的有效方式是捕获决定分子间反应的核心子结构。基于GIB的MRL从子结构和配对分子层面来识别核心子结构。具体来说，这些方法通过降低配对分子对子结构间的影响并增大子结构与预测目标的关联以生成决定化学反应的核心子结构。这种方法存在以下两方面的局限：（1）scaffold hopping导致核心子结构不一致:scaffold hopping是指具有相近性质的分子其结构是不同的。这导致分子与同种类的配对分子发生反应时起关键作用的子结构可能是不同的。基于GIB的方法仅从分子层面考虑配对分子导致当配对分子类别相同时其捕获的核心子结构可能是相同的，但实际上核心子结构是不同的。（2）忽略了化学反应的本质：化学反应的本质是子结构间的重新组合。因此，从分子层面考虑分子关系学习缺乏领域可解释性和泛化性。这些局限限制了相关方法在分布外数据中保持稳定关系学习的能力。因此，本文的研究重点是利用子结构间因果关系来增强MRL的稳定性。
% % % 上述两方面因素导致MRL模型在分布外数据中预测性能不稳定。
% % In this paper, we theoretically prove that even the data distribution changes, the molecular pairs interaction can be represented by the probability of core substructures interaction and the confounding substructure informations. With theoretical justification, we propose the Causal Subgraph Information Bottleneck (ReAlignFit) to improve the stability of MRL. First, ReAlignFit generates molecular pairs embedding based on the GNN encoder. Then, ReAlignFit constructs a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and prediction targets. Finally, we incorporate causal information into causal layer and subgraph interaction layer to generate stable molecular representations. Specifically, the subgraph interaction layer minimizes the confounding information between substructures by noise injection, and the causal layer encourages the model to learn the maximum causal mutual information between causal substructures and prediction targets. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model’s stability in distribution-shifted data.

% % % In this paper, we propose the Causal Subgraph Information Bottleneck (ReAlignFit) to improve the stability of MRL. 
% % % % We theoretically prove that even the data distribution changes, the molecular relationship can be stably represented by enhancing the mutual information between causally core substructures and prediction.
% % % \note{First[LOGIC?]}, ReAlignFit generates molecular pairs embedding based on the GNN encoder. Then, \note{we [ReAlignFit?]} construct a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and prediction. Finally, \note{we [ReAlignFit?]} incorporate causal information into causal layer and substructure interaction layer to generate stable molecular representations. Specifically, the subgraph interaction layer minimizes the confounding information between substructures by noise injection, and the causal layer encourages the model to learn the maximum causal mutual information between causal substructures and the prediction. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model’s stability in distribution-shifted data.
% % % 在本文中，我们提出了Causal Subgraph Information Bottleneck （CSGIB）来提升Molecular Relational Learning的稳定性。首先，我们从理论上证明，即使数据分布发生变化，如果具有因果关系的核心子结构与目标间的互信息在一定程度上被增强，则分子与目标间的关系可用上述互信息来稳定表示。然后，我们我们基于化学领域知识构建因果子结构模型，揭示子结构与目标间的因果关系。我们将子结构因果信息融入CSGIB的双层优化中，通过最小化混淆信息与因果信息的差值生成分子的稳定表示。子图交互层通过噪声注入最小化混淆子结构与因果子结构间的混淆信息，因果层则鼓励模型学习因果子结构与目标间因果互信息的最大值。在多个数据集上的实验结果表明，与最先进的模型相比，我们提出的方法不仅取得了最佳的预测性能，还显著提高了模型在分布变化数据中的预测稳定性。

% % % In this paper, we propose the Causal Subgraph Information Bottleneck (ReAlignFit) to improve the stability of MRL. We theoretically prove that even the data distribution changes, the molecular relationship can be stably represented by enhancing the mutual information between causally core substructures and prediction. Then, we construct a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and prediction. Finally, we incorporate substructure causal information into the dual-layer optimization of GIB to generate stable molecular representations by minimizing the difference between confounding information and causal information. Specifically, the subgraph interaction layer minimizes the confounding information between substructures by noise injection, and the causal layer encourages the model to learn the maximum causal mutual information between causal substructures and the prediction. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the model’s stability in distribution-shifted data. 0812version





% % % Therefore, we transform the stable MRL problem \note{into maximizing mutual information while also considering the influence of confounding substructures on molecular representation[??]}. Then, we construct a causal substructure model based on chemical domain knowledge to reveal the causal relationships between substructures and targets. We propose a two-layer optimization scheme for CSGIB at the subgraph level. Specifically, the subgraph interaction layer aims to minimize the informational impact of confounding and core substructures. In contrast, the causal layer encourages the model to maximize mutual information between causal substructures and targets. Finally, extensive experimental results in OOD datasets demonstrate that CSGIB significantly improves the stability of MRL. 
% % % 在本文中，我们提出了Causal Subgraph Information Bottleneck （CSGIB）来提升Molecular Relational Learning的稳定性。首先，我们从理论上证明，即使数据分布发生变化，如果具有因果关系的核心子结构与目标间的互信息在一定程度上被增强，则分子与目标间的关系可用上述互信息来稳定表示。然后，我们我们基于化学领域知识构建因果子结构模型，揭示子结构与目标间的因果关系。
% % % 因此，我们将稳定MRL问题转化为I(*)最大化问题，同时考虑混淆子结构对分子表征的影响。然后，我们基于化学领域知识构建因果子结构模型，揭示子结构与目标间的因果关系并引入到GIB中。我们在子图层级提出了CSGIB的双层优化方案。具体来看，子图交互层旨在最小化混淆子结构对因果子结构的信息影响，因果层则鼓励模型最大化因果子结构与目标之间的互信息。最后，在不同分布的数据集中的大量实验结果表明，CSGIB明显提升了MRL的稳定性。

% % In summary, the main contributions are listed below:
% % \begin{itemize}
% %     \item We propose ReAlignFit that introduces subgraph causal information into GIB to improve the stability of MRL. To the best of our knowledge, it is the first work to address the problem of causal substructure selection in different chemical reactions.
% %     % \note{[We propose a XXX model ReAlignFit to solve the issues of xxxx]}\note{According to our literature review[To the best of author's knowledge]}, it is the first work to introduce subgraph causal information into GIB to address the problem of substructure selectivity in the nature of chemical reactions.
% %     % 根据我们的文献调研，这是第一个将子图因果信息引入GIB中解决化学反应本质中子结构的选择性问题的工作。
% %     \item With theoretical justification, we provide a formal certification for the loss function design to capture nuanced differences between substructures by minimizing the difference between confounding and causal information.
% %     \item Experiments on two tasks in nine real-world datasets and three different data distributions demonstrate that the predictive performance and stability of ReAlignFit outperforms seven state-of-the-art models.
% %     % \item \textcolor{red}{With theoretical justification, we introduce substructure causality in ReAlignFit to capture the strong causal relationships between core substructures and targets while modeling the nature of chemical reactions. }
% %     % \item \textcolor{red}{We incorporate substructure causal information into the dual-layer optimization of GIB to generate stable molecular representations by minimizing the difference between confounding and causal information. }
% %     % % \item We proposed an optimization scheme for CSGIB with substructure causal information to improve the stability of molecular representations. To the best of our knowledge, this is the first work to optimize GIB from substructures only in MRL.
% %     % \item \textcolor{red}{Experiments on real-world datasets with nine different data distributions demonstrate that the predictive performance and stability of ReAlignFit outperforms seven state-of-the-art models.}
% % \end{itemize}

% % % 化学选择性”（chemoselectivity
% % % 总结来看，我们的主要贡献包含以下方面：
% % % 1）我们引入了子结构因果关系，在模拟化学反应本质的同时捕获核心子结构与目标间的强因果关系，该方法提高了模型捕获稳定分子表示的能力。我们从理论上证明了这一方案的可行性。
% % % 2）我们基于子结构因果信息提出了CSGIB的优化方案以提升分子的稳定表征。据我们所知，这是MRL中首个仅利用子结构优化GIB的工作。
% % % 3）在两种不同数据分布的真实世界数据集中的实验表明，相比于六种最先进的模型，CSGIB有效提高了分布外数据中MRL的稳定性。



% \section{Motivating Example}
% % 在本节，我们将以图{ref{fig1}为例，进一步详细解释注意力机制的虚假性如何影响化学反应位点的识别和MRL的可解释性。
% Reaction site ambiguity is a crucial factor that affects the interpretability of MRL. 
% In this section, we explain in further detail how the spuriousness of attention mechanisms impacts the identification of chemical reaction sites and the interpretability of MRL with the example in Fig. \ref{fig1}.
% % The reaction site ambiguity is a crucial factor affecting the interpretability of MRL. In this section, we will explain how reactive site ambiguity affects MRL interpretability from the spuriousness of attention mechanisms, the multiplicity of core substructures, and the complexity of synergy between substructures with the example in Fig. \ref{fig1}.
% % In this section, we further illustrate the motivation of ReAlignFit using the example of molecular chemical reactions, as shown in Fig. \ref{fig1}. Our motivation is to capture the causal substructure pairs that determine different chemical reactions at the substructure level, consequently improving the interpretability of MRL predictions. We describe the research motivation from three aspects:
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.85\columnwidth]{Figure/motivation1108.png}
% \caption{The motivating example. (a) The attentional mechanisms may obscure the true core substructure of the chemical reaction (e.g., \textcolor{c7}{\ce{-OH}}) during the expansion of the receptive field. (b) The properties of substructure (e.g., \textcolor{c5}{\ce{-CO}}) within molecular B are influenced by surrounding reactive atoms (\textcolor{c4}{\ce{-CI}}), leading to changes in its behavior in chemical reactions. (c) The core substructure involved in a chemical reaction changes with the pairing substructure. When molecule C reacts with molecules D and E, the core substructures are \textcolor{c2}{\ce{-COOH}} and \textcolor{blue}{\ce{-NH2}}, respectively.}
% % \caption{Functional groups exhibit versatile functionalities in different chemical reactions.}
% \label{fig1}
% \end{figure}
% % (a) The attention mechanism aggregates atomic information at different levels (e.g., \textcolor{c6}{\ce{-CH2OH}}) during the expansion of the receptive field, which can obscure the true core substructures in chemical reactions (e.g., \textcolor{c7}{\ce{-OH}}).
% % (a)注意力机制在感受野扩大过程中会聚合不同层次的原子信息（如-CH2OH），导致化学反应中真正的核心子结构被掩盖（如-OH）。（b）分子内子结构（-CO）的性质受周围活性原子（-CI）的影响导致其在化学反应中性质发生改变。(c)分子内参与化学反应的核心子结构随配对分子的变化而变化。分子C与分子D\E反应时的核心子结构分别为-cooh和-nh2
% % 
% % 如图\ref{fig1}(b)所示，分子B中具有强电子吸附性的\textcolor{c4}{-CI}增强了\textcolor{c5}{-CO}的反应活性。在酯化反应中，分子B中的\textcolor{c5}{-COCI}会代替\textcolor{c5}{-CO}与醇类物质中的醇基发生反应。
% % 我们将从注意力机制的虚假性、子结构间协同的复杂性、核心子结构的多重性三方面描述研究动机。（1）注意力机制的虚假性：在注意力聚合过程中，模型会学习过多的原子信息导致决定化学反应的核心子结构被忽略。在图1（a）中，随着注意力感受野的增大，模型会更关注\textcolor{c6}{\ce{-CH2OH}}的信息，而对分子A中真正的核心子结构\textcolor{c7}{\ce{-OH}}的学习产生偏差。
% % （2）核心子结构的多重性   
% % （3）子结构间协同的复杂性
% % 因此，我们需更加明确的理解参与化学反应的子结构之间的因果关系以识别决定化学反应的核心子结构，为MRL预测提供更加透明的解释性。
% % We describe the research motivation from three aspects: the spuriousness of attention mechanisms, the multiplicity of core substructures, and the complexity of synergy between substructures.

% % 我们将结合子结构特性的多重性和子结构对间交互的复杂性来解释注意力机制虚假性对MRL可解释性的影响。（1）子结构特性的多重性：分子子结构的性质易受周围原子的影响。如图\ref{fig1}(b)所示，分子 B 中的强\textcolor{c4}{ce{-CI}吸电子基团增强了\textcolor{c5}{ce{-CO}的反应活性。｝ 在酯化反应中，分子 B 中的\ce{-COCI}基团将取代\textcolor{c5}{ce{-CO}，并与醇中的羟基发生反应。
% % 而注意力机制通常会将更高的权重赋予在分子中出现概率更高的\textcolor{c5}{ce{-CO}，导致难以识别分子B中真正的反应位点\ce{-COCI}。另一方面，注意力机制感受野的增加使得模型会更多的关注图\ref{fig1}(a)中\textcolor{c6}{ce{-CH2OH}}的信息，而忽略了分子 A 中真正的核心亚结构\textcolor{c7}{ce{-OH}}。（2）子结构对间交互的复杂性：决定反应的核心子结构可能会因配对分子的不同而发生变化。如图\ref{fig1}(c)所示，当分子 C 与 D 反应时，C 中的反应亚结构是 \textcolor{c2}{ce{-COOH}}，而与 E 反应时，则是\textcolor{blue}{ce{-NH2}}。在化学反应中， \textcolor{c2}{ce{-COOH}}的反应活性略强于\textcolor{blue}{ce{-NH2}}。因此，\textcolor{c2}{ce{-COOH}}在化学反应中经常共现，进而导致注意力机制会过度聚焦该子结构，而忽略了部分反应中真正驱动化学反应的\textcolor{blue}{ce{-NH2}}。这说明注意力机制的共现现象会掩盖MRL中的真实的因果子结构，降低模型的可解释性。
% We will explain the impact of attentional mechanism spuriousness on the interpretability of MRL by considering the multiplicity of substructure properties and the complex interactions between substructure pairs. 
% \textbf{(1) The multiplicity of substructure properties}: The properties of molecular substructures are susceptible to influenced by surrounding atoms. As shown in Fig. \ref{fig1}(b), the strong electron-adsorption group \textcolor{c4}{\ce{-CI}} in molecule B enhances the reactivity of \textcolor{c5}{\ce{-CO}}. In the esterification reaction, the \ce{-COCI} group in molecule B will replace \textcolor{c5}{\ce{-CO}} and react with the hydroxyl group in alcohols. However, the attention mechanisms usually assign higher weights to substructures \textcolor{c5}{\ce{-CO}} that occur more frequently, making it difficult to identify the actual reaction site \ce{-COCl} in molecule B. Furthermore, with the attentional receptive field increases, the model focuses more on the information of \textcolor{c6}{\ce{-CH2OH}}, ignores the real core substructure \textcolor{c7}{\ce{-OH}} in Fig. \ref{fig1}(a).
% \textbf{(2) The complex interactions between substructure pairs}: The core substructure determines the reaction may change depending on the paired molecule. In Fig. \ref{fig1}(c), when molecule C reacts with D, the reactive substructure in C is \textcolor{c2}{\ce{-COOH}}, while with E, it is \textcolor{blue}{\ce{-NH2}}. The \textcolor{c2}{\ce{-COOH}} is slightly more reactive than \textcolor{blue}{\ce{-NH2}} in chemical reactions. Consequently, \textcolor{c2}{\ce{-COOH}} is frequently co-occurring in chemical reactions, which causes the attentional mechanism to over-focus on this substructure and ignore the \textcolor{blue}{\ce{-NH2}} that is really driving the chemical reaction in certain cases. This demonstrates that the co-occurrence phenomenon in attention mechanisms obscures the true causal substructures in MRL and reduces the interpretability of the model.
% Therefore, it is crucial to clearly understand the causal relationships between substructures involved in chemical reactions to identify the core substructures with higher interpretability regarding molecular relationships.



% % \textbf{(1) The Spuriousness of Attention Mechanisms:} During the attention aggregation process, the model tends to learn excessive atomic information, neglecting the core substructure that determines the chemical reaction. In Fig. \ref{fig1}(a), as the attentional receptive field increases, the model focuses more on the information of \textcolor{c6}{\ce{-CH2OH}}, ignores the real core substructure \textcolor{c7}{\ce{-OH}} in molecule A.
% % % resulting in the bias in learning the actual core substructure \textcolor{c7}{\ce{-OH}} in molecule A.
% % \textbf{(2) The Multiplicity of Core Substructures:} The properties of molecular substructures are impacted not only by the substructures themselves but also by the surrounding atoms. As shown in Fig. \ref{fig1}(b), the strong electron-adsorption group \textcolor{c4}{\ce{-CI}} in molecule B enhances the reactivity of \textcolor{c5}{\ce{-CO}}. In the esterification reaction, the \ce{-COCI} group in molecule B will replace \textcolor{c5}{\ce{-CO}} and react with the hydroxyl group in alcohols.
% % \textbf{(3) The Complexity of Synergy Between Substructures:} The rearrangement of substructures determines the occurrence of chemical reactions. When a molecule contains more than one substructure, the core substructure determines the reaction may change depending on the paired molecule. As shown in Fig. \ref{fig1}(c), when molecule C reacts with D, the reactive substructure in C is \textcolor{c2}{\ce{-COOH}}, while with E, it is \textcolor{blue}{\ce{-NH2}}. We can identify the core substructures of molecule C that have causal relationships with different paired molecules at the substructure level, which cannot be achieved at the molecular level.
% % Therefore, it is crucial to clearly understand the causal relationships between substructures involved in chemical reactions to identify the core substructures with higher interpretability regarding molecular relationships.

% % The rearrangement of substructures determines the occurrence of chemical reactions. When a molecule contains more than one substructure, the core substructure determines the reaction may change depending on the paired molecule. As shown in Fig. \ref{fig1}(a), when molecule A reacts with B, the reactive substructure in A is \textcolor{c2}{-COOH}, while with C, it is \textcolor{blue}{-$\text{NH}_2$}. We can identify the core substructures of molecule A that have causal relationships with different paired molecules at the substructure level, something that cannot be achieved at the molecular level. The properties of molecular substructures are impacted not only by the substructures themselves but also by the surrounding atoms. As shown in Fig. \ref{fig1}(b), molecules D and E contain a \textcolor{c5}{-CO}, but the substructures involved in reactions with the same molecule differ. This is because the \textcolor{c4}{-CI} in E has the stronger electron adsorption, enhancing the reactivity of \textcolor{c5}{-CO}, which leads to the -COCI substructure involved in the chemical reaction. Therefore, MRL needs to precisely capture these differences to distinguish the true core substructures in different chemical reactions. By establishing causal relationships between substructures and prediction targets, MRL can more effectively identify the substructures that truly determine chemical reactions.



 


% % % 在本节中，我们借助分子化学反应的示例进一步说明ReAlignFit的研究动机，如图1所示。我们的研究动机是在子结构层面捕获决定不同化学反应的核心子结构以生成分子的稳定表示。
% % % 子结构的重新组合决定了化学反应的发生。当同一个分子同时包含两种以上子结构时，其参与化学反应的子结构会随着配对分子的变化而变化。如图1（a）所示，当分子A与分子B发生反应时，A中参与化学反应的子结构是-COOH(橙色部分)，而与C反应时则是-NH2(蓝色部分)。只有从子结构层面来分析图1（a）中的化学反应才可以发现分子A决定不同化学反应的核心子结构，而从分子层面则不能。分子子结构的性质不仅受子结构的影响，还与周围原子密切相关。如图1（b）所示,分子C和D都含有-CO(红色部分)，但他们在与分子E发生反应时的子结构却不同。这是因为分子D中的-CI(绿色部分）具有更强的电子吸附性，增强了-CO的反应活性，导致-COCI与分子E发生了化学反应。因此，MRL需更加细致捕获这些差别以区别不同化学反应中真正的核心子结构。
% % % 子结构容易受到分子内活性原子的影响
% % % jue'ding捕获决定分子在不同化学反应中的
% % % 来提升MRL的稳定性。
% % % 捕获分子与目标间的因果关系生成核心子结构的唯一表征来提升模型在OOD中的预测稳定性。


% % \textcircled{1}
% % 用子结构间因果关系来增强MRL的稳定性。
% % 在本节中，我们借助分子化学反应的示例进一步说明ReAlignFit的研究动机，如图1所示。我们的研究动机是在子结构层面捕获分子与目标间的因果关系生成核心子结构的唯一表征来提升模型在OOD中的预测稳定性。在图1中，我们用分子表征反应物A，则在化学反应1中模型学习到A可以与B中的-CO发生化学反应。在化学反应2中时，由于B也存在-CO，模型会根据先前学到的知识判定-CO也是B的核心子结构，但事实上-COOCI才是B的核心子结构。如果我们从子结构层面考虑化学反应1和2，1的本质是A中的-OH与B中的-CO重组生成了新的产物。在化学反应2中，虽然反应物B中也存在-CO，但是有-CI构成的-COOCI子结构更加活跃。A中的-OH最终与B中的-COOCI发生重组。通过上述例子可以看出从分子层面考虑分子间的化学反应对于识别具有相似性质或结构的子结构是不充分的。


% % 具体来说，给定两个分子，

% % 我们的研究动机是在子结构层面捕获分子与目标间的因果关系生成核心子结构的唯一表征来提升模型在OOD中的预测稳定性。


% % 由于支架跳跃的存在，分子与具有相似性质的配对分子反应时起关键作用的子结构可能是不同的。
% % 分子与同种类的配对分子发生反应时起关键作用的子结构可能是不同的。从图分子层级难以区分，因为两个分子同属于一个类别，导致模型会将其视为近似的表征。从子结构层级，B、C中作用的官能团是不同的，因此更细致的区分子结构可以缓解支架跳跃导致的分子表征不一致问题。








% % % In this section, we introduce the Problem Formulation (including Definition and Stable Molecular Relational Learning) and the Theoretical Analysis.
% % % \subsection{Problem Formulation}
% % \subsubsection{Notations}
% % In this paper, we utilize RDKit to convert molecular SMILES sequences into undirected graphs to represent molecules. For any molecule $G$, it can be represented as $G=(V,E,X,A)$. Here, $V=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$ represents the set of nodes. $E\in N\times N$ represents the connections between atoms within the molecule, i.e., the chemical bonds between atoms, which is closely related to the adjacency matrix $A\in
% % {{\Re }^{N\times N}}$. If $({{v}_{i}},{{v}_{j}})\in
% % E$, then ${{A}_{ij}}=1$; otherwise, ${{A}_{ij}}=0$. $X\in {{\Re }^{N\times W}}$ is the feature matrix, consisting of the feature representations of each atom.
% % % 问题定义：Definition.在本文中，我们利用Rdkit将分子SMILES序列转化为无向图并作为分子的表示。对于任意分子$G$，可表示为$G=(V,E,X,A)$。其中，$V=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$代表节点的集合。$E\in N\times N$为分子内原子间的连接关系，即原子间的化学键，其与邻接矩阵$A\in {{\Re }^{N\times N}}$密切相关。如果$({{v}_{i}},{{v}_{j}})\in E$，则${{A}_{ij}}=1$；否则，${{A}_{ij}}=0$。$X\in {{\Re }^{N\times W}}$是特征矩阵，由每个原子的特征表示构成。
% % \subsubsection{Stable MRL}
% % For the given molecular pair $({{G}_{x}},{Y}_{xy},{{G}_{y}})$, the objective of Stable MRL is to generate a stable molecular embedding representation $Z(G^{st})$ and to use it for molecular relationship prediction in different data distributions. Formally, ${{Y}_{xy}}=F_s ({Z({G}_{x}^{st})},{Z({G}_{y}^{st}}))$. In regression tasks and classification tasks, $Y$ corresponds to $(-\infty ,+\infty )$ and $\{0,1\}$, respectively. Therefore, CSGIB aims to learn stable representations of molecules and improve the model's predictive performance on OOD data.
% % % For the given molecular pair $({{G}_{x}},{Y}_{xy},{{G}_{y}})$, the objective of Stable MRL is to generate a stable molecular embedding representation $Z(G^s_x)$ and use it as the unique embedding representation of molecule ${G}_{x}$ in different data distributions. Formally, ${{Y}_{xy}}=F_s ({Z({G}_{x}^s)},{Z({G}_{y}^s))$. In regression tasks and classification tasks, $Y$ corresponds to $(-\infty ,+\infty )$ and $\{0,1\}$, respectively. Therefore, CSGIB aims to learn stable representations of molecules and improve the model's predictive performance on OOD data.
% % % 对于给定的分子对$({{G}_{x}},{Y}_{xy},{{G}_{y}})$，Stable MRL的目标是生成稳定嵌入表示$Z(G^s_x)$，并用于不同分布数据中的分子关系预测。Formally，${{Y}_{xy}}=F_s ({Z({G}_{x}^s)},{Z({G}_{y}}^s))$。在回归任务和分类任务中，$Y$分别为$(-\infty ,+\infty )$和$\{0,1\}$。因此，CSGIB旨在学习分子的稳定表示并提升模型在OOD数据中的预测性能。





% % We define the prediction task as molecular relationship learning, which involves predicting the interaction relationship or intensity between molecules. For a given pair of molecules $({{G}_{x}},Y,{{G}_{y}})$, our goal is to construct a model $F(*)$ to extract the causal core substructures $(G_{x}^{co},G_{y}^{co})$ of the molecules and predict the relationship target ${{Y}_{xy}}$. Formally, $F({ {G}_{x}},{{G}_{y}})\to {{Y}_{xy}}$. In regression tasks and classification tasks, $Y$ is $(-\infty ,+\infty )$ and $\{0,1\}$, respectively. Therefore, CSGIB aims to optimize molecular relationship learning by utilizing the Causal Subgraph Information Bottleneck.
% % 任务定义：Prediction Task.在本文中，我们将预测任务定义为分子关系学习，即预测分子之间的相互作用关系或强度。对于给定的不同分子对$({{G}_{x}},Y,{{G}_{y}})$，我们的目标是构建一个模型$F(*)$提取分子的因果核心子结构\[(G_{x}^{co},G_{y}^{co})\]并进行间关系目标${{Y}_{xy}}$的预测。Formally，$F({{G}_{x}},{{G}_{y}})\to {{Y}_{xy}}$。在回归任务和分类任务中，$Y$分别为$(-\infty ,+\infty )$和$\{0,1\}$。因此，CSGIB旨在利用因果子图信息瓶颈优化分子关系学习。


% \section{Theoretical Analysis}
% \subsection{Definition}
% \textbf{Interpretability in MRL}: For any molecule $\mathcal{G}$, it can be represented as $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{X},\mathcal{A})$. Here, $\mathcal{V}=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$ denotes the set of nodes. $\mathcal{E}\in N\times N$ represents the connections between atoms within the molecule, which is closely related to the adjacency matrix $\mathcal{A}$. If $({{v}_{i}},{{v}_{j}})\in
% \mathcal{E}$, then ${\mathcal{A}_{ij}}=1$; otherwise, ${\mathcal{A}_{ij}}=0$. $\mathcal{X}$ is the feature matrix, consisting of the atom feature representations. For a given molecular pair $({\mathcal{G}_{x}}, \mathcal{Y}_{xy}, {\mathcal{G}_{y}})$, the objective of MRL is to capture the core substructures involved in chemical reactions as the interpretable molecular embeddings $\mathcal{Z}(\mathcal{G}^{st})$ and to use it for molecule pair interaction prediction in different data distributions. Formally, ${\mathcal{Y}_{xy}}=\mathcal{F}_s ({\mathcal{Z}(\mathcal{G}_{x}^{st})},{\mathcal{Z}(\mathcal{G}_{y}^{st}}))$. In regression tasks and classification tasks, $\mathcal{Y}$ corresponds to $(-\infty ,+\infty )$ and $\{0,1\}$, respectively. Therefore, ReAlignFit aims to capture the core substructures of molecules and improve the model's predictive performance and interpretability.
% % 对于给定的分子对 $({{G}_{x}},{Y}_{xy},{{G}_{y}})$，MRL的目的是捕获参与化学反应的核心子结构且生成具备可解释性的分子嵌入表征 $Z(G^{st})$，并将其用于不同数据分布下的分子对相互作用预测。因此，ReAlignFit 的目标是捕获分子的核心子结构，提高模型的预测性能和可解释性。

% % \begin{equation}
% %     \label{mi}
% %     \mathcal{I}(\mathcal{Y};{\mathcal{G}_{IB}})=\!\!\!\! \int_{\mathcal{Y}}\int_{\mathcal{G}_{IB}} \!\!\!\! p(\mathcal{Y};{\mathcal{G}_{IB}})\log \frac{p(\mathcal{Y};{\mathcal{G}_{IB}})}{p(\mathcal{Y})p({\mathcal{G}_{IB}})} d\mathcal{Y} d\mathcal{G}_{IB}
% % \end{equation}
% % which is calculated by the integral of the function $p(Y,{{G}_{IB}})\log \frac{p(Y,{{G}_{IB}})}{p(Y)p({{G}_{IB}})}$.
% % 图信息瓶颈（graph information bottleneck，GIB）是信息瓶颈理论在图论中的扩展和发展，其目的是学习输入图$G$的最小充分子图${{G}_{IB}}$。
% % Definition 3.1. (graph information bottleneck) GIB通过最大化${{G}_{IB}}$与目标间的互信息，同时引入噪声丢弃${{G}_{IB}}$与$G$之间的信息实现$G$向${{G}_{IB}}$的信息压缩。${{G}_{IB}}$表示为：

% % \textbf{Stable MRL}. For any molecule $\mathcal{G}$, it can be represented as $G=(V,E,X,A)$. Here, $V=\{{{v}_{1}},{{v}_{2}},\cdots ,{{v}_{N}}\}$ denotes the set of nodes. $E\in N\times N$ represents the connections between atoms within the molecule, which is closely related to the adjacency matrix $A$. If $({{v}_{i}},{{v}_{j}})\in
% % E$, then ${{A}_{ij}}=1$; otherwise, ${{A}_{ij}}=0$. $X$ is the feature matrix, consisting of the atom feature representations. For the given molecular pair $({{G}_{x}},{Y}_{xy},{{G}_{y}})$, the objective of Stable MRL is to generate a stable molecular embedding representation $Z(G^{st})$ and to use it for molecule pair interaction prediction in different data distributions. Formally, ${{Y}_{xy}}=F_s ({Z({G}_{x}^{st})},{Z({G}_{y}^{st}}))$. In regression tasks and classification tasks, $Y$ corresponds to $(-\infty ,+\infty )$ and $\{0,1\}$, respectively. Therefore, ReAlignFit aims to learn stable representations of molecules and improve the model's predictive performance.

% \subsection{Theoretical Analysis}
% The spuriousness of attention mechanisms leads the model to capture statistical correlations rather than the actual relationships between substructure pairs. This limitation makes it challenging to identify complex interaction relationships in MRL. Inspired by functional group theory \cite{lin2024functional}, the relationships between molecular pairs can be more precisely represented by maximizing mutual information between core substructures and minimizing confounding information among non-core substructures and solve the above problem.

% % Inspired by functional group theory \cite{lin2024functional}, we aggregate core substructures from different chemical reactions to capture interpretable molecular substructures by reducing the confounding impact of non-core substructures on core substructures. We theoretically demonstrate the feasibility of this idea.
% % 我们将来自不同化学反应的核心亚结构聚合在一起，通过减少非核心亚结构对核心亚结构的干扰影响来捕捉分子中可解释的亚结构。我们从理论上证明了这一想法的可行性。

% \textbf{Theorem 1.} \textit{Given the molecular pair $({\mathcal{G}_{x}},{\mathcal{G}_{y}})$ and the prediction target $\mathcal{Y}$, where the substructure ${\mathcal{G}^{s}}$ of $\mathcal{G}$ consists of core substructure ${\mathcal{G}^{c}}$ and confounding substructure ${\mathcal{G}^{n}}$. For $\forall$ ${\mathcal{G}_{x}},{\mathcal{G}_{y}}\in \mathcal{G}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. Furthermore, considering the correlation between ${\mathcal{G}^{c}}$ and ${\mathcal{G}^{n}}$, if there exists a minimal value $\eta $ such that:}
% % 给定分子对$({{G}_{x}},{{G}_{y}})$和目标$Y$，其中，$G$的子结构${{G}^{sub}}$由核心子结构${{G}^{c}}$和混淆子结构${{G}^{n}}$构成。对于$\forall {{G}_{x}},{{G}_{y}}\in G$，由条件互信息法则可知，$I({{G}_{x}},{{G}_{y}};Y)\le I({{G}^{c}};Y|{{G}^{n}})$。进一步，考虑到${{G}^{c}}$和${{G}^{n}}$的关联性，如果存在一个极小的数$\eta $，使得：
% \begin{equation}
% \label{eq:theorem}
% \left| \mathcal{P}(\!{\mathcal{G}_{x}},{\mathcal{G}_{y}};\!\mathcal{Y})\!-\!\mathcal{P}(\!\mathcal{G}_x^c,\mathcal{G}^c_y;\!\mathcal{Y})\!+\!\mathcal{P}(\!\mathcal{G}_x^c;\mathcal{G}_x^n)\!+\!\mathcal{P}(\!\mathcal{G}_y^c;\mathcal{G}_y^n) \right| \!\!\le \!\!\eta 
% \end{equation}
% \textit{where $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is the true probability between molecular pair and the prediction target. 
% % $\mathcal{P}({{G}^{c}};Y|{{G}^{n}})$ denotes the conditional probability between ${{G}^{c}}$ and $Y$ given ${{G}^{n}}$. 
% $\mathcal{P}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c};\mathcal{Y})$ is interaction probability between core substructure captured by model learning and prediction target called learning probability. $\mathcal{P}({\mathcal{G}^{c}_x};{\mathcal{G}^{n}_x})$ and $\mathcal{P}({\mathcal{G}^{c}_y};{\mathcal{G}^{n}_y})$ are the confounding probabilities between core and confounding substructures.}
% % 其中，$I({{G}_{x}},{{G}_{y}};Y)$ 是分子对与预测目标间的真实概率。$I({{G}^{c}};Y|{{G}^{n}})$ 是给定 ${{G}^{n}}$ 时，${{G}^{c}}$ 和 $Y$ 之间的条件概率。$I(G_{x}^{c},G_{y}^{c};Y)$是模型捕获的核心子结构与预测目标间的作用概率，可通过模型学习动态调整称为学习概率。$I({{G}^{c}_x};{{G}^{n}_x})$ 和$I({{G}^{c}_y};{{G}^{n}_y})$ 是核心子结构与混淆子结构间的混淆概率。
% % \textit{where $I({{G}_{x}},{{G}_{y}};Y)$ is true distribution, represented by the mutual information between $({{G}_{x}},{{G}_{y}})$ and $Y$. $I({{G}^{c}};Y|{{G}^{n}})$ is the conditional mutual information between ${{G}^{c}}$ and $Y$, given ${{G}^{n}}$. $I(G_{x}^{c},G_{y}^{c};Y)$, $I({{G}^{c}_x};{{G}^{n}_x})$ and $I({{G}^{c}_y};{{G}^{n}_y})$ together constitute learning distribution. $I(G_{x}^{c},G_{y}^{c};Y)$ indicates the mutual information between $(G_{x}^{c},G_{y}^{c})$ and $Y$, and $I({{G}^{c}};{{G}^{n}})$ is the mutual information between ${{G}^{n}}$ and ${{G}^{c}}$.} 
% % 其中，$I({{G}_{x}},{{G}_{y}};Y)$为$({{G}_{x}},{{G}_{y}})$与$Y$之间的互信息。$I({{G}^{c}};Y|{{G}^{n}})$代表${{G}^{n}}$已知情况下，${{G}^{c}}$与$Y$之间的条件互信息。$I(G_{x}^{c},G_{y}^{c};Y)$为核心子结构$(G_{x}^{c},G_{y}^{c})$与$Y$的互信息，$I({{G}^{c}};{{G}^{n}})$表示${{G}^{n}}$与${{G}^{c}}$的互信息。

% The $\eta $ measures the similarity between true probability, learning probability, and confounding probability. The discrepancy between true and learning probabilities is derived from model's prediction loss, whereas the impact of confounding information on core substructure representation evaluates confounding probability. Theorem 1 shows that when $\eta $ is sufficiently small, the relationships between molecular pairs can be closely approximated by the joint representation of the maximization of the learning probability and the minimization of the confounding information. Thus, Eq. (\ref{eq:theorem}) holds when both prediction loss and confounding information are sufficiently low. Inspired by Theorem 1, we optimize model learning by integrating prediction loss with confounding probability. 
% % $\eta $衡量真实概率与学习概率、混淆概率之间的相似性。真实概率与学习概率间的差值可由模型预测损失计算得到。混淆概率则可以通过计算混淆信息对核心子结构表征的影响得到。定理1表明，当$\eta $足够小时，分子对之间的关系可以近似为学习概率的最大化与混淆概率最小化的联合表示。
% % 即预测损失与混淆概率足够小时，公式（1）成立。受定理1的启发，我们利用预测损失与混淆概率相结合的方式来优化模型学习。

% \textbf{Proof of Theorem 1}. 
% % 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小。根据互信息法则，$I({{G}_{x}},{{G}_{y}};Y)$表示为：
% % \geq I({{G}^{c}};Y|{{G}^{n}})
% Since $\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)$ is non-negative and $\mathcal{G}_x^n$, $\mathcal{G}_y^n$ have little effect on $\mathcal{Y}$, according to the law of conditional probability, $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})$ is expressed as:
% \begin{equation}
% \begin{aligned}
%     \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) &= \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}_y^n;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,\mathcal{G}_y^c)\\
%     &+\mathcal{P}(\mathcal{G}^n_x; \mathcal{Y}|\mathcal{G}^c_x)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{Y}|\mathcal{G}_x^c,\mathcal{G}_x^n,)\\
%     &\geq \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)
% \end{aligned}   
% \end{equation}
% % % 由于$I({G}^n_x; Y|{G}^c_x)$是非负的且$G_x^n$、$G_y^$对$Y$影响很小，因此：
% % \begin{equation}
% %         I({{G}_{x}},{{G}_{y}};Y) \geq I({G}^c;Y)
% % \end{equation}
% % $I({G}^c_y;Y)$是${G}^c_y$和$Y$之间的无条件互信息，其至少包含了条件互信息$I({{G}^{c}_y};Y|{{G}^{c}_c})$。我们可得到：
% $\mathcal{P}(\mathcal{G}^c_y;\mathcal{Y})$ represents the probability between $\mathcal{G}^c_y$ and $\mathcal{Y}$, which contains at least the conditional probability $\mathcal{P}({\mathcal{G}^{c}_y};\mathcal{Y}|{\mathcal{G}^{c}_c})$. We can obtain $\mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y}) \geq \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}})$. According to the properties of conditional probability:
% \begin{equation}
%     \begin{aligned}
%         \mathcal{P}({\mathcal{G}^{c}};\mathcal{Y}|{\mathcal{G}^{n}}) &\approx \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y}|\mathcal{G}^c_x)\\
%         &\leq \mathcal{P}(\mathcal{G}_x^c;\mathcal{Y})+\mathcal{P}(\mathcal{G}^c_y; \mathcal{Y})=\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})
%     \end{aligned}
%     \label{eq2}
% \end{equation}
% % 根据Eq. (\ref{eq1})和Eq. (\ref{eq2})，同时考虑$I(G_x^c;G_x^n)$、$I(G_y^c;G_y^n)$是非负的，可以得到：

% Based on Eq. (\ref{eq2}), and considering that $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)$ and $\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ are non-negative, we have the equation as:
% \begin{equation}
%     \mathcal{P}({\mathcal{G}_{x}},\!{\mathcal{G}_{y}};\mathcal{Y})\!-\!\mathcal{P}(\mathcal{G}_x^c,\!\mathcal{G}^c_y;\!\mathcal{Y})\!+\!\mathcal{P}(\mathcal{G}_x^c;\!\mathcal{G}_x^n)\!+\!\mathcal{P}(\mathcal{G}_y^c;\!\mathcal{G}_y^n)\!\!\geq\!\! 0
%     \label{eq3}
% \end{equation}
% % 因此，我们可以通过增大$I({G}_x^c,{G}^c_y;Y)$和减小$I(G_x^c;G_x^n)+I(G_y^c;G_y^n)$，使得存在一个极小的正数$\eta $,满足以下关系：

% Therefore, by increasing $\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})$ and decreasing $\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)+\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n)$ (\textit{i.e.}, increasing the correlation between the core substructures captured by the model and prediction targets, and decreasing the impact of the confounding substructures on the core substructure representation), we can ensure that there exists a minimal positive number $\eta $ that satisfies the following relationship:
% \begin{equation}
% \left|\! \mathcal{P}({\mathcal{G}_{x}},{\mathcal{G}_{y}};\mathcal{Y})\!\!-\!\!\mathcal{P}(\mathcal{G}_x^c,\mathcal{G}^c_y;\mathcal{Y})\!\!+\!\!\mathcal{P}(\mathcal{G}_x^c;\mathcal{G}_x^n)\!\!+\!\!\mathcal{P}(\mathcal{G}_y^c;\mathcal{G}_y^n) \!\right|\!\!\le \!\eta 
% \end{equation}




% % Inspired by functional group theory \cite{lin2024functional}, 
% % we aggregate core substructures from different chemical reactions to generate stable molecular representations by reducing the confounding impact of non-core substructures on core substructures. We theoretically demonstrate the feasibility of this idea.
% % % 受官能团理论的启发，我们可以聚合不同化学反应中的重要子结构生成分子在不同数据分布中的稳定表示，以缓解分布外分子表征不一致的问题。我们从理论证明了上述想法的可行性。

% % \textbf{Theorem 1.} \textit{Given the molecular pair $({{G}_{x}},{{G}_{y}})$ and the prediction target $Y$, where the substructure ${{G}^{s}}$ of $G$ consists of core substructure ${{G}^{c}}$ and confounding substructure ${{G}^{n}}$. For $\forall$ ${{G}_{x}},{{G}_{y}}\in G$, according to the law of conditional probability, $\mathcal{P}({{G}_{x}},{{G}_{y}};Y)\geq \mathcal{P}({{G}^{c}};Y|{{G}^{n}})$. Furthermore, considering the correlation between ${{G}^{c}}$ and ${{G}^{n}}$, if there exists a minimal value $\eta $ such that:}
% % % 给定分子对$({{G}_{x}},{{G}_{y}})$和目标$Y$，其中，$G$的子结构${{G}^{sub}}$由核心子结构${{G}^{c}}$和混淆子结构${{G}^{n}}$构成。对于$\forall {{G}_{x}},{{G}_{y}}\in G$，由条件互信息法则可知，$I({{G}_{x}},{{G}_{y}};Y)\le I({{G}^{c}};Y|{{G}^{n}})$。进一步，考虑到${{G}^{c}}$和${{G}^{n}}$的关联性，如果存在一个极小的数$\eta $，使得：
% % \begin{equation}
% % \label{eq:theorem}
% % \left| \mathcal{P}\!(\!{{G}_{x}}\!,\!{{G}_{y}};\!Y)\!-\!\mathcal{P}\!(\!{G}_x^c\!,\!{G}^c_y;\!Y)\!+\!\mathcal{P}\!(\!G_x^c;\!G_x^n)\!+\!\mathcal{P}\!(\!G_y^c;\!G_y^n) \right| \!\!\le \!\!\eta 
% % \end{equation}
% % \textit{where $\mathcal{P}({{G}_{x}},{{G}_{y}};Y)$ is the true probability between molecular pair and the prediction target. 
% % % $\mathcal{P}({{G}^{c}};Y|{{G}^{n}})$ denotes the conditional probability between ${{G}^{c}}$ and $Y$ given ${{G}^{n}}$. 
% % $\mathcal{P}(G_{x}^{c},G_{y}^{c};Y)$ is interaction probability between core substructure captured by model learning and prediction target called learning probability. $\mathcal{P}({{G}^{c}_x};{{G}^{n}_x})$ and $\mathcal{P}({{G}^{c}_y};{{G}^{n}_y})$ are the confounding probabilities between core and confounding substructures.}
% % % 其中，$I({{G}_{x}},{{G}_{y}};Y)$ 是分子对与预测目标间的真实概率。$I({{G}^{c}};Y|{{G}^{n}})$ 是给定 ${{G}^{n}}$ 时，${{G}^{c}}$ 和 $Y$ 之间的条件概率。$I(G_{x}^{c},G_{y}^{c};Y)$是模型捕获的核心子结构与预测目标间的作用概率，可通过模型学习动态调整称为学习概率。$I({{G}^{c}_x};{{G}^{n}_x})$ 和$I({{G}^{c}_y};{{G}^{n}_y})$ 是核心子结构与混淆子结构间的混淆概率。
% % % \textit{where $I({{G}_{x}},{{G}_{y}};Y)$ is true distribution, represented by the mutual information between $({{G}_{x}},{{G}_{y}})$ and $Y$. $I({{G}^{c}};Y|{{G}^{n}})$ is the conditional mutual information between ${{G}^{c}}$ and $Y$, given ${{G}^{n}}$. $I(G_{x}^{c},G_{y}^{c};Y)$, $I({{G}^{c}_x};{{G}^{n}_x})$ and $I({{G}^{c}_y};{{G}^{n}_y})$ together constitute learning distribution. $I(G_{x}^{c},G_{y}^{c};Y)$ indicates the mutual information between $(G_{x}^{c},G_{y}^{c})$ and $Y$, and $I({{G}^{c}};{{G}^{n}})$ is the mutual information between ${{G}^{n}}$ and ${{G}^{c}}$.} 
% % % 其中，$I({{G}_{x}},{{G}_{y}};Y)$为$({{G}_{x}},{{G}_{y}})$与$Y$之间的互信息。$I({{G}^{c}};Y|{{G}^{n}})$代表${{G}^{n}}$已知情况下，${{G}^{c}}$与$Y$之间的条件互信息。$I(G_{x}^{c},G_{y}^{c};Y)$为核心子结构$(G_{x}^{c},G_{y}^{c})$与$Y$的互信息，$I({{G}^{c}};{{G}^{n}})$表示${{G}^{n}}$与${{G}^{c}}$的互信息。

% % The $\eta $ measures the similarity between true probability, learning probability, and confounding probability. The discrepancy between true and learning probabilities is derived from model's prediction loss, whereas the impact of confounding information on core substructure representation evaluates confounding probability. Theorem 1 shows that when $\eta $ is sufficiently small, the molecular pairs interaction can be closely approximated by the joint representation of learning and confounding probabilities. Thus, Eq. (\ref{eq:theorem}) holds when both prediction loss and confounding information are sufficiently low. Inspired by Theorem 1, we optimize model learning by integrating prediction loss with confounding probability. More details are provided in Appendix A.1.
% % % $\eta $衡量真实概率与学习概率、混淆概率之间的相似性。真实概率与学习概率间的差值可由模型预测损失计算得到。混淆概率则可以通过计算混淆信息对核心子结构表征的影响得到。定理1表明，当$\eta $足够小时，分子对之间的关系可以近似为学习概率与混淆概率间的联合表示。
% % % 即预测损失与混淆概率足够小时，公式（1）成立。受定理1的启发，我们利用预测损失与混淆概率相结合的方式来优化模型学习。
% % % The $\eta $ measures the similarity between the actual and learned distributions. Theorem 1 indicates that the learned distribution can be infinitely close to the true distribution if $\eta $ is made sufficiently small by increasing $I(G_{x}^{c},G_{y}^{c};Y)$ and decreasing $I({{G}^{c}};{{G}^{n}})$, i.e.
% % % $I({{G}_{x}},{{G}_{y}};Y)$ can be approximated by $I({{G}^{c}_x};{{G}^{n}_x})+I({{G}^{c}_y};{{G}^{n}_y})-I(G_{x}^{c},G_{y}^{c};Y)$. Theorem 1 and the above analysis provide theoretical support for the model's optimization and loss function design. More details are provided in Appendix A.1.


% % % This confirms that generating molecular representations through core substructures involved in chemical reactions improves the stability of MRL. The core lies in constructing the causal relationships between core substructures and targets and reducing the influence of confounding substructures on core substructures.
% % % $\eta $衡量真实分布和学习分布之间的相似性。定理1表明，如果通过增大$I(G_{x}^{c},G_{y}^{c};Y)$、减小$I({{G}^{c}};{{G}^{n}})$使得$\eta $足够小，即学习分布与真实分布无限接近。这证实了通过化学反应中的核心子结构生成分子表征可以提高MRL的稳定性。其核心为如何构建核心子结构与目标因果关系以及如何降低混淆子结构对核心子结构的影响。

% % % 受官能团理论的启发，我们可以聚合不同化学反应中的重要子结构生成分子在不同数据分布中的稳定表示，以缓解分布外分子表征不一致的问题。我们从理论证明了上述想法的可行性。












% \section{Methodology}
% \label{Methodology}
% % Inspired by Theorem 1, we propose an ReAlignFit containing A and B to enhance the interpretability of MRL, as shown in Fig. 2.
% Motivated by Theorem 1, we propose the ReAlignFit including embedding layer and dual-layer optimization to enhance the interpretability of MRL, as shown in Fig. \ref{model}. 

% In this section, we describe in detail the modeling of causal relationships between substructures, the model’s optimization objectives, and its transformation solution process. First, we derive the causal relationships between substructures and the prediction target by backdoor path analysis (Section \ref{Causal Substructure Model}), which supports the model design and optimization of ReAlignFit. Considering the advantages of the Graph Information Bottleneck (GIB), we propose the Causal Subgraph Information Bottleneck (CS-GIB) with the analytical results of Section \ref{Causal Substructure Model}. Combined with the theoretical analysis in Theorem 1, we further transformationally decompose the optimization objective of CS-GIB and design it as the optimization objective of ReAlignFit in Section \ref{CS-GIB-based Optimization Objective for Interpretable MRL}. Finally, we generate the embedding representation of substructures and optimize the model training in the embedding layer (Section \ref{Molecular Substructure Representation}) and dual-layer optimization (including subgraph interaction layer (Section \ref{Subgraph Interaction Layer}) and causal layer (Section \ref{Causal Layer})), respectively. ReAlignFit distinguishes causal and confusing substructures with the causal bias correction function in the dual-layer optimization. The causal and subgraph interaction layers are designed to optimize the mutual information representations between substructures, respectively.

% \begin{figure*}[htpb]
% \centering
% \includegraphics[width=0.9 \textwidth]{Figure/model1021.png} 
% \caption{The model structure of ReAlignFit. (a) Embedding layer generates substructure representations. (b) Dual-layer optimization distinguishes causal and confusing substructures with $\gamma$ in Eq (\ref{pxc}). It captures core substructures by minimizing $\mathcal{L}_{KL}$ in the subgraph interaction layer and $\mathcal{L}_{pred}$ in the causal layer. Finally, ReAlignFit aggregates these substructures into interpretable subgraphs for relational prediction.}
% \label{model}
% \end{figure*}
% % Motivated by Theorem 1, we propose the ReAlignFit to enhance the interpretability of MRL. In this section, we describe in detail the modeling of causal relationships between substructures, the model’s optimization objectives, and the process of objective transformation solving. First, we derive the causal relationships between substructures and the prediction target by combining causal models with backdoor path analysis in Section \ref{Causal Substructure Model}, which supports the subsequent design of GIB optimization objectives. Combining the analytical results of Theorem 1 and the substructure causality, we refine the GIB optimization objective into a subgraph-level causal mutual information optimization problem and propose the Causal Subgraph Information Bottleneck (CS-GIB) in Section \ref{CS-GIB-based Optimization Objective for Interpretable MRL}. 
% % We further transform the optimization objective of CS-GIB and incorporate it into the loss function of ReAlignFit. 
% % Finally, we distinguish causal and confusing substructures based on substructure embeddings (Section \ref{Molecular Substructure Representation}) and the causal bias correction function and employ the causal layer (Section \ref{Causal Layer}) and the substructure interaction layer (Section \ref{Subgraph Interaction Layer}) for optimization, respectively.


% % Motivated by Theorem 1, we propose the ReAlignFit with joint optimization of predictive loss and confusing information for improving MRL interpretability, as shown in Fig. \ref{model}. We sequentially introduce Causal Substructure Model in Section \ref{Causal Substructure Model}, Causal Subgraph Information Bottleneck (CS-GIB) based Objective for Interpretable MRL in Section \ref{CS-GIB-based Objective for Interpretable MRL}, Molecular Substructure Representation in Section \ref{Molecular Substructure Representation}, and Optimization of the Objective Function in Sections \ref{Subgraph Interaction Layer} and Section \ref{Causal Layer}.
% % , and provide the pseudocode for ReAlignFit in E.
% % 我们依次介绍了A,B,C,D，并在E中提供了ReAlignFit的伪代码。
% \subsection{Causal Substructure Model}
% \label{Causal Substructure Model}
% To more precisely represent the causal relationships between the substructures and the prediction target, we define and construct the Causal Substructure Model (CSM) for MRL to improve the model's interpretability, as shown in Fig. \ref{figcsm}.
% % Considering the effectiveness of causal learning in improving the interpretability of models, we define and construct the Causal Substructure Model (CSM) for MRL, as shown in Fig. \ref{figcsm}. 
% The CSM reveals the causal relationships between molecules ${\mathcal{G}_{x}}$ and ${\mathcal{G}_{y}}$, causal substructure ${\mathcal{G}^{c}}$, confounding substructure ${\mathcal{G}^{n}}$, molecular representation $Z$, and the prediction target $\mathcal{Y}$. The $\to$ indicates the mapping from cause to effect.
% The causal relationships illustrated in Fig. \ref{figcsm} are described as follows:
% \begin{figure}[htpb]
% \centering
% \includegraphics[width=0.85\columnwidth]{Figure/csm.png} 
% \caption{The CSM for stable molecular relational learning.}
% \label{figcsm}
% \end{figure}
% \begin{itemize}
%     \item ${\mathcal{G}_{x}}\to \mathcal{G}_{x}^{c}\leftarrow {\mathcal{G}_{y}}$: $\mathcal{G}_{x}^{c}$ is the causal core substructure of molecule ${\mathcal{G}_{x}}$, jointly determined by ${\mathcal{G}_{x}}$ and ${\mathcal{G}_{y}}$, and varies with changes in ${\mathcal{G}_{y}}$.
%     \item ${\mathcal{G}_{y}}\to \mathcal{G}_{y}^{c}\leftarrow {\mathcal{G}_{x}}$: $\mathcal{G}_{y}^{c}$ is the causal core substructure of molecule ${\mathcal{G}_{y}}$, jointly determined by ${\mathcal{G}_{y}}$ and ${\mathcal{G}_{x}}$, and varies with changes in ${\mathcal{G}_{x}}$.
%     \item ${\mathcal{G}_{x}}\to \mathcal{G}_{x}^{n}\leftarrow {\mathcal{G}_{y}}$: $\mathcal{G}_{x}^{n}$ is the confounding substructure of molecule ${\mathcal{G}{x}}$, representing the substructure involved of ${\mathcal{G}_{x}}$ other than $\mathcal{G}_{x}^{c}$.
%     \item ${\mathcal{G}_{y}}\to \mathcal{G}_{y}^{n}\leftarrow {\mathcal{G}_{x}}$: $\mathcal{G}_{y}^{n}$ is the confounding substructure of molecule ${\mathcal{G}_{y}}$, representing the substructure involved of ${\mathcal{G}_{x}}$ other than $\mathcal{G}_{y}^{c}$.
%     \item $\mathcal{G}_{x}^{c}\to {{Z}_{x}}$: ${{Z}_{x}}$ is the representation of the molecule ${\mathcal{G}_{x}}$, obtained by encoding the causal core substructure $\mathcal{G}_{x}^{c}$.
%     \item $\mathcal{G}_{y}^{c}\to {{Z}_{y}}$: ${{Z}_{y}}$ is the representation of the molecule ${\mathcal{G}_{y}}$, obtained by encoding the causal core substructure $\mathcal{G}_{y}^{c}$.
%     \item ${{Z}_{x}}\to Y\leftarrow {{Z}_{y}}$: $\mathcal{Y}$ represents the interaction between molecular pairs, which depends on both ${\mathcal{G}_{x}}$ and ${\mathcal{G}_{y}}$ in molecular relationship learning.
% \end{itemize}

% We identify the backdoor paths that impact the model's judgment of the causal relationship between ${\mathcal{G}^{c}}$ and $\mathcal{Y}$. The backdoor paths are $\mathcal{G}_{x}^{n}\leftarrow {\mathcal{G}_{y}}\to \mathcal{G}_{y}^{c}\leftarrow {\mathcal{G}_{x}} \to \mathcal{G}_{x}^{c} \to {{Z}_{x}} \to \mathcal{Y}$, ${\mathcal{G}_{y}} \to \mathcal{G}_{x}^{n}\leftarrow {\mathcal{G}_{x}} \to \mathcal{G}_{x}^{c} \to {{Z}_{x}} \to \mathcal{Y}$, ${\mathcal{G}_{x}} \to \mathcal{G}_{y}^{n}\leftarrow {\mathcal{G}_{y}} \to \mathcal{G}_{y}^{c} \to {{Z}_{y}} \to \mathcal{Y}$, and $\mathcal{G}_{x}^{n}\leftarrow {\mathcal{G}_{x}} \to \mathcal{G}_{x}^{c}\leftarrow {\mathcal{G}_{y}} \to \mathcal{G}_{y}^{c} \to {{Z}_{y}} \to \mathcal{Y}$, where ${\mathcal{G}^{c}}$ serves as backdoor criterion to enhance the relevance between substructures and prediction targets. Considering the specificity of molecular relationship learning, \textit{i.e.}, the substructure is either $\mathcal{G}^c$ or $\mathcal{G}^n$. Therefore, we aim to eliminate the impact of ${\mathcal{G}^{n}}$ on molecular representation to create a more transparent interpretability between ${\mathcal{G}^{c}}$ and $\mathcal{Y}$.
% % 因此，我们的目标是消除 ${mathcal{G}^{n}$ 对分子表征的影响，从而在 ${mathcal{G}^{c}$ 和 $\mathcal{Y}$ 之间建立更加透明的解释性。


% % \subsection{Joint Optimization Objective of ReAlignFit via CS-GIB}
% \subsection{CS-GIB-based Optimization Objective for ReAlignFit}
% \label{CS-GIB-based Optimization Objective for Interpretable MRL}

% We define CS-GIB based on the Graph Information Bottleneck in Section \ref{Causal Subgraph Information Bottleneck}. Subsequently, we refine the optimization objective of CS-GIB by incorporating the causal relationships in CSM and designing it as the loss function for ReAlignFit in Section \ref{The Optimization Objective of ReAlignFit}.

% % GIB理论已应用于学习输入图的重要子图，但从分子结构层面GIB难以捕获子结构间复杂的交互行为。因此，在本文中，我们将GIB理论由图层级引申到子图层级，在优化分子表征的同时识别MRL中的核心子结构。
% \subsubsection{Causal Subgraph Information Bottleneck}
% \label{Causal Subgraph Information Bottleneck}
% The Graph Information Bottleneck (GIB) is an extension and development of the Information Bottleneck theory in graph theory \cite{wu2020GIB}. The GIB has provided a novel pathway for improving MRL interpretability due to its advantage of selectively retaining substructures closely related to molecular relationships \cite{hu2024GIBsurvey,BrainIBtnnls2024,li2022oodtkde}.

% \textbf{Definition 4.1.} (\textit{\textbf{GIB}}) \textit{GIB achieves information compression from $\mathcal{G}$ to ${\mathcal{G}_{IB}}$ by maximizing the mutual information between ${\mathcal{G}_{IB}}$ and the target $\mathcal{Y}$ while introducing noise to discard information between ${\mathcal{G}_{IB}}$ and $\mathcal{G}$. ${\mathcal{G}_{IB}}$ is denoted as:}
% \begin{equation}
% \label{eq:GIB}
% \begin{aligned}
%  &{\mathcal{G}_{IB}}=\underset{{\mathcal{G}_{IB}}}{\mathop{\arg \min }}\,-\mathcal{I}(\mathcal{Y};{\mathcal{G}_{IB}})+\beta \mathcal{I}(\mathcal{G};{\mathcal{G}_{IB}})\\
%  \mathcal{I}(\mathcal{Y};&{\mathcal{G}_{IB}})=\!\!\!\! \int_{\mathcal{Y}}\int_{\mathcal{G}_{IB}} \!\!\!\! p(\mathcal{Y};{\mathcal{G}_{IB}})\log \frac{p(\mathcal{Y};{\mathcal{G}_{IB}})}{p(\mathcal{Y})p({\mathcal{G}_{IB}})} d\mathcal{Y} d\mathcal{G}_{IB}
% \end{aligned}
% \end{equation}
% where $\mathcal{I}$ is the mutual information between ${\mathcal{G}_{IB}}$ and $\mathcal{Y}$. $p(*)$ is the probability function.

% Although GIB improves the interpretability of the model, it is difficult for GIB to capture the complex interactions between substructures at the molecular structure level. Therefore, We extend the GIB theory \cite{wu2020GIB} to the feature representation of causal substructures and derive the definition of CS-GIB. Combined with Definition 4.1 and backdoor path analysis results in Section \ref{Causal Substructure Model}, the definition and optimization objective of CS-GIB are as follows:

% \textbf{Definition 4.2.} (\textit{\textbf{CS-GIB}}) \textit{Given a set of graphs and their interaction relationships $({\mathcal{G}_{x}}, \mathcal{Y}, {\mathcal{G}_{y}})$, $(\mathcal{G}^{c}_x, \mathcal{G}^{c}_y)$ and $(\mathcal{G}^{n}_x, \mathcal{G}^{n}_y)$ are the causal graph pairs and confounding graph pairs of $({\mathcal{G}_{x}}, {\mathcal{G}_{y}})$ and $\mathcal{Y}$, respectively. The subgraph $\mathcal{G}^{s}$ of graph $\mathcal{G}$ is defined as ${\mathcal{G}^{s}} = \{\mathcal{G}^{c}, \mathcal{G}^{n}\}$. According to the minimal sufficient principle of mutual information, the optimal causal subgraph and optimization objective are defined as:}
% \begin{equation}
% \label{eq:CSGIB}
% \mathcal{G}^{c}=\underset{{\mathcal{G}^{c}}}{\mathop{\arg \min }}(-\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})+\alpha \mathcal{I}(\mathcal{G}^{c},\mathcal{G}^{n}))
% \end{equation}

% The first term $\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ represents the predictive component formed by the causal subgraph pair $(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ and the prediction target $\mathcal{Y}$. This term encourages $\mathcal{G}_{y}^{c}$ to learn more about $\mathcal{Y}$ to capture interpretable subgraphs related to $Y$ when paired with the subgraph $\mathcal{G}_{x}^{c}$. The second term $\mathcal{I}(G^{c},\mathcal{G}^{n})$ is the confounding component, which reduces the impact of the confounding subgraph ${\mathcal{G}^{n}}$ on the causal subgraph ${\mathcal{G}^{c}}$. Therefore, the causal correlation information between the causal subgraph and the prediction target can be maximally preserved by optimizing Eq. (\ref{eq:CSGIB}). 



% % 当与子图$G_{x}^{c}$配对时，该项鼓励$G_{y}^{c}$学习更多关于$Y$的信息，以生成对Y具有可解释性的子图。
% % {\mathcal{G}_{IB}}=\underset{{\mathcal{G}_{IB}}}{\mathop{\arg \min }}\,-\mathcal{I}(\mathcal{Y};{\mathcal{G}_{IB}})+\beta \mathcal{I}(\mathcal{G};{\mathcal{G}_{IB}})
% % 我们将GIB理论扩展到子图的特征表示和优化中，并推导出CS-GIB的定义。结合Definition 3.1. CS-GIB定义和优化目标如下：
% % 给定一组图及其交互关系$({{G}_{x}},Y,{{G}_{y}})$，$(G^{c}_x,G^{c}_y)$ 和 $(G^{n}_x,G^{n}_y)$ 分别是 $({{G}_{x}},{{G}_{y}})$ 和 $Y$ 的因果图对和混杂图对。图${G}$的子图${G}^{s}={G^{c},G^{n}}$。根据互信息最小充分原则，最优因果子图和优化目标定义为：
% % Given a pair of graphs and their interaction $({{G}_{x}},Y,{{G}_{y}})$, $(G^{c}_x,G^{c}_y)$ and $(G^{n}_x,G^{n}_y)$ are the causal and confounding subgraph pairs of $({{G}_{x}},{{G}_{y}})$ and $Y$, respectively.



% \subsubsection{The Optimization Objective of ReAlignFit}
% \label{The Optimization Objective of ReAlignFit}
% Motivated by Theorem 1 and CSM, we further refine the optimization objective of CS-GIB as the loss function for interpretable MRL. Our interest is to maximize the causal mutual information $\mathcal{I}(\mathcal{Y}; \mathcal{G}_{x}^{c}, \mathcal{G}_{y}^{c})$ and minimize the confounding mutual information $\mathcal{I}(\mathcal{G}^{c}, \mathcal{G}^{n})$ to reduce the influence of the confounding substructure $\mathcal{G}^{n}$ on the interpretability of the causal substructure $\mathcal{G}^{c}$. The optimized CS-GIB objection is:
% \begin{equation}
% \label{eq:CSGIBnew}
% \mathcal{G}^{c}\!=\!\underset{{\mathcal{G}^{c}}}{\mathop{\arg \min }}(\underbrace{\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})\!+\!\beta \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})}_{\text{Section \ref{Subgraph Interaction Layer}}} \underbrace{-\!\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}_{\text{Section \ref{Causal Layer}}}\!)
% \end{equation}
% $- \mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ allows the model to thoroughly learn the causal substructure information related to molecular relationships and use it as the interpretable substructure of the molecule. $\alpha \mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n}) + \beta \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$ avoids the influence of confounding substructure on the causal substructure and eliminates confounding information from the interpretable substructure. To reduce the complexity of mutual information calculation, we transform Eq. (\ref{eq:CSGIBnew}) as follows:





% \textbf{(1) The lower bound of $\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$.} Inspired by \cite{lee2023shiftKDD, TGNN}, the lower bound of $\mathcal{I}_{ca}=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$ is equivalent to:
% \begin{equation}
% \label{eq:op2}
% \begin{aligned}
% \mathcal{I}_{ca}\!&=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\! p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}) \log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{p(\mathcal{Y})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
% &=\!H(\mathcal{Y})\!+\!\!\int\!\!{p(\mathcal{Y})\!\!\int\!\!\!\!\!\int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}\\
% &\ge \int{p(\mathcal{Y})(\int\!\!\!\!\!\int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}}\\
% & := \frac{1}{NM}\sum\limits_{n=1}^{N}{\sum\limits_{m=1}^{M}{q(\gamma (\mathcal{G}_{{{x}_{n}}}^{c},\mathcal{G}_{{{y}_{m}}}^{c})|{\mathcal{Y}})}}\\
% & =-{{\mathcal{L}}_{pred}}\\
% \mathcal{F}&_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\!=\!q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\log(\frac{p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}{q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})})
% % (q(\gamma (\mathcal{G}_{{{x}_{n}}}^{c},\mathcal{G}_{{{y}_{m}}}^{c})|{\mathcal{Y}})|\hat{\mathcal{Y}})
% \end{aligned}
% \end{equation}
% % \begin{equation}
% % \mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\!=\!q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\log(\frac{p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}{q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})})
% % \end{equation}
% where $q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$ is the variational approximation distribution used to approximate the posterior distribution $p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$. The Eq. (\ref{eq:op2}) indicates that minimizing the prediction loss ${{\mathcal{L}}_{pred}}$ achieves the minimization of $-\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$. Maximizing the mutual information between the causal bias correction function $\gamma $ and the prediction targets $\mathcal{Y}$ helps the model identify the causal relationship between $\mathcal{G}^{c}$ and $\mathcal{Y}$ in the CSM backdoor paths. 
% % 在AB等研究的启发下，我们将公式(\ref{eq:CSGIBnew})中$\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$的lower bound转化为：

% \textbf{(2) The upper bound of $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n}) + \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$.} Since $\mathcal{G}^{c}_x$, $\mathcal{G}^{n}_x$ are subgraphs of $\mathcal{G}_x$, according to principles of information theory, we have $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})\le \min (\mathcal{I}({\mathcal{G}^{c}_x},\mathcal{G}_x),\mathcal{I}({\mathcal{G}^{n}_x},\mathcal{G}_x))$. The minimization of the upper bound of $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})$ can be expressed as:
% \begin{equation}
% \label{eq:op1}
% \begin{aligned}
% \mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n})\!\!&\le\! \min \! (\!\!\int \!\!\!\!\! \int\!\!\!p(\mathcal{G}^c_x|\mathcal{G}_x\!,\!\mathcal{R})\! \log(\!\frac{p(\mathcal{G}^c_x|\mathcal{G}_x\!,\!\mathcal{R})}{p(\mathcal{G}^c_x)}\!) d\mathcal{G}^c_xd\mathcal{G}_x, \\
% &\quad  \!\!\!\int \!\!\!\!\! \int\!\!\!p(\mathcal{G}^n_x|\mathcal{G}_x,\mathcal{R}) \log(\frac{p(\mathcal{G}^n_x|\mathcal{G}_x,\mathcal{R})}{p(\mathcal{G}^n_x)}) d\mathcal{G}^n_xd\mathcal{G}_x )\\
% &:=\min(KL(p(\mathcal{G}^c_x|\mathcal{G}_x,\mathcal{R})||p(\mathcal{G}^c_x)),\\
% &\quad \quad \quad KL(p(\mathcal{G}^n_x|\mathcal{G}_x,\mathcal{R})||p(\mathcal{G}^n_x))))\\
% &=\min(\mathcal{L}_{KL_{x}}^{c},\mathcal{L}_{KL_{x}}^{n})
% \end{aligned}
% \end{equation}
% where $\mathcal{R}$ is substructure importance calculation function. Therefore, minimizing $\mathcal{L}_{KL}^{\alpha }=\min (\mathcal{L}_{K{{L}_{x}}}^{c},\mathcal{L}_{K{{L}_{x}}}^{n})$ provides an upper bound for the minimization of $I(G_{x}^{c},G_{x}^{n})$. Similarly, the upper bound for minimizing $I(G_{y}^{c},G_{y}^{n})$ can be obtained by minimizing $\mathcal{L}_{KL}^{\beta }=\min (\mathcal{L}_{K{{L}_{y}}}^{c},\mathcal{L}_{K{{L}_{y}}}^{n})$.
% % 加上与理论分析1相关的语句
% % 最终，我们得到了与Theorem 1的推导结论一致的模型优化目标，其由${\mathcal{L}}_{pred}$和$\mathcal{L}_{KL}$的总和构成。

% Eventually, we obtain the model optimization objective $\mathcal{L}$ consistent with the derived conclusion in Theorem 1, which consists of the sum of ${\mathcal{L}}_{pred}$ and $\mathcal{L}_{KL}$.
% % $\mathcal{L}$与公式10和Theorem 1的理论推导相一致。
% % Finally, we define the model training objective as the sum of ${\mathcal{L}}_{pred}$ and $\mathcal{L}_{KL}$ as follows:
% \begin{equation}
% \label{lossnum}
%    \mathcal{L}= {\mathcal{L}}_{pred}+\alpha \mathcal{L}_{KL}^{\alpha }+\beta \mathcal{L}_{KL}^{\beta }
% \end{equation}
% where $\alpha$ and $\beta$ are hyperparameters that adjust the weights of the confounding loss, consistent with the settings in Eq.~(\ref{eq:CSGIBnew}). To achieve the optimization of ReAlignFit, we designed the model structure shown in Fig. \ref{model}.





% \subsection{Molecular Substructure Representation}
% \label{Molecular Substructure Representation}
% Considering the irregularity of molecular spatial topologies and the effectiveness of Graph Neural Networks (GNN) in dealing with topological features, ReAlignFit utilizes the GNN encoder and an adjacency aggregation strategy to generate embeddings for irregular substructures. Furthermore, ReAlignFit optimizes substructure representations by the substructure importance function and noise injection. Finally, ReAlignFit inputs the embedding representations into the dual-layer optimization of GIB to capture interpretable substructure pairs.
% % 考虑到分子空间拓扑结构的不规则性和图神经网络（GNN）处理拓扑关键的有效性，ReAlignFit利用GNN编码器和邻接聚合策略生成不规则子结构的嵌入表征。更进一步的，ReAlignFit借助子结构重要性函数和噪声注入优化子结构的表示。最后，ReAlignFit将嵌入表征输入到由子图交互层和因果层组成的 GIB 双层优化，以捕捉可解释的子结构对。

% % ReAlignFit utilizes GNN encoders to generate embeddings for irregular substructure pairs and input them into the dual-layer optimization of GIB, consisting of the subgraph interaction layer and the causal layer, to capture interpretable substructure pairs. 
% Inspired by \cite{lee2023cgibICML,dasfaa}, we use neighborhood feature weighting consisting of adjacency matrix $\mathcal{E}$, node feature representation $Z(v_n)$, and neighborhood structural coefficient $\mathcal{C}$ as the input for irregular substructure generation. For the neighborhood feature weighting, we define the node feature representation $Z^{l+1}(v_n)$ in $(l+1)$th layer and neighborhood structural coefficient $\mathcal{C}$ as follows:
% \begin{equation}
% \label{node}
% \begin{aligned}
%     Z^{l+1}(v_n&)=\sigma ( \sum \nolimits_{v_u \in \mathcal{N}(v_n)} W_u Z^{l}(v_u) + W_{n} Z^{l}(v_n) )\\
%     &\mathcal{C}=\frac{\sigma (Z(v_n))\cdot \log \sigma (Z(v_n))}{\sum \nolimits_{v_u \in \mathcal{N}(v_n)}\sigma (Z(v_u))\cdot \log \sigma (Z(v_u))}
% \end{aligned}
% \end{equation}
% % \begin{equation}
% % \mathcal{C}=\frac{\sigma (Z(v_n))\cdot \log \sigma (Z(v_n))}{\sum \nolimits_{v_u \in \mathcal{N}(v_n)}\sigma (Z(v_u))\cdot \log \sigma (Z(v_u))}
% % % \frac{\textit{IE}_{{V}_{c}^{k}}}{\sum\nolimits_{{{V}_{s}}\in {{V}_{c}^{k}}}{\textit{IE}_{{V}_{s}^{k}}}}
% % \label{ck}
% % \end{equation}
% where $\mathcal{N}(v_n)$ denotes the set of neighbor nodes of node $v_n$. $W$ is the weight matrix and $\sigma$ is the activation function. $Z(*)$ is the embedding representation obtained from the GNN encoder, which can be selected from GIN \cite{gin}, MPNN \cite{mpnn}, GAT \cite{gat}, or GCN \cite{gcn}. For molecule $\mathcal{G}_x$, its irregular substructure $Z({\mathcal{G}_x^s})$ is obtained by aggregating the central node $v_n$ and the $K$-hop adjacent nodes along with $\mathcal{C}$ weighted accordingly:
% \begin{equation}
%     \label{irr}
%     Z({\mathcal{G}_x^s}) = \sum \nolimits_{k=1}^K \sum \nolimits_{{{v}_{u}}\in v_{n}^{k}}(Z(v_n)||{{\mathcal{C}}} \cdot Z(v_u))
% \end{equation}

% Similarly, the irregular substructure representation $Z({\mathcal{G}_y^s})$ for molecule $\mathcal{G}_y$ can be computed. We calculate the substructure importance $R$ of different substructures $G^{s}$ to the molecular $G$:
% \begin{equation}
% \label{eq:nos}
% \mathcal{R}_i\!\!=\!\!\sum{\!\!\theta \sigma (\!Z({\mathcal{G}^{s_i}}),Z(\mathcal{G})\!)}\!+\!(\!1\!-\!\theta )\!\!\sum\limits\!\!{\sigma \!(Z({\mathcal{G}^{s_j}}),Z(\mathcal{G})\!)}
% \end{equation}
% where $\theta$ is a parameter. $\sigma (*,*)$ is a probability function. We replace the representation of unimportant substructures with noise $ \eta$. ReAlignFit controls the impact of confounding information on causal substructures via $\mathcal{R}$ and $\lambda$, generating the optimized molecular representation $Z({\mathcal{G}}')$: 
% \begin{equation}
% \label{eq:gen}
% \begin{aligned}
%   Z(&{\mathcal{G}}'^s) = h({\mathcal{G}}^s,\lambda,\eta)={{\lambda }_{i}}Z({\mathcal{G}^{s_i}})+(1-{{\lambda }_{i}})\eta\\
%   &{{\lambda }_{i}}\!\sim\! {\text{Bernoulli}}( \sigma ( {\mathcal{R}_i} ) ), \eta \sim ( {{\mu }_{Z(\mathcal{G}_{{}}^{s})}},\sigma _{Z(\mathcal{G}^{s})}^{2})
% \end{aligned}
% \end{equation}
% where ${{\mu }_{Z(\mathcal{G}_{{}}^{s})}}$ and $\sigma _{Z(\mathcal{G}_{{}}^{s})}^{2}$ are mean and variance of $Z(\mathcal{G}_{{}}^{s})$, respectively. The optimized substructure representations $Z({\mathcal{G}}'^s_x)$ and $Z({\mathcal{G}}'^s_y)$ of molecules $\mathcal{G}_x$ and $\mathcal{G}_y$ were calculated from Eq. (\ref{eq:gen}) and used for dual-layer optimization of ReAlignFit.


% % 由公式（19）计算得到分子A和B优化后的子结构表征C和D并用于 ReAlignFit的双层优化。
% % 对于分子G，其不规则子结构由中心节点D和K跳邻接节点及其邻域结构系数加权聚合得到：
% % ReAlignFit 利用 GNN 编码器生成不规则子结构对的嵌入表示，并将其输入由子图交互层和因果层组成的 GIB 双层优化，以捕获可解释性的子结构对。受A的启发，我们使用邻接矩阵B、节点特征表示C和邻域结构系数D构成的邻域特征加权作为不规则子结构生成的输入。对于邻域特征加权，我们将第（l+1）层的节点特征表示C、邻域结构系数D定义为：
% % h_{v}^{(l+1)} = \sigma \left( \sum_{u \in \mathcal{N}(v)} W^{(l)} h_{u}^{(l)} + W_{\text{self}}^{(l)} h_{v}^{(l)} \right)


% % \subsection{Optimization of the Objective Function}
% % \label{Optimization of the Objective Function}

% \subsection{Subgraph Interaction Layer (Minimizing $\mathcal{I}(\mathcal{G}_{x}^{c},\mathcal{G}_{x}^{n}) + \mathcal{I}(\mathcal{G}_{y}^{c},\mathcal{G}_{y}^{n})$ in Eq. (\ref{eq:op1}))}
% \label{Subgraph Interaction Layer}
% To reduce the difficulty of mutual information computation in Eq. (\ref{eq:op1}), we decompose probability distributions $p(\mathcal{G}^c|\mathcal{G},\mathcal{R})$ and $p(\mathcal{G}^c)$ into variational approximation and multivariate Bernoulli distribution, respectively.

% In this section, we redefine $p(\mathcal{G}^s)$ using the variational approximation $\omega(\mathcal{G}^s)$. For the edges adjacent to the substructure $\mathcal{G}^s$, we sample them using a hyperparameter $\tau \in[0,1]$ and ${\text{Bernoulli}}(\tau)$ to obtain the Bernoulli distribution of $\omega(\mathcal{G}^s) =\tau^{|\mathcal{G}^s|}(1-\tau^{|\mathcal{G}|/|\mathcal{G}^s|})$. Inspired by \cite{TGNN}, we parameterize $p(\mathcal{G}^c_x|\mathcal{G}_x,\mathcal{R})$ as a multivariate Bernoulli distribution:
% \begin{equation}
%     \label{multbern}
%     p(\mathcal{G}^c_x|\mathcal{G}_x,\mathcal{R}) = \prod_{\mathcal{G}_x^{s_{i}} \in \mathcal{G}^c_x} p_x^c \cdot \prod_{\mathcal{G}_x^{s_{i}} \notin \mathcal{G}^c_x} (1 - p_x^c)
% \end{equation}
% where $p_x^c$ is the probability distribution given $\mathcal{G}_x$ and $\mathcal{R}$. Since $\mathcal{R}$ is incorporated into the substructure representation in Eq. (\ref{eq:gen}), $p_x^c$ can be computed via $p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)$. We adopt the causal bias correction function $\gamma$ to sample core substructures and select high contributing substructures as causal substructures, utilizing GIB to optimize molecular representations while implementing gradient propagation, as shown below:
% \begin{equation}
% \label{pxc}
% \begin{aligned}
%     \gamma &=\frac{\left\| \sigma(Z(\omega(\mathcal{G}_{x}^{s_i})),Z(\omega(\mathcal{G}_{y}^{s_j}))) \right\|_{2}^{2}}{\left\| \sigma(Z(\mathcal{G}_{x}^{s_i}),Z(\mathcal{G}_{y}^{s_j})) \right\|_{2}^{2}}\\  
%      p_x^c \!\!= \!\! p(\mathcal{G}^c_x|&\mathcal{G}^n_x,\mathcal{G}_x) \!\! = \! \sigma \! \left( Z(\mathcal{G}^c_x),(Z(\mathcal{G}^{n_1}_x)||\cdots||Z(\mathcal{G}^{n_i}_x)) \right)
% \end{aligned}
% \end{equation}
% % \begin{equation}
% %     \label{pxc}
% %     p_x^c \!\!= \!\! p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x) \!\! = \! \sigma \! \left( Z(\mathcal{G}^c_x),(Z(\mathcal{G}^{n_1}_x)||\cdots||Z(\mathcal{G}^{n_i}_x)) \right)
% % \end{equation}
% where $\sigma(*)$ is a sigmoid function. 
% Therefore, the $\mathcal{L}_{KL_{x}}^{c} $ in Eq. (\ref{eq:op1}) is calculated as follows:
% \begin{equation}
%     \label{klxc}
%     \begin{aligned}
%  \mathcal{L}_{KL_{x}}^{c}&=KL[p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)||\omega(\mathcal{G}^c)]\\
%         &=\mathbb{E}_{(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)} \!\!\! \left[\!\!\sum \limits_{\mathcal{G}^c_x} p_x^c \log\frac{p_x^c}{\tau}\! +\! (1\!-\!p_x^c)\log\frac{1\!\!-\!\!p_x^c}{1\!\!-\!\!\tau}\!\!\right]
%     \end{aligned}
% \end{equation}
% % \begin{equation}
% %     \label{conlossx}
% %     \begin{aligned}
% %        \mathcal{L}_{KL}^{\alpha } &= \min (\mathcal{L}_{K{{L}_{x}}}^{c},\mathcal{L}_{K{{L}_{x}}}^{n})\\ 
% %         &=\min (\mathbb{E}_{(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)}[*],\mathbb{E}_{(\mathcal{G}^n_x|\mathcal{G}^c_x,\mathcal{G}_x)}[*])
% %     \end{aligned}
% % \end{equation}
% Similarly, we can obtain the $\mathcal{L}_{KL_{x}}^{n}$, $\mathcal{L}_{KL_{y}}^{c}$ and $\mathcal{L}_{KL_{y}}^{n}$.
% % following:
% % \begin{equation}
% %     \label{conlossy}
% %     \mathcal{L}_{KL}^{\beta } = \min (\mathbb{E}_{(\mathcal{G}^c_y|\mathcal{G}^n_y,\mathcal{G}_y)}[*],\mathbb{E}_{(\mathcal{G}^n_y|\mathcal{G}^c_y,\mathcal{G}_y)}[*])
% % \end{equation}
% % 受A的启发，我们将$p(\mathcal{G}^c_x|\mathcal{G}_x,\mathcal{R})$参数化为多元伯努利分布：
% % 其中，$p_x^c$是给定$\mathcal{G}_x$和$\mathcal{R}$的概率分布。由于$\mathcal{R}$在Eq. （19）中已融入子结构表示中，因此，$p_x^c$可通过$p(\mathcal{G}^c_x|\mathcal{G}^n_x,\mathcal{G}_x)$计算得到。然后，我们采用因果相似性函数$\gamma$对核心子结构进行采样，在实现梯度传播的同时利用GIB优化分子表示。如下所示：
% % 在本节中，我们采用变分近似$\omega(\mathcal{G}^s)$重新定义了$p(\mathcal{G}^s)$。对于与子图$\mathcal{G}^s$邻接的边，我们通过超参数和A对其采样，以获得$\omega(\mathcal{G}^s)$的伯努利分布：

% \subsection{Causal Layer (Minimizing $-I(Y;G_{x}^{c},G_{y}^{c})$ in Eq. (\ref{eq:op2}))}
% \label{Causal Layer}

% % Eq. (\ref{eq:op2})中的$q(\gamma (\mathcal{G}_{{{x}}}^{c},\mathcal{G}_{{{y}}}^{c})|{\mathcal{Y}})$强调采用因果子结构对$(\mathcal{G}_{{{x}}}^{c},\mathcal{G}_{{{y}}}^{c})$对分子$\mathcal{G}_{{{x}}}$和$\mathcal{G}_{{{y}}}$之间的关系进行预测。ReAlignFit从causal similarity function $\gamma$中筛选高贡献度的子结构作为因果子结构，并聚合得到分子的可解释性子结构$\mathcal{G}^{is}$。我们从分子的可解释性子结构$\mathcal{G}^{is}$中得到分子最终的嵌入表示$\mathcal{H}$:
% The predictor term $q(\gamma (\mathcal{G}_{{{x}}}^{c},\mathcal{G}_{{{y}}}^{c})|{\mathcal{Y}})$ in Eq. (\ref{eq:op2}) emphasizes predicting the relationship between molecules $\mathcal{G}_{{{x}}}$ and $\mathcal{G}_{{{y}}}$ using the causal substructure pair $(\mathcal{G}_{{{x}}}^{c},\mathcal{G}_{{{y}}}^{c})$.

% ReAlignFit selects the high-contribution substructures from the causal bias correction function $\gamma$ as causal substructures and aggregates them to obtain the interpretable substructure $\mathcal{G}^{is}$ of the molecule. We obtain the final embedding representation $\mathcal{H}$ of the molecule $\mathcal{G}$ from the interpretability substructure $\mathcal{G}^{is}$.
% \begin{equation}
%     \label{embedding}
%     \mathcal{H}=\text{Readout}(\lambda_1 Z(\mathcal{G}^{c_1})|| \cdots || \lambda_n Z(\mathcal{G}^{c_n}) |\mathcal{G}^c \in \mathcal{G}^{is})
% \end{equation}

% According to Eq. (\ref{embedding}), the learned embeddings of molecules $\mathcal{G}_{{{x}}}$ and $\mathcal{G}_{{{y}}}$ are represented as $\mathcal{H}_x$ and $\mathcal{H}_y$, respectively. Finally, the prediction loss ${{\mathcal{L}}{pred}}$ of ReAlignFit is denoted as:
% % 根据公式(\ref{embedding}),分子$\mathcal{G}_{{{x}}}$和$\mathcal{G}_{{{y}}}$学习的嵌入表示分别为$\mathcal{H}_x$和$\mathcal{H}_y$。最后，ReAlignFit的预测损失${{\mathcal{L}}_{pred}}$表示为：
% \begin{equation}
%     \label{loss}
%     {{\mathcal{L}}_{pred}}=-\frac{1}{MN}\mathbb{E}_{(\mathcal{G}_x,\mathcal{G}_y)\sim \mathcal{Y}}\log [\sigma(\mathcal{H}_x,\mathcal{H}_y)]
% \end{equation}
% where $M$ and $N$ represent the number of molecules $\mathcal{G}$ and $\mathcal{G}$. In molecular relationship learning, ${{\mathcal{L}}_{pred}}$ can be chosen as the cross-entropy loss (for classification tasks) or the mean absolute error loss (for regression tasks), depending on the nature of the task. 




% % \section{Methodology}
% % \label{Methodology}
% % Motivated by Theorem 1, we propose the ReAlignFit with joint optimization of predictive loss and confusing information for stable MRL, as shown in Fig. \ref{model}. We construct a Causal Substructure Model (CSM) and define the Causal Subgraph Information Bottleneck theory (CSIB). We propose a dual-layer optimization for CSIB, including causal layer and subgraph interaction layer optimization to minimize $-I(Y;G_{x}^{c},G_{y}^{c})$ and $I(G^{c},G^{n})$, respectively.


% % % 受定理1的启发，我们提出了ReAlignFit用于提升提升MRL的稳定性。首先，我们构建了因果子结构模型以揭示核心子结构与预测目标间的因果关系。然后，我们在图信息瓶颈的基础上定义了因果子图信息瓶颈并提出了与理论1相一致的优化目标。接下来，我们描述了模型架构，如图所示。最后，我们提出了 GIB 的两层优化方案，包括因果层优化和子结构交互层优化，分别最小化 $-I(Y;G_{x}^{c},G_{y}^{c})$ 和 $I(G^{c},G^{n})$。

% % \subsection{Model Architecture}
% % ReAlignFit utilizes GNN encoders to generate molecular pair representations and inputs them into the dual-layer optimization of GIB, consisting of subgraph interaction layer and causal layer, to capture stable molecular representations.

% % % 我们基于GNN实现了CSGIB并用于分子关系学习。模型整体结构如图？所示。首先，我们利用GNN编码器和邻接矩阵生成子结构表示表示。然后，在子结构交互层引入混淆信息，在因果层嵌入子结构间的因果相关性。最后，结合图信息瓶颈理论优化子结构表征并生成分子嵌入表示。

% % % \mathcal{H}


% % We utilize substructure coefficient and neighbor feature weighted aggregation to extract molecular substructures. For the molecule $G$, the substructure $G^{s}$ consists of the central atom ${V}_{c}$ and its $K$-hopping neighboring nodes ${V}_{c}^{k}$. Formally, ${{G}^{s}}\!\!=\!\!\sum_{k=1}^K\sum_{{{V}_{s}}\in V_{c}^{k}}{{\mathcal{C}}^{k}}\text{GNN}^{k}({{V}_{s}})$. $\text{{GNN}}(*)$ can be selected from GIN \cite{gin}, MPNN \cite{mpnn}, GAT \cite{gat}, or GCN \cite{gcn}. The calculation of $\mathcal{C}^{k}$ is detailed in Appendix B.1. We use $G^{s}$ to obtain the molecular representation $Z(G)$.
% % % The definition is shown in Eq. (\ref{eq:gsub}).

% % % \begin{equation}
% % % \label{eq:gsub}
% % % {{G}^{s}}({{V}_{c}},G)=\sum_{l=1}^L\sum_{k=1}^K\sum_{{{V}_{s}}\in V_{c}^{k}}{{\mathcal{C}}^{k}}\cdot \text{GNN}_{l}^{k}({{V}_{s}})
% % % \end{equation}
% % % % 我们利用子结构系数和多跳邻居特征加权聚合提取不同大小和不规则形状的分子子结构。对于给定的分子图$G$，在经过$L$层GNN编码器后，由中心原子${{V}_{c}}$的$K$跳相邻节点${V}_{c}^{k}$组成的不规则子结构$G^{sub}$定义如式（\ref{eq1}）所示。
% % % where $\text{{GNN}}(*)$ can be selected from GCN, GAT, MPNN, or GIN. The calculation of $\mathcal{C}^{k}$ is detailed in Appendix A.2. We use $G^{s}$ to obtain the molecular representation $Z(G)$.
% % % 其中，$textit{GNN}(*)$可以从 GCN、GAT、MPNN、GIN中选择。$\mathcal{C}^{k}$计算详见附件A1。经过上述步骤，我们利用$G^{sub}$的特征表示组合得到分子$G$的嵌入表示$Z(G)$。

% % % Next, we calculate the substructure importance $R$ of different substructures $G^{s}$ to the molecular representation $Z(G)$ and replace the substructure representation by noise injection to reduce the impact of confounding substructures on the molecular representation. $R$ is defined as follows:
% % % 接下来我们计算不同子结构$G^{sub}$对分子表征$Z(G)$的重要性$R$，并通过噪声注入的方式替换子结构的特征表示以减少混淆子结构对分子表征的影响。$R$的定义如下：
% % Next, we calculate the substructure importance $R$ of different substructures $G^{s}$ to the molecular $G$:
% % \begin{equation}
% % \label{eq:nos}
% % {R}_i\!\!=\!\!\sum{\!\!\theta \sigma (\!Z({{G}^{s_i}}),Z(G)\!)}\!+\!(\!1\!-\!\theta )\!\!\sum\limits\!\!{\sigma \!(Z({{G}^{s_j}}),Z(G)\!)}
% % \end{equation}
% % where $\theta$ is a parameter. $\sigma (*,*)$ is a probability function. We replace the representation of unimportant substructures with noise $\varepsilon $. ReAlignFit controls the impact of confounding information on causal substructures via $R$ and $\lambda$, generating the optimized molecular representation $Z({G}')$. Then, the causal similarity between ${{G}_{x}}$ and ${{G}_{y}}$ is modeled by the substructure interaction function $\gamma $, defined as: $\gamma\!\!=\!\!\left\|\! Z(G_{x}^{s_i})\!-\!Z(G_{y}^{s_j}) \!\right\|_{2}^{2} / \!\!\left\|\! Z(G_{x}^{s_i})\!+\!Z(G_{y}^{s_j}) \!\right\|_{2}^{2}$. Based on  $\gamma $ and $Z({G}')$, generate $Z({G}'')$ and obtain the CSIB optimized core substructure $G^k$. Finally, we generate the stable molecular embedding representation $Z(G^{st})\!\!\!=\!\!\!\sum \text{MLP}Z(\!G^k\!)$ for MRL.
% % % 因果模型：
% % \subsection{Causal Substructure Model}
% % % \label{Causal Substructure Model}
% % We define and construct the CSM for stable molecular relational learning, as shown in Fig. \ref{figcsm}. 
% % The causal relationships illustrated in Fig. \ref{figcsm} are described as follows:
% % \begin{itemize}
% %     \item ${{G}_{x}}\to G_{x}^{c}\leftarrow {{G}_{y}}$: $G_{x}^{c}$ is the causal core substructure of molecule ${{G}_{x}}$, jointly determined by ${{G}_{x}}$ and ${{G}_{y}}$, and varies with changes in ${{G}_{y}}$.
% %     \item ${{G}_{y}}\to G_{y}^{c}\leftarrow {{G}_{x}}$: $G_{y}^{c}$ is the causal core substructure of molecule ${{G}_{y}}$, jointly determined by ${{G}_{y}}$ and ${{G}_{x}}$, and varies with changes in ${{G}_{x}}$.
% %     \item ${{G}_{x}}\to G_{x}^{n}\leftarrow {{G}_{y}}$: $G_{x}^{n}$ is the confounding substructure of molecule ${{G}{x}}$, representing the substructure involved of ${{G}_{x}}$ other than $G_{x}^{c}$.
% %     \item ${{G}_{y}}\to G_{y}^{n}\leftarrow {{G}_{x}}$: $G_{y}^{n}$ is the confounding substructure of molecule ${{G}_{y}}$, representing the substructure involved of ${{G}_{x}}$ other than $G_{y}^{c}$.
% %     \item $G_{x}^{c}\to {{Z}_{x}}$: ${{Z}_{x}}$ is the representation of the molecule ${{G}_{x}}$, obtained by encoding the causal core substructure $G_{x}^{c}$.
% %     \item $G_{y}^{c}\to {{Z}_{y}}$: ${{Z}_{y}}$ is the representation of the molecule ${{G}_{y}}$, obtained by encoding the causal core substructure $G_{y}^{c}$.
% %     \item ${{Z}_{x}}\to Y\leftarrow {{Z}_{y}}$: $Y$ represents the interaction between molecular pairs, which depends on both ${{G}_{x}}$ and ${{G}_{y}}$ in molecular relationship learning.
% % \end{itemize}

% % % The CSM reveals the causal relationships between molecules ${{G}_{x}}$ and ${{G}_{y}}$, causal core substructure ${{G}^{c}}$, confounding substructure ${{G}^{n}}$, molecular representation $Z$, and the prediction target $Y$. The $\to$ indicates the mapping from cause to effect. 
% % % The causal relationships illustrated in Figure \ref{figcsm} are described as follows:
% % % 我们在CSGIB中定义并构建了因果子结构模型（CSM）并用于分子关系学习，如图2所示。CSM揭示了分子${{G}_{x}}$和${{G}_{y}}$、因果核心子结构\[{{G}^{co}}\]、混淆子结构\[{{G}^{no}}\]、分子表示$Z$和预测目标\[Y\]之间的因果关系。$\to $表示原因到结果的映射，即因果关系。图2中的因果关系说明如下：
% % % \begin{itemize}
% % %     \item ${{G}_{x}}\to G_{x}^{c}\leftarrow {{G}_{y}}$: $G_{x}^{c}$ is the causal core substructure of molecule ${{G}_{x}}$, jointly determined by ${{G}_{x}}$ and ${{G}_{y}}$, and varies with changes in ${{G}_{y}}$.
% % %     \item ${{G}_{y}}\to G_{y}^{c}\leftarrow {{G}_{x}}$: $G_{y}^{c}$ is the causal core substructure of molecule ${{G}_{y}}$, jointly determined by ${{G}_{y}}$ and ${{G}_{x}}$, and varies with changes in ${{G}_{x}}$.
% % %     \item ${{G}_{x}}\to G_{x}^{n}\leftarrow {{G}_{y}}$: $G_{x}^{n}$ is the confounding substructure of molecule ${{G}{x}}$, representing the substructure involved of ${{G}_{x}}$ other than $G_{x}^{c}$.
% % %     \item ${{G}_{y}}\to G_{y}^{n}\leftarrow {{G}_{x}}$: $G_{y}^{n}$ is the confounding substructure of molecule ${{G}_{y}}$, representing the substructure involved of ${{G}_{x}}$ other than $G_{y}^{c}$.
% % %     \item $G_{x}^{c}\to {{Z}_{x}}$: ${{Z}_{x}}$ is the representation of the molecule ${{G}_{x}}$, obtained by encoding the causal core substructure $G_{x}^{c}$.
% % %     \item $G_{y}^{c}\to {{Z}_{y}}$: ${{Z}_{y}}$ is the representation of the molecule ${{G}_{y}}$, obtained by encoding the causal core substructure $G_{y}^{c}$.
% % %     \item ${{Z}_{x}}\to Y\leftarrow {{Z}_{y}}$: $Y$ represents the interaction relationship, which depends on both ${{G}_{x}}$ and ${{G}_{y}}$ in molecular relationship learning.
% % % \end{itemize}
% % % \[{{G}_{x}}\to G_{x}^{co}\leftarrow {{G}_{y}}\]：\[G_{x}^{co}\]是分子${{G}_{x}}$的因果核心子结构，由${{G}_{x}}$和${{G}_{y}}$共同决定且随着${{G}_{y}}$的变化而变化。
% % % \[{{G}_{y}}\to G_{y}^{co}\leftarrow {{G}_{x}}\]：\[G_{y}^{co}\]是分子${{G}_{y}}$的因果核心子结构，由${{G}_{y}}$和${{G}_{x}}$共同决定且随着${{G}_{x}}$的变化而变化。
% % % \[{{G}_{x}}\to G_{x}^{no}\leftarrow {{G}_{y}}\]：\[G_{x}^{no}\]是分子${{G}_{x}}$的混淆子结构，是分子${{G}_{x}}$与分子${{G}_{y}}$相互作用中除\[G_{x}^{co}\]之外的子结构。
% % % \[{{G}_{y}}\to G_{y}^{no}\leftarrow {{G}_{x}}\]：\[G_{y}^{no}\]是分子${{G}_{y}}$的混淆子结构，是分子${{G}_{y}}$与分子${{G}_{x}}$相互作用中除\[G_{y}^{co}\]之外的子结构。
% % % \[G_{x}^{co}\to {{Z}_{x}}\]：\[{{Z}_{x}}\]是分子${{G}_{x}}$的嵌入表示，由分子${{G}_{x}}$的因果核心子结构\[G_{x}^{co}\]编码得到。
% % % \[G_{y}^{co}\to {{Z}_{y}}\]：\[{{Z}_{y}}\]是分子${{G}_{y}}$的嵌入表示，由分子${{G}_{y}}$的因果核心子结构\[G_{y}^{co}\]编码得到。
% % % \[{{Z}_{x}}\to Y\leftarrow {{Z}_{y}}\]：\[Y\]是分子${{G}_{x}}$和${{G}_{y}}$作用关系，在分子关系学习中，\[Y\]同时取决于分子${{G}_{x}}$和${{G}_{y}}$。
% % We identify the backdoor paths that impact the model's judgment of the causal relationship between ${{G}^{c}}$ and $Y$. The backdoor paths are $G_{x}^{n}\leftarrow {{G}_{y}}\!\!\to \!\!G_{y}^{c}\leftarrow {{G}_{x}}\!\!\to\!\! G_{x}^{c}\!\!\to\!\! {{Z}_{x}}\!\!\to\!\! Y$, ${{G}_{y}}\!\!\to \!\!G_{x}^{n}\leftarrow {{G}_{x}}\!\!\to\!\! G_{x}^{c}\!\!\to\!\! {{Z}_{x}}\!\!\to\!\! Y$, ${{G}_{x}}\!\!\to\!\! G_{y}^{n}\leftarrow {{G}_{y}}\!\!\to\! \!G_{y}^{c}\!\!\to\!\! {{Z}_{y}}\!\!\to\!\! Y$, and $G_{x}^{n}\leftarrow {{G}_{x}}\!\!\to\!\! G_{x}^{c}\leftarrow {{G}_{y}}\!\!\to\!\! G_{y}^{c}\!\!\to \!\!{{Z}_{y}}\!\!\to\!\! Y$, where ${{G}^{c}}$ serves as backdoor criterion to enhance the relevance between substructures and prediction targets. Considering the specificity of molecular relationship learning, i.e., the substructure is either $G^c$ or $G^n$. Therefore, we aim to eliminate the impact of ${{G}^{n}}$ on molecular representation to establish a more stable interaction between ${{G}^{c}}$ and $Y$.
% % Appendix B.1 details the causal relationships in Fig. \ref{figcsm}.

% % \begin{figure}[htpb]
% % \centering
% % \includegraphics[width=0.9\columnwidth]{Figure/csm.png} 
% % \caption{The CSM for stable molecular relational learning.}
% % \label{figcsm}
% % \end{figure}
% % % 通过对图2的分析，我们发现在CSM中存在以下后门路径影响了模型对\[{{G}^{co}}\]与\[Y\]之间因果关系的判断，即\[G_{x}^{no}\leftarrow {{G}_{y}}\to G_{y}^{co}\leftarrow {{G}_{x}}\to G_{x}^{co}\to {{Z}_{x}}\to Y\]、${{G}_{y}}\to G_{x}^{no}\leftarrow {{G}_{x}}\to G_{x}^{co}\to {{Z}_{x}}\to Y$、\[{{G}_{x}}\to G_{y}^{no}\leftarrow {{G}_{y}}\to G_{y}^{co}\to {{Z}_{y}}\to Y\]、\[G_{x}^{no}\leftarrow {{G}_{x}}\to G_{x}^{co}\leftarrow {{G}_{y}}\to G_{y}^{co}\to {{Z}_{y}}\to Y\]。其中，\[{{G}^{c}}\]被作为后门标准来增强核心子结构与目标间的关联性。考虑到分子关系学习的特殊性，即\[{{G}^{co}}\]和\[{{G}^{no}}\]是非黑即白的。因此，我们要消除混淆子结构\[{{G}^{no}}\]对分子表征的影响以在因果子结构\[{{G}^{co}}\]和目标$Y$之间构建更加稳定的关联关系。% 附件A1详细介绍了图3中的因果关系
% % % 对模型预测的干扰，使模型利用因果核心子结构\[{{G}^{co}}\]进行关系预测。
% % % 附件A1详细介绍了图3中的因果关系
% % % \subsection{Graph Information Bottleneck}
% % % The Graph Information Bottleneck (GIB) is an extension and development of the Information Bottleneck theory in graph theory. which aims to learn the minimum sufficient subgraph ${{G}_{IB}}$ of the input graph $G$. 

% % % \textbf{Definition 3.1.} (\textit{GIB}) GIB achieves information compression from $G$ to ${{G}_{IB}}$ by maximizing the mutual information between ${{G}_{IB}}$ and the target $Y$ while introducing noise to discard information between ${{G}_{IB}}$ and $G$. ${{G}_{IB}}$  is denoted as:
% % % \begin{equation}
% % % \label{eq:GIB}
% % % {{G}_{IB}}=\underset{{{G}_{IB}}}{\mathop{\arg \min }}\,-I(Y;{{G}_{IB}})+\beta I(G;{{G}_{IB}}),
% % % \end{equation}
% % % where $I(Y;{{G}_{IB}})$ is the mutual information between 
% % % ${{G}_{IB}}$ and $Y$, which is calculated by the integral of the function $p(Y,{{G}_{IB}})\log \frac{p(Y,{{G}_{IB}})}{p(Y)p({{G}_{IB}})}$.
% % % 图信息瓶颈（graph information bottleneck，GIB）是信息瓶颈理论在图论中的扩展和发展，其目的是学习输入图$G$的最小充分子图${{G}_{IB}}$。
% % % Definition 3.1. (graph information bottleneck) GIB通过最大化${{G}_{IB}}$与目标间的互信息，同时引入噪声丢弃${{G}_{IB}}$与$G$之间的信息实现$G$向${{G}_{IB}}$的信息压缩。${{G}_{IB}}$表示为：
% % \subsection{Causal Subgraph Information Bottleneck}
% % We extend the GIB theory to the feature representation of causal substructures, thus deriving the CSIB. 
% % GIB \cite{wu2020GIB} generates the minimal sufficient subgraph ${{G}_{IB}}$ of the graph $G$ by maximizing the mutual information $I(Y;{{G}_{IB}})$ while introducing noise to discard information between ${{G}_{IB}}$ and $G$. Formally, ${{G}_{IB}}={\mathop{\arg \min }}\,-I(Y;{{G}_{IB}})+\beta I(G;{{G}_{IB}})$.
% % Based on the CSM, our research interest is to maximize the causal mutual information $I(Y;G_{x}^{c},G_{y}^{c})$ and minimize the confounding mutual information $I(G^{c},G^{n})$.
% % % 我们将图信息瓶颈理论推广到因果子结构的特征表示，从而得出因果子图信息瓶颈原理。基于4.1节中的CSM，我们的研究兴趣是通过GIB最大化分子关系学习中因果子结构G^co与目标Y之间的互信息，并最小化因果子结构与混淆子结构之间的信息以减少混淆子结构对分子表征的影响。

% % % \begin{equation}
% % % \label{eq:GIB}
% % % {{G}_{IB}}=\underset{{{G}_{IB}}}{\mathop{\arg \min }}\,-I(Y;{{G}_{IB}})+\beta I(G;{{G}_{IB}}).
% % % \end{equation}
% % % where $I(Y;{{G}_{IB}})$ is the mutual information between 
% % % ${{G}_{IB}}$ and $Y$.
% % % 图信息瓶颈的目标是学习输入图$G$的最小充分子图${{G}_{IB}}$。 GIB通过最大化${{G}_{IB}}$与目标间的互信息，同时引入噪声丢弃${{G}_{IB}}$与$G$之间的信息实现$G$向${{G}_{IB}}$的信息压缩。${{G}_{IB}}$表示为：

% % \textbf{Definition 1.} (\textit{\textbf{CSIB}})
% % \textit{Given a pair of graphs and their interaction $({{G}_{x}},Y,{{G}_{y}})$, $(G^{c}_x,G^{c}_y)$ and $(G^{n}_x,G^{n}_y)$ are the causal and confounding subgraph pairs of $({{G}_{x}},{{G}_{y}})$ and $Y$, respectively. Under the CSIB principle, the optimal causal subgraphs and the optimization objective are defined as:}
% % \begin{equation}
% % \label{eq:CSGIB}
% % G_{x}^{c}\!=\!{\mathop{\arg \min }}(\alpha I(G_{x}^{c},G_{x}^{n})\!+\!\beta I(G_{y}^{c},G_{y}^{n})\!-\!I(Y;G_{x}^{c},G_{y}^{c}))
% % \end{equation}
% % % Definition 4.1. （Causal Subgraph Information Bottleneck）给定一对图及其目标$({{G}_{x}},Y,{{G}_{y}})$，$(G^{c}_x,G^{c}_y)$和$(G^{n}_x,G^{n}_y)$分别是$({{G}_{x}},{{G}_{y}})$与$Y$对应的因果子图对和混淆子图对，在CSGIB原理下最优因果子图及优化目标为：

% % The first term $\alpha I(G_{x}^{c},G_{x}^{n})+ \beta I(G_{y}^{c},G_{y}^{n})$ is the confounding component, which reduces the impact of the confounding subgraph ${{G}^{n}}$ on the causal subgraph ${{G}^{c}}$. The second term $I(Y;G_{x}^{c},G_{y}^{c})$ represents the predictive component formed by the causal subgraph pair $(G_{x}^{c},G_{y}^{c})$ and the prediction target $Y$. This term encourages $G_{y}^{c}$ to learn more information about $Y$ to improve the stability of model predictions in distribution change scenarios when paired with the subgraph $G_{x}^{c}$. Therefore, the correlation information between the causal subgraph and the prediction target can be maximally preserved by optimizing Eq. (\ref{eq:CSGIB}).

% % % 第一项\[I(Y;G_{x}^{co},G_{y}^{co})\]为原因子图\[G_{x}^{co}\]、结果子图\[G_{y}^{co}\]和目标\[Y\]共同构成的预测项，它鼓励\[G_{y}^{co}\]在配对子图为\[G_{x}^{co}\]时学习更多关于信息以预测\[Y\]。第二项\[\beta I(G_{x}^{co},G_{x}^{no})+\alpha I(G_{y}^{co},G_{y}^{no})\]为混淆项，他将减少混淆子图${{G}^{no}}$对因果子图${{G}^{co}}$的影响。因此，通过优化公式2可以最大程度保留因果子图与预测目标之间的相关信息。

% % % Model Architecture：


% % % where, $\theta$ is a learnable optimization parameter. $\sigma (*,*)$ is a probability function represented by $softmax(*)$. $Z({{G}^{s}})$ indicates the molecular substructure embedding. We replaces the representation of unimportant substructures with noise, controlling the probability of information from the confounding substructures influencing the causal substructures, thus generating the optimized molecular representation $Z({G}')$. Then, the causal similarity between two molecules ${{G}_{x}}$ and ${{G}_{y}}$ is modeled by the substructure interaction function $\gamma $, defined as: $\gamma (G_{x}^{s_i},G_{y}^{s_j})=\left\| Z(G_{x}^{s_i})-Z(G_{y}^{s_j}) \right\|_{2}^{2} / \left\| Z(G_{x}^{s_i})+Z(G_{y}^{s_j}) \right\|_{2}^{2}$. Based on the interaction matrix $\gamma $ and $Z({G}')$, the core substructures $G^k$ are generated through CSGIB optimization. Finally, we generate the stable molecular embedding representation $Z(G^{st})=\sum \text{MLP}(G^k)$ based on the core substructures to molecular relational learning.

% % % $theta$是一个可学习的优化参数。$\sigma(*,*)$是一个由$softmax(*)$表示的概率函数。$Z({{G}^{c{{o}}}})$表示分子亚结构嵌入。\[R\]将不重要的子结构的嵌入表示替换为噪声，控制混淆子结构对因果子结构的信息概率，生成优化后的分子表示$Z({G}')$。
% % % 然后，两个分子${{G}_{x}}$和${{G}_{y}}$之间的因果相似性通过子结构交互函数$\gamma $建模，\[\gamma (G_{x}^{co},G_{y}^{co})=\frac{\left\| Z(G_{x}^{co})-Z(G_{y}^{co}) \right\|_{2}^{2}}{\left\| Z(G_{x}^{co})+Z(G_{y}^{co}) \right\|_{2}^{2}}\]。基于交互矩阵$\gamma $和$Z({G}')$结合CSGIB优化生成hexin子结构。最后，我们基于因果子结构生成分子的嵌入表示并进行关系学习。

% % \subsection{Dual-Layer Model Optimization}
% % To train ReAlignFit while identifying causal substructures, we use Eq. (\ref{eq:CSGIB}) as the optimization objective. In this section, we derive the upper bounds of 
% % $-I(Y;G_{x}^{c},G_{y}^{c})$ and $I(G^{c},G^{n})$, and construct the loss function $\mathcal{L}$ to ensure the minimization of objection. 
% % % 为了在识别因果子结构的同时训练CSGIB，我们将公式2作为模型的优化目标。在这一节中，我们将给出$-I(Y;G_{x}^{co},G_{y}^{co}$和$I(G^{co},G^{no}$的上界，并构建损失函数$\mathcal{L}$以保证在模型训练过程中最小化$\underset{G_{x}^{co}}{\mathop{\arg \min }}$。

% % \subsubsection{Subgraph Interaction Layer (Minimizing $I(G^{c},G^{n})$)}
% % The goal of the subgraph interaction layer is to extract subgraphs that contain less confounding information. Noise injection is a common approach to minimize $I_{si}\!\!=\!\!I(G^{c},\!G^{n}\!)$ in GIB \cite{tkdeGIBB}. During model training, we inject noise information into $G^{n}$ to reduce the impact of confounding information on $G^{c}$. We combine noise $\varepsilon $ to regenerate the subgraph representation $Z(G_{new}^{n})$:
% % % 子图交互层的目标是提取包含更少混淆信息的子图$G^{co}$。噪声注入是图信息瓶颈中最小化$I(G^{co},G^{no})$的常用方式（参考文献）。我们通过模型训练将噪声信息注入混淆子图$G^{no}$，减少混淆信息对$G^{co}$影响。具体来说，我们结合噪声$\varepsilon $重新生成子图表示$Z(G_{new}^{c})$
% % % \[R\]将不重要的子结构的嵌入表示替换为噪声，控制混淆子结构对因果子结构的信息概率，生成优化后的分子表示$Z({G}')
% % % 	\[Z(G_{new}^{co})={{\lambda }_{i}}Z(G_{{}}^{co})+(1-{{\lambda }_{i}})\varepsilon \]
% % % % 
% % \begin{equation}
% % \label{eq:gen}
% % Z(G_{new}^{n_i})\!\!=\!\!{{\lambda }_{i}}Z(G_{{}}^{n_i})\!+\!(1-{{\lambda }_{i}})\varepsilon, \epsilon \!\!\sim\!\!( {{\mu }_{Z(G_{{}}^{n})}},\sigma _{Z(G_{{}}^{n})}^{2})
% % \end{equation}
% % where ${{\lambda }_{i}}\!\sim\! {\text{Bernoulli}}\left( {\text{Sigmoid}}\left( {{R}_i} \right) \right)$, ${{\mu }_{Z(G_{{}}^{n})}}$ and $\sigma _{Z(G_{{}}^{s})}^{2}$ are mean and variance of $Z(G_{{}}^{c})$, respectively.
% % We select high-contributing substructures as causal substructures based on $\gamma$ in different chemical reactions. Through Eq. (\ref{eq:gen}), the learned probability $\lambda$ can selectively retain the information in $G^{c}$ and freely adjust the impact of $G^{n}$ on $G^{c}$. Inspired by \cite{lee2023cgibICML}, we minimize the upper bound of $I_{si}$ as the conditionally independent distribution between $G^{c}$, $G^{n}$, and the probability function $R$ in Eq. (\ref{eq:nos}). Since $G^{c}$, $G^{n}$ are subgraphs of $G$, according to principles of information theory, we have $I_{si}\le \min (I({{G}^{c}},G),I({{G}^{n}},G))$. The minimization of the upper bound of $I_{si}$ can be expressed as:
% % % 其中，${{\lambda }_{i}}\sim\operatorname{Bernoulli}\left( \operatorname{Sigmoid}\left( {{R}^{c{{o}_{i}}}} \right) \right)$，$\epsilon \simN\left( {{\mu }_{Z(G_{{}}^{co})}},\sigma _{Z(G_{{}}^{co})}^{2} \right)$,${{\mu }_{Z(G_{{}}^{co})}}$ and $\sigma _{Z(G_{{}}^{co})}^{2}$ are mean and variance of \[Z(G_{{}}^{co})\], respectively.我们基于不同化学反应中公式4的聚合结果选择高贡献度子结构作为因果子结构。通过公式（3），学习到的概率$R$可以选择性的保留$G^{co}$中的信息，并且 可以自由的调整$G^{no}$对$G^{co}$的信息影响。结合概率函数及（参考文献韩国）等人的工作，我们将$I(G^{co},G^{no})$的上界最小化为$G^{co}$、$G^{no}$和概率函数$R$之间的条件独立分布。由于$G^{c}$、$G^{n}$ 为$G$的子图，根据信息论相关原理有\[I({{G}^{co}},{{G}^{no}})\le \min (I({{G}^{co}},G),I({{G}^{no}},G))\]M。$I(G^{co},G^{no})$的上界最小化可表示为：
% % \begin{equation}
% % \label{eq:op1}
% % \begin{aligned}
% % I_{si}\!\!&\le\! \min (\!\!\int \!\!\!\!\! \int\!\!\!C\!P\!D(\!{{G}^{c}}\!\!,\!G)d{{G}^{c}}dG, \!\!\!\int \!\!\!\!\! \int\!\!\!C\!P\!D(\!{{G}^{n}}\!\!,\!G)d{{G}^{n}}\!dG)\\
% % &=\min(\mathcal{L}_{KL}^{c},\mathcal{L}_{KL}^{n})
% % \end{aligned}
% % \end{equation}
% % % \vspace{-3mm}
% % \begin{equation}
% % CPD({{G}^{c}},G)=p({{G}^{c}}|G,R)\log (\frac{p({{G}^{c}}|G,R)}{p({{G}^{c}})})
% % \end{equation}

% % Therefore, minimizing $\mathcal{L}_{KL}^{\alpha }\!\!=\!\!\min (\mathcal{L}_{K{{L}_{x}}}^{c},\mathcal{L}_{K{{L}_{x}}}^{n})$ provides an upper bound for the minimization of $I(G_{x}^{c},G_{x}^{n})$. Similarly, the upper bound for minimizing $I(G_{y}^{c},G_{y}^{n})$ can be obtained by minimizing $\mathcal{L}_{KL}^{\beta }\!\!=\!\!\min (\mathcal{L}_{K{{L}_{y}}}^{c},\mathcal{L}_{K{{L}_{y}}}^{n})$. Minimizing $I(G^{c},G^{n})$ helps reduce the impact of confounding substructures, thereby maximizing the promotion of learning the relationship between causal substructures and the prediction targets. For detailed calculations of the upper bound of $I(G^{c},G^{n})$, please refer to Appendix B.3.

% % \textbf{The proof of Eq. (\ref{eq:op1})}
% % % $I_c=I(G^c,G)$表示为积分形式：
% % The $I_c=I(G^c,G)$ is expressed in integral form as:
% % \begin{equation}
% % \label{eq:op1-1}
% % \begin{aligned}
% % I_{c}&=\int p(G^c,G)\log(\frac{p(G^c,G)}{p(G^c)p(G)})dG^cdG
% % % \\
% % % &=-H(G)-H(G^c|G).
% % \end{aligned}
% % \end{equation}
% % % 我们利用公式\ref{eq:nos}中的概率函数$R$调整条件概率分布$p(G^c,G)$，且$G^c$、$G^n$条件独立于$G$。因此，可以得到：

% % The probability function $R_i$ in equation \ref{eq:nos} is utilized to adjust the conditional probability distribution $p(G^c,G)$, and $G^c$, $G^n$ are conditionally independent of $G$. We can obtain:
% % \begin{equation}
% % p(G^c|G)=p(G^c|G,R)
% % \end{equation}
% % % 则公式\ref{eq:op1-1}进一步表示为：

% % Further, $I_{c}$ is expressed as:
% % \begin{equation}
% % \begin{aligned}
% % I_{c}&=\iint p(G^c|G,R) \log(\frac{p(G^c|G,R)}{p(G^c)}) dG^cdG\\
% % &\approx \frac{1}{N}\sum \limits_{i=1}^N D_{KL}(p(G^c|G,R)||p(G^c))\\
% % & = \mathcal{L}_{KL}^c(G^c,G)
% % \end{aligned}
% % \label{eq:op1-2}
% % \end{equation}

% % The result of Eq. (\ref{eq:op1-2}) indicates that $I_{c}$ can be computed by the KL divergence between $G^c$ and $G$. Similarly, $I_{n}=I(G^n,G)=\mathcal{L}_{KL}^n(G^n,G)$.
% % % $I_{c}$可用$G^c$和$G$之间的KL散度计算得到。同理我们可以得到$I_{n}=I(G^n,G)=\mathcal{L}_{KL}^n(G^n,G)$.

% % Finally, we have the following equation as:
% % \begin{equation}
% %     I(G^c,G)\le \min (\mathcal{L}_{KL}^c(G^c,G),\mathcal{L}_{KL}^n(G^n,G))
% % \end{equation}

% % % 因此，最小化\[\mathcal{L}_{KL}^{\beta }=\min (\mathcal{L}_{K{{L}_{x}}}^{co},\mathcal{L}_{K{{L}_{x}}}^{no})\]可以得到的$I(G_{x}^{co},G_{x}^{no})$的最小化上界。同理，可通过最小化\[\mathcal{L}_{KL}^{\alpha }=\min (\mathcal{L}_{K{{L}_{y}}}^{co},\mathcal{L}_{K{{L}_{y}}}^{no})\]得到的最小化上界$I(G_{y}^{co},G_{y}^{no})$。最小化$I(G^{co},G^{no})$有助于降低混淆子结构对模型学习的影响，最大化促进模型学习因果子结构与目标间的关系。有关$I(G^{co},G^{no})$上界转化的详细计算请参阅附录A1.2.。


% % \subsubsection{Causal Layer (Minimizing $-I(Y;G_{x}^{c},G_{y}^{c})$)}
% % The goal of the causal layer is to capture the stable representation of causal substructures that determine molecular relationship learning. We maximize $I_{ca}\!\!=\!\!I(Y;G_{x}^{c},G_{y}^{c})$ through the causal similarity score $\gamma $ of the substructures, the causal substructures $G_{x}^{c}$, $G_{y}^{c}$, and the prediction target $Y$:
% % \begin{equation}
% % \label{eq:op2}
% % \begin{aligned}
% % I_{ca}\!&\ge \int{p(Y)(\iint{PPD(G_{x}^{c},G_{y}^{c}|Y)}dG_{x}^{c}dG_{y}^{c})dY}\\
% % & \approx \frac{1}{NM}\sum\limits_{n=1}^{N}{\sum\limits_{m=1}^{M}{q(\gamma (G_{{{x}_{n}}}^{c},G_{{{y}_{m}}}^{c})|{{Y}_{nm}})}}\\
% % & =-{{\mathcal{L}}_{pred}}(q(\gamma (G_{{{x}_{n}}}^{c},G_{{{y}_{m}}}^{c})|{{Y}_{nm}})|\hat{Y})
% % \end{aligned}
% % \end{equation}
% % \begin{equation}
% % PPD(G_{x}^{c},\!G_{y}^{c}|Y)\!=\!q(G_{x}^{c},\!G_{y}^{c}|Y)\!\log(\!\frac{p(G_{x}^{c},\!G_{y}^{c}|Y\!)}{q(G_{x}^{c},\!G_{y}^{c})})
% % \end{equation}
% % % 4.4.2 因果层（最小化$-I(Y;G_{x}^{co},G_{y}^{co})$）
% % % 因果层的目标是捕获决定分子关系学习的因果子结构。我们通过子结构因果相似性得分$\gamma $、因果子结构$G_{x}^{co}$和$G_{y}^{co}$、预测目标$Y$来最大化$I(Y;G_{x}^{co},G_{y}^{co})$：
% % % 公式1
% % where $q(G_{x}^{c},G_{y}^{c}|Y)$ is the variational approximation distribution used to approximate the posterior distribution $p(G_{x}^{c},G_{y}^{c}|Y)$. The Eq. (\ref{eq:op2}) indicates that minimizing the prediction loss ${{\mathcal{L}}_{pred}}$ achieves the minimization of $-I(Y;G_{x}^{c},G_{y}^{c})$. In molecular relationship learning, ${{\mathcal{L}}_{pred}}$ can be chosen as the cross-entropy loss (for classification tasks) or the mean absolute error loss (for regression tasks), depending on the nature of the task. Maximizing the mutual information between the causal similarity score $\gamma $ of the substructures and the prediction targets $Y$ helps the model identify the causal relationship between $G^{c}$ and $Y$ in the CSM backdoor paths. For detailed calculations on the lower bound of $I(Y;G_{x}^{c},G_{y}^{c})$, please refer to Appendix B.4.

% % % 其中，$H(Y)$为$Y$的熵，是固定值。$q(G_{x}^{co},G_{y}^{co}|Y)$是变分近似分布，可用于近似后验分布$p(G_{x}^{co},G_{y}^{co}|Y)$。
% % % 公式1表明可通过最小化预测损失${{\mathcal{L}}_{pred}}$实现$-I(Y;G_{x}^{co},G_{y}^{co})$的最小化。在分子关系学习中，${{\mathcal{L}}_{pred}}$可根据任务性质选择交叉熵损失（分类任务）或平均绝对误差损失（回归任务）。子结构因果相似性信息$\gamma $与目标$Y$间互信息的最大化有助于模型识别CSM后门路径中$G^{co}$与$Y$的因果关系。有关$I(Y;G_{x}^{co},G_{y}^{co})$下界转化的详细计算请参阅附录A1.3

% % \textbf{The proof of Eq. (\ref{eq:op2})}
% % For the term $I_{ca}=I(Y;G_{x}^{c},G_{y}^{c})$, by definition:
% % \begin{equation}
% %     I_{ca} \!\!=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\!\! p(Y,G_{x}^{c},G_{y}^{c})\! \log (\frac{p(Y,G_{x}^{c},G_{y}^{c})}{p(Y)})dY\!dG_{x}^{c}dG_{y}^{c}
% % \end{equation}

% % By introducing the variational approximation distribution $q(G_{x}^{c},G_{y}^{c}|Y)$ to approximate the conditional probability distribution $p(G_{x}^{c},G_{y}^{c}|Y)$, we have the following equation:
% % % 引入变分近似分布A来近似条件概率分布B，我们可以得出以下等式：
% % \begin{equation}
% % \begin{aligned}
% %     I_{ca}\!\!&
% %     = \!\!\!\!\int\!\!p(Y)\!\!\int\!\!\!\!\! \int\!\! q(G_{x}^{c},G_{y}^{c}|Y)\log (\frac{p(Y,G_{x}^{c},G_{y}^{c})}{q(G_{x}^{c},G_{y}^{c})})dY\!dG_{x}^{c}dG_{y}^{c}\\
% %     &=\!\!\!\!\int\!\!p(Y)\!\!\!\int\!\!\!\!\! \int\!\! q(G_{x}^{c},\!G_{y}^{c}|Y\!)\!\log (\!\frac{p(Y)p(G_{x}^{c},\!G_{y}^{c}|Y)}{q(G_{x}^{c},\!G_{y}^{c})}\!)dY\!dG_{x}^{c}dG_{y}^{c}\\
% %     &=\!\!\!\!\int\!\!p(Y)(\log p(Y)+\!\!\!\int\!\!\!\!\! \int\!\!PPD(G_{x}^{c},\!G_{y}^{c}|Y)dG_{x}^{c}dG_{y}^{c})dY\\
% %     &=\!\!\!\!\int\!\!p(Y)\log p(Y)dY\\
% %     &+\!\!\!\!\int\!\!p(Y)\!\!\!\int\!\!\!\!\! \int\!\!PPD(G_{x}^{c},G_{y}^{c}|Y)dG_{x}^{c}dG_{y}^{c})dY\\
% %     &=\!H(Y)\!\!+\!\!\!\int\!\!{p(Y)\!\!\iint\!\!{PPD(G_{x}^{c},G_{y}^{c}|Y)}dG_{x}^{c}dG_{y}^{c}dY}
% % \end{aligned}
% % \end{equation}

% % $H(Y)$ is the entropy of $Y$. Finally, we have the following equation as:
% % \begin{equation}
% % I(Y;\!G_{x}^{c},\!G_{y}^{c})\!\ge\!\!\! \int\!\!{\!p(Y)\!\!\!\int\!\!\!\!\!\int\!\!\!{P\!P\!D(G_{x}^{c},\!G_{y}^{c}|Y\!)}dG_{x}^{c}dG_{y}^{c}dY}
% % \end{equation}

% % \subsubsection{Model Loss Optimization}
% % We define the model training objective as the sum of ${\mathcal{L}}_{pred}$ and $\mathcal{L}_{KL}$ as follows:
% % \begin{equation}
% %    \mathcal{L}= {\mathcal{L}}_{pred}+\alpha \mathcal{L}_{KL}^{\alpha }+\beta \mathcal{L}_{KL}^{\beta }
% % \end{equation}
% % where $\alpha$ and $\beta$ are hyperparameters that adjust the weights of the confounding loss, consistent with the settings in Eq.~(\ref{eq:CSGIB}). $\mathcal{L}_{KL}$ controls the impact of confounding substructures on the causal substructure representation. ${\mathcal{L}}_{pred}$ is computed by the Mean Absolute Error Loss for regression tasks and the Cross-Entropy Loss for classification tasks.

% % 基于图信息瓶颈的核心子结构学习：

% % \section{Theoretical Analysis}



% \section{Experiments}
% In this section, we validate the prediction performance of ReAlignFit in two types of tasks on nine datasets. Meanwhile, we evaluate the model's interpretability by fidelity, contrastivity, and sparsity metrics. Guided by the idea of differential testing, we provide further illustration of the model's interpretability. By analyzing and summarizing the relevant experiments, we aim to answer the following research questions: 
% % 在本节中，我们在九个数据集的两类任务中验证了ReAlignFit的预测性能。同时我们采用fidelity、contrastivity和sparsity评估了模型的可解释性。在差分测试思想的指引下，我们为模型的可解释性提供了更进一步的说明。通过对相关实验的分析和总结，我们将回答以下研究问题：
% \begin{itemize}
% \item \textbf{RQ1:} How does the performance of ReAlignFit in MRL, and whether ReAlignFit is susceptible to backbone?
% \item \textbf{RQ2:} How does the interpretability of ReAlignFit compare to comparative models, and can this interpretability be validated and supported?
% \item \textbf{RQ3:} Can ReAlignFit improve the stability of MRL in distribution-shifted data?
% \item \textbf{RQ4:} How does confounding information affect the performance of ReAlignFit, and what are the keys to ReAlignFit performance improvement?
% \item \textbf{RQ5:} Can ReAlignFit's mining of interpretable core substructures be supported by visual visualization?
%     % \item \textbf{RQ1}: How does the performance of ReAlignFit in MRL, and whether ReAlignFit is susceptible to backbone.
%     % \item \textbf{RQ2}: Can ReAlignFit improve the stability of MRL in OOD dataset.
%     % \item \textbf{RQ3}: How does confounding information affect the performance of ReAlignFit.
%     % \item \textbf{RQ4}: What is the key to ReAlignFit's performance improvement.
% \end{itemize}
% % ReAlignFit在MRL中的性能如何，ReAlignFit是否易受骨干网的影响。
% % 与比较模型相比，ReAlignFit 的可解释性如何，这种可解释性能否得到验证和支持？
% % ReAlignFit 能否提高 MRL 在分布偏移数据中的稳定性？
% % 混杂信息如何影响 ReAlignFit 的性能，提高 ReAlignFit 性能的关键是什么？
% % ReAlignFit 对可解释核心子结构的挖掘能否得到可视化的支持？



% \begin{table*}[t]
% \small
%     \centering
%     \renewcommand{\arraystretch}{1.1}
%     \caption{The performance of ReAlignFit and comparative models in MI prediction on In-Distribution Data (Original), with the best results highlighted in \colorbox{c1!50}{\textbf{bold}} and the second results highlighted in \underline{underline}. The results of comparison methods are from \cite{lee2023cgibICML,lee2023shiftKDD,dummgnn}.}
%     \setlength{\tabcolsep}{1.3mm}{
%     \begin{tabular}{lcccccccc}
%     \hline
%         \multirow{2}{*}{Model} & \multicolumn{3}{c}{Chromophore} & \multirow{2}{*}{MNSol} & \multirow{2}{*}{FreeSolv} & \multirow{2}{*}{CompSol} & \multirow{2}{*}{Abraham} & \multirow{2}{*}{CombiSolv}\\
%         \cline{2-4}
%          & Absorption & Emission & Lifetime &  &  &  &  &  \\
%           \hline
%           \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
%           \hline
%         GCN (ICLR'17) & 25.75$\pm$1.48 & 31.87$\pm$1.70 & 0.866$\pm$0.015 & 0.675$\pm$0.021 & 1.192$\pm$0.042 & 0.389$\pm$0.009 & 0.738$\pm$0.041  & 0.672$\pm$0.022\\
%         GAT (ICLR'18) & 26.19$\pm$1.44 & 30.90$\pm$1.01 & 0.859$\pm$0.016 & 0.731$\pm$0.007 & 1.280$\pm$0.049 & 0.387$\pm$0.010 & 0.798$\pm$0.038 & 0.662$\pm$0.021\\
%         MPNN (ICML'17) & 24.43$\pm$1.55 & 30.17$\pm$0.99 & 0.802$\pm$0.024 & 0.682$\pm$0.017 & 1.159$\pm$0.032 & 0.359$\pm$0.011  & 0.601$\pm$0.035 & 0.568$\pm$0.005\\
%        GIN (ICLR'19) & 24.92$\pm$1.67 & 32.31$\pm$0.26 & 0.829$\pm$0.027 & 0.669$\pm$0.017 & 1.015$\pm$0.041 & 0.331$\pm$0.016 & 0.648$\pm$0.024 & 0.595$\pm$0.014\\
%        CIGIN (AAAI'20) & 19.32$\pm$0.35 & 25.09$\pm$0.32 & 0.804$\pm$0.010 & 0.607$\pm$0.024 & 0.905$\pm$0.014 & 0.308$\pm$0.018 & 0.411$\pm$0.008 & 0.451$\pm$0.009\\
%         \hline
%         \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
%         \hline
%        CMRL (KDD'23) & 17.93$\pm$0.31 & 24.30$\pm$0.22 & 0.776$\pm$0.007 & 0.551$\pm$0.017 & 0.815$\pm$0.046 & \cellcolor{c1!50}\textbf{0.255$\pm$0.011} & \underline{0.374$\pm$0.011} & 0.421$\pm$0.008\\
%        CGIB (ICML'23) & 18.11$\pm$0.20 & 23.90$\pm$0.35 & \underline{0.771$\pm$0.005} & \cellcolor{c1!50}\textbf{0.538$\pm$0.007} & 0.852$\pm$0.022 & 0.276$\pm$0.017 & 0.390$\pm$0.006 & 0.422$\pm$0.005\\
%        MMGNN (IJCAI'24)& 18.01$\pm$0.25 & 23.87$\pm$0.31 & 0.773$\pm$0.007 & 0.547$\pm$0.012 & 0.902$\pm$0.026 & 0.267$\pm$0.012 & 0.385$\pm$0.008 & \cellcolor{c1!50}\textbf{0.303$\pm$0.033} \\
%         \hline
%         ReAlignFit & \cellcolor{c1!50}\textbf{16.82$\pm$0.25} & \cellcolor{c1!50}\textbf{22.95$\pm$0.33} & \cellcolor{c1!50}\textbf{0.769$\pm$0.005} & 0.541$\pm$0.010 & \underline{0.799$\pm$0.034} & 0.261$\pm$0.013 & \cellcolor{c1!50}\textbf{0.371$\pm$0.008} & 0.419$\pm$0.008\\
%         ReAlignFit$_\text{GCN}$ & \underline{17.23$\pm$0.27} & \underline{23.35$\pm$0.29} & 0.771$\pm$0.007 & \underline{0.539$\pm$0.012} & \cellcolor{c1!50}\textbf{0.796$\pm$0.035} & \underline{0.257$\pm$0.016} & 0.375$\pm$0.012 & \underline{0.416$\pm$0.006}\\
%         ReAlignFit$_\text{GAT}$ & 17.55$\pm$0.23 & 23.98$\pm$0.36 & 0.776$\pm$0.007 & 0.543$\pm$0.021 & 0.806$\pm$0.036 & \cellcolor{c1!50}\textbf{0.255$\pm$0.011} & 0.381$\pm$0.013 & 0.421$\pm$0.009\\
%         ReAlignFit$_\text{GIN}$ & 17.92$\pm$0.46 & 24.10$\pm$0.34 & 0.772$\pm$0.007 & 0.552$\pm$0.037 & 0.803$\pm$0.025 & 0.267$\pm$0.021 & 0.386$\pm$0.017 & 0.418$\pm$0.008\\
%         \hline
%     \end{tabular}
%     }
    
%     \label{tab:mi}
% \end{table*}

% \subsection{Datasets}
% Following the related research \cite{lee2023cgibICML,lee2023shiftKDD,boulougouri2024molecularMNI,dummgnn}, we conduct extensive Molecular Interaction (MI) prediction and Drug-Drug Interaction (DDI) prediction experiments on nine datasets, as detailed in Table \ref{dataset}. Chromophore \cite{chromophore}, MNSol \cite{mnsol}, FreeSolv \cite{freesolv}, CompSol \cite{comsol}, Abraham \cite{abraham}, and CombiSolv \cite{combisolv} are chromophore datasets used to describe the free energy of solutes in solvents. These five datasets are widely used for Molecular Interaction (MI) prediction \cite{lee2023cgibICML,lee2023shiftKDD,dummgnn}. ZhangDDI \cite{zhangddi}, HetionetDDI \cite{hddi}, and DrugBankDDI (pay version) \cite{drugbankddi} are commonly utilized for Drug-Drug Interaction (DDI) prediction \cite{zhang2024heterogeneousIJCAI}. Following the setup of related work in MRL \cite{lee2023cgibICML,lee2023shiftKDD}, we divide the datasets into training, validation, and test sets with the ratio of 6:2:2. For DDI datasets that contain only positive examples, we generated negative samples using rule matching and scaffold clustering methods, respectively. To better simulate the real-world data distribution, we set up in-distribution data (Original) with the same distribution as the original data and two different distribution-shifted data (P1 and P2) for DDI prediction, which were used for ReAlignFit learning.
% \begin{itemize}
%     \item \textbf{In-Distribution Data (Original)}: The original data generated negative samples based on rule matching and is randomly divided into training, validation, and test sets.
%     \item \textbf{Distribution-Shifted Data (P1)}: We generated negative samples for original data based on rule matching and partitioned them by ID to ensure that at least one drug does't repeat in training, validation, and test sets. P1 simulates application scenarios of discovering unknown interactions in existing molecules, such as drug repurposing.
%     \item \textbf{Distribution-Shifted Data (P2)}: We generated negative samples for the original data based on scaffold clustering. We used METIS \cite{karypis1998multilevelMETIS} to iteratively partition the drug interaction graph, ensuring that molecules in training, validation, and test sets are entirely distinct. P2 simulates scenarios of discovering interactions between previously unknown molecules, such as drug discovery.
%     % The original data is not processed in any way and is randomly divided into training, validation, and test sets.
%     % \item \textbf{Distribution-Shifted Data (P1)}: We partition the original data based on IDs and ensure that at least one drug is not repeated in the training, validation, and test sets. P1 simulates application scenarios of discovering unknown interactions in existing molecules, such as drug repurposing.
%     % % 基于规则的数据划分（P1）:我们基于药物ID对原始数据进行划分，并保证训练集、验证集和测试集中至少有一种药物不重复。P1用于模拟挖掘现有分子中未知作用关系的应用场景，如药物重定位等。
%     % \item \textbf{Distribution-Shifted Data (P2)}: We iteratively cut the drug interaction graph using the METIS \cite{karypis1998multilevelMETIS}, ensuring that the molecules in the training, validation, and test sets are entirely different. P2 simulates scenarios of discovering interactions between previously unknown molecules, such as drug discovery.
%     % % 基于图分割的数据划分（P2）:我们基于图分割中最小化切边的思想，借助MRTIS算法按比例迭代切割药物关系图，并保证训练集、验证集和测试集中的药物分子均不相同。P2用于模拟未知分子间作用关系的应用场景，如新材料设计等。
% \end{itemize}


% % \begin{itemize}
% %     \item \textbf{Rule-based Negative Sample Generation (N1)}: We randomly select a drug from the drug molecule set that does not interact with the target molecule, based on drug IDs, and create the interaction as the negative sample.
% %     \item \textbf{Scaffold-based Negative Sample Generation (N2)}: We categorize drug molecules by scaffold groups and select a drug from the same scaffold category that does not interact with the target molecule as the negative sample.
% % \end{itemize}




% % \footnote{ The code of ReAlignFit is available at \ref https://github.com/papercodeforreview/codeforAAAI2025}

% % To better simulate the real world data distribution, we set up three different data distributions in DDI prediction and use it for ReAlignFit learning. 
% % \begin{itemize}
% %     \item \textbf{Original Data Partitioning (Original)}: The original data is not processed in any way and is randomly divided into training, validation, and test sets.
% %     \item \textbf{Rule-based Data Partitioning (P1)}: We partition the original data based on IDs and ensure that at least one drug is not repeated in the training, validation, and test sets. P1 simulates application scenarios of discovering unknown interactions in existing molecules, such as drug repurposing.
% %     % 基于规则的数据划分（P1）:我们基于药物ID对原始数据进行划分，并保证训练集、验证集和测试集中至少有一种药物不重复。P1用于模拟挖掘现有分子中未知作用关系的应用场景，如药物重定位等。
% %     \item \textbf{Graph-based Data Partitioning (P2)}: We iteratively cut the drug interaction graph using the METIS \cite{karypis1998multilevelMETIS}, ensuring that the molecules in the training, validation, and test sets are entirely different. P2 simulates scenarios of discovering interactions between previously unknown molecules, such as drug discovery.
% %     % 基于图分割的数据划分（P2）:我们基于图分割中最小化切边的思想，借助MRTIS算法按比例迭代切割药物关系图，并保证训练集、验证集和测试集中的药物分子均不相同。P2用于模拟未知分子间作用关系的应用场景，如新材料设计等。
% % \end{itemize}



% % 我们在9个数据集中评估了ReAlignFit的性能，详细如表1所示。其中，Chromophore (Joung et al.， 2020)、MNSol (Marenich et al.， 2020)、FreeSolv(Mobley & Guthrie, 2014)、CompSol (Moine et al.， 2017)、Abraham (Grubbs et al.， 2010)和CombiSolv (Vermeire & Green, 2021)为色团数据集，用来描述溶质在溶剂中的自由能。这五个数据集被广泛的用于分子相互作用预测（Molecular Interaction Prediction）。ZhangDDI(Zhang et al.， 2017)、HetionteDDI和DrugBankDDI为药物-药物相互作用数据集，通常用于DDI预测。针对DDI数据集中仅包含正样例的情况，我们基于规则和基于药物分子脚手架两种方式生成了负样例。
% % 基于规则的负样例生成：我们基于药物ID从药物分子集合中随机选取一种与目标分子不存在相互作用的药物构成作用关系并作为负样例。
% % 基于脚手架的负样例生成：我们首先生成药物分子的脚手架类别，然后从药物分子集合中选取一种与目标分子属于相同脚手架类别但不存在相互作用的药物构成负样例。

% \subsection{Experimental Setup and Evaluation}


% \subsubsection{Evaluation Metrics}
% The Root Mean Square Error (RMSE) and  Mean Absolute Error (MAE) are commonly employed in molecular interaction prediction models, and the two evaluation metrics exhibit a high positive correlation \cite{lee2023cgibICML,lee2023shiftKDD,dummgnn}. Due to space constraints, we use RMSE for performance evaluation in molecular interaction prediction and evaluate the performance of DDI prediction tasks in terms of  AUROC, ACC, F1, Precision (Pre), and AUPR. Following \cite{chaudhuri2024transitivity,pope2019explainability}, we use Fidelity, Contrastivity, and Sparsity as evaluation metrics for ReAlignFit. Additionally, we innovatively introduce the concept of differential testing to further validate the interpretability of ReAlignFit.
% % 均方根误差（RMSE）和平均绝对误差（MAE）是分子相互作用预测模型中常用的指标，这两个评价指标呈现出高度的正相关性。由于篇幅限制，我们在分子相互作用预测中使用 RMSE 进行性能评估。我们根据AUROC、ACC、F1、Pre、AUPR评估药物-药物相互作用预测任务的性能。(Wang et al.， 2021)
% % 参照ABC等工作，我们采用fidelity, contrastivity, and sparsity作为ReAlignFit的评价指标。同时，我们创新性的引入差分测试思想进一步验证ReAlignFit的可解释性。

% \subsubsection{Training Details}
% In all experiments, we employ a three-layer MPNN as the encoder to extract molecular feature representations and train ReAlignFit based on two NVIDIA GeForce RTX 4090 24G GPUs. We optimize the parameters using the Adam optimizer and train the model for 50 epochs. The batch size is 128, and the dropout rate is 0.1. For other hyperparameters, we choose from a specific range: learning rate $lr \in \{0.01,0.005,0.001,0.0005,0.0001\}$, both $\alpha$ and $\beta$ are selected from $\{0.5,0.3,0.1,0.01,0.001\}$, and the iterations is searched from $\{1,3,5,10,15\}$.
% % 在所有实验中，我们采用三层MPNN作为编码器提取分子特征表示并使用两块NVIDIA GeForce RTX 4090 24G计算显卡训练了ReAlignFit。我们采用Adam优化器进行了参数优化并对模型进行了50次训练。模型批量大小为 128，丢包率为 0.1。对于其他超参数，我们从一定的范围进行选择：学习率$lr \in \{0.01,0.005,0.001,\textbf{0.0005},0.0001\}$,$a$和$b$均从${1e−1,1e−2,1e−3,1e−4}$中选择，The iterations are searched from $\{1,2,3,5,\textbf{10},15,20\}$。

% % \subsubsection{Comparison Methods}
% % We compare the performance of ReAlignFit with four backbone models (GCN \cite{gcn}, GAT \cite{gat}, MPNN \cite{mpnn}, and GIN \cite{gin}) and seven state-of-the-art models (CIGIN \cite{cigin}, SSI-DDI \cite{ssiddi}, MIRACLE \cite{miracle}, SA-DDI \cite{saddi}, DSN-DDI \cite{dsn}, CMRL \cite{lee2023shiftKDD}, and CGIB \cite{lee2023cgibICML}) in MI and DDI tasks. 
% % 对于不同的任务，我们将ReAlignFit同时与backbone和最先进的方法进行了比较。我们将ReAlignFit与4个backbone模型进行了对比，即GCN (Kipf & Welling, 2016)、GAT (Veliˇckovíc et al.， 2017)、MPNN (Gilmer et al.， 2017)和GIN (Xu et al.， 2018)。对于MI任务，我们重点比较了CIGIN、CMRL和CGIB与ReAlignFit之间的性能差异。对于DDI任务，我们选取了3个DDI领域最先进的模型（SSI-DDI、MIRACLE、SA-DDI和DSN-DDI）和三个通用领域模型（CIGIN、CMRL和CGIB）与ReAlignFit进行性能对比。
% % \begin{itemize}
% %     \item GCN (ICLR'17): GCN aggregates information from neighboring nodes by convolution operations and is widely used for graph representation learning.
% %     \item GAT (ICLR'18): GAT dynamically captures node importance through attention mechanisms, often applied in heterogeneous graph learning.
% %     \item MPNN (ICML'17): MPNN propagates features and aggregates information via message passing, which is extensively used in molecular science.
% %     \item GIN (ICLR'19): GIN employs specific aggregation functions to capture homogeneous graph information, primarily for graph classification.
% %     \item CIGIN (AAAI'20): CIGIN enhances relationship capture between molecules by contextual interactions.
% %     \item SSI-DDI (BIB'21): SSI-DDI generates substructure representations with cooperative attention, treating substructures as separate entities for DDI prediction.
% %     \item MIRACLE (WWW'21): MIRACLE improves message passing and molecular reaction prediction accuracy in GNNs with self-supervision.
% %     \item SA-DDI (CS'22): SA-DDI uses an attention mechanism to assign weights to atoms to generate molecular substructures for DDI prediction.
% %     \item DSN-DDI (BIB'23): DSN-DDI leverages multi-view learning to capture substructures from both individual drugs and drug pairs for DDI prediction.
% %     \item CMRL (KDD'23): CMRL integrates causal reasoning to identify core substructures in chemical reactions for molecular relational learning.
% %     \item CGIB (ICML'23): CGIB uses conditional information to identify core substructures and improve chemical reaction prediction performance.
% % \end{itemize}
% % % GCN利用卷积操作聚合邻居节点，被广泛用于图的表示学习。GAT利用注意力机制动态的捕获图中节点的权重，多用于异构图学习。MPNN通过消息传递机制进行特征传播和信息聚合，在分子科学中应用广泛。GIN采用特定的聚合函数捕获同构图的信息，主要用于图分类。
% % % SA-DDI采用注意力机制为原子分配不同权重以生成分子子结构，专注于DDI预测。SSI-DDI利用协同注意力生成子结构表示并将子结构视为单独的实体用于DDI预测。DSN-DDI借助于多视角同时从单一药物和药物对的视角学习要子结构并预测DDI。
% % % CIGIN利用上下文交互机制来更准确的捕获分子间的相互作用。MIRACLE在自监督机制的辅助下提升了GNN的消息传递和分子反应预测的精度。CMRL结合因果推理来识别化学反应中的核心子结构并用于分子关系学习。CGIB借助条件信息识别分子的核心子结构并提升化学反应预测的性能。

% % \subsubsection{C.5. Code Available}
% % The implemented code of ReAlignFit  available online at: \url{https://github.com/papercodeforreview/codeforAAAI2025}.
% % % \footnote{ The code of ReAlignFit is available at \ref https://github.com/papercodeforreview/codeforAAAI2025}





% % we conduct extensive experiments three different data distribution scenarios to answer the following questions: 

% % The main research questions are as follows: 

% % Following the related research \citep{lee2023cgibICML,lee2023shiftKDD,cigin}, we use MI Prediction and DDI Prediction as specific tasks to evaluate the performance of ReAlignFit and others methods. To better simulate the differences in real world data distribution, we generat DDI datasets with different distributions in the following two ways and use it for ReAlignFit learning\footnote{The code}. More detailed information on datasets, experimental settings, evaluation metrics, and comparison methods is provided in Appendix C.
% % 在本节中，我们在三种不同数据分布情况下进行了广泛的实验来回答以下问题：RQ1: CSGIB及其他对比模型的MRL性能及CSGIB是否容易受到backbone的影响。RQ2:CSGIB能否提高分布外数据中的MRL稳定性？RQ3:混淆信息如何影响CSGIB的性能？RQ4:哪些组件有助于CSGIB的性能提升？RQ5:CSGIB捕获的因果子结构能否可视化验证？附录B中给出了详细的数据集、实验设置、评价指标和对比方法信息。



% % To better simulate the differences in real-world data distributions, we generate datasets with different distributions based on the following two methods:
% % 分子相互作用预测和药物-药物相互作用预测是分子关系学习中重要的下游任务。因此，在本文中我们将采用MI和DDI作为评测ReAlignFit及对比方法性能的具体任务。参考MRL的相关工作，我们将数据集按照6:2:2的比例划分为训练集、验证集和测试集。为了更好的模拟真实世界中数据分布的差异，我们基于以下两种方式生成不同数据分布的数据集。


% \begin{table*}[htpb]
% \small
%     \centering
%      \caption{The performance of ReAlignFit and comparative methods in DDI prediction on In-Distribution Data (Original), with the best results highlighted in \colorbox{c1!50}{\textbf{bold}} and the second results highlighted in \underline{underline}. The results of comparison methods in ZhangDDI are from \cite{lee2023cgibICML,lee2023shiftKDD,dummgnn}.}
%     \renewcommand{\arraystretch}{1.15}
%     \setlength{\tabcolsep}{0.85mm}{
%     \begin{tabular}{lccccc|ccccc|ccccc}
%     \hline
%         \multirow{2}{*}{Model} & \multicolumn{5}{c|}{ZhangDDI} & \multicolumn{5}{c|}{HetionteDDI} & \multicolumn{5}{c}{DrugBankDDI}\\
%         \cline{2-16}
%          & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR  \\ \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
%           \hline
%         GCN(ICLR'17) & 83.31 & 91.64 & 82.91 & 77.67 & 91.33 & 87.62 & 92.91 & 90.07 & 84.66 & 91.26 & 84.02 & 90.96 & 82.87 & 80.04 & 88.36  \\
%         GAT(ICLR'18) & 84.14 & 92.10 & 83.42 & 78.45 & 91.21 & 87.94 & 94.35 & 89.78 & 83.52 & 92.83 & 84.33 & 91.26 & 83.34 & 80.58 & 89.13  \\
%         MPNN(ICML'17) & 84.56 & 92.34 & 83.32 & 78.15 & 91.46 & 88.73 & 95.13 & 89.52 & 83.66 & 93.21 & 87.30 & 95.90 & 88.53 & 80.73 & 95.08  \\
%         GIN(ICLR'19) & 85.59 & 93.16 & 85.07 & 79.82 & 92.11 & 87.37 & 93.01 & 87.91 & 84.05 & 91.23 & 85.21 & 92.58 & 85.88 & 83.57 & 90.92  \\
%         MIRACLE(WWW'21) & 84.90 & 93.05 & 83.94 & 77.81 & 91.25 & 87.01 & 92.88 & 87.34 & 83.91 & 90.88 & 84.98 & 92.17 & 85.43 & 83.22 & 90.28  \\
%         CIGIN(AAAI'20) & 85.54 & 93.28 & 84.36 & 80.23 & 91.89 & 90.87 & 92.77 & 90.33 & 88.98 & 94.62 & 91.02 & 93.62 & 91.18 & 89.35 & 94.88  \\
%         \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
%           \hline
%         SSI-DDI(BIB'21) & 85.35 & 93.14 & 81.96 & 78.97 & 92.09 & 87.95 & 93.83 & 88.56 & 84.28 & 91.60 & 84.16 & 91.23 & 84.64 & 82.13 & 89.47  \\
%         DSN-DDI(BIB'23) & 86.65 & 91.13 & 87.86 & 84.33 & 86.42 & 91.25 & \underline{95.31} & 91.59 & 89.52 & 94.66 & 90.49 & 96.15 & 90.58 & 89.76 & 95.27  \\
%         SA-DDI(CS'22) & 77.31 & 50.26 & 45.06 & 52.54 & 29.97 & 91.00 & 94.89 & 91.26 & 88.39 & 94.96 & 92.60 & 95.33 & 92.80 & 90.38 & 96.62  \\
%         CMRL(KDD'23) & 86.32 & 93.73 & 87.68 & 84.01 & 91.56 & 91.25 & 93.18 & 91.53 & 91.23 & 95.24 & 92.26 & 95.05 & 92.43 & 90.30 & 95.96  \\
%         CGIB(ICML'23) & 86.36 & 93.78 & 87.24 & 83.91 & 91.88 & 91.08 & 93.26 & 91.35 & 91.55 & 94.89 & 92.37 & 94.98 & 92.03 & 90.65 & 96.11  \\ 
%         MMGNN (IJCAI'24) & 87.65 & 94.01 & 89.67 & 84.68 & 92.55 & 91.15 & 92.98 & 91.34 & 92.34 & \underline{95.58} & 93.23 & 94.76 & 92.36 & 91.23 & 96.33 \\
%         \hline
%         ReAlignFit & \cellcolor{c1!50}\textbf{89.43} & \underline{95.68} & \cellcolor{c1!50}\textbf{90.88} & \cellcolor{c1!50}\textbf{87.68} & \cellcolor{c1!50}\textbf{93.36} & \cellcolor{c1!50}\textbf{92.39} & \cellcolor{c1!50}\textbf{97.17} & \cellcolor{c1!50}\textbf{92.62} & \cellcolor{c1!50}\textbf{95.48} & \cellcolor{c1!50}\textbf{96.05} & \underline{95.53} & 96.31 & 94.59 & \cellcolor{c1!50}\textbf{94.38} & \underline{97.05} \\ 
%         ReAlignFit$_\text{GCN}$ & 87.62 & \cellcolor{c1!50}\textbf{95.69} & 90.34 & \underline{87.54} & \underline{92.81} & 91.16 & 94.22 & 91.55 & 92.29 & 95.21 & \cellcolor{c1!50}\textbf{95.62} & 97.95 & \cellcolor{c1!50}\textbf{95.62} & \underline{94.37} & \cellcolor{c1!50}\textbf{97.09} \\
%         ReAlignFit$_\text{GAT}$ & \underline{88.35} & 94.22 & \underline{90.41} & 86.92 & 91.76 & 91.66 & 94.56 & 91.76 & 91.97 & 95.03 & 95.35 & \cellcolor{c1!50}\textbf{98.42} & 93.87 & 93.67 & 96.26 \\
%         ReAlignFit$_\text{GIN}$ & 85.23 & 94.35 & 89.34 & 86.89 & 90.67 & \underline{92.12} & 92.95 & \underline{91.91} & \underline{93.04} & 95.23 & 93.33 & \underline{98.09} & \underline{95.01} & 91.58 & 95.82\\
%         \hline
%     \end{tabular}
%     }
%     \label{tab:ddi}
% \end{table*}

% \subsection{Overall Performance (\textbf{RQ1})}
% We compare ReAlignFit with four backbone models: GCN \cite{gcn}, GAT \cite{gat}, MPNN \cite{mpnn}, and GIN \cite{gin}. We focus on comparing the performance differences between CIGIN \cite{cigin}, CMRL \cite{lee2023shiftKDD}, CGIB \cite{lee2023cgibICML}, MMGNN \cite{dummgnn} and ReAlignFit for MI tasks. For DDI tasks, we select four state-of-the-art DDI models (SSI-DDI \cite{ssiddi}, MIRACLE \cite{miracle}, SA-DDI \cite{saddi}, and DSN-DDI \cite{dsn}) in the DDI prediction and four general domain models (CIGIN \cite{cigin}, CMRL \cite{lee2023shiftKDD}, CGIB \cite{lee2023cgibICML}, and MMGNN \cite{dummgnn}) to compare their performance with ReAlignFit. The experimental results of ReAlignFit and comparative models in MI prediction on In-Distribution Data (Original) are shown in Table \ref{tab:mi} and Table \ref{tab:ddi}.



% \textbf{Comprehensive Performance Analysis}: 
% ReAlignFit demonstrates the best overall performance in MI and DDI predictions compared to the comparison models. Particularly, in DDI prediction, ReAlignFit achieves the best predictive performance in DDI prediction on three datasets, with an average improvement of 2.7\% in ACC and 3.0\% in AUROC. This suggests that reducing the impact of confounding information on substructures and enhancing the relationship between causal substructures and the prediction targets contributes to improving the model's predictive performance. We also observe that the MI and DDI prediction performance of the models that considers substructure information are significantly better than that ignores substructure. This indicates that the interactions between molecular substructures are essential determinants of molecular relationships. Encouraging models to learn substructure information within molecules can lead to better prediction of molecular interaction. This finding is consistent with the nature that substructure is the core of determining chemical reactions. 
% \textbf{ReAlignFit Compatibility Analysis}: 
% ReAlignFit flexibly integrates with different GNN encoders and causal information. By replacing the GNN encoder in ReAlignFit with GCN, GAT, GIN, and MPNN, we observe that its performance fluctuates slightly (most performance metrics in DDI prediction fluctuates around 1.0\%, and the prediction performance in MI prediction shows a similar phenomenon). This indicates that ReAlignFit is less affected by backbone networks and can be flexibly integrated with general graph models. In ithe integration with causal information, we find that the ACC and AUROC of models that do not fully consider causality (CIGIN, CGIB) are at least 3.0\% lower than those that fully consider causality (ReAlignFit) in DDI prediction. The difference in performance is even more pronounced in the MI prediction predictions. This underscores the importance of causal relationships between prediction targets and substructures in molecular relational learning tasks.


% % ReAlignFit demonstrates the best overall performance in MI and DDI predictions. Particularly, ReAlignFit achieves an average improvement of 2.7\% in ACC and 3.0\% in AUROC in DDI prediction.
% % % ReAlignFit's comprehensive performance outperforms other comparative methods in DDI prediction.
% % This suggests that reducing the impact of confounding information on substructures and enhancing the relationship between causal substructures and the prediction targets contributes to improving the model's predictive performance. \textbf{Substructure Importance Analysis}: The prediction performance of methods that consider substructure information is significantly better than those that ignore substructures. This indicates that the interactions between molecular substructures are essential determinants of molecular relationships. Encouraging models to learn substructure information within molecules can lead to better prediction of molecular interaction. This finding is consistent with the nature that substructure is the core of determining chemical reactions. \textbf{Causality Importance Analysis}: In DDI prediction, methods that do not fully consider  causality (CIGIN, CGIB) have ACC and AUROC at least 3.0\% lower than methods that fully incorporate causality (CSGIB).
% % % Methods that do not fully consider causality (CIGIN, CGIB) perform worse than methods that fully consider causality (ReAlignFit). 
% % This suggests that the causal relationships between the prediction targets and substructures are critical for molecular relational learning tasks. \textbf{ReAlignFit Compatibility Analysis}: By substituting different GNN models within ReAlignFit, we observe that its performance fluctuates slightly and is less affected by the backbone. This indicates that ReAlignFit integrates flexibly with general graph models.
% % 通过对比Table \ref{tab:mi} and Table \ref{tab:ddi}的实验结果，我们可以得出如下observations：1)CSGIB在MI预测和DDI预测中均表现出了最好的综合性能。尤其是在DDI预测中，其性能全面优于其他对比方法。我们认为，在MRL中降低混淆子结构对因果子结构的影响并捕获因果子结构与预测目标之间的强关联性有助于提升模型的预测性能。2）考虑子结构信息的方法（GCN、GAT、MPNN、GIN、MIRACLE、CIGIN）的预测性能明显优于忽略子结构的方法（SSI-DDI、DSN-DDI、SA-DDI、CMRL、CGIB、CSGIB）。这说明分子子结构间的相互作用是分子关系的重要决定因素。通过激励模型学习分子中的子结构信息可以更好的预测分子关系。这一发现与子结构是决定化学反应的核心这一本质相印证。3）未充分考虑因果关系的方法（CIGIN、CGIB）的性能比充分考虑因果关系的方法（CSGIB）差。这表明预测目标与子结构之间的因果关联对于分子关系学习任务至关重要。虽然识别核心子结构对分子表征很重要，但对于分子关系学习而言，因果子结构与目标间的关联具有同等重要性。4）通过替换CSGIB中不同的GNN模型，我们可以看出其性能波动较小，受backbone影响较小。这表明，CSGIB具有良好的兼容性和灵活性。同时也表明我们所提出的优化方案与基础模型的适配性较好，CSGIB可更广泛的应用于一般的图模型。
% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{Figure/inter_result.png} 
% \caption{The interpretability performance of ReAlignFit and comparison methods on Distribution-Shifted Data (P1).}
% \label{interresults}
% \end{figure*}
% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{Figure/molecular-inter.png} 
% \caption{The visualization results of molecular electrostatic potential and bond importance in molecules for ReAlignFit and interpretable GNN models. The red regions in figure (a) represent the active functional groups in MRL. The \textcolor{red}{red line}, \textcolor{c8}{purple line}, and \textcolor{black}{black line} in figure (b) represent the impact on molecular interaction prediction accuracy of \textcolor{red}{$> 50\%$}, \textcolor{c8}{$25\%-50\%$}, and \textcolor{black}{$< 25\%$}, respectively.}
% \label{difftest}
% \end{figure*}
% \subsection{Interpretability Analysis of ReAlignFit (\textbf{RQ2})}
% We use three metrics, \textit{i.e.}, Fidelity, Contrastivity, and Sparsity, to evaluate model interpretability. Following \cite{chaudhuri2024transitivity,pope2019explainability,TGNN}, we define Fidelity, Contrastivity, and Sparsity as follows: Fidelity is represented by the prediction accuracy after deleting substructures with a significance greater than 0.25, Contrastivity is expressed as the proportion of MRL predictions using core substructure representations consistent with predictions based on overall molecule representations, Sparsity is one minus the ratio of the number of nodes in the core substructure to the number of nodes in the molecular graph. The interpretability performance of ReAlignFit and comparison methods is shown in Fig. \ref{interresults}.
% % 我们使用3种指标，即Fidelity、Contrastive和Sparsity来评估模型的可解释性。其中，Fidelity采用删除显著性大于0.25的子结构后的预测准确率表示，Contrastive表示为采用核心子结构表征的MRL预测结果与采用分子整体表征预测结果相一致的比例，Sparsity定义为1减去核心子结构中节点数量与分子图中节点数量的比值。ReAlignFit及其对比方法的解释性能如图3所示。

% \textbf{Interpretability Analysis}: Based on the experimental results in Fig. \ref{interresults}, we observe the following result. ReAlignFit demonstrates significantly better interpretability performance than other models. The Fidelity, Contrastivity, and Sparsity of ReAlignFit improved by approximately 10\% in the three datasets relative to the comparison models. In contrast, MIRACLE and DSN-DDI have weaker interpretability, making it challenging to provide reliable explanations for MRL predictions. Notably, ReAlignFit has higher interpretability compared to the molecule-level GIB model (CGIB). This result indicates that causal relationships between substructures provide higher-quality explanations for MRL predictions.
% % 根据图3中的实验结果，我们得到以下结果：ReAlignFit的可解释性表现明显优于其他对比方法。与对比方法相比，ReAlignFit的Fidelity、Contrastive和Sparsity在三个数据集中提高了约10%。而MIRACLE和DSN-DDI则可解释性较差，难以为MRL预测提供可靠的解释。不可忽视的是，与从分子层面建模的GIB方法（CGIB）相比，ReAlignFit具有更高的可解释性。这表明，子结构间的因果关系为MRL预测提供了更高质量的解释。


% \textbf{Interpretability Verification based on Differential Testing Ideas}: Additionally, we compare the interpretability visualizations of ReAlignFit with feature importance-based graph explanation models (GNNExplainer \cite{GNNExplainer} and GraphLIME \cite{Graphlime}) the probabilistic graph explanation model (PGExplainer \cite{pe}) and the subgraph explanation model (SubgraphX \cite{subgraphx}). 
% % Guided by the idea of differential testing, ReAlignFit is further verified to be interpretable in MRL. The experimental results are presented in Fig. \ref{difftest}.
% Fig. \ref{difftest} illustrates the impact of edge reconstruction on prediction accuracy for the interaction between Aspirin (DB00945) and Warfarin (DB00682) during the optimization process in Eq. (\ref{eq:op1}). As shown in Fig. \ref{difftest}, both ReAlignFit and other GNN interpretability models identified the \ce{OCCC=O} in DB00682 and the \ce{OC=OC} in DB00945 as critical for DDI prediction. Guided by the idea of differential testing from software engineering, this result validates that ReAlignFit achieves interpretability comparable to other GNN interpretability models. 
% Additionally, we observed that ReAlignFit demonstrates a more substantial capability in identifying bonds and substructures critical for DDI, with higher weights assigned to key chemical bonds and substructures. These results align with the electrostatic potential visualization results in Fig. \ref{difftest}(a) and medical knowledge, where the coumarin structure in DB00682 inhibits coagulation factors and exhibits synergistic effects with DB00945~\cite{fihn1996risk,hirsh2001oral}. This result further demonstrates ReAlignFit’s interpretability in MRL.
% % Additionally, we observed that ReAlignFit demonstrates enhanced capability in identifying edges and substructures critical to DDI, consistent with medical knowledge that the coumarin structure in DB00682 inhibits coagulation factors and exhibits synergistic effects with DB00945 \cite{fihn1996risk,hirsh2001oral}. This result further demonstrates ReAlignFit’s interpretability in MRL.

% % ReAlignFit与可解释GNN模型的分子化学键重要性实验结果。红色线、蓝色线和黑色线分别代表对分子关系预测准确率的影响为大于50%，25%-50%和小于25%。
% % 除此之外，我们将特征重要性图解释模型 GNNExplainer 和 GraphLIME，概率性图解释模型PGExplainer，子图解释模型SubgraphX的解释可视化与 ReAlignFit 进行比较。在差分测试思想的指引下，进一步验证了ReAlignFit在MRL中可解释性。相关实验结果如图？所示。





% \subsection{Stability Analysis of ReAlignFit (\textbf{RQ3})}
% In this section, we analyze the stability of model prediction performance in different data distribution scenarios (Original, P1 and P2). The experimental results are shown in Fig. \ref{stableresults} and Table \ref{tab:app3}. 
% We evaluate model stability by calculating the degree of performance degradation $S$ in different data distributions: $S_i=({Res_{cau}-Res_{P_i}})/{Res_{cau}}$, where $Res_{cau}$ represents the model's prediction results in the original data distribution. $Res_{P_i}$ denotes the prediction results in the $P_i$ data distribution, and ${P_i}\in {P1,P2}$.
% The \textbf{values} in Fig. \ref{stableresults} represent the model performance fluctuations.
% % Specifically, we conducted extensive experiments on P1 and P2 data distributions to verify the impact of data distribution differences on model prediction performance. The experimental results are shown in Fig. \ref{stableresults} and Tables 3-6 in Appendix C.
% % 在本节中，我们分析了在不同数据分布场景中模型预测性能的稳定性。具体来说，我们P1和P2数据分布中分别进行了大量的实验以验证数据分布差异对模型预测性能的影响。实验结果如图4和附录C中的表3-6所示。

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{Figure/stable.png} 
% \caption{The performance and stability of ReAlignFit, CGIB and CIGIN in different data distributions.}
% \label{stableresults}
% \end{figure*}
% \begin{table*}[htpb]
% \small
%     \centering
%      \caption{The DDI prediction results for ReAlignFit and the comparison models in the Distribution-Shifted Data (P2), with the best results highlighted in \colorbox{c1!50}{\textbf{bold}} and the second results highlighted in \underline{underline}.}
%     \label{tab:app3}
%     \renewcommand{\arraystretch}{1.15}
%     \setlength{\tabcolsep}{0.85mm}{
%     \begin{tabular}{lccccc|ccccc|ccccc}
%     \hline
%         \multirow{2}{*}{Model} & \multicolumn{5}{c|}{ZhangDDI} & \multicolumn{5}{c|}{HetionteDDI} & \multicolumn{5}{c}{DrugBankDDI}\\
%         \cline{2-16}
%          & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR  \\ \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
%           \hline
%         GCN (ICLR'17) & 47.04 & 44.36 & 51.58 & 51.56 & 54.44 & 58.22 & 53.82 & 53.06 & 51.32 & 62.46 & 43.15 & 41.18 & 46.20 & 51.29 & 50.17  \\
%         GAT (ICLR'18) & 49.94 & 47.42 & 55.34 & 52.90 & 56.87 & 61.42 & 53.88 & 55.68 & 53.92 & 61.32 & 43.44 & 45.61 & 51.20 & 50.06 & 52.30  \\
%         MPNN (ICML'17) & 46.81 & 45.15 & 53.91 & 51.59 & 53.14 & 63.07 & 55.25 & 55.98 & 53.32 & 60.84 & 46.50 & 45.08 & 52.66 & 52.95 & 53.26  \\ 
%         GIN (ICLR'19) & 52.25 & 44.35 & 58.52 & 52.17 & 51.14 & 63.13 & 54.83 & 52.38 & 54.14 & 60.43 & 45.13 & 41.85 & 54.95 & 52.27 & 51.38  \\ 
%         MIRACLE (WWW'21) & 45.47 & 48.34 & 59.09 & 51.31 & 48.91 & 64.14 & 54.06 & 57.66 & 54.19 & 59.68 & 42.57 & 47.33 & 53.12 & 53.77 & 53.49  \\ 
%         CIGIN (AAAI'20) & 51.82 & 52.19 & 59.23 & 51.59 & 55.42 & 62.01 & 56.67 & 57.24 & 57.35 & 62.70 & 47.39 & 56.10 & 52.02 & 52.68 & 54.52  \\
%         \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
%           \hline
%         SSI-DDI (BIB'21) & 49.32 & 49.92 & 62.92 & 48.83 & 55.16 & 61.64 & 56.61 & \underline{58.44} & 54.04 & \cellcolor{c1!50}\textbf{67.70} & 45.75 & 54.77 & 53.96 & 54.62 & 54.59  \\ 
%         DSN-DDI (BIB'23) & 48.78 & 47.17 & 63.51 & 54.78 & 47.33 & 61.72 & 55.11 & 52.45 & 53.72 & 61.27 & 46.08 & \underline{57.44} & 53.49 & 52.28 & 55.71  \\ 
%         CMRL (KDD'23) & 53.70 & 56.79 & 65.39 & \underline{54.80} & \underline{57.84} & 63.83 & 56.17 & 55.42 & 55.81 & 60.69 & 52.61 & 57.16 & 54.36 & 54.25 & 54.53  \\ 
%         CGIB (ICML'23) & 54.61 & \underline{57.83} & 64.39 & 54.45 & 57.53 & 63.91 & 59.78 & 56.79 & \underline{59.96} & 58.26 & 54.07 & 57.19 & \underline{54.42} & \underline{55.64} & \underline{56.02}  \\
%         MMGNN (IJCAI'24) & \underline{55.68} & 56.53 & \underline{66.61} & 54.32 & 56.54 & \underline{65.09} & \underline{61.11} & 57.78 & 58.03 & 65.45 & \underline{58.32} & 56.72 & 53.33 & 53.68 & 55.53 \\
%         \hline
%         ReAlignFit & \cellcolor{c1!50}\textbf{60.85} & \cellcolor{c1!50}\textbf{59.49} & \cellcolor{c1!50}\textbf{68.28} & \cellcolor{c1!50}\textbf{57.76} & \cellcolor{c1!50}\textbf{61.78} & \cellcolor{c1!50}\textbf{69.75} & \cellcolor{c1!50}\textbf{68.64} & \cellcolor{c1!50}\textbf{62.62} & \cellcolor{c1!50}\textbf{65.91} & \underline{66.42} & \cellcolor{c1!50}\textbf{62.58} & \cellcolor{c1!50}\textbf{66.20} & \cellcolor{c1!50}\textbf{56.87} & \cellcolor{c1!50}\textbf{60.95} & \cellcolor{c1!50}\textbf{61.55}\\  \hline
%     \end{tabular}
%     }
% \end{table*}

% Although data distribution affects model predictive performance, ReAlignFit maintains optimal stability in distribution-shifted data. \textbf{Performance Fluctuation Analysis}: ReAlignFit maintains the best stability in different data distributions compared to methods that do not consider causal information. Compared to CGIB, ReAlignFit improved stability by at least 4.5\% and 6\% in two distribution scenarios.
% % ReAlignFit maintains the best stability in different data distributions with the lowest performance degradation in various scenarios compared to methods that do not consider causal information. 
% The minimal impact of data distribution on ReAlignFit can be explained by the ability of ReAlignFit to capture stable molecular representations. ReAlignFit learns molecular representations from causal substructures closely related to different chemical reactions. These molecular representations integrate information from various chemical reactions as thoroughly as possible, allowing the model to maintain stable predictive performance in different data distribution scenarios. The experimental results from Fig. \ref{stableresults} indicate that stability is the key to ReAlignFit's success, primarily attributed to the stability of molecular representations. \textbf{Predictive Performance Analysis}: As shown in  Table \ref{tab:app3}, ReAlignFit achieves the best prediction performance in different data distributions. ReAlignFit attributes the results to two main reasons. The molecular representations generated by ReAlignFit contain robust causal information between substructures and prediction targets. On the other hand, ReAlignFit mitigates the impact of confounding substructures on molecular representations by weakening the association of confounding substructures with causal substructures. Although the generalization and stability of ReAlignFit in distribution-varying data are better than other comparative methods, it cannot be ignored that the predictive performance of all models shows different degrees of performance degradation as the data distribution differences increase. Therefore, improving the interpretability and stability of models on distribution-shifted data remains a challenge in MRL.
% % \textbf{3) Challenges}:\note{[??]} OOD data still challenge the predictive performance of the model. Although the generalization and stability of ReAlignFit in distribution-varying data are better than other comparative methods, it cannot be ignored that the predictive performance of all models shows different degrees of performance degradation as the data distribution differences increase. Therefore, improving the generalization and stability of models on out-of-distribution data remains a significant challenge in MRL.
% % 挑战写在总结的未来工作中
% % 如图4所示，数据分布明显影响了模型的预测性能。通过分析相关实验结果，我们有以下观察结果：1）相比较于未考虑因果信息的方法，ReAlignFit在不同数据分布中保持了最佳的稳定性，在各种场景中性能下降均为最低。ReAlignFit受数据分布的影响最小可以用ReAlignFit捕获分子稳定表示的能力来解释。ReAlignFit从与不同化学反应密切相关的因果子结构中学习分子的表征。这种分子表征尽可能全面的聚合了不同化学反应信息，使模型在面对不同数据分布的场景时能保持预测性能的稳定。基于图3表3-6的实验结果，我们认为ReAlignFit成功的关键是稳定性，这主要归功于分子表征的稳定性。2）数据分布的变化造成了模型预测性能的波动，但ReAlignFit仍在不同数据分布中取得了最好的预测性能，这表明ReAlignFit具有更好的泛化性能和实用性。我们认为取得上述结果是两方面原因造成。一方面，ReAlignFit生成的分子表征包含了子结构与预测目标之间的强因果信息。另一方面，ReAlignFit通过弱化混淆子结构对因果子结构的关联降低混淆子结构对分子表征的影响。3）分布变化数据对模型的预测性能仍存在挑战。虽然ReAlignFit在分布变化数据中的泛化性和稳定性均优于其他的对比方法，但不可忽视是，所有模型的预测性能随着数据分布差异变大而呈现出不同程度的性能下降。因此，提升模型在分布外的泛化性和稳定性仍然是MRL面临的重要问题。
% % \subsection{Model Analysis}
% % In order to verify the effectiveness of each component in the model, we analyzed the performance of ReAlignFit from three perspectives: confounding information compression, ablation experiments, and visualization results analysis. The experimental results are shown in Fig. \ref{Analysisresults} and Fig. \ref{visualization}.
% % 为了验证模型中各部分的有效性，我们从混淆信息压缩、消融实验及可视化结果分析三方面分析了ReAlignFit的性能。实验结果如图5和图6所示。

% \subsection{Model Analysis of ReAlignFit (\textbf{RQ4})}
% \subsubsection{Confusion Analysis on $\alpha$ and $\beta$}
% % \subsubsection{Confusion Analysis on $\alpha$ and $\beta$ (\textbf{RQ3})}
% To verify the impact of confounding information $\alpha I(G_{x}^{c},G_{x}^{n})+ \beta I(G_{y}^{c},G_{y}^{n})$ in Eqs. (\ref{eq:CSGIBnew}) and (\ref{lossnum}) of ReAlignFit's predictive performance, we set $\alpha$ and $\beta$ to $\{0.001,0.01,0.1,0.3,0.5\}$ and conduct relevant experiments in three different data distributions. The experimental results are shown in Fig. \ref{confusion}. 
% % 为了验证公式（3）中混淆信息x对ReAlignFit预测性能稳定性的影响，我们将A和B设置为C并在三种不同数据分布中进行了相关实验。实验结果如图5（a）-(c)所示。
% \begin{figure}[htpb]
% \centering
% \includegraphics[width=\columnwidth]{Figure/confusion.png} 
% \caption{The experimental results of ReAlignFit in HetionteDDI dataset with different parameter settings.}
% \label{confusion}
% \end{figure}

% In Fig. \ref{confusion}, there exists an optimal value of $\alpha$ and $\beta$ (\textit{i.e.}, $\alpha \!\!=\!\! \beta \!\!= \!\!0.1$) that balances the model's predictive performance and the impact of confounding information on molecular representation. When $\alpha$ and $\beta$ are excessively large ($\alpha \!\!= \!\!\beta \!\!\ge \!\!0.3$), the model's predictive performance fluctuates significantly in different data distributions, and its stability decreases markedly. This occurs because high values of $\alpha$ and $\beta$  cause the model to learn excessive confounding information, making causal information difficult to use in MRL effectively. Conversely, when $\alpha$ and $\beta$ are smaller ($\alpha \!\!=\!\! \beta \!\!< \!\!0.1$), while the model's performance fluctuations are reduced, its predictive performance remains below optimal levels. In this case, the model struggles to adequately distinguish between confounding and causal substructures, consequently affecting its performance and generalization. Therefore, appropriately adjusting $\alpha$ and $\beta$ enables the model to more effectively balance the impact of confounding and core information on molecular representation to improve the model's prediction performance.
% % 由图5（a）和图5（b）可知，存在A和B的最优取值（即A和B为0.1），使模型预测性能与混淆信息对分子表征的影响之间存在平衡。A和B过大时，模型在不同数据分布中预测性能波动较大，模型稳定性降低明显。这是因为当A和B过大时过多的混淆子结构被用于分子表征中，掩盖了因果子结构对分子表征的影响，导致模型捕获的分子表征不稳定。当A和B较小时，虽然模型性能波动有所改善，但模型的预测性能仍表现不佳。我们认为，这种情况下模型难以充分捕获混淆子结构与因果子结构之间的区别进而影响了模型的性能和泛化性。因此，合理的控制A和B有助于模型更好的平衡混淆信息和关键信息对分子表征的影响，以提升模型预测性能的稳定性。

% \subsubsection{Ablation Experiment}
% In order to verify the impact of the components in the ReAlignFit on the predictive performance of the model, we set up the following three variants: ReAlignFit$_\text{none}$ (no optimizations applied in the model architecture), ReAlignFit$_\text{con}$ (only minimizing $I(G^{c},G^{n})$ in model optimization), and ReAlignFit$_\text{cau}$ (only maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ in model optimization). The results are shown in Fig. \ref{aba}.

% \begin{figure}[htpb]
% \centering
% \includegraphics[width=\columnwidth]{Figure/aba.png} 
% \caption{The DDI prediction performance of ReAlignFit and its variants in three data distributions.}
% \label{aba}
% \end{figure}



% % \begin{itemize}
% %     \item \textbf{Effectiveness of minimizing $I(G^{c},G^{n})$}: Comparing the experimental results in Fig. \ref{ablation-supp}, minimizing $I(G^{c},G^{n})$ improves AUROC by approximately 5\% in different data distributions but does not enhance model stability. Knowledge learned from confounding substructures improves molecular representation to some extent.
% %     % , but confounding information does not directly determine molecular relationships. 
% %     This indicates that confounding information is one of the critical components of molecular representation but not the determining factor. On the contrary, excessive introduction of confounding information can affect the model's learning of causal substructures, which in turn affects the predictive stability.
% %     % A的有效性：对比图5（d）-(f)中的实验结果，A提升了模型的不同数据分布中的预测性能，但却没有改善模型的稳定性。模型在混淆子结构中学习的知识在一定程度改善了分子的表征，但混淆信息并不能直接决定分子间关系。这说明混淆消息是分子表征中的重要组成部分之一，但并非决定因素。相反，过多的引入混淆信息会影响模型对因果子结构的学习，进而影响模型的预测稳定性.
% %     \item \textbf{Effectiveness of maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$}: The predictive performance of the model is substantially improved by maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$. Most predictive metrics improve by over 10\%, while the model's predictive performance is more stable. The performance gain achieved by maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ is much higher than that of minimizing $I(G^{c},G^{n})$. This suggests that causal information between substructures and the prediction targets is more important in MRL. However, it is important to note that the contribution of considering only maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ to improving model stability is limited. In particular scenarios, the stability and prediction performance are not significantly different from minimizing $I(G^{c},G^{n})$, as shown in Fig. \ref{ablation-supp}(a).
% % % B的有效性：通过B大幅度的提升了模型的预测性能，同时模型的预测性能也更加稳定。与A相比，B具有更大的性能增益。这说明，在MRL中，子结构与预测目标间的因果信息更加重要。但不可忽视的是，仅考虑B的方式对提升模型稳定的贡献是有限的。在特殊场景中，其稳定性与预测性能与A相比并没有明显差异，如图5（e）和图5（f）所示。
% % \item \textbf{Effectiveness of maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ and minimizing $I(G^{c},G^{n})$}: Among all variants, the model achieves the best predictive performance and stability only when both minimizing $I(G^{c},G^{n})$ and maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ are considered simultaneously. Although the predictive performance decreases in datasets with significant distribution differences, the decline range remains acceptable compared to other variants. These results indicate that incorporating confounding information into causal substructure representation helps the model better distinguish subtle differences between substructures, resulting in more robust molecular representations and improved predictive performance.
% % \end{itemize}


% Comparing the experimental results in Fig. \ref{aba}, minimizing $I(G^{c},G^{n})$ and maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ improves AUROC by 4\%, 8\%, 10\% and by 8\%, 16\%, 13\% in three data distributions, respectively. But the performance gain achieved by maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ is much higher than that of minimizing $I(G^{c},G^{n})$. 
% In terms of stability, minimizing $I(G^{c},G^{n})$ does not improve the model's stability. Maximizing $I(Y; G_{x}^{c}, G_{y}^{c})$ yields a more stable predictive performance. However, it is important to note that the contribution of considering only maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ to improving model's predictive performance is limited. Among all variants, the model achieves the best predictive performance only when both minimizing $I(G^{c},G^{n})$ and maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ are considered simultaneously. These results indicate that CSIB helps to further differentiate subtle differences between substructures to improve the predictive performance. 

% % To verify the impact of our proposed optimization scheme on the stability of the model's predictive performance in out-of-distribution data, we set up different model variants by modifying the optimization strategy. The experimental results are shown in Fig. \ref{Analysisresults}(d)-Fig. \ref{Analysisresults}(f). Detailed settings for the ablation experiments can be found in Appendix B.5. \note{[NEED PARAMS DESCRIPTIONS]}
% % % 为了验证我们提出的优化方案对模型在分布外数据中预测性能稳定性的影响，我们通过修改模型的优化策略设置了不同的模型变体。实验结果如图5（d）-(f)所示。消融实验详细设置请阅读附录B.5.
% % \begin{itemize}
% %     \item \textbf{Effectiveness of minimizing $I(G^{c},G^{n})$}: Comparing the experimental results in Fig. \ref{Analysisresults}(d)-Fig. \ref{Analysisresults}(f), minimizing $I(G^{c},G^{n})$ improves the model's predictive performance in different data distributions but does not enhance model stability. Knowledge learned from confounding substructures improves molecular representation to some extent, but confounding information does not directly determine molecular relationships. This indicates that confounding information is one of the critical components of molecular representation but not the determining factor. On the contrary, excessive introduction of confounding information can affect the model's learning of causal substructures, which in turn affects the predictive stability.
% %     % A的有效性：对比图5（d）-(f)中的实验结果，A提升了模型的不同数据分布中的预测性能，但却没有改善模型的稳定性。模型在混淆子结构中学习的知识在一定程度改善了分子的表征，但混淆信息并不能直接决定分子间关系。这说明混淆消息是分子表征中的重要组成部分之一，但并非决定因素。相反，过多的引入混淆信息会影响模型对因果子结构的学习，进而影响模型的预测稳定性.
% %     \item \textbf{Effectiveness of maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$}: The predictive performance of the model is substantially improved by maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$, while the predictive performance is more stable. The performance gain achieved by
% % maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ is much higher than that of minimizing $I(G^{c},G^{n})$. This suggests that causal information between substructures and the prediction target is more important in MRL. However, it is important to note that the contribution of considering only maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ to improving model stability is limited. In particular scenarios, the stability and prediction performance are not significantly different from minimizing $I(G^{c},G^{n})$, as shown in Fig. \ref{Analysisresults}(e) and Fig. \ref{Analysisresults}(f).
% % % B的有效性：通过B大幅度的提升了模型的预测性能，同时模型的预测性能也更加稳定。与A相比，B具有更大的性能增益。这说明，在MRL中，子结构与预测目标间的因果信息更加重要。但不可忽视的是，仅考虑B的方式对提升模型稳定的贡献是有限的。在特殊场景中，其稳定性与预测性能与A相比并没有明显差异，如图5（e）和图5（f）所示。
% %     \item \textbf{Effectiveness of maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ and minimizing $I(G^{c},G^{n})$}: Among all variants, the model achieves the best predictive performance and stability only when both minimizing $I(G^{c},G^{n})$ and maximizing $I(Y;\!G_{x}^{c},\!G_{y}^{c})$ are considered simultaneously. Although the predictive performance decreases in datasets with significant distribution differences, the decline range remains acceptable compared to other variants. These results indicate that incorporating confounding information into causal substructure representation helps the model better distinguish subtle differences between substructures, resulting in more robust molecular representations and improved predictive performance.
% %     % C的有效性：在所有变体中，只有同时考虑A和B时，模型的预测性能和稳定性是最好的。虽然模型在分布差异较大的数据中预测性能有所下降，但相比于其他变体，其下降范围仍在可接受范围内。这些结果说明在因果子结构表征中引入混淆信息有助于模型进一步区分子结构间的细微差别以生成更稳健的分子表征和提升预测性能。
% % \end{itemize}


% % We set up the following model variants to verify the impact of our proposed optimization scheme on the stability of the model's predictive performance:
% % % 我们设置了以下模型变体以验证我们提出的优化方案对模型预测性能稳定性的影响。
% % \begin{itemize}
% %     \item ReAlignFit$_\text{none}$: No optimization operations are applied in the model architecture. The relationship prediction relies solely on the substructures extracted by MPNN as the molecular feature representation.
% %     % ReAlignFit_none:在模型架构中不采取任何优化操作，即仅依靠MPNN提取的子结构作为分子的特征表示进行关系预测。
% %     \item ReAlignFit$_\text{con}$: Only minimizes B in model optimization and generates the molecule's representation directly after injecting the confusing information in the model. This variant considers only the influence of confounding information on molecular representation and ignores the influence of causal substructures.
% %     % ReAlignFit_con:在模型优化中仅最小化B，在模型中注入混淆信息后直接生成分子的表征。即只考虑混淆信息对分子表征的影响，而不考虑因果子结构对分子表征的影响。
% %     \item ReAlignFit$_\text{cau}$: Only maximizes A in model optimization, utilizing substructures with the highest mutual information with the prediction target to generate molecular representations. This variant considers only the association between substructures and the prediction target, ignoring the influence of confounding information.
% %     % ReAlignFit_cau:在模型优化中仅最大化A，并利用与预测目标具有最大互信息的子结构生成分子表征。即只考虑子结构与预测目标间的关联性，而忽略混淆信息对分子表征的影响。
% % \end{itemize}

% \subsection{Visualization Analysis of ReAlignFit}
% To validate ReAlignFit's ability to capture interpretable causal substructures in MRL, we visualize the node features and the interactions between substructures. The visualization results are shown in Fig. \ref{visualization}.
% % Fig. \ref{visualization} shows the experimental results of the qualitative analysis.
% % We visualized the node features of drug molecules based on the molecular representations captured by ReAlignFit and calculated the intermolecular substructure correlations using the Mantel test Fig. \ref{visualization} shows the experimental results of the qualitative analysis.
% % 我们根据ReAlignFit捕获的分子表征对药物分子的节点特征进行了可视化，并利用Mantel test计算了分子间子结构的关联性。图？展示了上述定性分析的实验结果。
% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{Figure/moleculaer.png} 
% \caption{The visualization of node features and interaction strengths between substructures.}
% \label{visualization}
% \end{figure*}
% \begin{figure}[t]
% \centering
% \includegraphics[width=0.5\textwidth]{Figure/tsne.png} 
% \caption{The visualization of molecular pairs interaction prediction results in the DDI dataset. Class 1 indicates molecular pairs with an interaction, while Class 2 indicates molecular pairs without an interaction.}
% \label{visualization2}
% \end{figure}


% In the node feature representations of drug molecules, the substructure information captured by ReAlignFit closely aligns with the actual molecular substructures and emphasizes the interactions between atoms within molecules. As shown in Figs. \ref{visualization}(c) and (f), ReAlignFit demonstrates satisfactory recognition capabilities for multiple complex functional groups within molecules. This result is consistent with the chemical knowledge of the strong interaction relationships between atoms within molecular functional groups. The Mantel test \cite{manteltest} results shown in the Figs. \ref{visualization}(b) and (e) display the interaction strength between different substructures. Darker and thicker lines represent stronger interactions between substructures. The results indicate the high interaction strength between the symmetrical -COOH groups (green and orange areas) in DB00548 and \textcolor{red}{Group1} (benzene ring) in DB01117. \textcolor{green}{Group4} in DB00802 strongly interacts with the functional group (purple region) in DB00440. This is consistent with the domain knowledge that chemical reactions occur between core substructures of molecules. Additionally, the \textcolor{orange}{Group3} in DB01117 exhibits a weaker interaction with substructures in DB00548. This is due to the inertness of \textcolor{orange}{Group3's} cyclic structure, which makes it difficult to react chemically with other substructures. In conclusion, ReAlignFit not only captures substructure information within molecules but also highlights the crucial role of this substructure information in determining the occurrence of chemical reactions. The above experimental results provide an effective explanation for ReAlignFit to capture the core substructure in MRL.
% % 在药物分子的节点特征表示中，ReAlignFit捕获的子结构信息与分子内实际的子结构几乎一致，且突出分子内原子之间的关联关系。ReAlignFit的特征捕获结果与分子官能团内原子作用关系密切的化学知识一致。图？中间图片的Mantel test计算结果展示了DB00548与DB01117中不同子结构之间的关联强度。线条颜色越深且越粗代表子结构之间的关联程度越强。结果表明，DB00548中处于对称位置的-COOH（绿色和橙色区域）与DB01117中的Group1(苯环，红色区域)之间关联强度高。这与化学反应发生在分子的子结构之间的领域知识一致。同时我们发现DB01117中的Group3（橙色区域）与DB00548中子结构的关联性较弱。这是因为Group3本身环状结构的惰性及左右两侧子结构性质较为活跃导致其难以与其他子结构发生化学反应。综上，ReAlignFit不仅可以捕获分子内的子结构信息，同时这些子结构信息对决定化学反应发生具有至关重要的作用。

% To further demonstrate the performance of ReAlignFit, we perform dimensionality reduction on the molecular pair features captured by ReAlignFit, as shown in Fig. \ref{visualization2}. ReAlignFit distinctly differentiates the interactions among molecular pairs in the three datasets. The visual presentation results confirm ReAlignFit's satisfactory performance in molecular relational learning.


% \section{Related Work}
% \subsection{Molecular Relational Learning}
% MRL is an essential task in molecular representation, encompassing various applications \cite{pei2024hagoAAAI,fang2024moltcACL,dummgnn}. 
% In this paper, we focus on MI prediction and DDI prediction. 
% In molecular interaction prediction, the model primarily predicts changes in chemical properties caused by chemical reactions. CIGIN \cite{cigin} employed a message-passing network and collaborative attention to encode intra-molecular atoms and predict solvation-free energy. Their interpretability study highlights the importance of atomic interactions in molecular relationships. CMRL \cite{lee2023shiftKDD} combined molecular representation with causal relationships, identifying substructures causally related to chemical reactions.
% DDI prediction focuses on identifying the interaction relationships between drug molecules, which is critical for co-medication in clinical settings \cite{wu2024mkgaaai}. 
% Substructure-based DDI prediction is currently a research hotspot. 
% SA-DDI \cite{saddi}, SSI-DDI \cite{ssiddi}, and DSN-DDI \cite{dsn} extract substructure information from drugs in various ways to better represent drug molecules. Additionally, TIGER \cite{zhang2024heterogeneousIJCAI} uses a Transformer architecture with a relation-aware attention mechanism to construct semantic relationships between drugs. 
% Although existing MRL methods have advantages in predictive performance, they do not adequately consider the difficulty in accurately identifying response sites due to the spuriousness of attention mechanisms, which significantly reduces the interpretability of the model.
% % aaaiversion
% % In MI prediction, CIGIN \cite{cigin} employed a message-passing network and collaborative attention to encode intra-molecular atoms and predict solvation energy. CMRL \cite{lee2023shiftKDD} combined molecular representation with causal relationships, identifying substructures causally related to chemical reactions. 
% % In DDI prediction, SA-DDI \cite{saddi}, SSI-DDI \cite{ssiddi}, and DSN-DDI \cite{dsn} extracted substructure information from drugs in various ways to better represent drug molecules.




% % Additionally, TIGER \citep{zhang2024heterogeneousIJCAI} uses a Transformer architecture with a relation-aware attention mechanism to construct semantic relationships between drugs. 
% % In molecular interaction prediction, the model primarily predicts changes in chemical properties caused by chemical reactions. CIGIN \citep{cigin} employs a message-passing network and collaborative attention to encode intra-molecular atoms and predict solvation-free energy. 
% % Their interpretability study highlights the importance of atomic interactions in molecular relationships. 

% % DDI prediction focuses on identifying the interaction relationships between drug molecules, which is critical for co-medication in clinical settings \citep{wu2024mkgaaai}. 
% % Substructure-based DDI prediction is currently a research hotspot. 
% % SA-DDI \citep{saddi}, SSI-DDI \citep{ssiddi}, and DSN-DDI \citep{dsn} extract substructure information from drugs in various ways to better represent drug molecules. Additionally, TIGER \citep{zhang2024heterogeneousIJCAI} uses a Transformer architecture with a relation-aware attention mechanism to construct semantic relationships between drugs. 

% \subsection{Graph Information Bottleneck}
% GIB \cite{wu2020GIB} is an extension and development of the Information Bottleneck theory within graph theory, which has been widely applied in molecular relational learning \cite{lee2023cgibICML,sun2022graphAAAI}, multi-agent communication \cite{ding2023robusttpami,ding2024learningAAAI}, and graph learning interpretability \cite{seo2024interpretableNeurIPS,WWW24}. 
% \cite{wu2020GIB} extended the Information Bottleneck theory by regularizing graph structures and proposed GIB for the first time. PGIB \cite{seo2024interpretableNeurIPS} introduces prototype information into the GIB framework and uses it as a compression bottleneck to generate subgraphs closely related to the prediction goal, improving the interpretability of graph representation learning. 
% \cite{ding2023robusttpami,ding2024learningAAAI} optimized robust communication in multi-agent systems by setting mutual information between message representations and action choices, as well as between agent features and message representations, as the GIB optimization objectives. CGIB \cite{lee2023cgibICML} combined molecular relational learning with the GIB framework to model the nature of chemical reactions. Based on Graph Information Bottleneck, TGIB \cite{TGNN} introduces randomness into each temporal event graph to provide explanations for event occurrences.
% GIB effectively improves the interpretability of graph representation learning models. Howeve, these methods fail to adequately consider the causal relationships between molecules and targets at the substructure level, thereby reducing the interpretability of the model. Therefore, our goal is to improve interpretability of the model by minimizing the difference between confounding information and causal information.



% % GIB \cite{wu2020GIB} is an extension and development of the Information Bottleneck theory within graph theory, which has been widely applied in molecular relational learning \cite{lee2023cgibICML,sun2022graphAAAI}, multi-agent communication \cite{ding2023robusttpami,ding2024learningAAAI}, and graph learning interpretability \cite{seo2024interpretableNeurIPS,WWW24}. Specifically, PGIB \cite{seo2024interpretableNeurIPS} introduced prototype information into the GIB framework and used it as a compression bottleneck to improve the interpretability of graph representation learning.
% % % Specifically, \citet{wu2020GIB} proposed GIB for the first time. 
% % % PGIB \citep{seo2024interpretableNeurIPS} introduces prototype information into the GIB framework and uses it as a compression bottleneck to improve the interpretability of graph representation learning.
% % % \citet{wu2020GIB} extended the Information Bottleneck theory by regularizing graph structures and proposed GIB for the first time. PGIB \citep{seo2024interpretableNeurIPS} introduces prototype information into the GIB framework and uses it as a compression bottleneck to generate subgraphs closely related to the prediction goal, improving the interpretability of graph representation learning. 
% % \cite{ding2023robusttpami,ding2024learningAAAI} optimized robust communication in multi-agent systems by setting mutual information between message representations and action choices, as well as between agent features and message representations, as the GIB optimization objectives. CGIB \cite{lee2023cgibICML} combined molecular relational learning with the GIB framework to model the nature of chemical reactions. 
% % 图信息瓶颈是信息瓶颈理论在图论中的扩展和发展。它为非欧几里得数据的互信息计算带来了新的机遇，在分子关系学习、多智能通信、图学习可解释性等领域中应用广泛。。具体来说，Wu et al.通过正则化图结构扩展了信息瓶颈理论，并首次提出了GIB。PGIB将原型信息引入GIB框架并以此为压缩瓶颈生成与预测目标紧密相关的子图，进而提升图表示学习的可解释性。丁等人将智能体的消息表示与动作选择间的互信息和代理特征与消息表示间的互信息作为图信息瓶颈优化目标以实现多智能体的稳健通信。CGIB将分子关系学习与图信息瓶颈框架相结合以模拟化学反应的实质。尽管GBI已经成功的应用于各种图表示学习中，但先前研究大多从图层级进行核心子图的识别，这降低了它们在分布外数据中的稳定性。据我们所知，CSGIB是第一个从子图层级进行分布外稳定关系学习的工作。


% % Despite the advantages of the related methods in MRL, they fail to adequately consider the causal relationships between molecules and targets at the substructure level. This limitation make it difficult to identify the true core substructures, thereby reducing the interpretability of the model. Therefore, our goal is to improve interpretability of the model by minimizing the difference between confounding information and causal information.
% % 尽管现有的方法在分子关系学习中取得了成功，但是它们没有从子结构层面充分考虑分子和目标之间的因果关系。这可能难以识别分子内真正的核心子结构进而降低模型的稳定性。因此，我们的目标是最小化混在信息与因果信息之间的差异来提升模型在OOD数据中的稳定性。


% % 分子关系学习是分子表征领域一个重要的任务，涵盖范围广泛。在本文中我们重点关注分子相互作用预测和药物-药物相互作用预测。在分子相互作用预测中，模型主要预测由化学反应引起的化学物质性质的变化。CIGIN (Pathak et al., 2020)利用消息传递网络和协作注意力实现了分子内原子的编码和溶剂自由能的预测。其通过可解释性研究表明了原子间相互作用对分子关系的重要性。CMRL将分子表征与因果关系相结合，识别了与化学反应因果相关的核心子结构并降低了快捷子结构的混淆效应。药物-药物相互作用预测则专注于药物分子间作用关系的识别，其对于临床共同用药等至关重要。基于药物子结构的DDI预测是目前的研究热点。SA-DDI、SSI-DDI和DSN-DDI均通过不同方式挖掘药物的子结构信息以更好的表征药物分子。除此之外，TIGER则通过Transformer架构与关系感知注意力机制构建药物间的语义关系。现有的MRL方法忽略了模型在OOD数据中性能下降的问题。在这项工作中，我们在理论论证的基础上提出了CSGIB来最小化混淆信息与因果信息的差值以提升模型的OOD数据中的稳定性。


% \section{Conclusion}
% In this paper, we propose the Causal Interpretable Subgraph Information to improve the interpretability of molecular relational learning. ReAlignFit incorporates substructure causal information into the dual-layer optimization of GIB to generate stable molecular representations by minimizing the difference between confounding information and causal information. Experimental results demonstrate that ReAlignFit achieves the best predictive performance and significantly improves the predictive interpretability of the model. In the future, we will consider validating ReAlignFit on additional datasets and tasks to improve the stability of the model in distribution-shifted data.





















% \section{Introduction}

% \begin{table*}[htpb]
% \caption{The performance of ReAlignFit and comparative methods in MI prediction, with the best results highlighted in \textbf{bold} and the second results highlighted in \underline{underline}. The results of comparison methods are from.\label{tab:table1}}
% \centering
% \renewcommand{\arraystretch}{1.05}
%     % \setlength{\tabcolsep}{1mm}{
%     \begin{tabular}{lcccccccc}
%     \hline
%         \multirow{2}{*}{Model} & \multicolumn{3}{c}{Chromophore} & \multirow{2}{*}{MNSol} & \multirow{2}{*}{FreeSolv} & \multirow{2}{*}{CompSol} & \multirow{2}{*}{Abraham} & \multirow{2}{*}{CombiSolv}\\
%         \cline{2-4}
%          & Absorption & Emission & Lifetime &  &  &  &  &  \\
%           \hline
%           \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
%           \hline
%         GCN(ICLR'17) & 25.75$\pm$1.48 & 31.87$\pm$1.70 & 0.866$\pm$0.015 & 0.675$\pm$0.021 & 1.192$\pm$0.042 & 0.389$\pm$0.009 & 0.738$\pm$0.041  & 0.672$\pm$0.022\\
%         GAT(ICLR'18) & 26.19$\pm$1.44 & 30.90$\pm$1.01 & 0.859$\pm$0.016 & 0.731$\pm$0.007 & 1.280$\pm$0.049 & 0.387$\pm$0.010 & 0.798$\pm$0.038 & 0.662$\pm$0.021\\
%         MPNN(ICML'17) & 24.43$\pm$1.55 & 30.17$\pm$0.99 & 0.802$\pm$0.024 & 0.682$\pm$0.017 & 1.159$\pm$0.032 & 0.359$\pm$0.011  & 0.601$\pm$0.035 & 0.568$\pm$0.005\\
%        GIN(ICLR'19) & 24.92$\pm$1.67 & 32.31$\pm$0.26 & 0.829$\pm$0.027 & 0.669$\pm$0.017 & 1.015$\pm$0.041 & 0.331$\pm$0.016 & 0.648$\pm$0.024 & 0.595$\pm$0.014\\
%        CIGIN(AAAI'20) & 19.32$\pm$0.35 & 25.09$\pm$0.32 & 0.804$\pm$0.010 & 0.607$\pm$0.024 & 0.905$\pm$0.014 & 0.308$\pm$0.018 & 0.411$\pm$0.008 & 0.451$\pm$0.009\\
%         \hline
%         \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
%         \hline
%        CMRL(KDD'23) & 17.93$\pm$0.31 & 24.30$\pm$0.22 & 0.776$\pm$0.007 & 0.551$\pm$0.017 & 0.815$\pm$0.046 & \cellcolor{c1!50}\textbf{0.255$\pm$0.011} & \underline{0.374$\pm$0.011} & 0.421$\pm$0.008\\
%        CGIB(ICML'23) & 18.11$\pm$0.20 & 23.90$\pm$0.35 & \underline{0.771$\pm$0.005} & \cellcolor{c1!50}\textbf{0.538$\pm$0.007} & 0.852$\pm$0.022 & 0.276$\pm$0.017 & 0.390$\pm$0.006 & 0.422$\pm$0.005\\
%         \hline
%         ReAlignFit & \cellcolor{c1!50}\textbf{16.82$\pm$0.25} & \cellcolor{c1!50}\textbf{22.95$\pm$0.33} & \cellcolor{c1!50}\textbf{0.769$\pm$0.005} & 0.541$\pm$0.010 & \underline{0.799$\pm$0.034} & 0.261$\pm$0.013 & \cellcolor{c1!50}\textbf{0.371$\pm$0.008} & 0.419$\pm$0.008\\
%         ReAlignFit$_\text{GCN}$ & \underline{17.23$\pm$0.27} & \underline{23.35$\pm$0.29} & 0.771$\pm$0.007 & \underline{0.539$\pm$0.012} & \cellcolor{c1!50}\textbf{0.796$\pm$0.035} & \underline{0.257$\pm$0.016} & 0.375$\pm$0.012 & \cellcolor{c1!50} \textbf{0.416$\pm$0.006}\\
%         ReAlignFit$_\text{GAT}$ & 17.55$\pm$0.23 & 23.98$\pm$0.36 & 0.776$\pm$0.007 & 0.543$\pm$0.021 & 0.806$\pm$0.036 & \cellcolor{c1!50}\textbf{0.255$\pm$0.012} & 0.381$\pm$0.013 & 0.421$\pm$0.009\\
%         ReAlignFit$_\text{GIN}$ & 17.92$\pm$0.46 & 24.10$\pm$0.34 & 0.772$\pm$0.007 & 0.552$\pm$0.037 & 0.803$\pm$0.025 & 0.267$\pm$0.021 & 0.386$\pm$0.017 & \underline{0.418$\pm$0.008}\\
%         \hline
%     \end{tabular}
% \end{table*}


% \begin{table*}[htpb]
% \caption{The performance of ReAlignFit and comparative methods in DDI prediction, with the best results highlighted in \colorbox{c1!50}{\textbf{bold}} and the second results highlighted in \underline{underline}. The results of comparison methods in ZhangDDI are from.\label{tab:table1}}
% \centering
% \renewcommand{\arraystretch}{1.05}
%     \setlength{\tabcolsep}{1.25mm}{
%     \begin{tabular}{lccccc|ccccc|ccccc}
%     \hline
%         \multirow{2}{*}{Model} & \multicolumn{5}{c|}{ZhangDDI} & \multicolumn{5}{c|}{HetionteDDI} & \multicolumn{5}{c}{DrugBankDDI}\\
%         \cline{2-16}
%          & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR & ACC & AUROC & F1 & Pre & AUPR  \\ \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
%           \hline
%         GCN(ICLR'17) & 83.31 & 91.64 & 82.91 & 77.67 & 91.33 & 87.62 & 92.91 & 90.07 & 84.66 & 91.26 & 84.02 & 90.96 & 82.87 & 80.04 & 88.36  \\
%         GAT(ICLR'18) & 84.14 & 92.10 & 83.42 & 78.45 & 91.21 & 87.94 & 94.35 & 89.78 & 83.52 & 92.83 & 84.33 & 91.26 & 83.34 & 80.58 & 89.13  \\
%         MPNN(ICML'17) & 84.56 & 92.34 & 83.32 & 78.15 & 91.46 & 88.73 & \underline{95.13} & 89.52 & 83.66 & 93.21 & 87.30 & 95.90 & 88.53 & 80.73 & 95.08  \\
%         GIN(ICLR'19) & 85.59 & 93.16 & 85.07 & 79.82 & 92.11 & 87.37 & 93.01 & 87.91 & 84.05 & 91.23 & 85.21 & 92.58 & 85.88 & 83.57 & 90.92  \\
%         MIRACLE(WWW'21) & 84.90 & 93.05 & 83.94 & 77.81 & 91.25 & 87.01 & 92.88 & 87.34 & 83.91 & 90.88 & 84.98 & 92.17 & 85.43 & 83.22 & 90.28  \\
%         CIGIN(AAAI'20) & 85.54 & 93.28 & 84.36 & 80.23 & 91.89 & 90.87 & 92.77 & 90.33 & 88.98 & 94.62 & 91.02 & 93.62 & 91.18 & 89.35 & 94.88  \\
%         \hline
%          \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
%           \hline
%         SSI-DDI(BIB'21) & 85.35 & 93.14 & 81.96 & 78.97 & 92.09 & 87.95 & 93.83 & 88.56 & 84.28 & 91.60 & 84.16 & 91.23 & 84.64 & 82.13 & 89.47  \\
%         DSN-DDI(BIB'23) & 86.65 & 91.13 & 87.86 & 84.33 & 86.42 & 91.25 & 95.31 & 91.59 & 89.52 & 94.66 & 90.49 & 96.15 & 90.58 & 89.76 & 95.27  \\
%         SA-DDI(CS'22) & 77.31 & 50.26 & 45.06 & 52.54 & 29.97 & 91.00 & 95.89 & 91.26 & 88.39 & 94.96 & 92.60 & 95.33 & 92.80 & 90.38 & 96.62  \\
%         CMRL(KDD'23) & 86.32 & 93.73 & 87.68 & 84.01 & 91.56 & 91.25 & 93.18 & 91.53 & 91.23 & \underline{95.24} & 92.26 & 95.05 & 92.43 & 90.30 & 95.96  \\
%         CGIB(ICML'23) & 86.36 & 93.78 & 87.24 & 83.91 & 91.88 & 91.08 & 93.26 & 91.35 & 91.55 & 94.89 & 92.37 & 94.98 & 92.03 & 90.65 & 96.11  \\ \hline
%         ReAlignFit & \cellcolor{c1!50}\textbf{89.43} & \underline{95.68} & \cellcolor{c1!50}\textbf{90.88} & \cellcolor{c1!50}\textbf{87.68} & \cellcolor{c1!50}\textbf{93.36} & \cellcolor{c1!50}\textbf{92.39} & \cellcolor{c1!50}\textbf{97.17} & \cellcolor{c1!50}\textbf{92.62} & \cellcolor{c1!50}\textbf{95.48} & \cellcolor{c1!50}\textbf{96.05} & \underline{95.53} & 96.31 & 94.59 & \cellcolor{c1!50}\textbf{94.38} & \underline{97.05} \\ 
%         ReAlignFit$_\text{GCN}$ & 87.62 & \cellcolor{c1!50}\textbf{95.69} & 90.34 & \underline{87.54} & \underline{92.81} & 91.16 & 94.22 & 91.55 & 92.29 & 95.21 & \cellcolor{c1!50}\textbf{95.62} & 97.95 & \cellcolor{c1!50}\textbf{95.62} & \underline{94.37} & \cellcolor{c1!50}\textbf{97.09} \\
%         ReAlignFit$_\text{GAT}$ & \underline{88.35} & 94.22 & \underline{90.41} & 86.92 & 91.76 & 91.66 & 94.56 & 91.76 & 91.97 & 95.03 & 95.35 & \cellcolor{c1!50}\textbf{98.42} & 93.87 & 93.67 & 96.26 \\
%         ReAlignFit$_\text{GIN}$ & 85.23 & 94.35 & 89.34 & 86.89 & 90.67 & \underline{92.12} & 92.95 & \underline{91.91} & \underline{93.04} & 95.23 & 93.33 & \underline{98.09} & \underline{95.01} & 91.58 & 95.82\\
%         \hline
%     \end{tabular}
%     }
% \end{table*}

% % \begin{table*}[htpb]
% % \caption{The performance of ReAlignFit and comparative methods in MI prediction, with the best results highlighted in {\textbf{bold} and the second results highlighted in \underline{underline}. The results of comparison methods are from \citet{lee2023cgibICML,lee2023shiftKDD}.}
% % \label{tab:mi}
% % \small
% %     \centering
% %     % \renewcommand{\arraystretch}{1.04}
% %     % \setlength{\tabcolsep}{1mm}{
% %     \begin{tabular}{lcccccccc}
% %     \hline
% %         \multirow{2}{*}{Model} & \multicolumn{3}{c}{Chromophore} & \multirow{2}{*}{MNSol} & \multirow{2}{*}{FreeSolv} & \multirow{2}{*}{CompSol} & \multirow{2}{*}{Abraham} & \multirow{2}{*}{CombiSolv}\\
% %         \cline{2-4}
% %          & Absorption & Emission & Lifetime &  &  &  &  &  \\
% %           \hline
% %           \multicolumn{9}{l}{\textbf{Substructure} \XSolidBrush}\\
% %           \hline
% %         GCN(ICLR'17) & 25.75$\pm$1.48 & 31.87$\pm$1.70 & 0.866$\pm$0.015 & 0.675$\pm$0.021 & 1.192$\pm$0.042 & 0.389$\pm$0.009 & 0.738$\pm$0.041  & 0.672$\pm$0.022\\
% %         GAT(ICLR'18) & 26.19$\pm$1.44 & 30.90$\pm$1.01 & 0.859$\pm$0.016 & 0.731$\pm$0.007 & 1.280$\pm$0.049 & 0.387$\pm$0.010 & 0.798$\pm$0.038 & 0.662$\pm$0.021\\
% %         MPNN(ICML'17) & 24.43$\pm$1.55 & 30.17$\pm$0.99 & 0.802$\pm$0.024 & 0.682$\pm$0.017 & 1.159$\pm$0.032 & 0.359$\pm$0.011  & 0.601$\pm$0.035 & 0.568$\pm$0.005\\
% %        GIN(ICLR'19) & 24.92$\pm$1.67 & 32.31$\pm$0.26 & 0.829$\pm$0.027 & 0.669$\pm$0.017 & 1.015$\pm$0.041 & 0.331$\pm$0.016 & 0.648$\pm$0.024 & 0.595$\pm$0.014\\
% %        CIGIN(AAAI'20) & 19.32$\pm$0.35 & 25.09$\pm$0.32 & 0.804$\pm$0.010 & 0.607$\pm$0.024 & 0.905$\pm$0.014 & 0.308$\pm$0.018 & 0.411$\pm$0.008 & 0.451$\pm$0.009\\
% %         \hline
% %         \multicolumn{9}{l}{\textbf{Substructure} \Checkmark}\\
% %         \hline
% %        CMRL(KDD'23) & 17.93$\pm$0.31 & 24.30$\pm$0.22 & 0.776$\pm$0.007 & 0.551$\pm$0.017 & 0.815$\pm$0.046 & \cellcolor{c1!50}\textbf{0.255$\pm$0.011} & \underline{0.374$\pm$0.011} & 0.421$\pm$0.008\\
% %        CGIB(ICML'23) & 18.11$\pm$0.20 & 23.90$\pm$0.35 & \underline{0.771$\pm$0.005} & \cellcolor{c1!50}\textbf{0.538$\pm$0.007} & 0.852$\pm$0.022 & 0.276$\pm$0.017 & 0.390$\pm$0.006 & 0.422$\pm$0.005\\
% %         \hline
% %         ReAlignFit & \cellcolor{c1!50}\textbf{16.82$\pm$0.25} & \cellcolor{c1!50}\textbf{22.95$\pm$0.33} & \cellcolor{c1!50}\textbf{0.769$\pm$0.005} & 0.541$\pm$0.010 & \underline{0.799$\pm$0.034} & 0.261$\pm$0.013 & \cellcolor{c1!50}\textbf{0.371$\pm$0.008} & 0.419$\pm$0.008\\
% %         ReAlignFit$_\text{GCN}$ & \underline{17.23$\pm$0.27} & \underline{23.35$\pm$0.29} & 0.771$\pm$0.007 & \underline{0.539$\pm$0.012} & \cellcolor{c1!50}\textbf{0.796$\pm$0.035} & \underline{0.257$\pm$0.016} & 0.375$\pm$0.012 & \cellcolor{c1!50} \textbf{0.416$\pm$0.006}\\
% %         ReAlignFit$_\text{GAT}$ & 17.55$\pm$0.23 & 23.98$\pm$0.36 & 0.776$\pm$0.007 & 0.543$\pm$0.021 & 0.806$\pm$0.036 & \cellcolor{c1!50}\textbf{0.255$\pm$0.012} & 0.381$\pm$0.013 & 0.421$\pm$0.009\\
% %         ReAlignFit$_\text{GIN}$ & 17.92$\pm$0.46 & 24.10$\pm$0.34 & 0.772$\pm$0.007 & 0.552$\pm$0.037 & 0.803$\pm$0.025 & 0.267$\pm$0.021 & 0.386$\pm$0.017 & \underline{0.418$\pm$0.008}\\
% %         \hline
% %     \end{tabular}
% %     % }
% % \end{table*}






% file is intended to serve as a ``sample article file''
% for IEEE journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later. The most common elements are covered in the simplified and updated instructions in ``New\_IEEEtran\_how-to.pdf''. For less common elements you can refer back to the original ``IEEEtran\_HOWTO.pdf''. It is assumed that the reader has a basic working knowledge of \LaTeX. Those who are new to \LaTeX \ are encouraged to read Tobias Oetiker's ``The Not So Short Introduction to \LaTeX ,'' available at: \url{http://tug.ctan.org/info/lshort/english/lshort.pdf} which provides an overview of working with \LaTeX.

% \section{The Design, Intent, and \\ Limitations of the Templates}
% The templates are intended to {\bf{approximate the final look and page length of the articles/papers}}. {\bf{They are NOT intended to be the final produced work that is displayed in print or on IEEEXplore\textsuperscript{\textregistered}}}. They will help to give the authors an approximation of the number of pages that will be in the final version. The structure of the \LaTeX\ files, as designed, enable easy conversion to XML for the composition systems used by the IEEE. The XML files are used to produce the final print/IEEEXplore pdf and then converted to HTML for IEEEXplore.

% \section{Where to Get \LaTeX \ Help --- User Groups}
% The following online groups are helpful to beginning and experienced \LaTeX\ users. A search through their archives can provide many answers to common questions.
% \begin{list}{}{}
% \item{\url{http://www.latex-community.org/}} 
% \item{\url{https://tex.stackexchange.com/} }
% \end{list}

% \section{Other Resources}
% See \cite{ref1,ref2,ref3,ref4,ref5} for resources on formatting math into text and additional help in working with \LaTeX .

% \section{Text}
% For some of the remainer of this sample we will use dummy text to fill out paragraphs rather than use live text that may violate a copyright.

% Itam, que ipiti sum dem velit la sum et dionet quatibus apitet voloritet audam, qui aliciant voloreicid quaspe volorem ut maximusandit faccum conemporerum aut ellatur, nobis arcimus.
% Fugit odi ut pliquia incitium latum que cusapere perit molupta eaquaeria quod ut optatem poreiur? Quiaerr ovitior suntiant litio bearciur?

% Onseque sequaes rectur autate minullore nusae nestiberum, sum voluptatio. Et ratem sequiam quaspername nos rem repudandae volum consequis nos eium aut as molupta tectum ulparumquam ut maximillesti consequas quas inctia cum volectinusa porrum unt eius cusaest exeritatur? Nias es enist fugit pa vollum reium essusam nist et pa aceaqui quo elibusdandis deligendus que nullaci lloreri bla que sa coreriam explacc atiumquos simolorpore, non prehendunt lam que occum\cite{ref6} si aut aut maximus eliaeruntia dia sequiamenime natem sendae ipidemp orehend uciisi omnienetus most verum, ommolendi omnimus, est, veni aut ipsa volendelist mo conserum volores estisciis recessi nveles ut poressitatur sitiis ex endi diti volum dolupta aut aut odi as eatquo cullabo remquis toreptum et des accus dolende pores sequas dolores tinust quas expel moditae ne sum quiatis nis endipie nihilis etum fugiae audi dia quiasit quibus.
% \IEEEpubidadjcol
% Ibus el et quatemo luptatque doluptaest et pe volent rem ipidusa eribus utem venimolorae dera qui acea quam etur aceruptat.
% Gias anis doluptaspic tem et aliquis alique inctiuntiur?

% Sedigent, si aligend elibuscid ut et ium volo tem eictore pellore ritatus ut ut ullatus in con con pere nos ab ium di tem aliqui od magnit repta volectur suntio. Nam isquiante doluptis essit, ut eos suntionsecto debitiur sum ea ipitiis adipit, oditiore, a dolorerempos aut harum ius, atquat.

% Rum rem ditinti sciendunti volupiciendi sequiae nonsect oreniatur, volores sition ressimil inus solut ea volum harumqui to see\eqref{deqn_ex1a} mint aut quat eos explis ad quodi debis deliqui aspel earcius.

% \begin{equation}
% \label{deqn_ex1a}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}

% Alis nime volorempera perferi sitio denim repudae pre ducilit atatet volecte ssimillorae dolore, ut pel ipsa nonsequiam in re nus maiost et que dolor sunt eturita tibusanis eatent a aut et dio blaudit reptibu scipitem liquia consequodi od unto ipsae. Et enitia vel et experferum quiat harum sa net faccae dolut voloria nem. Bus ut labo. Ita eum repraer rovitia samendit aut et volupta tecupti busant omni quiae porro que nossimodic temquis anto blacita conse nis am, que ereperum eumquam quaescil imenisci quae magnimos recus ilibeaque cum etum iliate prae parumquatemo blaceaquiam quundia dit apienditem rerit re eici quaes eos sinvers pelecabo. Namendignis as exerupit aut magnim ium illabor roratecte plic tem res apiscipsam et vernat untur a deliquaest que non cus eat ea dolupiducim fugiam volum hil ius dolo eaquis sitis aut landesto quo corerest et auditaquas ditae voloribus, qui optaspis exero cusa am, ut plibus.


% \section{Some Common Elements}
% \subsection{Sections and Subsections}
% Enumeration of section headings is desirable, but not required. When numbered, please be consistent throughout the article, that is, all headings and all levels of section headings in the article should be enumerated. Primary headings are designated with Roman numerals, secondary with capital letters, tertiary with Arabic numbers; and quaternary with lowercase letters. Reference and Acknowledgment headings are unlike all other section headings in text. They are never enumerated. They are simply primary headings without labels, regardless of whether the other headings in the article are enumerated. 

% \subsection{Citations to the Bibliography}
% The coding for the citations is made with the \LaTeX\ $\backslash${\tt{cite}} command. 
% This will display as: see \cite{ref1}.

% For multiple citations code as follows: {\tt{$\backslash$cite\{ref1,ref2,ref3\}}}
%  which will produce \cite{ref1,ref2,ref3}. For reference ranges that are not consecutive code as {\tt{$\backslash$cite\{ref1,ref2,ref3,ref9\}}} which will produce  \cite{ref1,ref2,ref3,ref9}

% \subsection{Lists}
% In this section, we will consider three types of lists: simple unnumbered, numbered, and bulleted. There have been many options added to IEEEtran to enhance the creation of lists. If your lists are more complex than those shown below, please refer to the original ``IEEEtran\_HOWTO.pdf'' for additional options.\\

% \subsubsection*{\bf A plain  unnumbered list}
% \begin{list}{}{}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{list}

% \subsubsection*{\bf A simple numbered list}
% \begin{enumerate}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{enumerate}

% \subsubsection*{\bf A simple bulleted list}
% \begin{itemize}
% \item{bare\_jrnl.tex}
% \item{bare\_conf.tex}
% \item{bare\_jrnl\_compsoc.tex}
% \item{bare\_conf\_compsoc.tex}
% \item{bare\_jrnl\_comsoc.tex}
% \end{itemize}





% \subsection{Figures}
% Fig. 1 is an example of a floating figure using the graphicx package.
%  Note that $\backslash${\tt{label}} must occur AFTER (or within) $\backslash${\tt{caption}}.
%  For figures, $\backslash${\tt{caption}} should occur after the $\backslash${\tt{includegraphics}}.

% \begin{figure}[!t]
% \centering
% \includegraphics[width=2.5in]{fig1}
% \caption{Simulation results for the network.}
% \label{fig_1}
% \end{figure}

% Fig. 2(a) and 2(b) is an example of a double column floating figure using two subfigures.
%  (The subfig.sty package must be loaded for this to work.)
%  The subfigure $\backslash${\tt{label}} commands are set within each subfloat command,
%  and the $\backslash${\tt{label}} for the overall figure must come after $\backslash${\tt{caption}}.
%  $\backslash${\tt{hfil}} is used as a separator to get equal spacing.
%  The combined width of all the parts of the figure should do not exceed the text width or a line break will occur.
% %
% \begin{figure*}[!t]
% \centering
% \subfloat[]{\includegraphics[width=2.5in]{fig1}%
% \label{fig_first_case}}
% \hfil
% \subfloat[]{\includegraphics[width=2.5in]{fig1}%
% \label{fig_second_case}}
% \caption{Dae. Ad quatur autat ut porepel itemoles dolor autem fuga. Bus quia con nessunti as remo di quatus non perum que nimus. (a) Case I. (b) Case II.}
% \label{fig_sim}
% \end{figure*}

% Note that often IEEE papers with multi-part figures do not place the labels within the image itself (using the optional argument to $\backslash${\tt{subfloat}}[]), but instead will
%  reference/describe all of them (a), (b), etc., within the main caption.
%  Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
%  labels, the optional argument to $\backslash${\tt{subfloat}} must be present. If a
%  subcaption is not desired, leave its contents blank,
%  e.g.,$\backslash${\tt{subfloat}}[].


 

% \section{Tables}
% Note that, for IEEE-style tables, the
%  $\backslash${\tt{caption}} command should come BEFORE the table. Table captions use title case. Articles (a, an, the), coordinating conjunctions (and, but, for, or, nor), and most short prepositions are lowercase unless they are the first or last word. Table text will default to $\backslash${\tt{footnotesize}} as
%  the IEEE normally uses this smaller font for tables.
%  The $\backslash${\tt{label}} must come after $\backslash${\tt{caption}} as always.
 
% \begin{table}[!t]
% \caption{An Example of a Table\label{tab:table1}}
% \centering
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{table}

% \section{Algorithms}
% Algorithms should be numbered and include a short title. They are set off from the text with rules above and below the title and after the last line.

% \begin{algorithm}[H]
% \caption{Weighted Tanimoto ELM.}\label{alg:alg1}
% \begin{algorithmic}
% \STATE 
% \STATE {\textsc{TRAIN}}$(\mathbf{X} \mathbf{T})$
% \STATE \hspace{0.5cm}$ \textbf{select randomly } W \subset \mathbf{X}  $
% \STATE \hspace{0.5cm}$ N_\mathbf{t} \gets | \{ i : \mathbf{t}_i = \mathbf{t} \} | $ \textbf{ for } $ \mathbf{t}= -1,+1 $
% \STATE \hspace{0.5cm}$ B_i \gets \sqrt{ \textsc{max}(N_{-1},N_{+1}) / N_{\mathbf{t}_i} } $ \textbf{ for } $ i = 1,...,N $
% \STATE \hspace{0.5cm}$ \hat{\mathbf{H}} \gets  B \cdot (\mathbf{X}^T\textbf{W})/( \mathbb{1}\mathbf{X} + \mathbb{1}\textbf{W} - \mathbf{X}^T\textbf{W} ) $
% \STATE \hspace{0.5cm}$ \beta \gets \left ( I/C + \hat{\mathbf{H}}^T\hat{\mathbf{H}} \right )^{-1}(\hat{\mathbf{H}}^T B\cdot \mathbf{T})  $
% \STATE \hspace{0.5cm}\textbf{return}  $\textbf{W},  \beta $
% \STATE 
% \STATE {\textsc{PREDICT}}$(\mathbf{X} )$
% \STATE \hspace{0.5cm}$ \mathbf{H} \gets  (\mathbf{X}^T\textbf{W} )/( \mathbb{1}\mathbf{X}  + \mathbb{1}\textbf{W}- \mathbf{X}^T\textbf{W}  ) $
% \STATE \hspace{0.5cm}\textbf{return}  $\textsc{sign}( \mathbf{H} \beta )$
% \end{algorithmic}
% \label{alg1}
% \end{algorithm}

% Que sunt eum lam eos si dic to estist, culluptium quid qui nestrum nobis reiumquiatur minimus minctem. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. Ulparci sunt, qui doluptaquis sumquia ndestiu sapient iorepella sunti veribus. Ro moluptat fuga. Itatquiam ut laborpo rersped exceres vollandi repudaerem. 
% \section{Mathematical Typography \\ and Why It Matters}

% Typographical conventions for mathematical formulas have been developed to {\bf provide uniformity and clarity of presentation across mathematical texts}. This enables the readers of those texts to both understand the author's ideas and to grasp new concepts quickly. While software such as \LaTeX \ and MathType\textsuperscript{\textregistered} can produce aesthetically pleasing math when used properly, it is also very easy to misuse the software, potentially resulting in incorrect math display.

% IEEE aims to provide authors with the proper guidance on mathematical typesetting style and assist them in writing the best possible article. As such, IEEE has assembled a set of examples of good and bad mathematical typesetting \cite{ref1,ref2,ref3,ref4,ref5}. 

% Further examples can be found at \url{http://journals.ieeeauthorcenter.ieee.org/wp-content/uploads/sites/7/IEEE-Math-Typesetting-Guide-for-LaTeX-Users.pdf}

% \subsection{Display Equations}
% The simple display equation example shown below uses the ``equation'' environment. To number the equations, use the $\backslash${\tt{label}} macro to create an identifier for the equation. LaTeX will automatically number the equation for you.
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation}
% \label{deqn_ex1}
% x = \sum_{i=0}^{n} 2{i} Q.
% \end{equation}
% \end{verbatim}

% To reference this equation in the text use the $\backslash${\tt{ref}} macro. 
% Please see (\ref{deqn_ex1})\\
% \noindent is coded as follows:
% \begin{verbatim}
% Please see (\ref{deqn_ex1})\end{verbatim}

% \subsection{Equation Numbering}
% {\bf{Consecutive Numbering:}} Equations within an article are numbered consecutively from the beginning of the
% article to the end, i.e., (1), (2), (3), (4), (5), etc. Do not use roman numerals or section numbers for equation numbering.

% \noindent {\bf{Appendix Equations:}} The continuation of consecutively numbered equations is best in the Appendix, but numbering
%  as (A1), (A2), etc., is permissible.\\

% \noindent {\bf{Hyphens and Periods}}: Hyphens and periods should not be used in equation numbers, i.e., use (1a) rather than
% (1-a) and (2a) rather than (2.a) for subequations. This should be consistent throughout the article.

% \subsection{Multi-Line Equations and Alignment}
% Here we show several examples of multi-line equations and proper alignments.

% \noindent {\bf{A single equation that must break over multiple lines due to length with no specific alignment.}}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}

% \noindent is coded as:
% \begin{verbatim}
% \begin{multline}
% \text{The first line of this example}\\
% \text{The second line of this example}\\
% \text{The third line of this example}
% \end{multline}
% \end{verbatim}

% \noindent {\bf{A single equation with multiple lines aligned at the = signs}}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% a &= c+d \\
% b &= e+f
% \end{align}
% \end{verbatim}

% The {\tt{align}} environment can align on multiple  points as shown in the following example:
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \noindent is coded as:
% \begin{verbatim}
% \begin{align}
% x &= y & X & =Y & a &=bc\\
% x' &= y' & X' &=Y' &a' &=bz
% \end{align}
% \end{verbatim}





% \subsection{Subnumbering}
% The amsmath package provides a {\tt{subequations}} environment to facilitate subnumbering. An example:

% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \noindent is coded as:
% \begin{verbatim}
% \begin{subequations}\label{eq:2}
% \begin{align}
% f&=g \label{eq:2A}\\
% f' &=g' \label{eq:2B}\\
% \mathcal{L}f &= \mathcal{L}g \label{eq:2c}
% \end{align}
% \end{subequations}

% \end{verbatim}

% \subsection{Matrices}
% There are several useful matrix environments that can save you some keystrokes. See the example coding below and the output.

% \noindent {\bf{A simple matrix:}}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{matrix}  0 &  1 \\ 
% 1 &  0 \end{matrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with parenthesis}}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{pmatrix} 0 & -i \\
%  i &  0 \end{pmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with square brackets}}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{bmatrix} 0 & -1 \\ 
% 1 &  0 \end{bmatrix}
% \end{equation}
% \end{verbatim}

% \noindent {\bf{A matrix with curly braces}}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Bmatrix} 1 &  0 \\ 
% 0 & -1 \end{Bmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with single verticals}}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{vmatrix} a &  b \\ 
% c &  d \end{vmatrix}
% \end{equation}\end{verbatim}

% \noindent {\bf{A matrix with double verticals}}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \begin{Vmatrix} i &  0 \\ 
% 0 & -i \end{Vmatrix}
% \end{equation}\end{verbatim}

% \subsection{Arrays}
% The {\tt{array}} environment allows you some options for matrix-like equations. You will have to manually key the fences, but there are other options for alignment of the columns and for setting horizontal and vertical rules. The argument to {\tt{array}} controls alignment and placement of vertical rules.

% A simple array
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccc}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% A slight variation on this to better align the numbers in the last column
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{cccr}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}

% An array with vertical and horizontal rules
% \begin{equation}
% \left( \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\ \hline
% a+b & u+v & z & 134
% \end{array}\right)
% \end{equation}
% is coded as:
% \begin{verbatim}
% \begin{equation}
% \left(
% \begin{array}{c|c|c|r}
% a+b+c & uv & x-y & 27\\
% a+b & u+v & z & 134
% \end{array} \right)
% \end{equation}
% \end{verbatim}
% Note the argument now has the pipe "$\vert$" included to indicate the placement of the vertical rules.


% \subsection{Cases Structures}
% Many times cases can be miscoded using the wrong environment, i.e., {\tt{array}}. Using the {\tt{cases}} environment will save keystrokes (from not having to type the $\backslash${\tt{left}}$\backslash${\tt{lbrace}}) and automatically provide the correct column alignment.
% \begin{equation*}
% {z_m(t)} = \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t) \\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*}
% {z_m(t)} = 
% \begin{cases}
% 1,&{\text{if}}\ {\beta }_m(t),\\ 
% {0,}&{\text{otherwise.}} 
% \end{cases}
% \end{equation*}
% \end{verbatim}
% \noindent Note that the ``\&'' is used to mark the tabular alignment. This is important to get  proper column alignment. Do not use $\backslash${\tt{quad}} or other fixed spaces to try and align the columns. Also, note the use of the $\backslash${\tt{text}} macro for text elements such as ``if'' and ``otherwise.''

% \subsection{Function Formatting in Equations}
% Often, there is an easy way to properly format most common functions. Use of the $\backslash$ in front of the function name will in most cases, provide the correct formatting. When this does not work, the following example provides a solution using the $\backslash${\tt{text}} macro:

% \begin{equation*} 
%   d_{R}^{KM} = \underset {d_{l}^{KM}} {\text{arg min}} \{ d_{1}^{KM},\ldots,d_{6}^{KM}\}.
% \end{equation*}

% \noindent is coded as follows:
% \begin{verbatim}
% \begin{equation*} 
%  d_{R}^{KM} = \underset {d_{l}^{KM}} 
%  {\text{arg min}} \{ d_{1}^{KM},
%  \ldots,d_{6}^{KM}\}.
% \end{equation*}
% \end{verbatim}

% \subsection{ Text Acronyms Inside Equations}
% This example shows where the acronym ``MSE" is coded using $\backslash${\tt{text\{\}}} to match how it appears in the text.

% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}(Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}

% \begin{verbatim}
% \begin{equation*}
%  \text{MSE} = \frac {1}{n}\sum _{i=1}^{n}
% (Y_{i} - \hat {Y_{i}})^{2}
% \end{equation*}
% \end{verbatim}

% \section{Conclusion}
% The conclusion goes here.


% % \section*{Acknowledgments}
% % This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



% % {\appendix[Proof of the Zonklar Equations]
% % Use $\backslash${\tt{appendix}} if you have a single appendix:
% % Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% % If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% % You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
% %  starts a section numbered zero.)}



% % %{\appendices
% % %\section*{Proof of the First Zonklar Equation}
% % %Appendix one text goes here.
% % % You can choose not to have a title for an appendix if you want by leaving the argument blank
% % %\section*{Proof of the Second Zonklar Equation}
% % %Appendix two text goes here.}



% \section{References Section}
% You can use a bibliography generated by BibTeX as a .bbl file.
%  BibTeX documentation can be easily obtained at:
%  http://mirror.ctan.org/biblio/bibtex/contrib/doc/
%  The IEEEtran BibTeX style support page is:
%  http://www.michaelshell.org/tex/ieeetran/bibtex/
 
%  % argument is your BibTeX string definitions and bibliography database(s)
% %\bibliography{IEEEabrv,../bib/paper}
% %
% \section{Simple References}
% You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
%  (used to reserve space for the reference number labels box).























% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{references}

\vspace{-20mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/Peilaing-Zhang.jpg}}]{Peiliang Zhang} (Student Member, IEEE) is pursuing his Ph.D. degree at Wuhan University of Technology, Wuhan, China. He is also currently a visiting Ph.D. student at Yonsei University, Republic of Korea. Before that, in 2022, he received his M.S. degree in software engineering from Dalian University, Dalian, China. His research interests include AI for Science, Molecular Representation and Medical Informatics.\end{IEEEbiography}
 
 \vspace{-20mm}
 \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/Jingling-Yuan.jpg}}]{Jingling Yuan} (Senior Member, IEEE) received the Ph.D. degree from the Wuhan University of Technology, China, in 2004. She is currently a Professor with the School of Computer Science and Artificial Intelligence, Wuhan University of Technology. She was a Visiting Scholar with the University of Florida from 2008 to 2009, and a Research Scholar with the University of Bristol in 2018. Her research interests include Machine Learning, Green Computing and AI for Science.
 
 She has published 80+ publications such as AAAI, ACM MM, ECCV, WSDM, DASFAA, IEEE TMM, ACM TKDD and so on.
 \end{IEEEbiography}

\vspace{-20mm}
 \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/xieqing.png}}]{Qing Xie} is currently an Associate Professor in School of Computer Science and Artificial Intelligence at Wuhan University of Technology. He received the BE degree in information science from University of Science and Technology of China in 2008, and the Ph.D. degree in computer science from the University of Queensland in 2013. After that, he worked as a postdoctoral research fellow at King Abdullah University of Science and Technology. His research interests include Multimedia Analysis, Recommender System and Knowledge Engineering.
 \end{IEEEbiography}
 
\vspace{-20mm}
 \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/zhuyongjun.png}}]{Yongjun Zhu} received his Ph.D. from Drexel University and he is currently an Associate Professor of the Department of Library and Information Science at Yonsei University, Republic of Korea. He researchers broadly Science of Science, Mental Health Informatics, and Philosophical and Societal Aspects of Artificial Intelligence.
 \end{IEEEbiography}

\vspace{-20mm}
 \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figure/Lin-Li.png}}]{Lin Li} (Senior Member, IEEE) 
 received her Ph.D. from The University of Tokyo, Japan in 2009. She is currently a Professor at the School of Computer Science and Artificial Intelligence, Wuhan University of Technology, China. Her research interests cover Cnformation Retrieval, Recommender System, and Multimodal Machine Learning. 

She has published 100+ publications such as IJCAI, AAAI, WWW, ICDM, SIGIR, ICMR, CIKM, DASFAA, ACM TOIS, ACM TOIT, ACM TIST, IEEE TSC, IEEE TKDE, IEEE TCSS and so on.
\end{IEEEbiography}



% \clearpage
% \section*{Appendix}
% % \section{Proof of Formula}
% \subsection{The proof of Eq. (\ref{eq:op2})}

% For the term $\mathcal{I}_{ca}=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$, by definition:
% \begin{equation}
%     \mathcal{I}_{ca} \!\!=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\!\! p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})\! \log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{p(\mathcal{Y})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}
% \end{equation}

% % For the term $\mathcal{I}_{ca}=\mathcal{I}(\mathcal{Y};\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})$, 
% % , by definition:
% % \begin{equation}
% %     I_{ca} \!\!=\!\!\!\int\!\!\!\!\!\int\!\!\!\!\!\int\!\!\! p(Y,G_{x}^{c},G_{y}^{c})\! \log (\frac{p(Y,G_{x}^{c},G_{y}^{c})}{p(Y)})dY\!dG_{x}^{c}dG_{y}^{c}
% % \end{equation}
% We introduce the variational approximation distribution $q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$ to approximate the conditional probability distribution $p(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})$. Then, the $\mathcal{I}_{ca}$ is expressed as:
% % 引入变分近似分布A来近似条件概率分布B，我们可以得出以下等式：
% \begin{equation}
% \begin{aligned}
%     \mathcal{I}_{ca}\!\!&
%     = \!\!\!\!\int\!\!\!p(\mathcal{Y})\!\!\!\int\!\!\!\!\! \int\!\!\! q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})\log (\frac{p(\mathcal{Y},\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})}{q(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c})})d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
%     &=\!\!\!\!\int\!\!\!p(\!\mathcal{Y}\!)\!\!\!\int\!\!\!\!\! \int\!\!\! q(\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c}|\mathcal{Y}\!)\!\log (\!\frac{p(\mathcal{Y})p(\mathcal{G}_{x}^{c}\!,\!\mathcal{G}_{y}^{c}|\mathcal{Y})}{q(\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c})}\!)d\mathcal{Y}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}\\
%     &=\!\!\!\!\int\!\!p(\mathcal{Y})(\log p(\mathcal{Y})+\!\!\!\int\!\!\!\!\! \int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}\\
%     &=\!\!\!\!\int\!\!p(\mathcal{Y})\log p(\mathcal{Y})d\mathcal{Y}\\
%     &+\!\!\!\!\int\!\!p(\mathcal{Y})\!\!\!\int\!\!\!\!\! \int\!\!{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c})d\mathcal{Y}\\
%     &=\!H(\mathcal{Y})\!\!+\!\!\!\int\!\!{p(\mathcal{Y})\!\!\iint\!\!{{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}
% \end{aligned}
% \end{equation}

% $H(\mathcal{Y})$ is the entropy of $\mathcal{Y}$. Finally, we have the following equation as:
% \begin{equation}
% \mathcal{I}(\mathcal{Y};\!\mathcal{G}_{x}^{c},\!\mathcal{G}_{y}^{c})\!\ge\!\! \int\!\!{p(\mathcal{Y})\!\!\int\!\!\!\!\!\int\!\!{{\mathcal{F}_{ca}(\mathcal{G}_{x}^{c},\mathcal{G}_{y}^{c}|\mathcal{Y})}}d\mathcal{G}_{x}^{c}d\mathcal{G}_{y}^{c}d\mathcal{Y}}
% \end{equation}


% \subsection{The proof of Eq. (\ref{eq:op1})}
% % % $I_c=I(G^c,G)$表示为积分形式：
% % The $\mathcal{I}(\mathcal{G}^c,\mathcal{G})$ is expressed in integral form as:
% % \begin{equation}
% % \label{eq:op1-1}
% % \begin{aligned}
% % \mathcal{I}(\mathcal{G}^c,\mathcal{G})=\iint p(\mathcal{G}^c,\mathcal{G})\log(\frac{p(\mathcal{G}^c,\mathcal{G})}{p(\mathcal{G}^c)p(\mathcal{G})})d\mathcal{G}^cd\mathcal{G}
% % % \\
% % % &=-H(G)-H(G^c|G).
% % \end{aligned}
% % \end{equation}
% % 我们利用公式\ref{eq:nos}中的概率函数$R$调整条件概率分布$p(G^c,G)$，且$G^c$、$G^n$条件独立于$G$。因此，可以得到：
% The $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ is expressed in integral form as:
% \begin{equation}
% \label{eq:op1-1}
% \begin{aligned}
% \mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)&=\int \!\!\!\! \int p(\mathcal{G}^c_x,\mathcal{G}_x)\log(\frac{p(\mathcal{G}^c_x,\mathcal{G}_x)}{p(\mathcal{G}^c_x)p(\mathcal{G}_x)})d\mathcal{G}^c_xd\mathcal{G}_x
% % \\
% % &=-H(G)-H(G^c|G).
% \end{aligned}
% \end{equation}

% The probability function $\gamma$ in Eq. (\ref{align}) is utilized to adjust the conditional probability distribution $p(\mathcal{G}^c_x,\mathcal{G}_x)$, and $\mathcal{G}^c_x$, $\mathcal{G}^n_x$ are conditionally independent of $\mathcal{G}_x$. We can obtain $p(\mathcal{G}^c_x|\mathcal{G}_x)=p(\mathcal{G}^c_x|\mathcal{G}_x,\gamma)$.
% % 则公式\ref{eq:op1-1}进一步表示为：
% Further, $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ is expressed as:
% \begin{equation}
% \begin{aligned}
% \mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)&\!\!=\!\!\int \!\!\!\! \int \!\! p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma) \log(\frac{p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma)}{p(\mathcal{G}^c_x)}) d\mathcal{G}_x^cd\mathcal{G}_x\\
% &:= {KL}(p(\mathcal{G}^c_x|\mathcal{G}_x, \! \gamma)||p(\mathcal{G}^c_x))\\
% & = \mathcal{L}_{KL_x}^c
% \end{aligned}
% \label{eq:op1-2}
% \end{equation}

% The result of Eq. (\ref{eq:op1-2}) indicates that $\mathcal{I}(\mathcal{G}^c_x,\mathcal{G}_x)$ can be computed by the KL divergence between $\mathcal{G}^c_x$ and $\mathcal{G}_x$. Similarly, $\mathcal{I}(\mathcal{G}^n_x,\mathcal{G}_x)={KL}(p(\mathcal{G}^n_x|\mathcal{G}_x,\gamma)||p(\mathcal{G}^n_x))=\mathcal{L}_{KL_x}^n$.
% % $I_{c}$可用$G^c$和$G$之间的KL散度计算得到。同理我们可以得到$I_{n}=I(G^n,G)=\mathcal{L}_{KL}^n(G^n,G)$.

% Finally, we have the following equation as:
% \begin{equation}
%     \mathcal{I}(\mathcal{G}^c_x,\mathcal{G}^n_x)\le \min (\mathcal{L}_{KL_x}^c,\mathcal{L}_{KL_x}^n)
%     % (\mathcal{L}_{KL}^c(\mathcal{G}^c_x,\mathcal{G}_x),\mathcal{L}_{KL}^n(\mathcal{G}^n_x,\mathcal{G}_x))
% \end{equation}

\end{document}


