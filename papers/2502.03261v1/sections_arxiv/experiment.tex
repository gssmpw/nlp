\section{Routing in benchmark case-studies}
\label{sec:routing-application}
We use CARROT (Algorithm \ref{alg:pareto-routers}) to perform routing on several benchmark datasets. %task on two benchmarks: {RouterBench} \citep{hu2024routerbench} and {Open LLM leaderboard v2} \cite{open-llm-leaderboard-v2}. The broad question that we address here is to understand the performance vs. cost trade-off in LLM predictions. 
% \SM{revisit this once we have all the expt. details.}


\subsection{Datasets} \label{sec:datasets}

%\begin{wrapfigure}[17]{r}{0.5\textwidth} % 'r' for right, '0.4\textwidth' for width

%\vspace{-0.3cm}
    %\centering
    %\includegraphics[width=\linewidth]{plots/open-llm.pdf} % Replace with your image file
    %\caption{This is a wrapped figure.}
    %\label{fig:wrapped}
    %\label{fig:open-llm-leaderboard}
%\end{wrapfigure}


\paragraph{RouterBench:} RouterBench \citep{hu2024routerbench} is a benchmark dataset for routing tasks consisting of approximately 30k prompts and responses from eleven ($M = 11$) different LLMs.  The data includes prompts from 8 benchmarks covering commonsense reasoning, knowledge-based understanding, conversation, math, and coding. %Paired with each question are model responses, estimated costs associated with the responses (which need to be predicted at test time), and finally a performance score on whether a model got the answer correct. All prompts have a single correct answer, and LLM predictions are compared with that correct answer to obtain a performance score as $0/1$, where $1$ corresponds to a correct answer.  
\paragraph{Open LLM leaderboard:} The {Open LLM leaderboard v2}\footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard}} \citep{open-llm-leaderboard-v2} is an open-source benchmarking platform that comprises responses and evaluations of a collection of LLMs on six benchmarks comprising a diverse collection of tasks. %As before, the dataset includes the questions, responses from LLMs, and their evaluation scores as $0/1$ based on whether the answer was correct. 
%For this dataset, we focus on the scenario where cost can be deterministically computed for each LLM once we know the input prompt; this scenario is a good approximation to situations in which LLMs are required to sample just a few tokens to accomplish the target task. To compute the cost of an LLM query, we tokenize the input prompt and compute the cost according to the guideline of TogetherAI \footnote{\url{https://www.together.ai/pricing}}. We use $M=18$ different open language models both available in the Open LLM leaderboard v2 and on TogetherAI, including model families such as LLaMa-3.1, Qwen2.5, and Gemma 2. More related details are provided in the Appendix \ref{append:llms_open}. 
%We focus on the following five benchmarks (around 21k examples) from the Open LLM leaderboard v2, which comprise multiple-choice questions: MATH Lvl 5 \citep{hendrycks2021measuringmathematicalproblemsolving}, MMLU-PRO \citep{wang2024mmluprorobustchallengingmultitask}, BBH \citep{suzgun2022challengingbigbenchtaskschainofthought}, GPQA \citep{rein2023gpqagraduatelevelgoogleproofqa}, MUSR \citep{sprague2023musr}. These five benchmarks are varied and designed to simulate real-world scenarios where LLMs encounter a wide range of prompts. MATH Lvl 5 focuses solely on mathematical word problems, whereas MMLU-PRO and GPQA include both mathematical and advanced science questions. In contrast, BBH emphasizes logical deduction and linguistic reasoning challenges. Lastly, MuSR serves as a benchmark for assessing multistep soft reasoning tasks framed within natural language narratives. In situations in which queries are diverse, the router can explore the strengths and weaknesses of each language model.
\paragraph{\newdata:} We introduce (and evaluate CARROT on) \newdata, a large and diverse dataset designed for training and evaluating routers. \newdata\ integrates 13 state-of-the-art language models and prompts from 6 benchmarks, including GPQA \citep{rein2023gpqagraduatelevelgoogleproofqa}, MuSR \citep{sprague2024musrtestinglimitschainofthought}, MMLU-Pro \citep{wang2024mmluprorobustchallengingmultitask}, MATH \citep{hendrycks2021measuringmathematicalproblemsolving}, OpenHermes \citep{teknium_openhermes_2.5}, and RAGBench \citep{friel2025ragbenchexplainablebenchmarkretrievalaugmented}. Compared to existing routing benchmarks such as RouterBench, \newdata\ offers several key advantages: 
\begin{enumerate}
    \item \newdata\ encompasses a highly diverse set of questions, including instruction queries.
    % , which is feasible due to the evaluation based on LLaMa-3.1-70b-Instruct. 
    \item Unlike previous benchmarks, it does not rely on few-shot prompting and utilizes chat templates appropriate for each model, making it more representative of real-world use cases.
    \item It leverages LLaMa-3.1-70b-Instruct \citep{grattafiori2024llama3herdmodels} to evaluate LLM responses against the ground truth, similarly to \citet{ni2024mixeval}. This is crucial for evaluating on open-ended instruction queries as well as mitigating errors associated with traditional automatic evaluation methods like exact match.
    % \footnote{Without in-context examples and when using chat templates LLMs tend to perform better, but exact match evaluation is no longer reliable due to varying answer styles of LLMs.}
    \item We provide input and output token counts for each LLM-prompt pair, enabling flexibility when conducting cost-aware analysis.
\end{enumerate}
We have released the \newdata\ in {huggingface}\footnote{\url{https://huggingface.co/datasets/CARROT-LLM-Routing/SPROUT}} and will open-source a platform that allows practitioners to extend \newdata\ by adding new queries and seamlessly evaluating state-of-the-art models on them. For further details, please refer to Appendix \ref{sec:append:sprout}.
% First,  Second,  Finally, 



%\SM{comment about the diversity in task in these two benchmarks? and how router can help? Connect this to intro.}

\subsection{Estimating the oracle router}
CARROT requires an estimate for the function $\Phi^\star_m(X) = \Ex_P[ Y_m\mid X]$. In our benchmark tasks, $Y_m$ is 2-dimensional, consisting of model performance measured as accuracy and model cost measured in dollars. In the RouterBench and \newdata\ datasets, the cost is treated as unknown, while in the Open LLM leaderboard v2, most evaluations are likelihood-based; thus, cost is essentially the length of the input, i.e., a known function. In all cases, the accuracy $Y_{\text{acc}, m}$ is binary, and thus %thus $\Phi^\star_m(X) = P(Z_m = 1\mid X)$.  
we can view its estimation as a binary classification problem, where our objective is to predict the probability that $m$-th model will answer the question $X$ correctly, \ie\ $P_m(X) = P(Y_{\text{acc}, m} = 1\mid X)$) $Y_{\text{acc}, m}$. Using this intuition, we train several multi-label classification models $\widehat P: \cX \to [0, 1]^M$ on a training data split consisting of $80\%$ of the full dataset, where the $m$-th coordinate of $\widehat{P}(X)$ is the predicted probability that $m$-th model accurately answers the question $X$. %, \ie\ $\widehat \Phi_m(X) = \widehat P(Z_m = 1\mid X)$. 
To train $\widehat P$ we consider two procedures: 
\begin{enumerate}
    \item  {\bf CARROT (KNN):} We embed the model inputs using the {text-embedding-3-small} model from OpenAI \citep{openai2023textembedding3small}. On these text embeddings, we train a multi-label K-nearest-neighbors (KNN) classifier.

    \item {\bf CARROT (Roberta):} We fine-tune the pre-trained weights of the {{roberta-base}}\footnote{\url{https://huggingface.co/FacebookAI/roberta-base}} architecture. In order to enhance efficiency, across $m$ we allow $\hat{P}$ to share the same network parameters, except for the final classification layer.
    
\end{enumerate}
%(i) We fine-tune the pre-trained weights of the {\texttt{roberta-base}}\footnote{https://huggingface.co/FacebookAI/\texttt{roberta-base}} architecture; to enhance efficiency across $m$ we allow $\hat{P}$ to share the same network parameters, except for the final classification layer. (ii) %{\texttt{roberta-base}}\footnote{https://huggingface.co/FacebookAI/\texttt{roberta-base}} and %\texttt{bert-base-case}\footnote{https://huggingface.co/google-bert/bert-base-uncased}. 
%o further enhance our predictive ability for model correctness, we take a simple multi-task learning approach \citep{caruana1993multitask} to train the $\widehat \Phi_m$'s; they share the same network parameters, except for the final classification layer, which is based on the assumption is that the representation at the penultimate layers for $\widehat \Phi_m$'s are identical. We utilize a \texttt{HuggingFace} API framework \footnote{https://huggingface.co/docs/transformers/en/tasks/sequence\_classification} for training the $\widehat\Phi$, whose exact choices of hyperparameters are provided in Appendix \ref{}. 

As mentioned above, in the RouterBench and \newdata\ task the cost remains to be estimated. We train multi-label regression models $\widehat C: \cX \to \reals^M$, where $\hat{C}_m(X) =\mathbb{E}[Y_{\text{cost}, m}|X]$ is the estimated cost of calling model $m$ for query $X$. In the case of \newdata\ we actually estimate the input and output token count, and convert this to a cost using collected pricing numbers (see Table \ref{tab:price_by_token_newdata}). Depending on the technique used for performance estimation, we use either the {roberta-base} or text-embedder plus KNN strategy outlined above for fitting $\hat{C}$ (note that the models may be altered for a multi-output regression task, but otherwise remain identical).  In the case of Open LLM leaderboard v2, we compute the cost of input $X$ by calculating its length measured as the number of input tokens and retrieving the price per token from TogetherAI (see Table  \ref{tab:llms_open} for prices per 1M of input tokens).

Having estimated the $\widehat \Phi_m$'s, we use them in Algorithm \ref{alg:pareto-routers} to estimate the oracle routers and predict their performances vs costs on the remaining $20\%$ test split of the dataset. These cost-accuracy tradeoff curves are provided in Figure \ref{fig:RouterBench-main} (for RouterBench), Figure  \ref{fig:rexp_open_llm} (Open LLM leaderboard v2), and Figure \ref{fig:ROOT In Distribution} (for \newdata) along with the accuracies and costs of the individual LLMs within the dataset. 

%\SM{also plot how many $\%$ of times GPT4 was called by the router?}

% Our Algorithm \ref{alg:pareto-routers} requires an estimate for the loss regression function $\Phi^\star_m(X) = \Ex_P[\ell\{ Y, f_m (X)\}\mid X]$. Furthermore, a key takeaway from our rate upper bound analysis in Section \ref{sec:upper-bound} is that the performance of our estimated routers depends on the rate of convergence for estimating $\Phi^\star_m(X) = \Ex_P[\ell\{ Y, f_m (X)\}\mid X]$. Alternatively speaking, we need to estimate $\Phi^\star_m(X)$ accurately. 

% Returning to our benchmark case-studies, the $Z_m = \ell\{ Y, f_m (X)\}$'s are binary, and thus $\Phi^\star_m(X) = P(Z_m = 1\mid X)$.  Therefore, we can view the estimation of $\Phi^\star_m(X)$ as a binary classification problem, where our objective is to predict whether the model $m$ will answer the question $X$ correctly. Using this idea, we train a multi-label classification model $\widehat \Phi: \cX \to [0, 1]^M$ on a training datasplit consisting of $80\%$ of the full dataset, where the $m$-th coordinate of $\widehat \Phi(X)$ is the predicted probability that $m$-th model accurately answers the question $X$, \ie\ $\widehat \Phi_m(X) = \widehat P(Z_m = 1\mid X)$. We train $\widehat\Phi$ with \texttt{\texttt{roberta-base}}, \texttt{bert-base-case} architectures. To further enhance our predictive ability for model correctness, we take a simple multi-task learning approach \citep{caruana1993multitask} to train the $\widehat \Phi_m$'s; they share the same network parameters, except for the final classification layer. 

% Having estimated the $\widehat \Phi_m$'s we use them in our Algorithm \ref{alg:pareto-routers} to estimate the Pareto routers and predict their performances vs costs on the remaining $20\%$ test split of the dataset. These accuracy-cost trade-off curves are provided in Figure \ref{}, along with the accuracies and costs of the individual LLMs within the dataset. 

\subsection{Baseline methods}
%We consider two families of benchmarks. The first is a collection of methods that learn a binary router from preference data, while the second baseline is an inefficient version of our estimates for the routing Pareto frontier.
\citet{ong2024routellmlearningroutellms} (RouteLLM) proposes a collection of methods for learning binary routers from preference data (data consisting of queries $q$ and labels $l_{i,j}$ indicating a winner between model $i$ and $j$). While the usage of preference data is slightly different from ours, we implement their methods on our data by creating pseudo-preference data between two models. In particular, we select a costly and high-performing model and a cheaper model and say the costly model wins if and only if it is correct while the cheaper model is incorrect. On this pseudo preference data, we fit two methods from \citet{ong2024routellmlearningroutellms} for learning win probabilities between expensive and cheap models: the first is a matrix factorization method, called {\bf RouteLLM (MF)}, while the second uses fine-tuned {roberta-base}, called {\bf RouteLLM (Roberta)}. A follow-up method to these is Routing on Random Forests (RoRF) from Not-Diamond \citep{notdiamond2023rorf}, referred to as {\bf Not-Diamond RoRF}. This method uses a text-embedder and random forest model to predict the win probability; we provide a comparison to this method with the {text-embedding-3-small} embedder from OpenAI. As in \citet{notdiamond2023rorf}, we use a slightly different procedure to construct the preference data; a label can take one of four possible values (one for each combination of correct/incorrect from each model), and the costly model is favored if either both models are wrong or the cheaper model is incorrect while the expensive model is correct. For RouterBench, we consider GPT-4 and mixtral 8x7b to be the costly and cheaper models while for the Open LLM Leaderboard, we use Qwen2.5 72b and Qwen2.5 7b.

%\textbf{ERM Router:} In the original RouterBench paper, the authors propose using empirical risk minimization (ERM) and linear interpolation to traverse the routing Pareto frontier \citep{hu2024routerbench}. We implement the ERM router and demonstrate that our method allows the practitioner to traverse the Pareto frontier smoothly. This essentially acts as an empirical verification of Lemma \ref{lemma:oracle-router}
\subsection{Results} %In both {RouterBench} and {Open LLM leaderboard v2} benchmarks we train the loss regressor $\widehat \Phi$ over ten different training and test splits of the datasets that are determined by some prespecified seed values. In both Figures \ref{fig:open-llm-leaderboard} and \ref{fig:RouterBench} we provide error-bars for both average accuracy and cost metrics calculated for the individual LLMs and estimated routers for \openllm\ and \RouterBench\ benchmarks. In both cases, the effect of the baseline architecture (\texttt{\texttt{roberta-base}} vs \texttt{bert-base-case}) were negligible, as we did not see significant differences in the estimated Pareto curves. 

% \begin{figure}
%     \centering
%         \includegraphics[width=0.38\textwidth]{plots/RouterBench (2).pdf} 
%         %\caption{Test 1}
%         %\label{fig:RouterBench}
%     %\hfill
%     %\begin{minipage}{0.49\textwidth}
%       % \includegraphics[width=\textwidth]{plots/router-vs-gpt4.pdf}
%       %  \caption{Test 2}
%      %   \label{fig:router-vs-gpt4}
%     %\end{minipage}
%     \vspace{-0.3cm}
% \caption{RouterBench: performance of several routers and individual LLMs on test data-split.}
% \label{fig:RouterBench-main}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\linewidth]{plots/hf-leaderboard1.pdf}
%     \vspace{-0.3cm}
%     \caption{Open LLM leaderboard v2: performance of several routers and individual LLMs on test data-split.}
%     \label{fig:rexp_open_llm}
% \end{figure}


\begin{figure}
       \centering
       \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \includegraphics[width=\linewidth]{plots/RouterBench2.pdf} 
           \caption{RouterBench}
           \label{fig:RouterBench-main}
       \end{subfigure}
       \hspace{0.05\textwidth} % Space between subfigures
       \begin{subfigure}[b]{0.45\textwidth}
           \centering
           \includegraphics[width=1.035\linewidth]{plots/hf-leaderboard1.pdf}
           \caption{Open LLM leaderboard v2}
           \label{fig:rexp_open_llm}
       \end{subfigure}
       \caption{Performance of several routers and individual LLMs on test data-split in Routerbench and Open-LLM-leaderboard-v2 benchmarks.}
       \label{fig:mainfig}
   \end{figure}

\paragraph{Performance against baselines:} In {RouterBench} we were unable to achieve significantly better accuracy than GPT-4; however, we were able to greatly reduce the prediction cost. A direct comparison between routers and GPT-4 with respect to average accuracy versus cost is provided in Figure \ref{fig:RouterBench-main}, where we see that routers can achieve an accuracy similar to GPT-4 at half the cost, while achieving $95\%$ of the accuracy at only $20\%$ of the cost. On the other hand, we showed that CARROT can outperform the best model (Qwen2-72B) by a large margin in {Open LLM leaderboard v2} (see Figure \ref{fig:rexp_open_llm}). In both datasets, we see that our routers significantly outperform the binary routers of \citet{ong2024routellmlearningroutellms} and \citet{notdiamond2023rorf}. This is due to the fact that we route to \emph{all possible models}, which increases the accuracy coverage and decreases the cost of the cheapest accurate model for a given query.

%In {Open LLM leaderboard v2}, the model {Qwen2-72B} has the best accuracy ($0.563$) %\pm 0.008)
%but has the highest cost (\$$10^{-9}  %\pm 5.5 \times 10^{-6}
%$ per query) while in RouterBench GPT-4 has the best accuracy %($0.833 %\pm %0.008
%$) but has the highest cost (\$$3.3\times 10^{-4} %\pm 5.5 \times 10^{-6}
%$ per query).% Through routing, on Open LLM leaderboard v2 we were able to improve the accuracy almost by $5\%$ and reduce the prediction cost to one-fourth of the previous cost (see Figure \ref{fig:rexp_open_llm}). %(for $\lambda = 0.995$  and  {\texttt{roberta-base}} architecture the accuracy is... %$0.592 \pm 0.007$ and the cost is $5.08\times 10^{-5} \pm 8.8\times 10^{-7}$). 

To verify our theory, we also compare to the ERM router, which directly minimizes ERM risk of \eqref{eq:RM} for a particular combination of accuracy and cost metrics. CARROT matches the ERM router performance, as demonstrated in Appendix Figures \ref{fig:rbenchsupp} and \ref{fig:rexp_open_llm2}, verifying Lemma \ref{lemma:oracle-router}.%the theoretical findings.

% For simplicity, we reserve comparisons of our CARROT to the ERM router, that directly minimizes ERM risk of \eqref{eq:RM} for a particular combination of accuracy and cost metrics, to the supplementary material (\cf\ Figures \ref{fig:rbenchsupp} and \ref{fig:rexp_open_llm2}).

\paragraph{Performance on \newdata:} The performance of CARROT on \newdata\ is illustrated in Figure \ref{fig:ROOT In Distribution}; a trend similar to our findings for RouterBench and the Open-LLM Leaderboard-v2 emerges. Moreover, Figure \ref{fig:IBMMix-spider} shows that CARROT can achieve similar or better performance of {GPT-4o} with a fraction of the cost. 
% \begin{figure}
%     \centering
% \includegraphics[width=.8\linewidth]{plots/fmselect_perf_cost.pdf}

% \vspace{-0.3cm}
%     \caption{Test performance on \newdata.}
%     \label{fig:ROOT In Distribution}
% \end{figure}


\begin{figure}[ht]
       \centering
       \begin{subfigure}[b]{0.5\textwidth}
           \centering
           \includegraphics[width=\linewidth]{plots/fmselect_perf_cost.pdf}
           \caption{Accuracy versus cost performance on \newdata.}
           \label{fig:ROOT In Distribution}
       \end{subfigure}
       \hspace{0.05\textwidth} % Space between subfigures
       \begin{subfigure}[b]{0.4\textwidth}
           \centering
           \includegraphics[width=\linewidth]{plots/fmselect_model_selection.pdf}
    \caption{Model selection proportions on \newdata\ across several accuracies on the test split.}
           \label{fig:model selection}
       \end{subfigure}
       \caption{CARROT routing analysis on the \newdata\ dataset.}
       \label{fig:fmselect}
   \end{figure}

%In \texttt{RouterBench}  the \texttt{gpt-4-1106-preview} model, which is often considered as the state of the art, had the highest accuracy ($0.788 \pm 0.004$) but at the highest cost ($0.003 \pm 4.02 \times 10^{-5}$). Although we were unable to achieve significantly better accuracy than GPT-4, we were able to greatly reduce the prediction cost. A direct comparison between routers and GPT-4 with respect to average accuracy versus cost is provided in Figure \ref{fig:router-vs-gpt4}, where we see that routers can achieve an accuracy similar to GPT-4 at half the cost, while achieving $95\%$ of the accuracy at only $20\%$ of the cost. 

%\paragraph{Ablation studies:}



% \begin{figure}[h]
%   \centering
%   \begin{subfigure}[b]{0.48\textwidth}
%     \includegraphics[width=\textwidth]{plots/RouterBench.pdf}
%     \caption{First subfigure}
%     \label{fig:RouterBench}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.48\textwidth}
%     \includegraphics[width=\textwidth]{plots/router-vs-gpt4.pdf}
%     \caption{Second subfigure}
%     \label{fig:router-vs-gpt4}
%   \end{subfigure}
%   \caption{A figure with two subfigures}
%   \label{fig:main}
% \end{figure}


\paragraph{How CARROT chooses models:} %We take this opportunity to provide interpretable insights into our model selection process in this application. 
Figure \ref{fig:model selection} presents the distribution of selected models in \newdata\ across some (average cost, average accuracy) pairs in the test split. As we move from left to right in Figure \ref{fig:model selection}, the selection strategy gradually shifts from prioritizing cost efficiency with smaller models such as {llama-3-2-1b-instruct} and {llama-3-2-3b-instruct} to favoring more capable and expensive models like {llama-3-3-70b-instruct}, {gpt-4o}, and {gpt-4o-mini}.

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\linewidth]{plots/fmselect_model_selection.pdf}
%     \vspace{-0.3cm}
%     \caption{Model selection proportions on \newdata\ across several accuracies on the test split.}
%     \label{fig:model selection}
% \end{figure}




