\section{Statistical efficiency of CARROT} 
\label{sec:lower-bound}

In this section we establish that, under certain conditions, the plug-in approach to routing is minimax optimal. To show this, we follow two steps:
\begin{itemize}
    \item First we establish an information theoretic lower bound on the sample complexity for learning the oracle routers (\cf\ Theorem \ref{thm:lower-bound}). 
    \item Next, establish an upper bound for the minimax risk of plug-in routers (\cf\ Theorem \ref{thm:upper-bound}). We show that under sufficient conditions on the estimates of $\Ex[Y\mid X]$ the sample complexity in the upper bound matches the lower bound. Together, they imply the statistical efficiency of the plug-in approach.  
    % We also suggest an estimate for $\Ex[Y\mid X]$ that meets the needed conditions for CARROT to be rate optimal.  
\end{itemize} 


%For our minimax analysis we begin with some notation. For the probability distribution $P $ defined on the space $\cX \times \reals^{M \times K}$, we denote the marginal distribution of $X$ by $P_X$. Let us denote $\supp(\cdot)$ as the support of a probability distribution. Within the space $\reals^d$, we denote $\Lambda_d$ as the Lebesgue measure, $\|\cdot\|_2$ and $\|\cdot\|_\infty$ as the $\ell_2$ and $\ell_\infty$-norms, and $\cB(x, r, \ell_2)$ and $\cB(x, r, \ell_\infty)$ as closed balls of radius $r$ and centered at $x$ with respect to the $\ell_2$ and $\ell_\infty$-norms. 

% We denote $\Ex_P[\ell \{ Y, f_m(X)\}\mid X]$ as $\Phi^\star_m(X)$. Then, following eq. \eqref{eq:reg-decomposition} the regression function $\eta_{\lambda, m}^\star(X) = \Ex_P[\eta_\lambda(X, Y)]$ has the decomposition $\eta_{\lambda, m}^\star(X) = \lambda \Phi^\star_m(X) + (1 - \lambda) \kappa_m(X)$. In the following lemma, we provide a formulation of oracle routers using this decomposition, which will be useful for developing their computationally efficient estimates. 
% \begin{lemma} \label{lemma:oracle-router}
%     For any $0 \le \lambda \le 1$  the oracle router $g_\lambda^\star$ that minimizes the loss $\cL_P(g, \lambda)$ is 
%     \begin{equation} \label{eq:oracle-router-2}
%         \textstyle g_\lambda^\star(X) = \argmin_m ~ \eta_{\lambda, m} ^\star(X) = \argmin_m ~ \{ \lambda \Phi^\star_m(X) + (1 - \lambda) \kappa_m(X)\}\,.
%     \end{equation}
% \end{lemma}

We begin with a notational convention for $g_\mu^\star(X)$. If the minimum is attained at multiple $m$'s, we consider $g_\mu^\star(X)$ as a subset of $[M]$. On the contrary, if the minimum is uniquely attained, then $g_\mu^\star(X)$ refers to both the index $m_X$ where the minimum is attained and the singleton set $\{m_X\} \subset [M]$. The distinction should be clear from the context.

We also generalize slightly to the setting where the last $K_2$ metrics are known functions of $X$, \ie\ for $m \in [M], k \in \{K - K_2 +1 , \dots K\}$ there exist known functions $f_{m, k}: \cX \to \reals$ such that $[Y]_{m, k} = f_{m, k}(X)$. Since $\Ex[[Y]_{m, k}\mid X] = f_{m, k}(X)$ are known for $k \ge K - K_2 +1 $ they don't need to be estimated. 
% We shall see the presence of known metrics has consequences for the sample complexity (\cf\ Remark \ref{remark:difficulty-routing}). We also define $K_1 = K - K_2$ as the number of known metrics.    

\subsection{Technical Assumptions}

%Let us discuss a notational convention for $g_\lambda^\star(X)$. The minimum can be attained at multiple $m$'s. In that case, $g_\lambda^\star(X) \subset [M]$. However, when the minimum is uniquely attained, the $g_\lambda^\star(X)$ refers to both the index $m_X$ where the minimum is attained and the singleton set $\{m_X\} \subset [M]$. The distinctions should be clear from the contexts.


%We also assume that the last $K_2$ many metrics are known functions of $X$, \ie\ for $m \in [M], k \in \{K - K_2 +1 , \dots K\}$ there exist known functions $f_{m, k}: \cX \to \reals$ such that $[Y]_{m, k} = f_{m, k}(X)$. Since $\Ex[[Y]_{m, k}\mid X] = f_{m, k}(X)$ are known for $k \ge K - K_2 +1 $ within the Algorithm \ref{alg:pareto-routers}, they don't need to be estimated. We shall see its consequence in the study sample complexity. Define $K_1 = K - K_2$.    



The technical assumptions of our minimax study are closely related to those in investigations of non-parametric binary classification problems with $0/1$ loss functions, \eg\  \citet{cai2019Transfer,kpotufe2018Marginal,maity2022minimax,audibert2007Fast}. In fact, our setting generalizes the classification settings considered in these papers on multiple fronts: (i) we allow for general loss functions, (ii) we allow for more than two classes, and (iii) we allow for multiple objectives. %So, before we describe the assumptions, 

To clarify this, we discuss how binary classification is a special case of our routing problem. %This connection will be later used for adapting the standard assumptions considered in these papers to our setting. 

\begin{example}[Binary classification with $0/1$-loss] \label{example:binary-classification}
    Consider a binary classification setting with $0/1$-loss: we have the pairs $(X, Z) \in \cX \times \{0, 1\}$ and we want to learn a classifier $h: \cX \to\{0, 1\} $ to predict $Z$ using $X$. This is a special case of our setting with $M = 2$ and $K= 1$, where for $m \in \{0, 1\}$ the $[Y]_{m, 1} = \bbI\{Z \neq m\}$. Then the risk for the classifier $h$, which can also be thought of as a router, is 
\begin{align*}
\textstyle \cR_P(h) & \textstyle = \Ex\big[\sum_{m \in \{0, 1\}}[Y]_{m, 1} \bbI\{h(X) = m\} \big]\\ 
& = \Ex\big[ \bbI\{h(X) \neq Z\} \big]\,,
\end{align*} the standard misclassification risk for binary classification. 
\end{example}

% \SM{Mention that for $\lambda = 0$ the oracle router is precisely known. Thus, we only focus on the cases of $\lambda > 0$.}

We assume that $\supp(P_X)$ is a compact set in $\reals^d$. This is a standard assumption in minimax investigations for non-parametric classification problems \citep{audibert2007Fast,cai2019Transfer,kpotufe2018Marginal,maity2022minimax}. 
Next,  we place H\"older smoothness conditions on the functions $\Phi_m^\star$. This controls the difficulty of their estimation. For a tuple $s = (s_1 , \dots, s_d) \in (\bN \cup \{0\})^d$ of $d$ non-negative integers  define $|s| = \sum_{j = 1}^d s_j$ and for a function $\phi: \reals^d\to \reals$ and $x = (x_1, \dots, x_d) \in \reals^d$ define the differential operator: 
\begin{equation}
  \textstyle  D_s(\phi, x) = \frac{\partial^{|s|}\phi(x)}{\partial x_1^{s_1} \dots \partial x_d^{s_d}}\,, 
\end{equation} assuming that such a derivative exists. Using this differential operator we now define the H\"older smoothness condition: 

\begin{definition}[H\"older smoothness]
   For $\beta, K_\beta >0$ we say that $\phi:\reals^d \to \reals$ is $(\beta, K_\beta)$-H\"older smooth on a set $ A \subset \reals^d$ if it is $\lfloor \beta \rfloor$-times continuously differentiable on $A$ and for any $x, y \in A $ 
   \begin{equation}
       |\phi(y) - \phi_x ^{(\lfloor \beta \rfloor)}(y)| \le K_\beta \|x - y\|_2^\beta\,,
   \end{equation} where 
$\phi_x ^{(\lfloor \beta \rfloor)}(y) = \sum_{|s| \le \lfloor \beta \rfloor} D_s(\phi, x) \{\prod_{j = 1}^d(y_j - x_j)^{s_j}\} $ is the $\lfloor \beta \rfloor$-order Taylor polynomial approximation of $\phi(y)$ around $x$. 
\end{definition}
With this definition, we assume the following:
\begin{assumption}\label{assmp:smooth}
    For $m \in [M]$ and $k \in [K_1]$ the 
    % functions $\kappa_m$ and 
    $[\Phi(X)]_{m, k}$ is $(\gamma_{k}, K_{\gamma, k})$-H\"older smooth. 
\end{assumption} 
%The H\"older smoothness assumption controls how well a non-parametric function can be estimated \citep{fan1997local}, where higher smoothness parameters lead to a smaller error in estimation. 
This smoothness parameter will appear in the sample complexity of our  plug-in router. Since the $[\Phi(X)]_{m, k}$ are known for $k \ge K_1 + 1$ we do not require any smoothness assumptions on them.

% Note that the assumption implies for any $\lambda\in [0, 1]$ and $m \in [M]$ the $\eta_{\lambda, m}^\star = \lambda \Phi_m^\star + (1 - \lambda) \kappa_m$ are also $(\beta, K_\beta)$-H\"older smooth.
% The smoothness on the $\Phi_m^\star$ functions controls their complexity: the higher smoothness implies a lower complexity and is easier to estimate. \SM{Talk about how different complexity would lead to the rate at lowest smoothness. Argue it through the estimation of the differences. And also talk about how smoothness in $\kappa_m$ is not necessary.}
% Before we move on, we want to make a few remarks about this smoothness assumption. As discussed in Section \ref{sec:setup} the core idea behind our approach is to plug-in an estimate of $[\Phi(X)]_{m, k}$ into the oracle router \eqref{eq:oracle-router-2}
%  \[
%     \textstyle g_\mu^\star(X)  = \argmin_m \big\{ \sum_{k = 1}^K \mu_k [\Phi (X)]_{m, k} \big\} \,.
%     \]
% A similar idea can also be found  in the context of binary classification in non-parametric settings: for $(X, Y) \in \cX \times \{0, 1\}$ drawn from the distribution $P$, they plug-in an estimator of $\eta(X) = P(Y = 1\mid X )$ into the Bayes classifier $f^\star(X) = \bbI\{\eta(X) \ge \nicefrac{1}{2}\}$ to obtain a minimax rate optimal classifier, which they call ``plug-in classifier''. In their context, the smoothness in $\eta$ controls how well it can be estimated from a dataset, which later affects the misclassification error for this plug-in classifier. Drawing a parallel to our context,
% \begin{enumerate}
%     \item Within the regression function $\eta_{\lambda, m}^\star(X) = \lambda \Phi^\star _m(X) + (1 - \lambda) \kappa_m(X)$ the $\kappa_m$ are already known and need not be estimated. Thus, we do not require any smoothness assumption for $\kappa_m$ and only require a smoothness condition for the unknown $\Phi_m^\star$.  
%     \item To make the setting more general, we could assume different smoothness parameters for different $\Phi_m^\star$; \eg\ $\Phi_m^\star$ is $\beta_m$ H\"older smooth, in which case it can be estimated at a minimax optimal $\ell_1$-error rate $\cO_P(n^{-{\beta_m}/{(2\beta_m + d)}})$ \citep{fan1997local}. But then the differences \[
%     \textstyle \eta_{\lambda, m_1}^\star(X) - \eta_{\lambda, m_2}^\star(X) = \lambda \big\{ \Phi^\star _{m_1}(X) -\Phi^\star _{m_2}(X)\big\}  + (1 - \lambda) \big \{ \kappa_{m_1}(X) - \kappa_{m_2}(X)\big \}\,, 
%     \] which are crucial for the prediction of oracle routers, will be estimated at a rate 
%     \[
%     \textstyle \cO_P\big(n^{-\frac{\beta_{m_1}}{2\beta_{m_1} + d}} \vee n^{-\frac{\beta_{m_2}}{2\beta_{m_2} + d}}\big) = \cO_P\big(n^{-\frac{\beta_{m_1} \wedge \beta_{m_2}}{2(\beta_{m_1} \wedge \beta_{m_2}) + d}} \big)\,,
%     \]
%     and in the worst case, at a rate $\cO_P(n^{-{\beta_{\min} }/{(2\beta_{\min}  + d})} )$ with respect to the smallest smoothness parameter $\beta_{\min} = \min_m \beta_m$. Since accurately estimating all of these pairwise differences is important to obtain a prediction similar to that of the oracle routers, the final rate of convergence rate for excess risk will be determined by the smallest smoothness parameter, in which case, the other smoothness parameters become irrelevant. Therefore, for a simpler exposition of the problem setting, we assume that the smoothness parameters of $\Phi_m^\star$ are all identical. 
% \end{enumerate}




Next, we introduce \emph{margin condition}, which quantifies the difficulty in learning the oracle router.  For a given $\mu$ define the margin as the difference between the minimum and second minimum of the risk values: 
{ \begin{equation}\label{eq:margin}
    \begin{aligned}
        & \textstyle \Delta_\mu(x) =  
    \begin{cases}
       \min\limits_{m \notin g_\mu(x)} \eta_{\mu, m}(x) - \min\limits_m \eta_{\mu, m}(x) & \text{if} ~ g_\mu^\star(x) \neq [M]\\ 
       0 & \text{otherwise}.
       \end{cases} 
    \end{aligned}
\end{equation}}

% At an $x$, the margin is simply the gap between the second-lowest and lowest coordinate values of $\eta_{\lambda, m}$. If all the coordinates are the same, then we set the margin at zero. 
\noindent Our definition of a margin generalizes the usual definition of the margin considered for binary classification with $0/1$ loss in \citet{audibert2007Fast}. Recall the binary classification example in \ref{example:binary-classification}, in which case, 
$[\Phi(X)]_{m , 1} =  P(Z \neq m\mid X) $. Since $K = 1$ we have 
$\eta_{\mu, m}(X) = P(Z \neq m\mid X) $, which further implies $\eta_{\mu, 0}(X) + \eta_{\mu, 1}(X) = 1$.
Thus for binary classification with $0/1$ loss, our definition of margin simplifies to 
\begin{align*}
\textstyle \min\limits_{m \notin g_\mu^\star(x)} \eta_{\mu, m}(x) - \min\limits_m \eta_{\mu, m}(x)
=  |\eta_{\mu, 1}(X) - \eta_{\mu, 0}(X)| = 2 |\eta_{\mu, 0}(X) - \nicefrac{1}{2}| \,,
\end{align*}
which is a constant times the margin $  |P(Y = 1\mid X) - \nicefrac{1}{2}| = |\eta_{\mu, 0}(X) - \nicefrac{1}{2}| $ in \citet{audibert2007Fast}. 


% Relating their framework to ours, for them $M = 2$ with the class indices $\{0, 1\}$. Moreover, for $m \in \{0, 1\}$ the loss regression function for classifying a sample $X$ as class $m$ is  $\eta_{\lambda, m}^\star(X) = P(Y \neq m\mid X) $, which satisfies $\eta^\star_{\lambda, 0}(X) + \eta^\star_{\lambda, 1}(X) = 1$. In this case, our definition of margin simplifies to $|\eta_{\lambda, 1}(X) - \eta_{\lambda, 0}(X)| = 2 |\eta_{\lambda, 1}(X) - \nicefrac{1}{2}|$, which is a constant multiplication of their definition of margin $  |P(Y = 1\mid X) - \nicefrac{1}{2}| = |\eta_{\lambda, 1}(X) - \nicefrac{1}{2}| $. 



Clearly, the margin determines the difficulty in learning the oracle router. A query $X$ with a small margin gap is difficult to route, because to have the same prediction as the oracle, \ie\  $\argmin_{m} \hat \eta_{\mu, m}(X) = \argmin_{m} \eta_{\mu, m}^\star(X)$ we need to estimate $ \eta_{\mu, m}^\star(X)$ with high precision. In the following assumption, we control the probability of drawing these ``difficult to route'' queries.

\begin{assumption}[Margin condition]\label{assmp:margin}
    For $\alpha, K_\alpha >0$ and any $t > 0$ the margin $\Delta_{\mu}$ \eqref{eq:margin} satisfies: \begin{equation}
        P_X \big\{0 < \Delta _\mu(X) \le t\big \}  \le K_\alpha t^{\alpha}\,. 
    \end{equation}
\end{assumption}
%From Proposition 3.4 of \cite{audibert2007Fast}, if $\alpha  (1 \wedge \gamma_{k})  \ge d$ for some $k$ then if $\mu = e_k$ for which the $g_\mu^\star$ never changeswe argue that when $\alpha  (1 \wedge \gamma_{k})  \ge d$ for some $k$ then for $\mu = e_k$ for which the $g_\mu^\star$ never changes its decision within the interior of $\supp(P_X)$. 
Following \citet{audibert2007Fast}, we focus on the cases where $\alpha < d$ and for every $k$ the $\alpha \gamma_k < d$. This helps to avoid trivial cases where routing decisions are constant over $P_X$ for some $\mu$.  %These are trivial cases, which we ignore. Thus, throughout our paper, we assume that $\alpha   < d$ and for every $k$ the $\alpha \gamma_k < d$.  
Next, we assume that $P_X$ has a density $p_X$ that satisfies a strong density condition described below.
% We start by formalizing the problem setup. We assume that the covariate space is $\cX$ is a compact set in $\reals^d$. Next, we assume that the density exists for the marginal probability $P_X$ and satisfies a strong density condition, which is formalized below. 
\begin{assumption}[Strong density condition] \label{assmp:strong-density}
Fix constants $c_0, r_0> 0$ and $0 \le \mu_{\min}  \le \mu_{\max} < \infty$. We say $P_X$ satisfies the strong density condition if its support is a compact $(c_0, r_0)$-regular set and it has density $p_X$ which is bounded: $\mu_{\min} \le p_X (x) \le \mu_{\max} $ for all $x$ within $\supp(P_X)$. A set $A \subset \reals^d$ is Lebesgue measurable and %\ie\ for the Lebesgue measure $\Lambda_d$ on $\reals^d$, any Lebesgue measurable set $A \subset \reals^d$ and 
$\text{for any} ~ 0 < r \le r_0, ~  x \in A$ it satisfies
\begin{equation}
    \Lambda_d (A \cap \cB(x, r, \ell_2)) \ge c_0 \Lambda_d(\cB(x, r, \ell_2)). %~ \text{for any} ~ 0 < r \le r_0, ~  x \in A,
\end{equation} %and the density $p_X$ is bounded as: $\mu_{\min} \le p_X (x) \le \mu_{\max} $ for all $x$ within $\supp(P_X)$. 
\end{assumption}
This is another standard assumption required for minimax rate studies in nonparametric classification problems \citep{audibert2007Fast,cai2019Transfer}. All together, we define $\cP(c_0, r_0, \mu_{\min}, \mu_{\max}, \beta_{m ,k}, K_{\beta, m, k}, \alpha, K_\alpha)$, or simply $\cP$, as the class of probabilities $P$ defined on the space $\cX \times \cY$ for which $P_X$  is  compactly supported and satisfies the strong density assumption \ref{assmp:strong-density} with parameters $(c_0, r_0, \mu_{\min}, \mu_{\max})$, and the H\"older smoothness assumption \ref{assmp:smooth} and the $(\alpha, K_\alpha)$-margin condition in Assumption \ref{assmp:margin} hold. We shall establish our minimax rate of convergence within this probability class. 










% In this section we provide a ``mini-max'' investigation on learning of the oracle $g^\star$. For this purpose, assume that 
% \begin{itemize}
    
%     \item {\bf Strong density condition:} $X$ is distributed in the space $[0, 1]^d$ that satisfies the strong density condition \citep{audibert2007Fast}.
%     \item {\bf H\"older smoothness:} The functions $\kappa_\lambda(x, l) \triangleq\ell (f^\star(x), f_l(x)) + \lambda c_l (x)$ are $\alpha$-H\"older smooth. 
% \item {\bf Noise condition:} For $t > 0$ there exists a $\gamma > 0$ such that 
% \[
% \textstyle P_X \big ( 0 <\max_l \big | \kappa_\lambda(x, l) - \min_{l' \neq l} \kappa_\lambda (x, l') \big | \le t \big ) = \cO(t^\gamma)
% \]
% \end{itemize}
% Denote $\cP$ as the class of all probabilities that satisfies the above conditions. 

% \SM{revise the lower bound in light of $\lambda  \ge c n^{\frac{\beta - \nicefrac d\alpha }{2\beta +d }}$ requirement.}
\subsection{The lower bound} 
Rather than the actual risk $\cR_P(\mu, g)$, we establish a lower bound on the excess risk:
\begin{equation}\label{eq:excess-risk}
    \cE_P(\mu, g) = \cR_P(\mu, g) - \cR_P(\mu, g_\mu^\star)\,,
\end{equation} that compares the risk of a proposed router to the oracle one. We denote $\Gamma = \{g: \cX \to [M]\}$ as the class of all routers. For an $n \in \bN$ we refer to the map $A_n: \cZ^n \to \Gamma$, which takes the dataset $\cD_n $ as an input and produces a router $A_n(\cD_n): \cX \to [M]$, as an algorithm. Finally, call the class of all algorithms that operate on $\cD_n$ as $\cA_n$. The following Theorem describes a lower bound on the minimax risk for any such algorithm $A_n$. 
\begin{theorem}\label{thm:lower-bound}
    For an $n \ge 1$  and $A_n \in \cA_n$ define  $\cE_P(\mu, A_n) = \Ex_{\cD_n}\big[\cE_P\big(\mu, A_n(\cD_n)\big)\big]$ as the excess risk of an algorithm $A_n$. There exists a constant $c> 0$ that is independent of both $n$ and $\mu$ such that for any $n\ge 1$ and $\mu\in \Delta^{K-1}$ we have the lower bound
    \begin{equation}\label{eq:lower-bound}
      \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(\mu, A_n) \ge c \big \{\sum_{k = 1}^{K_1} \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha} \,.
    \end{equation}
\end{theorem} 
This result is a generalization of that in \citet{audibert2007Fast}, which considers binary classification. 
\begin{remark} \label{remark:minimax-lower-bound}
    Consider the binary classification in Example \ref{example:binary-classification}. Since $K = 1$, the lower bound simplifies to $\cO(n^{-\nicefrac{\gamma_1 (1+ \alpha)}{2\gamma_1 + d}})$,  which matches with the rate in \citet[Theorem 3.5]{audibert2007Fast}. 
    Beyond $0/1$ loss, our lower bound also establishes that the rate remains identical for other classification loss functions as well. 
    
    % The case of $\lambda = 1$ is closely related to the usual classification tasks with a single objective function. In fact, binary classification with $0/1$-loss is a special case. To make this connection clear, consider $M = 2$ and the index set for the classes as $\{0, 1\}$. Moreover, assume that the loss is $0/1$, \ie\ for $m\in \{0, 1\}$ the  $Z_m = \ell\{ Y, f_m(X)\} \in \{0, 1\}$ and $Z_0 + Z_1 = 1$, in which case the loss of a router $g:\cX \to \{0, 1\}$ is 
    % \[
    % \begin{aligned}
    %      \textstyle \ell\{g; X, Y\} & \textstyle= \bbI\{ g(X) = 0\} Z_0 +  \bbI\{ g(X) = 1\} Z_1 \\
    %      & \textstyle= \bbI\{ g(X) = 0\} Z_0 +  \bbI\{ g(X) = 1\} (1 - Z_0) = \bbI \{g(X) \neq Z_0\} \,.
    % \end{aligned}
    % \] Thus, it is not surprising that for $\lambda = 1$ our rate of convergence $\cO(n^{\frac{-\beta(1 + \alpha)}{2\beta +d}})$ for the lower bound is exactly the same as in \citet[Theorem 3.5]{audibert2007Fast}. As such, {we broaden the framework of the minimax lower bound study for non-parametric classification tasks to (1) more than two classes, and (2) general loss functions.} To understand this, consider a classification task with $M$ classes and the loss function of a classifier $g: \cX \to [M]$ is $\ell\{ g; X, Y\} = \sum_{m = 1}^M \bbI \{g(X) = m\} \ell_m(X, Y)$ where $\ell_m(X, Y)$ is the loss incurred when a sample $(X, Y)$ is predicted as the class $m$. In that case, simply letting $\lambda = 1$ and $Z_m = \ell_{m}(X, Y)$ within the lower bound analysis, we obtain a rate of convergence $\cO(n^{\nicefrac{-\beta(1 + \alpha)}{(2\beta +d)}})$. Moreover, the analysis of the upper bound in Section \ref{sec:upper-bound} reveals that this rate is minimax optimal. 
\end{remark}


% \SM{mention how the true difficulty of the problem lies when the $\lambda$ are bounded away from zero because in that case one needs to accurately estimate the $\Phi_m^\star$. At $\lambda \to 0$ the importance is not on the unknown function.}

% \begin{remark}\label{remark:diff-in-lambda-lb} \label{remark:lower-bound-lambda}
%     The lower bound also highlights that ``it is easier to route for a smaller value $\lambda $''. Within the oracle router, we know the $(1 - \lambda) \kappa_m(X)$ part within the function $\eta_{\lambda, m}^\star(X) = \lambda \Phi_m^\star(X) + (1 - \lambda) \kappa_m(X)$. Thus, at an intuitive level, for smaller values of $\lambda$ there is less importance on the unknown $\lambda \Phi_m^\star(X)$ part, and thus the plug-in router in eq. \eqref{eq:plugin-router} can tolerate a larger noise in their estimations. Our lower bound makes this intuition precise: for a smoothness parameter $\beta$ the $\Phi_m^\star$ are estimated at a $\cO_P(n^{-\nicefrac{\beta}{(2\beta+d)}})$-rate, and thus both $\lambda \Phi_m^\star(X)$ and the whole $\eta_{\lambda, m}^\star(X)$ are estimated at a $\cO_P(\lambda n^{-\nicefrac{\beta}{(2\beta+d)}})$-rate. This intuition is more formally exposed in the upper bound study of excess risk in Theorem \ref{thm:upper-bound}, where we precisely quantify the relationship between the error in the estimation of $\Phi_m^\star$ and the convergence rate for excess risk (\cf\ Remark \ref{remark:upper-bound-lambda}).
%     Following this intuition, we can understand that learning a router for a smaller $\lambda$ is easier and for $\lambda = 1$ it is the hardest.
% \end{remark}

% This is the exact same rate of convergence obtained in \citet[Theorem 3.5]{audibert2007Fast}. Intuitively, for a fixed $\lambda$ the task of routing is equivalent to multi-class classification, so, it is not surprising that they have the same rate. 


% \SM{remark about $\lambda$}. 

% \begin{remark}
%     For a fixed $n$ our lower bound analysis in Theorem \ref{thm:lower-bound} is only valid for $\lambda \ge c_1 n^{\frac{\beta - \nicefrac{d}{\alpha}}{2\beta + d}}$. To address this gap, we note: 
%     \begin{itemize}
%         \item Firstly, because we know the cost functions $\kappa_m$, we can precisely find the oracle router $g^\star_{0}(X) = \argmin_m \kappa_m(X)$ at $\lambda = 0$. Furthermore, since we consider $\alpha \beta < d$ the $c_1 n^{\frac{\beta - \nicefrac{d}{\alpha}}{2\beta + d}}$ decreases to zero as $n$ grows to infinity. Thus, the gap in $\lambda$, where this lower bound is invalid, vanishes as the sample size increases to infinity. 
%         \item Regardless of this gap, we argue in Remark \ref{} that we can estimate the Pareto frontier for the performance-cost trade-off efficiently. 
%     \end{itemize}
% \end{remark}


% \SM{compare it to usual rate of convergence for classification when $\lambda = 1$}. 


% \subsection{Efficient learning of oracle routers}
% \label{sec:efficient-learning}
% Let us quickly recall our core idea behind it. For a $\lambda \in [0, 1]$ the true loss regression function $\eta_{\lambda, m}^\star(X)$ for the oracle router $g_\lambda^\star (X) = \argmin_m \eta_{\lambda, m}^\star(X)$ is  decomposed as: 
% \begin{equation} \label{eq:oracle-router}
%     \eta_{\lambda, m}^\star(X) = \lambda \Phi^\star_m(X) + (1 - \lambda) \kappa_m(X), ~ \Phi^\star _m(X) = \Ex_P[\ell\{ Y, f_m(X)\} \mid X ]\,. 
% \end{equation} Since we already know $\kappa_m(X)$ at a new $X$ the only unknown is the $\Phi^\star_m(X)$. Thus, we can plug-in its estimate  $\widehat \Phi_m(X)$ within eq. \eqref{eq:oracle-router} and estimate the oracle router as:
% \begin{equation}\label{eq:plugin-router}
%     \widehat g_\lambda(X) = \argmin_m \hat \eta_{\lambda, m}(X), ~~ \hat \eta_{\lambda, m}(X) = \lambda \widehat \Phi_m(X) + (1 - \lambda) \kappa_m(X)  
% \end{equation} 
% Moreover, we evaluate their prediction errors and costs on a test split of the dataset as: 
% \begin{equation}
%    \textstyle  \hat \cE_\lambda  =  \frac1{n_\text{test}} \sum_{i = 1}^{n_\text{test}}  \ell\{Y_i', f_{\widehat g_\lambda(X_i')}(X_i')\}, ~~ \hat \cC _\lambda =  \frac1{n_\text{test}} \sum_{i = 1}^{n_\text{test}}  \kappa_{\widehat g_\lambda(X_i')}(X_i')\,. 
% \end{equation} 
% This plug-in approach is computationally efficient, as we can estimate oracle routers for all $\lambda$ in one go, instead of minimizing \ref{eq:ERM} at different $\lambda$'s. In addition to being computationally efficient, through a study on the minimax upper bound on excess risk in the next section we establish that this plug-in approach is also statistically efficient. Furthermore, we extend this approach to a general multi-objective classification problem. For now, we end this section by summarizing our steps in Algorithm \ref{alg:pareto-routers}. 


% \begin{algorithm}
%     \begin{algorithmic}[1]
% \Require Dataset $\cD_n$
% \State Randomly split the dataset into training and test splits: $\cD_n = \cD_{\text{tr}} \cup \cD_{\text{test}}$. 
% \State  Learn an estimate $\widehat \Phi_m (X)$ of $\Phi_m^\star(X)$ using the training split $\cD_{\text{tr}}$. 
% \For{$\lambda \in [0, 1]$}
% \State  Define $\hat \eta_{\lambda, m}(X) =  \lambda \widehat \Phi_m(X) + (1 - \lambda) \kappa_m(X)  $ and 
%  $\widehat g_\lambda(X) = \argmin_m \hat \eta_{\lambda, m}(X)$. If there is a tie within the $\argmin$, break the tie randomly.
%  \State Calculate $\hat \cE_\lambda  =  \frac1{|\cD_{\text{test}}|} \sum_{(X, Y) \in \cD_{\text{test}}}  \ell\{Y, f_{\widehat g_\lambda(X)}(X)\}$
%  \State \quad\quad  and $\hat \cC_\lambda  =  \frac1{|\cD_{\text{test}}|} \sum_{(X, Y) \in \cD_{\text{test}}}  \kappa_{\widehat g_\lambda(X)}(X)$
% \EndFor

% \Return $\{g_\lambda: \lambda \in [0, 1]\}$ and $\hat\cF = \{(\hat \cE_\lambda, \hat \cC_\lambda): \lambda \in [0, 1]\}$. 
% \end{algorithmic}
% \caption{Learning of oracle routers}
% \label{alg:pareto-routers}
% \end{algorithm}


% \SM{talk about the computational efficiency, that we can obtain all the routers in one go. Also, provide a teaser that in the next section we shall establish that its statistically efficient as well.}





% Given a router $\widehat g:[0, 1]^d \to \Delta^ L$ we define the excess risk as:
% \begin{equation}
%    \textstyle \cE_P (\widehat g) = \cR_P(\widehat g) - \cR_P(g^\star) = \Ex_P\big[\sum_{l = 1}^L\{g_l(x_0)- g_l^\star(X_0)\}\kappa_\lambda(X_0 , l) \big]
% \end{equation} We shall show that 
% \begin{equation}
%    \inf_{\widehat g} \sup_{P\in \cP} \textstyle \Ex_P \big[ \cE_\lambda (\widehat g) \big ] \asymp n ^{- \frac{\alpha(1 + \gamma)}{2\alpha + d}}\,.
% \end{equation}



\subsection{The upper bound }\label{sec:upper-bound}
Next, we show that if algorithm the $A_n$ corresponds to CARROT, the performance of $\hat{g}_{\mu}$ matches the lower bound in Theorem \ref{thm:lower-bound} (\cf\ equation \ref{eq:lower-bound}). En-route to attaining $\hat{g}_{\mu}$, we need an estimate $\widehat \Phi(X)$ of $\Phi(X) = \Ex_P[Y \mid X ]$. %In this section, we ask what is the needed performance of this estimate? More importantly, we also study if the plug-in approach leads to statistically efficient estimates of the oracle routers, or in other words does the performance of $\hat{g}_{\mu}$ match \ref{eq:lower-bound} 
Our strategy will consist of two steps: 
\begin{itemize}
    \item First,  we establish an upper bound on the rate of convergence for excess risk \eqref{eq:excess-risk} for the plug-in router in terms of the rate of convergence for $\widehat \Phi(X)$. 
    \item Then we discuss the desired rate of convergence in $\widehat \Phi(X)$ so that the upper bound has the identical rate of convergence to the lower bound \eqref{eq:lower-bound}. Later in Appendix \ref{sec:reg-fn-estimate} we provide an estimate $\widehat \Phi(X)$ that has the required convergence rate. 
\end{itemize}
These two steps, together with the lower bound in \eqref{eq:lower-bound} establish that our plug-in router achieves the best possible rate of convergence in excess risk. 

We begin with an assumption that specifies a rate of convergence for $[\widehat \Phi(X)]_{m, k}$. 
\begin{assumption} \label{assmp:convergence}
    For some constants $\rho_1, \rho_2 > 0$ and any $n \ge 1$ and $t > 0$ and almost all $X$ with respect to the distribution $P_X$ we have the following concentration bound:
    \begin{align}\label{eq:concentration-phi}
        \max_{P\in \cP} P \big \{ \max_{m, k} a_{k, n}^{-1}\big |[\widehat \Phi (X)]_{m, k} - [\Phi  (X)]_{m, k}\big |
        \ge t\big \}  
        \le  \rho_1 \exp\big (- \rho_2  t^2 \big )\,,  
    \end{align}  where for each $k$ the  $\{a_{k,n}; n \ge 1\}\subset (0, \infty)$ is a sequence that decreases to zero. 
\end{assumption}
Using this high-level assumption, in the next theorem, we establish an upper bound on the minimax excess risk for CARROT that depends on both $a_{k, n}$ and $\mu$.  
\begin{theorem}[Upper bound]\label{thm:upper-bound}
  Assume \ref{assmp:convergence}.   If all the $P\in \cP$ satisfy the margin condition \ref{assmp:margin} with the parameters $(\alpha, K_\alpha)$ then there exists a $K> 0$ such that for any $n \ge 1$ and $\mu\in \Delta^{K-1} $ the excess risk for the router $\widehat g_\mu$ in Algorithm \ref{alg:pareto-routers} is upper bounded as 
    \begin{equation}
        \max_{P\in \cP} \Ex_{\cD_n}\big [\cE_P(\widehat g_\lambda,\lambda)\big ] \le\textstyle K \big \{\sum_{k = 1}^{K_1} \mu_k a_{k, n}\big\}^{1+\alpha} \,.
    \end{equation}
\end{theorem} 
% \begin{remark} \label{remark:upper-bound-lambda}
%     Recall the Remark \ref{remark:lower-bound-lambda}, where we discuss that it is easier to route for smaller $\lambda$. Indeed, under Assumption \ref{assmp:convergence} the same holds for $\hat \eta_{\lambda, m}(X) -  \eta_{\lambda, m}^\star(X) = \lambda \{ \widehat \Phi_m(X) - \Phi^\star_m(X)\}$ at a rate $\lambda a_n^{-1}$, which, under the $\alpha$-margin condition (Assumption \ref{assmp:margin}), reveals such a dependence for the minimax upper bound on $\lambda$. 
% \end{remark}


\begin{remark}[Rate efficient routers] \label{cor:efficient-routers}
    When $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k +d)}}$ the upper bound in Theorem \ref{thm:upper-bound} has the $\cO(\{\sum_{k = 1}^{K_1}\mu_k n^{-\nicefrac{\gamma_k}{(2\gamma_k+d)}}\}^{1+\alpha})$-rate, which is identical to the rate in the lower bound (\cf\ Theorem \ref{thm:lower-bound}), suggesting that the minimax optimal rate of convergence for the routing problem is 
\begin{equation}
    \label{eq:minimax-rate}
     \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(A_n, \lambda) \asymp \textstyle  \cO\big ( \big \{\sum_{k = 1}^{K_1} \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha}\big ) \,.
\end{equation}
   Following this, we conclude: When $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k +d)}}$ the plug-in approach in Algorithm \ref{alg:pareto-routers}, in addition to being computationally efficient, is also minimax rate optimal. 
%\begin{enumerate}
   % \item When $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k +d)}}$ the plug-in approach in Algorithm \ref{alg:pareto-routers}, in addition to being computationally efficient, is also minimax rate optimal in excess risk. 
   % \item  Following up on the Remark  \ref{remark:minimax-lower-bound}, this result generalizes the minimax study of non-parametric binary classification to (a) more than two classes, and (b) classification loss functions beyond $0/1$-loss. 
%\end{enumerate}
\end{remark} 
An example of an estimator $\widehat{\Phi}$ that meets the needed conditions for $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k +d)}}$ to hold is described in Appendix \ref{sec:reg-fn-estimate}. 
% \begin{remark} \label{remark:difficulty-routing}
% The optimal rate of convergence in \eqref{eq:minimax-rate} is particularly small for small values of $\sum_{k = 1}^{K_1} \mu_k$; in fact, it's identical to zero when $\sum_{k = 1}^{K_1} \mu_k = 0$. This is quite intuitive from the expression of $g_\mu^\star$ in Lemma \ref{lemma:oracle-router}.  For $\sum_{k = 1}^{K_1} \mu_k = 0$ the $g_\mu^\star(X) = \argmin_{m} \{\sum_{k\ge K_1+1} \mu_k [\Phi(X)]_{m, k}\}$ is precisely known to us, thus the excess risk for routing is identical to zero.
    
% \end{remark}

   
%     This leads to the two following conclusions: 
%     \begin{enumerate}
%         \item For any $\lambda$ the minimax optimal rate of convergence for the routing problem is 
%         \[
%         \textstyle \textstyle  \min_{A_n \in \cA_n} \max_{P \in \cP} ~~ \cE_P(A_n, \lambda) \asymp  \cO\big ( \big\{\lambda n^{-\nicefrac{\beta}{(2\beta+d)}}\big\}^{1+\alpha}\big ) \,.
%         \] 
%         This reveals 
        
        
%         Unsurprisingly, this is identical to the optimal rate of convergence in non-parametric classification problems \citep{audibert2007Fast}. Repeating the intuition again, for a fixed $\lambda$ the routing problem 
%         is essentially a multiclass identical minimax optimal rate of convergence for excess risk. 
%         \item  A more interesting observation is that if $\widehat \Phi_m$ converges to $\Phi_m^\star$ at a rate $a_n^{-\frac{1}{2}} = n^{-\frac{\beta}{2\beta +d}}$ then our one-shot approach in Algorithm \ref{alg:pareto-routers} achieves this optimal minimax rate of convergence in excess risk and thus is a \emph{rate efficient} estimator for Pareto routers. We shall show later in Section \ref{sec:reg-fn-estimate} that a local polynomial regression estimator with a suitable bandwidth achieves this desired rate. 
%         \end{enumerate}



% \begin{remark}
%     Remark about the importance of $\lambda$. 
% \end{remark}

% \begin{remark}
%     Let us quickly recall the gap in the validity in our lower bound in Theorem \ref{thm:lower-bound}, that the rate in the lower bound is not true when $0 < \lambda < c_1 n ^{\frac{\beta - \nicefrac{d}{\alpha}}{2\beta +d}}$. However, the rate in the upper bound continues to hold for all $0 \le \lambda \le 1$. 
% \end{remark}


% Now, we make this statement precise and show that for such an $\widehat \Phi $ the excess risk \eqref{eq:excess-risk} achieves the rate in the lower bound \eqref{eq:lower-bound}. 

% \begin{theorem}[Upper bound]\label{thm:upper-bound}
%     Suppose that for some $\rho_1, \rho_2 > 0$ and any $n \ge 1$ and $t > 0$ and almost all $x$ with respect to $P_X$ we have the following concentration bound for $\widehat \Phi$:
%     \begin{equation}\label{eq:concentration-phi}
%         \max_{P\in \cP} P_{\cD_n} \big \{ \|\widehat \Phi(x) - \Phi^\star (x)\|_1 \ge t\big \} \le  \rho_1 \exp\big (- \rho_2 a_n t^2 \big )\,, 
%     \end{equation}
%     where $\{a_n; n \ge 1\}\subset \reals$ is a sequence that increases to $\infty$.  
%     Fix a $\lambda \in [0, 1]$.  Then, if all the $P\in \cP$ satisfies the margin condition \ref{assmp:margin} with the parameters $(\alpha, K_\alpha)$ then there exists a $K> 0$ such that for any $n \ge 1$ the excess risk for the router $\widehat g_\lambda$ in \eqref{eq:eff-estimate-router} is upper bounded as 
%     \begin{equation}
%         \max_{P\in \cP} \Ex_{\cD_n}\big [\cE_P(\widehat g_\lambda,\lambda)\big ] \le K a_n^{-\frac{1+ \alpha}{2}}\,. 
%     \end{equation}
%     Thus, as long as $a_n = n^{{  2\beta/(2\beta + d)}}$  for any $\lambda \in [0, 1]$ the excess risk for the router $\widehat g_\lambda$ in \eqref{eq:eff-estimate-router} has the rate of convergence $a_n^{- {(1 + \alpha)}/{2}} = n^{- {\beta(1 + \alpha)}/{(2\beta + d)}}$ that matches with the lower bound rate in \eqref{eq:lower-bound}. 
% \end{theorem}






% \subsection{Efficiency of the estimation Pareto frontier}

% As mentioned in Section \ref{sec:efficient-learning} and Algorithm \ref{alg:pareto-routers} we estimate the performance-cost trade-off for Pareto routers in a very simple manner: first estimate the Pareto routers $\widehat g_\lambda$ in a training split of the dataset, then evaluate their average performances and costs in the remaining test split as
% \[
% \textstyle \hat \cE_\lambda  =  \frac1{n_\text{test}} \sum_{i = 1}^{n_\text{test}}  \ell\{Y_i', f_{\widehat g_\lambda(X_i')}(X_i')\}, ~~ \hat \cC _\lambda =  \frac1{n_\text{test}} \sum_{i = 1}^{n_\text{test}}  \kappa_{\widehat g_\lambda(X_i')}(X_i')\,. 
% \] In this section we evaluate the efficiency of estimating the Pareto front $\{ (\hat \cE_\lambda, \hat \cC_\lambda): 0 \le \lambda \le 1\}$ in this manner. Note that the true Pareto front is $\{ ( \cE_\lambda^\star,  \cC_\lambda^\star): 0 \le \lambda \le 1\}$ where 
% \[
% \textstyle \cE^\star_\lambda = \Ex_P [\ell\{ Y, f_{g_\lambda^\star(X)}(X)\}], ~~ \cC_\lambda^\star =  \Ex_P [\kappa _{g_\lambda^\star(X)}(X)]\,. 
% \] To evaluate their differences, focus only on $\hat \cE_\lambda - \cE_\lambda^\star$, as the analysis of $\hat \cC_\lambda - \cC_\lambda^\star$ is very similar. Defining $\widetilde \cE_\lambda = \Ex_P [\ell\{ Y, f_{\widehat g_\lambda(X)}(X)\}]$, we can decompose the difference as 
% \begin{equation}
%     \textstyle \hat \cE_\lambda - \cE_\lambda^\star = \{\hat \cE_\lambda - \widetilde \cE_\lambda\}  + \{\widetilde \cE_\lambda-  \cE_\lambda^\star\} 
% \end{equation} The first term is $\cO (n^{-\nicefrac12})$. Now, to bound the second term, notice that 
% \[
% \textstyle \cE_P(\widehat g_\lambda,\lambda) \le 
% \]


% that for $h = n ^{-1/(2\beta + d)}$ the concentration bound in \eqref{eq:concentration-phi} is satisfied with $a_n = n^{-2\beta/(2\beta + d)}$, therefore the router derived from such $\hat\Phi$ using \eqref{eq:eff-estimate-router} achieves the same rate of convergence as in the lower bound \eqref{eq:lower-bound}, \ie\ rate optimal. 