% \begin{proof}[Proof of the Theorem \ref{thm:lower-bound}]
% We required some ingredients to establish our lower bound. For an easier read, we put them in a list in the following definition.
% \begin{definition}
%     \begin{enumerate}
%     \item For an $h = L \times  \max_{k \le K_1} \mu_k^{\frac{\alpha}{(1 + \alpha)\gamma_k}} n^{-\frac{1}{2\gamma_k + d}}$ ($L > 0$ is a constant to be decided later) define $m =  \lfloor h^{-1} \rfloor $.   
%     \item  Define $\cG = \big[\{ ih + \frac{h}{2} : i = 0,  \dots, m - 1\}^d\big]$ as a uniform grid in $[0, 1]^d$ of size $m^d$ and $\cG_\eps$ as an $\epsilon$-net in $\ell_\infty$ metric, \ie\  $\cG_\eps = \cup_{x \in \cG} \cB(x, \eps, \ell_\infty) $, where $\cB(x, \eps, \ell_{\infty}) = \{y \in \cX : \|x-y\|_\infty \le \eps\}$.
%     \item Define $P_X = \text{Unif}(\cG_\eps) $. For such a distribution, note that  $\vol (\cG_\eps) = (m\eps)^d \le (h^{-1}\eps)^d $, which implies that for all $x \in \cG_\eps$ we have $p_X(x) = (h\eps^{-1})^d $. Setting $\eps = p_0 ^{-\nicefrac{1}{d}} h \wedge \frac{h}{3}$ we have $p_X(x) \ge p_0$ that satisfies the strong density assumption for $P_X$.
%     \item  Fix an $m_0 \le m^d$ and consider $\cG_0 \subset \cG$ such that $|\cG_0| =m_0$ and define $\cG_1 = \cG \backslash \cG_0$.
%     \item  For a function $\sigma: \cG_0 \to [M]$ define \begin{equation}
%     \Phi_{m, k}^{\sigma}(x) = \begin{cases}
%      \frac{1 - K_{\gamma, k} \eps^{\gamma_{k}}\bbI\{\sigma(y) = m\}}{2} & \text{when} ~ k \le K_1, ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
%      \frac{1}{2} & \text{elsewhere.}
%     \end{cases}
% \end{equation}
% \item    Consider a class of probability distributions $\{\mu_\theta: \theta \in \reals\}$ defined on the same support $\text{range}(\ell)$ that have mean $\theta$ and satisfy $\text{KL}(\mu_\theta, \mu_{\theta'})  \le c  (\theta - \theta')^2$ for some $c > 0$. A sufficient condition for constricting such a family of distributions can be found in Lemma \ref{lemma:KL-bound}. Some prominent examples of such family are location families of normal, binomial, Poisson distributions, etc.  Define the probability $P^\sigma([Y]_{m, k} \mid X = x) \sim \mu_{\Phi_{m, k}^{(\sigma)}(x)}$. 
% \end{enumerate}
% \end{definition}


% To see that $\eta_{\mu, m}^\sigma $ satisfies $\alpha$-margin condition, notice that
% \[
% \begin{aligned}
%     & \textstyle \eta_{\mu, m}^\sigma(x) = \begin{cases}
%      \frac{1 - h(\eps) \bbI\{\sigma(y) = m\}}{2} & \text{when} ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
%      \frac{1}{2} & \text{elsewhere.}
%     \end{cases}\\
%     & \textstyle h(\eps) = \sum_{k \le K_1} \mu_k K_{\gamma, k} \eps^{\gamma_{k}} 
% \end{aligned}
% \]
% Thus, for every $x \in \cB(y, \eps, \ell_\infty), y \in \cG_0$ the $\Phi_{\mu, m}^\sigma(x) = \frac12$ for all but one $m$ and at $m = \sigma(x)$ the $\Phi_{\mu, m}^\sigma(x) = \frac{1 - h(\eps)}2$. Thus, $\Delta_\mu^\sigma (x)  = \frac{h(\eps) }{2}$ at those $x$, and at all other $x$ we have $\Delta_\mu^\sigma(x) =0$. Thus, $P_X(0 < \Delta_\mu^\sigma(X) \le t) = 0 $ whenever $t < \frac{h(\eps) }{2} $ and for $t \ge \frac{h(\eps) }{2} $ we have 
% \[
% \begin{aligned}
%    P_X(0 < \Delta^\sigma(X) \le t) & \textstyle = P_X\big ( \Phi_m^\sigma(X) \neq \frac12 \text{ for some } m \in [M]\big ) \\
%    & \textstyle \le m_0 \eps^d \le K_\alpha\big(\frac{h(\eps) }{2}\big)^\alpha 
% \end{aligned}
% \] whenever
% \[
% \begin{aligned}
%     m_0  \textstyle \le K_\alpha \eps^{-d}\big(\frac{h(\eps) }{2}\big)^\alpha & \textstyle \le K_\alpha \eps^{-d} \max\{K_{\gamma, k}^\alpha; k \le K_1\} \eps^{\min_{ k} \alpha \gamma_{k}} 2^{-\alpha}\\
%     & \le K_\alpha 2^{-\alpha}  \{\max_{k} K_{\gamma, k}^\alpha\} \eps^{\min_k \alpha \gamma_k -d}
% \end{aligned}
% \] We set $m_0 = \lfloor K_\alpha 2^{-\alpha}  \{\max_{k} K_{\gamma, k}^\alpha\} \eps^{\min_{k} \alpha \gamma_{k} -d} \rfloor$ to meet the requirement.
% Since $d > \min_{k} \alpha\gamma_{k}$ for sufficiently small  $\eps$ we have $m_0 \ge 1$. 

% % $m_0 \le K_\alpha 2^{-\alpha }K_\beta ^\alpha  \eps^{\alpha \beta - d}$. We set $m_0 = \lfloor  K_\alpha 2^{-\alpha }K_\beta ^\alpha  \eps^{\alpha \beta - d} \rfloor$ to meet the requirement. 
% % Since $\alpha\beta < d$ for sufficiently small  $\eps$ we have $m_0 \ge 1$. 

% Furthermore, on the support of $P_X$ the $\Phi_{m, k}^\sigma$ are $(\gamma_{k}, K_{\gamma, k})$ H\"older smooth. To see this note that the only way $\Phi_{m, k}^\sigma(x)$ and $\Phi_{m,k}^\sigma(x')$ can be different if $\|x- x'\|_\infty  \ge \frac{h}{3}$. Since $\eps \le \frac h3$ for such a choice, we have 
% \[
% \begin{aligned}
%     \textstyle |\Phi_{m, k}^\sigma(x) - \Phi_{m, k}^\sigma(x') | & \textstyle \le \frac12 K_{\gamma, k} \eps ^\beta\\
%     & \textstyle \le  K_{\gamma, k} (\frac h3) ^\beta\\
%     & \textstyle \le K_{\gamma, k} \|x - x'\|_\infty ^\beta \le K_{\gamma, k} \|x - x'\|_2^\beta\,.
% \end{aligned}
% \]

% Now, consider the probability distribution $P^\sigma$ for the random pair $(X, Y)$ where $X \sim P_X$ and given $X$ the $\{[Y]_{m, k}; m \in [M], k \le K_1\}$ are all independent and distributed as $[Y]_{m, k} \mid X = x \sim \mu_{\Phi_{m, k}^{\sigma}(x)}$.  For two different $\sigma_1$ and $\sigma_2$ we want to calculate the $\text{KL}(P^{\sigma_1}, P^{\sigma_2})$. Here, 
% \[
% \begin{aligned}
%     & \text{KL}(P^{\sigma_1}, P^{\sigma_2}) & \\
%     & = \textstyle \int dP_X(x) \sum_{m=1}^M \sum_{k = 1 }^K\text{KL}\big(\mu_{\Phi_{m, k}^{(\sigma_1)}(x)} , \mu_{\Phi_{m,k}^{(\sigma_2)}(x)} \big)  & \\
%     & \le \textstyle \int dP_X(x) \sum_{m=1}^M \sum_{k = 1 }^Kc\big({\Phi_{m, k}^{(\sigma_1)}(x)} - {\Phi_{m,k}^{(\sigma_2)}(x)} \big)^2 \quad \quad (\text{KL}(\mu_\theta, \mu_{\theta'})  \le c  (\theta - \theta')^2)\\
%     & = \textstyle \sum_{y \in \cG_0 } \eps^d\sum_{m=1}^M \sum_{k \le K_1} \frac{cK_{\gamma, k}^2\eps^{2\gamma_{k}}}{4}\big(\bbI\{\sigma_1(y) = m\}  - \bbI\{\sigma_2(y) = m\} \big)^2\\
%     & \le \textstyle \frac{c \max_{m, k}K_{\gamma, k}^2 }{4}\sum_{y \in \cG_0 }   \sum_{k \le K_1} \eps ^{2\gamma_{k} + d }\times \bbI\{\sigma_1(y) \neq \sigma_2(y) \}  \\
%     & \textstyle =  \frac{c K_1\max_{m, k}K_{\gamma, k}^2 }{4} \eps^{2\min_k \gamma_k + d} \delta(\sigma_1 , \sigma_2) \le C h^{2\min\gamma_k + d}\delta(\sigma_1 , \sigma_2) \quad \quad  \textstyle (\eps \le \frac{h}{3})
% \end{aligned}
% \] for some $C>0$, where $\delta(\sigma_1, \sigma_2) = \sum_{y \in \cG_0 }   \bbI\{\sigma_1(y) \neq \sigma_2(y) \} $ is the Hamming distances between $\sigma_1$ and $\sigma_2$. Thus, for the joint distributions of the dataset $\cD_n$ the  Kulback-Leibler divergence  is upper bounded as:
% \[
% \begin{aligned}
%     \textstyle  \text{KL}\big(\{P^{\sigma_1}\}^{\otimes n}, \{P^{\sigma_2}\}^{\otimes n}\big) 
%      = n \text{KL}({P^{\sigma_1}}, {P^{\sigma_2}})  \le n C h^{2\min\gamma_k + d}\delta(\sigma_1 , \sigma_2)\,.
% \end{aligned}
% \]
% Now, we establish a lower bound for the excess risk 
% \[
% \textstyle \cE_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1}) = \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1}) - \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_0})
% \]
% where $g^\star_{\mu, \sigma_0}$ is the Bayes classifier for $P^{\sigma_0}$ defined as $g^\star_{\mu, \sigma_0}(x) = \argmin_m \Phi_{\mu, m}^{\sigma_0}(x)$. For the purpose, notice that
% \[
% \textstyle g^\star_{\mu, \sigma}(x) = 
%     \sigma(y)  ~~\text{whenever} ~ x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0\,.
% \] This further implies 
% \[
% \begin{aligned}
%      & \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1})\\
%     & \textstyle = \int dP_X(x) \sum_{m = 1}^M \bbI\{ g^\star_{\mu, \sigma_1}(x) = m \} \Phi_{\mu, m}^{\sigma_0}(x)\\
%     & \textstyle = \sum_{y \in \cG_0} \eps^d \sum_{m = 1}^M  \bbI\{ \sigma_1(y) = m \} \sum_{k \le K_1} \mu_k \frac{1}{2} \big \{1 - K_{\gamma, k} \eps^{\gamma_{k}} \bbI\{ \sigma_0(y) = m \} \big\} \\
%     & \textstyle \quad  + \sum_{y \in \cG_0} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \sum_{k \ge K_1 + 1}\frac{\mu_k}{2} + \sum_{y \in \cG_1} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \frac{1}{2} \\
%     & \textstyle =  -\sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}  \frac{\mu_k K_{\gamma, k} \eps^{\gamma_{k} + d}}{2}   \bbI\{ \sigma_0(y) = \sigma_1(y) = m \} \\
%     & \textstyle \quad + \sum_{y \in \cG_0\cup \cG_1} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \frac{1}{2}\\
%     & \textstyle =  -\sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}  \frac{\mu_k K_{\gamma, k} \eps^{\gamma_{k} + d}}{2}   \bbI\{ \sigma_0(y) = \sigma_1(y) = m \} +  \sum_{y \in \cG_0\cup \cG_1}   \frac{\eps^d}{2}\\
% \end{aligned}
% \] 
% By replacing $\sigma_1$ with $\sigma_0$ in the above calculations we obtain 
% \[
% \textstyle\Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_0})= -\sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}  \frac{\mu_k K_{\gamma, k} \eps^{\gamma_{k} + d}}{2}   \bbI\{ \sigma_0(y)  = m \} +  \sum_{y \in \cG_0\cup \cG_1}   \frac{\eps^d}{2}
% \]
% and hence 
% \[
% \begin{aligned}
%     & \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu)\\
%     & = \Ex_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu) - \Ex_{P^{\sigma_0}}(g^\star_{\mu, \sigma_0}, \mu)\\
%     & = \textstyle \sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}  \frac{\mu_k K_{\gamma, k} \eps^{\gamma_{k} + d}}{2}   \big\{ \bbI\{ \sigma_0(y) =  m \} -  \bbI\{ \sigma_0(y)  =\sigma_1(y) = m \}\big\}\\
%     & \ge \textstyle \frac{\min_{k}K_{\gamma, k}}{2}\sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d} \big\{ \bbI\{ \sigma_0(y) =  m \} -  \bbI\{ \sigma_0(y)  =\sigma_1(y) = m \}\big\}\\
%     & = \textstyle \frac{\min_{k}K_{\gamma, k}}{2}\sum_{y \in \cG_0}  \sum_{m = 1}^M  \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d}  \bbI\{ \sigma_0(y) =  m \}\times \bbI\{ \sigma_1(y) \neq  m \} \\
%     & = \textstyle \frac{\min_{k}K_{\gamma, k}}{2}\sum_{y \in \cG_0}   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d}   \bbI\{ \sigma_1(y) \neq  \sigma_0(y) \} \\
%     & = \textstyle c'   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d}   \delta(\sigma_0, \sigma_1) \\
% \end{aligned}
% \] for some $c'>0$. 


% To obtain a minimax lower bound let us recall the generalized Fano's lemma. 

% \begin{lemma}[Generalized Fano's lemma]
% \label{lemma:fano}
% Let $r \ge 2$ be an integer and let $\cM_r \subset \cP$ contains $r$ probability measures indexed by $\{1, \dots , r\}$ such that for a pseudo-metric $d$ (\ie\ $d(\theta , \theta') = 0$ if and only if $\theta = \theta'$) any $j \neq j'$
% \[
% \textstyle d\big (\theta(P_j), \theta(P_j')\big) \ge \alpha_r , ~~\text{and} ~~ \text{KL}(P_j , P_j') \le \beta_r\,.
% \] Then 
% \[
% \textstyle \max\limits_j \Ex_{P_j}\big[ d(\theta(P_j), \widehat \theta)\big ] \ge \frac{\alpha_r}{2} \big (1 - \frac{\beta_r + \log 2}{\log r}\big )\,. 
% \]
    
% \end{lemma} In our construction $\theta(P^\sigma)  = g^\star_{\mu, \sigma}$ and $d\big (\theta(P^{\sigma_0}), \theta(P^{\sigma_1})\big) = \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_{1}}, \mu)$. To construct the $\cM_r$ we  recall the Gilbert–Varshamov bound for linear codes. 
% \begin{lemma}[Gilbert–Varshamov bound]
% \label{lemma:Gilbert–Varshamov}
%    Consider the maximal $A_M(m_0, d) \subset [M]^{m_0}$ such that each element in $C$ is at least $d$ Hamming distance from each other, \ie\ for any $\sigma_1 , \sigma_2\in C$ we have $\delta(\sigma_1 , \sigma_2) \ge d$. Then 
%    \[
%  \textstyle   |A_M(m_0, d)| \ge \frac{M^{m_0}}{\sum_{i = 0}^{d-1} {m_0 \choose i} (M-1)^i }
%    \]
%     Furthermore, when $M\ge 2$ and $0 \le p \le 1 - \frac1M$ we have $|A_M(m_0, pm_0 )| \ge M^{m_0 (1 - h_{M}(p))}$ where $h_{M}(p) =  \frac{p \log(M - 1) - p\log p - (1 - p)\log(1 - p)}{\log M}$. 
% \end{lemma}
% For the choice $p = \frac14$ we have $- p\log p - (1 - p)\log(1 - p) \le \frac{1}{4}$ and thus 
% \[
% \textstyle h_{M}(p) \le \frac{\log(M-1)}{4\log M} + \frac{1}{4\log M} \le \frac14 + \frac{1}{4\log 2} \le \frac{3}{4}\,.
% \]
% Consequently, the lemma implies that we can find an  $A_M(m_0, \frac{m_0}{4}) \subset [M]^{m_0}$ such that $|A_M(m_0, \frac{m_0}{4})| \ge M^{\frac{m_0} 4}$ whose each element is at least $\frac{m_0}{4}$ Hamming distance apart. For such a choice, define the collection of probabilities as  $\cM_r = \{P^\sigma: \sigma \in A_M(m_0, \frac{m_0}{4})\}$ leading to $r \ge M^{\frac{m_0}{4}}$. In the generalized Fano's lemma \ref{lemma:fano} we require $r\ge 2$. To achieve that we simply set $m_0 \ge 8$, as it implies $r \ge M^2 \ge 4$.  

 
 
%  Now we find lower bound $\alpha_r$ for the semi-metric and upper bound $\beta_r$ for the Kulback-Leibler divergence. 
%  The semi-metric is lower bounded as 
% \[
% \begin{aligned}
%      d\big (\theta(P^{\sigma_0}), \theta(P^{\sigma_1})\big)
%     &= \textstyle \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu)\\
%     & \textstyle  \ge c'   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d}   \delta(\sigma_0, \sigma_1)\\
%     & \textstyle  \ge c_1   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d}   \frac{m_0} 4\\
%     & \textstyle \ge c_2   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} + d + \min_k \alpha\gamma_k - d}   \\
%     & \textstyle \ge c_2   \sum_{k \le K_1}   \mu_k \eps^{\gamma_{k} +   \alpha\gamma_k }    = \alpha_r 
% \end{aligned}
% \] for some constants $c', c_1, c_2 > 0$.
% Note that, for any $k \le K_1$ the 
% \[
% \textstyle \alpha_r \ge c_2 \mu_k \eps^{(1 + \alpha)\gamma_{k}  } \ge c_2 (\frac13 \wedge p_0^{-\frac1d}) L \mu_k \mu_k ^{\frac{\alpha }{(1 + \alpha) \gamma_k } \times (1+\alpha) \gamma_k} n^{-\frac{(1 + \alpha)\gamma_k}{2\gamma_k + d}} \ge c_3 \big\{ \mu_k n^{-\frac{\gamma_k}{2\gamma_k + d}} \big\}^{1+\alpha}
% \]
% for some $c_3>0$ and thus 
% \[
% \textstyle \alpha_r \ge c_4 \big\{\sum_{k \le K_1} \mu_k n^{-\frac{\gamma_k}{2\gamma_k + d}} \big\}^{1+\alpha}
% \] for some $c_4>0$. 

% The Kulback-Leibler divergence for the joint distribution of $n$ iid data-points is upper bounded as:
% \[
% \begin{aligned}
%      \text{KL}\big(\{P^{\sigma_1}\}^{\otimes n}, \{P^{\sigma_2}\}^{\otimes n}\big) 
%     & = n \text{KL}({P^{\sigma_1}}, {P^{\sigma_2}}) \\
%     & \le n C h^{2\min\gamma_k + d}\delta(\sigma_1 , \sigma_2)\\
%     & \le  n C h^{2\min\gamma_k + d} m_0 \\
%     & \le \textstyle  \frac C4 n  h^{2\min\gamma_k + d} \frac{\log r}{\log M}\quad \quad (r \ge M^{\frac{m_0}{4}})\\
%     & \le C' L  \log r = \beta_r 
% \end{aligned}
% \] where $C, C'>0$ absorbs all the constants that doesn't depend on $n$ and the last inequality holds because $r \ge M^{\frac{m_0}{4}}$. 
% We plug in the lower and upper bound in the Fano's lemma \ref{lemma:fano} to obtain the lower bound: 
% \[
% \textstyle \frac{\alpha_r}{2} \big (1 - \frac{\beta_r + \log 2}{\log r}\big ) = \frac{c_1  h^{\beta (1 + \alpha)}}{2} \big (1 - \frac{C' n h^{2\beta + d} \log r  + \log 2}{\log r}\big )
% \] Now, we obtain an upper bound for the $\frac{C' n h^{2\beta + d} \log r  + \log 2}{\log r}$ term. 
% Note that 
% \[
% \begin{aligned}
%     \textstyle \frac{C' n h^{2\beta + d} \log r  + \log 2}{\log r} & \textstyle = C' n h^{2\beta + d} + \frac{\log 2}{\log r}
% \end{aligned}
% \] Since $r \ge 4$ we have $\frac{\log 2}{\log r } \le \frac{\log 2}{\log 4 } = \frac12$. Next, since  $h = k \mu^{\frac{1}{\beta}} n^{-\frac{1}{2\beta + d}}$ we have  $C' n h^{2\beta + d} \le \frac{1}{4}$ whenever  $ k \le \frac{1}{4C'} \mu^{-2 - \frac{d}{\beta}} $. For such a choice the lower bound is
% \[
% \begin{aligned}
%     \textstyle \frac{c_1  h^{\beta (1 + \alpha)}}{2} \big (1 - \frac{C' n h^{2\beta + d} \log r  + \log 2}{\log r}\big ) & \textstyle \ge \frac{ c_1 \big(k^\beta \mu n^{-\frac{\beta}{2\beta + d}}\big)^{1 + \alpha}}{2}  (1 - \frac{1}{4} - \frac{1}{2})  \\
%     & \ge c_2 \big( \mu n^{-\frac{\beta}{2\beta + d}}\big)^{1 + \alpha}
% \end{aligned}
% \] for some $c_2>0$ that is independent of both $n$ and $\mu$ and thus we have the lower bound. 

% % Finally, recall that we require $m_0 \ge 8$. Since 
% % \[
% % \begin{aligned}
% %      m_0 & \le \textstyle K_\alpha 2^{-\alpha }K_\beta ^\alpha \lambda ^\alpha \eps^{\alpha \beta - d}\\
% %      & \textstyle \le  K_\alpha 2^{-\alpha }K_\beta ^\alpha \lambda ^\alpha (\nicefrac{h}{3})^{\alpha \beta - d}\\
% %      & \textstyle \le  \frac{K_\alpha K_\beta^\alpha}{2^\alpha 3^{\alpha\beta - d}}  \lambda ^\alpha (4C'n)^{-\frac{\alpha\beta -d}{2\beta +d }}
% % \end{aligned}
% % \] we require $\lambda \ge c_1 n^{-\frac{\beta -\nicefrac d\alpha}{2\beta +d }}$. This completes the proof. 




% % Consider an $h > 0$ such that $h^{-1}$ is an integer. Define $m =  h^{-1} $ and for an  set $P_X = \text{Unif}(\cG_\eps) $ where $\cG = \big[\{ ih : i = 0,  \dots, m - 1\}^d\big]$ is an uniform grid of dimensions $d$ and $\cG_\eps$ is an $\epsilon$-net in $\ell_\infty$ metric, \ie\  $\cG_\eps = \cup_{x \in \cG} \cB(x, \eps, \ell_\infty) $. Note that $\vol (\cG_\eps) = (m\eps)^d \le (h^{-1}\eps)^d $, which implies that for all $x \in \cG_\eps$ we have $p_X(x) = (h\eps^{-1})^d $. By setting $\eps = p_0 ^{-\nicefrac{1}{d}} h$ we have $p_X(x) \ge p_0$ that satisfies the strong density assumption for $P_X$.  Consider $\ell\{Y, f_l(X)\} \mid X = x_0 \sim \phi(\cdot  , \kappa^{(\sigma)}(x, l))$. 


% % Now, fix a $m_0 \le m^d$ and consider $\cG_0 \subset \cG$ such that $|\cG_0| =m_0$ and define $\cG_1 = \cG \backslash \cG_0$. Consider a function $\sigma: \cG_0 \to [L]$ and define 
% % \begin{equation}
% %     \kappa^{(\sigma)}(x, l) = \begin{cases}
% %         C & \text{for} ~ \|x - y \|_\infty \le \eps, y \in \cG_1\\
% %         C - K_\alpha \eps^{\alpha}\delta _{\sigma(y)} & \text{for} ~ ~ \|x - y \|_\infty \le \eps, y \in \cG_0\\
% %     \end{cases}
% % \end{equation} Note that, on the support of $P_X$ the $\kappa^{(\sigma)}(x, l)$ is $\alpha$-H\"older smooth. Furthermore, it also satisfies the $\gamma$-margin condition, since for $t \ge K_\alpha \eps^\alpha$
% % \[
% % \begin{aligned}
% %     & \textstyle P_X \big ( 0 <\max_l \big | \kappa^{\sigma}(x, l) - \min_{l' \neq l} \kappa^{\sigma} (x, l') \big | \le t \big )\\
% %      & \le \textstyle P_X \big (  \kappa^{\sigma}(x, l) \neq C  \big ) = m_0 \eps ^d \le  K_\gamma \{K_\alpha \eps^ \alpha\}^\gamma 
% % \end{aligned}
% % \] by choosing $m_0 = \lfloor K_\gamma K_\alpha^\gamma \eps ^{\alpha \gamma - d}\rfloor $. We can show that the KL divergences between the two distributions are 
% % \[
% % \text{KL}(P_{\sigma_0}, P_{\sigma_1}) \le C nh^{2\alpha + d} \log M 
% % \]
% % and the semimetric 
% % \[
% % \cR_{P_{\sigma_0}}(g^\star_{\sigma_1}) \ge 
% % m h^{\alpha + d} = h^{\alpha(1 + \gamma)}
% % \] Thus, we have the lower bound. 

    
% \end{proof}




%\begin{proof}[Proof of the Theorem \ref{thm:lower-bound}]
%Our strategy for establishing the lower bound is the following: for every $k \le K_1$ we shall establish that for any $\eps_k \in [0, 1]$ and $n \ge 1$
%\begin{equation} \label{eq:lower-bound-individual}
   %  \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(\mu, A_n) \ge c_k \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha} \,,
%\end{equation} for some constant $c_k > 0$. Then, defining $c = \min\{c_k: k \le K_1\}$ we have the lower bound
%\[
%\begin{aligned}
  %  \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(\mu, A_n) & \ge \textstyle  \max\limits_{k \le K_1 } c_k \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha}\\
  %  & \ge \textstyle  \max\limits_{k \le K_1 } c \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha}\\
  %  & \ge \textstyle  c  \big \{ \sum_{k \le K_1 }\frac{\mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}}{K}\big\}^{1+\alpha}\\
   % & \ge \textstyle  c K^{-1-\alpha}  \big \{ \sum_{k \le K_1 }{\mu_k n^{- %\frac{\gamma_k}{2\gamma_k + d}}}\big\}^{1+\alpha}\,,\\
%\end{aligned}
%\] which would complete the proof. 

%It remains to establish \eqref{eq:lower-bound-individual} for each $k \in [K_1]$. 

Next, we lay out the template for constructing the family $\cM_r$. Fix a $k_0 \in [K_1]$ and define the following. 
\begin{definition}
    \begin{enumerate}
    \item For an $h = L \times   \mu_{k_0}^{\frac{1}{\gamma_{k_0}}} n^{-\frac{1}{2\gamma_{k_0} + d}}$ ($L > 0$ is a constant to be decided later) define $m =  \lfloor h^{-1} \rfloor $.   
    \item  Define $\cG = \big[\{ ih + \frac{h}{2} : i = 0,  \dots, m - 1\}^d\big]$ as a uniform grid in $[0, 1]^d$ of size $m^d$ and $\cG_\eps$ as an $\epsilon$-net in $\ell_\infty$ metric, \ie\  $\cG_\eps = \cup_{x \in \cG} \cB(x, \eps, \ell_\infty) $, where $\cB(x, \eps, \ell_{\infty}) = \{y \in \cX : \|x-y\|_\infty \le \eps\}$.
    \item Define $P_X = \text{Unif}(\cG_\eps) $. For such a distribution, note that  $\vol (\cG_\eps) = (m\eps)^d \le (h^{-1}\eps)^d $, which implies that for all $x \in \cG_\eps$ we have $p_X(x) = (h\eps^{-1})^d $. Setting $\eps = p_0 ^{-\nicefrac{1}{d}} h \wedge \frac{h}{3}$ we have $p_X(x) \ge p_0$ that satisfies the strong density assumption for $P_X$.
    \item  Fix an $m_0 \le m^d$ and consider $\cG_0 \subset \cG$ such that $|\cG_0| =m_0$ and define $\cG_1 = \cG \backslash \cG_0$.
    \item  For a function $\sigma: \cG_0 \to [M]$ define \begin{equation}
    \Phi_{m, k}^{\sigma}(x) = \begin{cases}
     \frac{1 - K_{\gamma, k_0} \mu_{k_0}^{-1} \eps^{\gamma_{k_0}}\bbI\{\sigma(y) = m\}}{2} & \text{when} ~ k =k_0, ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
     \frac{1}{2} & \text{elsewhere.}
    \end{cases}
\end{equation}
\item    Consider a class of probability distributions $\{\mu_\theta: \theta \in \reals\}$ defined on the same support $\text{range}(\ell)$ that have mean $\theta$ and satisfy $\text{KL}(\mu_\theta, \mu_{\theta'})  \le c  (\theta - \theta')^2$ for some $c > 0$. A sufficient condition for constricting such a family of distributions can be found in Lemma \ref{lemma:KL-bound}. Some prominent examples of such family are location families of normal, binomial, Poisson distributions, etc.  Define the probability $P^\sigma([Y]_{m, k} \mid X = x) \sim \mu_{\Phi_{m, k}^{(\sigma)}(x)}$. 
\end{enumerate}
\end{definition}
The following two lemmas (along with the observation on the strong density condition) will establish that for a given $\sigma$, the distribution over $\cX, \cY$ given by $P^\sigma([Y]_{m, k} \mid X = x) \times \text{Unif}[\cG_{\epsilon}]$ is indeed a member of the class $\cP$.
\begin{lemma}
    \label{lem: margin condition}
    Fix a choice for $\sigma$ and let $\eta_{\mu, m}^\sigma = \sum_k \mu_k \Phi_{k,m}^{\sigma}(x)$, then $\eta_{\mu, m}^\sigma $ satisfies $\alpha$-margin condition.
\end{lemma}
\begin{proof}
To see that $\eta_{\mu, m}^\sigma $ satisfies $\alpha$-margin condition, notice that
\[
\begin{aligned}
    & \textstyle \eta_{\mu, m}^\sigma(x) = \begin{cases}
     \frac{1 - K_{\gamma, k_0} \eps^{\gamma_{k_0}} \bbI\{\sigma(y) = m\}}{2} & \text{when} ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
     \frac{1}{2} & \text{elsewhere.}
    \end{cases}
\end{aligned}
\]
Thus, for every $x \in \cB(y, \eps, \ell_\infty), y \in \cG_0$ the $\Phi_{\mu, m}^\sigma(x) = \frac12$ for all but one $m$ and at $m = \sigma(x)$ the $\Phi_{\mu, m}^\sigma(x) = \frac{1 - K_{\gamma, k_0} \eps^{\gamma_{k_0}} }{2}$, leading to $\Delta_\mu^\sigma (x)  = \frac{K_{\gamma, k_0} \eps^{\gamma_{k_0}} }{2}$ at those $x$, and at all other $x$ we have $\Delta_\mu^\sigma(x) =0$. This further implies $P_X(0 < \Delta_\mu^\sigma(X) \le t) = 0 $ whenever $t < \frac{K_{\gamma, k_0} \eps^{\gamma_{k_0}} }{2} $ and for $t \ge \frac{K_{\gamma, k_0} \eps^{\gamma_{k_0}} }{2} $ we have 
\[
\begin{aligned}
   P_X(0 < \Delta^\sigma(X) \le t) & \textstyle = P_X\big ( \Phi_m^\sigma(X) \neq \frac12 \text{ for some } m \in [M]\big ) \\
   & \textstyle \le m_0 \eps^d \le K_\alpha\big(\frac{K_{\gamma, k_0} \eps^{\gamma_{k_0}} }{2}\big)^\alpha 
\end{aligned}
\] whenever
\[
\begin{aligned}
    m_0  \textstyle \le  K_\alpha 2^{-\alpha}   K_{\gamma, k_0}^\alpha \eps^{ \alpha \gamma_{k_0} -d}
\end{aligned}
\] We set $m_0 = \lfloor K_\alpha 2^{-\alpha}   K_{\gamma, k_0}^\alpha \eps^{ \alpha \gamma_{k_0} -d} \rfloor$ to meet the requirement.
Since $d > \min_{k} \alpha\gamma_{k}$, for sufficiently small  $\eps$ we have $m_0 \ge 8$. 
\end{proof}
% $m_0 \le K_\alpha 2^{-\alpha }K_\beta ^\alpha  \eps^{\alpha \beta - d}$. We set $m_0 = \lfloor  K_\alpha 2^{-\alpha }K_\beta ^\alpha  \eps^{\alpha \beta - d} \rfloor$ to meet the requirement. 
% Since $\alpha\beta < d$ for sufficiently small  $\eps$ we have $m_0 \ge 1$. 
\begin{lemma}
\label{lem: Holder}
   On the support of $P_X$ the $\Phi_{m, k}^\sigma$ are $(\gamma_{k}, K_{\gamma, k})$ H\"older smooth. 
\end{lemma}
\begin{proof}
 Note that the only way $\Phi_{m, k}^\sigma(x)$ and $\Phi_{m,k}^\sigma(x')$ can be different if $\|x- x'\|_\infty  \ge \frac{h}{3}$. Since $\eps \le \frac h3$ for such a choice, we have 
\[
\begin{aligned}
    \textstyle |\Phi_{m, k}^\sigma(x) - \Phi_{m, k}^\sigma(x') | & \textstyle \le \frac12 K_{\gamma, k} \eps ^\beta\\
    & \textstyle \le  K_{\gamma, k} (\frac h3) ^\beta\\
    & \textstyle \le K_{\gamma, k} \|x - x'\|_\infty ^\beta \le K_{\gamma, k} \|x - x'\|_2^\beta\,.
\end{aligned}
\]
\end{proof}
In order transfer the inequality in Fano's lemma to a statement on rate of convergence, we need an upper bound on $ \text{KL}(P^{\sigma_1}, P^{\sigma_2})$ and a lower bound on the semi-metric $\cE_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1})$. These are established in the next two lemmas.
\begin{lemma}
   Consider the probability distribution $P^\sigma$ for the random pair $(X, Y)$ where $X \sim P_X$ and given $X$ the $\{[Y]_{m, k}; m \in [M], k \le K_1\}$ are all independent and distributed as $[Y]_{m, k} \mid X = x \sim \mu_{\Phi_{m, k}^{\sigma}(x)}$. Let $C$ be a positive constant and $\delta(\sigma_1, \sigma_2) = \sum_{y \in \cG_0 }   \bbI\{\sigma_1(y) \neq \sigma_2(y) \} $ the Hamming distance between $\sigma_1$ and $\sigma_2$. Then following upper bound holds on $\text{KL}(P^{\sigma_1}, P^{\sigma_2})$. 
   \[ \text{KL}(P^{\sigma_1}, P^{\sigma_2}) \leq C \mu_{k_0}^{-2} h ^{2\gamma_{k_0} + d }\delta(\sigma_1 , \sigma_2)\]
\end{lemma}
\begin{proof}
%Now, consider the probability distribution $P^\sigma$ for the random pair $(X, Y)$ where $X \sim P_X$ and given $X$ the $\{[Y]_{m, k}; m \in [M], k \le K_1\}$ are all independent and distributed as $[Y]_{m, k} \mid X = x \sim \mu_{\Phi_{m, k}^{\sigma}(x)}$.  For two different $\sigma_1$ and $\sigma_2$ we want to calculate the $\text{KL}(P^{\sigma_1}, P^{\sigma_2})$. Here, 
\[
\begin{aligned}
    & \text{KL}(P^{\sigma_1}, P^{\sigma_2}) & \\
    & = \textstyle \int dP_X(x) \sum_{m=1}^M \sum_{k = 1 }^K\text{KL}\big(\mu_{\Phi_{m, k}^{(\sigma_1)}(x)} , \mu_{\Phi_{m,k}^{(\sigma_2)}(x)} \big)  & \\
    & \le \textstyle \int dP_X(x) \sum_{m=1}^M \sum_{k = 1 }^Kc\big({\Phi_{m, k}^{(\sigma_1)}(x)} - {\Phi_{m,k}^{(\sigma_2)}(x)} \big)^2 \quad \quad (\text{KL}(\mu_\theta, \mu_{\theta'})  \le c  (\theta - \theta')^2)\\
    & = \textstyle \sum_{y \in \cG_0 } \eps^d\sum_{m=1}^M  \frac{cK_{\gamma, k_0}^2\eps^{2\gamma_{k_0}}\mu_{k_0}^{-2} }{4}\big(\bbI\{\sigma_1(y) = m\}  - \bbI\{\sigma_2(y) = m\} \big)^2\\
    & \le \textstyle \frac{c K_{\gamma, k_0}^2 }{4}\sum_{y \in \cG_0 }  \mu_{k_0}^{-2} \eps ^{2\gamma_{k_0} + d }\times \bbI\{\sigma_1(y) \neq \sigma_2(y) \}  \\
    & \textstyle \le   C \mu_{k_0}^{-2} h ^{2\gamma_{k_0} + d }\delta(\sigma_1 , \sigma_2) \quad \quad  \textstyle (\text{because} ~~\eps \le \frac{h}{3})
\end{aligned}
\] for some $C>0$, where $\delta(\sigma_1, \sigma_2) = \sum_{y \in \cG_0 }   \bbI\{\sigma_1(y) \neq \sigma_2(y) \} $ is the Hamming distances between $\sigma_1$ and $\sigma_2$. 
\end{proof}
Now, we establish a closed form for the excess risk 
\[
\textstyle \cE_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1}) = \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1}) - \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_0})
\]
where $g^\star_{\mu, \sigma_0}$ is the Bayes classifier for $P^{\sigma_0}$ defined as $g^\star_{\mu, \sigma_0}(x) = \argmin_m \Phi_{\mu, m}^{\sigma_0}(x)$. 
\begin{lemma}
    Let $\delta(\sigma_0, \sigma_1)$ denote the Hamming distance between $\sigma_0$ and $\sigma_1$ as before. Then
    \[\cE_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1}) = \textstyle \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d} \delta(\sigma_0, \sigma_1)}{2} \]
\end{lemma}
\begin{proof}
    
For the purpose, notice that
\[
\textstyle g^\star_{\mu, \sigma}(x) = 
    \sigma(y)  ~~\text{whenever} ~ x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0\,.
\] This further implies 
\[
\begin{aligned}
     & \Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_1})\\
    & \textstyle = \int dP_X(x) \sum_{m = 1}^M \bbI\{ g^\star_{\mu, \sigma_1}(x) = m \} \Phi_{\mu, m}^{\sigma_0}(x)\\
    & \textstyle = \sum_{y \in \cG_0} \eps^d \sum_{m = 1}^M  \bbI\{ \sigma_1(y) = m \}  \mu_{k_0} \frac{1}{2} \big \{1 - K_{\gamma, k_0} \mu_{k_0}^{-1} \eps^{\gamma_{k_0}} \bbI\{ \sigma_0(y) = m \} \big\} \\
    & \textstyle \quad  + \sum_{y \in \cG_0} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \sum_{k \neq k_0}\frac{\mu_k}{2} + \sum_{y \in \cG_1} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \frac{1}{2} \\
    & \textstyle =  -\sum_{y \in \cG_0}  \sum_{m = 1}^M    \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}   \bbI\{ \sigma_0(y) = \sigma_1(y) = m \} \\
    & \textstyle \quad + \sum_{y \in \cG_0\cup \cG_1} \eps^d \sum_{m = 1}^M \bbI\{ \sigma_1(y) = m \} \frac{1}{2}\\
    & \textstyle =  -\sum_{y \in \cG_0}  \sum_{m = 1}^M  \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}   \bbI\{ \sigma_0(y) = \sigma_1(y) = m \} +  \sum_{y \in \cG_0\cup \cG_1}   \frac{\eps^d}{2}\\
\end{aligned}
\] 
By replacing $\sigma_1$ with $\sigma_0$ in the above calculations we obtain 
\[
\textstyle\Ex_{P^{\sigma_0}}(\mu, g^\star_{\mu, \sigma_0})= -\sum_{y \in \cG_0}  \sum_{m = 1}^M  \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}   \bbI\{ \sigma_0(y) =  m \} +  \sum_{y \in \cG_0\cup \cG_1}   \frac{\eps^d}{2}
\]
and hence 
\[
\begin{aligned}
    & \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu)\\
    & = \Ex_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu) - \Ex_{P^{\sigma_0}}(g^\star_{\mu, \sigma_0}, \mu)\\
    & = \textstyle \sum_{y \in \cG_0}  \sum_{m = 1}^M  \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}  \big\{ \bbI\{ \sigma_0(y) =  m \} -  \bbI\{ \sigma_0(y)  =\sigma_1(y) = m \}\big\}\\
    & = \textstyle \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}\sum_{y \in \cG_0}  \sum_{m = 1}^M   \bbI\{ \sigma_0(y) =  m \}\times \bbI\{ \sigma_1(y) \neq  m \} \\
    & = \textstyle \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d}}{2}\sum_{y \in \cG_0}    \bbI\{ \sigma_0(y) \neq \sigma_1(y) \} \\
    & = \textstyle \frac{ K_{\gamma, k_0} \eps^{\gamma_{k_0} + d} \delta(\sigma_0, \sigma_1)}{2}   \,.
\end{aligned}
\] 
\end{proof}
The final technical ingredient we require is the Gilbert–Varshamov bound for linear codes. 
\begin{lemma}[Gilbert–Varshamov bound]
\label{lemma:Gilbert–Varshamov}
   Consider the maximal $A_M(m_0, d) \subset [M]^{m_0}$ such that each element in $C$ is at least $d$ Hamming distance from each other, \ie\ for any $\sigma_1 , \sigma_2\in C$ we have $\delta(\sigma_1 , \sigma_2) \ge d$. Then 
   \[
 \textstyle   |A_M(m_0, d)| \ge \frac{M^{m_0}}{\sum_{i = 0}^{d-1} {m_0 \choose i} (M-1)^i }
   \]
    Furthermore, when $M\ge 2$ and $0 \le p \le 1 - \frac1M$ we have $|A_M(m_0, pm_0 )| \ge M^{m_0 (1 - h_{M}(p))}$ where $h_{M}(p) =  \frac{p \log(M - 1) - p\log p - (1 - p)\log(1 - p)}{\log M}$. 
\end{lemma}
\begin{proof}[Proof of the Theorem \ref{thm:lower-bound}]
%To obtain a minimax lower bound let us recall the generalized Fano's lemma. 

%\begin{lemma}[Generalized Fano's lemma]
%\label{lemma:fano}
%Let $r \ge 2$ be an integer and let $\cM_r \subset \cP$ contains $r$ probability measures indexed by $\{1, \dots , r\}$ such that for a pseudo-metric $d$ (\ie\ $d(\theta , \theta') = 0$ if and only if $\theta = \theta'$) any $j \neq j'$
%\[
%\textstyle d\big (\theta(P_j), \theta(P_j')\big) \ge \alpha_r , ~~\text{and} ~~ \text{KL}(P_j , P_j') \le \beta_r\,.
%\] Then 
%\[
%\textstyle \max\limits_j \Ex_{P_j}\big[ d(\theta(P_j), \widehat \theta)\big ] \ge \frac{\alpha_r}{2} \big (1 - \frac{\beta_r + \log 2}{\log r}\big )\,. 
%\]
    
%\end{lemma} In our construction $\theta(P^\sigma)  = g^\star_{\mu, \sigma}$ and $d\big (\theta(P^{\sigma_0}), \theta(P^{\sigma_1})\big) = \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_{1}}, \mu)$. To construct the $\cM_r$ we  recall the Gilbert–Varshamov bound for linear codes. 
%\begin{lemma}[Gilbert–Varshamov bound]
%\label{lemma:Gilbert–Varshamov}
  % Consider the maximal $A_M(m_0, d) \subset [M]^{m_0}$ such that each element in $C$ is at least $d$ Hamming distance from each other, \ie\ for any $\sigma_1 , \sigma_2\in C$ we have $\delta(\sigma_1 , \sigma_2) \ge d$. Then 
  % \[
% \textstyle   |A_M(m_0, d)| \ge \frac{M^{m_0}}{\sum_{i = 0}^{d-1} {m_0 \choose i} (M-1)^i }
  % \]
   % Furthermore, when $M\ge 2$ and $0 \le p \le 1 - \frac1M$ we have $|A_M(m_0, pm_0 )| \ge M^{m_0 (1 - h_{M}(p))}$ where $h_{M}(p) =  \frac{p \log(M - 1) - p\log p - (1 - p)\log(1 - p)}{\log M}$. 
%\end{lemma}
For the choice $p = \frac14$ we have $- p\log p - (1 - p)\log(1 - p) \le \frac{1}{4}$ and thus 
\[
\textstyle h_{M}(p) \le \frac{\log(M-1)}{4\log M} + \frac{1}{4\log M} \le \frac14 + \frac{1}{4\log 2} \le \frac{3}{4}\,.
\]
Consequently, the lemma implies that we can find an  $A_M(m_0, \frac{m_0}{4}) \subset [M]^{m_0}$ such that $|A_M(m_0, \frac{m_0}{4})| \ge M^{\frac{m_0} 4}$ whose each element is at least $\frac{m_0}{4}$ Hamming distance apart. For such a choice, define the collection of probabilities as  $\cM_r = \{P^\sigma: \sigma \in A_M(m_0, \frac{m_0}{4})\}$ leading to $r \ge M^{\frac{m_0}{4}}$. In the generalized Fano's lemma \ref{lemma:fano} we require $r\ge 2$. To achieve that we simply set $m_0 \ge 8$, as it implies $r \ge M^2 \ge 4$.  




 
 
 Now we find lower bound $\alpha_r$ for the semi-metric and upper bound $\beta_r$ for the Kulback-Leibler divergence. 
Let's start with the upper bound. Since $\text{KL}(P^{\sigma_1}, P^{\sigma_2}) \le C \mu_{k_0}^{-2} h ^{2\gamma_{k_0} + d }\delta(\sigma_1 , \sigma_2)$ for the joint distributions of the dataset $\cD_n$ the  Kulback-Leibler divergence between $\{P^{\sigma_1}\}^{\otimes n}$ and $\{P^{\sigma_2}\}^{\otimes n}$ is upper bounded as:
\[
\begin{aligned}
    & \textstyle  \text{KL}\big(\{P^{\sigma_1}\}^{\otimes n}, \{P^{\sigma_2}\}^{\otimes n}\big) \\
     & = \textstyle n \text{KL}({P^{\sigma_1}}, {P^{\sigma_2}}) \\
     & \le n C \mu_{k_0}^{-2} h ^{2\gamma_{k_0} + d }\delta(\sigma_1 , \sigma_2)\\
     & =\textstyle  n C \mu_{k_0}^{-2} L^{2\gamma_{k_0} + d } \mu_{k_0}^{\frac{2\gamma_{k_0} + d}{\gamma_{k_0}}}  n ^{-\frac{2\gamma_{k_0} + d}{2\gamma_{k_0} + d}} \quad \quad \textstyle  (\text{because} ~ h \text{ is defined as } L \times   \mu_{k_0}^{\frac{1}{\gamma_{k_0}}} n^{-\frac{1}{2\gamma_{k_0} + d}} )\\
     & \le\textstyle   C  L^{2\gamma_{k_0} + d } \mu_{k_0}^{\frac{ d}{\gamma_{k_0}}}  \frac{\log r}{\log M} \quad \quad (\text{because} ~~ r \ge M^{\frac{m_0}{4}})\\
     & \le \textstyle C  L^{2\gamma_{k_0} + d }  \frac{\log r}{\log M} = \beta_r
\end{aligned}
\]
In the Lemma \ref{lemma:fano} we would like $\frac{\beta_r + \log 2}{\log r} \le \frac{3}{4}$ so that we have $1 - \frac{\beta_r + \log 2}{\log r} \ge \frac14$. 
Note that, 
\[
\begin{aligned}
   \textstyle  \frac{\beta_r + \log 2}{\log r} - \frac{3}{4} & \textstyle = \frac{\beta_r}{\log r}  + \frac{\log 2}{\log r} - \frac{3}{4} \\
   & \textstyle = \frac{C L^{2\gamma_{k_0} + d }}{\log M}  + \frac{\log 2}{\log 4} - \frac{3}{4} \quad \quad (\text{because} ~~ r \ge 4,~ \beta_r = C  L^{2\gamma_{k_0} + d }  \frac{\log r}{\log M} )\\
   & = \textstyle \frac{C L^{2\gamma_{k_0} + d }}{\log M} - \frac14 \le 0\\
\end{aligned}
\] for small $L > 0$. We set the $L$ accordingly. Returning to the semi-metric, it is lower bounded as 
\[
\begin{aligned}
     d\big (\theta(P^{\sigma_0}), \theta(P^{\sigma_1})\big)
    &= \textstyle \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_1}, \mu)\\
    & \textstyle  \ge \frac{ K_{\gamma, k_0} }{2} \eps^{\gamma_{k_0} + d} \delta(\sigma_0, \sigma_1)\\
   & \textstyle  \ge \frac{ K_{\gamma, k_0} }{2} \eps^{\gamma_{k_0} + d}    \frac{m_0} 4\\
    & \textstyle \ge \frac{ K_{\gamma, k_0} }{8} \eps^{\gamma_{k_0} + d}    K_\alpha 2^{-\alpha}   K_{\gamma, k_0}^\alpha \eps^{ \alpha \gamma_{k_0} -d} \\
    & \quad \quad  (\text{because} ~~ m_0 = \lfloor K_\alpha 2^{-\alpha}   K_{\gamma, k_0}^\alpha \eps^{ \alpha \gamma_{k_0} -d} \rfloor)  \\
    & \textstyle = c_1   \eps^{(1+\alpha) \gamma_{k_0}}\\
    & \textstyle \ge c_2 \big\{\mu_{k_0} n^{-\frac{\gamma_{k_0}}{2\gamma_{k_0} + d}}\big\}^{1 + \alpha}= \alpha_r 
\end{aligned}
\] for some constants $ c_1, c_2 > 0$.
We plug in the lower and upper bound in the Fano's lemma \ref{lemma:fano} to obtain the lower bound: 
\[
\textstyle \frac{\alpha_r}{2} \big (1 - \frac{\beta_r + \log 2}{\log r}\big ) \ge \frac{c_2 \big\{\mu_{k_0} n^{-\frac{\gamma_{k_0}}{2\gamma_{k_0} + d}}\big\}^{1 + \alpha}}{2} \times \frac14 \ge c_3 \big\{\mu_{k_0} n^{-\frac{\gamma_{k_0}}{2\gamma_{k_0} + d}}\big\}^{1 + \alpha} 
\] for some $c_3 > 0$ that is independent of both $n$ and $\mu$. 

\end{proof}


