\section{Routing in a general multi-objective framework}\label{sec:general-multi-obj}

In this section, we discuss a natural extension of the routing problem with general metrics. Since most of the constructions are very similar to the construction in the Introduction section \ref{sec:intro}, we keep our discussion short.  Say, we are provided $M$ pre-trained models, denoted as $f_1, \dots,f_M$. Given model $f_m$ and a sample point $(X, Y)$,  we are interested in $L$ different loss metrics, among which the first $L_1 $ many, denoted as $\ell_1\{f_m; X, Y\} , \dots, \ell_{L_1}\{f_m; X, Y\}$, depends on $f_m$, $X$ and $Y$, while the next $L_2 = L - L_1$ many, denoted as $\ell_{L_1+ 1}(f_m; X), \dots , \ell_{L}(f_m; X)$, depends only on $f_m$ and $X$.  In that case we consider a compromise between these $L$ losses: for an $\lambda = (\lambda_1 , \dots, \lambda_L) \in\Delta^{L-1} $ the linear loss trade-off in eq. \eqref{eq:linearized-loss} naturally extends to 
\begin{equation} \label{eq:linearized-loss-gen}
 \textstyle \eta_{\lambda, m}(X, Y) =  \sum_{l = 1}^{L_1}\lambda_l \ell_l\{f_m; X, Y\} + \sum_{l = L_1 + 1}^{L}\lambda_l \ell_l\{f_m; X\}   \,.
\end{equation} 
To make it even more general, assume that we have a classification task with $M$ classes, \ie \ the outcome $Y \in \{1, \dots, M\}$ is now categorical. Additionally, as a multi-objective problem we have $L$ different loss metrics, among which the first $L_1 $ many $\ell_1\{m; X, Y\} , \dots, \ell_{L_1}\{m; X, Y\}$ are losses  of predicting a sample $(X, Y)$ as the class $m$, while the next $L_2 = L - L_1$ many $\ell_{L_1+ 1}\{m; X\}, \dots , \ell_{L}\{m; X\}$ is the cost of predicting the sample $X$ as the class $m$. Then the aggregated loss is 
\begin{equation} \label{eq:linearized-loss-gen-2}
 \textstyle \eta_{\lambda, m}(X, Y) =  \sum_{l = 1}^{L_1}\lambda_l \ell_l\{m; X, Y\} + \sum_{l = L_1 + 1}^{L}\lambda_l \ell_l\{m; X\}   \,.
\end{equation} 

For a particular $\lambda \in \Delta^{L-1}$ we want to estimate the optimal/Bayes/admissible classifier $g_\lambda^\star$ that minimized the average of the aggregates loss trade-off:
\begin{equation}
    \textstyle g_\lambda^\star = \argmin_{g: \cX \to [M]}  \Ex_P\big[\sum_{m = 1}^M \bbI\{g(X) = m\} \eta_{\lambda, m} (X, Y) \big ]
\end{equation}
Similar to Lemma \ref{lemma:oracle-router}, defining $\Phi_{l, m}^\star(X) = \Ex[\ell_l\{f_m; X,Y\} \mid X]$ for $l \in [L_1]$ and $\Phi_{l, m}^\star(X) = \ell_l\{f_m; X\} $ for $l \in \{L_1 + 1, \dots, L\}$ the Bayes classifier $g_\lambda^\star$ has the explicit form  (\cf\ Lemma \ref{lemma:bayes-classifier-gen})
\begin{equation}\label{eq:oracle-router-gen}
  \textstyle  g_\lambda^\star (X) = \argmin_m  \eta_{\lambda, m}(X), ~~ \eta_{\lambda, m}(X) = \sum_{l = 1}^L \lambda_l \Phi_{l, m}^\star (X)\,.
\end{equation}
A natural extension of Algorithm \ref{alg:pareto-routers} requires us to estimate each of the functions $\Phi_{l, m}^\star$ as $\widehat \Phi_{l, m}$ then estimate all Bayes classifiers in a one-shot plug-in approach as:
\[
\textstyle \widehat g_\lambda(X) = \argmin_m  \{\sum_{l = 1}^L \lambda_l \widehat \Phi_{l, m} (X) \}\,.
\] 
Note that, for $l \ge L_1+ 1$ the functions $\Phi_{l, m}^\star$ are known, so they need not be estimated; in these cases, we simply let $\widehat \Phi_{l, m} = \Phi_{l ,m}^\star$. 
Having estimated $\widehat g_\lambda$, we can then evaluate them on a test data split with respect to each of the individual metrics to estimate the Pareto frontier and examine the optimal trade-off between the objectives.
We describe the procedure in Algorithm \ref{alg:pareto-routers-gen}.  

\begin{algorithm}
    \begin{algorithmic}[1]
\Require Dataset $\cD_n$
\State Randomly split the dataset into training and test splits: $\cD_n = \cD_{\text{tr}} \cup \cD_{\text{test}}$. 
\State  Learn estimates $\widehat \Phi_{l, m} (X)$ of the function $\Phi_{l, m}^\star(X)$ using the training split $\cD_{\text{tr}}$. 
\For{$\lambda \in \Delta^{L-1}$}
\State  Define $\widehat \eta_{\lambda, m}(X) =  \sum_{l = 1}^L \lambda_l \widehat \Phi_{l, m} (X)  $ and 
 $\widehat g_\lambda(X) = \argmin_m \widehat \eta_{\lambda, m}(X)$
 \For{$l \in \{1, \dots, L\}$}
 \State Calculate $\widehat \cE_{l, \lambda}  =  \frac1{|\cD_{\text{test}}|} \sum_{(X, Y) \in \cD_{\text{test}}}  \ell_l\{Y, f_{\widehat g_\lambda(X)}(X)\}$
 \EndFor
\EndFor

\Return $\{g_\lambda: \lambda \in \Delta^{L-1}\}$ and $\widehat\cF = \{(\widehat \cE_{1, \lambda}, \dots, \widehat \cE_{L, \lambda}): \lambda \in \Delta^{L-1}\}$. 
\end{algorithmic}
\caption{Learning of Bayes classifiers}
\label{alg:pareto-routers-gen}
\end{algorithm}


Except for some minor differences, the minimax rate analysis of the estimate of $g_\lambda^\star$ is identical to Section \ref{sec:lower-bound}. We assume that
\begin{enumerate}
    \item For $1 \le l \le L_1$ the $\Phi_{l, m}^\star$ functions are $(\beta_l, K_{\beta, l})$-smooth, which is similar to the Assumption \ref{assmp:smooth}. Recall the discussion following Assumption \ref{assmp:smooth}; (1) Since $\Phi_{l, m}$ are known for $l \ge L_1 + 1$ a smoothness assumption on them is not necessary, and (2) We could assume different smoothness for different $\Phi_{l, m}^\star$, \ie\ they are $\beta_{l, m}$ smooth, but the rate will only involve the smallest smoothness, \ie\ $\beta_{l, \min} = \min_m \beta_{l, m}$. Thus, for simplicity, we assume that for a given $l$ the $\beta_{l, m}$ are identical for different $m$. 
    \item The margin condition in Assumption \ref{assmp:margin} is satisfied with the new definition of $\eta_{\lambda, m}(X) = \Ex[\eta_{\lambda, m}(X, Y)\mid X]$ with parameters $(\alpha, K_\alpha)$.
    \item  The $P_X$ is compactly supported and satisfies the strong density condition, as in the assumption \ref{assmp:strong-density}.
\end{enumerate}

%  and that the margin condition is satisfied  Furthermore, we assume the 
% \SM{talk about how the worst smoothness parameter is the one that matters.}


Under these assumptions, we establish the upper and lower bound analysis for the minimax rate of convergence in excess risk
\[
\textstyle \cE_P(g, \lambda) = \cL_P(g, \lambda) - \cL_P(g_\lambda^\star, \lambda)\,.
\]
For this purpose, let us quickly recall the notation and definitions necessary to describe the lower and upper bounds. We denote the class of all probability distributions on $\cX \times \cY$ space by $\cP$.
We also denote $\Gamma = \{g: \cX \to [M]\}$ as the set of all classifiers and $\cA_n$ as the class of learning algorithms, such that an algorithm $\cA_n \ni A_n: \cZ^n \to \Gamma $ that takes the dataset $\cD_n $ as input and produces a classifier $A_n(\cD_n): \cX \to [M]$.

\begin{theorem}
\label{thm:bound-gen}
Suppose that $\max_{l \in  [L_1]}\beta_l <  \nicefrac{d}{\alpha}$. Then we have the following lower and upper bounds for the excess risk. 
\begin{itemize}
    \item {\bf Lower bound:} For $n \ge 1$  and $A_n \in \cA_n$ define $\cE_P(A_n, \lambda) = \Ex_{\cD_n}\big[\cE_P\big(A_n(\cD_n), \lambda\big)\big]$. There exist constants $c_1, c_2> 0$ that are independent of $n$ and $\lambda$ such that for any $n\ge 1$ and $ \lambda \in \Delta^{L-1}$ we have the lower bound
    \begin{equation} \label{eq:lower-bound-gen}
      \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(A_n, \lambda) \ge c \big \{\sum_{l = 1}^{L_1}\lambda_l n^{- \frac{\beta_l}{2\beta_l + d}}\big\}^{1+\alpha} \,.
    \end{equation}  
    \item  {\bf Upper bound:} For $l \le L_1$ and $m\in [M]$ suppose that there are estimators $\widehat\Phi_{l, m}$ for $\Phi^\star_{l, m}$ that satisfies the following: for constants $\rho_{l, 1}, \rho_{l, 2} > 0$ and any $n \ge 1$ and $t > 0$ and almost all $X$ with respect to $P_X$ we have 
    \begin{equation} \label{eq:concentration-phi-gen}
        \max_{P\in \cP} P \big \{ \max_m \big |\widehat \Phi_{l, m} (X) - \Phi^\star_{l, m}  (X)\big |  \ge t\big \} \le  \rho_{l, 1} \exp\big (- \rho_{l,2} a_{l, n}^2 t^2 \big )\,,  
    \end{equation}  where the sequence $\{a_{l, n}; n \ge 1\}\subset (0, \infty)$  increases to $\infty$. Then there exists a $K> 0$ such that for any $n \ge 1$ and $\lambda \in \Delta^{L-1} $ the excess risk for the router $\widehat g_\lambda$ in Algorithm \ref{alg:pareto-routers-gen} is upper bounded as 
    \begin{equation} \label{eq:upper-bound-gen}
      \textstyle   \max\limits_{P\in \cP} \Ex_{\cD_n}\big [\cE_P(\widehat g_\lambda,\lambda)\big ] \le K \big\{ \sum_{l = 1}^{L_1}\lambda_l a_{l, n}^{-1}\big\} ^{1+ \alpha}\,. 
    \end{equation}
    \item  {\bf Local polynomial estimator:} Assume that $\ell_l \{Y_i, f_m(X_i)\}$ are sub-Gaussian, \ie\ there exists constants $c_{l, 1}$ and $c_{l, 2}$ such that  
    \[
    \textstyle P\big ( |\ell_l \{Y, f_m(X)\} | > t \mid X\big ) \le c_{l, 1} e^{-c_{l, 2}t^2}\,. 
    \] If $\psi$ satisfies the regularity conditions with parameter $\beta_l$ in (\cf\ Definition \ref{def:kernel-reg})  and $k = \lfloor \beta_l \rfloor$ then for $h = n^{-\frac{1}{2\beta_l + d}}$ the inequality in \eqref{eq:concentration-phi-gen} holds for 
    \begin{equation}\label{LPR-gen}
    \widehat \Phi_{l, m}(x_0) = \widehat\theta_{x_0}^{(m)}(0), ~~ \widehat \theta_x^{(m)} \in \argmin_{\theta \in \Theta(k)}  \textstyle \sum_{i = 1}^n \psi (\frac{X_i - x_0}{h}) \big [\ell \{Y_i, f_m(X_i)\} - \theta (X_i -x_0 )\big]^2 \,. 
\end{equation}
    with $a_{l, n} = n^{\nicefrac{2\beta_l}{(2\beta_l + d)}}$, where $\Theta(k)$ is the class of all $k$ degree polynomials. 
\end{itemize} 

  
\end{theorem}
% \SM{Discuss the optimality in remarks.} 
\SM{comment about $\max_{l \in  [L_1]}\beta_l <  \nicefrac{d}{\alpha}$.}
We end our section with a few remarks. 

\begin{remark}[Rate optimality for plug-in routers]

Firstly, given that the upper bound in eq.  \eqref{eq:upper-bound-gen} for plug-in router described in Algorithm \ref{alg:pareto-routers-gen} with local-polynomial estimator for $\widehat \Phi_{l, m}(X)$ achieves the same rate of convergence as the lower bound in \eqref{eq:lower-bound} we conclude that the minimax optimal rate of convergence in excess risk is 
\begin{equation} \label{eq:optimal-rate-gen}
    \textstyle \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(A_n, \lambda) \asymp \cO \Big(\big \{\sum_{l = 1}^{L_1}\lambda_l n^{- \frac{\beta_l}{2\beta_l + d}}\big\}^{1+\alpha}\Big)\,. 
\end{equation} Moreover, this also implies that the plug-in router can achieve this optimal rate as long as the bounds in eq. \eqref{eq:concentration-phi-gen} are satisfied with $a_{l, n} = n^{\nicefrac{2\beta_l}{(2\beta_l + d)}}$. 
    
\end{remark}

\begin{remark}[Difficulty in routing with respect to $\lambda$]
Moreover, the rate in eq. \eqref{eq:optimal-rate-gen} implies that for a smaller $\lambda_l$ the errors in estimating $\Phi_{l, m}^\star$ have a lesser impact on the excess risk convergence. This conclusion is very much related to our Remark \ref{remark:diff-in-lambda-lb}, and thus we keep our discussion brief and ask readers to revisit the said remark. We end with a quick observation that the hardest instance to classify is when $\lambda_{l_{\min}} = 1$ for the lowest smoothness parameter, \ie\ $l_{\min} = \argmin_{l \le L_1} \beta_l$. 

    
\end{remark}


\begin{remark}

{\bf (Minimax rate study for non-parametric classification with multiple objectives)}
Compared to \citet{audibert2007Fast}, our study of the optimal minimax rate provided in this section generalizes the setting on three fronts: (1) the number of classes can be more than two, (2) general classification loss functions beyond $0/1$-loss and (3) multiple objectives. In this general setting, we show that the plug-in classifiers described in Algorithm \ref{alg:pareto-routers-gen} are both computationally and statistically (rate optimal) efficient for estimating the entire class of Bayes classifiers $\{g_\lambda^\star: \lambda\in \Delta^{L-1}\}$. 
    
\end{remark}