



\section{\newdata\ Construction details and plots}\label{sec:append:sprout}
In this section, we discuss data details for \newdata. \newdata\ will be released on HuggingFace hub as a HuggingFace datasets object. For convenience, the data is pre-divided into train, validation, and test splits. Consider the training set as an example; the features of this split are

\begin{tcolorbox}[
    title=Features of each split of \newdata,
    breakable,
    colback=gray!5,
    colframe=gray!70
]
\begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
features: [ "key", "dataset", "dataset level", "dataset idx", 
            "prompt", "golden answer", "claude-3-5-sonnet-v1", 
            "titan-text-premier-v1", "openai-gpt-4o", 
            "openai-gpt-4o-mini", "granite-3-2b-instruct", 
            "granite-3-8b", "llama-3-1-70b-instruct", 
            "llama-3-1-8b-instruct", "llama-3-2-1b-instruct", 
            "llama-3-2-3b-instruct", "llama-3-3-70b-instruct", 
            "llama-3-405b-instruct", "mixtral-8x7b-instruct-v01" ]
\end{minted}
\end{tcolorbox}

% \texttt{features: ["key", "dataset", "dataset level", "dataset idx", "prompt", "golden answer",
% "claude-3-5-sonnet-v1", "titan-text-premier-v1", "openai-gpt-4o", "openai-gpt-4o-mini",
% "granite-3-2b-instruct", "granite-3-8b", "llama-3-1-70b-instruct", "llama-3-1-8b-instruct",
% "llama-3-2-1b-instruct", "llama-3-2-3b-instruct", "llama-3-3-70b-instruct", "llama-3-405b-instruct", "mixtral-8x7b-instruct-v01"]} 

% \begin{lstlisting}[language=Python]

% features = ['key', 'dataset', 'dataset level', 'dataset idx', 'prompt', 'golden answer',
% 'aws-claude-3-5-sonnet-v1', 'titan-text-premier-v1', 'openai-gpt-4o', 'openai-gpt-4o-mini',
% 'granite-3-2b-instruct', 'granite-3-8b', 'llama-3-1-70b-instruct', 'llama-3-1-8b-instruct',
% 'llama-3-2-1b-instruct', 'llama-3-2-3b-instruct', 'llama-3-3-70b-instruct', 'llama-3-405b-instruct', 'mixtral-8x7b-instruct-v01']

% \end{lstlisting}

Each key corresponds to another list. \texttt{"prompt"} contains the model queries, the \texttt{"dataset"} list indicates which sub-task a given query falls in (\cf\ Table \ref{tab:dataset_splits} for info), and \texttt{golden answer} contains a desirable response for each query. Finally, the model keys each correspond to a list of dictionaries that contains further information on the responses of that model. The important keys in each dictionary of the list are \texttt{["num input tokens", "num output tokens", "response", "score"]}. They contain the number of input tokens for a query, the number of output tokens a model gives in response to a query, the actual response of the model, and finally the score that the judge provides for the response (using the corresponding \texttt{golden answer} entry). The conversion of token count to cost is given in Table \ref{tab:price_by_token_newdata} and additional details on the judging process are described in Section \ref{sec: sprout collection}.

\subsection{\newdata\ ingredients}
Table \ref{tab:dataset_splits} gives the benchmark ingredients for \newdata. Namely, we use the
MATH Lvl 1-5 \citep{hendrycks2021measuringmathematicalproblemsolving}, MMLU-PRO \citep{wang2024mmluprorobustchallengingmultitask}, %BBH \citep{suzgun2022challengingbigbenchtaskschainofthought}, 
GPQA \citep{rein2023gpqagraduatelevelgoogleproofqa}, MUSR \citep{sprague2023musr}, RAGBench \citep{friel2025ragbenchexplainablebenchmarkretrievalaugmented}, and openhermes \citep{teknium_openhermes_2.5} datasets. These six benchmarks are varied and designed to simulate real-world scenarios where LLMs encounter a wide range of prompts. MATH focuses solely on mathematical word problems, whereas MMLU-PRO and GPQA include both mathematical and advanced science questions. %In contrast, BBH emphasizes logical deduction and linguistic reasoning challenges. 
MuSR serves as a benchmark for assessing multistep soft reasoning tasks framed within natural language narratives. RAGBench is a retrieval augmented generation (RAG) benchmark dataset collected from Question-Answer (QA) datasets (CovidQA \citep{moller-etal-2020-covidqa}, PubmedQA \citep{jin-etal-2019-pubmedqa}, HotpotQA \citep{yang2018hotpotqa}, MS Marco \citep{nguyen2016msmacro}, CUAD \citep{hendrycks2021cuad}, EManual \citep{nandy-etal-2021-emanual}, TechQA \citep{castelli-etal-2020-techqa}, FinQA \citep{chen-etal-2021-finqa}, TAT-QA \citep{zhu-etal-2021-tatqa}, ExpertQA \citep{malaviya2024expertqa}, HAGRID \citep{kamalloo2023hagrid}), as well as one that was specifically adapted for RAG (DelucionQA \citep{sadat2024delucionqa}). This measures the ability of a LLM to incorporate retrieved documents along with user queries to generate accurate answers for problems that require in-depth domain knowledge. As such, RAGbench is grouped by the needed domain knowledge: bio-medical research (PubmedQA,
CovidQA), general knowledge (HotpotQA, MS Marco, HAGRID, ExperQA), legal contracts (CuAD),
customer support (DelucionQA, EManual, TechQA), and finance (FinBench, TAT-QA). Finally, openhermes is a collection of GPT4 generated questions designed to emulate real user queries to an LLM. 

%\begin{center}

%\end{center}

%\usepackage{booktabs}


\begin{table}
    \centering
        \caption{Dataset Splits for \newdata.}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Benchmark} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
        \midrule
        ragbench/expertqa & 98 & 17 & 16 \\
        MATH (test) & 1725 & 363 & 384 \\
        ragbench (emanual) & 82 & 27 & 23 \\
        ragbench (cuad) & 151 & 35 & 29 \\
        MuSR & 178 & 35 & 35 \\
        MATH & 5217 & 1061 & 1134 \\
        MuSR (team allocation) & 157 & 52 & 41 \\
        ragbench (hagrid) & 92 & 23 & 17 \\
        gpqa (extended) & 368 & 89 & 84 \\
        MuSR (object placements) & 169 & 47 & 34 \\
        ragbench (pubmedqa) & 92 & 14 & 26 \\
        ragbench (hotpotqa) & 89 & 22 & 21 \\
        ragbench (msmarco) & 85 & 24 & 23 \\
        ragbench (techqa) & 85 & 24 & 23 \\
        MMLU-Pro & 8204 & 1784 & 1798 \\
        openhermes & 13703 & 2917 & 2835 \\
        ragbench (tatqa) & 90 & 17 & 25 \\
        ragbench (finqa) & 97 & 15 & 20 \\
        ragbench (covidqa) & 162 & 38 & 41 \\
        ragbench (delucionqa) & 124 & 32 & 28 \\
        TOTAL & 30968 & 6636 & 6637 \\
        \bottomrule
    \end{tabular}

    \label{tab:dataset_splits}
\end{table}

\subsection{\newdata\ models and response collection}
\label{sec: sprout collection}
Table \ref{tab:price_by_token_newdata} provides the models and their associated costs that a router trained on \newdata\ can select between. The input and output token counts are collected by simply gathering the count of the tokenized queries and outputs of a model from its tokenizer. In order to emulate real-world use cases, responses from each LLM are collected using a corresponding \emph{chat template} with a generic prompt and \emph{zero shot prompting}.

Given the use of chat templates and zero-shot prompting, evaluation is challenging because model responses will not necessarily follow a specific format. To alleviate this, we adopt the evaluation protocol from MixEval \citep{ni2024mixeval} and use LLama-3.1-70B as a grader to score model queries against a given gold standard answer. The prompt format that we use is provided in \ref{sec: JP}. Note that this prompt format needs to be converted to openai-api compatible messages while prompting the LLMs, which can be inferred from the special delimiters contained within the prompt format.

% \texttt{'''<dmf>system
% <dmf>user
% I want you to act as a judge for how well a model did answering a user-defined task.
% You will be provided with a user-defined task that was given to the model, its golden answer(s), and the model's answer. The context of the task may not be given here. Your task is to judge how correct is the model's answer. Your task is to judge how correct the model’s answer is based on the golden answer(s), without seeing the context of the task, and then give a correctness score. The correctness score should be one of the below numbers: 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0 (totally right). You should also add a brief justification regarding how the model’s answer
% conforms to or contradicts the golden answer(s). Your response must follow the format
% \{\{
%    correctness score: your score,
%    justification: your justification
% \}\}
% Note that each one of the golden answers is considered correct. Thus if the Model’s Answer matches anyone of the golden answers, it should be considered correct.}


%\begin{lstlisting}[label=prompt-format, caption=Prompt format for the evaluator Llama 3.1 70b Instruct LLM.]
%<dmf>system
%<dmf>user
  %  I want you to act as a judge for how well a model did answering a user-defined task. You will be provided with a user-defined task that was given to the model, its golden answer(s), and the model's answer. The context of the task may not be given here. Your task is to judge how correct is the model's answer. Your task is to judge how correct the model's answer is based on the golden answer(s), without seeing the context of the task, and then give a correctness score. The correctness score should be one of the below numbers: 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0 (totally right). You should also add a brief justification regarding how the model's answer conforms to or contradicts the golden answer(s). 

%Your response must follow the format
 %  {correctness score: your score, justification: your justification}

%Note that each one of the golden answers is considered correct. Thus if the Model's Answer matches anyone of the golden answers, it should be considered correct. 
%\end{lstlisting}


\begin{table}
    \centering
    \caption{Models in \newdata\ dataset and their API prices according to token counts.}
    \begin{tabular}{c c c } 
 \toprule
 \multirow{2}{*}{Model} & Input Token Cost & Output Token Cost\\
 & (in \$ per 1M tokens) & (in \$ per 1M tokens)\\
% Model & Input Token Cost (\$ per million tokens) & Output Token Cost (\$ per million tokens) \\ [0.5ex] 
 \hline\hline
 claude-3-5-sonnet-v1 & 3 & 15  \\ 
 \hline
 titan-text-premier-v1 & 0.5 & 1.5  \\
 \hline
 openai-gpt-4o & 2.5 & 10  \\
 \hline
 openai-gpt-4o-mini & 0.15 & 0.6  \\
 \hline
 granite-3-2b-instruct & 0.1 & 0.1  \\ 
 \hline
 granite-3-8b-instruct & 0.2 & 0.2  \\
 \hline
 llama-3-1-70b-instruct & 0.9 & 0.9  \\ 
 \hline
 llama-3-1-8b-instruct & 0.2 & 0.2  \\ 
 \hline
  llama-3-2-1b-instruct & 0.06 & 0.06  \\ 
 \hline
 llama-3-2-3b-instruct & 0.06 & 0.06  \\ 
 \hline
 llama-3-3-70b-instruct & 0.9 & 0.9  \\ 
 \hline
 mixtral-8x7b-instruct & 0.6 & 0.6  \\ 
 \hline
 llama-3-405b-instruct & 3.5 & 3.5  \\ 
 \bottomrule
\end{tabular}
    \label{tab:price_by_token_newdata}
\end{table}







\section{Additional Plots and Experimental details}
\subsection{RouteBench}
Figure \ref{fig:routerbench-table} lays out the models and benchmarks present in the Routerbench dataset. To implement the transformer-based plug-in estimate of cost and accuracy, we utilize the \texttt{roberta-base} architecture with a learning rate of \texttt{3e-5} and a weight decay of $0.01$. A training, validation, test split of $0.72$, $0.8$, $0.2$ is used. Learning proceeds for $5$ epochs, and the model with the best validation performance is saved at the end. To fit the KNN-based router, the OpenAI text-embedding-small-3 model is used, while the KNN regressor utilizes the $40$-nearest neighbors measured by the 'cosine' similarity metric.

The same \texttt{roberta-base} parameters are used to fit the Roberta technique from RouteLLM \citep{ong2024routellmlearningroutellms}.  The matrix factorization method assumes that 
\[\Pr(\text{GPT-4 Win}|q) = \sigma(w_2^T(v_{\text{GPT-4}}\odot(W_1^T v_q+b)- v_{\text{mixtral}}\odot(W_1^T v_q+b)) )\]
where $v_{\text{GPT-4}}$,$v_{\text{mixtral}}$ are learnable embeddings of the model of interest. We use the \texttt{text-embeddder-small-3} from OpenAI to embed the queries, and a projection dimension of $d=128$. The model is fit using Adam, with a learning rate of $3e-4$ and a weight decay of $1e-5$. To fit RoRF from not-diamond, we again use \texttt{text-embeddder-small-3} while the default parameters from Not-Diamond are used (max-depth = 20, 100 estimators).
\begin{figure}[H]
    \centering
        \includegraphics[width=0.95\textwidth]{plots/routerbenchtable.png} 
        %\caption{Test 1}
        %\label{fig:routerbench}
    %\hfill
    %\begin{minipage}{0.49\textwidth}
      % \includegraphics[width=\textwidth]{plots/router-vs-gpt4.pdf}
      %  \caption{Test 2}
     %   \label{fig:router-vs-gpt4}
    %\end{minipage}
\caption{Routerbench models and benchmarks (\citet{hu2024routerbench} Table 1).}
\label{fig:routerbench-table}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.45\linewidth]{plots/routerbenchsupp.pdf}
    \caption{Router Bench Supplementary.}
    \label{fig:rbenchsupp}
\end{figure}

\subsection{Open LLM Leaderboard V2}\label{append:llms_open}


\paragraph{LLMs and costs:} Table \ref{tab:llms_open} gives all models used for the Open LLM Leaderboard experiment and their respective costs.
\begin{longtable}{lc}
\caption{Models used and their respective costs for the Open LLM Leaderboard experiment.}

\\ \hline
\textbf{Model Name} & \textbf{Price (USD per 1M tokens)} \\ \hline
NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO & 0.6 \\ \hline
01-ai/Yi-34B-Chat & 0.8 \\ \hline
Qwen/QwQ-32B-Preview & 1.2 \\ \hline
Qwen/Qwen2-72B-Instruct & 0.9 \\ \hline
Qwen/Qwen2.5-7B-Instruct & 0.3 \\ \hline
Qwen/Qwen2.5-72B-Instruct & 1.2 \\ \hline
alpindale/WizardLM-2-8x22B & 1.2 \\ \hline
deepseek-ai/deepseek-llm-67b-chat & 0.9 \\ \hline
google/gemma-2-27b-it & 0.8 \\ \hline
google/gemma-2-9b-it & 0.3 \\ \hline
google/gemma-2b-it & 0.1 \\ \hline
meta-llama/Llama-2-13b-chat-hf & 0.3 \\ \hline
meta-llama/Meta-Llama-3.1-70B-Instruct & 0.9 \\ \hline
%meta-llama/Meta-Llama-3.1-8B-Instruct & 0.2 \\ \hline
mistralai/Mistral-7B-Instruct-v0.1 & 0.2 \\ \hline
mistralai/Mistral-7B-Instruct-v0.2 & 0.2 \\ \hline
mistralai/Mistral-7B-Instruct-v0.3 & 0.2 \\ \hline
mistralai/Mixtral-8x7B-Instruct-v0.1 & 0.6 \\ \hline
nvidia/Llama-3.1-Nemotron-70B-Instruct-HF & 0.9 \\ \hline
%upstage/SOLAR-10.7B-Instruct-v1.0 & 0.3 \\ \hline
\label{tab:llms_open}
\end{longtable}

\paragraph{Model fitting:} The model fitting details for baseline methods are all the same as in the RouterBench experiment (following the original implementations). To fit our methods, we employ some hyperparameter tuning for both KNN and \texttt{roberta-base}. For KNN, we employ 5-fold cross-validation using ROC-AUC and the possible number of neighbors as 2, 4, 8, 16, 32, 64, 128, 256, or 512. For \texttt{roberta-base} hyperparameter tuning, we train for 3k steps, using 20\% of the training data for validation, a batch size of 8, and search for the best combination of learning rate, weight decay, and gradient accumulation steps in \{5e-5, 1e-5\}, \{1e-2, 1e-4\}, and \{1, 2, 4, 8\}. The final model is trained for 10k steps.

%\paragraph{Ablation study:} 

\begin{figure}[H]
    \centering
    \includegraphics[width=.45\linewidth]{plots/hf-leaderboard2.pdf}
    \caption{Open LLM leaderboard v2.}
    \label{fig:rexp_open_llm2}
\end{figure}



\section{Supplementary definitions, results and proofs}
\label{sec:proofs}


\subsection{Minimax approaches to learning the risk functions}
\label{sec:reg-fn-estimate}

In remark \ref{cor:efficient-routers} we discussed the required condition for $\widehat \Phi$ so that the plug-in router has minimax rate optimal excess risk. In this section we show that estimating $\widehat \Phi$ using \emph{local polynomial regression} (LPR) meets the requirement.  To describe the LPR estimates consider a kernel $\psi: \reals^d \to [0, \infty)$ that satisfies the regularity conditions described in the Definition \ref{def:kernel-reg} in  Appendix \ref{sec:proofs} with parameter $\max_k \gamma_k$ and define $\Theta(p)$ as the class of all $p$-degree polynomials from $\reals^d$ to $\reals$. For bandwidths $h_k > 0; k \in [K_1]$ we define the LPR estimate as 
\begin{align}\label{eq:LPR}
    [\widehat \Phi(x_0)]_{m, k} = \hat\theta_{x_0}^{(m, k)}(0); \nonumber \\
    ~~ \hat \theta_x^{(m, k)} \in \underset{\theta \in \Theta(p)}{\argmin}  \textstyle \sum_{i} \psi (\frac{X_i - x_0}{h}) \big \{[Y_i]_{m ,k} - \theta (X_i -x_0 )\big\}^2. 
\end{align}
In Theorem 3.2 of \citet{audibert2007Fast}, a similar rate of convergence for LPR estimates is established. In their case, the losses were binary. For our instance, we assume that the $Y_i$ are sub-Gaussian, but the conclusions are identical. We restate their result below.
% to \citet[Theorem 3.2]{audibert2007Fast}. 
\begin{lemma}
    Assume that $Y_i$ are sub-Gaussian random variables, \ie\ there exist constants $c_1$ and $c_2$ such that  
    \[
    \textstyle P\big ( \|Y_i\|_\infty > t \mid X\big ) \le c_1 e^{-c_2t^2}\,. 
    \] If $\psi$ is regular (\cf\ Definition \ref{def:kernel-reg}) with parameter $\max_k \gamma_k$  and $p \ge  \lfloor \max_k \gamma_k \rfloor$ then for $h_k = n^{-\nicefrac{1}{(2\gamma_k + d)}}$ the Assumption \ref{assmp:convergence} is satisfied with $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k + d)}}$, \ie\ for some constants $\rho_1, \rho_2 > 0$ and any $n \ge 1$ and $t > 0$ and almost all $X$ with respect to $P_X$ we have the following concentration bound for $\widehat \Phi$:
    \begin{align}\label{eq:concentration-phi-2}
        \max_{P\in \cP} P \big \{ \max_{m, k} a_{k, n}^{-1}\big |[\widehat \Phi (X)]_{m, k} - [\Phi  (X)]_{m, k}\big |  \ge t\big \} \nonumber \\
        \le  \rho_1 \exp\big (- \rho_2  t^2 \big )\,.
    \end{align}
\end{lemma}
This result is related to our Remark \ref{cor:efficient-routers} about the rate-efficient estimation of routers. Estimating $\Phi(X)$ with an LPR and a suitable bandwidth and polynomial degree leads to our desired rate of convergence $a_{k, n} = n^{-\nicefrac{\gamma_k}{(2\gamma_k + d)}}$ in Assumption \ref{assmp:convergence}. 

\subsection{Examples, additional assumptions and lemmas}

Next, we describe the regularity conditions needed for local polynomial regression in eq. \eqref{eq:LPR} and \eqref{eq:LPR}. These conditions are taken directly from \citet[Section 3]{audibert2007Fast}. 
\begin{definition}[Kernel regularity]
\label{def:kernel-reg}
    For some $\beta > 0$ we say that a kernel $K:\reals^d \to [0, \infty)$ satisfies the regularity condition with parameter $\beta$, or simply $\beta$-regular if the following are true: 
    \[
    \begin{aligned}
        & \text{for some } c> 0, K(x)\ge c, ~~ \text{for } \|x\|_2 \le c\,,\\
        & \textstyle \int K(x)dx = 1\\
        & \textstyle\int (1 + \|x\|_2^{4\beta}) K^2(x)dx < \infty, \\
        & \textstyle\sup\limits_{x} (1 + \|x\|_2^{2\beta}) K(x) < \infty\,. 
    \end{aligned}
    \]
\end{definition}
An example of a kernel that satisfies these conditions is the Gaussian kernel: $K(x) = \prod_{j = 1}^d \phi(x_j)$, where $\phi$ is the density of a standard normal distribution. 


% Next, establish the form of the Bayes classifier seen in eq. \eqref{eq:oracle-router-gen}.

% \begin{lemma}\label{lemma:bayes-classifier-gen}
%     For the classification loss 
%     \[
%      \textstyle \eta_{\lambda, m}(X, Y) =  \sum_{l = 1}^{L_1}\lambda_l \ell_l\{m; X, Y\} + \sum_{l = L_1 + 1}^{L}\lambda_l \ell_l\{m; X\}\,,
%     \]
%     for predicting a sample $(X, Y)$ as class $m$ the Bayes classifier
%     \[
%     \textstyle \textstyle g_\lambda^\star = \argmin_{g: \cX \to [M]}  \Ex_P\big[\sum_{m = 1}^M \bbI\{g(X) = m\} \eta_{\lambda, m} (X, Y) \big ]
%     \]
%     has the form 
%     \[
%      \textstyle  g_\lambda^\star (X) = \argmin_m  \big\{\sum_{l = 1}^L \lambda_l \Phi_{l, m}^\star (X) \big\}  , ~~ \Phi_{l, m}^\star (X) = \begin{cases}
%          \Ex_P[\ell_{l}\{m; X, Y\}\mid X] & 1 \le l \le L_1\\
%          \ell_{l}\{m; X\} & L_1 + 1 \le l\le L\,.
%      \end{cases}
%     \] 
% \end{lemma}
% \begin{proof}[Proof of the Lemma \ref{lemma:bayes-classifier-gen}]
%     Note that 
%     \[
%     \begin{aligned}
%         & \textstyle\Ex_P\big[\sum_{m = 1}^M \bbI\{g(X) = m\} \eta_{\lambda, m} (X, Y) \big ]\\
%         & \textstyle = \Ex_P\big[\Ex_P\big[\sum_{m = 1}^M \bbI\{g(X) = m\} \eta_{\lambda, m} (X, Y) \mid X\big ]\big]\\
%         & = \textstyle\sum_{m = 1}^M \bbI\{g(X) = m\} \eta_{\lambda, m}^\star (X)
%     \end{aligned}
%     \] Thus, the Bayes classifier is $g_\lambda^\star(X) = \argmin_m \eta_{\lambda, m}^\star (X)$ and $\eta_{\lambda, m}^\star (X) = \Ex_P[\eta_{\lambda, m}(X, Y)\mid X] = \sum_{l = 1}^L \lambda_l \Phi_{l, m}^\star (X)$. 
% \end{proof}

Next, we establish sufficient conditions for a class of distributions $\{p_\theta, \theta\in \reals\}$ to satisfy the condition that $\KL(p_\theta, p_{\theta'}) \le K(\theta - \theta')^2$ for some $K> 0$ and any $\theta, \theta'\in \reals$. 
\begin{lemma}\label{lemma:KL-bound}
    Assume that a parametric family of distributions $\{p_\theta, \theta\in \reals\}$ satisfies the following conditions: 
    \begin{enumerate}
        \item The distributions have a density $p_\theta$ with respect to a base measure $\mu$ such that $p_\theta$ is twice continuously differentiable with respect to $\theta$. 
        \item  $\textstyle \int  \partial_\theta  p_\theta (x) d\mu(x) = \partial_\theta \int \textstyle   p_\theta (x)d\mu(x) = 0$
        \item For some $K> 0$ and all $\theta \in \reals$ the  $\textstyle - \partial_\theta^2 \int \textstyle  \log p_\theta (x) p_\theta(x)d\mu(x) \le K$.
    \end{enumerate}
    Then $\KL(p_\theta, p_{\theta'}) \le \frac{K(\theta - \theta')^2}2$.
\end{lemma}
Some prominent examples of such family are location families of normal, binomial, Poisson distributions, etc.

\begin{proof}[Proof of the Lemma \ref{lemma:KL-bound}]
    Notice that 
    \[
\begin{aligned}
    &\textstyle \KL(\mu_\theta, \mu_{\theta'})\\
    & = \textstyle \int  p_{\theta}(x) \log\big\{  \frac{p_\theta (x)}{p_{\theta'}(x)}\big\} d\mu(x) \\
    & = \textstyle \int  p_{\theta}(x)\big\{  \log p_\theta(x) - \log p_{\theta'}(x)\big \}  d\mu(x)\\
    & = \textstyle \int  p_{\theta}(x)\big\{  \log p_\theta(x) - \log p_{\theta}(x) - (\theta' - \theta) \partial_\theta \log p_\theta (x)  - \frac{(\theta' - \theta)^2}{2}\partial^2_\theta \log p_{\tilde \theta} (x) \big \}  d\mu(x)\\
\end{aligned}
\] Here, 
$\int p_\theta (x) \partial _\theta \log p_\theta (x) d\mu(x) = \int \partial _\theta  p_\theta (x) d\mu(x) dx =0$ and  $- \int p_\theta (x) \partial^2 _\theta \log p_{\tilde \theta} (x)d\mu(x) \le K$. Thus, we have the upper bound $\KL(\mu_\theta, \mu_{\theta'}) \le \frac{K}{2}(\theta - \theta')^2$. 
\end{proof}

\subsection{Proof of Lemma \ref{lemma:oracle-router}}
\begin{proof}[Proof of Lemma \ref{lemma:oracle-router}]
    The $\mu$-th risk 
    \[
    \begin{aligned}
        \textstyle \cR_P(g, \mu) & \textstyle = \Ex\big [\Ex\big[Y\mu]_m\mid X\big] \bbI\{g(X) = m\} \big]\\
        & \textstyle =\Ex\big [\big\{ \sum_{k = 1}^K \mu_k [\Phi (X)]_{m, k} \big\}\bbI\{g(X) = m\} \big]
    \end{aligned}
    \] is minimized at $g(X) = \argmin_m \big\{ \sum_{k = 1}^K \mu_k [\Phi (X)]_{m, k} \big\}$.
\end{proof}


% \subsection{Other results}
% % \begin{lemma} \label{lemma:oracle-router}
% %     For a $\lambda > 0$ define $\eta_{\lambda, m} ^\star(X) = \Ex_P [\eta_{\lambda, m}(X, Y) \mid X]$. Then the oracle router $g_\lambda^\star$ that minimizes the loss $\cL_P(g, \lambda)$ is $g_\lambda^\star(X) = \argmin_l ~ \eta_{\lambda, m} ^\star(X)$\,. 
% % \end{lemma}

% \begin{proof}[Proof of Lemma \ref{lemma:oracle-router}] The loss 
%     \[
%     \begin{aligned}
%         \cL_P(g, \lambda) &=\textstyle  \Ex_P \big[\sum_{m = 1}^M \bbI \{ g(X) = l\} \eta_{\lambda, m}(X, Y) \big]\\
%         & = \textstyle  \Ex_P \big[\sum_{m = 1}^M \bbI \{ g(X) = l\} \Ex_P[\eta_{\lambda, m}(X, Y)\mid X] \big]\\
%         & = \textstyle  \Ex_P \big[\sum_{m = 1}^M \bbI \{ g(X) = l\} \eta_{\lambda, m}^\star(X) \big]
%     \end{aligned}
%     \] is minimized at $g_\lambda^\star(X) = \argmin_l ~ \eta_{\lambda, m} ^\star(X)$.
% \end{proof}

\subsection{The upper bound}


\begin{lemma} \label{lemma:excess-risk-bound-generalization}
    Suppose that we have a function $f:\cX \to \reals ^M $ for which we define the coordinate minimizer $g:\cX \to [M]$ as $g(x) = \argmin_m f_m(x) $ and the margin function 
    \[
   \Delta(x) =  \begin{cases}
       \min_{m \neq g(x)} f_m(x) - f_{g(x)}(x) & \text{if} ~ g(x) \neq [M]\\
       0 & \text{otherwise}\,.
    \end{cases}
    \]
    Assume that the margin condition is satisfied, \ie\ there exist $\alpha, K_\alpha$ such that 
\begin{equation}
     P_X \big\{0 < \Delta (X) \le t\big \}  \le K_\alpha t^{\alpha}\,.
\end{equation} Additionally, assume that there exists an estimator $\widehat f$ of the function $f$ such that it satisfies a concentration bound: 
for some $\rho_1, \rho_2 > 0$ and any $n \ge 1$ and $t > 0$ and almost all $x$ with respect to $P_X$ we have the following concentration bound for $\widehat \Phi$:
    \begin{equation}\label{eq:concentration-f}
         P_{\cD_n} \big \{ \|\widehat f(x) - f (x)\|_\infty  \ge t\big \} \le  \rho_1 \exp\big (- \rho_2 a_n^{-2} t^2 \big )\,, 
    \end{equation}
    where $\{a_n; n \ge 1\}\subset \reals$ is a sequence that decreases to zero. Then for $\widehat g(x) = \argmin_m \widehat f_m(x)$ there exists a $K> 0$ such that for any $n \ge 1$  we have the upper bound
    \begin{equation}
         \Ex_{\cD_n}\big [ \Ex_P\big [ f_{\widehat g(X)}(X) - f_{g(X)}(X)  \big]\big ] \le K a_n^{{1+ \alpha}}\,. 
    \end{equation}
\end{lemma}


\begin{proof}
    For an $x \in \cX$ define $\delta_m(x) = f_m(x) -  f_{g(x)}(x)$. Since $g(x) = \argmin_m f_m(x)$ we have $\delta_m(x) \ge 0$ for all $m$,  $\min_{m}\delta_m(x) = 0$. Furthermore, define $h(x) = \argmin\{m \neq g(x): f_m(x)  \}$, \ie\ the coordinate of $f(x)$ where the second minimum is achieved. Clearly, $\delta_{h(x)}(x) = \Delta(x)$. With these definitions, lets break down the excess risk as: 
    \begin{equation}\label{eq:tech-1}
    \begin{aligned}
         & \Ex_{\cD_n}\big [ \Ex_P\big [ f_{\widehat g(X)}(X) - f_{g(X)}(X)  \big]\big ]\\
         & = \textstyle  \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\}  \big]\big ] \\
         & = \textstyle  \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \Delta(X) \le \tau\}   \big]\big ] \\
         & \quad \textstyle + \sum_{i \ge 1} \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\big ]
    \end{aligned}
    \end{equation} where $\tau = 2\rho_2^{-\nicefrac{1}{2}}a_n$. 
    We deal with the summands one by one. First, if $\Delta(X) = 0$ then all the coordinates of $f(X)$ are identical, which further implies that $f_m(X) - f_{g(X)}(X) = 0$ for any $m$. Thus, 
    \[
    \begin{aligned}
        & \textstyle \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \Delta(X) \le \tau\}   \big]\big ]\\
        & \textstyle = \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ]
    \end{aligned}
    \] If $m = g(X)$ then the summand is zero. For the other cases, $\widehat g(X) = m $ if $\widehat f(X)$ has the minimum value at the $m$-th coordinate. This further implies $\widehat f_m (X) \le \widehat f_{ g(X)}(X)$. The only way this could happen if $|\widehat f_m(X) - f_m(X)| \ge \nicefrac{\delta_m(X)}{2}$ or $|\widehat f_{g(X)}(X) - f_{g(X)}(X)| \ge \nicefrac{\delta_m(X)}{2}$. Otherwise, if both are $|\widehat f_m(X) - f_m(X)| < \nicefrac{\delta_m(X)}{2}$ and $|\widehat f_{g(X)}(X) - f_{g(X)}(X)| < \nicefrac{\delta_m(X)}{2}$ this necessarily implies 
    \[
    \begin{aligned}
        \textstyle \widehat f_{g(X)} (X) & <\textstyle  f_{g(X)}(X) + \frac{\delta_m(X)}{2} \\
        & \textstyle = f_m(X) - \delta_m(X) + \frac{\delta_m(X)}{2}\\
        & = \textstyle f_m(X) - \frac{\delta_m(X)}{2} < \widehat f_m(X)\,, 
    \end{aligned}
    \] which means for $\widehat f(X)$ the minimum is not achieved at the $m$-th coordinate. Now, $|\widehat f_m(X) - f_m(X)| \ge \nicefrac{\delta_m(X)}{2}$ or $|\widehat f_{g(X)}(X) - f_{g(X)}(X)| \ge \nicefrac{\delta_m(X)}{2}$ implies $\|\widehat f(X) - f(X) \|_\infty \ge \nicefrac{\delta_m(X)}{2}$. With these observations we split the expectation as
    \[
    \begin{aligned}
        & \textstyle  \Ex_{\cD_n}\big [ \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ]\\
        & \textstyle = \Ex_{\cD_n}\big [ \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m = g(X)\} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ] \\
        & \textstyle \quad + \Ex_{\cD_n}\big [ \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m \neq  g(X)\} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ] 
    \end{aligned}
    \] The first part is zero, whereas the second part further simplifies as: 
    \[
    \begin{aligned}
        & \textstyle  \Ex_{\cD_n}\big [ \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m \neq  g(X)\} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ] \\
        & \le  \textstyle \Ex_{\cD_n}\big [ \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \bbI \big \{\|\widehat f(X) - f(X) \|_\infty \ge \frac{\delta_m(X)}{2} \big \} \bbI \{ 0 < \Delta(X) \le \tau\}   \big]\big ] \\
        & = \textstyle \Ex_P\big [ \{f_{m}(X) - f_{g(X)}(X)\} \Ex_{\cD_n}\big [ \bbI \big \{\|\widehat f(X) - f(X) \|_\infty \ge \frac{\delta_m(X)}{2} \big \}\big] \bbI \{ 0 < \Delta(X) \le \tau\}   \big ] \\
        & = \textstyle \Ex_P\big [ \delta_m(X) P_{\cD_n} \big \{\|\widehat f(X) - f(X) \|_\infty \ge \frac{\delta_m(X)}{2} \big \} \bbI \{ 0 < \Delta(X) \le \tau\}   \big ] \\
        & \le  \textstyle \Ex_P\big [ \delta_m(X) \rho_1 e^{- \frac{\rho_2 a_n^{-2}\delta_m^2(X)}4 } \bbI \{ 0 < \Delta(X) \le \tau\}   \big ] = \textstyle \Ex_P\big [ \delta_m(X) \rho_1 e^{- \frac{\delta_m^2(X)}{\tau^2} } \bbI \{ 0 < \Delta(X) \le \tau\}   \big ]
    \end{aligned}
    \]
    Notice that $\delta_m(X) \ge \Delta(X)$ whenever $\Delta(X) > 0$. Thus, we perform a maximization on $\delta_m(X)  e^{- \frac{\delta_m^2(X)}{\tau^2} }$ on the feasible set $\delta_m(X) \ge \Delta(X)$. Here, we use the result: 
    \begin{equation}\label{eq:tech-2}
        \max_{x \ge y} xe^{-
        \frac{x^2}{\tau^2 }} \le \begin{cases}
            \frac{\tau }{\sqrt{2e}} & \text{if} ~ \frac{\tau}{\sqrt{2}} \ge y\\
             ye^{-
        \frac{y^2}{\tau^2 }} & \text{otherwise}\,, 
        \end{cases}
    \end{equation} where $x = \delta_m(X)$ and $y = \Delta(X)$. Since $
    \Delta(X) \le \tau$ we have $\delta_m(X)  e^{- \frac{\delta_m^2(X)}{\tau^2} } \le \tau$ and thus
    \[
    \begin{aligned}
        \textstyle \Ex_P\big [ \delta_m(X) \rho_1 e^{- \frac{\delta_m^2(X)}{\tau^2} } \bbI \{ 0 < \Delta(X) \le \tau\}   \big ] \le \rho_1 \tau P\{0 < \Delta(X) \le \tau\} = \rho_1 \tau^{1 + \alpha}\,.
    \end{aligned}
    \] This finally results in 
    \[
    \begin{aligned}
        \textstyle  \Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \Delta(X) \le \tau\}   \big]\big ] \le M\rho_1\tau^{1+\alpha}\,,
    \end{aligned}
    \] which takes care of the first summand in eq. \eqref{eq:tech-1}. Now, for an $i \ge 1 $, let us consider the summand 
    \[
    \begin{aligned}
        & \textstyle\Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\big ]
    \end{aligned}
    \] Again, on the event $m = g(X)$ the the summand is zero and on the other cases we have $\|\widehat f(X) - f(X) \|_\infty \ge \nicefrac{\delta_m(X)}{2}$.
    Thus, we write 
    \[
    \begin{aligned}
        & \textstyle\Ex_{\cD_n}\big [ \Ex_P\big [ \sum _{m = 1}^M\{f_{m}(X) - f_{g(X)}(X)\} \bbI \{ \widehat g(X) = m\} \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\big ]\\
        & \le  \textstyle \textstyle\sum _{m = 1}^M\Ex_{\cD_n}\big [ \Ex_P\big [ \delta_m(X)\bbI \big \{\|\widehat f(X) - f(X) \|_\infty \ge \frac{\delta_m(X)}{2} \big \} \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\big ]\\
        & \le  \textstyle \textstyle\sum _{m = 1}^M\Ex_P\big [ \delta_m(X) \rho_1 e^{- \frac{\delta_m^2(X)}{\tau^2} } \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\\
    \end{aligned}
    \] Because $\Delta(X) \ge \tau 2^{i-1} > \nicefrac\tau{\sqrt{2}}$ we again use the inequality in eq. \eqref{eq:tech-2} to obtain
\[
\begin{aligned}
&   \textstyle \textstyle\sum _{m = 1}^M\Ex_P\big [ \delta_m(X) \rho_1 e^{- \frac{\delta_m^2(X)}{\tau^2} } \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\\
& \le  \textstyle \textstyle\sum _{m = 1}^M\Ex_P\big [ \Delta(X) \rho_1 e^{- \frac{\Delta^2(X)}{\tau^2} } \bbI \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \big]\\ 
& \le  \textstyle \textstyle\sum _{m = 1}^M \tau2^i \rho_1 e^{- \frac{\tau^2 2^{2i - 2}}{\tau^2} } P \{ \tau 2^{i -1} < \Delta(X) \le \tau 2^i \}   \\ 
& \le  \textstyle \textstyle M \tau2^i \rho_1 e^{- \frac{\tau^2 2^{2i - 2}}{\tau^2} } P \{ 0 < \Delta(X) \le \tau 2^i \} = M \rho_1 \tau^{1 + \alpha} 2^{i(1+\alpha)} e^{-2^{2i-2}}   \\ 
\end{aligned}
\] Combining all the upper bounds in \eqref{eq:tech-1} we finally obtain
\begin{equation}
    \begin{aligned}
        \Ex_{\cD_n}\big [ \Ex_P\big [ f_{\widehat g(X)}(X) - f_{g(X)}(X)  \big]\big ] \le \textstyle M\rho_1\tau^{1+\alpha} \big \{ 1+ \sum_{i \ge 1}2^{i(1+\alpha)} e^{-2^{2i-2}} \big \} 
    \end{aligned}
\end{equation} As $\sum_{i \ge 1}2^{i(1+\alpha)} e^{-2^{2i-2}}$ is finite we have the result. 
 \end{proof}


% \begin{proof}[Proof of Theorem \ref{thm:upper-bound}] The upper bound is a direct consequence of the Lemma \ref{lemma:excess-risk-bound-generalization}. For a $\lambda \in (0, 1]$ notice that 
% \[
% \textstyle \widehat\eta_{\lambda , m}(X) - \eta^\star_{\lambda, m}(X) = \lambda \big \{\widehat \Phi_m(X) - \Phi_m^\star(X) \big\} \,.
% \]Then, 
% the concentration inequality in eq. \eqref{eq:concentration-phi} implies  
% \[
% \textstyle \max_{P\in \cP} P \big \{ \max_m \big |\widehat\eta_{\lambda , m}(X) - \eta^\star_{\lambda, m}(X)\big |  \ge t\big \} \le  \rho_1 \exp\big (- \rho_2 \lambda^{-2}a_n^2 t^2 \big )\,.
% \] By letting $\widehat f_m = \widehat \eta_{\lambda, m}$ and $f_m = \eta_{\lambda, m}^\star$ in Lemma \ref{lemma:excess-risk-bound-generalization} we obtain the upper bound in Theorem \ref{thm:upper-bound}, where we replace $a_n$ with $\lambda^{-1} a_n$. 


% \end{proof}


\begin{proof}[Proof of Theorem \ref{thm:upper-bound}]

    The proof of the upper bound follows directly from the lemma \ref{lemma:excess-risk-bound-generalization} once we establish that for $a_n = \sum_{k = 1}^{K_1}\mu_k a_{k, n}$ the following concentration holds: 
    for constants $\rho_{ 1}, \rho_{ 2} > 0$ and any $n \ge 1$ and $t > 0$ and almost all $X$ with respect to $P_X$ we have 
    \begin{equation} \label{eq:concentration-phi-gen}
        \max_{P\in \cP} P \big \{ \max_m \big |\widehat \eta_{\mu, m} (X) - \eta^\star_{\mu, m}  (X)\big |  \ge t\big \} \le  \rho_1 \exp\big (- \rho_2 a_{ n}^{-2} t^2 \big )\,.   
    \end{equation} To this end, notice that 
    \[
    \begin{aligned}
        & \textstyle\max_m \big |\widehat \eta_{\mu, m} (X) - \eta_{\mu, m}  (X)\big | \\
        & \textstyle \le \sum_{k = 1}^{K} \mu_k  \max_m\big |[\widehat \Phi(X)]_{m, k} - [ \Phi(X)]_{m, k} \big| \\
        & = \textstyle \sum_{k = 1}^{K_1} \mu_k  \max_m\big |[\widehat \Phi(X)]_{m, k} - [ \Phi(X)]_{m, k} \big|
    \end{aligned}
    \] where the last equality holds because $[\widehat \Phi(X)]_{m, k} = [\Phi(X)]_{m, k}$ for $k \ge K_1+1$. Following this inequality, we have that for any $P\in \cP$ \[
    \begin{aligned}
        & \textstyle P \big \{ \max_m \big |\widehat \eta_{\mu, m} (X) - \eta_{\mu, m}  (X)\big |  \ge K_1 t\big \}\\
        & \le \textstyle \sum_{k = 1}^{K_1} P \big \{ \max_m \big |[\widehat \Phi(X)]_{m, k} - [ \Phi(X)]_{m, k} \big|  \ge \frac t{\mu_k}\big \} \\
        & \le \textstyle \sum_{k = 1}^{K_1}\rho_{k, 1} \exp\big (- \rho_{k, 2} \mu_k ^{-2}a_{ k, n}^{-2} t^2 \big )\\
        & \textstyle \le  \rho_1 \exp\big (- {\rho_{ 2}}{K_1^2} \{\wedge_{k = 1}^{K_1}\mu_k^{-1}a_{ k, n}^{-1}\}^2 t^2 \big )
    \end{aligned}
    \] where $\rho_1 = \frac{\max_{k \le K_1} \rho_{k, 1}}{K_1}$ and $\rho_2 = K_1^{-2} \times \{ \wedge_{k \le K_1} \rho_{k, 2}\} $. Note that 
    \[
    \textstyle K_1\{\wedge_{k = 1}^{K_1}\mu_k^{-1}a_{ k, n}^{-1}\}^{-1} = K_1 \max_{k = 1}^{K_1}\mu_k a_{ l, n} \ge  \sum_{k \le K_1 } \mu_k a_{ k, n} = a_n\,. 
    \] Thus, 
    \[
    \begin{aligned}
       & \textstyle P \big \{ \max_m \big |\widehat \eta_{\mu, m} (X) - \eta_{\mu, m}  (X)\big |  \ge K_1 t\big \}\\
        & \textstyle \le  \rho_1 \exp\big (- {\rho_{ 2}}{K_1^2} \{\wedge_{k = 1}^{K_1}\mu_k^{-1}a_{ k, n}^{-1}\}^2 t^2 \big ) \le \rho_1 \exp\big (- {\rho_{ 2}}a_n ^2 t^2 \big ) \,.
    \end{aligned}
    \] 

    
\end{proof}

\subsection{The lower bound}
To begin, we discuss the high-level proof strategy that will achieve our lower bound. Ultimately, for every $k \le K_1$ we shall establish that for any $\eps_k \in [0, 1]$ and $n \ge 1$
\begin{equation} \label{eq:lower-bound-individual}
     \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(\mu, A_n) \ge c_k \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha} \,,
\end{equation} for some constant $c_k > 0$. Then, defining $c = \min\{c_k: k \le K_1\}$ we have the lower bound
\[
\begin{aligned}
    \textstyle  \min\limits_{A_n \in \cA_n} \max\limits_{P \in \cP} ~~ \cE_P(\mu, A_n) & \ge \textstyle  \max\limits_{k \le K_1 } c_k \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha}\\
    & \ge \textstyle  \max\limits_{k \le K_1 } c \big \{ \mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}\big\}^{1+\alpha}\\
    & \ge \textstyle  c  \big \{ \sum_{k \le K_1 }\frac{\mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}}{K}\big\}^{1+\alpha}\\
    & \ge \textstyle  c K^{-1-\alpha}  \big \{ \sum_{k \le K_1 }{\mu_k n^{- \frac{\gamma_k}{2\gamma_k + d}}}\big\}^{1+\alpha}\,,\\
\end{aligned}
\] which would complete the proof. 

It remains to establish \eqref{eq:lower-bound-individual} for each $k \in [K_1]$. To obtain this, we construct a finite family of probability measures $\cM_r \subset \cP$ (indexed by $[r]$) and study $\max\limits_{P \in \cM_r}$. The technical tool which allows this to be fruitful is a generalized version of Fano's lemma. 

\begin{lemma}[Generalized Fano's lemma]
\label{lemma:fano}
Let $r \ge 2$ be an integer and let $\cM_r \subset \cP$ contains $r$ probability measures indexed by $\{1, \dots , r\}$ such that for a pseudo-metric $d$ (\ie\ $d(\theta , \theta') = 0$ if and only if $\theta = \theta'$) any $j \neq j'$
\[
\textstyle d\big (\theta(P_j), \theta(P_{j'})\big) \ge \alpha_r , ~~\text{and} ~~ \text{KL}(P_j , P_{j'}) \le \beta_r\,.
\] Then 
\[
\textstyle \max\limits_j \Ex_{P_j}\big[ d(\theta(P_j), \widehat \theta)\big ] \ge \frac{\alpha_r}{2} \big (1 - \frac{\beta_r + \log 2}{\log r}\big )\,. 
\]
    
\end{lemma} In our construction $\theta(P^\sigma)  = g^\star_{\mu, \sigma}$ and $d\big (\theta(P^{\sigma_0}), \theta(P^{\sigma_1})\big) = \cE_{P^{\sigma_0}}(g^\star_{\mu, \sigma_{1}}, \mu)$. 

\input{sections/lower_bound_supp}



% \begin{proof}[Proof of Theorem \ref{thm:bound-gen}]
% The proof of the local polynomial concentration follows directly from Theorem ?? in \citet{audibert2007Fast}. We prove the other two parts. 


% \paragraph{The lower bound.} The proof of the lower bound is almost similar to the proof of Theorem \ref{thm:lower-bound} except some minor chances. We describe them below. Fix a $\lambda \in \Delta^{L-1}$
% \begin{enumerate}
%     \item Instead of $h = k \lambda^{\frac{1}{\beta}} n^{-\frac{1}{2\beta + d}}$ we now consider $h = k \{ \max_{l \le L_1} \lambda_l^{\frac{1}{\beta_l}} n^{-\frac{1}{2\beta_l + d}} \}$. Moreover, define $l_0 = \argmax_{l \le L_1} \lambda_l^{\frac{1}{\beta_l}} n^{-\frac{1}{2\beta_l + d}}$.  
%     \item  For a function $\sigma: \cG_0 \to [M]$  define \begin{equation}
%     \Phi_{l, m}^{\sigma}(x) = \begin{cases}
%      \frac{1 - K_{\beta, l} \eps^{\beta_l}\bbI\{\sigma(y) = m\}}{2} & \text{when} ~ l \le L_1, ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
%      \frac{1 - K_{\beta, l_0} \eps^{\beta_{l_0}}\bbI\{\sigma(y) = m\}}{2} & \text{when} ~ l \ge L_1 + 1, ~  x \in\cB(x, \eps, \ell_{\infty})~ \text{for some}   ~  y \in \cG_0,\\
%      \frac{1}{2} & \text{elsewhere.}
%     \end{cases}
% \end{equation} For such a construction, for $l \le L_1$ the $\Phi_m^\sigma$ are $(\beta_l, K_{\beta, l})$-H\"older smooth on the support of $P_X$.
% \end{enumerate}
%     \SM{finish the proof.}
% \end{proof}

\section{Judge Prompt}
\label{sec: JP}
% \begin{tcolorbox}[
%     title=Prompt format for the evaluator Llama 3.1 70b Instruct LLM.,
%     breakable,
%     colback=gray!5,
%     colframe=gray!50
% ]
% \begin{verbbox}[\linewidth]

% \begin{mdframed}[backgroundcolor=gray!10]
% \begin{listing}[H]
% \caption{My Caption}
% \begin{lstlisting}[label=prompt-format, caption=Prompt format for the evaluator Llama 3.1 70b Instruct LLM.,xleftmargin=0.5cm]
\begin{tcolorbox}[
    title=Prompt format for the Llama 3.1 70b evaluator instruct LLM, label=prompt format, breakable, colback=gray!10, colframe=gray!50
]\label{prompt-format}
\begin{minted}[
    breaklines,
    breakanywhere,
    frame=none,
    fontsize=\normalsize,
]{text}
<dmf>user
I want you to act as a judge for how well a model did answering a user-defined task. You will be provided with a user-defined task that was given to the model, its golden answer(s), and the model's answer. The context of the task may not be given here. Your task is to judge how correct is the model's answer. Your task is to judge how correct the model's answer is based on the golden answer(s), without seeing the context of the task, and then give a correctness score. The correctness score should be one of the below numbers: 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0 (totally right). You should also add a brief justification regarding how the model's answer conforms to or contradicts the golden answer(s). 

Your response must follow the format
{{
   "correctness_score": your_score, 
   "justification": your_justification
}}

Note that each one of the golden answers is considered correct. Thus if the Model's Answer matches anyone of the golden answers, it should be considered correct.
<dmf>assistant
Certainly! I will act as a judge and adhere to the format you requested. Under oath, I will judge without consulting my internal knowledge of facts under any circumstances.
<dmf>user
User-defined task --- Sandy bought 1 million Safe Moon tokens. She has 4 siblings. She wants to keep half of them to herself and divide the remaining tokens among her siblings. After splitting it up, how many more tokens will she have than any of her siblings?

Golden Answer(s) --- \<answer 1\> 375000

Model's Answer --- Sandy will have more tokens than any sibling by 3/8 million.
<dmf>assistant
{{
    "correctness_score": 1.0,
    "justification": "The golden answer states that Sandy will have 375,000 more tokens than any of her siblings, which is a precise numerical value. The model's answer translates this scenario into a fraction of the total, saying Sandy will have more tokens than any sibling by 3/8 million. 1 million tokens * 3/8 =375,000 tokens. So the model provided an answer in fractional form that, when converted to a numerical value, exactly matches the golden answer's quantity."
}}
<dmf>user
User-defined task --- Extract the names and emails mentioned in the following text. Richard Matthew Stallman, also known by his initials, rms,[1] is an American free software movement activist and programmer. He can be reached at rms@gnu.org based on the information on his webpage.

Golden Answer(s) --- {{"names":  ["Richard Matthew Stallman"], "emails": ["rms@gnu.org"]}}

Model's Answer --- names: Richard Matthew Stallman
<dmf>assistant
{{
    "correctness_score": 0.5,
    "justification": "The model correctly identified the names but failed to extract the emails."
}}
<dmf>user
User defined task --- Question: In 2015 Edgar Lungu became prime minister of?

Golden Answer: 
<answer 1> Zambia; <answer 2> Zamibia; <answer 3> People of Zambia; <answer4> Zambian cuisine; <answer 5> Zambians; <answer 6> Culture of Zambia; <answer 7> Etymology of Zambia; <answer 8> Zambia; <answer 9> Health care in Zambia; <answer 10> ISO 3166-1:ZM; <answer11> Republic Of Zambia; <answer 12> Cuisine of Zambia; <answer 13> Sport in Zambia; <answer 14> Republic of Zambia; <answer 15> Zambian people; <answer 16> Name of Zambia

Model's Answer: Prime Minister
<dmf>assistant
{{
    "correctness_score": 0.0,
    "justification": "The golden answers provide a detailed list of entities all relating to Zambia, indicating that Edgar Lungu became the leader (specifically, they mentioned \"prime minister\") of Zambia in 2015. The model's answer, \"Prime Minister,\" merely repeats part of the question without answering it."
}}
<dmf>user
User defined task --- Give a one-line concise summary of the following news
PARIS (Reuters) - French President Emmanuel Macron will set out plans for reforming the European Union on Tuesday, including proposals for a separate eurozone budget, despite a German election result that is likely to complicate his far-reaching ambitions. German Chancellor Angela Merkel s conservatives saw their support slide in Sunday s election, though they remain the biggest parliamentary bloc. She is expected to seek a coalition with the liberal Free Democrats (FDP) - who have criticized Macron s ideas for Europe - and the Greens. Elysee officials said Macron, who has promised sweeping reforms to Europe s monetary union in coordination with Merkel, hoped the issues to be raised in his speech would be taken into account in Germany s coalition negotiations. One Elysee official said a eurozone budget, one of Macron s most contentious ideas, would be necessary in due course and that the president would therefore raise the issue in his speech, to be delivered at the Sorbonne University in Paris. Since his election in May, Macron has made the overhaul of the EU and its institutions one of his major themes. As well as his eurozone budget idea, he wants to see the appointment of a eurozone finance minister and the creation of a rescue fund that would preemptively help countries facing economic trouble. Ahead of Sunday s election, Merkel had indicated her willingness to work with Macron on a reform agenda, even if her own ideas may not reach as far as his. But the election results have left Merkel facing a difficult coalition-building task which is in turn likely to limit her flexibility on Europe. A coalition of Merkel s CDU/CSU bloc, the FDP and the Greens is unprecedented at the national level - and any attempt by the chancellor and Macron to press for greater EU integration will face opposition from the new German lower house Bundestag. The FDP has called for a phasing out of Europe s ESM bailout fund and changes to EU treaties that would allow countries to leave the euro zone. And the far-right, eurosceptic Alternative for Germany is now the third biggest party in the Bundestag, further curbing Merkel s room for maneuver. But Elysee officials noted that the FDP had reaffirmed its attachment to the EU and to strong Franco-German relations, a point Macron was likely to emphasize in his speech while at the same time not seeking to impose anything on his partners. Macron, the sources said, would propose that the whole EU move forward together, and that those who did not want to should not stand in the way of those that did. Coming just two days after the German election, Macron s speech is likely to be interpreted in Germany as an attempt to shape the debate before the coalition talks begin in earnest. German coalition agreements are strict, with the contours set out in them limiting the government s room for maneuver. In that respect, 
Merkel s ability to work with Macron on EU reform will be pre-determined by whatever coalition deal is struck. FDP leader Christian Lindner said on Monday he would not agree to any coalition that did not promise a change in the German government s direction. While that appeared to set the stage for tough talks, he also offered hope for Macron, saying when asked about the French president s eurozone budget ideas that the FDP had a strong interest in the strength of France.

Golden Answer(s) --- After German election, Macron to set out his vision for Europe

Model's Answer --- French President Emmanuel Macron to introduce plans for reforming the European Union amid the uncertain aftermath of German elections
<dmf>assistant
{{
    "correctness_score": 0.6,
    "justification": "While the golden answer is more concise, the model's answer is largely similar to the golden answer in its meaning."
}}
<dmf>user
User defined task --- {MODELTASK}
Golden Answer(s) --- {GOLDENRESPONSE}
Model's Answer --- {MODELRESPONSE}
\end{minted}
\end{tcolorbox}
% \end{lstlisting}
% \end{verbbox}
% \theverbbox
% \end{listing}
% \end{mdframed}


% \begin{minted}[
%     breaklines,
%     breakanywhere,
%     frame=none,
%     bgcolor=red!20,
%     fontsize=\normalsize,
% ]{text}
%     <dmf>assistant
% {{
%     "correctness_score": 0.6,
%     "justification": "While the golden answer is more concise, the model's answer is largely similar to the golden answer in its meaning."
% }}
% <dmf>user
% User defined task --- {MODELTASK}
% Golden Answer(s) --- {GOLDENRESPONSE}
% Model's Answer --- {MODELRESPONSE}
% \end{minted}


% \begin{tcolorbox}[
%     title=Prompt format for the evaluator Llama 3.1 70b Instruct LLM.,
%     breakable,
%     colback=gray!5,
%     colframe=gray!70
% ]
% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
% <dmf>user

% I want you to act as a judge for how well a model did answering a user-defined task. You will be provided with a user-defined task that was given to the model, its golden answer(s), and the model's answer. The context of the task may not be given here. Your task is to judge how correct is the model's answer. Your task is to judge how correct the model's answer is based on the golden answer(s), without seeing the context of the task, and then give a correctness score. The correctness score should be one of the below numbers: 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0 (totally right). You should also add a brief justification regarding how the model's answer conforms to or contradicts the golden answer(s). 

% Your response must follow the format
% {{
%    "correctness_score": your_score, 
%    "justification": your_justification
% }}

% Note that each one of the golden answers is considered correct. Thus if the Model's Answer matches anyone of the golden answers, it should be considered correct.



% <dmf>assistant

% Certainly! I will act as a judge and adhere to the format you requested. Under oath, I will judge without consulting my internal knowledge of facts under any circumstances.
% \end{minted}

% \noindent\rule{\linewidth}{0.4pt} % Horizontal line

% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
% <dmf>user

% User-defined task --- Sandy bought 1 million Safe Moon tokens. She has 4 siblings. She wants to keep half of them to herself and divide the remaining tokens among her siblings. After splitting it up, how many more tokens will she have than any of her siblings?

% Golden Answer(s) --- \<answer 1\> 375000

% Model's Answer --- Sandy will have more tokens than any sibling by 3/8 million.



% <dmf>assistant

% {{
%     "correctness_score": 1.0,
%     "justification": "The golden answer states that Sandy will have 375,000 more tokens than any of her siblings, which is a precise numerical value. The model's answer translates this scenario into a fraction of the total, saying Sandy will have more tokens than any sibling by 3/8 million. 1 million tokens * 3/8 =375,000 tokens. So the model provided an answer in fractional form that, when converted to a numerical value, exactly matches the golden answer's quantity."
% }}
% \end{minted}

% \noindent\rule{\linewidth}{0.4pt} % Horizontal line

% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
% <dmf>user

% User-defined task --- Extract the names and emails mentioned in the following text. Richard Matthew Stallman, also known by his initials, rms,[1] is an American free software movement activist and programmer. He can be reached at rms@gnu.org based on the information on his webpage.

% Golden Answer(s) --- {{"names":  ["Richard Matthew Stallman"], "emails": ["rms@gnu.org"]}}

% Model's Answer --- names: Richard Matthew Stallman



% <dmf>assistant

% {{
%     "correctness_score": 0.5,
%     "justification": "The model correctly identified the names but failed to extract the emails."
% }}
% \end{minted}

% \noindent\rule{\linewidth}{0.4pt} % Horizontal line

% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
% <dmf>user

% User defined task --- Question: In 2015 Edgar Lungu became prime minister of?

% Golden Answer: 
% <answer 1> Zambia; <answer 2> Zamibia; <answer 3> People of Zambia; <answer4> Zambian cuisine; <answer 5> Zambians; <answer 6> Culture of Zambia; <answer 7> Etymology of Zambia; <answer 8> Zambia; <answer 9> Health care in Zambia; <answer 10> ISO 3166-1:ZM; <answer11> Republic Of Zambia; <answer 12> Cuisine of Zambia; <answer 13> Sport in Zambia; <answer 14> Republic of Zambia; <answer 15> Zambian people; <answer 16> Name of Zambia

% Model's Answer: Prime Minister



% <dmf>assistant

% {{
%     "correctness_score": 0.0,
%     "justification": "The golden answers provide a detailed list of entities all relating to Zambia, indicating that Edgar Lungu became the leader (specifically, they mentioned \"prime minister\") of Zambia in 2015. The model's answer, \"Prime Minister,\" merely repeats part of the question without answering it."
% }}
% \end{minted}

% \noindent\rule{\linewidth}{0.4pt} % Horizontal line

% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}
% <dmf>user

% User defined task --- Give a one-line concise summary of the following news
% PARIS (Reuters) - French President Emmanuel Macron will set out plans for reforming the European Union on Tuesday, including proposals for a separate eurozone budget, despite a German election result that is likely to complicate his far-reaching ambitions. German Chancellor Angela Merkel s conservatives saw their support slide in Sunday s election, though they remain the biggest parliamentary bloc. She is expected to seek a coalition with the liberal Free Democrats (FDP) - who have criticized Macron s ideas for Europe - and the Greens. Elysee officials said Macron, who has promised sweeping reforms to Europe s monetary union in coordination with Merkel, hoped the issues to be raised in his speech would be taken into account in Germany s coalition negotiations. One Elysee official said a eurozone budget, one of Macron s most contentious ideas, would be necessary in due course and that the president would therefore raise the issue in his speech, to be delivered at the Sorbonne University in Paris. Since his election in May, Macron has made the overhaul of the EU and its institutions one of his major themes. As well as his eurozone budget idea, he wants to see the appointment of a eurozone finance minister and the creation of a rescue fund that would preemptively help countries facing economic trouble. Ahead of Sunday s election, Merkel had indicated her willingness to work with Macron on a reform agenda, even if her own ideas may not reach as far as his. But the election results have left Merkel facing a difficult coalition-building task which is in turn likely to limit her flexibility on Europe. A coalition of Merkel s CDU/CSU bloc, the FDP and the Greens is unprecedented at the national level - and any attempt by the chancellor and Macron to press for greater EU integration will face opposition from the new German lower house Bundestag. The FDP has called for a phasing out of Europe s ESM bailout fund and changes to EU treaties that would allow countries to leave the euro zone. And the far-right, eurosceptic Alternative for Germany is now the third biggest party in the Bundestag, further curbing Merkel s room for maneuver. But Elysee officials noted that the FDP had reaffirmed its attachment to the EU and to strong Franco-German relations, a point Macron was likely to emphasize in his speech while at the same time not seeking to impose anything on his partners. 
% Macron, the sources said, would propose that the whole EU move forward together, and that those who did not want to should not stand in the way of those that did. Coming just two days after the German election, Macron s speech is likely to be interpreted in Germany as an attempt to shape the debate before the coalition talks begin in earnest. German coalition agreements are strict, with the contours set out in them limiting the government s room for maneuver. In that respect, Merkel s ability to work with Macron on EU reform will be pre-determined by whatever coalition deal is struck. FDP leader Christian Lindner said on Monday he would not agree to any coalition that did not promise a change in the German government s direction. While that appeared to set the stage for tough talks, he also offered hope for Macron, saying when asked about the French president s eurozone budget ideas that the FDP had a strong interest in the strength of France.

% Golden Answer(s) --- After German election, Macron to set out his vision for Europe

% Model's Answer --- French President Emmanuel Macron to introduce plans for reforming the European Union amid the uncertain aftermath of German elections



% <dmf>assistant

% {{
%     "correctness_score": 0.6,
%     "justification": "While the golden answer is more concise, the model's answer is largely similar to the golden answer in its meaning."
% }}
% \end{minted}

% \noindent\rule{\linewidth}{0.4pt} % Horizontal line

% \begin{minted}[numberblanklines=true,showspaces=false,breaklines=true]{text}

% <dmf>user
% User defined task --- {MODELTASK}
% Golden Answer(s) --- {GOLDENRESPONSE}
% Model's Answer --- {MODELRESPONSE}
% \end{minted}
% \label{listing:2}
% \end{tcolorbox}