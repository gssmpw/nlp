\section{Introduction}
\label{sec:intro}


%The rapid advancement of large-language models (LLMs) is remarkable, as they 

% Large language models (LLMs) can now routinely tackle a variety of tasks in academic, industrial, and everyday contexts \citep{minaee2024large}. This continued success has inspired the rapid introduction of new LLMs for both general and specialized purposes. Although this abundance has the potential to provide practitioners with flexibility, the sheer number of options often makes it challenging to select a suitable model given budget and performance needs. In particular, given a question of interest, how should one know which LLM to send it to? In a perfect world, one might simply opt to use the most powerful model, but for many, this will quickly lead to prohibitive financial and computational costs. 


Large language models (LLMs) have demonstrated the capability to effectively address a diverse array of tasks across academic, industrial, and everyday settings \citep{minaee2024large}. This continued success has catalyzed the rapid development of new LLMs tailored for both general and specialized applications \citep{myrzakhan2024open}. While %the proliferation of these models
this offers practitioners increased flexibility, the vast number of available options may pose a daunting challenge in their real-world deployment. Particularly, determining the optimal LLM for a given query remains a significant challenge. In a perfect world, all queries can be routed to the most powerful model, but for many, this may quickly become prohibitively expensive.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.4\textwidth]{plots/fmselect_gpt4o_comparison.pdf} 
        %\caption{Test 1}
        %\label{fig:RouterBench}
    %\hfill
    %\begin{minipage}{0.49\textwidth}
      % \includegraphics[width=\textwidth]{plots/router-vs-gpt4.pdf}
      %  \caption{Test 2}
     %   \label{fig:router-vs-gpt4}
    %\end{minipage}
\caption{Percent of GPT-4o performance achieved by CARROT across dataset at various discounted costs, where the blue dotted line indicates similar ($100\%$) performance to GPT-4o.}
\label{fig:IBMMix-spider}
\end{figure}

%Although some proprietary models, like GPT-4, offer outstanding performance, they often come with significant costs due to high API prices.

%A common approach to address this issue is to optimize LLMs to maintain a desirable performance at a uniformly cheaper cost. To do so, researchers have tried chain-of-thought reasoning \citep{wei2022chain,wu2023chain} to improve performance (at a relatively fixed cost), and weight quantization \citep{lin2024awq,zhao2024atom} or model distillation \citep{hsieh2023distilling}  to reduce model size. These techniques can be effective in reducing the serving cost of a single model but have not uniformly addressed the cost of deployment that LLMs often incur. %given the rapid rise in the number of available LLMs, these techniques are certainly not scalable in the long run. 

A common approach to address this issue %Another popular approach 
is \emph{routing} \citep{shnitzer2023large, hu2024routerbench, ong2024routellmlearningroutellms, notdiamond2023rorf,akota2024}. There are two paradigms of routing; \emph{non-predictive} routers repeatedly call LLMs and evaluate the responses to select the best one for a given query. Examples include Fusion of Experts (FoE) \citep{wang2023fusing}, FrugalGPT \citep{chen2024frugalgptTMLR}, and techniques that cascade answers from weak to strong LLMs \citep{yue2024largelanguagemodelcascades}. The obvious disadvantage non-predictive routing is the required inference of many LLMS for all queries, even those that are not suitable for the task at hand. As a workaround, researchers have also considered \emph{predictive routers}, which take LLM queries as inputs and output guesses at the most appropriate LLM.  %Routing has several advantages over optimizing a single LLM. First, as we shall see later, it treats each LLM as an input-output black-box, avoiding the need to delve into intricate infrastructure details, thus making it both flexible and scalable. Secondly, routing can allow practitioners to ``customize" the criteria by which LLM's are selected. 
A key limitation of the prior literature on predictive routing is \emph{the avoidance of the cost prediction problem}. In \citet{shnitzer2023large} only performance is considered. The methods RouteLLM %their router cannot handle the inclusion of cost into the model selection desiderata. 
\citep{ong2024routellmlearningroutellms} RoRF \citep{notdiamond2023rorf} incorporates model cost by creating binary routers that select between a large costly model and a cheap small model. While this does incorporate cost, we shall see that the reduced flexibility of binary routing leads to performance degradation in practice. Finally, in \citet{akota2024} cost prediction is considered, but the authors assume that cost may be inferred from available API pricing; this does not hold if model responses are open-ended, and thus may vary in length. 


%\my{discuss limitations of key methods: they don't explicitly consider cost. My paper just tries to choose best model; RouteLLM implicitly considers cost by routing between small and big models, but this limits its ability to benefit from diverse strengths of many available closed- and open-source models.}
%while a single LLM, especially the small ones, may struggle to perform well on an expanding landscape of LLM tasks, by escaping the reliance on a single LLM, routing allows us to benefit from the diversity of LLMs. 
% \SM{mention somewhere that the collection of LLMs can be easily extended on the fly.}

% Let us understand the task of routing from a statistician's point of view. Given spaces of inputs $\cX$ and outputs $\cY$ we have a collection of input-output black-box models, denoted as $f_1, \dots, f_M: \cX \to \cY$.


To address these shortcomings, we develop CARROT: a Cost AwaRe Rate Optimal rouTer and collect the Smart Price-aware Routing (\newdata) dataset. \newdata\ is representative of both performance and cost of state-of-the-art LLMs across varying practical use-cases. Additionally, we use it to train the final version of CARROT. CARROT is designed to function as an estimate of \emph{the oracle router}. As a concrete example of the oracle router, consider model performance and inference cost as the metrics of interest. First, based on their needs, a practitioner selects a convex combination of performance and cost as the model metric. Next, the oracle router takes in a query and produces the LLM that minimizes this specific model metric. To do this selection, the oracle router needs to perfectly predict the metrics of interest (cost, performance) for each model given any query. Inspired by this, CARROT %\my{before talking about plug-in, should we explain what an "oracle" router would look like. Then its limitations motivating the plug-in approach} This leads us to 
utilizes a simple two-stage approach. We first attain an estimator for each of the metrics (\eg\ cost and performance) for each model given a query, then we plug in these estimators to the formed risk function and select a model that minimizes the appropriate convex combination of the estimated metrics. Although CARROT utilizes a simple approach, it has three distinct advantages.

 % \colorbox{red!5!white}
% {\parbox{\dimexpr\linewidth-2\fboxsep}{%
 % \begin{itemize}
  % \item {\bf Computational efficiency.} For any collection of models and any collection of metrics CARROT can estimate the oracle router for any convex combination of the metrics with only one stage of model training.
   %\item {\bf High-quality data} To obtain a router that can be subsequently used in practice, we collect a new routing dataset representative of both performance and cost of state-of-the-art LLMs and varying practical use-cases. 
   % verify the efficacy of CARROT on a new routing dataset  The final version of CARROT is trained on \newdata\, a new, high-quality, routing dataset we develop.
  % \item {\bf Statistical efficiency.} An information-theoretic minimax investigation reveals that CARROT is a statistically efficient estimator for the oracle router in terms of sample complexity.
 % \end{itemize}
 % }}

\paragraph{Computational efficiency:} A practitioner may be interested in several possible convex combinations to understand the trade-offs across various metrics. In that case, learning a router per combination may not be practical. Our approach allows re-use of a plug-in estimate of the metrics for each combination, easing computational concerns. Furthermore, the metrics of interest may vary from practitioner to practitioner. CARROT allows the flexible addition of any metrics; one only needs to acquire estimators for them. As a demonstration of CARROT's efficiency, we utilize it to estimate the Pareto frontier of performance and cost trade-off on 
% two classic benchmark datasets, 
{RouterBench}\footnote{\url{https://huggingface.co/datasets/withmartian/routerbench}}\citep{hu2024routerbench}, open-LLM-leaderboard-v2\
\footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard}} \citep{open-llm-leaderboard-v2}, and our new \newdata\ dataset.

\paragraph{New dataset for routing:} \newdata\ covers 13 state-of-the-art language models (e.g. Llama-3-herd \citep{grattafiori2024llama3herdmodels}, GPT-4o \citep{openai2024gpt4}, \etc ) and approximately $45$k prompts from 6 benchmarks covering RAG, science, reasoning, and GPT-4 generated user queries. For all models, we use zero-shot prompting and corresponding chat templates to represent practical use cases and collect input and output token counts to allow flexibility when studying cost-performance trade-offs. We defer further details about \newdata\ and its advantages over other routing datasets to Section \ref{sec:datasets} and Appendix \ref{sec:append:sprout}.

% Furthermore, to ensure that all \newdata\ queries represent real-world use cases, zero-shot prompting is used along with an evaluation system based on MixEval \citep{ni2024mixeval}. We defer further details about \newdata\ and its advantages over other routing datasets to Section \ref{sec:datasets} and Appendix \ref{sec:append:sprout}.

As a sneak peek, in Figure \ref{fig:IBMMix-spider}, we present the ratio of CARROT's performance to GPT-4o's \citep{openai2024gpt4} on several key benchmarks across diverse use cases represented in \newdata. At $30 \%$ of the cost, CARROT matches or exceeds the performance of GPT-4o on each benchmark. Both \newdata\ and the corresponding CARROT router will be publicly released.

% Of course, any router is only as good as the data that it is trained on. To ensure that CARROT is ready to accurately route a diverse set of queries to a state-of-the-art set of LLMs, we also develop (and train on) the Scalable Price-aware Routing (\newdata) dataset. \newdata\
 % covers 13 state-of-the-art language models (e.g. Llama-3-herd \citep{grattafiori2024llama3herdmodels}, GPT-4o \citep{openai2024gpt4}, \etc ) and approximately $45$k prompts from 6 benchmarks covering RAG, science, reasoning, and GPT-4 generated user queries. Furthermore, to ensure that all \newdata\ queries represent real-world use cases, zero-shot prompting is used along with an evaluation system based on MixEval \citep{ni2024mixeval}. We defer further details about \newdata\ and its advantages over other routing datasets to Section \ref{sec:datasets} and Appendix \ref{sec:append:sprout}.
 
%\my{discuss our new dataset - I'll write it}

% Collecting adequate data for training a router has its own challenges; one needs to collect answers along with their evaluations on performance and costs for each combination of LLMs and queries, which could make the whole data generating process expensive. Thus, statistical efficiency is also an important consideration for any routing procedure.

% Collecting adequate data for training router has its own challenge; inference for each query needs to be answers along with their evaluations on performances and costs need to be collected for each combination of LLMs and queries, which could make the data generating process expensive. Thus, statistical efficiency is also an important consideration for any routing procedure.

%Given that training data is not limitless \my{can mention that specifically for routing it is expensive to collect - gotta run all LLMs and evaluate everything - it could be made clear from previous paragraph too}, statistical efficiency is also an important consideration for any routing procedure. 

% \vspace{-0.2cm}

\paragraph{Statistical efficiency} Collecting adequate data to train a router is quite expensive as we need to generate and evaluate with every LLM for every query.
% challenging; for each LLM and every query one must collect a response \emph{and} an evaluation from a judge of that response. By necessity, this collection process must include inference from closed source models such as Claude 3.5 \citep{TheC3} and GPT-4o.  This makes the routing data gathering process expensive; for example, gathering \newdata\ for one model can cost hundreds of dollars.  %answers along with their evaluations on performance and costs for each combination of LLMs and queries, which makes whole data generating process expensive. 
% Clearly, routing data is not limitless, so
Thus, statistical efficiency is an important consideration for any routing procedure. To investigate statistical efficiency, we connect the routing problem to multi-objective classification with a wide class of possible loss functions (this depends on the convex combination of model metrics on hand). Here, identities of the possible models act as the label space of the multi-class classification problem. Given this, one contribution of our work is to extend previous minimax studies in nonparametric classification \citep{audibert2007Fast} on two fronts: (1) with more than two classes, and (2)  with general losses beyond $0/1$-loss. Both of these extensions require us to introduce a generalized definition of margin (\cf\ eq. \eqref{eq:margin}), which reduces to the usual margin definition as in \citet{audibert2007Fast} when the classification task is binary and the loss is $0/1$.  Additionally, our work analyzes minimax optimality on the entire collection of oracle classifiers in classification problems with all possible trade-offs between multiple objectives, which, to the best of our knowledge, is a novel contribution.

%Our theoretical contributions allow us to establish that simple plug-in routers are minimax rate optimal. This, along with our empirical demonstrations, shows that CARROT is both computationally and statistically efficient. %approach to estimate the entire collection of oracle classifiers.
 
%a model like GPT-4 \citep{openai2024gpt4} may offer outstanding performance, but model calls often come with significant costs due to high API prices. On the other hand, a practitioner could opt to query an open-source LLM (e.g. a model in the Llama family \citep{grattafiori2024llama3herdmodels}) which comes with reduced pricing but at a possibly reduced performance.
%if for any query and any LLM (e.g GPT-4 \citep{openai2024gpt4} or Llama3 \citep{grattafiori2024llama3herdmodels}) the practitioner knew the associated cost and performance, they could always select the cheapest model that provides a satisfactory response.

%At this point, let us take a step back and provide more context to the ``best-suited'' models. A practitioner may be interested in various (conflicting) metrics such as various notions of accuracy, cost, and run time. As a concrete example, consider model performance and inference cost; a model like GPT-4 \citep{openai2024gpt4} may offer outstanding performance, but model calls often come with significant costs due to high API prices. On the other hand, a practitioner could opt to query an open-source LLM (e.g. a model in the Llama family \citep{grattafiori2024llama3herdmodels}) which comes with reduced pricing but at a possibly reduced performance. 
%This hints at a possible trade-off between various metrics of interest. To make it general and satisfy a broad spectrum of users' needs, this paper will develop routers for arbitrary trade-offs.
%\my{small models still cost money or have to self-serve; also cite papers for models} 
%Next, having decided on such a trade-off, the goal of the router is to predict the identity of the LLM that is expected to have the best possible trade-off value.

%This is a standard multi-class classification problem: one can form a trade-off risk function by combining the desired metrics with a suitable convex combination; then the goal is to take a query and pick the model that minimizes the risk. Here, identities of the possible models act as the label space of the multi-class classification problem. In an ideal world, we could produce an oracle router that always picks the model with minimum. To do so, one would need to perfectly predict the metrics of interest (cost, performance) for each model for any query. Inspired by this, we %\my{before talking about plug-in, should we explain what an "oracle" router would look like. Then its limitations motivating the plug-in approach} This leads us to 
%propose a simple two stage router in Section \ref{sec:setup}. We first attain an estimator for each of the metrics (e.g cost or accuracy) for each model given query $X$, then we plug in these estimators to the formed risk function and select a model that minimizes the appropriate convex combination of the estimated metrics.

%Given this, a predictive router is essentially a classifier that predicts the index of the best model, and thus can be efficiently learned by minimizing a standard classification loss using the pairs of query and convex combination of metrics. 

%Although this is a simple approach, it has a couple of distinct advantages. First, a practitioner may be interested in several possible convex combinations to understand the trade-offs across various metrics. In that case, learning a router per combination through direct optimization may not be practical. Our approach allows re-use of a plug-in estimate of the metrics for each model, easing computational concerns. Furthermore, the metrics of interest may vary from practitioner to practitioner. Our approach allows the flexible addition of any metrics; one only needs to acquire estimators for them.



%In short, our plug-in approach  has three advantages: 
%\begin{activitybox}

 %\paragraph{Simplicity.} It is simple, easily interpretable, and adapts to a broad spectrum of users' preferred trade-off, even when a new model or metric is introduced to the problem.

 %\paragraph{Computational efficiency.} It provides a computationally scalable approach to estimate routers for an arbitrary combination of metrics.


 %\paragraph{Statistical efficiency.} An information-theoretic minimax investigation reveals that the plug-in approach is statistically efficient in terms of sample complexity.

%\end{activitybox}



% \colorbox{red!5!white}{\parbox{\dimexpr\linewidth-2\fboxsep}{%
% \begin{itemize}
%  \item {\bf Computational efficiency.} It provides a computationally scalable approach to estimate routers for all combinations of metrics.
%  \item {\bf Statistical efficiency.} An information-theoretic minimax investigation reveals that the plug-in approach is statistically efficient in terms of sample complexity.
% \end{itemize}
% }}
   
%Empirically, our approach has been compared against several state-of-the-art routers on two benchmark datasets, namely {RouterBench}\footnote{https://huggingface.co/datasets/withmartian/routerbench}\citep{hu2024routerbench} and open-LLM-leaderboard-v2\
%\footnote{https://huggingface.co/spaces/open-llm-leaderboard/open\_llm\_leaderboard} \citep{open-llm-leaderboard-v2}
%to estimate the oracle routers and the Pareto frontier of performance and cost trade-off. 
% Both of the benchmark datasets contain a diverse set of tasks, such as reasoning, math, coding, etc, which necessitates the routing rather than fine-tuning.  
%As a sneak peek, in routerbench through our routing approach, we can achieve performance similar to GPT4 at half the cost, and $95\%$ of performance at less than $20\%$ of the cost. 

%On the theoretical side, the routing problem, at its heart, is a multi-objective classification problem with a wide class of possible loss functions (this depends on the convex combination of model metrics on hand). Given this, one contribution of our work is to extend previous minimax studies in nonparametric classification \citep{audibert2007Fast} on two fronts: (1) with more than two classes, and (2)  with general losses beyond $0/1$-loss. Both of these extensions require us introduce a generalized definition of margin (\cf\ eq. \eqref{eq:margin}), which reduces to the usual margin definition as in \citet{audibert2007Fast} when the classification task is binary and the loss is $0/1$.  Additionally, our work analyzes minimax optimality on the entire collection of oracle classifiers in classification problems with all possible trade-offs between multiple objectives, which, to the best of our knowledge, is a novel contribution.
%\my{we haven't established the connection between routing and classification yet - maybe do it earlier when explaining oracle router and plug-in router}

% Our theoretical contributions allow us to establish that simple plug-in routers are minimax rate optimal. This, along with our empirical demonstrations, shows that CARROTS is both computationally and statistically efficient. %approach to estimate the entire collection of oracle classifiers.

%\begin{itemize}
    %\item In the context of our routing application, we were required to extend previous minimax studies in nonparametric classification \citep{audibert2007Fast} on two fronts: (1) with more than two classes, and (2)  with general losses beyond $0/1$-loss. Both of these extensions require us introduce a generalized definition of margin (\cf\ eq. \eqref{eq:margin}), which reduces to the usual margin definition as in \citet{audibert2007Fast} when the classification task is binary and the loss is $0/1$. 
    %\item  We establish minimax optimal rates for multi-objective classification problems, which, to the best of our knowledge, is the first to analyze minimax optimality on the entire collection of oracle classifiers in classification problems with multiple objectives. 
    %\item  For a multi-objective classification problem, we establish that simple plug-in routers are minimax rate optimal, which provides a computationally and statistically efficient approach to estimate the entire collection of oracle classifiers.  
%\end{itemize}


\subsection{Related literature}


\paragraph{Performance vs cost trade-off in LLM predictions.} Several recent studies have explored optimizing the cost and performance trade-offs in the implementation of
large-language models (LLMs). LLM-BLENDER \citep{jiang2023llm} ensembles outcomes from
multiple LLMs to select the best response. Frugal-ML, Frugal-GPT \citep{chen2020frugalml,chen2024frugalgptTMLR} and FrugalFoE \cite{wang2023fusing} employ an LLM cascade to sequentially query LLMs until a reliable response is found. AutoMix \citep{madaan2023automix} relies on a smaller model to self-verify its response before potentially considering a larger model. While these approaches rely
on multiple LLM queries, our approach routes each query to a single LLM, an approach also considered in \citet{hu2024routerbench}. We complement these works by providing a statistically principled approach to learning this performance vs. cost trade-off. 

\paragraph{Ensemble learning.} The routing problem is closely related to ensemble learning that combines multiple models to obtain better performance. Classical ensemble methods include bagging (bootstrap aggregating), boosting, and
stacking (model blending) \citep{breiman1996bagging,breiman1996stacked,freund1996experiments,friedman2001greedy,wolpert1992stacked}. Most of these works implicitly assume that the models in the ensemble have \emph{similar expertise}, and
thus it is beneficial to aggregate their predictions, whereas in our case, models may have \emph{complementary expertise}, and averaging their outputs might be detrimental because most of them may not be suitable for an input. Therefore, we choose to predict using the model with the best outcome, rather than aggregating them. 

\paragraph{Minimax studies in non-parametric classification.} One of the earliest works on the minimax rate of convergence in non-parametric classification is \citet{audibert2007Fast}. These techniques were later adopted for investigating the ability of transfer learning under a distribution shift
% ; some prominent examples being
\citep{kpotufe2018Marginal,cai2019Transfer,maity2022minimax}. All of these works consider binary classification with $0/1$ loss. In comparison, our minimax investigation differs on two fronts: we extend the settings to classification with more than two classes and general cost functions. 
% To quantify the difficulty of classification in our extended framework through a margin condition, as done in \citet{audibert2007Fast} for binary classification with $0/1$ loss, we needed to extend the notion of margin appropriately (\cf\ eq. 
 % \eqref{eq:margin}). 

% \paragraph{Multiobjective learning}

\subsection{Notation and preliminaries}

To begin, let us introduce our notation. We have $M$ pre-trained LLMs indexed as $m \in [M] = \{1, \dots, M\}$ and $K$ metrics indexed as $k\in [K] = \{1, \dots, K\}$. %Throughout our paper, we index the models as $m \in [M]$ and the metrics as $k \in [K]$. 
We denote a generic input or query as $X\in \cX$, where $\cX$ is the space of inputs.  Thus, for any input $X$, the metrics of interest are stored in a $M\times K$ matrix. We denote this matrix as $Y \in \reals^{M \times K}$, whose $(m, k)$-th entry $[Y]_{m, k}$ is the metric value for obtaining a prediction from the $m$-th model evaluated with respect to $k$-th metric.  For all metrics, we assume that a lower value is preferred. With this convention, we shall also refer to them as risks. For a probability distribution $P$ in the sample space $\cX \times \reals^{M\times K}$ we assume that the training dataset $\cD= \{(X_i, Y_i)\}_{i = 1}^n$ is an $\iid$ sample from $P$. 

 For the probability distribution $P $ defined on the space $\cX \times \reals^{M \times K}$, we denote the marginal distribution of $X$ by $P_X$. Let us denote $\supp(\cdot)$ as the support of a probability distribution. Within the space $\reals^d$, we denote $\Lambda_d$ as the Lebesgue measure, $\|\cdot\|_2$ and $\|\cdot\|_\infty$ as the $\ell_2$ and $\ell_\infty$-norms, and $\cB(x, r, \ell_2)$ and $\cB(x, r, \ell_\infty)$ as closed balls of radius $r$ and centered at $x$ with respect to the $\ell_2$ and $\ell_\infty$-norms. 