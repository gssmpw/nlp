\section{Efficient learning of Pareto routers} 
\label{sec:efficient-learning-algorithm}


\subsection{The upper bound}\label{sec:upper-bound}

Recall in Algorithm \ref{alg:pareto-routers} we say that we require an ``efficient estimate'' $\widehat \Phi$ of the function $\Phi^\star$, whose $l$-th coordinate is $\Phi^\star_l(x) = \Ex_P[\ell\{Y, f_l(X)\mid X = x]$. Now, we make this statement precise and show that for such an $\widehat \Phi $ the excess risk \eqref{eq:excess-risk} achieves the rate in the lower bound \eqref{eq:lower-bound}. 

\begin{theorem}[Upper bound]\label{thm:upper-bound}
    Suppose that for some $\rho_1, \rho_2 > 0$ and any $n \ge 1$ and $t > 0$ and almost all $x$ with respect to $P_X$ we have the following concentration bound for $\widehat \Phi$:
    \begin{equation}\label{eq:concentration-phi}
        \max_{P\in \cP} P_{\cD_n} \big \{ \|\widehat \Phi(x) - \Phi^\star (x)\|_1 \ge t\big \} \le  \rho_1 \exp\big (- \rho_2 a_n t^2 \big )\,, 
    \end{equation}
    where $\{a_n; n \ge 1\}\subset \reals$ is a sequence that increases to $\infty$.  
    Fix a $\lambda \in [0, 1]$.  Then, if all the $P\in \cP$ satisfies the margin condition \ref{assmp:margin} with the parameters $(\alpha, K_\alpha)$ then there exists a $K> 0$ such that for any $n \ge 1$ the excess risk for the router $\widehat g_\lambda$ in \eqref{eq:eff-estimate-router} is upper bounded as 
    \begin{equation}
        \max_{P\in \cP} \Ex_{\cD_n}\big [\cE_P(\widehat g_\lambda,\lambda)\big ] \le K a_n^{-\frac{1+ \alpha}{2}}\,. 
    \end{equation}
    Thus, as long as $a_n = n^{{  2\beta/(2\beta + d)}}$  for any $\lambda \in [0, 1]$ the excess risk for the router $\widehat g_\lambda$ in \eqref{eq:eff-estimate-router} has the rate of convergence $a_n^{- {(1 + \alpha)}/{2}} = n^{- {\beta(1 + \alpha)}/{(2\beta + d)}}$ that matches with the lower bound rate in \eqref{eq:lower-bound}. 
\end{theorem}




% {\bf Question:} How can we find Pareto front solutions efficiently without using a back-tracking technique, \ie\ fit different $g$ for different $\lambda$? The hint is to learn $f^\star(x)$ as $\widehat f(x)$ and for a $\lambda > 0$ then define 
% \[
% \textstyle \widehat g _\lambda (x) = \delta _{\widehat l(x)}, ~~ \widehat l(x) =  \argmin_ l\big\{\ell (\widehat f(x_0), f_l(x_0)) + \lambda c_l (x_0)\big\}
% \] Show that $\widehat g_\lambda$ achieved the rate at lower bound. 


\subsection{Efficient learning of the loss regression function}
\label{sec:reg-fn-estimate}
We consider \emph{local polynomial regression} (LPR) to estimate the loss regression function. For a sample point $Z_i = (X_i, Y_i)$ we define the observed loss vector $\ell_i \in \reals^L$ where $[\ell_i]_l = \ell\{Y_i,f_l(X_i)\}$. Consider a symmetric kernel $\psi: \reals^d \to \reals_+$ and define $\Theta(\lfloor \beta \rfloor)$ as the class of all $\lfloor \beta \rfloor$-degree polynomials from $\reals^d$ to $\reals$. For a bandwidth $h > 0$ we define the LPR estimate as 
\begin{equation}
    \widehat \Phi_l(x) = \widehat\theta_x^{(l)}(0), ~~ \widehat \theta_x^{(l)} \in \argmin_{\theta \in \Theta(\lfloor \beta \rfloor)}  \textstyle \sum_{i = 1}^n \psi (\frac{X_i - x}{h}) \big [\ell \{Y_i, f_l(X_i)\} - \theta (X_i -x )\big]^2 \,. 
\end{equation}
In their Theorem 3.2 \citet{audibert2007Fast} establishes that for $h = n ^{-1/(2\beta + d)}$ the concentration bound in \eqref{eq:concentration-phi} is satisfied with $a_n = n^{-2\beta/(2\beta + d)}$, therefore the router derived from such $\widehat\Phi$ using \eqref{eq:eff-estimate-router} achieves the same rate of convergence as in the lower bound \eqref{eq:lower-bound}, \ie\ rate optimal. 