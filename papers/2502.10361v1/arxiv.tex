
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{longtable}
\usepackage{rotating}
\usepackage[table]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{listings}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{arxiv}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{enumitem}%

\usepackage[capitalize,noabbrev]{cleveref}

\lstdefinelanguage{yaml}{
    keywords={true,false,null,y,n,yes,no},
    keywordstyle=\color{blue}\bfseries,
    sensitive=true,
    comment=[l]{\#},
    commentstyle=\color{gray}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    morestring=[b]',
    morestring=[b]",
    basicstyle=\ttfamily,
    showstringspaces=false,
    columns=fullflexible,
}

\lstset{
    language=yaml,
    frame=single,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{red},
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    xleftmargin=10pt,
    framexleftmargin=10pt,
    framexrightmargin=10pt,
    framexbottommargin=5pt,
}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\definecolor{Baseline_RAW}{HTML}{73d2de}
\definecolor{Best_RAW}{HTML}{d81159}

\definecolor{color_blind_red}{HTML}{f2b3c3}
\definecolor{color_blind_green}{HTML}{66c2b3}

\colorlet{Baseline}{Baseline_RAW!15}
\colorlet{Best}{Best_RAW!10}
\colorlet{AverageRank}{gray!20}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Enhancing Multilingual LLM Pretraining with Model-Based Data Selection}

\begin{document}

\twocolumn[
\icmltitle{Enhancing Multilingual LLM Pretraining with Model-Based Data Selection}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bettina Messmer}{equal,epfl}
\icmlauthor{Vinko Sabolčec}{equal,epfl}
\icmlauthor{Martin Jaggi}{epfl}
\end{icmlauthorlist}

\icmlaffiliation{epfl}{School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland}

\icmlcorrespondingauthor{Bettina Messmer}{bettina.messmer@epfl.ch}
\icmlcorrespondingauthor{Vinko Sabolčec}{vinko.sabolcec@epfl.ch}

\icmlkeywords{Machine Learning}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
Dataset curation has become a basis for strong large language model (LLM) performance.
While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English.
To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples.
Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data.
We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method.
Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15\% of the training tokens, while also improving across other benchmarks.
These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have demonstrated impressive performance improvements when trained on increasingly larger datasets and model sizes~\citep{brown2020language}. While~\citet{brown2020language} already observed the importance of using a cleaned version of Common Crawl for improved performance, the high cost of LLM training has further motivated research into better pretraining quality filters.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/mmlu/agg_score_plot.pdf}\vspace{-1mm}
    \caption{Pretraining benchmark performance (average accuracy) measured on Chinese (CMMLU), German (MMLU), and French (MMLU), while training for 119B tokens, comparing the baseline FineWeb-2 dataset against data filtered using our FastText (\emph{FT}) and Transformer Multi-Layer Perceptron (\emph{MLP}) embedding-based filtering methods trained on our data mixture \emph{MKC$^+$}. When using our approaches, the data retention rates are set to 10\%.}
    \label{fig:mmlu_avg_cmn_deu_fra}
\end{figure}


Deduplication and heuristic-based dataset cleaning have become standard practices in data curation~\citep{rae2021scaling, raffel2020exploring, de2024new}.
These quality filters are often complemented by additional filters, such as the removal of personally identifiable information (PII)~\citep{penedo2024fineweb} or model-based toxicity filtering~\citep{soldaini2024dolma}.
Recently, model-based filtering has also emerged as a promising method for quality filtering.
The release of FineWeb-Edu~\citep{penedo2024fineweb} demonstrated that pretraining on just 10\% of the tokens (38B) from an English dataset filtered using a model-based approach can achieve performance comparable to models trained on 350B tokens of unfiltered data.
Moreover, when trained on equivalent amounts of data, this model largely outperforms the baseline. Concurrently, the release of DCLM~\citep{li2024datacomp} showed that competitive performance can be achieved using a simple and efficient model-based approach, namely a FastText~\citep{joulin2017bag} classifier trained on a carefully selected training dataset.

However, these recent advances have primarily focused on English data. This emphasis risks further widening the disparity in LLM performance between  languages, as less than half of internet content is written in English\footnote{\href{https://w3techs.com/technologies/overview/content_language}{w3techs.com/technologies/overview/content\_language}}. To address this concern, we aim to extend model-based filtering frameworks to multilingual datasets. While model perplexity-based filtering is commonly applied to multilingual datasets~\citep{wenzek2019ccnet, laurenccon2022bigscience, nguyen2023culturax}, the current state-of-the-art, FineWeb-2~\citep{penedo2024fineweb-2}, primarily relies on heuristic-based filters. In this work, we focus on model-based filtering with a quality definition that emphasizes: 1) structured data and 2) knowledge-rich data samples, to enhance multilingual pretraining datasets.

To achieve this, we leverage embedding-based classification models. Firstly, we adopt the FastText quality filtering approach from DCLM to develop a unified framework for multilingual datasets that span diverse language families, scripts, and resource availability, focusing on Chinese, German, French, Arabic, and Danish as representative languages for our experiments.
Additionally, we extend this embedding-based approach by incorporating Transformer~\citep{vaswani2023attentionneed} embeddings, specifically XLM-RoBERTa~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning}, for filtering. We compare the performance between baseline FineWeb-2 data and our best FastText and Transformer embedding-based approaches in Figure~\ref{fig:mmlu_avg_cmn_deu_fra}.

In summary, our contributions are as follows:
\begin{itemize}[itemsep=1pt,topsep=0pt]
    \item We propose a transparent, simple, and unified framework for multilingual model-based filtering at web scale, enabling data curation across diverse language families, scripts and resource availability.
    \item We present comprehensive per-language ablation studies of embedding-based multilingual quality filtering on top of the FineWeb-2 dataset~\citep{penedo2024fineweb-2}, achieving performance comparable to the baseline while using as little as 15\% of the tokens. We additionally analyze the impact of dataset contamination and multilingual LLM training.
    \item We evaluate the impact of different training datasets for data selection classifiers on the downstream performance of LLMs.
    \item We release the refined pretraining dataset\footnote{\href{https://huggingface.co/epfml}{huggingface.co/epfml}} covering 20 languages\footnote{Russian, Chinese, German, Japanese, Spanish, French, Italian, Portuguese, Polish, Dutch, Indonesian, Turkish, Czech, Vietnamese, Swedish, Persian, Arabic, Greek, Danish, Hungarian}, filtered using our proposed framework, along with the codebase\footnote{\href{https://github.com/epfml/fineweb2-hq}{{github.com/epfml/fineweb2-hq}}}, to advance multilingual language modeling.
\end{itemize}

\section{Related Work}
\textbf{Data Curation.} In order to pretrain LLMs on a large amount of diverse texts, Common Crawl\footnote{\href{https://commoncrawl.org/}{commoncrawl.org}} is often used as the base dataset. However, early works already observed that performing quality filtering on Common Crawl is crucial for model performance~\citep{brown2020language}. There exist various data curation approaches, such as deduplication~\citep{lee-etal-2022-deduplicating}, PII removal~\citep{subramani-etal-2023-detecting}, or toxicity filtering~\citep{arnett2024toxicity}. Another important aspect is quality filtering of the documents. For this, the definition of quality is an important aspect. A common approach is to use heuristics to remove documents outside of the target distribution, such as filtering based on average word length, existence of punctuation, or document length~\citep{rae2021scaling, raffel2020exploring}. Another approach is to define model-based filters, where research has focused on perplexity measure of the text ~\citep{wenzek2019ccnet} or focused on educational~\citep{penedo2024fineweb} and conversational documents~\citep{li2024datacomp}. In this work, we build upon previous curated datasets based on heuristic filtering, specifically Finweb-2~\citep{penedo2024fineweb-2}, and focus on model-based filtering for structured and knowledge-rich documents relying on textual embedding representation.

\textbf{Curated English datasets.} One of the early curated datasets was C4~\citep{raffel2020exploring}, followed by MassiveText~\citep{rae2021scaling}. RefinedWeb~\citep{penedo2023refinedweb} was an important step forward, demonstrating that filtered web data can outperform selected high-quality data sources. While these datasets have not been made fully publicly available, their filtering techniques have been expanded upon in recent fully public datasets, such as Dolma~\citep{soldaini2024dolma}, FineWeb, and FineWeb-Edu~\citep{penedo2024fineweb}. While FineWeb primarily relies on filter heuristics for data quality, Dolma adopts model perplexity filtering. FineWeb-Edu takes model-based filtering a step further and relies on LLM-based quality assessment. Similarly, a concurrent work, DCLM, has achieved competitive performance using FastText~\citep{joulin2017bag} classifier trained on a carefully selected training dataset. In this work we adapt and extend this approach to the multilingual context.

\textbf{Curated Multilingual Datasets.} Analogously to the English datasets, there have been efforts in the multilingual space. An influential work has been CCNet~\citep{wenzek2019ccnet}, whose language identification and model perplexity filter for data quality has been re-used in later datasets. Again, while CCNet was not published directly, but rather provided the tools for data cleaning, RedPajama~\citep{together2023redpajama} is a prominent multilingual dataset relying on these filtering techniques. While RedPajama offers data in 5 European languages, other datasets, such as OSCAR~\citep{OrtizSuarezSagotRomary2019,AbadjiOrtizSuarezRomaryetal.2021,2022arXiv220106642A}, mC4~\citep{xue-etal-2021-mt5}, ROOTS~\citep{laurenccon2022bigscience}, MADLAD-400~\citep{kudugunta2023madlad400multilingualdocumentlevellarge}, CulturaX~\citep{nguyen2023culturax}, and HPLT~\citep{de-gibert-etal-2024-new-massive}, focus on expanding beyond, spanning a variety of language families and scripts. While they offer refined datasets for hundreds of languages, FineWeb-2~\citep{penedo2024fineweb-2} pushes the limit to thousands of languages and further improves the performance. Our work also focuses on filtering quality samples across various language families and scripts. However, we limit our scope to 20 languages, as the number of documents drops quickly and there is trade-off between retaining a sufficient number of pretraining tokens and ensuring data quality~\citep{muennighoff2023scalingdataconstrainedlanguagemodels, held2025optimizing}.
In our results, we observe the greatest benefits using stricter data filtering.

\textbf{Multilingual Embedding Models.} Early word embedding models like Word2Vec~\citep{mikolov2013efficientestimationwordrepresentations} and GloVe~\citep{pennington-etal-2014-glove} lacked contextual understanding. FastText~\citep{bojanowski2017enrichingwordvectorssubword} built upon them and improved performance by incorporating subword information. Transformer~\citep{vaswani2023attentionneed} models like BERT~\citep{devlin2019bertpretrainingdeepbidirectional} and GPT~\citep{radford2018improving} then revolutionized the field with context-aware embeddings. Multilingual models like mBERT, XLM~\citep{lample2019crosslinguallanguagemodelpretraining}, and XLM-RoBERTa~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning} further advanced cross-lingual understanding, with recent open-source LLMs pushing performance even higher~\citep{llama3, mistral_small3_blog}. Using such models,  documents as well as representative samples can be mapped into a shared embedding space to estimate their similarity. Focusing on transparency, simplicity and efficiency in our work, we use FastText and XLM-RoBERTa for our %
filtering, and analyze the trade-off between computational complexity and filtering performance.


\textbf{Multilingual Evaluation.} Evaluating LLMs requires diverse benchmarks testing linguistic and cognitive abilities like reading comprehension, reasoning, and knowledge.  While English benchmarks like MMLU~\citep{hendrycks2020measuring} and ARC~\citep{clark2018thinksolvedquestionanswering} exist, other languages often use translations from English, e.g., XNLI~\citep{conneau-etal-2018-xnli} and machine-translated version of MMLU~\citep{lai-etal-2023-okapi}. However, translations can be problematic, failing to capture cultural nuances or introducing "translationese"~\citep{romanou2024include}. Recent work by~\citet{romanou2024include,singh2024globalmmluunderstandingaddressing} emphasizes the need for culturally sensitive, natively collected benchmarks. Task difficulty and task formulation also impact model performance when trained for shorter durations~\citep{kydlicek2024finetasksmultilingualtasks}. In our work, we follow the recent evaluation tasks selection and methodology by~\citet{kydlicek2024finetasksmultilingualtasks} to assess our model-based filtering approaches across multiple languages.


\section{Methods}\label{sec:method}
In this work, we present our model-based filtering approaches. Our methodology is structured into two key components: 1) we select suitable training datasets, aiming to identifying a diverse set of structured and knowledge-rich samples and 2) we describe the different models, namely FastText and Transformer embedding-based filters, used to capture and leverage these characteristics.

\subsection{Classifier Training Dataset}
\textbf{Representative Sample Selection.} Our goal is to identify a diverse set of structured and knowledge-rich samples, especially within a multilingual context. We define two criteria for our training datasets: 1) the samples must be informative and well-structured and 2) the datasets must be available in multiple languages. While some multilingual benchmark datasets meet these criteria precisely, it is important to note that we do not train the LLM directly on this data. Instead, we train a proxy model to assess pretraining data quality. Nevertheless, we must remain cautious about potentially increased pretraining data contamination stemming from this approach, as discussed in Section~\ref{sec:decontamination}. 

Based on our criteria, we selected the following datasets as representative examples.
\begin{itemize}[itemsep=2pt,topsep=0pt]
\item \textbf{\emph{Aya Collection}.} A prompt completion dataset comprising $\sim$514M samples covering a wide variety of tasks, generated using instruction-style templates in 101 languages~\citep{singh2024aya}.
\item \textbf{\emph{Aya Dataset}.} Human-annotated instruction fine-tuning dataset consisting of $\sim$202K prompt-completion pairs in 65 languages~\citep{singh2024aya}.
\item \textbf{\emph{MMLU}.} Originally for English language, the dataset contains $\sim$14K multiple-choice knowledge questions in diverse subjects and areas~\citep{hendrycks2020measuring}. Multilingual version was translated into 14 languages by professional translators~\citep{openai2024mmmlu}.
\item \textbf{\emph{OpenAssistant-2}.} The dataset contains $\sim$14K user-assistant conversations with multiple messages in 28 languages~\citep{fischer2024open}.
\item \textbf{\emph{Include-Base-44}.} Multiple-choice questions focused on general and regional knowledge, as well as reasoning, extracted from academic and professional exams. Spanning 44 languages, it includes a total of $\sim$23K samples~\citep{romanou2024include}.
\end{itemize}

\textbf{Representative Sample Collection.} 
\emph{MMLU} and \emph{Include-Base-44} are highly curated benchmark datasets, containing structured, knowledge-rich samples. The \emph{Aya Dataset} is human-curated, while \emph{OpenAssistant-2} is partially human-curated and partially generated by large language models (LLMs). In contrast, the \emph{Aya Collection} consists of various AI-generated samples without quality guarantee, though it represents the largest and most multilingual of the five.  

To address this quality difference, we create two \emph{Multilingual Knowledge Collection (MKC)} configurations:
\begin{itemize}[itemsep=3pt,topsep=0pt]
    \item \textbf{\emph{MKC}}: Includes \emph{Include-Base-44}, \emph{OpenAssistant-2}, \emph{MMLU}, and the \emph{Aya Dataset}
    \item \textbf{\emph{MKC$^+$}}: Includes \emph{MKC} and the \emph{Aya Collection}
\end{itemize}
This allows us to evaluate the trade-off between data quality and scale.

\textbf{Dataset Creation.}
For our model-based filtering approaches, our goal is to identify documents from the pretraining dataset that are most similar to our representative samples, with the notion of similarity determined by the specific classifier used. We can measure the similarity to our training dataset directly, for example, by computing the cosine similarity to our training samples in the embedding space. Alternatively, following the approach of \citet{li2024datacomp}, the task can be framed as a binary classification problem, with the representative samples as the positive class. For the negative class, we can simply subsample documents from our pretraining dataset, under the assumption that the majority of these documents are neither well-structured nor knowledge-rich. We use both approaches for our classifiers.

To create the binary classification training dataset, we selected 80K random examples from the training set (\emph{MKC} or \emph{MKC$^+$}) as positive samples and 80K random examples from FineWeb-2 as negative samples. For smaller datasets, such as \emph{Include-Base-44}, the entire dataset was used. The same training dataset was utilized across all model-based filtering approaches, disregarding negative samples when unnecessary. Additionally, we created a training dataset for each language individually to avoid leaking language-specific biases to data of other languages.

\textbf{Sample Pre-processing.}
We applied no pre-processing to the FineWeb-2 (negative) samples but performed minimal pre-processing on the representative (positive) samples. For instance, in datasets like \emph{MMLU} or \emph{OpenAssistant-2}, we concatenated various sample components. For the \emph{Aya Collection}, we resolved encoding issues in non-Latin languages and removed samples containing \textit{\textless unk\textgreater} tokens, which were particularly prevalent in Arabic data (37.1\%).

\subsection{FastText-based Filtering (FT)}
To efficiently process datasets with over 100 million documents~\citep{penedo2024fineweb-2}, similar to DCLM~\citep{li2024datacomp}, we used a binary FastText classifier~\citep{joulin2017bag}. This classifier runs on CPU and can be easily deployed across multiple cores, for example using DataTrove~\citep{penedo2024datatrove}.

We trained our FastText classifier on the processed training set using 2-gram features (4-gram for Chinese). Additional details about the training process are given in Appendix~\ref{app:fasttext_details}. These classifiers were then used to assign scores to all documents in the pretraining dataset. To filter the dataset, we applied a score threshold based on the desired retention percentage of documents. This approach balances dataset size and the predicted quality of the samples.

\subsection{Transformer Embedding-based Filtering}
To leverage rich semantic information based on contextual relationships, we utilized the Transformer model embeddings. Specifically, we selected a pretrained XLM-RoBERTa base model~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning} due to its support of 100 languages, a relatively small size of approximately 279M parameters, and its transparent training procedure. This choice enabled us to process web-scale data efficiently without being restricted to a single language and to align with our commitment to open science.

To retain general embeddings that can be reused across methods, we opted against fine-tuning the model. For each document from our datasets, we computed the 768-dimensional embedding by mean pooling the embeddings of the output sequence. Since the model has a fixed maximum sequence length of 512 tokens, we considered only the first 512 tokens of each document, assuming they are representative of the entire document.

After computing the embeddings of our corpora, we experimented with two methods: 1) classification of embeddings using a multi-layer perceptron and 2) cosine similarity between the embeddings. As in the FastText approach, we scored each document and applied a threshold to retain the desired percentage of the highest-scoring documents.

\textbf{Multi-Layer Perceptron (MLP).} 
We trained a single-hidden-layer neural network with a hidden dimension of 256, the ReLU activation function, a dropout rate of 20\%, and the sigmoid function on the output. The network was trained for 6 epochs using the AdamW optimizer~\citep{loshchilov2017decoupled} with a constant learning rate $0.0003$ and binary cross-entropy loss. We computed document scores using the output layer of the MLP model, which used XML-RoBERTa document embeddings as input.

\textbf{Cosine Similarity (CS).}
We computed the document scores as the maximum cosine similarity between its embeddings and a set of $K$ randomly sampled positive sample embeddings. We experimented with varying values of $K$, including 1024, 2048, 4096, 8192, and 16384. However, we did not observe a significant differences in the documents with high scores across these variations when manually inspecting the data. To strike a balance between the diversity of the positive samples and computational efficiency, we chose $K = 8192$ for our experiments.

\section{Experiments}\label{sec:experiments}
\subsection{Experimental Setup}
\textbf{Technical Details.} We evaluate 1B-parameter Llama models~\citep{llama3} to demonstrate the effectiveness of our model-based filtering approaches. The models are trained on either 70B or 119B tokens, balancing token quality and diversity. The smaller dataset (70B tokens) exposes the model to each token at most once (with a few exceptions where some tokens appear twice). The larger dataset (119B tokens) simulates longer training, resulting in increased token repetition. Training utilizes the HuggingFace Nanotron library~\citep{nanotron} with the AdamW optimizer~\citep{loshchilov2017decoupled} and a WSD learning rate schedule~\citep{hagele2024scaling}.

To minimize the need for costly hyperparameter tuning, we maintain a consistent setup across all experiments. Specifically, we adopt the DeepSeek scaling law~\citep{deepseekai2024deepseekllmscalingopensource} with a batch size of 1.6M tokens, learning rate of 0.0008, and 2000 warmup steps. We provide our Nanotron config in Appendix~\ref{app:nanotron_details}.

As base dataset, we use FineWeb-2~\citep{penedo2024fineweb-2}, which has been shown to provide a strong baseline across a variety of languages. Since FineWeb-2 is globally deduplicated, we rehydrate both filtered and unfiltered data using the hyperparameters recommended by~\citet{penedo2024fineweb-2}.

To validate our method on English, we use three datasets: FineWeb~\citep{penedo2024fineweb} as the baseline, along with FineWeb-Edu~\citep{penedo2024fineweb} and DCLM~\citep{li2024datacomp}, both of which represent the current state-of-the-art. Tokenization is performed using the multilingual Mistral v3 (Tekken) tokenizer~\citep{tekkenV3}. All experiments are conducted using 80 NVIDIA GH200 chips.

\textbf{Evaluation.}
Our evaluation prioritizes a diverse range of tasks to ensure the models retain well-rounded capabilities, rather than focusing exclusively on knowledge-based tasks. Specifically, we include tasks covering reading comprehension, general knowledge, natural language understanding, common-sense reasoning, and generative tasks in the target language. To evaluate our approach, we use the HuggingFace LightEval library~\citep{lighteval}.

For French, Chinese, and Arabic, we utilize the FineTasks~\citep{kydlicek2024finetasksmultilingualtasks} multilingual evaluation suite, which is designed to provide meaningful signals even for models trained in the order of 100B tokens. We select analogous tasks for German and Danish.
For English, we rely on the SmolLM tasks suite~\citep{smollm}.
A complete list of tasks and their evaluation metrics for each language is provided in Appendix \ref{app:benchmarks}.

\textbf{Model Selection.} We follow the approach used in FineTasks for filter selection, computing a global rank score across individual metrics and languages to determine the optimal approach. 

\subsection{Experimental Results \& Discussion}
\subsubsection{Model Selection}\label{sec:model_selection}
In Section~\ref{sec:method}, we introduced several model-based filtering approaches. \textit{But which of these performs the best?} We evaluate which combination of our defined classifier training datasets (\emph{MKC} or \emph{MKC$^+$}) and filtering methods (\emph{FT}, \emph{MLP} or \emph{CS}) achieve the highest performance. Table~\ref{tab:ranking_threshold_10} presents the overall ranking across our representative language selection (Chinese, German, French, Arabic, Danish) and training runs of 70B and 119B tokens. Analogous to the DCLM filtering recipe~\citep{li2024datacomp}, the results are based on a dataset that retains 10\% of the documents for the high-resource datasets (Chinese, German, French) and keeps 56\% and 65\% of the documents for the lower-resource languages (Arabic and Danish, respectively). These percentages maintain approximately 70B tokens, under the assumption of uniform token distribution across documents. We also exclude approaches that use \emph{MKC} for training on Danish, as it lacks sufficient training data. For detailed, per-language results, please refer to Appendix \ref{app:model_selection}.

Table~\ref{tab:ranking_threshold_10} demonstrates that \emph{MLP MKC$^+$} approach outperforms all other approaches. Interestingly, the high- and low-scored samples presented in Appendix~\ref{app:examples} align with the observed rankings. Figure~\ref{fig:mmlu_plot} further highlights the strong performance of \emph{MLP MKC$^+$}, particularly for high-resource languages, where it largely outperforms the baseline. For lower-resource languages—where less data was filtered—the performance gains are less pronounced. Notably, \emph{FT} filtering is also competitive. Given the computational expense of XLM-RoBERTa embeddings, FastText can be a promising alternative in resource-constrained setups.

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison (average rank) between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 70B and 119B tokens were consumed.
}
\label{tab:ranking_threshold_10}
\begin{center}
\begin{tabular}{lr}
\toprule
Approach &  Average Rank\\
\midrule
\rowcolor{Best} \emph{MLP MKC$^+$} &  4.35 \\
\emph{MLP MKC} &  6.11 \\
\emph{FT MKC$^+$} &  7.17 \\
\emph{FT MKC} &  8.04 \\
\emph{CS MKC} &  8.10 \\
\rowcolor{Baseline} Baseline &  8.72 \\
\emph{CS MKC$^+$} &  8.79 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure*}[!htb]
    \centering
    \subfigure[English (MMLU)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/eng_Latn.pdf}}
    \subfigure[Chinese (CMMLU)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/cmn_Hani.pdf}} 
    \subfigure[German (MMLU)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/deu_Latn.pdf}}
    \subfigure[French (MMLU)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/fra_Latn.pdf}}
    \subfigure[Arabic (MMLU)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/arb_Arab.pdf}} 
    \subfigure[Danish (ARC)]{\includegraphics[width=0.33\textwidth]{plots/mmlu/dan_Latn.pdf}}
    \caption{
        Benchmark performance comparison (accuracy) during training on 119B tokens between the baseline methods (FineWeb, DCLM, FineWeb-Edu, and FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}), trained on \emph{MKC$^+$}. When using our approaches, the data retention rates are set to 10\% for English, Chinese, German, and French, 56\% for Arabic, and 65\% for Danish. For English, Chinese, German, and French, baseline-level performance is observed around 20B tokens consumed (16.7\% of the total).
    }
    \label{fig:mmlu_plot}
\end{figure*}

\subsubsection{Threshold Selection}\label{sec:threshold_selection}
In Section~\ref{sec:model_selection}, we base our model selection on experiments that retain top 10\% of the data for high-resource languages. \textit{But is this the optimal threshold?} Following the methodology of \citet{li2024datacomp}, we analyze the impact of varying filter strengths on performance for Chinese, German, and French, using our \emph{MLP} and \emph{FT} filtering methods. The results are summarized in Table~\ref{tab:ranking_threshold_10_15_20}, with a comprehensive analysis, including results for \emph{CS}, provided in Appendix~\ref{app:threshold_selection} (Table~\ref{tab:ranking_threshold_15_20_all}). Consistent with their findings, we observe that retaining top 10\% of the data is a competitive threshold, particularly for approaches using the \emph{MKC$^+$} dataset. Interestingly, approaches using \emph{MKC} perform better with higher retention. Motivated by the observed bias in certain approaches favoring the selection of shorter documents, we examine how this bias interacts with performance when retaining more documents. As demonstrated in Figure~\ref{fig:doc_length_de} for German, Appendix~\ref{app:threshold_selection} for other languages, and the retained token counts in Table~\ref{tab:token_count_threshold_10_56_65}, the \emph{MLP MKC} approach shows a tendency to retain shorter documents, while achieving higher performance with an increased number of retained documents. In contrast, the \emph{CS} and \emph{FT} filtering methods present mixed results, suggesting that the optimal threshold selection may be influenced by additional factors.

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison (average rank) between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining top 10\%, 15\% or 20\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated for Chinese, German and French after 70B and 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_15_20}
\begin{center}
\begin{tabular}{lcr}
\toprule
Approach & Threshold & Average Rank \\
\midrule
\rowcolor{Best} \emph{MLP MKC$^+$} & 10\% & 8.85 \\
\emph{MLP MKC$^+$} & 15\% & 9.44 \\
\emph{MLP MKC} & 20\% & 11.37 \\
\emph{MLP MKC} & 15\% & 11.70 \\
\emph{MLP MKC} & 10\% & 11.95 \\
\emph{MLP MKC$^+$} & 20\% & 11.97 \\
\emph{FT MKC$^+$} & 10\% & 13.92 \\
\emph{FT MKC} & 15\% & 14.62 \\
\emph{FT MKC} & 10\% & 14.74 \\
\emph{FT MKC} & 20\% & 15.62 \\
\emph{FT MKC$^+$} & 15\% & 16.27 \\
\emph{FT MKC$^+$} & 20\% & 16.51 \\
\rowcolor{Baseline} Baseline & -- & 18.55 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}[!htb]
    \centering
\includegraphics[width=0.99\linewidth]{plots/document_length/doc_length_bias_german.pdf}
\caption{Comparison of average document length and standard deviation in FineWeb-2 before and after filtering using one of our approaches retaining top 10\% of the documents. The average document length of FineWeb-2 is represented as a red horizontal line, while the medians are shown as red dots. Document length is measured based on number of space-separated tokens.}
    \label{fig:doc_length_de}
\end{figure}

\subsubsection{Training Data Analysis}
The experiments in Sections~\ref{sec:model_selection} and~\ref{sec:threshold_selection} are based on the training datasets \emph{MKC} and \emph{MKC$^+$}. \textit{But is the diversity introduced by combining various base datasets truly necessary?} We evaluate the impact of each base dataset individually and compare it to the combined \emph{MKC$^+$} dataset.
For this ablation study, we use our best filtering method (\emph{MLP} with a top 10\% retention) and train the models on 30B tokens. This token count is chosen to match the size of the smallest filtered dataset, ensuring consistency across comparisons. The results, presented in Table~\ref{tab:ranking_training_data}, show that despite the absence of a quality guarantee for all samples in the \emph{Aya Collection}, this dataset yields strong performance, making our approach applicable for various languages. Overall, we observe that the diversity resulting from combining all individual training datasets gives the best results.

Interestingly, models trained exclusively on \emph{Include-Base-44} and \emph{OpenAssistant-2} perform worse overall than the baseline. This may be due to the nature of these datasets. For instance, \emph{Include-Base-44} is relatively small and domain-specific, e.g., consisting primarily of driving license exam questions in its German subset. Similarly, \emph{OpenAssistant-2} includes a limited number of samples, with fewer than 2K positive samples per training set, which likely negatively impacts classifier performance. Again, we relate model performance to the average document length bias in Appendix~\ref{app:training_data_analysis} and confirm the findings from Section~\ref{sec:threshold_selection}, suggesting that factors beyond the retained document length bias may influence performance.

\begin{table}[!htb]\small
    \caption{
    Benchmark performance comparison (average rank) between the baseline (FineWeb-2) and the \emph{MLP} filtering method trained on either \emph{MKC$^+$} as a whole or its individual dataset components, retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish. The average rank is computed across FineTasks performance of 1B-parameter models trained on each language with 30B tokens.
    }
    \label{tab:ranking_training_data}
    \begin{center}
    \begin{tabular}{lr}
    \toprule
    Dataset & Average Rank \\
    \midrule
    \rowcolor{Best}\emph{MKC$^+$} & 2.52 \\
    \emph{Aya Collection} & 2.91 \\
    \emph{Aya Dataset} & 3.17 \\
    \emph{MMLU} & 3.57 \\
    \rowcolor{Baseline} Baseline & 4.09 \\
    \emph{OpenAssistant-2} & 4.53 \\
    \emph{Include-Base-44} & 5.42 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Replay of Original Data}
\textit{But does our model-based filtering introduce unwanted biases?}
We explore whether incorporating a small percentage of original raw data (replay) can help improve performance. We do this for our best FastText (\emph{FT MKC$^+$}) and Transformer approaches (\emph{MLP MKC$^+$}). Table \ref{tab:ranking_mixture} presents the results of experiments where 5\% and 10\% unfiltered data were mixed into the training dataset, alongside results from training without any replay. Although, the \emph{FT MKC$^+$} filters shows mixed signal, our \emph{MLP MKC$^+$} approach clearly demonstrates that replay does not improve performance, indicating the data selection already retains enough diversity. In cases of less diverse datasets, replay was shown to offer benefits \cite{bethune2025scalinglawsforgettingfinetuning,chen2023meditron70bscalingmedicalpretraining}.

\begin{table}[!htb]\small
    \caption{
    Benchmark performance comparison (average rank) of our \emph{MLP MKC$^+$} and \emph{FT MKC$^+$} approaches, retaining top 10\% of the documents while mixing in 0\%, 5\% or 10\% of the original FineWeb-2 dataset. The average rank is computed across FineTasks performance of 1B-parameter models evaluated for Chinese, German, or French, after consuming 70B and 119B tokens.
    }
    \label{tab:ranking_mixture}
    \begin{center}
    \begin{tabular}{lrr}
    \toprule
    Approach & Mixture Rate & Average Rank \\
    \midrule
    \rowcolor{Best} \emph{MLP MKC$^+$} & 0\% & 4.36 \\
    \emph{MLP MKC$^+$} & 5\% & 5.09 \\
    \emph{MLP MKC$^+$} & 10\% & 5.40 \\
    \emph{FT MKC$^+$} & 10\% & 7.17 \\
    \emph{FT MKC$^+$} & 0\% & 7.51 \\
    \emph{FT MKC$^+$} & 5\% & 8.66 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsubsection{Approach Validation on English}\label{sec:eng_validation}
Previous experiments have shown strong performance of our \emph{MLP MKC$^+$} approach. \textit{But do these results translate to English?}
Table~\ref{tab:ranking_english} presents the performance of \emph{MLP MKC$^+$} with 10\% retention applied to the English FineWeb dataset~\cite{penedo2024fineweb}.
Our method is compared against FineWeb and baselines using model-based filtered datasets, including DCLM~\cite{li2024datacomp} and FineWeb-Edu~\citep{penedo2024fineweb}. To save computational resources, we use the 6 most recent FineWeb and FineWeb-Edu dumps and the first partition of DCLM\footnote{\href{https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0-parquet}{huggingface.co/datasets/mlfoundations/dclm-baseline-1.0-parquet}}, which we denote with $^*$. Each of these subsets contains more than 119B tokens, with FineWeb retaining this size even after applying our filtering retaining top 10\% of the documents.

While each approach demonstrates strengths in different benchmarks, as seen from Table~\ref{tab:ranking_english} and Figure~\ref{fig:mmlu_plot}, the overall average rank results indicate that our method outperforms all other baselines.

\begin{table}[!htb]\small
\caption{Benchmark performance comparison for English of our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents) against baseline datasets: FineWeb, DCLM, and FineWeb-Edu. The average rank is computed across SmolLM task performance for 1B-parameter models trained on 119B tokens.}
\label{tab:ranking_english}
\begin{center}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{l>{\columncolor{Best}}lll>{\columncolor{Baseline}}r}
\toprule
Dataset & Ours & DCLM$^*$ & FW-Edu$^*$ & FW$^*$ \\
\midrule
\rowcolor{AverageRank} Average Rank & 1.8333 & 2.3889 & 2.4444 & 3.3333 \\
ARC (Challenge) & 0.3550 & 0.3530 & \textbf{0.3850} & 0.3010 \\
ARC (Easy) & 0.6670 & 0.6470 & \textbf{0.6970} & 0.5880 \\
CommonsenseQA & 0.3870 & \textbf{0.4100} & 0.3770 & 0.3850 \\
HellaSwag & \textbf{0.6040} & 0.5960 & 0.5700 & 0.5930 \\
MMLU & 0.3400 & 0.3160 & \textbf{0.3470} & 0.3030 \\
OpenBookQA & 0.3860 & 0.3840 & \textbf{0.4180} & 0.3560 \\
PIQA & 0.7510 & 0.7510 & 0.7410 & \textbf{0.7620} \\
WinoGrande & \textbf{0.5720} & 0.5610 & 0.5660 & 0.5550 \\
TriviaQA & 0.0820 & \textbf{0.1240} & 0.0320 & 0.0370 \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\subsubsection{Data Contamination Analysis}~\label{sec:decontamination}
Our LLMs are never trained on benchmark datasets. \textit{But is the strong performance observed in the previous sections primarily due to an increased ratio of data contamination?}
To ensure the validity of our approach, we conduct decontamination experiments, as web crawl data may include evaluation benchmark tasks. While~\citet{li2024datacomp} addressed similar concerns, our approach follows the methodology of~\citet{brown2020language}. Specifically, we perform 13-gram decontamination of the LLM training data separately for English and French evaluation benchmarks. However, unlike the original approach, we remove the entire document if it is flagged as contaminated, using the implementation provided in DataTrove~\citep{penedo2024datatrove}.

Tables \ref{tab:ranking_decont_eng_Latn_119} and \ref{tab:ranking_decont_fra_Latn_119} present the results of decontamination experiments for English and French, respectively. We used the following experimental setup (removed document contamination rates): baseline FineWeb English (0.16\%), \emph{MLP MKC$^+$} English with 10\% retention (0.19\%), baseline FineWeb-2 French (0.14\%), and \emph{MLP MKC$^+$} French with 10\% retention (0.14\%). As in our previous experiments, we train the models on 119B tokens. Additionally, we compare the results against equivalent training runs without decontamination to further analyze its impact. For an example of a contaminated sample, see Appendix \ref{app:decontmination}.

For English models, decontamination slightly reduces performance both for our approach and baseline FineWeb data. However, even when decontaminated, our approach still outperforms training on non-decontaminated baseline data. For French models, performance of our approach is comparable between decontaminated and non-decontaminated datasets, with both continuing to outperform baseline FineWeb-2 data. Interestingly, decontaminated baseline data yields better results than its non-decontaminated counterpart.

\begin{table}[!htb]\small
    \caption{
    Benchmark performance comparison in English for our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents), both decontaminated ($D$) and non-decontaminated, against the baseline FineWeb datasets, also in decontaminated and non-decontaminated variants. The average rank is computed across SmolLM task performance for 1B-parameter models trained on 119B tokens.
    }
    \label{tab:ranking_decont_eng_Latn_119}
    \begin{center}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{l>{\columncolor{Best}}ll>{\columncolor{Baseline}}lr}
    \toprule
    Dataset & Ours & Ours$_D$ & FW$^*$ & FW$^*_D$ \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 1.5000 & 2.1111 & 3.0556 & 3.3333 \\
    ARC (Challenge) & \textbf{0.3550} & 0.3440 & 0.3010 & 0.2880 \\
    ARC (Easy) & \textbf{0.6670} & 0.6520 & 0.5880 & 0.5700 \\
    CommonsenseQA & 0.3870 & \textbf{0.4000} & 0.3850 & 0.3820 \\
    HellaSwag & \textbf{0.6040} & \textbf{0.6040} & 0.5930 & 0.5890 \\
    MMLU & \textbf{0.3400} & 0.3220 & 0.3030 & 0.3050 \\
    OpenBookQA & \textbf{0.3860} & 0.3840 & 0.3560 & 0.3740 \\
    PIQA & 0.7510 & 0.7590 & \textbf{0.7620} & 0.7600 \\
    WinoGrande & \textbf{0.5720} & 0.5550 & 0.5550 & 0.5570 \\
    TriviaQA  & \textbf{0.0820} & 0.0380 & 0.0370 & 0.0250 \\
    \bottomrule
    \end{tabular}}
    \end{center}
\end{table}


\begin{table}[!htb]\small
    \caption{Benchmark performance comparison in French for our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents), both decontaminated ($D$) and non-decontaminated, against the baseline FineWeb-2 datasets, also in decontaminated and non-decontaminated variants. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.}
    \label{tab:ranking_decont_fra_Latn_119}
    \begin{center}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{l>{\columncolor{Best}}lll>{\columncolor{Baseline}}r}
    \toprule
    Dataset & Ours & Ours$_D$ & FW-2$_D$ & FW-2 \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 2.0556 & 2.0556 & 2.7222 & 3.1667 \\
    Belebele & 0.3533 & 0.3400 & \textbf{0.3778} & 0.3444 \\
    HellaSwag & \textbf{0.5380} & 0.5350 & 0.5180 & 0.5180 \\
    X-CSQA & 0.2740 & 0.2810 & 0.2730 & \textbf{0.2870} \\
    XNLI 2.0 & \textbf{0.7400} & \textbf{0.7400} & 0.7070 & 0.7180 \\
    FQuAD  & 0.2803 & 0.2620 & \textbf{0.2890} & 0.2401 \\
    MMLU & \textbf{0.2895} & 0.2875 & 0.2711 & 0.2706 \\
    Mintaka  & 0.0438 & \textbf{0.0797} & 0.0658 & 0.0712 \\
    X-CODAH & 0.2667 & \textbf{0.2900} & 0.2800 & 0.2633 \\
    ARC (Challenge) & \textbf{0.3180} & 0.3110 & 0.2880 & 0.2850 \\
    \bottomrule
    \end{tabular}}
    \end{center}
\end{table}

\subsubsection{Impact on multilingual model training}\label{sec:multilingual_llm}
Although not the primary focus of our work, we believe that refined datasets can contribute to advancing the performance of multilingual models. To investigate this, we conducted an ablation study by training a 1B-parameter model on 595B tokens (5$\times$119B), covering all five languages: Chinese, German, French, Arabic and Danish. We trained two models—the first one using our filtered FineWeb-2 dataset and the second one using unfiltered FineWeb-2 data. We then compared these results for each language against their monolingual counterparts trained on 119B tokens.

The results for French are presented in Table~\ref{tab:ranking_multilingual_fra_Latn_595}. We observe that the multilingual LLM outperforms its monolingual counterpart on our filtered datasets, whereas the monolingual model achieves better performance than the multilingual model on the FineWeb-2 dataset. This trend is consistent across all languages except Chinese. Detailed results for the other languages are provided in Appendix~\ref{app:multilingual}.

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison for French of multilingual LLMs ($M$) trained on FineWeb-2 or the refined dataset using our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish) trained on 595B tokens, against their monolingual counterparts trained on 119B tokens. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.
}
\label{tab:ranking_multilingual_fra_Latn_595}
\begin{center}
\begin{tabular}{ll>{\columncolor{Best}}l>{\columncolor{Baseline}}lr}
\toprule
Dataset & Ours$_M$ & Ours & FW-2 & FW-2$_M$ \\
\midrule
\rowcolor{AverageRank} Average Rank & 1.8333 & 2.0556 & 3.0000 & 3.1111 \\
Belebele & \textbf{0.3667} & 0.3533 & 0.3444 & 0.3511 \\
HellaSwag & 0.5270 & \textbf{0.5380} & 0.5180 & 0.4970 \\
X-CSQA & 0.2740 & 0.2740 & \textbf{0.2870} & 0.2750 \\
XNLI 2.0 & \textbf{0.7660} & 0.7400 & 0.7180 & 0.7330 \\
FQuAD  & \textbf{0.3212} & 0.2803 & 0.2401 & 0.2459 \\
MMLU & 0.2841 & \textbf{0.2895} & 0.2706 & 0.2735 \\
Mintaka  & 0.0456 & 0.0438 & \textbf{0.0712} & 0.0579 \\
X-CODAH & \textbf{0.2900} & 0.2667 & 0.2633 & 0.2567 \\
ARC (Challenge) & 0.2970 & \textbf{0.3180} & 0.2850 & 0.2670 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}
In this work, we introduced a novel framework for model-based filtering of web-scale multilingual pretraining datasets, demonstrating consistent improvements on LLM benchmarks across a wide range of languages. Our Transformer embedding-based classifier, \emph{MLP MKC$^+$}, outperforms state-of-the-art methods on both English and multilingual datasets, even when decontaminating the datasets or using them for training multilingual LLMs. This demonstrates that simple classifiers can achieve competitive results. While our FastText-based filtering approach performed well and shows promise in resource-constrained setups, \emph{MLP MKC$^+$} consistently outperformed all other methods and can be easily scaled to other languages. These results motivate us to expand our framework to 20 languages and release the corresponding refined pretraining datasets and our code, contributing to the advancement of multilingual language modeling.

\section*{Acknowledgements}

We thank Guilherme Penedo, Hynek Kydlíček, and Leandro von Werra for their help with FineWeb-2 data, and to Alex Hägele for providing feedback on the paper draft. 

This work was supported as part of the Swiss AI Initiative by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a06 on Alps.

\newpage
\begin{small}
\bibliography{arxiv}
\bibliographystyle{icml2025}
\end{small}

\newpage
\appendix
\onecolumn
\section{Additional Experimental Details}\label{app:addition_exp_details}
\subsection{FastText Training Details}\label{app:fasttext_details}
The FastText classifier was trained on the processed training set using \texttt{2-grams}, a \texttt{minCount} of 1, and the \texttt{softmax} loss function. All other parameters were automatically tuned using the FastText library. For Chinese, fixed parameters were used: 30 training epochs and a learning rate of 0.1 to ensure training stability. Additionally, \texttt{4-grams} and a \texttt{minCount} of 0 were selected based on manual evaluation of the results.

Prior to training the FastText models, we pre-processed the training data by removing \texttt{newlines}.

\subsection{Nanotron Configuration}\label{app:nanotron_details}
To facilitate the reproducibility of our model training, we provide the Nanotron~\citep{nanotron} configuration used in our experiments.
\begin{lstlisting}
checkpoints:
  checkpoint_interval: 1000
  checkpoints_path: checkpoints/
  checkpoints_path_is_shared_file_system: false
  resume_checkpoint_path: null
  save_initial_state: false
data_stages:
  - data:
      dataset:
        dataset_folder: template
      num_loading_workers: 1
      seed: 42
    name: General purpose training (Single dataset)
    start_training_step: 1
general:
  benchmark_csv_path: null
  consumed_train_samples: null
  ignore_sanity_checks: true
  project: template
  run: template
  seed: 42
  step: null
lighteval: null
logging:
  iteration_step_info_interval: 1
  log_level: info
  log_level_replica: info
model:
  ddp_bucket_cap_mb: 25
  dtype: bfloat16
  init_method:
    std: 0.025
  make_vocab_size_divisible_by: 1
  model_config:
    bos_token_id: 1
    eos_token_id: 2
    hidden_act: silu
    hidden_size: 1536
    initializer_range: 0.02
    intermediate_size: 6144
    is_llama_config: true
    max_position_embeddings: 1024
    num_hidden_layers: 24
    num_attention_heads: 16
    num_key_value_heads: 16
    pad_token_id: null
    pretraining_tp: 1
    rms_norm_eps: 1.0e-06
    rope_scaling: null
    tie_word_embeddings: true
    use_cache: true
    vocab_size: 131072
optimizer:
  optimizer_factory:
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-08
    name: adamW
    torch_adam_is_fused: true
  learning_rate_scheduler:
    learning_rate: 0.0008
    lr_decay_starting_step: 61001 # for 119B tokens (36001 for 70B tokens, 15001 for 30B tokens)
    lr_decay_steps: 12000 # for 119B tokens (7000 for 70B tokens, 4000 for 30B tokens)
    lr_decay_style: 1-sqrt
    lr_warmup_steps: 2000
    lr_warmup_style: linear
    min_decay_lr: 0.00
  zero_stage: 0
  clip_grad: 1.0
  weight_decay: 0.1
  accumulate_grad_in_fp32: true
parallelism:
  dp: 80
  expert_parallel_size: 1
  pp: 1
  pp_engine: 1f1b
  tp: 1
  tp_linear_async_communication: true
  tp_mode: REDUCE_SCATTER
profiler: null
tokenizer:
  tokenizer_max_length: null
  tokenizer_name_or_path: mistralai/Mistral-Nemo-Base-2407
  tokenizer_revision: null
tokens:
  batch_accumulation_per_replica: 1
  limit_test_batches: 0
  limit_val_batches: 0
  micro_batch_size: 20
  sequence_length: 1024
  train_steps: 73000  # for 119B tokens (43000 for 70B tokens, 19000 for 30B tokens)
  val_check_interval: -1
\end{lstlisting}


\section{Additional Results}\label{app:additional_results}
\subsection{Model Selection - Per Language Results}\label{app:model_selection}
For completeness, we present the individual benchmark results of the 1B-parameter model trained on 119B tokens for each language in the following tables: Table~\ref{tab:ranking_threshold_10_cmn_Hani_119} for Chinese, Table~\ref{tab:ranking_threshold_10_fra_Latn_119} for French, Table~\ref{tab:ranking_threshold_10_deu_Latn_119} for German, Table~\ref{tab:ranking_threshold_10_arb_Arab_119} for Arabic, and Table~\ref{tab:ranking_threshold_10_dan_Latn_119} for Danish.
    
\begin{table}[!htb]\small
\caption{
Benchmark performance comparison in Chinese between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining 10\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_cmn_Hani_119}
\begin{center}
\begin{tabular}{l>{\columncolor{Best}}lllll>{\columncolor{Baseline}}ll}
\toprule
Approach & \emph{MLP MKC$^+$} & \emph{MLP MKC} & \emph{CS MKC} & \emph{FT MKC} & \emph{FT MKC$^+$} & Baseline & \emph{CS MKC$^+$} \\
\midrule
\rowcolor{AverageRank} Average Rank & 1.7333 & 2.4333 & 4.0667 & 4.0667 & 4.4667 & 5.2333 & 6.0000 \\
AGIEval & \textbf{0.2995} & 0.2948 & 0.2897 & 0.2919 & 0.2817 & 0.2853 & 0.2773 \\
Belebele & \textbf{0.3300} & 0.3233 & 0.3178 & 0.3133 & 0.3133 & 0.3056 & 0.3022 \\
C$^3$ & \textbf{0.4550} & 0.4480 & 0.4400 & 0.4500 & 0.4400 & 0.4400 & 0.4370 \\
C-Eval & \textbf{0.3095} & 0.3060 & 0.2760 & 0.2903 & 0.2906 & 0.2878 & 0.2805 \\
CMMLU & \textbf{0.3312} & 0.3259 & 0.3041 & 0.3043 & 0.3060 & 0.3009 & 0.2995 \\
CMRC 2018 & 0.2224 & 0.2125 & 0.1614 & \textbf{0.2251} & 0.2164 & 0.1949 & 0.1866 \\
HellaSwag & 0.3790 & \textbf{0.3800} & 0.3530 & 0.3680 & 0.3660 & 0.3510 & 0.3370 \\
M3Exam & \textbf{0.3319} & 0.3245 & 0.3084 & 0.3201 & 0.3245 & 0.3216 & 0.3245 \\
X-CODAH & 0.3033 & 0.3000 & \textbf{0.3233} & 0.3100 & 0.2900 & 0.2967 & 0.3067 \\
X-CSQA & \textbf{0.2740} & 0.2680 & 0.2690 & 0.2610 & 0.2520 & 0.2510 & 0.2650 \\
XCOPA & 0.6200 & \textbf{0.6400} & 0.6180 & 0.5740 & 0.5740 & 0.6000 & 0.5620 \\
OCNLI & 0.5470 & 0.5470 & 0.5340 & 0.5250 & \textbf{0.5600} & 0.5420 & 0.5060 \\
Chinese-SQuAD & 0.0929 & \textbf{0.1097} & 0.0865 & 0.0889 & 0.0850 & 0.0777 & 0.0585 \\
XStoryCloze & \textbf{0.5800} & 0.5630 & 0.5710 & 0.5560 & 0.5610 & 0.5580 & 0.5570 \\
XWINO & 0.6429 & 0.6528 & \textbf{0.6587} & 0.6131 & 0.5992 & 0.6429 & 0.6111 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison in French between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining 10\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_fra_Latn_119}
\begin{center}
\begin{tabular}{ll>{\columncolor{Best}}lllll>{\columncolor{Baseline}}l}
\toprule
Approach & \emph{FT MKC$^+$} & \emph{MLP MKC$^+$} & \emph{MLP MKC} & \emph{FT MKC} & \emph{CS MKC} & \emph{CS MKC$^+$} & Baseline \\
\midrule
\rowcolor{AverageRank} Average Rank & 3.2222 & 3.5000 & 3.5556 & 3.7778 & 4.0000 & 4.6667 & 5.2778 \\
Belebele & 0.3378 & 0.3533 & \textbf{0.3678} & 0.3489 & 0.3444 & 0.3344 & 0.3444 \\
HellaSwag & \textbf{0.5380} & \textbf{0.5380} & 0.4990 & 0.5150 & 0.5280 & 0.5070 & 0.5180 \\
X-CSQA & 0.2820 & 0.2740 & 0.2730 & \textbf{0.2990} & 0.2850 & 0.2900 & 0.2870 \\
XNLI 2.0 & 0.7340 & 0.7400 & 0.7430 & 0.7230 & \textbf{0.7450} & 0.7330 & 0.7180 \\
FQuAD & 0.2597 & 0.2803 & \textbf{0.3032} & 0.2981 & 0.2411 & 0.2476 & 0.2401 \\
MMLU & 0.2896 & 0.2895 & \textbf{0.2925} & 0.2886 & 0.2806 & 0.2815 & 0.2706 \\
Mintaka & 0.0710 & 0.0438 & 0.0334 & 0.0670 & 0.0610 & \textbf{0.0976} & 0.0712 \\
X-CODAH & \textbf{0.3000} & 0.2667 & 0.2867 & 0.2767 & \textbf{0.3000} & 0.2800 & 0.2633 \\
ARC (Challenge) & 0.3120 & \textbf{0.3180} & 0.3090 & 0.3060 & 0.2950 & 0.2830 & 0.2850 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[!htb]\small

\caption{
Benchmark performance comparison in German between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining 10\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_deu_Latn_119}
\begin{center}
\begin{tabular}{l>{\columncolor{Best}}llllll>{\columncolor{Baseline}}l}
\toprule
Approach & \emph{MLP MKC$^+$} & \emph{FT MKC$^+$} & \emph{FT MKC} & \emph{CS MKC} & \emph{MLP MKC} & \emph{CS MKC$^+$} & Baseline \\
\midrule
\rowcolor{AverageRank} Average Rank & 3.1250 & 3.1250 & 3.5000 & 3.7500 & 4.5000 & 4.7500 & 5.2500 \\
MMLU & \textbf{0.2940} & 0.2879 & 0.2926 & 0.2770 & 0.2905 & 0.2764 & 0.2718 \\
ARC (Challenge) & 0.2760 & 0.2850 & 0.2820 & \textbf{0.2880} & 0.2830 & 0.2640 & 0.2680 \\
Mintaka & 0.0580 & 0.0548 & 0.0735 & 0.0576 & 0.0494 & \textbf{0.0766} & 0.0498 \\
Belebele & \textbf{0.3611} & 0.3578 & 0.3544 & 0.3544 & 0.3567 & 0.3422 & 0.3544 \\
X-CODAH & 0.3367 & 0.3500 & 0.3300 & 0.3567 & 0.3400 & \textbf{0.3600} & 0.3467 \\
X-CSQA & 0.2978 & \textbf{0.3008} & 0.2877 & 0.2887 & 0.2857 & 0.2918 & 0.2787 \\
HellaSwag & 0.4640 & 0.4710 & \textbf{0.4870} & 0.4820 & 0.4540 & 0.4390 & 0.4470 \\
XNLI 2.0 & 0.6620 & 0.6530 & 0.6740 & 0.6440 & 0.6610 & 0.6520 & \textbf{0.6890} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison in Arabic between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining 56\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_arb_Arab_119}
\begin{center}
\begin{tabular}{l>{\columncolor{Best}}lll>{\columncolor{Baseline}}llll}
\toprule
Approach & \emph{MLP MKC$^+$} & \emph{MLP MKC} & \emph{FT MKC$^+$} & Baseline & \emph{CS MKC$^+$} & \emph{CS MKC} & \emph{FT MKC} \\
\midrule
\rowcolor{AverageRank} Average Rank & 2.7812 & 3.2500 & 3.6875 & 3.9688 & 3.9688 & 5.0312 & 5.3125 \\
EXAMS & 0.3537 & \textbf{0.3656} & 0.3552 & 0.3582 & 0.3443 & 0.3262 & 0.3346 \\
MMLU & 0.4007 & 0.3909 & \textbf{0.4023} & 0.3894 & 0.3912 & 0.3781 & 0.3885 \\
ARC (Easy) & \textbf{0.4330} & 0.4230 & 0.4210 & 0.4120 & 0.4020 & 0.3940 & 0.4080 \\
AlGhafa SciQ & 0.6915 & \textbf{0.7005} & 0.6965 & 0.6854 & 0.6724 & 0.6683 & 0.6804 \\
Belebele & 0.3456 & 0.3356 & 0.3322 & 0.3311 & 0.3356 & \textbf{0.3567} & 0.3233 \\
SOQAL & \textbf{0.7333} & 0.6867 & 0.7000 & 0.7200 & 0.7267 & 0.6867 & 0.7133 \\
MLQA & 0.2386 & \textbf{0.2402} & 0.1928 & 0.1901 & 0.2189 & 0.2154 & 0.1793 \\
TyDi QA & \textbf{0.1547} & 0.1476 & 0.1230 & 0.1441 & 0.1223 & 0.1097 & 0.1182 \\
AlGhafa RACE & 0.3720 & \textbf{0.3740} & 0.3640 & 0.3710 & 0.3590 & 0.3660 & 0.3730 \\
ARCD & \textbf{0.3638} & 0.3505 & 0.3235 & 0.3354 & 0.3358 & 0.3432 & 0.3043 \\
X-CODAH & 0.2600 & 0.2533 & 0.2567 & \textbf{0.2633} & \textbf{0.2633} & 0.2500 & 0.2600 \\
AlGhafa PIQA & 0.6360 & 0.6320 & \textbf{0.6400} & 0.6240 & 0.6320 & 0.6320 & 0.6370 \\
X-CSQA & 0.2740 & 0.2810 & 0.2770 & \textbf{0.2900} & 0.2880 & 0.2720 & 0.2770 \\
XNLI 2.0 & 0.6570 & 0.6910 & 0.6990 & \textbf{0.7010} & 0.6910 & 0.6900 & 0.6770 \\
HellaSwag & 0.4270 & 0.4220 & 0.4280 & 0.4250 & 0.4260 & \textbf{0.4320} & 0.4150 \\
XStoryCloze & 0.6150 & 0.6100 & 0.6100 & 0.6070 & 0.6130 & \textbf{0.6180} & 0.5930 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[!htb]\small
\caption{
Benchmark performance comparison in Danish between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, and \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining 65\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated after 119B tokens were consumed.
}
\label{tab:ranking_threshold_10_dan_Latn_119}
\begin{center}
\begin{tabular}{ll>{\columncolor{Best}}ll>{\columncolor{Baseline}}l}
\toprule
Approach & \emph{CS MKC$^+$} & \emph{MLP MKC$^+$} & \emph{FT MKC$^+$} & Baseline \\
\midrule
\rowcolor{AverageRank} Average Rank & 1.0000 & 2.5000 & 3.1667 & 3.3333 \\
ARC (Challenge) & \textbf{0.2820} & 0.2650 & 0.2730 & 0.2560 \\
HellaSwag & \textbf{0.4950} & 0.4850 & 0.4750 & 0.4750 \\
Belebele & \textbf{0.3333} & 0.3289 & 0.3189 & 0.3289 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Threshold Selection}\label{app:threshold_selection}
To confirm that the \emph{CS} filtering method is not competitive with \emph{MLP} and \emph{FT}, even when a higher percentage of documents is retained, we present the complete threshold selection results, including the \emph{CS} method, in Table~\ref{tab:ranking_threshold_15_20_all} in addition to the results shown in Section~\ref{sec:threshold_selection} (Table~\ref{tab:ranking_threshold_10_15_20}).

We provide further results on the variation in the average length of documents retained by our model-based filtering approaches for Chinese, French, Arabic, and Danish. These results complement the findings for German discussed in Section~\ref{sec:threshold_selection} and are shown in Figure~\ref{fig:doc_length_other}. Table~\ref{tab:token_count_threshold_10_56_65} lists the actual dataset sizes (number of retained tokens) after tokenization for all languages.

\begin{table}[!htb]\small
\caption{Benchmark performance comparison (average rank) between the baseline (FineWeb-2) and our proposed filtering methods (\emph{FT}, \emph{MLP}, \emph{CS}) trained on \emph{MKC$^+$} or \emph{MKC}, retaining top 10\%, 15\% or 20\% of the documents. The average rank is computed across FineTasks performance of 1B-parameter models evaluated for Chinese, German and French after 70B and 119B tokens were consumed.}
\label{tab:ranking_threshold_15_20_all}
\begin{center}
\begin{tabular}{lcr}
\toprule
Approach & Threshold & Average Rank \\
\midrule
\rowcolor{Best} \emph{MLP MKC$^+$} & 10\% & 11.73 \\
\emph{MLP MKC$^+$} & 15\% & 12.13 \\
\emph{MLP MKC} & 20\% & 15.07 \\
\emph{MLP MKC} & 15\% & 15.09 \\
\emph{MLP MKC$^+$} & 20\% & 15.40 \\
\emph{MLP MKC} & 10\% & 16.09 \\
\emph{FT MKC$^+$} & 10\% & 18.61 \\
\emph{CS MKC} & 15\% & 19.02 \\
\emph{CS MKC} & 20\% & 19.24 \\
\emph{FT MKC} & 15\% & 19.84 \\
\emph{FT MKC} & 10\% & 20.02 \\
\emph{CS MKC} & 10\% & 20.67 \\
\emph{FT MKC} & 20\% & 20.80 \\
\emph{FT MKC$^+$} & 15\% & 22.05 \\
\emph{FT MKC$^+$} & 20\% & 22.52 \\
\emph{CS MKC$^+$} & 15\% & 24.66 \\
\emph{CS MKC$^+$} & 20\% & 25.08 \\
\rowcolor{Baseline} Baseline & -- & 25.54 \\
\emph{CS MKC$^+$} & 10\% & 26.94 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}[!htb]
    \centering    
    \includegraphics[width=0.97\textwidth]{plots/document_length/doc_length_bias_other_languages.pdf}
    \caption{Comparison of average document length and standard deviation in FineWeb-2 before and after filtering using one of our approaches retaining top 10\% of the documents for Chinese and French, 56\% for Arabic and 65\% for Danish. The average document length of FineWeb-2 is represented as a red horizontal line, while the medians are shown as red dots. Document length is measured based on number of space-separated tokens.}
    \label{fig:doc_length_other}
\end{figure}

\begin{table}[!htb]\small
\caption{
Comparison of retained tokens in FineWeb-2 before and after filtering using one of our proposed approaches retaining top 10\% of the documents for Chinese, French and German, 56\% for Arabic and 65\% for Danish. The token counts correspond to the size of the tokenized datasets, processed with the multilingual Mistral v3 (Tekken) tokenizer~\citep{tekkenV3}.
}
\label{tab:token_count_threshold_10_56_65}
\begin{center}
\begin{tabular}{lllllr}
\toprule
Approach & Chinese & French & German & Arabic & Danish \\
\midrule
\rowcolor{Best} \emph{MLP MKC$^+$} & 150B (9\%) & 89B (12\%) & 119B (12\%) & 78B (61\%) & 71B (66\%) \\
\emph{MLP MKC} & 105B (7\%) & 72B (10\%) & 87B (9\%) & 75B (59\%) & -- \\
\midrule
\emph{FT MKC$^+$} & 221B (14\%) & 70B (10\%) & 63B (6\% )& 77B (61\%) & 70B (65\%) \\
\emph{FT MKC} & 190B (12\%) & 43B (6\%) & 65B (7\%) & 80B (63\%) & -- \\
\midrule
\emph{CS MKC$^+$} & 170B (11\%) & 126B (17\%) & 166B (17\%) & 82B (65\%) & 77B (71\%) \\
\emph{CS MKC} & 161B (10\%) & 132B (18\%) & 172B (18\%) & 83B (65\%) & -- \\
\midrule
\rowcolor{Baseline} Baseline & 1597B & 730B & 973B & 127B & 108B \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Training Data Analysis}\label{app:training_data_analysis}
We give details on the variation in the average length of documents retained by our model-based filtering method \emph{MLP} for Chinese, French, Arabic, and Danish with different training datasets. The results are shown for German in Figure ~\ref{fig:doc_length_other_per_language_german} and for all other languages in Figure~\ref{fig:doc_length_other_per_language_others}.

\begin{figure}[!htb]
    \centering    
    \includegraphics[width=0.55\textwidth]{plots/document_length/doc_length_bias_per_lang_german.pdf}
    \caption{Comparison of average document length and standard deviation in FineWeb-2 before and after filtering using \emph{MLP} filtering method retaining top 10\% of the documents with different training datasets. The average document length of FineWeb-2 is represented as a red horizontal line, while the medians are shown as red dots. Document length is measured based on number of space-separated tokens.}
    \label{fig:doc_length_other_per_language_german}
\end{figure}

\begin{figure}[!htb]
    \centering    
    \includegraphics[width=0.97\textwidth]{plots/document_length/doc_length_bias_per_lang_others.pdf}
    \caption{Comparison of average document length and standard deviation in FineWeb-2 before and after filtering using \emph{MLP} filtering method retaining top 10\% of the documents for Chinese and French, 56\% for Arabic and 65\% for Danish with different training datasets. The average document length of FineWeb-2 is represented as a red horizontal line, while the medians are shown as red dots. Document length is measured based on number of space-separated tokens.}
    \label{fig:doc_length_other_per_language_others}
\end{figure}

\subsection{Impact on multilingual model training}\label{app:multilingual}
This section presents the results of our \emph{MLP MKC$^+$} approach on multilingual model training for Chinese (Table~\ref{tab:ranking_multilingual_cmn_Hani_595}), Arabic (Table~\ref{tab:ranking_multilingual_arb_Arab_595}), German (Table~\ref{tab:ranking_multilingual_deu_Latn_595}), and Danish (Table~\ref{tab:ranking_multilingual_dan_Latn_595}), in addition to the results for French discussed in Section~\ref{sec:multilingual_llm}.

\begin{table}[!htb]\small
    \caption{Benchmark performance comparison for Chinese of multilingual LLMs ($M$) trained on FineWeb-2 or the refined dataset using our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish) trained on 595B tokens, against their monolingual counterparts trained on 119B tokens. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.}
    \label{tab:ranking_multilingual_cmn_Hani_595}
    \begin{center}
    \begin{tabular}{l>{\columncolor{Best}}lll>{\columncolor{Baseline}}r}
    \toprule
    Dataset & Ours & Ours$_M$ & FW-2$_M$ & FW-2 \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 1.5667 & 2.1667 & 2.9000 & 3.3667 \\
    AGIEval & \textbf{0.2995} & 0.2863 & 0.2894 & 0.2853 \\
    Belebele & 0.3300 & \textbf{0.3456} & 0.3189 & 0.3056 \\
    C$^3$ & \textbf{0.4550} & 0.4520 & 0.4480 & 0.4400 \\
    C-Eval & \textbf{0.3095} & 0.2848 & 0.2683 & 0.2878 \\
    CMMLU & \textbf{0.3312} & 0.3064 & 0.2967 & 0.3009 \\
    CMRC 2018  & 0.2224 & \textbf{0.2689} & 0.2090 & 0.1949 \\
    HellaSwag & \textbf{0.3790} & 0.3740 & 0.3740 & 0.3510 \\
    M3Exam & \textbf{0.3319} & 0.3040 & 0.3304 & 0.3216 \\
    X-CODAH & 0.3033 & \textbf{0.3067} & 0.2800 & 0.2967 \\
    X-CSQA & 0.2740 & \textbf{0.2810} & 0.2780 & 0.2510 \\
    XCOPA & \textbf{0.6200} & 0.6020 & 0.5860 & 0.6000 \\
    OCNLI & \textbf{0.5470} & 0.5320 & 0.4910 & 0.5420 \\
    Chinese-SQuAD  & 0.0929 & \textbf{0.1304} & 0.1017 & 0.0777 \\
    XStoryCloze & \textbf{0.5800} & 0.5760 & 0.5650 & 0.5580 \\
    XWINO & 0.6429 & 0.6409 & \textbf{0.6468} & 0.6429 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!htb]\small
    \caption{Benchmark performance comparison for Arabic of multilingual LLMs ($M$) trained on FineWeb-2 or the refined dataset using our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish) trained on 595B tokens, against their monolingual counterparts trained on 119B tokens. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.}
    \label{tab:ranking_multilingual_arb_Arab_595}
    \begin{center}
    \begin{tabular}{ll>{\columncolor{Best}}l>{\columncolor{Baseline}}lr}
    \toprule
    Dataset & Ours$_M$ & Ours & FW-2 & FW-2$_M$ \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 1.9688 & 2.0000 & 2.7500 & 3.2812 \\
    EXAMS & 0.3336 & 0.3537 & \textbf{0.3582} & 0.3076 \\
    MMLU & 0.3828 & \textbf{0.4007} & 0.3894 & 0.3599 \\
    ARC (Easy) & 0.4190 & \textbf{0.4330} & 0.4120 & 0.3760 \\
    AlGhafa SciQ & 0.6764 & \textbf{0.6915} & 0.6854 & 0.6563 \\
    Belebele & \textbf{0.3511} & 0.3456 & 0.3311 & 0.3344 \\
    SOQAL & 0.7000 & \textbf{0.7333} & 0.7200 & 0.6533 \\
    MLQA  & 0.2208 & \textbf{0.2386} & 0.1901 & 0.2085 \\
    TyDi QA  & \textbf{0.1634} & 0.1547 & 0.1441 & 0.1429 \\
    AlGhafa RACE & \textbf{0.3830} & 0.3720 & 0.3710 & 0.3770 \\
    ARCD & 0.3377 & \textbf{0.3638} & 0.3354 & 0.2970 \\
    X-CODAH & \textbf{0.2767} & 0.2600 & 0.2633 & \textbf{0.2767} \\
    AlGhafa PIQA & 0.6170 & \textbf{0.6360} & 0.6240 & 0.6160 \\
    X-CSQA & 0.2860 & 0.2740 & \textbf{0.2900} & 0.2660 \\
    XNLI 2.0 & 0.7080 & 0.6570 & 0.7010 & \textbf{0.7340} \\
    HellaSwag & \textbf{0.4390} & 0.4270 & 0.4250 & 0.4240 \\
    XStoryCloze & \textbf{0.6370} & 0.6150 & 0.6070 & 0.6160 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!htb]\small
    \caption{Benchmark performance comparison for German of multilingual LLMs ($M$) trained on FineWeb-2 or the refined dataset using our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish) trained on 595B tokens, against their monolingual counterparts trained on 119B tokens. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.}
    \label{tab:ranking_multilingual_deu_Latn_595}
    \begin{center}
    \begin{tabular}{ll>{\columncolor{Best}}l>{\columncolor{Baseline}}lr}
    \toprule
    Dataset & Ours$_M$ & Ours & FW-2 & FW-2$_M$ \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 1.5000 & 2.1250 & 2.9375 & 3.4375 \\
    MMLU & 0.2918 & \textbf{0.2940} & 0.2718 & 0.2691 \\
    ARC (Challenge) & 0.2740 & \textbf{0.2760} & 0.2680 & 0.2640 \\
    Mintaka  & \textbf{0.0821} & 0.0580 & 0.0498 & 0.0660 \\
    Belebele & \textbf{0.3956} & 0.3611 & 0.3544 & 0.3633 \\
    X-CODAH & \textbf{0.3500} & 0.3367 & 0.3467 & 0.3167 \\
    X-CSQA & \textbf{0.3048} & 0.2978 & 0.2787 & 0.2787 \\
    HellaSwag & \textbf{0.4690} & 0.4640 & 0.4470 & 0.4430 \\
    XNLI 2.0 & 0.6420 & 0.6620 & \textbf{0.6890} & 0.6340 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[!htb]\small
    \caption{Benchmark performance comparison for Danish of multilingual LLMs ($M$) trained on FineWeb-2 or the refined dataset using our \emph{MLP MKC$^+$} approach (retaining top 10\% of the documents for Chinese, German, and French, 56\% for Arabic, and 65\% for Danish) trained on 595B tokens, against their monolingual counterparts trained on 119B tokens. The average rank is computed across FineTasks performance for 1B-parameter models trained on 119B tokens.}
    \label{tab:ranking_multilingual_dan_Latn_595}
    \begin{center}
    \begin{tabular}{ll>{\columncolor{Best}}ll>{\columncolor{Baseline}}r}
    \toprule
    Dataset & Ours$_M$ & Ours & FW-2$_M$ & FW-2 \\
    \midrule
    \rowcolor{AverageRank} Average Rank & 1.6667 & 2.1667 & 3.0000 & 3.1667 \\
    ARC (Challenge) & \textbf{0.2920} & 0.2650 & 0.2600 & 0.2560 \\
    HellaSwag & 0.4710 & \textbf{0.4850} & 0.4560 & 0.4750 \\
    Belebele & \textbf{0.3700} & 0.3289 & 0.3311 & 0.3289 \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\clearpage
\section{List of evaluation benchmarks and metrics}\label{app:benchmarks}
We provide a detailed overview of the evaluation benchmarks used to assess our models' performance, along with their respective evaluation metrics in Table~\ref{tab:benchmark_overview}. For non-English tasks and English MMLU, we use the \emph{cloze} multiple-choice prompt, which allows the model to directly predict each option instead of using the standard prompt format with A/B/C/D letter prefixes as targets. This approach was chosen because it has been shown to serve as a more reliable performance indicator earlier in training~\citep{kydlicek2024finetasksmultilingualtasks}. We evaluate the models in a 0-shot setting.

\begin{table}[!htb]\small
    \caption{List of evaluation benchmarks and metrics used in our setup for Chinese, French, German, Arabic, Danish, and English.}\label{tab:benchmark_overview}
    \begin{center}
\resizebox{0.99\textwidth}{!}{
    \begin{tabular}{|l|c|c|c|c|c|c|c|}
    \hline
        Benchmark & Chinese & French & German & Arabic & Danish & English & Evaluation metric \\ \hline
        AGIEval~\citep{zhong2023agievalhumancentricbenchmarkevaluating} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        AlGhafa ARC~\citep{almazrouei-etal-2023-alghafa} &  & & & \checkmark & &  & Normalized accuracy \\ \hline
        AlGhafa PIQA~\citep{almazrouei-etal-2023-alghafa} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        AlGhafa RACE~\citep{almazrouei-etal-2023-alghafa} &  &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        AlGhafa SciQ~\citep{almazrouei-etal-2023-alghafa} &  &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        ARC~\citep{clark2018thinksolvedquestionanswering} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        ARCD~\citep{mozannar2019neuralarabicquestionanswering} &  &  &  & \checkmark &  &  & F1 score \\ \hline
        Belebele~\citep{Bandarkar_2024} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  & Normalized accuracy \\ \hline
        C$^3$~\citep{sun-etal-2020-investigating} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        C-Eval~\citep{huang2023cevalmultilevelmultidisciplinechinese} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        Chinese-SQuAD~\citep{chinesesquad} & \checkmark &  &  &  &  &  & F1 score \\ \hline
        CMMLU~\citep{li2024cmmlumeasuringmassivemultitask} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        CMRC 2018~\citep{Cui_2019} & \checkmark &  &  &  &  &  & F1 score \\ \hline
        CommonsenseQA~\citep{talmor-etal-2019-commonsenseqa} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        EXAMS~\citep{hardalov2020examsmultisubjecthighschool} &  &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        FQuAD~\citep{dhoffschmidt2020fquadfrenchquestionanswering} &  & \checkmark &  &  &  &  & F1 score \\ \hline
        HellaSwag~\citep{zellers2019hellaswagmachinereallyfinish} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        M3Exam~\citep{zhang2023m3exammultilingualmultimodalmultilevel} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        Mintaka~\citep{sen2022mintakacomplexnaturalmultilingual} &  & \checkmark & \checkmark &  &  &  & F1 score \\ \hline
        MLMM ARC~\citep{lai-etal-2023-okapi} &  & \checkmark & \checkmark & & \checkmark &  & Normalized accuracy \\ \hline
        MLMM HellaSwag~\citep{lai-etal-2023-okapi} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark &  & Normalized accuracy \\ \hline
        MLMM MMLU~\citep{lai-etal-2023-okapi} &  & \checkmark & \checkmark & \checkmark &  &  & Normalized accuracy \\ \hline
        MLQA~\citep{lewis2020mlqaevaluatingcrosslingualextractive} &  &  &  & \checkmark &  &  & F1 score \\ \hline
        MMLU~\citep{hendrycks2020measuring} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        OCNLI~\citep{hu-etal-2020-ocnli} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        OpenBookQA~\citep{mihaylov2018suitarmorconductelectricity} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        PIQA~\citep{bisk2019piqareasoningphysicalcommonsense} &  &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        SOQAL~\citep{mozannar2019neuralarabicquestionanswering} &  &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        TriviaQA~\citep{joshi-etal-2017-triviaqa} &  &  &  &  &  & \checkmark & Quasi-exact match \\ \hline
        TyDi QA~\citep{clark2020tydiqabenchmarkinformationseeking} &  &  &  & \checkmark &  &  & F1 score \\ \hline
        WinoGrande~\citep{sakaguchi2019winogrande} &  &  &  &  &  & \checkmark & Normalized accuracy \\ \hline
        X-CODAH~\citep{lin-etal-2021-common} & \checkmark & \checkmark & \checkmark & \checkmark &  &  & Normalized accuracy \\ \hline
        XCOPA~\citep{ponti-etal-2020-xcopa} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
        X-CSQA~\citep{lin-etal-2021-common} & \checkmark & \checkmark & \checkmark & \checkmark &  &  & Normalized accuracy \\ \hline
        XNLI 2.0~\citep{upadhyay2023xnli20improvingxnli} &  & \checkmark & \checkmark & \checkmark &  &  & Normalized accuracy \\ \hline
        XStoryCloze~\citep{DBLP:journals/corr/abs-2112-10668} & \checkmark &  &  & \checkmark &  &  & Normalized accuracy \\ \hline
        XWINO~\citep{tikhonov2021itsheadsusingattention} & \checkmark &  &  &  &  &  & Normalized accuracy \\ \hline
    \end{tabular}
}
\end{center}
\end{table}

\clearpage
\section{FineWeb documents in different scoring approaches}\label{app:examples}
To illustrate the types of documents each classifier scores highly or poorly, we present the highest- and lowest-scoring FineWeb examples for each of our classifier approaches (\emph{FT MKC$^+$}, \emph{MLP MKC$^+$}, \emph{CS MKC$^+$}). These examples were selected from the randomly chosen FineWeb test dataset (10K samples) used to validate the training of our model-based classifiers.

\subsection{FastText Classifier (FT)}
\begin{tcolorbox}[colback=color_blind_green!10!white,colframe=color_blind_green!90!black,title=Highest score:]
hi. i couldn't solve my problem because it has two conditional logical propositions. the problem is:can anyone help me about this, thanks =)we're expected to know that: . is equivalent tofind a logically equivalent proposition for:by first writing its contrapositive, and then applying demorgan's lawand the equality forthey were trying to be helpful by outlining the steps we should follow,. . but i think they made it more confusing.i don't see the purpose of using the contrapositive here.. . i wouldn't have done it that way.besides, the statement is a tautology . . .which gives us: .and this is a tautology: "a thing implies itself" ... which is always true.i don't know of any "logically equivalent proposition" we can write . . .
\end{tcolorbox}

\begin{tcolorbox}[colback=color_blind_red!10!white,colframe=color_blind_red!95!black,title=Lowest score:]
\UseRawInputEncoding
\begin{lstlisting}[language=, basicstyle=\normalfont, breaklines=true, columns=fullflexible, numbers=none, frame=none, xleftmargin=0pt, xrightmargin=0pt, aboveskip=0pt, belowskip=0pt]
|starts||23 sep 2016 (fri) (one day only)|want to travel soon but don’t wish to fork out a fortune for flights? check out today’s promotion from jetstar featuring promo fares fr $35 all-in valid for travel period commencing 12 october 2016don’t miss out! all-in frenzy fares to hong kong, penang and more from $35.sale ends 23 sep, 11pm!|travelling||price||travel period||find flight||penang||$35^|| [...]
\end{lstlisting}
\end{tcolorbox}

\subsection{Multi-Layer Perceptron (MLP)}
\begin{tcolorbox}[colback=color_blind_green!10!white,colframe=color_blind_green!90!black,title=Highest score:]
Naqhadeh County is a county in West Azerbaijan Province in Iran. The capital of the county is Naqadeh. At the 2006 census, the county's population was 117,831, in 27,937 families. The county is subdivided into two districts: the Central District and Mohammadyar District. The county has two cities: Naqadeh and Mohammadyar.
\end{tcolorbox}

\begin{tcolorbox}[colback=color_blind_red!10!white,colframe=color_blind_red!95!black,title=Lowest score:]
Custom Wedding Gifts

Personalized photo frames, albums \& keepsakes. Heirloom quality!

Custom Engraved Journals

Handmade in Florence Italy. Dozens of sizes and paper styles!

Awesome Leather Journals

Personalized, Customizable, Artisan made in Santa Fe, NM.

Ink Rendering from Photos

100\% Hand painted with unique style by pro artists. From \$49.
\end{tcolorbox}

\subsection{Cosine Similarity (CS)}
\begin{tcolorbox}[colback=color_blind_green!10!white,colframe=color_blind_green!90!black,title=Highest score:]
When you are renting a 5, 10, 15, 20, 30 or 40 yard dumpster, you want a company you can trust with prices that make you smile. Give us a call today and see the difference we can make in your next construction or clean out project.

Simply give us a call and we will help you figure out your dumpster rental needs.

Our dumpsters usually go out same-day or next-day depending on when you call.

We provide top-notch service, while going easy on your bottom line. What more could you ask for?

Our trained operators are here to give you a fast and hassle-free experience from start to finish.[...]
\end{tcolorbox}

\begin{tcolorbox}[colback=color_blind_red!10!white,colframe=color_blind_red!95!black,title=Lowest score:]
Cooperative flat 206/J

- Cooperative flat 201/J - Sold

2(1)+kitchenette, 50,1 m2Cooperative flat 202/J - Sold

2(1)+kitchenette, 44,9 m2Cooperative flat 203/J - Sold

2(1)+kitchenette, 50,6 m2Cooperative flat 204/J - Sold

1+kitchenette, 27,1 m2Cooperative flat 205/J - Sold

2(1)+kitchenette, 50,1 m2Cooperative flat 206/J - On sale

3+kitchenette 86,7 m2[...]
\end{tcolorbox}

\section{Example of a contaminated document}\label{app:decontmination}
We present an example of a FineWeb document that was removed during our decontamination pipeline.
\begin{tcolorbox}[title=MMLU contaminated document (matched 13-gram in bold):]
Here is our diagram of the Preamble to the Constitution of the United States. It is based on our understanding of the use of "in order to" as a subordinating conjunction that introduces a series of infinitival clauses (without subjects) that, in turn, modify the compound verbs "do ordain" and "establish."

See A Grammar of Contemporary English by Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. Longman Group: London. 1978. p. 753.

We the People of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, \textbf{provide for the common defence, promote the general Welfare, and secure the Blessings} of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America.

If you have alternative rendering for this sentence, we would be happy to hear of it. Use the e-mail icon to the left.
\end{tcolorbox}

\end{document}
