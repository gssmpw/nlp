\section{Related Work}
\textbf{Data Curation.} In order to pretrain LLMs on a large amount of diverse texts, Common Crawl\footnote{\href{https://commoncrawl.org/}{commoncrawl.org}} is often used as the base dataset. However, early works already observed that performing quality filtering on Common Crawl is crucial for model performance____. There exist various data curation approaches, such as deduplication____, PII removal____, or toxicity filtering____. Another important aspect is quality filtering of the documents. For this, the definition of quality is an important aspect. A common approach is to use heuristics to remove documents outside of the target distribution, such as filtering based on average word length, existence of punctuation, or document length____. Another approach is to define model-based filters, where research has focused on perplexity measure of the text ____ or focused on educational____ and conversational documents____. In this work, we build upon previous curated datasets based on heuristic filtering, specifically Finweb-2____, and focus on model-based filtering for structured and knowledge-rich documents relying on textual embedding representation.

\textbf{Curated English datasets.} One of the early curated datasets was C4____, followed by MassiveText____. RefinedWeb____ was an important step forward, demonstrating that filtered web data can outperform selected high-quality data sources. While these datasets have not been made fully publicly available, their filtering techniques have been expanded upon in recent fully public datasets, such as Dolma____, FineWeb, and FineWeb-Edu____. While FineWeb primarily relies on filter heuristics for data quality, Dolma adopts model perplexity filtering. FineWeb-Edu takes model-based filtering a step further and relies on LLM-based quality assessment. Similarly, a concurrent work, DCLM, has achieved competitive performance using FastText____ classifier trained on a carefully selected training dataset. In this work we adapt and extend this approach to the multilingual context.

\textbf{Curated Multilingual Datasets.} Analogously to the English datasets, there have been efforts in the multilingual space. An influential work has been CCNet____, whose language identification and model perplexity filter for data quality has been re-used in later datasets. Again, while CCNet was not published directly, but rather provided the tools for data cleaning, RedPajama____ is a prominent multilingual dataset relying on these filtering techniques. While RedPajama offers data in 5 European languages, other datasets, such as OSCAR____, mC4____, ROOTS____, MADLAD-400____, CulturaX____, and HPLT____, focus on expanding beyond, spanning a variety of language families and scripts. While they offer refined datasets for hundreds of languages, FineWeb-2____ pushes the limit to thousands of languages and further improves the performance. Our work also focuses on filtering quality samples across various language families and scripts. However, we limit our scope to 20 languages, as the number of documents drops quickly and there is trade-off between retaining a sufficient number of pretraining tokens and ensuring data quality____.
In our results, we observe the greatest benefits using stricter data filtering.

\textbf{Multilingual Embedding Models.} Early word embedding models like Word2Vec____ and GloVe____ lacked contextual understanding. FastText____ built upon them and improved performance by incorporating subword information. Transformer____ models like BERT____ and GPT____ then revolutionized the field with context-aware embeddings. Multilingual models like mBERT, XLM____, and XLM-RoBERTa____ further advanced cross-lingual understanding, with recent open-source LLMs pushing performance even higher____. Using such models,  documents as well as representative samples can be mapped into a shared embedding space to estimate their similarity. Focusing on transparency, simplicity and efficiency in our work, we use FastText and XLM-RoBERTa for our %
filtering, and analyze the trade-off between computational complexity and filtering performance.


\textbf{Multilingual Evaluation.} Evaluating LLMs requires diverse benchmarks testing linguistic and cognitive abilities like reading comprehension, reasoning, and knowledge.  While English benchmarks like MMLU____ and ARC____ exist, other languages often use translations from English, e.g., XNLI____ and machine-translated version of MMLU____. However, translations can be problematic, failing to capture cultural nuances or introducing "translationese"____. Recent work by____ emphasizes the need for culturally sensitive, natively collected benchmarks. Task difficulty and task formulation also impact model performance when trained for shorter durations____. In our work, we follow the recent evaluation tasks selection and methodology by____ to assess our model-based filtering approaches across multiple languages.