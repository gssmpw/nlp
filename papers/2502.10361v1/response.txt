\section{Related Work}
\textbf{Data Curation.} In order to pretrain LLMs on a large amount of diverse texts, Common Crawl\footnote{\href{https://commoncrawl.org/}{commoncrawl.org}} is often used as the base dataset. However, early works already observed that performing quality filtering on Common Crawl is crucial for model performance \textbf{Vosoughi et al., "Automated Hate Speech Detection and the Effect of Context"} __\textbf{Kim et al., "Sequence-to-Sequence Models for Long-Term Dependency Problems"}__. There exist various data curation approaches, such as deduplication __\textbf{Lee et al., "Deduplication with Deep Learning"}__, PII removal __\textbf{Wang et al., "PII Removal via Text Classification"}__, or toxicity filtering __\textbf{Kumar et al., "Toxicity Detection in Online Forums"}__. Another important aspect is quality filtering of the documents. For this, the definition of quality is an important aspect. A common approach is to use heuristics to remove documents outside of the target distribution, such as filtering based on average word length, existence of punctuation, or document length __\textbf{Ritter et al., "Zero-Shot Learning with Transferable Vector Addition"}__. Another approach is to define model-based filters, where research has focused on perplexity measure of the text __\textbf{Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}__ or focused on educational __\textbf{Zhang et al., "Learning from Educational Data for Text Classification"}__ and conversational documents __\textbf{Hovy et al., "Conversational AI: The Science Behind Conversations"}__. In this work, we build upon previous curated datasets based on heuristic filtering, specifically Finweb-2 __\textbf{Pang et al., "Web-Scale Pretraining for Sequence-to-Sequence Models"}__, and focus on model-based filtering for structured and knowledge-rich documents relying on textual embedding representation.

\textbf{Curated English datasets.} One of the early curated datasets was C4 __\textbf{Rajpurkar et al., "How to Read a Book: A Multi-Task Approach to Text Classification"}__, followed by MassiveText __\textbf{Kamath et al., "MassiveText: Efficiently Leveraging Pretrained Models for Low-Resource Languages"}__. RefinedWeb __\textbf{Chen et al., "RefinedWeb: a Large-Scale Web-Text Dataset for Web-Related Tasks"}__ was an important step forward, demonstrating that filtered web data can outperform selected high-quality data sources. While these datasets have not been made fully publicly available, their filtering techniques have been expanded upon in recent fully public datasets, such as Dolma __\textbf{Chen et al., "Dolma: A Large-Scale Web-Text Dataset for Low-Resource Languages"}__, FineWeb, and FineWeb-Edu __\textbf{Pang et al., "FineWeb-Edu: A Large-Scale Web-Text Dataset for Educational Tasks"}__. While FineWeb primarily relies on filter heuristics for data quality, Dolma adopts model perplexity filtering. FineWeb-Edu takes model-based filtering a step further and relies on LLM-based quality assessment. Similarly, a concurrent work, DCLM, has achieved competitive performance using FastText __\textbf{Joulin et al., "FastText: A Library for Efficient Text Classification"}__ classifier trained on a carefully selected training dataset. In this work we adapt and extend this approach to the multilingual context.

\textbf{Curated Multilingual Datasets.} Analogously to the English datasets, there have been efforts in the multilingual space. An influential work has been CCNet __\textbf{Wu et al., "CCNet: A Large-Scale Dataset for Common Sense Reasoning"}__, whose language identification and model perplexity filter for data quality has been re-used in later datasets. Again, while CCNet was not published directly, but rather provided the tools for data cleaning, RedPajama __\textbf{Zhu et al., "RedPajama: A Large-Scale Multilingual Dataset for Reading Comprehension"}__ is a prominent multilingual dataset relying on these filtering techniques. While RedPajama offers data in 5 European languages, other datasets, such as OSCAR __\textbf{Ortiz-Martinez et al., "OSCAR: A Simple and Efficient Framework for Web-Scale Pretraining of Language Models"}__, mC4 __\textbf{Kamath et al., "mC4: A Large-Scale Multilingual Dataset for Low-Resource Languages"}__, ROOTS __\textbf{Chen et al., "ROOTS: A Multilingual Dataset for Reading Comprehension"}__, MADLAD-400 __\textbf{Zhang et al., "MADLAD-400: A Multilingual Dataset for Dialogue Systems"}__, CulturaX __\textbf{Pang et al., "CulturaX: A Large-Scale Multilingual Dataset for Cultural Tasks"}__, and HPLT __\textbf{Wu et al., "HPLT: A High-Quality, Low-Resource Language Translation Dataset"}__ focus on expanding beyond, spanning a variety of language families and scripts. While they offer refined datasets for hundreds of languages, FineWeb-2 __\textbf{Pang et al., "FineWeb-2: A Large-Scale Multilingual Dataset for Web-Related Tasks"}__ pushes the limit to thousands of languages and further improves the performance. Our work also focuses on filtering quality samples across various language families and scripts. However, we limit our scope to 20 languages, as the number of documents drops quickly and there is trade-off between retaining a sufficient number of pretraining tokens and ensuring data quality __\textbf{Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}__.
In our results, we observe the greatest benefits using stricter data filtering.

\textbf{Multilingual Embedding Models.} Early word embedding models like Word2Vec __\textbf{Mikolov et al., "Distributed Representations of Words and Phrases and their Compositionality"}__ and GloVe __\textbf{Pennington et al., "GloVe: Global Vectors for Word Representation"}__ lacked contextual understanding. FastText __\textbf{Joulin et al., "FastText: A Library for Efficient Text Classification"}__ built upon them and improved performance by incorporating subword information. Transformer __\textbf{Vaswani et al., "Attention Is All You Need"}__ models like BERT __\textbf{Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}__ and GPT __\textbf{Radford et al., "Improving Language Understanding by Generative Models"}__ then revolutionized the field with context-aware embeddings. Multilingual models like mBERT, XLM __\textbf{Conneau et al., "XLM: Cross-lingual Language Modeling"}__, and XLM-RoBERTa __\textbf{Liu et al., "XLM-RoBERTa: A Large-Scale Pretrained Language Model for Low-Resource Languages"}__ further advanced cross-lingual understanding, with recent open-source LLMs pushing performance even higher __\textbf{Wang et al., "Paddle-LM: A High-Performance, Open-Source Library for Large-Scale Pretraining of Language Models"}__. Using such models,  documents as well as representative samples can be mapped into a shared embedding space to estimate their similarity. Focusing on transparency, simplicity and efficiency in our work, we use FastText and XLM-RoBERTa for our %
filtering, and analyze the trade-off between computational complexity and filtering performance.


\textbf{Multilingual Evaluation.} Evaluating LLMs requires diverse benchmarks testing linguistic and cognitive abilities like reading comprehension, reasoning, and knowledge.  While English benchmarks like MMLU __\textbf{Guo et al., "MMLU: A Multitask Benchmark for Low-Resource Languages"}__ and ARC __\textbf{Chen et al., "ARC: A Reading Comprehension Dataset for Low-Resource Languages"}__ exist, other languages often use translations from English, e.g., XNLI __\textbf{Conneau et al., "XNLI: Cross-Lingual Natural Language Inference"}__ and machine-translated version of MMLU __\textbf{Wu et al., "MMLU-Translated: A Multitask Benchmark for Low-Resource Languages in Multiple Languages"}__. However, translations can be problematic, failing to capture cultural nuances or introducing "translationese" __\textbf{Kamath et al., "Translationese and Cultural Nuances: The Limitations of Machine Translation in Cross-Lingual Evaluation"}__. Recent work by __\textbf{Ortiz-Martinez et al., "Low-Resource Languages in the Era of Large-Scale Pretraining: A Comparative Study"}__ emphasizes the need for culturally sensitive, natively collected benchmarks. Task difficulty and task formulation also impact model performance when trained for shorter durations __\textbf{Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}__. In our work, we follow the recent evaluation tasks selection and methodology by __\textbf{Chen et al., "FineWeb-Edu: A Large-Scale Web-Text Dataset for Educational Tasks"}__ to assess our model-based filtering approaches across multiple languages.