\section{Related Work}
\textbf{Data Curation.} In order to pretrain LLMs on a large amount of diverse texts, Common Crawl\footnote{\href{https://commoncrawl.org/}{commoncrawl.org}} is often used as the base dataset. However, early works already observed that performing quality filtering on Common Crawl is crucial for model performance~\citep{brown2020language}. There exist various data curation approaches, such as deduplication~\citep{lee-etal-2022-deduplicating}, PII removal~\citep{subramani-etal-2023-detecting}, or toxicity filtering~\citep{arnett2024toxicity}. Another important aspect is quality filtering of the documents. For this, the definition of quality is an important aspect. A common approach is to use heuristics to remove documents outside of the target distribution, such as filtering based on average word length, existence of punctuation, or document length~\citep{rae2021scaling, raffel2020exploring}. Another approach is to define model-based filters, where research has focused on perplexity measure of the text ~\citep{wenzek2019ccnet} or focused on educational~\citep{penedo2024fineweb} and conversational documents~\citep{li2024datacomp}. In this work, we build upon previous curated datasets based on heuristic filtering, specifically Finweb-2~\citep{penedo2024fineweb-2}, and focus on model-based filtering for structured and knowledge-rich documents relying on textual embedding representation.

\textbf{Curated English datasets.} One of the early curated datasets was C4~\citep{raffel2020exploring}, followed by MassiveText~\citep{rae2021scaling}. RefinedWeb~\citep{penedo2023refinedweb} was an important step forward, demonstrating that filtered web data can outperform selected high-quality data sources. While these datasets have not been made fully publicly available, their filtering techniques have been expanded upon in recent fully public datasets, such as Dolma~\citep{soldaini2024dolma}, FineWeb, and FineWeb-Edu~\citep{penedo2024fineweb}. While FineWeb primarily relies on filter heuristics for data quality, Dolma adopts model perplexity filtering. FineWeb-Edu takes model-based filtering a step further and relies on LLM-based quality assessment. Similarly, a concurrent work, DCLM, has achieved competitive performance using FastText~\citep{joulin2017bag} classifier trained on a carefully selected training dataset. In this work we adapt and extend this approach to the multilingual context.

\textbf{Curated Multilingual Datasets.} Analogously to the English datasets, there have been efforts in the multilingual space. An influential work has been CCNet~\citep{wenzek2019ccnet}, whose language identification and model perplexity filter for data quality has been re-used in later datasets. Again, while CCNet was not published directly, but rather provided the tools for data cleaning, RedPajama~\citep{together2023redpajama} is a prominent multilingual dataset relying on these filtering techniques. While RedPajama offers data in 5 European languages, other datasets, such as OSCAR~\citep{OrtizSuarezSagotRomary2019,AbadjiOrtizSuarezRomaryetal.2021,2022arXiv220106642A}, mC4~\citep{xue-etal-2021-mt5}, ROOTS~\citep{laurenccon2022bigscience}, MADLAD-400~\citep{kudugunta2023madlad400multilingualdocumentlevellarge}, CulturaX~\citep{nguyen2023culturax}, and HPLT~\citep{de-gibert-etal-2024-new-massive}, focus on expanding beyond, spanning a variety of language families and scripts. While they offer refined datasets for hundreds of languages, FineWeb-2~\citep{penedo2024fineweb-2} pushes the limit to thousands of languages and further improves the performance. Our work also focuses on filtering quality samples across various language families and scripts. However, we limit our scope to 20 languages, as the number of documents drops quickly and there is trade-off between retaining a sufficient number of pretraining tokens and ensuring data quality~\citep{muennighoff2023scalingdataconstrainedlanguagemodels, held2025optimizing}.
In our results, we observe the greatest benefits using stricter data filtering.

\textbf{Multilingual Embedding Models.} Early word embedding models like Word2Vec~\citep{mikolov2013efficientestimationwordrepresentations} and GloVe~\citep{pennington-etal-2014-glove} lacked contextual understanding. FastText~\citep{bojanowski2017enrichingwordvectorssubword} built upon them and improved performance by incorporating subword information. Transformer~\citep{vaswani2023attentionneed} models like BERT~\citep{devlin2019bertpretrainingdeepbidirectional} and GPT~\citep{radford2018improving} then revolutionized the field with context-aware embeddings. Multilingual models like mBERT, XLM~\citep{lample2019crosslinguallanguagemodelpretraining}, and XLM-RoBERTa~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning} further advanced cross-lingual understanding, with recent open-source LLMs pushing performance even higher~\citep{llama3, mistral_small3_blog}. Using such models,  documents as well as representative samples can be mapped into a shared embedding space to estimate their similarity. Focusing on transparency, simplicity and efficiency in our work, we use FastText and XLM-RoBERTa for our %
filtering, and analyze the trade-off between computational complexity and filtering performance.


\textbf{Multilingual Evaluation.} Evaluating LLMs requires diverse benchmarks testing linguistic and cognitive abilities like reading comprehension, reasoning, and knowledge.  While English benchmarks like MMLU~\citep{hendrycks2020measuring} and ARC~\citep{clark2018thinksolvedquestionanswering} exist, other languages often use translations from English, e.g., XNLI~\citep{conneau-etal-2018-xnli} and machine-translated version of MMLU~\citep{lai-etal-2023-okapi}. However, translations can be problematic, failing to capture cultural nuances or introducing "translationese"~\citep{romanou2024include}. Recent work by~\citet{romanou2024include,singh2024globalmmluunderstandingaddressing} emphasizes the need for culturally sensitive, natively collected benchmarks. Task difficulty and task formulation also impact model performance when trained for shorter durations~\citep{kydlicek2024finetasksmultilingualtasks}. In our work, we follow the recent evaluation tasks selection and methodology by~\citet{kydlicek2024finetasksmultilingualtasks} to assess our model-based filtering approaches across multiple languages.