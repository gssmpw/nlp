%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\newcommand{\clH}{\mathcal{H}}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\renewcommand{\qedsymbol}{}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment}

\begin{document}

\twocolumn[
% \icmltitle{Towards Robust Knowledge Transfer: Multi-Domain Graph Foundation Models with Structure Alignment}
\icmltitle{Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment}

% Non-homophilic Multi-Domain Graph Foundation Model

% Towards Robust Knowledge Transfer: Multi-Domain Graph Foundation Models with Structure Alignment

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{}

\begin{icmlauthorlist}
\icmlauthor{Shuo Wang}{equal,sch}
\icmlauthor{Bokui Wang}{equal,sch}
\icmlauthor{Zhixiang Shen}{equal,sch}
\icmlauthor{Boyan Deng}{sch}
\icmlauthor{Zhao Kang}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{University of Electronic Science and Technology of China, Chengdu, Sichuan Province, China}

\icmlcorrespondingauthor{Shuo Wang}{2022080901005@std.uestc.edu.cn}
\icmlcorrespondingauthor{Zhao Kang}{zkang@uestc.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
% 在导言区或正文中重定义
% 在 LaTeX 文档导言区或正文中重定义

\printAffiliationsAndNotice{}
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.

\end{abstract}

\section{Introduction}

Graphs, as a versatile data structure, are widely used across various domains, such as citation networks \cite{ebesu2017neural}, social networks \cite{traud2012social}, and bioinformatics \cite{zhang2021graph}. Inspired by recent advances in CV and NLP \cite{vidit2023clip,cheng2024disentangled}, researchers have sought to develop general-purpose graph foundation models. In particular, multi-domain graph pre-training has gained significant attention for its ability to integrate knowledge from various domains and enable effective transfer learning \cite{yu2025samgpt,zhang2024multi}. This approach is viewed as a critical milestone toward the creation of truly general-purpose graph models.

Despite significant progress, a substantial gap remains in fully understanding the richness and diversity of graph structural knowledge. Existing methods primarily rely on fixed graph topologies and apply uniform encoding mechanisms across all domains \cite{zhao2024all, yu2025samgpt}, which severely limits their generalizability across diverse domains \cite{zhang2024survey}. In this paper, we revisit the multi-domain graph foundation model from a structural perspective and address a critical challenge: topology alignment across different domains. Achieving effective topology alignment is a non-trivial task, presenting two primary challenges that must be overcome to advance toward a truly general-purpose graph model.

First, structural knowledge across domains often exhibits significant semantic differences. For instance, citation networks predominantly display highly homophilic patterns, whereas social networks and webpage link graphs frequently contain numerous heterophilic edges \cite{zheng2022graph}. These differences necessitate domain-specific adaptations of encoding mechanisms. As a result, there is an urgent need for a unified framework that can adaptively capture critical information from both features and topology while effectively learning domain-invariant knowledge. Such a framework is essential to ensure robust generalization to downstream domains \cite{li2022learning}.

Second, real-world graphs are inherently noisy \cite{jin2020graph}, often containing unreliable edges characterized by irrelevant, misleading, or missing connections. Additionally, graph learning algorithms are highly susceptible to adversarial attacks \cite{zugner2020adversarial}, further exacerbating the challenges of graph representation learning. These issues highlight the limitations of conventional training paradigms that rely on fixed graph topologies. Therefore, developing a robust multi-domain pre-training framework capable of effectively handling complex noise distributions is critical for constructing trustworthy and reliable graph foundation models.

To tackle these challenges, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework designed to effectively align and leverage structural knowledge across domains. First, we introduce a decoupled embedding mechanism incorporating an adaptive balance token that dynamically weighs feature and topology information. To address inherent noise and achieve topology alignment, we integrate a Graph Structure Learning module that learns robust, domain-invariant knowledge, enhancing both generalization and transferability. Additionally, we develop an efficient prompt learning strategy to transfer knowledge from multiple source domains to a target domain. By aligning topologies effectively, MDGFM not only improves multi-domain pre-training but also enables efficient and robust knowledge transfer to unseen downstream domains. Theoretical analyses validate the effectiveness of our topological knowledge alignment and demonstrate superior domain generalization capabilities.

In summary, our contributions are as follows:
\begin{itemize}
    \item  We introduce a novel structural perspective on multi-domain graph foundation models, addressing the complexities arising from interconnected nodes and diverse attribute types.
    \item We propose MDGFM, a unified multi-domain graph foundation model that employs graph structure learning to reduce inherent noise and align topological structures. Our framework facilitates efficient and robust knowledge transfer to unseen domains, with theoretical guarantees on its effectiveness and domain generalization capabilities.
    \item We conduct extensive experiments on both homophilic and heterophilic benchmark graph datasets, demonstrating the effectiveness, robustness, and versatility of our approach.
\end{itemize}


\section{Related work}

\subsection{Graph Pre-training}

Graph pre-training aims to train Graph Neural Networks (GNNs) on large amounts of unlabeled data, enabling the transfer of the learned model to downstream tasks with limited supervision. This approach facilitates the acquisition of general knowledge about real-world graphs while reducing the dependence on labeled data \cite{hu2020gpt, jin2020self}.

Pre-training methods can be broadly categorized based on their downstream task-tuning strategies. The first category follows a pretrain-and-fine-tune paradigm, emphasizing effective pre-training strategies in the upstream phase through self-supervised learning. For example, DGI \cite{velickovic2019deep} enhances training by maximizing the mutual information between global and local representations. Similarly, GraphCL \cite{you2020graph} and SimGRACE \cite{xia2022simgrace} focus on minimizing the distance between representations of different augmentations, effectively capturing invariant and robust structural information.

The second type follows a pretrain-and-prompt-tuning paradigm, where pre-trained models are not fine-tuned for downstream tasks. Instead, these methods reformulate the input data to align with the pretext task \cite{gao2020making}. For instance, GPPT \cite{sun2022gppt} introduces a graph prompting function that transforms independent nodes into token pairs, reframing downstream node classification as an edge prediction task. Similarly, GPF \cite{fang2024universal} employs learnable perturbations in the feature space of downstream graphs, enabling implicit modifications to node features and graph structures. However, most existing methods are constrained to single-domain pre-training and tuning, significantly limiting their capacity to capture cross-domain knowledge and generalize to unseen domains.


\subsection{Multi-domain Generalization}

Domain generalization aims to achieve out-of-distribution (OOD) generalization by learning from multiple source domains \cite{zhou2022domain}. In contrast to domain adaptation—which transfers prior knowledge from a single source domain to a specific target domain—domain generalization focuses on leveraging diverse information from multiple source domains to generalize effectively to unseen domains. This approach addresses two critical challenges: domain shift and the absence of target domain data \cite{blanchard2011generalizing}.

Recently, domain generalization on graphs has gained attention. These methods integrate and extract knowledge from multiple source domains during the upstream pre-training phase, enabling the transfer of this knowledge to tackle various graph-related tasks in previously unseen downstream domains.
For example, GCOPE \cite{zhao2024all} integrates multi-source graph topologies during the pre-training phase by introducing interconnected virtual nodes. Additionally, MDGPT \cite{yu2025samgpt} incorporates domain-specific tokens in the pre-training phase to align node features from different domains, and employs prompt-tuning during downstream tasks for efficient knowledge transfer. Despite these advancements, significant gaps remain in understanding the semantic differences and reliability of multi-domain topologies. Consequently, there is an urgent need to design a universal foundation model capable of achieving robust and generalized knowledge transfer across domains.

\section{Problem Definition}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{Framework_Final.pdf}
    \caption{The overall framework of the proposed MDGFM. }
    \label{MDGFM}
\end{figure*}



%\subsection{Notations}


A graph is formally represented as $G = (V,E, X^{ori})=(A,X^{ori})$, where $V$ is the set of nodes and $E$ denotes the set of edges, $ X^{ori} \in \mathbb{R}^{|V| \times d^{\prime}} $ represents the original feature matrix of nodes. $A$ is the corresponding adjacency matrix. Each graph is associated with a domain $ D_i \in \mathcal{D} $. Without loss of generality, we assume $D_i \neq D_j$ for any pair of distinct graphs.

%\subsection{Graph Domain Generalization}

Given a set of graphs from different domains, say $G_i=(A_i, X^{ori}_i), i=1,...,N$. We pretrain our graph foundation model on all visible graphs and test the performance on graph $T$ from an unseen target domain $D_T \notin \mathcal{D}$. We summarize the notations in Appendix \ref{NotationsApp}.


\section{Methodology}


%\subsection{Overall Framework}

As illustrated in Figure \ref{MDGFM}, our proposed MDGFM consists of two modules: the \textit{Multi-domain pre-training} module and the \textit{Downstream target-domain adaptation} module.

\textbf{Multi-domain pre-training.} For graphs originating from multiple source domains, we begin by unifying their feature matrices to a common dimensionality, ensuring compatibility across domains. Next, we apply token operators specifically designed to achieve semantic alignment between disparate graphs. To address the challenges posed by heterophily in downstream tasks, we leverage graph structure learning (GSL) to refine each source domain, integrating both feature and topology information during the refinement process. Additionally, GSL enables the model to learn domain-invariant knowledge for better transfer, where we prove the effectiveness in Section \ref{theory}. Finally, the graph encoder is trained effectively using self-supervised signals, enabling robust representation learning.


\textbf{Downstream target-domain adaptation.} Given that graphs often share common intrinsic patterns, pre-trained knowledge can be effectively transferred to unseen graphs \cite{zhao2024all}. To bridge the gap between source and target domains, we propose a dual-stream prompt framework. This framework combines meta prompts, which transfer broadly learned knowledge, with task-specific prompts designed to align with the unique characteristics of the downstream domain.


% \subsection{Unifying graph features with tokens}
\subsection{Feature Projection into Unified Semantic Space}
% Unifying graph features from different domains with tokens
Graphs from different domains often exhibit diverse features and structural patterns. To address these discrepancies and customize the information propagation process in GNNs, we employ both numerical and semantic alignment to bridge domain gaps. First, we transform the feature matrix of each graph into a consistent dimensionality shared across all domains, enabling uniform representation. This transformation is described by:
\begin{equation}
    X_i=Proj(X_i^{ori}) \in \mathbb{R}^{|V_i|\times d}
\end{equation}
where $d$ represents the aligned dimensionality, and $Proj(\cdot)$ denotes a specific projection operation. In our approach, the \textit{Projection} function is implemented using Principal Component Analysis (PCA) \cite{abdi2010principal}. While this dimensional alignment ensures uniform feature dimensions across graphs, it primarily addresses numerical consistency and does not resolve the semantic disparities that remain between domains. 

To achieve semantic alignment, we propose the concept of \textit{domain tokens} denoted as $t_{D_i} \in \mathbb{R}^{d}$, which encode domain-specific characteristics by capturing unique contextual information from each domain. These tokens are applied via element-wise Hadamard multiplication $\odot$ with the feature matrices, acting as adaptive filters that modulate features to reflect their respective domain properties within a unified semantic space. This process not only preserves the distinctive traits of individual domains but also facilitates their alignment within a cohesive semantic framework.

A significant challenge in building a graph foundation model lies in the substantial semantic variations across domains, which can hinder effective knowledge transfer \cite{hassani2022cross}. To address this, we leverage shared, domain-agnostic information as a critical foundation for robust and efficient transfer learning. Specifically, we introduce the \textit{shared token} $t_S \in \mathbb{R}^{d}$ which acts as a common semantic anchor to bridge discrepancies between domains. By focusing on shared patterns, $t_s$ captures transferable knowledge broadly applicable to new domains, reducing dependence on domain-specific features that may lack generalizability. This shared representation enables consistent semantic alignment, enhancing the model's adaptability and robustness to domain variations. The overall feature unification procedure can be formalized as:
\begin{equation}
    X^{\prime}_i = t_S \odot \sigma(t_{D_i} \odot X_i)
\end{equation}
where $\sigma(\cdot)$ is non-linearity 
activation function to capture complex information.
% However, relational patterns or semantic attributes could be diverse in different domains, leading to difficulty in transferring knowledge cross domains. Besides domain-specific information, shared information across domains serves as a foundation for more effective and efficient transfer learning. Given the above consideration, we introduce the \textit{shared token} $t_s \in \mathbb{R}^{1\times d}$ to provide a universal reference point that bridges the semantic discrepancies across domains. By focusing on commonalities, $t_s$ captures transferable patterns that are broadly applicable to new domains, reducing reliance on domain-specific features that may not generalize well. This shared representation ensures consistent interpretability across domains and enhances the model's ability to generalize.

% \subsection{Alignment of graph structure patterns}

\subsection{Graph Topology-aware Alignment}

Previous research on graph prompting \cite{sun2023all,dong2019unified} has predominantly emphasized feature alignment, often overlooking the critical role of structural discrepancies between graphs from different domains. However, rich semantic information is embedded within relational patterns and topology credibility, as characterized by metrics like homophily and heterophily edge ratios \cite{zhu2020beyond}. These structural characteristics underscore the importance of aligning graph structure patterns to enable meaningful knowledge transfer \cite{sun2022beyond,zheng2022graph}.

Rather than solely focusing on direct structural unification, our approach seeks to synchronize more reliable topology information through graph structure refinement on both source and unseen downstream graphs. This is achieved via the incorporation of GSL, which enhances the robustness and effectiveness of cross-domain graph alignment. By addressing structural discrepancies, our framework bridges the gap between diverse graph domains, facilitating more comprehensive knowledge transfer.

Unlike prior GSL approaches \cite{zhu2021deep,jin2020graph}, which primarily optimize graph structures based on feature similarity, our method incorporates both semantic and topological information for a more comprehensive refinement process. Specifically, we utilize $A^{r}_iX^{\prime}_i$ to capture the original structural information, where $r$ represents the order of graph aggregation (default set to one). To effectively fuse $X^{\prime}_i$ and $A^{r}_iX^{\prime}_i$ during graph refinement, we introduce a \textit{balance token} $t_{B_i} \in \mathbb{R}^{2d}$, which operates in the following manner:
\begin{equation}\label{Hi}
    H_i=t_{B_i} \odot [X^{\prime}_i,A^r_iX^{\prime}_i]
\end{equation}
Here, $t_{B_i}$ dynamically balances the contributions of node features and aggregated structural information. 

Finally, consistent with existing GSL methods \cite{li2024gslb}, we apply post-processing techniques to reconstruct the refined adjacency matrix. Specifically, a similarity matrix is first computed using $H_i$ and then sparsified via $k$-nearest neighbors ($k$NN). In practice, we employ $k$NN sparsification with its locality-sensitive approximation to enhance efficiency \cite{fatemi2021slaps}. Subsequently, operations such as Symmetrization, Activation, and Normalization are performed sequentially to produce the final $A^{\prime}_i$. Detailed implementation steps are provided in Appendix \ref{processors}. Building on this topology-aware refinement process, we propose a general pretraining framework capable of generating high-quality embeddings across multiple domains using contrastive learning. By optimizing a contrastive-based objective, the model learns generalized representations by leveraging rich sample-to-sample relationships from diverse perspectives \cite{yao2022pcl}. Mathematically, we  maximize the mutual information between $G_{i1}=(A_i,X_i^{\prime})$ and $G_{i2}=(A_i^{\prime},X_i^{\prime})$ as follows:
\begin{equation}
    \mathcal{L}=-I(G_{i1};G_{i2}\dagger 
 I_e)-I(G_{i1};G_{i2}\dagger 
 A_i^{\prime})
\end{equation}
where we use $\dagger$ to indicate positive samples when computing similarities, often utilizing the identity matrix $I_e$ for this purpose. The second term of  $\mathcal{L}$ incorporates the refined graph structure $A_i^{\prime}$ to increase the number of positive samples. Detailed computations of the loss function are provided in Appendix \ref{mutual}. This loss formulation reduces the information gap between the original and refined graphs, ensuring that global structural properties are preserved during optimization. By maintaining semantic consistency in the graph structure, it enhances alignment and adaptability for downstream tasks.


% \subsection{Downstream target-domain adaptation}

\subsection{ Knowledge Transfer to Downstream Domain}

Similar to in-context learning, downstream adaptation aims to enhance a model's ability to learn tasks using only a few examples provided as demonstrations \cite{dong2022survey}, highlighting the need for upstream knowledge transfer.  Inspired by the ``pre-training $\&$ prompting" paradigm, our framework employs a dual-prompt strategy. Specifically, the \textbf{meta prompt} $p_m$ focuses on adjusting the distribution of learned knowledge by modeling the relationships between the target and source domains. This can be formalized as a function defined for feature $X$:
\begin{equation}
    p_m(X)=t_S\odot \sigma(\sum_{i=1}^N \alpha_it_{D_i}\odot X)
\end{equation}
where $\alpha_1,...,\alpha_N$ are trainable coefficients for the source domains. Meanwhile, the learnable \textbf{specific prompt} $p_s\in \mathbb{R}^d$ aims to directly align the target domain with the unified semantic space by learning from the limited available samples, ensuring precise adaptation to the downstream domain. 

As discussed, for the downstream graph $T(A_T, X_T)$ where the feature dimension aligns with the given $d$, the dual prompts  $p_m$ and $p_s$ operate on $X_T$  to achieve semantic unification. Since all tokens are optimized during the pretraining phase, only the coefficients and $p_s$ need to be trained in this process. The reduced number of learnable parameters enhances transfer performance by compensating for the scarcity of annotated samples. Consistent with upstream operations, we use $H_T=t\odot[X_T^{\prime},A^r_TX_T^{\prime}]$ to obtain $A^{\prime}_T$, where $t$ could be composed of $t_{D_i}$ (similar to $p_m$) or be directly trained. Finally, we obtain the node representations $Z$ as follows, which can be applied to downstream tasks:
\begin{equation}
    Z = GE(A_T^{\prime},\beta p_m(X_T)+(1-\beta)p_s\odot X_T;\theta_{pre})
\end{equation}
where $GE$ denotes the graph encoder and $\theta_{pre}$ the frozen parameters learned in pre-training phase. To mitigate the risk of harmful transfer from source graphs with significant discrepancies, as discussed by \cite{yan2024inductive}, the prompts are integrated using a learnable parameter, $\beta$, instead of relying solely on concatenation. For graphs that significantly differ from the source domains, the meta prompt weights are attenuated to minimize the transfer of irrelevant information, thereby reducing its potential negative impact on downstream tasks.
% For node classification task, the loss function $\mathcal{L}$ could be defined as:
% \begin{equation}
%     \mathcal{L}=-\frac{exp()}{}
% \end{equation}


For the downstream node classification task, the loss function $\mathcal{L}_{dst}$ follows a universal task template grounded in subgraph similarity. Given a labeled training set \( S = \{(x_1, y_1),  \dots,(x_i,y_i),\dots\} \), where \( x_i \) represents a node and \( y_i \in Y \) denotes the class label of \( x_i \) with $Y$ denoting the set of all possible class labels, the loss function is expressed as:
\begin{equation}
    \mathcal{L}_{dst} = -\sum_{(x_i, y_i) \in S} \ln \frac{\frac{1}{\tau} \, sim(z_{x_i}, \bar{z}_{y_i})}{\sum_{y \in Y} \exp\left(\frac{1}{\tau} \, sim(z_{x_i}, \bar{z}_y)\right)}
\end{equation}
Here, \( z_{x_i} \) denotes the final embedding of node \( x_i \), and \( \bar{z}_y \) represents the class embedding for class \( y \), computed as the mean embedding of all training instances belonging to \( y \). We practically calculate \( sim(\cdot, \cdot) \) by cosine similarity, while \( \tau \) serves as a temperature parameter to control the shape of the output distribution.

\section{Theoretical Analysis}\label{theory}

In this section, we provide a theoretical analysis to demonstrate the domain generalization capabilities of MDGFM. 
% These properties are further corroborated through extensive experimental evaluations presented in Section \ref{experiment}.

% \subsection{Robustness}

% \begin{definition}
%     \textbf{Grouping effect. \cite{GEKDD}} There are two similar nodes $i$ and $j$ in terms of local topology and node features, i.e., 
%     $
%     \mathcal{V}_i \rightarrow \mathcal{V}_j \Longleftrightarrow\left(\left\|A_i-A_j\right\|^2 \rightarrow 0\right) \wedge\left(\left\|x_i-x_j\right\|^2 \rightarrow 0\right)
%     $,
%     the matrix $G$ is said to have a grouping effect if
%     $ \mathcal{V}_i\rightarrow \mathcal{V}_j\Longrightarrow |G_{ip}-G_{jp}|\rightarrow0,\space \forall 1\leq p\leq N.$
%     \label{Groupingeffect}
% \end{definition}

% Specifically, here we prove the similarity matrix $S=HH^T$ have the grouping effect. Define the distance between filtered node $i$ and $j$ is 

% \begin{equation}
% \begin{aligned}
%     S&=HH^T=XX^T+AXX^TA^T \\
%     &=
% \end{aligned}
% \end{equation}

% \subsection{Domain Generalization}

Denote $P_\mathcal{X}^{t}$ as the data distribution on feature space in the target domain. $t$ could be changed into $i$ for source domain, while $\mathcal{X}$ could be replaced by label space $\mathcal{Y}$. With the covariate shift assumption, each domain is characterized by the distribution on $\mathcal{X}$. Thus, we can approximate the target domain distribution $P^t_{\mathcal{X}}$ within the convex hull of source domain distributions: $\Lambda := \{\sum_{i=1}^M \pi_i P^i_\mathcal{X} \mid \pi \in \Delta_M\}$, where $\Delta_M$ is the $(M-1)$-dimensional simplex so that each $\pi$ represents a normalized mixing weights. The generalization capability is quantified by the following:
\begin{theorem}[Domain generalization error bound]
    Let $\gamma := \min_{\pi \in \Delta_M} d_\clH (P^t_\mathcal{X}, \sum_{i=1}^M \pi_i P^i_\mathcal{X})$ with minimizer $\pi^*$ be the distance of $P^t_\mathcal{X}$ from the convex hull $\Lambda$, and $P^*_\mathcal{X} := \sum_{i=1}^M \pi^*_i P^i_\mathcal{X}$ be the best approximator within $\Lambda$.
    Let $\rho := \sup_{P'_\mathcal{X}, P''_\mathcal{X} \in \Lambda} d_\clH (P'_\mathcal{X}, P''_\mathcal{X})$ be the diameter of $\Lambda$.
    Then it holds that
    \begin{align}
        \epsilon^t(h) \le \sum_{i=1}^M \pi^*_i \epsilon^i(h) + \frac{\gamma + \rho}{2} + \lambda_{\clH, (P^t_\mathcal{X}, P^*_\mathcal{X})},
    \end{align}
    where $\lambda_{\clH, (P^t_\mathcal{X}, P^*_\mathcal{X})}$ is the ideal joint risk across the target domain and the domain with the best approximator distribution $P^*_\mathcal{X}$. $\epsilon^1, \cdots, \epsilon^M$ represent the source risks and $\epsilon^t$ denotes the target risk \cite{albuquerque2019generalizing}. 
    \label{bound}
\end{theorem}

To minimize the upper error bound, we introduce the concept of invariant graph learning:
\begin{assumption}
    Given a graph \( G \), there exists an optimal invariant graph learner \( \Phi^*(G) \) satisfying:  \\ \textit{Invariance Property:} $ \forall e,P^{e}(Y | \Phi^*(G)) = P^{e'}(Y | \Phi^*(G)).$ \\
    \textit{Sufficient Property:} $Y=\omega^*(g^*(\Phi^*(G)))+\epsilon,\epsilon \bot G$. \\
    where $g^*(\cdot)$ is a representation learning function, $\omega^*$ is the classifier, $e$ denotes the environments (i.e., domains), $\bot$ indicates statistical independence and $\epsilon$ is the random noise.  
\end{assumption}

$\Phi^*(G)$ could generate invariant graphs across different domains, which is implemented as GSL procedure on all domains in our work. Moreover, by maximizing the mutual information between $G_{i1}(A_i,X_i^{\prime})$ and $G_{i2}(A_i^{\prime},X_i^{\prime})$, our method retains sufficient task-relevant information, thereby satisfying the sufficient property. Motivated by Theorem \ref{bound}, domain invariant representation learning minimizes the risks over all source domains corresponding to the first term of the bound, as well as the representation distribution differences among source and target domains in the hope of reducing $\gamma$ and $\rho$. In general, based on the following theorem, our model achieves the \textbf{minimal error bound} due to its adherence to the invariant and sufficient properties.
\begin{theorem}
    Let $\Phi^*$ be the optimal invariant graph learner and denote the complement as $G\backslash \Phi^*(G)$, i.e., the corresponding variant subgraph. Then we can obtain the optimal predictor under distribution shifts as follows.
    \begin{equation}
        \arg\min_{\omega,g}\omega \circ g \circ \Phi^*(G)=\arg\min_{f}\sup_{e}\mathcal{R}(f|e)
    \end{equation}
    if the following conditions hold: (1) $\Phi^*(G) \bot G\backslash \Phi^*(G)$; (2) $\forall \Phi$ which is an invariant graph learner, $\exists e^{\prime}$ such that $P^{e^{\prime}}(G,Y)=P^{e^{\prime}}(\Phi(G),Y)P^{e^{\prime}}(G\backslash \Phi(G))$ and $P^{e^{\prime}}(\Phi(G))=P^{e}(\Phi(G))$ \cite{li2022learning}.
\end{theorem}

where $\mathcal{R}(f|e)=\mathbb{E}_{G,Y}^e[l(f(G),Y)]$ is the risk of the predictor $f$ on the domain $e$ and $l(\cdot,\cdot):\mathbb{Y\times Y \to R}$ denotes a loss function.

\section{Experiments }\label{experiment}

In this section, we evaluate our proposed MDGFM on few-shot node classification task. Specifically, we aim to answer the following four research questions:

\textbf{RQ1. }How does MDGFM perform on multi-graph tasks in few-shot learning scenarios compared to current baselines? 
\textbf{RQ2. }How do the key components benefit our model?
\textbf{RQ3. }Does our model rely on specific source domains and exhibit sensitivity to the removal of these source domains?
\textbf{RQ4. }Does our model demonstrate robustness against attacks and deletion on the source domain?

% \textbf{RQ3. }What is the impact of the hyperparameter $k$ in downstream graph structure learning $k$-nearest neighbors ($k$NN) on our model?

\subsection{Experimental Setups}

\textbf{Datasets. }To ensure a comprehensive comparison, we conduct experiments on six primary datasets, including three homophilic graphs—Cora \cite{sen2008collective}, Citeseer \cite{sen2008collective}, and Pubmed \cite{namata2012query}—and three heterophilic graphs—Cornell, Chameleon, and Squirrel \cite{pei2020geom}. Additionally, we include Penn94 \cite{traud2012social}, a large-scale graph dataset, as a downstream target domain. Detailed statistical information on these datasets is provided in Appendix \ref{datasets_appendix}.

\textbf{Baselines}. We compare our method against four categories of approaches:\\
\textbf{Supervised methods}: These methods train a GNN on downstream tasks and directly infer results. We employ two well-known models: GCN \cite{kipf2016semi} and GAT \cite{velickovic2017graph}.\\
    \textbf{Graph Pre-training Methods}: These approaches perform self-supervised pre-training across multiple isolated source domains, such as DGI \cite{velickovic2019deep} and GraphCL \cite{you2020graph}, before fine-tuning on a new downstream task. Notably, source domains are not trained simultaneously; instead, they are merged into a single batch object, forming an adjacency matrix composed of distinct blocks.\\
\textbf{Graph Prompting Methods}: These methods freeze the parameters of a pre-trained model, unify downstream tasks and tune a single type of prompt accordingly. Representative methods include GPPT \cite{sun2022gppt} and GPF \cite{fang2024universal}.\\
    \textbf{Multi-domain Pre-training Methods}: These methods integrate multiple source domains during upstream pre-training and transfer knowledge for few-shot learning on unseen target domains. For instance, GCOPE \cite{zhao2024all} constructs a unified large-scale dataset with inter-dataset connections, while MDGPT \cite{yu2025samgpt} encodes domain-specific characteristics using unique tokens, followed by fine-tuning and prompt adaptation.

For a fair comparison, we use GCN as the backbone for all methods.
%\textbf{Setup of multi-domain pre-training.} 
To evaluate performance on unseen target domains, we focus on scenarios where the model generalizes to domains not encountered during pre-training. For the Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Cornell datasets, we designate one dataset as the downstream target domain while using the remaining five as source domains during pre-training. Additionally, we extend this setup by using all six datasets as source domains and applying Penn94 as the target domain for few-shot learning. More details are provided in Appendix \ref{surd}.

%\textbf{Setup of downstream tasks.} In the downstream phase, we primarily evaluate node-level  one-shot and five-shot classification. %, maintaining a consistent \( K \)-shot learning setup for each target dataset. In the \( K \)-shot experimental setup, each class is provided with \( K \) labeled nodes during the downstream phase for learning. Consequently, for datasets with imbalanced class distributions, we assign different values of \( K \) in the few-shot tasks.



\subsection{Cross-domain Transfer Efficacy under Few-shot Learning Conditions (RQ1)}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[h]
\centering
\caption{Cross-domain transfer learning performance (mean accuracy $\pm$ std) of one-shot node classification. The highest result is \textbf{bolded} and the runner-up is highlighted with \textcolor{red}{red color}. The symbol ``OOM'' means out of memory.}
\vskip 0.1in
\renewcommand{\arraystretch}{1.1}
\label{tab:one-shot performance}
\resizebox{0.9\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Methods & Cora                                               & Citeseer                                           & Pubmed                                             & Cornell                                             & Squirrel                                           & Chameleon                                          & Penn94                       \\
\midrule
GCN      & 28.57$\pm$5.07                        & 31.27$\pm$4.53                        & 40.55$\pm$5.65                        &  31.81$\pm$4.71  & 20.00$\pm$0.29                        & 24.17$\pm$5.21                        & 50.45$\pm$1.79  \\
GAT      & 28.40$\pm$6.25                        & 30.76$\pm$5.40                        & 39.99$\pm$4.96                        & 28.03$\pm$13.19                        & 21.55$\pm$2.30                        & 23.93$\pm$4.11                        & 50.48$\pm$1.40  \\
\midrule
DGI      & 29.30$\pm$5.82                        & 30.03$\pm$4.88                        & 41.85$\pm$7.78                        &  31.54$\pm$15.66 & 21.15$\pm$1.68                        & 21.73$\pm$5.47                        & 50.22$\pm$0.74  \\
GraphCL  & 34.94$\pm$6.49                        & 30.58$\pm$4.58                        & 40.37$\pm$7.81                        & 27.15$\pm$12.64                        & 21.42$\pm$2.23                        & 22.49$\pm$3.02                        & 50.61$\pm$2.03 \\
\midrule
GPPT     & 17.52$\pm$5.52                        & 21.45$\pm$3.45                        & 36.56$\pm$5.31                        & 25.09$\pm$2.92                         & 20.09$\pm$0.91                        & 24.53$\pm$2.55                        & 48.93$\pm$1.39 \\
GPF      & 37.84$\pm$11.07                       & 37.61$\pm$8.87                        & \textcolor{red}{ 46.36$\pm$7.48} & \textcolor{red}{ 34.54$\pm$7.73}  & 21.92$\pm$3.50                        & \textcolor{red}{ 25.90$\pm$8.51}                        & OOM                          \\
\midrule
GCOPE    & 34.23$\pm$8.16                        & 39.05$\pm$8.82                        & 44.85$\pm$6.72                        & 34.02$\pm$11.94                        & \textcolor{red}{ 22.46$\pm$1.96} & 24.61$\pm$3.99 & \textcolor{red}{50.79$\pm$0.65}  \\
MDGPT    & \textcolor{red}{ 39.54$\pm$9.02} & \textcolor{red}{ 39.24$\pm$8.95} & 45.39$\pm$11.01                       & 33.58$\pm$10.38                         & 22.35$\pm$3.77                        & 23.68$\pm$1.56                        &                   50.78$\pm$3.05           \\
\midrule
 MDGFM     & \textbf{44.83$\pm$7.41}               & \textbf{42.18$\pm$6.41}               & \textbf{46.84$\pm$7.31}              & \textbf{40.77$\pm$5.96}                & \textbf{24.30$\pm$3.26}               & \textbf{28.36$\pm$3.65}               &   \textbf{52.36$\pm$0.86 } \\
\bottomrule
\end{tabular}
}
\end{table*}




We compare our proposed MDGFM against all baselines on 1-shot and $K$-shot node classification tasks. Each experiment is repeated five times, and we report the average results in Tables \ref{tab:one-shot performance} and \ref{tab:few-shot performance}.

In the one-shot setting, MDGFM consistently outperforms all baseline models, demonstrating superior performance on both homophilic and heterophilic graphs. This advantage stems from MDGFM’s ability to effectively capture both domain-specific information from each source domain and shared patterns across domains. Notably, we observe that supervised methods occasionally surpass graph pre-training approaches, suggesting the presence of negative transfer in certain scenarios.

Among multi-domain pre-training methods, MDGPT outperforms GCOPE on homophilic graphs, as GCOPE’s reliance on virtual nodes propagates homophilic graph information, inadvertently introducing noise. Conversely, on heterophilic graphs, GCOPE surpasses MDGPT, as MDGPT’s simple integration of source domain tokens fails to fully capture cross-domain commonalities. For graph prompting methods, GPPT generally underperforms across most datasets, as it is not specifically designed for few-shot learning. While GPF achieves competitive performance, its high memory requirements pose a significant limitation.
%Note that Additionally, we incorporate graph structure learning to refine the structures of both source and target domains.

In the few-shot setting, we observe trends similar to those in the one-shot scenario. Notably, on heterophilic graphs, certain methods exhibit a decline in performance compared to the one-shot case. This degradation is primarily due to the introduction of noise from the few-shot samples. In contrast, our model remains robust and does not experience performance deterioration as the number of training samples increases.

 %This demonstrates that our model effectively extracts valuable information from few-shot scenarios.

Quantitatively, across the seven datasets, MDGFM outperforms the second-best model by up to 18.04\% in the one-shot scenario and 8.11\% in the few-shot scenarios.

We further evaluate large 
\( K \) values on the large-scale Penn94 dataset. As shown in Table \ref{Penn94Kshot}, our model's performance improvement becomes more pronounced as 
\( K \) increases. In contrast, the performance of GCOPE and MDGPT remains relatively stable, showing minimal change.






%In the five-shot setting, MDGFM achieves a 2.80% to 8.11% improvement over the best competing model.

 %Since MDGFM does not involve the presence of the target domain in the source domains, thus both one-shot and few-shot experiments are conducted to evaluate both the knowledge transfer capability and learning ability of the proposed MDGFM. Furthermore, to evaluate the generalization ability of our model, we increase the value of \( K \) during downstream $K$-shot learning when using Penn94 as the target domain, with the results provided in Appendix \ref{penn94}. The findings demonstrate that our approach exhibits significant superiority when generalizing to graph data where topological information is nearly uninformative \cite{chen2024polygcl}, underscoring the necessity of GSL in downstream adaptation.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}



\begin{table*}[h]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Cross-domain transfer learning performance (mean accuracy $\pm$ std) of few-shot node classification, where the values within parentheses represent the $K$ values used in the context of $K$-shot learning.}
\vskip 0.1in

\label{tab:few-shot performance}
\resizebox{0.9\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Methods & Cora(5)                                            & Citeseer(5)                                       & Pubmed(5)                                          & Cornell(3)                                          & Squirrel(3)                                        & Chameleon(5)                                       & Penn94(5)                   \\
\midrule
GCN      & 60.15$\pm$5.33                        & 45.54$\pm$4.71                       & 57.82$\pm$8.26                        & 39.53$\pm$13.57                        & 21.61$\pm$4.22                        & 22.09$\pm$0.99                        & \textcolor{red}{52.07$\pm$1.01} \\
GAT      & 59.79$\pm$3.89                        & 50.48$\pm$2.94                       & 57.55$\pm$9.37                        & 34.53$\pm$13.01                        & 20.11$\pm$3.11                        & 20.83$\pm$1.52                        & 50.98$\pm$1.23\\
\midrule
DGI      & 56.76$\pm$11.29                       & 42.67$\pm$8.98                       & 54.04$\pm$11.59                       & 43.22$\pm$5.84                         & 20.23$\pm$1.12                        & 27.68$\pm$5.21                        & 50.13$\pm$0.81 \\
GraphCL  & \textcolor{red}{ 61.59$\pm$5.71} & 47.05$\pm$6.85                       & 58.50$\pm$7.38                        & 32.77$\pm$6.23                         & 21.18$\pm$0.96                        & 27.45$\pm$2.58                        & 51.84$\pm$1.58 \\
\midrule
GPPT     & 43.67$\pm$7.11                        & 47.31$\pm$6.93                       & 40.47$\pm$10.17                       & 34.69$\pm$8.54                         & \textcolor{red}{ 22.14$\pm$1.53} & \textcolor{red}{ 28.25$\pm$1.39} & 51.75$\pm$1.60 \\
GPF      & 51.21$\pm$11.44                       & \textcolor{red}{ 56.90$\pm$8.84} & \textcolor{red}{ 58.76$\pm$7.70} & 38.17$\pm$8.15                         & 21.62$\pm$3.10                        & 28.09$\pm$4.93                        & OOM                         \\
\midrule
GCOPE    & 54.63$\pm$3.98                        & 53.18$\pm$4.47                       & 57.74$\pm$2.73                        & \textcolor{red}{ 48.21$\pm$11.97} & 21.37$\pm$4.20                        & 25.50$\pm$1.23                        & 50.69$\pm$0.81 \\
MDGPT    & 59.64$\pm$5.73                        & 52.71$\pm$5.71                       & 58.65$\pm$7.54                        & 35.18$\pm$8.90                         & 21.42$\pm$4.16                        & 26.18$\pm$5.18                        &       50.41$\pm$3.13                     \\
\midrule
MDGFM     & \textbf{64.56$\pm$7.29}               & \textbf{61.24$\pm$4.82}              & \textbf{63.50$\pm$5.81}               & \textbf{49.56$\pm$6.92}                & \textbf{23.00$\pm$4.39}               &  \textbf{30.54$\pm$2.87}               &  \textbf{53.58$\pm$0.83} \\

\bottomrule
\end{tabular}
}
\end{table*}




\begin{table}[h]
\centering
\caption{$K$-shot node classification accuracy on Penn94.}
\vskip 0.1in
\resizebox{0.85\linewidth}{!}
{
\begin{tabular}{c|c|c|c}
\toprule
$K$ & GCOPE                       & MDGPT                       & MDGFM                                \\
\midrule
10                     & 50.30$\pm$1.03 & 50.59$\pm$4.35 & \textbf{54.43$\pm$0.93} \\
50                     & 51.36$\pm$0.28 & 50.73$\pm$3.85 & \textbf{55.69$\pm$1.28} \\ 
100                    & 51.61$\pm$0.33 & 51.14$\pm$4.88 & \textbf{58.51$\pm$1.15} \\
500                    & 52.88$\pm$0.40  & 51.52$\pm$3.64 & \textbf{63.25$\pm$0.82} \\
\bottomrule
\end{tabular}
}
\label{Penn94Kshot}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Ablation_new.pdf}
    \caption{Ablation studies on key components.}
    % \vskip 0.1in
    \label{fig:enter-label}
\end{figure}


\begin{figure*}[h]
	\centering
	\begin{subfigure}{0.21\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{outputadd.pdf}
		\caption{Adding edges (Cornell)}
  	\label{1m}
	\end{subfigure}
     \hspace{0.02\linewidth}
 	\centering
	\begin{subfigure}{0.21\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{outputremove.pdf}
		\caption{Deleting edges (Cornell)}
  	\label{2m}
	\end{subfigure}
      \hspace{0.02\linewidth}
	\centering
	\begin{subfigure}{0.21\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{outputmeta.pdf}
		\caption{Meta-attack (Cornell)}
  	\label{3m}
	\end{subfigure}
    \hspace{0.02\linewidth}
	\centering
	\begin{subfigure}{0.21\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{outputmetacora.pdf}
		\caption{Meta-attack (Cora)}
  	\label{4m}
	\end{subfigure}
	\caption{Performance of robustness analysis, where the dataset in parentheses represents the target domain.}
	\label{robustnessanalysis}
\end{figure*}


\subsection{Ablation Study (RQ2)}

In this section, we analyze the effectiveness of key components in our model by designing four variants and comparing their classification performance against MDGFM. \\
% Graph Structure Learning (GSL)
\textit{w/o-refinedadj}: Removes the GSL module.\\
\textit{w/o-sumtoken}: Excludes the shared token.\\
% used for information aggregation
\textit{w/o-topology}: Constructs the graph without considering topological information in the GSL process, i.e., merely use $X_i^{\prime}$ in Eq.(\ref{Hi}) rather than $[X_i^{\prime}, A_i^rX_i^{\prime}]$ for the $i$-th graph.\\
\textit{w/o-balance}: Removes the balance token.
% for graph construction

\textbf{Effectiveness of domain-invariant learning.} 
MDGFM captures shared information across domains through a shared token. As shown in Figure \ref{fig:enter-label}, the \textit{w/o-sumtoken} variant exhibits the weakest performance across most datasets, underscoring the importance of capturing common information for effective domain generalization.\\
\textbf{Effectiveness of topology structure alignment.} MDGFM utilizes graph structure learning to extract information from graph topology structure. The \textit{w/o-refinedadj}, \textit{w/o-topology},  \textit{w/o-balance} generally perform much worse than MDGFM, demonstrating the necessity of each component.%, as the variant without the balance token is fails to effectively integrate structural information for graph construction.

%All variants are configured with same hyperparameters, adopt GCN as the backbone, and follow the same training strategy as the original model. Figure \ref{fig:enter-label} presents the performance of all variants, demonstrating the necessity of each component. 
%Specifically, the performance degradation of the first three variants underscores the significance of domain-invariant learning, with the topology-aware GSL and shared token extracting invariant feature and topological knowledge respectively. Building upon this, after capturing both topological and feature information, we further introduce \textit{w/o-balance} to emphasize the importance of maintaining a proper balance between these two aspects.



\subsection{Domain Sensitivity (RQ3)}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.05}
\caption{Performance of domain sensitivity analysis.}
\vskip 0.1in
\label{tab:domain sensitivity}
\resizebox{\linewidth}{!}
{
\begin{tabular}{c|c|c|c}
\toprule
Removed Data              & GCOPE       & MDGPT      & MDGFM       \\
\midrule
-Pubmed                   & 33.38$\pm$8.19  & 31.42$\pm$9.53 & 39.66$\pm$5.51 \\
-Squirrel                 & 28.88$\pm$8.57 &  33.02$\pm$8.26 &  40.78$\pm$5.44 \\
\midrule
-Cora-Citeseer            & 30.25$\pm$8.99  &  30.04$\pm$8.77 & 39.96$\pm$5.99 \\
-Pubmed-Squirrel          &  30.50$\pm$8.41  &  31.10$\pm$7.43 &  42.68$\pm$5.64 \\
\midrule
-Citeseer-Cora-Pubmed    & 26.37$\pm$10.85 &  29.38$\pm$9.39 &  40.08$\pm$5.74 \\
-Pubmed-Citeseer-Squirrel & 31.50$\pm$13.18  &  30.90$\pm$7.61 &  42.20$\pm$5.02 \\
\midrule
Original Performance & 34.02$\pm$11.94 & 33.58$\pm$10.38 & 40.77$\pm$5.96 \\
\bottomrule
\end{tabular}
}
\end{table}

In this section, we examine the extent to which our model depends on specific source domains and its performance under limited source-domain conditions. To assess this, we systematically remove subsets of source domains and evaluate the model using Cornell as the target domain, comparing its performance against multi-domain pre-training baselines. Specifically, we exclude one to three source domains while keeping the target domain and all other parameters fixed.% Each experiment is repeated five times to obtain the averaged one-shot performance, which is reported in Table \ref{tab:domain sensitivity}.

According to the one-shot result in Table \ref{tab:domain sensitivity}, the performance of GCOPE and MDGPT deteriorates significantly, whereas MDGFM maintains strong performance and stability despite the removal of source domains. Our findings suggest that these methods extract varying degrees of knowledge from specific source domains. %In contrast, MDGFM demonstrates no dependency on any particular source domain. Furthermore, we observe that reducing the number of source domains generally degrades the downstream performance of baseline methods, though the impact varies significantly across domains. 
%MDGPT and MDGFM maintain strong performance and stability despite source domain remova. 
A counterintuitive phenomenon is that in some cases, removing some source domains leads to improved performance. This occurs because multi-domain graphs may result in conflicts or interferences rather than synergy \cite{yu2025samgpt}. Our model aims to mitigate such negative influences by extracting the shared informative and beneficial aspects of the available knowledge.

\subsection{Robustness Analysis (RQ4)}


% \begin{figure}[h]
% 	\centering
% 	\begin{subfigure}{0.47\linewidth}
% 		\centering
% 		\includegraphics[width=1.\linewidth]{outputadd.pdf}
% 		\caption{Adding edges}
%   	\label{robustness_adding}
% 	\end{subfigure}
%      \hspace{0.03\linewidth}
%  	\centering
% 	\begin{subfigure}{0.47\linewidth}
% 		\centering
% 		\includegraphics[width=1.\linewidth]{outputremove.pdf}
% 		\caption{Removing edges}
%   	\label{robustness_delete}
% 	\end{subfigure}
	
% 	\caption{Robustness analysis on Cornell.}
% 	\label{robust}
% \end{figure}


% \begin{wrapfigure}{r}{0.23\textwidth} % r: right
%     \centering
%     \includegraphics[width=\linewidth]{outputmeta.pdf} 
%     \caption{Performance under meta-attack.}
%     \label{metaattack}
% \end{wrapfigure}


To evaluate the robustness of our proposed MDGFM, we assess the node classification performance under diverse attacks. Simple modification attacks are implemented by randomly adding or removing edges. Considering the varying influence of individual domains on the target domain, we apply these random attacks to all source domains and the target domain under one-shot learning.

Furthermore, we employ Metattack \cite{zügner2018adversarial}, an advanced adversarial attack that perturbs the training data to degrade the model performance after training. Under identical attack conditions, we compare MDGFM against multi-domain methods. As shown in Figure \ref{robustnessanalysis}, the performance of GCOPE and MDGPT deteriorates as attack intensity increases. Notably, our model is quite stable and consistently outperforms all baselines across all attack scenarios, which can be attributed to our proposed topology alignment. Further analysis, including a sensitivity study of hyperparameters, is provided in Appendix \ref{additional}.


\section{Conclusion}

In this paper, we introduce MDGFM, a novel graph foundation model that unifies graphs from diverse domains into a universal semantic space. Our approach enhances domain generalization by extracting domain-invariant knowledge through both feature and topology alignment. We validate the effectiveness of MDGFM through theoretical analysis and empirical evaluation. To the best of our knowledge, this is the first work to explicitly address invariant information embedded in topology structures across homophilic and heterophilic domains. Our findings pave the way for advancements in graph-based domain generalization, with potential extensions to dynamic graphs and large-scale heterogeneous networks, further enhancing adaptability in real-world applications.

%\newpage

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\newpage
\nocite{langley00}

\newpage
% \bibliography{example_paper}
\bibliographystyle{icml2025}
\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdi \& Williams(2010)Abdi and Williams]{abdi2010principal}
Abdi, H. and Williams, L.~J.
\newblock Principal component analysis.
\newblock \emph{Wiley interdisciplinary reviews: computational statistics}, 2\penalty0 (4):\penalty0 433--459, 2010.

\bibitem[Albuquerque et~al.(2019)Albuquerque, Monteiro, Darvishi, Falk, and Mitliagkas]{albuquerque2019generalizing}
Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T.~H., and Mitliagkas, I.
\newblock Generalizing to unseen domains via distribution matching.
\newblock \emph{arXiv preprint arXiv:1911.00804}, 2019.

\bibitem[Blanchard et~al.(2011)Blanchard, Lee, and Scott]{blanchard2011generalizing}
Blanchard, G., Lee, G., and Scott, C.
\newblock Generalizing from several related classification tasks to a new unlabeled sample.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Chen et~al.(2024)Chen, Lei, and Wei]{chen2024polygcl}
Chen, J., Lei, R., and Wei, Z.
\newblock Polygcl: Graph contrastive learning via learnable spectral polynomial filters.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Cheng et~al.(2024)Cheng, Xu, Jiang, Wang, Li, and Gao]{cheng2024disentangled}
Cheng, D., Xu, Z., Jiang, X., Wang, N., Li, D., and Gao, X.
\newblock Disentangled prompt representation for domain generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  23595--23604, 2024.

\bibitem[Chien et~al.(2021)Chien, Peng, Li, and Milenkovic]{chien2021adaptive}
Chien, E., Peng, J., Li, P., and Milenkovic, O.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and Hon]{dong2019unified}
Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H.-W.
\newblock Unified language model pre-training for natural language understanding and generation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Ma, Li, Xia, Xu, Wu, Liu, et~al.]{dong2022survey}
Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Liu, T., et~al.
\newblock A survey on in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Ebesu \& Fang(2017)Ebesu and Fang]{ebesu2017neural}
Ebesu, T. and Fang, Y.
\newblock Neural citation network for context-aware citation recommendation.
\newblock In \emph{Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval}, pp.\  1093--1096, 2017.

\bibitem[Fang et~al.(2024)Fang, Zhang, Yang, Wang, and Chen]{fang2024universal}
Fang, T., Zhang, Y., Yang, Y., Wang, C., and Chen, L.
\newblock Universal prompt tuning for graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fatemi et~al.(2021)Fatemi, El~Asri, and Kazemi]{fatemi2021slaps}
Fatemi, B., El~Asri, L., and Kazemi, S.~M.
\newblock Slaps: Self-supervision improves structure learning for graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22667--22681, 2021.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
Gao, T., Fisch, A., and Chen, D.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Hassani(2022)]{hassani2022cross}
Hassani, K.
\newblock Cross-domain few-shot graph classification.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  6856--6864, 2022.

\bibitem[Hu et~al.(2020)Hu, Dong, Wang, Chang, and Sun]{hu2020gpt}
Hu, Z., Dong, Y., Wang, K., Chang, K.-W., and Sun, Y.
\newblock Gpt-gnn: Generative pre-training of graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining}, pp.\  1857--1867, 2020.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Derr, Liu, Wang, Wang, Liu, and Tang]{jin2020self}
Jin, W., Derr, T., Liu, H., Wang, Y., Wang, S., Liu, Z., and Tang, J.
\newblock Self-supervised learning on graphs: Deep insights and new direction.
\newblock \emph{arXiv preprint arXiv:2006.10141}, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Ma, Liu, Tang, Wang, and Tang]{jin2020graph}
Jin, W., Ma, Y., Liu, X., Tang, X., Wang, S., and Tang, J.
\newblock Graph structure learning for robust graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining}, pp.\  66--74, 2020{\natexlab{b}}.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Li et~al.(2022)Li, Zhang, Wang, and Zhu]{li2022learning}
Li, H., Zhang, Z., Wang, X., and Zhu, W.
\newblock Learning invariant graph representations for out-of-distribution generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 11828--11841, 2022.

\bibitem[Li et~al.(2024)Li, Sun, Luo, Zhu, Chen, Luo, Zhou, Liu, Wu, Wang, et~al.]{li2024gslb}
Li, Z., Sun, X., Luo, Y., Zhu, Y., Chen, D., Luo, Y., Zhou, X., Liu, Q., Wu, S., Wang, L., et~al.
\newblock Gslb: the graph structure learning benchmark.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Liang et~al.(2023)Liang, Deng, Ma, Zou, Morency, and Salakhutdinov]{liang2024factorized}
Liang, P.~P., Deng, Z., Ma, M.~Q., Zou, J.~Y., Morency, L.-P., and Salakhutdinov, R.
\newblock Factorized contrastive learning: Going beyond multi-view redundancy.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Lim et~al.(2021)Lim, Hohne, Li, Huang, Gupta, Bhalerao, and Lim]{lim2021large}
Lim, D., Hohne, F., Li, X., Huang, S.~L., Gupta, V., Bhalerao, O., and Lim, S.~N.
\newblock Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20887--20902, 2021.

\bibitem[Liu et~al.(2022)Liu, Zheng, Zhang, Chen, Peng, and Pan]{liu2022towards}
Liu, Y., Zheng, Y., Zhang, D., Chen, H., Peng, H., and Pan, S.
\newblock Towards unsupervised deep graph structure learning.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pp.\  1392--1403, 2022.

\bibitem[Liu et~al.(2024)Liu, Ding, Lu, Li, Zhang, and Pan]{liu2024towards}
Liu, Y., Ding, K., Lu, Q., Li, F., Zhang, L.~Y., and Pan, S.
\newblock Towards self-interpretable graph-level anomaly detection.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Namata et~al.(2012)Namata, London, Getoor, Huang, and Edu]{namata2012query}
Namata, G., London, B., Getoor, L., Huang, B., and Edu, U.
\newblock Query-driven active surveying for collective classification.
\newblock In \emph{10th international workshop on mining and learning with graphs}, volume~8, pp.\ ~1, 2012.

\bibitem[Pei et~al.(2020)Pei, Wei, Chang, Lei, and Yang]{pei2020geom}
Pei, H., Wei, B., Chang, K. C.-C., Lei, Y., and Yang, B.
\newblock Geom-gcn: Geometric graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:2002.05287}, 2020.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and Eliassi-Rad]{sen2008collective}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T.
\newblock Collective classification in network data.
\newblock \emph{AI magazine}, 29\penalty0 (3):\penalty0 93--93, 2008.

\bibitem[Shen et~al.(2024)Shen, Wang, and Kang]{shen2024beyond}
Shen, Z., Wang, S., and Kang, Z.
\newblock Beyond redundancy: Information-aware unsupervised multiplex graph structure learning.
\newblock \emph{Advances in neural information processing systems}, 2024.

\bibitem[Sun et~al.(2022{\natexlab{a}})Sun, Zhou, He, Wang, and Wang]{sun2022gppt}
Sun, M., Zhou, K., He, X., Wang, Y., and Wang, X.
\newblock Gppt: Graph pre-training and prompt tuning to generalize graph neural networks.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  1717--1727, 2022{\natexlab{a}}.

\bibitem[Sun et~al.(2023)Sun, Cheng, Li, Liu, and Guan]{sun2023all}
Sun, X., Cheng, H., Li, J., Liu, B., and Guan, J.
\newblock All in one: Multi-task prompting for graph neural networks.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  2120--2131, 2023.

\bibitem[Sun et~al.(2022{\natexlab{b}})Sun, Deng, Yang, Wang, Xu, Huang, Cao, Wang, and Chen]{sun2022beyond}
Sun, Y., Deng, H., Yang, Y., Wang, C., Xu, J., Huang, R., Cao, L., Wang, Y., and Chen, L.
\newblock Beyond homophily: Structure-aware path aggregation graph neural network.
\newblock In \emph{IJCAI}, pp.\  2233--2240, 2022{\natexlab{b}}.

\bibitem[Traud et~al.(2012)Traud, Mucha, and Porter]{traud2012social}
Traud, A.~L., Mucha, P.~J., and Porter, M.~A.
\newblock Social structure of facebook networks.
\newblock \emph{Physica A: Statistical Mechanics and its Applications}, 391\penalty0 (16):\penalty0 4165--4180, 2012.

\bibitem[Velickovic et~al.(2017)Velickovic, Cucurull, Casanova, Romero, Lio, Bengio, et~al.]{velickovic2017graph}
Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., et~al.
\newblock Graph attention networks.
\newblock \emph{stat}, 1050\penalty0 (20):\penalty0 10--48550, 2017.

\bibitem[Velickovic et~al.(2019)Velickovic, Fedus, Hamilton, Li{\`o}, Bengio, and Hjelm]{velickovic2019deep}
Velickovic, P., Fedus, W., Hamilton, W.~L., Li{\`o}, P., Bengio, Y., and Hjelm, R.~D.
\newblock Deep graph infomax.
\newblock \emph{ICLR (Poster)}, 2\penalty0 (3):\penalty0 4, 2019.

\bibitem[Vidit et~al.(2023)Vidit, Engilberge, and Salzmann]{vidit2023clip}
Vidit, V., Engilberge, M., and Salzmann, M.
\newblock Clip the gap: A single domain generalization approach for object detection.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  3219--3229, 2023.

\bibitem[Xia et~al.(2022)Xia, Wu, Chen, Hu, and Li]{xia2022simgrace}
Xia, J., Wu, L., Chen, J., Hu, B., and Li, S.~Z.
\newblock Simgrace: A simple framework for graph contrastive learning without data augmentation.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pp.\  1070--1079, 2022.

\bibitem[Yan et~al.(2024)Yan, Zhang, Fang, and Long]{yan2024inductive}
Yan, Y., Zhang, P., Fang, Z., and Long, Q.
\newblock Inductive graph alignment prompt: Bridging the gap between graph pre-training and inductive fine-tuning from spectral perspective.
\newblock In \emph{Proceedings of the ACM on Web Conference 2024}, pp.\  4328--4339, 2024.

\bibitem[Yao et~al.(2022)Yao, Bai, Zhang, Zhang, Sun, Chen, Li, and Yu]{yao2022pcl}
Yao, X., Bai, Y., Zhang, X., Zhang, Y., Sun, Q., Chen, R., Li, R., and Yu, B.
\newblock Pcl: Proxy-based contrastive learning for domain generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7097--7107, 2022.

\bibitem[You et~al.(2020)You, Chen, Sui, Chen, Wang, and Shen]{you2020graph}
You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock Graph contrastive learning with augmentations.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 5812--5823, 2020.

\bibitem[Yu et~al.(2025)Yu, Gong, Zhou, Fang, and Zhang]{yu2025samgpt}
Yu, X., Gong, Z., Zhou, C., Fang, Y., and Zhang, H.
\newblock Samgpt: Text-free graph foundation model for multi-domain pre-training and cross-domain adaptation.
\newblock In \emph{THE WEB CONFERENCE 2025}, 2025.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Liu, Wang, Shi, Chen, Li, Li, Li, and Ding]{zhang2024survey}
Zhang, K., Liu, S., Wang, S., Shi, W., Chen, C., Li, P., Li, S., Li, J., and Ding, K.
\newblock A survey of deep graph learning under distribution shifts: from graph out-of-distribution generalization to adaptation.
\newblock \emph{arXiv preprint arXiv:2410.19265}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Liang, Liu, and Tang]{zhang2021graph}
Zhang, X.-M., Liang, L., Liu, L., and Tang, M.-J.
\newblock Graph neural networks and their current applications in bioinformatics.
\newblock \emph{Frontiers in genetics}, 12:\penalty0 690049, 2021.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Hu, Chen, Guo, Liu, Zhang, Liang, Chen, and Zhang]{zhang2024multi}
Zhang, Y., Hu, B., Chen, Z., Guo, L., Liu, Z., Zhang, Z., Liang, L., Chen, H., and Zhang, W.
\newblock Multi-domain knowledge graph collaborative pre-training and prompt tuning for diverse downstream tasks.
\newblock \emph{arXiv preprint arXiv:2405.13085}, 2024{\natexlab{b}}.

\bibitem[Zhao et~al.(2024)Zhao, Chen, Sun, Cheng, and Li]{zhao2024all}
Zhao, H., Chen, A., Sun, X., Cheng, H., and Li, J.
\newblock All in one and one for all: A simple yet effective method towards cross-domain graph pretraining.
\newblock In \emph{Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  4443--4454, 2024.

\bibitem[Zheng et~al.(2022)Zheng, Wang, Liu, Li, Zhang, Jin, Yu, and Pan]{zheng2022graph}
Zheng, X., Wang, Y., Liu, Y., Li, M., Zhang, M., Jin, D., Yu, P.~S., and Pan, S.
\newblock Graph neural networks for graphs with heterophily: A survey.
\newblock \emph{arXiv preprint arXiv:2202.07082}, 2022.

\bibitem[Zhou et~al.(2022)Zhou, Liu, Qiao, Xiang, and Loy]{zhou2022domain}
Zhou, K., Liu, Z., Qiao, Y., Xiang, T., and Loy, C.~C.
\newblock Domain generalization: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45\penalty0 (4):\penalty0 4396--4415, 2022.

\bibitem[Zhu et~al.(2020)Zhu, Yan, Zhao, Heimann, Akoglu, and Koutra]{zhu2020beyond}
Zhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and Koutra, D.
\newblock Beyond homophily in graph neural networks: Current limitations and effective designs.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7793--7804, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Xu, Zhang, Liu, Wu, and Wang]{zhu2021deep}
Zhu, Y., Xu, W., Zhang, J., Liu, Q., Wu, S., and Wang, L.
\newblock Deep graph structure learning for robust representations: A survey.
\newblock \emph{arXiv preprint arXiv:2103.03036}, 14:\penalty0 1--1, 2021.

\bibitem[Z{\"u}gner et~al.(2020)Z{\"u}gner, Borchert, Akbarnejad, and G{\"u}nnemann]{zugner2020adversarial}
Z{\"u}gner, D., Borchert, O., Akbarnejad, A., and G{\"u}nnemann, S.
\newblock Adversarial attacks on graph neural networks: Perturbations and their patterns.
\newblock \emph{ACM Transactions on Knowledge Discovery from Data (TKDD)}, 14\penalty0 (5):\penalty0 1--31, 2020.

\bibitem[Zügner \& Günnemann(2019)Zügner and Günnemann]{zügner2018adversarial}
Zügner, D. and Günnemann, S.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\appendix
\onecolumn



\section{Notations}\label{NotationsApp}


\begin{table}[h]
\centering
\begin{minipage}{0.85\linewidth} 
\caption{Frequently used notations.}
\vskip 0.1in
\centering
\resizebox{0.75\linewidth}{!}
{
\begin{tabular}{l|l}
\toprule

\textbf{Notation }                                  & \textbf{Description}                                     \\
\midrule

$G_i=(A_i,X_i^{ori})$ & The $i$-th original input graph. \\
$G_{i1}=(A_i,X_i^{\prime})$ & Graph $i$ after feature projection. \\
$G_{i2}=(A_i^{\prime},X_i^{\prime})$ & Graph $i$ after feature projection topology aware alignment. \\
$T=(A_T,X_T)$ & The downstream target graph. \\

$X_i^{ori}$ & Features of the original input graph. \\
$X_i$ & Graph features after dimension alignment. \\
$X_i^{\prime}$ & Features after feature projection. \\

$Y$ & The set of all possible class labels. \\
% $y_i$ & Class label of node $i$. \\

% $A$ & The adjacency matrix of the original input graph \\
% $A_i$ & The adjacency matrix of input graph $i$. \\
$r$ & The order of graph aggregation. \\
$d^{\prime}$ & Dimension of the original node features. \\
$d$ & The unified dimension. \\
$V$ & The set of nodes. \\
$E$ & The set of edges. \\
$Z$ & Node embeddings. \\
$D$ & Set of source domains. \\
$D_{i}$ & The $i$-th domain from the source domain set. \\
$D_{T}$ & Target domain that does not belong to the source domain set. \\

% $S$ & A labeled training set. \\

\midrule
$Proj(\cdot)$ & A specific projection operation, here we use PCA. \\
$\sigma(\cdot)$ & Non-linearity activation function. \\
$\mathcal{L}$ & Loss function. \\
$\mathcal{L}_{dst}$ & Downstream loss function. \\
$sim(\cdot, \cdot)$ & Cosine similarity. \\
$\odot(\cdot)$ & Element-wise multiplication. \\

\midrule
$t_{D_i}$ & Domain token for domain $D_{i}$.\\
$t_{S}$ & Shared token. \\
$t_{B_i}$ & Balance token for domain $D_i$. \\
$p_m$ & Meta prompt. \\
$p_s$ & Specific prompt. \\
${\alpha}_i$ & Trainable coefficients for source domain $i$. \\
$\beta$ & A learnable parameter used for integrating prompts. \\
\bottomrule
\end{tabular}
}
\label{Notation_table}
\end{minipage}
\end{table}









\section{Datasets}\label{datasets_appendix}

\begin{table}[h]
\centering
\caption{Statistics of Datasets.}
\vskip 0.1in
\resizebox{0.7\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|c}
\toprule
Datasets  & Nodes & Edges  & Feature dimension & Node classes & Homophily ratio \\
\midrule
Cora      & 2,708 & 10,556 & 1,433               & 7           & 0.810     \\
Citeseer  & 3,327  & 9,104   & 3,703              & 6            & 0.736     \\
Pubmed    & 19,717 & 88,648  & 500               & 3            & 0.802     \\
\midrule
Squirrel  & 5,201  & 396,846 & 2,089              & 5            & 0.222     \\
Chameleon & 2,277  & 62,792  & 2,325              & 5            & 0.231     \\
Cornell   & 183  & 298  & 1,703               & 5            & 0.305     \\
\midrule
Penn94    &41,554 & 2,724,458 &4,814          &    2        &0.510 \\
\bottomrule
\end{tabular}
}
\label{Statistics_of_Datasets}
\end{table}

We provide an in-depth explanation of datasets, where their statistics details are presented in Table \ref{Statistics_of_Datasets}.

\textbf{Citation network}: The Cora and Citeseer \cite{sen2008collective} datasets represent a varied collection of scholarly articles in the field of computer science, where each node is associated with bag-of-words features and a categorical label denoting the corresponding topic of the paper. In contrast, the Pubmed \cite{namata2012query} dataset consists of articles focused on diabetes sourced from the PubMed database. Each node in this dataset is characterized by an attribute vector, along with a label that identifies the specific type of diabetes addressed in the publication.

\textbf{WebKB}: Cornell \cite{pei2020geom} is a subset derived from the WebKB dataset. In this dataset, each node corresponds to a web page, while the edges illustrate the hyperlinks that connect these pages. The features of nodes are expressed using bag-of-words features derived from the content of the web pages. Nodes are classified into five distinct labels: student, project, course, staff, and faculty.

\textbf{Wikipedia network}: The Chameleon and Squirrel \cite{pei2020geom} datasets comprise two page-to-page networks sourced from Wikipedia, each centered on specific themes. Nodes signify individual web pages, and edges represent their connections. Node attributes are characterized by collections of nouns gathered from the content of the pages. Additionally, each node is categorized according to the average monthly traffic that the corresponding web page receives.

\textbf{Online social network}: The Penn94 \cite{traud2012social} dataset is a large-scale social network derived from the Facebook 100 networks of university students from 2005, where each node corresponds to an individual student. The nodes are annotated with the reported gender of each user, and the objective is to predict this gender. The features associated with the nodes include major, secondary major/minor, dorm/house, year, and high school \cite{lim2021large}.
The homophily ratio of the Penn94 dataset is 0.51, indicating that its graph topology exhibits a mixture of homophilic and heterophilic patterns. Previous research has shown that such topological information is nearly meaningless, making it difficult for existing topology-fixed GNNs to adapt effectively to this type of graph \cite{chien2021adaptive,chen2024polygcl}.




\section{Additional Experiments}\label{additional}
\subsection{Expansion of Robustness Experiments}

We perform robustness experiments on the Cornell dataset before. To investigate whether the MDGFM model retains its robustness under the condition where the downstream target domain consists of homophilic graphs, we replicate the robustness experiments previously described on the Cora dataset, performance under metaattack is shown in the main text. Specifically, we apply random edge additions and deletions on all domains. We compare our results with those of MDGPT and GCOPE, as summarized in Figure \ref{Corarobust}. Our observations indicate that our model maintains commendable robustness when evaluated on homophilic graph datasets. Furthermore, we note that in the context of heterophilic graph datasets, GCOPE exhibits greater robustness compared to MDGPT, while the opposite trend is observed for homophilic graphs.



\begin{figure}[t]
	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{outputaddcora.pdf}
		\caption{Adding edges }
	\end{subfigure}
     \hspace{0.05\linewidth}
 	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{outputremovecora.pdf}
		\caption{Deleting edges }
	\end{subfigure}
 %    \hspace{0.02\linewidth}
 % 	\centering
	% \begin{subfigure}{0.31\linewidth}
	% 	\centering
	% 	\includegraphics[width=1.\linewidth]{outputmetacora.pdf}
	% 	\caption{Through meta-attack.}
	% \end{subfigure}
	
	\caption{Performance of robustness analysis, where Cora is the target domain.}
	\label{Corarobust}
\end{figure}

\subsection{Sensitivity Analysis}

We investigate the impact of the hyperparameter $k$ in $k$-nearest neighbors ($k$NN) for downstream graph structure learning, which is crucial to our model. The performance of our model varies with changes in the hyperparameter $k$ as illustrated in Figure \ref{ksense}. Overall, our model exhibits low sensitivity to changes in $k$, regardless of whether the downstream target domain is homophilic or heterophilic. When the target domain is homophilic graphs, larger values of $k$ (such as 20 or 30) yield the best results. In contrast, for heterophilic graphs, relatively smaller values of $k$ can produce satisfactory outcomes.


\begin{figure}[t]
	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{sensekhomo.pdf}
		\caption{Homophilic graphs}
	\end{subfigure}
     \hspace{0.03\linewidth}
 	\centering
	\begin{subfigure}{0.35\linewidth}
		\centering
		\includegraphics[width=1.\linewidth]{sensekheter.pdf}
		\caption{Heterophillic graphs}
	\end{subfigure}
	
	\caption{Sensitivity study of $k$.}
	\label{ksense}
\end{figure}

% \subsection{$K$-shot Node Classification on Penn94}\label{penn94}

% To evaluate the generalization capability of our model, we conduct extensive \( K \)-shot experiments on the large-scale Penn94 dataset, where \( K \) is selected from \(\{1,5,10,50,100,500\}\). As shown in Table \ref{Penn94Kshot}, our model consistently achieves the best performance across all \( K \) values, with the performance improvement widening as \( K \) increases, demonstrating both its learning capacity and knowledge transfer ability. Given that the edges in Penn94 contain limited useful information and are heavily noisy (homophily ratio = 0.51), previous topology-fixed methods like GCOPE and MDGPT struggle to learn effectively. This further highlights the advantage of our GSL-based topology alignment approach.




\section{Experimental Details}\label{surd}

For both one-shot and few-shot classification tasks, we pretrain the models on five datasets and subsequently perform predictions on the remaining dataset, ensuring that the downstream domain remains unseen during the training phase. The detailed experimental setup is summarized in Table \ref{Settings_multi_domain_transfer}, where a checkmark (\(\surd\)) indicates visibility during pre-training, while the absence of the mark denotes invisibility. For the large-scale dataset Penn94, it is exclusively used as the target domain to evaluate the generalization capability of the model.

All experiments are conducted on a platform equipped with an Intel(R) Xeon(R) Gold 5220 CPU and an NVIDIA A800 80GB GPU, using PyTorch 1.10.1 and DGL 0.9.1. Each experiment is run five times and the average results are reported. We employ the Adam optimizer and set the batch size to 128. During the upstream pre-training phase, we utilize Principal Component Analysis (PCA) to reduce the dimensionality of the initial features to 50 dimensions, thereby unifying the features from multiple source domains. For homogeneous graphs, we set $k$=30 for Graph Structure Learning (GSL), while for heterophilic graphs, we configure $k$=15. Additionally, we adjust the upstream and downstream learning rates across different datasets, as detailed in Table \ref{hyperparameters_datasets}. We fix the number of graph neural layers to 3, with a hidden dimension of 256 for the GCN model. When dealing with homophilic graphs, the number of Epochs is set to 60, whereas it is set to 100 for heterophilic graphs. For the downstream node classification tasks, we implement few-shot learning scenarios with 1-shot and 5-shot settings (3-shot for the Cornell and Squirrel datasets).  
We perform 50 resampling iterations in downstream few-shot classification. The $k$ values required for graph construction adhere to the configurations established during the upstream pre-training phase.

\begin{table}[t]
\centering
\caption{Settings of multi-domain transfer.}
\vskip 0.1in
\resizebox{0.6\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Target domain & Cora    & Citeseer & Pubmed  & Squirrel & Chameleon & Cornell \\
\midrule
Cora          &         & $\surd$  & $\surd$ & $\surd$  & $\surd$   & $\surd$ \\

Citeseer      & $\surd$ &          & $\surd$ & $\surd$  & $\surd$   & $\surd$ \\

Pubmed        & $\surd$ & $\surd$  &         & $\surd$  & $\surd$   & $\surd$ \\
\midrule
Squirrel      & $\surd$ & $\surd$  & $\surd$ &          & $\surd$   & $\surd$ \\

Chameleon     & $\surd$ & $\surd$  & $\surd$ & $\surd$  &           & $\surd$ \\

Cornell       & $\surd$ & $\surd$  & $\surd$ & $\surd$  & $\surd$   &         \\
\midrule
Penn94        & $\surd$ & $\surd$  & $\surd$ & $\surd$  & $\surd$   & $\surd$ \\
\bottomrule
\end{tabular}
}
\label{Settings_multi_domain_transfer}
\end{table}


\begin{table}[t]
\centering
\caption{The hyperparameters corresponding to each dataset.}
\vskip 0.1in
\resizebox{0.9\linewidth}{!}
{
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
Target domain & Pre-training learning rate & Downstream learning rate & Epoch &Unified dimension & Dropout & Downstream $k$ \\
\midrule
Cora & 0.0075 & 0.001 & 60 & 50 & 0.1 & 30 \\

Citeseer & 0.001 & 0.001 & 60 & 50 & 0.1 & 30 \\

Pubmed & 0.0001 & 0.0015 & 60 & 50 & 0.1 & 30 \\
\midrule
Squirrel & 0.01 & 0.0003 & 100 & 50 & 0.1 & 15 \\

Chameleon & 0.02 & 0.01 & 100 & 50 & 0.1 & 15 \\

Cornell & 0.02 & 0.0003 & 100 & 50 & 0.1 & 15 \\
\midrule
Penn94 & 0.0001 & 0.003 & 100 & 50 & 0.1 & 30 \\
\bottomrule
\end{tabular}
}
\label{hyperparameters_datasets}
\end{table}


\section{Methodology Details}\label{processors}

Following the approach proposed in previous works \cite{liu2022towards,shen2024beyond}, after constructing the cosine similarity matrix of $H_i$, we implement post-processing techniques to ensure that $A_i^{\prime}$ exhibits the characteristics of sparsity, non-negativity, symmetry, and normalization. For the convenience of discussion, the subscript $i$ is omitted hereinafter.

\textbf{$k$NN for sparsity}. In most applications, a fully connected adjacency matrix often has limited practical significance and incurs high computational costs. Therefore, we employ the $k$-Nearest Neighbors ($k$NN) operation to sparsify the learned graph. For each node, we retain the edges with the top-$k$ values and set the others to 0, thereby obtaining the sparse adjacency matrix $A^{sp}$. Note that we employ efficient $k$NN with locality-sensitive hashing \cite{fatemi2021slaps} to enhance the model's scalability. This approach avoids the resource-intensive computation and storage of explicit similarity matrices, reducing the complexity from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|B)$, where $|V|$ is the number of nodes and $B$ is the batch size of the sparse $k$NN.

 \textbf{Symmetrization and Activation}. Since real-world connections are typically bidirectional, we symmetrize the adjacency matrix. Additionally, the weight of each edge should be non-negative. Given the input $A^{sp}$, these operations can be expressed as follows:
\begin{equation}
    A^{sym}=\frac{\sigma(A^{sp})+\sigma(A^{sp})^{\top}}{2}
\end{equation}
where $\sigma(\cdot)$ represents a non-linear activation implemented by the ReLU function.

\textbf{Normalization}. The normalized adjacency matrix with self-loops can be obtained as follows:
\begin{equation}
    A^{\prime}=(\tilde{D}^{sym})^{-\frac{1}{2}}\tilde{A}^{sym}(\tilde{D}^{sym})^{-\frac{1}{2}}
\end{equation}
where $\tilde{D}^{sym}$ is the degree matrix of $\tilde{A}^{sym}$ with self-loops. Subsequently, for each view, we can acquire the adjacency matrix $A^{\prime}_i$, which possesses the desirable properties of sparsity, non-negativity, symmetry, and normalization. 

\section{Details of Loss Functions}\label{mutual}

Note that directly computing mutual information between two graphs is impractical due to the complexity of graph-structured data. Since we focus on node-level tasks, we assume the optimized graph should guarantee that each node's neighborhood substructure contains sufficient task-relevant information. Therefore, this requirement can be transferred to mutual information between node representations \cite{liu2024towards}, which can be easily computed using a sample-based differentiable lower bound. For any view $i$ and $j$, the lower bound $I_{lb}$ of the mutual information $I( Z^{i}; Z^{j})$ is \cite{liang2024factorized}:

\begin{equation}\label{lower_bound_equ}
  I_{lb}(Z^{i};Z^{j})= \mathbb{E}_{\substack{{z^{i},z^{j+}}\sim p(z^{i},z^{j}) \\ z^{j}  \sim p(z^{j})} }\left[log\frac{exp f(z^{i},z^{j+})}{ {\textstyle \sum_{N}exp f(z^{i},z^{j})} } \right]
\end{equation}
where $f(\cdot,\cdot)$ is a score critic approximated by a neural network. $p(z^{i},z^{j})$ denotes the joint distribution of node representations from views $i$ and $j$, while $p(z^{i})$ denotes the marginal distribution. $z^i$ and $z^{j+}$ are mutually positive samples, representing the representations of the same node in views $i$ and $j$ respectively.

Note that in our work, we redefine $I(G_{i1};G_{i2}\dagger 
 A_i^{\prime})$, where $A_i^\prime$ is the refined adjacency matrix to identify the positive samples. That is to say, $z^i$ and $z^{j+}$ are mutually positive samples if and only if node $i$ and node $j+$ are neighbors in matrix $A_i^\prime$. Recall the following loss function in the pre-training phase:
\begin{equation}
    \mathcal{L}=-I(G_{i1};G_{i2}\dagger 
 I_e)-I(G_{i1};G_{i2}\dagger 
 A_i^{\prime})
\end{equation}
where $I_e$ is the identity matrix. Particularly, the second term incorporates the refined graph structure $A_i^{\prime}$ to increase the number of positive samples. The minimization objective $\mathcal{L}$ in multi-domain pre-training is calculated as follows:

\begin{equation}
    \ell_{I_e}(Z^{i1}_m, Z^{i2}_m)=log \frac{e^{sim(\tilde {Z}_m^{i1},\tilde Z^{i2}_m)/\tau_c }}{\sum_{n=1}^{|V|} e^{sim(\tilde Z_m^{i1},\tilde Z^{i2}_n)/\tau_c }},  \quad     \ell_{A^\prime_i}(Z^{i1}_m, Z^{i2}_m)=log \frac{\sum_{\substack{n=1,A_i^\prime[m,n]\neq 0}}^{|V|} A_i^\prime [m,n]\cdot e^{sim(\tilde {Z}_m^{i1},\tilde Z^{i2}_m)/\tau_c }}{\sum_{n=1}^{|V|} e^{sim(\tilde Z_m^{i1},\tilde Z^{i2}_n)/\tau_c }}
\end{equation}
\begin{equation}
    \hat{\mathcal{L}}=-\frac{1}{2} \sum_{m=1}^{|V|}(\ell_{I_e}(Z^{i1}_m,Z^{i2}_m) +\ell_{I_e}(Z^{i2}_m,Z^{i1}_m))-\frac{1}{2} \sum_{m=1}^{|V|}(\ell_{A^\prime_i}(Z^{i1}_m,Z^{i2}_m) +\ell_{A^\prime_i}(Z^{i2}_m,Z^{i1}_m))
\end{equation}
where $Z^{i1}$ denotes the node representations of the original graph $G_{i1} = (A_i, X^\prime_i)$ obtained through the graph encoder, and $Z^{i2}$ denotes the node representations of the refined graph $G_{i2} = (A_i^\prime, X^\prime_i)$. $\tilde Z_m^{i1}$ is the non-linear projection of $Z_m^{i1}$.
$A^\prime_i[m,n]$ denotes the element in the $m$-th row and $n$-th column of the adjacency matrix $A^\prime_i$, which corresponds to the edge weights of the learned structure via GSL. $sim(\cdot,\cdot)$ denotes the cosine similarity. $\tau_c$ is the temperature parameter, which is fixed at $0.2$ in this work. During the pre-training phase, we sequentially minimize the objective $\hat{\mathcal{L}}$ for each input graph with different domains.


% \section{Additional Experiments}\label{additional}

% To better evaluate the robustness capacity of our proposed MDGFM, we conduct additional experiments on a homophilic dataset Cora. Following the experimental settings on Cornell, we add noises and attacks on all source domains, the results are summarized as follows:






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
