\section{Related work}
\subsection{Graph Pre-training}

Graph pre-training aims to train Graph Neural Networks (GNNs) on large amounts of unlabeled data, enabling the transfer of the learned model to downstream tasks with limited supervision. This approach facilitates the acquisition of general knowledge about real-world graphs while reducing the dependence on labeled data \cite{hu2020gpt, jin2020self}.

Pre-training methods can be broadly categorized based on their downstream task-tuning strategies. The first category follows a pretrain-and-fine-tune paradigm, emphasizing effective pre-training strategies in the upstream phase through self-supervised learning. For example, DGI \cite{velickovic2019deep} enhances training by maximizing the mutual information between global and local representations. Similarly, GraphCL \cite{you2020graph} and SimGRACE \cite{xia2022simgrace} focus on minimizing the distance between representations of different augmentations, effectively capturing invariant and robust structural information.

The second type follows a pretrain-and-prompt-tuning paradigm, where pre-trained models are not fine-tuned for downstream tasks. Instead, these methods reformulate the input data to align with the pretext task \cite{gao2020making}. For instance, GPPT \cite{sun2022gppt} introduces a graph prompting function that transforms independent nodes into token pairs, reframing downstream node classification as an edge prediction task. Similarly, GPF \cite{fang2024universal} employs learnable perturbations in the feature space of downstream graphs, enabling implicit modifications to node features and graph structures. However, most existing methods are constrained to single-domain pre-training and tuning, significantly limiting their capacity to capture cross-domain knowledge and generalize to unseen domains.


\subsection{Multi-domain Generalization}

Domain generalization aims to achieve out-of-distribution (OOD) generalization by learning from multiple source domains \cite{zhou2022domain}. In contrast to domain adaptation—which transfers prior knowledge from a single source domain to a specific target domain—domain generalization focuses on leveraging diverse information from multiple source domains to generalize effectively to unseen domains. This approach addresses two critical challenges: domain shift and the absence of target domain data \cite{blanchard2011generalizing}.

Recently, domain generalization on graphs has gained attention. These methods integrate and extract knowledge from multiple source domains during the upstream pre-training phase, enabling the transfer of this knowledge to tackle various graph-related tasks in previously unseen downstream domains.
For example, GCOPE \cite{zhao2024all} integrates multi-source graph topologies during the pre-training phase by introducing interconnected virtual nodes. Additionally, MDGPT \cite{yu2025samgpt} incorporates domain-specific tokens in the pre-training phase to align node features from different domains, and employs prompt-tuning during downstream tasks for efficient knowledge transfer. Despite these advancements, significant gaps remain in understanding the semantic differences and reliability of multi-domain topologies. Consequently, there is an urgent need to design a universal foundation model capable of achieving robust and generalized knowledge transfer across domains.