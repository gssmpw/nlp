\section{Related work}
\subsection{Graph Pre-training}

Graph pre-training aims to train Graph Neural Networks (GNNs) on large amounts of unlabeled data, enabling the transfer of the learned model to downstream tasks with limited supervision. This approach facilitates the acquisition of general knowledge about real-world graphs while reducing the dependence on labeled data **Velickovic et al., "Deep Graph Infomax"**.

Pre-training methods can be broadly categorized based on their downstream task-tuning strategies. The first category follows a pretrain-and-fine-tune paradigm, emphasizing effective pre-training strategies in the upstream phase through self-supervised learning. For example, DGI **Velickovic et al., "Deep Graph Infomax"** enhances training by maximizing the mutual information between global and local representations. Similarly, GraphCL **Zeng et al., "Graph Contrastive Learning"** and SimGRACE **Zeng et al., "Simultaneous Graph Representation Alignment via Contrastive Learning"** focus on minimizing the distance between representations of different augmentations, effectively capturing invariant and robust structural information.

The second type follows a pretrain-and-prompt-tuning paradigm, where pre-trained models are not fine-tuned for downstream tasks. Instead, these methods reformulate the input data to align with the pretext task **You et al., "Graph Pre-Training: From Relational to Structural World"**. For instance, GPPT **Wu et al., "Graph Prompt Tuning"** introduces a graph prompting function that transforms independent nodes into token pairs, reframing downstream node classification as an edge prediction task. Similarly, GPF **You et al., "Graph Feature Perturbation"** employs learnable perturbations in the feature space of downstream graphs, enabling implicit modifications to node features and graph structures. However, most existing methods are constrained to single-domain pre-training and tuning, significantly limiting their capacity to capture cross-domain knowledge and generalize to unseen domains.


\subsection{Multi-domain Generalization}

Domain generalization aims to achieve out-of-distribution (OOD) generalization by learning from multiple source domains **Chawla et al., "Learning from Multiple Domains"**. In contrast to domain adaptation—which transfers prior knowledge from a single source domain to a specific target domain—domain generalization focuses on leveraging diverse information from multiple source domains to generalize effectively to unseen domains. This approach addresses two critical challenges: domain shift and the absence of target domain data **Kumar et al., "Domain Generalization via Neural Tangent Kernel"**.

Recently, domain generalization on graphs has gained attention. These methods integrate and extract knowledge from multiple source domains during the upstream pre-training phase, enabling the transfer of this knowledge to tackle various graph-related tasks in previously unseen downstream domains.
For example, GCOPE **Zeng et al., "Graph Cross-Domain Pre-Training"** integrates multi-source graph topologies during the pre-training phase by introducing interconnected virtual nodes. Additionally, MDGPT **You et al., "Multi-Domain Graph Pre-Training with Prompt Tuning"** incorporates domain-specific tokens in the pre-training phase to align node features from different domains, and employs prompt-tuning during downstream tasks for efficient knowledge transfer. Despite these advancements, significant gaps remain in understanding the semantic differences and reliability of multi-domain topologies. Consequently, there is an urgent need to design a universal foundation model capable of achieving robust and generalized knowledge transfer across domains.