\begin{table*}
    \centering
    \begin{small}
    \begin{tabular}{ccrrrrr}
    \toprule
    \textbf{\tulu{}} & \textbf{Arith.} & \textbf{MultiArith} & \textbf{AddSub} & \textbf{SingleOp} & \textbf{SingleEq} & \textbf{SimulEq} \\
    \midrule
    \multicolumn{7}{c}{\it Greedy Decoding} \\
    \midrule
    \xmark & \xmark & 2.8 & 2.8 & 4.4 & 1.8 & 1.4 \\
    \cmark & \xmark & 39.4 & 11.9 & 45.3 & 43.1 & \textbf{6.8} \\
    \cmark & \cmark & \textbf{50.0} & \textbf{36.7} & \textbf{64.2} & \textbf{59.6} & 4.8 \\
    \midrule
    \multicolumn{7}{c}{\it Self-Consistency Decoding} \\
    \midrule
    \xmark & \xmark & 3.7 & 1.2 & 2.7 & 0.6 & 2.5 \\
    \cmark & \xmark & 56.1 & 18.7 & 51.4 & 47.7 & 5.3 \\
    \cmark & \cmark & \textbf{65.9} & \textbf{41.3} & \textbf{68.3} & \textbf{65.7} & \textbf{5.5} \\
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Accuracy (\%) achieved by the instruction-tuned GPT2-Large models on datasets in MAWPS. The first rows under both greedy and self-consistency decoding denote the pre-trained model.}
    \label{tab:mawps_detailed_results}
\end{table*}
