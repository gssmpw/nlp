\begin{table*}
    \centering
    \begin{small}
    \begin{tabular}{ccrrrrrr}
    \toprule
    \textbf{\tulu{}} & \textbf{Arith.} & \textbf{GSM8k} & \textbf{ASDiv} & \textbf{SVAMP} & \textbf{MAWPS} & \textbf{AQuA} & \textbf{GSM-Plus} \\
    \midrule
    \multicolumn{8}{c}{\it Greedy Decoding} \\
    \midrule
    \xmark & \xmark & 1.5 & 2.3 & 3.3 & 2.7 & 16.5 & 2.0 \\
    \cmark & \xmark & 15.0 & 24.5 & 19.9 & 30.3 & 18.1 & 7.0 \\
    \cmark & \cmark & \textbf{16.3} & \textbf{36.9} & \textbf{28.1} & \textbf{43.3} & \textbf{22.8} & \textbf{9.2} \\
    \midrule
    \multicolumn{8}{c}{\it Self-Consistency Decoding} \\
    \midrule
    \xmark & \xmark & 2.5 & 2.6 & 4.1 & 2.4 & 18.0 & 2.0 \\
    \cmark & \xmark & 17.3 & 30.9 & 26.4 & 37.4 & \textbf{21.5} & 9.4 \\
    \cmark & \cmark & \textbf{19.6} & \textbf{42.6} & \textbf{33.9} & \textbf{50.1} & 19.8 & \textbf{11.4} \\
    \bottomrule
    \end{tabular}
    \end{small}
    \caption{Accuracy (\%) achieved by GPT2-Large when instruction-tuned on the \tulu{} mixture with and without including the synthetic arithmetic dataset. The first rows under both greedy and self-consistency decoding denote the pre-trained model.}
    \label{tab:inst_tuning_results}
\end{table*}
