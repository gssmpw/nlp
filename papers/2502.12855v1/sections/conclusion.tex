\section{Conclusion}
In this work, we explored the impact of incorporating an arithmetic dataset through two approaches -- intermediate fine-tuning and integration within the instruction-tuning mixture. Our experiments demonstrated that both approaches boost mathematical reasoning performance in smaller models. While intermediate fine-tuning can make subsequent fine-tuning on other tasks more challenging, the issue can be mitigated by using a larger dataset. Additionally, we found that the model instruction tuned on a mixture including the arithmetic dataset outperformed the one trained without it. These findings highlight the crucial role of explicit arithmetic training in strengthening mathematical reasoning in smaller models.

% Our experiments showed that fine-tuning a model on a programmatically generated arithmetic dataset before a reasoning dataset helped improve the model's performance on the reasoning tasks. We evaluated our approach with small and large datasets, and intermediate fine-tuning resulted in better performance in both cases. Moreover, intermediate fine-tuning did not harm out-of-domain generalization, instead, models fine-tuned on the arithmetic dataset first showed better out-of-domain generalization. Finally, while this work does not offer a method for determining when to stop intermediate fine-tuning, initializing models from any checkpoints during the process yielded better results than fine-tuning them directly on GSM8k.
