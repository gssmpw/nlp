\section{Intermediate Fine-Tuning}
In this section, we present experiments with intermediate fine-tuning.

\subsection{Experimental Setup}
\paragraph{Tasks.}
We evaluate our approach on the GSM8k test set. We also test out-of-domain generalization using the following datasets -- (1) MultiArith \citep{roy-roth-2015-solving} with problems focused on basic arithmetic operations and relatively simpler than \gsm{}, (2) ASDiv \citep{miao-etal-2020-diverse} with diverse math problems focused on language usage patterns, and (3) SVAMP \citep{patel-etal-2021-nlp} with varying structures to ensure that a model cannot solve the problems by applying simple heuristics and ignoring question text.

\paragraph{Baseline.}
For the baseline, we consider models that are directly fine-tuned on a reasoning dataset without any intermediate fine-tuning.

\paragraph{Models and Training Details.}
We experiment with both encoder-decoder and decoder-only architectures. We use FlanT5 \citep{chung2024scaling} and GPT2 \citep{radford2019language}. The base and large versions of FlanT5, along with the base, medium, and large versions of GPT2, are used, with parameter counts ranging from 124M to 774M. We use the AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of $10^{-4}$, a weight decay of $10^{-4}$, and an effective batch size of 128. For FlanT5-Large and GPT2-Large, a learning rate warmup of 500 steps is used.

The intermediate fine-tuning is performed for two epochs without validation. To adapt these models for reasoning, we continue the training from these checkpoints on \gsm{}. The models are fine-tuned for 20 and 100 epochs on \gsmd{} and \gsmo{}, respectively. The best checkpoint is selected based on the \gsm{} validation performance.

\input{figures/bb_vs_gsm8k}

\paragraph{Decoding.}
We use greedy and self-consistency decoding at inference. For self-consistency decoding, nucleus sampling \citep{holtzman2019curious} is used with $T = 0.6$ and $p = 0.9$ to sample eight responses, and the most consistent final answer is chosen. As nucleus sampling is a stochastic decoding method, we repeat the evaluation three times and report the mean accuracy.

\subsection{Results}
\paragraph{In-Domain Performance.}
We first evaluate the models on the \gsm{} test set. Table~\ref{tab:main_results} shows the accuracy (\%) achieved by various models. These results indicate that FlanT5 benefits from the intermediate fine-tuning, significantly improving the \gsm{} performance. Additionally, these results demonstrate that intermediate fine-tuning helps with both \gsmo{}, which has a small training set, and \gsmd{}, which already contains significantly more training examples.

For GPT2, we observe a slight decline in the reasoning performance when the model is specialized in reasoning using \gsmo{} after fine-tuning it on the arithmetic dataset. However, this issue does not arise with \gsmd{}, where we see performance gains comparable to those of FlanT5. We attribute this behavior to the fact that intermediate fine-tuning optimizes the model for arithmetic tasks, making it more challenging to adapt to other tasks compared to the original model. A larger fine-tuning dataset mitigates this issue, as demonstrated by the significant performance improvement when reasoning specialization is performed using \gsmd{}.

\paragraph{Out-of-Domain Performance.}
Next, the models fine-tuned on \gsm{} are evaluated on MultiArith, ASDiv, and SVAMP, and the results are shown in Table~\ref{tab:main_results}. These results indicate that intermediate fine-tuning does not hurt out-of-domain generalization. Conversely, the models fine-tuned on the arithmetic dataset first generalize better than those directly fine-tuned on \gsm{}.

\paragraph{Arithmetic in Reasoning Context.}
While the models fine-tuned on the arithmetic dataset excel at basic arithmetic tasks compared to their original versions, do these skills transfer to reasoning tasks when they are fine-tuned on reasoning datasets? Moreover, are they the reason behind the better reasoning performance, as shown in Table~\ref{tab:main_results}? Accuracy on the test sets from various reasoning datasets, such as GSM8k and MultiArith, does not directly capture this, as an incorrect final answer can stem from multiple factors beyond arithmetic errors. To better understand the impact of arithmetic computations, we specifically look at them within the reasoning process, ensuring that all other reasoning steps remain correct.

\input{figures/gsm8k_arith_perf}

We use the \gsm{} test set and identify the arithmetic computations using the calculation annotations (enclosed within \texttt{<<>>}). Given a question and its solution up to an annotation, the models are asked to generate the next tokens, which are then compared to the ground truth. We define the accuracy of these tokens as GSM8k arithmetic accuracy. See Appendix~\ref{app:gsm8k_arith_details} for details. Figure~\ref{fig:gsm8k_arith_perf} shows how well the models from Table~\ref{tab:main_results} handle arithmetic within a reasoning context. These results suggest that intermediate fine-tuning reduces arithmetic errors compared to the models fine-tuned directly on GSM8k, leading to an average improvement of 11.7\% in arithmetic computations. Even GPT2, when fine-tuned on \gsmo{}, makes fewer arithmetic errors with intermediate fine-tuning. These results corroborate the hypothesis that the slight decline in the reasoning performance of GPT2 fine-tuned on \gsmo{} after the intermediate fine-tuning results from the model being optimized for arithmetic tasks, potentially making it challenging to adapt the model for reasoning tasks with a smaller training set. The issue is mitigated by using a larger training dataset, \gsmd{} in this case.

\paragraph{Prolonged Intermediate Fine-Tuning.}
We next investigate the impact of extending the intermediate fine-tuning beyond two epochs. We find that fine-tuning models on the arithmetic dataset for additional epochs provides no benefit or adversely affects the reasoning performance when models are later fine-tuned on a reasoning dataset. We attribute this outcome to two factors. First, prolonged fine-tuning on the arithmetic dataset further optimizes the model for arithmetic tasks, limiting its adaptability for other tasks. Second, the model learns the computations for solving GSM8k problems within the first few epochs, and additional fine-tuning does not improve these computations. Figure~\ref{fig:gsm8k_vs_ift_epochs} illustrates GSM8k arithmetic accuracy and overall performance as a function of intermediate fine-tuning epochs. The results show that extended intermediate fine-tuning does not improve GSM8k arithmetic accuracy while making models optimized for arithmetic tasks. This leads to no improvement or decline in GSM8k performance.


\input{tables/inst_tuning_results}