\section{Introduction}
Scaling the model and data sizes has had a tremendous effect on performance across various natural language processing (NLP) tasks \citep{chowdhery2023palm, achiam2023gpt, touvron2023llama2, jiang2023mistral}. These pre-trained models can learn from a few demonstrations using in-context learning and do not require task-specific fine-tuning \citep{brown2020language}. They also benefit from generating a sequence of reasoning steps before arriving at the final answer. These strategies have been particularly effective for mathematical reasoning \citep{wei2022chain, nye2022show, fu2022complexity, zhou2022least}. While large models exhibit excellent performance on various reasoning benchmarks \citep{lambert2024t, liu2024deepseek, jaech2024openai}, smaller models remain essential due to their efficiency and adaptability. They require significantly fewer computational resources, making them ideal for deployment with limited infrastructure. They also enable faster inference and lower latency, which is crucial for real-time applications. Additionally, they can be fine-tuned more efficiently for specific tasks without the high costs associated with training massive models.

\input{figures/arith_error_example}

Traditionally, smaller pre-trained models are adapted for downstream tasks through supervised fine-tuning on a dataset or a combination of datasets formatted as instructions. While this approach is effective for simpler tasks, it falls short when applied to mathematical reasoning, as smaller models struggle to achieve strong performance \citep{wei2022emergent}. This challenge arises because math reasoning datasets, like GSM8k \citep{cobbe2021training}, consist of a small number of reasoning problems, typically paired with one solution. The scarcity of training examples makes it difficult for the model to capture the complexity of mathematical reasoning. To overcome this issue, a widely explored research direction is to distill knowledge from large pre-trained teacher models into smaller student models. Some methods use questions from existing training datasets and use prompting to generate solutions for fine-tuning smaller models \citep{ho2023large, magister2023teaching, fu2023specializing, hsieh2023distilling, yuemammoth}. Others use various techniques to rephrase the questions to create more examples \citep{yu2024metamath} or multiple views of solutions \citep{liang2024mint} to achieve better reasoning performance.

Although these methods boost mathematical reasoning performance in smaller models, they still struggle with arithmetic computations. In many cases, models generate the correct reasoning steps but arrive at incorrect final answers due to numerical computation errors (Figures~\ref{fig:arith_error_example} and \ref{fig:gsm8k_arith_perf}). Some approaches address this issue by integrating external tools \citep{cobbe2021training, schick2023toolformer} or using programs \citep{pmlr-v202-gao23f, chen2023program, ye_satlm_2023}. In this work, we explore whether model performance can be improved by directly mitigating these errors without relying on external tools. Previous research has investigated ways to enhance arithmetic skills in Transformer-based models \citep{liu2023goat, mcleish2024transformers}, but the effective transfer of these skills to downstream tasks like mathematical reasoning remains largely unexplored. To bridge this gap, we focus on how improving arithmetic skills strengthens a model's mathematical reasoning abilities.

In this work, we utilize a programmatically generated arithmetic dataset to enhance mathematical reasoning abilities in small-frame models. We investigate two approaches for incorporating this dataset -- (1) intermediate fine-tuning, where a model is fine-tuned on the arithmetic dataset before being trained on a reasoning dataset, and (2) integrating the arithmetic dataset into the instruction-tuning mixture. The first approach is inspired by transfer learning, as prior research has shown that fine-tuning a model on a related dataset before training it on the target task can significantly improve its performance \citep{vu-etal-2020-exploring, phang2018sentence, pruksachatkun2020intermediate}. The second approach aligns with post-training techniques to refine pre-trained models by exposing them to diverse tasks, helping them acquire new skills and adapt better to various downstream tasks \citep{wei2021finetuned, chung2024scaling, lambert2024t}.

Empirical observations using several mathematical datasets lead to the following key takeaways:
\begin{itemize}
    \item Models fine-tuned on an arithmetic dataset before a reasoning dataset perform better than the ones directly fine-tuned on the reasoning dataset. Additionally, arithmetic datasets can be generated programmatically, eliminating the need for manual resources.

    \item Based on our observations with multiple datasets with varying mathematical reasoning tasks, we find that intermediate fine-tuning results in better out-of-domain generalization.

    \item Including an arithmetic dataset into the instruction-tuning mixture leads to better few-shot performance on multiple mathematical reasoning benchmarks.

    \item These models show better robustness to numerical variations such as numerical substitution and digit expansion than models instruction-tuned on a mixture without the arithmetic dataset.
\end{itemize}

This work highlights the importance of explicit arithmetic training as a key factor in improving mathematical reasoning in smaller models. Our source code and datasets are publicly available.\footnote{\url{https://anonymous.4open.science/r/reasoning-with-arith}}
