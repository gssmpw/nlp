\section{Related Work}
\paragraph{Model~Specialization~via~Distillation.}
Adapting a pre-trained model for a downstream task has been traditionally done through task-specific fine-tuning. However, this approach does not work for mathematical reasoning tasks because datasets, like GSM8k, do not contain enough examples to capture the complexity of mathematical reasoning. Several works have focused on distilling multi-step reasoning solutions from large language models (LLMs) to overcome this limitation. \citet{fu2023specializing} prompted Codex \citep{chen2021evaluating} to generate multiple multi-step solutions for the examples in the GSM8k training set and fine-tuned FlanT5 on the ones that led to the correct final answer. \citet{hsieh2023distilling} used PaLM-540B \citep{chowdhery2023palm} for generating solutions and fine-tuned T5 \citep{raffel2020exploring} in a multi-task setting to generate the labels and rationale. \citet{liu2023tinygsm} used GPT-3.5-turbo to generate synthetic GSM8k-like examples. \citet{yuemammoth} showed that a hybrid of chain-of-thought and program-of-thought solutions performed better than using either format individually. In addition to using LLMs to generate more solutions, \citet{yu2024metamath} used LLM rephrasing and backward reasoning to augment questions and created a new dataset called MetaMathQA.

\paragraph{Transfer Learning.}
Transfer learning has played a pivotal role in NLP. \citet{vu-etal-2020-exploring} and \citet{pruksachatkun2020intermediate} studied the effect of intermediate fine-tuning on the model's performance on a target task. \citet{conneau2019cross} explored cross-lingual model pre-training and showed improvements in natural language inference and machine translation. \citet{razdaibiedina2023progressive} introduced progressive prompts which is a continual learning approach with the forward transfer without catastrophic forgetting. Training on large multi-task mixtures is also a common trend in NLP \citep{aribandi2022ext5, wei2022finetuned, chung2024scaling, lambert2024t}. Another research direction explores identifying relevant examples for a given downstream task from a huge collection of datasets, like P3 \citep{sanh2021multitask}. These methods create embeddings for all examples of interest using hidden states \citep{ivison2023data} or gradients \citep{xia2024less}. Given a task, a small subset of relevant examples are selected based on similarity. These methods have been mainly applied to data-efficient instruction-tuning.