\section{Limitations}
While incorporating an arithmetic dataset improves a model's mathematical reasoning performance, smaller models still have considerable room for improvement. In this work, we include the arithmetic dataset in the training pipeline but do not investigate other factors, such as custom embedding schemes for arithmetic computation. A promising direction for future research is to incorporate insights from recent work, such as \citet{mcleish2024transformers}, into the model architecture. Another limitation pertains to the instruction-tuning mixture. While including the arithmetic dataset improves the few-shot performance on the mathematical reasoning benchmarks, data mixture ablations may be necessary to optimize the instruction-tuning mixture for better overall performance. Finally, while this study focuses on smaller models, its findings apply to larger models. The arithmetic capabilities of pre-trained models could be further enhanced by leveraging synthetic arithmetic datasets. We leave these explorations for future work.
