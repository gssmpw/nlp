\section{Our Approach}
Strong arithmetic abilities are crucial for developing robust mathematical reasoning skills. These skills serve as the foundation for solving a range of mathematical problems. While relatively smaller pre-trained models often struggle with arithmetic computations, they can enhance their proficiency through targeted fine-tuning on synthetic datasets. In this study, we investigate two approaches for transferring these skills to the more complex domain of mathematical reasoning -- (1) intermediate fine-tuning on an arithmetic dataset and (2) incorporating the arithmetic dataset during instruction tuning.

\paragraph{Intermediate Fine-Tuning.}
Fine-tuning a model on an intermediate task before a downstream task can improve the model's performance on the latter \citep{phang2018sentence, vu-etal-2020-exploring, pruksachatkun2020intermediate}. The downstream task is also referred to as the target task. This is called intermediate fine-tuning and can lead to successful knowledge transfer for similar intermediate and target tasks. Building on prior work on transfer learning, we use intermediate fine-tuning to transfer arithmetic abilities to mathematical reasoning.

Unlike natural language, mathematical computations are precise and do not contain redundancies, typically requiring more training examples for effective learning compared to NLP tasks. When a model is fine-tuned on a reasoning dataset, it must simultaneously learn to generate correct reasoning steps and arithmetic computations. Moreover, datasets like GSM8k contain a limited number of examples, restricting both the quantity and diversity of arithmetic computations. This limitation often leads to arithmetic errors during inference. To address these challenges, we adopt a two-step training approach. First, we fine-tune the model on an arithmetic dataset, allowing it to learn a broad range of numerical computations across diverse values. This intermediate fine-tuning ensures that the model develops a strong foundation in arithmetic. Following this, the model is fine-tuned on a reasoning dataset, where it focuses on applying its pre-learned arithmetic skills rather than learning them from limited examples. This two-step process leads to fewer arithmetic errors during mathematical reasoning compared to models directly fine-tuned on a reasoning dataset, leading to more accurate and reliable performance.

\paragraph{Instruction Tuning.}
We also explore the impact of incorporating an arithmetic dataset during post-training, which involves additional training on vast corpora of text. In particular, we focus on instruction tuning--a post-training technique to enhance the ability of pre-trained large language models to follow human instructions more effectively \citep{wei2021finetuned, chung2024scaling}. This process involves fine-tuning a pre-trained model on a diverse set of instructions and their responses. The fine-tuning mixture typically contains examples from a wide range of tasks, including the mathematical reasoning domain. To strengthen the arithmetic capabilities of the model, we include a synthetic arithmetic dataset in this mixture. Similar to intermediate fine-tuning, this process improves the model's arithmetic proficiency and enhances mathematical reasoning performance by enabling more accurate numerical computations within reasoning tasks.

\section{Datasets}
\subsection{Arithmetic Dataset}
A simple arithmetic dataset can be generated programmatically. We refer to \citet{liu2023goat} to generate our dataset. Their work has shown that LLaMA \citep{touvron2023llama} fine-tuned on a programmatically generated dataset outperforms GPT-4 \citep{achiam2023gpt} on arithmetic tasks. While the dataset from \citet{liu2023goat} contains basic arithmetic operations -- addition, subtraction, multiplication, and division, we extend it to include problems on fractions and percentages. Datasets used in this work do not require computations over large numbers, hence we limit the number of digits in the operands to seven. Furthermore, we use log-uniform sampling to ensure that the dataset is not skewed towards numbers with greater digits. This dataset contains nearly 1.3M examples.

\subsection{GSM8k Training Dataset}
For the intermediate fine-tuning experiments, we use \gsm{} for model specialization. As it does not have a validation set, we randomly sample 512 examples from the training set to create a validation set. We use two versions of \gsm{} in this work.

\paragraph{Original.}
In the first version, we use the remaining examples from the training set for model specialization. This dataset contains 6961 examples. We refer to this dataset as \gsmo{}.

\paragraph{Distilled.}
We generate a distilled dataset using the questions from \gsmo{} to evaluate if intermediate fine-tuning benefits tasks with large training datasets. This dataset is generated by prompting Mistral-7B \citep{jiang2023mistral} using the prompt from \citet{wei2022chain}. We generate 64 solutions per question and keep the ones that lead to the correct final answer. After removing duplicate solutions, this results in a dataset with close to 175k examples. We refer to this dataset as \gsmd{}.

\subsection{Instruction Tuning Dataset}
Following the recent work of \citet{lambert2024t}, we use the \tulu{} mixture as the instruction tuning dataset for our work. This dataset contains nearly 1M examples, making it significantly smaller compared to the Flan collection \citep{longpre2023flan}. However, it contains more mathematical reasoning examples compared to datasets like Flan-mini \citep{ghosal2023flacuna}. 

% \subsubsection{Evaluation}
% We use the \gsm{} test set to evaluate the models. We also use three additional datasets -- MultiArith \citep{roy-roth-2015-solving}, ASDiv \citep{miao-etal-2020-diverse}, and SVAMP \citep{patel-etal-2021-nlp} -- to test out-of-domain generalization. MultiArith contains problems focused on basic arithmetic operations and is relatively simpler than \gsm{}. ASDiv focuses on diverse language usage patterns and covers wide problem types taught in elementary school. SVAMP contains problems with varying structures to ensure that a model cannot solve the problems by applying simple heuristics and ignoring question text.

\input{tables/main_results}