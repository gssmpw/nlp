\section{Instruction Tuning}
In this section, we present experiments with instruction tuning.

\subsection{Experimental Setup}
\paragraph{Tasks.}
We use nine math reasoning datasets to evaluate the impact of including an arithmetic dataset in the mixture of instruction-tuning datasets. Similar to intermediate fine-tuning experiments, we evaluate the models on GSM8k, ASDiv, and SVAMP. In addition to MultiArith, we consider the following datasets from MAWPS \citep{koncel2016mawps} -- (1) AddSub \citep{hosseini-etal-2014-learning} which is a collection of addition and subtraction problems, (2) SingleEq \citep{roy2015reasoning} which contains single equation problems, (3) SingleOp \citep{roy2015reasoning} with single operation arithmetic problems, and (4) SimulEq \citep{kushman2014learning} with multiple equation math word problems. AQuA \citep{ling2017program}, which contains algebraic problems in multiple-choice format, is also used for evaluation.

We also evaluate the robustness of the models against various perturbations. For this purpose, we use two datasets -- GSM-Plus \citep{li2024gsm} and GSM-Symbolic \citep{mirzadeh2024gsm}. GSM-Plus is an adversarial grade school math dataset that introduces five variations for each problem in the \gsm{} test set -- numerical variation, arithmetic variation, rephrasing, distractor insertion, and omissions of necessary statements. GSM-Symbolic contains 100 test problems from GSM8k for which variations can be systematically generated by altering numerical values or proper names. As this work focuses on arithmetic computations, we generate 50 variations by modifying the numerical values in the original problems for our experiments.

\paragraph{Baselines.}
We use the following two baselines -- (1) the pre-trained model and (2) the model fine-tuned only on the \tulu{} mixture.

\paragraph{Model and Training Details.}
As large language models predominantly use the decoder-only architecture, we use the large version of GPT2 with 774M parameters for this experiment. The models are fine-tuned for five epochs. We use the AdamW optimizer with $2 \times 10^{-4}$ learning rate and a weight decay of $10^{-4}$. A learning rate warmup of 500 steps is used. As examples in the \tulu{} mixture have varied sequence lengths and differ significantly from the arithmetic examples in this regard, we use a variable batch size with approximately 0.5M tokens in each batch. The input to the model is truncated from the left to have at most 1024 tokens.

\paragraph{Evaluation and Decoding.}
We use few-shot prompting to evaluate the models. Four exemplars are used in the prompts due to the maximum sequence length limit in GPT2. We use exemplars from the prompts used in \citet{wei2022chain}. See Appendix~\ref{app:math_prompt} for more information. The models are asked to generate 256 tokens for each prompt, and the accuracy is computed based on the final answer generated by the model. We do not validate the reasoning steps. Similar to intermediate fine-tuning experiments, we use greedy and self-consistency decoding. We repeat the evaluation three times with the latter and report the mean accuracy.

\subsection{Results}
Table~\ref{tab:inst_tuning_results} shows the results of instruction tuning GPT2-Large with and without including the synthetic arithmetic dataset in the fine-tuning mixture. The model fine-tuned on the \tulu{} mixture and synthetic arithmetic examples achieves better performance across math reasoning datasets, except AQuA, than the model only fine-tuned on the \tulu{} mixture, with both greedy and self-consistency decoding. For self-consistency decoding, the former outperforms the latter in all three evaluation attempts across datasets. We also compute the GSM8k arithmetic accuracy for these models and find a 3\% increase in accuracy by including the arithmetic dataset in the fine-tuning mixture. See Appendix~\ref{app:mawps_detailed_results} for the results on individual datasets in MAWPS.

Neither model performs well on AQuA, and the performance of all models is close to random choice. Randomly choosing an option leads to an average accuracy of 19.9\% $\pm$ 2.7\% after 100 trials.

\subsection{Robustness to Perturbations}
We next evaluate the robustness of our models to perturbations. We use GSM-Plus and GSM-Symbolic for this evaluation.

\paragraph{GSM-Plus.}
The overall accuracy is shown in Table~\ref{tab:inst_tuning_results}. The breakup of the overall performance by perturbation types is illustrated in Figure~\ref{fig:gsm_plus_accuracy}. The model fine-tuned on the \tulu{} and arithmetic mixture performs better than the model only fine-tuned on the \tulu{} mixture across different perturbations. We further investigate the performance drop for the two models relative to the original \gsm{} dataset. In particular, we are interested in numerical variations which include numerical substitutions, digit expansions, and integer-decimal-fraction conversions. For these perturbations, we find that the model fine-tuned on the \tulu{} mixture and the arithmetic dataset sees a lower performance drop relative to the original GSM8k dataset than the model fine-tuned only on the \tulu{} mixture.

\input{figures/gsm_plus_accuracy}
\input{tables/gsm_symbolic}

\paragraph{GSM-Symbolic.}
Table~\ref{tab:gsm_symbolic_results} shows the performance of the instruction-tuned models on the problems from GSM-Symbolic. The performance of these models on the original problems is presented in the GSM8k column in the table. We observe that the model fine-tuned on the \tulu{} mixture with the arithmetic dataset sees a performance drop of 25.9\% compared to a drop of 44.4\% for the model fine-tuned only on the \tulu{} mixture, indicating better robustness to changing numbers in the original problems.

We observe a similar pattern with greedy decoding. See Appendix~\ref{app:robustness_greedy} for details. These results indicate that including arithmetic examples in the fine-tuning mixture helps make models more robust to numerical perturbations.
