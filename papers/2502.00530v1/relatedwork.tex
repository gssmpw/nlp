\section{Related work and motivation}
\textbf{Spatially Embedded Networks (SENs) and Network Connection Modeling}. The
spatially embedded network is a special type of complex network whose structure
is constrained by its embedded spatial environments. A simple synthetic
spatially embedded random network can be created by randomly placing the nodes
and creating edges based on the nodes' distances \cite{Barnett2007Spatially}. In
real world, many natural and man-made network systems show constant patterns
\cite{Dong2020networknetworks}. For example, the lengths of infrastructure
systems are often limited by construction costs in the real world
\cite{Cadini2015cascading}. Previous studies have observed that the lengths of
road segments in a transportation network follow a power-law distribution
\cite{Chen2019Correlations}, and the node degree density within a large power
system follows a logarithmic distribution \cite{Soltan2016Generation}. Because
of such constraints, the connection function within such SENs can be modeled
mathematically, such as the small-world networks have been used to model the
power networks \cite{Aksoy2019generative}, the single-parameter controlled
hierarchical planer and spatial networks \cite{Molinero2020model}, the tunable
spanning tree \cite{Dunton2023Generating}, and a combination of relative
neighborhood graphs (RNG), Gabriel Graphs (GG), and Erd\H{o}s-R\'enyi (ER)
random graph \cite{Hackl2019Modelling}.  Although previous studies have
demonstrated the connection functions of SENs follow specific patterns, it is
still challenging to identify accurate and efficient network connection models.

To better illustrate the concept of SENs and its relationship with the embedded
spatial environment, Figure \ref*{fig: spatially embedded networks} shows an
example of SEN and its embedded environment. Specifically, four types of
features may influence the connection patterns of the SEN, i.e., the node's
point feature $x$, the node's regional feature $r$, the node's position feature
$pos$, and the edge feature $e$. The node's point feature refers to the spatial
feature that located at the node's point, such as the population density, the
socioeconomic data, and geological data of the node's location. Unlike to the
node's point feature, the node's regional feature describes a region centered on
the node, such as the topography change or soil type change within a specific
distance of a node. The node's position feature is considered separately in this
study, which refers to the node's coordinates values. Lastly, the edge feature
represents a sequence of spatial values that sampled from the spatial
environment along the edges.

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.6\textwidth]{SEN_illustration.pdf}
  \caption{Spatially Embedded Networks ($x_v$ represents the node's point
    feature, $r_v$ represents the node's regional feature, $p_v^y, p_v^x$
    represent the node's position feature, and $e_{v,u}$ represents the edge
    feature between node $u$, and $v$.)}
  \label{fig: spatially embedded networks}
\end{figure*}


\textbf{Deep learning-based network representation learning:} Recently, the deep
learning methods have been emerging and showed more promising performance in
various applications. Unlike conventional mathematically modeled methods which
use rigorous mathematical equations to describe the SENs' intricate patterns,
geometric deep-learning models offer a more flexible and end-to-end learning
process, which facilitates network representation and intricate pattern
discovery \cite{Ding2024Artificial}. The deep learning process of the Graph
Convolutional Networks (GCNs) can be generally described as
Eq. \ref{eq:GCNequation}, i.e. the node features will be processed by the
neurons and then convolved along the edges of the graph. Notable variants of the
GCN include the GraphSAGE \cite{Hamilton2017Inductive} and Spatial Graph
Convolutional Networks (SGCN) \cite{Danel2020Spatial}. The former architecture
introduced advanced sampling strategies for the node's neighbors, resulting in a
higher node classification accuracy in multiple datasets. The latter SGCN
architecture first introduced the spatial features of nodes into the learning
process of a molecular classification task.  The GCN and its variants have been
receiving more and more attention in spatially embedded networks. For example, a
graph attention architecture was used to capture the spatial correlations within
traffic networks for traffic flow prediction \cite{Wang2023STGIN}. The GCNs have
also been used in power systems for fault detection, power outage prediction,
power flow simulation, and system control \cite{Liao2022Review}. However, most
of the GCN variants were mainly focusing on the features of nodes rather than
the edges. Considering the significant influence of both node features and edge
features on the spatially embedded networks, there is an urgent need for models
that can process such heterogeneous features simultaneously.

\begin{equation}
  \label{eq:GCNequation}
  H^{l+1} = \sigma(\hat{A}H^lW^l)
\end{equation}

\textbf{Multimodal Data Fusion:} Multimodal data fusion represents a fundamental
method for mining richer information from data with different distributions,
sources, and types \cite{Lv2017NextGeneration}. Compared to traditional big
data, multimodal big data is composed of several modalities to describe the same
thing. For example, an image and text information are often used together to
describe an event in a newspaper. The fusion of information from multimodal data
can be broadly classified into three groups, the early fusion, late fusion, and
intermediate fusion \cite{Gaw2022Multimodal}. The early fusion combines features
from multi-modalities before the neural network training. For example, the
eigenvector can be used as a representative of a data source. Then the
combination of eigenvectors from multimodal data can be used as input of a
traditional classifier, such as a Support Vector Machine(SVM)
\cite{Liu2017Deep}. On contrary to the early fusion, the late fusion combines
information from multimodal data after the training process. For example, after
obtaining the prediction results based on each modality data, the final
prediction can be made by using their averaging values or maximum values
\cite{Ramachandram2017Deep}. Lastly, in order to construct an end-to-end
framework of multimodal learning, the intermediate fusion has been widely
proposed. A typical process of intermediate fusion includes three steps, (1)
each modality is embedded into a latent space using a neural network layer, (2)
the representations of each modality is fused into a single representative, and
(3) a joint representation is learned to make a single prediction by using the
step 2 as inputs \cite{Gaw2022Multimodal}.