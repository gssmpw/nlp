\section{Proposed Method}


Our work aims to present a practical approach for learning the PSF of an imaging system. We utilize spatial frequency response (SFR) measurement, a technique widely used in the industry.
\begin{figure}[b]
\centering
\vspace{-0.2cm} 
    \includegraphics[width=1\linewidth]{figs/wavefront.pdf}
        % \vspace{-0.5cm} 
    \setlength{\abovecaptionskip}{-0.2cm} 
    \caption{Diagram of wavefront aberration and PSF. When light passes through an aberrated optical system, the real wavefront deviates from the ideal, causing defocus in the imaging plane. This deviation, varying with incidence angle and wavelength, creates a spatially varying, symmetric PSF. We focus on the PSF along the +Y axis, where normalized field height $\mathrm{H}$ and wavelength $\lambda$ define $\mbox{PSF}(\mathrm{H}, \lambda)$. Other PSFs are generated by rotating $\mbox{PSF}(\mathrm{H}, \lambda)$ by angle $\phi$ from the +Y axis, with positive $\phi$ indicating clockwise rotation (yellow box).   
 }
    \label{fig:wa_h}
    \vspace{-0.1cm} 
    
\end{figure}


PSF and SFR are interconnected, while the PSF reflects the system's capability to capture fine details, influencing its resolution, the spatial frequency response (SFR) is a key metric for quantifying resolution. The SFR can be derived from the PSF. To simplify the analysis, due to rotational symmetry in optical systems, the PSF is also symmetric (as shown in \cref{fig:wa_h} ). So this study focuses on the PSF along the +Y axis, SFR could be derived from PSF:
\begin{equation}\label{eq:sfr}
\mbox{SFR}(\mathrm{H},\lambda,\phi) = h(\mbox{PSF}(\mathrm{H},\lambda),\phi),
\end{equation}
here, $\lambda$ is wavelength, $\mathrm{H}$ is normalized field height  (shown in ~\cref{fig:wa_h}), and the $\phi$ is the rotation angle from the +Y axis on the image plane, with positive values indicating clockwise rotation (shown in ~\cref{fig:wa_h}), $h$ is the mapping function (see supplementary materials). For a given normalized field height $\mathrm{H}$ and wavelength $\lambda$, the PSF is a 2D distribution, while the SFR relates to directional blur, with the direction specified by the rotation angle $\phi$.

\subsection{Problem Formulation}

\subsubsection{PSF Estimation by Optimization}

The PSF of an imaging system is multi-dimensional, and directly estimating it for different configurations is challenging. A parametric model, such as the Seidel PSF model, can simplify this process.

To understand this model, we start with wavefront aberration, which represents the deviation of the real wavefront from the ideal shape at the exit pupil~\cite{goodman2005introduction}. This deviation leads to defocus on the image plane, resulting in the PSF (see~\cref{fig:wa_h}).  In incoherent imaging systems, the PSF is closely related to the wavefront aberration:  
\begin{equation}\label{eq:psf}
\scalebox{0.95}{$
\mathrm{PSF}(\mathrm{H},\negthinspace\lambda)=\left|\mathcal{F}\negthinspace\left(\negthinspace A( \mathbf{p})\exp\left(\frac{i2\pi W(\mathrm{H},\lambda,\mathbf{p})}{\lambda}\right)\right)\right|^2
$},
\end{equation}
where $W(\mathrm{H},\negthinspace\lambda,\negthinspace \mathbf{p})$ is the wavefront aberration, and $\mathbf{p}$ represents a point in polar coordinates on the pupil plane:

\begin{equation}
\mathbf{p} = \begin{pmatrix} \rho \\ \theta \end{pmatrix},
\end{equation} 
with $\rho \in [0,1]$ as the radial coordinate and $\theta \in [0, 2\pi]$ as the angular coordinate. $A(\mathbf{p})$ is the aperture function, typically known. By further decomposing the wavefront aberration into Seidel basis~\cite{gray2012analytic}, expressed as:
\begin{equation}
\label{eq:seidel}
\scalebox{0.97}{$
W \left( \mathrm{H}, \negthinspace\lambda, \negthinspace\mathbf{p}\right) = \sum \limits_ {\scriptstyle k=0}^\infty\sum\limits_{\scriptstyle l=0}^\infty\sum\limits_ {\scriptstyle m=0}^\infty W_{klm} \mathrm{H}^{k} \rho^{l} \cos^m\left( \frac{\pi}{2}-\theta \right)
$}
\end{equation}
where $ k=2p+m$ and $l=2n+m$ ($p, n, m \negthinspace\in\negthinspace \mathbb{N}$), this decomposition provides a set of Seidel coefficients $ W_{klm}$ (only select around the first 10 items), which theoretically represents a single-channel, spatially varying PSF of an imaging system.

PSF estimation is then framed as learning a set of  Seidel coefficients from observed SFR measurements. This learning is typically achieved through gradient descent optimization. The optimization process aims to adjust the Seidel coefficients to match SFR measurements across the entire image plane. 



\subsubsection{Mitigating Gradient Conflicts in Optimization}

However, learning the PSF (or Seidel coefficients) is not trivial for several reasons. Certain Seidel bases, such as spherical aberration ($\rho^2$), simultaneously impact SFR curves across multiple directions, which causes coupling among these directions (\cref{fig:toy}) and hinders accurate fitting to diverse SFR data. Moreover, the inverse problem is inherently ill-posed, particularly due to the exclusion of the phase component in~\cref{eq:psf}. The nonlinearity of transformation as shown in~\cref{eq:psf} further complicates the inversion process. Together, these factors create conflicting gradients during the optimization.

\emph{Gradient conflicts} are frequently discussed in multi-task learning, where the aim is to improve efficiency by sharing model structures across tasks. However, such conflicts can lead to poorer task performance compared to independent learning~\cite{liu2021conflict}. To address this, we build on existing methods for mitigating gradient conflicts and propose refined strategies. 

First, we propose a novel wavefront basis where each basis function influences only one direction of the SFR. The modified expression is:
\begin{align}
\scalebox{0.95}{$
\label{eq:wavefront1}
W(\mathrm{H},\negthinspace\lambda,\negthinspace\mathbf{p}) \!= \!\sum\limits_{\scriptstyle{(p,q,r) \in \mathcal{Q}}} W_{pqr}(\mathrm{H},\negthinspace\lambda) \rho^{p} (\sin \theta)^q (\cos \theta)^{r},
$}
\end{align}
where the set $\mathcal{Q}$ is defined as:
\begin{align}
\label{eq:set_N}
\mathcal{Q} &= \{(2,2,0),(2,0,2), (3,1,0), (3,3,0), (4,2,0), \nonumber\\
&\qquad (4,0,2), (5,1,0), (6,2,0), (6,0,2)\}.
\end{align}


In our modified basis, each term includes either a $\cos \theta$ or $\sin \theta$ component, ensuring it influences the SFR independently along either the vertical or horizontal axis. This approach helps mitigate gradient conflict during optimization, as shown in ~\cref{fig:toy}. For further information about the new basis, please refer to the supplementary materials.


Second, we optimize parameters to match the SFR within narrower field of view. Instead of targeting the SFR across the entire field of view, this approach focuses on smaller, less variable SFR targets, facilitating easier convergence. Although this is a discrete representation, adjusting the optimization step enables control over the PSF output density, allowing for either dense or sparse representations as needed.

Third, we learn the PSF by optimization progressively from the center to the edge~\cite{shi2023deep}. According to aberration theory, only spherical aberration impacts the center of the image plane, while coma and field curvature aberrations gradually appear toward the edges, creating a more complex PSF pattern. Following this progression, we apply curriculum learning~\cite{bengio2009curriculum} to gradually learn PSF from center to edge.




\begin{figure}[t]
\centering
\vspace{-0.0cm} 
    \includegraphics[width=1\linewidth]{figs/gradient_conflicts.pdf}
    \setlength{\abovecaptionskip}{-0.0cm} 
    \caption{An example demonstrating how the proposed wavefront basis mitigates gradient conflicts. \textbf{Top:} In the Seidel PSF model, the spherical aberration basis $\rho^2$ creates a circular PSF shape with $360^\circ$ of blur (orange arrow). This produces identical SFR in both the $0^\circ$ and $90^\circ$ directions. When attempting to optimize the coefficient $W_0$  to match real SFR measurements, which differ between $0^\circ$ and $90^\circ$, gradient conflicts arise. \textbf{Bottom:} In our proposed wavefront basis, each basis affects the SFR in only one direction. This allows the model to independently adjust coefficients $W_1$ and $W_2$ to better match the measured SFR without gradient conflict.
}
    \vspace{-0.0cm} 
    \label{fig:toy}
\end{figure}


\subsection{Implementation}
\begin{figure*}
\centering
% \hspace{2em} 
    \includegraphics[width=1\linewidth]{figs/method.pdf}
    \setlength{\abovecaptionskip}{-0.5cm} 
    \caption{Diagram of the proposed two-step PSF estimation framework, the first step involves learning monochromatic aberration per normalized image height $\mathrm{H}$. The network $\mathcal{G}_{\Theta1}$ processes $\mathrm{H}$ and $\mathrm{H}^2$ to output coefficients, generate wavefront aberration and transform it into the $\mbox{PSF}^*$, followed by calculating the modulation transfer function $\mbox{MTF}^*$, resulting in the spatial frequency response ( $\mbox{SFR}^*$) curve. Concurrently, a real $\mbox{SFR}$ curve at the same $\mathrm{H}$ of one color channel is derived from real capture. Discrepancies between these curves guide $\mathcal{G}_{\Theta1}$ to faithfully represent real aberration. The second step focuses on learning PSF shifts across channels. Using $\mathrm{H}$ as input, $\mathcal{G}_{\Theta2}$ calculates shifts, generates shifted PSF, and produces chromatic areas $\mbox{CA}^*$ through a physical process. Real chromatic areas $\mbox{CA}$ data at the same $\mathrm{H}$ are obtained from captures, the disparities between the two data guiding $\mathcal{G}_{\Theta2}$ to output $\mbox{CA}^*$ faithfully representing reality.  These two steps enable the learning of spatial-variant PSF of the whole imaging system.}
    \label{fig:method}
    \vspace{-0.0cm}     
\end{figure*}


\subsubsection{Image Capture}
\label{image capture}


Images are captured in a controlled environment~\cite{ISO12233-2014}, with a checkerboard test chart mounted on a holder and the imaging system positioned on a tripod at a fixed distance. To capture the SFR across the entire field of view in a single setup, the checkerboard is large enough to fill the image plane, and multiple consecutive images are taken. These images are recorded in raw format with the lowest gain, and exposure time is adjusted to prevent overexposure while maximizing the grayscale range. The images are then averaged to reduce noise. Finally, the averaged raw image is converted to a linear RGB format (\cref{fig:method}) for PSF estimation, minimizing the impact of subsequent nonlinear ISP operations that convert linear RGB to sRGB~\cite{brooks2019unprocessing}.



\subsubsection{Two-stage PSF Estimation}

To leverage advanced optimizers and the flexibility of neural networks to enhance the optimization process, we integrate two multi-layer perceptrons (MLPs) into the physical transformations, allowing the MLPs to adjust their neurons to learn the target~\cite{ulyanov2018deep}. As shown in~\cref{fig:method}, our approach follows a two-stage learning strategy. First, we independently estimate the PSF for each color channel. Next, we learn the PSF shifts across channels by analyzing chromatic area differences. Separating the PSF estimation into two subproblems, specifically monochromatic PSF and inter-channel PSF shift, simplifies the optimization process compared to single-stage approach. Here, we refer to the MLPs paired with physical transformations as a surrogate model that represents the PSF of the imaging system.

\paragraph {Monochromatic PSF Estimation}
 % wavelength $\lambda$ and
The MLP $\mathcal{G}_{\Theta_1}$ takes normalized field height $\mathrm{H}$ as input, and outputs coefficients $W_{pqr}^*$. These coefficients are then used to generate the SFR through transformation:
\begin{align}
\mbox{SFR}^*(\mathrm{H},\negthinspace\phi)= h(g(\mathcal{G}_{\Theta_1}(\mathrm{H}) ,\negthinspace\mathrm{H}),\negthinspace\phi),
\end{align}
where $g$ is the mapping function that generates the PSF as defined in~\cref{eq:psf,eq:wavefront1}, and 
$h$ is the mapping function that outputs the SFR as in ~\cref{eq:sfr}, the superscript $*$ denotes the surrogate output, distinguishing it from the ground-truth value. The goal of the surrogate model is to optimize the network parameters to closely match the SFR measurements:
\begin{equation}
\scalebox{0.95}{$
\label{eq:optimization_theta1}
\Theta_1^*(\mathrm{H}) = \mathop{\arg\min}\limits_{\Theta_1} \sum\limits_{\mathrm{H}} \sum\limits_{\phi=0}^{2\pi} \left| \mbox{SFR}^*(\mathrm{H}, \phi) - \mbox{SFR}(\mathrm{H}, \phi) \right|,
$}
\end{equation}
here, in each optimization step, $\mathrm{H}$  is restricted to a smaller region, defined by a narrow field of view, the field of view interval $\Delta\mathrm{H}$ ($\Delta\mathrm{H}\in(0.03,0.1)$). The value $\mathrm{H}$ is gradually increased from 0 to 1 to learn the PSF across the entire image plane.



\paragraph{Cross-Channel PSF Shift Estimation}
In addition to monochromatic aberrations, it is crucial to consider PSF shifts across different color channels, as these shifts can result in color misalignment and fringing, known as chromatic aberration. Building upon previous work~\cite{lluis2012chromatic}, we define the chromatic aberration area (CA) as the region enclosed by the edge gradient line of a blurred black-and-white edge image and the horizontal axis (in pixels). To quantify chromatic aberration, we define the chromatic area difference as:
\begin{equation}\label{eq:deltaca}
\scalebox{0.95}{$
\Delta\text{CA}(\mathrm{H}, \lambda, \phi) = \text{CA}(\mathrm{H}, \lambda, \phi) - \text{CA}(\mathrm{H}, \lambda_{\scalebox{0.6}G}, \phi),
$}
\end{equation}
where $\text{CA}$ is chromatic aberration area, $\Delta\text{CA}$ is chromatic aberration area difference, \(\lambda = \{\lambda_{\scalebox{0.6}{R}}, \lambda_{\scalebox{0.6}{B}}\}\) , and the green channel \(\lambda_{\scalebox{0.6}{G}}\) serving as the reference. 

% is used as the reference.

In practical image capture, chromatic aberration area differences $\Delta\text{CA}$ can be directly inferred from the fringe patterns observed in a captured checkerboard image. However, in a simulated surrogate model, this aberration is influenced by both the 2D distribution of the PSF and the rotation angle \(\phi\), expressed as:
\begin{equation}\label{eq:ca}
\scalebox{0.95}{$
\text{CA}^*(\mathrm{H}, \lambda, \phi)= \mathcal{L}(\text{PSF}_{\scalebox{0.6}{S}}^*(\mathrm{H}, \lambda, \mathbf{x}), \phi),
$}
\end{equation}
where $\mathcal{L}$ is a mapping function (see supplementary materials), and $\text{PSF}_{\scalebox{0.6}{S}}^*$ refers to the shifted $\text{PSF}^*$, which is derived from the monochromatic PSF estimation. To estimate these shifts, we introduce a second MLP \(\mathcal{G}_{\Theta_2}\), which takes the $\text{PSF}^*$ learned by \(\mathcal{G}_{\Theta_1}\), the wavelength  \(\lambda\), and the normalized field height $\mathrm{H}$ as input. It outputs the PSF shifts, which are applied to the PSF as follows:
\begin{align}
\scalebox{0.95}{$
\text{PSF}_{\scalebox{0.6}{S}}^*(\mathrm{H}, \lambda, \mathbf{x}) = \mbox{T}(\mathcal{G}_{\Theta_2}(\mathrm{H}, \lambda), \text{PSF}^*(\mathrm{H}, \lambda, \mathbf{x})),
$}
\end{align}
where $\mbox{T}$  denotes the shift operation, as shown in~\cref{fig:method}. \(\lambda = \{\lambda_{\scalebox{0.6}{R}}, \lambda_{\scalebox{0.6}{B}}\}\), with only the PSF of the red and blue channels being shifted.

The goal is to estimate the PSF shifts between channels to match the chromatic area differences in the measurements. To achieve this, the surrogate model \(\mathcal{G}_{\Theta_2}\) is trained to minimize the difference between the predicted and observed chromatic area differences:
\begin{equation}
\scalebox{0.9}{$
\Theta_2^*(\mathrm{H}, \lambda) = \mathop{\arg\!\min}\limits_{\Theta_2} \sum\limits_{\scriptstyle \mathrm{H}} \sum\limits_{\scriptstyle \phi=0}^{\scriptstyle 2\pi} \left| \Delta\text{CA}^*(\mathrm{H}, \lambda, \phi) - \Delta\text{CA}(\mathrm{H}, \lambda, \phi) \right|,
$}
\end{equation}
where, in each optimization step, $\mathrm{H}$  is restricted to a smaller region and gradually increased from 0 to 1 to learn the PSF shift across the entire image plane, following the same steps and intervals described in~\cref{eq:optimization_theta1}.


Since chromatic area differences arise from both monochromatic aberrations and PSF shifts across channels, we employ a two-stage learning process. The PSF shifts are estimated only after addressing monochromatic aberrations, ensuring a more accurate optimization process.


% \begin{equation}
% \text{S}^* = \mathop{\arg\!\min}_{\text{S}} \left| f(\text{S}) - \text{SFR} \right|

% \end{equation}


% \begin{equation}
% \text{S}^* = \arg\min_{\substack{\text{S}}} \left| f(\text{S}) - \text{SFR} \right|
% \end{equation}
% \begin{equation}
% \text{W}^* = \arg\min_{\substack{\text{W}}} \left| h(\text{W}) - \text{SFR} \right|
% \end{equation}








