\section{Experimental Results}
\label{sec:exp}
\begin{table*}
  \centering
  \resizebox{0.99\textwidth}{!}{
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{lc@{\qquad}ccc@{\qquad}ccc}
    \toprule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{\centering Comparison Chart}} && \multicolumn{3}{c}{Lens \#63762} & \multicolumn{3}{c}{Lens \#89752} \\
    \cmidrule{3-8}
    && Proposed & Degradation Transfer~\cite{chen2021extreme}& Fast Two-step~\cite{eboli2022fast}& Proposed & Degradation Transfer~\cite{chen2021extreme}& Fast Two-step~\cite{eboli2022fast}\\
    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=0 wo/ noise}}
                & PSNR $\uparrow$ &  41.981 $\pm$ 1.132 &  $ 40.822 \pm 1.016$& $ \textbf{42.240} $& 42.073 $\pm$ 1.058& $ 42.128 \pm 1.174$& $ \textbf{43.511} $\\

                & SSIM $\uparrow$   &  0.937 $\pm$ 0.066 & $ 0.939 \pm 0.071 $& $ \textbf{0.943} $& 0.945 $\pm$ 0.069 & $ 0.926\pm 0.073 $& $ \textbf{0.947} $\\

    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=0.7 wo/ noise}}
                & PSNR $\uparrow$ &  \textbf{49.185} $\pm$ \textbf{1.242} & $ 46.521 \pm 1.347$ & $ 43.741 $& \textbf{47.811} $\pm$ \textbf{1.115}& $ 45.896 \pm 1.012$& $ 44.171 $\\

                & SSIM $\uparrow$   &  \textbf{0.967} $\pm$ \textbf{0.061}& $ 0.959 \pm 0.079$& $ 0.933 $& \textbf{0.959} $\pm$ \textbf{0.067}& $ 0.951 \pm 0.078$& $ 0.942 $\\
    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=1 wo/ noise}}
                & PSNR $\uparrow$ &  \textbf{50.156}$\pm$ \textbf{1.606}& $ 44.920 \pm 1.592$& $44.993 $& \textbf{50.624}$\pm$ \textbf{1.537}& $ 43.872 \pm 1.516$& $44.801 $\\

                & SSIM $\uparrow$   &  \textbf{0.983} $\pm$ \textbf{0.064}& $ 0.966 \pm 0.088$& $ 0.933$& \textbf{0.979} $\pm$ \textbf{0.076}& $ 0.959 \pm 0.081$& $ 0.938$\\
    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=0 w/ 1\% noise}}
                & PSNR $\uparrow$ &  \textbf{42.075} $\pm$ \textbf{1.102}& $ 40.629 \pm 1.116$& $ 41.822 $&  \textbf{42.970} $\pm$ \textbf{0.792}& $ 41.053 \pm 1.044$& $ 41.790 $\\

                & SSIM $\uparrow$   &  \textbf{0.949}$\pm$ \textbf{0.065}& $ 0.925 \pm 0.085$& $ 0.937 $& \textbf{0.951}$\pm$ \textbf{0.077}& $ 0.947 \pm 0.080$& $ 0.941 $\\

    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=0.7 w/ 1\% noise}}
                & PSNR $\uparrow$ &  \textbf{47.467} $\pm$ \textbf{1.579}& $45.981 \pm 1.483$& $ 44.286 $& \textbf{46.284} $\pm$ \textbf{1.181}& $44.907\pm 1.177$& $ 43.812 $\\

                & SSIM $\uparrow$   &  \textbf{0.960} $\pm$ \textbf{0.071}& $ 0.926 \pm 0.083$& $ 0.950 $& \textbf{0.958} $\pm$ \textbf{0.081}& $ 0.930 \pm 0.087$& $ 0.938 $\\
    \midrule
    \multirow{2}{*}{\raisebox{-\heavyrulewidth}{H=1 w/ 1\% noise}}
                & PSNR $\uparrow$ &  \textbf{49.151} $\pm$ \textbf{1.622}& $ 43.981 \pm 1.629$& $ 44.554$& \textbf{49.803} $\pm$ \textbf{1.643}& $ 43.522 \pm 1.752$& $ 43.604$\\

                & SSIM $\uparrow$   &  \textbf{0.987} $\pm$ \textbf{0.073}& $ 0.936 \pm 0.086$& $ 0.931$& \textbf{0.969} $\pm$ \textbf{0.075}& $ 0.942 \pm 0.082$& $ 0.933$\\
    \bottomrule
  \end{tabular}}
  \caption{Evaluation of PSF accuracy using synthetic checkerboard patterns under different configurations, including variations in relative image height (H) and the presence or absence of noise. The proposed method quantitatively outperforms the Degradation Transfer~\cite{chen2021extreme} and Fast Two-step~\cite{eboli2022fast} methods in terms of PSNR and SSIM. For a fair comparison, all PSFs have been normalized so that the sum of each channel equals one. In most configurations, our method outperforms the other approaches.}
\label{tab:compare_psf}
   \vspace{-2mm}
\end{table*}


We evaluate the proposed method from two perspectives: the accuracy of the estimated PSF in simulations and the deblurring performance in both simulated and real-world scenarios.

\subsection{Dataset, Algorithms and Metrics}
% \subsection{Dataset and Metrics}

To evaluate deblurring performance, we select three state-of-the-art deep learning-based algorithms: \textbf{MPRNet}\cite{zamir2021multi}, \textbf{Restormer}~\cite{zamir2022restormer}, and \textbf{FFTFormer}~\cite{kong2023efficient}. During the training stage, we use 500 images from the Flickr2K dataset~\cite{lim2017enhanced} to ensure broad applicability across various natural scenes, with the blurred images   synthesized using estimated PSF. During the testing stage, we reserve 100 images from the same dataset, with the blurred images synthesized using ground-truth PSF.

We employ two metric sets to assess performance in simulation and real capture respectively:

\begin{itemize}
    \item \textbf{Full-reference metrics} to evaluate in simulation. We use PSNR and SSIM~\cite{wang2004image} to measure the difference between the output and the ground-truth.
    \item \textbf{Non-reference metrics} are applied for real capture evaluation. We employ MUSIQ~\cite{ke2021musiq} and MANIQA~\cite{yang2022maniqa} to assess the visual quality of the reconstructed images.
\end{itemize}


\subsection{Experiments on Simulation}
We evaluate both the accuracy of the estimated PSF and the deblurring performance by simulation. In this setup, the imaging system uses an IDS camera equipped with onsemi AR1820HS sensor. The imaging lenses are sourced from Edmund (\#63762 or \#89752), and the simulated PSF, generated by \textit{Zemax}\textsuperscript{\textregistered}, serve as the ground-truth. 

To evaluate the accuracy of the estimated PSF, we simulate degraded checkerboard patterns by convolving ground-truth PSF with ideal patterns, followed by estimating the PSF from these degraded patterns. The accuracy of the estimated PSF is compared to the ground truth PSF using PSNR and SSIM metrics. To further assess the robustness of the approach, noise is added to the degraded patterns. For comparison, the following two methods are selected:

\begin{itemize}
\item \textbf{Degradation Transfer}~\cite{chen2021extreme}: A deep linear model incorporating optical geometric priors.
\item \textbf{Fast Two-step}~\cite{eboli2022fast}: An empirical affine model that processes image gradients.
\end{itemize}

An ablation study is then conducted to evaluate the contribution of each component to the overall method.

In evaluating deblurring performance, we account for the ISP pipeline within the camera.



\subsubsection{Accuracy of Estimated PSF}
\label{accuracy of estimate}

As shown in~\cref{fig:psf_compare}, the imaging lens is \#63762 from Edmund, and estimated PSF from different methods are listed. PSF is channel-normalized for visualization. Comparatively, our method is closest to the ground-truth.



In traditional lens design, designers typically focus on three normalized field heights: 0, 0.7, and 1~\cite{smith2008modern}, as these provide a representative sampling of the image plane. Following this convention, we selected these normalized field heights for quantitative comparison. We compare two scenarios: one without noise (ideal) and one with noise (realistic) when performing SFR measurements. A 1\% noise level is set for realism, as multiple consecutive checkerboard images can be captured and averaged to reduce noise. As shown in~\cref{tab:compare_psf}, as an optimization method, both our approach and Degradation Transfer~\cite{chen2021extreme} produce variable results, while the Fast Two-step method outputs a consistent result each time.  In most configurations, our method outperforms the other approaches in both scenarios.

\begin{figure}
\centering
\vspace{0.0cm} 
\hspace{-3mm}
    \includegraphics[width=1\linewidth]{figs/psf_compare.pdf}
    \setlength{\abovecaptionskip}{0.3cm} 
    \caption{Estimated PSFs and ground-truth: The PSFs are arranged from left to right by increasing normalized field height $\mathrm{H}$. From top to bottom, the PSF estimates using Degradation Transfer~\cite{chen2021extreme}, Fast Two-step~\cite{eboli2022fast}, and our method, followed by the ground-truth PSF of lens \#63762 from Edmund.}
    \vspace{-0.2cm} 
    \label{fig:psf_compare}
\end{figure}


   

\begin{figure*}[t]
\vspace{-0.0cm} 
\centering
% \hspace{2em} 
    \includegraphics[width=1\linewidth]{figs/compare6.pdf}
    \setlength{\abovecaptionskip}{-0.3cm} 
    \caption{Performance comparison with state-of-the-art methods on real captures. From left to right: sharp output image deblurred by the pre-trained Restormer, using training data synthesized from our estimated PSF; real captured image patches from a custom-built imaging system (Edmund Lens: \#63762 and onsemi AR1820HS sensor); deblurred image patches from pre-trained Restormers using data synthesized with estimated PSFs from Degradation Transfer~\cite{chen2021extreme}, Fast Two-step~\cite{eboli2022fast}, and our approach. MUSIQ$\uparrow$ / MANIQA$\uparrow$ scores are shown in the bottom-right corner.}
    \vspace{-0.4cm} 
    \label{fig:comparision}
\end{figure*}




\subsubsection{Ablation Study}


To further evaluate the proposed method, we conduct an ablation study to quantify the impact of various factors on performance. \cref{tab:ablation} presents a comparison of the proposed method with three alternative configurations: (1) without optimization within a narrow field of view, i.e., without small interval optimization; (2) without the proposed wavefront basis in~\cref{eq:wavefront1}, using the Seidel basis instead; and (3) without optimization from center to edge based on curriculum learning, i.e., without curriculum learning. From these comparisons, we conclude that optimization within a narrow field of view, the proposed wavefront basis, and curriculum learning strategy all significantly enhance the estimation accuracy of the spatially variant PSF, particularly for larger fields of view. These design choices are essential for achieving precise results across the entire field of view.


\begin{table}[t]
\vspace{-0mm}
\scriptsize
  \caption{Quantitative assessment (PSNR/SSIM) of each component in the proposed method using the imaging system without noise (Edmund Lens \#63762 and onsemi AR1820HS sensor).}
   \vspace{-2mm}
     % \hspace{-1mm}
   \label{tab:ablation}
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{5pt}
 \centering
 \resizebox{\linewidth}{!}{
 \begin{tabular}{l|ccc}
        \hline
        &            H = 0&H = 0.7 &H = 1\\
        \hline
        w/o small interval optimization&   42.514/0.934& 42.682/ 0.937& 41.064/ 0.922\\
        w/o  proposed wavefront basis& 42.180/0.931&47.479/ 0.950&  44.579/ 0.954\\
        w/o curriculum learning&  41.562/0.937&48.580/ 0.957&46.023/ 0.955\\
        Proposed&  \textbf{42.643/0.940}&\textbf{49.079}/ \textbf{0.968}& \textbf{49.252}/ \textbf{0.981}\\
        \hline
    \end{tabular}
    }
\vspace{-0mm}
\end{table}




% , with the seed fixed using the “torch.manual\_seed(0)” function.

\subsubsection{Deblurring Results}
\label{deblurring results}



\begin{table}[b]
\vspace{1mm}
  \caption{ Quantitative evaluations (PSNR/SSIM) using the imaging system (Edmund Lens \#63762 and onsemi AR1820HS sensor). }
  \vspace{-2mm}
  \hspace{-5mm}
\scriptsize
\renewcommand{\arraystretch}{1.3}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\hline
\multirow{3}{*}{}           & \multicolumn{3}{c}{Deblurring methods}                            \\ 
                                & MPRNet   & \multicolumn{1}{l}{Restormer} & FFTFormer\\ \hline
Degradation Transfer  w/o noise& 30.546/0.873& 30.691/0.871& 30.534/0.872\\
Fast Two-step w/o noise&            30.217/0.870& 30.340/0.869& 30.335/0.868\\
Ours w/o noise&            \textbf{31.243}/\textbf{0.894}& \textbf{31.506}/\textbf{0.894}& \textbf{31.358}/\textbf{0.891}\\
\hline

Degradation Transfer~\cite{chen2021extreme} w/ noise&            30.326/0.860& 30.417/0.861& 30.308/0.860\\
Fast Two-step~\cite{eboli2022fast} w/ noise&            30.097/0.865& 30.144/0.863& 30.127/0.862\\
Ours w/ noise&           \textbf{31.018}/\textbf{0.889}& \textbf{31.271}/\textbf{0.887}& \textbf{31.145}/\textbf{0.887}\\ \hline
\end{tabular} \label{tab:compare_deblur}
}
\end{table}




Different from the approach in~\cref{accuracy of estimate}, it is crucial to account for the camera pipeline when evaluating deblurring results. To minimize the impact of non-linear operations in the ISP, we assume PSF-induced blur occurs in the linear RGB image.

Thus, we estimate the PSF from a linear RGB checkerboard image. Specifically, a clear checkerboard image is convolved with the ground-truth PSF, followed by mosaicing and demosaicing, to obtain a linear RGB checkerboard image from which we estimate the PSF. We evaluate both noise-free and noisy scenarios during SFR measurement, adding 1\% noise to the blurry checkerboard image in the noisy scenarios. These noisy checkerboard images are then used to estimate the PSF.

The estimated PSF is subsequently used to recover images. We evaluate deblurring performance using deblurring networks~\cite{zamir2021multi, zamir2022restormer, kong2023efficient}. During the training stage, we convert clear images from the Flickr2K dataset~\cite{lim2017enhanced} to linear RGB images through unprocessing~\cite{brooks2019unprocessing}. These images are then convolved with the estimated PSF, followed by color correction and gamma correction to produce blurred images. Both blurred and clear images are fed into the networks for training. In the testing stage, input blurry images are generated using the same process but with the ground-truth PSF.

As shown in~\cref{tab:compare_deblur}, our approach consistently outperforms others in both noise-free and noisy scenarios.



\subsection{Experiments on Real Captures}

We conduct experiments using real captures from the same device used in the simulations (Edmund Lens \#63762 and IDS camera with onsemi AR1820HS sensor). 


We capture checkerboard images in the laboratory to estimate the PSF, followed by training Restormer~\cite{zamir2022restormer} (as described in~\cref{deblurring results}). The pre-trained Restormer is subsequently applied to deblur the captured images. For comparison, we follow the same procedure with two other PSF estimation methods~\cite{chen2021extreme, eboli2022fast}.


% We capture checkerboard images in the laboratory to estimate PSF by our methods, followed by training the Restormer\cite{zamir2022restormer} using synthetic images. The pretrained network is then used to deblur captured images. Also the other two PSF estimation works~\cite{chen2021extreme,eboli2022fast} are used for comparison by following the same process.



\subsubsection{Experiment Setup}


% We capture checkerboard images in the laboratory by custom-built device (Edmund Lens \#63762 and IDS camera with onsemi AR1820HS sensor). The camera is mounted on a tripod to capture checkerboard images secured on a card holder, with two angled LED vertical surface lights providing uniform illumination~\cite{ISO12233-2014}. refer to \cref{image capture} for details.

We capture checkerboard images in the laboratory using a custom-built device comprising an Edmund Lens \#63762 and an IDS camera (onsemi AR1820HS sensor). The camera is mounted on a tripod, aimed at a checkerboard secured on a card holder, with two angled LED surface lights positioned vertically to provide uniform illumination~\cite{ISO12233-2014}. See \cref{image capture} for further setup details.



\subsubsection{Recovery Comparison}
We estimate the PSF according to the process outlined in~\cref{fig:method}. These estimated PSF are then applied to recover images, followed by an evaluation of the deblurring performance. To reduce cumulative degradation in ISP pipeline, we assume that convolution takes place in the linear RGB domain. Under this assumption, we estimate PSF from linear RGB checkerboard images. To prepare images in the training stage, we first convolve the PSF with linear RGB images generated by unprocessing method~\cite{brooks2019unprocessing},  then apply color correction, gamma correction, and tone mapping to generate blurry sRGB images.

As shown in~\cref{fig:comparision}, a comparison of image patches demonstrates that our method effectively sharpens the image, outperforming others in terms of MUSIQ and MANIQA scores (the higher the better), leading to improved image quality. 





