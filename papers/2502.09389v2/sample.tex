
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10pt, conference]{template/ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\usepackage[T1]{fontenc}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[]{changes}
\definechangesauthor[name={Quantao}, color=red]{qyg}
\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}
\usepackage{chngcntr}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{subfigure}
\usepackage{chngcntr}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{subfigure}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%
\usepackage{makecell}
\usepackage{diagbox}
\usepackage{hyperref}
\usepackage{balance}
\urlstyle{rm}  % Use sans-serif font for URLs



\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document


\title{\LARGE \bf
% S$^2$-Diffusion: Open-Vocabulary Semantic Diffusion Policy for \\Robot Manipulation Tasks \\
S$^2$-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation
}
\author{Quantao Yang$^{*1}$, Michael C. Welle$^{*1,2}$, Danica Kragic$^1$, and Olov Andersson$^1$ % <-this % stops a space
\thanks{This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation and the European Union's Horizon Europe Framework Programme under grant agreement No 101070596 (euROBIN).}% <-this % stops a space
\thanks{$*$These authors contributed equally.}% <-this % stops a space
\thanks{1 %Authors are with 
Division of Robotics, Perception and Learning (RPL), KTH Royal Institute of Technology, Sweden. {\tt (e-mail: quantao@kth.se)}.
}%
\thanks{2 %Authors are with 
INCAR Robotics AB, Sweden.}
}%


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
% Robot manipulation in unseen environments remains a challenging problem, particularly in the context of open-vocabulary tasks where generalization is crucial. 
% This work introduces a novel framework for skill learning, leveraging object-level spatial-semantic features to bridge the gap between instance-specific and category-level generalization. By conceptualizing a skill as a functional capability that progressively specializes into subskills—ranging from broad, category-level skills to instance-specific tasks—we demonstrate how manipulation policies can be trained on narrow distributions and still achieve robust generalization to new instances and categories. 
% Our approach integrates semantic understanding with geometric representations, using depth-informed semantic feature fields to learn an Open-Vocabulary spatial-semantic Diffusion policy (S$^2$-Diffusion). This enhances the robot's ability to remain invariant to task-irrelevant changes in observations and adapt to diverse objects and materials. We validate our framework through extensive evaluations on a diverse set of robotic manipulation tasks in both simulation and the real world, highlighting its efficacy in scenarios requiring nuanced semantic and spatial understanding. Our results show that this method facilitates skill transfer to novel object-specific tasks, achieving robust performance even in scenarios with limited training data. Videos and more details are available at \url{https://yquantao.github.io/S$^2$-Diffusion}.


Recent advances in skill learning has propelled robot manipulation to new heights by enabling it to learn complex manipulation tasks from a practical number of demonstrations. However, these skills are often limited to the particular action, object, and environment \textit{instances} that are shown in the training data, and have trouble transferring to other instances of the same category.
In this work we present an open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) which enables generalization from instance-level training data to category-level, enabling skills to be transferable between instances of the same category. We show that functional aspects of skills can be captured via a promptable semantic module combined with a spatial representation. We further propose leveraging depth estimation networks to allow the use of only a single RGB camera.
Our approach is evaluated and compared on a diverse number of robot manipulation tasks, both in simulation and in the real world. Our results show that S$^2$-Diffusion is invariant to changes in category-irrelevant factors as well as enables satisfying performance on other instances within the same category, even if it was not trained on that specific instance.
Full videos of all real-world experiments are available in the supplementary material.




% In this paper we introduce a novel approach that leverages object-level spatial-semantic features to improve the generalization ability of a manipulation policy. Our method integrates depth estimation into the semantic feature field to learn an Open-Vocabulary spatial-semantic Diffusion policy (S$^2$-Diffusion), which improves the robot's capacity %{\color{red} to understand and interact with diverse objects and environments. MW: this is a bit stron claim imo} 
% to be invariant to task irrelevant changes in the observations. We evaluate our method on a set of diverse robotic manipulation tasks in simulation as well as the real-world to demonstrate that our method leads to more generalizable and effective robotic manipulation policies. {\color{red}, particularly in scenarios where understanding the semantics and spatial structure of the environment is essential. MW: I donät think we can claim this as it's all in the demonstrations ...} Videos and more details are available at \url{https://yquantao.github.io/S$^2$-Diffusion}.





\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Introduction}}
\label{sec:introduction}




Imitation learning (IL)~\cite{mandlekar2020iris, mandlekar2020learning} has shown potential in enabling robotic manipulation in challenging real-world scenarios by learning complex skills from human demonstrations. Still, existing IL methods often struggle to generalize beyond the specific training environments from which the demonstrations are derived. This is an important obstacle as each new environment requires labor-intensive data collection, model fine-tuning, and retraining to adapt the learned policies.

For humans, transferring knowledge between tasks and skills, such as transferring the scooping skill from rice to cereals, is rather straightforward. Scooping rice or cereals may be considered as different instances of the same task for current IL methods. 
The ability to generalize over such instances is still a challenge and requires rather advanced spatial-semantic understanding~\cite{zhu2023viola}.
The ability to transfer and generalize over instances removes the necessity for extensive training and also allows for assessing what type of instances one can transfer over - for example, scooping ice cream may be very different from scooping granular material such as cereals or rice. Thus, granular materials may be seen as the same category as rice and cereals, while ice cream is an instance of another category which requires a very different policy when executing the scooping task. 

\begin{figure}[t]
	\centering
	%never specify both height and width for a figure! Let Latex make sure you keep aspect ratio.
	\includegraphics[width=0.9\linewidth]{fig/s2diff_fig1.png}
	%
	\vspace{-0.0cm}% <--- this magic squeezes the space between figure and caption
	\caption{Our open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) not only efficiently completes the task at hand but also enables the generalization of the same skill across diverse contexts and task variations. For instance, a rice-scooping policy can be directly transferred to scoop cereal without any fine-tuning.}
	\label{fig:frontpage} %always label figures for reference within the text
	\vspace{-0.0cm}% <--- this magic squeezes the space between the caption and the rest of the text
\end{figure}

Large pre-trained Visual-Language-Action (VLA) models~\cite{o2023open, team2024octo, wen2024tinyvla} generalize simple skills such as pick-and-place over a wide range of environments and objects. However, more complex non-prehensile manipulation such as scooping, mug-flipping, cooking shrimp or opening a bottle with a bottle opener stay elusive for such large general models.
Recent imitation learning approaches address learning of instances of challenging manipulation tasks ~\cite{mandlekar2020iris, mandlekar2020learning, zhao2023learning, gao2024prime, chi2023diffusionpolicy, ingelhag2024robotic, fu2024mobile} but the integration with semantic knowledge in highly-complex manipulation tasks remains a challenge~\cite{chi2023diffusionpolicy}. The aforementioned methods
often rely on raw perceptual features and environmental conditions, limiting their applicability 
to the instances observed during the training. Training a skill via imitation learning, such as a diffusion policy~\cite{chi2023diffusionpolicy}, relies on expert demonstrations that often do not cover several instances of the same task. We show 
that when trained only on a particular instance of a task - such as wiping red scribbles from a whiteboard - the skill fails to transfer already when the scribbles are now green, even if the required action and environment for wiping is exactly the same. This is because the policy did not learn a \textit{whiteboard-wiping} category skill but a single instance of this category namely \textit{red-whiteboard-wiping}.

Motivated by the above challenges, we present a novel approach that integrates spatial prediction with semantic segmentation features from large pre-trained models~\cite{liang2023open, yang2024depth} to generalize from expert demonstrations on a single instance of a task to its \textit{category-level skill} -- such as wiping different color scribbles from the whiteboard or scooping different granular material from bowl to bowl. %as seen during training. 
Our method uses a single RGB camera view combined with the proprioceptive information of the robot.
As shown in Fig.~\ref{fig:frontpage}, we extract semantic information using the prompted foundation model~\cite{ren2024grounded} and combine it with a depth estimation foundation model~\cite{yang2024depth2} to obtain \textit{Spatial-Semantic} observations for the visuomotor diffusion policy learning framework.
This allows for invariance to task-irrelevant factors such as background and object textures
%or distractor objects 
as well as the capability to generalize from instance-level to category-level skills. 
Our key contributions are threefold:
\begin{enumerate}
    \item We propose to endow visuomotor diffusion policies with spatial-semantic understanding to enable generalization from instance-level to category-level skills in robot manipulation tasks.
    \item We introduce an efficient representation of the spatial-semantic information via a combination of vision foundation models. The overall framework is real-time viable and requires only a single RGB camera and the robot's proprioceptive observations.
    \item Our extensive experiments evaluate the method on a set of robotics manipulation tasks in simulation and the real world, demonstrating the ability to learn generalizable and effective robotic manipulation policies.
\end{enumerate}
All real-world experimental videos ($174$) are available in the supplementary material.




% Robotic manipulation tasks, especially those performed in unstructured and dynamic environments, demand a high degree of adaptability and generalization. One of the key challenges in this domain is enabling robots to identify and interact with relevant objects while ignoring extraneous details. However, the ability to generalize learned skills across different scenarios and tasks remains a fundamental challenge in robotics. For humans, this process is facilitated by our innate ability to leverage \textit{spatial-semantic} information. When faced with a task, we instinctively focus on objects and features that are pertinent to the goal, such as picking up a cup, opening a door, or assembling parts. In robotic manipulation, replicating this level of understanding and adaptability is crucial~\cite{zhu2023viola}. This ability not only helps us efficiently complete the task at hand but also allows us to \textbf{generalize the same skill} across different contexts and variations of the task.

% Semantic information provides robots with the contextual understanding necessary to differentiate between relevant and irrelevant aspects of a scene. For example, when tasked with picking up a cup, a robot equipped with spatial-semantic knowledge can identify the cup among various other objects (\textit{semantic} features) and discern its graspable regions (\textit{spatial} features), ignoring distractions like the surrounding clutter. Moreover, semantic information allows for the transfer of learned skills to new, unseen environments by recognizing objects of similar categories, even if their appearances differ slightly from what the robot has encountered before.




% Recently, Imitation Learning (IL)~\cite{mandlekar2020iris, mandlekar2020learning, zhao2023learning, gao2024prime} has proven to be an effective method for training robot policies with diverse human demonstrations, enabling manipulation tasks of high complexity such as   sauce pouring and spreading, mug flipping~\cite{chi2023diffusionpolicy}, cooking shrimp and wiping wine~\cite{fu2024mobile}, opening caped bottles using a bottle opener~\cite{ingelhag2024robotic}. However, despite the significant progress in robotics, the integration of semantic understanding into manipulation tasks remains challenging~\cite{zheng2024survey}. The aforementioned imitation learning %~\cite{chi2023diffusionpolicy} 
% often rely on raw perceptual features and environmental conditions, limiting their applicability in real-world scenarios where variability and unpredictability are the norms. Training an imitation policy~\cite{chi2023diffusionpolicy} for a specific task typically requires a set of expert demonstrations. Additionally, transferring the learned policy to a new but related task can be challenging. 
% This gap highlights the need for approaches that can incorporate semantic information into robotic manipulation policies, generalizing learned skills and enabling robots to perform tasks with the same level of contextual awareness and adaptability that humans naturally exhibit in diverse settings.

% The ability to generalize a learned skill mirrors a fundamental human capability: applying context-aware, semantically grounded reasoning to guide actions. In robotic manipulation, this translates to understanding the semantic relationships between objects and actions, ensuring that task execution remains robust even in novel situations. However, integrating semantic understanding into manipulation tasks is a non-trivial challenge~\cite{zheng2024survey}, requiring methods that combine high-level reasoning with precise low-level control.

% In this work, we aim to address this challenge by developing a novel approach that integrates semantic segmentation features with spatial prediction from large pretrained Vision-Language Models (VLMs)~\cite{liang2023open, yang2024depth} to enhance a robot's ability to generalize across different tasks. By embedding spatial-semantic information into the robot's decision-making process, we enable it to focus on the most relevant objects and features for a given task while being more invariant to task-irrelevant changes, thereby improving its efficiency and versatility. Our key contributions for this work are threefold: 1) We propose to construct an object-level, depth-aware semantic representation by leveraging large-scale pretrained vision-language models; 2) We learn an Open-Vocabulary Diffusion policy (\textbf{S$^2$-Diffusion}) by utilizing the extracted semantic and spatial representation; 3) We comprehensively? evaluate our method on a set of robotic manipulation tasks in simulation and the  real world and demonstrate that our method leads to more generalizable and effective robotic manipulation policies.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Related Work}}
\label{sec:related_work}

\subsection{Visual Feature Based Imitation Learning}

In recent studies, visual imitation learning methods~\cite{jang2022bc, mees2022matters, xian2023chaineddiffuser, ha2023scaling} have shown great potential in diverse robot manipulation tasks. The recent work of Zhu et al.~\cite{zhu2023viola} demonstrates the potential of object-based representations for manipulation by using a conventional closed-set object detector. In~\cite{wu2023unleashing}, the authors validate that large-scale video generative pre-training is able to effectively benefit visual robot manipulation learning. However, in their works, the finetuned policy is not demonstrated to generalize to novel instances of the task. 
3D vision based imitation learning~\cite{ke20243d, shridhar2023perceiver, gervet2023act3d, shen2023distilled} has shown to improve performance but typically requires multiple posed views with RGB-D sensors to reconstruct the scene. Another work~\cite{qiu2024learning} also ulitizes pretrained VLMs to provide scene representation for robot tasks, but their approach needs to construct a 3D neural feature field by  scanning part of the scene. In our paper, we propose a visuomotor imitation learning method that takes advantage of object-aware semantic representation to focus on task-relevant objects for training a visuomotor policy. 

%we improve the generalization ability of imitation learning algorithms in vision-based manipulation tasks by endowing them with segmentation masks and the corresponding depth estimation for the scene. 

Recently, Wang et al.~\cite{wang2024gendp} introduce a novel framework that enhances generalization capabilities by incorporating explicit spatial and semantic information into policy learning. This method uses multi-view RGB-D observations to generate 3D descriptor fields and then constructs semantic fields that highlight task-relevant parts of objects (e.g., a knife handle or soda can tab). By combining 3D geometric and semantic information, this approach achieves significant improvements in generalization at the cost of requiring multiple RGB-D cameras.



\subsection{Vision Language Models in Robotics}

Vision-Language Models (VLMs), trained on vast amounts of internet data, have found widespread use in robotic applications~\cite{radford2021learning, shah2023lm, shang2024theia}.
Some works~\cite{liu2024ok, chen2023open, hao2024language, wang2024gendp} have explored leveraging 2D foundation vision models to construct open-vocabulary 3D representations. These approaches involve utilizing visual features from pretrained foundation models like CLIP~\cite{radford2021learning} or SAM~\cite{kirillov2023segment} to scene representations that benefit learning downstream robot policy. The most recent works~\cite{liang2023open, zou2024segment, oquab2024dinov} demonstrate superior open-vocabulary classification ability which succeed in generalizing to unseen classes.
Pretrained foundation models are also utilized by either being prompted or fine-tuned to generate high-level task plans~\cite{ahn2022can, chen2023open} and to shape reward functions~\cite{ma2023eureka, ma2024dreureka, mahmoudieh2022zero} for reinforcement learning. In~\cite{reuss2024multimodal}, the authors introduce a multimodal diffusion transformer policy for solving long-horizon tasks. In contrast, our approach concentrates on improving generalization ability for open-vocabulary manipulation policy. MOKA~\cite{liu2024moka} addresses robotic manipulation by using VLMs to mark open-world keypoint affordances. The core idea behind both MOKA and our method is similar, that is guiding the policy to focus on task-relevant objects. However, our method utilizes a more generalizable semantic representation and also depth information for the entire scene.


\subsection{Vision Language Actions in Robotics}
Large policies or Vision-Language Actions (VLAs), pretrained on a combination of Internet-scale data and diverse robot demonstrations, is another promising research direction for generalizable policy learning in robotics. Recent works~\cite{brohan2023rt, team2024octo, kim2024openvla, zhen20243d} propose to co-finetune robot trajectory data alongside vision-language data, demonstrating impressive generalization capabilities with novel objects and instructions. However, these are  complex and computationally costly attempts at general manipulation that while providing encouraging results at task reasoning have so far not been able to compete with imitation-based skill learning on non-prehensile and contact-rich manipulation tasks. To enhance generalization ability, \cite{doshi2024scaling} further proposes a cross-embodied robot policy trained on the diverse robot dataset to control robots with varying observation and action types. For comparison, our method focuses on policy generalization across different tasks by concentrating solely on the segmentation masks of target objects.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Towards Generalizable Robotic Skills}
%We start by a  Category-level and Instance-level skills in more detail. 

We start with a question: "What does it mean for a skill to be \textit{generalizable}?".
% When learning from expert demonstrations, especially when using RGB observations to inform the robot's policy, the skill's applicability is constrained by the requirement that the observations received during inference must come from the same distribution as the training observations. Otherwise, the skill's performance can quickly degrade.
Commonly, "mug flipping" trained on a specific mug with distinct appearance and geometry will not generalize to flipping other mugs. Instead, the learned skill remains instance-specific, as neural network-based methods optimize the training loss without explicit interpretability of the learned features. Consequently, if these (self-selected) features are absent in another mug instance, the skill will fail to perform effectively.

One approach to mitigate this limitation is to explicitly learn features that are shared across all instances within a category. For example, a "mug" could be characterized by shared geometric features, such as a cylindrical body and a handle. Works such as ShapeNet~\cite{chang2015shapenet} provide large-scale repositories of 3D shapes categorized into object classes, enabling methods that are able to learn category-level features that can be generalized over individual instances~\cite{geng2023gapartnet}. These approaches enable generalization across objects within the same class while preserving distinctions between categories.

This object-centric approach has proven effective for tasks involving well-categorized objects like mugs, shoes, pens, or soda cans. However, tasks like wiping or scooping present unique challenges that cannot be addressed solely by these methods. 
In this work, we follow a more \textit{action}-centric approach, that considers the \textit{semantic functionality} of the skill rather than specific object categories.

In this context, the concept of affordances~\cite{lopes2007affordance} provides a useful lens to interpret our action-centric approach. Affordances, which describe the potential actions that an object or environment can support, emphasize functionality over rigid categorization. While our work focuses on learning lower-level skills and contact-rich tasks, which are difficult to reduce to affordances, it shares a similar philosophy by focusing on the semantic functionality of skills rather than specific object categories.


%These tasks require understanding not only object categories but also material or task-specific properties, such as the granular texture and appearance of scooped material or the varying characteristics of surfaces and marking colors in wiping tasks. Addressing these complexities necessitates novel frameworks for material and action generalization, which will be discussed further in the subsequent sections.

\subsection{Functional and Semantic Abstractions}

When considering robotic skills, it is essential to move beyond instance-specific implementations and emphasize a leveled abstraction of skills. This facilitates generalization across instances and categories, aligning with the notion that skills can share functional similarities despite differing in object appearances, materials, or environmental contexts.
In Fig.~\ref{fig:skill_h} we show an example of such a skill levels - depicting the functional goals on the top, able to be achieved with different category-level skills and performed on instances of that category. 
\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{fig/skill_h.png}
    \caption{Skill level example for flipping and scooping. The functional goal is stated as an abstract action on the top row, second row shows examples of category-level skills that fall under this functional similarity. The bottom row shows Instances of the respective categories.}
    \label{fig:skill_h}
\end{figure}
As the example in Fig.~\ref{fig:skill_h} shows, 
the skill flipping represents a generic functional task: "the act of inverting an object". Within this abstraction, mug flipping is a category-level skill that applies to any object classified as a "mug". However, when trained specifically on a single mug (e.g., "a blue cylindrical mug with a narrow handle"), the skill becomes an instance-specific skill, which may not generalize to mugs with varying shapes, textures, or weights.
Similarly, the skill scooping provides another illustrative example. At its core, scooping refers to the functional action of transferring a granular or semi-solid material from one location to another.
Category-level skill: Bowl-to-bowl scooping captures the general action of transferring materials between two containers, encompassing a wide range of granular and semi-solid materials.
Instance-level skills: Tasks such as rice scooping or cereal scooping are more specific, as they have different visual appearances as well as grain size and density.

In this work, we define an instance-level skill as a skill learned from specific demonstrations that are tightly coupled to a particular object, material, or environmental setup. For example, a \textit{rice-bowl-to-bowl scooping} policy trained exclusively on a dataset containing only rice would constitute an instance-level skill, as the robot may fail to generalize when attempting to scoop a different granular material, such as cereal or sand.
On the other hand, a category-level skill is a more abstract representation of a skill that remains valid across multiple instances within a category. For example, \textit{bowl-to-bowl scooping} encompasses a variety of granular materials, such as rice, cereal, or lentils, enabling the robot to execute the task reliably across these different instances. Category-level skills incorporate a more generalized functional understanding, allowing the robot to adapt to new but related scenarios without explicit retraining.


In practice, all training data - in the real world - will come from instance-specific setups for a category-level skill. One way to achieve generalization is simple to present the imitation learning method with a wide range of instances in order to cover the whole category and achieve generalization. This approach, however, has the drawback of being very labor-intensive and therefore costly when done in the real world.
We address this shortcoming by focusing on Spatial-Semantic features that can be extracted from a single instance but are still informative enough so that generalization can happen. In other words when we learn a visuomotor diffuson policy with Spatial-Semantic observations - instead of RGB - we can generalize from instance-level to its category-level skill.




% We consider a skill as a broad functional capability that is progressively specialized into finer-grained subskills, based on category or instance-level requirements. For instance, the skill flipping represents a generic functional task: the act of inverting an object. Within this abstraction, mug flipping is a category-level skill that applies to any object classified as a "mug." However, when trained specifically on a single mug (e.g., a blue cylindrical mug with a narrow handle), the skill becomes an instance-specific skill, which may not generalize to mugs with varying shapes, textures, or weights.

% Similarly, the skill scooping provides another illustrative example. At its core, scooping refers to the functional action of transferring a granular or semi-solid material from one location to another.
% Category-level skill: Bowl-to-bowl scooping captures the general action of transferring materials between two containers, encompassing a wide range of granular and semi-solid materials.
% Instance-level skills: Tasks such as rice scooping or cereal scooping are more specific, as they have different visual appearances as well as grain size and density. the concept of this hierarchy and how on what level transfer is enabled is illustrated in Figure {\color{red} TODO}.

% Training data for robotic skills is often limited to instance-specific demonstrations, as collecting data across a wide range of objects, materials, and scenarios is both time-consuming and resource-intensive. This limitation means that models are typically exposed to a narrow distribution during training, which can hinder their ability to generalize to new instances or categories. To address this challenge, we propose a framework that enables the transfer of learned skills to new object-specific tasks, even when trained on data from only a single instance. By leveraging semantic understanding and geometric representations, this approach allows policies to generalize beyond the training distribution, achieving robust performance in tasks involving related but unseen objects or materials. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/OV-Diffusion-v3.pdf}
      \caption{\textbf{S$^2$-Diffusion Architecture}. The architecture is composed of three components: a pretrained semantic segmentation model \textit{Grounded-SAM2}~\cite{ren2024grounded}, a pretrained depth prediction model \textit{DepthAnythingV2}~\cite{yang2024depth} and a U-Net denoising diffusion policy~\cite{chi2023diffusionpolicy}. We design an object-aware spatial-semantic representation that is leveraged for denoising probabilistic model.}
      \vspace{0mm}
      \label{fig: architecture}
\end{figure*}


\section{\uppercase{Problem Formulation}}
Let $\Psi$ be the space of RGB images. Consider a manipulation task given the language instruction of the target object $\mathcal{L}$ (e.g., "coffee mug"). Our goal is to learn a generalizable imitation learning policy by leveraging spatial-semantic representation from pretrained Vision-Language models.

We consider a robotic task as a Markov Decision Process (MDP) defined by a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho, \gamma)$ of states, actions, transition probability, reward, initial state distribution, and discount factor. We assume access to a dataset $D=\{\tau_i\}_i^N$ of $N$ demonstrated expert trajectories $\tau_i=\{(s_0,a_0),...,(s_{T_i}, a_{T_i})\}$ for the task. Our method uses a denoising diffusion process~\cite{chi2023diffusionpolicy} to learn imitation policy $\pi$ from $D$. To generalize
from individual instances to unseen instances from the same category,
% {\color{red}to new object instances during deployment,} 
we propose to build a spatial-semantic representation. In this paper, $\mathcal{S}$ is the state space composed of visual spatial-semantic representation $f_v$ and
robot proprioception states $q$. $\mathcal{A}$ is the action space of robot end-effector commands. We aim to learn a policy $\pi_{\theta}(a|s): \mathcal{S} \rightarrow \mathcal{A}$ with parameters $\theta$ that predicts action $a$ according to current state $s$ for an MDP by leveraging the prior experience contained in the dataset $D$.



%We aim to learn a visuomotor policy $\pi_\theta$ that predicts the action $a_t$ of robot end-effector command given the current state $s_t$ consisting of semantic representation $o_t$, depth prediction $d_t$ and the robot states $r_t$:
%\begin{equation}
%	\pi_\theta(a_t|s_t)=\pi_\theta(a_t|o_t, d_t, r_t).
%\end{equation}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Method}}
% for example: Encoding motion behaviors as Gaussian Process models
\label{sec:method}

%\todo{Describe the work you have done in a way that other( student)s are able to re-implement it.}



Our objective is to develop an open-vocabulary spatial-semantic visuomotor policy that can generalize from an individual instance to other unseen instances resulting in a category-level skill.
% {\color{red}incorporates object-level semantic representation to improve the generalization ability of manipulation policies. }
We propose open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion), an approach that leverages three main components in policy learning: a semantic segmentation model, a depth prediction model and a diffusion policy shown in Fig.~\ref{fig: architecture}. The policy is trained with demonstrations from expert teleoperation, using only RGB images and robot proprioception as the state, and end-effetor velocities as the commanded actions respectively.

% Our proposed approach will enable the learning of a visuomotor policy that takes into account the task-relevant concepts from the segmentation masks provided by the segmentation model for efficient task execution.



% \begin{figure*}[t!]
% \centering
% %
% \subfigure[]{\includegraphics[height = 0.3\linewidth]{fig/sample_raster}
% \label{fig:method-a}
% }
% \subfigure[]{\includegraphics[height = 0.3\linewidth]{fig/sample_vector}
% \label{fig:method-b}
% }
% \vspace{-0.2cm}
% %
% %
% \caption{S$^2$-Diffusion architecture. Given an image observation and a language description, the model predicts 7-dimensional robot end-effector action commands. Our S$^2$-Diffusion model is composed of three }
% \label{fig: architecture}
% \vspace{-0.5cm}
% \end{figure*}



\subsection{Diffusion for Robot Skill Learning}
Following previous works~\cite{chi2023diffusionpolicy, Ze2024DP3}, we formulate the visuomotor policy as a conditional Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising, pearce2023imitating}. Starting from random action $a^K$ sampled from Gaussian noise, the diffusion probabilistic model $\epsilon_{\theta}$ performs $K$ iterations of denoising. This process gradually produces a sequence of actions with decreasing noise levels, $a^K, a^{k-1}, ..., a^0$, until the noise-free action $a^0$. Each action denoising iteration is described as: 
\begin{equation}
    a^{k-1} = \alpha_k(a_k-\gamma_k\epsilon_{\theta}(a^k, o, k))+\sigma_k\mathcal{N}(0, I),\\
\end{equation}
where $o$ is the observation for the policy. $\alpha_k$, $\gamma_k$ and $\sigma_k$ are referred as noise schedule for each iteration $k$, and $\mathcal{N}(0, I)$ is the Gaussian noise added to the action.

To learn the action predicting model $\pi_{\theta}$, we randomly sample the robot action $a^0$ from the demonstration dataset $D$ and add the noise $\epsilon^k$ for a random iteration $k$. The training loss for the diffusion model is formulated as:
\begin{equation}
\label{eq: training_loss_1}
%    \mathcal{L} = \textit{MSE}(\epsilon^k, \epsilon_{\theta}(\bar{\alpha_k}a^0+\bar{\beta_k}\epsilon^k,k,v,q)),\\
    \mathcal{L} = \textit{MSE}(a^0, \pi_{\theta}(a^0+\epsilon^k, o, k)), \\
\end{equation}
where we use an action sampling approach rather than a noise prediction model to enhance the generation of high-dimensional actions.



\subsection{Open-Vocabulary Spatial-Semantic Representation}
\label{sec:ov representation}
We aim to design spatial-semantic-aware representation that is leveraged for the observation of the above denoising probabilistic model. We utilize two pretrained VLMs, \textit{Grounded-SAM2}~\cite{ren2024grounded} and \textit{DepthAnythingV2}~\cite{yang2024depth}, for open-vocabulary semantic segmentation and depth map estimation respectively. We use \textit{Grounded-SAM2} model to perform zero-shot semantic segmentation, leveraging CLIP-based~\cite{radford2021learning} mask classification to segment unseen classes. We combine the extracted features to construct a spatial-semantic representation that is leveraged as the input for the visuomotor diffusion policy. 




We utilize \textit{Grounded-SAM2} to segment an image into a set of semantic masks $(z_1, z_2, ..., z_{n})$ from visual observations based on text descriptions. We apply pixel-wise maximum pooling for each pixel location $(i, j)$ across all segmentation masks:
\begin{equation}
z_f(i, j) = \max_{s=1}^{n} z_s(i, j),
\end{equation}
where \( z_s(i, j) \) represents the pixel value at position \( (i, j) \) in the \( s \)-th mask. In this way, we combine multiple segmentation masks into a single mask $z_f$ where each pixel represents the most confident prediction from the set of masks.

% Specifically,  It involves selecting the maximum value for each pixel across all available masks and results in a single mask 

\begin{algorithm}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \vspace{0.1cm}
    \Input{Semantic query $l$, image observation $o_t$, robot state $q$, \textit{Grounded-SAM2} model $C_1$, \textit{DepthAnythingV2} model $C_2$ 
    }
    % \Output{S$^2$-Diffusion policy $\epsilon_\theta(P_{t:\text{end}} | O_t, L,\epsilon_k)$}
    Collect demonstrated trajectories $D=\{\tau_i\}_i^N$\\
    \For{epoch n=1,N}
    {          
        Sample raw image $o_t$ and robot action $a^0$ \\
        Obtain spatial-semantic representation \\
        \hspace{1cm} $z_s=C_1(o_t, l)$, $z_d=C_2(o_t)$ \\
        \hspace{1cm} $z_f(i, j) = \max_{s=1}^{n} z_s(i, j)$ \\
        % \hspace{1cm} $f_v = z_f \|  z_d$ \\
        \hspace{1cm} $f_v = \text{ResNet}(z_f \|  z_d)$ \\
        % \hspace{1cm} $f=f_v \oplus q$
        
        Add Gaussian noise $\epsilon^k \sim \mathcal{N}(0, I)$ for step $k$ \\
        \hspace{1cm} $a^k=a^0+\epsilon^k$ \\
        
        Train the policy \\
        \hspace{1cm} $\mathcal{L} = \textit{MSE}(a^0, \pi_\theta (a^k, k, f_v, q))$
    }		
    
    \Return the trained policy $\pi_\theta(a_t|o_t, q, k)$
    \caption{Learning Open-Vocabulary Spatial-Semantic Diffusion Policy}
    \label{alg:training policy}
\end{algorithm}




To improve the spatial reasoning of the imitation policy, we propose to incorporate depth map of the task space into its semantic representation. Specifically, the input image is also processed separately with a pretrained 335M-parameter \textit{
DepthAnythingV2} model~\cite{yang2024depth} that shows promising performance in fine-grained details. The Depth-Anything model predicts depth estimates relative to the input RGB observations rather than absolute values. This can result in inconsistencies and inaccuracies over extended tasks or manipulation horizons. To address this, we normalize the depth maps during both training and evaluation, ensuring consistency across diverse scenes and improving the model's robustness and reliability.


The resulting semantic and spatial feature vectors are concatenated along the channel dimension to form the spatial-semantic representation. Upon generating the spatial-semantic representation, we leverage it as input for a visuomotor diffusion policy~\cite{chi2023diffusionpolicy}.

% \begin{algorithm}
%     \SetKwInOut{Input}{Input}
%     \SetKwInOut{Output}{Output}
%     \vspace{0.1cm}
%     %\Input{Learning rates $\lambda_\pi, \lambda_Q, \lambda_\alpha$}
%     %\Output{Trained policy $\pi_{\theta}(z_t|s_t)$}
%     Collect demonstrated trajectories $\tau_i$\\
%     % Initialize the policy $\pi_{\theta}(z_t|s_t)$\\
%     \For{each epoch=1,M}
%     {
        
%         % Select high-level action $z_t \sim \pi_{\theta}(z_t|s_t)$\\
%         %\textcolor{red}{Decode action sequence $\boldsymbol{a_t} \leftarrow p_{dec}(a_t|z_t)$}\\
%         %Execute action $z_t$ and receive reward $r_t$\\
%         % Decode and execute impedance action sequence $\boldsymbol{a}=\{a_t,\dotsb,a_{t+H-1}\}$, receive the reward $r_t$\\
%         % Store transition tuple ($s_t$, $z_t$, $r_t$, $s_{t+1}$) in $\mathcal{B}$\\
%         % Update the policy $\pi_\theta$ to maximize the return in~(\ref{eq:rl return}) by SAC
%     }		
%     \Return the trained policy $\pi_\theta(a_t|o_t)$
%     \caption{Learning Open-Vocabulary Spatial-Semantic Diffusion Policy}
%     \label{alg:training policy}
% \end{algorithm}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/sim-tasks-v2.pdf}
      \caption{\textbf{Simulated Tasks}. We perform evaluations on five single-stage tasks from a large-scale simulation framework RoboCasa~\cite{robocasa2024}: \textit{ServeMug}, \textit{CloseDoor}, \textit{TurnOnMicrowave}, \textit{TurnOffFaucet}, \textit{MoveSoda}. RoboCasa provides 50 expert demonstrations for each of the manipulation tasks. In the demonstration datasets for each task, we assume variations in the color and shape of the target object, as well as differences in the background environment. 
      % {\color{red} MW: Some of these images look like from a completely different point in the task ... it would be good to have it clear it's at the same "stage" maybe the first frame or so?}
      }
      \vspace{0mm}
      \label{fig:sim-tasks}
\end{figure*}




\begin{table*}[ht]
\centering
\caption{Success Rate of Simulation Experiments}
\label{table: sim results}
\begin{tabular}{cccccc}
\toprule
\textbf{Method} & \multicolumn{5}{c}{\textbf{Tasks}}\\
\cmidrule(lr){2-6}
& ServeMug & CloseDoor & TurnOnMicrowave & TurnOffFaucet & MoveSoda \\
\midrule
BC-RNN                                  & 0.00 & 0.02 & 0.00 & 0.26 & 0.02 \\
BC-Transformer                          & 0.14 & 0.58 & 0.78 & 0.26 & 0.08 \\
Diffusion Policy                        & 0.12 & 0.42 & 0.34 & 0.32 & 0.16 \\
%OV-BC Transformer                       & x & x & x & x & x \\
\rowcolor{gray!30} S$^2$-Diffusion (Ours)   & \textbf{0.72} & \textbf{0.78} & \textbf{0.84} & \textbf{0.70} & \textbf{0.82} \\
\bottomrule
\end{tabular}
\end{table*}


    
\subsection{Learning Semantic Diffusion Policy}
To effectively utilize our open-vocabulary spatial-semantic representations, we adopt the CNN-based diffusion policy architecture~\cite{chi2023diffusionpolicy} as our decision-making backbone. We use Denoising Diffusion Implicit Models (DDIM)~\cite{song2020denoising} as the noise scheduler. In our paper, we denote the observation with $s_t$ that is composed of spatial-semantic feature $f_v$ and robot proprioceptive state $q$. The training loss in Equation~\ref{eq: training_loss_1} is defined as:
\begin{equation}
    \label{eq: training_loss_2}
   \mathcal{L} = \textit{MSE}(a^0, \pi_{\theta}(\alpha_k a^0+\beta_k \epsilon^k,k,f_v,q)),\\
\end{equation}
where $\alpha_k$ and $\beta_k$ are used for noise schedule of each step. The learning process for our S$^2$-Diffusion method is described in Algorithm~\ref{alg:training policy}. We combine features inferred by VLMs \textit{Grounded-SAM2} and \textit{DepthAnythingV2} to construct the spatial-semantic representation for the action denoising model. By conditioning on the tuple $(a^k, k, f_v, q)$, the denoising model learns to predict the clean action by using mean square error loss for action supervision. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Evaluation}}
% for example: Encoding motion behaviors as Gaussian Process models
\label{sec:method}


The goals of all our experiments are three-fold:
% {\color{red}
% (1) to evaluate the performance of our method on open-vocabulary robotic manipulation tasks; (2) to test whether the semantic representations from pretrained VLMs can reduce the reliance on expert demonstrations; (3) to investigate whether our method is able to achieve better generalization ability compared to the baseline. 
% }
\begin{enumerate}
    \item to evaluate and compare the performance of our method on challenging robotic manipulation tasks;
    \item to validate that generalization from instance-level to category-level skill is achieved in a real-world setting;
    \item to perform ablations in order to investigate the role of the semantic and spatial components.
\end{enumerate}
We first describe the experiments performed in the simulation, followed by the experiments on physical hardware.

% {\color{red}
% In the following, we first describe the experimental setup in~\ref{experimental setup}, then present simulation and real-world experiment results for diverse manipulation tasks in \ref{simulation experiments} and \ref{real robot experiments} respectively, lastly show ablation experiments in~\ref{ablation evaluation}.
% }

\subsection{Simulation Experiments}

\noindent
\textbf{Experiment Setup.}
For our simulation experiments, we take advantage of a recent open-sourced large-scale simulation environment, \textit{RoboCasa}~\cite{robocasa2024}, which provides expert demonstrations for diverse everyday tasks. We evaluate our method and the baselines in a set of single-stage tasks as shown in Fig.~\ref{fig:sim-tasks}: \textit{ServeMug}, \textit{CloseDoor}, \textit{TurnOnMicrowave}, \textit{TurnOffFaucet}, \textit{MoveSoda} using the provided $50$ expert demonstrations from \textit{RoboCasa}.


\noindent
\textbf{Baselines.} We compare our method with three baseline methods in simulation: 1) \textbf{BC-RNN}: a behavior cloning method with recurrent neural network implementation; 2) \textbf{BC-Transformer}: a behavior cloning imitation learning method with transformer architecture as the policy implementation~\cite{mandlekar2021matters}; 3) \textbf{Diffusion Policy}: the image-based diffusion policy of \cite{chi2023diffusionpolicy}. For real-world experiments, we compare our method against the diffusion policy. In all our experiments, we train for $500$ epochs on an NVIDIA RTX 4090 GPU. We use a batch size of $64$ and a learning rate of $1\text{e}{-4}$.

\noindent
\textbf{Simulation Results.}
 We take advantage of the open-sourced implementation for the baseline methods from \textit{RoboMimic}~\cite{mandlekar2021matters}. In the demonstration datasets for each task, we assume \textbf{variations} set by \textit{RoboCasa} in the color and shape of the target object, as well as differences in the background environment. To test the performance of our method and the baselines, we evaluate $50$ trials for each task and the corresponding success rates are listed in Table~\ref{table: sim results}, demonstrating the superior performance of the proposed S$^2$-Diffusion approach.

 
Due to the challenging variations in target object and background seen within the \textit{RoboCasa} dataset, classical imitation learning policies struggle to solve the specific single-stage manipulation task. While the baseline methods—BC-RNN, BC-Transformer, and Diffusion Policy—show relatively poor performance, particularly on tasks like ServeMug, CloseDoor, and MoveSoda, S$^2$-Diffusion consistently achieves the highest success rates. For instance, S$^2$-Diffusion outperforms the other methods with a success rate of 0.72 on ServeMug and 0.78 on CloseDoor, while the best-performing baseline, BC-Transformer, achieves only 0.14 and 0.58, respectively. On other tasks like TurnOnMicrowave and MoveSoda, S$^2$-Diffusion maintains a strong performance with success rates of 0.84 and 0.82. These results highlight the effectiveness of S$^2$-Diffusion in improving task success across a diverse range of manipulation tasks. Similar to findings in \textit{RoboCasa}~\cite{robocasa2024}, the image-based Diffusion Policy is sensitive to in object color and background. In contrast, by leveraging spatial-semantic features our method shows better robustness and generality compared to state-of-the-art alternatives.
We attribute this to the rich visual representation of combining semantic information and depth estimation for the workspace scene.

%\noindent
% \textbf{Learning Efficiency.}
%\label{learning efficiency}

% {\color{red} IF TIME ALLWOS SIMULATION RESULTS ABOUT EFFICENCY GO HERE}



\subsection{Real-World Experiments}

\noindent
\textbf{Experiment setup.}
We evaluate our S$^2$-Diffusion method on two real-world category tasks: \textit{whiteboard-wiping} and \textit{bowl-to-bowl scooping}. We collected $40$ and $60$ expert demonstrations for the red-whiteboard-wiping and rice-bowl-to-bowl scooping tasks instances respectively. The demonstrations were obtained by teleoperating a $7$-DOF Franka Panda manipulator using the Quest2ROS~\cite{welle2024quest2ros} Oculus app.
A single camera providing RGB observation was mounted on the end-effector as well as task-specific tools - such as a sponge for the whiteboard wiping and a spoon for the scooping task.
The teleoperation setup is shown in Fig.~\ref{fig:wipe_real}.
The language prompts for obtaining the semantic observations were \textit{"handwriting. sponge."} for the whiteboard wiping task and \textit{"rice. bowl."} for the bowl-to-bowl scooping task.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{fig/wipe_setup.png}
    \caption{Real-world setup for the wiping task showing the teleoperation controller held by the operator on the right.}
    \label{fig:wipe_real}
\end{figure}

\noindent
\textbf{Baselines and Ablations.} 
We compare our S$^2$-Diffusion method with the visuomotor diffusion method from~\cite{chi2023diffusionpolicy} which uses RGB and proprioceptive information as a baseline, as well as perform a number of ablations on the scooping task - namely a version of our method that only has access to semantic observations (Semantic-Diffusion) and one that only observes the spatial observations (Spatial-Diffusion).


\begin{figure*}[t]
    \centering
    \subfigure[] {
        \includegraphics[width = 0.48\linewidth]{fig/wipe_real_results_v3.png}
        \label{fig:real wiping}
    }
    \subfigure[] {
        \includegraphics[width = 0.48\linewidth]{fig/scoop_real_results_v3.png}
        \label{fig:real scooping}
    }
    
    \caption{Comparison of our S$^2$-Diffusion and the baseline on two real-world environments: whiteboard wiping and bowl-to-bowl scooping. S$^2$-Diffusion and the baseline are trained on \textit{red-whiteboard-wiping} dataset and \textit{rice-bowl-to-bowl-scooping} dataset respectively, then evaluated on the known instances and transferred to unseen instances of the two tasks. Note that for \textit{choco-cereal-btb-scooping}, \textit{hearts-cereal-btb-scooping}, \textit{mixed-cereal-btb-scooping}, and \textit{green-whiteboard-wiping} the baseline diffusion policy shows $0\%$ success rate. }
    \label{fig: real-world results}
    \vspace{-0.2cm}
\end{figure*}


\noindent
\textbf{Real-World Results.}
% \subsection{Experimental Setup}
% \label{experimental setup}
The real-world experimental results are reported in Fig.~\ref{fig: real-world results}.
%, and both the policies (S$^2$-Diffusion and RGB-based Diffusion Policy) are evaluated on two real-world environments: wiping and scooping. 
%S$^2$-Diffusion and Diffusion Policy are trained on {\color{red}NUMBER} and $60$ demonstrations of  \texttt{red-whiteboard-wiping} and \texttt{rice-bowl-to-bowl-scooping} datasets respectively.
We evaluate our method and the baseline first on the seen environment (red-whiteboard-wiping and rice-bowl-to-bowl-scooping) and evaluate the transferability on other instances of the same category - namely
black-whiteboard- and green-whiteboard-wiping and choco-cereal-, hearts-cereal-, mixed-cereal-bowl-to-bowl-scooping.




% \begin{figure*}[t]
%     \centering
%     \subfigure[] {
%         \includegraphics[width = 0.48\linewidth]{fig/real_scooping_radar.pdf}
%         \label{fig:real scooping}
%     }
%     \subfigure[] {
%         \includegraphics[width = 0.48\linewidth]{fig/real_wiping_radar.pdf}
%         \label{fig:real wiping}
%     }
    
%     \caption{Comparison of our S$^2$-Diffusion and Diffusion Policy on two real-world environments: scooping and wiping. S$^2$-Diffusion and Diffusion Policy are trained on \texttt{ScoopRice} dataset and \texttt{WipeRed} dataset respectively, then evaluated on the origin task and  transferred to unseen tasks of two environments. Note that for \texttt{ScoopChoco}, \texttt{ScoopHeart}, \texttt{ScoopMixed}, and \texttt{WipeGreen} the baseline diffusion policy shows $0\%$ success rate. }
%     \label{fig: real-world results}
%     \vspace{-0.2cm}
% \end{figure*}


% \begin{figure}[t]
% 	\centering
% 	%never specify both height and width for a figure! Let Latex make sure you keep aspect ratio.
% 	\includegraphics[width=1.0\linewidth]{fig/real_scooping.pdf}
% 	%
% 	\vspace{-0.0cm}% <--- this magic squeezes the space between figure and caption
% 	\caption{Comparison of our S$^2$-Diffusion and Diffusion Policy on real-world scooping tasks. S$^2$-Diffusion and Diffusion Policy are trained on the \texttt{ScoopRice} dataset and evaluated on the \texttt{ScoopRice} task, then transferred to three unseen tasks. Note that for \texttt{ScoopChoco}, \texttt{ScoopHeart} and \texttt{ScoopMixed}, the baseline diffusion policy shows $0\%$ success rate. {\color{red} Fig or Table?}}
% 	\label{fig:real_scooping} %always label figures for reference within the text
% 	\vspace{-0.0cm}% <--- this magic squeezes the space between the caption and the rest of the text
% \end{figure}


% \begin{figure}[t]
% 	\centering
% 	%never specify both height and width for a figure! Let Latex make sure you keep aspect ratio.
% 	\includegraphics[width=1.0\linewidth]{fig/real_wiping.pdf}
% 	%
% 	\vspace{-0.0cm}% <--- this magic squeezes the space between figure and caption
% 	\caption{Comparison of}
% 	\label{fig:real_wiping} %always label figures for reference within the text
% 	\vspace{-0.0cm}% <--- this magic squeezes the space between the caption and the rest of the text
% \end{figure}



% \textbf{Expert demonstrations.} In all our experiments, we mainly focus on single-stage manipulation tasks rather than long-horizon tasks. For our simulation experiments, we take advantage of a recent open-sourced large-scale simulation environment, \textit{RoboCasa}~\cite{robocasa2024}, which provides expert demonstrations for diverse everyday tasks. For simulation, we evaluate our method and the baselines in a set of single-stage tasks as shown in Fig.~\ref{fig:sim-tasks}: \texttt{ServeMug}, \texttt{CloseDoor}, \texttt{TurnOnMicrowave}, \texttt{TurnOffFaucet}, \texttt{PnPConterToSink} using the provided $50$ expert demonstrations from \textit{RoboCasa}. For the real-world, we perform evaluation on four \texttt{Scoop} tasks. We train our S$^2$-Diffusion model using $60$ expert demonstrations of a 7-DOF Franka Panda arm using the Quest2ROS~\cite{welle2024quest2ros} teleoperation app and choco-pops as the granular material. The evaluation is performed on an unseen granular material - rice. {\color{red}for each task, both in simulation and in the real world.} During denoising policy training, we rely on two foundational VLMs for inference, which consume significant GPU memory. To address this, we construct the spatial-semantic representations offline using the pretrained VLMs. These precomputed representations are then utilized to train the diffusion policy efficiently. 




% \textbf{Baselines.} We compare our method with three baseline methods in simulation: 1) \textbf{BC-RNN}: a behavior cloning method with recurrent neural network implementation; 2) \textbf{BC-Transformer}: a behavior cloning imitation learning method with transformer architecture as the policy implementation~\cite{mandlekar2021matters}; 3) \textbf{Diffusion Policy}: an image-based diffusion policy~\cite{chi2023diffusionpolicy}. For real-world experiments, we compare our method against diffusion policy. In all our experiments, we train for $500$ epochs on a NVIDIA RTX $4090$ GPU. We use a batch size of $64$ and a learning rate of $1\text{e}{-4}$.


% \subsection{Simulation Results}
% \label{simulation experiments}







% We compare our method S$^2$-Diffusion against all the baseline methods in simulation. We take advantage of the open-sourced implementation for the baseline methods from \textit{RoboMimic}~\cite{mandlekar2021matters}. In the demonstration datasets for each task, we assume \textbf{variations} set by \textit{RoboCasa} in the color and shape of the target object, as well as differences in the background environment. To test the performance of our method and the baselines, we evaluate $50$ trials for each task and the corresponding success rates are listed in Table~\ref{table: sim results}, demonstrating the superior performance under this conditions of the proposed S$^2$-Diffusion approach. 








% Due to the variations in the target object and background within the dataset, classical imitation learning policies struggle to solve the specific single-stage manipulation task. While the baseline methods—BC-RNN, BC-Transformer, and Diffusion Policy—show relatively poor performance, particularly on tasks like \texttt{ServeMug}, \texttt{CloseDoor}, and \texttt{MoveSoda}, S$^2$-Diffusion consistently achieves the highest success rates. For instance, S$^2$-Diffusion outperforms the other methods with a success rate of 0.68 on \texttt{ServeMug} and 0.76 on \texttt{CloseDoor}, while the best-performing baseline, BC-Transformer, achieves only 0.14 and 0.58, respectively. On other tasks like \texttt{TurnOnMicrowave} and \texttt{MoveSoda}, S$^2$-Diffusion maintains a strong performance with success rates of 0.62 and 0.42. These results highlight the effectiveness of S$^2$-Diffusion in improving task success across a diverse range of manipulation tasks. Similar to findings in \textit{RoboCasa}~\cite{robocasa2024}, image-based Diffusion Policy is sensitive to in object color and background. In contrast, by leveraging spatial-semantic features our method shows better robustness and adaptability compared to state-of-the-art alternatives.
% We attribute this to the rich visual representation of combining semantic information and depth estimation for the workspace scene.


% \subsection{Real-World Results}
% \label{real robot experiments}





\noindent
\textbf{Whiteboard-wiping:} The task involves wiping scribbles of different colors (red, black, green) from a whiteboard using the end-effector-mounted wet sponge.
The results are shown in Fig.~\ref{fig:real wiping}.
We define success as the robot completely removing the targeted handwriting using a sponge within $15$ seconds.
For the task instances that are covered by the training data (red-whiteboard-wiping), the baseline is able to succeed in all nine trials, leading to a $100\%$ success rate, however, once the policy is deployed on different instances of the task such as black-whiteboard-wiping the performance deteriorates to $4/9$ ($44\%$) and for the green-whiteboard-wiping task even to $0\%$. 

This clearly shows how even small changes in the RGB observations such as changing the color of the scribbles can lead to great deterioration and even complete failure of the skill on this instance - underscoring that the skill learning was indeed on the instance-level and \textbf{not} category-level. On the other hand, our S$^2$-Diffusion method has consistent performances across all tasks. These results highlight the ability of our S$^2$-Diffusion to learn category-level skills for handling novel tasks without requiring additional training or fine-tuning, a capability that is essential for real-world robotic applications.

% The results, depicted in Fig.~\ref{fig:real wiping}, demonstrate the robustness of our approach. We define success as the robot completely removing the targeted handwriting using a sponge within $15$ seconds. For the seen task, \texttt{WipeRed}, our policy shows a decline in performance, primarily attributed to inaccuracies in the segmentation mask that hinder effective task execution. This failure highlights the reliance of the policy on precise segmentation for optimal performance.

% When we directly transfer the policy to two unseen tasks with handwriting colors not encountered during training, our S$^2$-Diffusion policy demonstrates strong generalization and sustains comparable performance levels. In contrast, the baseline diffusion policy suffers a significant performance drop, indicating its limited capacity for adaptation to unseen scenarios. These results emphasize the robustness and adaptability of S$^2$-Diffusion in handling novel tasks without additional training or fine-tuning, which is critical for real-world robotic applications.




\noindent
\textbf{Bowl-to-bowl-scooping:}
%The policies trained from rice scooping task are transferred to unseen granular material scooping tasks --- choco ball, heart cereal, as well as mixed choco-and-heart. 
% The scooping task is a circular task - meaning while the demonstration just showcased a single scoop per demonstration, during inference time it's possible to have the policy keep scooping the granular material from one bowl into the other. We record the performance by letting the policy run for $45$ seconds and weighing the amount of granular material in grams that were successfully scooped into the empty bowl.
The scooping task is to scoop granular materials from one bowl to another. The amount of material successfully scooped into the target bowl is measured in grams. Task success is defined as the policy's ability to scoop at least $3$ grams of material into the target bowl in under $30$ seconds.
In Fig.~\ref{fig:real scooping} the success rates are reported over $10$ trials. All real-world experimental videos are available on the project website.
%\footnote{\url{https://yquantao.github.io/OV-Diffusion}}.



For the in-distribution rice-bowl-to-bowl-scooping task, our S$^2$-Diffusion and baseline achieve success rates of $1.0$ and $0.8$, respectively. However, when transferring the policy to three unseen tasks—--choco-cereal-btb-scooping, hearts-cereal-btb-scooping, and mixed-cereal-btb-scooping—--the baseline diffusion policy fails entirely, with $0.0$ success rate, while our S$^2$-Diffusion policy maintains a high success rate of approximately $0.8$ by changing the semantic prompt to \textit{"cereal. bowl."} for all three instances. This demonstrates that our method effectively generalizes from individual instances to unseen other instances of the same category.

We can see frames of the baseline diffusion policy and our method in Fig.~\ref{fig:real-s2}, where the baseline can only succeed on the instance-level task it was trained on and fails to scoop out cereal as the visual observations are too different. Our method on the other hand extracts the semantic mask from the prompt as well as the spatial information via the synthetic depth observation. This leads to the successful execution of the scooping task and to the learning of a successful category-level skill trained on individual instances only using a single RGB observation (same as the baseline) as the original input.

\begin{figure*}[t!]
    \centering
    \vspace{-0.0mm}
    \includegraphics[width=1.0\linewidth]{fig/real_world_spatial_semantic.pdf}
      \caption{Frames of the baseline diffusion policy and our S$^2$-Diffusion method for real-world scooping tasks. We show the corresponding semantic mask and synthetic depth for each frame. The baseline can only succeed on the instance-level task it was trained on. Our method succeeds in learning a category-level skill trained on individual instances only using a single RGB observation.
      }
      \vspace{-0.0mm}
      \label{fig:real-s2}
\end{figure*}



% In the second real-world environment, the task involves wiping handwriting of different colors from a surface, providing a diverse testing scenario. The results, depicted in Fig.~\ref{fig:real wiping}, demonstrate the robustness of our approach. We define success as the robot completely removing the targeted handwriting using a sponge within $15$ seconds. For the seen task, \texttt{WipeRed}, our policy shows a decline in performance, primarily attributed to inaccuracies in the segmentation mask that hinder effective task execution. This failure highlights the reliance of the policy on precise segmentation for optimal performance.

% When we directly transfer the policy to two unseen tasks with handwriting colors not encountered during training, our S$^2$-Diffusion policy demonstrates strong generalization and sustains comparable performance levels. In contrast, the baseline diffusion policy suffers a significant performance drop, indicating its limited capacity for adaptation to unseen scenarios. These results emphasize the robustness and adaptability of S$^2$-Diffusion in handling novel tasks without additional training or fine-tuning, which is critical for real-world robotic applications.

\iffalse
\begin{table}[]
    \centering
    \vspace{1.cm}
    \caption{Success Rate of Real-World Experiments}
    \label{tab:RealWolrd}
    \begin{tabular}{lcccc}
    \toprule
    Method                        & \texttt{Rice}       & \texttt{Choco}  & \texttt{Heart}  & \texttt{Mixed}\\ \midrule
    S$^2$-Diffusion (Ours)       & \textbf{1.00}    & \textbf{0.90}      & \textbf{0.90}   & \textbf{0.80}   \\ 
    Diffusion Policy          & 0.80    & 0.00   & 0.00     & 0.00      \\ \bottomrule
    \end{tabular}            
    \vspace{0.1cm}
\end{table}
\fi








% \begin{table*}[ht]
% \centering
% \caption{Ablation Studies}
% \label{tab:ablation-studies}
% \begin{tabular}{ccc|c|ccccc|c}
% \toprule
% \textbf{Only Semantic} & \textbf{Only Depth}  & \textbf{Both Semantic and Depth} & \textbf{Data} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg. Len.} \\
% \midrule

% $\times$ & $\times$ & $\times$   & ABC $\rightarrow$ D & 0.823 & 0.609 & 0.425 & 0.318 & 0.225 & 2.40 \\
% $\times$ & \checkmark & $\times$  & ABC $\rightarrow$ D & 0.815 & 0.651 & 0.498 & 0.392 & 0.297 & 2.65 \\
% \rowcolor{gray!30}$\times$ & \checkmark & \checkmark  & ABC $\rightarrow$ D & 0.869 & 0.751 & 0.636 & 0.549 & 0.465 & 3.27 \\
% \checkmark & \checkmark & $\times$ & ABC $\rightarrow$ D & 0.854 & 0.712 & 0.596 & 0.497 & 0.401 & 3.06 \\
% \rowcolor{gray!30}\checkmark & \checkmark & \checkmark & ABC $\rightarrow$ D & 0.890 & 0.773 & 0.679 & 0.592 & 0.497 & 3.43 \\
% \rowcolor{gray!30}
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table}[]
%     \centering
%     \vspace{1.cm}
%     \caption{Ablation Study}
%     \label{tab:ablation}
%     \begin{tabular}{lcc}
%     \toprule
%     Method                        & \texttt{WipingRed}       & \texttt{WipingGreen}  \\ \midrule
%     S$^2$-Diffusion (Ours)       & x    & x             \\ 
%     Only Semantic Feature      & x    & x              \\
%     Only Depth Map      & x    & x              \\ \bottomrule
%     \end{tabular}            
%     \vspace{0.1cm}
% \end{table}


\subsection{Ablation Evaluation}
\label{ablation evaluation}

To investigate the significance of integrating spatial-semantic representation, we conduct additional experiments by training the policy using only the semantic representation (Semantic-Diffusion) and only the depth map estimation (Spatial-Diffusion) on the rice and mixed-cereal-bowl-to-bowl-scooping tasks. The results of these experiments are presented in Fig.~\ref{fig:ablation only mask only depth}. The full model S$^2$-Diffusion, which integrates both modalities, achieves the highest success rates, significantly outperforming the ablated versions both on the seen task and also the novel task. 

When the depth information is excluded, the performance of the policy drops drastically, with success rates falling to $0.4$ and $0.5$ for the seen and novel tasks, respectively. This significant decline underscores the critical role that spatial information plays in successfully executing 3D tasks. In contrast, when the mask (semantic representation) is removed, the performance also declines but to a lesser extent compared to the removal of depth information. While the policy's ability to generalize across tasks is still reasonable in this case, it clearly benefits from the combined use of both modalities.

From these results, we conclude that both semantic representation and depth estimation are indispensable for learning a generalizable skill trained on individual instances. The integration of spatial-semantic features enhances the robustness of the policy, enabling better performance across both familiar and novel environments, and ultimately leading to more effective task execution in 3D spaces.


\begin{figure}[t]
	\centering
	%never specify both height and width for a figure! Let Latex make sure you keep aspect ratio.
	\includegraphics[width=0.8\linewidth]{fig/ablation_scoop.png}
	%
	\vspace{-0.0cm}% <--- this magic squeezes the space between figure and caption
	\caption{Ablation study of Semantic and Spatial-diffusion only on a seen task instance (\textbf{left}) and an unseen task instance (\textbf{right}). We show how the semantic component and spatial component contribute to the performance of the policy. S$^2$-Diffusion outperforms models trained with only semantic or spatial observations.}
	\label{fig:ablation only mask only depth} %always label figures for reference within the text
	\vspace{-0.0cm}% <--- this magic squeezes the space between the caption and the rest of the text
\end{figure}




\section{Limitations}
% {\color{red}
% The paper must include a “Limitations” section, describing shortcomings and open problems related to the proposed contribution.
% }
As Fig.~\ref{fig:skill_h} shows, the functional goal of \textit{flipping} or \textit{scooping} is discretized in different category-level skills such as \textit{bowl-to-bowl} and \textit{pile-to-container} scooping. While our method does generalize from instance-level tasks to category-level tasks, in its current form it is not able to generalize well from one instance-level skill to an instance of a different category task. i.e. when only training on \textit{rice-bowl-to-bowl} the performance will degrade on a \textit{sand-pile-to-container} task.

While Fig.~\ref{fig:skill_h} makes the ordering of skills into instance and category levels look straightforward, there are some corner cases that would need to be addressed if this view was expanded into a more complex ontology. For one, the functional goal can depend on more context than what is shown in our example. For instance, \textit{flipping} is presented in our example as a way to turn over an object, but it could also be interpreted as an action the robot should perform with its own body.
Furthermore, given how current learning-from-demonstration techniques are implemented, the category \textit{bowl-to-bowl scooping} might require additional qualifiers to accurately encompass the correct instances. For example, if we were to generalize this skill to \textit{ice cream bowl-to-bowl scooping}, the policy would fail, as the forces required are completely different from those observed in the training instances. Ice cream, especially when frozen, requires significantly more force to be scooped in the first place. However, as the ice cream begins to melt, and the required forces become more similar to those seen during training, there is a point at which our category skill \textit{bowl-to-bowl scooping} would likely work.
We leave exploring such nuances to future work.

Another limitation of our work is its dependency on the performance of pre-trained vision foundation models.
Naturally if the semantic or spatial estimate is poor, the resulting category-level skill may generalize poorly. To address this, we plan to investigate fine-tuning VLMs in conjunction with the downstream robot policy network in future research. %Additionally, we aim to extend our method to tackle long-horizon manipulation tasks, which will further enhance its applicability and effectiveness in more complex scenarios.

\iffalse

{\color{red}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\uppercase{Conclusion}}
\label{sec:conclusion}


% We have presented an open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) that enables the robot to generalize to different instances of the same category while being trained on an individual instance.
%improves the robot's generalization capacity to understand and interact with diverse objects and environments. 
We have presented an open-vocabulary Spatial-Semantic Diffusion policy (S$^2$-Diffusion) that enables generalization from instance-level training data to category-level skills, facilitating skill transfer across instances within the same category.
Our approach integrates depth estimation with semantic features, which enables the representation to 
generalize beyond the particular instances observed during training.
%encompass both object-level understanding and reletive spatial prediction across the entire workspace. 
We evaluate our method on various robotic manipulation tasks in simulation and the real world and demonstrate that it significantly enhances the generalizability and effectiveness of robotic manipulation policies. This improvement is especially notable in situations where understanding the semantics and structure of the environment is critical.
}
\fi


\section{\uppercase{Conclusion}}
\label{sec:conclusion}

In this work, we have introduced S$^2$-Diffusion, an open-vocabulary spatial-semantic diffusion policy that enables generalization from instance-level training data to category-level skills in robotic manipulation. Our method integrates semantic understanding with spatial representations, leveraging vision foundation models to learn a policy that is invariant to task-irrelevant visual changes and generalizes across different instances of the same category. This allows robots to transfer learned skills beyond their training data without requiring additional fine-tuning or retraining.

Through extensive simulations and real-world evaluations, we demonstrated that S$^2$-Diffusion outperforms the baselines. In particular we showed that:

\begin{itemize}
    \item Spatial-Semantic representations enhance generalization – By combining spatial information with semantic segmentation, our method enables robots to focus on task-relevant features, improving instance-to-category transfer.
    \item Efficient real-time execution using only a single RGB camera – Unlike approaches that require multi-view setups or additional depth sensors, S$^2$-Diffusion extracts meaningful scene representations from a single RGB image, making it practical for real-world deployment.
    \item Category-level generalization – Our evaluations on a number of robotic manipulation tasks, including real-world wiping and scooping, demonstrate that the method successfully generalizes across unseen instances within the same category, achieving high performance where baseline policies fail.
\end{itemize}

In summary, S$^2$-Diffusion policies are less constrained by their training distribution and constitute a step toward the ability of humans to generalize skills across variations in objects, materials, and environments instances. 
%In future work, we plan to extend this approach to long-horizon, multi-stage tasks, further enhance its semantic reasoning, and explore joint fine-tuning of VLMs with robotic policies for improved real-world robustness.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\balance
\bibliographystyle{template/IEEEtran}
\bibliography{template/IEEEabrv,references}


\end{document}

