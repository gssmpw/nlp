% Suppress writing to the ToC during the main paper
\addtocontents{toc}{\protect\setcounter{tocdepth}{-1}}  % Disable ToC writing for the main paper

\section{Introduction}
\label{sec:intro}
The reliable and objective performance assessment, i.e., validation of AI models is crucial for both the measurement of scientific progress and translation into practice. Benchmarking for traditional narrow, task-specific AI already comes with numerous challenges~\cite{myllyaho2021systematic}, but validation has proven to be even more complex and error-prone in the emerging field of generalist multimodal foundation models~\cite{schaeffer2024areemergent}. In the context of Vision-Language Models (VLMs), one issue that has received limited attention is the heterogeneous and often non-targeted nature of model validation~\cite{tong2024cambrian1fullyopenvisioncentric, tong2024eyes}. Widely used VLM benchmarks span diverse domains and encompass a variety of tasks, providing a broad view of model capabilities across different contexts~\cite{fu2024blink,liu2024mmbench,ying2024mmtbench,altahan2024unibench,yue2023mmmu}. 

We identify three key trends that highlight the critical need for personalized benchmarking approaches:

\textbf{Domain-specific benchmark demand:} Numerous datasets and benchmarks are continually being released in the general computer vision field. According to our analyses,  $\sim$400 out of the 2,700 CVPR 2024 publications propose a new or modified dataset as detailed in~\cref{appendix_cvpr_paper_analysis}. These benchmarks cover a wide range of domains, from autonomous driving to wildlife monitoring, underscoring the need for \textit{domain-specific} benchmarks.

\textbf{Popular arena platforms do not scale from an individual user’s perspective:}
Arena-style platforms such as Chatbot Arena\footnote{lmarena.ai/?leaderboard; see the Arena(Vision) tab} or WildVision Arena\footnote{huggingface.co/spaces/WildVision/vision-arena} allow users to submit single tasks and rate the outputs of different (anonymized) models. The aggregated user ratings, in turn, can be used for the objective and comparative assessment of models. While this allows for personalized and domain-relevant evaluation, large-scale assessment from a single user perspective would be cumbersome due to the required annotation effort.

\textbf{Homogeneous evaluation:} Most existing VLM benchmarks \cite{fu2024blink, yue2023mmmu, wang2024journeybench, zhang2024mmereal}, generally evaluate models using a single question per image. While this can suffice when large datasets are available—allowing for a broad range of tasks—domain experts with smaller, curated datasets face a more significant limitation. From a resource standpoint, image acquisition may also be expensive, and few tasks emerge if there is only one question per image. Furthermore, such an approach provides little insight into whether a VLM truly comprehends broad aspects of an image’s semantic content.

Taking these three trends together we conclude that there is a lack of guidance on how to set up a framework that enables personalized, domain-specific benchmarking in a resource-efficient manner.
Such a framework must address the scarcity of labeled data, leverage task diversity by systematically generating multiple questions per image, and maintain resource efficiency to ensure accessibility for researchers working in specialized fields, such as wildlife monitoring, or autonomous driving. 

In this work, we propose a resource-efficient framework for creating domain-specific VLM benchmarks via task augmentation. Our approach transforms a single type of annotation—instance segmentation—into a diverse set of tasks that test a broad range of perception abilities, such as object counting, occlusion detection, brightness comparison, and more.
Specifically, we focus on 2D natural images that either (1) already include instance segmentations or (2) can be annotated using recent advances in semi-automatic labeling tools (e.g., SAM~\cite{ravi2024sam}). This approach allows even domains with limited labeled data to efficiently generate custom evaluation tasks. Our main contribution, summarized in~\cref{fig:figure1}, is a \textbf{resource-efficient framework} for creating domain-specific VLM benchmarks via task augmentation, transforming a single type of annotation (instance segmentation) into a diverse set of tasks. We \textbf{apply} this framework \textbf{to create seven new domain-specific VLM benchmarks} and comprehensively \textbf{evaluate 22 open and closed VLMs on over 37,000 tasks }(for the full model list see \cref{appendix_model_overview}). To establish strong reference points for model evaluation, we collected an additional 162,946 human baseline answers corresponding to 37,171 questions across 1,704 images.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/main_text_figs/cvpr_fig_2.pdf}
    \caption{\textbf{Framework for resource-efficient in-domain benchmarking.} Starting from a single task with fine-grained annotations (here: instance segmentations), metadata for each image is obtained from both automatic sources (heuristics and models) and a small number of manual sources (human annotations). This process transforms the initial task into a collection of tasks, enabling resource-efficient and easy to use in-domain benchmarking of general VLM capabilities while maintaining cross-domain comparability.}
    \label{fig:metadataaug}
\end{figure*}
