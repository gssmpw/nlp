\section{Methods}



\subsection{Framework for resource-efficient in-domain benchmarking
}
The framework for resource-efficient in-domain benchmarking is depicted in~\cref{fig:metadataaug}. Starting with domain images that include instance segmentations (existing or created with semi-automatic labeling tools, such as SAM~\cite{ravi2024sam}), metadata for each image is acquired from multiple sources (humans, pre-defined heuristics, and models) to transform the single task into a collection of perception tasks.

For our seven new datasets, we use existing instance segmentation as the core perceptual task to generate the diverse set of VLM benchmark tasks depicted in \Cref{appendix_overviewvlmtasks} (examples in ~\Cref{fig:example_cow} and more detailed in ~\Cref{sec_sub:app_person}).

The metadata enrichment is derived from three sources: 

1) \underline{Human annotators} were used to generate information that cannot be extracted from the existing annotations or using established models. To this end, we outsourced annotations to a professional annotation company (Quality Match GmbH in Heidelberg). Specifically, human raters were tasked with determining the presence of occlusion and truncation in the images. Furthermore, they were asked to assess the direction in which the objects were facing. These annotations cost $\sim$27 USD on average with a total turnover time of two days. 

2) \underline{Pre-defined heuristics and rules} were employed to transform existing information into metadata. For example, instance segmentations were utilized to quantify the number of objects within a specific class or to determine whether specific instance segmentation masks were touching each other. 

3) \underline{An existing depth foundation model}, Depth Anything v2~\cite{yang2024depth}, was used to generate depth maps for each image.


\subsection{Seven new datasets from diverse domains}
We applied our proposed framework to images from seven different domains. Overall, the input images and instance segmentations for our framework were extracted from KITTI ~\cite{geiger2012kitti}, COCO~\cite{lin2014microsoft}, and COCONut~\cite{deng2024coconut}. In summary, we added ~300,000 metadata annotations to a total of 1,704 images across seven domains. This includes 15 annotations per object (e.g. occlusion, relative\_size, segmask\_touches\_segmask, or average\_depth). For truncation, occlusion, and direction, we obtained up to five annotations per object from human annotators (UI example is displayed in ~\Cref{appendix_example_quality_match}). Early stopping was applied when four annotators reached a consensus. The complete list is provided in Appendix \Cref{appendix_three_sources_of_metadata}.

The metadata were then used to define a set of 25 different VLM tasks (see~\cref{fig:fig3}), including six tasks concerning the entire image, 13 related to individual objects, and six focused on object pairs.



\textbf{Setup for automatic task processing after metadata extraction:}
To create a concrete list of vision-language tasks for each image we employed a systematic process. We began by prioritizing images in the datasets that featured a higher number of classes and objects to maximize task diversity and complexity. Next, specific criteria for each task were evaluated to ensure appropriate task generation for each image. For instance, in tasks requiring the comparison of two objects, it was essential that both objects were present in the image and belonged to the relevant classes. Furthermore, we established minimum thresholds for various measures, such as requiring a substantial depth difference between objects, to ensure the correct answers for the task could be reliably determined. Overall, our objective was to generate as many of the 25 different tasks as possible for each image. No LLMs or VLMs were used for task generation, as these methods are prone to injecting hallucinations~\cite{wang2023instruct4v,wang2024journeybench}. We prioritized quality and reliability instead.

\textbf{Human ambiguity baseline}: To rate the difficulty and ambiguity for each of the 37,171 tasks, we further acquired annotations from six human raters per image. We implemented early stopping if four raters reached agreement on a task. Overall, this resulted in 162,946 human reference annotations. An overview of the resulting datasets is provided in~\cref{tab:dataset_statistics}and exemplary images for all generated datasets are included in~\cref{domain_sets_appendix}.




\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}  % Reduce column padding
\footnotesize
\begin{tabular}{|c|c|c|c|c|p{1.3cm}|}
\hline
\textbf{Domain} & \textbf{Icon} & \textbf{\#Images} & \textbf{\#Objects} & \textbf{\#Tasks} & \textbf{\#Human Annot.} \\
\hline
\textbf{Wildlife} & \includegraphics[width=0.4cm]{figures/icons_iclr/wildlife.png} & 268 & 853 & 5,528 & 24,024 \\
\textbf{Persons} & \includegraphics[width=0.4cm]{figures/icons_iclr/person.png} & 250 & 7,812 & 6,122 & 26,548 \\
\textbf{Vehicles} & \includegraphics[width=0.4cm]{figures/icons_iclr/vehicles.png} & 235 & 2,199 & 5,219 & 22,976 \\
\textbf{Animals} & \includegraphics[width=0.4cm]{figures/icons_iclr/animals.png} & 273 & 1,162 & 5,724 & 24,907 \\
\textbf{Kitchen} & \includegraphics[width=0.4cm]{figures/icons_iclr/kitchen.png} & 272 & 2,143 & 5,332 & 23,793 \\
\textbf{Food} & \includegraphics[width=0.4cm]{figures/icons_iclr/food.png} & 236 & 5,673 & 5,249 & 23,221 \\
\textbf{Kitti} & \includegraphics[width=0.4cm]{figures/icons_iclr/kitti.png} & 170 & 1,458 & 3,997 & 17,477 \\
\hline
\textbf{Total} & & \textbf{1,704} & \textbf{21,300} & \textbf{37,171} & \textbf{162,946} \\
\hline
\end{tabular}
\caption{\textbf{Dataset statistics across different domains.} The table presents the total number of images, objects, tasks, and human annotations across all domains.}
\label{tab:dataset_statistics}
\end{table}



\subsection{Benchmarking strategy}
VLM benchmarking results can vary substantially with various factors, such as the images used, the domain, and the applied prompts. This often renders comparison of results across papers infeasible. For example, Accuracy is a prevalence-dependent metric, meaning that results should not be compared across datasets. To address this bottleneck, we fully homogenized our benchmarking framework using the proposed framework.

\textbf{Model selection:}
We selected 22 frontier and open VLMs of various sizes and from various providers and sources, as illustrated in \Cref{appendix_model_overview}. The oldest model was released in January 2024, while the most recent one included was released at the end of September 2024.

 \textbf{Benchmarking workflow:} 
 To ensure fair and consistent evaluation of all selected VLMs, we developed a standardized benchmarking workflow applied uniformly across all models. We assessed them in a zero-shot setting without any additional fine-tuning or domain-specific training. We strictly followed the configurations and setups recommended by each model's authors, using the exact settings provided in their official repositories (e.g., on Hugging Face) to ensure that each model was evaluated under conditions intended by its creators. Each model was provided with a carefully crafted text prompt alongside the corresponding image. 
To eliminate potential ambiguities in the questions, we conducted iterative testing of these prompts among human evaluators in our department. Through four rounds of refinement, we adjusted the prompts until all four human evaluators consistently agreed on their interpretation. Furthermore, we evaluated the sensitivity of the VLMs to variations in image markers, as many questions involved marked objects. Altering the box colors used to highlight objects—from green and red to other colors—resulted in slight performance fluctuations in both directions across different VLMs. To maintain consistency, we used the commonly recognized colors red and green, assigning them to objects at random.


\textbf{VLM tasks:} We evaluated the models on a comprehensive set of 25 tasks derived from our task augmentation framework (overview in~\cref{fig:fig3}, full list in~\cref{appendix_overviewvlmtasks} and examples per dataset in~\Cref{domain_sets_appendix}). Each task was associated with specific evaluation criteria and standardized prompts. For instance, when dealing with multiple-choice questions or tasks involving object selection, we established clear guidelines on how options were presented and how objects were chosen within images. This attention to detail ensured that the evaluation was both rigorous and reproducible.



% \begin{table*}[h]
% \centering
% \small
% \begin{tabular}{lcccccccc}

% \toprule
%  & Overall & Wildlife & Animals & Kitti & Person & Vehicles & Food & Kitchen \\
% & & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/wildlife.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/animals.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/kitti.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/person.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/vehicles.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/food.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/kitchen.png} \\\midrule
% Human & 93.66 & 93.43 & 93.87 & 94.60 & 95.15 & 92.41 & 93.58 & 92.59 \\
% \toprule
% Gemini\_1.5\_pro & \cellcolor[rgb]{1,0.843,0}72.44 & \cellcolor[rgb]{1,0.843,0}74.58 & \cellcolor[rgb]{1,0.843,0}75.35 & \cellcolor[rgb]{1,0.843,0}77.96 & \cellcolor[rgb]{1,0.843,0}70.84 & \cellcolor[rgb]{1,0.843,0}71.74 & \cellcolor[rgb]{1,0.843,0}69.98 & \cellcolor[rgb]{1,0.843,0}66.60 \\
% GPT-4o & \cellcolor[rgb]{0.753,0.753,0.753}69.79 & \cellcolor[rgb]{0.565,0.933,0.565}71.35 & \cellcolor[rgb]{0.565,0.933,0.565}71.35 & \cellcolor[rgb]{0.753,0.753,0.753}76.16 & \cellcolor[rgb]{0.753,0.753,0.753}69.01 & \cellcolor[rgb]{0.753,0.753,0.753}69.25 & \cellcolor[rgb]{0.753,0.753,0.753}67.00 & \cellcolor[rgb]{0.804,0.498,0.196}64.42 \\
% Claude\_3.5\_Sonnet & \cellcolor[rgb]{0.804,0.498,0.196}69.00 & \cellcolor[rgb]{0.753,0.753,0.753}73.72 & \cellcolor[rgb]{0.804,0.498,0.196}74.28 & \cellcolor[rgb]{0.565,0.933,0.565}72.40 & \cellcolor[rgb]{0.804,0.498,0.196}65.34 & \cellcolor[rgb]{0.565,0.933,0.565}67.75 & \cellcolor[rgb]{0.565,0.933,0.565}65.69 & \cellcolor[rgb]{0.565,0.933,0.565}63.82 \\
% Qwen2\_72B & \cellcolor[rgb]{0.565,0.933,0.565}68.76 & \cellcolor[rgb]{0.867,0.627,0.867}70.79 & \cellcolor[rgb]{0.753,0.753,0.753}75.00 & \cellcolor[rgb]{0.804,0.498,0.196}74.63 & \cellcolor[rgb]{0.678,0.847,0.902}61.70 & \cellcolor[rgb]{0.804,0.498,0.196}68.23 & \cellcolor[rgb]{0.804,0.498,0.196}66.22 & \cellcolor[rgb]{0.753,0.753,0.753}64.74 \\
% Llama\_3.2\_90B & \cellcolor[rgb]{0.678,0.847,0.902}65.93 & \cellcolor[rgb]{0.678,0.847,0.902}71.33 & \cellcolor[rgb]{0.678,0.847,0.902}70.21 & \cellcolor[rgb]{0.867,0.627,0.867}68.63 & \cellcolor[rgb]{0.565,0.933,0.565}64.62 & \cellcolor[rgb]{0.867,0.627,0.867}63.13 & \cellcolor[rgb]{0.678,0.847,0.902}62.87 & \cellcolor[rgb]{0.867,0.627,0.867}60.75 \\
% Gemini\_1.5\_flash & \cellcolor[rgb]{0.867,0.627,0.867}65.72 & \cellcolor[rgb]{0.804,0.498,0.196}71.53 & \cellcolor[rgb]{0.867,0.627,0.867}70.16 & \cellcolor[rgb]{0.678,0.847,0.902}70.83 & \cellcolor[rgb]{0.867,0.627,0.867}60.23 & \cellcolor[rgb]{0.678,0.847,0.902}65.17 & \cellcolor[rgb]{0.867,0.627,0.867}61.14 & \cellcolor[rgb]{0.678,0.847,0.902}61.01 \\
% \bottomrule
% \end{tabular}
% \caption{\textbf{The rankings of models differ strongly across the tested domains.} Model Accuracies across different generated datasets. The 'Overall' column represents the mean accuracy across all datasets. \textcolor[rgb]{1,0.843,0}{\rule{0.5cm}{0.5cm}} 1st place (Gold) \quad \textcolor[rgb]{0.753,0.753,0.753}{\rule{0.5cm}{0.5cm}} 2nd place (Silver) \quad \textcolor[rgb]{0.804,0.498,0.196}{\rule{0.5cm}{0.5cm}} 3rd place (Bronze) \quad \textcolor[rgb]{0.565,0.933,0.565}{\rule{0.5cm}{0.5cm}} 4th place \quad \textcolor[rgb]{0.678,0.847,0.902}{\rule{0.5cm}{0.5cm}} 5th place \quad \textcolor[rgb]{0.867,0.627,0.867}{\rule{0.5cm}{0.5cm}} 6th place. Only the top six models are shown. The 'Overall' column represents the mean accuracy across all datasets. Due to space constraints, results for additional models are provided in Appendix \Cref{tab:model_accuracies}. Note that Accuracy does not account for shared images between questions; this issue is addressed in~\Cref{fig:acc_per_global_fig}.}
% \label{tab:model_accuracies_top7}
% \end{table*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lcccccccc}

\toprule
 & Overall & Wildlife & Animals & Kitti & Person & Vehicles & Food & Kitchen \\
& & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/wildlife.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/animals.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/kitti.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/person.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/vehicles.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/food.png} & \includegraphics[width=0.03\textwidth]{figures/icons_iclr/kitchen.png} \\\midrule
Human & 93.7 & 93.4 & 93.9 & 94.6 & 95.2 & 92.4 & 93.6 & 92.6 \\
\toprule
Gemini\_1.5\_pro & \cellcolor[rgb]{1,0.843,0}72.4 & \cellcolor[rgb]{1,0.843,0}74.6 & \cellcolor[rgb]{1,0.843,0}75.4 & \cellcolor[rgb]{1,0.843,0}78.0 & \cellcolor[rgb]{1,0.843,0}70.8 & \cellcolor[rgb]{1,0.843,0}71.7 & \cellcolor[rgb]{1,0.843,0}70.0 & \cellcolor[rgb]{1,0.843,0}66.6 \\
GPT-4o & \cellcolor[rgb]{0.753,0.753,0.753}69.8 & \cellcolor[rgb]{0.565,0.933,0.565}71.4 & \cellcolor[rgb]{0.565,0.933,0.565}71.4 & \cellcolor[rgb]{0.753,0.753,0.753}76.2 & \cellcolor[rgb]{0.753,0.753,0.753}69.0 & \cellcolor[rgb]{0.753,0.753,0.753}69.3 & \cellcolor[rgb]{0.753,0.753,0.753}67.0 & \cellcolor[rgb]{0.804,0.498,0.196}64.4 \\
Claude\_3.5\_Sonnet & \cellcolor[rgb]{0.804,0.498,0.196}69.0 & \cellcolor[rgb]{0.753,0.753,0.753}73.7 & \cellcolor[rgb]{0.804,0.498,0.196}74.3 & \cellcolor[rgb]{0.565,0.933,0.565}72.4 & \cellcolor[rgb]{0.804,0.498,0.196}65.3 & \cellcolor[rgb]{0.565,0.933,0.565}67.8 & \cellcolor[rgb]{0.565,0.933,0.565}65.7 & \cellcolor[rgb]{0.565,0.933,0.565}63.8 \\
Qwen2\_72B & \cellcolor[rgb]{0.565,0.933,0.565}68.8 & \cellcolor[rgb]{0.867,0.627,0.867}70.8 & \cellcolor[rgb]{0.753,0.753,0.753}75.0 & \cellcolor[rgb]{0.804,0.498,0.196}74.6 & \cellcolor[rgb]{0.678,0.847,0.902}61.7 & \cellcolor[rgb]{0.804,0.498,0.196}68.2 & \cellcolor[rgb]{0.804,0.498,0.196}66.2 & \cellcolor[rgb]{0.753,0.753,0.753}64.7 \\
Llama\_3.2\_90B & \cellcolor[rgb]{0.678,0.847,0.902}65.9 & \cellcolor[rgb]{0.678,0.847,0.902}71.3 & \cellcolor[rgb]{0.678,0.847,0.902}70.2 & \cellcolor[rgb]{0.867,0.627,0.867}68.6 & \cellcolor[rgb]{0.565,0.933,0.565}64.6 & \cellcolor[rgb]{0.867,0.627,0.867}63.1 & \cellcolor[rgb]{0.678,0.847,0.902}62.9 & \cellcolor[rgb]{0.867,0.627,0.867}60.8 \\
Gemini\_1.5\_flash & \cellcolor[rgb]{0.867,0.627,0.867}65.7 & \cellcolor[rgb]{0.804,0.498,0.196}71.5 & \cellcolor[rgb]{0.867,0.627,0.867}70.2 & \cellcolor[rgb]{0.678,0.847,0.902}70.8 & \cellcolor[rgb]{0.867,0.627,0.867}60.2 & \cellcolor[rgb]{0.678,0.847,0.902}65.2 & \cellcolor[rgb]{0.867,0.627,0.867}61.1 & \cellcolor[rgb]{0.678,0.847,0.902}61.0 \\
\bottomrule
\end{tabular}
\caption{\textbf{The rankings of models differ strongly across the tested domains.} Model Accuracies across different generated datasets. The 'Overall' column represents the mean accuracy across all datasets. \textcolor[rgb]{1,0.843,0}{\rule{0.5cm}{0.5cm}} 1st place (Gold) \quad \textcolor[rgb]{0.753,0.753,0.753}{\rule{0.5cm}{0.5cm}} 2nd place (Silver) \quad \textcolor[rgb]{0.804,0.498,0.196}{\rule{0.5cm}{0.5cm}} 3rd place (Bronze) \quad \textcolor[rgb]{0.565,0.933,0.565}{\rule{0.5cm}{0.5cm}} 4th place \quad \textcolor[rgb]{0.678,0.847,0.902}{\rule{0.5cm}{0.5cm}} 5th place \quad \textcolor[rgb]{0.867,0.627,0.867}{\rule{0.5cm}{0.5cm}} 6th place. Only the top six models are shown. The 'Overall' column represents the mean accuracy across all datasets. Due to space constraints, results for additional models are provided in Appendix \Cref{tab:model_accuracies}. Note that Accuracy does not account for shared images between questions; this issue is addressed in~\Cref{fig:acc_per_global_fig}.}
\label{tab:model_accuracies_top7}
\end{table*}



\textbf{Metrics and rankings:} Choosing an adequate strategy for performance assessment is far from trivial and a research topic of its own~\cite{maier2024metrics,reinke2024understanding}. In this work, we were specifically interested in relative performance differences rather than in the specific ability of VLMs to serve a specific task. To obtain aggregated performance values across images, we define the \textbf{Accuracy\%($t$)} metric with a threshold \(t \in [0, 1]\).  
For each image \(i\) in a dataset \(D\), let \(Q_i\) denote the set of questions associated with that image.  
Let \(C_{i,q,m} \in \{0, 1\}\) indicate whether model \(m\) correctly answered question \(q\) for image \(i\) (1 for correct, 0 otherwise). The model \(m\) is considered to meet the threshold \(t\) on image \(i\) if the fraction of questions \(q\) in \(Q_i\) answered correctly by the model is at least \(t\). Formally, we define:
% \[
% \text{Accuracy\%}_m(t)
%   = \frac{1}{\lvert D\rvert}\,\sum_{i \in D}\,
%     I\!\Bigl(\bigl(\tfrac{1}{\lvert Q_i\rvert}\,\sum_{q \in Q_i}C_{i,q,m}\bigr) \ge t\Bigr)
%     \times 100
% \]
% \begin{align*}
% \text{Accuracy\%}_m(t) &= \\
%     &\frac{1}{\lvert D\rvert}\,\sum_{i \in D}\,
%     I\!\Bigl(\bigl(\tfrac{1}{\lvert Q_i\rvert}\,\sum_{q \in Q_i}C_{i,q,m}\bigr) \ge t\Bigr)
%     \times 100
% \end{align*}
\begin{gather*}
\text{Accuracy\%}_m(t) = \\
\frac{1}{\lvert D\rvert}\,\sum_{i \in D}\,
    I\!\Bigl(\bigl(\tfrac{1}{\lvert Q_i\rvert}\,\sum_{q \in Q_i}C_{i,q,m}\bigr) \ge t\Bigr)
    \times 100
\end{gather*}
Here, \(I(\cdot)\) is an indicator function defined as:
\[
I(x \ge t)
 \;=\;
 \begin{cases}
  1, & \text{if } x \ge t,\\
  0, & \text{otherwise}.
 \end{cases}
\]
\paragraph{Explanation:}
\begin{itemize}
  \item \(\sum_{q \in Q_i} C_{i,q,m}\): Total number of correctly answered questions for image \(i\).
  \item \(\frac{1}{\lvert Q_i\rvert}\sum_{q \in Q_i} C_{i,q,m}\): Fraction of questions answered correctly for image \(i\).
  \item \(t \in [0,1]\): Desired minimum accuracy level assessed for each \(Q_i\).
\end{itemize}





