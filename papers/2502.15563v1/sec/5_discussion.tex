


\section{Discussion}

This paper contributes to the advancement of VLM benchmarking in three ways: 

\textbf{1) Framework for resource-efficient and domain-specific benchmarking:} We showed that task augmentation, using instance segmentation as the root task, enables the generation of a diverse set of VLM tasks and could thus evolve as a core method for resource-efficient domain-specific VLM benchmarking. The insights gained on the varying difficulty of presented VLM tasks will further guide the design of future benchmarks. The framework can be easily applied to other domains, even with a small number of images. The computational and monetary costs for each generated dataset are minimal and displayed in~\cref{appendix_comp_and_mon_cost}. 

\textbf{2) Seven new openly available datasets:} Our seven new datasets will help assess generalist capabilities of future VLMs. Furthermore, we release the six human annotations per task (totaling 162,946 annotations) to assist researchers working on human annotations. 

\textbf{3) New insights:} The insights on current capabilities of closed and open VLMs highlight the narrowing gap between closed and open models. Most importantly, we showcased the need for domain-specific validation.

Core strengths of our contribution include the broad applicability of our concept, the open dataset and benchmark contribution, and the wide range of state-of-the-art closed and open models investigated here.

As an implicit contribution, we introduced the new metric Accuracy\%($t$), which offers several key strengths. First, it captures model performance in a single very intuitive value. The metric is extendable with additional tasks, allowing for gradually increasing difficulty, and can be adapted to evaluate domain-specific tasks effectively. It is worth mentioning, however, that the specific properties of the metric require further analyses~\cite{reinke2024understanding}. For example, some questions require specific image conditions, such as the presence of multiple objects for comparison. This can result in a varying number of questions per image, which, in turn, has an influence on the metric. Furthermore, tasks are treated equally without any weighting, which may overlook differences in task difficulty or importance. Users can, however, easily modify the weighting scheme to better reflect their specific evaluation priorities.

A limitation of our work is model family dependence, as many models come from closely related families, which may hinder statistical analysis. For closed-source models, specific information about training and data is often unavailable, creating transparency issues. We provide further statistical analysis, such as ranking variability in~\cref{app_sec_stats}. Model performance showed small variations with prompt phrasing, which we mitigated through iterative testing for consistency.  Additionally, our human annotations were performed by professional annotators, which may introduce ambiguity since annotators aim to complete tasks quickly.

Future work should focus on expanding the number of tasks generated, further enhancing the diversity and comprehensiveness of VLM benchmarks. Additionally, our method can be adapted to different domains with domain-specific questions or scaled up to support continuous extension, providing a versatile approach for evaluating models across diverse applications.



\subsubsection*{Code / Datasets / Human Annotations}
Code, datasets, and annotations will be made available.



\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/appendix/appendix_sample_image_tasks.png}
   \caption{\textbf{Our framework yields a diverse set of tasks.} Exemplary tasks that were generated with the framework for a given image. A broad range of examples and errors for each generated dataset is provided in~\cref{domain_sets_appendix}.}
   \label{fig:example_cow}
\end{figure}
