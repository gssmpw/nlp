\section{Related Work}
\label{sec:related_work}

\subsection{Vision-Language Benchmarks}
Recent studies propose a range of evaluation benchmarks for VLMs, varying in size, number, and type of VL capabilities. Examples include Blink \cite{fu2024blink} and MMBench~\cite{liu2024mmbench} ($>$3,000 multiple-choice questions each), and MME~\cite{fu2024mme} (Yes/No questions on perception and cognition). The largest benchmarks include MMT-Bench~\cite{ying2024mmtbench} ($>$31,000 questions), MME-RealWorld~\cite{zhang2024mmereal} ($>$29,000 image-question pairs), and MMMU \cite{yue2023mmmu} ($>$11,500 questions). While these benchmarks cover multiple VL capabilities and domains, they require extensive labeling efforts. For example, MME-RealWorld involved 25 annotators and seven VLM experts, MMMU relied on 50 college students, while MMT-Bench lacks details on annotator numbers. Other benchmarks focus on much smaller question sets~\cite{chen2024we, yu2024mm}, integrating multiple existing benchmarks~\cite{jiang2024eff,altahan2024unibench}, or collecting individual human preferences~\cite{lu2024wildvision,xu2023lvlmehub}. \citet{tong2024cambrian1fullyopenvisioncentric} present a critical examination of multimodal LLM benchmarks.

Despite the variety of datasets and tasks, a resource-efficient and generalizable approach that enables extensive evaluation of VLMs across multiple (domain-specific) tasks is still lacking. Our framework addresses this gap by empowering users to create domain-specific VLM perception benchmarks from just a few images.



\subsection{Task augmentation and metadata}
Task augmentation refers to generating multiple diverse tasks from a single existing task~\cite{muennighoff2023octopack}. While task augmentation has been addressed from various directions \cite{johnson2017clevr,zhang2024task, Zamir_2018_CVPR,wang2023instruct4v, wang2024journeybench,kuznetsova2020v4,krishna2017genome}, an easy to use framework for evaluating VLMs by domain users on their own images is still missing. The closest works to ours are \citet{zhang2024task} and \citet{zhang2024provision}, which programmatically generate benchmarks using a library of visual assets and task templates. A comprehensive comparison to other task augmentations works and their applicability is provided in~\cref{appendix_task_augmentation_comparison}.


\subsection{Resource-efficient VLM benchmarking}
Most existing benchmarks often focus on performance metrics without considering the human and computational resources required to generate a benchmark (see, e.g.,~\cite{fu2024blink,liu2024mmbench}). The work that has been done on efficient benchmarking has been focused in the realm of unimodal language models~\cite{polo2024tinybenchmarks, perlitz2023efficient}. An exception has been~\citet{ging2024open}, who investigated the automatic creation of VLM benchmarks from classification datasets. Nevertheless, the increasing prominence of VLMs in research and industry ~\cite{li2024multimodal, yang2023dawn} is not yet reflected in efforts to increase efficiency during benchmark creation.

\begin{figure*}[t]
    \centering
\includegraphics[width=\linewidth]{figures/main_text_figs/fig_3.png}
\caption{\textbf{Our framework yields a diverse set of tasks.} (a) The spider diagram illustrates high Accuracy variability across tasks for the VLMs. We present the results of all the best ranked models while a comprehensive performance summary for all 22 tested models can be found in~\Cref{appendix_spider_figures}. (b) Based on a single image with instance segmentations, our framework enables the  generation of 25 tasks from eight different vision-language categories, ranging from pixel-level to image-level perception.}
    \label{fig:fig3}
\end{figure*}
