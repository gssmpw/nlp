


\section{Experiments and Results}
The primary purpose of our experiments was to showcase the benefit of our task augmentation approach (sec. \ref{sec:4.1}). To assess the value of each task for VLM benchmarking, we related it to average model performance, resources needed to create the task, and corresponding human ambiguity (sec. \ref{sec:4.2}). Finally, we leveraged our concept and data to explore the capabilities of the most recent open and closed VLMs (sec. \ref{sec:4.3}.).


\subsection{Benefit of the proposed framework
\label{sec:4.1}
}
\Cref{fig:metadataaug} shows aggregated performance values for all models, separated by imaging domain. As the tasks and prompts were homogenized, the results clearly indicate that performance varies substantially across domains, supporting the hypothesis that in-domain validation is crucial for real-world translation. Note that this holds true despite the fact that we purposely chose domains that are relatively common (presumably captured in the model training) and closely related to one another.


Furthermore, as shown in~\cref{fig:fig3}a, the performance of models varies substantially across VLM tasks, suggesting that the tasks generated by our framework are diverse. The hardest tasks on average across domains are (1) T7.2 “Jigsaw Puzzle Completion”, (2), T1.2 “Object Counting”, (3), T7.1 “Rotated Jigsaw Puzzle Completion”, (4), T2.1 “Object Occlusion Detection”, and (5) T5.2 “Second Brightest Image Selection”. The easiest task on average was T1.3 “Additional Object Presence Detection” (see~\cref{fig:fig6_instance}).

% \FloatBarrier
\begin{figure*}[ht]
    \centering
    
    %===========================
    % First subfigure
    %===========================
    \begin{subfigure}{0.95\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/main_text_figs/cvpr_fig_3.pdf}
        \caption{\textbf{The need for specific in-domain evaluation is demonstrated by the high performance variability across imaging domains.} 
        The performance of the overall best model Gemini 1.5 pro varies between domains from 22\% (Kitchen dataset) to 72\% (Kitti dataset). 
        For the displayed Accuracy\%(75), humans achieve an almost perfect score of 1 for all datasets (see Appendix). The top 10 models per dataset shown. 
        We display the full plots for all thresholds and models in \Cref{appendix_various_fixed_percentage_lines}.}
        \label{fig:acc_sub1}
    \end{subfigure}
    
    % \vspace{1.5em} % vertical space between sub-figures
    
    %===========================
    % Second subfigure
    %===========================
    \begin{subfigure}{0.95\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/main_text_figs/cvpr_fig_7a.pdf}
        \caption{The Area under the Accuracy\%($t$) Curve serves as a \textbf{metric for comprehensive image understanding}. 
        With a maximum possible value of 1, higher values indicate better performance. 
        Notably, the current state-of-the-art model, Gemini\_1.5\_pro, achieves only 0.53, highlighting significant room for improvement. 
        Only the top 10 models are shown. 
        Separate curves for all 22 tested models and datasets are displayed in~\cref{appendix_full_auc_curves}.}
        \label{fig:full_ACC_perc_curves}
    \end{subfigure}
    
    %===========================
    % Global caption
    %===========================
    \caption{\textbf{Performance varies across domains, highlighting the need for specialized in-domain evaluation; even the best models still lag behind human performance}. 
    The Accuracy\%($t$) metric represents the percentage of images for which at least a specified proportion of questions are correctly answered. 
    It can (a) be computed for specific thresholds or (b) be aggregated over multiple thresholds to remove dependence on a specific $t$. 
    The Area under the Accuracy\%($t$) Curve captures model performance in a single value, ranging from 0.37 to 0.53 for the top 10 models tested.}
    \label{fig:acc_per_global_fig}
\end{figure*}
% \FloatBarrier
\subsection{Human Ambiguity}
\label{sec:4.2}
As demonstrated in Appendix \ref{appendix_task_ranking_models_humans}, there is a high discrepancy in task rankings between humans and models. While the "Jigsaw Puzzle Completion” tasks ranked amongst the most challenging for the models, humans found "Object Occlusion Detection" and “Object Touching Detection” to be the most difficult.
 
From a resource perspective, tasks should be (1) hard to solve for models and (2) require as little human annotation as possible. This potential trade-off is captured in Appendix \autoref{appendix_fig6_instance}. It can be seen that many hard tasks, including the top four, can already be extracted from instance segmentations alone.

\subsection{Insights on current models}
\label{sec:4.3}
\Cref{fig:acc_per_global_fig} summarizes the performance of a model selection and reference baselines. Further detailed analysis, including all tested models, examples, and errors for each generated dataset are provided in the Appendix. The following insights can be extracted:

\textbf{Confirming common findings from the community:}
Our analysis reinforces several established patterns in the field. Closed models continue to demonstrate superior performance across tasks and domains, although open models have significantly narrowed this performance gap. In particular, Qwen2 72B stands out as the strongest performer among open models. The superiority of human evaluation remains evident, with human raters achieving near-perfect performance on most tasks, though they notably struggle with specific challenges such as counting, occlusion, and direction-related tasks—counting being particularly problematic. Regarding model scaling, larger variants typically show better performance, with some notable exceptions such as Molmo 7B outperforming Pixtral 12B.
% \begin{itemize} \item Closed models mostly outperform open models across tasks and domains. \item However, open models have narrowed the gap significantly. \item Among the ones tested, Qwen2 72B is by far the best performing open model. \item Human raters outperform VLMs by a large margin. While human raters achieve near-perfect performance on most tasks, they struggle with counting, occlusion, and direction-related tasks, with counting being the most difficult. \item Large models typically outperform their smaller variants, although exceptions exist (e.g., Molmo 7B generally outperforms Pixtral 12B). \end{itemize}



\textbf{Interesting new findings:}
The need for specific in-domain evaluation is highlighted by the high performance variability across imaging domains for the same perception tasks, see \Cref{tab:model_accuracies_top7} and~\Cref{fig:metadataaug}. The overall best model, Gemini 1.5 Pro, varies between domains from 22\% (Kitchen dataset) to 72\% (Kitti dataset). Qwen2 72B slightly surpasses Gemini 1.5 Pro on the kitchen and animals datasets but ranks only fifth on the person dataset. Additional insights emerge from model comparisons, with Qwen2 7B consistently outperforming Molmo 7B across most datasets, and Gemini Flash 1.5 showing superior Point Depth Comparison capabilities over Gemini Pro. These results indicate that our newly introduced metric, Accuracy\%($t$), can effectively capture model performance in a single value.


% \begin{figure*}[t]
%   \centering
%   % First subfigure
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/main_text_figs/cvpr_fig_3.pdf}
%     \caption{\textbf{The need for specific in-domain evaluation is demonstrated by the high performance variability across imaging domains.} The performance of the overall best model Gemini 1.5 pro varies between domains from 22\% (Kitchen dataset) to 72\% (Kitti dataset). For the displayed Accuracy\%(75), humans achieve an almost perfect score of 1 for all datasets (see Appendix). Top 10 models per dataset shown. We display the full plots for all thresholds and models in \Cref{appendix_various_fixed_percentage_lines}.}
%   \end{subfigure}
%   \hfill
%   % Second subfigure
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/main_text_figs/cvpr_fig_7a.pdf}
%     \caption{The Area under the Accuracy\%($t$) Curve serves as a \textbf{metric for comprehensive image understanding}. With a maximum possible value of 1, higher values indicate better performance. Notably, the current state-of-the-art model, Gemini\_1.5\_pro, achieves only 0.53, highlighting significant room for improvement. Only the top 10 models are shown. Separate curves for all 22 tested models and datasets are displayed in Appendix TODOREF.}
%     \label{fig:full_ACC_perc_curves}
%   \end{subfigure}
%   % Global caption
%   \caption{\textbf{Performance varies across domains, highlighting the need for specialized in-domain evaluation; even the best models still lag behind human performance. y}. The Accuracy\%($t$) metric represents the percentage of images for which at least a specified proportion of questions are correctly answered. It can (a) be computed for specific thresholds or (b) be aggregated over multiple thresholds to remove the dependency on a specific $t$.The Area under the Accuracy\%($t$) Curve captures model performance in a single value and ranges from 0.37 to 0.53 for the top 10 models tested.}
%   \label{fig:acc_per_global_fig}
% \end{figure*}

