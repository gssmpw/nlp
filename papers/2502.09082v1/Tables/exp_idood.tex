
\begin{table*}[htbp]
\centering
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.9}
\resizebox{\linewidth}{!}{\small
\begin{tabular}{lcccccccccc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{In-Domain}} & \multicolumn{5}{c}{\textbf{Out-of-Domain}} \\
\cmidrule(lr){2-6}
\cmidrule(lr){7-11}
\multirow{2}{*}{} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Storyline}\\ \scriptsize  \textbf{Consistency}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Anthro-}\\ \scriptsize  \textbf{pomorphism}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Character}\\ \scriptsize  \textbf{Fidelity}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Storyline}\\ \scriptsize  \textbf{Quality}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Average}\\ \scriptsize  \textbf{Score}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Storyline}\\ \scriptsize  \textbf{Consistency}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Anthro-}\\ \scriptsize  \textbf{pomorphism}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Character}\\ \scriptsize  \textbf{Fidelity}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Storyline}\\ \scriptsize  \textbf{Quality}}} & \multirow{2}{*}{\makecell{\scriptsize \textbf{Average}\\ \scriptsize  \textbf{Score}}} \\
 &  &  &  &  &  &  &  &  &  & \\
\midrule
\multicolumn{11}{c}{\textit{Close-source Models}} \\
\midrule
Abab7-preview & 55.31\tiny{$\pm1.4$} & 42.29\tiny{$\pm1.3$} & 42.94\tiny{$\pm3.2$} & 74.13\tiny{$\pm1.9$} & 53.67\tiny{$\pm1.0$} & 58.30\tiny{$\pm1.8$} & 46.17\tiny{$\pm2.6$} & 44.72\tiny{$\pm2.3$} & 75.54\tiny{$\pm1.5$} & 56.18\tiny{$\pm0.1$}\\
Doubao-pro & 60.37\tiny{$\pm1.8$} & 49.06\tiny{$\pm0.4$} & 45.76\tiny{$\pm3.4$} & 77.87\tiny{$\pm1.2$} & 58.26\tiny{$\pm0.9$} & 61.53\tiny{$\pm1.1$} & 50.38\tiny{$\pm0.3$} & 48.28\tiny{$\pm1.7$} & 80.69\tiny{$\pm0.4$} & 60.22\tiny{$\pm0.4$}\\
Step-1-Flash & 57.10\tiny{$\pm0.3$} & 48.31\tiny{$\pm1.0$} & 41.84\tiny{$\pm1.9$} & 76.18\tiny{$\pm1.5$} & 55.86\tiny{$\pm1.0$} & 58.39\tiny{$\pm1.2$} & 47.94\tiny{$\pm0.5$} & 47.12\tiny{$\pm1.0$} & 75.67\tiny{$\pm0.5$} & 57.28\tiny{$\pm0.2$}\\
Step-2 & \textbf{60.55\tiny{$\pm0.7$}} & 48.82\tiny{$\pm2.2$} & \textbf{47.90\tiny{$\pm1.1$}} & 77.33\tiny{$\pm1.0$} & \textbf{58.65\tiny{$\pm1.0$}} & \textbf{62.30\tiny{$\pm1.5$}} & 49.30\tiny{$\pm1.2$} & \textbf{46.77\tiny{$\pm1.7$}} & 78.59\tiny{$\pm1.2$} & \textbf{59.24\tiny{$\pm0.6$}}\\
GPT-3.5 & 55.58\tiny{$\pm0.9$} & 42.18\tiny{$\pm5.2$} & 40.37\tiny{$\pm3.2$} & 72.90\tiny{$\pm0.1$} & 52.76\tiny{$\pm2.0$} & 59.69\tiny{$\pm3.0$} & 44.25\tiny{$\pm2.1$} & 44.60\tiny{$\pm1.4$} & 74.66\tiny{$\pm2.3$} & 55.80\tiny{$\pm1.6$}\\
GPT-4o & 59.88\tiny{$\pm1.4$} & 48.11\tiny{$\pm1.0$} & 47.10\tiny{$\pm0.2$} & \textbf{79.06\tiny{$\pm1.5$}} & 58.54\tiny{$\pm0.7$} & 62.29\tiny{$\pm1.5$} & 49.48\tiny{$\pm1.2$} & 49.90\tiny{$\pm0.4$} & \textbf{80.18\tiny{$\pm0.9$}} & 60.46\tiny{$\pm0.1$}\\
GPT-4o Mini & 59.15\tiny{$\pm1.3$} & 46.94\tiny{$\pm1.5$} & 43.99\tiny{$\pm2.5$} & 77.33\tiny{$\pm0.6$} & 56.85\tiny{$\pm0.1$} & 61.02\tiny{$\pm1.6$} & 49.48\tiny{$\pm3.1$} & 45.77\tiny{$\pm1.1$} & 79.77\tiny{$\pm0.5$} & 59.01\tiny{$\pm1.4$}\\
Gemini Pro & 57.72\tiny{$\pm0.4$} & 50.94\tiny{$\pm1.8$} & 46.23\tiny{$\pm1.0$} & 76.22\tiny{$\pm1.6$} & 57.78\tiny{$\pm0.9$} & 60.50\tiny{$\pm1.9$} & 53.88\tiny{$\pm1.1$} & 49.43\tiny{$\pm0.3$} & 78.97\tiny{$\pm1.3$} & 60.69\tiny{$\pm0.8$}\\
Claude-3-Haiku & 57.61\tiny{$\pm0.5$} & 44.97\tiny{$\pm2.2$} & 40.61\tiny{$\pm1.4$} & 73.52\tiny{$\pm1.2$} & 54.18\tiny{$\pm0.6$} & 58.74\tiny{$\pm1.1$} & 44.36\tiny{$\pm1.9$} & 43.14\tiny{$\pm0.8$} & 74.76\tiny{$\pm1.7$} & 55.25\tiny{$\pm1.2$}\\
Claude-3.5-Sonnet & 56.44\tiny{$\pm1.5$} & 47.24\tiny{$\pm1.4$} & 44.89\tiny{$\pm0.6$} & 76.39\tiny{$\pm1.5$} & 56.24\tiny{$\pm1.0$} & 58.46\tiny{$\pm1.1$} & 49.75\tiny{$\pm3.3$} & 46.49\tiny{$\pm3.0$} & 78.06\tiny{$\pm1.5$} & 58.19\tiny{$\pm0.9$}\\
\midrule
\multicolumn{11}{c}{\textit{Open-source Models}} \\
\midrule
Mistral-7B & 60.29\tiny{$\pm1.9$} & 38.98\tiny{$\pm2.0$} & 42.93\tiny{$\pm0.5$} & 62.20\tiny{$\pm3.1$} & 51.10\tiny{$\pm1.8$} & 59.51\tiny{$\pm2.4$} & 41.02\tiny{$\pm1.3$} & 46.57\tiny{$\pm2.8$} & 61.65\tiny{$\pm1.3$} & 52.19\tiny{$\pm0.7$}\\
Qwen-2-7B & 50.77\tiny{$\pm1.3$} & 34.17\tiny{$\pm1.1$} & 29.92\tiny{$\pm2.9$} & 62.58\tiny{$\pm0.7$} & 44.36\tiny{$\pm0.7$} & 53.14\tiny{$\pm1.5$} & 36.79\tiny{$\pm1.0$} & 33.09\tiny{$\pm3.3$} & 63.79\tiny{$\pm1.4$} & 46.70\tiny{$\pm1.3$}\\
LLaMA-3.1-8B & 53.00\tiny{$\pm1.2$} & 43.64\tiny{$\pm0.8$} & 39.05\tiny{$\pm1.5$} & 71.34\tiny{$\pm1.3$} & 51.76\tiny{$\pm0.6$} & 55.20\tiny{$\pm2.1$} & 47.08\tiny{$\pm3.2$} & 41.38\tiny{$\pm1.0$} & 73.23\tiny{$\pm2.5$} & 54.22\tiny{$\pm1.8$}\\
CoSER-8B & 58.56\tiny{$\pm3.5$} & 46.78\tiny{$\pm1.1$} & 45.78\tiny{$\pm3.1$} & 73.38\tiny{$\pm1.8$} & 56.12\tiny{$\pm0.5$} & 58.66\tiny{$\pm1.6$} & 47.69\tiny{$\pm0.8$} & 48.03\tiny{$\pm1.3$} & 72.71\tiny{$\pm1.2$} & 56.77\tiny{$\pm0.6$}\\
Vicuna-13B-1.5 & 51.84\tiny{$\pm1.2$} & 38.88\tiny{$\pm0.4$} & 36.39\tiny{$\pm0.5$} & 58.15\tiny{$\pm0.7$} & 46.31\tiny{$\pm0.4$} & 53.66\tiny{$\pm2.1$} & 39.35\tiny{$\pm2.7$} & 39.69\tiny{$\pm1.6$} & 62.71\tiny{$\pm2.5$} & 48.85\tiny{$\pm2.1$}\\
Mixtral-8x7B & 51.18\tiny{$\pm3.1$} & 38.76\tiny{$\pm1.8$} & 34.37\tiny{$\pm2.8$} & 66.44\tiny{$\pm0.1$} & 47.69\tiny{$\pm1.7$} & 51.32\tiny{$\pm0.4$} & 38.13\tiny{$\pm0.5$} & 39.48\tiny{$\pm2.6$} & 68.93\tiny{$\pm1.7$} & 49.47\tiny{$\pm1.1$}\\
Qwen-2-72B & 56.34\tiny{$\pm2.4$} & 46.19\tiny{$\pm0.4$} & 45.59\tiny{$\pm2.2$} & 75.68\tiny{$\pm0.3$} & 55.95\tiny{$\pm1.0$} & 59.15\tiny{$\pm1.1$} & 48.37\tiny{$\pm1.3$} & 47.65\tiny{$\pm1.8$} & 77.52\tiny{$\pm0.4$} & 58.17\tiny{$\pm1.0$}\\
LLaMA-3.1-70B & 55.44\tiny{$\pm2.7$} & 44.19\tiny{$\pm1.9$} & 42.67\tiny{$\pm1.5$} & 73.90\tiny{$\pm1.8$} & 54.05\tiny{$\pm0.8$} & 59.48\tiny{$\pm1.4$} & 47.72\tiny{$\pm2.3$} & 44.78\tiny{$\pm0.9$} & 75.78\tiny{$\pm0.9$} & 56.94\tiny{$\pm0.7$}\\
Higgs-Llama-3-70B & 55.85\tiny{$\pm2.7$} & 41.18\tiny{$\pm3.3$} & 39.79\tiny{$\pm2.3$} & 73.77\tiny{$\pm0.6$} & 52.65\tiny{$\pm2.0$} & 58.35\tiny{$\pm0.5$} & 46.45\tiny{$\pm1.1$} & 45.04\tiny{$\pm1.0$} & 77.48\tiny{$\pm0.6$} & 56.83\tiny{$\pm0.6$}\\
CoSER-70B & \underline{57.77\tiny{$\pm1.3$}} & \underline{\textbf{51.60\tiny{$\pm1.3$}}} & \underline{45.82\tiny{$\pm0.9$}} & 74.27\tiny{$\pm1.1$} & \underline{57.37\tiny{$\pm0.7$}} & \underline{59.56\tiny{$\pm2.1$}} & \underline{\textbf{55.06\tiny{$\pm1.1$}}} & \underline{51.67\tiny{$\pm2.5$}} & 76.71\tiny{$\pm1.6$} & \underline{60.75\tiny{$\pm1.1$}}\\
DeepSeek-V3 & 55.36\tiny{$\pm0.5$} & 47.55\tiny{$\pm1.3$} & 43.10\tiny{$\pm0.3$} & \underline{74.91\tiny{$\pm2.0$}} & 55.23\tiny{$\pm0.6$} & 57.45\tiny{$\pm2.0$} & 48.19\tiny{$\pm0.9$} & 44.93\tiny{$\pm0.4$} & \underline{78.41\tiny{$\pm1.1$}} & 57.24\tiny{$\pm0.5$}\\
\bottomrule
\end{tabular}}
\caption{LLM performance (\%) on given-circumstance acting using \method Test, separated into the in-domain and out-of-domain splits for \method training.}
\label{tab:exp_idood}
\end{table*}
