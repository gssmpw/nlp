\begin{table*}[t]
\centering
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{0.9}
{\small
\begin{tabularx}{\textwidth}{@{}l*{4}{c}ccc@{}}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{Based on LLM Judges}} & \multicolumn{2}{c}{\textbf{Based on N-gram}} \\ \cmidrule(lr){2-6} \cmidrule(lr){7-8} & \multirow{2}{*}{\makecell{\textbf{Storyline}\\\textbf{Consistency}}} & \multirow{2}{*}{\makecell{\textbf{Anthro-}\\\textbf{pomorphism}}} & \multirow{2}{*}{\makecell{\textbf{Character}\\\textbf{Fidelity}}} & \multirow{2}{*}{\makecell{\textbf{Storyline}\\\textbf{Quality}}} & \multirow{2}{*}{\makecell{\textbf{Average}\\\textbf{Score}}} & \multirow{2}{*}{\makecell{\textbf{BLEU}}} & \multirow{2}{*}{\makecell{\textbf{ROUGE-L}}} \\
 &  &  &  &  &  &  &  \\
\midrule
\multicolumn{8}{c}{\textit{Close-source Models}} \\
\midrule
Abab7-preview & 56.81\scriptsize{$\pm1.47$} & 44.23\scriptsize{$\pm1.90$} & 43.83\scriptsize{$\pm2.71$} & 74.83\scriptsize{$\pm0.97$} & 54.92\scriptsize{$\pm0.57$} & 4.96\scriptsize{$\pm0.07$} & 11.50\scriptsize{$\pm0.06$} \\
Doubao-pro & 60.95\scriptsize{$\pm1.40$} & 49.72\scriptsize{$\pm0.23$} & 47.02\scriptsize{$\pm1.10$} & 79.28\scriptsize{$\pm0.82$} & 59.24\scriptsize{$\pm0.30$} & 6.38\scriptsize{$\pm0.08$} & 12.95\scriptsize{$\pm0.04$} \\
Step-1-Flash & 57.75\scriptsize{$\pm0.72$} & 48.12\scriptsize{$\pm0.39$} & 44.48\scriptsize{$\pm0.48$} & 75.93\scriptsize{$\pm0.99$} & 56.57\scriptsize{$\pm0.48$} & 5.95\scriptsize{$\pm0.15$} & 12.71\scriptsize{$\pm0.11$} \\
Step-2 & 61.43\scriptsize{$\pm0.88$} & 49.06\scriptsize{$\pm1.69$} & 47.33\scriptsize{$\pm0.70$} & 77.96\scriptsize{$\pm0.85$} & 58.94\scriptsize{$\pm0.75$} & 5.75\scriptsize{$\pm0.08$} & 12.50\scriptsize{$\pm0.11$} \\
GPT-3.5 & 57.22\scriptsize{$\pm0.13$} & 43.30\scriptsize{$\pm0.48$} & 42.29\scriptsize{$\pm1.47$} & 73.91\scriptsize{$\pm0.64$} & 54.18\scriptsize{$\pm0.63$} & 4.58\scriptsize{$\pm0.11$} & 11.80\scriptsize{$\pm0.10$} \\
GPT-4o & \textbf{61.59\scriptsize{$\pm0.66$}} & 48.93\scriptsize{$\pm0.48$} & \textbf{48.95\scriptsize{$\pm1.73$}} & \textbf{80.33\scriptsize{$\pm0.59$}} & \textbf{59.95\scriptsize{$\pm0.50$}} & 5.90\scriptsize{$\pm0.16$} & 12.11\scriptsize{$\pm0.13$} \\
GPT-4o Mini & 60.09\scriptsize{$\pm0.60$} & 48.21\scriptsize{$\pm1.09$} & 44.88\scriptsize{$\pm1.63$} & 78.55\scriptsize{$\pm0.14$} & 57.93\scriptsize{$\pm0.74$} & 3.90\scriptsize{$\pm0.07$} & 10.81\scriptsize{$\pm0.07$} \\
Gemini Pro & 59.11\scriptsize{$\pm0.82$} & 52.41\scriptsize{$\pm0.57$} & 47.83\scriptsize{$\pm0.37$} & 77.59\scriptsize{$\pm1.43$} & 59.24\scriptsize{$\pm0.25$} & 5.39\scriptsize{$\pm0.04$} & 11.65\scriptsize{$\pm0.06$} \\
Claude-3-Haiku & 58.18\scriptsize{$\pm0.72$} & 44.66\scriptsize{$\pm1.72$} & 41.88\scriptsize{$\pm0.34$} & 74.14\scriptsize{$\pm1.26$} & 54.71\scriptsize{$\pm0.84$} & 4.80\scriptsize{$\pm0.05$} & 12.02\scriptsize{$\pm0.02$} \\
Claude-3.5-Sonnet & 57.45\scriptsize{$\pm0.98$} & 48.50\scriptsize{$\pm2.35$} & 45.69\scriptsize{$\pm1.80$} & 77.23\scriptsize{$\pm0.88$} & 57.22\scriptsize{$\pm0.95$} & 5.17\scriptsize{$\pm0.12$} & 11.45\scriptsize{$\pm0.07$} \\
\midrule
\multicolumn{8}{c}{\textit{Open-source Models}} \\
\midrule
Mistral-7B & \underline{59.90\scriptsize{$\pm1.33$}} & 40.00\scriptsize{$\pm0.74$} & 44.75\scriptsize{$\pm1.14$} & 61.93\scriptsize{$\pm1.12$} & 51.64\scriptsize{$\pm0.55$} & 2.71\scriptsize{$\pm0.10$} & 9.28\scriptsize{$\pm0.12$} \\
Qwen-2-7B & 51.96\scriptsize{$\pm0.67$} & 35.48\scriptsize{$\pm0.62$} & 31.51\scriptsize{$\pm2.95$} & 63.18\scriptsize{$\pm0.79$} & 45.53\scriptsize{$\pm0.69$} & 4.21\scriptsize{$\pm0.21$} & 10.71\scriptsize{$\pm0.10$} \\
LLaMA-3.1-8B & 54.10\scriptsize{$\pm1.63$} & 45.36\scriptsize{$\pm1.91$} & 40.22\scriptsize{$\pm1.16$} & 72.29\scriptsize{$\pm1.75$} & 52.99\scriptsize{$\pm1.20$} & 4.59\scriptsize{$\pm0.11$} & 10.18\scriptsize{$\pm0.09$} \\
CoSER-8B & 58.61\scriptsize{$\pm2.46$} & 47.23\scriptsize{$\pm0.16$} & 46.90\scriptsize{$\pm2.06$} & 73.04\scriptsize{$\pm1.37$} & 56.45\scriptsize{$\pm0.56$} & 9.40\scriptsize{$\pm0.18$} & 14.21\scriptsize{$\pm0.11$} \\
Vicuna-13B-1.5 & 52.75\scriptsize{$\pm1.64$} & 39.12\scriptsize{$\pm1.21$} & 38.04\scriptsize{$\pm0.98$} & 60.43\scriptsize{$\pm1.58$} & 47.58\scriptsize{$\pm1.25$} & 1.67\scriptsize{$\pm0.10$} & 5.59\scriptsize{$\pm0.18$} \\
Mixtral-8x7B & 51.25\scriptsize{$\pm1.73$} & 38.44\scriptsize{$\pm1.18$} & 36.92\scriptsize{$\pm2.65$} & 67.69\scriptsize{$\pm0.80$} & 48.58\scriptsize{$\pm1.35$} & 5.28\scriptsize{$\pm0.06$} & 11.66\scriptsize{$\pm0.05$} \\
Qwen-2-72B & 57.75\scriptsize{$\pm1.26$} & 47.28\scriptsize{$\pm0.87$} & 46.62\scriptsize{$\pm1.69$} & 76.60\scriptsize{$\pm0.36$} & 57.06\scriptsize{$\pm1.00$} & 5.38\scriptsize{$\pm0.00$} & 11.85\scriptsize{$\pm0.03$} \\
LLaMA-3.1-70B & 57.46\scriptsize{$\pm1.65$} & 45.95\scriptsize{$\pm1.30$} & 43.72\scriptsize{$\pm1.17$} & 74.84\scriptsize{$\pm0.54$} & 55.49\scriptsize{$\pm0.33$} & 4.82\scriptsize{$\pm0.06$} & 10.98\scriptsize{$\pm0.06$} \\
Higgs-Llama-3-70B & 57.10\scriptsize{$\pm1.12$} & 43.82\scriptsize{$\pm2.18$} & 42.41\scriptsize{$\pm1.66$} & 75.62\scriptsize{$\pm0.15$} & 54.74\scriptsize{$\pm1.26$} & 3.99\scriptsize{$\pm0.33$} & 10.92\scriptsize{$\pm0.56$} \\
CoSER-70B & 58.66\scriptsize{$\pm1.34$} & \underline{\textbf{53.33\scriptsize{$\pm0.91$}}} & \underline{48.75\scriptsize{$\pm1.43$}} & 75.49\scriptsize{$\pm0.94$} & \underline{59.06\scriptsize{$\pm0.22$}} & \underline{\textbf{10.10\scriptsize{$\pm0.04$}}} & \underline{\textbf{14.78\scriptsize{$\pm0.09$}}} \\
DeepSeek-V3 & 56.40\scriptsize{$\pm0.95$} & 47.87\scriptsize{$\pm1.10$} & 44.02\scriptsize{$\pm0.13$} & \underline{76.66\scriptsize{$\pm1.26$}} & 56.24\scriptsize{$\pm0.46$} & 4.54\scriptsize{$\pm0.14$} & 11.02\scriptsize{$\pm0.15$} \\
\bottomrule
\end{tabularx}
}
\caption{Performance (\%) of various LLMs on given-circumstance acting using \method Test. 
\textbf{Bold} or \underline{underlined} values indicate best  performance across all models and open-source models, respectively. 
}
\label{tab:model-comparison}
\end{table*}
