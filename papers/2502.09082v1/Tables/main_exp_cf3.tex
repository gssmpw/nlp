
\begin{table*}[htbp]
\centering
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.9}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{3}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{Based on LLM Judges}} & \multicolumn{2}{c}{\textbf{Based on N-gram}} \\ \cmidrule(lr){2-6} \cmidrule(lr){7-8} & \multirow{2}{*}{\makecell{\textbf{Storyline}\\\textbf{Consistency}}} & \multirow{2}{*}{\makecell{\textbf{Anthro-}\\\textbf{pomorphism}}} & \multirow{2}{*}{\makecell{\textbf{Character}\\\textbf{Fidelity}}} & \multirow{2}{*}{\makecell{\textbf{Storyline}\\\textbf{Quality}}} & \multirow{2}{*}{\makecell{\textbf{Average}\\\textbf{Score}}} & \multirow{2}{*}{\makecell{\textbf{BLEU}}} & \multirow{2}{*}{\makecell{\textbf{ROUGE-L}}} \\
 &  &  &  &  &  &  &  \\
\midrule
\multicolumn{8}{c}{\textit{Close-source Models}} \\
\midrule
Abab7-preview & 65.25 & 55.26 & 55.95 & 79.68 & 64.03 & 10.53 & 15.99 \\
Doubao-pro & \textbf{68.31} & \textbf{59.51} & 59.23 & 80.15 & 66.80 & 11.83 & 17.13 \\
Step-1-Flash & 64.32 & 53.35 & 54.38 & 79.03 & 62.77 & 11.94 & 17.06 \\
Step-2 & 66.61 & 55.81 & 59.59 & 80.56 & 65.64 & 11.72 & 17.27 \\
GPT-3.5 & 65.72 & 54.34 & 56.48 & 77.67 & 63.55 & 10.80 & 16.39 \\
GPT-4o & 67.48 & 58.51 & 62.30 & \textbf{82.23} & \textbf{67.63} & 12.32 & 16.98 \\
GPT-4o Mini & 66.74 & 55.55 & 56.80 & 80.01 & 64.77 & 9.50 & 15.54 \\
Gemini Pro & 65.47 & 59.43 & \textbf{62.42} & 78.78 & 66.53 & 10.84 & 16.27 \\
Claude-3-Haiku & 64.51 & 54.01 & 57.13 & 77.26 & 63.23 & 10.11 & 16.18 \\
Claude-3.5-Sonnet & 64.54 & 54.57 & 58.76 & 79.89 & 64.44 & 8.64 & 14.94 \\
\midrule
\multicolumn{8}{c}{\textit{Open-source Models}} \\
\midrule
Mistral-7B & \underline{67.50} & 50.39 & 59.90 & 68.67 & 61.62 & 7.41 & 14.33 \\
Qwen-2-7B & 59.81 & 49.87 & 49.04 & 69.16 & 56.97 & 10.59 & 16.22 \\
LLaMA-3.1-8B & 60.90 & 51.36 & 50.37 & 74.89 & 59.38 & 7.86 & 13.82 \\
CoSER-8B & 67.22 & 58.19 & 58.80 & 76.44 & 65.16 & 13.17 & \underline{\textbf{18.42}} \\
Vicuna-13B-1.5 & 60.92 & 46.68 & 50.51 & 67.80 & 56.48 & 5.11 & 9.71 \\
Mixtral-8x7B & 64.66 & 51.21 & 54.08 & 74.01 & 60.99 & 11.21 & 16.97 \\
Qwen-2-72B & 67.27 & 55.87 & 59.84 & \underline{80.07} & \underline{65.76} & 11.92 & 16.96 \\
LLaMA-3.1-70B & 64.08 & 54.80 & 54.18 & 78.31 & 62.84 & 8.74 & 14.74 \\
Higgs-Llama-3-70B & 65.09 & 54.80 & 58.20 & 79.36 & 64.36 & 10.86 & 16.40 \\
CoSER-70B & 65.99 & \underline{59.24} & \underline{59.97} & 76.74 & 65.48 & \underline{\textbf{13.46}} & 18.18 \\
DeepSeek-V3 & 62.95 & 56.95 & 58.25 & 79.80 & 64.49 & 9.25 & 15.01 \\
\bottomrule
\end{tabular}}
\caption{Performance (\%) of various LLMs on \method Test  in conversation continuation setting ($k=3$), where RPLAs start from the first three messages in the authentic conversations.}
\label{tab:model-comparison-cf3}
\end{table*}
