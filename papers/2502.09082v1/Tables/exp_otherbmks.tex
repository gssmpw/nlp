\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Incharacter}} & \multirow{2}{*}{\makecell{\textbf{Life}\\\textbf{Choice}}} & \multirow{2}{*}{\makecell{\textbf{CroSS}\\\textbf{MR}}} \\
 & \textbf{Dim} & \textbf{Full} &  &  \\
\midrule
LLaMA-3.1-8B & 64.97 & 15.62 & 61.10 & 30.15 \\
CoSER-8B & 75.80 & 21.88 & 69.54 & 44.94 \\
 \hfill\hfill\hfill {\textit{trained w/o} \textsc{I.T.}} & 70.70 & 15.62 & 59.92 & 43.14 \\
LLaMA-3.1-70B & 72.16 & 31.25 & 86.48 & 61.30 \\
Higgs-Llama-3-70B & 74.52 & 28.12 & 74.03 & 60.12 \\
CoSER-70B & 75.80 & \textbf{34.38} & \textbf{93.47} & \textbf{64.49} \\
 \hfill\hfill\hfill {\textit{trained w/o} \textsc{I.T.}} & 73.12 & 32.14 & 93.18 & 63.14 \\
Qwen-2-72B & 74.52 & 31.25 & 81.14 & 62.57 \\
GPT-3.5 & 71.20 & 21.88 & 78.07 & 30.09 \\
GPT-4o & \textbf{76.54} & 32.62 & 75.96 & \textbf{64.49} \\
Claude-3.5-Sonnet & 72.61 & 21.88 & 86.07 & 30.59 \\
\bottomrule
\end{tabular}
\caption{LLM performance (\%) across three existing RPLA benchmarks. I.T. denotes inner thoughts. For InCharacter, we report accuracy for individual (Dim) and full (Full) dimensions on BFI. }
\label{tab:exp_other_bmks}
\end{table}

