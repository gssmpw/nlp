\section{\method Dataset}

In this section, we introduce the \method dataset, which covers authentic data of 17,966 characters from 771 renowned books. 
\method features its authentic, non-synthesized dialogues with real-world intricacies, and comprehensive data representations supporting various usages. 
In Table ~\ref{tab:dataset_stats}, we provide a comprehensive comparison with existing datasets. 
We illustrate our dataset's design principles in \S\ref{sec:data_design},  curation pipeline in \S\ref{sec:data_pipeline}, and statistical analysis in \S\ref{sec:data_statistics}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth, center]{Figures/CoSER-main.pdf}
    \vspace{0.2cm}
    \caption{
    Overview of \method's dataset, training and evaluation. 
    Left: The \method dataset is sourced from renowned books and processed via LLM-based pipeline. 
    It contains rich data types on plots, conversations and characters.  
    Right: 
    We apply given-circumstance acting to train and evaluate role-playing LLMs using these conversations.  
    For training, each sample trains the LLM to portray  a specific character in a conversation, using their  original dialogue.  
    For evaluation, 
    we build a multi-agent system for conversation simulation given the same scenario, and assess the simulated dialogue via  penalty-based LLM critics. 
    }
    \label{fig:main}
\end{figure*}

\subsection{Design Principles}
\label{sec:data_design}

As shown in Table ~\ref{tab:dataset_stats}, \method differs from previous RPLA datasets mainly in its:  
\textit{1)} rich data types, 
\textit{2)} internal thoughts and physical actions in messages,
\textit{3)} environment as a role.


\textbf{Rich Types of Data} \quad 
The persona data $\personadata_\persona$ can represent a character $\persona$ from fictional works in diverse forms, \eg, narratives, profiles, dialogues, experiences, \etc. 
Previous work focuses primarily on profiles and dialogues, which represent limited knowledge.  
Hence, we propose a more comprehensive set of data types that are: 
\textit{1)} Comprehensive: covering extensive knowledge about characters and plots from the books; 
\textit{2)} Orthogonal: carrying distinct, complementary information with little redundancy;
\textit{3)} Contextual-rich: providing sufficient context to enable $\agent_\persona$ to faithfully reproduce $\persona$'s behaviors and responses in given scenarios.


Specifically, we organizes knowledge from books hierarchically via three interconnected elements: plots, conversations and characters. 
Each \textbf{plot} comprises its raw text, summary, conversations in this plot, and key characters' current states and experiences in this plot. 
A \textbf{conversation} contains not only the dialogue transcripts, but also rich contextual settings including scenario descriptions and characters' motivations. 
\textbf{Characters} are associated with their conversations and plots, based on which we craft their profiles. 

\textbf{Thoughts and Actions in Messages} \quad 
Previous RPLA studies typically restrict RPLAs' output space to verbal speech alone, limiting their ability to fully represent human interactions. 
In this paper, we extend the message space of RPLAs and character datasets into three distinct dimensions: speech ($\mathcal{L}$), action ($\mathcal{A}$), and thought ($\mathcal{T}$), significantly enriching the expressiveness. 
For instance, an RPLA can convey silence by generating only thoughts and actions without verbal speech. 
The three dimensions are distinguished by markup symbols and function mechanisms:  
\begin{itemize}[itemsep=-3pt, topsep=0pt, partopsep=0pt]
    \item \textbf{Speech} is for verbal communications of characters.
    \item \textbf{Action} captures physical behaviors, body language, facial expressions, \etc. Similar to  tool use in agents~\citep{weng2023agent}, actions can be programmed to trigger downstream events in multi-agent systems. 
    \item \textbf{Thought} represents internal thinking processes, which enable RPLAs to simulate sophisticated human cognition. 
    Thoughts should be invisible to others, forming  information asymmetry~\citep{zhou2024sotopia}. 
\end{itemize}

\textbf{Environment as a Role} \quad 
In RPLA applications like AI TRPG~\footnote{Tabletop Role-Playing Games}~\citep{liang2023tachikuma}, LLMs often serve as world simulators that respond to players' actions. 
To promote this ability, we consider environment as a special role $e$, which provide environmental responses such as physical changes and reactions from unspecified characters or crowds.

\subsection{Dataset Curation} 
\label{sec:data_pipeline}

We curate the \method dataset through a systematic LLM-based pipeline that transforms book content into high-quality data for RPLAs  
~\footnote{In this paper, we employs Claude-3.5-Sonnet (20240620).}. 
The details are as follows. 

\textbf{Source Selection} \quad 
Our dataset is sourced from most acclaimed literary works to ensure data quality and character depth. 
We identify the top 1,000 books on \textit{Goodreads}'s \textit{Best Books Ever} list~\footnote{https://www.goodreads.com/list/show/1.Best\_Books\_Ever}, and obtain the content for 771 books.
As shown in Table~\ref{tab:selected_books}, these books  offer characters and narratives with literary significance and widespread recognition across diverse genres, time periods, and cultural backgrounds.

\textbf{Chunking} \quad 
We segment book contents into chunks to fit in LLMs' context window. 
We employs both static, chapter-based strategy and dynamic, plot-based strategy. 
Initially, we use regular expressions to identify chapter titles as natural chunk boundaries. 
Then, we merge adjacent small chunks and split large chunks to ensure moderate chunk sizes. 
However, static chunking neglects the storyline and truncates important plots or conversations. 
To address this, we implement dynamic plot-based chunking, \ie, during data extraction, we also prompt LLMs to identify truncated plots or trailing content in the current chunk, and concatenate them with the subsequent chunk to ensure plot integrity.


\textbf{Data Extraction} \quad 
We employ LLMs to extract plot and conversation data from book chunks, including (1) contents, summaries and character experiences of plots, and (2) dialogues and background settings of conversations. 
The extracted data representations are illustrated in Fig. ~\ref{fig:front} and introduced in \S\ref{sec:data_design}. 
In the messages, speeches are always extracted from the original dialogues, while actions and thoughts can either be extracted or inferred by LLMs based on the context. 
For evaluation purposes, we hold out data from the final 10\%  plots in each book.


\textbf{Organizing Character Data} \quad 
Based on the extracted data, we form the knowledge bases for characters in three steps.  
First, we unify character references by establishing name mappings between aliases and canonical names using LLMs, \eg, mapping \textit{Lord Snow} to an unified identifier \textit{Jon Snow}. 
Second, we aggregate relevant plots and conversations for each character. 
Finally, we leverage LLMs to generate character profiles based on their extracted data, describing them from multiple perspectives including background, experiences, physical characteristics, personality traits, core motivations, relationships, character arcs, \etc. 

For technical details, including our prompts, engineering implementation, and handling mechanisms for exception caused by LLMs, please refer to ~\S\ref{sec:app_dataset}. 



