\section{Related Work}

\input{Tables/dataset_features}

An RPLA leverages LLMs to create a simulated persona $\agent_\persona$ that emulates a real character $\persona$ based on its persona data $\personadata_\persona$. 
Effective RPLAs require both comprehensive, high-quality data $\personadata_\persona$ and advanced role-playing LLMs.
Among various persona types, we focus on RPLAs for established characters, which should faithfully align with their characters’ complex backgrounds and nuanced personalities. 


\textbf{Datasets for RPLAs} \quad 
Persona data $\personadata_\persona$ describe the real persona $\persona$ through various representations, including profiles~\citep{yuan2024evaluating}, dialogues~\citep{wang2023rolellm}, experiences~\citep{li2023chatharuhi} and multimodal information~\citep{dai2024mmrolecomprehensiveframeworkdeveloping}, \etc. 
As shown in Table ~\ref{tab:dataset_overview}, existing datasets have several limitations. 
\textit{1)} Many are synthesized via LLMs' responses to general instruction sets~\citep{wang2023rolellm} or character-specific questions~\citep{shao2023character}, such as RoleBench~\citep{wang2023rolellm}.  
However, LLM-synthesized data compromise authenticity and fidelity to original sources.
\textit{2)} Human-annotated datasets such as CharacterEval~\citep{tu2024charactereval} and CharacterDial~\citep{zhou2023characterglm} offer improved quality, but are expensive and difficult to scale.
\textit{3)} Several efforts extract authentic dialogues from fictional works, such as ChatHaruhi~\citep{li2023chatharuhi} and HPD~\citep{chen2023large}. However, they rely on human efforts for individual sources and are hence hard to scale as well.
\textit{4)} Furthermore, existing datasets offer limited representations and forms, \ie, mainly consisting of two-character or user-character question-answer pairs.
These datasets support various purposes, including prompting, training, retrieval augmentation, and evaluation of RPLAs.


\textbf{Evaluation for RPLAs} \quad
Existing evaluation methods are based on either LLM judges or multi-choice questions~\citep{chen2024from}. 
LLM-judged methods typically elicit LLMs’ role-playing performance via predefined questions, and score the performance using LLM judges or reward models~\citep{chen2024from}.
They assess various dimensions, including character-independent aspects such as anthropomorphism~\citep{tu2024charactereval} and attractiveness~\citep{zhou2023characterglm}, as well as character-specific traits such as language style, knowledge, and personality~\citep{wang2023rolellm, shao2023character}. 
However, LLM judges suffer from inherent biases, \eg, length and position bias~\citep{li2024judgesurvey}, and may lack the necessary knowledge for character-specific evaluation.
Other benchmarks evaluate role-playing LLMs through multiple-choice questions, assessing specific aspects such as knowledge~\citep{shen2023roleeval}, decision-making~\citep{xu2024character}, motivation recognition~\citep{yuan2024evaluating}, and personality fidelity~\citep{wang2024incharacter}.
