\onecolumn
\appendix

\section{Dataset}
\label{sec:app_dataset}


\subsection{Statistics and Analysis}
\label{sec:data_statistics}
As shown in Table \ref{tab:dataset_overview}, \method dataset is extensive and comprehensive, encompassing dialogue data from 771 books and 17,966 distinct characters. 
The dataset includes 30,069 unique plots and 29,798 conversations. 
On average, each conversation consists of approximately 13.2 utterances, with the entire dataset comprising a total of 392,900 utterances.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{\#Book} & \textbf{\#Plot} & \textbf{\#Conversation.} & \textbf{\#Chararacter} & \textbf{\#Utterance} \\ \hline
771     &   30,069      & 29,798                   & 17,966               & 392,900              \\ \hline
\end{tabular}
\caption{Statistics of \method Dataset.
}
\label{tab:dataset_overview}
\end{table}

Our book selection is derived from the \textit{Best Books Ever} list on \textit{Goodreads}, a curated collection of globally acclaimed literary works. 
These novels have garnered widespread recognition and appreciation from readers worldwide.
Table ~\ref{tab:selected_books} presents a comprehensive list of the top 100 books from our selection.

\input{Tables/top_books}

We analyze the genres of the selected books based on Supersummary classifications, with the statistical results presented in Figure \ref{fig:type}. 
Our dataset encompasses a wide range of genres, particularly fiction categories such as Fantasy, Historical, Science Fiction, Romance, and Mystery. It also features niche fiction genres, showcasing diverse narrative styles. 
In addition to fiction, the collection includes non-fiction genres such as memoirs, biographies, and other works, enhancing its versatility.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/type.pdf}
    \caption{Genre distribution of selected books in \method dataset.
    }
    \label{fig:type}
\end{figure}

\subsection{Data Splits}
For evaluation purposes, we held out the last 10\% of data from each book; that is, they are not included in our prompts or datasets for training or retrieval purposes. Additionally, we trained the \method models on only 90\% of the books. 

\subsection{Implementation Details for Construction}


\paragraph{Extracting Raw Text} 
LLMs often struggle to extract verbatim original content, especially with punctuation marks like quotation marks, making it difficult to extract raw text directly. 
Therefore, instead of asking LLMs to extract the complete text of a plot, we prompt LLMs to extract the first and last sentences of each plot. 
Since the extracted sentences may still contain typos, we apply lexical similarity to match them with the exact sentences from the raw text. 
Finally, we identify the complete raw text based on these first and last sentences.

\paragraph{Parsing Structured Data} 
During extraction, we instruct LLMs to output extracted data in JSON format. 
However, LLM-generated JSON strings may sometimes be unparseable or do not conform to the specified format (e.g., missing required keys). 

Towards this challenge, we adopt a repair-and-retry strategy to improve extraction success rate.
For each chunk to be extracted, we invoke LLMs and attempt to parse a valid JSON object.
If parsing fails, we employ LLMs to repair the invalid JSON string and retry. 
Some failures occur because the LLM attempts to output JSON that exceeds the maximum token limit, resulting in truncation. 
In such cases, we prompt the repairing LLM to truncate the JSON at an appropriate point. 
If it still fail, we restart the entire process from the beginning, making up to 5 attempts.

\paragraph{Refining Conversation Settings} 
During data extraction, we observe that the initially extracted conversation settings, including scenarios and character motivations, often fail to provide a comprehensive context. 
We attribute this to the LLM’s tendency to distribute information across different data fields when extracting multiple kinds of information simultaneously, rather than repeating it in different data fields. 
For instance, if certain information is already mentioned in the plot summary, it might be omitted from the scenario description.

Therefore, to provide a complete context for given-circumstance acting, we implement an additional LLM call to refine the conversation settings based on the extracted data. 
We instructed the refining LLM to provide a comprehensive  conversation setting, while carefully avoiding any disclosure of subsequent dialogue content or plot developments. 


For additional details, such as the regular expressions used for identifying chapter titles, please refer to our  code.

\subsection{Comparison with Existing Methods for Character Profiling}

Previous character profiling methods, including hierarchical updating~\citep{wu2021recursively}, incremental updating~\citep{chang2023booookscore}, and one-shot summarization~\citep{yuan2024evaluating}, typically only generate the profile of a single character at a time. 
Morevoer, ~\citet{papoudakis2024bookworm} shows that these methods, particularly hierarchical updating, perform suboptimally when generating multiple character profiles simultaneously.

\method's multi-stage, extract-then-aggregate pipeline addresses these limitations. 
It ensures comprehensive character profiles with high precision and recall of character knowledge, capturing evolving character arcs, and significantly improving processing efficiency.

\section{Training}
\label{sec:app_training}

\subsection{Training Samples}

We transform conversations from the \method dataset into training samples in the Sharegpt format.
We utilize 90\% of the books in the dataset for training, while the remaining 10\% are set aside to evaluate our models’ ability to generalize to out-of-domain characters and books. 
We construct one training sample for each character in every \method conversation, encompassing both main characters and minor roles.
When training on a character $c$, we designate $c$’s messages as targets for optimization, while using the system prompt and messages from other characters as inputs. Adjacent inputs are concatenated.

Towards general role-playing capabilities across diverse scenarios and applications, we dynamically generate role-playing instructions (system prompts) using varied phrasings, formats, and data types, as shown in Tables ~\ref{tab:prompts_agent} and ~\ref{tab:prompts_agent_2}. 
We consider instructions entirely in natural language, as well as those formatted with special symbols (such as \#\#\#, ===), randomly sampling different formats and various expressions for the same semantics. 
We consider various configurations of the available data. 
Each sample may include (50\%) or exclude (50\%)  the following elements : 
\textit{1)} Profiles of other characters in this conversation; 
\textit{2)} Summaries of the relevant plot;
\textit{3)} Inner thoughts within the messages.


Besides character role-playing, 
we train \method models for environment modeling and next speaker prediction (NSP) for multi-agent simulation. 
For environment modeling, we train LLMs $\agent_e$ to play the environment role $e$ in the same approach, leveraging environment messages in our dataset. 
For NSP, given setting $\mathcal{S}$ and messages $\{m_1,...,m_i\}$, we train LLMs to predict the speaker of $m_{i+1}$ (or ending the conversation).  

Our role-playing dataset comprises approximately 0.1B tokens, as measured using the LLaMA 3.1 tokenizer. 
To maintain general intelligence and instruction-following capabilities, we augment this with an equivalent volume (0.1B tokens) of general-purpose supervised fine-tuning data from Tulu 3~\citep{lambert2024tulu}~\footnote{https://huggingface.co/datasets/allenai/tulu-3-sft-mixture/tree/main/data}. 
This balanced mixture ensures that the model retains broad language understanding while developing specialized role-playing abilities.
If more role-playing data are expected, our data curation pipeline can be easily applied to additional books or other fictional works, thereby acquiring data on a much larger scale.

For more details, please refer to our code. 


\subsection{Hyperparameters}

We fine-tune the LLaMA 3.1 models using the following hyperparameters: a learning rate of \(1 \times 10^{-5}\), a sequence length of 16,384, training for 8 epochs, and a global batch size of 48.



\section{Experiment Settings}
\label{sec:exp_setting}

\subsection{Test Set Sampling}

The \method Test set contains 200 samples: 
100 from books used in \method training (in-domain) and 100 from books not used in training (out-of-domain). 
We employ a weighted sampling strategy to prioritize well-established characters with more persona data. The sampling process consists of the following steps:
First, for each book, we assign character weights as the square root of the number of plots in which they are involved.
Second, we calculate the weight of a conversation as the average of its characters’ weights, including both main characters and side roles. Finally, separately for the in-domain and out-of-domain settings, we rank all conversations by weight and perform weighted sampling from the top half of conversations with higher weights.

\subsection{Prompting Strategies for Exisitng RPLA Benchmarks}

For existing RPLA benchmarks, including InCharacter~\citep{wang2024incharacter}, LifeChoice~\citep{xu2024character}, and Cross-MR~\citep{yuan2024evaluating}, we adapt or refine their prompting strategies as follows:

\begin{enumerate}
    \item For InCharacter, we add \dq{\textit{You’re consulting with a personality assessment expert who will ask you some questions. Please provide honest and detailed responses to help with the analysis. Please think carefully and state your reasons when answering the questions.}} after the character profile. 
    This adaptation aims to ensure that RPLAs honestly express their true thoughts. After being trained on authentic character dialogues, the \method models, unlike general LLMs, tend to produce brief, conversational-style answers that may be too short or may decline to answer questions, thus failing to provide sufficient information for  personality assessment.
    \item For LifeChoice and Cross-MR, we reverse the order of their reasoning and answering processes. Specifically, we have them think before providing their choices, thus enabling RPLAs to make well-considered decisions. 
\end{enumerate}

\subsection{Length Correction}
\label{sec:length_correction}

In our evaluation, we use a penalty-based scoring mechanism that counts the flaws in RPLAs' performance. 
However, since longer simulations naturally accumulate more flaws, we need to implement length correction to reduce length bias in LLM judges, following previous work~\citep{li2024judgesurvey}. 

We analyze the phenomenon of length bias in penalty-based scoring. 
The initial score is defined as \( s = 100-\sum_{f\in\mathcal{F}}v_f \), where \( \mathcal{F} \) represents the set of flaws and \( v_f \) is their severity ranging from 1 to 5. 
Our analysis is conducted on the initial scores from simulations of three models on the \method Test set: \method 70B, LLaMA-3.1 70B, and GPT-4o, with or without retrieval augmentation (three Experience and one Conversation), totaling 1,200 cases. 
We examine the relationship between the number of rounds and the vanilla scores in these 1,200 cases. 
As shown in Figure~\ref{fig:length_correction}, we plot the data points for these cases and perform linear regression to fit these points. 
The fitted linear function is \( \text{score} = -1.5909 \times \text{rounds} + 59.0617 \), which means that for each additional round in the simulation, the score decreases by approximately 1.6 points. 

To mitigate this bias, we implement length correction by compensating  
for the points deducted due to increased rounds. 
Specifically, we compute the length-corrected score as \( s = 100-\sum_{f\in\mathcal{F}}v_f + \lambda|\bar{M}| \), where \( \lambda \) is set to 1.5 based on the analysis above.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/rounds_vs_scores_scatter.png}
    \caption{
    Linear regression results showing length bias of penalty-based LLM critics in GCA evaluation.
    }
    \label{fig:length_correction}
\end{figure*}

 \section{Examples and Case Study}

We present several examples of our extracted conversations, as well as corresponding  simulations in this given circumstance by LLMs.

Tables~\ref{tab:case_sansa} to~\ref{tab:case_sansa_2} illustrate a classic  conversation extracted from \textit{A Storm of Swords (A Song of Ice and Fire, \#3)} and the corresponding simulation by \method 70B. In \method 70B’s simulation, when confronted with \textit{Sansa Stark}, \textit{Lysa Arryn} utters her iconic line, \textit{“Grown enough to be wed, wed enough to be bedded”}, reflecting her personality and worldview. 
This indicates that \method models excellently recall and apply character-related knowledge from their pretrained data.

Tables \ref{tab:case_cerci} to \ref{tab:case_cerci4} present another scene from \textit{A Dance with Dragons (A Song of Ice and Fire, \#5)}, specifically the \textit{walk of atonement}, in which \textit{Cersei Lannister} is forced to walk naked through the streets, facing both physical and mental humiliation while striving to preserve her dignity. 
We present the original dialogue alongside simulations by \method 70B, GPT-4o, and Claude-3.5-Sonnet. 
Notably, \method 70B faithfully captures the suppressed anger of \textit{Cersei Lannister} as depicted in the original conversation, whereas the other models, including GPT-4o and Claude-3.5-Sonnet, resort to a stereotypical portrayal of her arrogance and pride.


\input{Tables/case_sansa}

\input{Tables/case_sansa_2}

\input{Tables/case_cerci}

\input{Tables/case_cerci_2}

\input{Tables/case_cerci_3}

\input{Tables/case_cerci_4}




\section{Additional Results}
\label{sec:additional_results}

\paragraph{Win Rates against GPT-4o and GPT-3.5}


To further evaluate existing LLMs in given-circumstance acting, we present their win rates against GPT-3.5~\citep{openai2022chatgpt} and GPT-4o~\citep{OpenAI2023GPT4TR} in Table~\ref{tab:model-comparison-wr}, in addition to the results shown in Table~\ref{tab:model-comparison}.

\input{Tables/exp_wr}

\paragraph{Generalization of \method Models to New Characters}

Table~\ref{tab:exp_idood} separately presents the performance of LLMs on test splits from books included in and excluded from \method training. 
The results demonstrate consistent trends across both splits, confirming that \method models maintain strong performance even on out-of-domain characters.

\input{Tables/exp_idood}

\paragraph{Conversation Continuation}

Table ~\ref{tab:model-comparison-cf3} %
shows experiment results when multi-agent systems continue conversations from the first $k=3$ original messages.  

Tables ~\ref{tab:model-comparison-cf3} presents detailed evaluation results where our multi-agent simulations start from the first $k=3$ original messages.

\input{Tables/main_exp_cf3}


\paragraph{\method Dataset for Retrieval Augmentation}

We validate the effectiveness of \method’s comprehensive data types for retrieval augmentation on the \method Test set. 
We explore three retrieval sources related to specific characters: dialogues from conversations, experiences from plots, and raw text from plots. 
We compare several combinations of these sources, including:
\textit{1)} None (Base)
\textit{2)} Raw text of one plot (Raw Text)
\textit{3)} One conversation (Conv.)
\textit{4)} Character experiences from three plots (Expr.3)
\textit{5)} Expr.3 combined with Conv.
\textit{6)} Expr.10 combined with Conv.
The complete results are presented in Table~\ref{tab:rag}.

\paragraph{Ablation Studies}

We examine the effectiveness of inner thoughts in both training and evaluation. 
The complete results are demonstrated in Table~\ref{tab:model-comparison-wo-cot-full}.


\input{Tables/exp_main_cot_full}

\input{Tables/exp_rag}





\section{Prompts}
\label{sec:prompts}

In this section, we list the detailed prompts for:
\textit{1)} dataset curation in Tables ~\ref{tab:prompts_data} to ~\ref{tab:prompts_data_3}; 
\textit{2)} RPLA and multi-agent simulation in Tables ~\ref{tab:prompts_agent} to ~\ref{tab:prompts_agent_2}, which 
have been carefully optimized based on our experience in multi-agent simulation; 
\textit{3)} Penalty-based LLM Judging in Tables ~\ref{tab:prompts_eval} to ~\ref{tab:prompts_eval_2}. 


\input{Tables/prompts_data}

\input{Tables/prompts_data_2}

\input{Tables/prompts_data_3}

\input{Tables/prompts_agent}

\input{Tables/prompts_agent_2}

\input{Tables/prompts_eval}

\input{Tables/prompts_eval_2}


\section{Limitations}

There are several limitations to this study:

First, evaluation via given-circumstance acting 
still faces challenges related to LLM judges. 
While the simulation stage effectively elicits RPLA performance, the judging stage still relies on LLM judges. 
Despite our penalty-based scoring mechanism and detailed rubrics, problems such as length bias persist~\citep{}. Moreover, LLM Judges may lack the necessary knowledge to accurately evaluate character fidelity.


Second, while the dialogues extracted from novels are authentic, their corresponding thoughts remain to be optimized by future work. 
Character thoughts are often sparse in the original content, and are inferred by LLMs based on limited context. 
The generated thoughts hardly capture characters' sophisticated thinking processes. 

Third, although we’ve developed comprehensive data representations and curation pipeline to obtain high-quality data, we have not yet addressed the issue of recall in data extraction. 
Our current dataset may not cover all plots, conversations and characters from the source material. Improving recall is hence an important area for future research.

Fourth, due to copyright concerns, we release only the processed data, not the raw content from the novels. 
This may hinder future studies aimed to explore the use of raw text for RPLA developments. 
Our dataset is intended for research purposes only, and we hope our research findings will benefit RPLA developers who respect copyright policies and develop applications with proper licensing. 


Finally, our evaluation may be influenced by the varying levels of familiarity that different actor LLMs have with the selected books. 
While we use renowned novels, we cannot confirm whether a specific LLM has thoroughly learned about a particular book. 
Therefore, comparing different pre-trained models may not be entirely fair. However, comparing models within the same series would be appropriate.
