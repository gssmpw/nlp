
\section{Training and Evaluation via GCA}
\label{sec:method}

In this section, we introduce \textit{given-circumstance acting} (GCA) for training and evaluating LLMs' role-playing abilities using the \method dataset, as shown in Fig. ~\ref{fig:main}. 


\subsection{Given-Circumstance Acting}
\label{sec:gca}

In \textit{Konstantin Stanislavski}'s acting methodologies, given-circumstance acting is a fundamental approach where actors are trained and judged through performance within specified conditions including environmental context, historical events and personal conditions ~\citep{stanislavski2008actor}. 

We propose to adapt this approach to a framework that trains and evaluates LLMs' role-playing skills, leveraging the comprehensive data in \method.
In this framework, given a conversation with dialogue messages $M$, involved characters $\mathcal{C}$, and contextual setting $\mathcal{S}$, an actor LLM sequentially plays the role of each character $c\in\mathcal{C}$ to simulate the conversation. 

\subsection{GCA Training and \method Models}
\label{sec:training}

We fine-tune LLMs' role-playing abilities through GCA.  
Each training sample is derived from a conversation and one of its character $c$ in \method dataset, and LLMs are trained on $c$'s utterances $M_c$. 
Specifically, we first compose a role-playing instruction $i_\persona$ comprising the scenario description, the character's profile $p_\persona$ and motivation, and profiles of other involved characters,  which provide comprehensive context for role-playing. 
The original dialogue messages are denoted as $M=[m_1, ..., m_T]$, where $T$ is the number of turns. 
Then, the training sample $[i_\persona, m_1, ..., m_T]$ is a concatenation of the instruction and messages, where the character's messages $M_\persona\subset M$ are treated as outputs for optimization, and the other parts serve as inputs.  

We train \method 8B and \method 70B based on LLaMA 3.1 Instruct models~\citep{dubey2024llama}, using 90\% books in our dataset. 
To effectively support diverse use cases of RPLAs, 
our training samples cover extensive settings: 
\textit{1)} The \method dataset contains massive characters and conversation settings from extensive books.  
We train models on all characters in each conversation, ranging from major characters with detailed profiles to minor roles driven only by the context;  
\textit{2)} To simulate real use cases, we incorporate role-playing instructions in diverse formats through instruction templates of varying formats. 
Besides, we consider different combinations of available data by including or excluding: profiles of other characters, plot summaries, and characters' motivations;   
\textit{3)} We train models both with and without characters' internal thoughts in the extracted dialogues. 

We extend \method's training beyond character role-playing to develop complementary capabilities in environment modeling and next speaker prediction (NSP), which facilitates RPLA applications.  
To maintain models' general abilities, we augment our training data with the Tulu-3 dataset~\citep{lambert2024tulu}. 
Please refer to \S\ref{sec:app_training} for more details.  




\subsection{GCA Evaluation}
\label{sec:eval}


Evaluating role-playing LLMs remains a significant challenge, primarily in two aspects: 
\textit{1)} providing appropriate scenarios to elicit role-playing performance, and 
\textit{2)} properly assess the performance. 
Towards these challenges, we propose GCA evaluation for actor LLMs' role-playing abilities, comprising two stages: 
multi-agent simulation and penalty-based LLM judging, as illustrated in Fig.~\ref{fig:main}.

\textbf{Multi-agent Simulation} \quad 
For a test conversation $M$, 
we build a multi-agent system to simulate a conversation $\bar{M}$, in the same setting as $M$. 
We create an RPLA $\agent_\persona$ for each character $c \in \mathcal{C}$ using the actor LLM.
We provide RPLAs with comprehensive data as described in \S\ref{sec:training}:   
scenario descriptions and involved character profiles offer crucial context, and character motivations promote RPLA proactiveness and a natural conversation flow. 
Following \S\ref{sec:data_design}, 
RPLAs are instructed to output in the speech-action-thought format. 
Each RPLA's motivations and inner thoughts are inaccessible to other RPLAs. 
We adopt an NSP model to select the speaker of each turn from $\mathcal{C}\cup\{e\}$, and another LLM as the environment model $\agent_e$ to provide environmental feedback. 
The simulation ends upon an $<$END$>$ signal from NSP, or reaching the maximum of 20 turns. 
In this way, we obtain a multi-turn, multi-character simulation that comprehensively reflects the actor LLMs' role-playing abilities. 

In addition, we introduce a continue-from parameter $k$, where the simulation starts from the first $k$ original messages in $\mathcal{M}$. 
Setting $k>0$ controls the story direction and language style, similar to in-context learning. 
Hence, it enables more controlled evaluation and reduces the influence of different language styles of LLMs. 

\textbf{Penalty-based LLM Judging} \quad 
In this stage, we assess the simulated dialogue $\bar{M}$ via LLM critics. 
Different from previous LLM-as-a-judge methods for RPLA evaluation, our LLM critics:  
\textit{1)} apply penalty-based scoring by identifying role-playing flaws following detailed rubrics, and 
\textit{2)} leverage the original conversation $M$ as reference. 

Specifically, we employ LLM critics~\footnote{This paper uses GPT-4o as the critic LLM by default. } to identify  flaw instances $\mathcal{F}$ in $\bar{M}$ of specific rubrics, such as  \dq{\textit{deviate from the original conversation}} or \dq{\textit{lack initiative and goals}}, instead of directly outputting a score in previous work~\citep{wang2023rolellm, tu2024charactereval}. 
Each flaw $f$ is assigned a severity $v_f$ from 1 (minor) to 5 (severe). 
The initial score for each dimension is calculated as $s = 100-\sum_{f\in\mathcal{F}}v_f$. 

The rubrics are derived from human-annotated issues in extensive human-RPLA conversations from real users and our multi-agent simulations. 
We also consider evaluation dimensions from previous work~\citep{shanahan2023role, shao2023character, chen2024from}. 
For more informed evaluation, LLM critics are provided with additional materials, \ie, the original conversation $M$ and plot summary, besides data available to actor LLMs. 
Each dimension is assessed in independent LLM requests. 
The evaluation dimensions and their rubrics are summarized as follows:
\begin{enumerate}[itemsep=-3pt, topsep=0pt, partopsep=0pt]
    \item \textbf{Anthropomorphism}: Evaluates whether RPLAs behave in a human-like manner, with rubrics covering self-identity, emotional depth, persona coherence, and social interaction. 
    \item \textbf{Character Fidelity}: Assesses whether RPLAs faithfully portray their characters, with rubrics examining language style, knowledge and background, personality and behavior, and social relationships. 
    \item \textbf{Storyline Quality}: Evaluates whether the simulated conversation develops naturally, with rubrics focusing on narrative flow and logical consistency. 
    \item \textbf{Storyline Consistency}:  Measures alignment between the simulated conversation $\bar{M}$ and original dialogue $M$, \ie, whether RPLAs' reactions (emotions, attitudes, behaviors) remain consistent with the original. 
\end{enumerate} 

As longer simulations naturally make more flaws, we implement length correction to reduce bias in LLM judging following ~\citet{dubois2024alpacaeval}. 
Specifically, we obtain the length corrected score as 
$s = 100-\sum_{f\in\mathcal{F}}v_f + \lambda|\bar{M}|$, 
where $\lambda$ is set to 1.5 based on statistical analysis in \S\ref{sec:length_correction}. 
For detailed prompts and rubrics, please refer to \S\ref{sec:prompts}.
