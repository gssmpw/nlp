\section{Experiments}

\subsection{Settings}

\textbf{Evaluation Protocol} \quad 
We evaluate LLMs' role-playing abilities through GCA on \method Test, a test set of held-out conversations from the final 10\% of each book. 
\method Test contains 200 conversations, with 100 from books used in \method training and 100 otherwise.  
We employ GPT-4o as the critic LLM and environment model, and \method 70B for NSP. 
We exclude characters' inner thoughts for LLM critics, and set the continue-from parameter $k=0$. 
The details are in \S\ref{sec:exp_setting}.

\textbf{Metrics} \quad 
We report LLM-judged scores for each dimension, and their average as the overall score.
For analysis, we also evaluate two traditional metrics based on N-gram, \ie,  BLEU~\citep{papineni2002bleu} and ROUGE-L\citep{lin2004rougel} compared against original dialogues. 
Additionally, we report win rates versus GPT-3.5 and GPT-4o in \S\ref{sec:additional_results}. %

\textbf{Models} \quad 
Our experiments cover numerous LLMs: 
\textit{1)} \textbf{Close models}, including 
Minimax Abab7-preview~\footnote{https://www.minimaxi.com/}, Doubao-pro~\footnote{https://team.doubao.com/en/ Version:241215}, Step-1-Flash and Step-2~\footnote{https://www.stepfun.com/ Version: 241111 for Step-1-Flash, 241223 for Step-2 (internal).}, GPT-3.5~\citep{openai2022chatgpt}, GPT-4o and GPT-4o Mini~\footnote{https://openai.com/index/hello-gpt-4o/ Version:240806}, Gemini-1.5-Pro~\footnote{https://deepmind.google/technologies/gemini/pro/}, Claude-3-Haiku and Claude-3.5-Sonnet~\footnote{https://www.anthropic.com/ Version: 20240307 for Claude-3-Haiku, 20240620 for Claude-3.5-Sonnet}; 
\textit{2)} \textbf{Open models}, including \method 8B and 70B, LLaMA-3.1-Instruct 8B and 70B~\citep{dubey2024llama},  Qwen-2-Instruct 7B and  72B~\citep{yang2024qwen2technicalreport}, Vicuna-13B-1.5~\citep{zheng2023judging}, Mixtral-8x7B~\citep{jiang2024mixtral}, DeepSeek-V3~\citep{liu2024deepseek} and Higgs-Llama-3-70B~\footnote{https://boson.ai/higgs-v2/}. 

\textbf{Other Benchmarks} \quad 
We also evaluate \method models on existing RPLA benchmarks based on multi-choice questions instead of LLM judges, including InCharacter~\citep{wang2024incharacter} for personality tests, LifeChoice~\citep{xu2024character} for decision-making, and CroSS~\citep{yuan2024evaluating} for motivation recognition. 
For InCharacter, we report its accuracy on the Big Five Inventory (BFI).  



\subsection{Main Results}

\input{Tables/main_exp}

\textbf{Performance of Various LLMs on \method Test} \quad 
We apply \method Test to evaluate extensive LLMs. 
The results shown in Table~\ref{tab:model-comparison} are  averaged across three runs, from which we observe that: 
\textit{1)} \method 70B achieves state-of-the-art performance across both LLM-judged  and N-gram-based metrics. 
For LLM-judged metrics, \method 70B outperforms all open models and shows competitive performance with GPT-4o. \method 8B similarly outperforms models of comparable scale.
For N-gram-based metrics, \method models demonstrate substantial improvements over existing models, exceeding the second-best performance by 58\% on BLEU; 
\textit{2)} Among all models, GPT-4o, Gemini Pro, Claude-3.5-Sonnet, Doubao-pro, Step-2, Qwen-2-72B, and \method 70B demonstrate superior performance, achieving average scores above 57\%; 
\textit{3)} Table~\ref{tab:exp_idood} presents LLM performance separately for test splits from books included or excluded for  \method training. 
The results show consistent trends across both splits, validating that \method models maintain strong performance on out-of-domain characters. 

\textbf{Authentic Conversations from High-quality Novels Improve LLMs' Role-playing Ability} \quad 
According to Table \ref{tab:model-comparison}, 
\method models demonstrate significant improvements over their LLaMA 3.1 baselines.
In contrast, Higgs-LLaMA-3 70B, fine-tuned on synthesized dialogues, performs below LLaMA-3.1 70B,
These results highlight the importance of high-quality, authentic role-playing data for LLM training. 

\textbf{Conversation Continuation Enables More Controlled Evaluation} \quad 
Table ~\ref{tab:model-comparison-cf3} shows experiment results when multi-agent systems start from the first $k=3$ original messages.  
In this setting, model obtain higher scores compared to simulations from scratch ($k=0$), with reduced performance gaps between different models, especially for BLEU and ROUGE-L results. 
For example, the average score gap between Qwen-2 72B and 7B decreases from 11.5\% ($k=0$) to 8.8\% ($k=3$).
This improvement occurs because the $k=3$ original messages guide the story direction and language style, particularly benefiting smaller models that typically struggle with complex role-playing instructions. 


\textbf{Results on Other Benchmarks} \quad 
We evaluate \method and other models on existing benchmarks for RPLAs based on multi-choice questions. 
As shown in Table~\ref{tab:exp_other_bmks}, \method 70B achieves state-of-the-art performance across these benchmarks. 
Notably, \method 70B achieves 93.47\% accuracy on LifeChoice, surpassing GPT-4o by 23\%. 
These results exhibit \method models' strong capability in nuanced portrayal of characters' personalities and behaviors. 

\input{Tables/exp_otherbmks}


\subsection{Ablation Studies}

\input{Tables/main_exp_cot}
\textbf{Inner Thoughts and Motivations Enhance RPLAs at Test Time} \quad 
Table~\ref{tab:model-comparison-inner-thought} compares LLMs' overall scores on \method Test with or without inner thoughts and  motivations. 
The results show consistent performance improvements across all models when inner thoughts and motivations are included. 


\textbf{Inner Thoughts Benefit Role-Playing Training} \quad 
We train \method model variants without inner thoughts, and evaluate them on various benchmarks.  
Results in Tables ~\ref{tab:exp_other_bmks} and ~\ref{tab:model-comparison-inner-thought}  show that models trained without inner thoughts consistently underperform regular \method models, exhibiting the value of inner thoughts for role-playing training.




        
\subsection{\method Dataset for Retrieval Augmentation}


We evaluate the value of our comprehensive data types for retrieval augmentation on \method Test. 
We explore three retrieval sources for a specific character: 
dialogues in related conversations (Conv.), as well as experiences (Expr.) and raw text in related plots. 
The retrieval system is based on FAISS~\citep{douze2024faiss} with BGE-M3~\citep{chen2024bge} for text embeddings.
As shown in Fig.~\ref{tab:rag}, we observe:
\textit{1)} Models consistently benefit from characters' retrieved experiences and conversations, especially for \method 70B;
\textit{2)} However, raw text retrieval barely enhances LLMs' performance. 
Detailed experimental settings and results are provided in \S\ref{sec:additional_results} and Table ~\ref{tab:rag}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/rag_comparison.jpg}
    \caption{LLM Performance on \method Test 
    with retrieval augmentation from various character data. 
    Expr. and Conv. denote experiences and conversations respectively.  
    }
    \label{fig:distribution}
\end{figure}


\subsection{Case studies}

  
We conduct case studies to analyze LLMs' performance in GCA simulation. 
Several cases are presented in Tables~\ref{tab:case_sansa} to~\ref{tab:case_cerci4}, from which we observe that:
\textit{1)} \method models, trained on authentic dialogues, communicate more naturally, closely aligning with human speech patterns.
\textit{2)} \method models better recall character-related knowledge, such as the iconic line \textit{\dq{Grown enough to be wed, wed enough to be bedded}} by \textit{Lysa Arryn} in Table~\ref{tab:case_sansa}.
This is consistent with their high BLEU and ROUGE-L scores.
\textit{3)} \method models better portray sophisticated thinking process of humans. 
For example, in Tables~\ref{tab:case_cerci} to~\ref{tab:case_cerci4}, 
\method 70B faithfully reproduces \textit{Cersei Lannister}â€™s suppressed anger as depicted in the original conversations, while other models, including GPT-4o and Claude-3.5-Sonnet, resort to a stereotypical portrayal of her arrogance and pride.
