\section{Hierarchical Scene Synthesis with LLM}

Given a text description $t_r$ and the scene size $s_r$ as conditions, our approach is composed of three stages, as illustrated in Figure~\ref{pipeline}. First, we prompt the pre-trained LLM to generate the hierarchical structure with text descriptions. Second, we train a hierarchy-aware graph neural network to infer the relative placement coordinates between objects.
% the fine-grained relative placements between objects corresponding to the LLM-generated textual spatial relations. 
Third, we design a divide-and-conquer optimization which optimizes the sub-layout for each functional area and then arranges their placements to form the entire scene.  

% \vspace{-5pt}
\subsection{Hierarchical Structure Generation with LLM} 
Given the user requirement, the pre-trained LLM takes the constructed prompt as input and outputs structured text to describe the hierarchical scene representation, including the node attributes. The key challenge is to generate reasonable and informative spatial relations to specify the scene layout.

Although existing works define dense object relations to describe layouts, the more detailed the descriptions are, the more incorrect or self-contradictory results they make, due to the lack of spatial reasoning ability of the LLM. Therefore, in our approach, we require the LLM to generate a hierarchical structure to ground the objects and only the spatial relations between objects belonging to the same area, only to roughly specify their arrangements.

We construct the input prompt with three components: 1) a description of the LLM's role and task, including a brief definition of the hierarchical structure with the meaning of the nodes and connections; 2) a description of the preferred data format and pre-defined constraints, including the types of functional areas, possible anchor objects, and spatial relations; and 3) an example of a simple scene in the preferred format and the specific user requirements. We don't require the example to be selected corresponding to the user requirement, but only to demonstrate the output format. In this stage, the LLM generates textual descriptions and size attributes of the functional areas and objects, as well as the textual descriptions of spatial relations.

% \subsection{Data-Driven Inference of Relative Placements}
% \subsection{Hierarchy-Aware Network Inference}
\subsection{Fine-Grained Relative Placement Inference}
We propose a hierarchy-aware graph neural network to infer the fine-grained relative placements between correlated objects. The relative placements within each functional area exhibit a more compact and generalizable prior, allowing us to train a network to infer the placements for various scenes.

% As illustrated in Figure~\ref{pipeline}, given the LLM-generated hierarchy, we construct the input graph $G=(V,E)$ with the nodes as objects and edges connecting all the objects belonging to the same functional area. Although the input includes all the objects in the scene, the functional areas are isolated from each other. We use Linear embeddings for the object sizes $s_o$ and the relative placement coordinates $[p_e, \theta_e, d_e]$, where $d_e$ is a binary indicator of the alignment between two objects, and the pre-trained CLIP text encoder~\cite{radford2021learningtransferablevisualmodels} for descriptions of objects and spatial relations,
As illustrated in Figure~\ref{pipeline}, given the LLM-generated hierarchy, we construct the input graph $G=(O,E)$ with the nodes as objects and edges connecting all objects belonging to the same functional area. Although the input includes all objects in the scene, the functional areas are isolated from each other. We use Linear embeddings for the object sizes $s_o$ and the ground truth relative placement coordinates $[p_e, \theta_e, d_e]$, where $d_e$ is a binary indicator of the alignment between two objects, and the pre-trained CLIP text encoder~\cite{radford2021learningtransferablevisualmodels} for descriptions of objects $t_o$ and spatial relations $t_e$. They are organized as node features $h_o$ and edge features $h_e$, i.e.
%Note that the $[p_e, \theta_e, d_e]$ is only used during the training process. We incorporate this information into each node and edge of the $G$ to conceptualize it as the contextual graph $G_c$ with the node embedding $h_o$ and edge embedding $h_e$, 

\begin{equation}
\begin{aligned}
& h_o = [\mathrm{CLIP_t}(t_o), \mathrm{LINEAR(s_o)}], \\
& h_e = [\mathrm{CLIP_t}(t_e), \mathrm{LINEAR}(p_e, \theta_e, d_e)],
\end{aligned}
\end{equation}
which forms the contextual graph for the following network processing. Note that since we only have textual spatial relations between the anchor object and the others, we use all-zero vectors as the text embeddings for the edges without corresponding textual spatial relations (dotted arrows). 

We adopt the variational graph neural network~\cite{zhai2024commonscenes} for the contextual graph with the $h_o$ and $h_e$. Both the encoder and decoder are composed of several MLPs for 5 rounds of message passing, including $g_e^{(k)}$ for updating the edge features with connected node features in the $k$th round and $g_o^{(k)}$ for updating the node features with the 1-ring neighbor nodes, i.e. 
\begin{equation}
\begin{aligned}
h_{e_{i\xrightarrow{} j}}^{(k+1)}&=g_e^{(k)}(h_{o_i}^{(k)}, h_{e_{i\xrightarrow{} j}}^{(k)}, h_{o_j}^{(k)}) \\
h_{o_i}^{(k+1)} &=h_{o_i}^{(k)} + g_o^{(k)}(\mathrm{AVG}(h_{o_j}^{(k)}|o_j \in N_ \mathcal{G}(o_i))),
\end{aligned}
\end{equation}
where $e_{i\xrightarrow{} j}$ represents an edge connecting two objects $o_i$ and $o_j$, $N_\mathcal{G}(o_i)$ represents the set of neighbor nodes connected with object $o_i$. The encoder takes the contextual graph as input and outputs the graph with updated features, where the edge features (specifically the relative placement components of edge features, as shown in Figure~\ref{pipeline}) are parameterized as a Gaussian distribution. The decoder takes the updated graph as input and randomly samples from the Gaussian distribution. Finally, we use separate MLPs to decode the relative placement $[\hat{p}_e, \hat{\theta}_e, \hat{d}_e]$.

During training, we freeze the CLIP text encoder and update all other network layers. The loss function is 
\begin{equation}
L=L_{KL} + L_{ep} + L_{e\theta} + L_{ed},
\end{equation}
where $L_{KL}$ is the Kullback-Liebler divergence between the Gaussian distribution and
posterior distribution of the edge feature components. $L_{ep}$ is L1 loss on the relative positions $p_e$. $L_{e\theta}$ and $L_{ed}$ are cross-entropy loss on the discretized relative orientation angles and the binary alignment indicator.

\subsection{Divide-and-Conquer Layout Optimization} 

Given the hierarchical scene with the relative placements between correlated objects, we develop a divide-and-conquer optimization to solve for the final layout. Our solution includes a local optimization for each functional area and then a global optimization to organize the areas into scenes. This optimization produces reasonable and physically feasible layouts more effectively than a simple global optimization or iteratively optimizing each object's placements.

\input{CameraReady/Figures/comparison_topview}

\noindent \textbf{Local optimization.} For each functional area, we use local optimization to solve the object placements w.r.t. the bounding box of the functional area. The local optimization is formulated to minimize the objects' relative placements and those inferred by the network, with constraints to avoid object overlap and out-of-boundary, i.e.
\begin{equation}
\begin{aligned}
\min_{o'_i\in O} &  \sum_{o_i\in N_\mathcal{G}(o_a)} |\mathrm{REL}(o'_i, o'_a)-[p_{e_{i\xrightarrow{} a}},\theta_{e_{i\xrightarrow{} a}}]|, \quad \\
s.t. \quad & C_{overlap}(o'_i, o'_j), \quad \forall o_i,o_j \in A \\
& C_{OOB}(o'_i, s_a), \quad \forall o_i \in A 
\end{aligned}
\end{equation}
where $o'_i$ and $o'_a$ refers to the placements (center positions and orientations) w.r.t. the functional area of an object $o_i$ and the anchor object $o_a$, respectively. $\mathrm{REL}$ computes the relative placements between two objects and $[p_{e_{i\xrightarrow{} a}},\theta_{e_{i\xrightarrow{} a}}]$ is the relative positions between object $o_i$ and $o_a$ predicted by the network. $A$ represents the set of objects within the area. $C_{overlap}$ constrains the overlap between oriented bounding boxes of any two objects as small as possible, and $C_{OOB}$ aims to avoid the object boxes lying out of the area boundary, whose size $s_a$ is generated from pre-trained LLM in stage 1.


\noindent \textbf{Global optimization.} We then organize the areas to form scenes with global optimization. Each functional area takes the orientation of its anchor object as its own orientation. Based on observations in our daily life, the optimization is formulated to place the functional areas against the walls and far from each other with orientations pointing inside the scene, while avoiding object overlap and out-of-boundary:
\begin{equation}
\begin{aligned}
\min_{a_i} & \sum_{a_i} |\mathrm{D_w}(a_i, s_r)| - \sum_{a_i, a_j} |\mathrm{D_a}(a_i, a_j)|, \\
s.t. \quad & C_{overlap}(a_i, a_j), \quad \forall a_i,a_j \in S \\
& C_{OOB}(a_i, s_r), \quad \forall a_i \in S  
\end{aligned}
\end{equation}
where $a_i$ denotes area placement (center position and orientation). $D_w$ is the distance between back side of the area and the boundary of the scene. $D_a$ is the distance between two areas' bounding boxes. $S$ is the set of areas within the scene. $C_{overlap}$ and $C_{OOB}$ are same as in local optimization.

After the optimizations, we transform the coordinate systems to obtain object positions and orientations in the frame of scenes. Finally, we retrieve 3D object models from Objaverse~\cite{objaverseXL} and 3D-Front datasets~\cite{fu20213d} based on the CLIP scores, i.e. cosine similarity between object images and text embeddings. The object models are then scaled and placed according to the scene layouts.


