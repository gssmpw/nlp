% \vspace{-8pt}
\section{Experiment and Results}
\input{CameraReady/Tables/comparison_table}

\subsection{Experiment Settings}

\noindent\textbf{Dataset.} We conduct the comparison and ablation study on the 3D-Front dataset~\cite{fu20213d}. That is, we train the deep-learning-based approaches on this dataset and constrain the LLM-assisted methods to synthesize scenes with object categories within this dataset, for a fair comparison. Following LayoutGPT~\cite{Feng2023LayoutGPTCV}, we take the room category and floor, i.e. its width and height, as input conditions and filter out the scenes with irregular floors. The sizes of the training sets are 3397 and 690 for bedrooms and living rooms, while the corresponding test sets are 60 and 53.

\noindent\textbf{Metrics.} We evaluate the generated scenes from two perspectives. One is the physical feasibility of the scenes, estimated by the overlap between oriented bounding boxes and out-of-boundary metrics, i.e. overlap and OOB. The other is the reasonable organization of scenes, for which we select some common object pairs, i.e. bed-nightstand, table-chair, table-sofa, and measure the averaged KL-divergence between the relative placement distributions of the ground-truth scenes in the test sets and the generated scenes.

\noindent\textbf{Implementation.} We use GPT-4\cite{achiam2023gpt} for all the LLM-assisted approaches for the evaluation (the open-source LLaMA also works well with our approach). We train the hierarchy-aware neural network with 500 epochs using the Adam optimizer, where the batch size is 4 and the learning rate is 1e-4. The network is trained on the combination of the bedroom and living room training sets, which takes about 8 hours on a Nvidia 4090 GPU. Our divide-and-conquer optimization is implemented with the GUROBI solver~\cite{gurobi}. Our approach takes about 2 minutes to synthesize a scene with 8 objects.  

%我们在一个具有40GB内存的NVIDIA A100 GPU上进行CommonScenes的训练、评估和可视化。我们采用初始学习率为1e-4的AdamW优化器对网络进行端到端训练。我们在所有的实验中设置{λ1， λ2， λ3} ={1.0, 1.0, 1.0}。分布Z中的Nc设为128,TSDF大小D设为64。
%We conduct the training, evaluation, and visualization of CommonScenes on a single NVIDIA A100 GPU with 40GB memory. We adopt the AdamW optimizer with an initial learning rate of 1e-4 to train the network in an end-to-end manner. We set {λ1, λ2, λ3} = {1.0, 1.0, 1.0} in all our experiments. Nc in distribution Z is set to 128 and TSDF size D is set as 64. We provide more details in the Supplementary Material.

% Only the hierarchy aware graph neural network needs to be trained in our approach. We train the network with 500 iterations using the Adam optimizer, where the batch size is 4 and the learning rate is 1e-4. The network is trained on the combination of the bedroom and living room training sets, which takes about 8 hours on a Nvidia 4090 GPU. Our divide-and-conquer optimization is implemented with the GUROBI solver~\cite{gurobi}, which takes about 2 minutes to synthesize a scene with 8 objects.


\subsection{Comparisons}

We compare with two types of state-of-the-art approaches, including those training deep neural networks from scratch, i.e. ATISS~\cite{Paschalidou2021ATISSAT} and DiffuScene~\cite{Tang2023DiffuSceneSG}, and LLM-assisted indoor scene synthesis, i.e. LayoutGPT~\cite{Feng2023LayoutGPTCV} and HOLODECK~\cite{Yang2023HolodeckLG}. We re-train the deep networks using their released code on the same train/test split. For the LLM-assisted approaches, we use their implementations of the pipelines and invoke the same version of GPT for the inference.

\noindent\textbf{Qualitative Evaluation.} Figure~\ref{fig:comparison_topview} presents generated scenes of different methods. The deep learning methods ATISS and DiffuScene generate results with reasonable placements of objects. But the networks are not guaranteed to ensure the physical feasibility of the scene layouts and sometimes cause object overlap and out of the floor boundary. LayoutGPT, which uses in-context learning to infer numerical layouts based on the demonstrated examples, generates many incorrect orientations and positions. HOLODECK produces relatively better results in terms of physical feasibility, but some objects are not placed in the optimal position as specified by the LLM. By contrast, our approach is able to produce more reasonable and feasible scene layouts.

\noindent\textbf{Quanlitative Evaluation.} Table~\ref{tab:comparison} validates the observations from the visual results. Among all the methods, we achieve the best in terms of both the physical feasibility (overlap and OOB) and the reasonable relative placements (KL Div.). It is interesting to see that the data-driven approaches are good at objects' relative positions and LLM-assisted optimization, i.e. HOLODECK, obtains more feasible results, while ours takes the merit of both and won the best on all the metrics.

\input{CameraReady/Tables/holodeck_table}

We further provide an additional quantitative comparison with HOLODECK, the closest work to ours, as both use the LLM to generate textual scene descriptions and then solve for the scene layouts. The difference is that HOLODECK requires dense and detailed spatial relations while ours uses hierarchical structures with sparse relations as well as a neural network to infer the fine-grained relative placements. Table~\ref{tab:comparison_holo} reports the semantic alignment between the LLM-generated descriptions and the generated scenes. Specifically, $\#$Rel. counts the percentage of relative placements matching with LLM-generated spatial relations and $\#$Obj. counts the existence of LLM-specified objects. Obviously, our results align better with LLM arrangements, implying the advantages of using hierarchical scene representation with our approach.

\input{CameraReady/Tables/perceptual_study}

\noindent\textbf{Perceptual Study.} The perceptual study evaluates the quality of scenes generated by different methods (those used in the comparison experiments). We present 25 generated scenes with the input user requirements to 30 participants, who rate them on a 5-point Likert scale from three aspects: scene effectiveness, physical feasibility, and layout rationality. Since both the rendered scenes and the topview visualizations are presented, the participants are sensitive to unreasonable placements which might be covered by the occlusion in the renderings. Table~\ref{tab:perceptual_study} shows that our results get the highest scores w.r.t. all three aspects, i.e. high scene effectiveness as we prompt the LLM with functional area considerations, high feasibility and layout rationality as our approach can effectively generate feasible scenes aligned with LLM arrangement. Otherwise, although HOLODECK prevents object overlap or out-of-boundary, since its results may break the LLM arrangements, it may affect possible human activity, such as shown in the top row case in Figure~\ref{fig:comparison_topview}.

\input{CameraReady/Figures/ablation_topview}
\input{CameraReady/Tables/ablation_table}

\subsection{Ablation Study}

The ablation study validates the key designs of our approach, i.e. the hierarchy-aware network and the divide-and-conquer optimization, by removing the corresponding stage from our pipeline. When removing the hierarchy-aware network, we pre-define the relative placement coordinates for the textual spatial relations to replace the predictions of the network. When removing the divide-and-conquer optimization, we directly require the LLM to generate coordinates of anchor objects and transform the relative placements of the other objects into global coordinates without optimization refinement. The results are shown in Table~\ref{tab:ablation_study} and Figure~\ref{fig:ablation_topview}. It indicates that our hierarchy-aware network is of vital importance in capturing the reasonable relative placements between objects, and the divide-and-conquer optimization ensures the physical feasibility of the generated scene layouts.
