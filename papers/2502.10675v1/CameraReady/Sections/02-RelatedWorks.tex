\section{Related Work}

\noindent \textbf{Indoor Scene Synthesis.} The common practice is to produce a set of objects and their placements~\cite{Patil2023AdvancesID}, i.e. scene layouts. Early works rely on the pre-defined rules~\cite{Yu2011MakeIH, Merrell2011InteractiveFL, Ma2016Actiondriven3I, Fu2017AdaptiveSO, Fu2020HumancentricMF} to generate interpretable and feasible scene layouts. To further capture diverse spatial arrangement, the data-driven approaches~\cite{fisher2012example, Qi2018HumanCentricIS, xu2014organizing, ma2018language, Sun2022SequentialFO, Sun2024SequentialSA} learn the object relationship from datasets~\cite{fu20213d, song2017semantic}. The researchers have developed all kinds of networks to learn the scenes represented as different data structures, including sequences~\cite{Wang2018DeepCP}, graphs~\cite{Zhou2019SceneGraphNetNM, Wang2019PlanITPA}, hierarchies~\cite{li2019grains, Gao2023SceneHGNHG}, sets~\cite{Paschalidou2021ATISSAT, Wei2023LEGONetLR, Tang2023DiffuSceneSG, zhai2024echoscene}, etc. However, due to the inherent complexity, it is difficult to capture the essential relationship from the observed layouts and generalize to other categories.

\input{CameraReady/Figures/hierarchy}

Incorporating additional knowledge enhances scene prior learning.  Graph-to-3D~\cite{Dhamo2021Graphto3DEG} and CommonScenes~\cite{zhai2024commonscenes} synthesize the layouts and object shapes for coherent scenes. Some methods~\cite{ye2022scene, yi2023mime} take human motion trajectory as conditions to populate the objects. Haisor~\cite{sun2024haisor} uses reinforcement learning with human interaction and space area consideration for scene synthesis. External expert knowledge~\cite{leimer2022layoutenhancer, yang2024learning} can also be incorporated during training to enhance network performance. These works refer to the same observation that indoor scene synthesis involves a comprehensive consideration of space partition, functional arrangement, and aesthetic creativity, thus requiring a generative model with extensive knowledge.


\noindent \textbf{Text-to-Scene Synthesis.} The challenges include semantic understanding of user requirements and scene synthesis. Given a natural language description, early works~\cite{ChangSM14, ChangMSPM15, ChangESM17, SavvaCA17, MaPFLPHYTGZ18, Yang2021} parse the input as scene templates, where the nodes represent objects and edges for spatial relations, and then sample the corresponding object models and placements. With the development of deep learning, some works~\cite{Paschalidou2021ATISSAT, Tang2023DiffuSceneSG, Dinh2024} take latent vectors such as textual embeddings as conditions, and train conditional scene synthesis model to output scenes. However, these works rely on detailed descriptions as input to specify the setting of target scenes, e.g. "there is a desk and there is a notepad on the desk", rather than reasoning the scene configuration from abstract instructions. Moreover, generalizing to diverse scene categories and open-vocabulary settings is a long-standing problem. 

\input{CameraReady/Figures/pipeline}

\noindent \textbf{LLM-Assisted Scene Synthesis.} Recent works have investigated utilizing the pre-trained LLMs to handle the multi-objects in the scenes, most of which focus on the 2D layout for controllable scene image synthesis~\cite{lian2023llm, gani2023llm, nie2024compositional}. LayoutGPT~\cite{Feng2023LayoutGPTCV} retrieves scene layouts in CSS format, with LLMs outputting numerical bounding boxes for each object. However, due to the lack of spatial reasoning ability, LLM cannot handle the complex relationships of 3D scenes, causing heavy object overlap and out-of-boundary problems.

Some others use LLM to generate a textual scene description and convert it to 3D scenes. Aladdin~\cite{Huang2023AladdinZH} introduced a pipeline to sample and generate 3D textured assets from an abstract description and manually organize them to construct a scene. Recent works~\cite{Wen2023AnyHomeOG, Yang2023HolodeckLG} require LLM to describe the object relations using pre-defined atom relations, which are interpreted as fixed relative positions between objects and refined using a rule-based optimization algorithm. However, our experiments show that defining a compact yet informative atom relations is difficult. Dense and detailed object relations are able to provide precise spatial arrangement but often cause self-contradiction in the LLM outputs, while a sparse set of coarse relations leads to coherent arrangements but fails to capture the diverse spatial placements among objects. 
