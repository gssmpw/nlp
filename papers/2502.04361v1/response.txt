\section{Related Work}
A large body of work has emerged over the last decade to enable VR authentication, summarized in multiple recent reviews **Zhao et al., "Recent Advances in VR Authentication"**. Among the first approaches to VR authentication has been porting of traditional credentials such as PIN and password, and their adaptation to VR environments, e.g., via object arrangements **Li, "Object Arrangement for Secure Credentials"**. Though some resistance to shoulder-surfing has been shown, successful malicious access of the credential compromises security. A number of recent approaches investigate the use of user interactions with VR headsets and hand controllers as a behavior signature, with work spanning the use of individual or combination of headset motion, hand movements, and eye gaze. Recent work uses deep neural networks **Kim et al., "Deep Neural Networks for VR Authentication"** based on their ability to represent non-linearities in the data. Various activities have been investigated, including ball-throwing **Suzuki, "Ball Throwing as a Behavior Signature"**, pointing **Miller, "Pointing with Hand Controllers"**, door-opening **Taylor, "Door Opening for Secure VR Authentication"**, pinching **Ahuja et al., "Pinching as a Secure Action"**, object movement **Li, "Object Movement in VR Environments"**, and gameplay **Chen et al., "Gameplay-Based Behavior Signature"**. 

Prior methods use movements as tracked by the tracking mechanisms available on existing VR devices. In devices such as the HTC VIVE that use lighthouses, IR light emitted by the lighthouse is tracked throughout performance by receptors on the headset and hand controllers, resulting in no more than 3 tracks, or 4 tracks in headsets that track eye gaze. Camera-based tracking such as on the Meta Quest has traditionally tracked the headset using an inside-out approach and the hand controllers through visual observations, resulting in a similar set of 3 or 4 tracks. Datasets used by nearly all approaches thus far have involved participants using hand controllers. **Suzuki et al., "Controller-Free Hand Tracking"** use controller-free hand tracking in the Meta Quest to obtain higher-resolution motion tracks for the fingers, enabling the use of pinching as a signature. However, while datasets like Alyx **Kim et al., "Alyx: A Dataset for VR Authentication"** exist, none of the approaches has external video data or use full-body joint articulation, e.g., at the elbows and shoulders, the torso, or the lower body, due to their reliance solely on the data tracked by the VR devices that are currently incapable of tracking full-body articulation, only **Miller et al., "External Video Recordings for VR Authentication"** provides external video recordings.

Our method provides the first approach for authentication using external videos to acquire detailed knowledge of full body articulation, while taking advantage of 3D information through intermediate prediction of the 3D controller motion. In conducting knowledge-augmentation by incorporating future 3D controller trajectory prediction using deep neural networks, our method is closest to the work of **Li et al., "Deep Neural Networks for VR Authentication"**. However, our work differs in that **Li et al., "VR Device 3D Trajectory Data as Input"** use VR device 3D trajectory data as input similar to existing methods.