
VR has seen rapid growth in critical domains such as education~\cite{noah2021exploring,agbo2021application,radianti2020systematic}, nursing and medicine~\cite{hamilton2021immersive,shorey2021use,barteit2021augmented,clarke2021virtual}, retail~\cite{pizzi2019virtual,xue2019virtual}, personal finance~\cite{campbell2019uses,weise2016virtual}, remote teleoperation and driving~\cite{macias2019measuring,lager2019remote,neumeier2019teleoperation,neumeier2018way}, and healthcare~\cite{munoz2022immersive,mehrabi2021seas,mehrabi2022immersive,karaosmanoglu2022canoe,farivc2021virtual,xu2021effect}. As VR devices become more affordable and portable, it is likely that more users will adopt them for everyday use. User activities in these domains can contain personal and sensitive data that must be protected from a malicious user. As a result, such critical applications must contain mechanisms to identify or authenticate a user. Early research in securing VR systems adopted traditional PIN and password-based credentials~\cite{alsulaiman2008three,alsulaiman2006novel,gurary2017leveraging,george2020gazeroomlock,yu2016exploration,george2017seamless,olade2020exploring,funk2019lookunlock,george2019investigating}. Techniques based on a password or a PIN are known to be unsafe, as once the malicious agent gains access to the credentials, the user's account is immediately compromised. The malicious agent may be an external agent or the genuine user deliberately handing their credentials to an ally to defeat a system. A genuine user handing over credentials to an ally is a second problem in environments where cheating or non-adherence is a prevalent issue, such as education or healthcare. A simple password, PIN, or multi-factor authentication mechanism will fail as the user can provide an ally with the credentials to complete a task, for example, in healthcare, asking an able ally to complete a fitness test. Non-adherence to at-home exercise routines is a serious concern~\cite{jenkins2017exercise}, with estimates of 30-50\% after injury or surgery~\cite{bassett2012measuring,argent2018patient}, and up to 70\% for musculoskeletal outpatient settings~\cite{jack2010barriers}. While immersive VR shows promise to combat non-adherence~\cite{kern2019immersive,manera2016feasibility}, studies so far are conducted with non-at-risk groups, and, as surveyed in \cite{miller2014effectiveness}, demonstrate inconsistency in adherence reporting and do not discuss continued post-study usage. 

To combat the challenges of PIN and password-based techniques, a large body of work has emerged to use user behavior in VR, e.g., the motion trajectories of VR headsets and hand controllers, as a biometric signature for securing access to a VR application~\cite{mustafa2018unsure,mmm2019vr,pfeuffer2019behavioural,ajit2019combining,miller2019,mathis2020knowledge,mathis2020rubikauth,mathis2020fast,miller2020within,olade2020biomove,miller2020personal,miller2021using,liebers2021understanding}. Identification accuracies have reached upwards of 95\%~\cite{mathis2020knowledge,miller2020within,olade2020biomove,miller2020personal,miller2021using}, and these approaches investigate identification and authentication for a number of tasks, e.g., watching a video, pointing, throwing a ball, turning a cube, and making a golf swing where tasks are easily remembered and largely repeatable. Recent work on the temporal effect on behavioral biometrics has explored the impact of short-, medium-, and long-timescale user behavior variations and demonstrated that deep learning techniques can be designed to be robust to changes in user behavior over time~\cite{miller2022temporal}. 

A fundamental deficiency of existing work on behavior-based biometrics for securing VR systems is the reliance on complete or near complete trajectories of user behavior for high accuracy results. Work by Kupin et al.~\cite{mmm2019vr}, Ajit et al.~\cite{ajit2019combining}, and Miller et al.~\cite{miller2020within,miller2021using} demonstrate that using smaller portions of the entire trajectory yield lower performance, with performance dropping significantly when less than 80\% of the trajectory is used. Waiting for a complete or near complete trajectory to perform high-assurance authentication leaves the environment vulnerable to attacks. In this paper, we propose the first approach that uses motion forecasting to predict plausible future motion trajectories. We leverage the idea that given a task with a starting point and end goal, for example throwing a ball at a target, shooting an arrow, or lifting a weight, a plausible trajectory path for a user can be predicted if we know the current and prior trajectory positions. 

Using motion forecasting for path, or trajectory, planning has received increased attention due to the growth of autonomous driving systems where motions of objects must be predicted ahead of time to prevent hazardous situations~\cite{liang2020learning,zeng2021lanercnn,zhou2022hivt,huang2022multi,li2020end,kong2022human,liu2021multimodal,yuan2021agentformer}. In our approach, we use the predicted motion trajectories to perform authentication and demonstrate that we can achieve higher accuracies. To validate our approach we use the 41-subject ball-throwing dataset of Miller et al.~\cite{miller2022temporal,miller2022combining}. We train a Transformer-based model~\cite{vaswani2017attention,devlin2018bert,zhou2021informer} to forecast the user's motion behavior trajectory for a period of time in the future using a portion of the starting trajectory. We combine the starting and forecasted trajectory and use it as input for a classifier to authenticate the user. We try a Fully Convolutional Network (FCN) and a Transformer encoder as the classifier. %We evaluate the performance of our approach using by computing the equal error rate (EER), the point at which the false acceptance rate equals the false rejection rate. 
Our approach improves the authentication equal error rate (EER) to a minimum of 4.84\% with an average reduction of 23.85\% and with a maximum of 36.14\%. The reduction of authentication EER suggests that our approach can be used to perform authentication using limited starting trajectory data, thereby reducing the likelihood of an impostor gaining access to the system.

\section{Related Work}
A growing number of approaches have arisen in the last decade on VR authentication. Their impact is supported by recent literature survey~\cite{jones2021literature,giaretta2022security}, a Systematization of Knowledge (SoK)~\cite{stephenson2022sok}, and position papers~\cite{alt2022beyond,viswanathan2022security} making recommendations on the future of VR security, e.g., use of multidimensional mechanisms~\cite{viswanathan2022security} and integration of multiple modalities such as physiological (e.g., face) and behavioral biometrics~\cite{alt2022beyond}, and the need to enable cross-device or cross-context security~\cite{alt2022beyond}. Sections \ref{subsec:pin} to \ref{subsec:behavior} discuss related work in passwords/PINs, iris/periocular biometrics, and behavioral biometrics for VR security. Section~\ref{subsec:motion} discusses work in motion forecasting for autonomous driving that inspires our work.

\subsection{Passwords and PINs}
\label{subsec:pin}
Traditional work in providing security in VR environments has largely addressed the question of enabling users to enter credentials such as passwords in the VR environment. The focus of investigation for these approaches tends to be to provide resistance to shoulder-surfing attacks, and to ensure usability by assessing how convenient it is for the user to enter the password. Some approaches focus on directly translating the concept of a 2D password to the VR environment. Mechanisms to seek entry of alphanumeric passwords can be challenging, as using controllers or gaze to interact with a VR keyboard can be cumbersome. As such, 2D passwords tend to largely be lock patterns similar to those on smart devices. Studies have investigated the security and usability of lock patterns imposed on axis aligned or inclined planes~\cite{yu2016exploration,george2017seamless}. The studies have conducted evaluations of the type of interaction that is most convenient for usability, e.g., pointing and pulling the controller trigger versus using a VR stylus or clicking the trackpad~\cite{george2017seamless}. Evaluations have also been conducted of resistance of VR lock patterns to shoulder surfing~\cite{olade2020exploring}. Other approaches advocate the use of the 3D space to provide novel 3D passwords. These passwords may either consist of a unique selection of 3D virtual objects~\cite{yu2016exploration,george2019investigating,funk2019lookunlock,george2020gazeroomlock}, or of a unique sequence of actions performed by the user in the virtual environment~\cite{gurary2017leveraging}. Inspiration for the latter comes from analyses of the action space for 3D passwords in a graphical environment and the ability of the action space to provide security guarantees~\cite{alsulaiman2006novel,alsulaiman2008three}. 3D passwords based on virtual object selection may be entered by selecting the object permutation using a controller~\cite{gurary2017leveraging}, using gaze to point at the objects comprising the sequence~\cite{funk2019lookunlock}, or using a combination of gaze- and controller-based selection~\cite{george2020gazeroomlock}. 

Most studies demonstrate high shoulder-surfing resistance of password entry mechanisms, with 3D passwords being more resistant to 2D passwords~\cite{yu2016exploration}. However, if an attacker gains access via an alternate mechanism, e.g., through a man-in-the-middle attack, the system is immediately compromised. Additionally, while 3D passwords may provide higher security guarantees~\cite{yu2016exploration}, since they are an uncommon form of password entry, users may face lower usability if memorizing the 3D password is more challenging or requires more time than traditional credentials. Gurary et al.~\cite{gurary2017leveraging} demonstrate that retention of 3D passwords based on action sequences is significantly higher than 2D passwords. George et al.~\cite{george2020gazeroomlock} show that multimodal approaches that combine gaze with controller-based selection reduce error rate in password entry, indicating higher memorability over unimodal approaches. Usability of a password entry mechanism depends on how familiar users are with the VR system and how comfortable they are in performing the interaction. Yu et al.~\cite{yu2016exploration} demonstrate that users found entering simple combinations of 3D passwords using the LeapMotion to be less usable than entering 2D passwords. George et al.~\cite{george2020gazeroomlock} demonstrate that using gaze in conjunction with controller selection provides the highest usability. However, more studies are needed to evaluate how users perceive usability and memorability during long-term use. Any form of password entry hampers continuous authentication, as it requires users to stop their activity to enter credentials. Long credential-entry times could prove detrimental to performance during, for instance, a high-stress examination or military routine, or hazardous to an operation such as control of a drone or a vehicle during VR-based remote teleoperation.

\subsection{Iris and Periocular Biometrics} Iris and periocular biometrics for VR has been explored by Boutros et al.~\cite{boutros2020periocular,boutros2020fusing,boutros2020benchmarking} that uses the OpenEDs eye dataset collected by Garbin et al.~\cite{garbin2019openeds} to evaluate off-the-shelf deep network architectures and hand-crafted features that have been commonly used for iris recognition in traditional environments. The OpenEDs dataset consists of iris and periocular images captured from 152 subjects using an iris-facing camera retro-fitted into a head-mounted device (HMD). Boutros et al.~\cite{boutros2020periocular} demonstrate that using the MobileNetV3~\cite{howard2019searching} network and performing transfer learning with VGG19~\cite{simonyan2014very} on cropped periocular images outperforms hand-crafted features such as tree local binary patterns and binary statistical independent features. In Boutros et al.~\cite{boutros2020fusing}, they show that fusing features separately extracted from the iris and the periocular region provides low  EER when evaluated using DeepIrisNet~\cite{gangwar2016deepirisnet}, ResNet~\cite{he2016deep}, and MobileNetV3, with MobileNetV3 providing lowest EER. In Boutros et al.~\cite{boutros2020benchmarking}, they compare DeepIrisNet, MobileNetV3, and transfer learning with ResNet and DenseNet~\cite{huang2017densely}, and find that DenseNet provides lowest EER in all their studies at 5.8\%. Additionally, the OpenEDs collection of Garbin et al.~\cite{garbin2019openeds} involves simple experiments on free viewing and looking at specific points. In a realistic scenario, iris scanners will need to capture iris images when users view and interact with arbitrary dynamic content, where the iris or periocular region may show specular reflections from the content projected by the VR display units. High specular reflections may obscure salient features for identifying a user and may negatively impact recognition performance. While the work demonstrates the applicability of iris images in enabling identification, at the current juncture it serves to be limiting, as it does not handle the variety of iris movements that happen as a person engages in a VR environment.

\subsection{Behavioral Biometrics} 
\label{subsec:behavior}Given the challenges with traditional credentials and the lack of biometric scanners embedded in VR devices, a large body of work has emerged on leveraging user behavior in VR as a biometric. Currently, user VR behavior is largely modeled by tracking the motions of the headset, hand controllers, and objects in the VR space while the user performs interactions in the VR environment. Mustafa et al.~\cite{mustafa2018unsure} provide an approach that uses support vector machines to classify users based on head movement while users listen to music on a Google Cardboard. Kupin et al.~\cite{mmm2019vr} use nearest neighbors to automatically identify users from the trajectories of the dominant hand controller as users throw a ball at a target in VR. To garner maximum benefit from the comprehensive motion of the user in the environment, most current behavioral biometrics research leverages a multimodal approach that combines features from motion tracks of the headset and controllers. Ajit et al.~\cite{ajit2019combining} use a perceptron to classify distances from position and orientation features acquired from the headset and hand controller trajectories in the input and library sessions for a user performing the ball-throwing action of Kupin et al.~\cite{mmm2019vr}. Miller et al.~\cite{miller2020within} extend the method of Ajit et al. to include velocity, angular velocity, and trigger features for performing identification using ball-throwing sessions provided within a single VR system, and using sessions spanning multiple VR systems. Pfeuffer et al.~\cite{pfeuffer2019behavioural} evaluate random forests and support vector machines on aggregate statistics drawn from unary features and pairwise relationships established amongst the headset, controllers, and target VR objects for activities such as picking, pointing, and grabbing. 

Miller et al.~\cite{miller2020personal} evaluate multiple off-the-shelf machine learning algorithms on a dataset of users watching 5 videos and performing question answering on the videos. Olade et al.~\cite{olade2020biomove} investigate nearest neighbors and support vector machines for classifying users performing dropping, grabbing, and rotating from their motion trajectories. To improve accuracy while removing reliance on hand-crafted features, more recent approaches have navigated toward using deep learning. Mathis et al.~\cite{mathis2020knowledge} use 1D convolutional neural networks (CNNs) to classify sliding window trajectory snippets from the headset and hand controllers for users using pointing interactions to select passwords on a cube. Liebers et al.~\cite{liebers2021understanding} use recurrent neural networks to classify users performing bowling and archery activities in VR. Miller et al.~\cite{miller2021using} use Siamese networks to learn cross-system relationships for improving identification and authentication when library and input data spans multiple VR systems.  

The reliability of VR behavioral biometrics depends on the consistency of user behavior in VR. Several VR datasets~\cite{olade2020biomove,mathis2020knowledge,miller2020personal} typically involve users providing data within a single session over the span of a few minutes, where behavior variability may be limited. Recent work on the temporal effect on behavioral biometrics has explored the impact of short-, medium-, and long-timescale user behavior variations on multimodal authentication~\cite{miller2022temporal}. The work reveals two concerns: (1)~authentication performance degrades when system-specific noise increases~\cite{miller2021using,miller2022combining}, and (2)~authentication improvement requires training with data from varying temporal separations. Behaviors explored in VR thus far are repeatable actions with clear spatial extents such as throwing a ball, bowling, or shooting an arrow, or action primitives such as picking or pointing. The approaches explored so far may be implementable for complex activities such as physical therapy or military drills that have necessarily repeatable routines. Our work leverages the repeatable nature of tasks in VR to forecast future user behavior based on the past behavior. Our work has a significant advantage in requiring only the initial motion behavior as the predicted behavior can be leveraged during authentication. Thus, unlike existing work, our work enables authentication with lesser data which limits the amount of time the system is vulnerable.  

\subsection{Motion Forecasting}
\label{subsec:motion}
Our work is inspired by motion forecasting in autonomous driving, where predicting the motion path and future positions of vehicles, passengers, and animals is vital to ensure hazardous situations do not arise. Waiting for complete or near complete trajectories of motion is infeasible as an autonomous vehicle would not have sufficient time to react. Yuan et al.~\cite{yuan2021agentformer} propose AgentFormer which jointly learns the time and social dimensions. Liu et al. ~\cite{liu2021multimodal} develop a multi-modal prediction framework, mmTransformer, that randomly generates trajectory proposals that are refined using contextual information. The approach of Huang et al.~\cite{huang2022multi} modifies the multi-head attention mechanism of traditional Transformers to a multi-modal attention, thereby allowing trajectory predictions to be conditioned using independent attention modes. Zhou et al.~\cite{zhou2022hivt} propose HiVT, or Hierarchical Vector Transformer, that uses local context extraction and global interaction modeling to perform agent modeling. The approach utilizes symmetries and hierarchical structure in the problem domain to avoid needing to re-normalize and recompute scene features for each target agent. For interested readers, Kong et al.~\cite{kong2022human} present a detailed survey of recent work on human action recognition and prediction.

% Transformer in behavioral biometrics: but does not do forecasting. https://arxiv.org/pdf/2206.01441.pdf

%\subsection{Transformer}
%The self-attention mechanism~\cite{vaswani2017attention} has become popular for various natural language processing tasks because it allows the model to attend to all input elements instead of relying on hand-crafted features, making it a more versatile and generalizable approach. 


% - comma comes before and after e.g.
% - write in short sentences 
% - write top down, not bottom up
% - a paragraph is three or more sentences 
% - a sentence is not more than three lines
% - the last line of a paragraph should always cover at least half of the linewidth
% - equations should read like text
% - always define abreviations
% - always define symbols
% - always write in present tense
% - always write in active voice
% - always refer to all figures and tables in the text
% - never use any of these words: 'can', 'because', 'therefore', 'but', 'nevertheless', 'nonetheless', 'moreover', 'furthermore', 'very', 'much', 'like', 'specifically', 'notably', 'in actuality'
% - never end a paragraph with an equation
% - never start a sentence with 'and', 'but', or 'this'

\section{Dataset}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=.8\linewidth]{figures/raw_trajectory_right/user06_session03.pdf}
    \caption{Right-hand controller trajectory of user 6, from day 2 session 3}
    \label{fig:raw_trajectory}
\end{figure}

One of the challenges in the VR biometrics community is the lack of a large-scale standardized longitudinal dataset consisting of multiple VR tasks, devices, and users. Miller et al.~\cite{miller2020personal} have a 511 subject dataset, however, the task is largely stationary and on a single VR device. While Pfeuffer et al.~\cite{pfeuffer2019behavioural} and Liebers et al.~\cite{liebers2021understanding} have multiple tasks, their datasets have less than 30 users and are on a single VR device. Miller et al.~\cite{miller2020within,miller2022temporal} provide a dataset for three VR devices and long-range temporal behavior for 16 subjects, however, they use a single task and the long-range behavior is for less than 30 subjects. We use the dataset of Miller et al.~\cite{miller2020within, miller2021using} consisting of 41 right-handed subjects performing a ball-throwing task using 3 VR systems as it is publicly available. The task consists of a user picking up a ball on a pedestal and throwing it at a target directly in front of them. Each user provides data using an HTC Vive, HTC Vive Cosmos, and Oculus Quest across two days separated by at least 24 hours. During each day users provide 10 sessions, for a total of 20 sessions per VR system. The physical characteristics and locations of the ball, target, and pedestal remain constant throughout the capture procedure across each trial and session. The dataset consists of $x$, $y$, and $z$ position and orientation values as Euler angle rotations around $x$, $y$, and $z$ axes for the headset and hand controllers, as well as trigger pressure for the controllers. The trigger pressure represents the amount of force applied to the trigger on the controller. For this paper, we only use data from the HTC consisting of the right-hand controller trajectory position, as shown in Figure~\ref{fig:raw_trajectory}, and trigger pressure. 


% \begin{figure}[tbp]
%     \centering
%     \begin{subfigure}[b]{\linewidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/raw_trajectory_right/user00_session01.pdf}
%          \caption{Subject ID 0, Day 1, Session 1}
%          \label{fig:y equals x}
%      \end{subfigure}
     % \hfill
     % \begin{subfigure}[b]{0.2\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/raw_trajectory_right/user06_session03.pdf}
     %     \caption{Subject ID 6, Day 1, Session 3}
     %     \label{fig:y equals x}
     % \end{subfigure}
     % \hfill
     % \begin{subfigure}[b]{0.2\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/raw_trajectory_right/user12_session06.pdf}
     %     \caption{Subject ID 12, Day 1, Session 6}
     %     \label{fig:y equals x}
     % \end{subfigure}
     % \hfill
     % \begin{subfigure}[b]{0.2\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/raw_trajectory_right/user31_session09.pdf}
     %     \caption{Subject ID 31, Day 1, Session 9}
     %     \label{fig:y equals x}
     % \end{subfigure}
     
%      \caption{Trajectories for the right-hand controller from subjects 0, 6, 12, and 31 with sessions 1, 3, 6, and 9 from Miller et al. dataset. }
%      \label{fig:raw_trajectory}
% \end{figure}


\subsection{Data Preparation}

We extract data over each session for each subject by sliding a window over the session data. We denote a session as $s^{u}_{i}$, where $u$ refers to the user id, and $i$ refers to the session number. Each session $s^{u}_{i}$ is a matrix of real numbers of size $T \times f$, where $T$ refers to the total number of timestamps and $f$ refers to the number of features. For each session we apply a sliding window of size $n \times f$ and stride $l$ to $s^{u}_{i}$ along the temporal dimension to extract time-varying chunks of the session data. %We vary $n$ from $25$ to $95$ with a step size of $5$ and we use $l=1$.

%We apply a sliding window technique to each session for all subjects. We take each session $s_{ui} \in \mathbb{R}^{T \times f}$ from the set of all sessions from all subjects, denoted by $\mathbb{S}$, where $u$ is the subject ID ranging from 1 to 41, $i$ represents the original session ID from 1 to 10, and $T$ and $f$ refer to the total number of timestamps and the number of features of an arbitrary session's data. We then divide $s_{ui}$ into a list of sub-sessions with a specified window size $n \times f$ by moving along with the timestamp direction with stride $l$, where each sub-session $s_{ui}^j \in \mathbb{R}^{n \times f}$ contains data of the same feature size as $s_{ui}$, but at a different time period. Here, $j$ denotes the ID of each sub-session in $\mathbb{S}_{ui}^{sub}$, ranging from 1 to $\frac{T-n}{l}$. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/wins.pdf}
    \caption{Left: To create the training set for authentication, we evenly sample sliding windows of size $n$ from day 1 trajectories of the genuine user. To create the impostor set, for each genuine sliding window, we randomly sample a subject and day 1 trajectory from the remaining users, and select a window from the trajectory sample at the same temporal location as the genuine sliding window. Right: we repeat the process with day 2 trajectories to create the test set, ensuring that the random ordering of subjects/sessions is different.
    }
    \label{fig:datageneration}
\end{figure}

\subsection{Impostor Data Generation}

Each session in the dataset utilized for this study represents authentic data from the subjects under investigation. However, to enable the network to learn effective identification and authentication capabilities, it is necessary to incorporate impostor data into the training process. Rather than generating arbitrary data, 
%\hl{we obtain each piece of impostor data by extracting the same length period data with the same start point of time as that in the corresponding genuine data from a random session belonging to randomly selected users.} ---> 
we obtain impostor data by extracting from other users selected at random, and each piece of impostor data has the same start and end point of time as that in the corresponding genuine data. as shown in Figure~\ref{fig:datageneration}. The random selection allows us to diversely represent the patterns and behaviors of an actual adversary, while still being independent from the genuine data of the current user. To ensure a fair comparison between the genuine and impostor data, we start the impostor data at the same timestamp as the genuine data for the current user, and make sure that the length of the impostor data matches that of the genuine data, so that the two types of data have the same temporal alignment. Overall, by using this approach to extract impostor data, we build a more realistic and balanced dataset for neural network training.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline3.pdf}
    \caption{Pipeline flowchart of our proposed approach. In the first step, the input data is processed using the sliding window technique to generate sub-sequences. These sub-sequences are then fed into the forecasting model, which generates the predicted sequence. The predicted sequence is then concatenated with the original input data to form a combined sequence. Finally, the combined sequence is fed into the classifier for authentication. $135$, $10$, and $4$  represent the total timestamps in raw data, number of sessions, and number of features for each session, respectively.}
    \label{fig:pipeline}
\end{figure*}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=.45\textwidth]{figures/models2.pdf}
    \caption{We use (a) an FCN and (b) a Transformer Encoder as baselines for authentication. (c) We use a modified Transformer for forecasting.}
    \label{fig:models}
\end{figure}

\section{Motion Forecasting}

Our proposed method for authentication is based on a novel approach that leverages time series data to predict future behavior trajectories. As shown in Figure~\ref{fig:pipeline}, our method involves breaking down the input time series data into segments, with each segment containing a fixed number of timestamps. For each segment, a Transformer-based model, as shown in Figure~\ref{fig:models}(c), is trained to forecast the subsequent time behavior trajectory. The forecasted output is then combined with the real input data, resulting in semi-synthetic complete data. The concatenated data is then input into a classifier as shown in Figure~\ref{fig:models}(a) and Figure~\ref{fig:models}(b) for authentication. 

\subsection{Feature Representation}
We use learned embeddings that map each timestamp's data to a higher-dimensional space of size $d_{model}$, to extract information from the input data,  which is originally in a 4-dimensional space ($x$, $y$, $z$ coordinates and trigger pressure measurement). We use the same approach as Vaswani et al.~\cite{vaswani2017attention} to preserve positional information of the input sequence. We encode the position information of each timestamp data using sine and cosine functions
\begin{equation}\label{pos1}
    PE(t, 2i) = \sin\left( {t} / \left({10000^{2i / d_{model}}}\right)\right)~\textrm{and}
\end{equation}
\begin{equation}\label{pos2}
    PE(t, 2i+1) = \cos\left({t} / \left({10000^{2i / d_{model}}}\right)\right),
\end{equation}
where $t$ is the timestamp and $i$ is the dimension. With this positional encoding, our model learns to distinguish and relate temporal information based on their positions.

Using the approach of Zhou et al.~\cite{zhou2021informer}, which encodes long-range time attributes such as year, month, week, and day to scalars, we define the function $TE(t)$ as
\begin{equation}\label{pos3}
    TE(t) = t / T - 0.5,
\end{equation}
to encode the short-range time data represented in milliseconds to a scalar in the range of -0.5 to 0.5. The value $t$ represents the timestamp and $T$ is the total number of timestamps. 

The value $d_{model}$ also represents the dimension of the output of positional and temporal encoding. We sum the learned input embeddings, positional encodings, and time encodings together, enabling us to represent the input data in a high-dimensional space that preserves positional and temporal relationships between timestamps.

\subsection{Encoder} 
Our encoder consists of multiple encoder layers, where each encoder layer is composed of a multi-head attention sub-layer, a position-wise fully connected feed-forward sub-layer, residual connection operation~\cite{he2016deep}, and layer normalization~\cite{ba2016layer} as shown in Figure~\ref{fig:models}(c). The multi-head attention sub-layer enables parallel computations in $n_{head}$ scaled single-head dot product self-attentions, with each self-attention focusing on different parts of the input sequence. The multi-head attention allows our model to capture more complex relationships between the input elements. As defined in the original Transformer paper~\cite{vaswani2017attention}, each single-head dot product attention unit computes a weighted sum of the values $V$ of the input sequence, as 
\begin{equation}\label{attn}
    Attention(Q, K, V) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d_K}}\right)V,
\end{equation}
where the weights are determined by the similarity of the query vector $Q$ and the key vector $K$ of each element, which is then scaled by the square root of the dimensionality of the key vector $d_K$ to ensure that the attention scores are not too large. We apply a softmax function to obtain a probability distribution over the weighted sum.

The residual connection operation~\cite{he2016deep} adds the output of the multi-head attention sub-layer to the original input to smooth the gradient flow during training and to facilitate learning of deeper representations. The position-wise dense feed-forward sub-layer applies a fully connected neural network to each element of the sequence independently. The fully connected sub-layer has an input and output dimension of $d_{model}$, and a hidden layer of dimension $d_{hidden}$. We perform layer normalization~\cite{ba2016layer} after each residual connection. In this work, we employ a stack of two identical encoder layers.

\subsection{Decoder}

We extract a subset with length $l_{overlap}$ from the input sequence of the encoder, as shown in Figure~\ref{fig:inout} in green. We initialize the region to be predicted, shown in red in Figure~\ref{fig:inout}, with zeros. We concatenate the encoder subset in green with the initialization in red to form the input to the decoder. The decoder inherits the learned patterns from the encoder. We apply input embedding, positional encoding, and temporal encoding to the decoder input as well to convert the input to a higher dimensional space. Similar to the traditional transformer decoder, we incorporate a masked self-attention sub-layer to correlate each element in the decoder input sequence and a masked cross-attention sub-layer to correlate the decoder input with the encoder output. The standard Transformer decoder~\cite{vaswani2017attention} operates on a one-step prediction basis, outputting the prediction result element by element. Their approach is not suitable for our goal of generating prediction results of multiple future timestamps at once. To address this issue, we use a fully connected feed-forward sub-layer at the end of the decoder, so that our model outputs forecasting results of an arbitrary length of timestamps, $l_{forecasting}$, at a time. Similar to the operations for the encoder, we perform residual connection~\cite{he2016deep} and layer normalization~\cite{ba2016layer} after each sub-layer.

\begin{figure}[t]
   \centering
   \includegraphics[width=\linewidth]{figures/inout2.pdf}
   % \includegraphics[width=.45\textwidth]{figures/inout2.pdf}
   \caption{Forecasting Model I/O. The input to the Encoder consists of the initial sequence (in gray) and the overlap sequence (in green), and the Decoder input consists of the overlap sequence (in green) and the sequence to be forecasted initialized with zeros (in red). }
    \label{fig:inout}
\end{figure}

\setlength{\tabcolsep}{5.6pt}
\begin{table*}[tb]
  \caption{Equal Error Rate of Baseline Experiment. The abbreviation `WS' refers to the window size and the subsequent numbers in the same row denote the values of window size, and the last column is the average value of each row. `FCN' stands for Fully Convolutional Networks~\cite{wang2017time}, `TF' represents the Transformer encoder~\cite{vaswani2017attention}, `EER' represents the equal error rate (where lower values are preferable). Each row is the average of all $41$ subjects under the corresponding column.}
  \label{tab:exp1}
  \scriptsize%
  \centering%
  \begin{tabu}{c|ccccccccccccccc | c
  	  % c| *{15}{c} %
  	  % 	*{7}{c}%
  	  % 	*{2}{c}%
  	}
  	\toprule
  	WS & 25 & 30 & 35 & 40 & 45 & 50 & 55 & 60 & 65 & 70 & 75 & 80 & 85 & 90 & 95 & Mean\\
  	% WS & Metric & 25 & 30 & 35 & 40 & 45 & 50 & 55 & 60 & 65 & 70 & 75 & 80 & 85 & 90 & 95\\
  	\hline \hline
        % \multirow{2}{*}{FCN~\cite{wang2017time}} & ACC & 87.87 & 89.05 & 89.85 & 90.91 & 91.78 & 91.96 & 91.77 & 93.02 & 92.47 & 93.93 & 94.4 & 93.58 & 92.87 & \textbf{95.23} & 93.4 \\
        % & EER & 0.121 & 0.110 & 0.101 & 0.091 & 0.082 & 0.080 & 0.082 & 0.061 & 0.075 & 0.060 & 0.062 & 0.064 & 0.071 & \textbf{0.048} & 0.066\\
        
  	% FCN~\cite{wang2017time} (ACC) & 87.87 & 89.05 & 89.85 & 90.91 & 91.78 & 91.96 & 91.77 & 93.02 & 92.47 & 93.93 & 94.4 & 93.58 & 92.87 & \textbf{95.23} & 93.4 \\
   % \midrule
    FCN~\cite{wang2017time} & 0.121 & 0.109 & 0.101 & 0.091 & 0.082 & 0.080 & 0.082 & 0.061 & 0.075 & 0.061 & 0.062 & 0.064 & 0.071 & \textbf{0.048} & 0.066 & 0.078\\

    % \multirow{2}{*}{TF~\cite{vaswani2017attention}} & ACC & 88.54 & 89.65 & 90.32 & 91.11 & 91.71 & 92.31 & 92.76 & 93.5 & 93.65 & 94.16 & \textbf{94.35} & 93.73 & 93.66 & 93.81 & 93.88 \\
    % & EER & 0.114 & 0.104 & 0.097 & 0.089 & 0.083 & 0.060 & 0.072 & 0.065 & 0.073 & 0.058 & \textbf{0.055} & 0.062 & 0.064 & 0.061 & 0.061\\
  	% TF~\cite{vaswani2017attention} (ACC)  & 88.54 & 89.65 & 90.32 & 91.11 & 91.71 & 92.31 & 92.76 & 93.5 & 93.65 & 94.16 & \textbf{94.35} & 93.73 & 93.66 & 93.81 & 93.88 \\
   % \midrule
        TF~\cite{vaswani2017attention} & 0.115 & 0.104 & 0.097 & 0.089 & 0.083 & 0.077 & 0.072 & 0.065 & 0.064 & 0.058 & \textbf{0.057} & 0.063 & 0.064 & 0.062 & 0.061 & \textbf{0.075}\\
  	\bottomrule
  \end{tabu}%
\end{table*}

\section{Authentication}
We utilize two baseline models for authentication as shown in Figure~\ref{fig:models}(a) and Figure~\ref{fig:models}(b), namely a Fully Convolutional Network (FCN)~\cite{wang2017time} and a Transformer encoder~\cite{vaswani2017attention}. We obtain the genuine data from the current subject using the sliding window technique with a window size of $n \times f$, where $n$ represents the window size and $f$ denotes the number of features, i.e., $x$, $y$, and $z$ positions of the right-hand controller and trigger pressure. We randomly select imposter data from the remaining subjects and each piece of the imposter data consists of the same timestamp data as the genuine data. We evaluate the performance of the trained models on a set of previously unseen data after each training epoch. 

\subsection{Fully Convolutional Network} 
We use the FCN architecture in Wang et al.~\cite{wang2017time} which consists of three convolutional blocks, each containing a convolutional layer and a 1D kernel. To enhance convergence and improve generalization, batch normalization layers~\cite{ioffe2015batch} are applied after each convolutional layer, followed by ReLU activation layers at the end of each block. A global average pooling layer~\cite{lin2013network} is employed after these three blocks, and a softmax layer provides the final output as shown in Figure~\ref{fig:models}(a). 

\subsection{Transformer Encoder}
Though FCNs have shown success in time series classification, they lack strength of attention networks in relating different portions of the trajectories. To capture intra-trajectory relationships in authentication, we evaluate a second network that uses the encoder of the Transformer architecture~\cite{vaswani2017attention} to perform authentication as shown in Figure~\ref{fig:models}(b). The Transformer encoder has the ability to capture global correlations between each element in an input sequence by the multi-head self-attention mechanism~\cite{vaswani2017attention}, which is an important characteristic for analyzing time series data. We employ the encoder only owing to its ability to extract meaningful features from the input sequence rather than generating a list of output elements. We eliminate the temporal encoding used for the forecasting Transformer.

\subsection{Loss Functions}
During training, we perform optimizations over the entire model parameters by minimizing the loss 
\begin{align}\label{LOSS}
    \mathtt{L} = \mathtt{L}_{L} + \lambda_{F}  \mathtt{L}_{F} + \lambda_{T}  \mathtt{L}_{T},
\end{align}
where 
\begin{equation}\label{LF}
    \mathtt{L}_{F} = (1/|W|) \Sigma_{w \in W} MSE(Tra_{pred}, Tra_{gt}).
\end{equation}
 $\mathtt{L}_{F}$ measures the discrepancy between the predicted right-hand controller trajectory and the corresponding ground truth trajectory. In Equation~\ref {LF}, $MSE$ is the mean squared error loss function, $Tra_{pred}$ and $Tra_{gt}$ are the forecasted trajectory and ground truth trajectory respectively. $|W|$ denotes the total number of windows while $w$ stands for a particular window of the whole window set $W$. We define
\begin{equation}
     \mathtt{L}_{T} = (1/|w|) \Sigma_{t \in w} BCE(Tri_{pred}, Tri_{gt}), \textrm{ and}
     \label{eq:bcetri}
\end{equation}
\begin{equation}
    \mathtt{L}_{L} = (1/|W|) \Sigma_{w \in W} BCE(Label_{pred}, Label_{gt}),
    \label{eq:bcelab}
\end{equation}
where BCE is the binary cross-entropy loss function. Equation~\ref{eq:bcetri} provides BCE for trigger pressure, $Tri$, and Equation~\ref{eq:bcelab} for predicted authentication label, $Label$. The value $t$ refers to a specific timestamp in the window $w$, and subscripts $pred$ and $gt$ stand for predictions and ground truth. We use the notation $\lambda_{F}$ and $\lambda_{T}$ in Equation~\eqref{LOSS} to denote the weights for the loss terms $\mathtt{L}_{F}$ and $\mathtt{L}_{T}$. We use Adam~\cite{kingma2014adam} to optimize the parameters of the network.

\section{Experimental Results}
We use the day 1 data of $41$ subjects in the Miller et al.~\cite{miller2020within,miller2021using} dataset for training the network, and day 2 data for evaluating the network's performance. In our `Baseline Experiment', we train the FCN and Transformer encoder to predict the classification label of the input data directly. In `Authentication with Forecasting Experiment', we use our proposed approach to forecast trajectory data and then combine it with the input data before performing classification. We evaluate our approach by computing the \textbf{equal error rate (EER)}. The EER indicates the point at which the false acceptance rate is equal to the false rejection rate, the lower the EER value, the better performance of the model. 

\subsection{Baseline Experiment: No Forecasting}
We vary the size of the sliding window, $l_{window}$, from $25$ to $95$ with a step size of $5$.
%\hl{delete, since may cause confusion, we don't split $l_{window}$ here} ---> \hl{where $l_{window} = l_{initial} + l_{overlap}$, as shown in Figure}~\ref{fig:inout}. 
In this experiment, we only compute the BCE loss using Equation~\ref{eq:bcelab}. We set the following parameters for the FCN and Transformer encoder:
\begin{itemize}
\item \textbf{FCN:} We use three convolutional blocks, each of them contains a convolutional layer with a filter size of \{$128$, $256$, $128$\} and a 1D kernel size of \{$8$, $5$, $3$\}, respectively. We use Adam~\cite{kingma2014adam} as the optimizer with a learning rate of $0.001$.
\item \textbf{Transformer Encoder:} We perform input embedding and positional encoding to the input sequence that projects the input data from its original dimension to $d_{model} = 512$. We employ a stack of two encoder layers, which are identical in structure, to process the input data in the classification task. Each encoder layer has a $n_{head} = 8$ multi-head attention sub-layer in it. Lengths of query, key, and value vectors for all the heads are $d_q = d_k = d_v = 64$. We use the Adam~\cite{kingma2014adam} optimizer with a learning rate of $0.0001$.
\end{itemize}

% \begin{table*}[t!]
% \setlength{\tabcolsep}{5.6pt}
% \begin{table*}[tb]
%   \caption{Equal Error Rate of Baseline Experiment. The abbreviation `WS' refers to the window size and the subsequent numbers in the same row denote the values of window size, and the last column is the average value of each row. `FCN' stands for Fully Convolutional Networks~\cite{wang2017time}, `TF' represents the Transformer encoder~\cite{vaswani2017attention}, `EER' represents the equal error rate (where lower values are preferable). Each row is the average of all $41$ subjects under the corresponding column.}
%   \label{tab:exp1}
%   \scriptsize%
%   \centering%
%   \begin{tabu}{c|ccccccccccccccc | c
%   	  % c| *{15}{c} %
%   	  % 	*{7}{c}%
%   	  % 	*{2}{c}%
%   	}
%   	\toprule
%   	WS & 25 & 30 & 35 & 40 & 45 & 50 & 55 & 60 & 65 & 70 & 75 & 80 & 85 & 90 & 95 & Mean\\
%   	% WS & Metric & 25 & 30 & 35 & 40 & 45 & 50 & 55 & 60 & 65 & 70 & 75 & 80 & 85 & 90 & 95\\
%   	\hline \hline
%         % \multirow{2}{*}{FCN~\cite{wang2017time}} & ACC & 87.87 & 89.05 & 89.85 & 90.91 & 91.78 & 91.96 & 91.77 & 93.02 & 92.47 & 93.93 & 94.4 & 93.58 & 92.87 & \textbf{95.23} & 93.4 \\
%         % & EER & 0.121 & 0.110 & 0.101 & 0.091 & 0.082 & 0.080 & 0.082 & 0.061 & 0.075 & 0.060 & 0.062 & 0.064 & 0.071 & \textbf{0.048} & 0.066\\
        
%   	% FCN~\cite{wang2017time} (ACC) & 87.87 & 89.05 & 89.85 & 90.91 & 91.78 & 91.96 & 91.77 & 93.02 & 92.47 & 93.93 & 94.4 & 93.58 & 92.87 & \textbf{95.23} & 93.4 \\
%    % \midrule
%     FCN~\cite{wang2017time} & 0.121 & 0.109 & 0.101 & 0.091 & 0.082 & 0.080 & 0.082 & 0.061 & 0.075 & 0.061 & 0.062 & 0.064 & 0.071 & \textbf{0.048} & 0.066 & 0.078\\

%     % \multirow{2}{*}{TF~\cite{vaswani2017attention}} & ACC & 88.54 & 89.65 & 90.32 & 91.11 & 91.71 & 92.31 & 92.76 & 93.5 & 93.65 & 94.16 & \textbf{94.35} & 93.73 & 93.66 & 93.81 & 93.88 \\
%     % & EER & 0.114 & 0.104 & 0.097 & 0.089 & 0.083 & 0.060 & 0.072 & 0.065 & 0.073 & 0.058 & \textbf{0.055} & 0.062 & 0.064 & 0.061 & 0.061\\
%   	% TF~\cite{vaswani2017attention} (ACC)  & 88.54 & 89.65 & 90.32 & 91.11 & 91.71 & 92.31 & 92.76 & 93.5 & 93.65 & 94.16 & \textbf{94.35} & 93.73 & 93.66 & 93.81 & 93.88 \\
%    % \midrule
%         TF~\cite{vaswani2017attention} & 0.115 & 0.104 & 0.097 & 0.089 & 0.083 & 0.077 & 0.072 & 0.065 & 0.064 & 0.058 & \textbf{0.057} & 0.063 & 0.064 & 0.062 & 0.061 & \textbf{0.075}\\
%   	\bottomrule
%   \end{tabu}%
% \end{table*}

Table~\ref{tab:exp1} summarizes the results of the Baseline Experiment, where the abbreviation `WS' in the first line refers to window size, and the subsequent numbers denote the specific values of window size we employed. The acronyms used in this table are as follows: `FCN' stands for Fully Convolutional Network~\cite{wang2017time}, `TF' represents the Transformer encoder~\cite{vaswani2017attention}, and `EER' represents the equal error rate (where lower values are preferable). Each row of Table~\ref{tab:exp1} represents the average testing EER of all $41$ subjects under the corresponding column.

Values from Table~\ref{tab:exp1} reveal that the EER of the two models exhibits a similar trend, decreasing with an increase in window size. We observe that the overall performance of TF is better than that of FCN. For most window sizes, TF provides lower EER values, except for window sizes $45$, $60$, and $90$. However, the lowest EER among all window sizes is achieved by FCN at window size $90$. We also find that for each of the models, FCN performs best when WS $=90$, whereas the Transformer encoder performs best when WS $=75$. We conclude from the last column in Table~\ref{tab:exp1} that the Transformer encoder outperforms the FCN model. It also demonstrates that the performance of the models is influenced by the window size, indicating that the choice of window size plays a crucial role in determining the effectiveness of the models.

\subsection{Authentication with Forecasting Experiment}
We aim to predict a sequence of data with a length of $l_{forecasting}$ based upon data within a window of length $l_{window}$, where $l_{window}$ is determined as the sum of the initial length $l_{initial}$ and the length of the overlapping data $l_{overlap}$ as shown in Figure~\ref{fig:inout}. To ensure the robustness of our results, we investigate various combinations of $l_{window}$ and $l_{forecasting}$. We vary $l_{window}$ from $25$ to $85$ at a step size of $10$, and $l_{forecasting}$ from $10$ to $70$ with a step size of $10$. We choose to terminate the sliding window process at a window size of 85, as 85 exceeds more than half of the original data length of 135 timestamps, and our goal is to evaluate the performance of using a reduced subset of data for authentication. We conduct multiple trials with varying $l_{overlap}$ sizes, from $5$ to $l_{window} - 5$ with stride $5$ for each set of fixed $l_{window}$ and $l_{forecasting}$ pairs, to investigate whether the length of the overlap area,  $l_{overlap}$, has an impact on the accuracy of the predicted trajectory. For the forecasting model, we set the following parameters:

\begin{itemize}
    \item \textbf{Transformer-based Forecasting:} We use $3$ encoder layers and $1$ decoder layer. The dimension of this model is $d_{model} = 512$, with a total of $n_{head} = 8$ attention heads for each layer. The query, key, and value dimensions are set to $d_q = d_k = d_v = 64$. We employ a fully connected layer with dimension $d_{hidden} = 2048$ for the model. We use the Adam~\cite{kingma2014adam} optimizer with a learning rate of $0.0001$.
\end{itemize}

\begin{figure*}[tbhp]
    \centering
    \includegraphics[width=.95\linewidth]{figures/forecasting_vis/forecast_vis_3in1_v5_update_P.pdf}
    \caption{Forecasting Visual Result. Gray dot lines are the input trajectory, blue dot lines are the ground truth trajectories to be foreca, and the red dot lines are the forecasted trajectories. We show the results of user $0$, $3$, $6$, $15$, $25$, and $28$ in each column and eight pairs of input length and forecasting length for eight rows. Numbers in the leftmost of each row represent input length - forecasting length.}
    \label{fig:forecasting_vis}
\end{figure*}

In Figure~\ref{fig:forecasting_vis} we show the qualitative results for the forecasted trajectories. We use gray dotted lines to represent the input trajectories, blue dotted lines for ground truth trajectories to be forecasted, and red dotted lines are the forecasted trajectories. We show results from randomly selected users $0$, $3$, $6$, $15$, $25$, and $28$ in each column. The leftmost numbers in each row mean lengths of the input, $l_{window}$, and the forecasting sequence, $l_{forecast}$.

From Figure~\ref{fig:forecasting_vis}, we observe that certain forecasted trajectories display a remarkable level of similarity with their respective ground truth trajectories, e.g., the results of user 6 (column 3). However, for trajectories that exhibit less correspondence with the ground truth in terms of detail, there are still similarities in the overall movement trends, e.g., user 15, 25, and 28 with 35 timestamps to forecast 60 future timestamps (row 3). We also see that for trajectories that are relatively smooth and regular, the forecasting trajectories are more consistent with the ground truth trajectories, as exemplified by the results for user 0 (column 1) and user 6 (column 3). On the other hand, for trajectories that are relatively irregular, there are discrepancies in finer details between the forecasted and ground truth trajectories, as observed in the results for user 3 (column 2), user 15 (column 4), user 25 (column 5), and user 28 (column 6). Overall, our trained model captures the distinct patterns of user behavior, thus enabling robust future behavior prediction.

% \setlength{\tabcolsep}{5.1pt}
\begin{table}[tb]
  \caption{Forecasting Trajectories MSE Scores. `WS' refers to the window size, and `+x' means the length of forecasted sequence is x.}
  \label{tab:fore_mes}
  \scriptsize%
  \centering%
  \begin{tabu}{
  	  % *{8}{c}%
             c| *{8}{c}
            *{9}{c}%
  	}
  	\toprule
  	WS & +10 & +20 & +30 & +40 & +50 & +60 & +70 \\
  	\midrule
        \midrule
  	25 & \textbf{0.204} & 0.275 & 0.318 & 0.344 & 0.375 & 0.386 & 0.405\\
        35 & \textbf{0.216} & 0.290 & 0.322 & 0.357 & 0.372 & 0.394 & --\\
  	45 & \textbf{0.215} & 0.287 & 0.332 & 0.357 & 0.380 & --    & --\\
        55 & \textbf{0.202} & 0.283 & 0.327 & 0.357 & --    & --    & --\\
        65 & \textbf{0.209} & 0.286 & 0.330 & --    & --    & --    & --\\
        75 & \textbf{0.212} & 0.291 & --    & --    & --    & --    & --\\
        85 & \textbf{0.215} & --    & --    & --    & --    & --    & --\\
  	\bottomrule
  \end{tabu}%
\end{table}

We show the quantitative results of the forecasting trajectories in Table~\ref{tab:fore_mes} using the Mean Squared Error (MSE) between the ground truth trajectories and the forecasted trajectories as the evaluation metric. In the table, we use `WS' to denote the window size, and `+x' to represent the length of forecasted sequence. For instance, the window size of 25 and `+20' means the input sequence consists of 25 timestamps and forecasts future 20 timestamps. From Table~\ref{tab:fore_mes}, we see a distinct trend where the MSE is directly proportional to the length of the forecasting sequence for a fixed window size, i.e., as the length of the forecasting sequence increases, the MSE also increases. However, when predicting sequences of the same length, we observe a weak linear trend between the window size and the MSE scores in Table~\ref{tab:fore_mes}, in other words, the MSE slightly increases as the window size increases, which suggests that smaller input windows are more likely to result in more precise forecasting when predicting a fixed-length sequence.

\begin{figure}[tbhp]
    \centering
    \includegraphics[width=.9\linewidth]{figures/overlap_compare.pdf}
    \caption{MSE scores of six fixed-length pairs of input and forecasting sequences with varying overlap length. All pairs (dotted lines) share the same $x$ and $y$ axis. Input window sizes are $25$, $35$, $45$, $65$, $75$, and $85$, and forecasting lengths are $20$, $60$, $40$, $30$, $20$, and $10$, corresponding to each line. Lengths of overlap data range from 5 to $20$, $30$, $40$, $60$, $70$, and $80$, respectively, with the same step size of $5$.}
    \label{fig:overlap_compare}
\end{figure}

We conduct multiple trials by varying $l_{overlap}$, and see no evidence that the length of overlap data affects the accuracy of forecasting output trajectory. Figure~\ref{fig:overlap_compare} displays six pairs experimental results of $l_{window}$ and $l_{forecast}$, where $l_{window}$ takes on the values $25$, $35$, $45$, $65$, $75$, and $85$, and $l_{forecast}$ takes on the values $20$, $60$, $40$, $30$, $20$, and $10$, respectively corresponding to each lines in Figure~\ref{fig:overlap_compare}. For each pair of fixed $l_{window}$ and $l_{forecast}$ (each line in the figure), $l_{overlap}$ varies from $5$ to $l_{window}-5$ with stride $5$. We do not observe any trend indicating that $l_{overlap}$ significantly affects the forecasting accuracy in terms of MSE, As a result, we use the median of $l_{overlap}$ for each pair of $l_{window}$ and $l_{forecast}$ across the entire experiment.

\subsection{Authentication After Forecasting Results}
\setlength{\tabcolsep}{5.1pt}
\begin{table}[tb]
  \caption{EER of FCN as a Classifier with Forecasted Trajectory. `+x' means the length of forecasted sequence is x. `+0' means with no forecasting}
  \label{tab:exp3fcn}
  \scriptsize%
  \centering%
  \begin{tabu}{
  	  % *{8}{c}%
             c| *{8}{c}
            *{9}{c}%
  	}
  	\toprule
  	WS & +0 & +10 & +20 & +30 & +40 & +50 & +60 & +70 \\
  	\midrule
        \midrule
  	25 & 0.121 & 0.099 & 0.093 & 0.089 & 0.084 & \textbf{0.082} & 0.086 & 0.083\\
        35 & 0.101 & 0.085 & 0.082 & 0.077 & 0.072 & \textbf{0.067} & 0.073 & --\\
  	45 & 0.082 & 0.079 & 0.070 & 0.069 & \textbf{0.061} & 0.063 & --    & --\\
        55 & 0.082 & 0.068 & 0.063 & 0.057 & \textbf{0.055} & --    & --    & --\\
        65 & 0.075 & 0.063 & 0.058 & \textbf{0.052} & --    & --    & --    & --\\
        75 & 0.062 & 0.060 & \textbf{0.059} & --    & --    & --    & --    & --\\
        85 & 0.071 & \textbf{0.066}  & --    & --    & --    & --    & --    & --\\
  	\bottomrule
  \end{tabu}%
\end{table}

\begin{table}[tb]
  \caption{EER of Transformer Encoder as a Classifier with Forecasted Trajectory. `+x' means the length of forecasted sequence is x. `+0' means with no forecasting}
  \label{tab:exp3tf}
  \scriptsize%
  \centering%
  \begin{tabu}{%
  	  % *{10}c%
            c| *{8}{c}
  	  	*{9}{c}%
  	  	%*{2}{c}%
  	}
  	\toprule
  	WS & +0 & +10 & +20 & +30 & +40 & +50 & +60 & +70 \\
  	\midrule
        \midrule
  	25 & 0.115 & 0.097 & 0.091 & 0.086 & \textbf{0.080} & 0.081 & 0.081 & 0.084\\
        35 & 0.097 & 0.080 & 0.075 & 0.070 & 0.068 & \textbf{0.064} & 0.065 & --\\
  	45 & 0.083 & 0.069 & 0.064 & 0.061 & 0.054 & \textbf{0.053} & --    & --\\
        55 & 0.072 & 0.062 & 0.057 & 0.054 & \textbf{0.049} & --    & --    & --\\
        65 & 0.064 & 0.057 & 0.053 & \textbf{0.048} & --    & --    & --    & --\\
        75 & 0.057 & 0.055 & \textbf{0.051} & --    & --    & --    & --    & --\\
        85 & 0.064 & \textbf{0.055} & --    & --    & --    & --    & --    & --\\
  	\bottomrule
  \end{tabu}%
\end{table}

In Table~\ref{tab:exp3fcn} and Table~\ref{tab:exp3tf}, we summarize the quantitative authentication results using EER, where `WS' and `+x' are the same as those in Table~\ref{tab:fore_mes} and stand for the window size and the length of forecasted sequence. We use `+0' to represent no forecasting, i.e., the EER scores in the `+0' column are directly from Table~\ref{tab:exp1}. We compare the authentication performance between models with forecasting and the baseline models without forecasting by calculating the EER reduction. We compute the EER reduction by subtracting the lowest EER score obtained from the results with forecasting sequences from the baseline EER score, then we divide the difference by the baseline EER score, giving us a percentage that represents the degree on improve authentication performance. We observe, from the EER reduction, that authentication using the forecasted trajectory outperforms the baseline for all window sizes we used in forecasting, as shown in Table~\ref{tab:exp3fcn} and Table~\ref{tab:exp3tf}, the lowest EER scores do not appear in the `+0' column, but in the columns with forecasted sequences.
Results from Table~\ref{tab:exp3fcn} show an average EER reduction of $23.85\%$ when using FCN as the classifier, with the largest decrease of $33.66\%$ achieved when forecasting $50$ timestamps with $35$ input timestamps of data. 
Similarly, from Table~\ref{tab:exp3tf} we observe that the average EER reduction with the Transformer encoder as the classifier is $26.02\%$, and we achieve the highest reduction of $36.14\%$ when forecasting $50$ future timestamps using an input with 45 timestamps. The overall observations suggest that our approach of forecasting future behavior enables early authentication, leading to more secure systems.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/attn_map3.pdf}
    \caption{Attention maps of layer 0 from user 6, using 55 timestamps to forecast 40 timestamps.}
    \label{fig:attn_map}
\end{figure}

\section{Discussion}
In this paper, we present the first approach that uses motion forecasting for behavioral biometrics in VR. We use a Transformer-based model to forecast motion trajectories given an initial trajectory of a user performing an action in VR. We merge the initial and forecasted trajectory and perform authentication. We compare the performance of two classifiers, a Transformer encoder and FCN, and demonstrate the effectiveness of our approach using the 41-subject ball-throwing dataset of Miller et al.~\cite{miller2022combining,miller2022temporal}. We show that our approach can provide an average EER reduction of 23.85\% with a maximum of 33.66\% when using the FCN as the classifier, and an average reduction of 26.02\% and a maximum of 36.14\% using the Transformer encoder. In all instances, our forecasting approach outperforms the baseline that does not use forecasting. Thus, our approach enables VR applications to be designed with early detection systems that do not need to rely on near complete trajectories to determine if the user is genuine or an impostor. 

Our current approach uses a ball-throwing task, which has a starting point, i.e. lifting the ball from the pedestal, and an end goal, i.e. attempting to hit the target, with little variability in the intermediary steps. However, critical VR applications may have intermediary steps with high degrees of variability within and across users. For example, in a virtual banking application where a user enters a bank to deposit a check, we can have different intermediary steps between the starting point, i.e. the user opening the door, to the ending goal, i.e. depositing the check. In one session after opening the door and before depositing the check a user may speak to a virtual teller or in another session look at the newest CD rates. These differences in intermediary steps may vary for the same user from session to session, for example, a user looking at the new CD rates at the start of a month. The intermediary steps may also vary between users, where one user may always speak to a virtual teller before depositing a check while another user directly deposits the check.  While it may seem that the variable intermediary steps can make motion forecasting challenging, they are no different from the unpredictable behavior of pedestrians and other agents in autonomous driving~\cite{liang2020learning,zeng2021lanercnn,zhou2022hivt,huang2022multi,li2020end,kong2022human,liu2021multimodal,yuan2021agentformer}. In the future, we will investigate the robustness of Transformer-based forecasting models used in autonomous driving in complex VR scenarios with multiple intermediary pathways, such as a person depositing a check in a bank or a student taking an examination. 

In Figure~\ref{fig:attn_map}, we present the attention maps generated by one of the encoder layers in our forecasting model. The attention maps demonstrate the positional and temporal correlations between all element pairs in the input sequence learned by the eight attention heads, which allows the model to capture complex latent patterns from multiple dimensions. In the attention maps, the length of the rows and columns is equivalent to $l_{window}$, which represents all the elements in the input sequence. Each position ($i, j$) in the attention maps indicates the attention score between the $i_{th}$ element and the $j_{th}$ element. The brightness of each position ($i, j$) corresponds to the strength of the correlation, with higher values (brighter) indicating stronger correlations. We observe that the brighter areas are predominantly in the middle and on the right side of the attention maps, indicating that, in the short-term time series forecasting task, the data closer to the timestamp to be forecasted are more relevant. On the other hand, we noticed that the patterns in the first half of the input data may not receive enough attention from our forecasting model so we will improve our model's architecture in the future to enhance its ability to capture those features.

The Miller et al.~\cite{miller2022combining,miller2022temporal} dataset provides data for 3 VR devices, namely the HTC Vive, Oculus Quest, and HTC Vive Cosmos. In this paper, we used data from the HTC Vive as it uses lighthouse-based tracking, which does not suffer from camera-based trajectory corruption during fast motions found in the Quest and Cosmos. In future work, we will investigate how our forecasting-based approach works for the Quest and Cosmos and perform cross-device authentication to compare against prior cross-device work by Miller et-al.~\cite{miller2020within,miller2021using}. Work by Ajit et al.~\cite{ajit2019combining} shows that orientation is an effective discriminating feature, in future work we will design loss functions that incorporate orientation features from the dataset. 

Work in behavioral biometrics in other domains, such as smartphone, mouse, and keystrokes, benefit from large-scale datasets due to the ease of collecting datasets at scale using web-based or downloadable applications. Datasets for VR biometrics have struggled to reach that scale due to the need of bulky VR headsets, with the largest being the 511 subject dataset of Miller et al.~\cite{miller2020personal} where users watch a series of videos and answer a set of questions. Current VR data collection is lab-based, limiting the ability to perform data collection with thousands of users. Longitudinal data collection with thousands of users in a lab-based environment is infeasible. With the rapid reduction in cost of VR devices and the potential for mass adoption in the future, data collection at scale may be possible by providing downloadable applications for a variety of serious applications. However, the collection of human behavior data without prior consent raises ethical concerns, as users may not wish to provide data without understanding how their information will be used. Behavior-based approaches provide the ability to perform seamless continual authentication without needing the user to interrupt their session. Users who wish to have a seamless secure VR experience in critical applications may be incentivized to provide their data. 

The ethics of non-verbal social behavior forecasting has been well studied. Barquero et al.~\cite{barquero2022didn} discuss non-verbal social behavior for good, such as personalized pedagogical agents or empathetic assistive robots. However, as the authors discuss, forecasting may also lead to improper outcomes in border security and human rights. Moreover, forecasting human behavior without informing the user leads to concerns about the ethical use of AI since a user is unaware that their future actions are being predicted and used. In our case, using motion forecasting to perform authentication can have negative consequences in the future. In the current state, behavior-based approaches for securing VR applications require near complete trajectories, making it harder for a malicious user to `beat' the genuine user to the end goal and access the protected secret. In this paper, we demonstrate a task-based user independent model for forecasting that could be used by a malicious agent to forecast the behavior and attack the application prior to the genuine user completing the task. In future work, we will investigate how a malicious can gain high-level access to the task, synthesize behavior data, and use our current forecasting model to generate an attack.  