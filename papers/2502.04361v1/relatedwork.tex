\section{Related Work}
A large body of work has emerged over the last decade to enable VR authentication, summarized in multiple recent reviews~\cite{giaretta2022security,stephenson2022sok,odeleye2023virtually}. Among the first approaches to VR authentication has been porting of traditional credentials such as PIN and password, and their adaptation to VR environments, e.g., via object arrangements~\cite{george2019investigating,george2020gazeroomlock}. Though some resistance to shoulder-surfing has been shown, successful malicious access of the credential compromises security. A number of recent approaches investigate the use of user interactions with VR headsets and hand controllers as a behavior signature, with work spanning the use of individual or combination of headset motion, hand movements, and eye gaze. Recent work uses deep neural networks~\cite{mathis2020knowledge,miller2021using,miller2022combining,liebers2021using,liebers2021understanding,li2024evaluating,li2024using,liebers2024kinetic} based on their ability to represent non-linearities in the data. Various activities have been investigated, including ball-throwing~\cite{miller2022combining}, pointing~\cite{pfeuffer2019behavioural}, door-opening~\cite{li2024evaluating}, pinching~\cite{suzuki2023pinchkey}, object movement~\cite{mathis2020knowledge}, and gameplay~\cite{liebers2023exploring}. 

Prior methods use movements as tracked by the tracking mechanisms available on existing VR devices. In devices such as the HTC VIVE that use lighthouses, IR light emitted by the lighthouse is tracked throughout performance by receptors on the headset and hand controllers, resulting in no more than 3 tracks, or 4 tracks in headsets that track eye gaze. Camera-based tracking such as on the Meta Quest has traditionally tracked the headset using an inside-out approach and the hand controllers through visual observations, resulting in a similar set of 3 or 4 tracks. Datasets used by nearly all approaches thus far have involved participants using hand controllers. Suzuki et al.~\cite{suzuki2023pinchkey} use controller-free hand tracking in the Meta Quest to obtain higher-resolution motion tracks for the fingers, enabling the use of pinching as a signature. However, while datasets like Alyx~\cite{rack2023alyx} exist, none of the approaches has external video data or use full-body joint articulation, e.g., at the elbows and shoulders, the torso, or the lower body, due to their reliance solely on the data tracked by the VR devices that are currently incapable of tracking full-body articulation, only Miller et al.~\cite{miller2022combining} provides external video recordings.

Our method provides the first approach for authentication using external videos to acquire detailed knowledge of full body articulation, while taking advantage of 3D information through intermediate prediction of the 3D controller motion. In conducting knowledge-augmentation by incorporating future 3D controller trajectory prediction using deep neural networks, our method is closest to the work of Li et al.~\cite{li2024using}. However, our work differs in that Li et al.~\cite{li2024using} use VR device 3D trajectory data as input similar to existing methods.