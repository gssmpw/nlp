
\section{Introduction}
\label{sec:intro}

Today's production use of generative AI (gen AI), such as code assistants~\cite{devin}, AI customer service and sales~\cite{salesforce-agentforce}, and AI-assisted search~\cite{vertex_ai_search}, often involves multiple steps of gen-AI model calling, tool/API calling, data retrieval, or generic code execution. 
%\fixme{add why workflows are important? widely used?}
In this paper, we call such gen-AI software that goes beyond a single model call {\em gen-AI workflows}~\footnote{Terms such as agentic workflows~\cite{andrew_ng_ai_agentic_workflows} and compound AI~\cite{databricks-compound-ai} are also used by others.}.
Gen-AI workflows allow for more customization, integration, and capabilities, thus being the de facto gen-AI product solutions for many business uses.
%Figure~\ref{fig-workflow} illustrates the core steps of a gen-AI workflow for text-to-SQL generation taken from ~\cite{gao2023texttosql}.
%A more generalized form of gen-AI workflows allows for arbitrary control flows such as conditional statements and loops.

Despite gen-AI workflows' appeal, their full capabilities are yet to be unlocked due to today's gen-AI workflow practice.
After developing a workflow's basic logic, engineers manually explore various variations to its structure (\eg, by adding a step), test different models, and tune various prompts.
This manual, ad-hoc workflow tuning practice is time-consuming and ineffective, as humans cannot efficiently navigate through a vast search space of tuning options. As a result, gen-AI workflows today still suffer from subpar generation quality and other metrics like end-to-end execution latency. These problems are exacerbated by the rapidly increasing amounts of gen-AI tuning options, such as new models and new prompt engineering techniques~\cite{yao2022react,yao2023tree}.
% ~\cite{Brown-2020,Nye-2022,Wei-2022,yao2022react,yao2023tree,Madaan-2023,Wang-2023,Zhou-2023a,LATS}. 
As more gen-AI workflows are introduced, automated gen-AI workflow optimization is a must. 
Unfortunately, existing automated gen-AI optimizers either focus on single model calls~\cite{routellm,tensor-opera-router} or cater to only a selected set of workflow tuning methods and evaluation metrics~\cite{khattab2024dspy,Trace,textgrad,symbolic}.

We believe workflow auto-tuning should be conducted under a systematic and extensible framework. %, for which we make the following key insights.
Our insights are that within a workflow, we can view each computation step (a model call, a tool call, a code block, a data retrieval) 
%\jingbo{I like this explanation. We should create a one-column picture at the top-right corner of the page 1 to illustrate this workflow definition and the options of different blocks .}
as a node and messages directed from one computation step to another %(according to program semantics) 
as directed edges. The ``weight'' of an edge in a gen-AI workflow is the alteration to the original message sent via the edge (\eg, adding few-shot examples to a prompt message). 
We categorize workflow tuning methods into three types as shown in Figure~\ref{fig-analogy}:
(1) {\em architecture changes} that alter the original workflow by adding/removing/moving steps or edges, such as task decomposition and ensembling,
(2) {\em step changes} that alter the operations performed at a step, such as calling different models or rewriting code pieces,
and (3) {\em weight changes} that alter the value of edges, such as augmentation to prompts.


%While all four stages are important to gen-AI workflows' end-to-end results, only the first stage has been well supported today with programming environments like LangChain~\cite{langchain-repo}, DSPy~\cite{DSPy-repo}, LlamaIndex~\cite{llamaindex}, OpenAI Swarm~\cite{swarm}, CrewAI~\cite{crewai}, and Dify~\cite{Dify}, Vellum~\cite{vellum}, and Coze~\cite{coze}. Unlike ML engineers, today's workflow engineers undergo significant manual effort to try different workflow structure variations and code variations, select models for model-call steps, and tune prompts and other workflow configurations. Such manual and ad-hoc approaches are not only time-consuming but highly ineffective in achieving the best end-to-end workflow results. While there are some recent automatic workflow tuning approaches~\cite{DSPy-2024-paper,Trace,textgrad,symbolic}, they focus on specific optimization techniques, missing the big picture and thus has limited improvement or even degredation (as we will show in Section~\ref{sec:results}).

Although traditional ML models also have similar concepts that manifest as AutoML (or Neural-Architecture Search, NAS) and weight training, classical %AutoML and training 
methods like Reinforcement-Learning- %\cite{watkins1989learning, mnih2013playing} 
and Bayesian-Optimization-based NAS~\cite{zoph17nas,falkner2018bohb, kandasamy2018nasbot,ENAS,hutter2019automated} and SGD-based weight training~\cite{lecun1998gradient} do not work for gen-AI workflows for several reasons.
First, unlike ML model weights, ``weights'' in workflows like added prompting messages are not differentiable and thus cannot be trained using SGD. 
Second, workflows (tens of steps and edges) are significantly smaller than ML models, potentially allowing search-based approaches to be applied for all types of workflow tuning changes.
Third, users often have a limited budget (\eg, 50 evaluation iterations) for their workflow optimization, not allowing enough search points for RL to learn from or for traditional BO to converge, especially when considering more types of cogs (\ie, search dimensions).
%Second, workflows (tens of steps and edges) and their tuning variations are significantly smaller than ML models and their variations, not giving enough experimental points for RL to learn from and be effective. Meanwhile, each experimental point requires the execution of a workflow and paying model API calls, making RL approaches extremely expensive.
%Second, unlike classical ML models that have a single loss function, gen-AI workflows in production usually have multiple objectives, including high generation quality, low execution cost, and low request latency. 
%as they may optimize 
%and it needs to be used efficiently to find the best combinations of workflow configurations and weights possible.
%Finally, a single pass of workflow execution involves multiple model calls and can be costly when involving large models like OpenAI GPT-4o and o1, ruling out classical NAS approaches like reinforcement learning.

To confront these challenges, we propose an adaptive, hierarchical search-based algorithm called \textbf{\textit{\search}}. 
\search\ separates architecture, step, and weight changes into different number of search layers based on the user-defined total search budget and adaptively distributes the budget across layers based on each layer's complexity. %Doing so allows each layer of change to receive some search budget even when the total budget is small.
This adaptive budget distribution ensures good search space coverage (with more layers), while allowing for enough local exploration (with fewer layers). %it could potentially spend budgets searching configurations that are not good.
%We mitigate this issue with an evaluation-based budget redistribution method.
Within each layer, \search\ uses TPE~\cite{bergstra2011tpe} to sample a small number of configurations at a time, and for each sampled configuration, \search\ recursively samples lower-layer configurations. When all layers have been sampled, \search\ evaluates the workflow with the chosen set of configurations using a user-provided training dataset and evaluator. \search\ stops searching the configurations that yield the lowest $X$\% evaluation results or when the search converges, % (\ie, improvements over previous iterations is below a threshold), 
and \search\ redistributes these saved search budget to more promising configurations=.
%\search\ adapts the number of layers according to user-defined total budgets.

%\search\ unifies workflow structure optimization, step optimization, and weight setting with a hierarchical search approach. Thus, it is feasible to use a Bayesian-Optimizer-based search process where each iteration evaluates the effectiveness of a set of configurations in the search space based on the end-to-end workflow results (\ie, final generation quality, total execution cost, total execution time) using user-supplied input data and quality evaluator.
%Thus, the amount of variations (\ie, the search space) for a workflow is significantly smaller than that of an ML model, allowing for approaches like Bayesian Optimization (BO). 
%This basic BO search works well when given plenty of search budgets. However, in our setting, users can set a small budget (\eg, 50 evaluation iterations) that can quickly run out before standard BO discovers satisfying results.
%To solve this issue, we propose an adaptive hierarchical BO algorithm. 

%By default, \search\ sets structure cogs as the top layer (in the outmost loop), step cogs as the middle layer (in the middle-level loop), and weight cogs as the bottom layer (in the innermost loop), and \search\ assigns search budget to different layers based on each layers number of cogs and configurations. When user-set budget is low, \search\ adaptively merges layers. For each layer, \search\ separates the search into buckets, within each of which \search\ gradually directs more search budgets to explore configurations that yield better results. As \search\ adapt search allocation based on evaluation results, it can efficiently use limited budgets to cover more valuable search space.

%This evaluation-driven, search-based optimization approach generalizes well to different cogs and workflows, as they all manifest as impacts on a workflow's final metrics, similar to how traditional model hyperparameter tuning works.
%We further propose a novel algorithm that incorporates Bayesian Optimization, Hyperband, and convergence when assigning the total search budget (expressed as the maximum number of iterations) across outer and inner loops.

%While single-ML-model optimization techniques like hyperparameter tuning and single-LLM prompt engineering have been well studied, optimizing gen-AI workflows presents new challenges. Notably, gen-AI workflows' key metrics like final generation quality, total execution cost, and end-to-end execution time depend on all its internal steps and how these steps are composed together. Optimizing each step in separation may not achieve satisfying end-to-end results or even negatively impact them.

Based on the \search\ algorithm, we build \textbf{\textit{\sysname}}, a gen-AI workflow optimization framework that systematically and comprehensively optimizes workflows for multiple objectives (final generation quality as defined by a user-provided evaluator, end-to-end request latency, and total workflow execution monetary cost). 
%In \sysname, each optimization option in the search space is a {\textit{cog}}. We categorize cogs into three categories: structure, step, and weight optimization. Structure cogs are placed in the outer-most loop, while weight cogs are in the inner-most loop. Depending on search budgets and workloads, layers can be merged and assigned different search budgets using the \search\ algorithm.
%, and we stop a loop as soon as it converges or hit our assigned sub-budget.
%This hierarchical optimization approach largely reduces the search space while maintaining the search effectiveness.
%Furthermore, we propose a set of bootstrapping and prepping methods to improve search efficiency.
%to improve individual tuning options by designing workflow-specific heuristics to reduce the number of optimization trials. 
%For example, instead of allowing all steps in a workflow to be decomposed by the decomposition \Outer\ cog, we first evaluate which steps are complex and only decompose those during the search. %\fixme{give another bootstrapping example}
%Finally, we employ a parallel optimization process to further reduce the user-perceived optimization time.
%to improve the optimization efficiency, we 
We evaluated \sysname\ with six representative gen-AI workflows, including question-and-answer~\cite{yang2018hotpotqa}, code generation~\cite{humaneval}, data visualization~\cite{datavis}, text-to-SQL generation~\cite{gao2023texttosql}, financial analysis~\cite{finrobot}, and the Big-bench~\cite{bigbench}. We compare \sysname\ with no workflow optimization, DSPy~\cite{DSPy-repo}, and Trace~\cite{Trace}.
Overall, \sysname\ improves workflow generation quality by up to 2.8\x, reduces execution cost by up to 10\x, and reduces end-to-end latency by up to 2.7\x{} compared to original expert-written workflows. \sysname\ outperforms DSPy and Trace with up to 2.6\x{} higher generation quality, up to 10\x{} cost reduction, and up to 3\x{} latency reduction.
%It also reaches the quality-cost Pareto frontier with a small number of optimization iterations.

Overall, this paper makes the following contributions.

\begin{itemize}
    \item The first formal and comprehensive definition of the gen-AI workflow autotuning problem.
    \item Key insights of how autotuning gen-AI workflows is different from classical AutoML and model weight training.
    \item The \search\ algorithm and its adaptive hierarchical search approach.
    \item The implementation of \sysname, its new programming model (Appendix~\ref{sec:programming-model}), and its full-scale evaluation.
    
\end{itemize}
\sysname\ is open-sourced at \url{https://github.com/GenseeAI/cognify}.
