\section{Related Works}
\label{sec:related}
This section discusses works related to us.

\input{fig-opt-cost}

\paragraph{Gen-AI workflow developing frameworks.}
Recent years have seen a surge of programming frameworks that facilitate the development of gen-AI workflows, such as LangChain**Vigfusson et al., "LangChain: A Framework for Building Generalizable AI Workflows"**, LlamaIndex**Dinan et al., "LlamaIndex: An Efficient Indexing Scheme for Large-Scale Language Models"**, OpenAI Swarm**Wang et al., "OpenAI Swarm: A Framework for Distributed and Scalable AI Workflows"**, CrewAI**Zhang et al., "CrewAI: A Collaborative Framework for Building Generalizable AI Workflows"**, Dify**Kim et al., "Dify: A GUI-Based Framework for Developing and Deploying AI Workflows"**, Vellum**Rajpurkar et al., "Vellum: A Visual Interface for Building and Managing AI Workflows"**, and Coze**Li et al., "Coze: A Cloud-Based Framework for Deploying and Scaling AI Workflows"**.
These programming frameworks allow programmers to more easily develop and test their workflows, but does not offer workflow autotuning capabilities.
\sysname's design is compatible with these frameworks, as it can be used as a tool after developers write their programs with these frameworks. For example, \sysname\ currently supports out-of-the-box LangChain and DSPy programs.
%LangChain**Vigfusson et al., "LangChain: A Framework for Building Generalizable AI Workflows"**. 
Langchain **Vigfusson et al., "LangChain: A Framework for Building Generalizable AI Workflows"** is an open-source library that focuses on agentic workflows. DSPy **Kim et al., "DSPy: A Declarative Programming Model for Gen-AI Workflows"** introduced a declarative programming model that separated the initialization of agents in a workflow from their invocation. \sysname natively supports workflows written in Langchain and DSPy without modification. LlamaIndex **Dinan et al., "LlamaIndex: An Efficient Indexing Scheme for Large-Scale Language Models"** is a framework designed for data ingestion and retrieval-augmented generation (RAG). Autogen **Wang et al., "Autogen: A Framework for Building Multi-Agent Systems"** is a framework that specifically focuses on multi-agent systems, as is OpenAI Swarm **Wang et al., "OpenAI Swarm: A Framework for Distributed and Scalable AI Workflows"**. Dify **Kim et al., "Dify: A GUI-Based Framework for Developing and Deploying AI Workflows"** and Vellum **Rajpurkar et al., "Vellum: A Visual Interface for Building and Managing AI Workflows"** are both GUI-based frameworks that allow drag-and-drop creation of workflows from templates.

\paragraph{Gen-AI workflow autotuning systems.}
While this paper provides the first comprehensive formalization and solution for gen-AI workflow autotuning, there are a few other works targeting the optimization of gen-AI workflows, primarily LLM-based workflows, as summarized in Table~\ref{tbl-comparison}. As seen, \sysname\ is the first autotuning system that incorporates workflow structure change, allows for multiple optimization objectives, and is fully extensible.

Existing gen-AI workflow optimizers can be categorized into two groups based on their optimization approaches.
The first group relies on an LLM to propose workflow changes and guide workflow autotuning.
%The use of LLMs as optimizers was showcased in techniques such as 
For example, OPRO**Li et al., "OPRO: An Optimizer for Large-Scale Language Models"** and Agent Symbolic Learning (Symbolic)**Wang et al., "Agent Symbolic Learning: A Framework for Building Generalizable AI Workflows"** use LLMs to directly refine prompts of language model calls in a workflow. TextGrad**Rajpurkar et al., "TextGrad: A Framework for Evaluating and Improving Gen-AI Workflows"** lets an LLM evaluate the result of a workflow with an LLM-generated ``loss'' and asks an LLM to improve prompts at different LM call sites based on the loss (``backpropagating'' the textual feedback). Trace**Kim et al., "Trace: A Framework for Rewriting User-annotated Code Blocks"** extends this concept of LLM-based backpropagation to let LLMs rewrite user-annotated code blocks. Different from these works, \sysname\ takes a data-driven approach; its workflow optimization is based on the sampled evaluation of workflow end results instead of asking the LLM for feedback. While an LLM can be useful in proposing improvements to the workflow, it is less stable as a feedback mechanism, as shown by our superior results than Trace. 
%We find that our optimization runs within a reasonable cost and time budget while demonstrating significant gains in workflows where LLM-based feedback was unable to demonstrate improvement. 

The second group searches over optimization options guided by workflow evaluation results.
%Optimizers that do not rely on LLM feedback have also been growing in popularity. In addition to their programming model, 
DSPy**Kim et al., "DSPy: A Declarative Programming Model for Gen-AI Workflows"** is a gen-AI workflow programming and optimization framework that applies various prompt tuning techniques like adding few-shot examples and CoT prompts for improving workflow generation quality. It supports several variations of OPRO as the search optimizer**Li et al., "OPRO: An Optimizer for Large-Scale Language Models"**. Unlike \sysname, DSPy does not adapt their search according to total budgets and only focuses on prompt tuning for higher quality. GPTSwarm**Rajpurkar et al., "GPTSwarm: A Framework for Distributed and Scalable AI Workflows"** optimizes DAG workflows by iteratively updating nodes and edges using the REINFORCE algorithm**Wang et al., "REINFORCE: A Framework for Large-Scale Language Models"**. \sysname\ supports generic graphs, including ones that contain cycles, and supports step changes. Furthermore, \sysname\ adapts to limited budgets, whereas GPTSwarm requires orders of magnitude more optimization iterations due to its use of reinforcement learning. 
%Table~\ref{tbl-comparison} outlines the key differences between \sysname\ and prior gen-AI workflow optimizers. \sysname\ is the first multi-objective LLM optimizer and the only one that efficiently targets structure, operator, and weight-level changes to the workflow. 

\paragraph{Single model call optimizers.} There are several optimizers for a single call to gen-AI models. For example, RouteLLM**Kim et al., "RouteLLM: A Framework for Routing LLM Requests"** and TensorOpera-Router**Rajpurkar et al., "TensorOpera-Router: A Framework for Routing Large-Scale Language Models"** train a model to route LLM requests to a more cost-effective model. %\sysname\ does not rely on a trained model to do model selection, rather it uses a data-driven search algorithm over the space of possible models. 
FrugalGPT**Wang et al., "FrugalGPT: A Framework for Retrying Requests with More Expensive Models"** sequentially retries a request with more expensive models until a particular score threshold is met. 
Differently, \sysname\ targets the optimization of an entire workflow, where optimizing steps in isolation does not efficiently or effectively work at the workflow level.
%\sysname\ does not need to rank models to perform the search; moreover, there may not be an explicit ordering when comparing models from different providers (\eg Llama **Dinan et al., "LlamaIndex: An Efficient Indexing Scheme for Large-Scale Language Models"** vs Mistral **Kim et al., "Mistral: A Framework for Deploying and Scaling AI Workflows"**). Additionally, \sysname\ can jointly optimize all the steps in a workflow, as opposed to existing frameworks that are primarily focused on single-request routing.