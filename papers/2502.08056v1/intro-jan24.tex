


\section{Introduction}
\label{sec:intro}

Today's production use of generative AI (gen AI), such as code assistants~\cite{devin}, AI customer service and sales~\cite{salesforce-agentforce}, and AI-assisted search~\cite{vertex_ai_search}, often involves multiple steps of gen-AI model calling, tool/API calling, data retrieval, or generic code execution. 
%goes beyond a single model call and manifests as a pipeline, or {\em workflow}, of modules.
In this paper, we call such gen-AI software that goes beyond a single model call {\em gen-AI workflows}~\footnote{Other terms such as agentic workflows~\cite{andrew_ng_ai_agentic_workflows} and compound AI~\cite{databricks-compound-ai} are also used by others.}.
Recent years have seen a surge of programming models that assist programmers in writing structured gen-AI workflow programs, such as LangChain~\cite{langchain-repo}, DSPy~\cite{DSPy-repo}, LlamaIndex~\cite{llamaindex}, OpenAI Swarm~\cite{swarm}, CrewAI~\cite{crewai}, Dify~\cite{Dify}, Vellum~\cite{vellum}, and Coze~\cite{coze}.
While they ease the development of gen-AI workflow software, engineers still need to undergo significant manual effort to test, evaluate, and tune their workflows. 
Moreover, the effectiveness of manual tuning is highly dependent on the person performing it and is often sub-par.
%Despite their increasing popularity, the deployment cycles of gen AI workflows are time-consuming and error-prone, as the development, tuning, testing, and model selection of gen AI workflows are performed largely manually. 
%Such manual processes not only incur huge engineering and monetary costs but could also impact the generation quality of gen AI workflows.
These problems are exacerbated by the rapidly increasing amounts of gen AI tuning options such as the variety of models and prompt engineering techniques~\cite{yao2022react,yao2023tree}
% ~\cite{Brown-2020,Nye-2022,Wei-2022,yao2022react,yao2023tree,Madaan-2023,Wang-2023,Zhou-2023a,LATS}. 
As more gen-AI workflows are introduced, automated gen-AI workflow optimization is a must. 
%\zijian{Think here we can mention more agentic optimization examples, maybe emphasize that today's optimization is everywhere, and the effectiveness of each technique can vary significantly across diff tasks. lack a systematic way of combining them in a complex staged workflow.}

While single-ML-model optimization techniques like hyperparameter tuning and single-LLM prompt engineering have been well studied, optimizing gen-AI workflows presents new challenges. Notably, gen-AI workflows' key metrics like final generation quality, total execution cost, and end-to-end execution time depend on all its internal steps and how these steps are composed together. Optimizing each step in separation may not achieve satisfying end-to-end results or even negatively impact them.

Recently, several automated gen-AI workflow optimization techniques have been proposed. For example, DSPy~\cite{DSPy-repo} supports several workflow-level prompt-tuning techniques to improve the generation quality of language-model-based workflows written in the DSPy programming model; Trace~\cite{Trace} and TextGrad~\cite{textgrad} uses an LLM to improve code pieces and language-model prompts in a workflow based on the workflow's final generation quality.
%\zijian{can say DSPy applies black-box optimization techniques to tune instructions and few-shot examples.}
%\reyna{other examples: GPTSwarm, Symbolic Learning Agents}. 
These existing gen-AI workflow optimizers have several key limitations.
First, each optimizer is specific to one or a few optimization techniques and cannot be easily extended to compose support new techniques. 
Second, existing optimizers have the single optimization goal of improving generation quality. However, the execution cost and latency of gen-AI workflows often need to be co-optimized in practice.
%as large-model costs keep increasing, the execution cost of gen AI workflows is also important and should be co-optimized with quality.
Third, existing optimizers are limited to improvements under the original workflow structure. Yet, changing workflow structures (\eg, decomposing a step, changing control flow) would enable another level of workflow improvements. 
%\zijian{One more challenge: as the workflow becomes more complex and optimization techniques emerging, the search space will soon become intractable. so the optimization itself need more guidance and improve the efficiency.}
%like hyperparameter tuning
%we are NAS plus hyperparameter tuning
Finally, existing optimizers such as DSPy are limited to supporting only a specific programming model and cannot work with different frontends. 

We propose \textbf{\textit{\sysname}}, a gen-AI workflow compiler and optimization framework that systematically and comprehensively optimizes workflows for multiple objectives. \sysname\ addresses the limitations of prior works by 
(1) treating optimization techniques as pluggable knobs, or {\textit{cog}s}, that can be flexibly added, enabled, or disabled for different workflow optimizations; 
%versioning
(2) generating multiple optimization versions that are on the Pareto frontier of different end-to-end workflow optimization objectives, including final generation quality, total execution cost, and total execution; 
(3) supporting optimization techniques (\ie, cogs) that change workflow structures (\eg, expansion, decomposition) and efficiently composing them with other cogs during optimization; 
and (4) incorporating a frontend connector to translate workflows written in different programming models into a unified intermediate representation before applying cogs. 
%Unlike previous works that propose individual optimization techniques for a single objective and a programming model, \sysname\ provides a unified, extensible gen-AI workflow optimization platform that enables a wide range of use cases and gen-AI optimization innovations.
%Users run \sysname\ by submitting their gen-AI-workflow source code, the evaluator of the workflow's generation, and sample inputs. \sysname\ optimizes the user-deployed workflow by compiling (\ie, converting) it to a set of intermediate representations, each representing a different quality-cost combination on the Pareto frontier. The intermediate representations can be directly executed on backends like OpenAI APIs.
%; they can also be mapped back to the original format. \zijian{convert back to original front-end currently not supported}


Realizing the above goals in \sysname\ presents unique challenges.
First, as \sysname\ supports different types of optimization techniques (cogs) and workflow types, we need an optimization method that can generalize well.
Our idea is to treat cogs and workflow steps as {\em black boxes} and evaluate the effectiveness of cogs applied on workflow steps only based on the end-to-end workflow results (\ie, final generation quality, total execution cost, total execution time).
%We propose a data-driven optimization methodology that works across different cogs and workflows.
Specifically, we execute a workflow in iterations with a set of user-supplied input data and use a customized Bayesian Optimizer to navigate through the cog search space based on the end-to-end metrics of each iteration.
This evaluation-driven, search-based optimization approach generalizes well to different cogs and workflows, as they all manifest as impacts on a workflow's final metrics, similar to how traditional model hyperparameter tuning works.
%while only requiring users to supply a small set of training data and 
%Users of \sysname\ supply an evaluator of their workflow's final generation

One challenge needs to be tackled to make the above approach practical: the search space of the optimization grows exponentially with the number of steps in a workflow and the number of cogs. As in \sysname, every optimization technique (even model selection) is a cog, the vast search space, coupled with multiple optimization objectives, can make the optimization process of complex workflows lengthy and costly.
%the makes the optimization process challenging.
%In \sysname, every optimization technique is a cog. For example, automated LLM model selection is a cog that can be applied to every LLM call; multi-step reasoning and few-shot examples are two cogs for improving LLM prompts; task decomposition and task ensemble are two cogs that change the structure of workflows.

To tackle this challenge, we first separate cogs into two categories, structure-changing ({\textit{\Outer}}) and structure-maintaining ({\textit{\Inner}}), and vary \Inner\ cogs (in an inner loop) while fixing \Outer\ cogs (in an outer loop). 
We further propose a novel algorithm that incorporates Bayesian Optimization, Hyperband, and convergence when assigning the total search budget (expressed as the maximum number of iterations) across outer and inner loops.
%, and we stop a loop as soon as it converges or hit our assigned sub-budget.
This hierarchical optimization approach largely reduces the search space while maintaining the search effectiveness.
Furthermore, we propose a set of bootstrapping and prepping methods to improve search efficiency.
%to improve individual tuning options by designing workflow-specific heuristics to reduce the number of optimization trials. 
For example, instead of allowing all steps in a workflow to be decomposed by the decomposition \Outer\ cog, we first evaluate which steps are complex and only decompose those during the search. %\fixme{give another bootstrapping example}
Finally, we employ a parallel optimization process to further reduce the user-perceived optimization time.
%to improve the optimization efficiency, we 

We evaluated \sysname\ with seven representative gen-AI workflows, including QA~\cite{yang2018hotpotqa}, document retrieval~\cite{hover}, code generation~\cite{humaneval}, data visualization~\cite{datavis}, text-to-SQL generation~\cite{gao2023texttosql}, maths problem solving~\cite{hendrycksmath2021}, and the Big-bench~\cite{bigbench}. We compare \sysname\ with no workflow optimization, DSPy~\cite{DSPy-repo}, and Trace~\cite{Trace}.
Overall, \sysname\ improves workflow generation quality by up to 48\%, reduces execution cost by up to 9\x, and reduces end-to-end latency by up to 3.7\x. 
%It also reaches the quality-cost Pareto frontier with a small number of optimization iterations.

