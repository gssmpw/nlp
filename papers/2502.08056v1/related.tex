\section{Related Works}
\label{sec:related}
This section discusses works related to us.

\input{fig-opt-cost}

\paragraph{Gen-AI workflow developing frameworks.}
Recent years have seen a surge of programming frameworks that facilitate the development of gen-AI workflows, such as LangChain~\cite{langchain-repo}, LlamaIndex~\cite{llamaindex}, OpenAI Swarm~\cite{swarm}, CrewAI~\cite{crewai}, and Dify~\cite{Dify}, Vellum~\cite{vellum}, and Coze~\cite{coze}.
These programming frameworks allow programmers to more easily develop and test their workflows, but does not offer workflow autotuning capabilities.
\sysname's design is compatible with these frameworks, as it can be used as a tool after developers write their programs with these frameworks. For example, \sysname\ currently supports out-of-the-box LangChain and DSPy programs.
%LangChain~\cite{langchain}. Langchain \cite{langchain} is an open-source library that focuses on agentic workflows. DSPy \cite{dspy} introduced a declarative programming model that separated the initialization of agents in a workflow from their invocation. \sysname natively supports workflows written in Langchain and DSPy without modification. LlamaIndex \cite{llamaindex} is a framework designed for data ingestion and retrieval-augmented generation (RAG). Autogen \cite{autogen} is a framework that specifically focuses on multi-agent systems, as is OpenAI Swarm \cite{swarm}. Dify \cite{dify} and Vellum \cite{vellum} are both GUI-based frameworks that allow drag-and-drop creation of workflows from templates.

\paragraph{Gen-AI workflow autotuning systems.}
While this paper provides the first comprehensive formalization and solution for gen-AI workflow autotuning, there are a few other works targeting the optimization of gen-AI workflows, primarily LLM-based workflows, as summarized in Table~\ref{tbl-comparison}. As seen, \sysname\ is the first autotuning system that incorporates workflow structure change, allows for multiple optimization objectives, and is fully extensible.

Existing gen-AI workflow optimizers can be categorized into two groups based on their optimization approaches.
The first group relies on an LLM to propose workflow changes and guide workflow autotuning.
%The use of LLMs as optimizers was showcased in techniques such as 
For example, OPRO~\cite{opro} and Agent Symbolic Learning (Symbolic)~\cite{symbolic} use LLMs to directly refine prompts of language model calls in a workflow. TextGrad~\cite{textgrad} lets an LLM evaluate the result of a workflow with an LLM-generated ``loss'' and asks an LLM to improve prompts at different LM call sites based on the loss (``backpropagating'' the textual feedback). Trace~\cite{Trace} extends this concept of LLM-based backpropagation to let LLMs rewrite user-annotated code blocks. Different from these works, \sysname\ takes a data-driven approach; its workflow optimization is based on the sampled evaluation of workflow end results instead of asking the LLM for feedback. While an LLM can be useful in proposing improvements to the workflow, it is less stable as a feedback mechanism, as shown by our superior results than Trace. 
%We find that our optimization runs within a reasonable cost and time budget while demonstrating significant gains in workflows where LLM-based feedback was unable to demonstrate improvement. 

The second group searches over optimization options guided by workflow evaluation results.
%Optimizers that do not rely on LLM feedback have also been growing in popularity. In addition to their programming model, 
DSPy~\cite{khattab2024dspy,dspy-2-2024,DSPy-repo} is a gen-AI workflow programming and optimization framework that applies various prompt tuning techniques like adding few-shot examples and CoT prompts for improving workflow generation quality. It supports several variations of OPRO as the search optimizer~\cite{dspy-2-2024}. Unlike \sysname, DSPy does not adapt their search according to total budgets and only focuses on prompt tuning for higher quality. GPTSwarm~\cite{gptswarm} optimizes DAG workflows by iteratively updating nodes and edges using the REINFORCE algorithm~\cite{reinforce}. \sysname\ supports generic graphs, including ones that contain cycles, and supports step changes. Furthermore, \sysname\ adapts to limited budgets, whereas GPTSwarm requires orders of magnitude more optimization iterations due to its use of reinforcement learning. 
%Table~\ref{tbl-comparison} outlines the key differences between \sysname\ and prior gen-AI workflow optimizers. \sysname\ is the first multi-objective LLM optimizer and the only one that efficiently targets structure, operator, and weight-level changes to the workflow. 

\paragraph{Single model call optimizers.} There are several optimizers for a single call to gen-AI models. For example, RouteLLM~\cite{routellm} and TensorOpera-Router~\cite{tensor-opera-router} train a model to route LLM requests to a more cost-effective model. %\sysname\ does not rely on a trained model to do model selection, rather it uses a data-driven search algorithm over the space of possible models. 
FrugalGPT~\cite{frugalgpt} sequentially retries a request with more expensive models until a particular score threshold is met. 
Differently, \sysname\ targets the optimization of an entire workflow, where optimizing steps in isolation does not efficiently or effectively work at the workflow level.
%\sysname\ does not need to rank models to perform the search; moreover, there may not be an explicit ordering when comparing models from different providers (\eg Llama \cite{llama3} vs Mistral \cite{jiang2023mistral}). Additionally, \sysname\ can jointly optimize all the steps in a workflow, as opposed to existing frameworks that are primarily focused on single-request routing.

