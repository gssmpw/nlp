\section{Formalizing Gen-AI Workflow Autotuning}
\label{sec:theory}

This section presents how we formalize the problem of gen-AI workflow autotuning with a general, extendable framework and how it resonates but differs from classical AutoML \cite{hutter2019automated} and weight training \cite{rumelhart1986learning}.

We formalize the definition of gen-AI workflows as generic program flows that contain at least one generative AI model call. A {\em step}, $s_i$, in a workflow represents a unit whose internal states are not autotuned. For example, a model call is a step as its internal weights are not autotuned at the workflow level. A code block can be a step if it is always tuned as one piece. All steps in a workflow forms the node set, $S$. %Workflow programming models like LangChain, DSPy, and our own (Appendix~\ref{XXX}) usually define constructs that can be used to identify steps. 
We define an {\em edge}, $e_{ij}$, for every pair of steps $s_i$ and $s_j$ where $s_i$ sends its output to $s_j$. As we allow arbitrary gen-AI workflow program structures, edges in a workflow can represent control paths and form cycles. We define the edge set, $E$. We define the weight of an edge, $w_{ij}$, as the auxilary values or operations applied to the output $e_{ij}$ from $s_i$ to $s_j$. For example, $w_{ij}$ can be few-shot examples added to the output message $o_{ij}$ to form a prompt to the next LLM step, $s_j$. %), where it performs $LLM(w_{ij}^\frown o_{ij})$ ($^\frown$ is the concatenation operation). 
%More generally, $w_{ij}$ can be a function applied to the output $o_{ij}$, such as compression.
A gen-AI workflow, $W$, is $W = (S,E)$.

%Connecting all of the above pieces, each gen-AI workflow can be viewed as a directed graph structure with tunable weights on its edges, what we call a ``\textbf{\textit{super-model}}''. 
%Similar to classical ML, 
After a workflow architecture is defined, its structure, step operators, and edge weights can all be altered to achieve better results. We view all these processes as {\em autotuning} a gen-AI workflow and call each type of alteration a {\em cog}. Each cog can have many different values and forms one dimension in the entire autotuning search space. We categorize cogs into three types.
The first type, {\em architecture cogs, $C_a$}, alters the structure of a workflow by adding or removing steps and edges to get a new workflow graph $W_\delta = (S_\delta,E_\delta)$. For example, \sysname\ includes two architecture cogs, task decomposition and task ensembling (Section~\ref{sec:structure-cog}), $c_a^{TD}$ and $c_a^{TE}$. Each cog can be applied to different locations of a workflow, \ie, having different values along their dimensions. %an LLM call could be broken into two sub-calls, esentially adding a step and an edge to the workflow structure. 
The second type, {\em step cogs, $C_s$}, alters a step's operation without changing its input and output, \ie, $s_i\rightarrow s_i^{\delta}$. For example, the model cog $c_s^{MD}$ \sysname\ supports uses different models at different model-call steps.
%; a code step can be rewritten to use a different function implementation. 
The third method, {\em weight cogs, $C_w$}, alters the values of edges, $e_{ij}$.
For language model steps, their inputs are prompts, and weights are various prompt engineering methods added to the original prompt message.
%--- auxilary values or functions added to the raw output of source steps before inputting to destination steps. 
%For example, meta-prompts like Chain-of-Thought instructions and few-shot examples can be viewed as weights added to edges that go to language-model steps.

The effectiveness of gen-AI workflow autotuning is measured by user-defined metrics, $M=(M_1, M_2,..)$. For example, a quality evaluator can be the percentage of workflow generations exactly matching the ground truth. It can also be a numerical score given by an LLM as a judge. Other metrics include end-to-end request execution latency and total workflow execution monetary cost. A workflow is judged by a combination of user-defined metrics with a user-defined evaluator function, $E(M)$. For example, an evaluator can be maximizing the product of quality and cost-effectiveness (the inverse of monetary cost), $E(M_q,M_c)=max(M_q\times \frac{1}{M_c})$. The workflow autotuning process' goal is to apply alterations $(C_a,C_s,C_e)$ to a workflow $W$ to achieve the best $E(M)$.

The above definitions have their similar counterparts in classical ML: ML model architecture search, operator search, weight training, and loss functions.
However, techniques from classical ML do not apply to gen-AI workflow autotuning for several key reasons.
First, architecture, step, and weight cogs are non-differentiable, and metrics to evaluate workflows are often discrete and multi-dimensional.
%,  ``loss functions''. Different from classical ML's loss functions, gen-AI workflows' loss, or in another word values, is more complicated. 
%each cog dimension has similar orders of magnitude of discrete values. 
Thus, unlike classical ML that performs weight training using SGD, all types of cogs in workflow autotuning including weight cogs could use discrete search methods.
Second, gen-AI workflows usually have no more than tens of steps and edges, orders of magnitude smaller than modern ML models that have billions of parameters.
The search space of cogs is significantly smaller than the search space in traditional AutoML, allowing for search-based approaches like Bayesian Optimization.
Third, alterations to a workflow (\ie, cog values) are sometimes non-structured and have no known super set, as some cog values can be proposed by an LLM. Thus, traditional NAS approaches based on super-models~\cite{ENAS} do not work for workflows.
Finally, unlike traditional AutoML and weight training, gen-AI workflows often need to be tuned more frequently because of the fast development of gen-AI models and their use cases. As each workflow autotuning is expected to incur a monetary burden, users are likely to use relatively small search budgets (\eg, tens to hundreds of iterations). Small budgets render RL-based search solutions infeasible, as they cannot acquire enough experience. 

%It can include the final generation's quality measured by exact match, text similarity, or judged by an LLM~\cite{llm-as-a-judge}, total workflow execution cost, and end-to-end workflow request exeuction latency. In practice, the evaluation of a workflow usually involve one or more metrics. 
%Because of the above three key differences, we argue that super-model architecture, step, and weight tuning (\ie, training) can all be more effectively conducted with a search-based approach like \search.