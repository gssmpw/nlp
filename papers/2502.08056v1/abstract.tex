\begin{abstract}
Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone.
%After developing such gen-AI workflows, engineers today manually tune them by trying different models and prompts at different call sites and by experimenting with various workflow flow changes. Such ad-hoc manual tuning is 
In this paper, we propose a systematic approach for automatically tuning gen-AI workflows.
Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques.
%like NAS and weight training , but three key differences make traditional AutoML and weight training methods unfit for gen-AI workflows: prompts (unlike weights) are discrete and non-differentiable, workflow architectures and their tuning search space are much smaller than ML model architectures like DNN, and users usually have a limited budget for tuning a workflow.
%Based on this insight, 
We propose \search, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. \search\ organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, \search\ redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results.
%\jingbo{what's the difference between this work and AutoML? AutoML, if specifically working on neural network models, it's also like building legos. Why AutoML methods cannot be applied/adapted here?}
%\jingbo{please define the ``workflow'' too. What are the options in this ``AutoWorkFlow''? How large is the search space?}
We implement \search\ in a workflow autotuning framework called \sysname\ and evaluate \sysname\ using six types of workflows such as RAG-based QA and text-to-SQL transformation.
%\jingbo{What exactly is this bayesian optimization? Is it just applying some existing technique or we have something novel? More details are needed.}
%\sysname\ uses a multi-layer Bayesian-Optimization approach to automatically search for the best workflow structure changes, step changes, and weight changes based on user-supplied training dataset and optimization objectives. 
%While there are a few automated gen-AI workflow optimizers, they only support a single objective, limited optimization techniques, and workflow types. We propose \sysname, a generic compiler and optimization framework that systematically optimizes gen-AI workflows for multiple objectives with extensible optimization techniques. \sysname\ centers around a data-driven, search-based optimization methodology that works with different optimization techniques, workflows, and objectives. %Specifically, we execute a workflow in iterations with a set of user-supplied input data and use a customized Bayesian Optimizer to navigate through the cog search space based on the end-to-end metrics of each iteration. Moreover, we provide a mechanism to assign the total search budget across outer and inner loops, and we stop a loop as soon as it converges or hits our assigned sub-budget. Finally, we perform bootstrapping and prepping methods to improve search efficiency.
Overall, \sysname\ improves these workflows' generation % \jingbo{I think workflow varies significantly across domains. What are the xperiment domains here?} 
 quality by up to 2.8\x{}, reduces execution monetary cost by up to 10\x{}, and reduces end-to-end latency by 2.7\x{}. 
%\jingbo{what is the difference between execution cost and end-to-end latency? These shall be highly correlated right?}
%\jingbo{How about manually crafted workflow vs. this automatically generated workflow?}

\end{abstract}