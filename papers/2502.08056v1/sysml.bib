@misc{dspy-2-2024,
      title={Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs}, 
      author={Krista Opsahl-Ong and Michael J Ryan and Josh Purtell and David Broman and Christopher Potts and Matei Zaharia and Omar Khattab},
      year={2024},
      eprint={2406.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11695}, 
}

@article{dspy-repo,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@misc{ENAS,
      title={Efficient Neural Architecture Search via Parameter Sharing}, 
      author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
      year={2018},
      eprint={1802.03268},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.03268}, 
}

@inproceedings{
khot2023decomposed,
title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
author={Tushar Khot and Harsh Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=_nGgzQjzaRy}
}

@misc{zoph2017neuralarchitecturesearchreinforcement,
      title={Neural Architecture Search with Reinforcement Learning}, 
      author={Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1611.01578},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.01578}, 
}

@misc{kulkarni25gold,
  author = {Kulkarni, Sandip and Savelieva, Alexandra},
  title = {The Path to a Golden Dataset, or How to Evaluate Your RAG},
  year = {2025},
  url = {https://medium.com/data-science-at-microsoft/the-path-to-a-golden-dataset-or-how-to-evaluate-your-rag-045e23d1f13f},
  note = {Accessed: 2025-02-10}
}


@misc{gpt-j-6b,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@inproceedings{bergstra2011tpe,
  author    = {James Bergstra and Rémi Bardenet and Yoshua Bengio and Balázs Kégl},
  title     = {Algorithms for Hyper-Parameter Optimization},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems (NeurIPS)},
  year      = {2011},
  pages     = {2546--2554}
}

@inproceedings{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  booktitle={Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)},
  pages={1--12},
  year={2023},
  organization={NeurIPS}
}


@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023},
  month = Nov,
}

@InProceedings{xiao2021next,
    author    = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
    title     = {NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {9777-9786}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}

@incollection{lecun1998gradient,
  author    = {Yann LeCun and Léon Bottou and Genevieve B. Orr and Klaus-Robert Müller},
  title     = {Efficient BackProp},
  booktitle = {Neural Networks: Tricks of the Trade},
  pages     = {9--50},
  year      = {1998},
  publisher = {Springer},
  doi       = {10.1007/3-540-49430-8_2}
}


@phdthesis{watkins1989learning,
  author    = {Christopher J. C. H. Watkins},
  title     = {Learning from Delayed Rewards},
  school    = {King's College, University of Cambridge},
  year      = {1989}
}

@inproceedings{mnih2013playing,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  booktitle = {Proceedings of the Deep Learning Workshop, NIPS},
  year      = {2013},
  archivePrefix = {arXiv},
  eprint    = {1312.5602}
}

@article{jones1998efficient,
  author    = {Donald R. Jones and Matthias Schonlau and William J. Welch},
  title     = {Efficient Global Optimization of Expensive Black-Box Functions},
  journal   = {Journal of Global Optimization},
  volume    = {13},
  number    = {4},
  pages     = {455--492},
  year      = {1998},
  publisher = {Springer},
  doi       = {10.1023/A:1008306431147}
}


@inproceedings{falkner2018bohb,
  author    = {Stefan Falkner and Aaron Klein and Frank Hutter},
  title     = {BOHB: Robust and Efficient Hyperparameter Optimization at Scale},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year      = {2018},
  pages     = {1437--1446},
  archivePrefix = {arXiv},
  eprint    = {1807.01774}
}


@inproceedings{kandasamy2018nasbot,
  author    = {Kirthevasan Kandasamy and Willie Neiswanger and Jeff Schneider and Barnabas Poczos and Eric P. Xing},
  title     = {Neural Architecture Search with Bayesian Optimisation and Optimal Transport},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
  pages     = {2016--2025},
  archivePrefix = {arXiv},
  eprint    = {1802.07191}
}



@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017},
month = Dec,
address = {Long Beach, CA}
}


@article{wu2023fast,
      title={Fast Distributed Inference Serving for Large Language Models}, 
      author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Gang Huang and Xuanzhe Liu and Xin Jin},
      year={2023},
month = May,
journal = {arXiv preprint arXiv:2305.05920},
      eprint={2305.05920},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zheng2023efficiently,
      title={Efficiently Programming Large Language Models using SGLang}, 
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2023},
month = Dec,
journal = {arXiv preprint arXiv:2312.07104},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{yen2024longcontext,
      title={Long-Context Language Modeling with Parallel Context Encoding}, 
      author={Howard Yen and Tianyu Gao and Danqi Chen},
      year={2024},
      eprint={2402.16617},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gim2024prompt,
      title={Prompt Cache: Modular Attention Reuse for Low-Latency Inference}, 
      author={In Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong},
      year={2024},
      month = May,
      address = {Santa Clara, CA},
Booktitle = {Proceedings of the 7th MLSys Conference}
      
}

@inproceedings{zhong2024distserve,
      title={DistLLM: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving}, 
      author={Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
      year={2024},
month = Jul,
address = {Santa Clara, CA},
Booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI '24)}
}



@inproceedings{
zhang2023automatic,
title={Automatic Chain of Thought Prompting in Large Language Models},
author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
month = May,
address = {Kigali, Rwanda},
url={https://openreview.net/forum?id=5NTt8GFjUHkr}
}

@inproceedings{Arpan2020clock,
author = {Gujarati, Arpan and Karimi, Reza and Alzayat, Safya and Hao, Wei and Kaufmann, Antoine and Vigfusson, Ymir and Mace, Jonathan},
title = {Serving DNNs like clockwork: performance predictability from the bottom up},
year = {2020},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
pages={443--462}
}

@inproceedings{10.5555/3488766.3488791,
author = {Gujarati, Arpan and Karimi, Reza and Alzayat, Safya and Hao, Wei and Kaufmann, Antoine and Vigfusson, Ymir and Mace, Jonathan},
title = {Serving DNNs like clockwork: performance predictability from the bottom up},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {Machine learning inference is becoming a core building block for interactive web applications. As a result, the underlying model serving systems on which these applications depend must consistently meet low latency targets. Existing model serving architectures use well-known reactive techniques to alleviate common-case sources of latency, but cannot effectively curtail tail latency caused by unpredictable execution times. Yet the underlying execution times are not fundamentally unpredictable--on the contrary we observe that inference using Deep Neural Network (DNN) models has deterministic performance.Here, starting with the predictable execution times of individual DNN inferences, we adopt a principled design methodology to successively build a fully distributed model serving system that achieves predictable end-to-end performance. We evaluate our implementation, Clockwork, using production trace workloads, and show that Clockwork can support thousands of models while simultaneously meeting 100 ms latency targets for 99.9999\% of requests. We further demonstrate that Clockwork exploits predictable execution times to achieve tight request-level service-level objectives (SLOs) as well as a high degree of request-level performance isolation.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {25},
numpages = {20},
series = {OSDI'20}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2023},
month=Mar,
}

@article{symbolic,
  title={Symbolic learning enables self-evolving agents},
  author={Zhou, Wangchunshu and Ou, Yixin and Ding, Shengwei and Li, Long and Wu, Jialong and Wang, Tiannan and Chen, Jiamin and Wang, Shuai and Xu, Xiaohua and Zhang, Ningyu and others},
  journal={arXiv preprint arXiv:2406.18532},
  year={2024}
}

@article{tensor-opera-router,
  title={TensorOpera Router: A Multi-Model Router for Efficient LLM Inference},
  author={Stripelis, Dimitris and Hu, Zijian and Zhang, Jipeng and Xu, Zhaozhuo and Shah, Alay Dilipbhai and Jin, Han and Yao, Yuhang and Avestimehr, Salman and He, Chaoyang},
  journal={arXiv preprint arXiv:2408.12320},
  year={2024}
}

@book{hutter2019automated,
  title={Automated machine learning: methods, systems, challenges},
  author={Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year={2019},
  publisher={Springer Nature}
}

@inproceedings{zoph17nas,
    author = {Zoph, Barrett and Le, Quoc},
    title = {Neural Architecture Search With Reinforcement Learning},
booktitle = {Proceedings of International Conference on Learning Representations},
    year = {2017}
}

@misc{sora,
    author = {OpenAI},
    title = {Video generation models as world simulators},
    year = {2024},
    howpublished={\url{https://openai.com/index/video-generation-models-as-world-simulators}}
}

@inproceedings{yu2023self,
  title   = {Self-Chained Image-Language Model for Video Localization and Question Answering},
  author  = {Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year    = {2023},
month = Dec,
address = {New Orleans, Louisiana},
}

@misc{tensorRTllm,
    title = {NVIDIA TensorRT-LLM Supercharges Large Language Model Inference on NVIDIA H100 GPUs},
    author = {Vaidya, Neal and Comly, Nick and DeLaere, Joe and Patel, Ankit and Oh, Fred},
    year = {2023},
    howpublished = {\url{https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/}} 
}

@inproceedings{miao2023specinfer,
author = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and Shi, Chunan and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
title = {SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
year = {2024},
location = {La Jolla, CA, USA},
month = Apr,
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems}
}

@misc{azureTrace,
    title = {Azure LLM inference trace 2023} ,
    year = {2023},
    howpublished = {\url{https://github.com/Azure/AzurePublicDataset/blob/master/AzureLLMInferenceDataset2023.md}}
}

@article{jacobs2023deepspeed,
      title={DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models}, 
      author={Sam Ade Jacobs and Masahiro Tanaka and Chengming Zhang and Minjia Zhang and Shuaiwen Leon Song and Samyam Rajbhandari and Yuxiong He},
      year={2023},
      month = Oct,
      journal = {arXiv preprint arXiv:2309.14509},
      eprint={2309.14509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{liu2023ring,
      title={Ring Attention with Blockwise Transformers for Near-Infinite Context}, 
      author={Hao Liu and Matei Zaharia and Pieter Abbeel},
      year={2023},
month = Nov,
journal = {arXiv preprint arXiv:2310.01889},
      eprint={2310.01889},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{munkhdalai2024leave,
      title={Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention}, 
      author={Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
      year={2024},
month = Apr,
journal = {arXiv preprint arXiv:2404.07143},
      eprint={2404.07143},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{esser2020taming,
      title={Taming Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Robin Rombach and Björn Ommer},
      year={2020},
      address = {Los Alamitos, CA},
      month = Jun,
      booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@software{Chase_LangChain_2022,
author = {Chase, Harrison},
month = oct,
title = {{LangChain}},
url = {https://github.com/langchain-ai/langchain},
year = {2022}
}

@online{openai-chatgpt-whisper-apis,
  author = {Greg Brockman and Atty Eleti and Elie Georges and Joanne Jang and Logan Kilpatrick and Rachel Lim and Luke Miller and Michelle Pokrass},
  title = {Introducing ChatGPT and Whisper APIs},
  year = {2023},
  month = {March 1},
  howpublished = {OpenAI Blog},
  url = {https://openai.com/blog/introducing-chatgpt-and-whisper-apis},
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{patil2023gorilla,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
  year={2023},
  journal={arXiv preprint arXiv:2305.15334},
} 

@online{greyling-chatgpt-api,
  author = {Cobus Greyling},
  title = {When Using the ChatGPT API, Users Will Have to Manage the Context},
  year = {2023},
  month = {March 6},
  howpublished = {Medium},
  url = {https://cobusgreyling.medium.com/when-using-the-chatgpt-api-users-will-have-to-manage-the-context-ba5869238913},
}


@online{wolfram-chatgpt,
  author = {Stephen Wolfram},
  title = {ChatGPT Gets Its 'Wolfram Superpowers'!},
  year = {2023},
  howpublished = {Stephen Wolfram Writings},
  url = {https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/},
}


@misc{nllbteam2022language,
      title={No Language Left Behind: Scaling Human-Centered Machine Translation}, 
      author={Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
      year={2022},
      eprint={2207.04672},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@InProceedings{gptswarm,
  title = 	 {{GPTS}warm: Language Agents as Optimizable Graphs},
  author =       {Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J\"{u}rgen},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {62743--62767},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhuge24a/zhuge24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zhuge24a.html},
  abstract = 	 {Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. Our code is public.}
}


@article{frugalgpt,
  title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

@inproceedings{self-refine,
 author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46534--46594},
 publisher = {Curran Associates, Inc.},
 title = {Self-Refine: Iterative Refinement with Self-Feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{reinforce,
  title={Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author={Williams, Ronald J.},
  booktitle={Proceedings of the 8th International Conference on Neural Information Processing Systems (NIPS)},
  year={1992},
  pages={877--884},
  publisher={MIT Press}
}


@misc{mialon2023augmented,
      title={Augmented Language Models: a Survey}, 
      author={Grégoire Mialon and Roberto Dessì and Maria Lomeli and Christoforos Nalmpantis and Ram Pasunuru and Roberta Raileanu and Baptiste Rozière and Timo Schick and Jane Dwivedi-Yu and Asli Celikyilmaz and Edouard Grave and Yann LeCun and Thomas Scialom},
      year={2023},
      eprint={2302.07842},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{yao2023tree,
 author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
month = Dec,
address = {New Orleans, Louisiana}
}

@article{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
month = Jun,
journal = {arXiv preprint arXiv:2205.01068},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{openai2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@book{baeza1999modern,
  author = {Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier and others},
  title = {Modern Information Retrieval},
  volume = {463},
  publisher = {ACM Press},
  year = {1999},
  address = {New York}
}

@inproceedings{lewis20rag,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}


@online{opentable-chatgpt,
  author = {OpenTable},
  title = {NEW: ChatGPT restaurant recs, powered by OpenTable},
  year = {2023},
  month = {March 23},
  url = {https://www.opentable.com/blog/chatgpt/},
}


@misc{izacard2022atlas,
      title={Atlas: Few-shot Learning with Retrieval Augmented Language Models}, 
      author={Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
      year={2022},
      eprint={2208.03299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chatgpt-plugin,
author={{OpenAI}},
title={{ChatGPT plugins}},
howpublished={\url{https://openai.com/blog/chatgpt-plugins}},
month=mar,
year={2023},
}

@inproceedings{crankshaw2017clipper,
  title={Clipper: A $\{$Low-Latency$\}$ online prediction serving system},
  author={Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and Franklin, Michael J and Gonzalez, Joseph E and Stoica, Ion},
  booktitle={14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
  pages={613--627},
  year={2017}
}

@article{tfserving,
  title={Tensorflow-serving: Flexible, high-performance ml serving},
  author={Olston, Christopher and Fiedel, Noah and Gorovoy, Kiril and Harmsen, Jeremiah and Lao, Li and Li, Fangwei and Rajashekhar, Vinu and Ramesh, Sukriti and Soyke, Jordan},
  journal={arXiv preprint arXiv:1712.06139},
  year={2017}
}

@inproceedings{vLLM,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  year={2023},
address = {Koblenz, Germany},
  month=Oct,
}

@inproceedings {Orca,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = "{Orca: A Distributed Serving System for {Transformer-Based} Generative Models}",
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI '22)},
year = {2022},
address = {Carlsbad, CA},
month = jul
}

@article{chunk_prefill_sarathi,
  title={SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills},
  author={Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2308.16369},
  year={2023},
  month=Aug
}


@article{agrawal2024taming,
  title={Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={Proceedings of 18th USENIX Symposium on Operating Systems Design and Implementation, 2024, Santa Clara},
  year={2024}
}

@inproceedings{patel2024splitwise,
author = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, Íñigo and Maleki, Saeed and Bianchini, Ricardo},
title = {Splitwise: Efficient generative LLM inference using phase splitting},
booktitle = {ISCA},
year = {2024},
month = {June},
abstract = {Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs. Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memory-intensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. Despite state of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost.

Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today’s GPU clusters. Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power. Compared to current designs, Splitwise clusters achieve up to 1.4x higher throughput at 20% lower cost. Alternatively, they can deliver 2.35x more throughput under the same power and cost budgets.},
url = {https://www.microsoft.com/en-us/research/publication/splitwise-efficient-generative-llm-inference-using-phase-splitting/},
}

@article{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      month = Oct,
      journal = {arXiv preprint arXiv:2310.06825},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{llama3,
  author = {Meta},
  title = {Meta Llama 3},
  howpublished = {\url{https://llama.meta.com/llama3/}},
  year = {2024},
}

@misc{llama3.1,
  author = {Meta},
  title = {Introducing Llama 3.1: Our most capable models to date},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3-1/}},
  year = {2024},
}

@inproceedings{
opro,
title={Large Language Models as Optimizers},
author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V Le and Denny Zhou and Xinyun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Bb4VGOWELI}
}

@misc{fireworksai2024api,
  author = {Fireworks AI},
  title = {API Reference - Fireworks AI},
  year = {2024},
  howpublished = {\url{https://docs.fireworks.ai/api-reference/introduction}},
  note = {Accessed: 2025-02-10}
}


@misc{nvidia2023fastertransformer,
  author = {NVIDIA},
  title = {FasterTransformer},
  howpublished = {\url{https://github.com/NVIDIA/FasterTransformer}},
  year = {2023},
}

@inproceedings{aminabadi2022deepspeed,
  title={DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2022},
  organization={IEEE},
    address = {Dallas, Texas},
  month = Nov
}


@misc{sheng2023flexgen,
      title={FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}, 
      author={Ying Sheng and Lianmin Zheng and Binhang Yuan and Zhuohan Li and Max Ryabinin and Daniel Y. Fu and Zhiqiang Xie and Beidi Chen and Clark Barrett and Joseph E. Gonzalez and Percy Liang and Christopher Ré and Ion Stoica and Ce Zhang},
      year={2023},
      eprint={2303.06865},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{li2023alpaserve,
author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
title = {{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
address = {Boston, MA},
month = jul
}

@article{auglm_survey,
  title={Augmented language models: a survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}

@inproceedings{wei2022chain,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2024},
address = {New Orleans, LA},
booktitle = { Proceedings of the 36th International Conference on Neural Information Processing Systems},
month = Nov,
}

@inproceedings{
yao2022react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
month=Apr,
address={Kigali, Rwanda},
url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@article{parisi2022talm,
  title={Talm: Tool augmented language models},
  author={Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  journal={arXiv preprint arXiv:2205.12255},
  year={2022}
}

@inproceedings{hao2023toolkengpt,
 author = {Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
 booktitle = {Advances in Neural Information Processing Systems 36},
 title = {ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8fd1a81c882cd45f64958da6284f4a3f-Paper-Conference.pdf},
 year = {2023},
month = Dec,
address = {New Orleans, Louisiana}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023},
month = Feb,
}

@article{guo2024stabletoolbench,
      title={StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models},
      author={Guo, Zhicheng and Cheng, Sijie and Wang, Hao and Liang, Shihao and Qin, Yujia and Li, Peng and Liu, Zhiyuan and Sun, Maosong and Liu, Yang},
      journal = {arXiv preprint arXiv:2403.07714},
      year={2024},
      month={Mar},
      eprint={2403.07714},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2024agents,
      title={More Agents Is All You Need}, 
      author={Junyou Li and Qin Zhang and Yangbin Yu and Qiang Fu and Deheng Ye},
      year={2024},
month = Feb,
journal = {arXiv preprint arXiv:2402.05120},
      eprint={2402.05120},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wu2023autogen,
      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation}, 
      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},
      year={2023},
month = Oct,
journal = {arXiv preprint arXiv:2308.08155},
      eprint={2308.08155},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
month = Oct,
journal = {arXiv preprint arXiv:2307.16789},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{wang2024mint,
      title={MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback}, 
      author={Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},
      booktitle = {The Twelfth International Conference on Learning Representations},
      year={2024},
    month = May,
address = {Vienna, Austria},
}

@article{agentHospital,
      title={Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents}, 
      author={Junkai Li and Siyu Wang and Meng Zhang and Weitao Li and Yunghwei Lai and Xinhui Kang and Weizhi Ma and Yang Liu},
      year={2024},
month = May,
journal = {arXiv preprint arXiv:2405.02957},
      eprint={2405.02957},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{juravsky2024hydragen,
      title={Hydragen: High-Throughput LLM Inference with Shared Prefixes}, 
      author={Jordan Juravsky and Bradley Brown and Ryan Ehrlich and Daniel Y. Fu and Christopher Ré and Azalia Mirhoseini},
      year={2024},
month = May,
journal = {arXiv preprint arXiv:2402.05099},
      eprint={2402.05099},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks},
  year={2021},
  month = Dec,
}



@inproceedings{
vicuna_share_gpt,
title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
month = Dec,
address = {New Orleans, Louisiana},
url={https://openreview.net/forum?id=uccHPGDlao}
}

@misc{4o-mini,
  author = {OpenAI},
  title = {GPT-4o mini: Advancing Cost-Efficient Intelligence},
  year = {2024},
  url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
  note = {Accessed: 2025-02-10}
}

@misc{openaiapi,
  author = {OpenAI},
  title = {openai-python},
  year = {2024},
  howpublished = {\url{https://github.com/openai/openai-python}},
  note = {Accessed: 2025-02-10}
}


@misc{gemini,
    author = {Pichai, Sundar and Hassabis, Demis},
    title = {Introducing Gemini: our largest and most capable AI model},
    year = {2023},
    howpublished = {\url{https://blog.google/technology/ai/google-gemini-ai/}}
}

@article{GAREY1976237,
title = {Some simplified NP-complete graph problems},
journal = {Theoretical Computer Science},
volume = {1},
number = {3},
pages = {237-267},
year = {1976},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(76)90059-1},
url = {https://www.sciencedirect.com/science/article/pii/0304397576900591},
author = {M.R. Garey and D.S. Johnson and L. Stockmeyer},
}

@article{shridhar2020alfworld,
  title={Alfworld: Aligning text and embodied environments for interactive learning},
  author={Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew},
  journal={arXiv preprint arXiv:2010.03768},
  year={2021},
month = Mar,
}
@misc{tts_sunoai2023bark,
  author = {Suno AI},
  title = {Bark: Text-to-Speech Model},
  howpublished = {\url{https://github.com/suno-ai/bark}},
  year = {2023},
}

@misc{stable_diffusion,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{rapidapi,
  author = {{RapidAPI}},
  title = {{RapidAPI: World’s Largest API Hub

}},
  howpublished = {\url{https://rapidapi.com/}},
  year = {2023},
}
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{chatgpt,
    author = {OpenAI},
    title = {Introducing ChatGPT},
    year = {2022},
    howpublished = {\url{https://openai.com/index/chatgpt}}
}

@inproceedings{Gu_2022,
   title={Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions},
   url={http://dx.doi.org/10.18653/v1/2022.acl-long.524},
   DOI={10.18653/v1/2022.acl-long.524},
   booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Gu, Jing and Stefani, Eliana and Wu, Qi and Thomason, Jesse and Wang, Xin},
   year={2022} }


@InProceedings{huang22language,
  title = 	 {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author =       {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9118--9147},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/huang22a/huang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/huang22a.html},
  abstract = 	 {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.}
}


@article{finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@article{nijkamp2023codegen2,
      title={CodeGen2: Lessons for Training LLMs on Programming and Natural Languages}, 
      author={Erik Nijkamp and Hiroaki Hayashi and Caiming Xiong and Silvio Savarese and Yingbo Zhou},
      year={2023},
month = Jul,
journal = {arXiv preprint arXiv:2305.02309},
      eprint={2305.02309},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hu2023look,
      title={Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning}, 
      author={Yingdong Hu and Fanqi Lin and Tong Zhang and Li Yi and Yang Gao},
      year={2023},
      eprint={2311.17842},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@misc{brysbaert_2019,
 title={How many words do we read per minute? A review and meta-analysis of reading rate},
 url={osf.io/preprints/psyarxiv/xynwg},
 DOI={10.31234/osf.io/xynwg},
 publisher={PsyArXiv},
 author={Brysbaert, Marc},
 year={2019},
 month={Apr}
}

@inproceedings{ma2015haptic,
  title={Haptic keyclick feedback improves typing speed and reduces typing errors on a flat keyboard},
  author={Ma, Zhaoyuan and Edge, Darren and Findlater, Leah and Tan, Hong Z},
  booktitle={2015 IEEE World Haptics Conference (WHC)},
  pages={220--227},
  year={2015},
  organization={IEEE}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}




@misc{vllm-pr-for-prefix-sharing,
 title={Automatic Prefix Caching},
author={Moore, Sage and Li, Zhouhan},
year={2024},
month={March},
 howpublished={\url{https://github.com/vllm-project/vllm/issues/2614}},
}

@misc{routellm,
      title={RouteLLM: Learning to Route LLMs with Preference Data},
      author={Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
      year={2024},
      eprint={2406.18665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18665},
}

@article{textgrad,
      title={TextGrad: Automatic "Differentiation" via Text},
      author={Mert Yuksekgonul and Federico Bianchi and Joseph Boen and Sheng Liu and Zhi Huang and Carlos Guestrin and James Zou},
      year={2024},
      eprint={2406.07496},
      archivePrefix={arXiv}
}
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}