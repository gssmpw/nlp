%\input{fig-overall-1}
%\input{fig-overall-2}
% \input{fig-all-workflows-cost}
% \input{fig-all-workflows-latency}
\section{Evaluation Results}
\label{sec:results}

We evaluate Cognify across six types of tasks: question-answering (HotPotQA~\cite{yang2018hotpotqa}), text-to-SQL (BIRD~\cite{gao2023texttosql}), data visualization (MatPlotAgent benchmark~\cite{datavis}), financial analysis (FinRobot \cite{finrobot}), code generation (HumanEval~\cite{humaneval}), and BigBench~\cite{bigbench}. The quality evaluators used in these tasks, respectively, are F1 score (text similarity to the ground truth answer), pass rate (exactly correct SQL results out of all test data), LLM-as-a-Judge, a combination of F1-score and LLM-as-a-judge, pass rate (exactly correct code results out of all test data), and exact match rate (for word sorting and object counting). Out of these workflows: FinRobot employs a leader-agent that routes requests to other different workers; text-to-SQL and data visualization both loop over model calls for iterative refinement; question-answering involves multi-step retrieval-augmented generation (RAG) \cite{lewis20rag}. 
%The exact details about each original workflow can be found in Appendix~\ref{XXX}. 

\input{tbl-bbh}

We compare \sysname\ with three baselines: the original workflows from open-source repositories, DSPy~\cite{DSPy-repo}, and Trace~\cite{Trace}. 
Unless otherwise specified, we give \sysname, DSPy, and Trace all a search budget of 64. \sysname\ chooses from two language models for its model-step cog: GPT-4o-mini~\cite{4o-mini} (via OpenAI API endpoints~\cite{openaiapi}) and Llama-8B~\cite{llama3.1} (via Fireworks API endpoints~\cite{fireworksai2024api}). As the baselines do not have automated model selection capabilities, we run each of them fully on GPT-4o-mini or all on Llama-8B.
%For the original workflow, we plot one point that uses GPT-4o for each step in the workflow and another point that uses GPT-4o-Mini. DSPy and Trace do not natively support model selection during optimization. Hence, we only plot one point for each, using GPT-4o. Cognify's optimization process generates a set of points on the quality-cost-latency Pareto frontier. Since multiple results are generated in a single optimization run, we plot each point in the frontier.

\subsection{Overall Workflow Optimization Results}

We first present the overall results across all workloads, highlighting the first four in Figure~\ref{fig-biggrid}.
Overall, \sysname\ improves generation quality (by up to 2.8\x{}), reduces execution cost (by up to 10\x{}), and execution latency (by up to 2.7\x{}) compared to the original workflows across all the workloads by pushing their Pareto frontiers. 
\sysname\ also improves quality, cost, and latency over DSPy and Trace across workloads.

Comparing across workloads, \sysname\ has the largest improvements on HotpotQA (2.8\x{} over the original workflow using 4o-mini) and data visualization (38\% higher than the original workflow using only 4o-mini or only Llama 3.1-8B). \sysname\ achieves its quality improvement for HotpotQA due to its good selection of few-shot examples at various edges. \sysname\ achieves its benefit for data visualization from inserting chain-of-thought reasoning at the beginning of the workflow and planning steps during the initial code generation phase. %\sysname\ is also able to achieve 10\x{} cost reduction by only including chain-of-thought in the final refinement step and avoiding it for code generation, with no loss in quality. 
\sysname\ has the largest cost saving on text-to-SQL (10\x{} cheaper) by introducing reasoning; the original workflow included a significant number of retries that \sysname\ avoids with improved generation in earlier steps of the workflow. \sysname\ has the largest latency cut for FinRobot (2.5\x{} faster) because the model selection cog chooses the faster Llama-8B model for certain steps. 


For all workloads, DSPy performs better than Trace, although still worse than \sysname. This is because DSPy generates few-shot examples during its optimization process, whereas Trace primarily relies on rewriting user-annotate code blocks. While code rewriting may be effective to generate Trace is especially ineffective for the Text-2-SQL and FinRobot workloads, as its optimized workflow yields 0 quality, \ie, no generated SQL queries / analyses are correct. This is because Trace has a strong tendency to overfit to specific training examples or generate erroneous code rewrites.
DSPy is also not as effective on the FinRobot workload, as their optimized workflows have worse results than the original workflow running GPT-4o-mini. DSPy is unable to combine complex prompt optizations (\eg reasoning) with structured output generation, which is required by the FinRobot task.  
%Trace is unable to generate high-quality output and tends to overfit to specific examples due to its LLM-based optimization approach. Cognify's improvement largely comes from choosing strong few-shot examples and model selection.

%On \textbf{text-to-SQL}, Cognify demonstrates a 24\% quality improvement over the original workflow. This quality improvement comes with just 0.2\x the cost. Cognify also finds a 19\% improvement at 0.1\x the cost and 0.6\x the latency of the original. When compared with DSPy, Cognify demonstrates a 16\% improvement in quality and at comparable quality, a 10\% reduction in cost. 
%Trace is unable to generate any valid results on this workload because it overfits during the optimzation process and generates invalid code rewrites. The improvement in text-to-SQL generation comes primarily from ensembling various steps and aggregating their results with LLM-based consensus, as well as model selection. 

%On \textbf{data-visualization}, Cognify achieves a 36\% quality improvement over the original workflow when the original uses GPT-4o-mini and a 3\% improvement when the original uses GPT-4o. On this workflow, using GPT-4o will yield a point on the Pareto frontier. However, DSPy and Trace both experience degradation in quality, even when using GPT-4o \reyna{we used 4o for DSPy and Trace, right?}, whereas Cognify is able to generate a slight improvement due to \fixme{few shot? cot?}. Cognify also finds an optimization with 0.1\x the cost and 0.8\x the latency of the original workflow using 4o-mini and performs at 5\% higher quality.

%On \textbf{financial analysis}, Cognify improves the quality by 19\% while operating at 0.9\x the cost and 0.6\x the latency of the original workflow. Here, the original workflow is only using GPT-4o-mini due to \fixme{add 4o results to the sheet or need an explanation here}. With a 13\% overall quality improvement, Cognify can cut costs down to 0.5\x the original and latency down to 0.4\x. 
%On this workflow, DSPy and Trace \fixme{add these baselines to the sheet or need an explanation here}. Cognify finds that \fixme{few shot? cot? ensemble?} is responsible for the improvement across all three axes.

\input{fig-budget-layer-metrics}
\input{fig-search-trace}
% \input{fig-budget-sensitivity}

We use three additional benchmarks, code generation from HumanEval, word sorting from BigBench, and object counting from BigBench, to demonstrate Cognify's ability to focus on a single objective, as DSPy and Trace are both designed for quality improvement. The evaluator in these tasks is pass rate (either the workflow output is correct or incorrect) or exact match. This forces a stricter quality expectation. \sysname\ improves the accuracy of code gen over the original workflow by 30\% and over Trace and DSPy by 4\%. % to a strong choice of few-shot examples. We also evaluate Cognify on two tasks from the Big Bench Hard (BBH) dataset. 
On BBH-object counting, \sysname\ demonstrates an impressive 95\% accuracy, which is 9\% higher than Trace and 2.2\x{}\ higher than DSPy. On BBH-word sorting, \sysname\ is near-perfect, achieving 99\% accuracy, which is 11\% higher than Trace. Both DSPy (on word sorting) and the original workflow (on word sorting and objecting counting) are unable to generate the answer that matches exactly with the expected output. \sysname's code rewriter is primarily responsible for the improvement in quality on these workflows.

\subsection{Detailed Evaluation Results}

We now explain \sysname's benefits and sensitivity with more detailed experiments.

\subsubsection{Search Effectiveness}


\input{fig-input-sensitivity}
\paragraph{Comparison to grid search.}
To evaluate the effectiveness of \sysname's search, we perform an exhaustive grid search of 4096 configurations for the HotpotQA question-answering workflow.
Figure~\ref{fig-searchtrace} plots the grid search results and \sysname's 128 iteration results. As indicated by the heatmap, \sysname\ quickly moves towards the highest quality and lowest cost in approximately 20 iterations. With just 1/32 of the grid search budget, \sysname\ finds 5 new points on the Pareto frontier and misses only one point found by grid search. 

\paragraph{Layering and search budgets.}

We validate our hypothesis that an increasing budget should correspond to an increase in the number of layers used by \sysname. Figure~\ref{fig-layering-influence} shows the effect of using different number of layers with different budgets across all metrics on the text-to-SQL workload. When the budget is small (\ie, 16 iterations), a single layer will yield the best results due to faster convergence to a local optimum. As budget increases, layering allows a more diverse exploration of the search space. At 64 iterations, a 2-layer approach performs the best, and at 128 iterations (our maximum budget used for experiments), 3 layers is superior. This pattern holds across quality, cost, and latency independently.

\paragraph{Sensitivity to training input size.}
We evaluate the effectiveness of \search\ with varying training input sizes on the FinRobot workload in Figure~\ref{fig-input-sensitivity}. With just 6 or more examples, \search\ is able to find higher quality optimized workflows. In less than 50 examples, \search\ generates a 13\% improvement in quality and a 19\% improvement in less than 100 examples. This mirrors the size of typical gold datasets in production workflows \cite{kulkarni25gold}. The only instance in which the search degrades in quality relative to the original workflow is when the training dataset contains only 3 examples, which is an unlikely scenario for workflow developers.

\input{tbl-comparison}
\input{fig-ablation}
\subsubsection{Ablation Study}
To understand where \sysname's benefits come from, we evaluate the effect of different techniques by adding one at a time on the Text-to-SQL workload, as shown in Figure~\ref{fig-ablation-study}. %For each setting, we allow up to 128 maximum number of evaluations, and we focus on the best quality found so far as the optimization continues. Below we explain the effect of each technique.
The initial version is a non-hierarchical search approach that places all cogs in a single layer.
We then incorporate our adaptive hierarchical layer approach and budget distribution, which results in improved quality configurations after 24 search iterations.
Afterward, we add the per-layer chunk-based successive-halving approach, yielding additional quality improvements as seen by the green line.
Finally, we enable early exiting, achieving the best quality as shown in the red line.
%At the early stage, i.e. in 24 iterations, it benefits similarly to the non-hierarchical method from the ability to find a local optimal fast. Beyond that, it continues the search with 2-layers from $32^{th}$ iterations until the budget exhausts. With separated layers, the optimizer can avoid from trapping in the local optimal, achieving  $0.8\%$ increase in the pass rate compared to non-hierarchical method. Next, we add the successive-halving based budget re-allocation. It effectively assigns more resources to exploit promising sub-regions in the search space. With this uneven budget assignment, \sysname achieved another $0.1\%$ increase in the quality. Lastly, we apply early-existing. It effectively re-distribute the budget across different stages during the optimization: we start 2-layer search at $29^th$ iteration and 3-layer search at $59^th$ iteration, which gives the last $0.1\%$ improvement in the pass rate.

\subsubsection{Search Time and Cost}

We compare the optimization cost and time of \sysname\ and DSPy in Figure~\ref{fig-opt-time-cost}. %For all workloads, we used DSPy's MIPROv2 optimizer. 
\sysname\ completes its optimization in 1.7\x{} to 2.5\x{} less time than DSPy due to \sysname's efficient use of the overall training budget and its parallel search mechanism. %Furthermore, \sysname\ ranges between \$1.50 to \$10 for its optimization. 
While DSPy's optimizer can be marginally cheaper in \$ cost for smaller workflows, its cost bloats for more complex workflows. For example, the cost of optimizing text-to-SQL with DSPy is \$24, which is 2.4\x{} more expensive than \sysname.


%\boldpara{Impacts of different search techniques.}

%\boldpara{Impacts of different cogs}
%\input{fig-cog-study}

% \subsection{LLM-as-Optimizer Microbenchmarks}
% \input{fig-llm-micro}