\section{Related Work}
\label{sec:related_work}
  
  In this section, remote sensing semantic segmentation methods based on deep learning are reviewed. Notably, the application of the vision language model for remote sensing semantic segmentation tasks is discussed. Furthermore, the semantic segmentation quality assessment metrics are also elaborated.

\subsection{Remote Sensing Image Semantic Segmentation}\label{section RSss}
Deep learning methods have substantially improved the performance of semantic segmentation on remote sensing images. Among them, fully convolutional networks (FCNs) is a widely used architecture \cite{9076866}, enabling pixel-level spatial segmentation. To enhance the performance of FCNs on RSI, studies have incorporated techniques such as multi-scale feature fusion \cite{chen2017deeplab}, and the integration of auxiliary information (e.g., infrared images, digital surface models) \cite{kniaz2019deep}. Another widely adopted architecture is the encoder-decoder structure, exemplified by the renowned U-Net \cite{ronneberger2015u}. U-Net-like methods \cite{wang2022unetformer,rs16162930,9487010,wang2022novel} effectively combines deep and shallow features through skip connections, exhibiting excellent performance. To address the issue of easy loss of small-target information in RSI, multi-scale feature fusion-based methods have also gained traction. Representative models like FPN \cite{song2020semantic}, PSPN \cite{zhao2017pyramid}, and RefineNet \cite{lin2017refinenet} leverage the inherent pyramid structure of deep networks to combine features at different scales, preserving detailed information and achieving robust results on small-target segmentation tasks \cite{guo2020multi,cui2020sanet}.
Inspired by the success of Transformer in natural language processing, Transformer-based segmentation models have also emerged recently. These methods directly divide the image into patches, feeding them into the Transformer module for segmentation, fully exploiting Transformer's global modeling capability. Exemplary works like ResT \cite{zhang2021rest} and Segmenter \cite{strudel2021segmenter} have also demonstrated promising performance.

Recent work has embarked on exploring Vision-Language Models (VLMs) for RSI that can be fine-tuned for semantic segmentation. Prior works leverage the contrastive learning strategy of image-text pairs in RS semantic segmentation\cite{10005113}. RingMo\cite{sun2022ringmo} is the first generative self-supervised RS foundation model, pre-trained on two million RS images, achieving SOTA on four downstream tasks, including semantic segmentation. Cha et al. \cite{cha2023billion} introduce the first billion-scale foundation model in the RS field which achieves the best performance on the Potsdam and LoveDA \cite{wang2021loveda} datasets. Moreover, more remote sensing  VLMs\cite{kuckreja2024geochat,liu2024remoteclip} demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations.
\subsection{Semantic Segmentation Quality Assessment}\label{section ml}
\subsubsection{Natural Image Semantic Segmentation Quality Assessment Methods}\label{section nml}
Segmentation quality assessment methods are mainly divided into subjective methods and objective methods depending on whether segmentation results are evaluated by human or algorithm. 
A few approaches evaluate from a subjective aspect\cite{chen2019visual}, providing human opinions to examine whether objective measures coincide with the human visual system. The objective assessment focus on measure the degree that semantic segmentation results are close to ground truth, which can be divided into region-based, contour-based and mixture metrics. Region-based metrics count the pixels correctly classified in segmentation results according to ground truth. 
The confusion matrix is an important tool for measuring the accuracy of segmentation, including four indicators: True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN). The secondary indicators Accuracy, Precision, Recall and Specificity, as well as the tertiary indicator F1-Score, are widely applied in semantic segmentation competitions.
Contour-based metrics focus on object boundaries such as  mean distance (MD) \cite{csurka2013good} while mixture metrics consider both regions and contours simultaneously \cite{cheng2021boundary}.


\subsubsection{Remote Sensing Semantic Segmentation Quality Assessment Methods}\label{section rsml}
A wide variety of metrics for examining remote sensing image segmentation quality are proposed, generally categorized into supervised and unsupervised methods. Some supervised metrics are closely related to the objective assessment metrics discussed in Section \ref{section nml}. Others\cite{ZHANG201573,article222,LIU2012144,SU2017256,article,YANG2015186} evaluate the match between reference polygons and the computer-generated segment results, where multiple segmentation with different parameters combinations are evaluated to minimize segmentation discrepancies for further analysis. These metrics are classified into three categories: Under-Segmentation (US) metrics, Over-Segmentation (OS) metrics and Combined (UO) metrics. For unsupervised methods, some early approaches focus on measuring inter-class heterogeneity (IHE) and intra-class homogeneity (IHO) of image segments to analyze segmentation quality \cite{article31666, WOODCOCK1987311, articleroclv, Zhihua}. 
With the development of machine-learning, some researchers predict segmentation accuracy such as Kappa coefficient\cite{xia2015quality} from images. Fractal \cite{chen2019effects} extracts local morphological dimensions and global multifractal structures, while Convolutional \cite{wei2021effects} employs convolutional sparse coding to construct regressor for assessing segmentation quality. However, methods based on intra-class homogeneity struggle with the inherent dispersion in RSI. Machine learning approaches, constrained to ROI-based pixel classification, fail to generalize to deep learning methods. Overall, unsupervised quality assessment for RS semantic segmentation still requires significant advancement.
\begin{figure*}
\centering %表示居中
\includegraphics[height=8cm,width=18cm]{figure/V3_nosamacliprs.pdf}

\caption{ \textbf{Illustration of Our Framework.} High-level semantic features are extracted from CLIP-RS visual encoder, while deep segmentation features are obtained from the RS semantic segmentation model and simplified via average pooling. The features from both branches are fused using a cross-gating block and then input into a quality prediction head to generate the quality score.}
%图片的名称
\label{figure1}
%图片的标签，用于文章中的引用，注意到标签的数字与实际文章显示的数字可能不同
\end{figure*}