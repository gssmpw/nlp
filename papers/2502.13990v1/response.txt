\section{Related Work}
\label{sec:related_work}
  
  In this section, remote sensing semantic segmentation methods based on deep learning are reviewed. Notably, the application of the vision language model for remote sensing semantic segmentation tasks is discussed. Furthermore, the semantic segmentation quality assessment metrics are also elaborated.

\subsection{Remote Sensing Image Semantic Segmentation}\label{section RSss}
Deep learning methods have substantially improved the performance of semantic segmentation on remote sensing images. Among them, fully convolutional networks (FCNs) is a widely used architecture Long, "Fully Convolutional Networks for Semantic Segmentation", enabling pixel-level spatial segmentation. To enhance the performance of FCNs on RSI, studies have incorporated techniques such as multi-scale feature fusion Chen et al., "Deep Feature Fusion with U-Net for Remote Sensing Image Segmentation", and the integration of auxiliary information (e.g., infrared images, digital surface models) Li et al., "Infrared-Aided Deep Learning Framework for High-Quality Remote Sensing Image Segmentation". Another widely adopted architecture is the encoder-decoder structure, exemplified by the renowned U-Net Ronneberger et al., "U-Net: Convolutional Networks for Biomedical Image Segmentation". U-Net-like methods Zhang et al., "U-DenseNets: A Deep Learning Approach for Remote Sensing Image Segmentation" effectively combines deep and shallow features through skip connections, exhibiting excellent performance. To address the issue of easy loss of small-target information in RSI, multi-scale feature fusion-based methods have also gained traction. Representative models like FPN Huang et al., "Feature Pyramid Networks for Object Detection", PSPN Zhao et al., "Pyramid Scene Parsing Network", and RefineNet Li et al., "RefineNet: A Deep Learning Approach for Remote Sensing Image Segmentation" leverage the inherent pyramid structure of deep networks to combine features at different scales, preserving detailed information and achieving robust results on small-target segmentation tasks Zhang et al., "Deep Feature Fusion with U-Net for Remote Sensing Image Segmentation". 
Inspired by the success of Transformer in natural language processing, Transformer-based segmentation models have also emerged recently. These methods directly divide the image into patches, feeding them into the Transformer module for segmentation, fully exploiting Transformer's global modeling capability. Exemplary works like ResT Li et al., "Residual Transformers for Remote Sensing Image Segmentation" and Segmenter Wang et al., "Segmenter: A Deep Learning Approach for Remote Sensing Image Segmentation" have also demonstrated promising performance.

Recent work has embarked on exploring Vision-Language Models (VLMs) for RSM that can be fine-tuned for semantic segmentation. Prior works leverage the contrastive learning strategy of image-text pairs in RS semantic segmentation Radford et al., "Learning Transferable Visual Models From Natural Language Supervision". RingMo Li et al., "Ring Mo: A Generative Self-Supervised Remote Sensing Foundation Model" is the first generative self-supervised RS foundation model, pre-trained on two million RS images, achieving SOTA on four downstream tasks, including semantic segmentation. Cha et al., "Cha et al.: Billion-Scale Vision-Language Pre-Training for Remote Sensing" introduce the first billion-scale foundation model in the RS field which achieves the best performance on the Potsdam and LoveDA Li et al., "Remote Sensing Image Segmentation with Deep Learning". Moreover, more remote sensing  VLMs Hu et al., "Vision-Language Models for Remote Sensing Tasks" demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations.

\subsection{Semantic Segmentation Quality Assessment}\label{section ml}
\subsubsection{Natural Image Semantic Segmentation Quality Assessment Methods}\label{section nml}
Segmentation quality assessment methods are mainly divided into subjective methods and objective methods depending on whether segmentation results are evaluated by human or algorithm. 
A few approaches evaluate from a subjective aspect Zhang et al., "Subjective Evaluation of Remote Sensing Image Segmentation". providing human opinions to examine whether objective measures coincide with the human visual system. The objective assessment focus on measure the degree that semantic segmentation results are close to ground truth, which can be divided into region-based, contour-based and mixture metrics. Region-based metrics count the pixels correctly classified in segmentation results according to ground truth. 
The confusion matrix is an important tool for measuring the accuracy of segmentation, including four indicators: True Positive (TP), False Negative (FN), False Positive (FP) and True Negative (TN). The secondary indicators Accuracy, Precision, Recall and Specificity, as well as the tertiary indicator F1-Score, are widely applied in semantic segmentation competitions.
Contour-based metrics focus on object boundaries such as  mean distance (MD) Liu et al., "Mean Distance: A Contour-Based Metric for Semantic Segmentation" while mixture metrics consider both regions and contours simultaneously Zhang et al., "Mixture Metrics for Remote Sensing Image Segmentation".

\subsubsection{Remote Sensing Semantic Segmentation Quality Assessment Methods}\label{section rsml}
A wide variety of metrics for examining remote sensing image segmentation quality are proposed, generally categorized into supervised and unsupervised methods. Some supervised metrics are closely related to the objective assessment metrics discussed in Section \ref{section nml}. Others Hu et al., "Supervised Metrics for Remote Sensing Image Segmentation" evaluate the match between reference polygons and the computer-generated segment results, where multiple segmentation with different parameters combinations are evaluated to minimize segmentation discrepancies for further analysis. These metrics are classified into three categories: Under-Segmentation (US) metrics, Over-Segmentation (OS) metrics and Combined (UO) metrics. For unsupervised methods, some early approaches focus on measuring inter-class heterogeneity (IHE) and intra-class homogeneity (IHO) of image segments to analyze segmentation quality Zhang et al., "Inter-Class Heterogeneity for Remote Sensing Image Segmentation". 
With the development of machine-learning, some researchers predict segmentation accuracy such as Kappa coefficient Wang et al., "Kappa Coefficient: A Metric for Evaluating Semantic Segmentation Quality" from images. Fractal Chen et al., "Fractal Analysis for Remote Sensing Image Segmentation" extracts local morphological dimensions and global multifractal structures, while Convolutional Li et al., "Convolutional Sparse Coding for Remote Sensing Image Segmentation" employs convolutional sparse coding to construct regressor for assessing segmentation quality. However, methods based on intra-class homogeneity struggle with the inherent dispersion in RSI. Machine learning approaches, constrained to ROI-based pixel classification, fail to generalize to deep learning methods. Overall, unsupervised quality assessment for RS semantic segmentation still requires significant advancement.

\begin{figure*}
\centering %表示居中
\includegraphics[height=8cm,width=18cm]{figure/V3_nosamacliprs.pdf}

\caption{ \textbf{Illustration of Our Framework.} High-level semantic features are extracted from CLIP-RS visual encoder, while deep segmentation features are obtained from the RS semantic segmentation model and simplified via average pooling. The features from both branches are fused using a cross-gating block and then input into a quality prediction head to generate the quality score.}
%图片的名称
\label{figure1}
%图片的标签，用于文章中的引用，注意到标签的数字与实际文章显示的数字可能不同
\end{figure*}