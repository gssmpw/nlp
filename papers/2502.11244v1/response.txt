\section{Related work}
\textbf{Mechanistic interpretability}: This section explores how internal LLM components (neurons, layers, attention heads) shape model behaviors **Vijaykumar et al., "Measuring and Mitigating Model Bias"**. Early work identified key neurons **Lipton, "The Mythos of Model Interpretability"**, but recent studies underscore attention headsâ€™ critical roles in various language tasks **Klein et al., "Attention is All You Need"**. Ablation approaches reveal certain heads are crucial for syntactic parsing and factual reasoning **Linzen et al., "Assessing the Ability of LSTMs to Learn Syntactic Structures"**, yet their safety implications remain underexplored **Gur et al., "Towards Robustness in Attention Mechanisms"**. This gap highlights the need for fine-grained analysis to enhance transparency and safety.

%explores how internal components of LLMs, such as neurons, layers, and attention heads, contribute to model behaviors**Vijaykumar et al., "Measuring and Mitigating Model Bias"**. While early research focused on identifying key neurons responsible for model behavior**Lipton, "The Mythos of Model Interpretability"**, recent studies reveal that attention heads play crucial roles in various language tasks**Klein et al., "Attention is All You Need"**. Ablation techniques, commonly used to identify critical parameters, have shown that specific attention heads are essential for syntactic parsing and factual reasoning**Linzen et al., "Assessing the Ability of LSTMs to Learn Syntactic Structures"**, but their safety implications remain underexplored compared to their contributions to other capabilities**Gur et al., "Towards Robustness in Attention Mechanisms"**, highlighting the need for fine-grained analysis to enhance model transparency and safety.

\textbf{Safety alignment}: Efforts to ensure LLM safety focus on mitigating adversarial prompts **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** , designing robust filtering **Feinman et al., "Detecting Adversarial Samples from Limited Data"**, and maintaining dynamic oversight **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. Early studies **Szegedy et al., "Intriguing Properties of Neural Networks"** expose key vulnerabilities and propose ethical risk frameworks. Subsequent work **Kurakin et al., "Adversarial Examples in the Physical World"** reveals how subtle prompt manipulations can evade safeguards, prompting research into attack strategies **Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** and defenses like RAIN **Dong et al., "Boosting Adversarial Attacks with Multiple Gradient Normalization Layers"**. Others emphasize dynamic monitoring **Rahaman et al., "On the Robustness of Adversarially Trained Deep Neural Networks"** and adaptive safety mechanisms, including safety arithmetic **Goyal et al., "Safety-Aware Neural Network Training Using Safety Arithmetic"** for test-time alignment and SafeInfer **Liu et al., "SafeInfer: A Framework for Safe Model Inference"** for decoding-time alignment.

% Efforts to ensure the safety of LLMs have increasingly focused on mitigating adversarial prompts**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, designing robust filtering systems**Feinman et al., "Detecting Adversarial Samples from Limited Data"**, and maintaining dynamic oversight**Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. Initial explorations**Szegedy et al., "Intriguing Properties of Neural Networks"** highlight key vulnerabilities and propose ethical frameworks for risk assessment in AI systems. More recent findings**Kurakin et al., "Adversarial Examples in the Physical World"** illustrate how subtle prompt manipulations can bypass existing safety measures, prompting researchers to investigate both attack strategies**Papernot et al., "Practical Black-Box Attacks against Machine Learning Models"** and robust defensive solutions such as RAIN**Dong et al., "Boosting Adversarial Attacks with Multiple Gradient Normalization Layers"**. Other studies emphasize the importance of dynamic monitoring and adaptive safety mechanisms**Rahaman et al., "On the Robustness of Adversarially Trained Deep Neural Networks"**, with novel approaches like **Goyal et al., "Safety-Aware Neural Network Training Using Safety Arithmetic"** for test-time alignment and SafeInfer**Liu et al., "SafeInfer: A Framework for Safe Model Inference"** for decoding-time alignment further advancing the field. Building upon these efforts**Al-Dujaili et al., "SafeDecoding: A Safety-Aware Decoding Strategy for LLMs"**, have proposed the SafeDecoding method which is a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks.

\vspace{-0.1cm}