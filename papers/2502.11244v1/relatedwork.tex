\section{Related work}
\textbf{Mechanistic interpretability}: This section explores how internal LLM components (neurons, layers, attention heads) shape model behaviors \cite{geiger2021causal, stolfo2023a, gurnee2023finding}. Early work identified key neurons \cite{zou2023transparency, chen2024findingsafetyneuronslarge}, but recent studies underscore attention headsâ€™ critical roles in various language tasks \cite{vig2019multiscalevisualizationattentiontransformer, wu2025retrieval}. Ablation approaches reveal certain heads are crucial for syntactic parsing and factual reasoning \cite{NEURIPS2019_2c601ad9, meng2023locatingeditingfactualassociations}, yet their safety implications remain underexplored \cite{gould2023successorheadsrecurringinterpretable, wang2023interpretability}. This gap highlights the need for fine-grained analysis to enhance transparency and safety.\\
%explores how internal components of LLMs, such as neurons, layers, and attention heads, contribute to model behaviors~\cite{geiger2021causal, stolfo2023a, gurnee2023finding}. While early research focused on identifying key neurons responsible for model behavior~\cite{zou2023transparency, chen2024findingsafetyneuronslarge}, recent studies reveal that attention heads play crucial roles in various language tasks~\cite{vig2019multiscalevisualizationattentiontransformer, wu2025retrieval}. Ablation techniques, commonly used to identify critical parameters, have shown that specific attention heads are essential for syntactic parsing and factual reasoning~\cite{NEURIPS2019_2c601ad9, meng2023locatingeditingfactualassociations}. However, the safety impact of attention heads remains underexplored compared to their contributions to other capabilities~\cite{gould2023successorheadsrecurringinterpretable, wang2023interpretability}, highlighting the need for fine-grained analysis to enhance model transparency and safety.\\
\textbf{Safety alignment}: Efforts to ensure LLM safety focus on mitigating adversarial prompts \cite{xie2018mitigating}, designing robust filtering \cite{xiao2024ritfisrobustinputtesting}, and maintaining dynamic oversight \cite{kenton2024scalableoversightweakllms, wang-etal-2024-languages}. Early studies \cite{YAO2024100211} expose key vulnerabilities and propose ethical risk frameworks. Subsequent work \cite{sachdeva2025turninglogicprobing, banerjee2024unethicalinstructioncentricresponsesllms} reveals how subtle prompt manipulations can evade safeguards, prompting research into attack strategies \cite{10.5555/3692070.3694246} and defenses like RAIN \cite{li2023rainlanguagemodelsalign}. Others emphasize dynamic monitoring \cite{bhardwaj2024languagemodelshomersimpson} and adaptive safety mechanisms, including safety arithmetic \cite{hazra2024safetyarithmeticframeworktesttime} for test-time alignment and SafeInfer \cite{banerjee2024safeinfercontextadaptivedecoding}, SafeDecoding~\cite{xu2024safedecodingdefendingjailbreakattacks} for decoding-time alignment.
% Efforts to ensure the safety of LLMs have increasingly focused on mitigating adversarial prompts~\cite{xie2018mitigating}, designing robust filtering systems~\cite{xiao2024ritfisrobustinputtesting}, and maintaining dynamic oversight~\cite{kenton2024scalableoversightweakllms, wang-etal-2024-languages}. Initial explorations~\cite{YAO2024100211} highlight key vulnerabilities and propose ethical frameworks for risk assessment in AI systems. More recent findings~\cite{sachdeva2025turninglogicprobing, banerjee2024unethicalinstructioncentricresponsesllms} illustrate how subtle prompt manipulations can bypass existing safety measures, prompting researchers to investigate both attack strategies~\cite{10.5555/3692070.3694246} and robust defensive solutions such as RAIN~\cite{li2023rainlanguagemodelsalign}. Other studies emphasize the importance of dynamic monitoring and adaptive safety mechanisms~\cite{bhardwaj2024languagemodelshomersimpson}, with novel approaches like \textit{safety arithmetic}~\cite{hazra2024safetyarithmeticframeworktesttime} for test-time alignment and SafeInfer~\cite{banerjee2024safeinfercontextadaptivedecoding} for decoding-time alignment further advancing the field. Building upon these efforts~\cite{xu2024safedecodingdefendingjailbreakattacks} have proposed the SafeDecoding method which is a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks.
\vspace{-0.1cm}