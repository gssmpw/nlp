\section{Additive-error Estimator}

We start by presenting our estimator.  
In this section, we assume that the required counting range ($N$) is known in advance. 
We later show in Section~\ref{sec:dynamicN} how to dynamically increase the counting range. 
Our additive error estimator can count up to $N$ with an \emph{additive} error of at most $N\epsilon$, with probability at least $1-\delta$. 
We emphasize again that additive guarantees are uncommon in estimator algorithms, which typically provide \emph{multiplicative} error~\cite{ICE-Buckets,CASE,ANLSUpscaling,DISCO}.
We choose additive error as it allows for smaller estimators, and it is similar to the error of common frequency estimation and heavy hitter algorithms~\cite{SpaceSavings,CountMinSketch}. That is,
additive error is unavoidable even if we integrate multiplicative counters into such algorithms. Another argument for additive error is that our estimator size is independent of $N$ while the size of multiplicative error estimators cannot be independent of $N$. 

\subsection{Unit Weight Estimators}
\label{sec:single}
%\textcolor{red}{[*** So now we are using inconsistent terminology.  We are calling this a counter algorithm, where in the introduction, we call these things estimators, and use counting algorithm to refer to the cache-based counter algorithms for multiple objects.  I'm not as up on the prior literature, I'm happy to call them anything, but we should be consistent within the paper, and in an ideal world consistent with prior work.]}
%\textcolor{red}{[*** I suspect that where we use the term "probabilistic counting algorithm"  here we should always use estimator, and make clearer in the introduction that we use estimator to refer to single counters throughout].}  

A unit weight estimator supports the {\sc Probabilistic Increment (PIncrement}, or in short PI) and {\sc Query} methods. 
The {\sc PIncrement} method 
%is our probabilistic increment operation;  it 
adds one to our estimator with a (fixed) probability $p$ which we determine below.  The {\sc Query} method
estimates the number of PIs attempted by returning the value $C/p$ where $C$ is the estimator value. 
%
%Notice that $\epsilon\ge 1/2$ allows the algorithm to simply return $N\epsilon/2$; thus, we henceforth assume that $\epsilon<1/2$.
%We use a variant of the Hoeffding Bound~\cite{} that states that for a binomial random variable $C\sim \mbox{Bin}(I,p):$
%$$
%\forall \gamma>0:\Pr\brackets{|C-\mathbb E[C]|\ge I \gamma}\le 2e^{-2I\gamma}.
%$$
%\begin{align}
%&\forall \gamma\in (0,1):\Pr\brackets{|C-\mathbb E[C]|\ge\gamma\mathbb E[C]}\le 2e^{-\mathbb %E[C]\gamma^2/3}\\
%&\forall \gamma>1:\Pr\brackets{C\ge(1+\gamma)\mathbb E[C]}\le e^{-\mathbb E[C]\gamma/3}.\label{eq:CH2}
%\end{align}
To determine $p$ we first set $N'=\ceil{2\cdot(1+\epsilon/3)\cdot\epsilon^{-2}\cdot{\ln2\delta^{-1}}}$, and $p=\frac{N'}{N}$.  

Since we know that the maximal query return value is $N$, our estimator only need to count to $N\cdot p =N'$. Intuitively, if we want to increase the estimator above $N'$ it is always due to oversampling.  As a result, we require $\ceil{\log_2(1+N')}\approx 2\log_2\epsilon^{-1} + \log_2\log_2\delta^{-1}+1$ bits. 
%Observe that the number of required bits is independent of $N$. 
Note that the number of bits we require to count until $N$ (estimator value of $N'$) with an additive error of $\epsilon \cdot N$ is independent of $N$. That is, our estimators have an unbounded counting range within the additive error model (note that the  error in the additive model depends on $N$). 
%\MM{I didn't get what this previous sentence was aiming at;  clarify?}.\GI{Is that better?} 
We note that representing $p$ requires $\Omega(\log N)$ bits which implies that our memory consumption still depends on $N$. However, when we move to using arrays of these estimators, since all of the estimators use the same $p$, encoding $p$ introduces a negligible overhead. 

Theorem~\ref{thm:goldbach} shows that our estimation method has the desired property. 
The proof is delayed to Appendix~\ref{app:singleCounterProof}.
\begin{restatable*}[Single Estimator]{thm}{single}
\label{thm:goldbach}
For any number of probabilistic increments $I\le N$, \mbox{we have $\Pr[|C/p-I|> N\epsilon]\le\delta.$}
\end{restatable*}

As an example, Theorem~\ref{thm:goldbach} implies that a $24$-bit estimator can approximate any count up to any pre-specified $N$ within an additive error of $N\epsilon$ for $\epsilon=0.1\%$, and be correct with probability $(1-\delta)$ of $99.95\%$.

%\textcolor{red}{[*** The footnote only matters if we have multiple counters, so perhaps we need to say that {\em in later sections} where  we make use of an array of counters, we benefit from the fact that $p$ only needs to be stored once for all the counters.] GIL: DONE}


%\begin{theorem}
%For any number of increments $I\le N$, we have that $\Pr[|C/p-I|> N\epsilon]\le 1-\delta$.
%\end{theorem}

\subsection{Weighted Estimators}\label{sec:weighted}
We now consider a weighted estimator where the desired increment can be an arbitrary number (and not just by 1). Such estimators are useful for applications that, for example, rely on the byte volume of flows rather than their packet counts. Further, most existing sketches (e.g., Count Min~\cite{CountMinSketch} and Count Sketch~\cite{CountSketch}) and counter-based algorithms (including Space Saving~\cite{SpaceSavings}, Frequent~\cite{BatchDecrement,frequent4} and RAP~\cite{RAP}) support weighted updates.
\mbox{The recent estimators by~\cite{Infocom2019} support it as well.}

Our weighted estimator supports the {\sc Add}($\mathfrak w$) method, and the {\sc Query} method estimates the sum of all add operations. For example, {\sc PIncrement} is equivalent to {\sc Add}($1$). We generalize $N$ to be the sum of all add operations when discussing weighted measurements.  The notation $N'$ and $p$ are unchanged. 
% remains the same, and so is the probability $p=\frac{N'}{N}$.

 
%The goal is for {\sc Query} to provide an $N\epsilon$-additive approximation with probability $1-\delta$ as long as the sum of additions does not exceed $N$.
%It is worth noting that counter-based algorithms either require \textcolor{red}{I don't think require is right here, unless these papers have a lower bound.  I'd say current counter-based algorithms either utilize or take} logarithmic time for such updates or take twice as many counters~\cite{dimsum,AndersonIMSUM}; our approach provides constant amortized update time with less space than the logarithmic time algorithms. 
% \textcolor{blue}{GIL: We need to define p in a similar manner to the unweighted estimator, perhaps add a new notation to the sum of all weights... how about (B)?}
In the {\sc Add}($\mathfrak w$) method, we break the update into two parts.  Let $w_1 = \floor{\mathfrak w p}$ and $w_2 = \mathfrak w - w_1/p$. We
increase the estimator (deterministically) by $w_1$, and with a probability of $w_2 p$ (notice that $w_2<1/p$ and this is a valid probability), we further increase the estimator by 1. 
In Appendix~\ref{app:weightedUpdatesProof} we prove the correctness of this approach. 

\subsection{Estimator Arrays}
We now discuss how to efficiently implement an estimator array, which is an important building block for sketch algorithms. 
An \emph{estimator array} supports the {\sc PIncrement}$(i)$ and {\sc Query}$(i)$ methods, for $i\in\set{1,\ldots,w}$. Here, $w$ is the number of estimators in the array, also referred to as its \emph{width}.
$N$ is then defined as the overall number of probabilistic increments across all $i$'s and the goal is to estimate the number of {\sc PIncrement}$(i)$'s to within an $N\epsilon$ additive error.
%As a first approach, we replace each counter with our single-counter algorithm. 

We can further reduce the size of the array since the sum of all
estimators is unlikely to be much larger than $N'$, as an estimator value of $N'$ yields an estimation of $N$. 
%\textcolor{red}{I'm not clear what the above sentence means.  What is $N'$ here, it hasn't been defined?  We just said the overall number of increments is $N$, now we're saying it's $N'$.  What's going on?  I think you mean the overall number of "actual increments" -- e.g., successful coin flips that increment a counter -- as opposed to increment operations.  You need to define and consistently use terms that separate these two, or it will confuse the reader (me). Gil: please read again, I rephrased}
Specifically, in Appendix~\ref{app:sumOfCountersBoundProof} we prove that the total number of actual increments to the array is at most $\widetilde{N'}\triangleq N'+\sqrt{3N'\ln\delta_o^{-1}}$ with probability $1-\delta_o$ (the $o$ subscript denotes \emph{oversampling} error probability to distinguish it from the other error sources).

Our goal is to use shorter estimators, and to do so we consider a threshold value $T<N'$, such that each estimator is $\ceil{\log_2 T}$ bits long. \emph{Heavy estimators} are ones which reach the maximal estimator value of $T$, these counters \emph{overflow} to a secondary data structure. Since we keep the sum of all counters bounded by $\widetilde{N'}$, \mbox{there can be at most $\floor{\widetilde{N'}/T}$ heavy counters.}

We store the list of heavy estimators in a hash table where the key is the index of the heavy estimator and the value contains the most significant bits of that estimator. For example, if $N' = 2^{24}-1$, we can have two byte (16 bit) estimators, and extend estimators that require more than 16 bits with another 8 bits. %There can be at most $ 2^{24}-1$
%\ran{can't we encode just the most significant bits as value? That's how I got the $\log_2 w + O(1)$ bits quantity. If this is not correct we should revise.}
%where the key is the index of the heavy counter, and the value is the 
In practice, we suggest storing the heavy counters in  a compact hash table such as~\cite{TinyTable,TinyTable2} which adds an additional $\log_2 w + O(1)$ bits per heavy counter or $\floor{\widetilde{N'}/T}(\log_2 w + O(1))$ bits overall.
This means that our total space requirement is $w\ceil{\log_2 T}+\floor{\widetilde{N'}/T}(\log_2 w + O(1))$. 
%$$T\log T = \frac{N'\log_2 w}{w}$$
%$$T = \frac{N'\log_2 w}{w\log_2\parentheses{ \frac{N'\log_2 w}{w}}}$$
We minimize this quantity by setting $\delta_o \ll \delta$ and $T\approx \frac{\widetilde{N'}\log_2 w}{w\log_2\parentheses{ \frac{N'\log_2 w}{w}}}$ which gives a total space of $w\cdotpa {\log_2{\frac{N'\log_2 w}{w}}+O(1)}$ bits.~\footnote{For performance, it may be better to set $T=2^{8z}$ for some integer parameter $z$. This allows byte alignment and faster implementation.} That is, we save nearly $\log_2 w$ bits per counter by encoding the heavy ones separately. 
For example, if $w=1024, \epsilon=0.1\%$ and $\delta=99.95\%$, we can set $T=2^{16}$ to encode each counter with two bytes and have at most $253$ heavy counters (even if $\delta_o=2\cdot10^{-15}$), for a total memory of less than $2.5$KB. 
In comparison, allocating 3 bytes for each counter, as in the previous sections, requires $3B\cdot2^{10}=3$KB  (20\% more space).  %That is, we reduce the required space by 17\%. 


%textcolor{red}{Why is this constant amortized time?}
%The Chernoff's inequality allows bounding the sum of 
%independent Bernoulli variables with varying probabilities and our analysis extends simlessly to show that the same $N'$ and $p$ values are adequate.

%\textcolor{red}{We said we'd talk about unbounded counters here, but it seems to have gone missing...}

\begin{comment}
%we could follow two approaches. First, 
we can use the geometric sampling to determine how many bytes (size units) to ignore before sampling the next byte. If $\mathfrak w =  O(1/p)$ then this approach works well it will increase the counter $O(\mathfrak wp)=O(1)$ times and have a constant update time. 
However, if $\mathfrak w\gg 1/p$, this may be computationally expensive. 
%A second approach is to sample a binomial random variable $\mathcal I\sim \mbox{Bin}(\mathfrak w, p)$ that would determine the value in which we need to increase the counter. 
Instead, we first break the update in two and set $w_1 = \floor{\mathfrak w p}$ and $w_2 = \mathfrak w - w_1/p$. We then deterministically increment the counter by $w_1$ and follow the geometric approach with the residual weight $w_2$.
Since $w_2<1/p$, our expected update time is constant. Furthermore, the deterministic update only decreases the variance in our counter and improves the accuracy.
\end{comment}

\subsection{Dynamically increasing $N$}
\label{sec:dynamicN}
Heretofore, we have assumed that $N$ is known, which allowed us to tune our sampling rate $p$. 
Sometimes $N$ may not be known in advance (e.g., in the case where the measurement length is defined in time and not packets). We propose two algorithms for such a scenario -- {\sc MaxAccuracy} and {\sc MaxSpeed}. Intuitively, {\sc MaxAccuracy} aims for the best accuracy possible given the counter size, while {\sc MaxSpeed} uses the minimal sampling probability to preserve the accuracy guarantee and is therefore faster.

In {\sc MaxAccuracy}, we start with $p=1$, and whenever some counter needs to exceed its maximal value we
%divide $p$ and \emph{all counters} by two. 
independently replace each $C$-valued counter with a generated binomial random variable $\mbox{Bin}(C,1/2)$ and halve the value of $p$. This procedure is called \emph{downsampling} and was first introduced in~\cite{gibbons1998new}.
That is, once \emph{some} counter overflows we decrease the value of \emph{all} counters.
This simulates a process where each {\sc PIncrement} increased the value of the estimator with the current value of $p$. As a result, our accuracy guarantees seamlessly follow for the new estimator, given that $\epsilon,\delta$ are such that $N'$ is smaller than $2^\ell$ for estimators of length $\ell$.
For example, if we are using $\ell=16$-bit counters, then once a counter is incremented for the $(2^{16})$'th time, we halve $p$ and downsample the estimator. 
%We keep an additional counter $H$ that reflects how many times we downsampled the estimators (i.e., $p=2^{-H}$ at all times). 
%Finally, we track the total number of increments using a single counter $n$ and do not increase estimators beyond $np$ as this is always the result of oversampling. 

{\sc MaxSpeed} does not wait for a counter to reach its maximal value, but instead tracks the number of PIs, which we denote by $n$, and uses a sampling probability $\min\set{1,2^{-\floor{\log_2 (n/ N')}}}$.
%\MM{Note that $n=0$ is a problem, and as I read this, it doesn't quite make sense -- are the $n$ and $N'$ in the wrong place, as n gets larger the floor expression is always 0?}\ran{Yes, I wrote it the write way in the pseudo code, but wrong here. Good catch. Regarding $n=0$, I don't think its a problem, when you get the first packet you have $n=1$. Not sure why we should define the sampling probability for $0$ packets.}
That is, the first $2N'$ PIs are performed with probability $1$, the next $2N'$ PIs with probability $1/2$, then for $4N'$ PIs it is reduced to $1/4$, etc.
Whenever we halve the sampling probability, we also downsample the counter to maintain the accuracy guarantees. We note that this estimator requires $\approx \log_2(2N')=1+\log_2 N'$ bits, i.e., one additional bit compared to our \mbox{estimator when knowing $N$ in advance.}
% \MM{The previous sentence doesn't make sense;  "We note hat since",
% did you mean We note that this?}\ran{Yeah, it got a little messed up, fixed.}

The pseudocode for {\sc MaxAccuracy} is given in Algorithm~\ref{alg:maxACC}, and for {\sc MaxSpeed} in Algorithm~\ref{alg:maxSpeed}.  These are generic algorithms that apply to many sketch and cache-based algorithms. Such algorithms vary in the way they implement Line~\ref{linegenaccuracy} in Algorithm~\ref{alg:maxACC}, and Line~\ref{linegenspeed} in Algorithm~\ref{alg:maxSpeed}. The line returns the counters of $x$, which are algorithm dependent. For example, in the CM Sketch~\cite{CountMinSketch} and the CU Sketch~\cite{CUSketch} the set contains a single counter from each array chosen by applying a hash function to $x$. In Space Saving~\cite{SpaceSavings} and RAP~\cite{RAP}, the counter is $x's$ counter if it is monitored, or the minimal counter if it is not monitored. 
Notice that the algorithms may take steps in addition to increasing the counters using our algorithm. For example, Space Saving and RAP may replace the identifier associated with the minimal counter in addition to increasing it.%\vspace{4mm}

\noindent\textbf{Deterministic Downsampling.}\quad{}
We now propose a deterministic method for reducing the estimator values (in both {\sc MaxAccuracy} and {\sc MaxSpeed}). Specifically, when downsampling a $C$-valued estimator, we replace its value with $\floor{C/2}$ instead of $\mbox{Bin}(C,1/2)$.\footnote{One can get slightly more accurate results by randomized rounding up the estimator by $1$ with probability 50\% if $C$ was odd. However, as this improvement is negligible compared with the error of the estimator we \mbox{eschew it for faster implementation.}} The intuition is that this allows us to reduce the variance in the estimation.
We have run experiments to confirm that the accuracy of the deterministic downsampling is superior to that of the probabilistic one. The theoretical accuracy guarantee of the deterministic downsampling is left for future work.
The experiments, whose results are depicted in Figure~\ref{fig:downsampling}, are obtained by running each point $100$ times and reporting its 95\% interval according to Student t-test~\cite{student1908probable}. As shown, the \mbox{deterministic downsampling is indeed more accurate.}
\begin{figure}[h]
\vspace*{-8mm}
\subfloat[8-bit estimators]
{\includegraphics[width =0.495\columnwidth, height=3.2cm]
{figs/8bits.pdf}}
\subfloat[16-bit estimators]
{\includegraphics[width =0.495\columnwidth, height=3.2cm]
{figs/16bits.pdf}}
%\centering
%\captionsetup{justification=centering,margin=.2cm}
\vspace*{-1mm}
\caption{\small \mbox{Comparing probabilistic and deterministic downsampling.\label{fig:downsampling}}}
\vspace*{-3mm}
\end{figure}
%In Figure~\ref{fig:downsampling} we show that indeed halving the counters is better than down sampling.  

\noindent\textbf{Deamortized Downsampling.}\quad{} Both algorithm variants include a downsampling operation that requires linear time. In some deployments, having a long maintenance operation may cause high latency and even packet drops. To deamortize the downsampling operation and ensure low worst-case update time, we add a \emph{generation} bit to each counter, which specifies if it was downsampled an even number of times. Then, for each packet, we downsample a number of counters that asymptotically equals the amortized update time (e.g., with $2^{18}$ sixteen-bit counters, we can downsample $8$ counters in each update). Importantly, if a counter that has not been downsampled yet overflows, we immediately downsample it and switch its generation bit, \mbox{to identify it once the maintenance operation reaches it.}

\subsection{Optimizing the Update Speed}
While our proposed estimator saves space, we designed it in a manner that can also reduce the update time. The key aspect of our approach is that the probability for updating an estimator \emph{does not} depend on its current value. In comparison, the update probability in all the estimator techniques surveyed in this work~\cite{ICE-Buckets,CASE,CEDAR,ANLSUpscaling,DISCO,SAC,Infocom2019} depends \mbox{on the current estimator value.}

Specifically, we can decide if an estimator is updated prior to calculating the sketch hash functions, and without reading any data structure. When $N$ is large enough, most packets require no additional work as they do not update any estimator. 
Further, we can use Geometric Sampling~\cite{Nitro} to determine how many packets to skip before an estimator is updated. If each packet is sampled with probability $p$, then the number of packets until the next sample is distributed geometrically with mean $p^{-1}$. Geometric Sampling simply generates \mbox{a single} variable $G\sim \mbox{Geo}(p)$ (i.e., $\Pr[G=x]=p(1-p)^{x-1}$) by using the Inverse Transform Sampling method.
The method sets $G=\ln U / \ln (1-p)$ for a uniform random variable $U\sim\mbox{Uniform}[0,1]$; it requires a single uniform variate and a few floating-point operations. The variable $G$ is shared across \emph{all} estimators and thus does not impose a significant memory overhead (e.g., it can be implemented as a 64-bit integer). 
% The geometric distribution also has a useful property of \emph{memorylessness}.
While a similar approach for acceleration appears in NitroSketch~\cite{Nitro}, it does not allow for shorter counters as they add $p^{-1}$ to the sampled counters and vary $p$ over time. 
%As a result, we only need to draw a single random variable for each incremented (sampled) estimator. We use a \emph{single} 
% \MM{I'm pretty sure we discuss geometric sampling earlier so if we're going to use this explanation let's see where we first use it and explain it there.}\ran{I moved things around, this is the first place now. I added some explanations.}
%To do so, we use a single variable $X$ which is drawn from the geometric distribution with parameter $p$ (i.e., $\Pr[X=x]=p(1-p)^{x-1}$). $X$ reflects how many arrays should be \emph{skipped} and which array should be incremented. 
%The geometric variable $x$ saves us from making making $d$ independent coin flips to determine which estimators to increment.  Since $X$ is shared among all estimators, it barely affects the space requirements (e.g., it can safely be implemented using a $64$ bits word).


For sketches that associate each flow with $d$ estimators, such as the Count Min Sketch and Conservative update,
the geometric sampling  only requires $d\cdot N'$ operations per $N$ packets, which gives an amortized complexity of $1+\frac{d\cdot N'}{N}=O\parentheses{1+\frac{\epsilon^{-2}\log^2\delta^{-1}}{N}}$. That is, we have a constant update time for streams in which $N=\Omega(\epsilon^{-2}\log^2\delta^{-1})$.

While cache-based algorithms such as Space-Saving and Frequent have data structures that allow constant-time updates~\cite{CormodeCode}, they may require seven pointers per entry. %(which adds $28$ bytes in 32-bit systems or $56$-bytes in 64-bit Operating systems!). 
Alternative approaches include a heap-implementation~\cite{CormodeCode} that, while being space-efficient, requires a logarithmic update time.
Our approach allows using a heap while keeping the amortized update complexity constant (in streams in which $N=\Omega(\epsilon^{-2}\log\epsilon^{-1}\log\delta^{-1})$). 

%\ran{----------------------------------------I got to this point}

%This approach allows us to use geometric sampling;  that is, we determine the next successful increment according to a random variables with a geometric distribution, which allows a more efficient implementation.  
%To maintain consistency, we could downsample the existing counters by replacing a counter with value $k$ with the value from a binomial random variable $Bin(k,1/2)$;  this "re-samples" the existing counter according to the new sampling probability, maintaining our previous theoretical results.  (This idea has appeared elsewhere, as in \cite{}.)  
%However, we have found in practice that if after changing the sampling probability, we deterministically divide all the counters by 2 (rounding down) then we obtain better approximations with less variance, and we use this optimization in our experiments.  







%Halving the counters is difficult to analyze, but w
%We can analyze {\sc MaxAccuracy} by bounding the value of $p$ at the end of the measurement.
%\textbf{Gil: Why is MaxAccuracy an algorithm without knowing N? We can also do it when N is known.}
%\MM{We say "we can analyze MaxAccuracy"... do we actually do it?
%If not I'd be a bit careful what we say here -- the dependency here looks complex/nontrivial to analyze.  For ANALYSIS purposes, the correct approach is to "resample" the existing count each with probability 1/2 so that everything remains sampled with the right probasbility.  
%That is, we replace a count of X by Bin(X,1/2).  Doing this deterministically may be better -- but it may not be -- and is much harder to analyze.  
%We should point out this idea is old -- see, e.g., 
%https://dl.acm.org/citation.cfm?id=276334
%This paper uses the idea, I think in section 4.
%}


% \MM{Again, we should experiment as to which is better -- dividing counters by two or "resampling", but in terms of analysis, dividing by 2 is hard, resampling maintains the sampling throughout.}  
% This way, our algorithm guarantees that the error when queried after $N$ increments cannot be larger than our original algorithm that always samples at rate $N'/N$.
% \MM{I'm not clear why we think this is true, and we should be more careful about such statements...}

\begin{algorithm}[ht]
\small
\caption {{\sc MaxAccuracy} Algorithm with $n$-bits counter\label{alg:maxACC}}
\begin{algorithmic}[1]
\Statex Initialization: $p\gets 1$
\Procedure{Add}{$\langle x,w\rangle$}\Comment{A $w$-sized packet from flow $x$}
    \State $w_1\gets \floor{w\cdot p}$
    %\State 
    ,\quad{}
    $w_2\gets w-w_1$
    \State \label{linegenaccuracy}$\mathcal C\gets \mbox{$x$'s counters}$ \Comment{Algorithm dependent}
    %\While {$\exists C\in \mathcal C: C \ge 2^n-w_1$}\Comment{Overflow event}\label{line:maxACCOverflow1}
    \While {$\max\set{\mathcal C} \ge 2^n-w_1$}\Comment{Overflow event}\label{line:maxACCOverflow1}
        \State Divide \emph{all} counters by two \Comment{Not only $x$'s counters}
        %\State $p\gets p/2,\ w_1\gets \floor{w_1/2},\ w_2\gets \floor{w_2/2}$
        \State $p\gets p/2$
        %\State $w_1\gets \floor{w_1/2},\quad w_2\gets \floor{w_2/2}$
        \State $w_1\gets \floor{w\cdot p}$
        %\State 
        ,\quad{}
        $w_2\gets w-w_1$\label{line:endMaxACCOverflow1}
    \EndWhile
%    \Statex\ran{An alternative to %lines~\ref{line:maxACCOverflow1}-\ref{line:endMaxA%CCOverflow1}:}
%    \State\ran{ $C_{\max}\gets \max_{C\in \mathcal C} %C$}
%    \State\ran{ $D \gets \max\set{0,\ceil{\log_2 %(C_{\max}+w_1+1)}-n}$}
%    \State\ran{ Divide all counters by $2^D$}
%    \State\ran{ $p\gets p/2^D$}
%    \State \ran{$w_1\gets \floor{w\cdot p}$,\quad{}
%        $w_2\gets w-w_1$}
    \For {${C\in \mathcal C}$}
        \State $C\gets C + w_1$
        \If {$U[0,1] \le w_2\cdot p$}\Comment{With probability $w_2\cdot p$}
            \If {$C = 2^n-1$}\Comment{Overflow event}
                \State Divide all counters by two
                %\State $p\gets p/2,\ w_1\gets \floor{w_1/2},\ w_2\gets \floor{w_2/2}$
                \State $p\gets p/2$
                \State $w_1\gets \floor{w\cdot p}$
        %\State 
        ,\quad{}
        $w_2\gets w-w_1$
            \EndIf
            \State $C\gets C+1$
        \EndIf
    \EndFor
\EndProcedure
\Procedure{Query}{x}\Comment{Estimate flow $x$'s size}
    \State $q \gets$ Algorithm's estimate\Comment{Algorithm dependent}
    \State\Return $q/p$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\small
\caption {{\sc MaxSpeed} Algorithm with $n$-bits counter\label{alg:maxSpeed}}
\begin{algorithmic}[1]
\Statex Initialization: $p\gets 1, N\gets 0, g\gets 1$
\Statex \qquad{}\qquad{}\qquad{} $N'\gets\ceil{2\cdot(1+\epsilon/3)\cdot\epsilon^{-2}\cdot{\ln2\delta^{-1}}}$
\Procedure{Add}{$\langle x,w\rangle$}\Comment{A $w$-sized packet from flow $x$}
    \State $n\gets n + w$
    \State $p_{\mbox{new}}\gets 2^{-\floor{\log_2 n/N'}}$
    %\State $p_{\mbox{new}}\gets \min\set{1,2^{-\floor{\log_2 N/N'}}}$
    %\Statex \ran{setting $p_{\mbox{new}}\gets 2^{-\floor{\log_2 N/N'}}$ also works}
    \If {$p_{\mbox{new}} < p$}
        \State $D=\log_2 (p/p_{\mbox{new}})$
        \State Divide all counters by $2^D$
        \State $p\gets p_{\mbox{new}}$
        \State $G\gets \mathit{Geo}(p)$ \Comment{Geometric random variable}
    \EndIf
    \State $w_1\gets \floor{w\cdot p}$
    %\State 
    ,\quad{}
    $w_2\gets w-w_1$
    \State \label{linegenspeed}$\mathcal C\gets \mbox{$x$'s counters}$ \Comment{Algorithm dependent}
    \For {${C\in \mathcal C}$}
        \State $C\gets C + w_1$
        \While {$G \le w_2$}\Comment{Simulate $w_2$ coin flips}
            \State $C\gets C+1$
            \State $w_2\gets w_2 - G$
            \State $G\gets \mathit{Geo}(p)$ \Comment{Geometric random variable}
        \EndWhile
        \State $G\gets G - w_2$
    \EndFor
\EndProcedure
\Procedure{Query}{x}\Comment{Estimate flow $x$'s size}
    \State $q \gets$ Algorithm's estimate\Comment{Algorithm dependent}
    \State\Return $q/p$
\EndProcedure
\end{algorithmic}
\end{algorithm}