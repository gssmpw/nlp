\section{Integrating Estimator Arrays with Sketches}\label{sec:counterArrays}
Sketch data structures utilize several independent counter arrays. 
Intuitively, each array provides an estimation which is (roughly) accurate with a constant probability, and additional arrays amplify the success probability.
For example, the Count Min Sketch (CMS)~\cite{CountMinSketch} employs $d=O(\log\delta^{-1})$ arrays $A_1,\ldots,A_d$ of $w=O(\epsilon^{-1})$ counters each. Whenever an element $x$ arrives, it uses $d$ uncorrelated pairwise-independent hash functions $h_1,\ldots,h_d$ that map the input to the range $[0,w)$, and for each $j=1,\ldots,d$ it increments the counter $A_j[h_j(x)]$ of the $j$'th array.
When receiving a query for the multiplicity of $x$, we take the minimum over all $j$ of $A_j[h_j(x)]$.  Clearly, CMS can be implemented using our estimator array algorithm above, replacing increment operations with the probabilistic increment operations. For example, with $d=5$ arrays of $w=2^{10}$ counters each, we require about $12.5$KB for the entire encoding.

The sketch itself also has an error that is caused by collisions of different items that increment the same counter. For CMS, it guarantees that the error will be bounded by $N\epsilon_A$ with probability $1-\delta_A$, for $\epsilon_A = e/w$ and $\delta_A=e^{-d}$. Combining the error from the sketch with that of the counter arrays, we have an error of at most $\epsilon+\epsilon_A$ with probability at least $1-d\cdot \delta-\delta_A$. For example, if $d=5$ and $w=2^{10}$ then replacing the CMS's counters (assuming they are 32-bits each) with our estimators reduces the space from $20$KB to $12.5$KB while increasing the error from 0.271\% to 0.371\% and the error probability from 0.67\% to 0.97\%. We note that a CMS configured for a 0.371\% error except with probability 0.97\% would still require more space ($>13.5$KB) than our solution (while also being considerably slower). %\textbf{GIL: Can you say how much space it would require?}

\section{Cache-based Counter Algorithms}
\label{sec:CBA}
Sketches are a popular design choice for hardware as they are easy to implement in hardware.
In software, however, one can generally get a better accuracy to space tradeoff by using cache-based counter algorithms~\cite{SpaceSavingIsTheBest2010,SpaceSavingIsTheBest2011}. Specifically, algorithms like Space Saving~\cite{SpaceSavings}, Misra-Gries~\cite{misra1982finding}, and Frequent~\cite{BatchDecrement,frequent4} use $O(\epsilon^{-1})$ counters (as opposed to $O(\epsilon^{-1}\log\delta^{-1})$ in sketches such as Count Min). %Randomized Admission Policy (RAP) optimizes such algorithms for heavy tail workloads, $d$-way RAP is an efficient implementation suggestion that minimizes the overheads for maintaining data structures by using the limited associativity model. 
%\MM{This next sentence does not appear to belong in this paragraph, unless we add a sentence that says we can use our estimator to improve Space Saving and other cache-based approaches.  Or just remove the nexgt sentence and wait to talk about these.}
%Moreover, our approach can also compress and accelerate modern counter algorithms such as RAP and $d$-way RAP~\cite{RAP}.
%\MM{Why are these "modern" ones separated out...}

%In cache algorithms, each entry has two fields -- the identifier of the item associated with it, and a counter. We detail the Space Saving algorithm.
%When an item that has an entry arrives, Space Saving increments its counter. If the current item $x$ does not have an entry, Space Saving finds a entry with a minimal counter and changes its identifier to $x$ while increasing the counter (as if $x$ owned the entry). The operation of finding the minimum over a large set of counters is challenging in hardware~\cite{HashPipe}, but easier in software. 

In this section, we consider compact cache-based algorithms that can benefit from utilizing estimators, rather than full-sized counters. To obtain maximal benefits, we concurrently aim to minimize the overhead from the flow identifiers. For example, flows are typically defined by five-tuples that are 13 bytes long, whereas counters are typically 4 to 8 bytes long.  In such a setting, reducing a 4-byte counter to a 2-byte estimator offers only marginal space improvements.
% Thus, replacing the counters with estimators only marginally improves the overall memory consumption.  For example, if we replace the counter with half sized estimators (without accounting for additional error), we can only improve the overall space by $12\%$ for $13$ bytes $5$-tuple identifiers.
We therefore propose replacing the identifiers with \emph{fingerprints}, i.e., short pseudo-random bitstrings generated as hashes of the identifiers. 
Fingerprints were proposed before (e.g., see~\cite{HeavyHitters}) to compress identifiers; however, the following analysis, which asks for the shortest size at which an element experiences additive error at most $N\epsilon$ appears to be new. 
In particular, it allows us to use shorter fingerprints compared to previous analyses.
If the stream contains $D\le N$ distinct items, then fingerprints of size $O(\log D)$ suffice to ensure that no two items have a fingerprint collision (with suitably high probability) and thus the accuracy is essentially unaffected by this compression. However, while fingerprints may be smaller than the $13$ bytes required for encoding five-tuples, they may still be significantly larger than the estimator.
We can do better by not requiring no collisions, and instead finding the minimal fingerprint length ($L$) that allows an error of at most $N\epsilon_f$ with probability $1-\delta_f$.  We show that $L\approx \log\epsilon_f^{-1}\delta_f^{-1}$ suffices, implying that the fingerprint length can be \mbox{of the same order as our estimators.}

We use a weighted variant of the Chernoff bound which states 
%https://pdfs.semanticscholar.org/0cfb/537793001c70f40998dfba24b54fdd8498da.pdf
that for independent random variables $Y_1,\ldots,Y_n$ with values in the interval $[0,z]$ for some $z>0$, the sum $Y=\sum_{i=1}^n Y_i$ satisfies for all $t>0$, 
$
\Pr[Y > t]\le (e\cdot \mathbb E[Y]/t)^{t/z}.
$

Given a parameter $\alpha\in [0,1]$, we split the items into large and small ones. Let $\mathcal L$  denote the set of items whose size is at least $(\alpha\cdot N\epsilon_f)$, and let $\mathcal S$ denote the remaining. 
Further, let $S_{\mathcal L}$ denote the total size of the large items and let $S_{\mathcal S}$ denote the total size of the small ones. We have that $S_{\mathcal L}+S_{\mathcal S}\le N$.
We want to set the fingerprint size $L$ such that with probability $1-\delta_f$ \emph{none} of the large items collide with $x$ and the sum of sizes for the small colliding items is at most $N\epsilon_f$. 
Using the union bound, and the fact that $|\mathcal L|\le \alpha^{-1}\cdot \frac{S_{\mathcal L}}{N\epsilon_f}$, we have that the probability for a collision with a large item is at most $\alpha^{-1}\cdot\frac{2^{-L}S_{\mathcal L}}{N\epsilon_f}$.
For each small item $i$ with size $f_i\le \alpha\cdot N\epsilon_f$, we define the random variable $Y_i$ to take the value $f_i$ if $i$ has the same fingerprint as $x$ and $0$ otherwise. The total volume that collides with $x$ is then $Y=\sum_{i\in\mathcal S}Y_i$ (i.e., $\mathbb E[Y]=2^{-L}S_{\mathcal S}$). Since each $Y_i$ is bounded by $z=(\alpha\cdot N\epsilon_f)$, we use the Chernoff bound with $t=N\epsilon_f$ to conclude that
{\small \vspace*{-2mm}$$
\Pr[Y > N\epsilon_f]\le (e\cdot \mathbb E[Y]/t)^{t/z}
\le \parentheses{\frac{e\cdot 2^{-L}\cdot S_{\mathcal S}}{N\epsilon_f}}^{\alpha^{-1}}
.
$$\vspace*{-2mm}}
%$$
%\Pr[Y > N\epsilon_f]\le (e\cdot \mathbb E[Y]/t)^{t/z}
%\le (e\cdot 2^{-L}/\epsilon_f)%^{8}
%.
%$$

Therefore, the overall chance of failure is at most 
{\small
\begin{align}
\vspace*{-2mm}
\label{eq:ssErrorProb}
\alpha^{-1}\cdot\frac{2^{-L}S_{\mathcal L}}{N\epsilon_f} +  \parentheses{{\frac{e\cdot 2^{-L}\cdot S_{\mathcal S}}{N\epsilon_f}}}^{\alpha^{-1}}.%\le \frac{e\cdot 2^{-L}}{\epsilon_f}.$$ 
\vspace*{-2mm}
\end{align}
}

%Therefore, the overall chance of failure is at most $2^{-L}/\epsilon_f $. 
To account for all possible splits of $N$ packets into large and small flows and guarantee that~\eqref{eq:ssErrorProb} is at most $\delta_f$, we choose 
$$L=\ceil{\max\set{\log_2(\alpha^{-1}\cdot\epsilon_f^{-1}\delta_f^{-1}),\log_2(e\cdot\epsilon_f^{-1}\delta_f^{-\alpha})}}$$ to conclude that with probability $1-\delta_f$ at most $N\epsilon_f$ packets collide with the fingerprint of $x$.
%\ran{Is this clearer? 
%$$L=\ceil{\log_2 \epsilon_f^{-1}+\log_2\parentheses{\max\set{\alpha^{-1}\cdot\delta_f^{-1},e\cdot \delta_f^{-\alpha}}}}$$}
%\MM{I prefer the first.}
For example, by setting $\alpha=10/11$, we find that two byte identifiers yield $\epsilon_f,\delta_f\approx 0.5\%$, three bytes yield an error lower than $\epsilon_f,\delta_f=0.03\%$, and $32$-bit identifiers yield $\epsilon_f,\delta_f < 0.002\%$.

Space Saving, Misra Gries, and Frequent are all deterministic and have an additive error of $N\epsilon_A$, where $\epsilon_A=1/w$ and $w$ is again the width. Therefore, combining them with our estimators (with an $\epsilon,\delta$ guarantee) yields an overall error of $N\cdot(\epsilon+\epsilon_A+\epsilon_f)$ 
with probability at least $1-\delta-\delta_f$.

For brevity, we next provide two numerical examples with $w{=}2^{10}, \epsilon_f{=}0.03\%$ and $\delta_f{=}0.03\%$.

\noindent\textbf{Example 1.} Consider $\epsilon{=}0.1\%, \delta{=}0.05\%$ and $T=2^{16}$; we get an error lower than $0.23\%$ with probability at least $99.92\%$, while compressing the identifiers into three bytes and replacing the counters with two-byte estimators. 
That is, our example requires 5-bytes per entry, compared with $13+4=17$  bytes in the original.
We also have at most $253$ large counters (see Section~\ref{sec:counterArrays}), for an overall memory of $5.5$KB. In contrast, for a $0.23\%$ error guarantee, these algorithms would need $435$ \mbox{counters, requiring more space.}

%For a better tradeoff, 
%we can set $\epsilon=\epsilon_f=2^{-13},\delta=\delta_f=2^{-16}$ and $T=2^{24}$;

%As another example,
\noindent\textbf{Example 2.} Consider $\epsilon=2^{-13},\delta=2^{-16}$ and $T=2^{24}$. That is, we require 24-bit estimators and have at most $32$ large estimators. This configuration has a total error of at most $0.14\%$ with probability $\approx 99.97\%$ and requires 6.2KB. In comparison, the uncompressed variants require nearly $14$KB of space for the same guarantees. 
\GI{MM please review}
%We elaborate on the implementation alternatives in Section~\ref{}.
%Space Saving is a deterministic algorithm with an additive error of $N\epsilon_A$ for $\epsilon_A=1/w$ when allocated with $w$ counters.
%\MM{The above is getting confusing.  How do these algorithms use large counters?  We were talking about bytes per entry which I understood and then we switched to "counters" and I'm confused.}
%\ran{I think that it's almost the same as in the estimator-array case. The algorithms make a single PI per packet and therefore the sum of all counters is still likely under $\widetilde{N'}$. Therefore we can still use two bytes per counter and encode an additional byte only for the counters that exceed $2^{16}$. How would you write it?}
%\GI{Gil: Ran - please clarify these examples, we need a single example with a clear bottom line I got lost here.}
%\MM{Agree with Gil.  Let's pick one example and make it clear.}  

%~\cite[Exercise 4.16]{} which states that for any $a_1,\ldots,a_n\in[0,1]$, $\gamma\in[0,1]$ and %independent Bernoulli variables $Y_1,\ldots,Y_n$ the sum $Y=\sum_{i=1}^na_iY_i$ satisfies:
%$$
%\Pr\brackets{Y \ge (1+\gamma)\mathbb E[Y] } \le e^{-2\gamma^2(\mathbb E[Y])^2 / n}.
%$$


%Fix some item $x$ and let $Z$ denote the frequency of all items that has the same fingerprint as $x$. Additionally, let $\mathfrak p=2^{-L}$ denote the chance of such collision. Then according to the Chernoff inequality:
%$$
%\Pr[Z\ge (1+\gamma)\mathbb E[Z]]\le e^{-\mathbb E[Z]\gamma^2/3}.
%$$
%We have that $\mathbb E[Z] \le N2^{-L}$ and thus we choose $\gamma=\frac{\epsilon_f}{2^{-L}}$
%\input{SpaceSaving}




%Recently, Ben Basat et al. proposed a deterministic data structure that allows constant update time  without the use of pointers~\cite{dimsum}, but it requires doubling the number of counters.\ran{the last sentence may belong elsewhere}

\section{Evaluation}\label{sec:eval}
We evaluate our algorithms on two real packet traces: the first 98M packets of (1) the CAIDA equinix-newyork 2018 (NY18)~\cite{CAIDA2018} and (2) the CAIDA equinix-newyork 2016 (CH16)~\cite{CAIDA2016} backbone traces. 
We picked these traces as they are somewhat different: CH16 contains 2.5M flows while NY18 exhibits a heavier tail and has nearly 6.5M flows. 
%and the CH16 data center trace~\cite{UWISC} that contains 98M packets as well.  
We implement our algorithms in C++ and compare them with the, state of the art, SAC estimators~\cite{Infocom2019} whose code we obtained from the authors. The Baseline code for Space Saving was taken from~\cite{CormodeCode} and we extended it to implement the RAP and dWay-RAP algorithms. For a fair comparison, all algorithms employ the same hash function (BobHash).
The default setting for our algorithm is {\sc MaxAccuracy} and we evaluate the difference from {\sc MaxSpeed} in Section~\ref{sec:maxspeedeval}. 
We ran the evaluation on a PC with an Intel Core i7-7700 CPU @3.60GHz and 16GB DDR3 2133MHz RAM. Finally, we refer to a PI as increment, to be \mbox{consistent across all algorithms.}

\begin{figure}[t]
    \centering
    \subfloat[8-bit estimators, Error]
    {\label{2a} \includegraphics[width =0.25\textwidth]
    {figs/single_counter_error_8_embedded.pdf}}
    \subfloat[8-bit estimators, Speed]
    {\label{2b}\includegraphics[width =0.25\textwidth]
    {figs/single_counter_speed_8.pdf}}\\
   % {\includegraphics[width =1.0\columnwidth]
    %{figs/single_counter_error_legend_with_baseline.pdf}}\\
    \hspace*{1mm}
   {\includegraphics[width =1.0\columnwidth]
    {figs/single_counter_error_legend.pdf}\vspace*{-4mm}}%\vspace*{-1mm}
    \\
    \subfloat[16-bit estimators, Error]
    {\label{2c}\includegraphics[width =0.25\textwidth]
    {figs/single_counter_error_16.pdf}}
    \subfloat[16-bit estimators, Speed]
    {\label{2d}\includegraphics[width =0.25\textwidth]
    {figs/single_counter_speed_16.pdf}}\\
    \vspace*{-2mm}
    \caption{\small A comparison of the speed and accuracy of single~estimators.}\label{fig:singleCounter} 
    \vspace*{-2mm}
\end{figure}

%\ran{In Univ2 there are only 171.3K flows. In NY2018 there are 6.5M flows. In Chicago2016 there are 2.5M.}
%The NY18 trace contains 98M packets, and we considered the first 98M packets of NY18.
%[clip, trim=0 10.3cm 0 10.5cm, width=0.99\linewidth]
% \MM{Since our comparisons will be the SAC remind the readers here what they are and why they're the natural comparison -- e.g., most recent?}\ran{Done. Called them "state of the art"}.
\begin{figure*}[t]
    \centering
    \hspace*{-3mm}
    \subfloat[CM Sketch, Error, NY18]
    {\label{3a}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cms_error_caida.pdf}}
    \subfloat[CM Sketch, Error, CH16]
    {\label{3b}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cms_error_caida_2016.pdf}}
    \subfloat[CM Sketch, Speed, NY18]
    {\label{3c}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cms_speed_caida.pdf}}
    \subfloat[CM Sketch, Speed, CH16]
    {\label{3d}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cms_speed_caida_2016.pdf}}    \\ %\vspace*{-2mm}
    {\includegraphics[width =1.2\columnwidth]
    {figs/single_counter_error_legend_with_baseline.pdf}\vspace*{-2mm}}\\
    \hspace*{-3mm}
    \subfloat[CU Sketch, Error, NY18]
    {\label{3e}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cus_error_caida.pdf}}
    \subfloat[CU Sketch, Error, CH16]
    {\label{3f}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cus_error_caida_2016.pdf}}
    \subfloat[CU Sketch, Speed, NY18]
    {\label{3g}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cus_speed_caida.pdf}}
    \subfloat[CU Sketch, Speed, CH16]
    {\label{3h}\includegraphics[width =0.25\textwidth]
    {figs/sketches/cus_speed_caida_2016.pdf}} 
    \vspace*{-1mm}
    \caption{Speed and accuracy of sketch algorithms. All SAC and AEE counters are 16-bits while Baseline uses 32-bits. \label{fig:sketches} }
    \vspace*{-10mm}
\end{figure*}
We use the following metrics; for speed, we use Million operations per second (Mops). For accuracy, on single-estimator experiments, we use Normalized Error, which is defined as the absolute error divided by the number of increments (or the sum of additions in the weighted experiment). 
For sketches and cache-based algorithms, we use on-arrival Normalized RMSE as a metric of accuracy. The on-arrival model (see~\cite{HeavyHitters,RAP,ConextPaper} and references therein), asks for the size of the arriving flow for \emph{each packet} and is motivated by the need to make real-time decisions based on the measurement data. In a stream with $N$ packets, we get error values $e_1,\ldots,e_N$ and the Root Mean Square Error (RMSE) is defined as $\mbox{RMSE}\triangleq\sqrt{\frac{1}{N}\sum_{i=1}^N e_i^2}$. The \emph{Normalized} RMSE is defined as  $\frac{1}{N}\cdot \mbox{RMSE}$ and has the range $[0,1]$ (unitless). For weighted streams, $N$ is replaced by the total byte-volume of the stream. We consider RMSE and not relative metrics like Mean Relative Error since the inherent error of sketches and cache-based algorithms is additive and not multiplicative.
Finally, we run every data point 10 times and use \mbox{Student t-test~\cite{student1908probable} to report the 95\% confidence intervals. }

 
\subsection{Single Estimator}
We begin by estimating the error and throughput of a single estimator as a function of the number of increments. We compare our Additive Error Estimator (AEE) to Static SAC~\cite{Infocom2019}  and Dynamic SAC~\cite{Infocom2019}. Figure~\ref{2a} shows the normalized error for each 8-bit estimator as a function of the number of increments. 
%\MM{On what dataset?}\ran{This is not data-based. Just calling PIncrement that many times.} 
AEE retains roughly the same normalized error regardless of the number of increments.
In contrast, Static SAC and Dynamic SAC experience higher error and can only count until about $10^3$. 
\MM{I'm going to suggest we remove the 8-bit graphs and just describe verbally that SAC fails with that few bits.  Figure 2 is pretty unreadable;  whatever is going on in 2a, the blowup box within the chart, is way too difficult for to understand.  We don't want to ask reviewers to decipher charts.  We could just show the 16 bit...}\ran{I like the idea of 8-bit estimators. SAC did discuss it a lot (although their estimator is really bad with 8 bits). We can describe in the text the embedded figure in 2(a). Gil/Shay, how do you feel about it?}\ran{we may actually need to cut it for space reasons}
%The main reason for this limitation is their encoding. 
This is because each SAC counter requires few bits to encode its sampling probability, which leaves very few bits for the estimator itself. 
In contrast, all the AEE estimators use the same sampling probability, which means that we can leverage all 8 bits. Figure~\ref{2b} shows the speed of an 8-bit estimator. AEE is orders of magnitude faster since we do not need to access it to decides whether to increment.  %We estimate that our platform can run about one billion increment operations per second, and therefore our estimator is not much slower than plain uint32\_t counter.
%For reference, the line Baseline (uint32\_t) shows the speed that we can increment a (plain) 32-bit unsigned integer as a reference.
% \MM{If Baseline is there, I don't see it.  Again, the pictures are too crowded;  make them larger, or just say we're only XXX times the Baseline.}\ran{We removed the Baseline, will edit the text}  
%AEE can nearly match this Baseline primarily because of geometric sampling, which allows the implementation to determine how many packets to skip until the next update. When the sampling probability is low, steps simply reduce the counter until the next sample is taken by 1. 
Figure~\ref{2c} and Figure~\ref{2d} repeat this experiment for a 16 bit counter. Static SAC and Dynamic SAC perform better than in the 8-bit case but eventually experience increasing error when the count becomes sufficiently large. In comparison, AEE's error remains the same regardless of the number of increments and is always lower (or equal) to that of Static SAC and Dynamic SAC. Figure~\ref{2d} compares the speed, 
showing that AEE is considerably faster. The non-monotone shape of the AEE curve is due to the computationally expensive random numbers generation. Specifically, AEE is especially fast when not sampling (less than $2^{16}$ increments) and when sampling aggressively (when $N$ is large, and $p$ is small). In between, there is a range in which sampling occurs with a relatively high probability (e.g., 1/2) slowing AEE down. 
%\GI{let's throw a number like x80 faster or something}.   
% \MM{In the above, I'm again not clear we're using increment consistently with how it's used in the rest of the paper -- increment vs. probabilistic increment.  I would make this clear early in the section.}  \ran{added a comment about it in the beginning of the eval sec}


 
\subsection{Sketch Algorithms}
Next, we evaluate the accuracy and speed of the CM sketch~\cite{CountMinSketch} and the CU Sketch~\cite{CUSketch}, using standard 32-bit counters (denoted Baseline), AEE, Dynamic SAC, and Static SAC estimators. Let us first consider the error in the NY18 trace (Figure~\ref{3a} and Figure~\ref{3e}). 
All estimators attain a similar accuracy, which is better than Baseline for both CM Sketch and CU Sketch. Then, as the Memory increases, the precision of the estimator based sketches stops improving while that of the Baseline improves further. 
Intuitively, the error of estimator based sketches has two components. 
One is the sketch error that decreases as we allocate more estimators to the sketch. Another comes from the estimator error that stays the same. Thus, as we gradually reduce the sketch error, it eventually becomes negligible compared to the estimation error. Since the CU Sketch is more accurate than the CM Sketch \cite{CUSketch}, the estimation error becomes the bottleneck earlier. Figure~\ref{3b} and Figure~\ref{3f} repeats this experiment on the CH16 trace.  
The main difference is that the CH16 trace contains only 2.5M distinct flows, while the NY18 trace contains 6.4M distinct flows. As such, the sketch error is considerably lower in the CH16 trace (as there are fewer flows that receive the same counters). Indeed, we see that the error of estimator based sketches does not improve, which implies that estimation error is the dominant one throughout the range. Notably, AEE attains lower error than Static SAC and Dynamic SAC. Figure~\ref{3c}, Figure~\ref{3d}, Figure~\ref{3g} and Figure~\ref{3h} show the speed for the CM Sketch and the CU Sketch.  Static SAC and Dynamic SAC are slower than Baseline because their sampling probability depends on the specific counter. Therefore, for each increment, we first access the sketch counters (and calculate multiple hash functions), and only then determine the sampling probability.
In contrast, in AEE, the sampling probability is identical for all counters. Thus, we first flip a coin and access the sketch counters only if we need to update them. As a result, AEE is \mbox{considerably} faster than Baseline. %as it rarely access the sketch counters. 


%depends on the counter value. Thus, in order to process a packet SAC needs to first access the Sketch counters (which requires computing multiple hash counters), and then draw a random number according to the sampling probability of the counters. 

%(and calculate multiple hash functions) and 

%are slower than the baseline since they have to read the counter v


%\subsection{Single Counter}

\subsection{Cache-based Algorithms}
We evaluate our cache-based algorithms compared to their vanilla baseline. Specifically, we compare Space Saving in the original implementation by~\cite{SpaceSavingIsTheBest} (denoted BaselineSS), 
RAP and 16-Way RAP (denoted BaselineRAP and Baseline16W-RAP), and our compressed versions of these algorithms (denoted AAE-SS, AEE-RAP, and AEE-16W-RAP respectively). 
Figure~\ref{4a} shows the update speed. AEE algorithms are an order of magnitude faster than the Baseline algorithms as we do not {need to update the data structures for each packet.}

Figure~\ref{4c}
depicts the error for the NY18 trace. 
At the beginning of the range, each AEE algorithm is more accurate than its corresponding Baseline, and the most accurate ones are Baseline16W-RAP and AEE-16W-RAP. 
At first glance, it may seem strange that we gain better accuracy in the limited associativity model than in the fully associative model. 
However, 16W-RAP can be implemented efficiently in an array, whereas RAP uses the same heap data structure as in the Space Saving implementation,
which requires about 41 bytes per entry~\cite{CormodeCode}. In contrast, 16W-RAP only takes 13 bytes for flow identifier and 4 bytes for the estimator, or a total of 17 bytes per entry. AEE-16W-RAP takes it one step further with just 4 bytes for a fingerprint and 2 for the estimator, i.e., six bytes per entry overall.
%\MM{These last few sentences are very in the weeds and confusing, and we may want to omit.   Or remind the readers about associativity in this context.}\ran{I rewrote it, is it better now?} 
Thus, for a given space, Baseline16W-RAP %\MM{Was this 16 Way not 6-way? This isn't the same notation as before?}
has more entries than BaselineRAP, and AEE-16W-RAP has even more. 
As we increase the amount of space, all Baseline algorithms improve, while the AEE algorithms improve until the estimation error becomes the dominant one. 

%Intuitively, we use the additional memory to allocate more entries, which reduces the algorithmic error. 
%In contrast, the estimator length remains the same and (when the algorithmic error is small enough) the bottleneck is the estimator error. Nevertheless, our estimator is intended to save space for scenarios in which memory is of the essence; when this is not the case, it naturally may not be beneficial to replace counters with smaller estimators.
% \MM{Add some justification here that this is OK -- e.g., remind the reader the whole point of this was to deal with the low memory case, and that's why we compress counters!}   \ran{Sure. Added something.}

\begin{figure}[t]
    \centering
    \subfloat[Error, NY18]
    {\label{4c}\includegraphics[width =0.25\textwidth]
    {figs/caches/caida_cache_error.pdf}}    
    \subfloat[Speed, NY18]
    {\label{4a}\includegraphics[width =0.25\textwidth]
    {figs/caches/caida_cache_speed.pdf}}
\\  
    \hspace*{2mm}{\includegraphics[width =1.0\columnwidth]
    {figs/caches/cache_legend.pdf}\vspace*{-1mm}}
%    \subfloat[Error, NY18]
%    {\label{4c}\includegraphics[width %=0.25\textwidth]
%    {figs/caches/caida_cache_error.pdf}}
%    \subfloat[Error, CH16]
%    {\label{4d}\includegraphics[width %=0.25\textwidth]
%    {figs/caches/univ_cache_error.pdf}}\\
    \caption{A comparison of cache-based algorithms. 
    %The SAC counters are 16-bits.
    \label{fig:cache-based}}\vspace*{-3mm}
\end{figure} 
\subsection{Weighted Counters}
%We now evaluate the performance of the different estimators on weighted traces.
We estimate the total byte volume of the NY18 trace using a single estimator. %We then return to the problem of estimating byte counts for individual flows using the Count Min sketch with our estimators as counters.
%For the single-estimator evaluation we try to estimate the total byte volume of the trace, while for Count Min we use the actual flow identifiers and packet sizes. 
%\MM{This last sentence reads confusing.  Maybe:  We firsty try to use estimate the total byte volume on the trace using a single estimator.  We then return to the problem of estimating byte counts for individual flows using the Count Min sketch with our estimators as counters.}  The static SAC counter was not implemented for weighted updates and thus we only compare with the dynamic variant.
The results are depicted in Figures~\ref{5a} and~\ref{5b}. As in the unweighted case, AEE has better accuracy (${\approx}100\times$) and speed (${\approx}8\times$) compared with Dynamic SAC. 

Figures~\ref{5c} and~\ref{5d} show results for per-flow byte volume estimation on the NY18 trace. 
AEE is more accurate than the baseline ($\approx 7\times$) until the estimation error becomes dominant ($\approx 800KB$). AEE is also faster than the Baseline ($\approx 4.5\times$). 
For accuracy, Dynamic SAC shows a similar trend, but its estimation error becomes dominant at a smaller size. %In addition, Dynamic SAC is slower than the baseline.  
%Further, it is notably faster than Baseline and, when the space is under 512KB, it is also more accurate.
\begin{figure}[t]
    \centering
    \subfloat[16-bit estimators, Error, NY18]
    {\label{5a} \includegraphics[width =0.25\textwidth]
    {figs/weighted/single_weighted_counter_error_16.pdf}}
    \subfloat[16-bit estimators, Speed, NY18]
    {\label{5b}\includegraphics[width =0.25\textwidth]
    {figs/weighted/single_weighted_counter_speed_16.pdf}}\\
    {\includegraphics[width =1.04\columnwidth]
    {figs/weighted/weighted_cms_legend.pdf}\vspace*{-5mm}}
    \subfloat[Weighted CM Sketch, Error, NY18]
    {\label{5c} \includegraphics[width =0.25\textwidth]
    {figs/weighted/weighted_cms_error.pdf}}
    \subfloat[Weighted CM Sketch, Speed, NY18]
    {\label{5d}\includegraphics[width =0.25\textwidth]
    {figs/weighted/weighted_cms_speed.pdf}} 
    \vspace*{-1mm}
    \caption{\small A comparison of the speed and accuracy of single weighted estimators and weighted Count Min Sketch (NY18 trace). The SAC and AEE estimators are 16-bits while the Baseline uses 64.}\label{fig:weighted}
    \vspace*{-3mm}
\end{figure} 

\vspace*{-1mm}
\subsection{The {\sc MaxSpeed} Variant}\label{sec:maxspeedeval}
We now evaluate {\sc MaxSpeed} versus {\sc MaxAccuracy} (which we used in previous sections). 
As shown in Figure~\ref{fig:maxspeed}, {\sc MaxSpeed} is about $4\times$ faster than {\sc MaxAccuracy} while offering similar accuracy when the allocated memory is small. We conclude that {\sc MaxSpeed} is suitable when space is tight or if one requires extremely high speeds.
\begin{figure}[t]
    \centering
    \subfloat[CM Sketch, Error, NY18]
    {\label{6a} \includegraphics[width =0.25\textwidth]
    {figs/maxspeed/cms_error_vs_acc_caida.pdf}}
    \subfloat[CM Sketch, Speed, NY18]
    {\label{6b}\includegraphics[width =0.25\textwidth]
    {figs/maxspeed/cms_speed_vs_acc_caida.pdf}}\\
    {\includegraphics[width =1.02\columnwidth]
    {figs/maxspeed/cc_speed_vs_acc_cms_legend.pdf}}
    \vspace*{-3mm}
    \caption{Comparing the {\sc MaxAccuracy} and {\sc MaxSpeed} variants of AEE \mbox{on Count Min Sketch and NY18 data.}}\label{fig:maxspeed} 
    \vspace*{-4mm}
\end{figure} 
\begin{comment}
\subsection{An Extension -- Weighted Schemes}
Sometimes, it is beneficial to consider \emph{weighted updates}, e.g., for measuring flows' byte volume and not just the number of packets. To that end, we consider replacing our {\sc Increment} operation by {\sc Add}($\mathfrak w$) which increases the count by $\mathfrak w$ (i.e., {\sc Increment} is the equivalent of {\sc Add}($1$)).
Most existing sketches (e.g., Count Min~\cite{CountMinSketch} and Count Sketch~\cite{CountSketch}) and counter-based algorithms (including Space Saving, Frequent and RAP) support weighted updates. It is worth noting that counter-based algorithms either require logarithmic time for such updates or take twice as many counters~\cite{dimsum,AndersonIMSUM}; our approach would give constant amortized time with less space than the logarithmic time algorithms. 
The state of the art sketch compression work~\cite{yang2019generic} supports it as well.

We now describe how to extend our solution to weighted updates; we explain the concept for a single counter, if multiple counters per flow are utilized (such as in sketches), we extend the solution as in Section~\ref{}.
For supporting {\sc Add}($\mathfrak w$), 
%we could follow two approaches. First, 
we can use the geometric sampling to determine how many bytes (size units) to ignore before sampling the next byte. If $\mathfrak w =  O(1/p)$ then this approach works well it will increase the counter $O(\mathfrak wp)=O(1)$ times and have a constant update time. 
However, if $\mathfrak w\gg 1/p$, this may be computationally expensive. 
%A second approach is to sample a binomial random variable $\mathcal I\sim \mbox{Bin}(\mathfrak w, p)$ that would determine the value in which we need to increase the counter. 
Instead, we first break the update in two and set $w_1 = \floor{\mathfrak w p}$ and $w_2 = \mathfrak w - w_1/p$. We then deterministically increment the counter by $w_1$ and follow the geometric approach with the residual weight $w_2$.
Since $w_2<1/p$, our expected update time is constant. Furthermore, the deterministic update only decreases the variance in our counter and improves the accuracy.

\begin{algorithm}[ht]
\caption {Weighted Algorithm for an $n$-bit counter} \label{alg:estimateK}
\begin{algorithmic}[1]
\Statex Initialization: $C\gets 0$, $\mathfrak I \gets 1$
\Procedure{Add}{$\mathfrak w$}
    \State $w_1 = \floor{\mathfrak w p}$
    \State $w_2 = \mathfrak w - w_1/p$
    %\If {$C+w_1\ge 2^n$}
    %    \State $h\gets \floor{\log_2 (C+w_1+1)} - n$ \Comment{\# halvings}
        \State $h\gets \max\set{0,\floor{\log_2 (C+w_1+1)} - n}$ \Comment{\# halvings}
        \State $C \gets \floor{(C+w_1) / 2^h}$
        \State $p\gets p/2^h$
        \While{$w_2\ge \mathfrak I$}
            \If {$C = 2^n - 1$}
                \State $C \gets 2^{n-1}$
                \State $p \gets p/2$
            \Else
                { $C \gets C + 1$}
            \EndIf
            \State $\mathfrak I\gets \mbox{Geo}(p)$
        \EndWhile
    %\EndIf
\EndProcedure
\end{algorithmic}
\label{alg:CFS-FR}
\end{algorithm}
\end{comment}

%\vspace*{-1mm}
\section{Discussion}
Our work explores the opportunities offered by replacing full-sized counters in approximate measurement algorithms with short estimators. Specifically, we observe that the target algorithms provide additive error guarantees, while most estimators are designed to provide multiplicative error, which adds needless complexity in this context. 

We introduce an Additive Error Estimator (AEE) that offers benefits over multiplicative estimators when combined with sketches and cache-based counting algorithms. Most notably, it maintains the same $N\epsilon$ additive error guarantee over any counting range.  Namely, AEE allows us to count indefinitely without overflowing while maintaining the accuracy guarantee.  
Further, AEE offers faster update speed as it increments all counters with the same probability and avoids computing hash functions for non-sampled packets. 
%Further, the AEE estimator is fast, even when used in sketches or cache-based counting algorithms.  
%The speed is due to the AEE using the same probability to increment for all counters, allowing it to utilize geometric sampling.  
% Thus, AEE allows us to design approximate algorithms that can count indefinitely without overflowing while ensuring the same accuracy guarantee. 
%
% Update speed is another shortcoming of previous estimators. 
% Traditionally, integrating estimators into most approximate algorithms (cache- or sketch-based) decreased update speed.
% The reason is that previous approaches use the value of an estimator to determine its probability of increment. 
% This leads to overheads such as hash calculations and hash table accesses. In contrast, our AEE estimator utilizes the same increment probability for all counters, which allows us to determine if we should update the estimator without looking at is value. Through geometric sampling, for most packets, we do nothing but decrement a counter until the next sample, improving the update speed. We have empirically shown that our AEE estimator is both faster and more accurate than existing estimators~\cite{Infocom2019}. 
Our empirical results show that the AEE estimator is faster and more accurate than existing estimators.  
The evaluation also shows the limitations of our estimator, which are in line with the theoretical results.   

The code of our algorithms is available as open source~\cite{opensource}.
\vspace*{-1mm}
%In some cases, we show that using our estimator improves the formal accuracy guarantees of the overall construction. Our work also explains the limitations of our estimator through formal arguments as well as through empirical evidence. Specifically, we also show that in some conditions, AEE  cannot improve the empirical accuracy since its error is the dominating factor. 

% Through a trace-driven simulation, we demonstrate a speed improvement for the Space Saving~\cite{SpaceSavings}, RAP~\cite{RAP}, 16Way-RAP~\cite{RAP}, CM Sketch~\cite{CountMinSketch}, and CU Sketch~\cite{CUSketch}. \MM{This last sentence is messed up -- some of these are sketches, some of these are other estimators?  I think we just want to say we show the speed improvement of the estimator carries over to the sketches.}  We also demonstrate improved accuracy, especially when the available memory is limited.  In summary, our AEE estimator offers improved performance over previous estimators, leading to improvements when it is used with various cache and sketch-based algorithms. To benefit the community, we make the code of all our algorithms available as open source~\cite{opensource}.