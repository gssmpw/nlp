%\documentclass[onecolumn]{IEEEtranTIE}
\documentclass[journal]{IEEEtranTIE}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{picinpar}
\usepackage{amsmath}
\usepackage{url}
\usepackage{flushend}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{color}
\usepackage{alltt}
\usepackage[hidelinks]{hyperref}
\usepackage{enumerate}
\usepackage{siunitx}
\usepackage{breakurl}
\usepackage{epstopdf}
\usepackage{pbox}
% \usepackage{algorithm} %format of the algorithm
% \usepackage{algorithmic} %format of the algorithm
\usepackage[vlined,ruled]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % 用于绘制三线表格
\usepackage{multirow}  % 合并多行
\usepackage{xcolor}
\usepackage{threeparttable}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{booktabs}


\begin{document}


\title{ Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments }

\author{
	\vskip 1em
	
	% First A. Author1, \emph{Student Membership},
	% Second B. Author2, \emph{Membership},
	% \\ and Wenzheng Chi*, \emph{IEEE Senior Member}
    Yanbin Li, Wei Zhang*, \emph{IEEE Member}, Zhiguo Zhang, Xiaogang Shi, Ziruo Li, \\Mingming Zhang,  Hongping Xie and Wenzheng Chi, \emph{IEEE Senior Member}

	\thanks{
		This work is supported by National Science Foundation of China grant \#62273246 and by Science and Technology Research
        Foundation of State Grid Co.Ltd (5700-202318270A-1-1-ZN).
		(* corresponding author)
        
		Yanbin Li, Wei Zhang, Zhiguo Zhang, Xiaogang Shi are with School of Electronics Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China
        (yanbinli@bupt.edu.cn; weizhang13@bupt.edu.cn; zhangzhiguo@bupt.edu.cn;
        shixiaogang@bupt.edu.cn).
        
        Ziruo Li is with Key Lab of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing 100083, China (liziruo123@cau.edu.cn).
        
        Mingming Zhang is with School of Integrated Circuit Science and Engineering, Beihang University, Beijing 100191, China (zmm@buaa.edu.cn).

        Hongping Xie is with Co.Ltd Construction Branch, State Grid Jiangsu Electric Power, Jiangsu 226000, China (sg\_supporting@126.com).

        Wenzheng Chi is with the Robotics and Microsystems Center, School of Mechanical and Electric Engineering, Soochow University, Suzhou 215021, China (wzchi@suda.edu.cn).
	}
}

\maketitle

 
\begin{abstract}

Simultaneous localization and mapping (SLAM) based on particle filtering has been extensively employed in indoor scenarios due to its high efficiency. 
However, in geometry feature-less scenes, the accuracy is severely reduced due to lack of constraints. 
In this article, we propose an anti-degeneracy system based on deep learning. 
Firstly, we design a scale-invariant linear mapping to convert coordinates in continuous space into discrete indexes, in which a data augmentation method based on Gaussian model is proposed to ensure the model performance by effectively mitigating the impact of changes in the number of particles on the feature distribution.
Secondly, we develop a degeneracy detection model using residual neural networks (ResNet) and transformer which is able to identify degeneracy by scrutinizing the distribution of the particle population.
Thirdly, an adaptive anti-degeneracy strategy is designed, which first performs fusion and perturbation on the resample process to provide rich and accurate initial values for the pose optimization, and use a hierarchical pose optimization combining coarse and fine matching, which is able to adaptively adjust the optimization frequency and the sensor trustworthiness according to the degree of degeneracy, in order to enhance the ability of searching the global optimal pose.
Finally, we demonstrate the optimality of the model, as well as the improvement of the image matrix method and GPU on the computation time through ablation experiments, and verify the performance of the anti-degeneracy system in different scenarios through simulation experiments and real experiments. 

This work has been submitted to IEEE for publication. Copyright may be transferred without notice, after which this version may no longer be available.
\end{abstract}


\begin{IEEEkeywords}
Degeneracy Optimization, Degeneracy Detection, Lidar SLAM, Particle Filter.

\end{IEEEkeywords}



\definecolor{limegreen}{rgb}{0.2, 0.8, 0.2}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{greenhtml}{rgb}{0.0, 0.5, 0.0}

\begin{figure}[!t]\centering
	\includegraphics[width=9cm]{photo_new/show_nnew.pdf}
	\caption{The schematic shows the state of two particle swarms before and after anti-degeneracy improvement.}\label{problem_archi_show}
\end{figure}

\section{Introduction}



\IEEEPARstart{S}{lam} is the core of realizing navigation for robots, which enables devices to perform real-time localization and map construction in unknown environments.
By utilizing SLAM technology, devices such as service robots and automated guided vehicles are able to accurately locate themselves based on the environment map, providing the necessary preconditions for path planning and obstacle avoidance.


However, in geometry feature-less scenarios, such as long straight corridors, underground pipelines and spacious halls, the accuracy of SLAM is relatively low. 
This is due to the sparseness of the environmental features causes SLAM to lack sufficient constraints for accurate self-localization.




In monotonous color walls or scenes with repetitive textures, camera-based SLAM systems have difficulty extracting enough feature points for tracking, resulting in degeneracy of positioning accuracy and map quality. 
In addition, in low-light or strongly reflective conditions, the quality of the image captured by the camera can also be significantly degraded, affecting feature extraction and depth estimation \cite{engel2014lsd,forster2014svo,pumarola2017pl,mur2015orb,mur2017orb,campos2021orb}.
Lidar-based SLAM is difficult to perform effective feature matching in degraded environments due to insufficient captured point cloud features. 
Especially in long straight corridors, the lack of orthogonal orientation constraints makes it difficult for the robot to perform accurate localization \cite{zhang2014loam,shan2018lego,shan2020lio,wang2021f}.

Particle swarm distribution is the most intuitive representation of SLAM convergence state, so it is different from other degeneracy detection work based on SLAM process eigenvalues, in this article, we propose a detection method based on particle distribution, and design a self-tuning optimization strategy based on the degree of degeneracy. Compared to other robust SLAM methods with fixed optimization strategies like \cite{9981107}, our method can adaptively adjust the optimization strategy based on the level of degeneracy. This flexibility allows the system to better adapt to different environmental conditions. First, it uses linear mapping and Gaussian augmentation to obtain training samples of degeneracy particles. Through supervised learning, a neural network is endowed with degeneracy detection capabilities. Subsequently, the framework employs the model to analyze particle distribution to detect the degree of degeneracy. Finally, we use an optimization framework based on the degree of degeneracy, which adaptively adjusts strategy to enhance the global optimality of pose estimation and change the contributions of different sensors.
The main contributions of this article are summarized as follows:       


\begin{enumerate}[1)]
	\item A linear mapping method from continuous space to discrete indexes with scale invariance and a data augmentation strategy based on Gaussian model can ensure the accuracy of particle feature distribution and improve the model performance;
	\item A degeneracy detection model based on ResNet and Transformer architectures, with excellent degeneracy detection capability; and
    \item An adaptive anti-degeneracy strategy that enriches initial values through fusion and perturbation during resampling and employs hierarchical pose optimization with coarse and fine matching. This method adaptively adjusts optimization frequency and sensor trustworthiness based on degeneracy levels, improving the ability to find the global optimal pose.
	
\end{enumerate}






\section{Related work}

\subsection{Degeneracy Detection}


Degeneracy detection is able to identify the degeneracy of sensor in real time, so the system can immediately deploy anti-degeneracy strategies to improve the accuracy using the information about degeneracy.
Lee $\textit{et al.}$ \cite{lee2024switch} detected degeneracy by checking the convergence of the optimization process based on predefined thresholds derived from physical assumptions and statistical significance. Tuna $\textit{et al.}$ \cite{tuna2023x} conducted degeneracy detection through fine-grained localizability analysis based on point cloud correspondences, and determines the degeneracy directions by analyzing the principal components of the optimization directions. Zhang $\textit{et al.}$ \cite{zhang2016degeneracy} achieved degeneracy detection by analyzing the geometric structure of the constraints in the optimization problem and identifying the degeneracy directions through the eigenvalues and eigenvectors of the Hessian matrix. Zhou $\textit{et al.}$ \cite{zhou2020lidar}  estimated the sensor state by minimizing the sum of Mahalanobis norms of all measurement residuals. Chen $\textit{et al.}$ \cite{10816047} proposed a P2d degeneracy detection algorithm that uses adaptive voxel segmentation to integrate local geometric features and calculates degeneracy factors from point cloud distribution changes between frames. However, these methods may perform well in specific environments, yet be less robust when faced with conditions such as rapidly changing environments, and relies on a pre-set threshold.

\subsection{Anti-Degeneracy Strategies}


Anti-degeneracy strategy is the key element to improve SLAM accuracy in degraded environments.
Li $\textit{et al.}$ \cite{li2022intensity} dealt with the degeneracy problem through geometric and intensity-based feature extraction by designing two multi-weight functions to fully extract the features of planar and edge points. Zhang $\textit{et al.}$ \cite{zhang2024lvio} presented an innovative tightly coupled lidar-vision-inertial odometry, which enables accurate state estimation in degraded environments through sensor fusion strategies. Zhang $\textit{et al.}$ \cite{zhang2023graph} utilized ARTag as a visual marker to assist in localization, which employs a bitmap optimization method to reduce the error. Lee $\textit{et al.}$ \cite{lee2024switch} proposed a switching-based SLAM that achieves high accuracy by switching from lidar to visual odometry upon detection of lidar degeneracy. 
However, these methods have requirements on the geometry of environment and the strength of features. Meanwhile, the multi-sensor fusion strategy increases the deployment cost, the parameters need to be adjusted for adaptation, which leads to the fact that these methods do not have good robustness.




\section{System overview}


The framework diagram of the anti-degeneracy system presented in this article is shown in Fig. \ref{system_pipeline}. 
Particle filter-based SLAM represents a posteriori probability distribution of robot positions and maps by maintaining a set of random particles \cite{montemerlo2002fastslam,grisetti2007improved}.
Particles are first preliminarily updated using the odometry data from the IMU and motor encoder through the motion model, as shown in (\ref{motion_model}). Subsequently, the observation model is employed to update the particles based on the lidar data to determine the optimal pose through pose estimation. The particle weights are then calculated according to (\ref{weight}), reflecting the uncertainty of the pose estimation. Finally, resampling and map updating operations are carried out.
\begin{equation}
\label{motion_model}
\begin{bmatrix} X ^ { \prime }  \\ Y ^ { \prime }  \\ \theta ^ { \prime }  \end{bmatrix} = \begin{bmatrix} X \\ Y \\\theta\end{bmatrix} + \begin{bmatrix} \delta _ { t } . \cos ( \theta + \delta _ { r1 } ) \\ \delta _ { t } . \sin ( \theta + \delta _ { r1 } ) \\ \delta _ { r1 } + \delta _ { r2 } \end{bmatrix}
\end{equation}
where $(X, Y) \in \mathbb{R}^2$ represent the position of robot, and $\theta \in \mathbb{S}^1$ represents its orientation. $\delta_t$ is the translation amount, while $\delta_{r1}$ and $\delta_{r2}$ are the rotation amounts before and after translation, respectively.

\begin{equation}
\label{weight}
w _ { i } = w _ { i } \times \prod _ { j = 1 } ^ { n } p ( z _ { j } | x _ { i } )
\end{equation}
where $w_i \in \mathbb{R}^+$ and $x_i \in \mathbb{R}^2$ represent the weight and pose of the $i_{th}$ particle, respectively, while $z_j$ denotes the $j_{th}$ measurement and $n$ is the amount of measurement.

Degeneracy is typically manifested as the uncertainty of the pose, which is reflected by the weights and distribution of the particles. Dense particle swarms can better capture the nonlinear characteristics of the state space and reduce the effect of random errors. In feature-rich environments, the particle swarms tend to be aggregated, which represents high localization accuracy, conversely, in degraded environments, the particle swarms tend to be dispersed, which represents low accuracy, as shown in Fig. \ref{problem_archi_show}. To this end, we transform the degeneracy detection into a classification task for a neural network, analyzing the degree of degeneracy through the dispersion of particles.

We utilize robot operating system (ROS) to transmit particle coordinates to the degeneracy detection node, where they are converted into images via linear mapping and processed using data augmentation based on a Gaussian model. After the model predicts the degree of degeneracy, the anti-degeneracy node receives the prediction feedback via ROS and adaptively adjusts the optimization strategy according to the degeneracy level to enhance the search for the optimal pose and dynamically adjust the weights of different sensors.







\begin{figure}[!t]\centering
	\includegraphics[width=9cm]{photo_new/system_process.pdf}
	\caption{Overall flow chart of the system.}\label{system_pipeline}
\end{figure}

\begin{figure*}[t]\centering
	\includegraphics[width=16.7cm]{photo_new/model_arch_new.pdf}
	\caption{The architecture of degeneracy detection model.}\label{model}
\end{figure*}


\section{METHODOLOGY}




\subsection{Neural Network Based Degeneracy Detection}

\subsubsection{Linear Mapping with Scale Invariance}


The quality of the particle image dataset is crucial for the model training. The uncertainty in the coordinate size of the particles leads to them being stretched or shrunk in various dimensions, making the relative position and distribution of the particles distorted. In order to solve this problem, we design a mapping method from continuous space to discrete index for particle swarms of various sizes that ensures they maintain original distribution and result in scale-invariant images.


We first define the canvas size of the image as 10m in reality, which defines the accuracy threshold for localization and the outliers are considered invalid particles. 
Subsequently, the image boundary are modified according to (\ref{resize}). The procedure is to first modify the extreme values of the particle coordinates in the x and y dimensions, ensuring that the coordinates in each dimension have the same discrete mapping metric respectively.
\begin{equation}
\label{resize}
\begin{cases} 
r_{\text{min}} = r_{\text{min}} - \frac{s - l}{2}, & \text{if } l_r < s \\ \\
r_{\text{max}} = r_{\text{max}} + \frac{s - l}{2}, & \text{if } l_r < s \\
\end{cases}
\end{equation}
where $r_{max}$ and $r_{min}$ represent the maximum and minimum values of the particle coordinates in each dimension, $l$ is the absolute value of the difference between $r_{max}$ and $r_{min}$, and $s$ is the size of the boundary of the image.


Then we discretize the continuous spatial coordinates of the particle swarm into pixel indices, translating physical locations to image space pixels. 
By linearly mapping the coordinate extremes changes in (\ref{resize})  to all particles, the scale invariance can be matained across both the physical and image spaces, as detailed in (\ref{index}), where $i$ represents the discrete index corresponding to the particle coordinates in the image, which represents the pixel position.
\begin{equation}
\label{index}
\text{i} =  \frac{(\text{r} - r_{\text{min}}) \cdot \text{s}}{r_{\text{max}} - r_{\text{min}}}, 
\end{equation}



\subsubsection{Data Augmentation based on Gaussian Model}


Particle distributions can be affected by changes in number, which in turn affects model performance. For example, particle swarm in a clustered state may become sparse due to a decrease in number. In addition, too many particles can lead to overlapping problem, which can also change the original distribution.


In order to address this problem, we adopt the Gaussian model to simulate the influence of each particle on the surrounding like mesh, where the position of the particle serves as the center of the Gaussian distribution, and the influence of the particle decreases exponentially with distance.
The Gaussian function is defined as:
\begin{equation}
G(x, y) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{(x - x_p)^2 + (y - y_p)^2}{2\sigma^2}\right),
\end{equation}
where $(x, y) \in \mathbb{Z}^2$ and $(x_p, y_p) \in \mathbb{Z}^2$ represent the grid positions influenced by the particles and of the particles themselves, respectively; and $\sigma$ is the standard deviation, we adjust it to make the influence range of the Gaussian be a 5*5 grid, as we found that the augmentation effect of 5*5 grid is good for the commonly used number of particles.



For each grid, we compute the Gaussian contributions of other particles to it and then accumulate these contributions to update the occupancy of that grid:
\begin{equation}
\text{o}(x, y) = \sum_{i=1}^{5}\sum_{j=1}^{5} G(x, y),
\end{equation}
where $o$ stands for the occupancy, we sum Gaussian influence within a 5*5 range around each grid. If the value exceeds a threshold, it is considered occupied. We prefer a fixed low threshold, as more occupied grids would not affect the result.





\subsubsection{Degeneracy Detection Model}



As the number of particles increases, the model needs to deepen its architecture to capture complex features. Compared to other classical models, the design based on ResNet can more flexibly expand its depth. After introducing transformer, the accuracy is increased by about 5\%.

The overall architecture of the model is shown in Fig. \ref{model}. After obtaining the image dataset of the particle swarm, we first grayscale the image to preserve the contour and shape of the particle swarm and remove redundant information.
Subsequently, the grayscaled data first passes through a convolutional layer with a convolutional kernel size of 7*7 and a step size of 2, which facilitates the extraction of abstract features by the model.
Then it passes through a batch normalization (BN) layer, which solves the problem of bias of internal covariates in the network by regularization method. This process involves calculating the mean and variance of the input feature x, as shown in (\ref{mean_variance}). Then the features are normalized using (\ref{x'}), with a $\epsilon$ added to the denominator to prevent division by zero. Finally introduce the learnable parameters $\gamma$ and $\beta$, and perform the scaling and panning according to (\ref{y}). 
This network layer can speed up the convergence of the model and alleviate the problem of exploding or vanishing gradients. The next layer is the activation function layer, where we employ the ReLU function to enhance the nonlinear representation of the model. This is followed by a maximum pooling layer for downsampling to further reduce the computational effort.
\begin{equation}
\label{mean_variance}
\begin{cases} 
\mu = \frac{1}{N} \sum_{i=1}^{N} x_i \\ \\
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 \\
\end{cases}
\end{equation}
\begin{equation}
\label{x'}
x^{\prime} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}},
\end{equation}
\begin{equation}
\label{y}
 y = \gamma x^{\prime} + \beta,
\end{equation}


Next, based on the ResNet architecture, we design the backbone of the network, which consists of a total of 4 layers, and each layer gradually extracts the features of the image through a different number of BasicBlocks, and the abstraction level of the particle distribution features is gradually increased from the shallow layer to the deeper layer. The BasicBlock is a residual block structure, which contains 3*3 convolutional layers , BN, and ReLU functions, the convolution layer can reduce the spatial size of the feature map $f \in \mathbb{R}^{C \times H \times W}$, making the feature map more compact and highlighting the important features. BasicBlock also maintains the gradient flow by constant mapping to make the features skip certain network layers for propagation. The use of BN and ReLU activation function in it makes the network training more stable, accelerates the convergence, improves the generalization ability, and, finally, generates feature maps enriched with high-level semantic information and ensures consistent feature dimensions when the residuals are connected by downsample layer.


We take the transformer as the neck part of the network, and firstly, the feature map with the size of 256*28*28 output from the backbone is spread, and then the features are adjusted to match the dimensions of the input from the transformer through a linear layer. The adjusted features, which are input to the transformer encoder, are further enhanced through the self-attention mechanism and feed-forward network processing sequences to enable the model to capture long range dependencies and improve the representation of the features.


Finally, in the head portion of the network, we use a fully connected layer as a classification header, which is capable of converting high-dimensional features extracted from complex network structures into class-specific predictions, and the output is a vector consisting of two values ranging from 0 to 1 to describe the degree of degradation.




\subsection{Hierarchical Anti-Degeneracy Strategies}


\subsubsection{Global Position Optimization Based on the Degree of Degeneracy}


This article proposes a global pose optimization strategy based on degeneracy confidence. The schematic is shown in Fig. \ref{pose_optimize}.
\begin{figure}[t]\centering
	\includegraphics[width=9cm]{photo_new/pose_estimation.pdf}
	\caption{Schematic diagram of global position optimization.
}\label{pose_optimize}
\end{figure}
The position is first initially adjusted using the odometry model, and then the scan matching of observation model is used for further optimization.
We divide the lidar-based position optimization into a combination of coarse and fine matching. 
In the coarse matching phase, the robot will do displacements in eight directions with a step size of $\delta_l \in \mathbb{R}$ and rotations with an angle of $\delta_a \in \mathbb{S}^1$, and after each action has been performed, the current matching score $S \in \mathbb{R}^+$ will be calculated, the one with the highest score will be selected as the best position and updated, whereas in the fine matching phase, the scales of $\delta_l/2$ and $\delta_a/2$ will be used to perform the same process. 
The number of executions of the above matching combinations will adaptively change with the degeneracy level to ensure the accuracy of the search space. In addition, we design an factor $\varphi$ that includes degeneracy level $c$ to control the dependence on different sensors by changing the matching score, which tends to trust the position given by the odometry when the lidar degeneracy is severe, as shown in (\ref{pose_es}).
\begin{equation}
\begin{cases}
\label{pose_es}
\varphi =  -e^{0.7c}+e^{0.35}+1 \\
s = \sum_{i=1}^{n} (s_i+e^{(-\psi^2 / \sigma )}) \cdot \varphi  \\
\end{cases}
\end{equation}
where $c$ is the degeneracy confidence given by the model, $s_i$ is the fraction of the $i^{th}$ beam, $\psi$ is the distance between the predicted value and the true value at the hit point, and $\mu$ is the standard deviation of the Gaussian distribution.




\subsubsection{Enhanced Resampling for Particles}


The resampling reduces particle diversity. To address this, we design an enhanced resampling method that includes selecting, fusing, and perturbing operations. First, we remove low-weight particles and keep high-weight ones. Then, for each particle, we select a particle with the closest Euclidean distance to perform a mean-crossing operation. This combines the positional information of two particles to generate new ones, increasing diversity.
Subsequently, we add a random perturbation to the fusion result to further increase the diversity, allowing the particle population to better represent the state space in which the robot may be located, as shown in (\ref{eq:random_perturbation}). 
The Gaussian form of the perturbation is more stable and balances the probability of the exploration of different regions of the state space, thus increases the probability of exploring the optimal pose.
\begin{equation}
\label{eq:random_perturbation}
\begin{cases} 
x' = x_n + \frac{1}{w } \cdot \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\
y' = y_n + \frac{1}{w } \cdot \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\mu)^2}{2\sigma^2}} \\
\end{cases}
\end{equation}
where $(x_n,y_n) \in \mathbb{R}^2$ are the coordinates after performing mean crossing, $(x',y') \in \mathbb{R}^2$ are the new coordinates after adding the perturbation, $w$ represents the particle weights.




\begin{table*}[]
\centering
\caption{Results of the ablation experiments.}
\label{performance_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccccc}
\hline
 &
   &
   &
   &
   &
  \multicolumn{2}{c}{Loss} &
  \multicolumn{2}{c}{Optimizer} &
   &
   &
   \\ \cline{6-7} \cline{8-9}
\multirow{-2}{*}{Model} &
  \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Linear\\ Mapping\end{tabular}} &
  \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Gaussian \\ Augmentation\end{tabular}} &
  \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}GPU\\ Acceleration\end{tabular}} &
  \multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Image\\ Matrix\end{tabular}} &
  \multicolumn{1}{c}{CrossEntropyLoss} &
  BCELoss &
  \multicolumn{1}{c}{Adam} &
  RMSprop &
  \multirow{-2}{*}{Accuracy} &
  \multirow{-2}{*}{F1 Score} &
  \multirow{-2}{*}{Time(ms)} \\ \hline
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.901 &
  0.851 &
  20 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  0.8965 &
  0.839 &
  24 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.8934 &
  0.853 &
  25 \\ 
\multirow{-4}{*}{MobileNetV3} &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  0.8828 &
  0.85 &
  22 \\ \hline
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  - &
  - &
  130 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  - &
  - &
  45 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  - &
  - &
  112 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  0.975 &
  0.933 &
  {\color[HTML]{3531FF} \textbf{14}} \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.979 &
  0.942 &
  15 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  \multicolumn{1}{c}{-} &
  $\checkmark$ &
  {\color[HTML]{3531FF} \textbf{0.981}} &
  {\color[HTML]{3531FF} \textbf{0.963}} &
  17 \\ 
 &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  {\color[HTML]{FE0000} \textbf{0.993}} &
  {\color[HTML]{FE0000} \textbf{0.972}} &
  {\color[HTML]{FE0000} \textbf{10}} \\ 
 &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.826 &
  0.783 &
  - \\ 
 &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.852 &
  0.823 &
  - \\ 
\multirow{-10}{*}{DD-Model(Ours)} &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{CBCEFB}- &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \cellcolor[HTML]{FFCCC9}$\checkmark$ &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  \multicolumn{1}{c}{$\checkmark$} &
  - &
  0.912 &
  0.89 &
  - \\ \hline
\end{tabular}%
}
\end{table*}






\section{Experimental studies and results}





We use a laptop computer with an Intel i7-7700HQ CPU and an NVIDIA GeForce GTX 1060 GPU to remotely maneuver an wheeled robot for data collection and validation of anti-degeneracy system. 
The robot is equipped with an LSlidar M10P lidar, which is powered by an ARM Cortex-A72 64-bit CPU and a Broadcom VideaCore VI GPU, as shown in Fig. \ref{robot_platform}.
In simulations, we adopt a computer with ubuntu 20.04 system with a cpu configuration of 13th Gen Intel Core i7-13700K, GPU of NVIDIA GeForce RTX 2060 super.


\begin{figure}[t]\centering
	\includegraphics[width=5cm]{photo_new/instrument.pdf}
	\caption{Experimental platforms.}\label{robot_platform}
\end{figure}


\subsection{Performance of Degeneracy Detection}



\subsubsection{Dataset Generation}

About the dataset, we record the coordinates of the particles in different scenarios, including straight corridor scenes, feature-sparse outdoor scenes, and feature-rich indoor scenes. We compare the ratio $R$ of the variance to the mean of the particle weights with the degeneracy threshold to generate binary labels. The calculation of $R$ is shown in (\ref{label_explain}). 
\begin{equation}
\label{label_explain}
R = \frac { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } ( w _ { i } - \overline { w } ) ^ { 2 } } { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } w _ { i } }
\end{equation}
where $w$ is the particle weight, $\overline { w }$ is the mean of the weights, and $N$ is the total number of particles.

The degeneracy threshold is determined to be 1 by averaging the values of $R$ from a large number of degeneracy samples. Schematics of the scene used to collect the data and several examples of the particle dataset are shown in Fig. \ref{dataset}, where there are 3937 degraded images and 3827 non-degraded images, and the ratio of training set, test set and validation set is 6:2:2. 







\begin{figure}[t]\centering
	\includegraphics[width=8.5cm]{photo_new/dateset_photo.pdf}
	\caption{Examples of data collection scenarios and datasets.}\label{dataset}
\end{figure}

\begin{figure*}[htb]\centering
	\includegraphics[width=18cm]{photo_new/simul_result.pdf}
	\caption{Evaluation plot of trajectory accuracy for simulation experiments.}\label{simul_locate_result}
\end{figure*}


\begin{table*}[htb]
\centering
\fontsize{7.5}{5}\selectfont
\begin{threeparttable}
\caption{Positioning errors in simulation experiments.}
\label{simul_com}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccccccccccccc}
	\toprule
 
	\multirow{3}{*}{\textbf{Method}}
    
    &\multicolumn{3}{c}{\textbf{Indoor environment}}
    &\multicolumn{3}{c}{\textbf{Straight corridor}}
    &\multicolumn{3}{c}{\textbf{Circular corridor}}
    &\multicolumn{3}{c}{\textbf{Outdoor open environment}}\cr
    % &\multirow{3}{*}{\shortstack{\textbf{Average}\\\textbf{Precision(\%)}} }
    
 
	\cmidrule(l){2-4} % 命令表格划横线
	\cmidrule(l){5-7} % 命令表格划横线
	\cmidrule(l){8-10} % 命令表格划横线
	\cmidrule(l){8-10} % 命令表格划横线
	\cmidrule(l){11-13} % 命令表格划横线
    
	&ATE&$Error_x$&$Error_y$&ATE&$Error_x$&$Error_y$&ATE&$Error_x$&$Error_y$&ATE&$Error$... \\


 
	\midrule
	\multirow{1}{*}{Gmapping(60)}
    
    &0.1319&0.07183&0.08162
    &0.22059&0.17182&0.3505
    &1.99140&0.98137&1.063
    &2.78390&1.05844&1.10095\cr
    
	\multirow{1}{*}{Gmapping(45)}
    
    &0.1373&0.07405&0.08959
    &0.47990&0.25928&0.45251
    &3.61930&1.60959&1.92411
    &3.27640&1.15916&1.42285\cr
    
	\multirow{1}{*}{Gmapping(30)}
    
    &0.1498&0.0780&0.1042
    &0.75860&0.30329&0.7886
    &5.68730&2.40482&3.03794
    &3.54170&1.49342&1.58323\cr

    \multirow{1}{*}{Ours}
    
    &0.1180&0.0384&0.0402
    &0.15760&0.10483&0.11407
    &0.55660&0.18372&0.20154
    &0.72270&0.14142&0.19495\cr
    
	\bottomrule
\end{tabular*}
% \begin{tablenotes}
% \item 111
% \end{tablenotes}
\end{threeparttable}
\end{table*}



\begin{table}[]
\centering
\caption{Comparison of Switch-SLAM and ours.}
\label{detection_comparsion}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccc}
\hline
                     & Accuracy & Precision & Recall & F-1 Score \\ \hline
Switch-SLAM {\cite{lee2024switch}} & 0.775    & 0.767     & 0.735  & 0.751    \\ 
Ours                 & \textbf{0.965}    & \textbf{0.971}     & \textbf{0.945}  & \textbf{0.952}     \\ \hline
\end{tabular}%
}
\end{table}



\subsubsection{Ablation Experiments}

We conduct ablation experiments to evaluate the contributions of different components in our degeneracy detection model. We test various loss functions and optimizers on two model architectures: MobileNetV3 and DD-Model, and measured accuracy, F1 score, and inference time on the test set as shown in Table. \ref{performance_comparison}. The results show that MobileNetV3 achieve an accuracy of around 90\% and an F1 score of around 0.85, indicating good performance with room for improvement. In contrast, DD-Model achieve an accuracy of 99.3\% and an F1 score of 0.972, demonstrating superior degeneracy detection capability and the ability to identify the vast majority of degraded states. Based on these results, we select CrossEntropyLoss and Adam as our hyperparameters.

We also test the impact of GPU acceleration and image matrix on computation time. The results show that these optimizations reduce the computation time from 130ms to 112ms and 45ms respectively, with the fastest speed reaching 10ms, ensuring the real-time performance of degeneracy detection and SLAM. 
Finally, we verify the contributions of linear mapping and Gaussian augmentation. Experiments show that removing linear mapping and directly feeding the particle coordinates to the model, or removing Gaussian Augmentation, both lead to a decrease in accuracy. This demonstrates that both modules significantly contribute to improving model accuracy.





\begin{figure}[t]\centering
	\includegraphics[width=8.9cm]{photo_new/simul_env_map.pdf}
	\caption{The simulation scenarios and the constructed maps.}\label{simul_ex}
\end{figure}


\begin{figure*}[htb]\centering
	\includegraphics[width=18cm]{photo_new/actual_location_result.pdf}
	\caption{Evaluation plot of trajectory accuracy for actual experiments.}\label{actual_locate_result}
\end{figure*}


\begin{table*}[htb]
	
\centering
\fontsize{7.5}{5}\selectfont
\begin{threeparttable}
\caption{Positioning errors in actual experiments.}
\label{actual_com}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccccccccccccc}
	\toprule
 
	\multirow{3}{*}{\textbf{Method}}
    
    &\multicolumn{3}{c}{\textbf{Straight corridor A}}
    &\multicolumn{3}{c}{\textbf{Straight corridor B}}
    &\multicolumn{3}{c}{\textbf{Circular corridor}}
    &\multicolumn{3}{c}{\textbf{Outdoor open environment}}\cr
    % &\multirow{3}{*}{\shortstack{\textbf{Average}\\\textbf{Precision(\%)}} }
    
 
	\cmidrule(l){2-4} % 命令表格划横线
	\cmidrule(l){5-7} % 命令表格划横线
	\cmidrule(l){8-10} % 命令表格划横线
	\cmidrule(l){8-10} % 命令表格划横线
	\cmidrule(l){11-13} % 命令表格划横线
    
	&ATE&$Error_x$&$Error_y$&ATE&$Error_x$&$Error_y$&ATE&$Error_x$&$Error_y$&ATE&$Error$... \\


 
	\midrule
	\multirow{1}{*}{Gmapping(60)}
    
    &0.5481&0.3146&0.39707
    &0.6806&0.33272&0.41618
    &0.8391&0.42145&0.75231
    &5.2080&2.12375&3.91135\cr
    
	\multirow{1}{*}{Gmapping(45)}
    
    &0.7613&0.50234&0.56628
    &0.7819&0.41485&0.54816
    &1.4794&1.0563&1.22102
    &5.5455&2.51211&4.24782\cr
    
	\multirow{1}{*}{Gmapping(30)}
    
    &0.8864&0.6478&0.6782
    &0.9285&0.5699&0.66769
    &2.0767&1.27385&1.64918
    &5.9426&2.86529&4.65336\cr

    \multirow{1}{*}{Ours}
    
    &0.2313&0.01996&0.02598
    &0.5520&0.01237&0.01297
    &0.4683&0.08895&0.1624
    &0.8229&0.22784&0.20634\cr
    
	\bottomrule
\end{tabular*}
% \begin{tablenotes}
% \item 111
% \end{tablenotes}
\end{threeparttable}
\end{table*}






\subsubsection{Accuracy of Degeneracy Detection}

In the degeneracy detection experiment, we use \cite{lee2024switch} as the baseline and replace the eigenvalues of the Hessian matrix with those of the covariance matrix $\Sigma$ of particle weights, the calculation is shown in (\ref{w_vector})-(\ref{w_eigen}). 
\begin{equation}
\label{w_vector}
w = \left[ w _ { 1 } , w _ { 2 } , \ldots , w _ { N } \right]
\end{equation}
\begin{equation}
\label{w_eigen}
\Sigma = \frac { 1 } { N - 1 } \sum _ { i = 1 } ^ { N } ( w _ { i } - \overline { w } ) ( w _ { i } - \overline { w } ) ^ { T }
\end{equation}
where $w$ is a vector of particle weights, $\Sigma$ is the covariance matrix of the weights, and $\overline { w }$ is the mean of the weights.

This is because small eigenvalues can reflect the uncertainty in particle estimation, similar to the degeneracy direction indicated by the eigenvalues of the Hessian matrix. Degeneracy states are detected by comparing with the degeneracy threshold defined in \cite{lee2024switch}. The comparative results on validation set are shown in Table. \ref{detection_comparsion}. Our method shows significant improvements in all metrics compared to \cite{lee2024switch}, indicating that our detection method has higher accuracy and robustness.







\begin{figure}[!t]\centering
	\includegraphics[width=8.9cm]{photo_new/actual_env_map.pdf}
	\caption{The real-world scenarios and the constructed maps.}\label{actual_ex}
\end{figure}

\subsection{Performance of Anti-Degeneracy Systems in Simulation Environments}

We verify the performance of anti-degeneracy SLAM in various environments through simulation experiments. 
In terms of experimental scenarios, we build a 10m*10m indoor environment, a 140m*1m long straight corridor, a 100m*100m circular corridor, and an open scenario with sparse landmarks, as shown in the first row of Fig. \ref{simul_ex}. 
In terms of evaluation indicators, we plot the maps given by Gmapping which is particle filter-based SLAM with 60 particle as well as by the anti-degeneracy SLAM, in rows 2 and 3 of Fig. \ref{simul_ex}, and also plot the localization trajectories of Gmapping when the number of particles is 60, 45, and 30, respectively, the anti-degeneracy SLAM with 30 particles and the ground truth given by gazebo, as shown in Fig. \ref{simul_locate_result}. 
In addition, the absolute value of the difference between each trajectory and the true value in the x and y dimensions is plotted in Fig. \ref{simul_locate_result}, and its mean value ($Error_x$, $Error_y$) is shown in Table. \ref{simul_com}.
Finally, the accuracy is quantitatively evaluated by absolute trajectory error (ATE) using evo \cite{grupp2017evo}.


As can be seen from the maps in Fig. \ref{simul_ex}, the anti-degeneracy SLAM is able to accurately construct the environmental structure of each scene, while the Gmapping is less effective in constructing maps in degraded environments, with skewed straight corridors in the second scene, corridors that are not closed in the third, and landmarks in the fourth with incorrect relative positions.
Meanwhile, the anti-degeneracy SLAM has the highest fit between the localization trajectory and the true value, and $Error_x$ and $Error_y$ are maintained within 1m indicating that its localization error is small in all dimensions. We find that the ATE metrics of the anti-degeneracy SLAM decreased in all the scenarios, especially in the corridor scenario, the results are the best, with an accuracy improvement of 90.21\%. Theoretically, the higher the number of particles, the higher the SLAM accuracy, but the required computational resources will also rise sharply. And the above results show that increasing the number of particles does not significantly improve the robustness of SLAM in degraded scenes, on the contrary, our anti-degeneracy SLAM is able to maintain high localization accuracy with good robustness in degraded as well as non-degraded environments.












\subsection{Performance of Anti-Degeneracy Systems in Real-World Environments}

We also test anti-degeneracy SLAM in real-world environments. For the experimental scenarios, the setup is a 50m*2.5m long corridor, a 60m*3m long corridor, a 34m*24m circular corridor and an outdoor open scenario, as shown in Fig. \ref{actual_ex}. In terms of evaluation metrics, it is the same as the simulation experiments, in which the trajectory truth values are manually measured.

As shown in lines 2 and 3 of Fig. \ref{actual_ex}, the anti-degeneracy SLAM constructs accurate maps of the environment. In contrast, the Gmapping produces less accurate maps that exhibit several issues, like long straight corridors appear skewed, circular corridors fail to close properly, and there are errors in the relative position of landmarks in outdoor scenes.

In terms of localization accuracy, it can be seen from Fig. \ref{actual_locate_result} that even if the localization of other SLAM systems is gradually shifted, our proposed SLAM system is able to ensure the accuracy of localization by anti-degeneracy strategy with minimum gap between the true value. And Table. \ref{actual_com} demonstrates that the Gmapping has a large ATE in all degraded scenarios and is unable to maintain good localization accuracy. The ATE of the anti-degeneracy SLAM decreases in all scenarios, especially in the outdoor open environment, where the ATE decreases from 5.9426 to 0.8229, which is roughly 86.2\%, and the $Error_x$ and $Error_y$ are both at a lower value level as well. The above experiments show that in degraded scenarios, the sparseness of environmental features leads to insufficient constraints of SLAM, which makes the localization suffer from error accumulation. And anti-degeneracy SLAM can ensure the accuracy of localization and improve the robustness in various environments.








\section{Conclusion}

This article presents a deep learning-based anti-degeneracy SLAM system. We first propose a linear mapping and Gaussian-based data augmentation to ensure high-quality training datasets. Next, we develop a degeneracy detection model using ResNet and transformer, which effectively classifies particle distributions to determine degeneracy levels. Finally, we design a hierarchical anti-degeneracy strategy that enhances resampling through selection, fusion, and perturbation, providing accurate initial values for pose optimization. The optimization process can adaptively adjust the optimization frequency and sensor trustworthiness based on degeneracy levels, improving global pose estimation and reducing localization drift.
We demonstrate the optimality of the model and the contribution of each module in terms of accuracy and computation time through ablation experiments. In addition, simulation experiments and real experiments are conducted to verify the performance of the anti-degeneracy SLAM in various scenarios. The experimental results show that the anti-degeneracy SLAM is able to construct accurate maps for most of the scenarios, and the reduction of ATE indicates the improvement of its localization accuracy with good robustness.




















\bibliographystyle{Bibliography/IEEEtranTIE}
\bibliography{Bibliography/IEEEabrv,references.bib}\ %IEEEabrv instead of IEEEfull




    

\end{document}