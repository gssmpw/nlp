@article{kitcher1981explanatory,
  title={Explanatory unification},
  author={Kitcher, Philip},
  journal={Philosophy of science},
  volume={48},
  number={4},
  pages={507--531},
  year={1981},
  publisher={Cambridge University Press}
}

@article{valentino2024nature,
  title={On the nature of explanation: An epistemological-linguistic perspective for explanation-based natural language inference},
  author={Valentino, Marco and Freitas, Andr{\'e}},
  journal={Philosophy \& Technology},
  volume={37},
  number={3},
  pages={88},
  year={2024},
  publisher={Springer}
}

@inproceedings{
zheng2024take,
title={Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
author={Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=3bq3jsvcQ1}
}

@article{Valentino_Thayaparan_Ferreira_Freitas_2022, title={Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21392}, DOI={10.1609/aaai.v36i10.21392}, abstractNote={Regenerating natural language explanations in the scientific domain has been proposed as a benchmark to evaluate complex multi-hop and explainable inference. In this context, large language models can achieve state-of-the-art performance when employed as cross-encoder architectures and fine-tuned on human-annotated explanations. However, while much attention has been devoted to the quality of the explanations, the problem of performing inference efficiently is largely under studied. Cross-encoders, in fact, are intrinsically not scalable, possessing limited applicability to real-world scenarios that require inference on massive facts banks. To enable complex multi-hop reasoning at scale, this paper focuses on bi-encoder architectures, investigating the problem of scientific explanation regeneration at the intersection of dense and sparse models. Specifically, we present SCAR (for Scalable Autoregressive Inference), a hybrid framework that iteratively combines a Transformer-based bi-encoder with a sparse model of explanatory power, designed to leverage explicit inference patterns in the explanations. Our experiments demonstrate that the hybrid framework significantly outperforms previous sparse models, achieving performance comparable with that of state-of-the-art cross-encoders while being approx 50 times faster and scalable to corpora of millions of facts. Further analyses on semantic drift and multi-hop question answering reveal that the proposed hybridisation boosts the quality of the most challenging explanations, contributing to improved performance on downstream inference tasks.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Valentino, Marco and Thayaparan, Mokanarangan and Ferreira, Deborah and Freitas, André}, year={2022}, month={Jun.}, pages={11403-11411} }

@inproceedings{valentino-etal-2021-unification,
    title = "Unification-based Reconstruction of Multi-hop Explanations for Science Questions",
    author = "Valentino, Marco  and
      Thayaparan, Mokanarangan  and
      Freitas, Andr{\'e}",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.15/",
    doi = "10.18653/v1/2021.eacl-main.15",
    pages = "200--211",
    abstract = "This paper presents a novel framework for reconstructing multi-hop explanations in science Question Answering (QA). While existing approaches for multi-hop reasoning build explanations considering each question in isolation, we propose a method to leverage explanatory patterns emerging in a corpus of scientific explanations. Specifically, the framework ranks a set of atomic facts by integrating lexical relevance with the notion of unification power, estimated analysing explanations for similar questions in the corpus. An extensive evaluation is performed on the Worldtree corpus, integrating k-NN clustering and Information Retrieval (IR) techniques. We present the following conclusions: (1) The proposed method achieves results competitive with Transformers, yet being orders of magnitude faster, a feature that makes it scalable to large explanatory corpora (2) The unification-based mechanism has a key role in reducing semantic drift, contributing to the reconstruction of many hops explanations (6 or more facts) and the ranking of complex inference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed explanations can support downstream QA models, improving the accuracy of BERT by up to 10{\%} overall."
}

@inproceedings{valentino-etal-2022-case,
    title = "Case-Based Abductive Natural Language Inference",
    author = "Valentino, Marco  and
      Thayaparan, Mokanarangan  and
      Freitas, Andr{\'e}",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.134/",
    pages = "1556--1568",
    abstract = "Most of the contemporary approaches for multi-hop Natural Language Inference (NLI) construct explanations considering each test case in isolation. However, this paradigm is known to suffer from semantic drift, a phenomenon that causes the construction of spurious explanations leading to wrong conclusions. In contrast, this paper proposes an abductive framework for multi-hop NLI exploring the retrieve-reuse-refine paradigm in Case-Based Reasoning (CBR). Specifically, we present Case-Based Abductive Natural Language Inference (CB-ANLI), a model that addresses unseen inference problems by analogical transfer of prior explanations from similar examples. We empirically evaluate the abductive framework on commonsense and scientific question answering tasks, demonstrating that CB-ANLI can be effectively integrated with sparse and dense pre-trained encoders to improve multi-hop inference, or adopted as an evidence retriever for Transformers. Moreover, an empirical analysis of semantic drift reveals that the CBR paradigm boosts the quality of the most challenging explanations, a feature that has a direct impact on robustness and accuracy in downstream inference tasks."
}

@inproceedings{ranaldi-freitas-2024-self,
    title = "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    author = "Ranaldi, Leonardo  and
      Freitas, Andre",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.139/",
    doi = "10.18653/v1/2024.emnlp-main.139",
    pages = "2325--2347",
    abstract = "The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models."
}

@inproceedings{quan-etal-2024-enhancing,
    title = "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
    author = "Quan, Xin  and
      Valentino, Marco  and
      Dennis, Louise  and
      Freitas, Andre",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.1/",
    pages = "1--22",
    abstract = "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."
}

@article{ranaldi2024eliciting,
  title={Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations},
  author={Ranaldi, Leonardo and Valentino, Marco and Freitas, Andr{\`e}},
  journal={arXiv preprint arXiv:2410.22874},
  year={2024}
}

@inproceedings{dalal-etal-2024-inference,
    title = "Inference to the Best Explanation in Large Language Models",
    author = "Dalal, Dhairya  and
      Valentino, Marco  and
      Freitas, Andre  and
      Buitelaar, Paul",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.14/",
    doi = "10.18653/v1/2024.acl-long.14",
    pages = "217--235",
    abstract = "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes \textit{IBE-Eval}, a framework inspired by philosophical accounts on \textit{Inference to the Best Explanation (IBE)} to advance the interpretation and evaluation of LLMs' explanations. \textit{IBE-Eval} estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: \textit{consistency}, \textit{parsimony}, \textit{coherence}, and \textit{uncertainty}. Extensive experiments are conducted on \textit{Causal Question Answering (CQA)}, where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that \textit{IBE-Eval} can successfully identify the best explanation with up to 77{\%} accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that \textit{IBE-Eval} is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools."
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}


@misc{quan2024verificationrefinementnaturallanguage,
      title={Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving}, 
      author={Xin Quan and Marco Valentino and Louise A. Dennis and André Freitas},
      year={2024},
      eprint={2405.01379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01379}, 
}

@misc{chen2023program,
      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, 
      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
      year={2023},
      eprint={2211.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ling2017program,
  title={Program induction by rationale generation: Learning to solve and explain algebraic word problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1705.04146},
  year={2017}
}

@article{gema2024we,
  title={Are We Done with {MMLU}?},
  author={Gema, Aryo Pradipta and Leang, Joshua Ong Jun and Hong, Giwon and Devoto, Alessio and Mancino, Alberto Carlo Maria and Saxena, Rohit and He, Xuanli and Zhao, Yu and Du, Xiaotang and Madani, Mohammad Reza Ghasemi and others},
  journal={arXiv preprint arXiv:2406.04127},
  year={2024}
}

@article{zhang2024careful,
  title={A careful examination of large language model performance on grade school arithmetic},
  author={Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Slack, Dylan and Lyu, Qin and others},
  journal={arXiv preprint arXiv:2405.00332},
  year={2024}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{zhang2023evaluating,
  title={Evaluating the performance of large language models on gaokao benchmark},
  author={Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2305.12474},
  year={2023}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@inproceedings{lyu2023faithful,
    title = "Faithful Chain-of-Thought Reasoning",
    author = "Lyu, Qing  and
      Havaldar, Shreya  and
      Stein, Adam  and
      Zhang, Li  and
      Rao, Delip  and
      Wong, Eric  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.20",
    doi = "10.18653/v1/2023.ijcnlp-main.20",
    pages = "305--329",
}

@article{xu2024faithful,
  title={Faithful Logical Reasoning via Symbolic Chain-of-Thought},
  author={Xu, Jundong and Fei, Hao and Pan, Liangming and Liu, Qian and Lee, Mong-Li and Hsu, Wynne},
  journal={arXiv preprint arXiv:2405.18357},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@book{harrington1985harvey,
  title={Harvey Friedman's research on the foundations of mathematics},
  author={Harrington, Leo A and Morley, Michael D and {\v{S}}cedrov, A and Simpson, Stephen G},
  year={1985},
  publisher={Elsevier}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{gilpin2018explaining,
  title={Explaining explanations: An overview of interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
  pages={80--89},
  year={2018},
  organization={IEEE}
}


@article{herman2017promise,
  title={The promise and peril of human evaluation for model interpretability},
  author={Herman, Bernease},
  journal={arXiv preprint arXiv:1711.07414},
  year={2017}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={17682--17690},
  year={2024}
}

@article{wang2023plan,
  title={Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models},
  author={Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2305.04091},
  year={2023}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{cummins1991conditional,
  title={Conditional reasoning and causation},
  author={Cummins, Denise D and Lubart, Todd and Alksnis, Olaf and Rist, Robert},
  journal={Memory \& cognition},
  volume={19},
  pages={274--282},
  year={1991},
  publisher={Springer}
}

@article{robinson1965machine,
  title={A machine-oriented logic based on the resolution principle},
  author={Robinson, John Alan},
  journal={Journal of the ACM (JACM)},
  volume={12},
  number={1},
  pages={23--41},
  year={1965},
  publisher={ACM New York, NY, USA}
}

@inproceedings{amayuelas2022neural,
  title={Neural methods for logical reasoning over knowledge graphs},
  author={Amayuelas, Alfonso and Zhang, Shuai and Rao, Xi Susie and Zhang, Ce},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{gerasimova2023comparative,
  title={Comparative analysis of logic reasoning and graph neural networks for ontology-mediated query answering with a covering axiom},
  author={Gerasimova, Olga and Severin, Nikita and Makarov, Ilya},
  journal={IEEE Access},
  year={2023},
  publisher={IEEE}
}

@article{ye2023generating,
  title={Generating Data for Symbolic Language with Large Language Models},
  author={Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao},
  journal={arXiv preprint arXiv:2305.13917},
  year={2023}
}

@article{gaur2023reasoning,
  title={Reasoning in large language models through symbolic math word problems},
  author={Gaur, Vedant and Saunshi, Nikunj},
  journal={arXiv preprint arXiv:2308.01906},
  year={2023}
}

@article{pan2023logic,
  title={Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning},
  author={Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.12295},
  year={2023}
}

@article{jiang2024leanreasoner,
  title={LeanReasoner: Boosting Complex Logical Reasoning with Lean},
  author={Jiang, Dongwei and Fonseca, Marcio and Cohen, Shay B},
  journal={arXiv preprint arXiv:2403.13312},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{luo2024improve,
  title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  year={2024}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{jacovi2020towards,
  title={Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.03685},
  year={2020}
}

@article{turpin2024language,
  title={Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{pruthi2019learning,
  title={Learning to deceive with attention-based explanations},
  author={Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C},
  journal={arXiv preprint arXiv:1909.07913},
  year={2019}
}

@inproceedings{slack2020fooling,
  title={Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}

@inproceedings{bengio2019system,
  title={From system 1 deep learning to system 2 deep learning},
  author={Bengio, Yoshua and others},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{jin2024impact,
  title={The impact of reasoning step length on large language models},
  author={Jin, Mingyu and Yu, Qinkai and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan and others},
  journal={arXiv preprint arXiv:2401.04925},
  year={2024}
}

@article{yang2022language,
  title={Language models as inductive reasoners},
  author={Yang, Zonglin and Dong, Li and Du, Xinya and Cheng, Hao and Cambria, Erik and Liu, Xiaodong and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10923},
  year={2022}
}

@article{li2023symbolic,
  title={Symbolic chain-of-thought distillation: Small models can also" think" step-by-step},
  author={Li, Liunian Harold and Hessel, Jack and Yu, Youngjae and Ren, Xiang and Chang, Kai-Wei and Choi, Yejin},
  journal={arXiv preprint arXiv:2306.14050},
  year={2023}
}

@article{lu2022survey,
  title={A survey of deep learning for mathematical reasoning},
  author={Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2212.10535},
  year={2022}
}

@article{gupta2024changing,
  title={Changing answer order can decrease mmlu accuracy},
  author={Gupta, Vipul and Pantoja, David and Ross, Candace and Williams, Adina and Ung, Megan},
  journal={arXiv preprint arXiv:2406.19470},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{lee2024holistic,
  title={Holistic evaluation of text-to-image models},
  author={Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah and Bellagente, Marco and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023bert,
  title={{BERT} Is Not The Count: Learning to Match Mathematical Statements with Proofs},
  author={Li, Weixian Waylon and Ziser, Yftah and Coavoux, Maximin and Cohen, Shay B},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={3581--3593},
  year={2023}
}

@article{10.1162/tacl_a_00594,
    author = {Meadows, Jordan and Freitas, André},
    title = "{Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {1162-1184},
    year = {2023},
    month = {09},
    abstract = "{Automating discovery in mathematics and science will require sophisticated methods of information extraction and abstract reasoning, including models that can convincingly process relationships between mathematical elements and natural language, to produce problem solutions of real-world value. We analyze mathematical language processing methods across five strategic sub-areas (identifier-definition extraction, formula retrieval, natural language premise selection, math word problem solving, and informal theorem proving) from recent years, highlighting prevailing methodologies, existing limitations, overarching trends, and promising avenues for future research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00594},
    url = {https://doi.org/10.1162/tacl\_a\_00594},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00594/2159104/tacl\_a\_00594.pdf},
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{tam2024let,
  title={Let me speak freely? a study on the impact of format restrictions on performance of large language models},
  author={Tam, Zhi Rui and Wu, Cheng-Kuang and Tsai, Yi-Lin and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2408.02442},
  year={2024}
}

@article{zhong2024achieving,
  title={Achieving> 97\% on GSM8K: Deeply Understanding the Problems Makes {LLMs} Perfect Reasoners},
  author={Zhong, Qihuang and Wang, Kang and Xu, Ziyang and Liu, Juhua and Ding, Liang and Du, Bo and Tao, Dacheng},
  journal={arXiv preprint arXiv:2404.14963},
  year={2024}
}

@article{ahn2024large,
  title={Large language models for mathematical reasoning: Progresses and challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}

@article{yee2024dissociation,
  title={Dissociation of Faithful and Unfaithful Reasoning in {LLMs}},
  author={Yee, Evelyn and Li, Alice and Tang, Chenyu and Jung, Yeon Ho and Paturi, Ramamohan and Bergen, Leon},
  journal={arXiv preprint arXiv:2405.15092},
  year={2024}
}

@article{li2024towards,
  title={Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners},
  author={Li, Jiachun and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2405.18915},
  year={2024}
}

@article{imani2023mathprompter,
  title={Mathprompter: Mathematical reasoning using large language models},
  author={Imani, Shima and Du, Liang and Shrivastava, Harsh},
  journal={arXiv preprint arXiv:2303.05398},
  year={2023}
}

@article{sprague2024cot,
  title={To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning},
  author={Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
  journal={arXiv preprint arXiv:2409.12183},
  year={2024}
}

@article{jiang2024llms,
  title={{LLMs} can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought},
  author={Jiang, Zhuoxuan and Peng, Haoyuan and Feng, Shanshan and Li, Fan and Li, Dongsheng},
  journal={arXiv preprint arXiv:2405.06705},
  year={2024}
}

@article{chu2023survey,
  title={A survey of chain of thought reasoning: Advances, frontiers and future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2309.15402},
  year={2023}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{romera2024mathematical,
  title={Mathematical discoveries from program search with large language models},
  author={Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M Pawan and Dupont, Emilien and Ruiz, Francisco JR and Ellenberg, Jordan S and Wang, Pengming and Fawzi, Omar and others},
  journal={Nature},
  volume={625},
  number={7995},
  pages={468--475},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{chen2024masked,
  title={Masked thought: Simply masking partial reasoning steps can improve mathematical reasoning learning of language models},
  author={Chen, Changyu and Wang, Xiting and Lin, Ting-En and Lv, Ang and Wu, Yuchuan and Gao, Xin and Wen, Ji-Rong and Yan, Rui and Li, Yongbin},
  journal={arXiv preprint arXiv:2403.02178},
  year={2024}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@misc{bi2023programofthoughtsworkreasoning,
      title={When Do Program-of-Thoughts Work for Reasoning?}, 
      author={Zhen Bi and Ningyu Zhang and Yinuo Jiang and Shumin Deng and Guozhou Zheng and Huajun Chen},
      year={2023},
      eprint={2308.15452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.15452}, 
}


@inproceedings{ahn-etal-2024-large,
    title = "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
    author = "Ahn, Janice  and
      Verma, Rishu  and
      Lou, Renze  and
      Liu, Di  and
      Zhang, Rui  and
      Yin, Wenpeng",
    editor = "Falk, Neele  and
      Papi, Sara  and
      Zhang, Mike",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-srw.17",
    pages = "225--237",
    abstract = "Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.",
}

@article{Bronkhorst2019,
  title = {Logical Reasoning in Formal and Everyday Reasoning Tasks},
  volume = {18},
  ISSN = {1573-1774},
  url = {http://dx.doi.org/10.1007/s10763-019-10039-8},
  DOI = {10.1007/s10763-019-10039-8},
  number = {8},
  journal = {International Journal of Science and Mathematics Education},
  publisher = {Springer Science and Business Media LLC},
  author = {Bronkhorst,  Hugo and Roorda,  Gerrit and Suhre,  Cor and Goedhart,  Martin},
  year = {2019},
  month = dec,
  pages = {1673–1694}
}

@misc{dong2024surveyincontextlearning,
      title={A Survey on In-context Learning}, 
      author={Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Jingyuan Ma and Rui Li and Heming Xia and Jingjing Xu and Zhiyong Wu and Tianyu Liu and Baobao Chang and Xu Sun and Lei Li and Zhifang Sui},
      year={2024},
      eprint={2301.00234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.00234}, 
}

@misc{arakelyan2024flarefaithfullogicaidedreasoning,
      title={FLARE: Faithful Logic-Aided Reasoning and Exploration}, 
      author={Erik Arakelyan and Pasquale Minervini and Pat Verga and Patrick Lewis and Isabelle Augenstein},
      year={2024},
      eprint={2410.11900},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.11900}, 
}

@inproceedings{patel-etal-2021-nlp,
    title = "Are {NLP} Models really able to Solve Simple Math Word Problems?",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Goyal, Navin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.168/",
    doi = "10.18653/v1/2021.naacl-main.168",
    pages = "2080--2094",
    abstract = "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered {\textquotedblleft}solved{\textquotedblright} with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs."
}

@misc{zhou2023leasttomostpromptingenablescomplex,
      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, 
      author={Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},
      year={2023},
      eprint={2205.10625},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.10625}, 
}

@inproceedings{zhang-etal-2019-paws,
    title = "{PAWS}: Paraphrase Adversaries from Word Scrambling",
    author = "Zhang, Yuan  and
      Baldridge, Jason  and
      He, Luheng",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1131/",
    doi = "10.18653/v1/N19-1131",
    pages = "1298--1308",
    abstract = "Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS ({\ensuremath{<}}40{\%} accuracy); however, including PAWS training data for these models improves their accuracy to 85{\%} while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons."
}


@misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark}, 
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}, 
}


@misc{leang2024comatchainmathematicallyannotated,
      title={CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning}, 
      author={Joshua Ong Jun Leang and Aryo Pradipta Gema and Shay B. Cohen},
      year={2024},
      eprint={2410.10336},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.10336}, 
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    editor = "M{\`a}rquez, Llu{\'i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075/",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642"
}

@article{Wang1954,
  title = {The formalization of mathematics},
  volume = {19},
  ISSN = {1943-5886},
  url = {http://dx.doi.org/10.2307/2267732},
  DOI = {10.2307/2267732},
  number = {4},
  journal = {Journal of Symbolic Logic},
  publisher = {Cambridge University Press (CUP)},
  author = {Wang,  Hao},
  year = {1954},
  month = dec,
  pages = {241–266}
}

@article{MILLER20191,
title = {Explanation in artificial intelligence: Insights from the social sciences},
journal = {Artificial Intelligence},
volume = {267},
pages = {1-38},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
author = {Tim Miller},
keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}

@article{PENNINGTON1993123,
title = {Reasoning in explanation-based decision making},
journal = {Cognition},
volume = {49},
number = {1},
pages = {123-163},
year = {1993},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(93)90038-W},
url = {https://www.sciencedirect.com/science/article/pii/001002779390038W},
author = {Nancy Pennington and Reid Hastie},
abstract = {A general theory of explanation-based decision making is outlined and the multiple roles of inference processes in the theory are indicated. A typology of formal and informal inference forms, originally proposed by Collins (1978a, 1978b), is introduced as an appropriate framework to represent inferences that occur in the overarching explanation-based process. Results from the analysis of verbal reports of decision processes are presented to demonstrate the centrality and systematic character of reasoning in a representative legal decision-making task.}
}

@article{Mayer1998,
  volume = {26},
  ISSN = {0020-4277},
  url = {http://dx.doi.org/10.1023/A:1003088013286},
  DOI = {10.1023/a:1003088013286},
  number = {1/2},
  journal = {Instructional Science},
  publisher = {Springer Science and Business Media LLC},
  author = {Mayer,  Richard E.},
  year = {1998},
  pages = {49–63}
}

@article{Fischer2012,
  title = {The Process of Solving Complex Problems},
  volume = {4},
  ISSN = {1932-6246},
  url = {http://dx.doi.org/10.7771/1932-6246.1118},
  DOI = {10.7771/1932-6246.1118},
  number = {1},
  journal = {The Journal of Problem Solving},
  publisher = {Purdue University (bepress)},
  author = {Fischer,  Andreas and Greiff,  Samuel and Funke,  Joachim},
  year = {2012},
  month = feb 
}

@article{GARCIA201983,
title = {Planning, execution, and revision in mathematics problem solving: Does the order of the phases matter?},
journal = {Studies in Educational Evaluation},
volume = {61},
pages = {83-93},
year = {2019},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X18302918},
author = {Trinidad García and Jan Boom and Evelyn H. Kroesbergen and José Carlos Núñez and Celestino Rodríguez},
keywords = {Mathematics education, Problem solving, Elementary school, Process, Self-perception},
abstract = {The present study analysed the mathematical problem-solving processes, in terms of linearity and recursion, and the relationship with actual and self-perceived performances of a sample of 524 students of upper elementary students. The results showed a more linear than recursive process while performing the tasks, mainly characterized by continuity. The use of planning strategies before execution and the use of revision strategies after this phase were both significantly related to good performance, even if rates of success were low. The presence of a linear and hierarchical resolution process was related to students’ judgments of success, while recursion, or going back in the process, was associated with judgments of failure. Results are discussed in the light of current research on mathematics problem-solving.}
}

@misc{dinh2023largelanguagemodelscode,
      title={Large Language Models of Code Fail at Completing Code with Potential Bugs}, 
      author={Tuan Dinh and Jinman Zhao and Samson Tan and Renato Negrinho and Leonard Lausen and Sheng Zha and George Karypis},
      year={2023},
      eprint={2306.03438},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03438}, 
}

@article{zhao2024stepwise,
  title={Stepwise Self-Consistent Mathematical Reasoning with Large Language Models},
  author={Zhao, Zilong and Rong, Yao and Guo, Dongyang and G{\"o}zl{\"u}kl{\"u}, Emek and G{\"u}lboy, Emir and Kasneci, Enkelejda},
  journal={arXiv preprint arXiv:2402.17786},
  year={2024}
}

@article{bentham2024chain,
  title={Chain-of-Thought Unfaithfulness as Disguised Accuracy},
  author={Bentham, Oliver and Stringham, Nathan and Marasovi{\'c}, Ana},
  journal={arXiv preprint arXiv:2402.14897},
  year={2024}
}

@article{lu2024mitigating,
  title={Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models},
  author={Lu, Zhenyi and Tian, Jie and Wei, Wei and Qu, Xiaoye and Cheng, Yu and Chen, Dangyang and others},
  journal={arXiv preprint arXiv:2406.07001},
  year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{gaur2023reasoning,
  title={Reasoning in large language models through symbolic math word problems},
  author={Gaur, Vedant and Saunshi, Nikunj},
  journal={arXiv preprint arXiv:2308.01906},
  year={2023}
}

@article{meadows2023symbolic,
  title={A symbolic framework for systematic evaluation of mathematical reasoning with transformers},
  author={Meadows, Jordan and Valentino, Marco and Teney, Damien and Freitas, Andre},
  year={2023}
}

@inproceedings{ferreira2020premise,
  title={Premise selection in natural language mathematical texts},
  author={Ferreira, Deborah and Freitas, Andr{\'e}},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7365--7374},
  year={2020}
}

@inproceedings{ranaldi2024aligning,
  title={Aligning large and small language models via chain-of-thought reasoning},
  author={Ranaldi, Leonardo and Freitas, Andre},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1812--1827},
  year={2024}
}

@article{meadows2022survey,
  title={A survey in mathematical language processing},
  author={Meadows, Jordan and Freitas, Andre},
  journal={arXiv preprint arXiv:2205.15231},
  year={2022}
}

@article{he2023solving,
  title={Solving math word problems by combining language models with symbolic solvers},
  author={He-Yueya, Joy and Poesia, Gabriel and Wang, Rose E and Goodman, Noah D},
  journal={arXiv preprint arXiv:2304.09102},
  year={2023}
}

@article{ye2024satlm,
  title={Satlm: Satisfiability-aided language models using declarative prompting},
  author={Ye, Xi and Chen, Qiaochu and Dillig, Isil and Durrett, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{cohen2005feature,
  title={Feature selection based on the shapley value},
  author={Cohen, Shay B and Dror, Gideon and Ruppin, Eytan},
  booktitle={Proceedings of IJCAI},
  pages={1--6},
  year={2005}
}

@article{keinan2004fair,
  title={Fair attribution of functional contribution in artificial and biological networks},
  author={Keinan, Alon and Sandbank, Ben and Hilgetag, Claus C and Meilijson, Isaac and Ruppin, Eytan},
  journal={Neural computation},
  volume={16},
  number={9},
  pages={1887--1915},
  year={2004},
  publisher={MIT Press}
}

@incollection{kuhn1953value,
  title={A value for $ n $-person games},
  author={Kuhn, HW and Tucker, AW and others},
  booktitle={Contributions to the Theory of Games II},
  volume={28},
  pages={307--317},
  year={1953},
  publisher={Princeton Univ. Press}
}

@article{shapley1953value,
  title={A value for n-person games},
  author={Shapley, Lloyd S},
  journal={Contribution to the Theory of Games},
  volume={2},
  year={1953}
}

@misc{jiang2024rationalystpretrainingprocesssupervisionimproving,
      title={RATIONALYST: Pre-training Process-Supervision for Improving Reasoning}, 
      author={Dongwei Jiang and Guoxuan Wang and Yining Lu and Andrew Wang and Jingyu Zhang and Chuyu Liu and Benjamin Van Durme and Daniel Khashabi},
      year={2024},
      eprint={2410.01044},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.01044}, 
}

@article{mirzadeh2024gsm,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@inproceedings{dua-etal-2019-drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1246/",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
    abstract = "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4{\%} F1 on our generalized accuracy metric, while expert human performance is 96{\%}. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51{\%} F1."
}


@inproceedings{zhu-etal-2023-solving,
    title = "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
    author = "Zhu, Xinyu  and
      Wang, Junjie  and
      Zhang, Lin  and
      Zhang, Yuxiang  and
      Huang, Yongfeng  and
      Gan, Ruyi  and
      Zhang, Jiaxing  and
      Yang, Yujiu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.245/",
    doi = "10.18653/v1/2023.acl-long.245",
    pages = "4471--4485",
    abstract = "Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6{\%} increase over best baselines."
}


@misc{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,
      title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models}, 
      author={Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
      year={2024},
      eprint={2410.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05229}, 
}