\section{ML Productivity Goodput}\label{sec:goodput}

The optimization of ML fleet efficiency is a complex, cyclical challenge, as illustrated in \autoref{fig:ml_stack}. The first challenge is measuring, understanding, and reporting fleetwide efficiency, establishing a baseline for current performance. Second, we must identify and quantify fleet-wide inefficiencies, pinpointing areas that require improvement. Third, we must eliminate these inefficiencies by implementing changes across the fleet, which in turn leads back to the first stage as we measure the impact of these changes. This cycle ensures ongoing optimization and adaptation to the ever-evolving landscape of ML workloads and hardware. To keep pace with this lifecycle, we require a metric that not only quantifies current performance but also guides future optimization efforts across the fleet, which is why Google has developed the \mpg metric. %

In this section, we present an in-depth discussion of MPG, a new metric for quantifying ML fleet efficiency. We refer to this as the iron law of performance for ML fleets, drawing a corollary to the iron law of processor performance~\cite{emer1984ironlaw}. MPG, defined in \autoref{fig:mpg}, is a means for measuring efficiency gains and guiding exploration of optimization strategies across various fleet components.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/mpg.png}
    \caption{ML Productivity Goodput (MPG) and its components.}
    \label{fig:mpg}
\end{figure}





\subsection{Pitfalls \& Myths of Traditional Metrics}


Before we set the stage for the new metric, we examine the common pitfalls of historical approaches for fleetwide measurement. \autoref{fig:utilization} illustrates how computer architects have historically tended to think about performance metrics \cite{li2023analyzing, mars2011bubble,kanev2015profiling}. These traditional performance metrics can sometimes fall short in providing a holistic view, given the unique challenges we have discussed in \autoref{sec:background} and \autoref{sec:fleet}. 





\textbf{Myth 1: High \underline{Capacity} equates to high resource availability.}
While capacity can tell us how many individual accelerators may be available in the fleet at a given time, it does not take into account the topological shape of those accelerators. For example, an ML training workload requesting thousands of chips in a certain physical mesh shape may never be scheduled if the only available accelerators are fragmented across different clusters or data centers. Other factors, such as the geographical location of data storage cells and accelerators, are not included in the capacity metric, even though they significantly affect job scheduling. Therefore, high capacity by itself as a metric does not necessarily effectively translate to high availability for workloads, and we should instead opt for scheduling efficiency as a more robust metric.

\textbf{Myth 2: High \underline{Occupancy} guarantees productivity.}
Occupancy is defined as the fraction of accelerators allocated to jobs and is often measured by the scheduler (e.g. Borg \cite{verma2015borg}). Occupancy is traditionally seen as a key efficiency indicator, but it can be misleading as it masks inefficiencies in the system stack. For example, an accelerator might be successfully allocated but stuck in I/O wait or running poorly optimized code, thus resulting in a high occupancy but very little actual progress being made towards the workload task. This is important for long-running tasks such as ML model training, where frequent pre-emptions may hinder checkpoint progress but still result in a nominally high occupancy. The traditional occupancy metric therefore does not distinguish between productive and unproductive use of allocated resources.

\textbf{Myth 3: \underline{Duty Cycle} accurately represents useful work.}
Duty Cycle measures whether an accelerator is in use, not how much of its compute capacity is used. When looking at an ML workload running on a TPU, duty cycle does not provide any signal on how much the matrix-multiply units (MXUs) are utilized \cite{jouppi2023tpuv4opticallyreconfigurable}. It is agnostic of the program-level efficiency and does not take into account the effectiveness of the operations being performed. An accelerator could have a high duty cycle while executing unnecessary or redundant computations. So, we require a more sophisticated metric.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{final_figs/efficiency_util.png}
\caption{Historical utilization-based fleet efficiency metrics. We propose replacing this approach and using goodput as a measure of fleet efficiency rather than utilization.}
\label{fig:utilization}
\end{figure}

\textbf{The Overarching Misconception: Utilization Equals Productivity.}
The common thread among these metrics is the assumption that keeping accelerators busy equates to productive work. However, this overlooks critical factors. (1) \textit{Quality of Computations:} None of these metrics assess whether the operations being performed are actually contributing to the desired output. (2) \textit{Workload Efficiency:} They do not consider whether the workloads are optimally designed for the hardware. (3) \textit{System-level Bottlenecks:} Focusing solely on accelerator usage ignores potential bottlenecks in data loading, memory access, or inter-accelerator communication. (4) \textit{Forward Progress:} These metrics provide no insight into how much useful work is being accomplished towards completing an actual ML task.







\subsection{Metric Features}
Ideally, the MPG metric must be a clearly defined and accurate measure of forward progress; improvements in the metric must also reflect real improvements in the efficiency of the fleet. This metric must be capable of overcoming two significant challenges.

\begin{enumerate}
\item \textbf{It must capture the dynamic nature of ML fleets}: The fleet is constantly fluctuating due to variables such as changes in workload composition, updates to the code stack, and evolving hardware. To effectively improve efficiency, we must ensure that any change in the metric is explainable despite these fluctuating variables.
\item \textbf{It must explain the trade-offs between individual and aggregate efficiency}. At a fleetwide scale, jobs must be scheduled in concert with one another to ensure maximum aggregate efficiency of the fleet. However, individual jobs may have certain service-level requirements, meaning that this metric must be decomposable based on workload characteristics.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/mpg_breakdown.png}
    \caption{Breakdown of a ML workload using \mpg. 
    }
    \label{fig:mpg_breakdown}
\end{figure}

\subsection{A New Approach: ML Productivity Goodput}

\mpg (MPG) is designed to address the myriad challenges discussed in Section~\ref{sec:fleet}, as well as to overcome the limitations of existing approaches. Just as the Iron Law of Processor Performance~\cite{emer1984ironlaw} breaks down CPU performance into $\frac{instructions}{program}$$\times$ $\frac{cycles}{instruction}$$\times$$\frac{time}{cycle}$, the MPG metric decomposes ML fleet efficiency into scheduling, runtime, and program components (see \autoref{fig:mpg}).



This multi-layered structure, as illustrated in \autoref{fig:mpg_breakdown}, offers several advantages over the traditional metrics. First, it allows for precise identification of performance bottlenecks or improvements at specific layers of the stack, facilitates a more granular analysis of efficiency trends over time, and mitigates the risk of misleading interpretations that can arise from aggregated data, such as Simpson's paradox.\footnote{A statistical phenomenon where a trend that is evident within individual groups disappears or reverses when the population groups are combined.} Second, by decoupling these submetrics, we enable more targeted optimization efforts and gain deeper insights into the complex interactions within the ML fleet. Finally, this approach not only enhances our ability to measure current performance but also provides a framework for guiding improvements, discussed in \autoref{sec:improvements}.  

\textbf{Scheduling Goodput:}
\emph{How often does an application have all necessary resources to make progress?} 

Scheduling Goodput (SG) quantifies the efficiency of resource allocation in an ML fleet. It measures the fraction of time that an application has all the required resources simultaneously available to make progress. This metric can be lower than traditional Occupancy, particularly in distributed, bulk-synchronous applications where all required chips must be available concurrently. The numerator of SG is calculated as the simultaneous uptime of all tasks in a distributed ML application that must be connected to make synchronous progress, as shown in \autoref{fig:scheduling}. This is referred to as ``allocated chip-time'' or ``all-allocated'' time. The denominator is fleet capacity, expressed as chip-time. This provides a full view of how effectively the scheduling layer is using the fleet's resources. 

Scheduling Goodput offers insights into potential inefficiencies in resource allocation, such as fragmentation of available resources, delays in coordinating multiple chips for distributed applications, and mismatches between application requirements and available resources. By optimizing SG, we can improve the overall efficiency of resource utilization in the ML fleet, ensuring that applications have the necessary resources to make consistent progress.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/scheduling.png}
    \caption{The scheduling goodput for training workloads measures the percentage of time when all of the TPU workers are available to work at the same time. In other words, it measures the portion of time that all of the necessary resources are available to make progress. 
}
    \label{fig:scheduling}
\end{figure}

\textbf{Runtime Goodput:}
\emph{Of the time that an application has all necessary resources, how often is it making progress?} 

\rg (RG) measures the efficiency of the orchestration layers in managing the execution of ML applications once resources are allocated. This metric focuses on the actual productive time of an application, accounting for various overheads in the runtime environment. The orchestration layer is responsible for critical tasks such as initializing chips, connecting them into slices for bulk-synchronous progress, loading and compiling programs, feeding data to these programs, and ensuring that training progress is regularly saved through checkpoints. The numerator of RG is the productive chip-time of the application's progress that has been saved in checkpoints; work done between the last checkpoint and failure (or preemption) doesn't count as "productive" time and is therefore not included in RG. The denominator of RG is the allocated chip-time defined as the numerator of SG. 

\rg can help with identifying bottlenecks in the runtime environment, such as slow data loading, inefficient checkpointing, or suboptimal program compilation. It can guide the efforts to streamline the execution pipeline and improve the overall throughput of ML workloads.

\textbf{Program Goodput:}
\emph{Of the time that an application is making progress, how close is it to the ideal roofline?}

Program Goodput (PG) assesses the efficiency of the application code itself, measuring how effectively it utilizes the available computational resources. While a traditional roofline performance model~\cite{williams2009roofline} might seem suitable for this purpose, it falls short in capturing the true efficiency of modern ML workloads. The traditional roofline model is highly sensitive to compiler decisions, such as how ML operators are fused or rematerialized \cite{briggs1992rematerialization}, or which operands are placed in which memory space. It rewards individual ops that are close to peak utilization, but penalizes correct optimizations that result in computation graphs where the utilization may be lower, but overall execution time is shorter. 

To overcome these limitations, we use a compute-based roofline model that compares the ideal execution time of the workload against its actual execution time. The ideal predicted execution time, which is the numerator of PG, can be computed from intrinsic properties of the machine learning model being run. By analyzing the shape of the unoptimized  high-level operations (HLO) graph, we can estimate how many floating point operations (FLOPs) the program would require at its theoretical peak performance. Since we are analyzing the computation graph before any compiler optimizations, this prediction is agnostic to compiler decisions. 

The denominator of PG is the actual execution time. The PG metric can thus be interpreted as a percentage reflecting how well optimized the ML program is, with a score of 100\% indicating perfect performance matching the theoretical peak.




