\section{Anatomy of an ML Fleet}\label{sec:fleet}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/ml_stack.png}
    \caption{The ML fleet system stack of a production system at Google. The multi-layered architecture of a fleet is complex; each layer is a critical component in the ML system, with interactions between layers affecting overall performance and efficiency. Segmenting the fleet based on these layers provides actionable metrics which can be used to improve performance. }
    \label{fig:ml_stack}
\end{figure}
In this section, we dive into the anatomy of a production ML fleet to provide perspective on the complexity of managing it. We begin by dissecting the fleet based on its distinctive components and characteristics, starting with the hardware foundation and progressing to the user application level.  \autoref{fig:ml_stack} shows the various layers that comprise the system stack and mediate user access to the ML fleet. This stack illustrates the intricate ecosystem that underpins modern ML operations. 

We supplement our discussion in this section with data from a snapshot of Google's TPU fleet for internal workloads, providing concrete examples of the challenges these systems face in practice. By examining actual usage patterns, resource allocation, and performance metrics from a production ML fleet, we can ground our discussion in real-world scenarios and offer insights based on empirical evidence. This data-driven approach will allow us to illustrate the complexities of managing large-scale ML operations and demonstrate how theoretical concepts translate into practical challenges and opportunities for optimization. %
\subsection{Accelerators}


ML fleets are distinguished from other types of large-scale compute systems by their accelerator-centric architecture. The ML computing landscape is dominated by domain-specific hardware, such as GPUs and other ASICs. In order to tailor to the vector and matrix intensive operations that underpin ML workloads, new accelerators such as Google's Tensor Processing Units (TPUs) \cite{jouppi2018motivation} have been developed. In general, there has been a Cambrian explosion of ML hardware accelerators \cite{hennessy2019anewgoldenage}, with new accelerators being deployed at an unprecedented rate compared to traditional WSC fleets \cite{jouppi2021ten}. \autoref{fig:five-years} vividly illustrates this dynamism, revealing dramatic shifts in our ML fleet's hardware makeup for internal workloads over just a few years.

ML fleets typically incorporate a diverse array of hardware including CPUs, GPUs, TPUs, and other accelerators, each fulfilling specific roles. For example, CPUs may be responsible for scheduling, GPUs for training tasks, and edge accelerators \cite{yazdanbakhsh2021edge} for deployment and serving. The challenge lies in effectively orchestrating these heterogeneous accelerators to maximize their individual strengths---a complexity rarely encountered in general compute fleets.

Moreover, the heterogeneity extends beyond just accelerator type. Even within a single class of hardware accelerators, there are many different versions of the hardware, adding another layer of complexity to fleet management. Each hardware generation introduces unique features that require significant optimizations to extract peak ML workload efficiency. One notable example is the integration of the SparseCore (SC) in TPUv4 \cite{jouppi2023tpuv4opticallyreconfigurable}, which was designed to significantly boost performance for embedding-heavy models. Subsequent large-embedding model teams would likely then consider the hardware specifications of the SparseCore when designing their embedding configurations. Design points such as embedding dimension, vocabulary size, valence, and others might also be co-designed to optimize performance on the hardware platform. This demonstrates how hardware-software co-design is becoming increasingly important in improving the efficiency of these diverse accelerators, forming a symbiotic relationship where the computational needs of future workloads affect the next generation of hardware, and the hardware capabilities inform the types of workloads that the ML fleet is best equipped to handle \cite{shi2020learned}.







\subsection{Scheduler}\label{sec:scheduler}
\begin{figure}[t!]
    \centering
    \includegraphics[width=3in]{final_figs/job_size_bars.png}
    \caption{A sample breakdown of Google's ML fleet for internal workloads, segmenting on workload topology size (the number of accelerators requested by a given job). Progressive snapshots over the course of one year illustrate the ML fleet's growing share of jobs using an "extra-large" number of accelerators. This demonstrates how an ML fleet scheduler must be able to adapt to changing conditions, as the evolution of job sizes and topologies in response to shifting ML workloads presents unique challenges for the entire fleet.}
    \label{fig:job_size}
\end{figure}
The scheduler directly manages the hardware in a fleet by coordinating the allocation of resources; for the case study presented in this paper, it coordinates TPU allocations for Google's internal-facing ML workloads. There are two interconnected challenges that a scheduler must address when allocating hardware for an ML fleet: (1)~optimizing performance across various hardware types, and (2)~balancing utilization with stability and fault tolerance.

\autoref{fig:job_size} illustrates these challenges. It shows the allocation of workloads in Google's internal-facing ML fleet with different chip requirements over time, categorized into sizes based on the total number of TPU chips in the required topology. In this categorization, workloads with size "small" refer to jobs that request a single TPU or a handful of TPUs, while workloads with size "extra-large" refer to jobs that request the largest number of TPUs (often requiring multiple pods, as described in \citet{kumar2021exploring}). \autoref{fig:job_size} demonstrates that over the course of just one year, the allocation distribution can shift dramatically, reflecting the changing nature of ML workloads in the fleet. As large-scale ML models become more prevalent in an ML fleet, an increasing number of workloads will require correspondingly larger meshes of connected accelerators.







Optimizing the scheduling of jobs while meeting these resource requirements is difficult because it presents an NP-hard bin-packing problem. Each workload may specify a different accelerator type, chip topology, and location requirement and needs to be scheduled according to fleet constraints in a way that reduces overall fragmentation of the fleet. Since workloads are constantly being started and completed, the machine availability of the fleet is constantly changing, requiring a robust defragmentation algorithm. In addition, latency requirements may require accelerators for a workload to be grouped together near certain locations or data cells, adding another constraint to the scheduling optimization problem. 

The utilization of fleet resources must also be balanced with stability and fault tolerance. For example, to reduce disruptions, some machines may intentionally remain underutilized so that higher priority jobs may be more easily scheduled when needed. While high utilization is desirable for cost-efficiency, pushing hardware to its limits can lead to thermal issues, increased failure rates, and unpredictable performance. In large-scale ML fleets, hardware failures are inevitable, and the scheduler must be robust enough to handle these failures gracefully, redistributing workloads and ensuring job continuity without significant performance degradation.






\begin{figure}[t!]
    \centering
    \includegraphics[width=3in]{final_figs/life_of_mlapp.png}
    \caption{An ML workload requires all requested TPUs to be allocated before the task can start. In this example of a training workload, forward progress is saved via checkpoints. Delays during workload initialization and checkpoint writing, which are part of the Runtime and Framework layers, can reduce overall system efficiency.}
    \label{fig:life_of_mlapp}
\end{figure}

\subsection{Runtime/Compiler}

The runtime and compiler layers form an important component in the ML fleet system stack. They are responsible for bridging the gap between high-level ML models and the underlying hardware accelerators. The runtime layer focuses on the execution environment of ML programs. It handles important tasks such as program setup, data feeding, result management, and checkpoint creation, as illustrated in \autoref{fig:life_of_mlapp}. Depending on the system design, it either triggers just-in-time compilation of user-written code into accelerator-specific instructions or invokes pre-compiled operation kernels from vendor-specific libraries. The runtime layer can also manage the distribution strategy of code execution, as with notable runtimes like Pathways \cite{barham2022pathways}.

\autoref{fig:pathways} shows the growth of Pathways-based workloads in our production fleet. It highlights the demand for runtimes that support efficient distributed execution for ML workloads. It also emphasizes the rapidly shifting distribution of workload runtimes in a fleet.




The compiler layer, working with the runtime, transforms high-level ML model code into executable code optimized for specific accelerators. It operates on graph intermediate representations, applying both platform-independent and platform-dependent optimizations. The output is a program tailored to the target accelerator, such as a specific version of a TPU. Domain-specific compilers, like XLA (Accelerated Linear Algebra) \cite{xla}, have significantly improved the performance of ML workloads. For instance, in MLPerf BERT benchmarks \cite{mattson2020mlperf}, XLA demonstrated a remarkable 7$\times$ performance boost and 5$\times$ batch size improvement \cite{kumar2021exploring} over previous records, emphasizing the potential of specialized compilation techniques. We note that there are many types of accelerators, some of which do not require an explicit compiler for code generation. 

Compiler optimization in ML fleets faces unique challenges due to the rapid evolution of hardware accelerators, requiring frequent updating of optimization strategies to leverage the specific features of each new hardware generation. Moreover, the impact of optimizations can be difficult to generalize, as an optimization that improves one workload may degrade another due to differences in computation or communication patterns. This emphasizes the need for a balanced approach to optimization, considering both platform-independent techniques for flexibility and platform-specific optimizations for maximum performance.






\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/pathways.png}
    \caption{The prevalence of fleet-wide workloads using the Pathways runtime over a sample of one year, illustrating the rapid shift of fleet-wide runtimes to accommodate changing workloads. Pathways adoption has increased rapidly, as it provides better support for distributed execution and data processing.}
    \label{fig:pathways}
\end{figure}
\subsection{Framework}

The framework layer sits on top of the runtime/compiler. It is the interface between ML practitioners and the underlying complex hardware and software infrastructure. This layer encompasses various ML frameworks and libraries, such as TensorFlow \cite{abadi2016tensorflow}, JAX \cite{frostig2018compiling}, and PyTorch \cite{paszke2019pytorch}, each offering unique features and optimizations. 

The framework layer provides high-level abstractions and APIs that allow developers to build and deploy ML models efficiently. These frameworks are responsible for translating user-written code into representations that can be understood and optimized by lower-level layers such as compilers and runtimes. This translation process bridges the gap between user intent and system execution.



One of the key responsibilities of ML frameworks is defining the structure of distributed ML applications. For example, TensorFlow's Distribution Strategy \cite{abadi2016tensorflowlargescalemachinelearning} provides a framework for distributing training across multiple devices or machines. These can have single-client or multi-client architectures, depending on workload needs, as shown in \autoref{fig:multi_single_frameworks}. These frameworks must also map ML primitives to hardware-specific designs to achieve optimal performance. This is important for specialized hardware like TPUs, which are designed for bulk-synchronous training. Frameworks like JAX are more targeted towards ML workloads, with features that facilitate ease of interpretability when analyzing ML performance, such as high-level tracing for just-in-time compilation. In the ML fleet, JAX usage has increased over time, most likely due to these features and the emergence of more ML-heavy workloads \cite{frostig2018compiling}.


In addition, ML frameworks often provide auxiliary services to improve efficiency of the entire ML fleet. For instance, TensorFlow's \texttt{tf.data} \cite{murray2021tfdata} service optimizes the performance of the data pipeline. These features, while abstracted from the user, can impact the overall system efficiency, as shown in \autoref{fig:life_of_mlapp}. Underneath these high-level frameworks lies a foundation of general-purpose libraries and datacenter services. Frameworks like TensorFlow utilize libraries such as gRPC \cite{grpc}, protobuf \cite{protobuf}, and tcmalloc \cite{tcmalloc} for various low-level operations, and interface with datacenter services for storage (e.g., Colossus \cite{ghemawat2003gfs} \cite{colossus}) and monitoring (e.g., Monarch \cite{adams2020monarch}). 


\begin{figure}[t!]
    \centering
    \includegraphics[width=.75\linewidth]{final_figs/multi_single_frameworks.png}
    \caption{
Comparing single-client frameworks with multi-client frameworks. 
    }
    \label{fig:multi_single_frameworks}
\end{figure}







As the primary point of interaction for users, the framework layer serves as a key bridge in the ML system stack, as it not only abstracts underlying complexities but also plays an important role in determining the overall efficiency and capabilities of the fleet. Frameworks must balance the need for user-friendly APIs with the need to leverage underlying hardware-specific optimizations, while also managing the complexities of distributed computing, data pipeline optimization, and integration with lower-level services. 









\subsection{ML Model \& Data}

To characterize the ML Fleet at the highest level of the stack, we generally want to know: What types of workloads are we spending most of our compute cycles on? This is a critical question because it drives nearly every design decision we make for the ML Fleet at every level of the stack, from the hardware (how many training vs. inference vs. other chips) to the software (JAX vs. other frameworks, runtime distribution strategies, and compiler optimizations). Workload heterogeneity analysis is useful for understanding what kind of models are prevalent in the production fleet, especially since different workloads stress the hardware in different ways. This understanding can drive decisions about which accelerators to deploy and how many, or which compiler optimizations to carry out.







In practice, we observe that the model and data layer of the ML fleet stack are the most affected by fluctuating user demands. User requirements such as the model architecture, size of the training dataset, or even use of different numerical formats in the training model can impact the efficiency of the job, which can have a cascading effect on the efficiency of the overall ML fleet.

While ML workloads share some computational patterns, particularly in their use of matrix operations and data-intensive processing, the specific architectures and resource requirements can vary significantly.  As new model architectures and learning tasks emerge, they prompt rapid shifts in workload composition, leading to fluctuations in resource demands across various model types. 

In a production ML fleet, there are varying proportions of workloads dedicated to each phase of the ML model life cycle; training, bulk inference, and real-time serving. Thus, the fleet must be flexible enough to handle the requirements of each of these phases; for example, training workloads may be compute intensive while real-time serving workloads may focus on minimizing latency.
















