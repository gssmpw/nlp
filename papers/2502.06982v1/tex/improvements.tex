\section{Improving Fleetwide Efficiency}\label{sec:improvements}
 \begin{figure}[t]
    \centering
    \includegraphics[height=2in]{final_figs/pg_cl.png}
    \caption{This figure demonstrates the effect of an XLA algebraic simplification optimization on Program Goodput (PG) across a benchmark of the top 150 fleet workloads. Looking at the PG in this way allows us to bisect which code changes improved or regressed overall fleet efficiency.}
    \label{fig:pg_cl}
\end{figure}

We show how \mpg is a robust quantifier of ML Fleet performance through optimization examples from Google's ML Fleet in production. We present a breakdown of the MPG components using segmented fleet data and demonstrate how this procedure can help identify potential optimization techniques. Additionally, we showcase the effects of deploying these optimizations and how MPG helps verify and track performance improvements.

Looking at the aggregated MPG of the fleet does not necessarily help ML practitioners identify what kinds of improvements will make the largest impact on the fleet; this is where the decomposability of the metric comes into play, as shown in \autoref{tab:improvements}. By breaking MPG into its three components; Program Goodput, Runtime Goodput, and Scheduling Goodput, we can diagnose fleetwide issues and identify the types of optimizations that would most improve fleet efficiency. Furthermore, we can segment the fleet using the characteristics described in Section~\ref{sec:fleet} in order to identify issues with specific workload types and propose model-level optimizations. 


\renewcommand*{\arraystretch}{1.25}
\begin{table*}[h!]
\caption{Optimizing different components of \mpg.}
\label{tab:improvements}
\resizebox{\textwidth}{!}{%
\scriptsize
\begin{tabular}{@{}lp{2cm}p{3cm}p{3cm}p{4cm}@{}}
\toprule
\textbf{ML Fleet Stack Layer} & \textbf{Program Goodput $\times$} & \textbf{Runtime Goodput $\times$} & \textbf{Scheduling Goodput $=$} & \textbf{Workload \mpg} \\ \bottomrule
\textbf{Compiler:} \\ On-duty step time decreases &  \textbf{Increases} & Decreases if device-bound \newline Decreases if host-bound & Decreases if device-bound \newline No change if host-bound & \textbf{Increases if device-bound}  \newline No change if host-bound
 \\ \midrule
\textbf{Runtime:} \\ Off-duty time or preemption waste decreases & No change & \textbf{Increases} & Decreases & \textbf{Increases} \\ \midrule
\textbf{Scheduler:} \\ Partially-allocated time decreases & No change & No change & \textbf{Increases}  & \textbf{Increases} 
\\ \bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Program Goodput Optimizations}
We present various techniques and strategies that have been employed at \google over recent years for improving Program Goodput in our ML fleets, ranging from parallelization methods to compiler optimizations. Recall that PG measures the effective utilization of computational resources. As ML models grow in size and complexity, optimizing PG becomes increasingly important to make efficient use of hardware and reduce compute times. With PG instrumentation, we have been able to pinpoint which segments of the fleet require further optimization at the compiler or ML model level. 

 \begin{figure}[t]
    \centering
    \includegraphics[height=2in]{final_figs/pg_chip.png}
    \caption{Tracking the Program Goodput (PG) versus allocation trends for a particular domain-specific chip in an ML fleet. Looking at the disaggregated segments of MPG can help reveal fleetwide trends and interactions between different layers in the ML fleet stack, informing future design decisions.}
    \label{fig:pg_chip_type}
\end{figure}


 



 \textbf{Overlapping communication and computation.}
To identify potential system optimizations that can improve fleet efficiency, we can look at the PG of workloads segmented by performance characteristics. In other words, how many of the workloads in the fleet are compute-bound versus communication-bound? By segmenting the PG in this way, it is possible for us to identify that many high-cost workloads are communication-bound. 

To address this issue at the high-level operation (HLO) level, a technique that overlaps communication with computation was developed and deployed in our production fleet (described in ~\citet{wang2022overlap}). This technique decomposes communication collectives, along with the dependent computation operations, into a sequence of finer-grained operations to hide data transfer latency so that better system utilization is achieved. This approach improved the overall system throughput by up to 1.38$\times$ and achieves 72\% FLOPS utilization on 1024 TPU chips for a large language model with 500 billion parameters~\cite{wang2022overlap}.

\textbf{Compiler autotuning.}
At the fleet level, we have also developed and deployed optimizations that improve code-generation quality and can be generalized to any workload in the fleet. XTAT~\cite{phothilimthana2021flexible} is an autotuner for production ML compilers that tunes multiple compiler stages, including tensor layouts, operator fusion decisions, tile sizes and code generation parameters. Evaluated over 150 ML training and inference models on TPUs, XTAT offers speedups over the heavily-optimized XLA compiler in the fleet.

\textbf{Example: Quantifying the impact of an XLA optimization on the TPU fleet.}
It is rare for any single optimization to have a significant impact on overall fleet-wide PG. But we can track the impact of these optimizations by looking at the change in PG for a fixed set of benchmarked workloads or segment of the production fleet over time. For example, looking at a benchmark of the top 150 most costly workloads in the fleet, \autoref{fig:pg_cl} pinpoints the effect of a code change that was submitted to the XLA compiler - in this case, an algebraic simplification in the compiler graph. The dramatic increase in PG for the benchmark of 150 workloads suggests that the positive impact of this optimization can be generalized to the ML fleet as a whole. 

It is also helpful to look at PG fluctuations across hardware segments of the fleet. \autoref{fig:pg_chip_type} illustrates a notional example where looking at segmented PG can uncover insights that would otherwise be hidden by looking at aggregate metrics. In this case, the segmented data suggests that when a new ML accelerator chip is introduced to the fleet, the workloads running on that chip may initially have a low PG, since the model / compiler code has not been fully tailored for that chip yet. As user adoption increases and accelerator-specific software optimizations are rolled out to the fleet, PG gets closer to theoretical peak efficiency. In other words, hardware accelerator maturity tends to yield greater PG over time. As the chip nears the end of its lifecycle (represented by decreasing allocation in the fleet, and illustrated in \autoref{fig:pg_chip_type} by the ``Chip decommissioned'' label), the PG decreases due to lower chip usage and natural workload/compiler drift. This highlights the importance of co-design across all layers of the ML fleet to make sure that both the software (compiler) and hardware (chips) are optimized for the latest workloads.
\subsection{Runtime Goodput Optimizations}
Outside of device time, host overhead and pre-emptions can be major bottlenecks for some workloads, and can be tracked by measuring Runtime Goodput. For example, training jobs usually use input pipelines to ingest and transform input data, which could be bottlenecks for certain models that ingest large amounts of data. Some solutions have been proposed to reduce host overhead, such as Plumber~\cite{kuchnik2022plumber}, a tool to find bottlenecks in ML input pipelines. 

We can improve the RG of the ML fleet with asynchronous strategies such as sharding the dataflow graph, as proposed by Pathways \cite{barham2022pathways}. The segmented analysis of RG shows that the particular workloads on Pathways tend to have higher RG scores over time, validating the benefits of Pathways for our particular ML Fleet. Also, techniques such as asynchronous checkpointing \citep{maurya2024datastates, nicolae2020deepfreeze} can reduce the time spent fetching previous model training checkpoints where the accelerators temporarily pause training and are completely idle.  


Other strategies, such as ahead-of-time compilation, where programs are compiled on less expensive hardware such as CPUs and then executed on TPUs, can also improve RG. By offloading compilation to a less expensive chip and storing the results in a compilation cache, we can reduce the total runtime of more specialized accelerators. These techniques are often implemented in common ML frameworks, such as TensorFlow \cite{aotTF} and JAX \cite{aotJAX}.


To demonstrate the benefits of these framework-specific optimizations, we can examine the RG of the ML Fleet at the framework level of the system stack described in \autoref{fig:ml_stack}. This can help us understand which frameworks or runtime strategies may be better suited for which workloads.
\autoref{fig:segmented_runtime_goodput} shows RG scores from a sample of Google's ML fleet for internal workloads, segmented based on characteristics such as model architecture, product area or workload phase (training, real-time serving, or bulk inference), and compared to a baseline of top fleet workloads. Although the segments in \autoref{fig:segmented_runtime_goodput} are not explicitly identified, we demonstrate how segmenting RG based on workload characteristics (Segment~A, Segment~B, and Segment~C) can reveal trends that would otherwise be hidden by aggregate fleet metrics (represented by the "Top Fleet Workloads" segment). For example, training workloads running JAX with Pathways may tend to have a higher RG, possibly due to the fact that Pathways is single-client \cite{barham2022pathways} and therefore better optimized for training than multi-client frameworks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{final_figs/segmented_runtime_goodput.png}
    \caption{Runtime goodput speedups over the course of one quarter, segmented by fleet workload types. Speedup is normalized to the top N workloads in the fleet, measured at the beginning of the quarter. }
    \label{fig:segmented_runtime_goodput}
\end{figure}

Examining the data along a different axis, training versus real-time serving versus bulk inference, can also be helpful. Using a sample from Google's ML fleet, \autoref{fig:train_inf_rg} illustrates that training workloads tend to have a higher RG than serving workloads. This is most likely due to the inherently different nature of serving and training.  Typically, training workloads have more constant computational demands, while real-time serving can fluctuate based on user demand. The slight decrease in serving RG can be attributed to transitory demands on the fleet, but it remains relatively stable in comparison to the bulk inference segment. The huge fluctuation in RG for bulk inference highlights the changing nature of production fleet demands. Previously, the bulk inference segment of the fleet was dominated by workloads running on a single core, where each chip contained a replica of the model, resulting in more easily accessible checkpoints/data and less accelerator wait time. However, as we move to larger models, the weights must be sharded across multiple chips, resulting in more expensive data reads. Additionally, the rise of expert-based models \cite{shazeer2017moe} has made bulk inference runtime much more complex to optimize, as some machines must wait for others for distillation of weight updates in a student-teacher model. This has resulted in an temporary decrease of RG for the bulk inference segment between "Month 3" and "Month 6" of \autoref{fig:train_inf_rg}. This example illustrates how analyzing disaggregated RG can allow ML fleet architects to make informed decisions about their runtime stack by pinpointing segments that may be more susceptible to shifting fleet demands.

\begin{figure}[t]
    \centering
    \includegraphics[height=1.8in]{final_figs/train_inf_rg.png}
    \caption{Runtime Goodput trends for a notional slice of a sample ML fleet over a period of six months, segmented by workload phase.}
    \label{fig:train_inf_rg}
\end{figure}



\subsection{Scheduling Goodput Optimizations}

The optimization of Scheduling Goodput (SG) can be presented as a bin packing problem, as described in \autoref{sec:scheduler}. Users launch workloads with varying TPU topology sizes, and the scheduling algorithm must determine how to best fit these workloads into the existing fleet of allocated chips. This process presents numerous challenges, primarily due to the wide variety of job sizes in the fleet, ranging from single-chip to multipod configurations \cite{kumar2021exploring}.

A significant complication in job scheduling is that it requires more than mere availability in the fleet. The topology of the available hardware must also satisfy the topology requirements of the workload, which is sometimes impossible without first pre-empting other jobs. Consequently, suboptimal scheduling can have a cascading effect on other components of \mpg.



We can identify availability issues in the ML Fleet by looking at the SG for jobs with different chip allocation requirements, as shown in \autoref{fig:sg_job_size}. The data shows that the overall SG is already close to optimal, due to defragmentation techniques and scheduling optimizations. However, it is interesting to note which jobs tend to have the highest SG: extra-large jobs which require the greatest number of chips or possibly multiple TPU pods, as well as smaller jobs which require only a single chip or a few chips. 

This is likely due to the way the scheduler deals with evictions; evicting extremely large jobs would have a severe negative impact on the overall \mpg score due to their huge startup overhead. Once the extra-large job is running, it is also immensely dependent on checkpointing and data sharding, affecting the Runtime and Program Goodput components as well. In short, evicting extra-large jobs from the hardware would present a cascading series of failures, strongly incentivizing the scheduler to reduce churn for these jobs and evict medium-sized jobs instead. 

On the other hand, extremely small jobs usually do not get prematurely evicted since they are more likely to finish quickly, and if pre-empted, it is usually quicker to find topologically matching availability. With extremely small jobs, the scheduler has more flexibility to intelligently allocate the workloads to optimal compute cells in order to defragment the overall ML fleet availability. It is also important to note that for workloads of all sizes, the SG is greater than 95\% due to the particular pre-emption preferences of the scheduler. The pre-emption preferences of the scheduler can be tuned, e.g. to require a SG of greater than 95\% for medium-sized jobs, but this could reduce the SG for other segments of the fleet.


\begin{figure}[t]
    \centering
    \includegraphics[height=1.8in]{final_figs/sg_job_size.png}
    \caption{Scheduling goodput by job size. Extra-large and small jobs tend to have better scheduling goodput due to the scheduler's preemption algorithm.}
    \label{fig:sg_job_size}
\end{figure}

