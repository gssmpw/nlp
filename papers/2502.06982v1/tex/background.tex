\section{Background}\label{sec:background}

We introduce the concept of an ML fleet, a specialized form of computing infrastructure designed specifically for ML workloads at scale. To help set the stage for an ML fleet, we compare ML fleets with two related large-scale computing paradigms: traditional warehouse-scale computers (WSC) \cite{barroso2009wsc} and high-performance computing (HPC) supercomputers \cite{asanovic2009berkeleyview}. An ML fleet can be considered an evolution of a WSC, adapted for high-performance ML workloads. 

Table~\ref{tab:comparison} summarizes the key features of each system type, highlighting their distinct characteristics. We examine these features to better understand how the unique demands of ML workloads have shaped the design and optimization strategies of ML fleets, setting them apart from their predecessors in large-scale computing.
\subsection{Workloads}
WSCs~\cite{barroso2009wsc} traditionally focus on Internet-scale services, which serve web content, process user queries, and handle data storage and retrieval for billions of users. WSCs must manage diverse, often latency-sensitive tasks such as search indexing \cite{brin1998anatomy}, social media feeds \cite{nishtala2013memcache}, and transaction processing services. The workloads are characterized by their burstiness and high concurrency, with millions of small, independent tasks that require rapid response times.

High-performance computing, on the other hand, has focused on simulation models for research, such as climate modeling \cite{menemenlis2005nasa}, and molecular dynamics for drug discovery \cite{schulz2009scaling}. These workloads involve complex, tightly-coupled computations that require both significant processing power and high-bandwidth, low-latency communication between nodes \cite{stegailov2019angara}. They operate on large datasets and require sustained performance over long periods of execution.

ML fleets combine the scale of WSC workloads with the computational intensity of HPC workloads. They consist of workloads which are focused on running deep neural networks \cite{alexnet2012}, such as recommendation models \citep{naumov2019deeplearningrecommendationmodel,zhao2022understanding} for e-commerce and social media, computer vision tasks for autonomous vehicles \cite{janai2020computer} and medical imaging \cite{esteva2021deep}, and natural language processing (NLP) applications like large language models for chatbots and translation services~\cite{zhao2021understanding}. 

ML workloads involve heavy matrix-matrix \cite{fatahalian2004understanding} or matrix-vector \cite{bell2008efficient} operations and parallel processing that often require high computational throughput and memory bandwidth. ML workloads also tend to be more communication-bound rather than compute-bound \cite{li2014communication}, especially with large models since they are often sharded across many chips and require large datasets to be fed into the model.

ML fleets also face another key challenge, which is rapidly fluctuating user demand and huge tectonic shifts in ML model architectures, as discussed in \autoref{sec:intro}. As a result, the designers of an ML fleet need to trade off specialization with fungibility to adapt quickly while still delivering peak performance. 
\subsection{Hardware}
The hardware composition of these systems also reflects the demands of its users. WSCs usually rely on general-purpose commodity CPUs~\cite{barroso2009wsc}, balancing diverse computational needs for web services, and scaling massively to accommodate varying user demands \cite{ranganathan2024twentyfive}. HPC supercomputers, designed for complex scientific calculations, utilize specialized vector processors \cite{odajima2020fujitsu} and accelerators to achieve extreme performance for tightly-coupled computations. An ML fleet must utilize hardware that can provide both the scalability of a WSC and the performance intensity of a HPC supercomputer. They are mostly comprised of accelerators such as GPUs, TPUs, and/or other ML application-specific integrated circuits (ASICs) that are optimized for the compute-intensive parallel processing demands of machine learning tasks. They also make significant use of general-purpose chips like CPUs for hosting or scheduling tasks.

Memory and storage architectures are also highly tailored to their specific workloads' needs. ML fleets typically employ high-throughput, local memory systems coupled with ML-optimized SSD storage, which facilitates fast data access and parallelism for iterative learning algorithms\cite{kumar2021exploring}.  WSCs, on the other hand, utilize distributed, commodity-based memory and storage systems, prioritizing cost-effectiveness and redundancy for handling diverse, large-scale web services \cite{barroso2009wsc}. HPC supercomputers feature low-latency, parallel-optimized memory architectures and parallel file systems, enabling efficient processing of massive scientific datasets. These distinctions in memory and storage design influence each system's data handling capabilities: WSCs ensure robust, scalable data management for varied web applications; HPC systems focus on minimizing latency for complex, data-intensive scientific computations; and ML fleets optimize for repeated, high-throughput access to training data and weight updates.  
\subsection{System Efficiency}

Network infrastructure and job scheduling  in ML fleets, WSCs, and HPC supercomputers are also optimized for their respective workloads. WSCs employ distribution-optimized networks to handle diverse, geographically dispersed web traffic, using cloud-based or custom schedulers \cite{burns2016borg} designed for varied, often short-lived tasks. HPC supercomputers, however, feature ultra-low latency networks crucial for tightly-coupled parallel computations, alongside specialized job schedulers like SLURM\cite{yoo2003slurm} that manage complex, long-running scientific workloads. ML fleets prioritize high-bandwidth networks \cite{zu2024resiliency} to facilitate rapid data movement for distributed training, coupled with job schedulers like Borg \cite{verma2015borg}, Kubernetes~\cite{rensin2015kubernetes} and MAST\cite{choudhury2024mast} that efficiently manage GPU/TPU resources. These differences in network and scheduling approaches directly impact each system's ability to handle its target applications.

Efficiency considerations and workloads vary across ML fleets, WSCs, and HPC supercomputers. ML fleets exhibit workload dependent energy efficiency, with high initial costs due to model training, but improved efficiency with tasks like bulk inference. WSCs prioritize high energy efficiency at scale and low per-unit costs, despite high aggregate expenses, handling diverse web services \cite{spanner} and cloud computing workloads. Meanwhile, HPC supercomputers focus on maximum performance for tightly-coupled scientific simulations, often at the expense of energy efficiency\cite{kamil2008power}. 

These distinctions in efficiency and workload optimization reflect each system's primary purpose: ML fleets are designed for flexible, scalable AI computation; WSCs for cost-effective, large-scale web services; and HPC systems for pushing the boundaries of computational performance in scientific research. The trade-offs between cost, energy efficiency, and performance in each system are carefully balanced to best serve their intended applications. In particular, if we wish to measure the performance of the ML fleet and identify opportunities for further optimization, we must study its characteristics across all levels of the system stack. 

