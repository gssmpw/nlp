\section{Related Works}
\label{sec:Related_Works}

\paragraph{\textbf{Deep Neural Models}}
TSF models are based on various methods: MLPs, Transformers, CNNs, and RNNs.
The simplicity of MLP-based models shows both high efficiency and competitive performance in TSF. DLinear~\cite{zeng22Dlinear} focues on only temporal information by processing each channel independently. 
TSMixer~\cite{chen23TSMixer} extends this by introducing cross-channel MLPs emphasizing the importance of learning channel correlations.
TimeMixer~\cite{wang24TM} learns both channel correlation and temporal information by decomposing data into seasaonality and trend in multi-resolution by downsampling.
Furthermore, Transformer-based models have become dominant in TSF. Informer~\cite{informer} improves efficiency with ProbSparse attention across all channels. 
In contrast, Autoformer~\cite{autoformer} and FEDformer~\cite{fedformer} captures channel dependencies through trend-seasonality decomposition and frequency-domain representations respectively.
PatchTST~\cite{nie23} focuses solely on local temporal patterns in each channel independently with patching mechanism. 
On the contrary, Crossformer~\cite{zhang23} captures multi-resolution temporal and cross-channel dependencies with multiple attention mechanisms.
iTransformer~\cite{liu24} reinterprets the Transformer architecture by applying feature-based attention across channels to capture channel dependencies.
CNN- and RNN-based models remain competitive in TSF.
MICN~\cite{MICN} employs isometric convolution for capturing both local and global channel correlation.
TimesNet~\cite{timesnet} reshapes time series into 2D representations, capturing periodic patterns and inter-channel dependencies via convolution.
Although many models consider channel dependence, most recent SOTA models in the TSF domain are based on channel independence.
This is because commonly used datasets, can be easily forecasted without accounting for channel correlation.

\paragraph{\textbf{Channel Dependence and Independence}}
\label{sec:CDI}
TSF models can be categorized based on whether they leverage channel dependencies. Channel dependence refers to considering the dependencies between different features~\cite{chen23TSMixer}, whereas channel independence treats multivariate data as separate univariate series, focusing on intra-channel temporal relationships~\cite{nie23}.
For example, PatchTST~\cite{nie23} captures temporal information for each channel individually.
In contrast, iTransformer~\cite{liu24} focuses solely on capturing channel dependencies. Additionally, some models leverage both temporal and cross-channel information.
For instance, models such as TSMixer~\cite{chen23}, Crossformer~\cite{zhang23}, and TimesNet~\cite{timesnet} emphasize learning both inter-/intra-channel dependencies.
\cite{ci_distri} argues that channel independence is more effective and robust, particularly due to distribution drift in datasets. When datasets are split into training and testing sets, their distributions often differ significantly, making it difficult for a model to capture dependencies between channels.
In contrast, ~\cite{cd_rethinking} suggests that channel dependence remains relevant, especially when accounting for time lags between different variables.
However, recent studies primarily focus on standard datasets, where channel dependence is often easy to overlook. As a result, there has been little in-depth exploration of the importance of channel dependence in the TSF domain.