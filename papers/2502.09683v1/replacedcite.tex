\section{Related Works}
\label{sec:Related_Works}

\paragraph{\textbf{Deep Neural Models}}
TSF models are based on various methods: MLPs, Transformers, CNNs, and RNNs.
The simplicity of MLP-based models shows both high efficiency and competitive performance in TSF. DLinear____ focues on only temporal information by processing each channel independently. 
TSMixer____ extends this by introducing cross-channel MLPs emphasizing the importance of learning channel correlations.
TimeMixer____ learns both channel correlation and temporal information by decomposing data into seasaonality and trend in multi-resolution by downsampling.
Furthermore, Transformer-based models have become dominant in TSF. Informer____ improves efficiency with ProbSparse attention across all channels. 
In contrast, Autoformer____ and FEDformer____ captures channel dependencies through trend-seasonality decomposition and frequency-domain representations respectively.
PatchTST____ focuses solely on local temporal patterns in each channel independently with patching mechanism. 
On the contrary, Crossformer____ captures multi-resolution temporal and cross-channel dependencies with multiple attention mechanisms.
iTransformer____ reinterprets the Transformer architecture by applying feature-based attention across channels to capture channel dependencies.
CNN- and RNN-based models remain competitive in TSF.
MICN____ employs isometric convolution for capturing both local and global channel correlation.
TimesNet____ reshapes time series into 2D representations, capturing periodic patterns and inter-channel dependencies via convolution.
Although many models consider channel dependence, most recent SOTA models in the TSF domain are based on channel independence.
This is because commonly used datasets, can be easily forecasted without accounting for channel correlation.

\paragraph{\textbf{Channel Dependence and Independence}}
\label{sec:CDI}
TSF models can be categorized based on whether they leverage channel dependencies. Channel dependence refers to considering the dependencies between different features____, whereas channel independence treats multivariate data as separate univariate series, focusing on intra-channel temporal relationships____.
For example, PatchTST____ captures temporal information for each channel individually.
In contrast, iTransformer____ focuses solely on capturing channel dependencies. Additionally, some models leverage both temporal and cross-channel information.
For instance, models such as TSMixer____, Crossformer____, and TimesNet____ emphasize learning both inter-/intra-channel dependencies.
____ argues that channel independence is more effective and robust, particularly due to distribution drift in datasets. When datasets are split into training and testing sets, their distributions often differ significantly, making it difficult for a model to capture dependencies between channels.
In contrast, ____ suggests that channel dependence remains relevant, especially when accounting for time lags between different variables.
However, recent studies primarily focus on standard datasets, where channel dependence is often easy to overlook. As a result, there has been little in-depth exploration of the importance of channel dependence in the TSF domain.