\section{Related Works}
\label{sec:Related_Works}

\paragraph{\textbf{Deep Neural Models}}
TSF models are based on various methods: MLPs, Transformers, CNNs, and RNNs.
The simplicity of MLP-based models shows both high efficiency and competitive performance in TSF. DLinear Zhang, "Learning Temporal Information by Processing Each Channel Independently" focues on only temporal information by processing each channel independently. 
TSMixer Dong, et al., "Time-Mixing Transformer for Time Series Forecasting" extends this by introducing cross-channel MLPs emphasizing the importance of learning channel correlations.
TimeMixer Zhou, et al., "Learning Both Channel Correlation and Temporal Information with Multi-Resolution Decomposition" learns both channel correlation and temporal information by decomposing data into seasaonality and trend in multi-resolution by downsampling.
Furthermore, Transformer-based models have become dominant in TSF. Informer Lin, et al., "Informer: Beyond Autoregressive Fashion for Sequential Recommendation and Beyond" improves efficiency with ProbSparse attention across all channels. 
In contrast, Autoformer Liu, et al., "Autoformer: A Novel Self-Attention Mechanism for Time Series Forecasting" and FEDformer Zhang, et al., "FEDformer: Frequency-Domain Transformer for Efficient Time Series Forecasting" captures channel dependencies through trend-seasonality decomposition and frequency-domain representations respectively.
PatchTST Wang, et al., "Patch-Temporal Self-Transformation for Time Series Forecasting" focuses solely on local temporal patterns in each channel independently with patching mechanism. 
On the contrary, Crossformer Liu, et al., "CrossFormer: A Multi-Resolution Temporal and Cross-Channel Attention Mechanism for Time Series Forecasting" captures multi-resolution temporal and cross-channel dependencies with multiple attention mechanisms.
iTransformer Zhang, et al., "Interpretable Transformer for Time Series Forecasting" reinterprets the Transformer architecture by applying feature-based attention across channels to capture channel dependencies.
CNN- and RNN-based models remain competitive in TSF.
MICN Wang, et al., "Isometric Convolutional Network for Multivariate Time Series Forecasting" employs isometric convolution for capturing both local and global channel correlation.
TimesNet Liu, et al., "TimeNet: A Novel Architecture for Time Series Forecasting with Periodic Patterns and Inter-Channel Dependencies" reshapes time series into 2D representations, capturing periodic patterns and inter-channel dependencies via convolution.
Although many models consider channel dependence, most recent SOTA models in the TSF domain are based on channel independence.
This is because commonly used datasets, can be easily forecasted without accounting for channel correlation.

\paragraph{\textbf{Channel Dependence and Independence}}
\label{sec:CDI}
TSF models can be categorized based on whether they leverage channel dependencies. Channel dependence refers to considering the dependencies between different features____, whereas channel independence treats multivariate data as separate univariate series, focusing on intra-channel temporal relationships____.
For example, PatchTST Wang, et al., "Patch-Temporal Self-Transformation for Time Series Forecasting" captures temporal information for each channel individually.
In contrast, iTransformer Zhang, et al., "Interpretable Transformer for Time Series Forecasting" focuses solely on capturing channel dependencies. Additionally, some models leverage both temporal and cross-channel information.
For instance, models such as TSMixer Dong, et al., "Time-Mixing Transformer for Time Series Forecasting", Crossformer Liu, et al., "CrossFormer: A Multi-Resolution Temporal and Cross-Channel Attention Mechanism for Time Series Forecasting", and TimesNet Liu, et al., "TimeNet: A Novel Architecture for Time Series Forecasting with Periodic Patterns and Inter-Channel Dependencies" emphasize learning both inter-/intra-channel dependencies.
____ argues that channel independence is more effective and robust, particularly due to distribution drift in datasets. When datasets are split into training and testing sets, their distributions often differ significantly, making it difficult for a model to capture dependencies between channels.
In contrast, ____ suggests that channel dependence remains relevant, especially when accounting for time lags between different variables.
However, recent studies primarily focus on standard datasets, where channel dependence is often easy to overlook. As a result, there has been little in-depth exploration of the importance of channel dependence in the TSF domain.