@misc{autoformer,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.13008}, 
}

@inproceedings{chen23TSMixer,
    title={TSMixer: An All-MLP Architecture for Time Series Forecasting},
    author={Chen, Si-An and Li, Chun-Liang and Yoder, Nate and Arik, Sercan O. and Pfister, Tomas},
    booktitle={arXiv preprint arXiv:2303.06053},
    year={2023},
    url={https://arxiv.org/abs/2303.06053}
}

@ARTICLE{ci_distri,
  author={Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={The Capacity and Robustness Trade-Off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting}, 
  year={2024},
  volume={36},
  number={11},
  pages={7129-7142},
  keywords={Time series analysis;Forecasting;Predictive models;Robustness;Training;Transformers;Biological system modeling;Channel independence;forecasting;multivariate time series;robustness},
  doi={10.1109/TKDE.2024.3400008}}

@article{fedformer,
  author       = {Tian Zhou and
                  Ziqing Ma and
                  Qingsong Wen and
                  Xue Wang and
                  Liang Sun and
                  Rong Jin},
  title        = {FEDformer: Frequency Enhanced Decomposed Transformer for Long-term
                  Series Forecasting},
  journal      = {CoRR},
  volume       = {abs/2201.12740},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.12740},
  eprinttype    = {arXiv},
  eprint       = {2201.12740},
  timestamp    = {Wed, 04 Dec 2024 16:44:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-12740.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{informer,
      title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
      author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
      year={2021},
      eprint={2012.07436},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.07436}, 
}

@misc{timesnet,
      title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis}, 
      author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
      year={2023},
      eprint={2210.02186},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.02186}, 
}

@inproceedings{wang24TM,
	title={TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting},
	author={Wang, Shiyu and Wu, Haixu and Shi, Xiaoming and Hu, Tengge and Luo, Huakun and Ma, Lintao and Zhang, James Y and ZHOU, JUN},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024}
}

@inproceedings{zeng22Dlinear,
    title={Are Transformers Effective for Time Series Forecasting?},
    author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
    booktitle={arXiv preprint arXiv:2205.13504},
    year={2022},
    url={https://arxiv.org/abs/2205.13504}
}

