\section{Related Works}
\subsection{Information Cascade Modeling}
    Information cascade modeling can be primarily categorized into two types: micro-level~\cite{qiu2018deepinf,yang2019neural} and macro-level~\cite{li2017deepcas,CasCN}.
    The former focuses on predicting next affected user, while the latter concentrates on the overall trends, such as its popularity or outbreak status.
    In this paper, we focus on the cascade popularity prediction and categorize existing methods into three types:
    
    \noindent \textbf{Feature-based methods.}
    These works focus on making hand-crafted features for cascade and conducting popularity prediction via traditional machine learning approaches~\cite{szabo2010predicting}.
    However, feature-based methods heavily rely on the expert knowledge, has high customization costs, and exhibits limited generalization and suboptimal performance~\cite{qiu2018deepinf}. 

    \noindent \textbf{Statistics-based methods.}
    These studies assume that information diffusion follow a specific probability statistical model, such as the Poisson process~\cite{iwata2013discovering}, Hawkes process~\cite{rizoiu2018sir}. 
    Statistics-based methods are interpretable but have strong parametric assumptions, making them unsuitable for real-word applications~\cite{zhou2021survey}.
    
    \noindent \textbf{Deep leaning methods.}
    These methods adopt deep learning techniques to promote cascade popularity prediction.
    Early representative works, such as DeepCas~\cite{li2017deepcas} and DeepHawkes~\cite{cao2017deephawkes}, focus on capturing the temporal dynamics of cascades via RNN or LSTM.
    Considering the topology in information cascade, GNNs have been introduced to capture local structural patterns. 
    For example, CasCN~\cite{CasCN} and CoupledGNN~\cite{cao2020popularity} adopt variant GNNs to model the interactions between users and the spread influence.
    Apart from local structure, CasFlow~\cite{casflow} introduces the social network as global context to enhance popularity prediction.
    Advanced techniques, such as VAEs~\cite{zhou2020variational,wang2021dydiff}, Transformers~\cite{Castformer,yu2022transformer}, and Neural ODEs~\cite{rubanova2019latent,cheng2024information}, have been further explored in cascade modeling.
    For more comprehensive reviews, please refer to~\cite{zhou2021survey,li2021capturing}.

    \noindent \textbf{Towards LLM-based methods.}
    Due to the strong generalization capabilities, various fields such as vision~\cite{awais2025foundation,zhang2024vision} and time-series~\cite{liu2024autotimes,liang2024foundation} are renovates by the general frameworks based on LLMs. 
    To our best knowledge, this is the first attempt to introduce an LLM-based method in information cascade modeling.

\subsection{Autoregressive Modeling}
    Autoregression is a fundamental concept in sequence modeling, which uses observations from previous time steps to predict the next value~\cite{gregor2014deep}.
    This paradigm, which provides fine-grained supervision, has become the best practice for training LLMs ~\cite{radford2019language,achiam2023gpt,bai2023qwen} and has also inspired other fields~\cite{van2016pixel,tian2024visual,rajasegaran2025empirical}.
    Here, we briefly categorize existing works into three types:
    
    \noindent \textbf{RNN-based methods.}
    Early studies perform the autoregressive modeling based on RNN variants, and achieve success across various domains~\cite{ranzato2014video,van2016conditional,van2016pixel}.
    However, these approaches come with imitations of RNNs~\cite{lipton2015critical}, including low computational efficiency and limited capability in long-distance dependencies.
    
    \noindent \textbf{Transformer-based methods.}
    Following the introduction of Transformer~\cite{vaswani2017attention}, the potential of autoregressive modeling has been further explored, with representative works including iGPT~\cite{chen2020generative}, Autoformer~\cite{wu2021autoformer} and VAR~\cite{tian2024visual}.
    Currently, transform-based methods dominate the field of autoregressive modeling.
    
    \noindent \textbf{LLM-based methods.}
    Built upon Transformer architecture, LLMs with large-scale parameters are pretrained on massive datasets, demonstrating superior capabilities in autoregressive modeling.
    Therefore, researchers attempt to investigate the feasibility of reusing LLMs for autoregressive modeling.
    For example, Toto~\cite{rajasegaran2025empirical} treats videos as sequences of visual tokens and reuses the LLMs as backbones to autoregressively predict future tokens.
    Similar ideas have also been applied in the field of time-series forecasting~\cite{liu2024autotimes}.

\subsection{Prompt Learning}
    Prompt learning~\cite{liu2023pre} has emerged as a novel learning paradigm to adapt LLMs to specific tasks by designing textual prompts.
    Due to the widespread application of LLMs, designing sophisticated textual prompts for specific tasks has become the research hotspot across various fields~\cite{gong2024self}.
    For example, 
    LLM4NG~\cite{yu2023empower} designs the class-level semantic prompt templates based on text-attribute on graphs to facilitate the node classification in few-shot scenarios.
    Autotimes~\cite{liu2024autotimes}  introduces textual timestamps of time-series to enhance LLM-based forecasting.
    Building on advanced prompting techniques, deft textual prompts ~\cite{jin2023time,liu2024unitime} for time-series are further explored.
    Since LLMs have not yet fully entered the field of information cascade modeling, studies on prompt learning for cascade data are still lacking. 
    However, textual information is prevalent in cascade diffusion process, so designing cascade prompt templates based on textual information for cascade popularity prediction holds significant promise.