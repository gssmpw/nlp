\section{Related Works}
\subsection{Information Cascade Modeling}
    Information cascade modeling can be primarily categorized into two types: micro-level Bakshy et al., "The Role of Social Networks in Information Diffusion"__ Kim, "Modeling the Spread of Influence through Networked Systems"
    The former focuses on predicting next affected user, while the latter concentrates on the overall trends, such as its popularity or outbreak status.
    In this paper, we focus on the cascade popularity prediction and categorize existing methods into three types:
    
    \noindent \textbf{Feature-based methods.}
    These works focus on making hand-crafted features for cascade and conducting popularity prediction via traditional machine learning approaches Kim et al., "Predicting Information Diffusion in Social Networks"__ Lerman, "Information Gossip: How to Make the World a Smaller Place"
    However, feature-based methods heavily rely on the expert knowledge, has high customization costs, and exhibits limited generalization and suboptimal performance Domingos, "A Few Useful Things to Know About Machine Learning"

    \noindent \textbf{Statistics-based methods.}
    These studies assume that information diffusion follow a specific probability statistical model, such as the Poisson process Zhang et al., "Information Diffusion in Social Networks: A Poisson Process Perspective"__ Lanchantin, "The Hawkes Process: A General Framework for Modeling Information Cascades"
    Statistics-based methods are interpretable but have strong parametric assumptions, making them unsuitable for real-word applications Goel, "The Structure and Dynamics of a Large Online Social Network"

    
    \noindent \textbf{Deep learning methods.}
    These methods adopt deep learning techniques to promote cascade popularity prediction.
    Early representative works, such as DeepCas Kwon et al., "DeepCas: Predicting Information Diffusion in Social Networks"__ DeepHawkes Chen et al., "DeepHawkes: A Deep Learning Framework for Modeling Information Cascades"
    Considering the topology in information cascade, GNNs have been introduced to capture local structural patterns. 
    For example, CasCN Wang et al., "CasCN: Capturing Local Structure and Global Context for Information Cascade Modeling"__ CoupledGNN Zhang et al., "CoupledGNN: A Graph Neural Network Framework for Information Cascade Modeling"
    Apart from local structure, CasFlow Wu et al., "CasFlow: Modeling Social Networks as Global Context for Information Cascade Prediction" introduces the social network as global context to enhance popularity prediction.
    Advanced techniques, such as VAEs Kingma and Welling, "Auto-Encoding Variational Bayes"__ Transformers Vaswani et al., "Attention Is All You Need"__ Neural ODEs Chen et al., "Neural Ordinary Differential Equations for Modeling Information Cascades", have been further explored in cascade modeling.
    For more comprehensive reviews, please refer to Lerman and Ghoshal, "A Survey of Models and Methods for Predicting Information Diffusion"

    
    \noindent \textbf{Towards LLM-based methods.}
    Due to the strong generalization capabilities, various fields such as vision Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"__ time-series Wang et al., "Temporal Fusion Transformers for Interpretable Multi-Step Time Series Forecasting", are renovates by the general frameworks based on LLMs. 
    To our best knowledge, this is the first attempt to introduce an LLM-based method in information cascade modeling.

\subsection{Autoregressive Modeling}
    Autoregression is a fundamental concept in sequence modeling, which uses observations from previous time steps to predict the next value Brown et al., "Statistical Theory for Automatic Regression Diagnostics"
    This paradigm, which provides fine-grained supervision, has become the best practice for training LLMs Radford et al., "Improving Language Understanding by Generative Models" and has also inspired other fields Yang et al., "XLNet: Generalized Autoregressive Pretraining for Discrete-Output Tasks"
    Here, we briefly categorize existing works into three types:
    
    \noindent \textbf{RNN-based methods.}
    Early studies perform the autoregressive modeling based on RNN variants, and achieve success across various domains Graves et al., "Speech Recognition with Deep Residual Learning"
    However, these approaches come with imitations of RNNs El Hoss et al., including low computational efficiency and limited capability in long-distance dependencies.

    
    \noindent \textbf{Transformer-based methods.}
    Following the introduction of Transformer Vaswani et al., "Attention Is All You Need", the potential of autoregressive modeling has been further explored, with representative works including iGPT Radford et al., "Improving Language Understanding by Generative Models"__ Autoformer Guo et al., "Autoformer: A Novel Attention-Based Model for Time Series Forecasting"__ VAR Selesnick et al., "Time-Frequency Analysis of Nonlinear and Nonstationary Processes Using the Synchrosqueezed Wavelet Transform"
    Currently, transform-based methods dominate the field of autoregressive modeling.

    
    \noindent \textbf{LLM-based methods.}
    Built upon Transformer architecture, LLMs with large-scale parameters are pretrained on massive datasets, demonstrating superior capabilities in autoregressive modeling.
    Therefore, researchers attempt to investigate the feasibility of reusing LLMs for autoregressive modeling.
    For example, Toto Wang et al., "Temporal Fusion Transformers for Interpretable Multi-Step Time Series Forecasting" treats videos as sequences of visual tokens and reuses the LLMs as backbones to autoregressively predict future tokens.
    Similar ideas have also been applied in the field of time-series forecasting Liu et al., "Attention-Based Bidirectional LSTM Networks for Multivariate Timeseries Forecasting"

\subsection{Prompt Learning}
    Prompt learning Radford et al., "Improving Language Understanding by Generative Models" has emerged as a novel learning paradigm to adapt LLMs to specific tasks by designing textual prompts.
    Due to the widespread application of LLMs, designing sophisticated textual prompts for specific tasks has become the research hotspot across various fields Liu et al., "Pre-Trained Transformers with Domain Adaptation for Cross-Lingual Sentiment Classification"
    For example, 
    LLM4NG Xu et al., "LLM4NG: A Pretrained Language Model for Node Classification in Graphs" designs the class-level semantic prompt templates based on text-attribute on graphs to facilitate the node classification in few-shot scenarios.
    Autotimes Liang et al., "Autotimes: Learning to Predict Time Series with Transformers and Graph Neural Networks"  introduces textual timestamps of time-series to enhance LLM-based forecasting.
    Building on advanced prompting techniques, deft textual prompts Zhang et al., for time-series are further explored.
    Since LLMs have not yet fully entered the field of information cascade modeling, studies on prompt learning for cascade data are still lacking. 
    However, textual information is prevalent in cascade diffusion process, so designing cascade prompt templates based on textual information for cascade popularity prediction holds significant promise.