\section{Related Work}
\label{sec:current_work}

Sign language translation is the task of mapping a sign language video into its spoken language counterpart. Recently, computational approaches have been introduced to improve recognition of sign language instances in a continuous manner. Hao Zhou \textit{et al}., 2020. \cite{9354538} proposed an architecture that seeks to serve in the Continuous Sign Language Recognition (SLR) task. The main idea of this work is to identify the relevance of the parameters involved in the production of signs through a feature extractor process by employing a CNN as their backbone; then a temporal modelling next to a sequence learning process is performed in order to recognize a sequence of \textit{glosses} \footnote{Sign glosses are spoken language words that match the meaning of
signs.} from a sign language video. However, this approach focuses solely on the recognition of signs, putting aside the translation task. Kayo Yin \textit{et al}., 2020. \cite{yin2020better} explored the mapping between the sequence of glosses identified in \cite{9354538} and it's correspondence in spoken language as a text-to-text task. A Transformer-based architecture \cite{vaswani2017attention} is employed to first encode a proper representation of sign glosses that will be further decoded taking advantage of the Transformer's self-attention mechanisms to identify the temporal correspondences between both sequences. Nevertheless, in this approach the translation task heavily relies on the quality of the gloss-recognition process which also implies modularizing the translation while failing to fully exploit the potential of encoder-decoder-based architectures to perform the task in and end-to-end manner. On the other hand, Rodriguez Jefferson \textit{et al}., 2021. \cite{https://doi.org/10.1049/cvi2.12037} proposed an encoder-decoder architecture to tackle end-to-end sign language translation while exploring optical flow as a more suitable representation of sign features. Optical flow representations allow to highlight spacial kinematic patterns which are subsequently processed by Bidirectional Recurrent Neural Network (BRNN) units in conjunction with attention modules to better exploit more complex temporal relationships along video descriptors. This approach is mostly based on recurrence, which currently has been surpassed by the introduction of Transformer networks that have been proved to be more efficient regarding information processing while being more powerful identifying contextual relationships. On the same line, Camgoz Necati \textit{et al}., 2020. \cite{camgoz2020sign} proposed a Transformer network to perform sign language translation while introducing a sign language recognition system within the encoder. Although this approach has represented a significant advance in the introduction of continuous sign language translation systems in real-life scenarios; this work, as well as most of the aforementioned approaches, generate a 1D representation of sign language images that might lack of relevant spatial information associated to the dimentional downsampling carried out during the feature extractor process.\\


\begin{figure*}[ht!]
\centering
\includegraphics[scale=0.35]{images/arch.io.drawio.png}
\label{Fig:pipeline}
\caption{Pipeline of the proposed Transformer architecture. 1) Input representation. 2) Two-dimensional Self-Attention. 3) Feed-forward network.}
\end{figure*}