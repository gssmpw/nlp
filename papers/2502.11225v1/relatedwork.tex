\section{Previous works on hybridization and \ad}
%of \algInPaper} 
\label{sec:Previous works on hybridization}
The goal of creating hybrid \mhs is to exploit the strengths that different optimization techniques can offer to solve optimization problems%, while simultaneously reducing any potential drawbacks
~\cite{GroAbr07:hea,Tal2013hm,BluRai2016:book,StuLop2019hb,CalArmMas2017:Learnheuristics}.
In the following, we discuss the main ideas proposed in hybrids of \algInPaper, as well as some modular \msfs that have been proposed to study these metaheuristics and their hybrids in an \ad context.
Note, however, that this review of the literature is by no means exhaustive, as we focus on papers proposing ideas that are particularly amenable to be implemented in \MetafoR, such as single-objective hybrids and not overly complicated hybrids.
%we excluded papers proposing %(i) heavily modified variants of \algInPaper (such as \cite{LiLeiYua21:aitel} and \cite{WarAraBie22:gecco}, that make use of several additional components) and (ii) variants for multi-objective optimization problems.

\subsection{\PSO and \DE hybrids}
%Tim Hendtlass. 2001. Discussed in PSO-DE 
Hendtlass proposed an algorithm called swarm differential evolution algorithm (SDEA) \cite{Hendtlass01:iea-aie}. SDEA works mainly as the standard \PSO algorithm, but intermittently applies the \DE mutation operator to the particles' current solutions to avoid local minima. %The particles keep their velocity after being modified by the application of \DE mutation.
%Wen-Jun Zhang and Xiao-Feng Xie. 2003. Discussed in PSO-DE 
Zhang and Xie proposed an algorithm called DEPSO, in which the \stanPSO is applied during even iterations of the algorithm, and the \DE mutation operator is applied to the particles' personal best during odd iterations~\cite{ZhaXie03:ieee-smc}.
%Das, Swagatam, Amit Konar, and Uday K. Chakraborty, 2005 
Das \etal proposed a hybrid algorithm in which the personal component of the velocity update rule of the \stanPSO is modified based on the mutation operator of \DE~\cite{DasKonCha05:gecco}.
%M. G. H. Omran, A. P. Engelbrecht, and A. Salman. 2007. Discussed in PSO-DE 
In \cite{OmrEng07:ieee-sis}, Omran \etal introduced a modified version of Hendtlass’ SDEA that uses a probabilistic approach based on the "barebones" \PSO. 
%(In the barebones \PSO~\cite{Kennedy2003barebones_PSO}, the velocity vector is replaced with a vector of random numbers sampled from a Gaussian distribution.)
In their algorithm, with probability $p_r$, particles update their position using the barebones \PSO position update rule and add a vector difference, and with probability $1-p_r$, they update their position to the personal best of a randomly selected particle.
%M. Pant, R. Thangaraj, C. Grosan, and A. Abraham. 2008. Discussed in PSO-DE 
Pant \etal introduced a two-phase hybrid version of \DE that uses \PSO as perturbation mechanism~\cite{PanThaGro08:ieee-dim}. Pant \etal's algorithm follows the usual \DE procedure up to the point where the trial vector is generated; if the trial vector is rejected, the algorithm applies the \PSO velocity and position update rules to generate a new solution.
%M.G. Epitropakis, V.P. Plagianakos, M.N. Vrahatis, 2012 
Epitropakis \etal proposed to evolve the social and cognitive components of a swarm by applying the three usual \DE operators (i.e., mutation, recombination and selection) to the personal best positions particles (i.e., vectors $\vec{p}^{i}$ in \Eq~\ref{eq:psoVelocity})~\cite{EpiPlaVra12:is}. 

%B. Xin, J. Chen, J. Zhang, H. Fang, Z.-H. Peng, 2012 
% A taxonomy of \PSO-\DE hybrids was provided by Xin et al. \cite{XinCheZha11:ieee-tsmc}.
% The authors identified three main ways to hybridize \PSO and \DE: 
% (i) collaboration-based, where the two algorithms keep their usual procedures and exchange accumulated information during the search (e.g., \cite{Hendtlass01:iea-aie} and \cite{ZhaXie03:ieee-smc});
% (ii) embedding-based, where the components of one of the two algorithms are modified and the individual contribution of each algorithm to the optimization process cannot be clearly distinguished (e.g., \cite{DasKonCha05:gecco} and \cite{EpiPlaVra12:is});
% and 
% (iii) assistance-based, in which one of the algorithms serve as assistance to the other to provide parts of the solution that are evaluated by the objective function or to fine-tune the base algorithm parameter values (e.g., \cite{OmrEng07:ieee-sis}). 

\subsection{\PSO and \CMAES hybrids}
%Chang-Tai HsiehChih-Ming ChenYing-ping Chen, 2007 \cite{HsiCheChe07:gecco}
%Particle Swarm Guided Evolution Strategy
One of the earliest \PSO--\ESs hybrids is the "particle swarm guided evolution strategy" proposed by Hsieh \etal~\cite{HsiCheChe07:gecco}.
Although this approach is not properly based on \CMAES, the authors introduced the idea that \ESs can be used to focus on exploiting good quality solutions, while \PSO can be used to focus on performing effective search space exploration.
%Chang-Tai and ChenYing-ping' algorithm uses the so-called guided mutation, where the mutation vector is added to each solution is first (i) re-scale dimension-wise according to a learning rate and the mutation step-size, and (ii) rotated such that it points toward the global best solution.
%Note that the idea of attracting the individuals towards the global best solution is the only concept from \PSO used in this algorithm.
%
%C. L. Müller, B. Baumgartner, and I. F. Sbalzarini, 2009, “Particle swarm CMA evolution strategy for the optimization of multi-funnel landscapes,” in Proceedings of the IEEE Congress on Evolutionary Computation(CEC). IEEE, 2009, pp. 2685–2692.
%Based on the idea of Chang-Tai \etal, 
Müller \etal proposed to run multiple \CMAES instances in parallel considering each instance as an individual particle in \PSO~\cite{MulBauSba09:ieee-cec}.
The proposed algorithm is divided into two phases: a \CMAES phase, which follows the usual \CMAES algorithm, and a \PSO phase, where the best solutions found by each \CMAES instance applies the \stanPSO velocity and position update rules.
The equation to adapt the covariance matrix of \CMAES was modified to combine the \textit{local} information gathered by a \CMAES with the \textit{global} information of \PSO.
%In particular, the \PSO covariance matrix is obtained by rotating the \CMAES covariance matrix %such that its principal eigenvector is aligned with the vector that points from mean of the sampling distribution toward the global best position.
%in the direction of the global best position.
Müller \etal also added a bias to the mean of the sampling distribution of \CMAES in the following cases: (i) when the instance has already converged to a local minimum located far from the the global best solution, and (ii) when the instance is different from the one that produced the global best solution and the step-size has fallen below a certain threshold.
%
%Peilan Xu, Wenjian Luo, Xin Lin, Yingying Qiao and Tao Zhu, 2019 
%Hybrid of PSO and CMA-ES for Global Optimization
In \cite{XuLuoLin19:ieee-cec}, Peilan \etal introduced a hybrid, three-phase algorithm that uses multiple populations and two different versions of \PSO, the \stanPSO and a \PSO with time windows (PSOtw).
In PSOtw, particles can only access their personal best if it is within a certain time window $tw$ given in number of iterations.
%Peilan \etal's algorithm is divided into three phases.
The first phase of Peilan \etal's algorithm consists in applying PSOtw by each population for a number of iterations; in the second phase, the best and second best solutions in each population are selected to create a temporary population $P_t$; in the third phase and last, the \stanPSO and \CMAES algorithms are applied one after the other to $P_t$ for a number of function evaluations and the best solutions in $P_t$ are selected for the next iteration.
%
%%%% This is a good article for the next version of the framework %%%%
%Wei Li, Zhou Lei, Junqing Yuan, Haonan Luo and Qingzheng Xu, 2021, \cite{LiLeiYua21:aitel}
%Enhancing the competitive swarm optimizer with covariance matrix adaptation for large scale optimization

\subsection{\CMAES and \DE hybrids}
%Jérôme Henri Kämpf, Darren Robinson, 2008 \cite{KamJerRob09:asoc} A hybrid CMA-ES and HDE optimisation algorithm with application to solar energy potential
Kämpf and Robinson proposed to execute \CMAES and \DE in sequential order, with \CMAES followed by \DE~\cite{KamJerRob09:asoc}.
The elite solutions found by \CMAES are input to \DE, but since \DE uses a larger population size compared to \CMAES, the \DE population is completed with randomly created solutions.
%Saurav Ghosh, Swagatam Das, Subhrajit Roy, S.K. Minhazul Islam, P.N. Suganthan, 2011 \cite{GhoDasRoy12:is} A Differential Covariance Matrix Adaptation Evolutionary Algorithm for real parameter optimization
Ghosh \etal introduced a hybrid algorithm that incorporates the operators of \DE into the structure of \CMAES~\cite{GhoDasRoy12:is}.
The standard mechanism of \CMAES is used in each iteration to sample new solutions from a multivariate normal distributions, after which the population is handled as in \DE.
At each iteration, Ghosh \etal' algorithm performs the following steps: (i) update the mean of the sampling distribution; (ii) adapt the covariance matrix;
(iii) create a population of mutants using components from the eigen decomposition of the covariance matrix; and (iv) apply the crossover and selection operators of the usual \DE algorithm.
%Shu-Mei Guo, Member, IEEE, and Chin-Chang Yang, 2015 \cite{GuoYan14:tevc}
%Enhancing Differential Evolution Utilizing Eigenvector-Based Crossover Operator
In \cite{GuoYan14:tevc}, Guo and Yang proposed a crossover operator that allows individuals to exchange information between the target vector and the mutant vector using the eigenvector basis instead of the natural basis, thus rotating the coordinate system and making the algorithm rotation invariant.
%Xiaoyu He, Yuren Zhou, 2018 \cite{HeZho18:asoc}
%Enhancing the performance of differential evolution with covariance matrix self-adaptation
In \cite{HeZho18:asoc}, He and Zhou presented a hybrid algorithm based on a new mutation operator and a simplified step-size control rule.
The mutation operator used the information of previously rejected solutions and a Gaussian distribution to sample probabilistically "better" solutions.
The standard covariance matrix update of \CMAES was modified to include only the rank-$\mu$ update, and not the rank-one update.
He and Zhou also added a local search that takes place only if there are still at least 10\% of the total number of function evaluations available by the time the algorithm has converged. 
%Zhe Chen and Yuanxing Liu, 2022 \cite{CheLiu22:nature-scirep}
%Individuals redistribution based on differential evolution for covariance matrix adaptation evolution strategy
Chen and Liu proposed a hybrid algorithm in which \DE is used to break the stagnation state of \CMAES~\cite{CheLiu22:nature-scirep}.
The algorithm measures the average fitness improvement of the solutions after each iteration of \CMAES and, when the average falls below a predefined threshold, \DE is executed to regenerate the population. 
To make sure that the new solutions obtained by \DE are redistributed in the search space, only the offspring are selected for survival.

%Eryk Warchulski, Jarosław Arabas, Rafał Biedrzycki \cite{WarAraBie22:gecco}
%Improving the Differential Evolution Strategy by coupling it with CMA-ES

\subsection{Modular \msfs for \algInPaper}
\Msfs (\MSFs) that can be used to automatically generate implementations of \algInPaper have already been proposed in the literature: \PSOX~\cite{CamDorStu2022:tec}, \aCMAES~\cite{deNVerWan2021:gecco-CMAESframework}, \aDE~\cite{VerCarKon2023:autoDE:gecco} and \modPSODE~\cite{BokWanBac2020:gecco-PSO-DE}. %\PaEO~\cite{CahMelTal2004,DreLieVer2021:gecco-oparadiseo}.
Both \PSOX, \aCMAES and \aDE are parameterized \MSFs that allow users to create many different implementations by simply changing the parameters used to execute the framework. 
This aspect makes them particularly suitable for an \ad context, as they can be easily coupled with an \AACT tool such as \irace.
On the other hand, the main drawback of these \MSFs is that they only contain components of a specific metaheuristic, \PSO, \CMAES and \DE, respectively.

The modular \modPSODE framework is, to the best of our knowledge, the only \MSF specifically developed for creating \PSO and \DE hybrids in an automatic design context~\cite{BokWanBac2020:gecco-PSO-DE}.
Unfortunately, it has two important downsides: (i) a limited number of components of \PSO and \DE compared to \PSOX and \aDE; and (ii) lack of flexibility to combine components at a fine-grained level.
The \modPSODE framework is based on the idea of simultaneously generating two populations, one using \PSO components and the other using \DE components, and combining them into a single population that is then reduced to a predefined population size using a selection operator.
This type of hybridization is not particularly useful to explore the interaction of \PSO components in \DE and vice-versa.%\todo{General question: The real question I have if these are "all" the things introduced in the literature. I mean, PSO and DE seems to terminate in 2012, which is 12 years from now. In the others, its true, there are more recent ones.}
%\todo{Christian: I added a sentence at the beginning of the section explaining why two more recent paper were left out of the literature review.}