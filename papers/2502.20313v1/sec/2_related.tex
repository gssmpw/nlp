\section{Related Work}
\label{sec:related}



VQ-VAE \cite{vqvae} introduces a groundbreaking two-stage image generation paradigm: (1) encoding the image into a latent space and quantizing it to the nearest code in a fixed-size codebook; (2) modeling the discretized code using PixelCNN \cite{pixelcnn}, which predicts the probability distribution of each code in raster scan order. This two-stage paradigm has laid the foundation for many subsequent works.


\noindent \textbf{Raster-scan Manner}
Building on the aforementioned foundation, \cite{esser2021taming, ramesh2021zero} perform autoregressive learning in latent space with Transformer architecture. VQVAE-2 \cite{vqvae2} and RQ-Transformer \cite{lee2022autoregressive} use extra scales or stacked codes for next-image-token prediction. These works further advance the field and achieve impressive results.
Recently, \cite{llamagen, liu2024lumina} utilize a GPT-style next-token-prediction strategy to achieve high-quality image generation. 
\cite{he2024mars} further improves this paradigm by introducing a mixture of autoregressive models, while \cite{aim} incorporates Mamba structure \cite{gu2023mamba} to accelerate image generation.
\cite{xie2024show, zhou2024transfusion, gu2024dart} combine diffusion processes into autoregressive modeling to address the information loss caused by quantization, which potentially degrades the quality of generated images.




\noindent \textbf{Random-scan Manner}
Masked-prediction models learn to predict masked tokens in a BERT-style manner \cite{devlin2018bert, he2022masked, bao2021beit}.  They introduce a bidirectional transformer that predicts masked tokens by attending to unmasked conditions, thus generating image tokens in a random-scan manner. This approach enables parallel token generation at each step, significantly improving inference efficiency. Specifically, \cite{chang2022maskgit, chang2023muse} apply masked-prediction models in class-to-image and text-to-image generation, respectively. MagViT series \cite{yu2023magvit, luo2024open} adapts this approach to videos by introducing a VQVAE for both images and videos. NOVA \cite{deng2024autoregressive} first predicts temporal frames and then predicts spatial sets within each frame to achieve high-quality image/video generation.



\noindent \textbf{Scaling-scan Manner}
VAR \cite{var} establishes a new generation paradigm that redefines autoregressive learning on images from next-token-prediction to next-scale-prediction. VAR in parallel predicts image tokens at one scale, significantly reducing the number of inference steps. 
Following VAR, VAR-CLIP \cite{zhang2024var}  achieves text-to-image generation by converting the class condition token into text tokens obtained from the CLIP Text Encoder.
In terms of operational efficiency, \cite{code} introduces an efficient decoding strategy, \cite{ren2024m} incorporates linear attention mechanisms to accelerate image generation, and \cite{li2024imagefolder} designs a lightweight image quantizer, significantly reducing training costs. 
Regarding generation quality, \cite{tang2024hart, Ren2024FlowAR} optimize image details by using continuous tokenizers in combination with flow matching or diffusion model. Infinity \cite{han2024infinity} redefines the visual autoregressive model under a bitwise token prediction framework, remarkably enhancing generation capability and detail.


 