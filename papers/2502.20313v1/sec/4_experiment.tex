
\section{Experiments}
\label{sec:exp}

\subsection{Implementation details}
\noindent \textbf{FlexVAR tokenizer.}
Our scalable VQVAE tokenizer is configured with a downsampling factor of 16 and is initialized with the pre-trained weights from LlamaGen \cite{llamagen}, the codebook size is set to 8912, and the latent space dimension is set to 32. The quantization of each scale shares the same codebook. We follow the VQVAE training recipe of LlamaGen. The training is on OpenImages \cite{openimages} with a constant learning rate of $10^{-4}$, AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, weight decay = 0.05,  a batch size of 128, and for 20 epochs. $K$ is set to 5 by default, indicating that each latent space is randomly sampled into 5 different resolutions.

\noindent \textbf{FlexVAR transformer.}
We provide FlexVAR in three scales, with detailed configurations for each scale provided in Tab \ref{tab:scale}.
FlexVAR is trained on the ImageNet-1K 256$\times$256 using 80GB A100 GPUs. The training process employs the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, and a weight decay rate of 0.05. The learning rate is set to 1e-4, with the training epochs varying between 180 and 350 depending on the model scale.

\input{table/scaling-up}


\input{table/main}

\subsection{State-of-the-art image generation}
We compare FlexVAR with existing generative methods on the ImageNet-1K benchmark, including GAN, diffusion models, random-scan, raster-scan, and scaling-scan autoregressive models.  As shown in Tab. \ref{tab:main}, \ref{tab:main-512}.

\noindent \textbf{Overall comparison on ImageNet 256$\times$256.} To ensure a fair comparison, we present models with a size smaller than 1B in Tab. \ref{tab:main}. Our FlexVAR achieves state-of-the-art performance in all generative methods, and performs remarkably well compared to the VAR counterparts.  Specifically, we achieve -0.45, -0.56, and -0.12 FID improvement compared with VAR at different model sizes.

\noindent \textbf{Zero-shot inference with more steps.} 
We use 13 steps for image generation without training, as shown in the last row of Tab. \ref{tab:main}. FlexVAR can flexibly adopt more steps to improve image quality. By using 13 inference steps, FlexVAR further enhances the performance to 2.08 FID and 315 IS, manifesting strong flexibility and generalization capabilities.
The specific steps design is detailed in the Appendix.

\input{table/main-512}

\noindent \textbf{Zero-shot inference on ImageNet 512$\times$512 benchmark.} 
We use FlexVAR-$d24$ to generate 512$\times$512 images and evaluate on ImageNet-512 benchmark without training, as shown in Tab. \ref{tab:main-512}. Surprisingly, our FlexVAR-$d24$ exhibits competitive performance when compared to VAR, despite FlexVAR being trained only on resolutions $\leq$ 256$\times$256 and having only 1.0B parameters.


\subsection{Ablation study}
\input{table/abla-comp}
We conduct ablation studies on various  design choices  in FlexVAR
% to show their contribution to the final results
.
Due to the limited computational resources, we report the results trained with a short training scheme in Tab. \ref{tab:ab-comp}, \ref{tab:ab-mamba}, \ref{tab:ab-posi}, \textit{i.e.}, 40 epochs  ($\sim$ 70K iterations).

\noindent \textbf{Component-wise ablations.} 
To understand the effect of each component, we start with standard VAR and progressively add each design. (Tab. \ref{tab:ab-comp}):
\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{Baseline:} VAR uses a residual prediction paradigm and exhibits decent performance (1$^{st}$ result), but its flexibility in image generation does not meet expectations (as described in Sec. \ref{sec:intro}).
\item \textbf{Prediction type:} It is infeasible to directly convert the prediction type to GT, as seen in the 2$^{nd}$  and 3$^{rd}$ results. We employ the VQVAE tokenizers from VAR and Llamagen, both of which yield inferior performance. This is not surprising, as the current tokenizers lack robustness to images with varying latent space, while we force these tokenizers to obtain multi-scale latent features during training  (we provide a detailed analysis in Fig. \ref{fig:abla-vae}).
% which motivates us to train a scalable tokenizer to adapt to the GT prediction paradigm.
\item \textbf{Tokenizer:}  Our scalable tokenizer obtains reasonable multi-scale latent features during training, resulting in an improvement of -13.87 FID (the 4$^{th}$ results). However, flexible image generation is not accomplished yet.
\item \textbf{Position embedding:} The introduction of our scalable PE (last result) provides high flexibility for image generation, and further enhances the performance to 3.71 FID.
\end{itemize}


\input{figs/tex/abla-vae}
\noindent \textbf{Reconstruct images with different VQVAEs.} 
In Fig. \ref{fig:abla-vae}, we reconstruct multi-scale images by scaling the latent features in VQVAE tokenizers. Existing VQVAE tokenizers typically do not support scaling the latent features across a range of small to large scales. 
VAR's VQVAE \cite{var} uses a residual-based training recipe, directly applying it to non-residual image reconstruction does not yield the anticipated results (the 1$^{st}$ row).
The VQVAE tokenizer from Llamagen \cite{llamagen} shows excellent reconstruction performance only at the original latent space, indicating that it is not feasible for scale-wise autoregressive modeling (the 2$^{nd}$ row).





\noindent \textbf{Transfer FlexVAR to Mamba.} 
Recent work, AiM \cite{aim}, uses the Mamba architecture for token-wise autoregressive modeling. Inspired by this, we modify FlexVAR with Mamba to evaluate the performance (Tab. \ref{tab:ab-mamba}).
With similar model parameters, Mamba demonstrates competitive results compared to transformer models, indicating the GT prediction paradigm can effectively adapt to linear attention mechanisms like Mamba. However, considering that this Mamba architecture does not reflect the speed advantage, we do not integrate Mamba into our final version.

Mamba's inherent unidirectional attention mechanism prevents image tokens from achieving global attention within the same scale. To address this issue, we employ 8 scanning paths in different Mamba layers to capture global information. The specific Mamba architecture is detailed in the Appendix.

\input{table/abla-mamba}

\input{table/abla-posi}

\noindent \textbf{Position Embedding.} 
In Tab. \ref{tab:ab-posi}, we experiment with several types of step PE and x-y coordinate PE. To make the model robust to inference steps and enable it to generate images at any resolution, we remove the fixed-length step embedding (results in the second row), and the performance showed only slight changes. We adopt a non-parametric variant, similar to ViT \cite{vit}, which shows a 0.03 FID difference compared to the learnable variant.




\subsection{Analysis of the GT prediction paradigm}
\noindent \textbf{Convergence rate.}
We compared the training loss of VAR and our FlexVAR, as shown in Fig. \ref{fig:loss}. FlexVAR demonstrates significantly lower loss values and faster convergence, indicating that predicting ground-truth rather than residuals is more friendly for training.
This may be attributed to the residuals at different scales lacking semantic continuity, and this implicit prediction approach might limit the training convergence rate.

\input{figs/tex/loss}

\noindent \textbf{Generate images at any resolution.}
We show generated images at different resolutions using FlexVAR-$d24$ in Fig \ref{fig:abs} and Fig. \ref{fig:any-reso}. By controlling the inference steps, our FlexVAR can generate images at any resolution, despite being trained only on images with resolutions $\leq$ 256px. The generated images demonstrate strong semantic consistency across multiple scales, and the higher resolutions exhibit more detailed clarity. See the Appendix for more zero-shot high-resolution generation samples and step designs. 

\input{figs/tex/any-reso}

\noindent \textbf{Generate images at any ratio.}
We show generated samples with various aspect ratios in Fig. \ref{fig:abs} and Fig. \ref{fig:any-ratio}. By controlling the aspect ratio at each step of the inference process, our FlexVAR allows for generating images with various aspect ratios, demonstrating the flexibility and controllability of our FlexVAR.

\input{figs/tex/any-ratio}

\noindent \textbf{Generate images at any step.}
In Fig. \ref{fig:any-step}, we investigate the FID and IS for generating 256$\times$256 images from 6 to 16 steps with 3 different sizes (depth 16, 20, 24). As the number of steps increases, the quality of the generated images improves. The improvement is more significant in larger models (\textit{e.g.}, FlexVAR-$d24$), as larger transformers are thought able to learn more complex and fine-grained image distributions. 
During training, we use up to 10 steps to avoid OOM (out-of-memory) problem. Surprisingly, in the inference stage, using 13 steps results in a performance gain of -0.13 FID. This observation indicates that our FlexVAR is flexible with respect to inference steps, allowing for fewer steps to speed up image generation or more steps to achieve higher-quality images. The details of various step designs are provided in the Appendix.
\input{figs/tex/any-step}

\noindent \textbf{Refine image at high resolution.}
In Fig. \ref{fig:super_reso}, we input low-resolution images (e.g., 256$\times$256) and enable FlexVAR to output high-resolution refined images. Despite being trained only on $\leq$ 256px images, FlexVAR effectively refines image details by increasing the input image resolution, such as the eyes of the dogs in the example. This demonstrates the high flexibility of FlexVAR in image-to-image generation.
\input{figs/tex/super_reso}

\noindent \textbf{Image in-painting and out-painting.}
For in-painting and out-painting, we teacher-force ground-truth tokens outside the mask and let the model only generate tokens within the mask. Class label information is also injected. The results are visualized in Fig. \ref{fig:edit-paint}. Without modifications to the architecture design or training, FlexVAR achieves decent results on these image-to-image tasks.
\input{figs/tex/edit-paint}

\noindent \textbf{Image extension.}
For image extension, we generate images with an aspect ratio of 1:2 for the target class, with the ground-truth tokens forced to be in the center. FlexVAR shows decent results in image extension, indicating the strong generalization ability of our FlexVAR.
\input{figs/tex/edit-extent}


\noindent \textbf{Failure case.}
FlexVAR fails to generate images with a resolution 3$\times$ or more than the training resolution, as illustrated in Fig. \ref{fig:failure}. These cases typically feature noticeable wavy textures and blurry areas in the details. This failure is likely due to the overly homogeneous structure of the current training dataset. \textit{i.e.}, ImageNet-1K generally lacks multi-scale objects ranging from coarse to fine, leading to errors in generating details of high-resolution objects.

We hypothesize that training the model with a more complex dataset that includes images with fine-grained details, the model might become robust for higher resolutions.
\input{figs/tex/failure}
