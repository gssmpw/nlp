
\section{Methodology}
\label{sec:method}


\subsection{Scale-wise Autoregression (Preliminary)}
Scale-wise autoregressive models tokenize the input image into a sequence of multi-scale discrete image token maps $T=\{t_1, t_2, ..., t_n\}$, where $t_i$ is the token map with the resolution of $h_i\times w_i$ downsampling from $t_n \in \mathcal{R}^{h_n\times w_n}$. 
Each autoregressive step generates an entire token map, rather than a single token.
Compared to next-token-prediction, which contains one token at each step, $t_i$ contains $h_i\times w_i$ tokens and is able to maintain the 2D structure.

Previous approaches \cite{var, zhang2024var, code, ren2024m, li2024imagefolder, tang2024hart} typically follow a residual prediction paradigm. They only regress the ground-truth at the first scale ($g_1$), while at subsequent $i^{th}$ scale, the residual between the preceding scale ($g_{i-1}$) and the current scale ($g_{i}$) is predicted. We formulate these residuals ($\{r_i\}^{n}_{i=2}$) as :
\begin{equation}
    r_i =  g_i - \mathrm{Upsample^i}(g_{i-1}), 
\end{equation}
here $\mathrm{Upsample^i}$ represents upsample $g_{i-1}$ to the $i^{th}$ scale. The autoregressive likelihood is formulated as:
\begin{align}
    p(g_1, r_2,  \dots, r_n) = \prod_{i=1}^{n} p(r_i \mid g_1, r_2,  \dots, r_{i-1})
\end{align}
% \begin{align}
    % u_i = \sum_{j=1}^{i} \mathrm{Upsample^j}(r_{j})
% \end{align}
attention mechanisms (\textit{e.g.}, Transformer \cite{vaswani2017attention}) are utilized to instantiate this modeling.  
During the \(i^{th}\) autoregressive step, all preceding residuals are merged in the autoregressive model, which then predicts the probability distribution of \(r_i\). The \(h_i \times w_i\) tokens in \(r_i\) are generated in parallel, conditioned on all preceding units.
% , ranging from the first one to the \((i-1)^{th}\) one.
Thus, image token maps can be redefined as: $T=\{g_1, r_2, r_3 ..., r_n\}$. Finally, each image token map in $T$ is upsampled to $\mathcal{R}^{h_n\times w_n}$ and summarized for image generation.

% attention mechanisms (\textit{e.g.}, Transformer \cite{vaswani2017attention}, Mamba \cite{gu2023mamba}) are utilized to instantiate this modeling.
% During the $i^{th}$ autoregressive step, $h_i \times w_i$ tokens in $r_i$ are generated in parallel, conditioned on all preceding scales, ranging from the first one to the  $(i-1)^{th}$ one $\{g_1, r_2, \dots, r_{i-1}\}$.



\subsection{Overview of FlexVAR}
Our FlexVAR is a flexible visual autoregressive image generation paradigm that allows autoregressive learning with ground-truth prediction rather than residual, enabling to generate reasonable images at any step independently.
% The complete framework is shown in Fig.~\ref{fig:framework}.
Within our approach: (1) A scalable VQVAE tokenizer quantizes the input images into tokens at various scales and reconstructs images, as detailed in Sec. \ref{sec:vae}.
(2) A FlexVAR transformer is trained via scale-wise autoregressive modeling, with the removal of residuals, as detailed in Sec. \ref{sec:transformer}.

\subsection{Quantize \& reconstruct images at various scales}
\label{sec:vae}
Mainstream VQVAE tokenizers perform well at a single resolution. However, when scaling the latent space, they often fail to reconstruct images (as shown in Fig. \ref{fig:abla-vae}). This observation motivates us to explore a scalable tokenizer that quantizes input images into tokens at various scales and reconstructs images.
Specifically,  the proposed scalable tokenizer first encodes an image into multi-scale latent space, and then uses a quantizer to convert latent space features into discrete tokens, finally a decoder is used to reconstruct the original images from the discrete tokens at each scale. 


\noindent \textbf{Encoding.}
Given an input image $I \in \mathcal{R}^{H\times W}$, an autoencoder $\mathcal{E}(\cdot)$ \cite{esser2021taming} is used to convert $I$ into latent space $f$:  
\begin{align}
    f = \mathcal{E}(I), ~~ f \in \mathcal{R}^{C \times h\times w} 
\end{align}
here $h = \frac{H}{16},w = \frac{W}{16}$. We then downsample $f$ at $K$ random scales to obtain multi-scale latent features $\mathcal{F}=\{f_1, f_2,...,f_K\}$. $f_k$ represents represents the latent feature of the $k^{th}$  downsample from $f$. $f_K$ matches the original resolution of $f$.


\noindent \textbf{Quantizing.}
The quantizer $\mathcal{Q}(\cdot)$ includes a codebook $Z \in \mathcal{R}^{V\times C}$ containing $V$ learnable vectors.
The quantization process $q = \mathcal{Q}(f)$ is implemented by finding the Euclidean nearest code $q^{(k,i,j)}$ of each feature vector $f^{(k,i,j)}$ in multi-scale latent features $\mathcal{F}$:
\begin{align}
    q^{(k,i,j)} = \left( \mathrm{argmin}_{v \in [V]} \| \mathrm{Select}(Z, v) - f^{(k,i,j)} \|_2 \right) \in [V]
\end{align}
where $\mathrm{Select}(Z, v)$ denotes selecting the $v^{th}$ vector in codebook $Z$.
Based on $\mathcal{F}$, we extract all $q^{(k,i,j)}$ and minimize the distance between $q$ and $f$ to train the quantizer $\mathcal{Q}$.


\noindent \textbf{Decoding.}
The multi-scale images $\hat{\mathcal{I}}=\{\hat{I_1}, \hat{I_2},...,\hat{I_K}\}$ are reconstructed using the decoder $\mathcal{D}(\cdot)$ \cite{esser2021taming} given $q^{(k,i,j)}$.
We follow Llamagen \cite{llamagen} to adopt the same loss functions ($\mathcal{L}_{vae}$) to train $\{\mathcal{E}, \mathcal{Q}, \mathcal{D}\}$ at each scale without special design. Therefore, the final loss function can be formulated:
\begin{align}
    % \hat{\mathcal{I}} &= \mathcal{D}(\hat{f}), \\
    \mathcal{L} &= \sum_{k=1}^{K}\mathcal{L}_{vae} \left((I_k, \hat{I_k}), (f_k, q_k) \right)
\end{align}



\subsection{Visual autoregressive modeling without residual}
\label{sec:transformer}

We reconceptualize the next-scale-prediction progress from residual prediction to GT prediction. As illustrate in Fig. \ref{fig:intro-var} (b).
Here, each autoregressive step predicts the GT of current scale, rather than the residual.
We start by sampling $N$ multi-scale token maps $\{g_1, g_2, \dots, g_N\}$ from latent feature $f$, each at an increasingly higher resolution $h_n\times w_n$, culminating in $g_N$ matches the original feature map's resolution $\mathcal{R}^{C \times h\times w}$.
The autoregressive likelihood is reformulated as:
\begin{align}
    p(g_1, g_2, \dots, g_n) = \prod_{i=1}^{n} p(g_i \mid g_1, g_2, \dots, g_{n-1}).
\end{align}
During the $i^{th}$ autoregressive step,  $g_i \in \mathcal{R}^{h_i\times w_i}$ contains $h_k \times w_k$ tokens are generated in parallel, conditioned on all preceding scales  $\{g_1, g_2, \dots, g_{i-1}\}$.


\noindent \textbf{Scalable Position Embedding.}
VAR utilizes fix-length Position Embedding (PE) by adding learnable queries to each step and h-w coordinates. This requires both training and inference to follow a fixed number of steps and resolutions, which limits the flexibility of the autoregressive process.

In our FlexVAR, we design a 2D scalable PE ($\mathcal{P} \in \mathcal{R}^{d\times 2h\times 2w}$) adding to the h-w coordinates. It contains $2h\times 2w$ learnable queries with $d$ channels. At the i$^{th}$ step, $\mathcal{P}$ is upsampled/downsampled to match the scale of $g_i$. To ensure stability during linear interpolation across various scales, we set $\mathcal{P}$ to $2\times$ the size of the max latent space in traning. $\mathcal{P}$ is initialized using 2D sin-cosine PE \cite{vit} to ensure the 2D positional correlation. Additionally, we experimentally find that in our ground-truth prediction paradigm, incorporating PE for step embeddings is unnecessary (Tab. \ref{tab:ab-posi}). Therefore, we remove the step embeddings to ensure the flexibility of steps in autoregressive modeling.


\noindent \textbf{Step sampling.}
During training, we randomly sample the scale size in each step to enhance FlexVAR's capability to perceive any scale. Specifically, we set the maximum number of steps to 10, fixing the scale size of the first step to 1$\times$1 and the last step to 16$\times$16 (corresponding to 256$\times$256 input images), and randomly sampling the scale sizes for the intermediate steps. Each step is dropped with a 5\% probability, with a maximum of 4 steps being dropped. Thus, the number of steps during training is from 6 to 10.

During inference, we use a default of 10 steps: \{1, 2, 3, 4, 5, 6, 8, 10, 13, 16\} (same as VAR). Our experimental results show more steps yield better performance (Fig. \ref{fig:any-step}).
