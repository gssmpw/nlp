\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Inference steps}
\label{sec:supp-steps}
In Tab. \ref{tab:supp-steps}, we list the scales corresponding to different inference steps. The scales in each step are not fixed and can be flexibly adjusted during inference.
Note that during training, we only limit the maximum number of steps to 10 and randomly sample the scale for each step, so the scales during the training process do not follow Tab. \ref{tab:supp-steps}
\input{table/supp-steps}


\section{Extent visual autoregressive modeling with Mamba}
\label{sec:supp-mamba}

Unlike attention mechanisms that utilize explicit query-key-value (QKV) interactions to integrate context, Mamba faces
challenges in handling bi-directional interaction. Therefore, prior Mamba-based visual autoregressive work \cite{ren2024m} only used Mamba to model the unidirectional relationship between scales, relying on additional Transformer layers to process tokens within one scale.

In this work, we adopt a composition-recomposition strategy to obtain global information in Mamba network. Specifically, we utilize a Zigzag scanning strategy \cite{hu2024zigma} over the spatial dimension. We alternate between eight distinct scanning paths across different Mamba layers (as shown in Fig. \ref{fig:mamba}), which include:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
\item (a) top-left to the bottom-right.
\item (b) top-left to the bottom-right.
\item (c) bottom-left to the top-right.
\item (d) bottom-left to the top-right.
\item (e) bottom-right to the top-left.
\item (f) bottom-right to the top-left.
\item (g) top-right to the bottom-left.
\item (h) top-right to the bottom-left.
% \item (a) top-left to the bottom-right, following a “downward first, then rightward” direction.
% \item (b) top-left to the bottom-right, following a “downward right, then downward” direction.
% \item (c) bottom-left to the top-right, following a “upward first, then rightward” direction.
% \item (d) bottom-left to the top-right, following a “rightward first, then upward” direction.
% \item (e) bottom-right to the top-left, following a “upward first, then leftward” direction.
% \item (f) bottom-right to the top-left, following a “leftward first, then upward” direction.
% \item (g) top-right to the bottom-left, following a “downward first, then leftward” direction.
% \item (h) top-right to the bottom-left, following a “leftward first, then downward” direction.
\end{itemize}

\input{figs/tex/supp-mamba}

\section{Qualitative results with different steps.}
In Fig. \ref{fig:supp-step}, we show some generated samples with \{6, 8, 10, 12\} steps. Our FlexVAR uses up to 10 steps for autoregressive modeling during training to avoid OOM (out-of-memory), while it can naturally transfer to any number of steps during inference. The samples generated with different steps are highly similar, differing only in some details. Generally, more steps result in better image details.

\section{Qualitative results with various resolutions.}
Fig. \ref{fig:supp-reso} shows some generated samples with \{256, 384, 512\} resolutions. FlexVAR uses up to 256$\times$256 resolution images for training, it can generate images with higher resolutions such as 384 and 512. The generated images demonstrate strong semantic consistency across multiple scales, and the higher resolutions display more detailed clarity.


\section{Qualitative results with different VQVAE tokenizers.}
\label{sec:vis-vae}
\noindent \textbf{Image reconstruction.}
We compare more image reconstruction results in Fig. \ref{fig:supp-vae1}. First, we encode the image into the latent space and performe multi-scale downsampling, then reconstruct the original image through the VQVAE decoder. It is evident that only our scalable VQVAE can perform image reconstruction at various scales.



\noindent \textbf{Generate images with GT prediction.}
We visualize the generated samples with VQVAE tokenizers from VAR, Llamagen, and ours, corresponding to the 2$^{nd}$, 3$^{rd}$ and 5$^{th}$ results in Tab. \ref{tab:ab-comp}. As shown in Fig. \ref{fig:supp-vae2}, the VAR tokenizer, trained with a residual paradigm, fails to generate images under GT prediction; the generation samples of Llamagen's tokenizer are not up to the mark, due to its discrete tokens at intermediate steps being suboptimal.

\section{Addtional Visual Results.}
We show more generated samples in Fig. \ref{fig:supp-vis1}.
\input{figs/tex/supp-step}
\input{figs/tex/supp-vae1}
\input{figs/tex/supp-reso}
\input{figs/tex/supp-vae2}
\input{figs/tex/supp-vis1}
\clearpage
