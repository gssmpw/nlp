
\def\thefootnote{\arabic{footnote}}
\setcounter{footnote}{0}

\section{Introduction}
\label{sec:intro}


Autoregressive (AR) models aim to learn the probability distribution of the next token, offering great flexibility by generating tokens of any length. This design brings significant advancements in the field of Natural Language Processing (NLP), demonstrating satisfactory generality and transferability \cite{gpt3, chatgpt, gpt4}.
Concurrently, the computer vision field has been striving to develop large autoregressive models \cite{lu2022unified, lu2024unified, bai2024sequential, team2024chameleon, luo2024open, ren2025videoworld}. These models employ visual tokenizers to discretize images into a series of 1D tokens \cite{razavi2019generating, lee2022autoregressive, yu2021vector, zheng2022movq, llamagen} or 2D scales \cite{var, zhang2024var, tang2024hart, ren2024m, li2024imagefolder} and then utilize AR to model the next unit.
However, these image autoregressive models typically output images at a single resolution, \textbf{the flexibility of AR has not yet been realized.}


Recently, in image generation, VAR \cite{var} has pioneered scale-wise autoregressive modeling, completing image autoregression based on 2D sequences. This approach predicts the next scale rather than the next token, thereby preserving the 2D structure of images and mitigating the issue of limited receptive fields in 1D causal transformers.
Specifically, VAR predicts the ground-truth (GT)\footnote{To avoid confusion, we use ground-truth (GT) to represent image latent feature, and residual to represent residual latent feature.} of the smallest scale in the first step. Subsequently, at each step, it predicts the residuals of the current scale and the prior one. Finally, the outputs of each scale are upsampled to a uniform size and undergo weighted summation to generate the final output, as illustrated in Fig. \ref{fig:intro-var}(a). Successors \cite{zhang2024var, tang2024hart, ren2024m, li2024imagefolder} have all adopted the residual design, assuming it to be effective.
Although this technique achieves commendable performance, it encounters a primary challenge: The residual prediction relies on a rigid step design, restricting the flexibility to generate images with varying resolutions and aspect ratios, thus limiting the adaptability and flexibility of image generation.
Meanwhile, residuals at different scales often lack semantic continuity, and this implicit prediction approach may limit the model's capacity to represent diverse image variations.


In this work, we examine the necessity of residual prediction
in visual autoregressive modeling. Our intuition is that, 
in scale-wise autoregressive modeling, the ground-truth value of the current scale can be reliably estimated from the prior series of scales, rendering residual prediction (\textit{i.e.}, predicting the bias between the current scale and the preceding one) unnecessary.
Notably, predicting GT ensures semantic coherence between adjacent scales, making it more conducive for modeling the probability distribution of the scale. Additionally, this structure can output reasonable results at any step, breaking the rigid step design of the residual prediction and endowing autoregressive modeling with great flexibility.

% However, \textit{directly transferring the residual prediction to ground-truth prediction is not feasible}: 
% Existing VQVAE tokenizers usually do not support scaling the latent features in VQVAE to a range of small to large scales, nor do they provide reliable prior information for visual autoregressive modeling.
% % existing VQVAE tokenizers (e.g., \cite{llamagen, aim}) usually do not support image reconstruction at arbitrary scales and typically exhibit good reconstruction quality only at one single scale. 
% In Fig. \ref{fig:intro-vae}, we follow the next-scale-prediction paradigm to reconstruct multi-scale images by scaling the latent features in VQVAE tokenizers. The VQVAE tokenizer from Llamagen \cite{llamagen} only shows excellent reconstruction performance at one single resolution. When scaling in the latent features, the reconstruction quality significantly degrades, indicating that it is not feasible to use existing VQVAE tokenizers for visual autoregressive modeling.

\input{figs/tex/intro}


Motivated by this, we systematically design the paradigm of visual autoregressive modeling without residual prediction, referred to as FlexVAR. Within FlexVAR, the ground-truth is predicted at each step instead of the residuals. 
Specifically, we design a scalable VQVAE tokenizer with multi-scale constraints, enhancing the VQVAE's robustness to various latent scales and thereby enabling image reconstruction at arbitrary resolutions. Then, the FlexVAR Transformer learns the probability distribution of a series of multi-scale latent features, modeling the ground-truth of the next scale, as shown in Fig. \ref{fig:intro-var}(b).
Additionally, we propose scalable 2D positional embeddings, which incorporate 2D learnable queries initialized with 2D sin-cosine weights. This approach enables the scale-wise autoregressive modeling to be extended to various resolutions/steps, including those beyond the resolutions/steps used during training, as shown in Fig. \ref{fig:abs}. 


In a nutshell, this non-residual modeling approach ensures continuous semantic representation between adjacent scales. Simultaneously, it avoids the rigid step design inherent in residual prediction, significantly expanding the flexibility of image generation. FlexVAR can (1) generate images of various resolutions and aspect ratios, even exceeding the training resolutions; (2) support image-to-image tasks such as in/out-painting, image refinement, and image expansion without the need for fine-tuning; (3) enjoy flexible inference steps, allowing for accelerated inference with fewer steps or improved image quality with more steps.
% refer fig1


% vae trade-off
