% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}


% add my package
\usepackage{tabularx}
\usepackage[export]{adjustbox}
\usepackage{multirow,multicol}
\usepackage{colortbl}
\usepackage{makecell}
\definecolor{mygray}{gray}{0.6}
\newcommand{\pub}[1]{\color{mygray}{\scriptsize{[{#1}]}}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{4615} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{
CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting
}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}


\begin{document}
% \maketitle



\clearpage
\setcounter{page}{1}
\maketitlesupplementary

In the supplementary material, we first clarify the citation format error of the main manuscript in Sec. \ref{sec:cite}, and then introduce the implementation details in Sec. \ref{sec:imple}. Next, sampled 3D shapes are shown in Sec. \ref{sec:3dshapes}, including both the excluded and retained 3D shapes. Moreover, we provide additional retrieval results in Sec. \ref{sec:supp-retri}.


\section{Citation Correction Notice}
\label{sec:cite}

Due to a formatting oversight, the citation of Uni3D [\textcolor{cvprblue}{55}] in the main manuscript is linked to the incorrect paper. The table below provides a mapping between the incorrect citations and the correct ones.

% \noindent
% \begin{table}{\columnwidth}{|l|l|}
%   \hline
%        Incorrect Citation & [\textcolor{cvprblue}{55}] \\ 
%       \hline
%       Incorrect Reference & Zhang et al., ``Uni3d: A unified baseline for multi-dataset 3d object detection'', CVPR 23  \\     
%       Correct Reference & Zhou et al., ``Uni3d: Exploring unified 3d representation at scale'', ICLR 24\\  
%   \hline
% \end{table}

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!} {
\begin{tabular}{|l|l|}
  \hline
  Incorrect Citation & [\textcolor{cvprblue}{55}] in the main manuscript\\ 
  \hline
  \multirow{2}{*}{Incorrect Reference} & Zhang et al., ``Uni3d: A unified baseline \\ 
  & for multi-dataset 3D object detection'', CVPR 23 \\     
  \hline
  \multirow{2}{*}{Correct Reference} & Zhou et al., ``Uni3d: Exploring unified \\
  & 3D representation at scale'', ICLR 24 \\  
  \hline
\end{tabular}
}
\caption{Corrections for citation and reference details.}
\label{tab:corrections}
\end{table}


\noindent \textbf{Note to Readers:}
We apologize for any confusion caused by the errors in citation numbers in the main manuscript. This section provides the correct reference. We commit to correcting the citation numbers in the revised version.


\section{Implementation details}
\label{sec:imple}
We employ EVA02-E-14-plus from OpenCLIP \cite{ilharco2openclip}.  We sample 10,000 points from the 3DGS. During training, we freeze the vision and text encoders in EVA-CLIP-T, only leaving CLIP-GS as the learnable component. The model is optimized with the Adam \cite{kingma2014adam} optimizer with a weight-decay of 0.05. The learning rate is set to $5 \times 10^{-4}$ for GS-Tokenizer, and $1 \times 10^{-4}$ for other modules. The model is trained for 5 epochs on the triplets with 8 NVIDIA A6000 GPUs. \textit{e.g.}, CLIP-GS-B with $\sim$ 9 million parameters, converges in approximately 14 hours with batch size = 32 on each A6000 GPU.


\section{Excluded and Retained 3D shapes}
\label{sec:3dshapes}
We filter the 3D shapes in Objaverse \cite{objaverse, objaversexl} and select those with a diversity of colors and textures. In this section, we provide a visual comparison of the retained and excluded 3D shapes, as shown in Fig. \ref{fig:supp-vis1}. We excluded monochromatic, meaningless 3D shapes. \textit{e.g.}, items like the Christmas tree, car, and bucket in the first row of Fig. \ref{fig:supp-vis1} only contain the contours of the 3D objects, with colors that are single-toned and lack texture information. The retained 3D shapes have colors with intricate texture details, which are difficult for point clouds to depict.
\input{figs/tex/supp-vis1}

\section{Retrieval Results}
\label{sec:supp-retri}


\input{figs/tex/supp-vis2}
In Fig. \ref{fig:supp-vis2}, we showcase how CLIP-GS successfully retrieves 3D shapes from text or real-world images. We retrieve the most similar or the Top 2 / Top 3 similar 3D shapes according to the corresponding images or text. CLIP-GS performs well when retrieving real-world images (Fig. \ref{fig:supp-vis2} top). CLIP-GS has learned the encoding of 3DGS and can align the features of 3DGS well with the image spaces, allowing it to retrieve the most suitable 3D shapes based on the input of one or two images. Moreover, the results indicate that CLIP-GS retrieves reasonable 3D shapes based on text in the query set (Fig. \ref{fig:supp-vis2} bottom). This retrieval is not limited by category, and CLIP-GS can retrieve reasonable expressions (\textit{e.g.}, smiling), textures (\textit{e.g.}, antique), and other information that is easily lost in point cloud representations.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
