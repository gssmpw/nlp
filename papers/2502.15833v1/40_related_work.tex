\section{Related Work}

This section reviews recent advancements in OOD detection, provides an overview of the latest innovations to enhance KAN performance, and explores the diverse sectors where KANs have demonstrated successful applications.

\subsection{Out-Of-Distribution detection}
OOD detection focuses on identifying instances with semantic shifts, a special case of distributional shift.  
OOD detection methods can be broadly classified into the following categories \citep{yang2024generalizedoutofdistributiondetectionsurvey}. 
\textbf{Classification-based methods} use the output of classification models, such as softmax scores, to distinguish between InD and OOD samples. Examples include Maximum Softmax Probability (MSP) \citep{hendrycks2017a}, which uses the softmax score of the predicted class as a confidence score, and ODIN \citep{liang2018enhancing}, which applies temperature scaling and input perturbations to enhance the separability of InD and OOD samples. 
More recent methods that fall in this category are SCALE \citep{xu2024scaling}, ASH \citep{djurisic2023extremely}, VIM \citep{haoqi2022vim}, and KNN \citep{sun2022knnood}. 
Gradient-based methods also belong to this category. 
Examples include GradNorm \citep{huang2021importance} and NAC \citep{liu2024neuron}, which use gradients calculated from the KL divergence between the model's output and a uniform probability distribution.
\textbf{Density-based methods} model the probability distribution of the training data to identify deviations. 
This is often achieved using a Gaussian mixture model \citep{zong2018deep} or normalizing flows \citep{zisselman2020deepresidualflowdistribution, jiang2022revisiting}.
\textbf{Reconstruction-based methods} typically use autoencoders to reconstruct input samples and measure the reconstruction error as a signal for OOD detection \citep{jiang2023readaggregatingreconstructionerror, 9878470}.
\textbf{Distance-based methods} rely on distance metrics in the feature space to identify OOD samples. 
The Mahalanobis distance-based detector \citep{NEURIPS2018_abdeb6f5} first models the feature distribution with a class-conditional Gaussian distribution and then it derives the InD score using the Mahalanobis distance between the InD centroids and the input sample. 
fDBD \citep{liu2024fast} measures the distance between the latent feature of the sample and the class decision boundaries. 
Our method also falls into this category, as it computes the InD score by measuring the distance between the network's regions activated during training (InD regions) and those activated by the test sample.


\subsection{Kolmogorov-Arnold Networks}
\label{sec:kan_rel_works}

The recently introduced KAN \citep{hou2024comprehensivesurveykolmogorovarnold} represents a significant advancement in neural network architectures, offering a potential alternative to traditional MLPs by not only enhancing accuracy but also leading to more interpretable models.
As a result, numerous studies have tried to innovate and refine KANs further. 
For example, many articles replace the spline architecture with more efficient or accurate alternatives such as Chebyshev polynomials \citep{ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks}, wavelet-based structures \citep{bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks}, sinusoidal functions \citep{reinhardt2024sinekankolmogorovarnoldnetworksusing}, and radial basis functions \citep{li2024kolmogorovarnoldnetworksradialbasis}. 
Others try to replicate advanced neural network architectures using KAN's characteristics. 
This includes convolutional neural networks \citep{bodner2024convolutionalkolmogorovarnoldnetworks} and graph neural networks \citep{kiamari2024gkangraphkolmogorovarnoldnetworks, bresson2024kagnnskolmogorovarnoldnetworksmeet, zhang2024graphkanenhancingfeatureextraction}, further demonstrating the versatility and potential of KANs. 
Applications of KANs have rapidly expanded across various domains, including time series analysis \citep{vacarubio2024kolmogorovarnoldnetworkskanstime, xu2024kolmogorovarnoldnetworkstimeseries}, solving ordinary and partial differential equations \citep{koenig2024kanodeskolmogorovarnoldnetworkordinary, wang2024kolmogorovarnoldinformedneural}, hyperspectral image classification \citep{seydi2024unveilingpowerwaveletswaveletbased, jamali2024learnmoreexploringkolmogorovarnold}, and computer vision \citep{azam2024suitabilitykanscomputervision, li2024ukanmakesstrongbackbone, cheon2024kolmogorovarnoldnetworksatelliteimage}. 
Additionally, KANs have recently been applied to fields similar to OOD detection, such as abnormality detection \citep{Huang2024.06.04.24308428} and AI-generated image detection \citep{anon2024detectingundetectablecombiningkolmogorovarnold}. 
These studies leverage the superior accuracy and interpretability of KANs \citep{liu2024kankolmogorovarnoldnetworks} to uncover more complex patterns in the data. 
While their work focuses on developing robust models that demonstrate KANs' capacity to generalize effectively to unseen samples, they do not address the detection of these samples. 
In contrast, we present a novel OOD detection method that leverages the unique local plasticity property of KANs, applicable to any backbone architecture.


