@inproceedings{
liu2024neuron,
title={Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization},
author={Yibing Liu and Chris XING TIAN and Haoliang Li and Lei Ma and Shiqi Wang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=SNGXbZtK6Q}
}

@misc{
liu2024kankolmogorovarnoldnetworks,
title={KAN: Kolmogorov-Arnold Networks}, 
author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
year={2024},
eprint={2404.19756},
archivePrefix={arXiv},
primaryClass={cs.LG},
url={https://arxiv.org/abs/2404.19756}, 
}

@article{zhang2023openood,
  title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
  author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:2306.09301},
  year={2023}
}

@inproceedings{NEURIPS2022_d201587e,
 author = {Yang, Jingkang and Wang, Pengyun and Zou, Dejian and Zhou, Zitang and Ding, Kunyuan and PENG, WENXUAN and Wang, Haoqi and Chen, Guangyao and Li, Bo and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Hendrycks, Dan and Li, Yixuan and Liu, Ziwei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {32598--32611},
 publisher = {Curran Associates, Inc.},
 title = {OpenOOD: Benchmarking Generalized Out-of-Distribution Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/d201587e3a84fc4761eadc743e9b3f35-Paper-Datasets_and_Benchmarks.pdf},
 volume = {35},
 year = {2022}
}

@ARTICLE{6296535,
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine}, 
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]}, 
  year={2012},
  volume={29},
  number={6},
  pages={141-142},
  keywords={Machine learning},
  doi={10.1109/MSP.2012.2211477}}

@inproceedings{37648,title	= {Reading Digits in Natural Images with Unsupervised Feature Learning},author	= {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},year	= {2011},URL	= {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},booktitle	= {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}}

@InProceedings{Cimpoi_2014_CVPR,
author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
title = {Describing Textures in the Wild},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@ARTICLE{7968387,
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Places: A 10 Million Image Database for Scene Recognition}, 
  year={2018},
  volume={40},
  number={6},
  pages={1452-1464},
  keywords={Object recognition;Deep learning;Image recognition;Leearning (artificial intelligence);Image classification;Image analysis;Scene classification;visual recognition;deep learning;deep feature;image dataset},
  doi={10.1109/TPAMI.2017.2723009}}


@article{
azizmalayeri2023unmasking,
title={Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data},
author={Mohammad Azizmalayeri and Ameen Abu-Hanna and Giovanni Cina},
journal={arXiv preprint arXiv:2309.16220},
year={2023},
}

@inproceedings{
gorishniy2021revisiting,
title={Revisiting Deep Learning Models for Tabular Data},
author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=i_Q1yrOegLY}
}

@Article{Pollard2018,
author={Pollard, Tom J.
and Johnson, Alistair E. W.
and Raffa, Jesse D.
and Celi, Leo A.
and Mark, Roger G.
and Badawi, Omar},
title={The eICU Collaborative Research Database, a freely available multi-center database for critical care research},
journal={Scientific Data},
year={2018},
month={Sep},
day={11},
volume={5},
number={1},
pages={180178},
abstract={Critical care patients are monitored closely through the course of their illness. As a result of this monitoring, large amounts of data are routinely collected for these patients. Philips Healthcare has developed a telehealth system, the eICU Program, which leverages these data to support management of critically ill patients. Here we describe the eICU Collaborative Research Database, a multi-center intensive care unit (ICU)database with high granularity data for over 200,000 admissions to ICUs monitored by eICU Programs across the United States. The database is deidentified, and includes vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, treatment information, and more. Data are publicly available after registration, including completion of a training course in research with human subjects and signing of a data use agreement mandating responsible handling of the data and adhering to the principle of collaborative research. The freely available nature of the data will support a number of applications including the development of machine learning algorithms, decision support tools, and clinical research.},
issn={2052-4463},
doi={10.1038/sdata.2018.178},
url={https://doi.org/10.1038/sdata.2018.178}
}

@inproceedings{NIPS2011_b571ecea,
 author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generalizing from Several Related Classification Tasks to a New Unlabeled Sample},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b571ecea16a9824023ee1af16897a582-Paper.pdf},
 volume = {24},
 year = {2011}
}

@misc{vacarubio2024kolmogorovarnoldnetworkskanstime,
      title={Kolmogorov-Arnold Networks (KANs) for Time Series Analysis}, 
      author={Cristian J. Vaca-Rubio and Luis Blanco and Roberto Pereira and Màrius Caus},
      year={2024},
      eprint={2405.08790},
      archivePrefix={arXiv},
      primaryClass={eess.SP},
      url={https://arxiv.org/abs/2405.08790}, 
}

@misc{xu2024kolmogorovarnoldnetworkstimeseries,
      title={Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability}, 
      author={Kunpeng Xu and Lifei Chen and Shengrui Wang},
      year={2024},
      eprint={2406.02496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02496}, 
}

@misc{koenig2024kanodeskolmogorovarnoldnetworkordinary,
      title={KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for Learning Dynamical Systems and Hidden Physics}, 
      author={Benjamin C. Koenig and Suyong Kim and Sili Deng},
      year={2024},
      eprint={2407.04192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04192}, 
}

@misc{wang2024kolmogorovarnoldinformedneural,
      title={Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving forward and inverse problems based on Kolmogorov Arnold Networks}, 
      author={Yizheng Wang and Jia Sun and Jinshuai Bai and Cosmin Anitescu and Mohammad Sadegh Eshaghi and Xiaoying Zhuang and Timon Rabczuk and Yinghua Liu},
      year={2024},
      eprint={2406.11045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11045}, 
}

@misc{seydi2024unveilingpowerwaveletswaveletbased,
      title={Unveiling the Power of Wavelets: A Wavelet-based Kolmogorov-Arnold Network for Hyperspectral Image Classification}, 
      author={Seyd Teymoor Seydi},
      year={2024},
      eprint={2406.07869},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.07869}, 
}

@misc{jamali2024learnmoreexploringkolmogorovarnold,
      title={How to Learn More? Exploring Kolmogorov-Arnold Networks for Hyperspectral Image Classification}, 
      author={Ali Jamali and Swalpa Kumar Roy and Danfeng Hong and Bing Lu and Pedram Ghamisi},
      year={2024},
      eprint={2406.15719},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.15719}, 
}

@misc{azam2024suitabilitykanscomputervision,
      title={Suitability of KANs for Computer Vision: A preliminary investigation}, 
      author={Basim Azam and Naveed Akhtar},
      year={2024},
      eprint={2406.09087},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09087}, 
}

@misc{li2024ukanmakesstrongbackbone,
      title={U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation}, 
      author={Chenxin Li and Xinyu Liu and Wuyang Li and Cheng Wang and Hengyu Liu and Yifan Liu and Zhen Chen and Yixuan Yuan},
      year={2024},
      eprint={2406.02918},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2406.02918}, 
}

@article {Huang2024.06.04.24308428,
	author = {Huang, Zhaojing and Cui, Jiashuo and Yu, Leping and Herbozo Contreras, Luis Fernando and Kavehei, Omid},
	title = {Abnormality Detection in Time-Series Bio-Signals using Kolmogorov-Arnold Networks for Resource-Constrained Devices},
	elocation-id = {2024.06.04.24308428},
	year = {2024},
	doi = {10.1101/2024.06.04.24308428},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {This study uses Kolmogorov-Arnold Networks (KANs) to analyze electrocardiogram (ECG) signals in order to detect cardiac abnormalities. These novel networks have demonstrated potential for application in biosignal analysis, particularly ECG, due to their flexibility and smaller parameter requirements, making them candidates for wearable devices. The network structure comprises a simple KAN model with a single hidden layer of 64 neurons. It was trained on the Telehealth Network of Minas Gerais (TNMG) dataset and tested for generalization on the Chinese Physiological Signal Challenge 2018 (CPSC) dataset. The KAN model delivered reasonably promising results, achieving an F1-score of 0.75 and an AUROC of 0.95 on the TNMG dataset. During the out-of-sample generalization test on the CPSC dataset, it achieved an F1-score of 0.62 and an AUROC of 0.84. It has also shown resistance to missing data channels by maintaining a reasonable performance, down to only a single lead left of ECG data instead of the initial 12 leads. Compared with traditional Multi-Layer Perceptrons (MLP) and Neural Circuit Policy (NCP, aka. Liquid Time Constant Networks), KANs exhibit superior flexibility, adaptability, interpretability, and efficiency. Their compact size and reduced computational requirements make them potential candidates for deployment on hardware, particularly in personalized medical devices.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThe authors declare no conflicts of interest.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesI confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesThis paper uses the Chinese Physiological Signal Challenge 2018 (CPSC) dataset for analysis and experiments, which is openly available. The Telehealth Network of Minas Gerais (TNMG) dataset is not openly accessible. Access and permission to this data should be discussed with and granted by the custodian of the data.},
	URL = {https://www.medrxiv.org/content/early/2024/06/04/2024.06.04.24308428},
	eprint = {https://www.medrxiv.org/content/early/2024/06/04/2024.06.04.24308428.full.pdf},
	journal = {medRxiv}
}

@misc{cheon2024kolmogorovarnoldnetworksatelliteimage,
      title={Kolmogorov-Arnold Network for Satellite Image Classification in Remote Sensing}, 
      author={Minjong Cheon},
      year={2024},
      eprint={2406.00600},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.00600}, 
}

@misc{ss2024chebyshevpolynomialbasedkolmogorovarnoldnetworks,
      title={Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation}, 
      author={Sidharth SS and Keerthana AR and Gokul R and Anas KP},
      year={2024},
      eprint={2405.07200},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.07200}, 
}

@misc{bozorgasl2024wavkanwaveletkolmogorovarnoldnetworks,
      title={Wav-KAN: Wavelet Kolmogorov-Arnold Networks}, 
      author={Zavareh Bozorgasl and Hao Chen},
      year={2024},
      eprint={2405.12832},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12832}, 
}

@misc{bodner2024convolutionalkolmogorovarnoldnetworks,
      title={Convolutional Kolmogorov-Arnold Networks}, 
      author={Alexander Dylan Bodner and Antonio Santiago Tepsich and Jack Natan Spolski and Santiago Pourteau},
      year={2024},
      eprint={2406.13155},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.13155}, 
}

@misc{xu2024fourierkangcffourierkolmogorovarnoldnetwork,
      title={FourierKAN-GCF: Fourier Kolmogorov-Arnold Network -- An Effective and Efficient Feature Transformation for Graph Collaborative Filtering}, 
      author={Jinfeng Xu and Zheyu Chen and Jinze Li and Shuo Yang and Wei Wang and Xiping Hu and Edith C. -H. Ngai},
      year={2024},
      eprint={2406.01034},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2406.01034}, 
}

@misc{li2024kolmogorovarnoldnetworksradialbasis,
      title={Kolmogorov-Arnold Networks are Radial Basis Function Networks}, 
      author={Ziyao Li},
      year={2024},
      eprint={2405.06721},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.06721}, 
}

@misc{kiamari2024gkangraphkolmogorovarnoldnetworks,
      title={GKAN: Graph Kolmogorov-Arnold Networks}, 
      author={Mehrdad Kiamari and Mohammad Kiamari and Bhaskar Krishnamachari},
      year={2024},
      eprint={2406.06470},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.06470}, 
}

@misc{bresson2024kagnnskolmogorovarnoldnetworksmeet,
      title={KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning}, 
      author={Roman Bresson and Giannis Nikolentzos and George Panagopoulos and Michail Chatzianastasis and Jun Pang and Michalis Vazirgiannis},
      year={2024},
      eprint={2406.18380},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.18380}, 
}

@misc{zhang2024graphkanenhancingfeatureextraction,
      title={GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov Arnold Networks}, 
      author={Fan Zhang and Xin Zhang},
      year={2024},
      eprint={2406.13597},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.13597}, 
}

@misc{hou2024comprehensivesurveykolmogorovarnold,
      title={A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)}, 
      author={Yuntian Hou and Di Zhang},
      year={2024},
      eprint={2407.11075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.11075}, 
}

@misc{reinhardt2024sinekankolmogorovarnoldnetworksusing,
      title={SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation Functions}, 
      author={Eric A. F. Reinhardt and P. R. Dinesh and Sergei Gleyzer},
      year={2024},
      eprint={2407.04149},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04149}, 
}

@misc{anon2024detectingundetectablecombiningkolmogorovarnold,
      title={Detecting the Undetectable: Combining Kolmogorov-Arnold Networks and MLP for AI-Generated Image Detection}, 
      author={Taharim Rahman Anon and Jakaria Islam Emon},
      year={2024},
      eprint={2408.09371},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.09371}, 
}

@inproceedings{
hendrycks2017a,
title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
author={Dan Hendrycks and Kevin Gimpel},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Hkg4TI9xl}
}

@inproceedings{
liang2018enhancing,
title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
author={Shiyu Liang and Yixuan Li and R. Srikant},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=H1VGkIxRZ},
}

@inproceedings{
xu2024scaling,
title={Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement},
author={Kai Xu and Rongyu Chen and Gianni Franchi and Angela Yao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=RDSTjtnqCg}
}

@inproceedings{
djurisic2023extremely,
title={Extremely Simple Activation Shaping for Out-of-Distribution Detection},
author={Andrija Djurisic and Nebojsa Bozanic and Arjun Ashok and Rosanne Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ndYXTEL6cZz}
}

@inproceedings{haoqi2022vim,
title = {ViM: Out-Of-Distribution with Virtual-logit Matching},
author = {Wang, Haoqi and Li, Zhizhong and Feng, Litong and Zhang, Wayne},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year = {2022}
}

@article{sun2022knnood,
  title={Out-of-distribution Detection with Deep Nearest Neighbors},
  author={Sun, Yiyou and Ming, Yifei and Zhu, Xiaojin and Li, Yixuan},
  journal={ICML},
  year={2022}
}

@inproceedings{sun2022dice,
  title={DICE: Leveraging Sparsification for Out-of-Distribution Detection},
  author={Sun, Yiyou and Li, Yixuan},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@inproceedings{NEURIPS2018_abdeb6f5,
 author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{liu2024fast,
  title={Fast Decision Boundary based Out-of-Distribution Detector},
  author={Liu, Litian and Qin, Yao},
  journal={ICML},
  year={2024}
}

@article{liu2020energy,
  title={Energy-based Out-of-distribution Detection},
  author={Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
} 

@misc{yang2024generalizedoutofdistributiondetectionsurvey,
      title={Generalized Out-of-Distribution Detection: A Survey}, 
      author={Jingkang Yang and Kaiyang Zhou and Yixuan Li and Ziwei Liu},
      year={2024},
      eprint={2110.11334},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.11334}, 
}

@inproceedings{huang2021importance,
  title={On the Importance of Gradients for Detecting Distributional Shifts in the Wild},
  author={Huang, Rui and Geng, Andrew and Li, Yixuan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{
zong2018deep,
title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
author={Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and Cristian Lumezanu and Daeki Cho and Haifeng Chen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJJLHbb0-},
}

@misc{zisselman2020deepresidualflowdistribution,
      title={Deep Residual Flow for Out of Distribution Detection}, 
      author={Ev Zisselman and Aviv Tamar},
      year={2020},
      eprint={2001.05419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.05419}, 
}

@inproceedings{
jiang2022revisiting,
title={Revisiting flow generative models for Out-of-distribution detection},
author={Dihong Jiang and Sun Sun and Yaoliang Yu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=6y2KBh-0Fd9}
}

@misc{jiang2023readaggregatingreconstructionerror,
      title={READ: Aggregating Reconstruction Error into Out-of-distribution Detection}, 
      author={Wenyu Jiang and Yuxin Ge and Hao Cheng and Mingcai Chen and Shuai Feng and Chongjun Wang},
      year={2023},
      eprint={2206.07459},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.07459}, 
}

@INPROCEEDINGS{9878470,
  author={Zhou, Yibo},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection}, 
  year={2022},
  volume={},
  number={},
  pages={7369-7377},
  keywords={Measurement;Computer vision;Uncertainty;Semantics;Pipelines;Training data;Detectors;Others; Recognition: detection;categorization;retrieval; Self-& semi-& meta- & unsupervised learning},
  doi={10.1109/CVPR52688.2022.00723}}

@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@article{cifar10dataset,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@article{cifar100dataset,
title= {CIFAR-100 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).
Here is the list of classes in the CIFAR-100:

Superclass	Classes
aquatic mammals	beaver, dolphin, otter, seal, whale
fish	aquarium fish, flatfish, ray, shark, trout
flowers	orchids, poppies, roses, sunflowers, tulips
food containers	bottles, bowls, cans, cups, plates
fruit and vegetables	apples, mushrooms, oranges, pears, sweet peppers
household electrical devices	clock, computer keyboard, lamp, telephone, television
household furniture	bed, chair, couch, table, wardrobe
insects	bee, beetle, butterfly, caterpillar, cockroach
large carnivores	bear, leopard, lion, tiger, wolf
large man-made outdoor things	bridge, castle, house, road, skyscraper
large natural outdoor scenes	cloud, forest, mountain, plain, sea
large omnivores and herbivores	camel, cattle, chimpanzee, elephant, kangaroo
medium-sized mammals	fox, porcupine, possum, raccoon, skunk
non-insect invertebrates	crab, lobster, snail, spider, worm
people	baby, boy, girl, man, woman
reptiles	crocodile, dinosaur, lizard, snake, turtle
small mammals	hamster, mouse, rabbit, shrew, squirrel
trees	maple, oak, palm, pine, willow
vehicles 1	bicycle, bus, motorcycle, pickup truck, train
vehicles 2	lawn-mower, rocket, streetcar, tank, tractor

Yes, I know mushrooms aren't really fruit or vegetables and bears aren't really carnivores. },
keywords= {Dataset},
terms= {}
}


@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@Article{Liu1989,
author={Liu, Dong C.
and Nocedal, Jorge},
title={On the limited memory BFGS method for large scale optimization},
journal={Mathematical Programming},
year={1989},
month={Aug},
day={01},
volume={45},
number={1},
pages={503-528},
abstract={We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
issn={1436-4646},
doi={10.1007/BF01589116},
url={https://doi.org/10.1007/BF01589116}
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}


@inproceedings{huang2021mos,
  title={MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space},
  author={Huang, Rui and Li, Yixuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{
ming2023how,
title={How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?},
author={Yifei Ming and Yiyou Sun and Ousmane Dia and Yixuan Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=aEFaE0W5pAd}
}

@INPROCEEDINGS{7780542,
  author={Bendale, Abhijit and Boult, Terrance E.},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Towards Open Set Deep Networks}, 
  year={2016},
  volume={},
  number={},
  pages={1563-1572},
  keywords={Visualization;Sports equipment;Training;Adaptation models;Computational modeling;Whales;Extraterrestrial measurements},
  doi={10.1109/CVPR.2016.173}}

@inproceedings{10.5555/3305381.3305518,
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
title = {On calibration of modern neural networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1321–1330},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@misc{ren2021simplefixmahalanobisdistance,
      title={A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection}, 
      author={Jie Ren and Stanislav Fort and Jeremiah Liu and Abhijit Guha Roy and Shreyas Padhy and Balaji Lakshminarayanan},
      year={2021},
      eprint={2106.09022},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.09022}, 
}


@InProceedings{pmlr-v119-sastry20a,
  title = 	 {Detecting Out-of-Distribution Examples with {G}ram Matrices},
  author =       {Sastry, Chandramouli Shama and Oore, Sageev},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8491--8501},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sastry20a/sastry20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sastry20a.html},
  abstract = 	 {When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions; detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and predicted class. We find that characterizing activity patterns by Gram matrices and identifying anomalies in Gram matrix values can yield high OOD detection rates. We identify anomalies in the Gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and neither requires access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. We empirically demonstrate applicability across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).}
}

@inproceedings{NEURIPS2020_f5496252,
 author = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21464--21475},
 publisher = {Curran Associates, Inc.},
 title = {Energy-based Out-of-distribution Detection},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f5496252609c43eb8a3d147ab9b9c006-Paper.pdf},
 volume = {33},
 year = {2020}
}

@INPROCEEDINGS {9710170,
author = {S. Kong and D. Ramanan},
booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
title = {OpenGAN: Open-Set Recognition via Open Data Generation},
year = {2021},
volume = {},
issn = {},
pages = {793-802},
abstract = {Real-world machine learning systems need to analyze novel testing data that differs from the training data. In K-way classification, this is crisply formulated as open-set recognition, core to which is the ability to discriminate open-set data outside the K closed-set classes. Two conceptually elegant ideas for open-set discrimination are: 1) discriminatively learning an open-vs-closed binary discriminator by exploiting some outlier data as the open-set, and 2) unsupervised learning the closed-set data distribution with a GAN and using its discriminator as the open-set likelihood function. However, the former generalizes poorly to diverse open test data due to overfitting to the training outliers, which unlikely exhaustively span the open-world. The latter does not work well, presumably due to the instable training of GANs. Motivated by the above, we propose OpenGAN, which addresses the limitation of each approach by combining them with several technical insights. First, we show that a carefully selected GAN-discriminator on some real outlier data already achieves the state-of-the-art. Second, we augment the available set of real open training examples with adversarially synthesized &quot;fake&quot; data. Third and most importantly, we build the discriminator over the features computed by the closed-world K-way networks. Extensive experiments show that Open-GAN significantly outperforms prior open-set methods.},
keywords = {training;image segmentation;image recognition;semantics;training data;machine learning;generative adversarial networks},
doi = {10.1109/ICCV48922.2021.00085},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00085},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@inproceedings{
sun2021react,
title={ReAct: Out-of-distribution Detection With Rectified Activations},
author={Yiyou Sun and Chuan Guo and Yixuan Li},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=IBVBtz_sRSm}
}

@article{hendrycks2019anomalyseg,
  title={Scaling Out-of-Distribution Detection for Real-World Settings},
  author={Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Zou, Andy and Kwon, Joe and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
  journal={ICML},
  year={2022}
}

@inproceedings{song2022rankfeat,
  title={RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection},
  author={Song, Yue and Sebe, Nicu and Wang, Wei},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{
zhang2023outofdistribution,
title={Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy},
author={Jinsong Zhang and Qiang Fu and Xu Chen and Lun Du and Zelin Li and Gang Wang and xiaoguang Liu and Shi Han and Dongmei Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=KkazG4lgKL}
}

@inproceedings{Liu2023GEN,
title = {GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection},
author = {Liu, Xixi and Lochman, Yaroslava and Christopher, Zach},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year = {2023}
}

@Article{Yang2023,
author={Yang, Jingkang
and Zhou, Kaiyang
and Liu, Ziwei},
title={Full-Spectrum Out-of-Distribution Detection},
journal={International Journal of Computer Vision},
year={2023},
month={Oct},
day={01},
volume={131},
number={10},
pages={2607-2622},
abstract={Existing out-of-distribution (OOD) detection literature clearly defines semantic shift as a sign of OOD but does not have a consensus over covariate shift. Samples experiencing covariate shift but not semantic shift from the in-distribution (ID) are either excluded from the test set or treated as OOD, which contradicts the primary goal in machine learning---being able to generalize beyond the training distribution. In this paper, we take into account both shift types and introduce full-spectrum OOD (F-OOD) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and design three benchmarks. These new benchmarks have a more fine-grained categorization of distributions (i.elet@tokeneonedot, training ID, covariate-shifted ID, near-OOD, and far-OOD) for the purpose of more comprehensively evaluating the pros and cons of algorithms. To address the F-OOD detection problem, we propose SEM, a simple feature-based semantics score function. SEM is mainly composed of two probability measures: one is based on high-level features containing both semantic and non-semantic information, while the other is based on low-level feature statistics only capturing non-semantic image styles. With a simple combination, the non-semantic part is canceled out, which leaves only semantic information in SEM that can better handle F-OOD detection. Extensive experiments on the three new benchmarks show that SEM significantly outperforms current state-of-the-art methods. Our code and benchmarks are released in https://github.com/Jingkang50/OpenOOD.},
issn={1573-1405},
doi={10.1007/s11263-023-01811-z},
url={https://doi.org/10.1007/s11263-023-01811-z}
}

@ARTICLE{1056489,
  author={Lloyd, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={Least squares quantization in PCM}, 
  year={1982},
  volume={28},
  number={2},
  pages={129-137},
  keywords={},
  doi={10.1109/TIT.1982.1056489}}

@inproceedings{NIPS2001_801272ee,
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 volume = {14},
 year = {2001}
}

@Article{Murtagh2014,
author={Murtagh, Fionn
and Legendre, Pierre},
title={Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion?},
journal={Journal of Classification},
year={2014},
month={Oct},
day={01},
volume={31},
number={3},
pages={274-295},
abstract={The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. Two algorithms are found in the literature and software, both announcing that they implement the Ward clustering method. When applied to the same distance matrix, they produce different results. One algorithm preserves Ward's criterion, the other does not. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward's hierarchical clustering method.},
issn={1432-1343},
doi={10.1007/s00357-014-9161-z},
url={https://doi.org/10.1007/s00357-014-9161-z}
}

@INPROCEEDINGS{8974537,
  author={Rohilla, Vinita and kumar, Ms Sanika Singh and Chakraborty, Sudeshna and Singh, Ms. Sanika},
  booktitle={2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)}, 
  title={Data Clustering using Bisecting K-Means}, 
  year={2019},
  volume={},
  number={},
  pages={80-83},
  keywords={hierarchical clustering;Bisecting Clustering;K-Means.},
  doi={10.1109/ICCCIS48478.2019.8974537}}

@article{10.1145/235968.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: an efficient data clustering method for very large databases},
year = {1996},
issue_date = {June 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/235968.233324},
doi = {10.1145/235968.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
journal = {SIGMOD Rec.},
month = jun,
pages = {103–114},
numpages = {12}
}

@inproceedings{10.5555/3001460.3001507,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},
location = {Portland, Oregon},
series = {KDD'96}
}

@article{KELLEYPACE1997291,
title = {Sparse spatial autoregressions},
journal = {Statistics And Probability Letters},
volume = {33},
number = {3},
pages = {291-297},
year = {1997},
issn = {0167-7152},
doi = {https://doi.org/10.1016/S0167-7152(96)00140-X},
url = {https://www.sciencedirect.com/science/article/pii/S016771529600140X},
author = {R. {Kelley Pace} and Ronald Barry},
keywords = {Spatial autoregression, SAR, Sparse matrices},
abstract = {Given local spatial error dependence, one can construct sparse spatial weight matrices. As an illustration of the power of such sparse structures, we computed a simultaneous autoregression using 20 640 observations in under 19 min despite needing to compute a 20 640 by 20 640 determinant 10 times.}
}

@misc{wine_quality_186,
author = {Cortez, Paulo and Cerdeira, A. and Almeida, F. and Matos, T. and Reis, J.},
title = {{Wine Quality}},
year = {2009},
howpublished = {UCI Machine Learning Repository},
note = {{DOI}: https://doi.org/10.24432/C56S3T}
}

@article{10.1214/aos/1176347963,
author = {Jerome H. Friedman},
title = {{Multivariate Adaptive Regression Splines}},
volume = {19},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {1 -- 67},
keywords = {AID, CART, multivariable function approximation, multivariate smoothing, Nonparametric multiple regression, recursive partitioning, splines, statistical learning neural networks},
year = {1991},
doi = {10.1214/aos/1176347963},
URL = {https://doi.org/10.1214/aos/1176347963}
}

@INPROCEEDINGS {8579012,
author = { Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge },
booktitle = { 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ The iNaturalist Species Classification and Detection Dataset }},
year = {2018},
volume = {},
ISSN = {},
pages = {8769-8778},
abstract = { Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning. },
keywords = {Observers;Training;Computer vision;Biodiversity;Face;Computational modeling;Dogs},
doi = {10.1109/CVPR.2018.00914},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00914},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@inproceedings{
vaze2022openset,
title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=5hLP5JY9S2d}
}

@misc{bitterwolf2023outfixingimagenetoutofdistribution,
      title={In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation}, 
      author={Julian Bitterwolf and Maximilian Müller and Matthias Hein},
      year={2023},
      eprint={2306.00826},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.00826}, 
}

@inproceedings{conf/icml/RechtRSS19,
  added-at = {2019-06-11T00:00:00.000+0200},
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  biburl = {https://www.bibsonomy.org/bibtex/2b58fc3460861d0c00fde8b20dd1999d4/dblp},
  booktitle = {ICML},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  ee = {http://proceedings.mlr.press/v97/recht19a.html},
  interhash = {c5ec3a9e185982372a5fdd28d6b567b2},
  intrahash = {b58fc3460861d0c00fde8b20dd1999d4},
  keywords = {dblp},
  pages = {5389-5400},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  timestamp = {2019-06-12T11:41:24.000+0200},
  title = {Do ImageNet Classifiers Generalize to ImageNet?},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2019.html#RechtRSS19},
  volume = 97,
  year = 2019
}
%  crossref = {conf/icml/2019},

@article{hendrycks2019robustness,
  title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author={Dan Hendrycks and Thomas Dietterich},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@article{hendrycks2021many,
  title={The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization},
  author={Dan Hendrycks and Steven Basart and Norman Mu and Saurav Kadavath and Frank Wang and Evan Dorundo and Rahul Desai and Tyler Zhu and Samyak Parajuli and Mike Guo and Dawn Song and Jacob Steinhardt and Justin Gilmer},
  journal={ICCV},
  year={2021}
}

@INPROCEEDINGS{5206848,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

