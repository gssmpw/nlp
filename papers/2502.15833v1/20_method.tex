\section{KAN-based OOD detection}
This section begins by providing a short background on KANs and their working principle. 
Next, we delve into the core concept underlying our proposed method for OOD detection. 
Finally, we describe the primary limitation of the KAN detector and propose a strategy to enable its deployment in complex real-world scenarios.

\subsection{Background}
KANs are neural network architectures based on the Kolmogorov-Arnold representation theorem. 
This theorem states that any continuous multivariate function can be represented as a sum of continuous functions of a single variable. 
Hence, KANs approximate high-dimensional functions using simpler, univariate components, effectively addressing the curse of dimensionality in machine learning.

In practice, KANs decompose multivariate functions into univariate B-spline functions with learnable coefficients. 
Let $x_p$ be the p-th component (feature) of the input vector $\textbf{x} \in \mathbb{R}^{n_{\text{in}}}$ and let $y_q$ be the q-th component (feature) of the output vector $\textbf{y} \in \mathbb{R}^{n_{\text{out}}}$. 
A KAN layer transforms $\textbf{x}$ into $\textbf{y}$ using a matrix of univariate functions \(\Phi = \{ \phi_{p, q} \}\), where each \(\phi_{p, q}\) is parameterized by a B-spline.
Each B-spline consists of a linear combination of $G+k$ B-spline basis functions with learnable coefficients $c_{p, q, i}$. 
The spline order is denoted as $k$ (usually $k = 3$) and $G$ is the grid size.

\begin{equation}
\label{eq:spline}
   y_q = \sum_{p} \phi_{p, q}(x_p) \quad \textrm{with:} \quad \phi_{p, q}(x_p) = \sum_{i=0}^{G+k} c_{p, q, i} B_i(x_p). 
\end{equation}

KAN layers can be stacked to construct deeper networks, allowing for complex transformations across multiple stages. 
Performance is further enhanced by incorporating residual connections, which add flexibility to the spline functions through trainable weights and additional basis functions \citep{liu2024kankolmogorovarnoldnetworks}.

Local neuroplasticity in KANs is facilitated by two key properties.
First, each input feature \( x_p \) is processed independently by its own set of activation functions \( \{\phi_{p, q} \mid \forall q \} \). 
Second, during backpropagation, only the spline coefficients near sample \( x_p \) are modified, leaving the other areas of the activation function largely unchanged.

\subsection{OOD Detection with KANs}
We propose leveraging the local plasticity of KANs for OOD detection. 
The InD data seen during training only affects specific regions (spline grid coefficients) of the network.
By determining whether a region contains InD data and inspecting which regions are activated by each sample, the KAN-based detector can distinguish between InD and OOD samples. 
This differentiation is achieved by comparing the output of the trained activation functions with their values prior to training.
The step-by-step procedure is as follows:

\begin{itemize}
    \item \textbf{Setup}: Initialize an untrained KAN and create a copy. Train one KAN with the InD dataset while keeping the other untrained.
    \item \textbf{Detection}: Perform a forward pass on both networks with the given sample $\textbf{x}$, and save the output of the activation functions:
    \begin{equation}
        \phi_{p, q}^{\text{trained}}(x_p), \quad \phi_{p, q}^{\text{untrained}}(x_p) \quad \forall p, q
    \end{equation}
    Compute the difference between the responses:
    \begin{equation}
        \Delta_{p, q}(x_p) = \left| \phi_{p, q}^{\text{trained}}(x_p) - \phi_{p, q}^{\text{untrained}}(x_p) \right|.
    \end{equation}
    Analyze the difference matrix \( \Delta \). OOD samples will tend to have a higher ratio of the entries in the \( \Delta \) matrix close to zero. 
    To obtain a scalar InD score $S(\textbf{x})$, we aggregate the differences using a scoring function \( F_{\text{score}} \) (detailed in Appendix~\ref{app:scoring_and_agg}):
    \begin{equation}
        S(\textbf{x}) = F_{\text{score}}(\Delta(\textbf{x})).
    \end{equation}
\end{itemize}

To clarify our method's working principle, let us rewrite $\Delta_{p,q}(x_p)$ using Eq. \ref{eq:spline}:

\begin{equation}
    \Delta_{p,q}(x_p) = \sum_{i} \left| c_{p,q,i}^{\text{trained}} - c_{p,q,i}^{\text{untrained}}\right| \cdot B_i(x_p).
\end{equation}

The terms $\left| c_{p,q,i}^{\text{trained}} - c_{p,q,i}^{\text{untrained}}\right|$ define the locations within the network where InD information is stored, while $B_i(x_p)$ serves as a mask and specify the regions activated by the sample $\textbf{x}$. 
Consequently, multiplying these two terms provides a quantitative measure of the overlap between InD regions and the given sample. 
This overlap is subsequently utilized to compute the InD score.

Once the InD score is obtained, it is thresholded to classify the sample as InD or OOD:

\begin{equation}
D(\textbf{x}) = 
\begin{cases} 
\text{InD,} & \text{if } S(\textbf{x}) \geq \lambda \\
\text{OOD,} & \text{if } S(\textbf{x}) < \lambda, 
\end{cases}
\end{equation}

where \(\lambda\) is a predefined threshold. 
A test sample with a InD score less than \(\lambda\) is categorized as OOD. 
Otherwise, it is classified as InD. 

Figure \ref{fig:toy_example} illustrates the working principle of the proposed algorithm using a modified version of the toy example proposed by \citet{liu2024kankolmogorovarnoldnetworks}. 
The dataset is a one-dimensional regression task featuring five Gaussian peaks. 
We used two of these peaks as the training set and InD test set, while the remaining three peaks are the OOD test set. 
Here the KAN model is composed of a single layer with one input and one output, i.e. a single univariate function $\phi$ with 200 spline coefficients.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/Figure2.pdf}
\end{center}
\vspace{-5pt}
\caption{(a) Visualization of the training dataset, showing the relationship between inputs and targets. 
(b) Response of the untrained KAN model across the entire input range. 
(c) Response of the trained KAN model across the entire input range. 
(d) Test dataset illustrating inputs versus targets, created by combining the training dataset (InD) with three additional Gaussian peaks over the remaining input range (OOD). 
(e) InD score $S(\textbf{x}) \forall \textbf{x} \in [-1, 1]$ using the median as scoring function ($F_{\text{score}}$). 
(f) Final results after applying a threshold ($\lambda=1e-3$) to the InD scores: blue regions indicate predicted InD areas and red regions indicate predicted OOD areas.}
\label{fig:toy_example}
\end{figure}

\vspace{1pt}
\subsection{Capturing the joint feature distribution}
Like MLPs, KANs are capable of processing multivariate inputs and producing multivariate outputs. 
However, differently from MLPs where activation functions receive a weighted sum of all inputs, in KANs, each activation function receives only a single input. 
While this characteristic allows the KAN detector to effectively capture the marginal distributions of input features, it also constrains its ability to model the joint distribution of features.

To overcome this limitation we propose to partition the InD dataset and train separate KAN models for each partition. 
In this way, the complex training distribution is decomposed into smaller parts that can be accurately described using only the marginal feature distribution.
Various techniques can be employed to partition the dataset. 
A simple, yet effective approach is to split the dataset based on class labels.
An alternative approach, which also works when class labels are absent, such as in regression tasks, is to apply a clustering algorithm like k-means \citep{1056489}.
Formally, the dataset \( \mathcal{D} \) is partitioned into $\mathcal{P}$ non-overlapping subsets \( \mathcal{D}_1, \mathcal{D}_2, \ldots, \mathcal{D}_\mathcal{P} \). 
For each partition \( \mathcal{D}_i \), we train a separate detector, denoted as \( \text{KAN}_i \).
While the partition \( \mathcal{D}_i \) is different for each \(\text{KAN}_i \), the training task is always the same (e.g., classification).
During inference, we compute the InD score for a sample \( \textbf{x} \) by aggregating the InD score from each KAN model. 
Let \( \Delta^{i}(\textbf{x})\) be the difference matrix of \( \text{KAN}_i \):
\begin{equation}
S(\textbf{x}) = F_{\text{agg.}}(F_{\text{score}}(\Delta^{1}(\textbf{x})), F_{\text{score}}(\Delta^{2}(\textbf{x})), \ldots, F_{\text{score}}(\Delta^{\mathcal{P}}(\textbf{x})),
\end{equation}
where \( F_{\text{agg.}} \) is a suitable aggregation function, such as the maximum function.
Since the partitions are non-overlapping, for InD samples, there will be only one model that recognises the sample as InD (high InD score), while the other will flag it as OOD (low InD score).

Through this partitioning, our detector is now composed of multiple KAN models, and the strategy resembles ensemble methods. 
Furthermore, if all models are initialized with the same weights, the untrained KAN can be shared, reducing the number of forward passes at inference time.

To demonstrate the effectiveness of our proposed improvement method, we designed a specialized L-shaped dataset where the base KAN detector fails. 
This dataset consists of 2D points, with the training task being regression to a predefined constant. 
Figure \ref{fig:joint_dist} illustrates the results, showing the performance of the default KAN detector compared to the partitioning method. 

\begin{figure}[ht]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\linewidth]{figures/figure3.pdf}
\end{center}
\vspace{-5pt}
\caption{2D L-shape toy dataset: The blue point cloud shows the training distribution and the red points are OOD test samples. The black contour represents the thresholded score function (median) separating InD from OOD samples. 
(a) Default KAN detector limited by the marginal distribution of \(x_1\) and \(x_2\). 
(b) Improved performance by partitioning the training dataset with KMeans clustering ($\mathcal{P}=2$) and $F_{\text{agg.}}=\text{max}$ as aggregation function.}
\label{fig:joint_dist}
\end{figure}