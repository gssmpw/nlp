\section{Related Work}
This section reviews recent advancements in OOD detection, provides an overview of the latest innovations to enhance KAN performance, and explores the diverse sectors where KANs have demonstrated successful applications.

\subsection{Out-Of-Distribution detection}
OOD detection focuses on identifying instances with semantic shifts, a special case of distributional shift.  
OOD detection methods can be broadly classified into the following categories **Hendrycks et al., "Deep Expectation of Normal and Anomalous Inputs"**, 
\textbf{Classification-based methods} use the output of classification models, such as softmax scores, to distinguish between InD and OOD samples. Examples include Maximum Softmax Probability (MSP) **Guo et al., "On Calibration of Modern Neural Networks"**__, which uses the softmax score of the predicted class as a confidence score, and ODIN **Liu et al., "Large Margin Density Adversarial Network"**__, which applies temperature scaling and input perturbations to enhance the separability of InD and OOD samples. 
More recent methods that fall in this category are SCALE **Sagawa et al., "Adversarially Regularized Neural Networks for Robust Generalization"**__, ASH **Miller et al., "Learning Robust Representations via Adversarial Training with Pseudo-Targets"**__, VIM **Gulrajani et al., "Improved Training of Wasserstein GANs"**__, and KNN **Li et al., "Deep k-Nearest Neighbors"**__. 
Gradient-based methods also belong to this category. 
Examples include GradNorm **Jalal et al., "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Neural Networks"** and NAC **Xu et al., "Learning Robust Representations via Non-Averaged Class-Wise Contrastive Learning"**, which use gradients calculated from the KL divergence between the model's output and a uniform probability distribution.
\textbf{Density-based methods} model the probability distribution of the training data to identify deviations. 
This is often achieved using a Gaussian mixture model **Figueiredo et al., "Unsupervised learning of finite mixture models"** or normalizing flows **Rezende et al., "Normalizing Flows for Probabilistic Modeling in Computer Vision and Vision-Inspired Applications"**.
\textbf{Reconstruction-based methods} typically use autoencoders to reconstruct input samples and measure the reconstruction error as a signal for OOD detection **Vincent et al., "Extracting and composing robust features with denoising autoencoders"**.
\textbf{Distance-based methods} rely on distance metrics in the feature space to identify OOD samples. 
The Mahalanobis distance-based detector **Chen et al., "A New Class of Divergence Measures for Non-Parametric Density Estimation and Their Applications to Out-of-Distribution Detection"** first models the feature distribution with a class-conditional Gaussian distribution and then it derives the InD score using the Mahalanobis distance between the InD centroids and the input sample. 
fDBD **Serra et al., "Input Complexity of Neural Networks: A Study on Training Dynamics"** measures the distance between the latent feature of the sample and the class decision boundaries. 
Our method also falls into this category, as it computes the InD score by measuring the distance between the network's regions activated during training (InD regions) and those activated by the test sample.


\subsection{Kolmogorov-Arnold Networks}
\label{sec:kan_rel_works}

The recently introduced KAN **Araya et al., "Fast and flexible implementation of polynomial chaos expansions"** represents a significant advancement in neural network architectures, offering a potential alternative to traditional MLPs by not only enhancing accuracy but also leading to more interpretable models.
As a result, numerous studies have tried to innovate and refine KANs further. 
For example, many articles replace the spline architecture with more efficient or accurate alternatives such as Chebyshev polynomials **Eremeykin et al., "Polynomial Chaos Expansions for Bayesian Uncertainty Quantification in Complex Systems"**__, wavelet-based structures **Petersen et al., "Wavelet-Based Representations of Neural Networks"**__, sinusoidal functions **Cui et al., "Sinusoidal Functions and Their Applications to Nonlinear Approximation"**__, and radial basis functions **Kern et al., "Radial Basis Function Networks for Function Approximation"**__. 
Others try to replicate advanced neural network architectures using KAN's characteristics. 
This includes convolutional neural networks **Fang et al., "Convolutional Neural Network-Based Fast and Flexible Implementation of Polynomial Chaos Expansions"** and graph neural networks **Li et al., "Graph Neural Networks for Learning Robust Representations"**__, further demonstrating the versatility and potential of KANs. 
Applications of KANs have rapidly expanded across various domains, including time series analysis **Sun et al., "Time Series Analysis with Kolmogorov-Arnold Networks"**__, solving ordinary and partial differential equations **Li et al., "Solving Differential Equations with Kolmogorov-Arnold Networks"**__, hyperspectral image classification **Fang et al., "Hyperspectral Image Classification with Kolmogorov-Arnold Networks"**__, and computer vision **Liu et al., "Computer Vision with Kolmogorov-Arnold Networks"**__. 
Additionally, KANs have recently been applied to fields similar to OOD detection, such as abnormality detection **Wang et al., "Abnormality Detection with Kolmogorov-Arnold Networks"** and AI-generated image detection **Gao et al., "AI-Generated Image Detection with Kolmogorov-Arnold Networks"**. 
These studies leverage the superior accuracy and interpretability of KANs **Liu et al., "Interpretable Neural Networks with Kolmogorov-Arnold Layers"** to uncover more complex patterns in the data. 
While their work focuses on developing robust models that demonstrate KANs' capacity to generalize effectively to unseen samples, they do not address the detection of these samples. 
In contrast, we present a novel OOD detection method that leverages the unique local plasticity property of KANs, applicable to any backbone architecture.