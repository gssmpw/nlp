\section{Experiments}
First, we describe the benchmarks, metrics, and implementation details used in our study.  
The results demonstrate the superior performance of our method, highlighting its key advantages. 
Finally, a comprehensive ablation study analyzes each hyperparameter and component, elucidating their impact on performance.

\subsection{Evaluation protocol}

\paragraph{Setup.} 
The evaluation of the proposed method is performed on seven different benchmarks from two different domains: OOD detection in images and tabular medical data. 

For OOD detection in images, the experimental setup adheres to the OpenOOD \citep{NEURIPS2022_d201587e} benchmark protocol. 
We evaluate the KAN detector on the CIFAR-10 benchmark, using CIFAR-10 \citep{cifar10dataset} as the InD dataset. 
The OOD datasets are categorized into near OOD datasets (CIFAR-100 \citep{cifar100dataset} and Tiny ImageNet (TIN) \citep{le2015tiny}) and far OOD datasets (MNIST \citep{6296535}, SVHN \citep{37648}, Textures \citep{Cimpoi_2014_CVPR}, and Places365 \citep{7968387}). 
The CIFAR-100 benchmark contains the same datasets as the CIFAR-10 benchmark except for the CIFAR-10 and CIFAR-100 datasets which have an inverted role (CIFAR-100 as training data and CIFAR-10 as OOD dataset).
To evaluate the scalability of our method, we also tested it on the ImageNet-200 FS and ImageNet-1K FS benchmarks. 
Compared to CIFAR-10 and CIFAR-100, these benchmarks features five to twenty times more training images, each with a size seven times larger.
The full-spectrum version increases the detection challenge and, at the same time, makes it closer to real-world applications by enriching the InD test set with covariate-shifted InD samples \citep{Yang2023}.
The datasets used in this benchmark are: ImageNet-200 or ImageNet-1K \citep{5206848} as training set, ImageNet-V2 \citep{conf/icml/RechtRSS19}, ImageNet-C \citep{hendrycks2019robustness}, ImageNet-R \citep{hendrycks2021many} as covariate-shifted InD test set, SSB-hard \citep{vaze2022openset}, NINCO \citep{bitterwolf2023outfixingimagenetoutofdistribution} as near OOD, and iNaturalist \citep{8579012}, Textures, OpenImage-O \citep{haoqi2022vim} as far OOD.

For OOD detection in tabular medical data, we follow the benchmark proposed by \citet{azizmalayeri2023unmasking}. 
We consider the benchmarks derived from the eICU dataset \citep{Pollard2018}, which contains clinical data of tens of thousands of Intensive Care Unit (ICU) patients in several hospitals.
In the near OOD benchmarks, the eICU dataset is divided into InD and OOD according to some features such as ethnicity (Caucasian as InD) or age (older than 70 as InD). 
The feature used for splitting the dataset is then removed.
In the synthetic OOD benchmark, the OOD data is generated by scaling a single feature from the InD set by a factor $\mathcal{F}$. 
For each factor, the experiment is repeated 100 times with different features, to minimize the impact of the chosen feature. 
By varying the scaling factor, the generated samples range from near to far OOD.

In contrast to training-time regularization methods (e.g., MOS \citep{huang2021mos}, CIDER \citep{ming2023how}), our detector operates in a post-hoc manner and can be seamlessly integrated with any pre-trained classifier, regardless of model architecture, training procedures, or types of OOD data.
The backbone is used to perform the classification or regression task and in the case of post-hoc methods it is trained independently from the detector.
The OOD detector only uses the latent features of the backbone for InD/OOD classification.
The considered OpenOOD benchmarks employ a pre-trained ResNet backbone \citep{he2015deepresiduallearningimage} for feature extraction, while the tabular medical benchmarks use an FT-Transformer backbone \citep{gorishniy2021revisiting}.

Given that the benchmarks we considered are all based on classification tasks and require a pre-trained backbone network, we conducted additional experiments on regression-based datasets, applying the detector directly to the data without a feature extractor. The results, presented in Appendix \ref{app:regression}, demonstrate that our method also performs well in these scenarios.

\paragraph{Metrics.} 
In all benchmarks, the primary metric used to evaluate the OOD detection performance is the Area Under the Receiver Operating Characteristic curve (AUROC). 
This threshold-free metric provides a robust assessment of the model's ability to distinguish between InD and OOD samples. 

In our evaluation, we focus on the average AUROC across all test datasets, including both near and far OOD (for more details on how the overall average is computed see Appendix \ref{app:avg_metric}). 
This approach is motivated by the desire to develop a method that performs well across diverse datasets, as real-world applications often encounter unknown types of OOD samples. 
We acknowledge that achieving consistent performance across multiple datasets is challenging, as many methods excel on specific datasets but struggle to generalize.

Following the OpenOOD benchmark guidelines, we report the results averaged over three seeds, corresponding to three pre-trained backbones. 
This does not apply to the ImageNet-1K FS benchmark where only one pre-trained backbone is available.
The results are averaged over five seeds for the tabular medical data benchmarks.
Our approach also introduces some stochasticity due to the KAN initialization. 
To assess its impact on performance, we initialized the detector with five different seeds for each pre-trained backbone. The results indicate that the stochasticity due to the KAN initialization is lower than the one due to the backbone training (see Appendix \ref{app:kan_stoch}).

\paragraph{Implementation details.} 
\label{sec:imp_details}

\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-10pt} % Adjust the value as needed to remove the extra space above
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/figure4_ink.pdf}
    \end{center}
    %\vspace{-10pt} % Adjust the value as needed to remove the extra space below
    \caption{Distribution of activation's differences ($\Delta$) for three different samples (InD, near and far OOD). The InD sample tends to produce bigger values in the $\Delta$ matrix compared to the OOD samples. Using the median as a scoring function (vertical dashed lines) effectively separates InD from OOD.}
    \label{fig:delta_median}
    \vspace{-20pt} % Adjust the value as needed to remove the extra space above
\end{wrapfigure}

The detectors are trained using the InD dataset. 
During the evaluation phase, InD scores are calculated for all the test data. 
All hyperparameters are tuned using the validation set, according to the OpenOOD benchmark guidelines. 
The tabular medical data benchmarks follow a similar structure.

On all benchmarks, we used the median as the scoring function and the maximum as the aggregation function.
The median is particularly effective due to its robustness to outliers, making it reliable for distinguishing between InD and OOD samples, as illustrated in Figure \ref{fig:delta_median}. 
These choices for both (scoring and aggregation) are further motivated in Appendix \ref{app:scoring_and_agg}.

The latent features of the backbones exhibited a highly skewed distribution. To address this skewness and achieve a more balanced distribution that fully utilizes the KAN's grid range, we applied histogram normalization.

We leverage information from multiple latent layers of the pre-trained backbone. 
As demonstrated by \citet{liu2024neuron}, this multi-layer integration enriches the feature representation, leading to improved detection accuracy. Specifically, the authors claim that the last layer contains predominantly semantic information while including the layers closer to the input allows the detector to capture also the covariate information.

\subsection{Results}
\label{sec:results}

Table \ref{tab:CIFAR_results} presents the results of our experiments on the CIFAR-10 and CIFAR-100 benchmarks, comparing the KAN detector with several SOTA OOD detection methods (see Appendix~\ref{app:baselines} for a list of all the considered baselines). 
On top of the numerous baselines provided by the benchmark we also compare our approach to the current best post-hoc method on the CIFAR leaderboard: the NAC \citep{liu2024neuron}.
The results show that the KAN detector outperforms all previous methods on both benchmarks, demonstrating the effectiveness of leveraging spline-based local activation functions for OOD detection.
In each column of this and the following tables, we highlight in \textbf{bold} the best-performing method. Where multiple seeds of the backbone are available we also highlight any other methods that do not show a statistically significant difference from the best-performing method. 
Statistical significance is assessed using Welch's t-test with $p<0.05$.

\begin{table}[ht]
\caption{Comparison of OOD detection performance (AUROC) on CIFAR-10 and CIFAR-100 benchmarks.
}
\label{tab:CIFAR_results}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\multicolumn{1}{c}{\bf Method} & \multicolumn{2}{c}{\bf Near OOD} & \multicolumn{4}{c}{\bf Far OOD} & \multicolumn{1}{c}{\bf Avg Near} & \multicolumn{1}{c}{\bf Avg Far} & \multicolumn{1}{c}{\bf Avg Overall} \\
\cmidrule(r){2-3} \cmidrule(r){4-7}
 & \multicolumn{1}{c}{\bf CIFAR} & \multicolumn{1}{c}{\bf TIN} & \multicolumn{1}{c}{\bf MNIST} & \multicolumn{1}{c}{\bf SVHN} & \multicolumn{1}{c}{\bf Textures} & \multicolumn{1}{c}{\bf Places365} \\
\midrule
\multicolumn{10}{c}{\textbf{CIFAR-10 Benchmark}} \\
OpenMax & 86.91\scriptsize{$\pm$0.31} & 88.32\scriptsize{$\pm$0.28} & 90.50\scriptsize{$\pm$0.44} & 89.77\scriptsize{$\pm$0.45} & 89.58\scriptsize{$\pm$0.60} & 88.63\scriptsize{$\pm$0.28} & 87.62\scriptsize{$\pm$0.29} & 89.62\scriptsize{$\pm$0.19} & 88.95\scriptsize{$\pm$0.41} \\
ODIN & 82.18\scriptsize{$\pm$1.87} & 83.55\scriptsize{$\pm$1.84} & \textbf{95.24}\scriptsize{$\pm$1.96} & 84.58\scriptsize{$\pm$0.77} & 86.94\scriptsize{$\pm$2.26} & 85.07\scriptsize{$\pm$1.24} & 82.87\scriptsize{$\pm$1.85} & 87.96\scriptsize{$\pm$0.61} & 86.26\scriptsize{$\pm$1.73} \\
MDS & 83.59\scriptsize{$\pm$2.27} & 84.81\scriptsize{$\pm$2.53} & 90.10\scriptsize{$\pm$2.41} & 91.18\scriptsize{$\pm$0.47} & 92.69\scriptsize{$\pm$1.06} & 84.90\scriptsize{$\pm$2.54} & 84.20\scriptsize{$\pm$2.40} & 89.72\scriptsize{$\pm$1.36} & 87.88\scriptsize{$\pm$2.05} \\
MDSEns & 61.29\scriptsize{$\pm$0.23} & 59.57\scriptsize{$\pm$0.53} & \textbf{99.17}\scriptsize{$\pm$0.41} & 66.56\scriptsize{$\pm$0.58} & 77.40\scriptsize{$\pm$0.28} & 52.47\scriptsize{$\pm$0.15} & 60.43\scriptsize{$\pm$0.26} & 73.90\scriptsize{$\pm$0.27} & 69.41\scriptsize{$\pm$0.40} \\
RMDS & 88.83\scriptsize{$\pm$0.35} & 90.76\scriptsize{$\pm$0.27} & 93.22\scriptsize{$\pm$0.80} & 91.84\scriptsize{$\pm$0.26} & 92.23\scriptsize{$\pm$0.23} & \textbf{91.51}\scriptsize{$\pm$0.11} & 89.80\scriptsize{$\pm$0.28} & 92.20\scriptsize{$\pm$0.21} & 91.40\scriptsize{$\pm$0.40} \\
Gram & 58.33\scriptsize{$\pm$4.49} & 58.98\scriptsize{$\pm$5.19} & 72.64\scriptsize{$\pm$2.34} & \textbf{91.52}\scriptsize{$\pm$4.45} & 62.34\scriptsize{$\pm$8.27} & 60.44\scriptsize{$\pm$3.41} & 58.66\scriptsize{$\pm$4.83} & 71.73\scriptsize{$\pm$3.20} & 67.37\scriptsize{$\pm$5.04} \\
ReAct & 85.93\scriptsize{$\pm$0.83} & 88.29\scriptsize{$\pm$0.44} & \textbf{92.81}\scriptsize{$\pm$3.03} & 89.12\scriptsize{$\pm$3.19} & 89.38\scriptsize{$\pm$1.49} & \textbf{90.35}\scriptsize{$\pm$0.78} & 87.11\scriptsize{$\pm$0.61} & 90.42\scriptsize{$\pm$1.41} & 89.31\scriptsize{$\pm$1.96} \\
VIM & 87.75\scriptsize{$\pm$0.28} & 89.62\scriptsize{$\pm$0.33} & 94.76\scriptsize{$\pm$0.38} & 94.50\scriptsize{$\pm$0.48} & \textbf{95.15}\scriptsize{$\pm$0.34} & 89.49\scriptsize{$\pm$0.39} & 88.68\scriptsize{$\pm$0.28} & 93.48\scriptsize{$\pm$0.24} & 91.88\scriptsize{$\pm$0.37} \\
KNN & \textbf{89.73}\scriptsize{$\pm$0.14} & \textbf{91.56}\scriptsize{$\pm$0.26} & 94.26\scriptsize{$\pm$0.38} & 92.67\scriptsize{$\pm$0.30} & 93.16\scriptsize{$\pm$0.24} & \textbf{91.77}\scriptsize{$\pm$0.23} & \textbf{90.64}\scriptsize{$\pm$0.20} & 92.96\scriptsize{$\pm$0.14} & 92.19\scriptsize{$\pm$0.27} \\
ASH & 74.11\scriptsize{$\pm$1.55} & 76.44\scriptsize{$\pm$0.61} & 83.16\scriptsize{$\pm$4.66} & 73.46\scriptsize{$\pm$6.41} & 77.45\scriptsize{$\pm$2.39} & 79.89\scriptsize{$\pm$3.69} & 75.27\scriptsize{$\pm$1.04} & 78.49\scriptsize{$\pm$2.58} & 77.42\scriptsize{$\pm$3.76} \\
SHE & 80.31\scriptsize{$\pm$0.69} & 82.76\scriptsize{$\pm$0.43} & \textbf{90.43}\scriptsize{$\pm$4.76} & 86.38\scriptsize{$\pm$1.32} & 81.57\scriptsize{$\pm$1.21} & 82.89\scriptsize{$\pm$1.22} & 81.54\scriptsize{$\pm$0.51} & 85.32\scriptsize{$\pm$1.43} & 84.06\scriptsize{$\pm$2.16} \\
GEN & 87.21\scriptsize{$\pm$0.36} & 89.20\scriptsize{$\pm$0.25} & 93.83\scriptsize{$\pm$2.14} & 91.97\scriptsize{$\pm$0.66} & 90.14\scriptsize{$\pm$0.76} & 89.46\scriptsize{$\pm$0.65} & 88.20\scriptsize{$\pm$0.30} & 91.35\scriptsize{$\pm$0.69} & 90.30\scriptsize{$\pm$1.02} \\
NAC & \textbf{89.83}\scriptsize{$\pm$0.29} & \textbf{92.02}\scriptsize{$\pm$0.20} & 94.86\scriptsize{$\pm$1.37} & 96.06\scriptsize{$\pm$0.47} & \textbf{95.64}\scriptsize{$\pm$0.45} & \textbf{91.85}\scriptsize{$\pm$0.28} & \textbf{90.93}\scriptsize{$\pm$0.23} & 94.60\scriptsize{$\pm$0.50} & \textbf{93.37}\scriptsize{$\pm$0.64} \\
\rowcolor[HTML]{E7E6E6}
\textbf{KAN} & \textbf{90.06}\scriptsize{$\pm$0.47} & \textbf{91.92}\scriptsize{$\pm$0.52} & \textbf{97.86}\scriptsize{$\pm$0.73} & \textbf{97.39}\scriptsize{$\pm$0.42} & \textbf{95.85}\scriptsize{$\pm$0.28} & \textbf{91.64}\scriptsize{$\pm$0.91} & \textbf{90.99}\scriptsize{$\pm$0.50} & \textbf{95.69}\scriptsize{$\pm$0.22} & \textbf{94.12}\scriptsize{$\pm$0.59} \\
\midrule
\multicolumn{10}{c}{\textbf{CIFAR-100 Benchmark}} \\
OpenMax & 74.38\scriptsize{$\pm$0.37} & 78.44\scriptsize{$\pm$0.14} & 76.01\scriptsize{$\pm$1.39} & 82.07\scriptsize{$\pm$1.53} & 80.56\scriptsize{$\pm$0.09} & 79.29\scriptsize{$\pm$0.40} & 76.41\scriptsize{$\pm$0.25} & 79.48\scriptsize{$\pm$0.41} & 78.46\scriptsize{$\pm$0.88}\\
ODIN & 78.18\scriptsize{$\pm$0.14} & 81.63\scriptsize{$\pm$0.08} & 83.79\scriptsize{$\pm$1.31} & 74.54\scriptsize{$\pm$0.76} & 79.33\scriptsize{$\pm$1.08} & 79.45\scriptsize{$\pm$0.26} & 79.90\scriptsize{$\pm$0.11} & 79.28\scriptsize{$\pm$0.21} & \textbf{79.49}\scriptsize{$\pm$0.77}\\
MDS & 55.87\scriptsize{$\pm$0.22} & 61.50\scriptsize{$\pm$0.28} & 67.47\scriptsize{$\pm$0.81} & 70.68\scriptsize{$\pm$6.40} & 76.26\scriptsize{$\pm$0.69} & 63.15\scriptsize{$\pm$0.49} & 58.69\scriptsize{$\pm$0.09} & 69.39\scriptsize{$\pm$1.39} & 65.82\scriptsize{$\pm$2.66}\\
MDSEns & 43.85\scriptsize{$\pm$0.31} & 48.78\scriptsize{$\pm$0.19} & \textbf{98.21}\scriptsize{$\pm$0.78} & 53.76\scriptsize{$\pm$1.63} & 69.75\scriptsize{$\pm$1.14} & 42.27\scriptsize{$\pm$0.73} & 46.31\scriptsize{$\pm$0.24} & 66.00\scriptsize{$\pm$0.69} & 59.44\scriptsize{$\pm$0.93}\\
RMDS & 77.75\scriptsize{$\pm$0.19} & 82.55\scriptsize{$\pm$0.02} & 79.74\scriptsize{$\pm$2.49} & 84.89\scriptsize{$\pm$1.10} & 83.65\scriptsize{$\pm$0.51} & \textbf{83.40}\scriptsize{$\pm$0.46} & 80.15\scriptsize{$\pm$0.11} & 82.92\scriptsize{$\pm$0.42} & \textbf{82.00}\scriptsize{$\pm$1.15}\\
Gram & 49.41\scriptsize{$\pm$0.58} & 53.91\scriptsize{$\pm$1.58} & 80.71\scriptsize{$\pm$4.15} & \textbf{95.55}\scriptsize{$\pm$0.60} & 70.79\scriptsize{$\pm$1.32} & 46.38\scriptsize{$\pm$1.21} & 51.66\scriptsize{$\pm$0.77} & 73.36\scriptsize{$\pm$1.08} & 66.12\scriptsize{$\pm$1.98}\\
ReAct & 78.65\scriptsize{$\pm$0.05} & 82.88\scriptsize{$\pm$0.08} & 78.37\scriptsize{$\pm$1.59} & 83.01\scriptsize{$\pm$0.97} & 80.15\scriptsize{$\pm$0.46} & 80.03\scriptsize{$\pm$0.11} & 80.77\scriptsize{$\pm$0.05} & 80.39\scriptsize{$\pm$0.49} & \textbf{80.52}\scriptsize{$\pm$0.79}\\
VIM & 72.21\scriptsize{$\pm$0.41} & 77.76\scriptsize{$\pm$0.16} & 81.89\scriptsize{$\pm$1.02} & 83.14\scriptsize{$\pm$3.71} & 85.91\scriptsize{$\pm$0.78} & 75.85\scriptsize{$\pm$0.37} & 74.98\scriptsize{$\pm$0.13} & 81.70\scriptsize{$\pm$0.62} & \textbf{79.46}\scriptsize{$\pm$1.62}\\
KNN & 77.02\scriptsize{$\pm$0.25} & \textbf{83.34}\scriptsize{$\pm$0.16} & 82.36\scriptsize{$\pm$1.52} & 84.15\scriptsize{$\pm$1.09} & 83.66\scriptsize{$\pm$0.83} & 79.43\scriptsize{$\pm$0.47} & 80.18\scriptsize{$\pm$0.15} & 82.40\scriptsize{$\pm$0.17} & \textbf{81.66}\scriptsize{$\pm$0.87}\\
ASH & 76.48\scriptsize{$\pm$0.30} & 79.92\scriptsize{$\pm$0.20} & 77.23\scriptsize{$\pm$0.46} & 85.60\scriptsize{$\pm$1.40} & 80.72\scriptsize{$\pm$0.70} & 78.76\scriptsize{$\pm$0.16} & 78.20\scriptsize{$\pm$0.15} & 80.58\scriptsize{$\pm$0.66} & \textbf{79.79}\scriptsize{$\pm$0.69}\\
SHE & 78.15\scriptsize{$\pm$0.03} & 79.74\scriptsize{$\pm$0.36} & 76.76\scriptsize{$\pm$1.07} & 80.97\scriptsize{$\pm$3.98} & 73.64\scriptsize{$\pm$1.28} & 76.30\scriptsize{$\pm$0.51} & 78.95\scriptsize{$\pm$0.18} & 76.92\scriptsize{$\pm$1.16} & 77.59\scriptsize{$\pm$1.78}\\
GEN & \textbf{79.38}\scriptsize{$\pm$0.04} & \textbf{83.25}\scriptsize{$\pm$0.13} & 78.29\scriptsize{$\pm$2.05} & 81.41\scriptsize{$\pm$1.50} & 78.74\scriptsize{$\pm$0.81} & 80.28\scriptsize{$\pm$0.27} & \textbf{81.31}\scriptsize{$\pm$0.08} & 79.68\scriptsize{$\pm$0.75} & \textbf{80.23}\scriptsize{$\pm$1.10}\\
NAC & 72.02\scriptsize{$\pm$0.69} & 79.86\scriptsize{$\pm$0.23} & 93.26\scriptsize{$\pm$1.34} & 92.60\scriptsize{$\pm$1.14} & \textbf{89.36}\scriptsize{$\pm$0.54} & 73.06\scriptsize{$\pm$0.63} & 75.94\scriptsize{$\pm$0.41} & \textbf{87.07}\scriptsize{$\pm$0.30} & \textbf{83.36}\scriptsize{$\pm$0.84} \\
\rowcolor[HTML]{E7E6E6}
\textbf{KAN} & 72.97\scriptsize{$\pm$0.17} & 81.37\scriptsize{$\pm$0.22} & 92.29\scriptsize{$\pm$1.85} & \textbf{87.16}\scriptsize{$\pm$4.46} & \textbf{89.43}\scriptsize{$\pm$0.39} & 77.42\scriptsize{$\pm$0.35} & 77.17\scriptsize{$\pm$0.17} & \textbf{86.57}\scriptsize{$\pm$0.70} & \textbf{83.44}\scriptsize{$\pm$1.99} \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{table}[ht]
\caption{Comparison of OOD detection performance (AUROC) on ImageNet-200 FS and ImageNet-1K FS benchmarks.}
\label{tab:imagenet200_results}
\begin{center}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{c}{\bf Method} & \multicolumn{2}{c}{\bf Near OOD} & \multicolumn{3}{c}{\bf Far OOD} & \multicolumn{1}{c}{\bf Avg Near} & \multicolumn{1}{c}{\bf Avg Far} & \multicolumn{1}{c}{\bf Avg Overall} \\
\cmidrule(r){2-3} \cmidrule(r){4-6}
 & \multicolumn{1}{c}{\bf SSB-hard} & \multicolumn{1}{c}{\bf NINCO} & \multicolumn{1}{c}{\bf iNaturalist} & \multicolumn{1}{c}{\bf Textures} & \multicolumn{1}{c}{\bf OpenImage-O} \\
\midrule
\multicolumn{9}{c}{\textbf{ImageNet-200 FS Benchmark}} \\
OpenMax	        &   47.64\scriptsize{$\pm$0.20}    &	54.15\scriptsize{$\pm$0.23}    &	72.44\scriptsize{$\pm$0.87}    &	69.12\scriptsize{$\pm$0.36}    &	62.31\scriptsize{$\pm$0.24}    &	50.89\scriptsize{$\pm$0.18}    &	67.96\scriptsize{$\pm$0.39}    &   61.13\scriptsize{$\pm$0.46} \\    
ODIN	        &   44.31\scriptsize{$\pm$0.02}    &	52.36\scriptsize{$\pm$0.08}    &	70.19\scriptsize{$\pm$0.92}    &	67.10\scriptsize{$\pm$0.34}    &	61.48\scriptsize{$\pm$0.31}    &	48.33\scriptsize{$\pm$0.05}    &	66.25\scriptsize{$\pm$0.42}    &   59.09\scriptsize{$\pm$0.46} \\
MDS	            &   48.59\scriptsize{$\pm$0.88}    &	56.65\scriptsize{$\pm$0.94}    &	68.25\scriptsize{$\pm$1.51}    &	73.84\scriptsize{$\pm$0.75}    &	61.90\scriptsize{$\pm$0.57}    &	52.62\scriptsize{$\pm$0.90}    &	68.00\scriptsize{$\pm$0.87}    &   61.85\scriptsize{$\pm$0.98} \\
MDSEns	        &   34.22\scriptsize{$\pm$0.44}    &	41.58\scriptsize{$\pm$0.17}    &	43.63\scriptsize{$\pm$0.48}    &	67.54\scriptsize{$\pm$0.35}    &	48.38\scriptsize{$\pm$0.36}    &	37.90\scriptsize{$\pm$0.20}    &	53.18\scriptsize{$\pm$0.39}    &   47.07\scriptsize{$\pm$0.38} \\
RMDS	        &   56.24\scriptsize{$\pm$0.62}    &	60.95\scriptsize{$\pm$0.94}    &	71.71\scriptsize{$\pm$1.49}    &	64.61\scriptsize{$\pm$1.07}    &	63.52\scriptsize{$\pm$0.83}    &	58.59\scriptsize{$\pm$0.77}    &	66.62\scriptsize{$\pm$1.11}    &   63.41\scriptsize{$\pm$1.03} \\
Gram	        &   \textbf{59.12}\scriptsize{$\pm$0.73}    &	\textbf{63.35}\scriptsize{$\pm$0.76}    &	58.42\scriptsize{$\pm$0.75}    &	75.86\scriptsize{$\pm$0.10}    &	61.51\scriptsize{$\pm$0.39}    &	\textbf{61.23}\scriptsize{$\pm$0.74}    &	65.26\scriptsize{$\pm$0.31}    &   63.65\scriptsize{$\pm$0.61} \\
ReAct	        &   47.25\scriptsize{$\pm$0.57}    &	53.84\scriptsize{$\pm$0.55}    &	69.45\scriptsize{$\pm$3.94}    &	71.45\scriptsize{$\pm$2.04}    &	62.30\scriptsize{$\pm$2.32}    &	50.55\scriptsize{$\pm$0.19}    &	67.73\scriptsize{$\pm$2.76}    &   60.86\scriptsize{$\pm$2.27} \\
VIM	            &   45.34\scriptsize{$\pm$0.72}    &	57.09\scriptsize{$\pm$1.03}    &	71.34\scriptsize{$\pm$1.68}    &	\textbf{82.54}\scriptsize{$\pm$0.73}    &	65.70\scriptsize{$\pm$0.94}    &	51.22\scriptsize{$\pm$0.86}    &	73.19\scriptsize{$\pm$1.10}    &   64.40\scriptsize{$\pm$1.08} \\
KNN	            &   44.05\scriptsize{$\pm$0.42}    &	54.51\scriptsize{$\pm$0.62}    &	71.53\scriptsize{$\pm$1.32}    &	81.88\scriptsize{$\pm$0.19}    &	62.12\scriptsize{$\pm$0.79}    &	49.28\scriptsize{$\pm$0.51}    &	71.84\scriptsize{$\pm$0.72}    &   62.82\scriptsize{$\pm$0.77} \\
ASH	            &   50.96\scriptsize{$\pm$0.93}    &	58.51\scriptsize{$\pm$0.60}    &	77.96\scriptsize{$\pm$1.58}    &	79.39\scriptsize{$\pm$0.61}    &	\textbf{69.09}\scriptsize{$\pm$0.71}    &	54.74\scriptsize{$\pm$0.74}    &	75.48\scriptsize{$\pm$0.95}    &   67.18\scriptsize{$\pm$0.96} \\
SHE	            &   52.82\scriptsize{$\pm$0.65}    &	56.64\scriptsize{$\pm$0.69}    &	72.20\scriptsize{$\pm$2.65}    &	74.27\scriptsize{$\pm$0.63}    &	64.95\scriptsize{$\pm$1.25}    &	54.73\scriptsize{$\pm$0.67}    &	70.47\scriptsize{$\pm$1.39}    &   64.18\scriptsize{$\pm$1.41} \\
GEN	            &   48.33\scriptsize{$\pm$0.27}    &	54.85\scriptsize{$\pm$0.42}    &	68.94\scriptsize{$\pm$0.63}    &	66.58\scriptsize{$\pm$0.47}    &	60.87\scriptsize{$\pm$0.28}    &	51.59\scriptsize{$\pm$0.34}    &	65.46\scriptsize{$\pm$0.44}    &   59.91\scriptsize{$\pm$0.43} \\
NAC             &   45.42\scriptsize{$\pm$0.11}    &    53.80\scriptsize{$\pm$0.08}    &    65.83\scriptsize{$\pm$1.22}    &    74.41\scriptsize{$\pm$0.35}    &    60.79\scriptsize{$\pm$0.23}    &    49.61\scriptsize{$\pm$0.06}    &    67.01\scriptsize{$\pm$0.53}    &   60.05\scriptsize{$\pm$0.58} \\
\rowcolor[HTML]{E7E6E6}
\textbf{KAN}             &   \textbf{58.37}\scriptsize{$\pm$0.47}    &    61.10\scriptsize{$\pm$0.53}    &    \textbf{84.13}\scriptsize{$\pm$0.35}    &    \textbf{83.30}\scriptsize{$\pm$0.35}    &    \textbf{70.40}\scriptsize{$\pm$0.26}    &    \textbf{59.74}\scriptsize{$\pm$0.46}    &    \textbf{79.28}\scriptsize{$\pm$0.18}    &   \textbf{71.46}\scriptsize{$\pm$0.40} \\
\midrule
\multicolumn{9}{c}{\textbf{ImageNet-1K FS Benchmark}} \\
OpenMax	    &   53.79  &	60.28   &	80.30   &	73.54   &	71.88   &    57.03  &	75.24   &   67.96   \\     
ODIN	    &   54.22  &	60.59   &	77.43   &	76.04   &	73.40   &    57.41  &	75.62   &   68.34   \\   
MDS	        &   39.22  &	52.83   &	54.06   &	86.26   &	60.75   &    46.02  &	67.02   &   58.62   \\   
MDSEns	    &   37.13  &	47.80   &	53.32   &	73.39   &	53.24   &    42.47  &	59.98   &   52.98   \\   
RMDS	    &   56.61  &	67.50   &	73.48   &	74.25   &	72.13   &    62.06  &	73.29   &   68.79   \\   
Gram	    &   51.93  &	60.63   &	71.36   &	84.83   &	69.40   &    56.28  &	75.20   &   67.63   \\     
ReAct	    &   55.34  &	64.51   &	87.93   &	81.08   &	79.34   &    59.93  &	82.78   &   73.64   \\   
VIM	        &   45.88  &	59.12   &	72.22   &	93.09   &	75.01   &    52.50  &	80.10   &   69.06   \\   
KNN	        &   43.78  &	59.86   &	67.79   &	90.29   &	69.98   &    51.82  &	76.02   &   66.34   \\   
ASH	        &   54.66  &	66.38   &	89.23   &	89.53   &	81.47   &    60.52  &	86.75   &   76.25   \\   
SHE	        &   \textbf{58.15}  &	64.27   &	84.71   &	87.48   &	76.92   &    61.21  &	83.04   &   74.31   \\   
GEN	        &   52.95  &	62.73   &	78.47   &	71.82   &	72.62   &    57.84  &	74.31   &   67.72   \\ 
NAC         &   52.48  &	66.49   &	88.92   &	92.77   &	80.76   &    59.48  &   87.48   &   76.28   \\ 
\rowcolor[HTML]{E7E6E6}
\textbf{KAN}	        & 55.88	    & \textbf{69.55}	    & \textbf{91.55}	    & \textbf{93.45}	    & \textbf{82.15}	    & \textbf{62.71}	    & \textbf{89.05}	    & \textbf{78.52} \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

The KAN detector ranks first on both ImageNet-200 FS and ImageNet-1K FS benchmarks as shown in Table \ref{tab:imagenet200_results} and it consistently ranks in the top three across all tabular medical data benchmarks as reported in Tables \ref{tab:tabmed_eth_results}, \ref{tab:tabmed_age_results} and \ref{tab:tabmed_results}.

In Appendix \ref{app:cifar_full} we report the results with all the baselines available in the benchmarks for the AUROC and FPR@95 metrics.


\begin{table}[!ht]
    \begin{minipage}[t]{0.24\textwidth}
    \captionsetup{justification=centering}
    \caption{Tab. Med.\\Caucasian Eth. as InD\\(AUROC metric).}
    \label{tab:tabmed_eth_results}
    \begin{center}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lc}
    \toprule
    \multicolumn{1}{c}{\bf Method} & \multicolumn{1}{c}{\bf eICU - Eth.} \\
    \midrule
    MDS & \textbf{58.5}\scriptsize{$\pm$2.2} \\            
    RMDS & 51.6\scriptsize{$\pm$1.5} \\           
    KNN & 55.8\scriptsize{$\pm$1.9} \\            
    VIM & 57.3\scriptsize{$\pm$2.3} \\            
    SHE & 50.5\scriptsize{$\pm$1.7} \\            
    KLM & 51.6\scriptsize{$\pm$2.1} \\            
    OpenMax & 48.7\scriptsize{$\pm$0.8} \\        
    \rowcolor[HTML]{E7E6E6}
    \textbf{KAN} & \textbf{61.4}\scriptsize{$\pm$3.1} \\
    \bottomrule
    \end{tabular}
    }
    \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.24\textwidth}
    \captionsetup{justification=centering}
    \caption{Tab. Med.\\$>70$ y.o. as InD\\(AUROC metric).}
    \label{tab:tabmed_age_results}
    \begin{center}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lc}
    \toprule
    \multicolumn{1}{c}{\bf Method} & \multicolumn{1}{c}{\bf eICU - Age} \\
    \midrule
    MDS & \textbf{50.8}\scriptsize{$\pm$1.1} \\ 
    RMDS & 48.3\scriptsize{$\pm$0.7} \\ 
    KNN & 49.6\scriptsize{$\pm$0.2} \\ 
    VIM & 48.8\scriptsize{$\pm$0.1} \\ 
    SHE & \textbf{50.4}\scriptsize{$\pm$0.7} \\ 
    KLM & \textbf{51.0}\scriptsize{$\pm$0.7} \\ 
    OpenMax & 48.1\scriptsize{$\pm$0.5} \\ 
    \rowcolor[HTML]{E7E6E6}
    \textbf{KAN} & \textbf{50.5}\scriptsize{$\pm$0.5} \\
    \bottomrule
    \end{tabular}
    }
    \end{center}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
    \captionsetup{justification=centering}
    \caption{Tab. Med.\\Feature multiplication\\(AUROC metric).}
    \label{tab:tabmed_results}
    \begin{center}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{c}{\bf Method} & \multicolumn{3}{c}{\bf eICU - Synthetic OOD} & \multicolumn{1}{c}{\bf Avg Overall} \\
    \cmidrule(r){2-4}
     & \multicolumn{1}{c}{\bf $\mathcal{F}=10$} & \multicolumn{1}{c}{\bf $\mathcal{F}=100$} & \multicolumn{1}{c}{\bf $\mathcal{F}=1000$} \\
    \midrule
    MDS &        59.9\scriptsize{$\pm$1.4}   & 79.5\scriptsize{$\pm$1.4}    & 87.5\scriptsize{$\pm$0.9}  & 75.63\scriptsize{$\pm$1.26}\\
    RMDS &       51.5\scriptsize{$\pm$1.3}   & 57.8\scriptsize{$\pm$7.4}    & 64.0\scriptsize{$\pm$13.0} & 57.77\scriptsize{$\pm$8.67}\\
    KNN &        57.3\scriptsize{$\pm$1.4}   & 75.4\scriptsize{$\pm$2.2}    & 86.5\scriptsize{$\pm$1.3}  & 73.07\scriptsize{$\pm$1.68}\\
    VIM &        57.9\scriptsize{$\pm$1.6}   & 77.6\scriptsize{$\pm$1.3}    & \textbf{88.3}\scriptsize{$\pm$0.7}  & 74.60\scriptsize{$\pm$1.26}\\
    SHE &        55.7\scriptsize{$\pm$1.3}   & 71.2\scriptsize{$\pm$2.9}    & 80.4\scriptsize{$\pm$1.6}  & 69.10\scriptsize{$\pm$2.05}\\
    KLM &        54.1\scriptsize{$\pm$0.8}   & 63.1\scriptsize{$\pm$1.1}    & 72.1\scriptsize{$\pm$4.2}  & 63.10\scriptsize{$\pm$2.55}\\
    OpenMax &    51.0\scriptsize{$\pm$0.7}   & 56.1\scriptsize{$\pm$2.7}    & 71.4\scriptsize{$\pm$3.2}  & 59.50\scriptsize{$\pm$2.45}\\
    \rowcolor[HTML]{E7E6E6}
    \textbf{KAN} & \textbf{64.6}\scriptsize{$\pm$2.2} & \textbf{83.0}\scriptsize{$\pm$2.6} & \textbf{89.8}\scriptsize{$\pm$1.8} & \textbf{79.13}\scriptsize{$\pm$2.22} \\
    \bottomrule
    \end{tabular} 
    }
    \end{center}
    \end{minipage}
\end{table}

Moreover, our method demonstrates significant robustness to variations in the number of training samples. 
Table \ref{tab:train_size} analyses this phenomenon on the CIFAR-10 and CIFAR-100 benchmarks, comparing the performance of the three previously best-performing methods and our approach by evaluating all methods with different dataset sizes only.
Unlike other methods that achieve peak performance only with an optimal number of training samples, our approach consistently performs well across different dataset sizes. 
The performance of VIM and KNN is closely tied to the size of the InD dataset, while NAC achieves its best results when only 2\% of the training samples are used. 
In contrast, the KAN detector maintains high performance across a wide range of training dataset sizes, with only a minor decrease observed in the extreme case of five samples per class.
Robustness to variations in training dataset sizes is crucial in real-world scenarios where the number of training samples may be insufficient to capture the underlying distribution's characteristics. 
Additionally, this property is advantageous when scaling to large datasets. 
We attribute the strong performance of our approach across all considered benchmarks also to this key characteristic.

\begin{table}[!ht]
    \centering
    \caption{The effect of training dataset sizes on AUROC performance.}
    \label{tab:train_size}
    \resizebox{0.85\textwidth}{!}{
    \begin{tabular}{l|cccc|ccc}
        \toprule
        \multicolumn{1}{c}{\bf Method} & \multicolumn{4}{c}{\bf CIFAR-10 benchmark} & \multicolumn{3}{c}{\bf CIFAR-100 benchmark} \\
        \cmidrule(r){2-5} \cmidrule(r){6-8}
         & \multicolumn{1}{c}{\textbf{100\%}} & \multicolumn{1}{c}{\textbf{10\%}} & \multicolumn{1}{c}{\textbf{1\%}} & \multicolumn{1}{c}{\textbf{0.1\%}} & \multicolumn{1}{c}{\textbf{100\%}} & \multicolumn{1}{c}{\textbf{10\%}} & \multicolumn{1}{c}{\textbf{1\%}}\\
        \midrule
        VIM  & 91.88\scriptsize{$\pm$0.37} & 91.69\scriptsize{$\pm$0.38} & 88.67\scriptsize{$\pm$1.29} & 76.38\scriptsize{$\pm$3.83} & 79.46\scriptsize{$\pm$1.62} & 78.83\scriptsize{$\pm$1.67} & 67.06\scriptsize{$\pm$2.63}\\
        KNN  & 92.19\scriptsize{$\pm$0.27} & 91.72\scriptsize{$\pm$0.28} & 88.94\scriptsize{$\pm$0.70} & 8.15\scriptsize{$\pm$0.86} & 81.66\scriptsize{$\pm$0.87} & 80.05\scriptsize{$\pm$0.85} & 27.03\scriptsize{$\pm$1.71}\\
        NAC  & 87.05\scriptsize{$\pm$1.14} & 89.74\scriptsize{$\pm$0.90} & 93.09\scriptsize{$\pm$0.65} & 89.29\scriptsize{$\pm$0.78} & 80.80\scriptsize{$\pm$0.67} & 81.72\scriptsize{$\pm$0.59} & 80.97\scriptsize{$\pm$1.09}\\
        \rowcolor[HTML]{E7E6E6}
        \textbf{KAN}    & \textbf{94.12}\scriptsize{$\pm$0.59} & \textbf{93.95}\scriptsize{$\pm$0.61} & \textbf{93.90}\scriptsize{$\pm$0.62} & \textbf{93.21}\scriptsize{$\pm$0.53} & \textbf{83.44}\scriptsize{$\pm$1.99} & \textbf{83.11}\scriptsize{$\pm$2.43} & \textbf{81.44}\scriptsize{$\pm$1.21}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Ablation Study}
\label{sec:ablations}
\paragraph{Parameter analysis.}
The main hyperparameters that regulate the performance of the proposed method are the number of partitions $\mathcal{P}$ and the grid size $G$.
Table \ref{tab:partitions_and_grid_size} illustrates the variations in AUROC performance as a function of the number of partitions obtained through k-means clustering. 
Increasing $\mathcal{P}$ enhances the detector's ability to capture the joint distribution of features, resulting in higher AUROC values. 
However, there is an upper limit beyond which further increasing the number of partitions does not lead to performance improvements.
The choice of k-means clustering over other methods is justified by its simplicity and excellent scaling performance. Additionally, empirical evidence, as reported in Appendix \ref{app:clustering}, demonstrates that the choice of the clustering algorithm does not significantly affect detection performance.

According to the authors of KAN \citep{liu2024kankolmogorovarnoldnetworks}, varying the grid size has a similar effect to varying the width and depth of a traditional MLP. 
A fine-grained grid (higher \(G\)) should improve the accuracy of the network. 
In the case of our detector, as reported in Table \ref{tab:partitions_and_grid_size}, increasing the density of the grid above a certain threshold does not result in higher OOD detection performance.



\begin{wraptable}{l}{0.55\textwidth}
    \centering
    \caption{KAN detector performance \textit{w.r.t} different datasets partitions and grid sizes over CIFAR-10.}
    \label{tab:partitions_and_grid_size}
    \resizebox{0.55\textwidth}{!}{
    \begin{tabular}{c|c||c|c}
        \toprule
        \textbf{Partitions $(\mathcal{P})$} & \textbf{AUROC} & \textbf{Grid size $(G)$} & \textbf{AUROC}\\
        \midrule
        $\mathcal{P} = 1$   & 46.08\scriptsize{$\pm$15.58} & $k = 5$    & 87.20\scriptsize{$\pm$1.52}\\
        $\mathcal{P} = 5$   & 90.39\scriptsize{$\pm$2.78} & $k = 10$    & 91.72\scriptsize{$\pm$0.54} \\
        $\mathcal{P} = 10$  & \textbf{94.12}\scriptsize{$\pm$0.59} & $k = 50$    & 93.92\scriptsize{$\pm$0.40}\\
        $\mathcal{P} = 20$  & 94.10\scriptsize{$\pm$0.62} & $k = 100$   & \textbf{94.12}\scriptsize{$\pm$0.59} \\
        $\mathcal{P} = 30$  & 94.05\scriptsize{$\pm$0.53} &  $k = 200$   & 94.03\scriptsize{$\pm$0.49}\\
        \bottomrule
    \end{tabular}
    }
\end{wraptable}





\paragraph{Splines' smoothing operation.}
KANs incorporate splines that perform an essential smoothing operation, which is crucial given the continuous nature of the input space. 
To demonstrate the superiority of the KAN detector, we implemented a baseline histogram method by replacing all the univariate functions $\phi_{p,q}$ in KAN with simple histograms that record the presence of InD samples in a binary manner. 
The histogram method achieves an overall AUROC of 85.29\%, which is approximately 9\% lower than that of the KAN detector, clearly demonstrating the superiority of the KAN's splines approach.

\paragraph{Partitioning alternatives.}
To capture the joint feature distribution, the partitioning method is not the only solution.
Another approach is to augment the input features with new features that are combinations of the original ones. 
This can be efficiently achieved using techniques like Principal Component Analysis (PCA) or autoencoders. 
PCA provides features that are linear combinations of the original ones, while autoencoders generate features that are non-linear combinations.
Although this technique worked well on a toy L-shaped dataset (Appendix \ref{app:feature_augmentation}), it did not yield the desired results on high-dimensional feature spaces in other benchmarks. 
It resulted in a lower AUROC compared to the partitioning method.

\paragraph{Influence of the training task.}
Since KANs are differentiable, they can be trained similarly to conventional MLPs using backpropagation. 
In our approach, the KAN is trained with latent features extracted from the backbone as inputs, and the training task mirrors that of the backbone network, specifically multi-class classification. 
For more details on the used training parameters see Appendix \ref{app:kan_training}.
Importantly, the training task does not need to directly relate to the OOD detection problem.
Similar to the histogram baseline, our primary objective is to \textit{register} all input samples within the correct spline coefficients. 
Any training task that adjusts the spline coefficients in the vicinity of the InD samples can yield a valid OOD detector. 

We experimentally verified our hypothesis by training the KAN using a different loss function and an unrelated task, namely regression to a constant value. 
The results from this regression task demonstrate that the detector effectively distinguishes between InD and OOD samples. 
Compared to the KAN trained on the classification task, we observed an improvement of approximately $0.2\%$ in AUROC performance on the image benchmarks. 
However, on the tabular data benchmarks, the performance decreased by approximately $3\%$.
These findings indicate that while modifying the training task of the detector can still yield satisfactory performance, the extent of this effect appears to be benchmark-dependent.
