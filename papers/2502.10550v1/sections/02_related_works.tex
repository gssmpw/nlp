\section{Related Works}
\label{sec:related_works}

Multiple RL benchmarks are designed to assess agents' memory capabilities. DMLab-30~\citep{dmlab} provides 3D navigation and puzzle tasks, focusing on long-horizon exploration and spatial recall. PsychLab~\citep{psychlab} extends DMLab by incorporating tasks that probe cognitive processes, including working memory. MiniGrid and MiniWorld~\citep{minigrid_miniworld} emphasize partial observability in lightweight 2D and 3D environments, while MiniHack~\citep{minihack} builds on NetHack~\citep{nethack}, offering small roguelike scenarios that require both short- and long-term memory. BabyAI~\citep{babyai} combines natural language instructions with grid-based tasks, requiring memory for multi-step command execution. POPGym~\citep{popgym2023} standardizes memory evaluation with tasks ranging from pattern-matching puzzles to complex sequential decision-making. BSuite~\citep{bsuite} offers a suite of carefully designed experiments that test core RL capabilities, including memory, through controlled tasks on exploration, credit assignment, and scalability. 
Memory Gym~\citep{pleines2023memory} offers a suite of 2D grid environments with partial observability, designed to benchmark memory capabilities in decision-making agents, including endless versions of tasks for evaluating memory over extremely long time intervals.
Memory Maze~\citep{memory_maze} presents 3D maze navigation tasks that require memory to solve efficiently.

While these benchmarks offer valuable insights into memory mechanisms, they generally focus on abstract puzzles or navigation tasks.  However, none of them fully encompass the broad range of memory utilization scenarios an agent may encounter, and the tasks themselves often differ fundamentally across benchmarks, making direct comparison of memory-enhanced agents difficult.

In the robotics domain, memory requirements become particularly challenging due to the physical nature of manipulation tasks. Unlike abstract environments, robotic manipulation involves complex physical interactions and multi-step procedures that demand both spatial and temporal memory. Existing memory-intensive benchmarks, while useful for diagnostic purposes, struggle to capture these domain-specific challenges. The physical control and object interaction inherent in manipulation tasks introduce additional complexities that are not addressed by traditional memory evaluation frameworks.

Additionally, efforts have been made to classify memory-intensive environments based on specific attributes. For instance, \citet{shine_rl} categorizes these environments into memory/credit assignment, distinguishing them by temporal horizons.
\citet{yue2024learning} introduces memory dependency pairs, which capture the influence of past events on current decisions, enabling agents to leverage historical context for improved imitation learning in partially observable tasks.
\citet{memory_rl} provides a formal division of agent memory into long-term and short-term depending on the agents' context length, as well as into declarative and procedural memory depending on the number of environments and episodes, and formalizes the notion of memory-intensive environments.
\citet{psychlab} takes a different approach by directly adapting established tasks from cognitive psychology and visual psychophysics, providing a standardized way to evaluate agents on well-studied human cognitive benchmarks. 

While these classification approaches offer insights into aspects of memory, they overlook physical dimensions in robotics. The interplay between physical interaction and memory remains unexplored, motivating the need for a framework that addresses spatio-temporal aspects in real-world tasks.