\section{Background}
\label{sec:background}

\subsection{Partially Observable Markov Decision Process}
\label{app:pomdp}
Partially Observable Markov Decision Process (POMDP)~\citep{pomdp_new} extend MDP to account for partial observability, where an agent observes only noisy or incomplete information about the true environments state. POMDP defined by a tuple $(S,A,T,R,\Omega,O,\gamma)$, where: \input{tables/benchmark_baseline}$S$ is the set of states representing the complete environment configuration; $A$ is the action space; $T(s'|s,a): S \times A \times S \to [0,1]$ is the transition function defining the probability of reaching state $s'$ from state $s$ after taking action $a$; $R(s,a): S \times A \to \mathbb{R}$ is the reward function specifying the immediate reward for taking action $a$ in state $s$; $\Omega$ is the observation space containing all possible observations; $O(o|s,a): S \times A \times \Omega \to [0,1]$ is the observation function defining the probability of observing $o$ after taking action $a$ and reaching state $s$; $\gamma \in [0,1)$ is the discount factor determining the importance of future rewards. The objective is to find a policy $\pi$ that maximizes the expected discounted cumulative reward: $\mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t R(s_t,a_t)\right]$, where $a_t \sim \pi(\cdot|o_{1:t})$ depends on the history of observations rather than the true state. Relying on partial observations makes POMDPs harder to solve than MDPs.

\subsection{Memory-intensive environments}
Memory-intensive environment is an environment where agents must leverage past experiences to make decisions, often in problems with long-term dependencies or delayed rewards. More formally, following~\citet{memory_rl}, a memory-intensive task is a POMDP where there exists a correlation horizon $\xi>1$, representing the minimum number of timesteps between an event critical for decision-making and when that information must be recalled. Popular memory-intensive environments in RL are listed in~\autoref{tab:behcmark-baseline}. One way to solving memory-intensive environments is to augment agents with memory mechanisms (see~\autoref{app:memory-mechanisms}). 

\subsection{Robotic Tabletop Manipulation}
\label{app:tabletop}

Robotic tabletop manipulation~\citep{shridhar2022cliport} involves robots manipulating objects on flat surfaces through actions like grasping, pushing, and picking. While crucial for real-world applications~\citep{levine2018learning}, most existing simulators treat these tasks as MDPs without memory requirements, failing to capture the spatio-temporal dependencies present in real scenarios. This limitation hinders the development of memory-enhanced agents for practical applications.