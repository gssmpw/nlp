\section{MIKASA-Robo}
\label{sec:maniskill-memory}
The landscape of robotic manipulation frameworks reveals significant limitations in addressing memory-intensive tasks. First, while partial observability is extensively studied in navigation tasks, 
manipulation scenarios are predominantly evaluated under full observability, with memory requirements receiving limited attention (see \autoref{tab:memory-frameworks}). Second, among the few frameworks that incorporate memory-intensive manipulation tasks, significant limitations exist. BEHAVIOR-1k~\citep{li2024behavior} and iGibson 2.0~\citep{li2022igibson} employ highly complex, non-atomic tasks that obscure the evaluation of specific memory mechanisms. Similarly, VIMA~\citep{jiang2022vima} relies on high-level actions that inadequately capture memory performance over extended time horizons. To the best of our knowledge, there are no benchmarks specifically designed to evaluate memory in RL in the robotic manipulation domain. To fill this gap, we introduce the MIKASA-Robo framework for the RL.


\subsection{MIKASA-Robo Benchmark}
\textbf{MIKASA-Robo} is a benchmark designed for memory-intensive robotic tabletop manipulation tasks, simulating real-world challenges commonly encountered by robots. These tasks include locating occluded objects, recalling previous configurations, and executing complex sequences of actions over extended time horizons. By incorporating meaningful partial observability, this framework offers a systematic approach to test an agent’s memory mechanisms.

Building upon the robust foundation of ManiSkill3 framework~\citep{tao2024maniskill3}, our benchmark leverages its efficient parallel GPU-based training capabilities to create and evaluate these tasks.


\subsection{MIKASA-Robo Manifestation}
In designing the tasks, we drew inspiration from the four memory types identified in our classification framework (\autoref{sec:mem-class}). We developed 32 tasks across 12 categories of robotic tabletop manipulation, each targeting specific aspects of object memory, spatial memory, sequential memory, and memory capacity. These tasks feature varying levels of complexity, allowing for systematic evaluation of different memory mechanisms. For instance, some tasks test object permanence by requiring the agent to track occluded objects, while others challenge sequential memory by requiring the reproduction of a strict order of actions. A summary of these tasks and their corresponding memory types is provided in \autoref{tab:maniskill-memory}, with detailed descriptions in~\autoref{app:tasks-description}.

To illustrate the concept of our memory-intensive framework, we present \texttt{ShellGameTouch-v0},  \texttt{RememberColor-v0}, and \texttt{RotateLenientPos-v0} tasks in \autoref{fig:envs-demo}. In the \texttt{ShellGameTouch-v0} task, the agent observes a red ball placed in one of three positions over the first 5 steps ($t \in [0, 4]$). At $t = 5$, the ball and the three positions are covered by mugs. The agent must then determine the location of the ball by interacting with the correct mug. In the simplest mode (\texttt{Touch}), the agent only needs to touch the correct mug, whereas in other modes, it must either push or lift the mug. In the \texttt{RememberColor-v0} task, the agent observes a cube of a specific color for 5 steps ($t \in [0, 4]$). After the cube disappears for 5 steps, 3, 5, or 9 (depending on task mode) cubes of different colors appear at $t = 10$. The agent’s task is to identify and select the same cube it initially saw. In the \texttt{RotateLenientPos-v0} task, the agent must rotate a randomly oriented peg by a specified clockwise angle.

\begin{figure*}[t!]
\centering
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/demo-ppo-state.pdf}
    \caption{Performance of PPO-MLP trained in \texttt{state} mode, i.e., in MDP mode without the need for memory. These results suggest that the proposed tasks are inherently solvable with a success rate of 100$\%$.}
    \label{fig:demo-state}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/demo-ppo-rgb-joint-dense.pdf}
    \caption{PPO with MLP and LSTM backbones trained in \texttt{RGB+joints} mode on the \texttt{RememberColor-v0} environment with dense rewards. Both architectures fail to solve medium and high complexity tasks.}
    \label{fig:demo-rgb-joint-dense}
\end{minipage}
\vspace{-10pt}
\end{figure*}

The MIKASA-Robo benchmark offers multiple training modes: \texttt{state} (complete vector information including oracle data and Tool Center Point (TCP) pose), \texttt{RGB} (top-view and gripper-camera images with TCP position), \texttt{joints} (joint states and TCP pose), \texttt{oracle} (task-specific environment data for debugging), and \texttt{prompt} (static task instructions). While any mode combination is possible, \textbf{\texttt{RGB+joints} serves as the standard memory testing configuration}, with \texttt{state} mode reserved for MDP-based tasks.

The MIKASA-Robo benchmark implements two types of reward functions: dense and sparse. The dense reward provides continuous feedback based on the agent's progress towards the goal, while the sparse reward only signals task completion. While dense rewards facilitate faster learning in our experiments, sparse rewards better reflect real-world scenarios where intermediate feedback is often unavailable, making them crucial for evaluating practical applicability of memory-enhanced agents.

\subsection{Performance of classic baselines on MIKASA-Robo benchmark}
For our experimental evaluation, we selected PPO~\citep{schulman2017proximal} with two backbone architectures: Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM)~\citep{lstm}. The MLP variant serves as a memory-less baseline, while LSTM represents a widely-adopted memory mechanism in RL, known for its effectiveness in solving POMDPs~\citep{ni2021recurrent}. This choice of architectures enables direct comparison between memory-less and memory-enhanced agents while validating our benchmark's ability to assess memory. We focus specifically on these fundamental architectures as they align with our primary goal of benchmark validation rather than comprehensive algorithm comparison. To demonstrate that all proposed environments are solvable with 100\% success rate (SR), we trained a PPO-MLP agent using \texttt{state} mode, where it had full access to system information. Results for the demo environments are presented in \autoref{fig:demo-state}, with additional results for all tasks in \autoref{app:results}.

Training under the \texttt{RGB+joints} mode with dense rewards reveals the memory-intensive nature of our tasks. Using the \texttt{RememberColor-v0} task as an example, PPO-LSTM demonstrates superior performance compared to PPO-MLP when distinguishing between three colors (see \autoref{fig:demo-rgb-joint-dense}). However, both agents' success rates drop dramatically to near-zero as the task complexity increases to five or nine colors. Moreover, under sparse reward conditions, both architectures fail to solve even the three-color variant (see Appendix,~\autoref{fig:exp-rgb-joint-mlp-lstm-sparse}). These results validate our benchmark's effectiveness in evaluating agents' memory, showing clear performance degradation as memory demands increase.

Our baseline experiments reveal key insights: (1) the proposed tasks are inherently solvable, as demonstrated by perfect performance in \texttt{state} mode; (2) the tasks effectively challenge memory capabilities, shown by the performance gap between memory-less (MLP) and memory-enhanced (LSTM) architectures; and (3) primitive memory mechanisms show limitations as task complexity increases, particularly under sparse rewards. These findings validate MIKASA-Robo as an effective benchmark for evaluating and developing memory-enhanced RL agents in robotic tasks.