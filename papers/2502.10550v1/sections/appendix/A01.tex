\newpage

\addcontentsline{toc}{section}{Appendix}
\part{\vspace{-30pt}}
\parttoc

\newpage
\section{MIKASA-Robo Implementation Details}
\label{app:maniskill-memory-imp-details}

An example of running the environment from the MIKASA-Robo benchmark is shown in \autoref{lst:quick}. For ease of debugging, we also added various wrappers (found in \texttt{mikasa\_robo/utils/wrappers/}) that display useful information about the episode on the video (\autoref{lst:hello}). Thus, \texttt{RenderStepInfoWrapper()} displays the current step in the environment; \texttt{DebugRewardWrapper()} displays information about the full reward at the current step in the environment; \texttt{DebugRewardWrapper()} displays information about each component that generates the reward function at the current step. In addition, we also added task-specific wrappers for each environment. For example, \texttt{RememberColorInfoWrapper()} displays the target color of the cube in the \texttt{RememberColor-v0} task, and \texttt{ShellGameRenderCupInfoWrapper()} displays which mug the ball is actually under in the \texttt{ShellGame-v0} task.

\input{codes/maniskill-quick-start}
\input{codes/maniskill-memory-demo}

\newpage
\section{MIKASA-Base Implementation Details}
\label{app:mikasa-code}

An example of running an environment from the MIKASA-Base benchmark is shown in \autoref{lst:memory}. MIKASA-Base supports the standard Gymnasium API and is fully compatible with all its wrappers. This allows users to leverage various functionalities, including parallelization using \texttt{AsyncVectorEnv}. MIKASA-Base provides a predefined set of environments with different levels of difficulty. However, users can customize the environment parameters by passing specific arguments (see \autoref{lst:memory}). 

\input{codes/memory-length-demo}

\section{Memory Mechanisms in RL}
\label{app:memory-mechanisms}
In RL, memory mechanisms are techniques or models used to enable agents to retain and recall information from past interactions with the environment. 

There are several approaches to incorporating memory into RL, including recurrent neural networks (RNNs)~\citep{rnn,lstm,gru} which uses hidden states to store information from previous steps~\citep{rpg,drqn}, state-space models (SSMs)~\citep{gu2021efficiently,s5,gu2023mamba} which uses system state to store historical information~\citep{rssm,r2i}, transformers~\citep{vaswani2017attention} which uses attention mechanism to capture sequential dependencies inside the context window~\citep{gtrxl,hcam,shine_rl}, graph neural networks (GNNs)~\citep{gnn} which uses graphs to store information~\cite{vmg,gbmr} etc. Popular agents with memory mechanisms are summarized in~\autoref{tab:behcmark-baseline}.


\section{Classic baselines performance on the MIKASA-Robo benchmark}
\label{app:results}

In this section, we present a comprehensive evaluation of PPO-MLP and PPO-LSTM baselines on our MIKASA-Robo benchmark. Our experiments with PPO-MLP in \texttt{state} mode using dense rewards demonstrate perfect performance across all tasks, consistently achieving 100\% success rate, as shown in \autoref{fig:all-environments} and \autoref{fig:all-environments-group-2}. This remarkable performance serves as a crucial validation of our benchmark design: when an agent has access to complete state information and receives dense rewards, it can master these tasks completely. Therefore, any performance degradation in \texttt{RGB+joints} mode observed with other algorithms or training configurations must stem from the algorithmic limitations or learning challenges rather than any inherent flaws in the task design. This empirical evidence confirms that our environments are well-calibrated and properly designed, establishing a solid foundation for evaluating memory-enhanced algorithms. All results are presented as mean $\pm$ standard error of the mean (SEM), where the mean is computed across three independent training runs, and each trained agent is evaluated on 16 different random seeds to ensure robust performance assessment.

The performance evaluation of PPO-MLP and PPO-LSTM with dense rewards in the \texttt{RGB+joints} mode is presented in \autoref{fig:exp-rgb-joint-mlp-lstm-dense}. This mode specifically tests the agents' memory capabilities, as it requires remembering and utilizing historical information to solve the tasks. Our results demonstrate a clear distinction between memory-less and memory-enhanced architectures, while also revealing the limitations of conventional memory mechanisms.

Consider the \texttt{RememberColor-v0} environment as an illustrative example. In its simplest configuration with three cubes, the memory-less PPO-MLP achieves only 25\% success rate. In contrast, PPO-LSTM, leveraging its memory mechanism, achieves perfect performance with 100\% success rate. However, as task complexity increases to five or nine cubes, even the LSTM's memory capabilities prove insufficient, with performance degrading significantly. 

These results validate two key aspects of our benchmark: first, its effectiveness in distinguishing between memory-less and memory-enhanced architectures, and second, its ability to challenge even sophisticated memory mechanisms as task complexity increases. This demonstrates that MIKASA-Robo provides a competitive yet meaningful evaluation framework for developing and testing advanced memory-enhanced agents.

Our evaluation of PPO-MLP and PPO-LSTM baselines under sparse reward conditions in \texttt{RGB+joints} mode reveals the true challenge of our benchmark tasks. As shown in \autoref{fig:exp-rgb-joint-mlp-lstm-sparse}, both architectures -- even the memory-enhanced LSTM -- consistently fail to achieve any meaningful success rate across nearly all considered environments. This striking result underscores the extreme difficulty of memory-intensive manipulation tasks when only terminal rewards are available, highlighting the substantial gap between current algorithms and the level of memory capabilities required for real-world robotic applications.

\newpage
\input{figures/ppo-mlp-state-dense}

\newpage
\input{figures/ppo-mlp-state-dense-group-2}

\input{figures/ppo-mlp-lstm-dense}
\input{figures/ppo-mlp-lstm-sparse}

\input{sections/appendix/maniskill-memory-tasks-description}

\newpage
\input{tables/envs_unified_bench_table}

\input{sections/appendix/memory-benchmark-tasks-description}