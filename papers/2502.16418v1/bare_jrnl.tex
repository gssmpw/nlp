%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/
%\usepackage{indentfirst}
\setlength{\parindent}{2em}

\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}
\usepackage{easyReview}
\usepackage{comment}

\usepackage{multirow}
%\usepackage{multicolumn}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{epstopdf}
%\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}%解决公式在表格中位置太小的问题
\usepackage{booktabs}%解决表格横向粗细问题
\setcellgapes{3pt}

\setcellgapes{3pt}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[justification=centering]{caption}
\usepackage{color}
\usepackage{multirow}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{tabularx}
\usepackage{amssymb}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

%\usepackage[style=alphabetic,maxnames=1,minnames=3,maxbibnames=99]{biblatex}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
%\usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
%\usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{M4SC: An MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication System}


\author{Feibo Jiang, \textit{Senior Member, IEEE}, Siwei Tu, Li Dong,  Kezhi Wang, \textit{Senior Member, IEEE}, Kun Yang, \textit{Fellow, IEEE}, Cunhua Pan,  \textit{Senior Member, IEEE}
%	\thanks{Feibo Jiang is with Hunan Normal University, China; Kezhi Wang is with Northumbria University, UK; Kun Yang is with University of Essex, UK; Li Dong is with Hunan University of Commerce, China; Cunhua Pan is with Queen Mary University of London, UK.}
	%	\thanks{The paper was submitted on \today.}\\
}
%\author{Feibo Jiang, Kezhi Wang, Kun Yang, Li Dong  and Cunhua Pan}
%	\thanks{
%		This work was supported in part by the National Natural Science Foundation
%		of China under Grant nos. 41604117, 41904127, 41874148. 
%		
%\thanks{Feibo Jiang (jiangfb@hunnu.edu.cn) is with Hunan Provincial Key Laboratory of Intelligent Computing and Language Information Processing, Hunan Normal University, Changsha, China, Li Dong (Dlj2017@hunnu.edu.cn) is with Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, Hunan University of Commerce, Changsha, China.}
%%	\thanks{The paper was submitted on \today.}\\
%}


%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

%\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%        John~Doe,~\IEEEmembership{Fellow,~OSA,}
%        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
%\thanks{M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Submitted for Review}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.

\begin{abstract}
Multi-modal Large Language Models (MLLMs) are capable of precisely extracting high-level semantic information from multi-modal data, enabling multi-task understanding and generation. This capability facilitates more efficient and intelligent data transmission in semantic communications. In this paper, we design a tailored MLLM for semantic communication and propose an MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication (M4SC) system. First, we utilize the Kolmogorov-Arnold Network (KAN) to achieve multi-modal alignment in MLLMs, thereby enhancing the accuracy of semantics representation in the semantic space across different modalities. Next, we introduce a multi-task fine-tuning approach based on task instruction following, which leverages a unified task instruction template to describe various semantic communication tasks, improving the MLLM's ability to follow instructions across multiple tasks. Additionally, by designing a semantic sharing mechanism, we transmit the public and private semantic information of multiple users separately, thus increasing the efficiency of semantic communication. Finally, we employ a joint KAN-LLM-channel coding strategy to comprehensively enhance the performance of the semantic communication system in complex communication environments. Experimental results validate the effectiveness and robustness of the proposed M4SC in multi-modal, multi-task, and multi-user scenarios.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Multi-modal large language model, Semantic communication, KAN, multi-task learning, Semantic sharing.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
Semantic communication is a communication paradigm centered around conveying the ``meaning" of source data, with its primary goal being to ensure that the receiver understands the sender's intent, rather than simply receiving precise bit-level data. A semantic communication system typically consists of several key components: (1) \emph{Semantic encoder}, which processes the source data at the semantic level, extracting and encoding the core semantic information of the source data rather than transmitting the raw data directly. (2) \emph{Channel encoder}, which further processes the semantic-encoded information to adapt them to the transmission characteristics of the communication channel, such as noise and attenuation. (3) \emph{Channel decoder}, which decodes the signal received through the physical channel and restores the semantic information. (4) \emph{Semantic decoder}, which decodes the semantic information and ultimately reconstructs the data into a form that the user can understand. (5) \emph{Knowledge base}, which contains various background and prior information that aids the system in better understanding and recovering the true meaning of the source data during the encoding and decoding process. The core objective of the entire semantic communication is to achieve efficient and accurate semantic transmission, surpassing the traditional reliance on bitstreams in communication systems, and therefore performing more effectively in resource-constrained environments \cite{yang2022semantic}. In recent years, with the advancement of deep learning, semantic communication has gradually become a key research focus in 6G. It is evolving towards multi-modal, multi-task, and multi-user development trends, as shown in Fig. \ref{fig:threeKind}.

\subsection{Multi-modal semantic communication}
\subsubsection{Definition}
Multi-modal semantic communication refers to communication systems capable of simultaneously processing and transmitting multiple types of modal data, such as text, images, and video. The core objective is to extract semantic information from different modalities to enable the efficient sharing and expression of cross-modal information. Multi-modal semantic communication leverages the semantic relationships between modalities in both time and space to ensure high-quality semantic understanding, creating more comprehensive semantic expressions and thereby enhancing the integrity of the transmitted information \cite{jiang2024large}.

\subsubsection{Challenges}
Different modalities inherently differ in their data forms and semantic representations, and their feature spaces and semantic distributions are not aligned. For example, text is based on vocabulary and syntactic structures, images rely on pixels and visual features, and audio and video consist of time-series signals. This heterogeneity makes it exceptionally challenging to achieve a unified understanding across modalities at the semantic level. Cross-modal alignment is a key technology in multi-modal semantic communication, where data from different modalities are mapped into a shared semantic space through semantic analysis or feature extraction. This enables the modalities to understand one another and ultimately achieves consistent semantic representation.

\subsection{Multi-task semantic communication}
\subsubsection{Definition}
Multi-task semantic communication refers to a communication architecture that supports the simultaneous transmission of semantic information for multiple tasks. Its goal is to optimize the semantic requirements of different tasks, enabling knowledge sharing and collaborative communication across tasks through a unified semantic representation. Multi-task semantic communication allows for the dynamic adjustment of communication strategies to accommodate varying task priorities and service quality demands\cite{zhang2024unified}.

\subsubsection{Challenges}
The semantic spaces and objectives of different tasks may differ or even conflict, requiring the model parameters in multi-task semantic communication to be balanced across tasks. For instance, when the system learns a new task, adjustments to the model parameters may result in the loss of important semantic information from previous tasks, leading to the phenomenon of ``catastrophic forgetting" in multi-task learning. Therefore, during the training of multi-task semantic communication, the system typically needs to optimize multiple tasks in the semantic and channel encoders, ensuring that the learning of new tasks and adaptation to new environments can proceed without disrupting the performance of existing tasks.

\subsection{Multi-user semantic communication}
\subsubsection{Definition}
Multi-user semantic communication refers to communication systems that support the simultaneous transmission of semantic information for multiple users. It is primarily based on traditional multiple access technologies that divide physical resources (such as frequency division multiple access, time division multiple access, and code division multiple access). The objective is to optimize the semantic transmission efficiency among users through joint modeling and transmission of user semantic information, thereby significantly reducing bandwidth consumption while ensuring fairness and service quality \cite{xie2021task}.

\subsubsection{Challenges}
The key challenges in multi-user semantic communication research lie in how to effectively allocate resources and optimize strategies to meet the diverse needs of multiple users while maximizing the overall semantic transmission efficiency of the system. Therefore, multi-user semantic communications must scientifically allocate limited communication resources to avoid resource wastage and interference among users. Additionally, they must fully consider the collaborative mechanisms among users, such as reducing information redundancy through shared semantic information or collaborative encoding/decoding, in order to achieve global optimization of semantic transmission.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.5cm]{fig/3multiple.png}
	\caption{The structure of three different semantic communications.}
	\label{fig:threeKind}
\end{figure}

\subsection{Main contributions}
Multi-modal Large Language Models (MLLMs) represent the cutting-edge research direction in generative artificial intelligence and demonstrate significant potential in semantic information processing. By leveraging cross-modal semantic understanding, MLLMs integrate heterogeneous data such as text, images, and video, enabling multimodal information comprehension at the semantic level. Through the establishment of shared semantic representations across multiple tasks, MLLMs optimize task performance in the semantic encoder, enabling robust multi-task learning. Additionally, MLLMs capture the similarities and differences in semantics between different users in the semantic space, utilizing collaborative mechanisms to improve the overall transmission efficiency of semantic communication. Therefore, in this paper, we design a tailored MLLM for semantic communication and propose an MLLM-based Multi-modal, Multi-task, and Multi-user Semantic Communication (M4SC) system. Our contributions can be summarized as follows:

\subsubsection{KAN-based multi-modal alignment}
To address the challenge of semantic alignment in multi-modal semantic communication, we propose a cross-modal projection alignment method based on the Kolmogorov-Arnold Network (KAN). This method decomposes visual semantic information into several low-dimensional nonlinear subspaces through a set of learnable activation functions. It then dynamically learns the optimal combination of visual features, aligning the visual semantic information more smoothly and accurately with textual semantic information of Large Language Models (LLMs) in the semantic space. This significantly improves the accuracy and efficiency of multi-modal semantic alignment.

\subsubsection{Multi-task instruction fine-tuning}
To overcome the challenge of catastrophic forgetting in multi-task semantic communication, we propose a multi-task fine-tuning method based on the task instruction following. By using natural language instructions to uniformly describe the objectives, inputs, and outputs of different tasks, we fine-tune the MLLM to enhance its multi-task learning capability and generalization ability. The design of the instructions emphasizes task clarity, semantic consistency, and scalability, enabling the model to understand the latent relationships between tasks. This approach significantly improves the robustness of semantic communication in multi-task scenarios, allowing it to dynamically adapt to task demands while reducing interference between tasks.

\subsubsection{Multi-user semantic sharing transmission}
To achieve efficient collaboration among multi-user communication, we establish a shared semantic space between users, where the semantic information of different users is compared and classified. Semantic information that is identical or similar is merged to form public information. On the sender side, we transmit the public and private information separately, making more efficient use of spectrum resources. On the receiver side, users reconstruct the original data by combining the public information with their respective private information, thus enhancing the overall semantic transmission efficiency of the system.

%\subsection{Organizational structure}
The remainder of this paper is organized as follows: Section \Rmnum{2} introduces the tailored MLLM for semantic communication systems; Section \Rmnum{3} presents a detailed introduction to the proposed M4SC; Section \Rmnum{4} details the experimental setup and results; %Section \Rmnum{5} discusses open issues; 
and Section \Rmnum{5} provides a summary and discussion of the paper.

\section{MLLM for Semantic Communication Systems}
In recent years, breakthroughs in LLMs for natural language understanding and text generation have injected new vitality into semantic communication. By harnessing the powerful semantic reasoning and generative capabilities of LLMs, semantic communication can achieve higher-level information transmission. MLLMs are an extension of LLMs, developed to handle and understand data from multiple modalities, including text, images, and video. MLLMs inherit several prominent features of LLMs, such as zero-shot learning, In-Context Learning (ICL), Chain-of-Thought (CoT), and instruction following \cite{jiang2024large2}. Notable examples of MLLMs include GPT-4o, Claude 3, and LLaVA. %\cite{liu2024visual}. 
By introducing cross-modal semantic analysis capabilities, MLLMs can map semantic information from different modalities into a unified semantic space, further enhancing the depth and breadth of semantic expression, and becoming a key driving force in the future development of semantic communications.

\subsection{Advantages of MLLM in semantic communication}

\subsubsection{High-precision multi-modal alignment}
MLLMs, through large-scale pre-training, map the semantic information of data from different modalities into a shared semantic space. In this semantic space, common features between modalities can be captured and associated. For example, the description of a ``red apple" in text can be naturally aligned with the corresponding visual semantic information in an image. This unified semantic space provides a solid foundation for the coordination and fusion of multi-modal data, making it one of the core technologies for multi-modal alignment. To achieve high-precision alignment, MLLMs typically employ a cross-modal projector to transform visual semantic information into textual semantic representations that are compatible with LLMs. By training the cross-modal projector, MLLMs can learn the mapping rules between text and visual semantic information. This deep cross-modal understanding enables MLLMs to efficiently perform semantic encoding and decoding between multi-modal data in semantic communications.

\subsubsection{Superior robustness in multi-task learning}
The core challenge in multi-task semantic communication is enabling the system to quickly adapt to the requirements of different tasks. MLLMs, with their powerful generalization capabilities and flexibility, provide effective support for overcoming this challenge. Through large-scale pre-training and instruction fine-tuning, MLLMs exhibit exceptional zero-shot and few-shot learning abilities. Consequently, MLLMs inherently possess a fundamental understanding of various tasks and can rapidly adapt to new task requirements under zero-shot or few-shot conditions using advanced learning methods such as ICL, CoT, and Retrieval-Augmented Generation (RAG). This ability significantly reduces the training and updating costs of semantic communication systems.

\subsubsection{High-efficiency multi-user semantic transmission}
In multi-user semantic communication, precise semantic expression and comparison among users are key to achieving efficient communication. To achieve this, the system needs to compare and extract similar semantic information from a shared semantic space across multiple users, minimizing information redundancy while ensuring the integrity and consistency of user semantics. In MLLM-based semantic communications, the system can leverage their powerful semantic understanding and encoding capabilities to accurately identify both the commonalities and differences in the semantic information of different users, thereby improving overall communication efficiency.

\subsection{The tailored MLLM}
Due to the advantages of MLLMs, we design a tailored MLLM for semantic communications. First, it is a lightweight MLLM architecture that supports semantic communication for both image and text modalities on resource-constrained devices. Second, it is an extension of existing LLMs, requiring minimal computational resources for multi-modal training, which facilitates subsequent joint encoding for semantic communications. The structure of the proposed MLLM is shown in Fig. \ref{fig:arch}.

\subsubsection{Vision semantic encoder}
The vision semantic encoder in the MLLM is primarily responsible for converting image data into visual semantic information. It employs visual processing architectures such as Convolutional Neural Networks (CNNs) or Vision Transformers (ViT) to extract visual features from images through convolutional kernels or self-attention mechanisms, and then convert these features into precise visual semantic information. In the proposed MLLM, we use the pre-trained Siglip ViT as the vision semantic encoder \cite{zhai2023sigmoid}. Siglip, trained through self-supervised contrastive learning on both text and image data, enables the ViT structure to efficiently and accurately extract semantic information from images.

\subsubsection{Cross-modal semantic projector}
The cross-modal semantic projector in the MLLM is responsible for aligning visual semantic information with the textual semantic space of the LLM. It transforms the visual semantic information extracted by the vision semantic encoder into a vector compatible with the textual semantic information, using a nonlinear transformation. This process allows the image and text to be compared and fused in the same semantic space, thereby achieving cross-modal information integration and alignment. In the proposed MLLM, we use the KAN as the cross-modal semantic projector. KAN utilizes a set of learnable spline functions as activation functions, enhancing the model's ability to capture subtle patterns in the semantic space, enabling smoother and more precise alignment of visual semantic information with the textual semantic information \cite{liu2024kan}.

\subsubsection{LLM-based semantic encoder}
The text embedding layer of the LLM converts text data into textual semantic information, which is then fused with the visual semantic information from the cross-modal semantic projector and input into the LLM's encoder. This allows the LLM to not only understand the textual content but also to combine visual semantic information for more accurate reasoning and generation. The LLM-based semantic encoder adopts a Transformer encoder architecture, utilizing stacked self-attention layers and fully connected layers to perform high-quality multimodal semantic encoding.

\subsubsection{LLM-based semantic decoder}
The LLM-based semantic decoder employs a linear layer followed by a softmax layer to convert the multi-modal semantic information encoded by the semantic encoder into a probability distribution for the generated content. It then selects the most probable content for semantic decoding, producing the final output. In the proposed MLLM, we use Google's Gemma2-2b-it \cite{team2024gemma} as the LLM-based semantic encoder and decoder. With only 2 billion parameters, Gemma2-2b-it offers a lightweight design and advantages for multitask generation. It operates efficiently in resource-constrained semantic communication systems and performs excellently in various text generation tasks such as question answering, text summarization, and logical reasoning.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.5cm]{fig/MLLM-arch.png}
	\caption{The structure of the proposed MLLM.}
	\label{fig:arch}
\end{figure}

\section{MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communications}
\begin{figure*}[htpb]
	\centering
	\includegraphics[width=17cm]{fig/ours.png}
	\caption{The structure of the proposed M4SC.}
	\label{fig:ours}
\end{figure*}
Based on the tailored MLLM, we design a multi-modal, multi-task, and multi-user semantic communication system, as illustrated in Fig. \ref{fig:ours}. In M4SC, we consider a multi-user semantic communication scenario from the Base Station (BS) to the users. As a specific example, at the BS, two users perform image captioning (Task 1) and Visual Question Answering (VQA) (Task 2), both of which are multimodal tasks, providing their respective image and text inputs. The multimodal encoder, consisting of a text embedding layer, a vision semantic encoder, and a cross-modal semantic projector, simultaneously aligns the inputs from the image and text modalities, which are then semantically encoded by the LLM-based semantic encoder. Subsequently, the semantic comparator divides the semantic information of the two users into public semantic information (e.g., the semantic information represented by "Jerry" in the two images) and private semantic information (e.g., the semantic information represented by "yellow duck" in Image 1 and "female mouse" in Image 2). The public and private semantic information is then encoded and transmitted by the public channel encoder and the user-specific channel encoder, respectively. On the receiver side, users reorganize the received public and private semantic information and sequentially use the channel decoder and the LLM-based semantic decoder to obtain the output, thus completing the semantic communication tasks for each user.

\subsection{KAN-based cross-modal alignment}
To achieve more precise multi-modal understanding and cross-modal alignment, we use the KAN as the cross-modal semantic projector in the M4SC. Based on the Kolmogorov-Arnold representation theorem, KAN offers an efficient and accurate neural network architecture for representing nonlinear functions. It is capable of more precisely capturing the multi-dimensional semantic information of input data while maintaining computational efficiency and ensuring the accuracy of the output results. The workflow of KAN is as follows:

\subsubsection{Activation and decomposition Stage}
In this stage, the input signals are passed through a set of learnable activation functions, each of which is independently adjustable. These activation functions, constructed using spline functions, transform the complex multi-modal semantic information into simple polynomial combinations. The activation values for each input node are computed multiple times and sequentially aggregated in the order of the activation functions.

\subsubsection{Aggregation and Mapping Stage}
After the activation values are aggregated, the sum of these values serves as the output for the current layer. The output node count corresponds to the number of times each input node is activated. This sequential aggregation process enables the network to establish more intricate mapping relationships between layers, allowing for more accurate representation and alignment of multi-modal data, thus capturing deeper associations between different modalities. The network’s hierarchical structure is further enriched and optimized through iterative activations, improving overall performance in complex data mapping tasks.




\subsection{Multi-task instruction fine-tuning}
To enhance the performance and generalization ability of the multi-task semantic communication, we propose a multi-task fine-tuning method based on the task instruction following. This approach unifies the description of the objectives and input-output formats for different tasks through natural language instructions and employs a multi-task mixed training strategy to optimize the model’s generalization capability. The model can thus understand the potential relationships between tasks, leading to an overall improvement in multi-task performance \cite{wei2021finetuned}. %The multi-task instruction fine-tuning method includes the following two steps:
%\subsubsection{Clarify task types}
%The first step in multi-task instruction fine-tuning is to clarify the types of tasks. Different types of tasks have their unique objectives, so appropriate classification is necessary. Task classification aims to help the model understand the specific requirements of each task, enabling it to provide the most suitable responses. Task classification is typically based on the nature and goals of the tasks. For example, tasks can be classified into categories such as text generation, classification, translation, and question answering, depending on the task objectives.

%\subsubsection{Construct task instructions}
%Each task has its specific data structure and processing methods. Therefore, the structure and content of task instructions should be adjusted according to the nature of the task.
A complete task instruction consists of the following components: Instruction, Input, Output, and Metadata. The Instruction clearly describes the content and objective of the task; the Input provides the contextual information for the instruction; the Output provides a high-quality result based on the instruction and input that meets the task requirements; and the Metadata (optional) includes additional details such as task notes. These elements work together to enable the M4SC to accurately understand and perform semantic communication tasks.


For example, in Fig. \ref{fig:ours}, the task instruction of the image caption is as follows: \emph{\textbf{Instruction}: ``Please carefully observe the provided image and identify the main elements, including the scene (e.g., indoor, outdoor, city, nature, etc.), key objects (e.g., animals, people, buildings, tools, etc.), and any actions or activities (e.g., running, eating, reading, etc.). Based on this information, generate a natural language description that covers the key details of the image. Ensure the description is fluent, grammatically correct, and avoids excessive speculation, only describing what can be clearly observed in the image." \textbf{Input Image}: \textless Image 1\textgreater \ \textbf{Input Text}: ``Image caption." \textbf{Output}: ``The image features Jerry standing at the entrance of an arched door, looking up at a small, cheerful yellow duck."}

The task instruction of VQA is as follows: \emph{\textbf{Instruction}: ``Please observe the provided image and read the question carefully to understand its specific requirements. Based on the content of the image, answer the question, ensuring that the answer is based on clearly observable information. If the question involves specific details (e.g., quantity, color, location), provide precise answers. If the question cannot be answered based on the image, respond with: `Cannot obtain an answer from the image.' Finally, provide a concise and clear answer that fully satisfies the semantic requirement of the question." \textbf{Input Image}: \textless Image 2\textgreater \   \textbf{Input Text}: ``What is Jerry doing?" \textbf{Output}: ``Jerry is looking at the female mouse with a friendly expression."} 

\subsection{Semantic shared transmission for multi-user}
Due to the potential high similarity in the metadata of data across different users at the semantic level, this phenomenon provides a theoretical foundation for the comparison and sharing of semantic information between users. In M4SC, we consider a multi-user semantic communication scenario from the BS to users, where similar or identical semantic information vectors from different users are merged to form a public semantic signal for transmission. This approach makes more efficient use of spectrum resources, improving the overall semantic transmission efficiency. The process of semantic shared transmission is as follows:

%At the base station, we establish a multi-user shared semantic space and introduce a semantic comparator to analyze, identify, and extract public and private semantic information from the soured data of different users. In the wireless transmission scenario, this process effectively reduces the amount of information that needs to be ultimately transmitted, optimizing transmission efficiency.

\subsubsection{Shared semantic space mapping}
At the BS, we convert each user's input source data into multimodal semantic information and map them into a unified semantic space using the cross-modal semantic projector.

\subsubsection{Semantic comparison}
In the semantic space, we perform a detailed, token-level comparison of the semantic information from different users, calculating their similarity. The statistical characteristics of the semantic information vectors (e.g., mean, variance) are compared to a threshold to determine whether there are identical or similar semantics \cite{zhang2023model}.

\subsubsection{Public and private semantic partitioning}
Identical or similar semantics between users are merged to form public semantic information, while the unique, personalized semantic information (private semantic information) of each user is retained.

\subsubsection{Semantic shared transmission}
During wireless transmission, the encoded public semantic information is sent through a dedicated public channel and broadcast to all users. The encoded private semantic information is transmitted to the corresponding receiving users through their assigned channels.

\subsubsection{Semantic reconstruction}
Each receiving user combines the private semantic information with the public semantic information they have received, then performs channel decoding followed by semantic decoding to complete the semantic communication task.

\subsection{Training process of the semantic communication system}

To further optimize the performance of the M4SC, we customize a three-phase training process that includes two phases of MLLM training and a third phase for joint encoding of the semantic communication system, as shown in Fig. \ref{fig:train}.

\subsubsection{Multi-modal alignment training}
The first phase focuses on training the KAN-based cross-modal semantic projector. Specifically, the goal of this phase is to train an efficient tokenizer that adapts to the visual modality for a frozen LLM. This tokenizer is designed to map image semantic information into textual semantic information compatible with the LLM, thereby enhancing the model's understanding of image modality inputs. This training phase ensures that the cross-modal semantic projector can efficiently adapt to the image data required in multi-modal semantic communication without needing to train the LLM.

\subsubsection{Multi-task instruction fine-tuning}
In the second phase, we no longer freeze the LLM but continue training both KAN and LLM parameters. The purpose of this phase is to strengthen the system's multi-modal understanding capabilities while improving the model’s ability to follow instructions for different tasks. This joint optimization approach not only ensures the accuracy of MLLM in understanding multi-modal tasks but also enhances its robustness when executing multiple tasks simultaneously. Additionally, to accelerate computation and reduce resource consumption, we apply Low-Rank Adaptation (LoRA) for efficient fine-tuning of the LLM.

\subsubsection{Joint KAN-LLM-channel encoding}
In the third phase, we further integrate the optimized KAN and LLM with the channel encoder-decoder for joint training. The focus of this phase is to improve the overall performance of the semantic communication system. In the joint KAN-LLM-channel encoding process, we allow the parameters of KAN and the channel encoder-decoder to undergo full updates to accommodate the complex channel conditions. We also use LoRA to fine-tune the LLM. This strategy significantly reduces training costs while maintaining model performance, making it more suitable for joint encoding training in M4SC.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.5cm]{fig/train.png}
	\caption{The three-stage training process of M4SC.}
	\label{fig:train}
\end{figure}

\section{Simulation Results}

\subsection{Experimental setup}
The specific experimental setup is as follows: The vision semantic encoder of the MLLM is a pre-trained Siglip ViT, with 0.3 billion parameters. The cross-modal semantic projector is a two-layer KAN, with 76 million parameters. The LLM used in the MLLM is Gemma2-2b-it, with 2 billion parameters. The channel encoder consists of a linear layer, with an input feature dimension of 2304 and an output feature dimension of 512. To maintain information consistency, the channel decoder adopts a structure that is the inverse of the channel encoder. The wireless channel model adopts similar settings to those presented in \cite{jiang2024large}.

%Regarding the training strategy, the AdamW optimizer and cosine learning rate scheduling are used. %The initial learning rates for multi-modal alignment training and multi-task instruction following training are set to 1e-3 and 2e-5, respectively, to ensure training stability. 


In multi-modal alignment training, 1.2 million image-text description pairs are used, with datasets including LLaVA Images and ALLaVA Caption \cite{chen2024allava}. Multi-task instruction fine-tuning uses 1.5 million single-turn or multi-turn image-text dialogue pairs, with datasets including COCO train2017, GQA, OCR-VQA, TextVQA, VisualGenome part1/part2, ShareGPT4V-100K, LAION GPT4V, ALLaVA Instruction, DocVQA, ChartQA, DVQA, and AI2D \cite{li2024mini}. The joint KAN-LLM-channel encoding uses the CLEVR dataset \cite{johnson2017clevr}. Moreover, the AdamW optimizer and cosine learning rate scheduling are used in the training process.

%, which includes a training set of 70,000 images and 699,989 questions, as well as a test set with 15,000 images and 149,991 questions
%In the joint KAN-LLM-channel encoding phase, the learning rate is initialized to 2e-4, and the rank of the LoRA module is 8, with a scaling factor of 8 and a dropout rate of 0.1. 

Training and testing are conducted on a server equipped with an Intel Xeon CPU (2.6 GHz, 1007 GB RAM) and 8*NVIDIA A100 GPUs (80 GB SGRAM), with the operating environment being Python 3.10. The training framework used is PyTorch 2.0.1, and the training strategy employs DeepSpeed ZeRO-2.

\subsection{Evaluation results}
\subsubsection{Multi-modal and multi-task performance evaluation}
To assess the multi-modal performance of the M4SC, we compare it with DeepSC-VQA \cite{xie2022task} on the VQA task. The dataset used for the VQA task is CLEVR\cite{johnson2017clevr}. To evaluate the multi-task performance of the M4SC, we compared it with both DeepSC-VQA and DeepSC \cite{xie2021deep} across various downstream tasks, including VQA and text classification. The dataset used for text classification is the IMDB dataset. The evaluation metric for both experiments is accuracy.

The performance comparison results for multi-modal and multi-task tasks are shown in Fig. \ref{fig:p1}. It can be observed that, whether for the multi-modal task of VQA or the unimodal task of text classification, M4SC consistently outperforms the semantic communication systems designed for single tasks. This demonstrates that the powerful multi-modal alignment and multi-task learning capabilities of MLLM enable it to excel in multi-modal and multi-task scenarios. Specifically, MLLM effectively maps data from different modalities into a unified semantic space and flexibly adapts to various semantic task requirements through multi-task instruction fine-tuning.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.5cm]{fig/p1.png}
	\caption{Comparison of multi-modal and multi-task performance.}
	\label{fig:p1}
\end{figure}

\subsubsection{Multi-user performance evaluation}
To evaluate the advantages of M4SC in multi-user communication, we compare the size of the transmitted data for the VQA task between M4SC and DeepSC-VQA under different numbers of users using CLEVR dataset \cite{johnson2017clevr}, as shown in Fig. \ref{fig:p2}. %The data used for this evaluation comes from the.

As shown in Fig. \ref{fig:p2}, with the increase in the number of users, M4SC demonstrates significant advantages in terms of the size of the transmitted data. Compared to the DeepSC-VQA, the proposed M4SC effectively reduces redundant data transmission through its semantic sharing mechanism. Specifically, when the number of users is small, there is little difference in data transmission between the two systems. However, as the number of users increases, the M4SC significantly reduces the overall transmission data size by extracting and transmitting public semantic information. This result indicates that the proposed semantic sharing transmission mechanism has a remarkable bandwidth optimization effect in multi-user scenarios, effectively improving the overall efficiency of the system.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=8.5cm]{fig/p2.png}
	\caption{Comparison of multi-user performance.}
	\label{fig:p2}
\end{figure}

\begin{comment}
\section{Open Issues}

\subsubsection{Resource constraints in edge deployment}
Although the MLLM-based multi-modal, multi-task, and multi-user semantic communication system offers significant advantages in applications, large-scale practical deployment may face challenges related to computational resource limitations and latency issues. This is especially true in resource-constrained environments such as mobile terminals or edge devices, where ensuring real-time performance and efficiency becomes a critical challenge. To address this issue, accelerating the inference of the semantic communication model is essential. For instance, model compression, knowledge distillation, and quantization techniques can be employed to reduce computational resource consumption while maintaining system accuracy. 


\subsubsection{Adaptability enhancement in dynamic environments} 
In highly dynamic communication environments, channel characteristics such as bandwidth fluctuations, noise interference, and signal attenuation often lead to unstable communication quality, which in turn affects the overall system performance. To address these challenges, optimizing the joint KAN-LLM-channel encoding strategy to enhance its adaptability is crucial. By integrating intelligent channel estimation techniques, the encoding scheme can be dynamically adjusted in real-time to minimize the impact of channel uncertainties. In this process, deep learning models can be used to predict channel states and adjust the joint encoding strategy, enabling the system to flexibly adapt to various complex environmental changes. Additionally, introducing adaptive algorithms such as reinforcement learning can facilitate self-learning and self-improvement of the system, thereby achieving more efficient semantic communication.
 

\subsubsection{Scalability of cross-modal semantic space} 
Current semantic communication primarily relies on traditional modality data, such as text and images. However, with the advancement of technology, emerging modalities such as video and 3D data are gradually becoming the focus of research. To effectively expand support for more emerging modality data and ensure its consistency with existing modalities in the semantic space, it is necessary to design multi-modal fusion mechanisms. This mechanism can maintain the independence of each modality while ensuring the uniformity of the semantic space through efficient feature fusion and alignment techniques. For example, employing models like Mamba or transformer models for cross-domain mapping and fusion between modalities can ensure that the information from emerging modalities is effectively aligned and interacted within a unified semantic space.
\end{comment}

\section{Conclusion}
This study presented a novel MLLM designed for semantic communication and introduced an MLLM-based multi-modal, multi-task and multi-user semantic communication system, named M4SC. The M4SC achieved precise alignment of multi-modal data in a shared semantic space by incorporating KAN as a cross-modal semantic projector, significantly enhancing the accuracy of multi-modal semantic representation. 
Furthermore, a multi-task instruction fine-tuning method was proposed, where unified instructions provide natural language descriptions for different tasks, thus improving the M4SC's multi-task learning. In multi-user scenarios, the M4SC employed a semantic sharing mechanism to separate the transmission of public and private semantic information, further enhancing the overall semantic transmission efficiency. Simulation experiments demonstrated that M4SC not only excels in multi-modal and multi-task semantic communication but also exhibits remarkable bandwidth optimization in multi-user semantic communication.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%




% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill                                                                                    . When the MEC number increases,

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}






\bibliographystyle{ieeetran}
\bibliography{bare_jrnl_bobo}
\section*{Biographies}
\textbf{Feibo Jiang} (jiangfb@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. He is currently an Associate Professor at Hunan Normal University, China.



\textbf{Siwei Tu} (tusiwei@hunnu.edu.cn) is currently pursuing the master’s degree with Hunan Normal University, China. 

\textbf{Li Dong} (Dlj2017@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. She is currently an Associate Professor at Hunan University of Technology and Business, China.

\textbf{Kezhi Wang} (Kezhi.Wang@brunel.ac.uk) received Ph.D. degree from University of Warwick, U.K. in 2015. Currently he is a Senior Lecturer with the Department of Computer Science, Brunel University London, U.K.

\textbf{Kun Yang} (kunyang@essex.ac.uk) received his PhD from the Department of Electronic \& Electrical Engineering of University College London (UCL), U.K. He is currently a Chair Professor in the School of Computer Science \& Electronic Engineering, University of Essex, U.K.

\textbf{Cunhua Pan} (cpan@seu.edu.cn) received Ph.D. degrees from Southeast University, China, in 2015. 
He is a full professor in Southeast University, China. 

%\textbf{Dusit Niyato} (dniyato@ntu.edu.sg) received the Ph.D. degree in electrical and computer engineering from the University of Manitoba in Canada in 2008. He is a professor in the School of Computer Science and Engineering, Nanyang Technological University, 639798 Singapore.

%\textbf{Octavia A. Dobre} (odobre@mun.ca) is a professor and Canada Research Chair Tier 1 at Memorial University, Canada. She is a Fellow of the Canadian Academy of Engineering, Fellow of the Engineering Institute of Canada, and elected member of the European Academy of Sciences and Arts. She is the Director of Journals of the IEEE Communications Society.
%\textbf{Li Dong} (Dlj2017@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. She is currently an Associate Professor at Hunan University of Technology and Business, China.

%\textbf{Feibo Jiang} (jiangfb@hunnu.edu.cn) received Ph.D. degree from the Central South University, China. He is currently an Associate Professor at Hunan Normal University, China.

%\textbf{Yubo Peng} (pengyubo@hunnu.edu.cn) is currently pursuing the master’s degree with Hunan Normal University, China. 

%\textbf{Kezhi Wang} (Kezhi.Wang@brunel.ac.uk) received Ph.D. degree from University of Warwick, U.K. in 2015. Currently he is a Senior Lecturer with the Department of Computer Science, Brunel University London, U.K.


%\textbf{Kun Yang} (kunyang@essex.ac.uk) received his PhD from the Department of Electronic \& Electrical Engineering of University College London (UCL), UK. He is currently a Chair Professor in the School of Computer Science \& Electronic Engineering, University of Essex, UK. He is also an affiliated professor at UESTC, China. 


%\textbf{Cunhua Pan} (cpan@seu.edu.cn) received Ph.D. degrees from Southeast University, China, in 2015. %From 2015 to 2016, he was a Research Associate at the University of Kent, U.K. 
%He held a post-doctoral position at Queen Mary University of London, U.K., from 2016 and 2019. From 2019 to 2021, he was a Lecturer in the same university. From 2021, he is a full professor in Southeast University, China. 
%He serves as Lead Guest Editor of IEEE JSTSP special issue on RIS, Editor of IEEE CL, IEEE WCL, and IEEE ACCESS.

%\textbf{Robert Schober} (robert.schober@fau.de) is an Alexander von Humboldt Professor and the Institute for Digital Communications (IDC)in Friedrich-Alexander University (FAU) Erlangen-Nürnberg, Erlangen, Germany.



\end{document}





