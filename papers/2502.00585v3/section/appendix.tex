\section{Pseudocode for Synvolution and Kernelution}
\begin{algorithm}[!h]
\caption{PyTorch-like pseudocode for Parallel Scan.}
\label{alg:pscan}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
	backgroundcolor=\color{white},
	basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
	columns=fullflexible,
	breaklines=true,
	captionpos=b,
	commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
	keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# b: batch size, n: length, d: feature dimension

class PScan(torch.autograd.Function):
    @staticmethod
    def expand_(A, X):
        if A.size(1) == 1:
            return
        T = 2 * (A.size(1) // 2)
        Aa = A[:, :T].view(A.size(0), T//2, 2, -1)
        Xa = X[:, :T].view(X.size(0), T//2, 2, -1)
        Xa[:, :, 1].add_(Aa[:, :, 1] * Xa[:, :, 0])
        Aa[:, :, 1].mul_(Aa[:, :, 0])
        PScan.expand_(Aa[:, :, 1], Xa[:, :, 1])
        Xa[:, 1:, 0].add_(Aa[:, 1:, 0] * Xa[:, :-1, 1])
        Aa[:, 1:, 0].mul_(Aa[:, :-1, 1])
        if T < A.size(1):
            X[:, -1].add_(A[:, -1] * X[:, -2])
            A[:, -1].mul_(A[:, -2])

    @staticmethod
    def acc_rev_(A, X):
        if X.size(1) == 1:
            return
        T = 2 * (X.size(1) // 2)
        Aa = A[:, -T:].view(A.size(0), T//2, 2, -1)
        Xa = X[:, -T:].view(X.size(0), T//2, 2, -1)
        Xa[:, :, 0].add_(Aa[:, :, 1].conj() * Xa[:, :, 1])
        B = Aa[:, :, 0].clone()
        B[:, 1:].mul_(Aa[:, :-1, 1].conj())
        PScan.acc_rev_(B, Xa[:, :, 0])
        Xa[:, :-1, 1].add_(Aa[:, 1:, 0].conj() * Xa[:, 1:, 0])
        if T < A.size(1):
            X[:, 0].add_(A[:, 1].conj() * X[:, 1])

    @staticmethod
    def forward(ctx, A, X, Y_init):
        ctx.A = A[:, :, None].clone()
        ctx.Y_init = Y_init[:, None, :].clone()
        ctx.A_star = ctx.A.clone()
        ctx.X_star = X.clone()
        PScan.expand_(ctx.A_star, ctx.X_star)
        return ctx.A_star * ctx.Y_init + ctx.X_star

    @staticmethod
    def backward(ctx, grad_output):
        U = grad_output * ctx.A_star.conj()
        A = ctx.A.clone()
        R = grad_output.clone()
        PScan.acc_rev_(A, R)
        Q = ctx.Y_init.expand_as(ctx.X_star).clone()
        Q[:, 1:].mul_(ctx.A_star[:, :-1].conj()).add_(ctx.X_star[:, :-1])
        grad_A = (Q.conj() * R).sum(-1)
        return grad_A, R, U.sum(dim=1)

pscan = PScan.apply
\end{lstlisting}
\end{algorithm}

Algorithm~\ref{alg:fast_order-1_dhhp_pytorch} presents the PyTorch-like pseudocode for $1$-DHHP as a discrete unitary transform and its inverse transform, while Algorithm~\ref{alg:kernelution} demonstrates the PyTorch-like pseudocode for Synvolution and Kernelution.

\begin{algorithm}[!h]
\caption{PyTorch-like pseudocode for $1$-DHHP as a discrete unitary transform and its inverse transform.}
\label{alg:fast_order-1_dhhp_pytorch}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
	backgroundcolor=\color{white},
	basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
	columns=fullflexible,
	breaklines=true,
	captionpos=b,
	commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
	keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# b: batch size, n: length, d: feature dimension
# m: permutation mapping dimension

def dhhp_trans(transform=True, x, g_l_ii, g_l_ij, g_l_ji, g_l_jj, g_u_ii, g_u_ij, g_u_ji, g_u_jj, diag):
    y = torch.zeros_like(x)
    z = torch.zeros_like(x)

    if transform is True:
        x = x.reshape(b, n // m, m, d).transpose(1, 2).reshape(b, n, d)
    else:
        x = torch.einsum('bn,bnd->bnd', diag, x)
    
    x_, y = torch.zeros_like(x), torch.zeros_like(x)
    x_[:, :-1, :] = g_u_ii.unsqueeze(-1) * x[:, :-1, :]
    x_ = x_.flip(1)
    g_u_ij = g_u_ij.flip(1)
    g_u_ij = torch.cat([g_u_ij, torch.zeros_like(g_u_ij[:, :1])], dim=1)
    p_u = torch.zeros_like(g_u_ij)
    p_u[:, 1:] = g_u_ij[:, :-1].clone()
    h_u_init = x_[:, 0, :].clone()
    h_u = pscan(p_u, x_, h_u_init)
    h_u = h_u.flip(1)
    y[:, 1:, :] = g_u_ji.unsqueeze(-1) * x[:, :-1, :] + g_u_jj.unsqueeze(-1) * h_u[:, 1:, :]
    y[:, 0, :] = h_u[:, 0, :]

    y_, z = torch.zeros_like(y), torch.zeros_like(y)
    y_[:, 1:, :] = g_l_jj.unsqueeze(-1) * y[:, 1:, :]
    g_l_ji = torch.cat([g_l_ji, torch.zeros_like(g_l_ji[:, :1])], dim=1)
    p_l = torch.zeros_like(g_l_ji)
    p_l[:, 1:] = g_l_ji[:, :-1].clone()
    h_l_init = y_[:, 0, :].clone()
    h_l = pscan(p_l, y_, h_l_init)
    z[:, :-1, :] = g_l_ii.unsqueeze(-1) * h_l[:, :-1, :] + g_l_ij.unsqueeze(-1) * y[:, 1:, :]
    z[:, n-1, :] = h_l[:, n-1, :]

    if transform is True:
        z = torch.einsum('bn,bnd->bnd', diag, z)
    else:
        z = z.reshape(b, m, n // m, d).transpose(1, 2).reshape(b, n, d)

    return z

def inverse_dhhp_trans(transform=False, x, g_l_ii_conj_trs, g_l_ij_conj_trs, g_l_ji_conj_trs, g_l_jj_conj_trs, g_u_ii_conj_trs, g_u_ij_conj_trs, g_u_ji_conj_trs, g_u_jj_conj_trs, diag_conj_trs):
    return dhhp_trans(transform, x, g_u_ii_conj_trs, g_u_ij_conj_trs, g_u_ji_conj_trs, g_u_jj_conj_trs, g_l_ii_conj_trs, g_l_ij_conj_trs, g_l_ji_conj_trs, g_l_jj_conj_trs, diag_conj_trs)
\end{lstlisting}
\end{algorithm}

\begin{algorithm}[!h]
\caption{PyTorch-like pseudocode for Synvolution and Kernelution.}
\label{alg:kernelution}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
	backgroundcolor=\color{white},
	basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
	columns=fullflexible,
	breaklines=true,
	captionpos=b,
	commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
	keywordstyle=\fontsize{7.2pt}{7.2pt},
}
\begin{lstlisting}[language=python]
# b: batch size, n: length, d: feature dimension

def kernel_polynomial_method(seq, K, g, mu):
    Tx_0 = torch.ones_like(seq) # b, n
    cheb_gibbs = Tx_0 * mu[0]
    if K == 0:
        return cheb_gibbs
    
    Tx_1 = seq
    cheb_gibbs = cheb_gibbs + Tx_1 * g[1] * mu[1]
    if K == 1:
        return cheb_gibbs

    if K >= 2:
        for k in range(2, K+1):
        Tx_2 = 2 * seq * Tx_1 - Tx_0
        cheb_gibbs = cheb_gibbs + Tx_2 * g[k] * mu[k]
        Tx_0, Tx_1 = Tx_1, Tx_2

    return cheb_gibbs

def givens_rot_para(alpha, beta, gamma):
    g_ii = torch.exp(-1j * (alpha + beta) / 2) * torch.cos(gamma / 2)
    g_ij = -torch.exp(1j * (alpha - beta) / 2) * torch.sin(gamma / 2)
    g_ji = torch.exp(-1j * (alpha - beta) / 2) * torch.sin(gamma / 2)
    g_jj = torch.exp(1j * (alpha + beta) / 2) * torch.cos(gamma / 2)

    return g_ii, g_ij, g_ji, g_jj

def givens_rot_para_conj_trs(g_ii, g_ij, g_ji, g_jj):
    g_ii_conj_trs = g_jj
    g_ij_conj_trs = -g_ij
    g_ji_conj_trs = -g_ji
    g_jj_conj_trs = g_ii

    return g_ii_conj_trs, g_ij_conj_trs, g_ji_conj_trs, g_jj_conj_trs

def kernelution(eigenvalue, K, g, mu, x, alpha_l, beta_l, gamma_l, alpha_u, beta_u, gamma_u, theta):
    if enable_kernelution is True:
        eigenvalue = kernel_polynomial_method(eigenvalue, K, g, mu)

    g_l_ii, g_l_ij, g_l_ji, g_l_jj = givens_rot_para(alpha_l, beta_l, gamma_l)
    g_u_ii, g_u_ij, g_u_ji, g_u_jj = givens_rot_para(alpha_u, beta_u, gamma_u)
    diag = torch.exp(2j * math.pi * theta)
    
    g_l_ii_conj_trs, g_l_ij_conj_trs, g_l_ji_conj_trs, g_l_jj_conj_trs = givens_rot_para_conj_trs(g_l_ii, g_l_ij, g_l_ji, g_l_jj)
    g_u_ii_conj_trs, g_u_ij_conj_trs, g_u_ji_conj_trs, g_u_jj_conj_trs = givens_rot_para_conj_trs(g_u_ii, g_u_ij, g_u_ji, g_u_jj)
    diag_conj_trs = diag.conj()

    x = dhhp_trans(True, x, g_l_ii, g_l_ij, g_l_ji, g_l_jj, g_u_ii, g_u_ij, g_u_ji, g_u_jj, diag)
    x = torch.einsum('bn,bnd->bnd', eigenvalue, x)
    x = inverse_dhhp_trans(False, x, g_l_ii_conj_trs, g_l_ij_conj_trs, g_l_ji_conj_trs, g_l_jj_conj_trs, g_u_ii_conj_trs, g_u_ij_conj_trs, g_u_ji_conj_trs, g_u_jj_conj_trs, diag_conj_trs)
    
    return x
\end{lstlisting}
\end{algorithm}

\section{Synvolution and Other Attention Mechanisms}
We compare Synvolution with common attention and attention-like mechanisms in Table~\ref{tab:attention}. Notice, we do not assume the relation between the length size $N$ and embedded size $D$ for input signal $\mathbf{X} \in \mathbb{R}^{N \times D}$. Sparse attention, which reduces computation by only attending to a subset of tokens, includes \citep{child2019generating,NEURIPS2020_c8512d14}. Following the categorization in \citep{cao2021choose}, linear attention can be classified into Fourier-type and Galerkin-type. Fourier-type attention, which compute query-key pairs first, includes \citep{Shen_2021_WACV,cao2021choose,zhang-etal-2021-sparse,Koohpayegani_2024_WACV}. Galerkin-type attention, compute key-value pairs first, includes \citep{pmlr-v119-katharopoulos20a,cao2021choose,Koohpayegani_2024_WACV}. Kernel attention, which leverages kernel methods to approximate the attention operation, includes \citep{tsai-etal-2019-transformer,choromanski2021rethinking,NEURIPS2021_10a7cdd9,xiong2021Nystromformer}. Synthetic dense attention includes \citep{pmlr-v139-tay21a}. Chord attention includes \citep{9878955}. 

While sparse attention theoretically offers lower time complexity, its practical implementation remains challenging. When implemented directly in PyTorch without CUDA optimization~\citep{10.1145/1365490.1365500} or other GPU frameworks, matrix multiplication between sparse and dense tensors paradoxically consumes more GPU memory than multiplication between two dense tensors of equivalent size.

Linear attention mechanisms, whether Fourier-type or Galerkin-type, achieve lower time complexity compared to scaled dot-product attention. However, their attention matrices remain low-rank, similar to self-attention. For kernel attention mechanisms, their computational characteristics must be analyzed on a case-by-case basis due to implementation variations.

Synthesizer demonstrates potential through synthetic dense attention, though its time and space complexity remains equivalent to scaled dot-product attention. Chord attention, derived from the inverse process of Chord factorization~\citep{KHALITOV2022160}, lacks CUDA optimization in its implementation. This leads to higher GPU memory consumption than linear attention but lower than scaled dot-product attention, similar to the challenges faced by sparse attention implementations.

Our proposed method, Synvolution, is an attention-like mechanism that leverages a space-for-time approach. Its attention matrix possesses several advantageous properties: it is learnable, full-rank, dense, and unitary. These characteristics enable Synvolution to achieve effectiveness comparable to self-attention across diverse domains.

\begin{table*}[!h]
\centering
\caption{Overview of common attention and attention-like mechanisms.}\label{tab:attention}
\begin{tabular}{lccc}
\toprule
Method & Attention Matrix & Time Complexity & Space Complexity \\
\midrule
Scaled Dot-Product Attention & Learnable, Low-Rank, and Dense & $\mathcal{O}(N^{2}D + ND^{2})$ & $\mathcal{O}(N^{2} + ND)$ \\
Sparse Attention & Learnable, High/Full-Rank, and Sparse & $\mathcal{O}(|\mathcal{E}|D + ND^{2})$ & $\mathcal{O}(|\mathcal{E}| + ND)$ \\
Fourier-Type Attention & Learnable, Low-Rank, and Dense & $\mathcal{O}(N^{2}D + ND^{2})$ & $\mathcal{O}(N^{2} + ND)$ \\
Galerkin-Type Attention & Learnable, Rank-dependent, and Dense & $\mathcal{O}(ND^{2})$ & $\mathcal{O}(D^{2} + ND)$ \\
Kernelized Attention & Learnable, Low-Rank, and Dense & Depends on kernel & Depends on kernel \\
Synthetic Dense Attention & Learnable, Rank-Dependent, and Dense & $\mathcal{O}(N^{2}D + ND^{2})$ & $\mathcal{O}(N^{2} + ND)$ \\
Chord Attention & Learnable, Full-Rank, and Sparse & $\mathcal{O}(ND\log^{2}{N})$ & $\mathcal{O}(N\log^{2}{N} + ND)$ \\
\midrule
Synvolution & Learnable, Full-Rank, and Dense & $\mathcal{O}(ND\log{N}+ND^{2})$ & $\mathcal{O}(ND)$ \\
\bottomrule
\end{tabular}
\end{table*}

\section{Kernel Functions in Kernel Polynomial Method}
In Table~\ref{tab:kernel_func}, we summarize all currently known kernel functions used in the kernel polynomial method. To illustrate the approximation capabilities of different kernels when the Gibbs phenomenon occurs, we present a comparison in Figure~\ref{fig:kpm}, where the target function $f(x)$ is a sign step function. For more details about the kernel polynomial method, see \citet{RevModPhys.78.275} and \citet{Wei√üe2008}.

\begin{table}[!h]
\centering
\caption{Overview of kernel functions}\label{tab:kernel_func}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
Kernel & Gibbs damping factor $g_k$ & Hyperparameters & Remarks \\
\midrule
Dirichlet & 1 & None & least favorable choice \\ 
Fej{\'e}r~\citep{Fejer1904} & $1-\frac{k}{K+1}$ & None & mainly of academic interest \\ 
Jackson~\citep{RevModPhys.78.275} & $\frac{(K+2-k)\cos(\frac{k\pi}{K+2})+\sin(\frac{k\pi}{K+2})\cot(\frac{\pi}{K+2})}{K+2}$ & None & optimal for most applications, but lacked rigorous proof \\ 
Lanczos~\citep{alma990004236840205776} & $\sinc^{M}(\frac{k\pi}{K+1})$ & $M \in \mathbb{N}$ & $M = 3$ closely matches the Jackson kernel \\ 
Lorentz~\citep{Vijay2004} & $\frac{\sinh\left[\xi(1-\frac{k}{K+1})\right]}{\sinh(\xi)}$ & $\xi \in \mathbb{R}$ & optimal for Green functions \\ 
Veki{\'c}~\citep{PhysRevLett.71.4283} & $\frac{1}{2} - \frac{1}{2}{\tanh}\left[\frac{\frac{k}{K+1}-\frac{1}{2}}{\frac{k}{K+1}(1-\frac{k}{K+1})}\right]$ & None & found empirically \\ 
Wang~\citep{PhysRevB.49.10154} & $\eu^{-\left(\frac{ak}{K+1}\right)^{b}}$ & $a, b \in \mathbb{R}$ & found empirically \\
\bottomrule
\end{tabular}}
\end{table}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.45]{figure/kpm_step_func.png}
\end{center}
\caption{An illustration of Gibbs phenomenon when using the kernel polynomial method with different kernels to approximate a step function.}
\label{fig:kpm}
\end{figure}

\section{Kernel Polynomial Loss}
Here is the complete derivation process for Equation~\ref{eq:kpl}.
\begin{equation}
\centering
\begin{split}
\int_{-1}^{1}\left|\frac{\diff{f(x)}}{\diff{x}}\right|^{2}\diff{x}
&\approx \int_{-1}^{1}\left|\frac{\diff}{\diff{x}}\sum_{k=0}^{K}\mu_{k}T_k(x)\right|^{2}\diff{x} \\
&= \int_{-1}^{1}\left|\sum_{k=1}^{K}\mu_{k}T'_k(x)\right|^{2}\diff{x} \\
&= \int_{-1}^{1}\left(\sum_{k=1}^{K}\mu_{k}T'_k(x)\right)\left(\sum_{m=1}^{K}\overline{\mu_{m}}T'_m(x)\right)\diff{x} \\
&= \sum_{k=1}^{K}\sum_{m=1}^{K}\mu_{k}\overline{\mu_{m}}\int_{-1}^{1}T'_k(x)T'_m(x)\diff{x} \\
&= \sum_{k=1}^{K}|\mu_{k}|^{2}\int_{-1}^{1}|T'_k(x)|^{2}\diff{x} \\
&= \sum_{k=1}^{K}\pi k^{2}|\mu_{k}|^{2}
\end{split}
\end{equation}

\section{Proofs}
\noindent\textbf{Proof of Proposition~\ref{prop:dhhp_universal}}
\begin{proof}
First, we note that DFT, DWHT, DCT, and DST are all discrete unitary transforms, meaning their matrices are unitary. By Assumption~\ref{asmp:dhhp_givens_num_upper_bound}, any $N \times N$ unitary matrix can be exactly constructed using $L$-DHHP where $1 \leq L \leq \lceil\frac{N}{4}\rceil$. Therefore, as specific cases of unitary matrices, DFT, DWHT, DCT, and DST can all be exactly represented by $L$-DHHP. For their inverses, since the inverse of a unitary matrix is its conjugate transpose, they can also be exactly represented by $L$-DHHP.
\end{proof}

\noindent\textbf{Proof of Proposition~\ref{prop:dhhp_time_complexity}}
\begin{proof}
According to the definition of $L$-DHHP, the transform matrix $\bm{\Phi}$ can be decomposed as $\bm{\Phi} = \mathbf{D}\left(\prod_{l=1}^{L}\mathbf{H}^{(l)}_{\text{l}}\mathbf{H}^{(l)}_{\text{u}}\mathbf{P}^{(l)}\right)$, where $\mathbf{D}$ is a unitary diagonal matrix, $\mathbf{H}^{(l)}_{\text{l}}$ is a lower unitary Hessenberg matrix, $\mathbf{H}^{(l)}_{\text{u}}$ is a upper unitary Hessenberg matrix, and $\mathbf{P}^{(l)}$ is a permutation matrix. The computation of $\mathbf{y} = \bm{\Phi}\mathbf{x}$ can be broken down into sequential steps:
\begin{enumerate}
    \item For each order $l$ from $1$ to $L$:
        \item Multiplication with $\mathbf{P}^{(l)}$ requires $\mathcal{O}(N)$ operations
        \item Multiplication with $\mathbf{H}^{(l)}_{\text{u}}$ requires $\mathcal{O}(N\log{N})$ operations
        \item Multiplication with $\mathbf{H}^{(l)}_{\text{l}}$ requires $\mathcal{O}(N\log{N})$ operations
    \item The multiplication of diagonal matrix $\mathbf{D}$ with input signal $\mathbf{x}$ requires $\mathcal{O}(N)$ operations.
\end{enumerate}

As shown in Algorithm~\ref{alg:fast_order-1_dhhp_pytorch}, these multiplications can be implemented efficiently through Hadamard products, the total time complexity is $\mathcal{O}(LN\log{N})$.
\end{proof}

\noindent\textbf{Proof of Proposition~\ref{prop:dhhp_full_rank}}
\begin{proof}
($\Leftarrow$) First, we prove that if $\mathbf{D}$ is unitary, then $L$-DHHP is full-rank. By definition, every lower unitary Hessenberg matrix $\mathbf{H}^{(l)}_{\text{l}}$, upper unitary Hessenberg matrix $\mathbf{H}^{(l)}_{\text{u}}$, and permutation matrix $\mathbf{P}^{(l)}$ is unitary. Since the product of unitary matrices is unitary, and $\mathbf{D}$ is given to be unitary, the entire $L$-DHHP matrix is unitary. As every unitary matrix is full-rank, $L$-DHHP is full-rank.

($\Rightarrow$) Now we prove that if $L$-DHHP is full-rank, then $\mathbf{D}$ must be unitary. Let $\bm{\Phi} = \mathbf{D}\left(\prod_{l=1}^{L}\mathbf{H}^{(l)}_{\text{l}}\mathbf{H}^{(l)}_{\text{u}}\mathbf{P}^{(l)}\right)$ be $L$-DHHP. Since $\mathbf{H}^{(l)}_{\text{l}}$, $\mathbf{H}^{(l)}_{\text{u}}$, and $\mathbf{P}^{(l)}$ are all unitary, their product is unitary and thus full-rank. For $\bm{\Phi}$ to be full-rank, $\mathbf{D}$ must also be full-rank.
As $\mathbf{D}$ is diagonal, it is full-rank if and only if all its diagonal entries have unit magnitude, which is equivalent to $\mathbf{D}$ being unitary.
\end{proof}

\noindent\textbf{Proof of Theorem~\ref{theorem:cpi_diff}}
\begin{proof}
The complete proof can be found in \citet{10.1137/1.9781611975949}.
\end{proof}

\noindent\textbf{Proof of Theorem~\ref{theorem:cpi_ana}}
\begin{proof}
The complete proof can be found in \citet{10.1137/1.9781611975949}.
\end{proof}

\section{Additional Experimental Details}
\noindent\textbf{Baseline Implementations.} We use the PyTorch implementation released by the authors or the third part for all baseline models. We list all baseline model implementations as following.
\begin{itemize}
\item \textbf{Transformer}: \href{https://github.com/hyunwoongko/transformer}{https://github.com/hyunwoongko/transformer}
\item \textbf{Linformer}: \href{https://github.com/lucidrains/linformer}{https://github.com/lucidrains/linformer}
\item \textbf{Performer}: \href{https://github.com/lucidrains/performer-pytorch}{https://github.com/lucidrains/performer-pytorch}
\item \textbf{Synthesizer}: \href{https://github.com/10-zin/Synthesizer}{https://github.com/10-zin/Synthesizer}
\item \textbf{Nystr{\"{o}}mformer}: \href{https://github.com/mlpen/Nystromformer}{https://github.com/mlpen/Nystromformer}
\item \textbf{FNet}: \href{https://github.com/erksch/fnet-pytorch}{https://github.com/erksch/fnet-pytorch}
\item \textbf{cosFormer}: \href{https://github.com/OpenNLPLab/cosFormer}{https://github.com/OpenNLPLab/cosFormer}
\item \textbf{Paramixer}: \href{https://github.com/wiedersehne/Paramixer}{https://github.com/wiedersehne/Paramixer}
\end{itemize}

\subsection{Experimental Settings}
We provide the complete set of hyperparameters used to train all models and report their results. The optimal hyperparameters were determined using Bayesian optimization under a 16 GB GPU memory constraint. After selecting the best hyperparameters based on minimum validation loss, we evaluated the corresponding model on the test set and reported its performance. For all four datasets and seven baseline models, we employed the AdamW optimizer with cross-entropy loss. The training configurations and hyperparameters for each neural network and dataset are summarized in Tables~\ref{tab:baseline_configs} and Table~\ref{tab:converter_configs}.

\begin{table}[!h]
\centering
\caption{The final baseline model hyperparameters used in experiments. Abbreviations: PE for position embedding, ES for embedding size, HS for hidden size, NB for number of blocks, NH for number of heads, Pool for pooling strategy, BS for batch size, LR for learning rate, WR for weight decay, and N/A indicates that the corresponding parameter is not present in the architecture.}
\label{tab:baseline_configs}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccccccc}
\hline
Dataset  & Model & PE & ES & HS & NB & NH & Pool & BS & LR & WD & PE Dropout Rate & Value Dropout Rate & FFN Dropout Rate \\
\hline
LongDoc16K      & Transformer   & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0005 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Linformer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Performer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Synthesizer   & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0005 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & FNet          & RPE & 128 & 512 & 2 & N/A & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & cosFormer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Paramixer     & RPE & 128 & 512 & 2 & N/A & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
\hline
LongDoc16K      & Transformer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0005 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Linformer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Performer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Synthesizer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0005 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & FNet          & RPE & 128 & 512 & 2 & N/A & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & cosFormer     & RPE & 128 & 512 & 2 & 2 & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Paramixer     & RPE & 128 & 512 & 2 & N/A & MEAN & 4 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
\hline
Ensembl (B/S)   & Transformer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Linformer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Performer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Synthesizer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & FNet          & RPE & 64 & 256 & 2 & N/A & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & cosFormer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Paramixer     & RPE & 64 & 256 & 2 & N/A & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
\hline
Ensembl (M/R)   & Transformer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Linformer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Performer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Synthesizer   & RPE & 64 & 256 & 2 & 1 & MEAN & 2 & 0.0002 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & FNet          & RPE & 64 & 256 & 2 & N/A & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & cosFormer     & RPE & 64 & 256 & 2 & 2 & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
                & Paramixer     & RPE & 64 & 256 & 2 & N/A & MEAN & 2 & 0.0001 & 0.0001 & 0.1 & 0.1 & 0.1 \\
\hline
\end{tabular}}
\end{table}


\begin{table}[!h]
    \centering
    \caption{The hyperparameters of Converter for all experiments. Abbreviations: PE for position embedding, ES for embedding size, HS for hidden size, K for the maximum order of KPM, Pool for pooling strategy, BS for batch size, LR for learning rate, and WR for weight decay.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l c c c c c c c c c c c c c c}
    \toprule
    Dataset  & PE & ES & HS & K & $\eta$ & Pool & BS & LR & WD & PE Dropout Rate & Value Dropout Rate & GFFN Dropout Rate & Eigenvalue Dropout Rate & Eigenvector Dropout Rate \\ 
    \midrule
    ListOps & RPE & 32 & 128 & 2 & 0.001 & MEAN & 128 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    Text & RPE & 64 & 256 & 2 & 0.001 & MEAN & 128 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    Retrieval & RPE & 64 & 256 & 2 & 0.001 & MEAN & 256 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    Image & RPE & 64 & 256 & 2 & 0.01 & MEAN & 128 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    Pathfinder & RPE & 64 & 256 & 2 & 0.001 & MEAN & 256 & 0.0002 & 0.0002 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    \midrule
    LongDoc16K & RPE & 128 & 512 & 2 & 0.1 & MEAN & 4 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    LongDoc32K & RPE & 128 & 512 & 2 & 0.1 & MEAN & 4 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    \midrule
    Ensembl (B/S) & RPE & 128 & 512 & 2 & 0.1 & MEAN & 32 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    Ensembl (M/R) & RPE & 128 & 512 & 2 & 0.1 & MEAN & 32 & 0.001 & 0.001 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\
    \bottomrule
    \end{tabular}}
    \label{tab:converter_configs}
\end{table}