\section{Experiments}
In this section, we test the scalability and performance of Converter in three different domains: (1)~Long-Range Arena benchmark, (2)~long document classification, and (3)~DNA sequence-based taxonomy classification. We conducted all experiments on a NVIDIA DGX-1 equipped with two 20-core Intel Xeon E5-2698 v4 CPUs @ 2.2 GHz, 512 GB of RAM, and 8 NVIDIA Tesla V100 GPUs, each with 16 GB of GPU memory. The code is implemented using PyTorch~\citep{NEURIPS2019_bdbca288}. Following \citet{neishi-yoshinaga-2019-relation}, we adopt a 2-layer GRU for position embedding, which is denoted as RPE. We adopt the AdamW optimizer~\citep{loshchilov2018decoupled} and apply cross-validation to report the best hyperparameters. We apply the following loss functions as the metric to evaluate our model.
\begin{equation}
\centering
\begin{split}
\mathcal{L} = (1-\eta)\cdot\mathcal{L}_{\text{CE}} + \eta\cdot\mathcal{L}_{\text{KP}}
\end{split}
\end{equation}
Here, $\eta \in {[0, 1)}$ is a tunable hyperparameter that needs to be selected manually, and $\mathcal{L}_{\text{CE}}$ denotes cross-entropy loss.

\subsection{Long-Range Arena Benchmark}

\begin{table*}[!ht]
\centering
\caption{Accuracy results (\%) on the Long-Range Arena benchmark. The best result is in bold and the second best is underlined.}
\label{tab:lra_res}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{ListOps}~$\uparrow$ & \textbf{Text}~$\uparrow$ & \textbf{Retrieval}~$\uparrow$ & \textbf{Image}~$\uparrow$ & \textbf{Pathfinder}~$\uparrow$ & \textbf{Avg.}~$\uparrow$ \\
\midrule
Vanilla Trans.~\citep{NIPS2017_3f5ee243} & 36.37 & 64.27 & 57.46 & 42.44 & 71.40 & 54.39 \\
Sparse Trans.~\citep{child2019generating} & 17.07 & 63.58 & 59.59 & 44.24 & 71.71 & 51.24 \\
Reformer~\citep{Kitaev2020Reformer} & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & 50.67 \\
Longformer~\citep{beltagy2020longformer} & 35.63 & 62.85 & 56.89 & 42.22 & 69.71 & 53.46 \\
Linformer~\citep{wang2020linformer} & 35.70 & 53.94 & 52.27 & 38.56 & 76.34 & 51.36 \\
BigBird~\citep{NEURIPS2020_c8512d14} & 36.05 & 64.02 & 59.29 & 40.83 & 74.87 & 55.01 \\
Linear Trans.~\citep{pmlr-v119-katharopoulos20a} & 16.13 & 65.90 & 53.09 & 42.34 & 75.30 & 50.55 \\
Sinkhorn Trans.~\citep{pmlr-v119-tay20a} & 33.67 & 61.20 & 53.83 & 41.23 & 67.45 & 51.29 \\
Performer~\citep{choromanski2021rethinking} & 18.01 & 65.40 & 53.82 & 42.77 & 77.05 & 51.41 \\
Synthesizer~\citep{pmlr-v139-tay21a} & 36.99 & 61.68 & 54.67 & 41.61 & 69.45 & 52.88 \\
Nystr{\"{o}}mformer~\citep{xiong2021Nystromformer} & 37.15 & 65.52 & \underline{79.56} & 41.58 & 70.94 & 58.95 \\
Luna-256~\citep{ma2021luna} & 37.98 & 65.78 & \underline{79.56} & \underline{47.86} & 78.55 & \underline{61.95} \\
FNet~\citep{lee-thorp-etal-2022-fnet} & 35.33 & 65.11 & 59.61 & 38.67 & 77.80  & 55.30 \\
cosFormer~\citep{zhen2022cosformer} & 37.90 & 63.41 & 61.36 & 43.17 & 70.33 & 55.23 \\
Paramixer (Chord)~\citep{9878955} & \underline{39.71} & \underline{78.87} & 78.73 & 44.68 & \underline{79.16} & 58.91 \\
\midrule
Converter (ours) & \textbf{60.38} & \textbf{86.44} & \textbf{83.41} & \textbf{61.02} & \textbf{88.43} & \textbf{75.94} \\
\bottomrule
\end{tabular}
\end{table*}

The Long-Range Arena (LRA)~\citep{tay2021long} is a public benchmark established with the aim of evaluating the ability of efficient Transformers to model long-sequence data. This benchmark contains five multi-class classification tasks from distinct domains, including ListOps~\citep{nangia-bowman-2018-listops}, Text~\citep{maas-etal-2011-learning}, Retrieval~\citep{radev-etal-2009-acl}, Image~\citep{krizhevsky2009learning}, and Pathfinder~\citep{NEURIPS2018_ec895663,Kim2020Disentangling}. ListOps consists of digits, operators such as MAX, MEAN, MEDIAN, and SUM\_MOD, and brackets. Each operator in a sequence processes the items in a list and outputs a digit. Text consists of sequences represented at the byte/character-level, which significantly increases its difficulty. In this task, models must classify each review as positive or negative, making it a binary classification task. Retrieval is similar to the Text task with a byte/character-level setting. Image is the CIFAR-10 task~\citep{krizhevsky2009learning} for image classification. The input data consists of sequences of pixels derived from flattening $32 \times 32$ images into a 1D array with the length of 1024. Pathfinder is motivated by cognitive psychology~\citep{Houtkamp2010Parallel}. In this task, a synthetic image measures $32 \times 32$ pixels and features two highlighted endpoints depicted as circles, connected by a dashed path. Each image contains distractor paths, adding complexity. The models must determine whether a dashed path connects the two highlighted endpoints. As in the Image task, the input has to be converted into a sequence with length 1024.

In the interest of ensuring a fair comparison, we follow the experiment settings outlined in \citep{tay2021long} and evaluate Converter on the aforementioned tasks. For baselines, we include the vanilla Transformer~\citep{NIPS2017_3f5ee243} and 14 Transformer variants: Sparse Transformer~\citep{child2019generating}, Reformer~\citep{Kitaev2020Reformer}, Longformer~\citep{beltagy2020longformer}, Linformer~\citep{wang2020linformer}, BigBird~\citep{NEURIPS2020_c8512d14}, Linear Transformer~\citep{pmlr-v119-katharopoulos20a}, Sinkhorn Transformer~\citep{pmlr-v119-tay20a}, Performer~\citep{choromanski2021rethinking}, Synthesizer~\citep{pmlr-v139-tay21a}, Nystr{\"{o}}mformer~\citep{xiong2021Nystromformer}, Luna~\citep{ma2021luna}, FNet~\citep{lee-thorp-etal-2022-fnet}, cosFormer~\citep{zhen2022cosformer}, and Paramixer~\citep{9878955}.

As shown in Table~\ref{tab:lra_res}, Converter consistently surpasses all baseline models in all five tasks with the best classification accuracy: ListOps (60.38\%), Text (86.44\%), Retrieval (83.41\%), Image (61.02\%), and Pathfinder (88.43\%). Converter attains an average accuracy of 75.94\%, substantially outperforming the second-best model Luna-256 (61.95\%) by a margin of 14 percentage points. Notably, on the challenging ListOps task, Converter (60.38\%) surpasses the second-best performer Paramixer (39.71\%) by more than 20.57\%, demonstrating its superior capability in handling structured sequential data. Furthermore, for the Image classification task, Converter (61.02\%) significantly outperforms the runner-up Luna-256 (47.86\%), showcasing its exceptional ability in visual feature extraction. These experimental results confirm the comprehensive advantages that Converter demonstrates in processing long-sequence tasks.

\subsection{Long Document Classification}
This task aims to evaluate the capability of Converter in modeling complex long-term dependencies for NLP tasks. We utilized a publicly available dataset collected from arXiv~\citep{a11080109}. Following \citet{9878955}, we selected four document categories: cs.AI, cs.NE, math.AC, and math.GR, yielding a dataset of 11956 documents. The documents were encoded at the character level, and all comparative models employed zero padding to achieve uniform length. The dataset was partitioned into 60\% training, 20\% validation, and 20\% test sets. Similar to the LRA benchmark, we created two standardized versions of the dataset through truncation: LongDoc16K with sequences of 16384 tokens and LongDoc32K with sequences of 32768 tokens. We then evaluated Converter and various Transformer-based architectures on these datasets.

\begin{table}[!h]
\centering
\caption{Accuracy results (\%) on long document classification. The best result is in bold and the second best is underlined.}
\label{tab:longdoc}
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & \textbf{LongDoc16K}~$\uparrow$ & \textbf{LongDoc32K}~$\uparrow$ \\
\midrule
Vanilla Transformer & 68.39 & 70.84 \\
Linformer (Layerwise) & 49.03 & 45.00 \\
Performer & 62.26 & 65.65 \\
Synthesizer (Dense) & 76.05 & \underline{77.02} \\
Nystr{\"{o}}mformer & 67.26 & 69.27 \\
FNet & 44.92 & 46.94 \\
cosFormer & 64.44 & 67.26 \\
Paramixer (Chord) & \underline{79.60} & 74.76 \\
\midrule
Converter & \textbf{81.77} & \textbf{82.34} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:longdoc} illustrates that Converter surpasses all baseline models. Notably, synthetic attention-based Transformer variants (Synthesizer, Paramixer, and Converter) provide better results than self-attention approximation-based Transformers (Linformer, Performer, and Nystr{\"{o}}mformer) and the vanilla Transformer. Meanwhile, FNet, a parameter-free and attention-free attention-based Transformer variant, performs poorly in long document classification tasks. This demonstrates that high-rank or full-rank attention plays a crucial role in modeling long-term dependencies.

\subsection{DNA Sequence-based Taxonomy Classification}
We evaluated Converter against other models using biological data. We obtained cDNA sequences and their taxonomic labels from Ensembl~\footnote{\href{https://www.ensembl.org/index.html}{https://www.ensembl.org/index.html}} and designed two binary classification tasks. Similar to the long document classification task, each dataset was truncated to a fixed length of 16384 and partitioned into 60\% training, 20\% validation, and 20\% test sets. The first dataset, Ensembl (B/S), focuses on vertebrate organisms, comparing sequences from the genera Bos and Sus. While the classes are nearly balanced (51973 Bos and 50027 Sus sequences), this dataset is particularly challenging due to extreme variations in sequence length (ranging from 63 to 447010 bases). The second dataset, Ensembl (M/R), represents our most computationally intensive classification task at the genus level. This dataset compares genes from Mus and Rattus, featuring significant class imbalance with a nearly 2:1 ratio (275636 Mus and 133310 Rattus sequences). Sequence lengths vary substantially, spanning from 32 to 261093 bases.

\begin{table}[!h]
\centering
\caption{Accuracy results (\%) on DNA sequence-based taxonomy classification. The best result is in bold and the second best is underlined.}
\label{tab:dna}
\begin{tabular}{l|cc}
\toprule
\textbf{Model} & \textbf{Ensembl (B/S)}~$\uparrow$ & \textbf{Ensembl (M/R)}~$\uparrow$ \\
\midrule
Vanilla Transformer & 66.71 & 58.50 \\
Linformer (Layerwise) & 62.76 & 51.63 \\
Performer & 63.18 & 55.16 \\
Synthesizer (Dense) & 66.07 & 56.34 \\
Nystr{\"{o}}mformer & 66.15 & \underline{58.51} \\
FNet & 65.70 & 56.30 \\
cosFormer & 65.73 & 56.30 \\
Paramixer (Chord) & \underline{66.77}  & 56.37 \\
\midrule
Converter & \textbf{84.59} & \textbf{59.49} \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:dna}, our model achieves strong performance. In Ensembl (B/S), Converter is 17.82\% more accurate than Paramixer. In Ensembl (M/R), Converter achieves an accuracy that is 0.98\% higher than Nystr{\"{o}}mformer. Moreover, our model consistently surpasses the vanilla Transformer in all two tasks.

\subsection{Ablation Studies}
We study the influence of different mechanisms used in Converter by ablating the corresponding components. Table~\ref{tab:lra_as} records the experimental results of Converter equipped with distinct components on the Long-Range Arena benchmark. NoPE means without position embedding, APE means learnable absolute position embedding~\citep{pmlr-v70-gehring17a}, and SPE means sinusoidal position embedding~\citep{NIPS2017_3f5ee243}. The ablation studies highlight the importance of RPE~\citep{neishi-yoshinaga-2019-relation}. Notably, Converter achieves the highest performance when using PRE, followed by APE and SPE in descending order, while the NoPE variant yields the lowest results. This finding contradicts recent papers regarding NoPE~\citep{haviv-etal-2022-transformer,chi-etal-2023-latent,NEURIPS2023_4e85362c}. Both versions of Converter with Kernolution (with and without the kernel polynomial loss) outperform Converter with Synvolution in all tasks.

\begin{table}[!h]
\centering
\caption{Ablation studies of Converter on the LRA benchmark.}
\label{tab:lra_as}
\begin{tabular}{l|ccccc}
\toprule
\textbf{Method} & \textbf{ListOps}~$\uparrow$ & \textbf{Text}~$\uparrow$ & \textbf{Retrieval}~$\uparrow$ & \textbf{Image}~$\uparrow$ & \textbf{Pathfinder}~$\uparrow$ \\
\midrule
Converter & 60.38 & 86.44 & 83.41 & 61.02 & 88.43 \\
\midrule
w/ NoPE & 36.44 & 62.31 & 66.85 & 41.01 & 77.48 \\
w/ SPE & 37.45 & 71.15 & 79.52 & 46.89 & 80.55 \\
w/ APE & 39.80 & 79.20 & 79.82 & 48.38 & 80.89 \\
w/ Synv. & 57.96 & 82.89 & 82.39 & 58.23 & 87.29 \\ 
w/o KPL & 59.32 & 83.60 & 83.11 & 59.75 & 88.18 \\
\bottomrule
\end{tabular}
\end{table}