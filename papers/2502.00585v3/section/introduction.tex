\section{Introduction}
Through the diligent efforts of researchers, Transformers~\citep{NIPS2017_3f5ee243} have played a crucial role in addressing natural language processing tasks~\citep{devlin-etal-2019-bert} since their inception. At the heart of Transformers is the self-attention mechanism that utilizes the scaled dot-product of a query matrix and a key matrix to generate similarity scores, which guide a value matrix. This enables Transformers to capture long-range dependencies and perform parallel computation. Recently, Transformers have even dominated computer vision~\citep{dosovitskiy2021an} and the biology domain~\citep{10.1145/3388440.3412467}.

Because the softmax function binds a query matrix and a key matrix together to compute attention scores~\citep{NIPS2017_3f5ee243}, the high computational cost of self-attention hinders its ability to handle large datasets. Recently, researchers have focused on developing self-attention alternatives with lower time complexity. Approximating the softmax function via kernel functions is a popular choice~\citep{tsai-etal-2019-transformer,choromanski2021rethinking,zhen2022cosformer}. However, this may cause the attention matrix in each attention layer to become low-rank. In this situation, the expressive capability of Transformers significantly declines~\citep{pmlr-v139-dong21a}.

The necessity of the softmax function in self-attention has been questioned. First, the softmax function does not have sufficient ability to express the true data distribution, which constrains the representational capacity of language models and leads to the softmax bottleneck issue~\citep{yang2018breaking}. Moreover, the softmax function inherently fails at robust reasoning across all inputs due to coefficient dispersion with increasing input elements~\citep{velickovic2024softmax}.

Therefore, replacing self-attention has become another well-known option~\citep{pmlr-v139-tay21a,lee-thorp-etal-2022-fnet,9878955}. However, achieving high-rank or full-rank attention matrices while maintaining a time complexity lower than quadratic is a challenge. We address this challenge via synthetic digraph convolution~\footnote{In this work, directed graphs are abbreviated as digraphs.}. Since self-attention is closely related to digraph convolution~\citep{NEURIPS2020_c8512d14}, why not replace self-attention with digraph convolution? This work is based on this hypothesis. We synthesize a unitary digraph convolution called Synvolution, which achieves high performance while maintaining linearithmic time complexity for long sequences. We also apply the kernel polynomial method~\citep{10.1142/S0129183194000842,PhysRevB.49.10154,PhysRevLett.73.1039,Vijay2004,RevModPhys.78.275,Wei√üe2008} as an alternative technique for the multi-head operation. We refer to Synvolution with the kernel polynomial method as Kernelution. Supported by the theoretical foundation of the kernel polynomial method, the filter of Kernelution can function as any corresponding unitary filter required by distinct datasets.

In this work, we introduce \textbf{Converter}, a Transformer that is converted into a DGNN form. We have evaluated Converter on Long-Range Arena benchmark~\citep{tay2021long}, long document classification~\citep{9878955}, and DNA sequence-based taxonomy classification~\citep{9878955}. The experimental results demonstrate that Converter outperforms previous Transformer variants. This demonstrates that Converter is a lightweight, efficient, and powerful neural network. Our key contributions are summarized as follows:
\begin{itemize}
\item We propose Synvolution, a novel self-attention alternative with linearithmic time complexity.
\item We apply the kernel polynomial method as an alternative to the multi-head operation, and propose kernel polynomial loss to simulate a dynamic kernel. We name Synvolution with the kernel polynomial method as Kernelution.
\item We propose Gated Feed-Forward Networks in place of the vanilla Feed-Forward Networks for complex-valued input and real-valued output.
\item We apply PostNorm with ScaleNorm for both real-valued and complex-valued tensor.
\end{itemize}