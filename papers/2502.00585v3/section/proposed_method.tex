\section{Proposed Method}
\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.4]{figure/converter_arc.png}
\end{center}
\caption{Converter architecture.}
\label{fig:converter_architecture}
\end{figure}

\subsection{Synvolution}
Let $\mathbf{A} = \mathbf{X}\mathbf{W}_{\text{Q}}(\mathbf{X}\mathbf{W}_{\text{K}})^{\mathrm{T}}/{\tau} \in \mathbb{R}^{n \times n}$ be an affinity matrix, the attention matrix $\mathcal{A} = \mathrm{softmax}(\mathbf{A})$ in Equation~\ref{eq:shsa} can be reformulated as a right stochastic normalized affinity form $\mathrm{SA}(\mathbf{X}) = \widetilde{\mathbf{D}}^{-1}\widetilde{\mathbf{A}}\mathbf{X}\mathbf{W}_{\text{V}}$, where $\widetilde{\mathbf{A}} = \exp(\mathbf{A}) \in \mathbb{R}^{n \times n}$ is defined as the affinity matrix after the element-wise exponentiation operation $\exp(\cdot)$, and $\widetilde{\mathbf{D}}_{u,u} = \sum_{v}\widetilde{\mathbf{A}}_{u,v} \in \mathbb{R}^{n \times n}$ is the corresponding degree matrix. When treating $\widetilde{\mathbf{A}}$ as a digraph adjacency matrix, we found that self-attention closely resembles digraph convolution. First, each element in either an attention matrix or a DGSO can be considered as a similarity from source entity to target entity. Second, both self-attention and digraph convolution can be degenerated to graph convolution form. For self-attention, this occurs when the query matrix is equal to the key matrix in each head, resulting in unidirectional symmetric self-attention. Similarly, for digraph convolution, the achievement of graph convolution can be implemented by symmetrizing the adjacency matrix of a digraph. Third, the softmax function in self-attention results in a row-wise normalized digraph adjacency form. 

Since digraph convolution closely resembles self-attention, we investigated replacing self-attention with digraph convolution. Under this hypothesis, a Transformer can be converted into a DGNN form. Based on this insight, we propose Converter. In this work, we decide to construct the DGSO directly. We develop a learnable unitary matrix as a DGSO through the inverse process of eigendecomposition. Our method consists of two phases. In the first phase, we synthesize the required eigenvalues through the following process.
\begin{equation}\label{eq:eigenvalue_gap}
\centering
\eu^{\ramuno\mathbf{\Lambda}}
= \exp\left[\ramuno\cdot\mathrm{diag}\left(\mathrm{pool}_{\text{avg}}\left[\mathrm{SIREN}(\mathbf{X})\right]\right)\right].
\end{equation}
Here, $\mathrm{SIREN}$ represents a 2-layer MLP with the sine function~\citep{NEURIPS2020_53c04118}, $\mathrm{pool}_{\text{avg}}(\cdot)$ is a 1D global average pooling, and $\mathrm{diag}(\cdot)$ is a diagonalize operation. We adopt the sine function because it demonstrates a remarkable ability in signal processing~\citep{NEURIPS2020_53c04118}.

In the second phase, we focus on constructing the necessary unitary eigenvector matrix through the inverse process of LQ factorization. Based on the Givens rotation method~\citep{doi:10.1137/0106004}, an arbitrary square matrix $\bm{\Phi} \in \mathbb{C}^{N \times N}$ can be decomposed into a product of a lower triangular matrix and Givens rotation matrices. Hence, we have
\begin{equation}\label{eq:givens_lq}
\centering
\bm{\Phi} 
= \mathbf{L}\mathbf{Q} 
= \mathbf{L}\left(\prod_{j=N}^{2}\prod_{i=j-1}^{1}\mathbf{G}_{i,j}\right),
\end{equation}
where $\mathbf{L} \in \mathbb{C}^{N \times N}$ is a lower triangular matrix, and $\mathbf{G}_{i,j} \in \mathbb{C}^{N \times N}$ is a Givens rotation matrix that resembles an identity matrix with the exception of the elements
\begin{equation}\label{eq:givens_rotation_matrix}
\centering
\begin{bmatrix}
G_{ii} & G_{ij} \\
G_{ji} & G_{jj}
\end{bmatrix} =
\begin{bmatrix}
\overline{c} & -s \\
\overline{s} & c
\end{bmatrix} =
\begin{bmatrix}
\eu^{-\ramuno(\frac{\alpha+\beta}{2})}\cos{(\frac{\gamma}{2})} 
& -\eu^{\ramuno(\frac{\alpha-\beta}{2})}\sin{(\frac{\gamma}{2})} \\
\eu^{-\ramuno(\frac{\alpha-\beta}{2})}\sin{(\frac{\gamma}{2})} 
& \eu^{\ramuno(\frac{\alpha+\beta}{2})}\cos{(\frac{\gamma}{2})}
\end{bmatrix},
\end{equation}
which characterized by parameters $\alpha$, $\beta$, and $\gamma \in [0, 2\pi]$. This methodology necessitates ${\left(N(N-1)\right)}/{2}$ pairs of Givens rotation matrices, i.e., it requires $\mathcal{O}(N^{2})$ space complexity. By reorganizing Givens rotation matrices, inserting permutation matrices, and repeating the patten, we have
\begin{equation}\label{eq:givens_lhhp}
\centering
\begin{split}
\bm{\Phi} 
&= \mathbf{L}\left(\prod_{l=1}^{L}\left(\prod_{i=N-1}^{1}\mathbf{G}^{(l)}_{i,i+1}\right)\left(\prod_{j=1}^{N-1}\mathbf{G}^{(l)}_{j,j+1}\right)\mathbf{P}^{(l)}\right)\\
&= \mathbf{L}\left(\prod_{l=1}^{L}\mathbf{H}^{(l)}_{\text{l}}\mathbf{H}^{(l)}_{\text{u}}\mathbf{P}^{(l)}\right).
\end{split}
\end{equation}
Here, $\mathbf{H}^{(l)}_{\text{l}} \in \mathbb{C}^{N \times N}$ is a lower unitary Hessenberg matrix, $\mathbf{H}^{(l)}_{\text{u}} \in \mathbb{C}^{N \times N}$ is an upper unitary Hessenberg matrix, and $\mathbf{P}^{(l)} \in \mathbb{R}^{N \times N}$ is a permutation matrix that either learnable~\citep{mena2018learning}, fixed~\citep{pmlr-v162-dao22a}, or even an identity matrix $\mathbf{I}_{N} \in \mathbb{R}^{N \times N}$.

We refer to Equation~\ref{eq:givens_lhhp} as the order-$L$ LHHP parametrization, $L$-LHHP for short, denoted by $\bm{\Phi}_{L-\text{LHHP}}$. In particular, when the lower triangular matrix $\mathbf{L}$ degenerates to a diagonal matrix $\mathbf{D}$, we term this pattern the order-$L$ DHHP parametrization, $L$-DHHP for short, denoted by $\bm{\Phi}_{L-\text{DHHP}}$. It requires $2L(N-1)$ pairs of Givens rotation matrices, which means the space complexity is $\mathcal{O}(LN)$. We observed that each unitary factor matrix resulting from the multiplication of lower and upper unitary Hessenberg matrices in the order-$L$ DHHP parametrization is dense rather than sparse, unlike the schemes proposed in \citep{KHALITOV2022160}. Since our method is based on the Givens rotation method, we make Assumption~\ref{asmp:dhhp_givens_num_upper_bound}. Under this assumption, we can establish the following propositions.

\begin{assumption}\label{asmp:dhhp_givens_num_upper_bound}
For constructing an arbitrary $N \times N$ dense unitary matrix, at most $\lceil\frac{N}{4}\rceil$ orders are sufficient for $L$-DHHP.
\end{assumption}

\begin{proposition}\label{prop:dhhp_universal}
$L$-DHHP captures the discrete unitary transforms, including discrete Fourier transform (DFT), the discrete Walshâ€“Hadamard transform (DWHT), the discrete cosine transform (DCT), the discrete sine transform (DST), and their inverses exactly.
\end{proposition}

\begin{proposition}\label{prop:dhhp_time_complexity}
Given an input signal $\mathbf{x} \in \mathbb{C}^{N}$ and an output signal $\mathbf{y} \in \mathbb{C}^{N}$, the time complexity of $L$-DHHP as a discrete unitary transform with the fast implementation as $\mathbf{y} = \bm{\Phi}\mathbf{x}$ is $\mathcal{O}(LN\log{N})$.
\end{proposition}

\begin{proposition}\label{prop:dhhp_full_rank}
$L$-DHHP is full-rank if and only if the diagonal matrix $\mathbf{D}$ is unitary.
\end{proposition}

We refer to this self-attention alternative as Synvolution:
\begin{equation}\label{eq:synvolution}
\centering
\mathrm{Synv}(\mathbf{X}\mathbf{W}_{\text{V}})
= \bm{\Phi}^{-1}\left[\exp(\ramuno\mathbf{\Lambda})\odot(\bm{\Phi}\mathbf{X}\mathbf{W}_{\text{V}})\right],
\end{equation}
where $\mathbf{X}\mathbf{W}_{\text{V}} \in \mathbb{C}^{N \times D}$ is denoted as the value matrix. Unlike FFT-based convolution~\citep{mathieu2013fast}, where the discrete unitary matrix is fixed and data-independent, the required parameters in $L$-DHHP are learnable and data-dependent. We adopt a similar processing method to that described in Equation~\ref{eq:eigenvalue_gap} to obtain the synthetic eigenvector matrix. For convenience, we set $L=1$, $\mathbf{P}^{(1)} = \mathbf{I}_{N}$, and $\mathbf{D}$ is unitary to obtain a dense unitary matrix that serves as the desired unitary eigenvector matrix. More details about the fast implementation of $1$-DHHP as a discrete unitary transform are provided in the appendix.

% We recognize that the core essence of self-attention can be characterized as a fast weight programmer~\citep{pmlr-v139-schlag21a}. In brief, it involves a non-linear projection of the input feature into a weighted tensor, known as a fast weight, which guides either the input feature itself or its linear projection. This operation is not unique to self-attention, but is also applicable to non-local operations~\citep{Wang_2018_CVPR}. For instance, channel attention~\citep{Hu_2018_CVPR}, a well-known instance of non-local operation, commonly employs the squeeze-and-excitation technique. The first squeeze operation aggregates the frequency information from the spatial extent, whereas the second excitation operation redistributes the gathered feature responses back to the local features. Another example is Involution~\citep{Li_2021_CVPR}.

\subsection{Kernelution}

\begin{figure*}[t]
\begin{center}
\includegraphics[scale=0.4]{figure/kernelution.png}
\end{center}
\caption{Illustration of the entire Kernelution process.}
\label{fig:kernelution}
\end{figure*}

\subsubsection{Chebyshev Polynomial Interpolation}
The multi-head operation, a common approach to enhance performance in Transformers, lacks solid theoretical support. In the contrast, FIR filters have a theoretical support in spectral graph theory~\citep{chung1997spectral}. Let $f(x)$ be the target function, then our goal is to approximate it with the smallest round-off error. Directly manipulating orthogonal polynomials to filter complex-valued signals is challenging, but using them to represent the argument function of signals is straightforward. To achieve it, we can choose an arbitrary orthogonal polynomial basis such as the Bernstein basis, Jacobi basis (including Chebyshev, Gegenbauer, Legendre, and Zernike bases), or even monomial basis. Consider the Chebyshev basis as an example. Given an arbitrary continuous function $f(x) \in C([-1,1])$ and a truncated Chebyshev polynomial $p$ with $K$ orders, then the target function $f(x)$ can be approximated as
\begin{equation}\label{eq:cpi}
\centering
f(x) \approx p(x) = \frac{1}{2}{\mu}_{0} + \sum_{k=1}^{K}{\mu}_{k}{T}_{k}(x),
\end{equation}
where $\mu_{k} \approx \frac{2}{K+1}\sum_{j=0}^{K}f(x_{j})T_{k}(x_{j})$ is the Chebyshev coefficient, and $x_{j}$ is the sampling Chebyshev node. This technique is termed the Chebyshev polynomial interpolation (CPI)~\citep{10.1137/1.9781611975949}. The operation on Chebyshev polynomial interpolation is considerably straightforward since Chebyshev polynomials are isomorphic with Fourier series. For differentiable or analytic functions, we have the following theorems.

\begin{theorem}[CPI for differentiable functions~\citep{10.1137/1.9781611975949}]\label{theorem:cpi_diff}
Let $\upsilon \geq 0$ and $\kappa > \upsilon$ be integers. Consider a function $f(x)$ whose derivatives up to order $\upsilon-1$ are absolutely continuous on $[-1, 1]$, and suppose $\lVert{\frac{\diff^{\upsilon}}{\diff{x}^{\upsilon}} f(x)}\rVert_{1} = \Upsilon$. For the $\kappa$-th degree Chebyshev interpolant $p(x)$, the following bounds hold: (1)~$\lVert{\mu_{\kappa}}\rVert \leq \frac{2\Upsilon}{\pi(\kappa-\upsilon)^{\upsilon+1}}$. (2)~$\lVert{f(x) - p(x)}\rVert \leq \frac{4\Upsilon}{\pi\upsilon(\kappa-\nu)^{\upsilon}}$.
\end{theorem}

\begin{theorem}[CPI for analytic functions~\citep{10.1137/1.9781611975949}]\label{theorem:cpi_ana}
Let $\kappa \geq 1$ be an integer and $f(x)$ an analytic function on $[-1, 1]$ that extends analytically to the open Bernstein ellipse $E_{\rho}$ with $\lVert{f(x)}\rVert \leq M$ for some $M$. For the $\kappa$-th degree Chebyshev interpolant $p(x)$, the following bounds hold: (1)~$\lVert{\mu_{0}}\rVert \leq M$. (2)~$\lVert{\mu_{\kappa}}\rVert \leq 2M\rho^{-\kappa}$. (3)~$\lVert{f(x)-p(x)}\rVert \leq \frac{4M\rho^{-\kappa}}{\rho-1}$.
\end{theorem}

Both Theorem~\ref{theorem:cpi_diff} and Theorem~\ref{theorem:cpi_ana} tell us that we can utilize the Chebyshev polynomial filter to approximate any continuous target function that lies in the range of $C[-1, 1]$ with a small round-off error.

\subsubsection{Kernel Polynomial Method}
In reality, the target function is probably discontinuous or singular in the polynomial interpolation interval. In this situation, the accuracy of the Chebyshev polynomial interpolation reduces to $\mathcal{O}(1)$ near discontinuities or singularities. Sufficiently far away from discontinuities or singularities, the convergence will be slowed to $\mathcal{O}(K^{-1})$. During the approximation process, oscillations will be present near discontinuities or singularities and they will not diminish as $K \to \infty$. This type of oscillation is termed the Gibbs oscillation, and this situation is known as the Gibbs phenomenon~\citep{Hewitt1979}.

To mitigate Gibbs oscillations, we apply a Gibbs damping factor $g_{k}$, which represented as a function of $\frac{k}{K+1}$, to each term of the Chebyshev polynomials. For any $f(x)$, we have
\begin{equation}\label{eq:kpm}
\centering
{f(x)}\approx{p}_{\text{KP}}(x) = \frac{1}{2}g_{0}\mu_{0} + \sum_{k=1}^{K}g_{k}\mu_{k}T_{k}(x).
\end{equation}
This modification of the Chebyshev coefficients is equivalent to the convolution of $p(x)$ with a kernel $\mathcal{K}(x,x_{0}) = \frac{2}{\pi\sqrt{1-x^{2}}}\left(\frac{1}{2}g_{0} + \sum_{k=1}^{K}g_{k}T_{k}(x)T_{k}(x_{0})\right)$ that $p_{\text{KP}}(x) = \int_{-1}^{1}\mathcal{K}(x,x_{0})f(x_{0})\diff{x_{0}}$. Thus, this method is also called the kernel polynomial method. It is widely employed in computational physics for calculating the density of states and other spectral properties of large quantum systems. 

Gibbs damping factors are a family of coefficients that satisfy three conditions: (1)~$g_{k} > 0$. (2)~$g_{0} = 1$. (3)~$\lim_{K \to \infty} {g_{1} \to 1}$. The conditions (1) and (2) are particularly valuable in real-world applications~\citep{RevModPhys.78.275,WeiÃŸe2008}. The first condition ensures that approximations of positive quantities remain positive, while the second conserves the integral of the expanded function $\int_{-1}^{1}p_{\text{KPM}}(x)\diff{x} = \int_{-1}^{1}f(x)\diff{x}$. Notably, $g_{k} = 1$ is the simplest Gibbs damping factor attributed to the Dirichlet kernel. More details about Gibbs damping factors are in the appendix.

Clearly, finding an appropriate kernel is crucial for approximation, as it determines whether the round-off error is minimized or not. As indicated in \citep{RevModPhys.78.275,WeiÃŸe2008}, kernel choices are data-dependent. More specifically, given a target function, we need to match an appropriate kernel and manually tune its hyperparameters (if the kernel has any) based on experience. Since the target function is unknown, we relax each $\mu_{k}$ with a learnable parameter $w_{k}$. The effectiveness of the Gibbs damping factors lie in their ability to reduce the weight of each term of the Chebyshev coefficients, thereby mitigating the contributions of higher-order terms. Based on this observation, and in order to prevent over-fitting, we propose the following loss function which is named the kernel polynomial loss (KPL):
\begin{equation}\label{eq:kpl}
\centering
\mathcal{L}_{\text{KP}} = \int_{-1}^{1}\abs{\frac{\diff{f(x)}}{\diff{x}}}^{2}\diff{x} \approx \sum_{k=1}^{K}{\pi}{k}^{2}\abs{w_{k}}^{2}.
\end{equation}
This results in an intuitive penalty applied to the Chebyshev coefficients, with higher order Chebyshev coefficients incurring greater penalties than the lower ones. It causes the Chebyshev polynomial interpolation with the kernel polynomial loss to simulate the kernel polynomial method with a learnable kernel. We apply the kernel polynomial method with Synolution, which turns out what we call Kernelution. The corresponding formula is defined as
\begin{equation}\label{eq:kernelution}
\centering
\mathrm{Kern}(\mathbf{X}\mathbf{W}_{\text{V}})
= \bm{\Phi}^{-1}\left[\exp\left(\ramuno\cdot{p}_{\text{KP}}(\mathbf{\Lambda})\right)\odot(\bm{\Phi}\mathbf{X}\mathbf{W}_{\text{V}})\right].
\end{equation}
It is worth noting that the kernel polynomial method is not the only operation compatible with Synvolution. Depending on practical requirements, Synvolution can also be made compatible with the multi-head operation, similar to other attention mechanisms. This means Synvolution can be equipped as a substitute for self-attention in Transformer-based models.

\subsection{Gated Feed-Forward Network and PostScaleNorm}
Both Synvolution and Kernelution effectively represent the direction and model the relationship between feature tokens in the spectral domain. A tricky problem is that the output of either Synvolution or Kernelution is complex-valued, whereas the labels are real-valued. This conflict motivates us to design a layer that maps a complex-valued tensor into a real-valued tensor. We propose a Gated Feed-Forward Network (GFNN) to solve this issue.
\begin{equation}\label{eq:gffn}
\centering
\mathrm{GFFN}(\mathbf{X}) = \left[\mathrm{softplus}(\Re(\mathbf{X})\mathbf{W}_{\Re})\odot\tanh(\Im(\mathbf{X})\mathbf{W}_{\Im})\right]\mathbf{W}_{\text{O}},
\end{equation}
where $\mathbf{W}_{\Re} \in \mathbb{R}^{D \times D_\text{hid}}$, $\mathbf{W}_{\Im} \in \mathbb{R}^{D \times D_\text{hid}}$ and $\mathbf{W}_{\text{O}} \in \mathbb{R}^{D_\text{hid} \times D}$ are trainable weight matrices. We let the real part to learn the magnitude, and the imaginary part to learn the sign. Besides, we apply the PostNorm architecture~\citep{wang-etal-2019-learning-deep} with ScaleNorm~\citep{nguyen-salazar-2019-transformers} across the whole model, namely PostScaleNorm. Specifically, we apply $\mathrm{ScaleNorm}(\mathbf{Z} + \zeta \cdot \Re(\mathbf{Z}) + (1 - \zeta) \cdot \Im(\mathbf{Z}))$ for a complex-valued signal $\mathbf{Z}$, where $\zeta \in [0, 1]$ is a learnable parameter.