\section{Conclusion and Future Work}
In this work, we introduce Converter, a Transformer variant that replaces self-attention with a synthetic unitary digraph convolution called Synvolution. By leveraging the inverse process of eigendecomposition and LQ factorization, we synthesize a unitary digraph shift operator with learnable eigenvalues and eigenvectors. Our fast $1$-DHHP implementation achieves linearithmic time complexity while preserving the full-rank property of Synvolution. We further enhance Synvolution by incorporating the kernel polynomial method, resulting in Kernelution. We propose a kernel polynomial loss to enable dynamic kernel adaptation during training.

We evaluate Converter on the Long-Range Arena benchmark, long document classification, and DNA sequence-based taxonomy classification tasks. The experimental results demonstrate the strong performance of Converter. This work takes a solid step forward in applying digraph convolution to large datasets under the architecture of Transformer. Future work will focus on developing the decoder component for cross-attention. We believe our synthetic unitary digraph convolution approach will inspire further research into the relationships among self-attention, digraph convolution, and convolution, opening new possibilities for designing more powerful and efficient neural network architectures that combine the strengths of these different approaches.