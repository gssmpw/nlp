\section{Background and Preliminary}
\subsection{Self-Attention and Multi-Head Self-Attention}
Given an input signal $\mathbf{X} \in \mathbb{R}^{N \times D}$, the self-attention (SA)~\citep{NIPS2017_3f5ee243} is defined as
\begin{equation}\label{eq:shsa}
\centering
\mathrm{SA}(\mathbf{X}) = 
\mathrm{Softmax}\left(\frac{\mathbf{X}\mathbf{W}_{\text{Q}}(\mathbf{X}\mathbf{W}_{\text{K}})^{\mathrm{T}}}{\tau}\right)\mathbf{X}\mathbf{W}_{\text{V}},
\end{equation}
where $\tau=\sqrt{D_{h}}$ is the temperature parameter, the softmax function is applied row-wise, $\mathrm{T}$ represents the transpose operation, $\mathbf{X}\mathbf{W}_{\text{Q}} \in \mathbb{R}^{N \times D_{h}}$, $\mathbf{X}\mathbf{W}_{\text{K}} \in \mathbb{R}^{N \times D_{h}}$, and $\mathbf{X}\mathbf{W}_{\text{V}} \in \mathbb{R}^{N \times D_{h}}$ are referred to as the query, key, and value matrix, in which $\mathbf{W}_{\text{Q}} \in \mathbb{R}^{D \times D_{h}}$, $\mathbf{W}_{\text{K}} \in \mathbb{R}^{D \times D_{h}}$, and $\mathbf{W}_{\text{V}} \in \mathbb{R}^{D \times D_{h}}$ are learnable parameters. Conventionally, the Multi-Head Self-Attention (MHSA) with $H$ heads is commonly chosen to enhance performance. It is defined as
\begin{equation}\label{eq:mha}
\centering
\mathrm{MHSA}(\mathbf{X}) = \left(\Big{\Vert}_{h=1}^{H}\mathrm{SA}(\mathbf{X})_{h}\right)\mathbf{W}_{\text{O}},
\end{equation}
where $\Big{\Vert}$ represents the concatenation operation, and $\mathbf{W}_{\text{O}} \in \mathbb{R}^{HD_{h} \times D}$ is a learnable parameter. Here, $D_{h} = D/H$.

\subsection{Digraph Signal Processing}
In this work, we follow the definitions of digraph signal processing (DGSP)~\citep{10388222,6409473,6638850,6808520}. A digraph is represented as $G = \{\mathcal{V},\mathcal{E}\}$, where $\mathcal{V}$ denotes the set of vertices with $|\mathcal{V}| = N$, and $\mathcal{E} \subseteq \mathcal{V}\times\mathcal{V}$ represents the set of edges. A digraph adjacency matrix is denoted by $\mathbf{A} \in \mathbb{C}^{N \times N}$, where each element corresponds to an edge, and the module of the weight signifies the degree of the edge.

In DGSP, a digraph shift operator (DGSO) $\mathbf{S}_{G}$ is a matrix defining the manner in which a digraph signal transitions from one node to its neighboring nodes based on the underlying digraph topology. More precisely, a DGSO constitutes a local operator that substitutes the digraph signal value at each node with a linear combination of its neighboring nodes' values. It is a fundamental assumption to employ a diagonalizable (normalized) digraph adjacency or Laplacian matrix as a DGSO to perform a digraph convolution. In this situation, every digraph signal can be represented as a linear combination of the eigenvectors of the DGSO. 

A digraph filter $\mathcal{H}_{\mathbf{\theta}}(\mathbf{S}_{G}) \in \mathbb{C}^{N \times N}$ is a function of the DGSO termed the digraph frequency response function where eigenvalues are perceived as digraph frequencies. In DGSP, a kind of widely adopted digraph filter is based on polynomials, such a digraph filter is defined as $\mathcal{H}_{\mathbf{\theta}}(\mathbf{S}_{G}) = \sum^{K}_{k=0}{{\theta}_{k}\mathbf{S}_{G}^{k}}$, where ${\theta}_{k}$ is the corresponding coefficient. This form of digraph filter is called the Finite Impulse Response (FIR) filter. A typical FIR filter is based on the Chebyshev polynomials of the first kind~\citep{HAMMOND2011129}~\footnote{The following is abbreviated as Chebyshev polynomials.}. For input $x \in [-1, 1]$, through three-term recurrence relations, the Chebyshev polynomials are obtained as $T_{k}(x) = 2x\cdot{T}_{k-1}(x) - T_{k-2}(x)$, with $T_{0}(x) = 1$ and $T_{1}(x) = x$. Combining a polynomial based filter, the procedure of the digraph convolution (DGConv) can be articulated as
\begin{equation}\label{eq:digraph_conv_eigenvalue}
\centering
\mathrm{DGConv}(\mathbf{S}_{G}, \mathbf{X}) 
= \mathbf{U}^{-1}\left[\mathcal{H}_{\mathbf{\theta}}(\mathbf{\Lambda})\odot(\mathbf{U}\mathbf{X})\right],
\end{equation}
where $\mathbf{U} \in \mathbb{C}^{N \times N}$ is the eigenvector matrix of $\mathbf{S}_{G}$. $\hat{\mathbf{X}} = \mathbf{U}\mathbf{X} \in \mathbb{C}^{N \times D}$ represents the digraph Fourier transform applied to the input signals, while $\mathbf{X} = \mathbf{U}^{-1}\hat{\mathbf{X}} \in \mathbb{C}^{N \times D}$ denotes the inverse transform.

% \subsection{Structured Matrices}
% Given an input signal $\mathbf{x} \in \mathbb{C}^{N}$, a structured random or learnable weighted matrix $\mathbf{\Phi} \in \mathbb{C}^{N \times N}$, we can describe the whole process as $\mathbf{y} = \mathbf{\Phi}\mathbf{x}$. When the time complexity is equals to or less than sub-quadratic,~\citet{moczulski2016acdc} refer to this type of linear layers as the Structured Efficient Linear Layer. All current known SELL matrices~\citep{10.1145/1132516.1132597,CHARIKAR20043,pmlr-v28-le13,Yang_2015_ICCV,Cheng_2015_ICCV,NIPS2016_53adaf49} are summarized as
% \begin{equation}\label{eq:sell}
% \centering
% \begin{split}
% \mathbf{\Phi}_{\text{SELL}} = \mathcal{C}(\mathbf{D}, \mathbf{B}, \mathbf{S}, \mathbf{P}),
% \end{split}
% \end{equation}
% where $\mathcal{C}(\cdot)$ denotes combination, $\mathbf{D}$ is a diagonal matrix, $\mathbf{B} \in \{\mathbf{C}, \mathbf{F}, \mathbf{H}\}$ for bases such as cosine, Fourier, and Walsh–Hadamard, $\mathbf{S}$ is a sparse matrix, and $\mathbf{P}$ is a permutation matrix.

% SELL matrices including both random and learnable structured matrices, which means SELL can be considered as either random or linear projection. Based on the Johnson–Lindenstrauss lemma~\citep{MR0737400} and structured matrices, fast random projection is executable. In SRHT~\citep{10.1145/1132516.1132597}, $\mathbf{\Phi}_{\text{SRHT}} = \mathbf{D}\mathbf{H}\mathbf{S}$, where $\mathbf{D}$ is a random diagonal matrix with its diagonal entries sampled from the Rademacher distribution. As for Count Sketch, the structured matrix is defined as $\mathbf{\Phi}_{\text{CS}} = \mathbf{D}\mathbf{S}$, where $\mathbf{D}$ is the same diagonal matrix defined in SRHT. The Fastfood matrix is $\mathbf{\Phi}_{\text{Fastfood}} = \frac{1}{\sigma\sqrt{N}}\mathbf{D}_{1}\mathbf{H}\mathbf{P}\mathbf{D}_{2}\mathbf{H}\mathbf{D}_{3}$, where $\sigma$ means standard deviation, while all diagonal matrices are learnable in the Adaptive Fastfood matrix. $\mathbf{\Phi}_{\text{SORF}} = \frac{1}{{\sigma}N}\mathbf{H}\mathbf{D}_{1}\mathbf{H}\mathbf{D}_{2}\mathbf{H}\mathbf{D}_{3}$, $\mathbf{D}$ is the same diagonal matrix defined in SRHT. For ACDC, $\mathbf{\Phi}_{\text{ACDC}} = \prod_{l=1}^{L}\mathbf{D}^{(l)}_{1}\mathbf{C}\mathbf{D}^{(l)}_{2}\mathbf{C}^{-1}$. For AFDF, $\mathbf{\Phi}_{\text{AFDF}} = \prod_{l=1}^{L}\mathbf{D}^{(l)}_{1}\mathbf{F}\mathbf{D}^{(l)}_{2}\mathbf{F}^{-1}$. 

% In short, both ACDC and AFDF, share the same parametrization pattern, are the special cases of DCD decomposition~\citep{Huhtanen2015}.
% \begin{equation}\label{eq:dcd_decomposition}
% \centering
% \begin{split}
% \mathbf{\Phi}_{\text{DCD}} = \left(\prod_{l}^{L}\mathbf{D}^{(l)}\mathbf{C}^{(l)}\right)\mathbf{D},
% \end{split}
% \end{equation}
% where $\mathbf{C}^{(l)} \in \mathbb{C}^{N \times N}$ is a unitary circulant matrix. The following theorem ensures that if the order of both ACDC and AFDF is large enough, they can construct an arbitrary real-valued and complex-valued square matrix, respectively.
% \begin{theorem}(DCD decomposition~\citep{Huhtanen2015}.)
% For factorizing an arbitrary $N \times N$ squared matrix with DCD decomposition, at most $L = N-1$ orders are sufficient.
% \end{theorem}

% Another structured matrix factorization is so-called Chord sparse matrix factorization, which is defined as
% \begin{equation}\label{eq:chord_factorization}
% \centering
% \begin{split}
% \mathbf{\Phi}_{\text{Chord}} = \prod_{l=1}^{\log{N}}\mathbf{C}^{(l)},
% \end{split}
% \end{equation}
% where each $\mathbf{C}^{(l)} \in \mathbb{R}^{N \times N}$ is a Chord sparse sub-matrix with $N\log{N}$ non-zero elements that
% \begin{equation}
% \centering
% \begin{split}
% \mathbf{C}^{(l)}_{i,j} =
% \left\{
% \begin{aligned}
% \mathbf{E}^{(l)}_{i,1} & = & \text{if}\ j=i,\\
% \mathbf{E}^{(l)}_{i,k} & = & \text{if}\ j=(1+2^{k-2}) \mod k,\\
% 0 & = & \text{otherwise}.
% \end{aligned}
% \right.
% \end{split}
% \end{equation}
% Here, $\mathbf{E}^{(l)} \in \mathbb{R}^{N \times \log{N}}$ is a learnable embedding matrix.

% The Butterfly matrix in \citep{pmlr-v97-dao19a} is defined as
% \begin{equation}\label{eq:butterfly_factorization}
% \centering
% \begin{split}
% \mathbf{\Phi} = \prod_{l=1}^{N\log{N}}\mathbf{B}^{(l)}
% \end{split}
% \end{equation}
% Enlighten by the $4$-step FFT algorithm~\citep{Bailey1990},~\citet{pmlr-v162-dao22a} propose the Monarch matrix as
% \begin{equation}\label{eq:monarch}
% \centering
% \begin{split}
% \mathbf{\Phi} = \mathbf{P}\mathbf{L}\mathbf{P}^{\mathrm{T}}\mathbf{R}.
% \end{split}
% \end{equation}
% Here, both $\mathbf{L}$ and $\mathbf{R}$ are block diagonal matrices that $\mathbf{L}$ is obtained by multiplying together the first $(\log{N})/2$ butterfly factor matrices in Equation~\ref{eq:butterfly_factorization} and $\mathbf{R}$ is obtained by multiplying together the rest butterfly factor matrices, and $\mathbf{P}$ is a fixed permutation matrix that reshapes input into square or rectangle, transposes it, and reshapes back.