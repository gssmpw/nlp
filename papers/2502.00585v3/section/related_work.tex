\section{Related Work}
\noindent\textbf{Self-Attention Alternatives.}
Central to self-attention is the scaled dot-product operation performed on a pair of query and key matrices, yielding an affinity matrix with a softmax function. Due to its high time complexity, various alternative methodologies have been proposed to approximate or replace scaled dot-product attention. Approximate approaches typically involve sparse~\citep{child2019generating,Kitaev2020Reformer,NEURIPS2020_c8512d14}, low-rank~\citep{choromanski2021rethinking}, or sparse $\text{+}$ low-rank~\citep{NEURIPS2021_9185f3ec} techniques. For replacements, common approaches include convolution~\citep{yu2022metaformer}, pooling~\citep{Yu_2022_CVPR}, and discrete Fourier transform~\citep{lee-thorp-etal-2022-fnet}. Recently, a spatial construction method~\citep{pmlr-v139-tay21a,9878955} has emerged, utilizing the inverse process of matrix decomposition to synthesize attention matrices. This approach has inspired us to propose a synthetic attention.

\noindent\textbf{Multi-Head Attention.}
Multi-head attention may not be more efficient than single-head. \citet{NEURIPS2019_2c601ad9} discover that over 50\% attention heads can be pruned during the testing phase. Similarly, \citet{voita-etal-2019-analyzing} conclude that only a small subset of heads is critical for translation tasks. Furthermore, \citet{cordonnier2020multihead} identify redundant feature representations in multi-head self-attention. Additionally, \citet{pmlr-v119-bhojanapalli20a} observe that a large number of heads cause a low-rank bottleneck, which restricts the representation capacity of Transformers. Based on these observations, we focus on single-head attention.

\noindent\textbf{Digraph Fourier Transform.}
The Digraph Fourier Transform serves as the cornerstone for digraph convolution, based on the fundamental assumption that it requires a diagonalizable digraph shift operator~\citep{10388222}. There are two distinct categories of methods that aim to achieve the digraph Fourier transform. The first category involves replacing the eigendecomposition with an alternative matrix decomposition to build orthogonal or unitary bases~\citep{6409473,6638850,6808520,7746675}, while the second entails spatially constructing a normal digraph shift operator~\citep{Chung2005,PhysRevE.95.022302,FANUEL2018189}. We draw inspiration from the two categories of methods and propose a unique one that synergizes both approaches.

\noindent\textbf{Structured Matrices.}
In random and linear projections, structured matrices are aimed to reduce time complexity. One major effect in random projection is improving Johnson-Lindenstrauss transform~\citep{10.1145/1132516.1132597,10.1145/1806689.1806737}. Another prominent effect of structured matrices is random features~\citep{pmlr-v28-le13,NIPS2016_53adaf49}. By replacing random entities with learnable parameters, linear projection has been in developed recent years. A well-known example is the Adaptive Fastfood transform~\citep{Yang_2015_ICCV}, which extends the vanilla Fastfood transform~\citep{pmlr-v28-le13} by incorporating learnable diagonal matrices. Building on these developments, \citet{moczulski2016acdc} unify these structured matrices under the SELL matrix family and introduce ACDC and AFDF matrices. These approaches enlighten us to design structured unitary matrices under quadratic time complexity.