\documentclass{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath,amssymb,amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{commath}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{upgreek}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\sech}{\mathrm{sech}}
\newcommand{\eu}{\mathrm{e}\mkern1mu}
\newcommand{\ramuno}{\mathrm{i}\mkern1mu}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\sinc}{sinc}


\newcommand{\apbbox}[1]{AP$^\text{bbox}_\text{#1}$}
\newcommand{\apmask}[1]{AP$^\text{mask}_\text{#1}$}
\newcommand{\ap}[1]{AP$_\text{#1}$}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\newcommand{\cgap}[2]{
	\fontsize{6pt}{1em}\selectfont{(${#1}${#2})}
}
\definecolor{Gray}{gray}{0.5}
\newcommand{\demph}[1]{\textcolor{Gray}{#1}}
\definecolor{Highlight}{HTML}{39b54a}  % green
\newcommand{\cgaphl}[2]{
	\fontsize{6pt}{1em}\selectfont{\textcolor{Highlight}{(${#1}$\textbf{#2})}}
}
\newcommand{\hl}[1]{\textcolor{Highlight}{#1}}


\title{Converting Transformers into DGNNs Form}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\newif\ifuniqueAffiliation
% Comment to use multiple affiliations variant of author block 
\uniqueAffiliationtrue


\author{
Jie Zhang \\
National Central University, Taiwan\\
\texttt{hazdzz@g.ncu.edu.tw}
\And
Mao-Hsuan Mao \\
National Central University, Taiwan\\
\texttt{mmh.nuss@gmail.com}
\And
Bo-Wei Chiu \\
National Central University, Taiwan\\
\texttt{h23468270@g.ncu.edu.tw}
\And
Min-Te Sun \\
National Central University, Taiwan\\
\texttt{msun@csie.ncu.edu.tw}
}



% Uncomment to override  the `A preprint' in the header
% \renewcommand{\headeright}{Technical Report}
% \renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Converting Transformers into DGNNs Form}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Converting Transformers into DGNNs Form},
pdfsubject={cs.LG, cs.CL},
pdfauthor={Jie Zhang, Kuan-Chieh Wang, Bo-Wei Chiu, Min-Te Sun},
pdfkeywords={Transformer, DGNN},
}

\begin{document}
\maketitle

\begin{abstract}
Recent advances in deep learning have established Transformer architectures as the predominant modeling paradigm. Central to the success of Transformers is the self-attention mechanism, which scores the similarity between query and key matrices to modulate a value matrix. This operation bears striking similarities to digraph convolution, prompting an investigation into whether digraph convolution could serve as an alternative to self-attention. In this study, we formalize this concept by introducing a synthetic unitary digraph convolution based on the digraph Fourier transform. The resulting model, which we term Converter, effectively converts a Transformer into a Directed Graph Neural Network (DGNN) form. We have tested Converter on Long-Range Arena benchmark, long document classification, and DNA sequence-based taxonomy classification. Our experimental results demonstrate that Converter achieves superior performance while maintaining computational efficiency and architectural simplicity, which establishes it as a lightweight yet powerful Transformer variant.
\end{abstract}


\input{section/introduction}
\input{section/related_work}
\input{section/preliminary}
\input{section/proposed_method}
\input{section/experiments}
\input{section/conclusion}


\newpage
\bibliographystyle{unsrtnat}
\bibliography{references}

\newpage
\appendix
\input{section/appendix}


\end{document}
