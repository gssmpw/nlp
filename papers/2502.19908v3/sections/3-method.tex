
\section{Method}




\subsection{Preliminaries}

\textbf{MDP} is used to model sequential decision problems, formulated as a tuple $\left<\mathcal{S}, \mathcal{A}, P_{\tau}, R, \rho_0, \gamma, T\right>$.
$\mathcal{S}$ is the state space.
$\mathcal{A}$ is the action space.
$P_{\tau} : \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})$ \footnote{$\Delta(\mathcal{X})$ denotes the set of probability distribution over set $\mathcal{X}$.} is the state transition probability.
$R : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$  denotes the reward function and is bounded.
$\rho_0 \in \Delta(\mathcal{S})$ is the initial state distribution.
$T$ is the time horizon and $\gamma$ is the discount factor of future rewards.
The state-action sequence is defined as $\tau = (\boldsymbol{s}_0, a_0, \boldsymbol{s}_1, a_1, \dots, \boldsymbol{s}_T)$, where $\boldsymbol{s}_t \in \mathcal{S}$ and $a_t \in \mathcal{A}$ are the state and action at time step $t$.
The objective of RL is to maximize the expected return:
\begin{equation}
\setlength{\abovedisplayskip}{0pt}
\begin{split}
    \max_{\pi} \mathbb{E}_{\boldsymbol{s}_t \sim {P_{\tau}}, a_t \sim \pi} [\sum_{t=0}^{T}\gamma^{t} R(\boldsymbol{s}_t, a_t)].  \label{equation:mdp}
\end{split}
\setlength{\belowdisplayskip}{0pt}
\end{equation}

\noindent\textbf{Vectorized state representation}. State $\boldsymbol{s}_t$ contains map and agent information in vectorized representation~\cite{gao2020vectornet}.
Map information $m$ includes the road network, traffic lights, etc, which are represented by polylines and polygons.
Agent information includes the current and past poses of ego vehicle and other traffic agents, which are represented by polylines. The index of ego vehicle is $0$ and the indices of traffic agents range from $1$ to $N$. For each agent $i$, its history is denoted as $s^{i}_{t-H:t}, i \in \{0, 1, \dots, N\}$, where $H$ is the history time horizon.


\subsection{Problem Formulation}

We model the trajectory planning task as a sequential decision process and decouple the auto-regressive models into policy and transition models.
The key to connect trajectory planning and auto-regressive models is to define the action as the next pose of ego vehicle, i.e., $a_t = s^{0}_{t+1}$. Therefore, after forwarding the auto-regressive model, the decoded pose is collected to be the ego-planned trajectory.
Specifically, we can reduce the state-action sequence to the state sequence under this definition and vectorized representation:
\begin{equation}
\setlength{\abovedisplayskip}{0pt}
\begin{split}
    &P(\boldsymbol{s}_0, a_0, \boldsymbol{s}_1, a_1, \dots, \boldsymbol{s}_T) \\
    &= P(m, s^{0:N}_{-H:0}, s^{0}_{1}, m, s^{0:N}_{1-H:1}, s^{0}_{2}, \dots, m, s^{0:N}_{T-H:T}) \\
    &= P(m, s^{0:N}_{-H:0}, m, s^{0:N}_{1-H:1}, \dots, m, s^{0:N}_{T-H:T}) \\
    &= P(\boldsymbol{s}_0, \boldsymbol{s}_1, \dots, \boldsymbol{s}_T).
    \\[-0.1cm]
\end{split}
\setlength{\belowdisplayskip}{0pt}
\end{equation}
The state sequence can be further formulated in an auto-regressive fashion and decomposed into policy and transition model:
\begin{equation}
\setlength{\abovedisplayskip}{0pt}
\begin{split}
    &P(\boldsymbol{s}_0, \boldsymbol{s}_1, \dots, \boldsymbol{s}_T) = \rho_{0}(\boldsymbol{s}_0) \prod_{t=0}^{T-1} P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t) \\
    &= \rho_{0}(\boldsymbol{s}_0) \prod_{t=0}^{T-1} P(s^0_{t+1}, s^{1:N}_{t+1} | \boldsymbol{s}_t) \\
    &= \rho_{0}(\boldsymbol{s}_0) \prod_{t=0}^{T-1} \underbrace{\pi(a_t | \boldsymbol{s}_t)}_{\text{Policy}} \underbrace{P_{\tau}(s^{1:N}_{t+1} | \boldsymbol{s}_t)}_{\text{Transition Model}}.
    \\[-0.15cm]
\end{split}
\label{equation:incons}
\setlength{\belowdisplayskip}{0pt}
\end{equation}
From \cref{equation:incons}, we can clearly identify the inherent problem associated with the typical auto-regressive approach: inconsistent behaviors across time steps arise from the policy distribution, which depends on random sampling from the action distribution.

To solve the above problem, we introduce consistent mode information $\boldsymbol{c}$ that remains unchanged across time steps into the auto-regressive fashion:
\begin{equation}
\setlength{\abovedisplayskip}{0pt}
\begin{split}
&P(\boldsymbol{s}_0, \boldsymbol{s}_1, \dots, \boldsymbol{s}_T) = \int_{\boldsymbol{c}} P(\boldsymbol{s}_0, \boldsymbol{s}_1, \dots, \boldsymbol{s}_T, \boldsymbol{c}) d\boldsymbol{c}  \\
&= \rho_0(\boldsymbol{s}_0) \int_{\boldsymbol{c}} P(\boldsymbol{c} | \boldsymbol{s}_0) P(\boldsymbol{s}_1, \dots, \boldsymbol{s}_T | \boldsymbol{c}) d\boldsymbol{c} \\
&= \rho_0(\boldsymbol{s}_0) \prod_{t=0}^{T-1} \underbrace{P_{\tau}(s^{1:N}_{t+1} | \boldsymbol{s}_t)}_{\text{Transition Model}} \int_{\boldsymbol{c}} \underbrace{P(\boldsymbol{c} | \boldsymbol{s}_0)}_{\text{Mode Selector}} \prod_{t=0}^{T-1} \underbrace{\pi(a_t | \boldsymbol{s}_t, \boldsymbol{c})}_{\text{Policy}}   d\boldsymbol{c}. 
\\[-1.cm]
\label{equation:car}
\end{split}
\setlength{\belowdisplayskip}{0pt}
\end{equation}
Since we focus on the ego trajectory planning, the consistent mode $\boldsymbol{c}$ does not impact transition model.

This consistent auto-regressive formulation defined in \cref{equation:car} reveals a generation-selection framework where the mode selector scores each mode based on the initial state $s_0$ and the trajectory generator generates multi-modal trajectories via sampling from the mode-conditioned policy. 



\noindent\textbf{Non-reactive transition model.} The transition model formulated in \cref{equation:car} {needs to be employed in every time step since it} produces the poses of traffic agents at time step $t+1$ based on current state $\boldsymbol{s}_t$. In practice, this process is time-consuming and we do not observe a performance improvement by using this transition model, therefore, we use trajectory predictors $P(s^{1:N}_{1:T} | \boldsymbol{s}_0)$ as non-reactive transition model {that produces all future poses of traffic agents in one shot given initial state $\boldsymbol{s}_0$}.

\subsection{Planner Architecture}

The framework of our proposed \textbf{CarPlanner} is illustrated in \cref{figure:planner}, comprising four key components: 1) the non-reactive transition model, 2) the mode selector, 3) the trajectory generator, and 4) the rule-augmented selector.

Our planner operates within a generation-selection framework. Given an initial state $s_0$ and all possible  $N_{\text{mode}}$ modes, the trajectory selector evaluates and assigns scores to each mode. The trajectory generator then produces $N_{\text{mode}}$ trajectories that correspond to their respective modes.
For trajectory generator, the initial state $s_0$ is replicated $N_{\text{mode}}$ times, each associated with one of the $N_{\text{mode}}$ modes, effectively creating $N_{\text{mode}}$ parallel worlds. The policy is executed within these previewed worlds. During the policy rollout, a trajectory predictor acts as the state transition model, generating future poses of traffic agents across all time horizons.


\subsubsection{Non-reactive Transition Model}

This module takes the initial state $s_0$ as input and outputs the future trajectories of traffic agents. The initial state is processed by agent and map encoders, followed by a self-attention Transformer encoder~\cite{vaswani2017attention} to fuse the agent and map features. The agent features are then decoded into future trajectories.

\noindent\textbf{Agent and map encoders.} The state $s_0$ contains both map and agent information. The map information $m$ consists of $N_{m,1}$ polylines and $N_{m,2}$ polygons. The polylines describe lane centers and lane boundaries, with each polyline containing $3N_p$ points, where $3$ corresponds to the lane center, the left boundary, and the right boundary. Each point is with dimension $D_m=9$ and includes the following attributes: x, y, heading, speed limit, and category. When concatenated, the points of the left and right boundaries together with the center point yield a dimension of $N_{m,1} \times N_p \times 3D_m$. We leverage a PointNet~\cite{qi2017pointnet} to extract features from the points of each polyline, resulting in a dimensionality of $N_{m,1} \times D$, where $D$ represents the feature dimension. The polygons represent intersections, crosswalks, stop lines, etc, with each polygon containing $N_p$ points. We utilize another PointNet to extract features from the points of each polygon, producing a dimension of $N_{m,2} \times D$. We then concatenate the features from both polylines and polygons to form the overall map features, resulting in a dimension of $N_{m} \times D$. The agent information $A$ consists of $N$ agents, where each agent maintains poses for the past $H$ time steps. Each pose is with dimension $D_a=10$ and includes the following attributes: x, y, heading, velocity, bounding box, time step, and category. Consequently, the agent information has a dimension of $N \times H \times D_a$. We apply another PointNet to extract features from the poses of each agent, yielding an agent feature dimension of $N \times D$.


\subsubsection{Mode Selector}
This module takes \( s_0 \) and longitudinal-lateral decomposed mode information as input and outputs the probability of each mode. The number of modes $N_{\text{mode}} = N_{\text{lat}} N_{\text{lon}}$.

\noindent\textbf{Route-speed decomposed mode.} To capture the longitudinal behaviors, we generate \( N_{\text{lon}} \) modes that represent the average speed of the trajectory associated with each mode. Each longitudinal mode \( c_{\text{lon},j} \) is defined as a scalar value of \( \frac{j}{N_{\text{lon}}} \), repeated across a dimension \( D \). As a result, the dimensionality of the longitudinal modes is \( N_{\text{lon}} \times D \). For lateral behaviors, we identify \( N_{\text{lat}} \) possible routes from the map using a graph search algorithm. These routes correspond to the lanes available for the ego vehicle. The dimensionality of these routes is \( N_{\text{lat}} \times N_r \times D_m \). We employ another PointNet to aggregate the features of the \( N_r \) points along each route, producing a lateral mode with a dimension of \( N_{\text{lat}} \times D \). To create a comprehensive mode representation $\boldsymbol{c}$, we combine the lateral and longitudinal modes, resulting in a combined dimension of \( N_{\text{lat}} \times N_{\text{lon}} \times 2D \). To align this mode information with other feature dimensions, we pass it through a linear layer, mapping it back to \( N_{\text{lat}} \times N_{\text{lon}} \times D \).

\noindent\textbf{Query-based Transformer decoder.} This decoder is employed to fuse the mode features with map and agent features derived from \( s_0 \). In this framework, the mode serves as the query, while the map and agent information act as the keys and values. The updated mode features are decoded through a multi-layer perceptron (MLP) to yield the scores for each mode, which are subsequently normalized using the softmax operator.


\subsubsection{Trajectory Generator}
This module operates in an auto-regressive manner, recurrently decoding the next pose of the ego vehicle $a_t$, given the current state $\boldsymbol{s}_t$, and consistent mode information $\boldsymbol{c}$.

\noindent\textbf{Invariant-view module (IVM).} Before feeding the mode and state into the network, we preprocess them to eliminate time information. For the map and agent information in state $\boldsymbol{s}_t$, we select the $K$-nearest neighbors~(KNN) to the ego current pose and only feed these into the policy.
$K$ is set to the half of map and agent elements respectively.
Regarding the routes that capture lateral behaviors, we filter out the segments where the point closest to the current pose of the ego vehicle is the starting point, retaining $K_r$ points. In this case, $K_r$ is set to a quarter of $N_r$ points in one route. Finally, we transform the routes, agent, and map poses into the coordinate frame of the ego vehicle at the current time step $t$. We subtract the historical time steps $t-H:t$ from the current time step $t$, yielding time steps in range $-H:0$.

\noindent\textbf{Query-based Transformer decoder.} We employ the same backbone network architecture as the mode selector, but with different query dimensions. Due to the IVM and the fact that different modes yield distinct states, the map and agent information cannot be shared among modes. As a result, we fuse information for each individual mode. Specifically, the query dimension is \(1 \times D\), while the dimensions of the keys and values are \((N + N_{m}) \times D\). The output feature dimension remains \(1 \times D\).
Note that Transformer decoder can process information from multiple modes in parallel, eliminating the need to handle each mode sequentially.


\noindent\textbf{Policy output.} The mode feature is processed by two distinct heads: a policy head and a value head. Each head comprises its own MLP to produce the parameters for the action distribution and the corresponding value estimate. We employ a Gaussian distribution to model the action distribution, where actions are sampled from this distribution during training. In contrast, during inference, we utilize the mean of the distribution to determine the actions.




\subsubsection{Rule-augmented Selector}
This module is only utilized during inference and takes as input the initial state \( s_0 \), the multi-modal ego-planned trajectories, and the predicted future trajectories of agents. It calculates driving-oriented metrics such as safety, progress, comfort. A comprehensive score is obtained by the weighted sum of rule-based scores and the mode scores provided by the mode selector. The ego-planned trajectory with the highest score is selected as the output of the planner.



\subsection{Training}
We first train the non-reactive transition model and freeze the weights during the training of the mode selector and trajectory generator. Instead of feeding all modes to the generator, we apply a winner-takes-all strategy, wherein a positive mode is assigned based on the ego ground-truth trajectory and serves as a condition for the trajectory generator.

\noindent\textbf{Mode assignment.}
For the lateral mode, we assign the route closest to the endpoint of ego ground-truth trajectory as the positive lateral mode.
For the longitudinal mode, we partition the longitudinal space into \(N_{\text{lon}}\) intervals and assign the interval containing the endpoint of the ground-truth trajectory as the positive longitudinal mode.

\noindent\textbf{Reward function.} To handle diverse scenarios, we use the negative displacement error (DE) between the ego future pose and the ground truth as a universal reward. We also introduce additional terms to improve trajectory quality: collision rate and drivable area compliance. If the future pose collides or falls outside the drivable area, the reward is set to -1; otherwise, it is 0.

\noindent\textbf{Mode dropout.}
In some cases, there are no available routes for ego to follow. However, since routes serve as queries in Transformer, the absence of a route can lead to unstable or hazardous outputs. To mitigate this issue, we implement a mode dropout module during training that randomly masks routes to prevent over-reliance on this information.
% To prevent over-reliance on mode or route information due to Transformers' residual connections, we implement a mode dropout module during training that randomly masks the route to mitigate this issue.

\noindent\textbf{Loss function.}
For the selector, we use cross-entropy loss that is the negative log-likelihood of the positive mode and a side task that regresses the ego ground-truth trajectory.
For the generator, we use PPO~\cite{schulman2017proximal} loss that consists of three parts: policy improvement, value estimation, and entropy. Full description can be found in supplementary.


