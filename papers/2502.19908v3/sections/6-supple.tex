
\clearpage
% \setcounter{page}{1}
\maketitlesupplementary
\appendix


\section{Training Procedure}


\cref{alg:training} outlines the training process for the CarPlanner framework.
% The procedure involves two primary steps: 1)~training the non-reactive transition model, and 2)~training the mode selector and the trajectory generator.
Notably, during the training of the trajectory generator, we have the flexibility to employ either RL or IL, but, in this work, we do not combine RL and IL simultaneously, opting instead to explore their distinct characteristics separately.
The definitions of the loss functions are given in the following.

\noindent\textbf{Loss of non-reactive transition model.}
The non-reactive transition model $\beta$ is trained to simulate agent trajectories based on the initial state $\boldsymbol{s}_0$. For each data sample $(\boldsymbol{s}_0, s^{1:N,\text{gt}}_{1:T}) \in \mathcal{D}$, the model predicts trajectories $s^{1:N}_{1:T} = \beta(\boldsymbol{s}_0)$, and the training objective minimizes the L1 loss:
\begin{equation}
L_{\text{tm}} = \frac{1}{T} \sum_{t=1}^T \sum_{n=1}^N \left\| s_t^n - s_t^{n,\text{gt}} \right\|_1.
\end{equation}

\noindent\textbf{Mode selector loss.}
This contains two parts: cross-entropy and side-task loss.
The cross-entropy loss is defined as:
\begin{equation}
    \text{CrossEntropyLoss}(\boldsymbol{\sigma}, c^*) = -\sum_{i=1}^{N_{\text{mode}}} \mathbb{I}(c_i = c^*) \log \sigma_i,
\end{equation}
where $\sigma_i$ is the assigned score for mode $c_i$, $N_{\text{mode}}$ is the number of candidate modes, and $\mathbb{I}$ is the indicator function.
The side-task loss is defined as:
\begin{equation}
    \text{SideTaskLoss}(\bar{s}^{0}_{1:T}, s^{0,\text{gt}}_{1:T}) = \frac{1}{T} \sum_{t=1}^T \left\| \bar{s}_t^0 - s_t^{0,\text{gt}} \right\|_1,
\end{equation}
where $\bar{s}_t^0$ is the output ego future trajectory.


\noindent\textbf{Generator loss with RL.}
The PPO~\cite{schulman2017proximal} loss consists of three parts: policy, value, and entropy loss.
The policy loss is defined as:
\begin{equation}
\begin{split}
    &\text{PolicyLoss}(a_{0:T-1}, d_{0:T-1, \text{new}}, d_{0:T-1}, A_{0:T-1}) \\
    = &- \frac{1}{T} \sum_{t=0}^{T-1} \min \left( r_t A_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t \right),
\end{split}
\end{equation}
\input{blocks/algorithm.tex}
where the ratio $r_t$ is given by $r_t = \frac{\text{Prob}(a_t, d_{t,\text{new}})}{\text{Prob}(a_t, d_{t})}$, $d_{t,\text{new}}$ and $d_t$ are the policy distributions (mean and standard deviation of Gaussian distribution) at time step $t$ induced by $\pi$ and $\pi_{\text{old}}$ respectively, the function $\text{Prob}(a, d)$ calculates the probability of a given action $a$ under a distribution $d$, and $A_t$ is the advantage estimated using GAE~\cite{schulman2015high}.
The value and entropy loss are defined as:
\begin{equation}
    \text{ValueLoss}(V_{0:T-1, \text{new}}, \hat{R}_{0:T-1}) = \frac{1}{T} \sum_{t=0}^{T-1} \left\| V_{t,\text{new}} - \hat{R}_t \right\|_2^2,
\end{equation}
\begin{equation}
    \text{Entropy}(d_{0:T-1, \text{new}}) = \frac{1}{T} \sum_{t=0}^{T-1} \mathcal{H}(d_{t,\text{new}}),
\end{equation}
\noindent where $V_{t,\text{new}}$ and $\hat{R}_t$ are the predicted and actual returns, and $\mathcal{H}$ represents the entropy of the policy distribution $d$. 


\noindent\textbf{Generator loss with IL.}
In IL, the generator minimizes the trajectory error between the ego-planned trajectory $s^0_{1:T}$ and the ground-truth trajectory $s^{0,\text{gt}}_{1:T}$. The loss is defined as:
\begin{equation}
    L_{\text{generator}} = \frac{1}{T} \sum_{t=1}^T \left\| s_t^0 - s_t^{0,\text{gt}} \right\|_1.
\end{equation}





\section{Implementation Details}


The hyperparameters of model architecture, PPO-related parameters, and loss weights are summarized in \cref{table:param}. The magnitudes of value, policy, and entropy loss are $10^3$, $10^0$, and $10^{-3}$, respectively. The trajectory generator generates trajectories with a time horizon of 8 seconds at 1-second intervals, corresponding to time horizon $T = 8$. During testing, these trajectories are interpolated to 0.1-second intervals.
The weight of scores generated by the rule and mode selectors is set to a ratio of $1:0.3$.
In cases where no ego candidate trajectory satisfies the safety criteria evaluated by the rule selector, an emergency stop is triggered.
For the Test14-Random benchmark, a replanning frequency of 10Hz is employed, adhering to the official nuPlan simulation configuration. In contrast, for the Reduced-Val14 benchmark, a replanning frequency of 1Hz is used to ensure a fair comparison with Gen-Drive~\cite{huang2024gen}.



\begin{table}[t]
\centering
% \captionsetup{font=large}
\centering
    \begin{tabular}{l c}
    \toprule
    {Parameter} & Value \\
    \midrule
    Feature dimension $D$ & 256 \\
    Static point dimension $D_m$ & 9 \\
    Agent pose dimension $D_a$ & 10 \\
    Activation & ReLU \\
    Number of layers & $3$ \\
    Number of attention heads & 8 \\
    % Hidden size & 512 \\
    % Dimension of key and value & 32 \\
    Dropout & 0.1 \\
    \midrule
    discount factor $\gamma$ & 0.1 \\
    GAE parameter $\lambda$ & 0.9 \\
    Clip range $\epsilon$ & 0.2 \\
    Update interval $I$ & 8 \\
    \midrule
    Weight of selector loss & 1 \\
    Weight of value loss & 3 \\
    Weight of policy loss & 100 \\
    Weight of entropy loss & 0.001 \\
    Weight of IL loss & 1 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters of model architecture, PPO-related parameters, and loss weights.}
    \label{table:param}
\end{table}%


\input{blocks/table_supple_rl.tex}


\section{Ablation Study on RL Training}

\input{blocks/figure_time_horizon.tex}
\input{blocks/figure_vis2.tex}

In this part, we examine the training efficiency of CarPlanner, performance of vanilla and consistent auto-regressive frameworks, the use of reactive and non-reactive model in RL training, and the impact of varying the time horizon.


\noindent\textbf{Training efficiency.}
We compare the efficiency of our model-based framework with that of ScenarioNet~\cite{li2023scenarionet}, which is an open-source platform for model-free RL training in real-world datasets~\cite{ettinger2021large, caesar2021nuplan}.
As shown in \cref{table:efficiency}, CarPlanner achieves a remarkable improvement in sampling efficiency, outperforming ScenarioNet by two orders of magnitude. Furthermore, CarPlanner not only excels in efficiency but also achieves SOTA performance, surpassing ScenarioNet by a wide margin.
% This indicates that our framework achieves greater training efficiency.

\noindent\textbf{Vanilla vs. consistent auto-regressive framework.}
The results are shown in \cref{table:supple-abla-rl-reward,table:consistency}.
The consistent auto-regressive framework generates multi-modal trajectories by conditioning on mode representations. In contrast, the vanilla framework relies on random sampling from the action Gaussian distribution to produce multi-modal trajectories. To ensure comparability in the number of modes generated by both frameworks, we sample 60 trajectories in parallel for the vanilla framework.
Given that random sampling introduces variability, we average the results across 3 random seeds.
For the consistent framework, we use displacement error (DE) and final error (FE) as guide functions to assist the policy in generating mode-aligned trajectories.
For the vanilla framework, DE is compared against a progress reward, which encourages longitudinal movement along the route while discouraging excessive lateral deviations that move the vehicle too far from any possible route.
The consistent ratio computes the ratio of generated trajectories that fall in their corresponding modes in longitudinal and lateral directions separately.

Overall, our proposed consistent framework outperforms the vanilla framework in terms of closed-loop performance, highlighting the benefits of incorporating consistency. Furthermore, RL provides more consistant trajectories than the vanilla framework and IL-based methods. Additionally, we find that DE serves as an effective guide function for policy training, further enhancing closed-loop performance.

\noindent\textbf{Reactive vs. non-reactive transition model.}
We compare the performance of the CarPlanner framework when trained with reactive and non-reactive transition models. The reactive transition model shares a similar architecture with the auto-regressive planner for the ego vehicle, utilizing relative pose encoding~\cite{zhang2024real} as the backbone network to extract features of traffic agents and predict their subsequent poses. The training loss and hyperparameters are consistent with those used for the non-reactive transition model.
As shown in \cref{table:supple-abla-rl-tm}, except for the S-Area metric, using non-reactive transition model outperforms the reactive transition model in our current implementation.
The primary difference lies in the assumptions about traffic agents: the reactive transition model assumes that the ego vehicle can negotiate with traffic agents and share the same priority, whereas in the non-reactive model, traffic agents do not respond to the ego vehicle, effectively assigning them higher priority.
A representative example is presented in \cref{figure:vis2}.
When trained with the reactive transition model, the planner assumes pedestrians will yield to the vehicle, leading it to attempt to move forward.
However, at $t_{\text{sim}} = 12s$, the planner collides with pedestrians, triggering an emergency brake, which negatively impacts safety, progress, and comfort metrics.
Although the performance of using reactive transition model is not satisfied currently, it is a more realistic assumption and we will further investigate this in future work.


\noindent\textbf{Time horizon.} We evaluate the CarPlanner framework by training it with different time horizons, including 1, 3, 5, and 8 seconds, and testing the planners in each time horizon.
The results in \cref{figure:supple-time-horizon} confirm that increasing the time horizon has a positive effect on the performance for both training and testing.
A special case is when the training time horizon is set to 1, all tested time horizons exhibit poor performance, highlighting the importance of multi-step learning in RL.
Additionally, the observation that increasing the training time horizon enhances closed-loop performance suggests the potential for further improvements by extending the time horizon beyond 8 seconds.
However, due to current limitations in data preparation, which is designed for horizons up to 8 seconds, expanding the time horizon would not provide map information or ground-truth trajectories, hindering further analysis. Consequently, we leave this exploration for future work.


\section{Comparison with Differentiable Loss}
\input{blocks/table_supple_diff_loss.tex}
\input{blocks/figure_diff_loss.tex}

In typical IL setting, the supervision signal provided to the trajectory generator is the displacement error (DE) between the ego-planned trajectory and the ground-truth trajectory. Several works~\cite{suo2021trafficsim, huang2023gameformer, cheng2024pluto} propose to convert non-differentiable metrics, such as avoiding collision (Col) and adherence to drivable area (Area), into differentiable loss functions that can directly backpropagate to the generator. In contrast, CarPlanner leverages an RL framework, which introduces surrogate objectives to indirectly optimize these non-differentiable metrics.

In this part, we compare these two approaches which provide rich supervision signals to the trajectory generator. The results are summarized in \cref{table:supple-abla-diff-loss}.
In IL training, the Col and Area metrics are converted into differentiable loss functions, whereas in RL training, Col and Area are treated as reward functions, contributing to the quality reward as described in the main paper.
It is important to note that the implementations for differentiable loss functions and reward functions are identical, except that gradient flow is enabled for differentiable loss functions.
The open-loop metrics compute the Col and Area values across all candidate multi-modal trajectories, with the Mean, Min, and Max referring to the mean, minimum, and maximum values of the Col and Area metrics within the candidate trajectory set.

Our findings suggest that incorporating Col loss benefits the open-loop Col metric and improves the closed-loop S-CR metrics, thereby enhancing closed-loop performance. However, incorporating Area loss results in better open-loop Area metrics but deteriorates closed-loop performance. Compared to differentiable loss functions, RL with Col and Area as quality rewards yields the trajectory set with the highest overall quality, as evidenced by smaller Mean and Max metrics in open-loop metrics.
This improvement can be attributed to RL's ability to optimize the reward-to-go using surrogate objectives that account for future rewards, while differentiable loss functions are limited to timewise-aligned optimization in our current implementation.
This distinction is illustrated in \cref{figure:diff-loss}: in (a), the loss at time step $t$ is directly computed from $s^0_t$, meaning that during backward propagation, the loss at time step $t$ cannot influence the optimization of prior time steps.
In (b), however, the non-differentiable reward is aggregated into a return (reward-to-go), which serves as a reference for computing the loss at time step $t$. Through this process, the reward at time step $t$ can influence the trajectory at earlier time steps $t'$ ($t' < t$). In the future, we aim to combine the advantages of differentiable loss which can provide low-variance gradients, and RL which can provide long-term foresight, by model-based RL optimization techniques~\cite{claveramodel, hansen2022temporal}.






\section{Effect of Mode Representation}

In this part, we examine the impact of mode representations on performance. The results are presented in \cref{table:supple-abla-mode}.
For both the vanilla and consistent frameworks, we disable the use of random sampling to focus solely on mode-aligned trajectories. As a result, the vanilla framework can only generate single-modal trajectories, leading to the lowest performance.
In the consistent framework, we explore two types of mode representations: Lon and Lon-Lat. The Lon representation assigns modes based on longitudinal movements along the route, whereas the Lon-Lat representation decomposes modes by both longitudinal and lateral movements.
Aligned with the main paper, we use ego-history dropout and backbone sharing only for IL training. For the Lon representation, we close mode dropout since it does not rely on any map or agent representation in initial state.
The results indicate that introducing consistency provides greater benefits to RL training, with the Lon-Lat representation proving to be more effective than the Lon representation. This suggests that decomposing mode representations into both longitudinal and lateral components enhances the model's ability by providing more explicit mode information.



% \section{Qualitative Results}

% Qualitative behaviors of our proposed CarPlanner can be found in \texttt{video.mp4}.
