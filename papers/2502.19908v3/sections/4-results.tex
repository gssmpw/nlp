


\section{Experiments}

\subsection{Experimental Setup}

\noindent\textbf{Dataset and simulator.} We use nuPlan~\cite{caesar2021nuplan}, a large-scale closed-loop platform for studying trajectory planning in autonomous driving, to evaluate the efficacy of our method. The nuPlan dataset contains driving log data over 1,500 hours collected by human expert drivers across 4 diverse cities. It includes complex, diverse scenarios such as lane follow and change, left and right turn, traversing intersections and bus stops, roundabouts, interaction with pedestrians, etc. As a closed-loop platform, nuPlan provides a simulator that uses scenarios from the dataset as initialization. During the simulation, traffic agents are taken over by log-replay (non-reactive) or an IDM~\cite{treiber2000idm} policy (reactive). The ego vehicle is taken over by user-provided planners. The simulator lasts for 15 seconds and runs at 10 Hz. At each timestamp, the simulator queries the planner to plan a trajectory, which is tracked by an LQR controller to generate control commands to drive the ego vehicle.


\noindent\textbf{Benchmarks and metrics.} We use two benchmarks: Test14-Random and Reduced-Val14 for comparing with other methods and analyzing the design choices within our method. The Test14-Random provided by PlanTF~\cite{cheng2024rethinking} contains 261 scenarios. The Reduced-Val14 provided by PDM~\cite{Dauner2023CORL} contains 318 scenarios.

We use the closed-loop score (CLS) provided by the official nuPlan devkit\footnote{\url{https://github.com/motional/nuplan-devkit}} to assess the performance of all methods.
The CLS score comprehends different aspects such as safety (S-CR, S-TTC), drivable area compliance (S-Area), progress (S-PR), comfort, etc.
Based on the different behavior types of traffic agents, CLS is detailed into CLS-NR (non-reactive) and CLS-R (reactive). 
% To further analyze various components of our methods, we also use open-loop metrics, such as loss of trajectory generator and selector.

\input{table/sota.tex}

\noindent\textbf{Implementation details.} We follow PDM~\cite{Dauner2023CORL} to construct our training and validation splits. The size of the training set is 176,218 where all available scenario types are used, with a number of 4,000 scenarios per type. The size of the validation set is 1,118 where 100 scenarios with 14 types are selected. We train all models with 50 epochs in 2 NVIDIA 3090 GPUs. The batch size is 64 per GPU. We use AdamW optimizer with an initial learning rate of 1e-4 and reduce the learning rate when the validation loss stops decreasing with a patience of 0 and decrease factor of 0.3. For RL training, we set the discount $\gamma = 0.1$ and the GAE parameter $\lambda = 0.9$. The weights of value, policy, and entropy loss are set to 3, 100, and 0.001, respectively. The number of longitudinal modes is set to 12 and a maximum number of lateral modes are set to 5.


\subsection{Comparison with SOTAs}

\input{table/ablation_rl.tex}


\noindent\textbf{SOTAs.} We categorize the methods into Rule, IL, and RL based on the type of trajectory generator.
(1) PDM~\cite{Dauner2023CORL} wins the nuPlan challenge 2023, its IL-based and rule-based variants are denoted as PDM-Open and PDM-Closed, respectively. PDM-Closed follows the generation-selection framework where IDM is used to generate multiple candidate trajectories and rule-based selector considering safety, progress, and comfort is used to select the best trajectory.
(2) PLUTO~\cite{cheng2024pluto} also obeys the generation-selection framework and uses contrastive IL to incorporate various data augmentation techniques and trains the generator.
(3) Gen-Drive~\cite{huang2024gen} is a concurrent work that follows a pretrain-finetune pipeline where IL is used to pretrain a diffusion-based planner and RL is used to finetune the denoising process based on a reward model trained by AI preference.

\noindent\textbf{Results.} We compare our method with SOTAs in Test14-Random and Reduced-Val14 benchmark as shown in \cref{table:main-results1} and \cref{table:main-results2}.
Overall, our CarPlanner demonstrates superior performance, particularly in non-reactive environments.

In the non-reactive setting, our method achieves the highest scores across all metrics, with an improvement of 4.02 and 2.15 compared to PDM-Closed and PLUTO, establishing the potential of RL and the superior performance of our proposed framework.
Moreover, CarPlanner reveals substantial improvement in the progress metric S-PR compared to PDM-Closed in \cref{table:main-results2} and comparable collision metric S-CR, indicating the ability of our method to improving driving efficiency while maintaining safe driving.
Importantly, we do not apply any techniques commonly used in IL such as data augmentation~\cite{cheng2024rethinking,cheng2024pluto} and ego-history masking~\cite{Guo-RSS-23}, underscoring the intrinsic capability of our approach to solving the closed-loop task.

In the reactive setting, while our method performs well, it falls slightly short of PDM-Closed. This discrepancy arises because our model was trained exclusively in non-reactive settings and has not interacted with the IDM policy used by reactive settings; as a result, our model is less robust to disturbances generated by reactive agents during testing.


\subsection{Ablation Studies}

\input{table/ablation_il_rl.tex}

We investigate the effects of different design choices in RL training. The results are shown in \cref{table:abla-rl}.

\noindent\textbf{Influence of reward items.}
% The results demonstrate that the DE and quality rewards are complementary.
When using the quality reward only, the planner tends to generate static trajectories and achieves a low progress metric. This occurs because the ego vehicle begins in a safe, drivable state, but moving forward is at risk of collisions or leaving the drivable area.
% On the other hand, compared to using DE reward only, incorporating the quality reward significantly improves closed-loop metrics.
On the other hand, when the quality reward is incorporated alongside the DE reward, it leads to significant improvements in closed-loop metrics compared to using the DE reward alone.
For instance, the S-CR metric rises from 97.49 to 99.22, and the S-Area metric rises from 96.91 to 99.22. These improvements indicate that the quality reward encourages safe and comfortable behaviors.

\noindent\textbf{Effectiveness of IVM.}
The results show that the coordinate transformation and KNN techniques in IVM notably improve closed-loop metrics and generator loss.
For instance, with the coordinate transformation technique, the overall closed-loop score increases from 90.78 to 94.07, and S-PR rises from 91.37 to 95.06.
These improvements are attributed to the enhanced accuracy of value estimation in RL, leading to generalized driving in closed-loop.


\begin{figure*}[ht]
    \begin{center}
    \includegraphics[width=0.98\textwidth]{image/vis.pdf}
    %%% trim={<left> <lower> <right> <upper>}
    \end{center}
    \vspace{-0.7cm}
    \caption{Qualitative comparison of PDM-Closed and our method in non-reactive environments. The scenario is annotated as \texttt{waiting\_for\_pedestrian\_to\_cross}. In each frame shot, ego vehicle is marked as {\color{LimeGreen} green}. Traffic agents are marked as {\color{DeepSkyBlue} sky blue}. Lineplot with {\color{Blue} blue} is the ego planned trajectory.
    }
    \vspace{-0.5cm}
    \label{figure:vis}
\end{figure*}


\subsection{Extention to IL}
\label{section:abla-il}

In addition to designing for RL training, we also extend the CarPlanner to incorporate IL. We conduct rigorous analysis to compare the effects of various design choices in IL and RL training, as summarized in \cref{table:abla-il-rl}. Our findings indicate that while mode dropout and selector side task contribute to both IL and RL training, ego-history dropout and backbone sharing, often effective in IL, are less suitable for RL.

\noindent\textbf{Ego-history dropout.} Previous works~\cite{Ogale-RSS-19, Guo-RSS-23, cheng2024rethinking, cheng2024pluto} suggest that planners trained via IL may rely too heavily on past poses and neglect environmental state information. To counter this, we combine techniques from ChauffeurNet~\cite{Ogale-RSS-19} and PlanTF~\cite{cheng2024rethinking} into an ego-history dropout module, randomly masking ego history poses and current velocity to alleviate the causal confusion issue.

Our experiments confirm that ego-history dropout benifits IL training, as it improves performance across closed-loop metrics like S-CR and S-Area. However, in RL training, we observe a negative impact on advantage estimation due to ego-history dropout, which significantly affects the value part of generator loss, leading to closed-loop performance degradation.
This suggests that RL training naturally addresses the causal confusion problem inherent in IL by uncovering causal relationships that align with the reward signal, which explicitly encodes task-oriented preferences. This capability highlights the potential of RL to push the boundaries of learning-based planning.

\noindent\textbf{Backbone sharing.} This choice, often used in IL-based multi-modal planners, promotes feature sharing across tasks to improve generalization. While backbone sharing helps IL by balancing losses across trajectory generator and selector, we find it adversely affects RL training. Specifically, backbone sharing leads to higher losses for both the trajectory generator and selector in RL, indicating that gradients from each task interfere. The divergent objectives in RL for trajectory generation and selection tasks seem to conflict, reducing overall policy performance. Consequently, we avoid backbone sharing in our RL framework to maintain task-specific gradient flow and improve policy quality.






\subsection{Qualitative Results}

We provide qualitative results as shown in \cref{figure:vis}. In this scenario, ego vehicle is required to execute a right turn while navigating around pedestrians. In this case, Our method shows a smooth, efficient performance.
From $t_{\text{sim}} = 0s$ to $t_{\text{sim}} = 9s$, all methods wait for the pedestrians to cross the road.
At $t_{\text{sim}} = 10s$, an unexpected pedestrian goes back and prepares to re-cross the road. PDM-Closed is unaware of this situation and takes an emergency stop, but it still intersects with this pedestrian. In contrast, our IL variant displays an awareness of the pedestrian's movements and consequently conducts a braking maneuver. However, it still remains close to the pedestrian. Our RL method avoids this hazard by starting up early up to $t_{\text{sim}} = 9s$ and achieves the highest progress and safety metrics.




