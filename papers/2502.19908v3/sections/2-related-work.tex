
\section{Related Work}

\begin{figure*}[t]
    \begin{center}
    \includegraphics[width=0.95\textwidth]{image/planner.pdf}
    %%% trim={<left> <lower> <right> <upper>}
    \end{center}
    \vspace{-0.6cm}
    \caption{CarPlanner contains four parts. (1) The non-reactive transition model takes initial state $\boldsymbol{s}_0$ as input and predicts the future trajectories of traffic agents. (2) The mode selector outputs scores based on the initial state and the modes $\boldsymbol{c}$. (3) The trajectory generator obeys an auto-regressive structure condition on the consistent mode and produces mode-aligned multi-modal trajectories. (4) The rule-augmented selector compensates the mode scores by safety, comfort, and progress metrics.
    }
    \vspace{-0.5cm}
    \label{figure:planner}
\end{figure*}



\subsection{Imitation-based Planning}
The use of IL to train planners based on human demonstrations has garnered significant interest recently. This approach leverages the driving expertise of experienced drivers who can safely and comfortably navigate a wide range of real-world scenarios, along with the added advantage of easily collectible driving data at scale~\cite{ettinger2021large, houston2021one, caesar2021nuplan}. Numerous studies~\cite{chitta2022transfuser, scheel2022urban, huang2023gameformer} have focused on developing innovative networks to enhance open-loop performance in this domain. However, the ultimate challenge of autonomous driving is achieving closed-loop operation, which is evaluated using driving-oriented metrics such as safety, adherence to traffic rules, comfort, and progress. This reveals a significant gap between the training and testing phases of planners. Moreover, IL is particularly vulnerable to issues such as distribution shift~\cite{ross2011reduction} and causal confusion~\cite{de2019causal}. The first issue results in suboptimal decisions when the system encounters scenarios that are not represented in the training data distribution. The second issue arises when networks inadvertently capture incorrect correlations and develop shortcuts based on input information, primarily due to the reliance on imitation loss from expert demonstrations. Despite efforts in several studies~\cite{Ogale-RSS-19, cheng2024rethinking, zhang2024pep, cheng2024pluto} to address these challenges, the gap between training and testing remains substantial.

\subsection{RL in Autonomous Driving}
In the field of autonomous driving, RL has demonstrated its effectiveness in addressing specific scenarios such as highway driving~\cite{leurent2019social, wang2021deep}, lane changes~\cite{li2022decision, he2022robust}, and unprotected left turns~\cite{leurent2019social, Wang-RSS-23}. Most methods directly learn policies over the control space, which includes throttle, brake, and steering commands. Due to the high frequency of control command execution, the simulation process can be time-consuming, and exploration can be inconsistent~\cite{Wang-RSS-23}. Several works~\cite{zhou2023accelerating, Wang-RSS-23} have proposed learning trajectory planners with actions defined as ego-planned trajectories, which temporally extend the exploration space and improve training efficiency. However, a trade-off exists between the trajectory horizon and training performance, as noted in ASAP-RL~\cite{Wang-RSS-23}. Increasing the trajectory horizon results in less reactive behaviors and a reduced amount of data, while a smaller trajectory horizon leads to challenges similar to those encountered in control space. Additionally, these methods typically employ a model-free setting, making them difficult to apply to the complex, diverse real-world scenarios found in large-scale driving datasets. In this paper, we propose adopting a model-based formulation that can facilitate RL training on large-scale datasets. Under this formulation, we aim to overcome the trajectory horizon trade-off by using a transition model, which can provide a preview of the world in which our policy can make multi-step decisions during testing.
