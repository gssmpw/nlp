
\section{Conclusion}

In this paper, we introduce CarPlanner, a consistent auto-regressive planner aiming at large-scale RL training.   
Thanks to the proposed framework, we train an RL-based planner that outperforms existing RL-, IL-, and rule-based SOTAs.
Furthermore, we provide analysis indicating the characteristics of IL and RL, highlighting the potential of RL to take a further step toward learning-based planning.

\noindent\textbf{Limitations and future work.}
RL needs delicate design and is prone to input representation.
RL can overfit its training environment and suffer from performance drop in unseen environments~\cite{kirk2023survey}. 
Our method leverages expert-aided reward design to guide exploration. However, this approach may constrain the full potential of RL, as it inherently relies on expert demonstrations and may hinder the discovery of solutions that surpass human expertise.
Future work aims to develop robust RL algorithms capable of overcoming these limitations, enabling autonomous exploration and generalization across diverse environments.


\section{Acknowledgement}

Many thanks to Jingke Wang for helpful discussions and all reviewers for improving the paper.
This work was supported by
Zhejiang Provincial Natural Science Foundation of China under Grant No. LD24F030001, and by
the National Nature Science Foundation of China under Grant 62373322.
