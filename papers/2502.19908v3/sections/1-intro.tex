
\section{Introduction}

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.44\textwidth]{image/teaser.png}
    %%% trim={<left> <lower> <right> <upper>}
    \end{center}
    \vspace{-0.5cm}
    \caption{Frameworks for multi-step trajectory generation. (a)~Initialization-refinement that generates an initial trajectory and refines it iteratively. (b)~Vanilla auto-regressive models that decode subsequent poses sequentially. (c)~Our consistent auto-regressive model that integrates time-consistent mode information.
    }
    \vspace{-0.6cm}
    \label{figure:teaser}
\end{figure}

Trajectory planning~\cite{tampuu2020survey} is essential in autonomous driving, utilizing outputs from perception and trajectory prediction modules to generate future poses for the ego vehicle. A controller tracks this planned trajectory, producing control commands for closed-loop driving. Recently, learning-based trajectory planning has garnered attention due to its potential to automate algorithm iteration, eliminate tedious rule design, and ensure safety and comfort in diverse real-world scenarios~\cite{tampuu2020survey}. 

Most existing researches~\cite{scheel2022urban,Guo-RSS-23,hu2023planning} employ imitation learning~(IL) to align planned trajectories with those of human experts. However, this approach suffers from distribution shift~\cite{ross2011reduction} and causal confusion~\cite{de2019causal}. Reinforcement learning~(RL) offers a potential solution, addressing these challenges and providing richer supervision through reward functions. {Although RL shows effectiveness in domains such as games~\cite{silver2016mastering}, robotics~\cite{ibarz2021train}, and language models~\cite{ouyang2022training}, it still struggles with training inefficiencies and performance issues in the large-scale driving task.
}
To the extent of our knowledge, no RL methods have yet achieved competitive results on large-scale open datasets such as nuPlan~\cite{caesar2021nuplan}, which features diverse real-world scenarios.

Thus, this paper aims to tackle two key challenges in RL for trajectory planning: 1) training inefficiency and 2) poor performance. Training inefficiency arises from the fact that RL typically operates in a model-free setting, necessitating an inefficient simulator running on a CPU to repeatedly roll out a policy for data collection. To overcome this challenge, we propose an efficient model-based approach utilizing neural networks as transition models. Our method is optimized for execution on hardware accelerators such as GPUs, rendering our time cost comparable to that of IL-based methods.

To apply RL to solve the trajectory planning problem, we formulate it as a multi-step sequential decision-making task utilizing a Markov Decision Process~(MDP). Existing methods that generate the trajectory\footnote{In this paper, the term ``trajectory'' refers to the future poses of the ego vehicle or traffic agents. To avoid confusion, we use the term ``state (action) sequence'' to refer to the ``trajectory'' in the RL community.} in multiple steps generally fall into two categories: initialization-refinement~\cite{shi2022motion,huang2023gameformer, jiang2023motiondiffuser,zhou2024smartrefine} and auto-regressive models~\cite{rhinehart2019precog,seff2023motionlm,zhou2024behaviorgpt,wu2024smart}.

The first category, illustrated in \cref{figure:teaser} (a), involves generating an initial trajectory estimate and subsequently refining it through iterative applications of RL. However, recent studies, including Gen-Drive~\cite{huang2024gen}, suggest that it continues to lag behind SOTA IL and rule-based planners. 
One notable limitation of this approach is its neglect of the temporal causality inherent in the trajectory planning task. Additionally, the complexity of {direct optimization over high-dimensional trajectory space} can hinder the performance of RL algorithms.
The second category consists of auto-regressive models, shown in \cref{figure:teaser} (b), which generate the poses of the ego vehicle recurrently using a single-step policy within a transition model. In this category, ego poses at all time steps are consolidated to form the overall planned trajectory. {As taking temporal causality into account, current auto-regressive models allow for interactive behaviors}. However, a common limitation is their reliance on auto-regressively random sampling from action distributions to generate multi-modal trajectories. This vanilla auto-regressive procedure may compromise long-term consistency and {unnecessarily} expand the exploration space in RL, leading to poor performance.

To address the limitations of auto-regressive models, we introduce \textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive model designed for efficient, large-scale RL-based \textbf{Planner} training (see \cref{figure:teaser} (c)). The key insight of CarPlanner is its incorporation of consistent mode representation as conditions for the auto-regressive model. Specifically, we leverage a longitudinal-lateral decomposed mode representation, where the longitudinal mode is a scalar that captures average speeds, and the lateral mode encompasses all possible routes derived from the current state of the ego vehicle along with map information. This mode remains constant across time steps, providing stable and consistent guidance during policy sampling.

{Furthermore, we propose a universal reward function that suits large-scale and diverse scenarios, eliminating the need for scenario-specific reward designs. This function consists of an expert-guided and task-oriented term.
The first term quantifies the displacement error between the ego-planned trajectory and the expert's trajectory, which, along with the consistent mode representation, narrows down the policy's exploration space.
The second term incorporates common senses in driving tasks including the avoidance of collision and adherence to the drivable area.
Additionally, we introduce an Invariant-View Module~(IVM) to supply invariant-view input for policy, with the aim of providing time-agnostic policy input, easing the feature learning and embracing generalization. To achieve this, IVM preprocesses state and lateral mode by transforming agent, map, and route information into the ego's current coordinate and by clipping information that is distant from the ego.}

\textit{\textbf{To our knowledge, we are the first to demonstrate that RL-based planner outperforms state-of-the-art~(SOTA) IL and rule-based approaches on the challenging large-scale nuPlan dataset}}. In summary, the key contributions of this paper are highlighted as follows:
\begin{itemize}
    \item We present \textbf{CarPlanner}, a consistent auto-regressive planner that trains an RL policy to generate consistent multi-modal trajectories.
    \item We introduce an expert-guided universal reward function and IVM to simplify RL training and improve policy generalization, {leading to enhanced closed-loop performance}.
    \item We conduct a rigorous analysis on the characteristics of IL and RL training, providing insights into their strengths and limitations, while highlighting the advantages of RL in tackling challenges such as distribution shift and causal confusion.
    \item {Our framework showcases exceptional performance, surpassing all  RL-, IL-, and rule-based SOTAs on the nuPlan benchmark. This underscores the potential of RL in navigating complex real-world driving scenarios.}
\end{itemize}
