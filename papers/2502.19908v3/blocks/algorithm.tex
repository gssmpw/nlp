

\newcommand{\rightcomment}[1]{\hfill\textcolor{gray}{\% #1}}
\newcommand{\comment}[1]{\NoNumber{\textcolor{gray}{\% #1}}}



\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Training Procedure of CarPlanner}
\label{alg:training}

\small

\State \textbf{Input:} Dataset $\mathcal{D}$ containing initial states $\boldsymbol{s}_0$ and ground-truth trajectories $s^{0:N,\text{gt}}_{1:T}$, longitudinal modes $\boldsymbol{c}_{\text{lon}}$, discount factor $\gamma$, GAE parameter $\lambda$, update interval $I$.
\State \textbf{Require:} Non-reactive transition model $\beta$, mode selector $f_{\text{selector}}$, policy $\pi$, policy old $\pi_{\text{old}}$.

\State \textbf{\textcolor{gray}{Step 1: Training Transition Model}}
\For{$(\boldsymbol{s}_0, s^{1:N,\text{gt}}_{1:T}) \in \mathcal{D}$}
    \State Simulate agent trajectories $s^{1:N}_{1:T} \gets \beta(\boldsymbol{s}_0)$
    \State Calculate loss $L_{\text{tm}} \gets \text{L1Loss}(s^{1:N}_{1:T}, s^{1:N,\text{gt}}_{1:T})$
    \State Backpropagate and update $\beta$ using $L_{\text{tm}}$
\EndFor

\State \textbf{\textcolor{gray}{Step 2: Training Selector and Generator}}
\State Initialize {training\_step} $\gets 0$
\State Initialize policy old $\pi_{\text{old}} \gets \pi$
\For{$(\boldsymbol{s}_0, s^{0,\text{gt}}_{1:T}) \in \mathcal{D}$}
    \State \textbf{Non-Reactive Transition Model:} 
    \State Simulate agent trajectories $s^{1:N}_{1:T} \gets \beta(\boldsymbol{s}_0)$

    \State \textbf{Mode Assignment:}
    \State Determine $\boldsymbol{c}_{\text{lat}}$ based on $\boldsymbol{s}_0$
    \State Concatenate $\boldsymbol{c}_{\text{lat}}$ and $\boldsymbol{c}_{\text{lon}}$ to get $\boldsymbol{c}$
    \State Determine positive mode $c^*$ based on $s^{0,\text{gt}}_{1:T}$ and $\boldsymbol{c}$
    
    \State \textbf{Mode Selector Loss:}
    \State Compute scores $\boldsymbol{\sigma}, \bar{s}^{0}_{1:T} \gets f_{\text{selector}}(\boldsymbol{s}_0, \boldsymbol{c})$
    \State $L_{\text{selector}} \gets \text{CrossEntropyLoss}(\boldsymbol{\sigma}, c^*) + \text{SideTaskLoss}(\bar{s}^{0}_{1:T}, s^{0,\text{gt}}_{1:T})$

    \State \textbf{Generator Loss:}
    \If{Reinforcement Learning (RL) Training}
        \State Use $\pi_{\text{old}}$, $\boldsymbol{s}_0$, $c^*$, and $s^{1:N}_{1:T}$ to collect rollout data $(\boldsymbol{s}_{0:T-1}, a_{0:T-1}, d_{0:T-1}, V_{{0:T-1}}, R_{0:T-1})$
        \State Compute advantage $A_{0:T-1}$ and return $\hat{R}_{0:T-1}$ using GAE~\cite{schulman2015high}: $A_{0:T-1}, \hat{R}_{0:T-1} \gets \text{GAE}(R_{0:T-1}, V_{{0:T-1}}, \gamma, \lambda)$
        \State Compute policy distribution and value estimates: $(d_{0:T-1, \text{new}}, V_{0:T-1, \text{new}}) \gets \pi(\boldsymbol{s}_{0:T-1}, a_{0:T-1}, c^*)$
        \State $L_{\text{generator}} \gets \text{ValueLoss}(V_{0:T-1, \text{new}}, \hat{R}_{0:T-1}) + \text{PolicyLoss}(d_{0:T-1, \text{new}}, d_{0:T-1}, A_{0:T-1}) - \text{Entropy}(d_{0:T-1, \text{new}})$
    \ElsIf{Imitation Learning (IL) Training}
        \State Use $\pi$, $\boldsymbol{s}_0$, $c^*$, and $s^{1:N}_{1:T}$ to collect action sequence $a_{0:T-1}$
        \State Stack action sequence as ego-planned trajectory $s^{0}_{1:T} \gets \text{Stack}(a_{0:T-1})$
        \State $L_{\text{generator}} \gets \text{L1Loss}(s^{0}_{1:T}, s^{0,\text{gt}}_{1:T})$
    \EndIf
    
    \State \textbf{Overall Loss:}
    \State $L \gets L_{\text{selector}} + L_{\text{generator}}$
    \State Backpropagate and update $f_{\text{selector}}, \pi$ using $L$

    \State \textbf{Policy Update:}
    \State Increment {training\_step} $\gets$ {training\_step} $+ 1$
    \If{{training\_step} $\%$ $I$ $== 0$}
        \State Update $\pi_{\text{old}} \gets \pi$
    \EndIf
\EndFor

\end{algorithmic}
\end{algorithm}


