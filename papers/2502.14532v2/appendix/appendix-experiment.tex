\section{Additional experiments for \dskc}
\label{sec:add-exp:dense}

In this section, we present further details of our experiments. 
We run the experiments on a machine with an Intel Xeon Processor E5 2630 v4 and 
64GB RAM. Our algorithms are implemented in Python. 
We use Mosek to solve the semidefinite programs. 

\subsection{Datasets}
\label{sec:add-exp:data}

The first four datasets comprise Wikipedia politician page networks, and to
obtain them we follow the method of~\citet{neumann2022sublinear} with minor
changes: 
instead of constructing edges according to the hyperlinks in the whole Wikipedia page, 
we only constructing edges using the hyperlinks that appear in the articles.
By doing so, we avoid constructing edges simply because the two politicians belong to the same category. 
These networks represent politicians and party activities from different countries, 
annotated according to their respective parties. 
We select each graph based on the country and choose all the nodes belong to one party 
as the vertices of the initial subgraph. 

The second type of datasets we utilize are obtained from SNAP networks~\citep{snapnets} with ground-truth communities. 
Specifically, we select \dblp\footnote{com-DBLP: \url{https://snap.stanford.edu/data/com-DBLP.html}}, 
\amazon\footnote{com-Amazon: \url{https://snap.stanford.edu/data/com-Amazon.html}}, and 
\youtube\footnote{com-Youtube: \url{https://snap.stanford.edu/data/com-Youtube.html}}. 
For these datasets, we always choose the largest community as the initial subgraph.

We also use synthetic datasets generated from the Stochastic Block Model (SBM) with planted communities, 
namely \balanced, \sparse, and \dense. Each graph has four communities. 
We generate \sparse and \dense using the same parameters for generating the SBM,
and they consist of three sparse communities and one dense community (see below
for the concrete parameter values we picked). They only differ in their initial subgraphs.
In \sparse, we use a sparse community as the initial subgraph,
while in \dense, we use the dense community as the initial subgraph. 
For \balanced, all four communities have equal density.
With this configuration, the densest subgraph for \balanced is the whole graph,
while for \sparse and \dense, the denset subgraph is the planted dense community.


\label{sec:add-exp:data:parameter}
We elaborate on the parameters of the three SBM graphs. 
In each dataset, we establish four communities of equal size. 
For \balanced, all four communities are created with an edge probability of $0.3$, 
and the inter-community edge probabilities are set to $0.1$.

In contrast, for the \sparse and \dense datasets, we create one dense community with edge probabilities $0.8$, 
alongside three sparse communities with edge probabilities $0.2$. The edge probability between communities for these two datasets remains at $0.1$.

Concerning the initial subgraph, we adopt a different community for each dataset. For \balanced, we choose any one of the planted communities. 
We select a sparse community for the \sparse dataset. Conversely, for \dense, the dense community serves as our initial subgraph.

\subsection{Additional experimental results for \dskc}
\subsubsection{Performance when initializing with ground-truth subgraphs with $k$ nodes removed}

In Figure~\ref{fig:appendix:density-out-select-ratio}, we present additional
experimental results when $k$~nodes were removed from a ground-truth subgraph.

\begin{figure}[t!]
	\centering
	\inputtikz{ds_plots/legend_move_out}
	\begin{tabular}{cccc}
		\resizebox{0.3\columnwidth}{!}{%
			\inputtikz{ds_plots/large_ratio_sb_model_sparse_results_move_out_ratio}
		}&
		\hspace{-1.3em}
		\resizebox{0.29\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_dense_results_move_out_ratio}
		}&
		\hspace{-1.3em}
		\resizebox{0.29\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_balanced_results_move_out_ratio}
		}\\
		{\footnotesize \sparse} &
		{\footnotesize \dense} &
		{\footnotesize \balanced} \\
	\end{tabular}
	\caption{Relative increase of density for varying values of $k$. We initialize $U$ as a ground-truth subgraph and then
	remove $k$ nodes uniformly at random.}
	\label{fig:appendix:density-out-select-ratio}
\end{figure}

\subsubsection{Performance when initializing with ground-truth subgraphs}

In Table~\ref{tab:appendix:move-out-10} and Figure~\ref{fig:appendix:density-not-out-select-ratio}, 
we present additional experimental results when $U$~is chosen a ground-truth
subgraph.

\begin{table*}[t]
	\centering
	\caption{
	\small{Network statistics and average relative increase of density with respect to the ground-truth subgraph with $k= 10\%\, n_0$ random nodes moved out. 
	Here, $n$ and $m$ are the number of nodes and edges of the graph;
  $n_0$ and $\rho_0$ are the number of nodes and density of the ground-truth subgraph;
  $n^*$ and $\rho^*$ are the number of nodes and density of the Densest Subgraph.
	Next, \_ denotes that an algorithm does not finish in time (5 hours for SNAP datasets, and 30 minutes for other datasets);
	X~denotes that \denseSQD does not output a result, as no
	$\sigma$-quasi-elimination-ordering exists (we set $\sigma =5$).}}
	\label{tab:appendix:move-out-10}
	\resizebox{\textwidth}{!}{
		\input{tables/dense-move-out-10.tex}
	}
\end{table*}


\begin{figure}[t!]
	\centering
	\inputtikz{ds_plots/legend_not_move_out}
	\begin{tabular}{cccc}
		\resizebox{0.3\columnwidth}{!}{%
			\inputtikz{ds_plots/large_ratio_sb_model_sparse_results_not_move_out_ratio}
		}&
		\hspace{-1.3em}
		\resizebox{0.29\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_dense_results_not_move_out_ratio}
		}&
		\hspace{-1.3em}
		\resizebox{0.29\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_balanced_results_not_move_out_ratio}
		}\\
		{\footnotesize \sparse} &
		{\footnotesize \dense} &
		{\footnotesize \balanced} \\
	\end{tabular}
	\caption{Relative increase of density for varying values of $K$. We initialized $U$ as a ground-truth subgraph.}
	\label{fig:appendix:density-not-out-select-ratio}
\end{figure}

\subsubsection{Running time dependency on $k$}
\label{sec:add-exp:running-time-by-k}

In Figure~\ref{fig:density-out-synthetic-k-time} we present the running time on 
synthetic datasets. 
Notice that for those synthetic datasets, we set a different range on $k$, i.e., 
$k \in [10, 50]$. This is because it gives us a larger range on $k$ 
compared to $k \in [2\%n_0, 10\%n_0]$, since $n_0$ for those synthetic 
datasets is $250$. 
We notice that \denseSDPalgo, \denseSDPMerge and \denseSQD are slower on 
\sparse than on \dense. We do not get results for \denseSQD on \balanced.  
In addition, \denseSDPalgo is always slower than \denseSDPMerge, 
since \denseSDPalgo contains more constraints.  

\begin{figure}[t!]
	\inputtikz{ds_plots/legend_move_out}
	\centering 
    \begin{tabular}{ccc}
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_balanced_results_move_out_k_time}
		}&
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/sb_model_dense_results_move_out_k_time}
		}&
		\resizebox{0.30\columnwidth}{!}{
			\inputtikz{ds_plots/sb_model_sparse_results_move_out_k_time}
		}
		\\
		\balanced &
		\dense &
		\sparse \\
	\end{tabular}
	\caption{Running time for varying values of $k$. We initialize $U$ as a ground-truth subgraph and then
	remove $k$ nodes uniformly at random. We set $k = 10, 20, 30, 40, 50$.
		}
	\label{fig:density-out-synthetic-k-time}
\end{figure}

In Figure~\ref{fig:density-out-country-ratio-time} we present the running time
on the larger datasets. 

\begin{figure}[t!]
	\inputtikz{ds_plots/legend_huge_graph}
	\centering 
    \begin{tabular}{cccc}
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/dblp_results_move_out_ratio_time}
		}&
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/amazon_results_move_out_ratio_time}
		}&
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/youtube_results_move_out_ratio_time}
		}\\
		(a) \dblp &
		(b) \amazon &
		(c) \youtube \\		
	\end{tabular}
	\caption{Running time for varying values of $k$. We initialize $U$ as a ground-truth subgraph and then
	remove $k$ nodes uniformly at random. We set $k = 2\%, 4\%, 6\%, 8\%, 10\% n_0$.
		}
	\label{fig:density-out-country-ratio-time}
\end{figure}

\subsubsection{Running time dependency on $n$} 
\label{sec:add-exp-running-time-by-n}
Next, we vary the number of nodes in the synthetic datasets \dense and \sparse.
In Figure~\ref{fig:density-out-country-ratio-n-time} we present the 
algorithms' running times for varying values of $n$. 
The running time of \denseSDPalgo and \denseSDPMerge increases quadratically as $n$ increases; 
this is due to the fact that solving the \sdp is very costly. Therefore these
algorithms are not very scalable.


\begin{figure}[t!]
	\centering 
	\inputtikz{ds_plots/legend_move_out}
    \begin{tabular}{cc}
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/change_n_sb_model_dense_results_move_out_n_time}
		}&
        \resizebox{0.30\columnwidth}{!}{%
			\inputtikz{ds_plots/change_n_sb_model_sparse_results_move_out_n_time}
		}
		\\
		(a)~\dense &
		(b)~\sparse \\
	\end{tabular}
	\caption{
		Running time for varying values of $n$. We initialize $U$ as a ground-truth subgraph and then
	remove $k= 10\%n_0$ nodes uniformly at random.
	We set $n = 1\,000, 1\,250, 1\,500, 1\,750, 2\,000$. }
	\label{fig:density-out-country-ratio-n-time}
\end{figure}

\subsection{An analysis on \sparse} 
\label{sec:add-exp:dense:sparse}
To better understand our experimental results on the \sparse dataset, we present
an analysis of two na\"ive baselines.
In particular, we consider \sparse where we 
initialize $U$ as a ground-truth subgraph and then remove $k$ nodes uniformly at random. 
In Figure~\ref{fig:sparse-analysis}
we plot the expected increase in density by two na\"ive baselines:
We present the first baseline in blue, which directly recovers $k$ removed nodes
(i.e., if $k=\abs{U}$ it does exactly the same as \denseinit).
We present the second baseline in orange, which selects $k$ nodes from a dense community uniformly at random.

We observe that when $k\leq 84$, the baseline which picks nodes from the sparse
community exhibits better performance. 
However, for larger values of $k$, the algorithm that select nodes from the dense community performs better. 
This figure can serve as a reference for us in understanding the selection of nodes by the actual algorithms. 


\begin{figure}[t!]
	\centering 
	\includegraphics[width=1\columnwidth]{figures/ds_plots/sparse_density_analysis}
	\caption{On dataset \sparse. 
	We first uniformly randomly remove $k$ nodes from the sparse community. 
	We compute the expected relative increase on density by recovering $k$ nodes from the sparse community, 
	or by select $k$ nodes from the dense community. 
		When $k = 84$ those two are equal.}
	\label{fig:sparse-analysis}
\end{figure}

In particular, a connection may be drawn between \sparse in Figure~\ref{fig:density-out-select-ratio} and Figure~\ref{fig:sparse-analysis}. 
Upon analyzing the performances of \denseSDPalgo, \denseSDPMerge, and \densePeelMerge, we find that when $k$ is small, 
their performances mirror that of the \emph{recovery of the sparse community}, in the sense that their performances are closely aligned with \denseinit. 
However, as $k$ increases, the performance of these three algorithms tends to resemble the \emph{selection of $k$ nodes from the dense community}.

In contrast, \denseGreedy always recovers the sparse community. 
Consequently, this algorithm does not exhibit a better performance when $k$ is large.


\subsection{A discussion of \denseSQD's performance}
\label{sec:add-exp:sqd}
Next, we provide a more detailed discussion of the performance of the \denseSQD
algorithm.

The \denseSQD algorithm contains two phases. 
In Phase 1, the algorithm constructs a dense graph with at least $\frac{k}{2}$ nodes. 
In Phase 2, the algorithm tries to satisfy the cardinality constraint. 
If the dense graph found from the phase 1 contains more than $k$ nodes, 
the algorithm in phase $2$ removes nodes according to the $\sigma$-elimination order; 
otherwise, the algorithm randomly adds nodes. 

We observe that \denseSQD performs better than \denseGreedy on the \dblp dataset
(see Table~\ref{tab:not-move-out-10}). 
This is because that on the \dblp dataset, Phase 1 of \denseSQD constructs a
dense while small subgraph,
and in the Phase 2 it only randomly adds extra nodes while still gaining larger density than \denseGreedy. 


We also observe that on some datasets \denseSQD does not produce results. 
This is because the graph constructed after Phase 1 either does not have a
$\sigma$-quasi-elimination ordering,
or due to the high time and memory complexity of computing a
$\sigma$-quasi-elimination ordering.
For example, on small graphs, \balanced and \dense in Table~\ref{tab:not-move-out-10}, and \dense in Table~\ref{tab:appendix:move-out-10}, 
\denseSQD constructs a graph with significantly larger than $10\%\cdot n_0$
nodes in Phase 1.
In fact, the size of the graoh is equal to the size of the whole remaining graph. 

Even for those datasets where \denseSQD produces results, if computing the
$\sigma$-quasi-elimination order is necessary, it is very costly. 
In Figure~\ref{fig:density-out-synthetic-k-time} on \sparse, we see that running
\denseSQD is even more costly than running \denseSDPalgo. 
We should also note that for the largest datasets (which are relatively sparse),
such as \dblp, \denseSQD does compute the $\sigma$-elimination ordering and
obtains results.






\section{Experiments for Max-Cut with $k$~refinements}
\label{sec:exp:cut}

In this section, we conduct an extensive evaluation of our algorithms on a
variety of datasets to solve \maxcutkc. Our focus in this section will be two primary questions:

\begin{description}
\item[RQ1:] Do our algorithms consistently produce a larger cut when initialized with a random partition?
\item[RQ2:] Which algorithm is the best among our proposed algorithms?
\end{description}

\subsection{Algorithms} 
We use our SDP and Greedy algorithms and denote them as \cutSDPalgo and
\cutGreedy, respectively.

Additionally, we utilize the following black-box solvers for \maxcut to obtain
algorithms via the reduction in Theorem~\ref{thm:max-cut-black-box}:
\begin{enumerate}
\item A Greedy algorithm, where each node's side is chosen in a way that
	maximizes the cut at each step. We denote it by \cutBlackGreedy.
\item A Local Search algorithm which makes refinements to each node's side if
	the cut can be improved.  The algorithm either starts with a randomly
	initialized partition or uses the result of the Greedy algorithm as the
	initial partition.  We denote them
	as \cutBlackLocalOne and \cutBlackLocalTwo, respectively.
\item The \sdp algorithm of~\citet{goemans1995improved} which we denote by \cutBlackSDP.
\end{enumerate}

\subsection{Datasets} 
We use the same datasets as for \dskc. 
Since \dense and \sparse essentially have the same graph structure and since in
\maxcutkc the ground-truth communities are not relevant for us, we use \dense to represent them. 
The networks statistics are presented in Table~\ref{tab:statistics-cut}.

\begin{table*}[t]
	\centering
	\caption{\small{Network statistics, 
	and average relative increase of cut in $5$ runs with $k= 50$. 
	Here, $n$ and $m$ are the number of nodes and edges of the graph; $\mathsf{cut}_0$ is the average of $5$ random cuts.
	Next, \_ denotes that an algorithm does not finish in time (2 hours for SNAP datasets, and 30 minutes for other datasets).}}
	\label{tab:statistics-cut}
	\resizebox{0.85\textwidth}{!}{
		\input{tables/cut-k-50.tex}
	}
\end{table*}

\subsection{Evaluation}
In our experiments, we proceeds as follows.
We uniformly randomly partition each node in the graph to either side, and
repeat this procedure $5$ times.  At each time, we run our algorithms and
calculate the cut.  We compute the relative increase on the initial given cut at
each run, and take the average of these $5$ runs. We present the results in
Table~\ref{tab:statistics-cut}. 

Here, it is worth noting that for \maxcut, a random partition already yields a
2-approximate solution compared to the maximum cut in expectation.
Thus, since we initialize the original cut randomly, this means that the given cut
will already be large (with a relatively large probability) and therefore
improving the cut significantly will be difficult for our algorithms.

\subsection{Performance}
In Table~\ref{tab:statistics-cut}, we report the average of the relative
increases of the cut value by different algorithms. 
In Figure~\ref{fig:cut-country}, we show the relative increase of the cut values for 
our Wikipedia politician networks for varying values of $k$. 

We notice from the plots and the table that, for all the settings of $k$, \cutSDPalgo is always 
the best, as long as \cutSDPalgo outputs a result.
As $k$ increases, there is a trend that \cutSDPalgo performs better. 
Even though \cutGreedy does not perform better than \cutSDPalgo, in many cases, 
it performs better than the other competitors. 
This addresses \textbf{RQ2}.

Additionally, the local-search based algorithm \cutBlackLocalTwo only very
slightly improves upon \cutBlackGreedy
(which it uses to obtain its initial partition)
and does not perform significantly better than \cutBlackLocalOne
(which uses a random initialization).
This suggests that the initial solution returned by \cutBlackGreedy is not very
good, as is also suggested by comparison with \cutGreedy and \cutSDPalgo.
However, we will see below that \cutBlackGreedy has a significantly lower
running time than \cutBlackLocalOne.
This addresses \textbf{RQ1}.

\begin{figure}[t!]
	\centering 
	\inputtikz{cut_plots/legend}
    \begin{tabular}{cccc}
        \resizebox{0.25\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_gb_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_de_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_balanced_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_dense_results_k}
		}
		\\
		\gb &
		\de &
		\balanced &
		\dense \\
        \resizebox{0.25\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_us_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/dblp_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/amazon_results_k}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/youtube_results_k}
		}
		\\
		\us &
		\dblp &
		\amazon &
		\youtube \\
	\end{tabular}
	\caption{Relative increase of the cut value. We used $k \in [10, 50]$ on smaller
		graphs, and $k \in [10, 100]$ for larger graphs.}
	\label{fig:cut-country}
\end{figure}



\sbpara{Running time dependency on $k$.}
In Figure~\ref{fig:cut-country-k-time} we present the running time on 
three datasets with varying values of $k$.  
We notice that \cutSDPalgo requires much longer time than \cutBlackSDP.
This is because \cutBlackSDP only needs to solve the standard Max-Cut 
\sdp, while \cutSDPalgo adds additional constraints, resulting in
longer running time. \cutBlackLocalOne requires more time than \cutBlackLocalTwo, this 
is because the local search algorithm with random initialization takes much
longer time to converge.

\begin{figure}[t!]
	\centering 
	\inputtikz{cut_plots/legend}
    \begin{tabular}{cccc}
        \resizebox{0.25\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_gb_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_de_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_balanced_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_dense_results_k_time}
		}
		\\
		\gb &
		\de &
		\balanced &
		\dense \\
        \resizebox{0.25\columnwidth}{!}{%
			\inputtikz{cut_plots/wiki_us_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/dblp_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/amazon_results_k_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.23\columnwidth}{!}{%
			\inputtikz{cut_plots/youtube_results_k_time}
		}
		\\
		\us &
		\dblp &
		\amazon &
		\youtube \\
	\end{tabular}
	\caption{Running time of our algorithms. We set $k \in [10,  50]$ on smaller graphs, and $k \in [10, 100]$ for larger graphs.}
	\label{fig:cut-country-k-time}
\end{figure}

\sbpara{Running time dependency on $n$.} 
\label{sec:cut-running-time-by-n}
We increase the size of synthetic datasets \dense and \balanced, 
and in Figure~\ref{fig:cut-n-time} we give the 
running time in terms of increasing $n$. 
\denseSDPalgo and \denseSDPMerge increase largely as $n$ increases, as we expect, indicating these two algorithms are not scalable. 


\begin{figure}[t!]
	\centering 
	\inputtikz{cut_plots/legend}
    \begin{tabular}{cc}
        \resizebox{0.3\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_dense_results_n_time}
		}&
		\hspace{-1.3em}
        \resizebox{0.3\columnwidth}{!}{%
			\inputtikz{cut_plots/sb_model_balanced_results_n_time}
		}
		\\
		(a)~\dense &
		(b)~\balanced \\
	\end{tabular}
	\caption{Running time of the algorithms for fixed $k = 50$ and varying $n = 1\,000, 1\,250, 1\,500, 1\,750, 2\,000$.}
	\label{fig:cut-n-time}
\end{figure}

\subsection{A case study}
\cite{matakos2020tell} propose a method to maximize the diversity of a social
network by flipping $k$ nodes' opinions, where each opinion is either $1$ or
$-1$.  In our problem setting, the term \emph{diversity} is mathematically equivalent to \emph{cut} 
and the nodes of the graph can be partitioned based on their opinions (where the
		cut is given by all nodes with opinion~$1$ on one side and all other
		nodes on the other side).
It is important to note that our problem setting differs from theirs in several ways: 
they operate under a budget constraint, whereas we operate under a cardinality constraint; 
they require an inequality constraint, while our problem necessitates an equality constraint. 
We executed their algorithm on our datasets by setting the cost of all nodes to $1$, 
allowing for a comparison of our methods.
The performance, running time, and the average number of changed nodes are presented in 
Table~\ref{table:diversity-comparison}. We note that we do not report results for~\us,
since the algorithm of \citet{matakos2020tell} runs out of memory on this dataset.

We observe that, somewhat surprisingly, the \cutSDPalgo algorithm proposed by \citet{matakos2020tell} 
takes significantly more time than the \cutSDPalgo algorithm proposed in our paper (as shown in Figure~\ref{fig:cut-country-k-time}). 
In addition, despite having the \emph{at most} constraint instead of the equality constraint, 
their algorithm does not yield better performance. 
Specifically, on the dataset \de, our \cutSDPalgo algorithm performs twice as well.

\begin{table}[t]
\centering
\caption{
	The results of the \cutSDPalgo algorithm proposed by \cite{matakos2020tell},
	where we set $k = 50$ on several datasets. 
	We use \emph{Matakos et al.} to indicate the average relative increase of the cut
	value with their approach in $5$ runs, and \emph{SDP} to indicate the result of our \cutSDPalgo-based algorithm.
	}
\label{table:diversity-comparison}
\begin{tabular}{@{}lllll}
\toprule
\textsf{dataset} & SDP & Matakos et al. & \textsf{Time (s)} & \textsf{\# changed nodes} \\
\midrule
\es & 0.533 & 0.408 & 22.1848 &  48.3\\
\de & 0.182 & 0.105 & 784.401 &  46.4\\
\gb & 0.074 & 0.031 & 5780.306 &  48.5\\
\bottomrule
\end{tabular}
\end{table}
