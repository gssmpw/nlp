

\section{Omitted content and proofs of Section~\ref{sec:max-cut:sos}}
\label{sec:sos_detailed}
In this section, we elaborate on the sum-of-squares (SoS) algorithm from Section \ref{sec:max-cut:sos}. 
In particular, we apply the method proposed by ~\cite{DBLP:conf/soda/RaghavendraT12}.
In detail, we will explain how to relax \maxcutkc into the Lasserre \sdp hierarchy (or, Lasserre hierarchy), how to obtain globally uncorrelated solutions to the Lasserre hierarchy, and how to apply a rounding scheme to the solutions so that the cardinality constraint is almost satisfied. 

For a detailed description and properties of the Lasserre \sdp hierarchy, we refer the readers to~\citep{DBLP:journals/siamjo/Lasserre02,rothvoss2013lasserre}.
For more technical details in this section, we refer the readers to  
\citep{DBLP:journals/talg/AustrinBG16,DBLP:conf/soda/RaghavendraT12,Chlamtac2012}.


The remainder of this section is structured as follows.
Section~\ref{subsection:lassere_hierarchy:preliminary} briefly introduces properties of the Lasserre hierarchy and mutual information to be applied in the following analysis. 
Section~\ref{appendix:lasserre:program} formulates a degree $d$ Lasserre hierarchy program for \maxcutkc.
Section~\ref{appendix:lasserre:decorrelate} describes how to obtain a \emph{decorrelated} solution via conditioning.
Section~\ref{appendix:lesserre:rounding}
formulates the degree $2$ Lasserre hierarchy and the hyperplane rounding algorithm we apply.
Section~\ref{appendix:lasserre:roundingcombined} proves
Lemma~\ref{lem:bound-variance} that we presented in the main content. 



\subsection{Preliminaries}
\label{subsection:lassere_hierarchy:preliminary}

\subsubsection{Notation}
\label{subsection:lassere_hierarchy:notations}
Let $G=(V,E,W)$ be an undirected weighted graph,
and let $\vecx^0 \in \left \{-1,1\right\}^{V}$ 
encode the initial partition, i.e., $(\OldSet, \OldComSet)$. 
The $i$-th entry of $\vecx^0$ is denoted by $x^0_i$.
Let $S \subseteq V$, and $\mu_{S}$ be a (pseudo)-distribution on $\{-1, 1\}^S$. We will elaborate on its properties in Section~\ref{subsection:lassere_hierarchy_properties}.
Let $\vecx \in \left \{-1, 1\right\}^{V}$ be a random vector that encodes a random partition.
We use $\vecx_{S}$ to indicate the partition constrained on $S$.
Moreover, for $\xi \in \{-1, 1\}^S$, we denote by $\Pr_{\mu_{S}}(\vecx_{S} = \xi)$ the probability of assigning $\vecx_{S}$ with $\xi$ under the distribution $\mu_{S}$.

Let $k \leq \abs{V}$ be the number of refinements and $k = \tau n$.
We let $\varsigma=(n-2k)/n, \varsigma\in (-1,1)$. 
Let us fix $\varepsilon>0$ to be a small constant, and let $\delta=(\varepsilon/3)^{96}$. 
Let $d'= \lceil 1+ \frac{1}{\delta} \rceil $. We will construct a degree $d=d'+2$ Lasserre hierarchy. 

\subsubsection{Properties of the Lasserre hierarchy} 
\label{subsection:lassere_hierarchy_properties}
\para{Consistent local distribution.} 
Any Lasserre hierarchy configuration can be written in terms of local
distributions \citep{DBLP:journals/siamjo/Lasserre02}. In particular, in the
degree $d$ Lasserre hierarchy, for each set $S \subseteq V, |S| \leq d$, there
is a distribution $\mu_S$ on $\left \{-1,1\right\}^S$. Furthermore, Lasserre
hierarchy ensures that for any two such
probability distributions, one on $\{-1, 1\}^S$, denoted as $\mu_S$, and one on
$\{-1, 1\}^T$, denoted as $\mu'_T$, their marginal distribution on $\{-1, 1\}^{S \cap T}$ is the same: $\mu_{S \cap T} = \mu'_{S \cap T}$. 
In other words, local probability distributions are consistent. 

\para{Semi-definiteness property.}
Local distributions of the Lasserre hierarchy also satisfy positive semi-definiteness properties.
In the degree $d$ Lasserre hierarchy, for each $S, T \subseteq V, |S \cup T| \leq d,$ and $\xi_1 \in \left \{-1,1\right\}^S,\xi_2 \in \left \{-1,1\right\}^T$, there exist vectors $\vecv_{S,\xi_1},\vecv_{T,\xi_2}$ such that 
\begin{equation*}
\Pr_{\mu_{S \cup T}} (\vecx_S = \xi_1, \vecx_T = \xi_2) =  \vecv_{S,\xi_1} \cdot \vecv_{T,\xi_2}, 
\end{equation*}
where $\vecx_T,\vecx_S$ are random variables taking values in $\left
\{-1,1\right\}^S, \left \{-1,1\right\}^T$, respectively, and $\cdot$ denotes the
dot product of two vectors. 

\para{Conditioning of assignments.}
One property that we will use in our algorithm is that the local probability distributions can be conditioned.
In particular, any event $\vecx_S = \zeta$
can be conditioned on an 
an event $\vecx_U = \xi$ given that $\abs{S \cup U} \leq d$ and $\Pr_{\mu_{S \cup U}}(\vecx_U = \xi) \neq 0$. 

After the conditioning, we obtain a new 
Lasserre hierarchy of degree $d-|U|$ with local distributions $\left \{\bar{\mu}_S\right\}_{S \subseteq V,|S| \leq d-|U|}$.
In particular, 
\begin{equation*}
\begin{split}
	\Pr_{\bar{\mu}_{S}}\left( \vecx_S = \zeta \right) 
 = \Pr_{\mu_{S \cup U }}  \left( \vecx_S = \zeta \mid \vecx_U = \xi\right)
 = \frac{\vecv_{S, \zeta} \cdot \vecv_{U, \xi}}{\vecv_{U, \xi} \cdot \vecv_{U, \xi}}.
\end{split}
\end{equation*}


\subsubsection{Mutual information}
\label{subsection:information_theory}
Next, we illustrate how to use conditioning to reduce the correlation between fractional solutions of the vertices, allowing us to round values of different vertices \emph{almost independently on average}. 
We need to use mutual information to measure the correlation between vertices.
\begin{definition}
	Let $x_i,x_j$ be two random variables taking values in $\left \{-1,1\right\}$, which indicates the partition of node $i$ and $j$.
    Let $S \supseteq \left \{i,j\right\},U \ni i,T \ni j$ in place of $\left \{i,j\right\}, \left \{i\right\}, \left \{j\right\}$. The \emph{mutual} information between $x_i$ and $x_j$ is then defined as 
	\begin{equation*}
		I(x_i,x_j) =  \sum_{a,b \in \{-1,1\}} \Pr_{\mu_S}(x_i=a,x_j=b) \cdot \log\left(\frac{\Pr_{\mu_S}(x_i=a,x_j=b)}{\Pr_{\mu_U}(x_i=a)\Pr_{\mu_T}(x_j=b)}\right) .
	\end{equation*}
\end{definition}
This follows from the fact that probability distributions are consistent under intersections, and hence we can take $S \cap \left \{i,j\right\}$  to obtain the 
distribution on $\left \{i,j\right\}$, etc. \par
We will aim to get 
\begin{equation*}
	\E_{i,j \sim V} \left[ I(x_i, x_j)\right] \leq \delta.
\end{equation*}

This goal will be achieved by conditioning and utilizing the observation from Lemma~\ref{lemma:low_mutual_information}.
In particular, Lemma~\ref{lemma:low_mutual_information} implies that the
following equation holds:
\begin{equation}
\label{eq:multral-info-goal}
\min_{\substack{\{i_1, \ldots, i_t\} \subseteq V, t \leq d' \\ \{a_{i_1}, \ldots, a_{i_t}\} \in \{-1, 1\}^{\{i_1, \ldots, i_t\}}}} \E _{i,j \sim V}\left[ I(x_i,x_j \mid x_{i_1} = a_{i_1},\ldots,x_{i_t} = a_{i_t}) \right] \leq 1/(d'-1).
\end{equation}

\begin{lemma}[Lemma 4.5 in~\citet{DBLP:conf/soda/RaghavendraT12}]
\label{lemma:low_mutual_information}
		There exists $t \leq d'$ such that 
		\begin{equation*}
			\E_{i_1,\hdots,i_t \sim V} \E _{i,j \sim V}\left[ I(x_i,x_j \mid x_{i_1},\hdots,x_{i_t}) \right] \leq 1/(d'-1).
		\end{equation*}
\end{lemma}
This lemma holds true for any Lasserre hierarchy over a binary\footnote{The original lemma was shown for nonbinary alphabets (alphabets of size $q$), with the right-hand side containing an additional $\log_2 q$ factor. }  alphabet. 

\subsection{Lasserre hierarchy program for \maxcutkc}
\label{appendix:lasserre:program}
The Lasserre hierarchy relaxation of degree $d$ for \maxcutkc is as follows:
\begin{equation}\label{eq:lassere_formulation}
	\begin{aligned}
		\max\, & \sum_{ i < j } W_{ij} \Pr_{\mu_{\{i,j\}}} (x_i \neq x_j)  \\
		\textrm{s.t.} \, & \sum_{j=1}^n\Pr_{\mu_{S \cup \{j\}}} (x_j = x_j^0 \cap \vecx_{S} = \xi) = (n-k) \Pr_{\mu_{S}}(\vecx_{S} = \xi), \\
  & \forall S \subseteq V, \xi \in \{-1, 1\}^S, |S \cup \{j\}| < d.
	\end{aligned}
\end{equation}

We can show that Equation~\eqref{eq:lassere_formulation} is indeed a relaxation of \maxcutkc. 
Let $\hat{\vecx}$ be any feasible solution of \maxcutkc, and notice that it satisfies the constraint $\sum_{i} x_i^0 \hat{x}_i = n-2k$. 
Now we independently assign that $\Pr_{\mu_{S \cup \{j\}}}(x_j = \hat{x}_j) = 1$, and check whether this assignment satisfies the constraint. 
Notice that for any $\vecx_{S}$ and $\xi$, if $\Pr_{\mu_{S \cup \{j\}}}(\vecx_{S} = \xi) = 0$, the constraint trivially holds. 
Otherwise, we condition the probability on $\vecx_{S} = \xi$, and we get 
\begin{equation*}
\begin{aligned}
    \sum_{j=1}^n\Pr_{\mu_{S \cup \{j\}}} (x_j = x_j^0 \mid \vecx_{S} = \xi) 
    &= \sum_{j=1}^n\Pr_{\mu_{S \cup \{j\}} } (\hat{x}_j = x_j^0 \mid \vecx_{S} = \xi) \\
    &\stackrel{(a)}{=} \sum_{j=1}^n\Pr_{\mu_{S \cup \{j\}}} (\hat{x}_j = x_j^0) \\
    &\stackrel{(b)}{=} n \frac{(n-k)}{n} = n-k. \\
\end{aligned}
\end{equation*}
In the above formulation, $(a)$ holds as the assignment is independent, and $(b)$ holds by the constraint from Equation~\ref{ip:densest-subgraph} that $\sum_{i} x_i^0 \hat{x}_i = n-2k$.

Notice that an equivalent \sdp can be formulated as follows. For every $S \subseteq V, |S \cup \{j\}| \leq d$, and $\xi \in \{-1, 1\}^S$, we introduce a 
variable $\vecv_{S, \xi}$.
Then we obtain:
\begin{equation}\label{eq:lassere_formulation_matrix}
	\begin{aligned}
		\max\, & \sum_{ i < j } W_{ij} (\vecv_{i,1} \cdot \vecv_{j,-1} + \vecv_{i,-1} \cdot \vecv_{j,1})  \\
		\textrm{s.t.} \, &  \sum_{j=1}^n x_j^0 \vecv_{j,1} \cdot \vecv_{S,\xi} - \sum_{j=1}^n x_j^0 \vecv_{j,-1} \cdot \vecv_{S,\xi} = (n-2k) \vecv_{S,\xi} \cdot \vecv_{S,\xi}, \\
  & \forall S \subseteq V, \xi \in \{-1, 1\}^S, |S \cup \{j\}| < d.\\
	\end{aligned}
\end{equation}


Since this is a semidefinite relaxation with a polynomial ($n^{O(d)}$) number of
constraints, we can obtain the optimal value of this program in polynomial time
up to precision $\delta>0$. Let us write $\Sol$ to denote the value of this solution and $\OPT$ to denote the optimal value \maxcutkc. We also use $\left \{\vecv_S\right\}_{S \subseteq V, |S| \leq d}$ to denote the vectors $\vecv_S$  under which the value $\Sol$ is attained, and hence we have 
\begin{equation*}
	(1-\delta)\OPT \leq  \sum_{ i < j } W_{ij} (\vecv_{i,1} \cdot \vecv_{j,-1} + \vecv_{i,-1} \cdot \vecv_{j,1}) = \Sol.
\end{equation*}


\subsection{Obtaining a ``decorrelated'' solution via conditioning}
\label{appendix:lasserre:decorrelate}

After we solve Equation~\eqref{eq:lassere_formulation}, we obtain the local distributions of $\vecx$, which are consistent on the marginal distributions up to $d$ entries. 
We will apply a conditioning process in order to obtain a Lasserre hierarchy
such that the mutual information of each pair of entries is bounded, i.e.,
$\E_{i,j \sim V} \left[ I(x_i, x_j)\right] \leq \delta$. In particular, a step in the conditioning 
the procedure works as follows: 
\begin{itemize}
	\item[] Pick a vertex $i$ uniformly at random, and fix its value to either $1$ or $-1$, according to its marginal distribution $\mu_{\{i\}}$. Update the probability distributions $\{\mu_{S}\}_{S \subseteq V}$ according to this fixing, as discussed in Subsection \ref{subsection:lassere_hierarchy_properties}.
\end{itemize}
Let us observe that the objective value of Equation \eqref{eq:lassere_formulation} does not change in expectation. 
Furthermore, if we denote $\tilde{\vecv}_{j, -1}, \tilde{\vecv}_{j, 1}, \tilde{\vecv}_{S, \xi}$ the vectors obtained after the conditioning,
then in expectation we obtain that
\begin{equation*}
\sum_{j=1}^n x_j^0 \tilde{\vecv}_{j,1} \cdot \tilde{\vecv}_{S,\xi} - \sum_{j=1}^n x_j^0 \tilde{\vecv}_{j,-1} \cdot \tilde{\vecv}_{S,\xi} = (n-2k) \tilde{\vecv}_{S,\xi} \cdot \tilde{\vecv}_{S,\xi}
\end{equation*}
is satisfied. 



Next, we apply Lemma~\ref{lemma:low_mutual_information} to obtain Lemma~\ref{lemma:conditioning}, which is analogous to Lemma 4.6 from \citet{DBLP:conf/soda/RaghavendraT12}. The difference is that our problem \maxcutkc has a different constraint; hence, a different analysis on the lower bound on the objective value is needed in order to apply the Markov inequality.

\begin{lemma}\label{lemma:conditioning}
There exists an algorithm running in time 
\begin{equation*}
	O\left(n^{2}d' \left(\frac{en}{d'} \right)^{d'}2^{d'}\right),
\end{equation*}
which, given a Lasserre solution outlined in Equation~\eqref{eq:lassere_formulation} of degree $d= d'+2$ with value $\Sol$, finds 
a degree $2$  Lasserre solution with objective value 
$\Sol (1-\delta)$, and which satisfies $\E[I(x_i, x_j)] \leq \delta$.
\end{lemma}
\begin{proof}
	Using the probabilistic method, we first show that with some probability, conditioning of variables does not reduce the 
	objective value by more than $\delta \Sol$. Then, we show that with high probability, conditioning satisfies the inequality 
	\begin{equation*}
		\E[I(x_i, x_j)] \leq \delta.
	\end{equation*}
	The randomness in the expression above comes from the random choices in our conditioning algorithm. 
	Then, by union bound, we can conclude that there is a conditioning with value at least $\Sol - \delta \Sol$ and with low average mutual information. We then conclude the lemma by discussing how this conditioning can be found efficiently in the time described in the lemma statement.

	Let us now provide details. Let us denote with $W \in \mathbb{R}_+$ the
	value $W=\sum_{i<j} W_{ij}$. First, consider the conditioning procedure
	outlined in this section. Since we are sampling the variables according to
	their marginal distributions, the expected value of the program
	\eqref{eq:lassere_formulation} remains $\Sol$. Let us use $X$  to denote a
	random variable whose value is the value of the Lasserre hierarchy after $t$
	conditioning steps, where $t$ is the same number as the number from Lemma~\ref{lemma:low_mutual_information}. We can then calculate
	\begin{equation*}
		\pr\left( X < (1-\delta) \E[X] \right) = \pr\left(W-X \geq  W- (1-\delta) \E[X] \right).  
		\end{equation*}
	Now, since $X \in [0,W]$, by Markov's inequality we have 
	\begin{equation}\label{appendix_aleksa:eq:markov_x}
		\pr\left( X < (1-\delta) \E[X] \right) \leq \frac{W- \E[X]}{W-(1-\delta) \E[X]}  = \frac{1- \E[X]/W}{1-(1-\delta)\E[X]/W}.
	\end{equation}
	Let us use $\zeta=\E[X]/W$. In order to find a suitable upper bound on the expression above, we first prove a lower bound 
	on $\zeta$. 
	

	For this, we consider a feasible solution to 
 Equation~\eqref{eq:lassere_formulation}: for each 
	$i \in V$, we independently assign $x_i^0$  to vertex $i$  according to probability $\Pr_{\mu_{i}}(x_i = x_i^0)= \frac{n-k}{n} =(1+\varsigma)/2$, 
	and $-x_i^0$ otherwise. 
 Let us now give a lower bound for $\E[X]$,  by giving a lower bound of a term 
	\begin{equation*}
		\Pr_{\mu_{\{i,j\}}} (x_i \neq x_j) = \frac{1-\varsigma^2}{2},
	\end{equation*}
	according to the assignment.
	Going back to Equation~\eqref{appendix_aleksa:eq:markov_x}, this shows that $\zeta=\E[X]/W$  satisfies $\zeta \geq (1-\varsigma^2)/2$.
	Hence, we can write
	\begin{equation*}
		\pr\left( X < (1-\delta) \E[X] \right) \leq \frac{1- \zeta}{1-(1-\delta)\zeta}  
		\leq \frac{1}{1+\delta\frac{1-\varsigma^{2}}{1+\varsigma^{2}}}.
	\end{equation*}
 
	\par
	Let us denote with $I$ the random variable $I=I(X_i,X_j \mid X_{i_1},\hdots,X_{i_t})$.  By Markov's inequality
	\begin{equation*}
		\pr \left( I \geq \sqrt{\frac{1}{d'-1}} \right) \leq  \frac{\E[I]}{\sqrt{\frac{1}{d'-1}} } \leq \sqrt{\frac{1}{d'-1}},
	\end{equation*}
	and therefore the probability of the randomized algorithm not finding a fixing for which $\E[I_{i,j}] \leq \sqrt{\frac{1}{d'-1}}$ is at most $\sqrt{\frac{1}{d'-1}}$. Hence, the probability of the algorithm failing is at most 
	\begin{equation*}
		\begin{split}
		\frac{1}{1+\delta \cdot (1-\varsigma^{2})/(1+\varsigma^{2})} + \sqrt{\frac{1}{d'-1}}
		= \frac{1}{1+\delta\cdot (1-\varsigma^{2})/(1+\varsigma^{2})}  + \frac{1}{4}\cdot \delta(1-\varsigma^2) <1,
		\end{split}
	\end{equation*}
	and therefore by union bound there is a fixing which satisfies $\E[I_{i,j}] \leq \delta$ and with value at least 
	$\Sol(1-\delta)$.

	\par 
Such a fixing, i.e., computing Equation~\eqref{eq:multral-info-goal}, can be found using brute-force search. The size of the space of all possible fixings can be calculated
	as follows. First, we observe that there\footnote{Since we do not know the value $t$  before hand, we need to test all $i=1,\hdots,d'$.} are
	$\sum_{i=0}^{d'} {n \choose i} $ possible variables. Furthermore, once we fix $i$  variables, the space of possible 
	values these variables can be assigned to is of size $2^{i}$. Hence, using the upper bound ${n \choose i} \leq (n e / i ) ^ i$, the size of this space is at most 
	\begin{equation*}
		d' \left( \frac{en}{d'}\right)^{d'} 2^{d'}.
	\end{equation*}
	Since calculating mutual information takes time $O(n^{2})$, this gives us the final running time of our algorithm.
\end{proof}

\subsection{Description of the hyperplane rounding algorithm}
\label{appendix:lesserre:rounding}
In this section, we will describe the hyperplane rounding algorithm to the solution of the degree $2$ Lasserre hierarchy, which is obtained by using the conditioning outlined in the proof of Lemma \ref{lemma:conditioning}. 
Let us first write the formulation of the degree $2$ Lasserre hierarchy in
Equation~\eqref{eq:lassere_formulation_2_degree}:
\begin{equation}\label{eq:lassere_formulation_2_degree}
	\begin{aligned}
		& \sum_{ i < j } W_{ij} (\vecv_{i,1} \cdot \vecv_{j,-1} + \vecv_{i,-1} \cdot \vecv_{j,1}) \geq \Sol (1-\delta), \quad \textrm{and}  \\
		 &  \sum_{j=1}^n x_j^0 \vecv_{j,1} \cdot \vecv_{i,1} - \sum_{j=1}^n x_j^0 \vecv_{j,-1} \cdot \vecv_{i,1} = (n-2k) \vecv_{i,1} \cdot \vecv_{i,1}, \quad \textrm{and}\\
  &  \sum_{j=1}^n x_j^0 \vecv_{j,1} \cdot \vecv_{i,-1} - \sum_{j=1}^n x_j^0 \vecv_{j,-1} \cdot \vecv_{i,-1} = (n-2k) \vecv_{i,-1} \cdot \vecv_{i,-1}.\\
	\end{aligned}
\end{equation}

If we define $\vecv_i \coloneqq \vecv_{i, -1} - \vecv_{i, 1}$, 
and $\vecv_0 \coloneqq \vecv_{i, -1} + \vecv_{i, 1}$, 
the solution presented in Equation~\eqref{eq:lassere_formulation_2_degree} implies a solution that satisfies the following constraints:

\begin{equation*}
	\begin{split}
		\frac{1}{2}	& \sum_{i<j}W_{ij} (1-\vecv_i \cdot \vecv_j) \geq \Sol \cdot (1- \delta), \quad \textrm{and}\\
		 &\sum_{i \in V} x_i^0 \vecv_i\cdot \vecv_0   = \tau n.
	\end{split}
\end{equation*}

Next, we describe the hyperplane rounding procedure, which differs from the ones
in the other parts of our paper. 
The procedure is essentially the same as used by \citet[Section 5.4]{DBLP:conf/soda/RaghavendraT12}, with a mild difference in how we set the bias $\kappa_i$. Here we set $\kappa_i \coloneqq \vecv_i \cdot \vecv_0$. 
The goal is to ensure that the cardinality constraint in \maxcutkc holds in expectation.

Let $\bar{\vecv}_i$ be defined as
\begin{equation*}
	\bar{\vecv}_i = \begin{cases}
		\frac{\vecv_i - \kappa_i \vecv_0 }{ \| \vecv_i - \kappa_i \vecv_0\| }, & \textrm{if } |\kappa_i| \neq 1,\\
		\textrm{a unit vector orthogonal to all other vectors}, 			   & \textrm{if } |\kappa_i| = 1.
	\end{cases} 
\end{equation*}
 Let us use $\varPhi$ to denote the cumulative distribution function of 
 a single variable Gaussian distribution, and let us define the assignment $\{\bar{x}_i\}_{i=1}^n$ as 
\begin{equation*}
	\begin{split}
	g \sim 		\textrm{standard $n$-dimensional Gaussian vector},\\
	\bar{x}_i = \begin{cases}
		-1, & \textrm{if } \bar{\vecv}_i \cdot g \leq \varPhi^{-1}\left( \frac{1-\kappa_i}{2} \right),\\
		1,  & \textrm{otherwise.}
	\end{cases}
	\end{split}
\end{equation*}


Notice that $\bar{\vecx}$ encodes the partition after the hyperplane rounding. 
We remark that that $\Exp{\bar{x_i}} = \kappa_i$. A computer-assisted proof shows that rounding the vectors $\vecv_i$  incurs a loss of at most $0.858$, as claimed in \citet[Section 5.4]{DBLP:conf/soda/RaghavendraT12}. 

We present our main result of this hyperplane rounding procedure in Lemma~\ref{lem:rounding-procedure}.

\begin{lemma}
\label{lem:rounding-procedure}
After the hyperplane rounding procedure, it holds that:
\begin{enumerate}
	\item[(a)] $\frac{1}{2} \sum_{i<j} W_{ij}(1 - \bar{x}_i \bar{x}_j) \geq 0.858 \frac{1}{2}\sum_{i<j}W_{ij} (1-\vecv_i \cdot \vecv_j)$, and
	\item[(b)] $\Exp{\sum_{i\in V} x_i^0 \bar{x}_i} = \tau n$.
\end{enumerate}
\end{lemma}

Denote by $\overline{\Rnd}$ the cut induced by the partition $\bar{\vecx}$
encodes. Then Part~(a) of the lemma implies that $\overline{\Rnd} \geq 0.858
\Sol (1-\delta)$, and Part~(b) implies that, in expectation, the cardinality
constraint of \maxcutkc is satisfied. 


Lemma~\ref{lem:rounding-procedure} provides us with a cut that approximates the optimal solution well. However, the number of refinements satisfies the cardinality constraints only in expectation. 
In particular, $\sum_{i \in V} x_i^0 \cdot \bar{x}_i = (n-2k)$ might do not hold.

In the last step we show that $\sum_{i \in V} x_i^0 \cdot \bar{x}_i $ is very close to $(n-2k)$. Hence, 
we can change the signs of a very small number of $\bar{x}_i$  to obtain the final assignment $x_i$ for which the value of the objective function does not change much.


\subsection{Obtaining a rounded solution which almost satisfies constraints}
\label{appendix:lasserre:roundingcombined}
In the previous section, we proved that for our rounding procedure it holds that $\Exp{\sum_{i\in V} x_i^0 \bar{x}_i} = \tau n$.
In this section, we will show that $\sum_{i \in V} x_i^0 \cdot \bar{x}_i $ is indeed very close to $n-2k$, by bounding its variance. 

\begin{lemma}
\label{lem:varaince-bound}
    $\E_{i,j \in V}[I(x_i, x_j) ] \leq \delta$ implies that 
\begin{equation*}
  \Var\left[ \sum_{i \in V} x_i^0 \bar{x}_i  \right]	\leq C |V|^2 \delta^{1/12},
\end{equation*}
after we apply the hyperplane rounding procedure according to the above section, where $C$ is some constant. 
\end{lemma}
\begin{proof}[Partial proof]
    We only highlight the parts different from~\citet[Theorem 5.6]{DBLP:conf/soda/RaghavendraT12}.
    The differences arise as we consider the initial partition encoded as $\vecx_0$. 
    The remaining part of the analysis is the same.
	We obtain:
    \begin{equation*}
		\begin{split}
	\Var\left[ \sum_{i \in V} x_i^0 \bar{x}_i  \right]	
	&= \sum_{i, j \in V} (\mathbb{E}[\bar{x}_i x_i^0 \bar{x}_j x_j^0] - \mathbb{E}[\bar{x}_i x_i^0] \mathbb{E}[\bar{x}_j x_j^0]) \\
	&\leq \sum_{i, j \in V} 4 \max_{a \in \{-1,1\}, b \in \{-1, 1\}} \Pr(\bar{x}_i=a, \bar{x}_j = b) - \Pr(\bar{x}_i=a) \Pr(\bar{x}_i=b) \\
	&=\sum_{i, j \in V} \bigO \left(\sqrt{I(\bar{x}_i, \bar{x}_j)}\right) \\
        &\leq  C |V|^2 \delta^{1/12}.
		\end{split}	
    \end{equation*}
    The last inequality follows directly from the analysis of \citet[Theorem 5.6]{DBLP:conf/soda/RaghavendraT12}.  
\end{proof}

Hence, by Chebyshev's inequality, we have that 
\begin{equation*}
  \pr\left(  \left|\sum_{i \in V} x_i^0 \bar{x}_i - (n-2\cdot k) \right| \geq \delta^{1/48} |V|\right) \leq  \frac{|V|^2 C(\delta^{1/12})}{\delta^{1/24}|V|^2} = C\delta^{1/24}.
\end{equation*}

In other words, with probability $1- C\delta^{1/24}$ the values $\bar{x}_i$ satisfy 
\begin{equation}
	\left|\sum_{i \in V} x_i^0 \bar{x}_i - (n-2k) \right| \leq \delta^{1/48} |V|,
\end{equation}

Now we are ready to prove the main Lemma that we leave in the main content.
\boundvariance*

\begin{proof}
	The coefficient $0.858 (1-\delta)$ comes from the rounding algorithm described in the previous subsection and obtains a rounded solution of value at least $0.858$ from 
	the SDP value, in expectation. We need to repeat rounding $\Omega(\log(1/\delta))$
	times to ensure that with a high probability we obtain a rounded solution which incurs a loss of at most $0.858-\delta $. 
 The other $(1 - \delta)$-factor comes from the choice of precision when we solve the SDP formulation. 
Furthermore,
	for a single rounding step we have that 
\begin{equation}\label{eq:no_vertices_to_move}
	\left|\sum_{i \in V} x_i^0 \bar{x}_i - (n-2k) \right| \leq \delta^{1/48} |V|,
\end{equation}
with probability $1- C\delta^{1/24}$. Hence, for sufficiently small $\delta = (\varepsilon/3)^{96}$ by the union bound 
we can ensure that the properties stated in the lemma hold.
\end{proof}








