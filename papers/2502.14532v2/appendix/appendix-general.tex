\section{Omitted content of Section~\ref{sec:general-framework}}
\label{appendix:general}

\subsection{Pseudocode for {\sdp}-based algorithm for \kdense and \dskc (in Section~\ref{sec:general:sdp})}
\label{appendix:densest-subgraph:sdp:pseudocode}

The pseudocode for the {\sdp}-based algorithm for \kdense and \dskc, as discussed in Section~\ref{sec:general:sdp}, 
is illustrated in Algorithm~\ref{algo:sdp-k-dense}.

\begin{algorithm}[H]
\caption{An \sdp-based algorithm for \kdense and \dskc}
\label{algo:sdp-k-dense}
\begin{algorithmic}[1]
\Require{$(G(V, E, w), \OldSet, k)$, number of iterations $N$}
\State \textbf{Step 1: Solve the SDP.} Solve the \sdp~\eqref{sdp:densest-subgraph} to obtain $\vecv_0, \ldots, \vecv_n$
\For{$i = 1$ to $N$}
\State \textbf{Step 2: Hyperplane Rounding.} Obtain the indicator vector $\overline{\vecx}$
\State Sample a unit vector $\vecr$ uniformly randomly from $\mathcal{S}_n$
\For{$i = 1$ to $n$}
\If{$(\vecr \cdot \vecv_0)(\vecr \cdot \vecv_i) \geq 0$}
\State $\overline{x}_i \gets 1$
\Else
\State $\overline{x}_i \gets -1$
\EndIf
\EndFor
\State Set $\SelectSet \gets \{i \colon \overline{x}_i \neq x^0_i\}$ and
$\TempSet \gets \{i \colon \overline{x}_i = 1\}$
\State \textbf{Step 3: Fixing $\SelectSet$.}  Set $\FixSelectSet \gets \SelectSet$
\If{$\abs{\FixSelectSet} < k$}
\State Add $k - \abs{\FixSelectSet}$ arbitrary vertices from $V \setminus (\FixSelectSet \cup \TempSet)$ to $\FixSelectSet$
\ElsIf{$\abs{\FixSelectSet} > k$}
\If{$\abs{\FixSelectSet \setminus \TempSet} \geq \abs{\FixSelectSet} - k$}
\State Remove $\abs{\FixSelectSet} - k$ arbitrary vertices in $\FixSelectSet \setminus \TempSet$ from $\FixSelectSet$
\Else
\State Remove all vertices in $\FixSelectSet \setminus \TempSet$ from $C$
\State Greedily choose $\abs{\FixSelectSet \cap \TempSet} - k$ minimum weighted
	degree nodes in subgraph $G[\FixSelectSet \cap \TempSet]$ and them remove from $\FixSelectSet$
\EndIf
\EndIf
\EndFor
\State \Return{$\FixSelectSet$}
\end{algorithmic}
\end{algorithm}

\subsection{Proof of Theorem~\ref{thm:densest-subgraph-sdp}}
\label{sec:proof:thm-densest-subgraph-overview}

The pseudocode of our algorithm can be found in
Algorithm~\ref{algo:sdp-k-dense}. In the present section, we demonstrate that
Algorithm~\ref{algo:sdp-k-dense} provides a constant factor approximation to the \kdense problem. As implied by Lemma~\ref{lemma:k-densify-local-changes-connection}, this solution also offers a constant approximation for the \dskc problem under the theorem's specified condition.


\subsubsection{Fixing $\SelectSet$}
\label{sec:proof:subgraph-sdp:fix-c}
Notice that at the end of a round of hyperplane rounding, we get $\SelectSet = \{i \colon x_i \neq x_i^0\}$, and $\TempSet = \OldSet \symm \SelectSet = \{i \colon x_i = 1\}$. 
In addition, we use Corollary~\ref{lem:dense-bound-size-3} to bound $\Exp{\abs{E[\TempSet]}} \geq \beta W_{\sdp}^*$. 
Since $\abs{\SelectSet}$ is not necessarily equal to $k$, the algorithm needs to alter $\SelectSet$. 
This is done by considering a new variable $\FixSelectSet=\SelectSet$ and
making changes to $\FixSelectSet$ until $\abs{\FixSelectSet} = k$. 

To give a lower bound on $\abs{E[\OldSet \symm \FixSelectSet]}$, we notice that only the operation that greedily removes nodes in $\FixSelectSet \cap \TempSet$ from $\FixSelectSet$ decreases $\abs{E[\OldSet \symm \FixSelectSet]}$. In the worst case scenario, all $\abs{
\SelectSet} - k$ nodes are removed.
Lemma~\ref{lem:dense-move-all} bounds the decrease of $\abs{E[\TempSet]}$ after this greedy procedure. 

\densemoveall* 

\begin{proof}
Notice that if $\tau \geq \mu$, to satisfy the cardinality constraint, $\TempSet$ adds more nodes, thus edges, and the sum of edge weights increases. In this case, $\SdpRnd \geq \overline{\SdpRnd}$.

When $\tau < \mu$, we let $\FixTempSet = \OldSet \symm \FixSelectSet$. We allow $\FixTempSet$ to change correspondingly with $\FixSelectSet$. Initially, $\FixTempSet = \TempSet$, and we will demonstrate that, in the end, $\abs{E[\FixTempSet]} \geq \abs{E[\TempSet]}\frac{k^2}{\abs{\SelectSet}^2}(1 - \frac{1}{k})$.

We analyze three operations that modify $\FixSelectSet$. We first note that the
operation which adds nodes from $\vertices \setminus (\FixSelectSet \cup \TempSet)$ to $\FixSelectSet$ only occurs when $\abs{\SelectSet} < k$. This operation increases $\abs{E[\FixTempSet]}$.

For scenarios where $\abs{\SelectSet}>k$, two operations exist. The operation
that removes nodes from $\FixSelectSet \setminus \TempSet$ further adds nodes to $\FixTempSet$, thereby increasing $\abs{E[\FixTempSet]}$.
It remains to consider the operation, which greedily removes nodes from
$\FixSelectSet \cap \TempSet$ in $\FixSelectSet$ and which decreases
$\abs{E[\FixTempSet]}$.

We now define $\FixTempSet_0$ as the initial $\FixTempSet$ of this greedy procedure. As the previous two operations contribute additional nodes to $\FixTempSet$, we conclude that $\abs{E[\FixTempSet_{0}]} \geq \abs{E[\TempSet]}$. Similarly, we assign $\FixSelectSet_0$ as the initial $\FixSelectSet$ of the greedy procedure.

Let $\FixTempSet_t$ represent $\FixTempSet$ and $\FixSelectSet_t$ represent $\FixSelectSet$ at the $t$-th step of the greedy procedure. We observe that at the $t$-th step, the decrease in the sum of edge weights is at most $\frac{2\abs{E[\FixTempSet_t]}}{\abs{\FixSelectSet_t}}$, as suggested by the pigeonhole principle. In the worst-case scenario, the sum of edge weights decreases $\abs{\FixSelectSet_0} - k$ times, until $\abs{\FixSelectSet} = k$. The value of $\abs{E[\FixTempSet]}$ at this point is lower bounded by:

\begin{displaymath}
\begin{split}
&\abs{E[\FixTempSet_{0}]} \left(1 - \frac{2}{\abs{\FixSelectSet_0}}\right)\left(1 - \frac{2}{\abs{\FixSelectSet_1}}\right) \ldots \left(1 - \frac{2}{k+1}\right) \\
&= \abs{E[\FixTempSet_{0}]} \frac{(\abs{\FixSelectSet_0} -2)}{\abs{\FixSelectSet_0}} \frac{(\abs{\FixSelectSet_1} -2)}{\abs{\FixSelectSet_1}}\ldots \frac{k-1}{k+1} \\
&= \abs{E[\FixTempSet_{0}]} \frac{k(k-1)}{\abs{\FixSelectSet_0}(\abs{\FixSelectSet_0} -1)}.
\end{split}
\end{displaymath}

Given that $\abs{\FixSelectSet_0} \leq \abs{\SelectSet}$ and $\abs{E[\FixTempSet_{0}]} \geq \abs{E[\TempSet]}$, the above expression is at least $\abs{E[\TempSet]}\frac{k(k-1)}{\abs{\SelectSet}(\abs{\SelectSet} -1)} \geq \abs{E[\TempSet]}\frac{k(k-1)}{\abs{\SelectSet}^2}$.
Notice that we assume $k \in \Omega(n)$, and thus $1 - \frac{1}{k} = 1 -
\frac{1}{\Theta(n)}$. This concludes our proof. 
\end{proof}

Recall that our goal is to compute the approximation ratio, which can be formulated as 
\begin{equation}
	\max_{\gamma \in [0.1, 5], \eta \in \left[\frac{1-\alpha + 0.01}{\alpha}, +\infty\right)} \min_{\mu \in [0 ,1]} \kappa_{\tau}(\mu) \SdpRatio_{\tau}(\mu, \gamma, \eta).
\end{equation}
Let us define $f_{\tau}(\mu, \gamma, \eta) = \kappa_{\tau}(\mu) \SdpRatio_{\tau}(\mu, \gamma, \eta)$.
Notice that for \kdense, according to the above lemma, when $\mu \leq \tau$, $\kappa_{\tau}(\mu) = 1$.
Hence, for the worst-case analysis, we only need to discuss the scenario when $\mu \in (\tau, 1]$. Below, we give more details on computing the approximation ratio.
We ignore the factor $(1 - \frac{1}{\Theta(n)})$ in our computation as it only introduces a small multiplicative error in our result.

\subsubsection{Computing the approximation ratio}
\label{sec:proof:subgraph-sdp:fix-c-lowerbound-compute}
We first find the stationary point of $f_{\tau}(\mu, \gamma, \eta)$ w.r.t.\ $\mu$ by solving 
\begin{equation}
	\begin{aligned}
		\frac{\partial f_{\tau}(\mu, \gamma, \eta)}{\partial \mu} = 0.
	\end{aligned}
\end{equation}

Let this stationary point be $\mu_0$. We notice that $\mu_0$ can be written as a function of $\gamma$, $\eta$ and $\tau$. 
Since $\mu_0$ should be in the domain $[\tau, 1]$, we introduce the following step function $g_{\tau}(\gamma, \eta)$ to 
tackle the situation when $\mu_0$ is invalid (chosen outside of the domain):
\begin{equation}
	\begin{aligned}
		g_{\tau}(\gamma, \eta) =
		\begin{cases}
			f_{\tau}(\mu_0, \gamma, \eta) & \text{for } \tau \leq \mu_0 \leq 1, \\ 
			1 & \text{otherwise}.
		\end{cases}
	\end{aligned}
\end{equation}

For any given $\gamma$ and $\eta$, the minimization of the function
$f_{\tau}(\mu, \gamma, \eta)$ w.r.t.\ $\mu$ can be written as the minimum of three constant functions. 
The two boundary cases when $\mu = \tau$ or $\mu = 1$ are given by 
$f_{\tau}(\mu_0, \gamma, \eta)$ and $f_{\tau}(1, \gamma, \eta)$, whereas
$f_{\tau}(\mu_0, \gamma, \eta)$ represents the case when $\mu \in [\tau, 1]$. 
Thus, 
$\min_{\mu \in [\tau, 1]}f_{\tau}(\mu, \gamma, \eta) = \min \left\{f_{\tau}(\tau, \gamma, \eta), f_{\tau}(1, \gamma, \eta), g_{\tau}(\gamma, \eta)\right\}$. 
Now we obtain:
\begin{equation*}
	\begin{aligned}
f(\tau) &= \max_{\gamma, \eta} \min_{\mu \in [\tau, 1]}f_{\tau}(\mu, \gamma, \eta)  \\
&= \max_{\gamma, \eta} \min \left\{f_{\tau}(\tau, \gamma, \eta), f_{\tau}(1, \gamma, \eta), g_{\tau}(\gamma, \eta)\right\}.
	\end{aligned}
\end{equation*}

Then for any $\tau \in (0, \frac{1}{2}]$, we numerically select $\gamma$ and $\eta$ to maximize $f_{\tau}(\cdot)$ and $g_{\tau}(\cdot)$, and we obtain Figure~\ref{fig:dense-ratio}. 

\begin{figure}
\centering
\resizebox{0.6\columnwidth}{!}{%
\inputtikz{plots/dense-ratio}
}
\caption{Our approximation ratios for $\kdense$ where $\tau = \frac{k}{n}$. We compare the approximation ratio $f(\tau)$ with $\tau$ and $\tau^2$.
Here, $f(\tau)$ is the worst case approximation ratio of our \sdp-based
algorithm for the \kdense problem. The approximation ratio for \dskc is $f(\tau) \frac{1-c}{1+c}$.}
\label{fig:dense-ratio}
\end{figure}

In particular, if we set $\gamma = 0.920$ and $\eta = 1.65$, we get $f(\tau = 0.5) > 0.58$. 
Note if we set the same parameter as~\cite{DBLP:journals/jal/feigel01} ($\gamma
	= 3.87$ and $\eta = \frac{0.65}{3.87}$), we obtain the same approximation
ratio as \cite{DBLP:journals/jal/feigel01} obtained for \dks with $f(\tau = 0.5) = 0.517$. 

\subsection{Pseudocode for {\sdp}-based algorithm for \maxcutkc (in Section~\ref{sec:general:sdp})}
\label{appendix:maxcutkc:sdp:pseudocode}

The pseudocode for the {\sdp}-based algorithm for \maxcutkc, as discussed in Section~\ref{sec:general:sdp}, 
is illustrated in Algorithm~\ref{algo:maxcutkc-sdp}.

\begin{algorithm}[H]
\caption{An \sdp-based algorithm for \maxcutkc}
\label{algo:maxcutkc-sdp}
\begin{algorithmic}[1]
\Require{$(G(V, E, w), \OldSet, k)$, number of iterations $N$}
\State \textbf{Step 1: Solve SDP.} Solve the \sdp~\eqref{sdp:densest-subgraph} to obtain the solution $\vecv_0, \ldots, \vecv_n$
\For{$i = 1$ to $N$}
\State \textbf{Step 2: Hyperplane Rounding.} Obtain the indicator vector $\overline{\vecx}$
\State Sample a unit vector $\vecr$ uniformly randomly from $\mathcal{S}_n$
\ForAll{$i \in \{0, \ldots, n\}$}
    \If{$(\vecr \cdot \vecv_0)(\vecr \cdot \vecv_i) \geq 0$}
        \State $\overline{x}_i \leftarrow 1$
    \Else
        \State $\overline{x}_i \leftarrow -1$
    \EndIf
\EndFor
\State Set $\SelectSet \leftarrow \{i \colon \overline{x}_i \neq x^0_i\}$
\State \textbf{Step three, Fix $\SelectSet$:} Set $\FixSelectSet \gets \SelectSet$ 
\While{$\abs{\FixSelectSet} \neq k$}
    \If{$\abs{\FixSelectSet} < k$}
        \State $u^* \leftarrow \arg\max_{u \in V \setminus \FixSelectSet} \cut{\OldSet \symm (\FixSelectSet \cup \{u\})}$
        \State $\FixSelectSet \leftarrow \FixSelectSet \cup \{u^*\}$
    \ElsIf{$\abs{\FixSelectSet} > k$}
        \State $u^* \leftarrow \arg\max_{u \in \FixSelectSet} \cut{\OldSet \symm (\FixSelectSet \setminus \{u\})}$
        \State $\FixSelectSet \leftarrow \FixSelectSet \setminus \{u^*\}$
    \EndIf
\EndWhile
\EndFor 
\State \Return{$\FixSelectSet$}
\end{algorithmic}
\end{algorithm}



\subsection{Proof of Theorem~\ref{thm:max-cut-sdp}}
\label{appendix:mc-proofs:sdp}

The pseudocode for our algorithm can be found in
Algorithm~\ref{algo:maxcutkc-sdp}. In the present section, we demonstrate that
Algorithm~\ref{algo:maxcutkc-sdp} provides a constant factor approximation to
the \maxcutkc problem.
Note that the first two steps (i.e., solving the SDP and hyperplane rounding) are identical to those of Algorithm~\ref{algo:sdp-k-dense}. 
The third step (i.e., fixing $\SelectSet$) is identical to the third step of Algorithm~\ref{algo:maxcutkc-blackbox}.

\subsubsection{Fixing $\SelectSet$}
\label{appendix:mc-proofs:sdp:fix-c}
Again we let $\abs{\SelectSet} = \mu n$ and $k = \tau n$. 
We notice the greedy procedure (i.e., fixing $\SelectSet$) that appears in Algorithm~\ref{algo:maxcutkc-sdp} is the same as the counterpart that appears in Algorithm~\ref{algo:maxcutkc-blackbox}.
Thus, we can directly apply Lemma~\ref{lem:cut-blackbox-deduction} to derive a lower bound on the cut for fixing $\SelectSet$, as in Lemma~\ref{lemma:cut-move-all}.

\cutmoveall*

\begin{proof}
	The proof is identical to Lemma~\ref{lem:cut-blackbox-deduction}. 
\end{proof}

\subsubsection{Approximation ratio}
\label{appendix:mc-proofs:sdp:approx}
Let $k = \tau n$ and let $\abs{\SelectSet} = \mu n$. We start with the following corollary of Lemma~\ref{lem:cut-bound-z}.
Notice that the corollary slightly differs from Lemma~\ref{lem:cut-bound-z} in
terms of introducing different random variables $Z_1$ and $Z_2$, respectively, that work for different regimes of $\abs{\SelectSet}$. We use this design because it helps to obtain better approximation ratios for \maxcutkc. 

\begin{corollary}
	\label{cor:cut-bound-z-two-case}
	Let $\gamma_1, \gamma_2 \in [0.1, 5]$ and $\eta_1, \eta_2 \in [\frac{1-\alpha+0.01}{\alpha}, +\infty)$ be four constants. 
	Let $Z_1$ and $Z_2$ be two random variables such that 
	\begin{displaymath}
		\left\{ \begin{array}{rcl}
Z_1 = \frac{\cut{\TempSet}}{W^*_{SDP}} + \gamma_1 \eta_1 \frac{n-\abs{\SelectSet}}{n-k} + \gamma_1 \frac{\abs{\SelectSet}(2k - \abs{\SelectSet})}{n^2} & \mbox{if} & \abs{\SelectSet} > k, \\ 
Z_2 = \frac{\cut{\TempSet}}{W^*_{SDP}} + \gamma_2 \eta_2 \frac{\abs{\SelectSet}}{k} + \gamma_2 \frac{\abs{\SelectSet}(2k - \abs{\SelectSet})}{n^2} & \mbox{if} & \abs{\SelectSet} <k.
\end{array}\right.
	\end{displaymath}
	By repeating the hyperplane rounding at least $N = 2\cdot\frac{1 - p + \epsilon p}{\epsilon p} \log(\frac{1}{\epsilon})$ times, where $p$ is a constant lower bounded by $0.0005$. 
	Then either $\abs{\SelectSet} \geq k$ happens at least $\frac{N}{2}$ times, and with probability at least $1 - \epsilon$, $Z_1 \geq (1-\epsilon) [\alpha + \gamma_1 \eta_1 \alpha + \gamma_1 (\alpha (1 - \tau)^2 - 1 + 2\tau)]$; 
	or $\abs{\SelectSet} < k$ happens at least $\frac{N}{2}$ times, and with probability at least $1 - \epsilon$, $Z_2 \geq (1-\epsilon) [\alpha + \gamma_2 \eta_2 \alpha + \gamma_2 (\alpha (1 - \tau)^2 - 1 + 2\tau)]$.
\end{corollary}

Next, we use Corollary~\ref{cor:cut-bound-z-two-case} to bound our approximation
ratio by solving
\begin{equation}
	\max_{\gamma \in [0.1, 5], \eta \in \left[\frac{1-\alpha + 0.01}{\alpha}, +\infty\right)} \min_{\mu \in [0 ,1]} \kappa_{\tau}(\mu) \SdpRatio_{\tau}(\mu, \gamma, \eta).
\end{equation}

Notice that according to Lemma~\ref{lemma:cut-move-all}, $\kappa_{\tau}(\mu) = \frac{\tau^2}{\mu^2} (1 - \frac{1}{\Theta(n)})$ when $\mu \geq \tau$, and $\kappa_{\tau}(\mu) = \frac{(1- \tau)^2}{(1-\mu)^2}(1 - \frac{1}{\Theta(n)})$ when $\mu < \tau$. 
According to Corollary~\ref{cor:cut-bound-z-two-case}, 
\begin{equation}
\lambda_{\tau}(\mu, \gamma_1, \eta_1) \geq \alpha + \gamma_1 \eta_1 \alpha + \gamma_1 (\alpha (1 - \tau)^2 - 1 + 2\tau) - \gamma_1 \eta_1 (1-\mu)\frac{1}{1-\tau} - \gamma_1 \mu (2\tau - \mu) - \frac{1}{\Theta(n)},
\end{equation}
when $\mu \geq \tau$;
and 
\begin{equation}
\lambda_{\tau}(\mu, \gamma_2, \eta_2) \geq \alpha + \gamma_2 \eta_2 \alpha + \gamma_2 (\alpha (1 - \tau)^2 - 1 + 2\tau) - \gamma_2 \eta_2 \frac{\mu}{\tau} - \gamma_2 \mu (2\tau - \mu) - \frac{1}{\Theta(n)},
\end{equation}
when $\mu < \tau$. 

We ignore the factor $(1 - \frac{1}{\Theta(n)})$ in our computation as it only introduces a small multiplicative error in our result.

\emph{Case 1:} $\abs{\SelectSet} \geq k$. This implies that $1 \geq \mu \geq
\tau$. In this domain of $\mu$, let us define the function $f^1_{\tau}(\mu, \gamma_1, \eta_1) = \kappa_{\tau}(\mu)\lambda_{\tau}(\mu, \gamma_1, \eta_1)$. 
The approximation ratio we get from this case is 
$$f^1(\tau) = \max_{\gamma_1 \in [0.1, 5], \eta_1 \in \left[\frac{1-\alpha+0.01}{\alpha}, +\infty\right)} \min_{\mu \in [\tau, 1]} f^1_{\tau}(\mu, \gamma_1, \eta_1).$$ 


\emph{Case 2:} $\abs{\SelectSet} <k$. This implies $\tau > \mu > 0$. In this
domain of $\mu$, let us define the function $f^2_{\tau}(\mu, \gamma_2, \eta_2) = \kappa_{\tau}(\mu)\lambda_{\tau}(\mu, \gamma_2, \eta_2)$. 
The approximation ratio we get from this case is 
$$f^2(\tau) =\max_{\gamma_2 \in [0.1, 5], \eta_2 \in \left[\frac{1-\alpha+0.01}{\alpha}, +\infty\right)} \min_{\mu \in (0, \tau]} f^2_{\tau}(\mu, \gamma_2, \eta_2).$$ 

\emph{Combining Case 1 and Case 2}, consider $1 \geq \mu > 0$. 
We define the function 
\begin{equation*}
	\begin{aligned}
&h(\tau) = \min\left\{f^1(\tau), f^2(\tau) \right\},
	\end{aligned}
\end{equation*}
and note that $h(\tau)$ is the approximation ratio we get in the end. We plot
$h(\tau)$ in Figure~\ref{fig:cut-ratio}. 


\begin{figure}
\centering
\resizebox{0.6\columnwidth}{!}{%
\inputtikz{plots/cut-ratio}
}
\caption{Our approximation ratios for \maxcutkc, where $\tau = \frac{k}{n}$. We
compare the approximation ratio $h(\tau)$ of our \sdp-based algorithm with $\min\{\tau, 1-\tau\}$ and $\min\{\tau^2, (1-\tau)^2\}$.}
\label{fig:cut-ratio}
\end{figure}

\clearpage 
