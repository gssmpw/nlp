\documentclass[11pt, letterpaper]{article}

\usepackage{fullpage}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage[breaklinks]{hyperref}
\usepackage[svgnames]{xcolor}         %
\hypersetup{colorlinks={true},urlcolor={blue},linkcolor={DarkBlue},citecolor=[named]{DarkGreen}}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{enumitem} %
\usepackage{xspace}
\usepackage{amsthm, amsxtra}
\usepackage{bm}%
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{thm-restate} %
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{lmodern} 
\usepackage{authblk}
\usepackage[authoryear, square]{natbib}

\setlist[itemize]{leftmargin=*,noitemsep,topsep=0pt}
\setlist[enumerate]{leftmargin=*,noitemsep,topsep=0pt}


\newcommand{\red}[1]{{\color{red} #1}}

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\BPTIME}{BPTIME}

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\st}{\ensuremath{\textrm{s.t.}}}

\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\sign}{sgn}

\newcommand{\Rnd}{\ensuremath{\mathbf{SoS}}\xspace} %
\newcommand{\SdpRnd}{\ensuremath{\mathbf{SDP}}\xspace} %
\newcommand{\SdpRatio}{\ensuremath{\lambda}\xspace} %
\newcommand{\SdpRatioAvg}{\ensuremath{\varphi}\xspace} %

\newcommand{\Exp}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\pr}{\Pr}
\newcommand{\integers}{\ensuremath{\mathbb{Z}}\xspace} 
\newcommand{\Real}{\ensuremath{\mathbb{R}}\xspace} %
\newcommand{\Gaussian}{\mathcal{N}}
\newcommand{\OPT}{\ensuremath{\mathbf{Opt}}\xspace}
\newcommand{\Sol}{\ensuremath{\mathbf{Sol}}\xspace}


\newcommand{\A}{\mathbf{A}\xspace}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\D}{\mathbf{D}\xspace}
\renewcommand{\L}{\mathbf{L}\xspace}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rcal}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\X}{\mathbf{X}\xspace}

\newcommand{\vecv}{\bm{v}\xspace}
\newcommand{\vecx}{\bm{x}\xspace}
\newcommand{\vecu}{\bm{u}\xspace}
\newcommand{\vecr}{\bm{r}\xspace}
\newcommand{\vecone}{\bm{1}\xspace}
\newcommand{\veczero}{\bm{0}\xspace}

\newcommand{\algo}{\mathcal{A}}
\newcommand{\algoB}{\mathcal{B}}

\newcommand{\Pclass}{\mathsf{P}\xspace}
\newcommand{\Poly}{\ensuremath{\Pclass}}
\newcommand{\NP}{\ensuremath{\mathsf{NP}}}
\newcommand{\sharpP}{\ensuremath{\#\Pclass}\xspace}
\newcommand{\sharpR}{\#\Rcal}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\infnorm}[1]{\left|\left| #1 \right|\right|_\infty}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|_2}
\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\rank}{\operatorname{rank}}

\renewcommand{\lg}{\log}
\newcommand{\poly}{\operatorname{poly}}

\DeclareRobustCommand{\OPT}{%
	\ifmmode
		\operatorname{\bf Opt}
	\else
		\textbf{Opt}\xspace
	\fi
}
\DeclareRobustCommand{\ALG}{%
	\ifmmode
		\operatorname{ALG}
	\else
		\text{ALG}\xspace
	\fi
}

\newcommand{\sbpara}[1]{{\smallskip\noindent\textbf{#1}}}
\newcommand{\bpara}[1]{{\noindent\textbf{#1}}}
\newcommand{\separa}[1]{{\smallskip\noindent\emph{#1}}}
\newcommand{\epara}[1]{{\noindent\emph{#1}}}

\newcommand{\heuristic}{\textsf{\small heuristic}\xspace}

\newcommand{\inputtikz}[1]{%
  \includegraphics{figures/#1}%
}


\newcommand{\parzero}{\ensuremath{c_0}\xspace}
\newcommand{\parone}{\ensuremath{c_1}\xspace}
\newcommand{\partwo}{\ensuremath{c_2}\xspace}
\newcommand{\parthree}{\ensuremath{c_3}\xspace}

\newcommand{\graph}{\ensuremath{G}\xspace}
\newcommand{\vertices}{\ensuremath{V}\xspace}
\newcommand{\edges}{\ensuremath{E}\xspace}
\newcommand{\setU}{\ensuremath{U}\xspace}
\newcommand{\bigO}{\ensuremath{\mathcal{O}}\xspace}
\newcommand{\bigOtilde}{\ensuremath{\Tilde{\mathcal{O}}}\xspace}
\newcommand{\dks}{{\small D}\ensuremath{k}{\small S}\xspace}
\newcommand{\dksplus}{{\small D}\ensuremath{(k\!+\!1)}{\small S}\xspace} %
\newcommand{\dskc}{{\small DS}-\ensuremath{k}{\small R}\xspace}
\newcommand{\maxcut}{{\small M}\textsc{ax}\text{-}{\small C}\textsc{ut}\xspace}
\newcommand{\maxcutkc}{{\maxcut}\text{-}\ensuremath{k}{\small R}\xspace}
\newcommand{\maxksection}{{{\maxcut} with cardinality constraints}\xspace}
\newcommand{\ccmaxcut}{{\maxcut}\text{-}{\small CC}\xspace}
\newcommand{\maxbis}{{\small M}\textsc{ax}\text{-}{\small B}\textsc{isection}\xspace}
\newcommand{\ugc}{{\small UGC}\xspace}
\newcommand{\kdense}{\ensuremath{k}\text{-}{{\small D}\textsc{ensify}}\xspace}
\newcommand{\sdp}{{\small SDP}\xspace}


\newcommand{\ccmaxuncut}{{\small M}\textsc{ax}\text{-}{\small U}\textsc{ncut}\text{-}{\small CC}\xspace} %
\newcommand{\ccvc}{{\small VC}\text{-}{\small CC}\xspace} %
\newcommand{\maxuncutkc}{{\small M}\textsc{ax}-{\small U}\textsc{ncut}-\ensuremath{k}{\small R}\xspace} %
\newcommand{\vckc}{{\small VC}-\ensuremath{k}{\small R}\xspace} %

\newcommand{\gpkc}{{\small GP}-\ensuremath{k}{\small R}\xspace} %
\newcommand{\maxgp}{{\small MAX}-{\small GP}\xspace} %


\newcommand{\spara}[1]{\smallskip\noindent\textbf{#1}}
\newcommand{\para}[1]{\noindent\textbf{#1}}

\newcommand{\SelectSet}{\ensuremath{C}\xspace} %
\newcommand{\FixSelectSet}{\ensuremath{C}^{''}\xspace} %
\newcommand{\OldSet}{\ensuremath{U}\xspace} %
\newcommand{\TempSet}{\ensuremath{\OldSet'}\xspace}%
\newcommand{\FixTempSet}{\ensuremath{\OldSet^{''}}\xspace}%
\newcommand{\OldComSet}{\ensuremath{\overline{\OldSet}}\xspace} %
\newcommand{\TempComSet}{\ensuremath{\overline{\OldSet'}}\xspace}%

\newcommand{\smallconst}{\ensuremath{p}\xspace}

\newcommand{\symm}{\triangle}
\newcommand{\comp}[1]{\overline{#1}}
\newcommand{\cutnode}[2]{\mathrm{cut}_{#1}\!\left(#2\right)}

\makeatletter
\newcommand{\cut}{\@ifnextchar\bgroup\cut@i{\mathrm{cut}}}
\newcommand{\cut@i}[1]{\ensuremath{\mathrm{cut}\!\left(#1\right)}}

\newcommand{\partition}{\@ifnextchar\bgroup\partition@i{\mathcal{G}}}
\newcommand{\partition@i}[1]{\ensuremath{\mathcal{G}\!\left(#1\right)}}

\makeatother

\newcommand{\Greedy}{\textsf{\small Greedy}\xspace}
\newcommand{\Peel}{\textsf{\small Peel}\xspace}
\newcommand{\SDPalgo}{\textsf{\small SDP}\xspace}
\newcommand{\Random}{\textsf{\small Rnd}\xspace}
\newcommand{\Local}{\textsf{\small Local}\xspace}
\newcommand{\Init}{\textsf{\small Init}\xspace}

\newcommand{\Blackbox}{\textsf{\small B:}\xspace}
\newcommand{\MoveOut}{\textsf{\small MoveOut}}

\newcommand{\TypeOne}{\textsf{\small :I}\xspace}
\newcommand{\TypeTwo}{\textsf{\small :II}\xspace}

\newcommand{\denseGreedy}{\Greedy}
\newcommand{\denseSDPalgo}{\SDPalgo}
\newcommand{\denseSDPMerge}{\Blackbox\SDPalgo}
\newcommand{\densePeelMerge}{\Blackbox\Peel}
\newcommand{\denseSQD}{\textsf{\small SQD}\xspace}
\newcommand{\denseinit}{\Init}
\newcommand{\denserandom}{\Random}

\newcommand{\cutGreedy}{\Greedy}
\newcommand{\cutSDPalgo}{\SDPalgo}
\newcommand{\cutBlackSDP}{\Blackbox\SDPalgo}
\newcommand{\cutBlackGreedy}{\Blackbox\Greedy}
\newcommand{\cutBlackLocalOne}{\Blackbox\Local\TypeOne}
\newcommand{\cutBlackLocalTwo}{\Blackbox\Local\TypeTwo}

\newcommand{\Wiki}{\textsf{\small Wiki}\xspace}
\newcommand{\balanced}{\textsf{\small SBM-Balanced}\xspace}
\newcommand{\dense}{\textsf{\small SBM-DenseSubg}\xspace}
\newcommand{\sparse}{\textsf{\small SBM-SparseSubg}\xspace}
\newcommand{\gb}{\textsf{\small Wiki-GB}\xspace}
\newcommand{\de}{\textsf{\small Wiki-DE}\xspace}
\newcommand{\es}{\textsf{\small Wiki-ES}\xspace}
\newcommand{\us}{\textsf{\small Wiki-US}\xspace}
\newcommand{\dblp}{\textsf{\small SNAP-DBLP}\xspace}
\newcommand{\youtube}{\textsf{\small SNAP-Youtube}\xspace}
\newcommand{\amazon}{\textsf{\small SNAP-Amazon}\xspace}







\title{\bf OptiRefine: Densest subgraphs and maximum cuts with $k$ refinements}

\author[1]{Sijing Tu}
\author[1,2]{Aleksa Stankovic} 
\author[3]{Stefan Neumann} 
\author[1]{Aristides Gionis} 


\affil[1]{KTH Royal Institute of Technology}
\affil[2]{Qubos Systematic}
\affil[3]{TU Wien}


\begin{document}
\maketitle 

\begin{abstract}
Data-analysis tasks often involve an iterative process, which requires refining previous solutions.  
For instance, when analyzing dynamic social networks, we may be interested in monitoring
the evolution of a community that was identified at an earlier snapshot. 
This task requires finding a community in the current snapshot of data that is ``close'' to the earlier-discovered community of interest.
However, classic optimization algorithms, which typically find solutions from scratch, potentially
return communities that are very dissimilar to the initial one. 
To mitigate these issues, we introduce the \emph{OptiRefine framework}.
The framework optimizes initial solutions by making a small number of \emph{refinements}, thereby ensuring that the new solution remains close to the
initial solution and simultaneously achieving a near-optimal solution for the optimization problem.
We apply the OptiRefine framework to two classic graph-optimization problems:
\emph{densest subgraph} and \emph{maximum cut}. For the \emph{densest-subgraph problem}, we
optimize a given subgraph's density by adding or removing $k$~nodes. We show
that this novel problem is a generalization of $k$-densest subgraph, and provide
constant-factor approximation algorithms for $k=\Omega(n)$~refinements.  
We also study a version of \emph{maximum cut} in which the goal is to improve a given cut.
We provide connections to maximum cut with cardinality constraints and provide an
optimal approximation algorithm in most parameter regimes under the Unique Games
Conjecture for $k=\Omega(n)$~refinements.
We evaluate our theoretical methods and scalable heuristics on synthetic and
real-world data and show that they are highly effective in practice.
\end{abstract}



\maketitle


\section{Introduction}
\label{sec:intro}

Graphs are commonly used to model entities and their relationships in various application domains. 
Numerous methods have been developed to address diverse graph mining tasks,
including analyzing graph properties, finding patterns, discovering communities, and achieving other application-specific objectives. 
These graph mining tasks are generally formulated as optimization problems. 
Notable examples of such problems include graph-partitioning tasks, 
identifying densest subgraphs or determining the maximum cut of a graph.

In real-world applications, graph mining is often an iterative process that does not start from scratch. 
This process often begins with preliminary and sub\-optimal results, which are
subsequently refined and improved.
Intuitively, let~$U$ be the initial solution to an optimization problem. 
When new information becomes available for the graph~\graph, or a new request
from the data analysts arrives, we seek to modify~$U$ into a new solution~$U'$
that is of higher quality in the updated graph or meets the request; 
in addition, to ensure continuity between the outcomes of the algorithm, 
the solution~$U'$ must be close enough to~$U$.
More concretely, consider the following two examples:

\begin{enumerate}
    \smallskip
    \item 
    Consider the task of detecting the community of highly-influential users
        in an online social network with respect to political commentary in a given country, say, Germany.
        One way to approach this problem is to start with a hand-crafted list of 
        known users who are likely to be part of the community, say, politicians or political journalists.
        Let us call this set of users $U$.
        Some of those users, however, may not be active in the social network. Instead, there might be other users, unknown to us, who are more active and have many followers.
        We could then seek to identify a set of users $U'$, 
        which is close enough to $U$ (so that the community is still on the same topic), 
        and $U'$ has a high graph density 
        (so that it consists of a set of influential users who are tightly connected with each other).
        \smallskip
    \item Next, suppose that a network host aims to maximize the diversity of the
	social network by altering a few users' exposure to diverse news outlets
	(which can be realized, for instance, through recommendation). 
	As formulated by recent work \citep{matakos2020tell}, the exposure of each user can be modeled 
    by a value in the set $\{-1, 1\}$, e.g., representing two sides on a polarized topic, and the
    network diversity can be measured by the size of the cut indicated by the partition
    $(U, \bar{U})$, where $U$ consists of the users whose exposure is $-1$
    and $\bar{U}$ consists of the users whose exposure is $1$. 
    Thus, the network diversity can be increased by changing
    the exposure of a small set of users in order to maximize the cut, 
    where the network host's power to alter the users' exposure 
    can be encoded as a budgetary-difference constraint. 
    \end{enumerate} 
    
\smallskip
The above examples present a
\emph{solution refinement process}. 
Specifically, an initial solution is provided, 
such as an online community in the first example or 
the exposure of all network users to news outlets in the second example.
The objective is to refine the solution
so as to maximize an objective function, 
i.e., either finding a densest subgraph or a maximum cut, 
while maintaining proximity to the initial solution.
The refinement process we consider in this paper appears in many graph-mining tasks.
It naturally generalizes classic optimization algorithms, such as finding the
densest subgraph or the maximum cut of a graph, by incorporating existing initial solutions. 

However, from an algorithmic perspective,  
standard algorithms that solve classical problems do not readily extend to these new tasks. 
This limitation makes existing algorithms non-applicable 
when a preliminary solution is available and a locally-optimal solution is desired. 
By ``locally optimal,'' we mean that the solution should optimize the objective function 
while remaining similar (with a budgetary-difference constraint) to the initial solution.
Hence, we need to develop algorithms that \emph{take as input a feasible solution} and then
\emph{optimize the objective} by making \emph{a small number of refinements}. 

In this paper, we introduce the \emph{OptiRefine framework}, which models this set of optimization problems. 
We apply our framework to \emph{graph-partitioning} problems~\citep{DBLP:journals/jal/feigel01}, in particular, the popular \emph{densest subgraph} and \emph{max-cut} problems, for which we obtain nearly optimal results. 
The \emph{densest-subgraph problem} is typically used to 
identify the tight-knit set of entities, 
with applications to finding trending stories~\citep{angel2012dense}, 
detecting bots and fraud content in social media~\citep{beutel2013copycatch},
finding correlated genes in gene-expression data~\citep{saha2010dense}, and more.
The \emph{max-cut problem} has applications in
graph-partitioning settings~\citep{ding2001min}, 
in discovering conflicting communities in social networks~\citep{bonchi2019discovering},
and in measuring the diversity of exposure to news outlets in online social networks~\citep{matakos2020tell}.
The methods that we introduce in this paper are useful, 
when a community has been detected and we want to find another community that is ``close'' to 
the current community.

Beyond our concrete findings, we also identify a close relationship between (1)~problems in our OptiRefine framework and (2)~the corresponding problems under cardinality constraints on the solution size.
Indeed, for densest subgraph and for max-cut, we obtain approximation algorithms that almost match the best approximation ratios of algorithms that solve the cardinality-constrained versions of the problems. 
We believe that our insights will be helpful in applying the OptiRefine framework to a broad class of problems and in developing practical algorithms with theoretical guarantees for a wide range of applications.  


\subsection{Our results}
\label{intro:results}

\begin{figure}[t]
  \centering 
    \begin{tabular}{cc}
    \resizebox{0.45\columnwidth}{!}{%
      \inputtikz{plots/dense-local-change-approx}
    }&
    \resizebox{0.45\columnwidth}{!}{%
      \inputtikz{plots/cut-local-change-approx}
    }\\
    \hspace{-1.3em}
    (a)~{\small \kdense} &
    (d)~{\small \maxcutkc} \\
  \end{tabular}
  \caption{{\small 
  Approximation ratios of different algorithms for \kdense and \maxcutkc. 
  The approximation ratio for \dskc is the one of \kdense times $\frac{1-c}{1+c}$, 
  where $k \leq c\abs{\OldSet}$.
  For \kdense, we assume that the black-box solver is the algorithm for \dks by~\citet{DBLP:journals/jal/feigel01}.
  For \maxcutkc, we assume that the black-box solver is the algorithm for \maxcut by~\citet{goemans1994approximation}.}
  \label{fig:plots-approx-ratio}}
\end{figure}

First, we apply our OptiRefine framework to the classic densest subgraph and max-cut problems. 
For the densest-subgraph problem, we introduce the problem \emph{densest subgraph with $k$~refinements} (\dskc) in which the input consists of a weighted graph $G = (V, E, w)$, an initial subset of vertices $\OldSet \subseteq V$ and an integer $k \in \mathbb{N}$.  
The goal is to add or remove $k$~vertices from $\OldSet$ to obtain a subset of vertices~$U'$ that maximizes the density of its induced subgraph, denoted by $G[U']$ (see Problem~\ref{prob:densest-subgraph-local} for the formal definition).
For the max-cut problem, we introduce the problem of \emph{max-cut with $k$~refinements} (\maxcutkc) in which we are given a weighted graph $G=(V, E, w)$, an initial subset of vertices $\OldSet\subseteq V$ and an integer $k\in\mathbb{N}$.
The goal is to find a set of vertices $U' \subseteq V$ such that $U$ and $U'$ differ by $k$~vertices and the cut $(U', V\setminus U')$ is maximized (see Problem~\ref{prob:max-cut-local} for the formal definition).
For both \dskc and \maxcutkc, 
we give reductions showing that they are closely connected to their corresponding classic problems, 
densest subgraph and maximum cut, respectively.
In particular, we show that \dskc can be solved using solvers for the classic densest
$k$-subgraph problem~\citep{dblp:conf/coco/feigeseltser} and the \kdense
problem~\citep{matakos2022strengthening}.
Furthermore, \maxcutkc can be solved using a solver for the classic max-cut
problem~\citep{goemans1994approximation} with only constant factor loss in
approximation ratio. 
The approximation ratios using the black-box solvers are shown in Figure~\ref{fig:plots-approx-ratio} with label \emph{Black-box}. 
Moreover, we show that the reduction also works in the other way, and we present the hardness results for \dskc and \maxcutkc. 

Second, we apply our OptiRefine framework to the more general \emph{maximum
graph-partitioning problem} (\maxgp)~\citep{DBLP:journals/jal/feigel01,han2002improved,dblp:journals/rsa/halperinz02} (see Problem~\ref{prob:max-graph-partition} for formal definition), which characterizes
several graph partitioning problems~\citep{han2002improved} and can be solved approximately by the semidefinite program-based (\sdp-based) approaches.
In particular, \maxgp characterizes problems through parameter settings, including \emph{densest-$k$ subgraph} (\dks), \emph{max-cut with cardinality constraints} (\ccmaxcut), \emph{max-uncut with cardinality constraints} (\ccmaxuncut), and \emph{vertex cover with cardinality constraints} (\ccvc). 
We define the general problem under the OptiRefine framework, 
\emph{graph partition with $k$ refinements} (\gpkc) 
(see Problem~\ref{prob:graph-partition-local} for the formal definition),
and transfer the \sdp-based approaches that solve \maxgp to solve \gpkc. 
For these problems, we show constant-factor approximation ratio results in the setting of $k=\Omega(n)$~refinements, where $n$~is the number of vertices in the graph. 
Here, the parameter setting of $k=\Omega(n)$ is necessary for our analysis, as
it is also necessary in all existing works for \maxgp problems. 
In particular, we give a detailed analysis of the approximation ratios on \dskc and \maxcutkc. 
We notice that our theoretical results almost match the \sdp-based approximation ratios for \dks and \ccmaxcut, which our problems generalize. 
Along the way, we improve upon the results of~\citet{matakos2022strengthening} for \kdense, by providing approximation algorithms for larger graph classes (see Section~\ref{sec:related} for details).
The approximation ratios of \sdp-based approaches for \dskc and \maxcutkc are
shown in Figure~\ref{fig:plots-approx-ratio} with label~\sdp.

Third, for \maxcutkc, we provide approximation algorithms that almost match the best-known algorithms for max-cut with cardinality constraints. 
In particular, we use the sum-of-squares hierarchy (SoS)~\citep{DBLP:journals/siamjo/Lasserre02} to show that for $k=\Omega(n)$, \maxcutkc admits the same approximation ratio as the state-of-the-art algorithm for cardinality-constrained max-cut~\citep{DBLP:conf/soda/RaghavendraT12}. 
This approximation ratio is optimal for almost all parameter choices $k=\Omega(n)$, assuming the Unique Games Conjecture~\citep{DBLP:conf/stoc/Khot02a}.
We present the approximation ratio for \maxcutkc using SoS in Figure~\ref{fig:plots-approx-ratio}(b).

From a practical point of view, we present scalable heuristics for \dskc and \maxcutkc. 
We implement our theoretical algorithms and heuristics and evaluate them on synthetic and real-world datasets. 
For \dskc, we find that our algorithms are scalable and improve the densities of the given subgraphs, outperforming previous methods.
For \maxcutkc, we show that our \sdp-based approaches clearly outperform the methods by~\citet{matakos2020tell}.

\subsection{Notation}
\label{preliminary}
Throughout the paper we let $G=(V,E,w)$ be an un\-directed graph 
with non-negative edge weights. We set $n=|V|$ and $m=|E|$.
For a subset of vertices $U\subseteq V$, 
we write $E[U]$ to denote the set of edges with both endpoints in $U$, 
and $G[U]$ to denote the subgraph $(U,E[U])$.
The \emph{density} of a subgraph $G[\OldSet]$ is defined as $d(\OldSet) = \frac{|{E[\OldSet]}|}{|\OldSet|}$, 
where $|E[\OldSet]| = \sum_{(i,j) \in E[\OldSet]} w(i,j)$. 
We write $\OldComSet$ to denote the complement of $U$, 
i.e., $\OldComSet = V\setminus U$. 
Given a partition 
$(U,\OldComSet)$ of $V$, we write $E[U,\OldComSet]$ to denote the set of edges that
have one endpoint in $U$ and one in $\OldComSet$.
We let $\cut{\OldSet} = \sum_{(i,j)\in E[\OldSet,\OldComSet]} w(i,j)$ denote the
number of edges over the cut $(U,\OldComSet)$.
For two sets $A$ and $B$ we denote their symmetric difference by 
$A\symm B = (A \setminus B) \cup (B \setminus A)$. 
The operator $\log$ stands for $\log_2$.


\section{Related work}
\label{sec:related}

The densest-subgraph problem has received considerable attention in the literature. Here, we discuss only the results that are most relevant to our work. 
We refer to the recent surveys by \citet{lanciano2023survey} and \citet{luo2023survey} for more discussion.

The problem of the densest subgraph with $k$~refinements (\dskc), as introduced in this paper, is a generalization of \emph{densest $k$-subgraph} (\dks)~\citep{DBLP:conf/focs/KortsarzP93}.
Despite significant attention in the theory community, there is still a large gap between the best approximation algorithms and hardness results for 
\dks~\citep{%
DBLP:journals/dam/AsahiroHI02,%
DBLP:conf/stoc/Barman15,%
DBLP:conf/stoc/BhaskaraCCFV10,%
DBLP:conf/stoc/Feige02,%
DBLP:journals/algorithmica/FeigePK01,%
DBLP:conf/stoc/LeeG22,%
DBLP:journals/algorithms/Manurangsi18},
where the best polynomial-time algorithm for \dks offers an approximation guarantee of $\bigO(n^{1/4+\varepsilon})$ for any constant $\varepsilon >0$~\citep{DBLP:conf/stoc/BhaskaraCCFV10}.

The unconstrained version of the densest-subgraph problem has been used in practice to find communities in graphs. 
\citet{goldberg1984finding} has shown that this problem can be solved in polynomial
time and~\citet{charikar00approximation} provides a $2$-approximation algorithm.  
Besides the case of static graphs, efficient methods have been designed for  dynamic~\citep{bhattacharya2015space,sawlani2020near} and streaming settings~\citep{mcgregoar2015densest}.

Many variants of the densest-subgraph problem have been studied. For instance, \citet{sozio2010community} consider finding a densely connected subgraph that contains a set of query nodes.
\citet{dai2022anchored, ye2024efficient} consider finding a dense subgraph whose vertices are close to a set of reference nodes and contain a small set of anchored nodes.
\citet{tsourakakis2013denser} observe that, in practice, the densest subgraph is typically large and 
aim to find smaller, denser subgraphs by modifying the objective~function. 
Another line of work aims at finding subgraphs that are dense for multiple graph snap\-shots \citep{charikar2018finding,jethava2015finding,semertzidis2019finding}.

\citet{matakos2022strengthening} introduce the \kdense problem, where the goal is to add
$k$~vertices to a given subgraph to increase its density. 
They obtain $O(\sigma)$-approximation algorithms for graphs that have a $\sigma$-quasi-elimination~ordering. 
This class of graphs includes, for instance, chordal graphs. However, general graphs may not have a small $\sigma$-quasi-elimination~ordering.\footnote{Their algorithm requires a predefined $\sigma$.}
In contrast, our algorithms from Section~\ref{sec:densest-subgraph} provide constant-factor approximations for general graphs, but we need the assumption~$k=\Omega(n)$. 
As our hardness results show, this assumption is necessary to obtain $O(1)$-approximation algorithms assuming the 
Small Set Expansion Conjecture~\citep{dblp:conf/stoc/raghavendras10}.

The problem of max-cut with $k$~refinements (\maxcutkc) is a generalization of \emph{max-cut with cardinality constraints} ({\ccmaxcut})~\citep{DBLP:journals/algorithmica/FriezeJ97,DBLP:journals/jcss/PapadimitriouY91}, in which the size of the cut is restricted to be equal to $k$, for a given $k \in [n]$.
The most studied version of {\ccmaxcut} is known as \maxbis, in which $k=n/2$
\citep{%
DBLP:journals/talg/AustrinBG16,%
DBLP:journals/jal/feigel01,%
DBLP:journals/algorithmica/FriezeJ97,%
DBLP:conf/soda/Manurangsi19,%
DBLP:conf/soda/RaghavendraT12%
}. 
The best polynomial-time approximation algorithm for {\ccmaxcut} achieves an approximation ratio of approximately $0.858$~\citep{DBLP:conf/soda/RaghavendraT12}.
This ratio is improved to approximately $0.8776$ for \maxbis 
\citep{DBLP:journals/talg/AustrinBG16}.
With respect to hardness of approximation for {\ccmaxcut}, the best result is due to~\citet{DBLP:conf/approx/AustrinS19} who give hardness of approximation with $k=\tau n$  as a function of~$\tau$, assuming the Unique Games Conjecture~\citep{DBLP:conf/stoc/Khot02a}.
A matching approximation algorithm for $\tau \in (0,0.365) \cup (0.635,1)$ is given by \citet{DBLP:conf/soda/RaghavendraT12}.
Our approximation algorithm for \maxcutkc obtains the same approximation ratio. As we show that \maxcutkc generalizes \ccmaxcut, in the parameter setting above, our algorithms are also optimal under the Unique Games Conjecture~\citep{DBLP:conf/stoc/Khot02a}.

The problem of graph partitioning with $k$ refinements (\gpkc) is a
generalization of \emph{maximum graph partitioning}
(\maxgp)~\citep{DBLP:journals/jal/feigel01, han2002improved,
	dblp:journals/rsa/halperinz02}, which generalizes several graph partitioning
	problems.
Besides the two problems \dks, \ccmaxcut that we generalize in this paper, two
other problems, namely, \emph{max-uncut with cardinality constraints} (\ccmaxuncut),
	  and \emph{vertex cover with cardinality constraints} (\ccvc) are also presented
	  as applications of \maxgp. 
All of the problems can be solved by \sdp-based approaches, and constant
approximation results are obtained assuming $k \in \Omega(n)$, where the
concrete approximation ratios depend on the value of~$k$. 

Finally, we remark that independently and concurrently with our project, \citet{fellows2023solution} and \citet{ grobler2023solution} propose reconfiguration frameworks that have similarities with our OptiRefine framework.
In addition, \citet{dalirrooyfard2024graph} study the $r$-move-$k$-partition problem, where the goal is to \emph{minimize} the multi\-way cut among $k$ partitions by moving $r$ nodes' positions. 
We note that this line of research focuses on developing fixed-parameter
algorithms for different sets of problems. Hence, their results apply in the
setting of small values of~$k$, whereas we concentrate on large values of~$k$.
Therefore, the concrete results of these papers are incomparable with ours, but
highlight the importance of studying refinement problems.

\vspace{1mm}
\para{Structure of the paper.} 
This paper is structured as follows.
In Section~\ref{sec:densest-subgraph}, we define \dskc, and we illustrate its
connections to \kdense and \dks. Moreover, we present a black-box
solution for \dskc by applying a solver for \dks. 
Section~\ref{sec:max-cut} presents \maxcutkc, and we illustrate its connections
to \ccmaxcut. Moreover, we consider a black-box solution for \maxcutkc by applying a solver for \maxcut. 
In Section~\ref{sec:general-framework}, we use a general problem, graph
partitioning with $k$ refinements (\gpkc), that captures both \dskc and
\maxcutkc. We then propose an \sdp-based algorithm and
obtain constant approximation results assuming $k \in \Omega(n)$. 
In Section~\ref{sec:max-cut:sos}, we describe a sum-of-squares algorithm to optimally solve \maxcutkc under certain regimes of $k \in \Omega(n)$.
Section~\ref{sec:experiments} presents our experiments, in which we evaluate our
algorithms for \dskc using multiple datasets. Moreover, in the appendix, we present more
experimental evaluations for \dskc and \maxcutkc, as well as all
omitted proofs from the main text.

\section{Densest subgraph with $k$~refinements}
\label{sec:densest-subgraph}

In this section, we formally define the problem of finding the densest subgraph with
$k$~refinements (\dskc) and other relevant problems. 
We then study the relationship of these problems.

\begin{problem}[Densest subgraph with $k$~refinements (\dskc)]
\label{prob:densest-subgraph-local}
Given an undirected graph $G = (V, E, w)$, a subset $\OldSet \subseteq V$, and an integer $k \in \mathbb{N}$, \dskc seeks to find a subset of vertices $\SelectSet \subseteq V$ 
with $|\SelectSet| = k$ such that the density 
$d(\OldSet \symm \SelectSet)$ is maximized.
\end{problem}

In the definition of Problem~\ref{prob:densest-subgraph-local}, 
the operation $\symm$ denotes the symmetric difference of sets and 
$d(\OldSet\symm \SelectSet)$ is the density of the subgraph 
$G[\OldSet\symm \SelectSet]$, which is obtained by 
removing the vertices in $\OldSet \cap \SelectSet$ and adding the 
vertices in $\SelectSet \setminus \OldSet$. 
It is important to note that when $G[\OldSet]$ is already the 
densest subgraph, $G[\OldSet\symm \SelectSet]$ may have a lower 
density than $G[\OldSet]$ due to the constraint $|\SelectSet| = k$.\footnote{
	We note that it may also be interesting to study the problem with the
	inequality constraint $\abs{\SelectSet} \leq k$. However, from a theoretical
	point of view, the results of~\citet{khuller2009finding} and
	the proof of Lemma~\ref{lem:densest-subgraph-generalization} imply that (up
	to constant factors) the inequality-constraint version of the problem is at least as
	hard to approximate as the equality-constraint version. Therefore, we study
	the equality-constraint version, because it simplifies the analysis.
	From a more practical point of view, we can solve the
	inequality-constraint version by running the equality-constraint version for
	multiple values of~$k$.
}

Below, in Theorem~\ref{thm:densest-subgraph-sdp} we show that we can obtain a
constant factor approximation algorithm for \dskc. In this section, we focus on
its computational complexity and its relationship with other classic problems.
First, we introduce the \kdense problem, %
which serves as an intermediate step in our analysis.
\begin{problem}[\kdense~\citep{matakos2022strengthening}]
\label{prob:k-densify}
	Given an undirected graph $G=(V, E, w)$, a set $U \subseteq V$ and an integer $k \in \mathbb{N}$, \kdense seeks to find a set of vertices $C \subseteq V$ with $\abs{C} = k$ such that $|E[\OldSet \symm \SelectSet]|$ is maximized.\footnote{We note that in the
		definition given by \citet{matakos2022strengthening}, the objective is
		$|E[\OldSet \cup \SelectSet]|$. However, they implicitly require
		that $k\leq n - \abs{U}$; observe that in this case there are always $k$ nodes
		selected from $V \setminus U$, and the maximum of $\abs{E[\OldSet \cup
		\SelectSet]}$ is equal to $\abs{E[\OldSet \symm \SelectSet]}$.
		Hence, we adopt the objective function $|E[\OldSet \symm
			\SelectSet]|$ to make it consistent with \dskc.}
\end{problem}
Note that the \kdense problem has the same cardinality constraint as \dskc, i.e., $|C|=k$. 
However, the objective function of \kdense maximizes $|E[\OldSet \symm \SelectSet]|$, 
unlike \dskc, which maximizes the density $d(\OldSet\symm \SelectSet)$. That is,
while in \dskc we can add \emph{and remove} vertices from the given subgraph, in
\kdense we can only add vertices to the subgraph.


Finally, we introduce densest $k$-subgraph (\dks).
\begin{problem}[Densest $k$-subgraph (\dks)]
\label{prob:densest-k-subgraph}
Given an undirected graph $G=(V,E,w)$ and an integer
$k\in\mathbb{N}$, \dks seeks to find a set of vertices $\OldSet\subseteq V$
with $|\OldSet|=k$ such that the density $d(\OldSet)$ is maximized.
\end{problem}


\subsection{Relationships of the problems}
\label{sec:relationships-densest}

We show the connection of the three problems above.
In the subsequent analysis, the approximation ratios that we derive will
typically depend on $k$.  To emphasize the dependence on $k$, we will denote the
approximation ratios using the notation $\alpha_k$. 


\spara{Relationship of \dskc and \dks.}
First, we show that \dskc \emph{generalizes}
the classic \emph{densest $k$-subgraph} (\dks) problem.  

\begin{lemma}
\label{lem:densest-subgraph-generalization}
	Let $k\in\mathbb{N}$.
	If there exists an $\alpha_k$-approximation algorithm for 
	\dskc with running time $T(n,m,k)$, then there exists an
	$\alpha_k$-approximation algorithm for \dks running in
	time $\bigO(T(n,m,k))$.
\end{lemma} 

Lemma~\ref{lem:densest-subgraph-generalization} allows us to obtain
hardness-of-approximation results for \dskc based on the hardness of
\dks~\citep{DBLP:conf/stoc/LeeG22}.

\begin{corollary}[Hardness of approximation for \dskc]
\label{cor:densest-subgraph-hardness}
	Any hardness of factor $\beta_k$ for approximating \dks implies
	hardness of approximating \dskc within a factor $\beta_k$. 
	In particular, if $k=\tau n$ for $\tau\in (0,1)$, 
	then \dskc is hard to approximate within 
	$\SelectSet \tau \log(1/\tau)$, 
	where $\SelectSet \in \mathbb{R}_+$ is some fixed constant,
	assuming the Small Set Expansion Conjecture~\citep{dblp:conf/stoc/raghavendras10}. 
\end{corollary}

Corollary~\ref{cor:densest-subgraph-hardness} implies that for $k=o(n)$ 
(and, therefore, $\tau=o(1)$), it is not possible to obtain constant-factor approximation
algorithms for \dskc. Additionally, for $k=\Omega(n)$, the problem is hard to
approximate within a constant~factor.

\spara{Relationship of \kdense and \dskc.}
We show that \dskc can be effectively solved by applying an algorithm for
\kdense on the same input instance, with little impact on the approximation
ratio.
Specifically, Lemma~\ref{lemma:k-densify-local-changes-connection} shows that running an
$\alpha_k$-approximation algorithm for \kdense on an instance of \dskc only
loses a factor of factor~$\frac{1-c}{1+c}$ in approximation, where $c \in (0,1)$
is a constant such that~$k \leq c\abs{U}$. We note that in practice, it is
realistic that $c$ is small since we only want to make a small fraction of
changes to~$U$.  This implies that $\frac{1-c}{1+c}$ is large; for
instance, $\frac{1-c}{1+c} \geq 0.9$ if $c=5\%$.
Notice that the condition $k \leq n - \abs{U}$ is an implicit requirement from the definition of the \kdense problem. 

\begin{lemma}
    \label{lemma:k-densify-local-changes-connection}
	Let $(G=(V, E, w), U, k)$ be an instance for \dskc.
    Let us assume that $k \leq c \abs{U}$ for some $c \in (0, 1)$, and $k \leq n - \abs{U}$. 
	Applying an $\alpha_k$-approximation algorithm for \kdense on $G$ 
    provides a $\left(\frac{1-c}{1+c}\, \alpha_k\right)$-approximate solution for \dskc. 
\end{lemma}

\spara{Relationship of \kdense and \dks.}
We show that an algorithm for the Densest $(k+1)$-Subgraph problem (\dksplus)
can be effectively utilized to solve \kdense, with only small losses in the approximation ratio.

\begin{lemma}
\label{lemma:k-densify-dks}
	Suppose there exists an $\alpha_{k+1}$-approximation algorithm for \dksplus,
	then there exists a $\frac{k-1}{k+1}\alpha_{k+1}$-approximation algorithm
	for \kdense.
\end{lemma}

\begin{proof}[Proof sketch]
On a high level, the reduction from \kdense to \dksplus works as follows (see
Appendix~\ref{appendix:ds-proofs:kdensifydks} for details). Given an instance
$(G=(V,E,w),\OldSet,k)$ for \kdense, we build a graph~$G'$ in which we
\emph{contract} all vertices in $\OldSet$ into a special vertex~$i^*$. Then we
solve \dksplus on the graph~$G'$.  We then consider two cases:
either~$i^*$ is included in the (approximate) solution~$U'$ of \dksplus, or not.  If
$i^*$ is part of the solution, we have added exactly $k$~vertices
to~$\OldSet$. Otherwise, we add all vertices from $U'$ to $U$, except the vertex
from $U'$ of lowest degree in $G[U\cup U']$; this implies that we also added
$k$~vertices to~$U$. Then we can show
that the $k$~additional vertices chosen by the \dksplus-algorithm form a
solution for \kdense subgraph with the desired approximation guarantee.
\end{proof}

\subsection{Black-box reduction}
\label{sec:densest-subgraph:black-box}

By combining the results from the previous section, we can now show that any approximation
algorithm for \dks implies (under some assumptions on $|\OldSet|$ and~$k$) an
approximation algorithm for \dskc.  This result can be viewed as the reverse
direction of the reduction in Lemma~\ref{lem:densest-subgraph-generalization},
and is obtained by applying
Lemmas~\ref{lemma:k-densify-local-changes-connection}
and~\ref{lemma:k-densify-dks}.  Our formal result is as follows.
\begin{theorem}
\label{thm:densest-subgraph-reduction}
Suppose there exists an $\alpha_k$-approximation algorithm for the \dks problem with running time $T(m,n,k)$.
If $k \leq c\abs{\OldSet}$, for some $c \in (0, 1)$, 
there exists a $\left(\frac{1-c}{(1+c)}\, \frac{k-1}{k+1} \, \alpha_{k+1}\right)$-approximation algorithm for 
the \dskc problem with running time $\bigO(T(m,n,k+1) + m)$.
\end{theorem}


By utilizing the approximation algorithm for \dks by \citet{DBLP:journals/jal/feigel01} in
Theorem~\ref{thm:densest-subgraph-reduction}, we obtain
Corollary~\ref{cor:densest-subgraph-reduction}.  We
include the assumption $k = \Omega(n)$ in the corollary, as it is a requirement
of~\citet{DBLP:journals/jal/feigel01}.

\begin{corollary}
\label{cor:densest-subgraph-reduction}
	If $\abs{\OldSet}=\Omega(n)$, $k=\Omega(n)$, and 
	$k \leq c\abs{\OldSet}$ for a constant~$c \in (0, 1)$,
    there exists an $\bigO(1)$-approximation algorithm
	for~\dskc.
\end{corollary}

In our experiments, we will implement the algorithm from
Corollary~\ref{cor:densest-subgraph-reduction} and denote it
as \emph{black-box algorithm}, since the algorithm is using a black-box solver
for \dks.

\section{Max-cut with $k$ refinements}
\label{sec:max-cut}

In this section, we study max-cut with $k$ refinements (\maxcutkc). 
We start by providing formal definitions for \maxcutkc and for cardinality-constrained max-cut (\ccmaxcut). 
We then present a hardness result for \maxcutkc through \ccmaxcut.  
In the reverse direction, we show that any approximation algorithm for unconstrained \maxcut 
provides an approximation algorithm for \ccmaxcut, in a black-box reduction. 
Later, in Sections~\ref{sec:general-framework} and~\ref{sec:max-cut:sos}, we will
also provide constant-factor approximation algorithms for \maxcutkc.

We start by formally defining \maxcutkc. 

\begin{problem}[Max-cut with $k$ refinements (\maxcutkc)]
\label{prob:max-cut-local}
	Given an undirected graph $G = (V, E, w)$, a subset $\OldSet \subseteq V$, and an integer $k \in \mathbb{N}$, 
	\maxcutkc seeks to find a set of
	vertices $\SelectSet\subseteq V$ such that $|\SelectSet|=k$ and 
	$\cut{\OldSet \symm \SelectSet}$ is maximized.
\end{problem}

As with \dskc problem, 
observe that if $\cut{\OldSet}$ is the maximum cut,
$\cut{\OldSet\symm \SelectSet}$ may have a lower value than $\cut{\OldSet}$ 
due to the constraint $|\SelectSet| = k$.  

We also introduce the problem of \emph{maximum cut with cardinality constraints} ({\ccmaxcut}), 
which we use for the analysis of the hardness of \maxcutkc.
\begin{problem}[Max-cut with cardinality constraints (\ccmaxcut)]
Given an undirected graph $G = (V, E, w)$, and an integer $k \in \mathbb{N}$, 
	\ccmaxcut seeks to find a set of vertices $\OldSet\subseteq V$ of size $|\OldSet|=k$ 
	such that $\cut{\OldSet}$ is maximized. 
\end{problem}

\spara{Relationship of \maxcutkc and \ccmaxcut.}
First, we show that \maxcutkc is a generalization of \ccmaxcut in Lemma~\ref{lem:max-cut-generalization}.

\begin{lemma}
\label{lem:max-cut-generalization}
	Let $k\in\mathbb{N}$.
	If there exists an $\alpha_k$-approximation algorithm for
	\maxcutkc with running time $T(n,m,k)$, then there exists an
	$\alpha_k$-approximation algorithm for 
	\ccmaxcut with running time~$\bigO(T(n,m,k))$.
\end{lemma}

Combining Lemma~\ref{lem:max-cut-generalization} and the results of~\citet{DBLP:conf/approx/AustrinS19}, we obtain the following hardness result for \maxcutkc.

\begin{corollary}
\label{cor:max-cut-hardness}
	Any hardness of factor $\beta_k$ for approximating {\ccmaxcut} 
    implies hardness of approximating {\maxcutkc} within factor $\beta_k$. 
	In particular, suppose that $k=\tau n$ for $\tau\in (0,1)$. 
	For $\tau\leq 0.365$ or $\tau\geq 0.635$, we have that
	$\beta_k\approx0.8582$ and for $\tau\in(0.365,0.635)$ we have
   	$\beta_k\in[0.8582, 0.8786]$.
\end{corollary}


\subsection{Black-box reduction}
\label{sec:max-cut:black-box}

Finally, we show that algorithms for the
(unconstrained) {\maxcut} problem can be used to solve {\maxcutkc}.
\begin{theorem}
\label{thm:max-cut-black-box}
	Consider an $\alpha$-approximation algorithm for \maxcut with running time
	$T(m,n)$. Then there exists an algorithm for 
	\maxcutkc with approximation ratio 
    $\alpha \left(1 -\Theta\!\left(\frac{1}{n}\right)\right)
    \min\left\{\frac{k^2}{n^2}, \left(1-\frac{k}{n}\right)^2\right\}$ and
	running time $\bigO(T(m,n) + m)$.
\end{theorem}

In our reduction, we run an algorithm 
(e.g., the {\sdp} relaxation by~\citet{goemans1994approximation}) for {\maxcut} without
cardinality constraint, i.e., the cut may contain an arbitrary number of
vertices on each side. Suppose this algorithm returns a cut $(\TempSet,\TempComSet)$.
Then we make refinements to $\TempSet$ to bring it ``closer'' to $\OldSet$,
i.e., we greedily move vertices until $\TempSet=\OldSet\symm \SelectSet$ 
for some set $\SelectSet$ with $|\SelectSet|=k$.  
Using the assumptions from the theorem, we obtain an $\bigO(1)$-approximation
algorithm assuming that $k=\Omega(n)$.
The pseudo\-code for this method is presented in the appendix. 


\section{Graph partitioning with $k$ refinements}
\label{sec:general-framework}

The \emph{maximum graph-partitioning problem} (\maxgp) characterizes \dks and \ccmaxcut, as well as \emph{max-uncut with cardinality constraints} (\ccmaxuncut), and \emph{vertex cover with cardinality constraints} (\ccvc) with specific parameter settings. 
Previous papers~\citep{DBLP:journals/jal/feigel01,han2002improved,dblp:journals/rsa/halperinz02}
provided \sdp-based approximation algorithms for \maxgp.
Let us define \maxgp the same way as \citep{han2002improved}, by introducing parameters $\parzero, \parone, \partwo, \parthree$ into the \maxgp objective function, specified in Table~\ref{tab:case_distinctions}.
Our analysis in the remainder of this section holds for these specific settings. 

\begin{table}[t]
\centering
\caption{
Partition measures and corresponding values of 
$\parzero, \parone, \partwo, \parthree$ for the problems we study in this paper.
}
\begin{tabular}{lllrrrr}
\toprule
{Problem} \gpkc & {Problem} \maxgp & {Measure} $\partition$ & \parzero & \parone & \partwo & \parthree\\  \midrule \smallskip
\kdense & \dks & $\abs{E(\cdot)}$ & $\frac{1}{4}$ & $\frac{1}{4}$ & $\frac{1}{4}$ & $\frac{1}{4}$ \\ \smallskip %
\maxcutkc & \ccmaxcut &$\cut(\cdot)$ & $\frac{1}{2}$ & $0$ & $0$ & $-\frac{1}{2}$\\  \smallskip %
\maxuncutkc & \ccmaxuncut &$\abs{E} - \cut(\cdot)$ & $\frac{1}{2}$ & $0$ & $0$ & $\frac{1}{2}$\\  %
\vckc & \ccvc &$\abs{E(\cdot)} + \cut(\cdot)$ & $\frac{3}{4}$ & $\frac{1}{4}$ & $\frac{1}{4}$ & $-\frac{1}{4}$\\
\bottomrule
\end{tabular}
\label{tab:case_distinctions}
\end{table}

\begin{problem}[Maximum graph-partitioning (\maxgp)]
	\label{prob:max-graph-partition}
Given an undirected graph $G = (V, E, w)$ where $\abs{V}=n$ and an integer $k \in \mathbb{N}$, 
the predefined parameters $\parzero$, $\parone$, $\partwo$, $\parthree$ and the partition measure $\partition{\cdot}$,
\maxgp seeks to find a set of vertices $\OldSet \subseteq V$ where $|\OldSet| = k$, such that the partition measure $\partition{\OldSet}$ is maximized.

In particular, 
    let $\vecx\in\{-1,1\}^n$ be an indicator vector such that $x_i=1$ if and only if $i\in \OldSet$. 
    \maxgp is formulated as follows:
     \begin{equation}
    	\label{ip:max-graph-partition}
    \begin{aligned}
    \max_{\vecx} \quad 	& 
    \sum_{i<j} w(i,j) \, (\parzero + \parone x_i + \partwo x_j + \parthree x_i x_j),\\
    \quad \text{such that }	& \sum_i x_i = -n + 2k  \text{ and }\\ 
	& \vecx \in \{-1,1\}^n.
    \end{aligned}
    \end{equation}
The predefined parameters and partition measures are presented in Table~\ref{tab:case_distinctions}. 
\end{problem}

In this section, we generalize \maxgp to take into account refinements of
existing solutions. We call our generalized problem \emph{graph partitioning with $k$ refinements} (\gpkc) and define it formally below.
The \kdense and \maxcutkc from above are instances of \gpkc by specifying the partition measure $\partition{\cdot}$.

\begin{problem}[Graph partitioning with $k$ refinements (\gpkc)]
	\label{prob:graph-partition-local}
Given an undirected graph $G = (V, E, w)$ where $\abs{V} = n$, a subset $\OldSet \subseteq V$, and an integer $k \in \mathbb{N}$,
the predefined parameters $\parzero$, $\parone$, $\partwo$, $\parthree$ and the partition measure $\partition{\cdot}$,
\gpkc seeks to find a subset of vertices $\SelectSet \subseteq V$ with $\abs{\SelectSet} = k$, such that the partition measure $\partition{\OldSet \symm \SelectSet}$ is maximized.

In particular, let $\vecx^0 \in \{-1, 1\}^n$ be an indicator vector such that $x^0_i = 1$ if and only if $i \in \OldSet$ and 
    let $\vecx\in\{-1,1\}^n$ be an indicator vector such that $x_i=1$ if and only if $i\in \OldSet\symm \SelectSet$. 
    \gpkc is formulated as follows:
     \begin{equation}
    	\label{ip:densest-subgraph}
    \begin{aligned}
    \max_{\vecx} \quad 	& 
    \sum_{i<j} w(i,j) \, (\parzero + \parone x_i + \partwo x_j + \parthree x_i x_j),\\
    \quad \text{such that }	& 
    \sum_i x^0_i x_i = n-2k \text{ and } \\ & 
    \vecx \in \{-1,1\}^n.
    \end{aligned}
    \end{equation}
The predefined parameters and partition measures are presented in Table~\ref{tab:case_distinctions}. 
\end{problem}

Notice that when $\OldSet=\emptyset$ (and $\vecx^0=-\vecone$), the formulation of \gpkc becomes the same as  \maxgp. 
Taking into consideration the initial solution ($\OldSet \neq \emptyset$) changes the formulation of the constraint part. 
Hence, we mainly illustrate the constraint.  
We use the indicator vector $\vecx^0\in\{-1,1\}^{V}$ to encode the set $\OldSet$, i.e., we set $x^0_i = 1$ if $i\in \OldSet$ and $x^0_i = -1$ if $i\in V\setminus \OldSet$.  
We then set our solution~$\SelectSet$ to all $i$ such that
$x_i \neq x^0_i$, i.e., $\SelectSet=\{i\colon x_i\neq x^0_i\}$.
Observe that the constraint of the integer program implies that $\SelectSet$ contains exactly
$k$ vertices: we have $x^0_i x_i = 1$ if and only if $x^0_i = x_i$ and $x^0_i
x_i = -1$ if and only if $x^0_i \neq x_i$. 
Hence, the sum of all $x^0_i x_i$ is equal to $n-2\ell$, 
where $\ell=|\{ i \colon x_i \neq x^0_i\}|$ is the
number of entries in which $\vecx$ and $\vecx^0$ differ. Since in the constraint we
set $\sum_i x^0_ix_i=n-2k$, we get $|\SelectSet|=k$.

The objective functions of \gpkc are essentially the same as \maxgp, as well as the parameter settings.
Next, we illustrate the settings of \kdense and \maxcutkc. For \maxuncutkc and \vckc, we refer readers to \citet{han2002improved} for more details.
For \kdense, the term $(\frac{1}{4}+\frac{1}{4}x_i+\frac{1}{4}x_j+\frac{1}{4}x_ix_j)$ equals $1$ if $x_i=x_j=1$ and equals $0$ otherwise. 
Hence, the objective aims to maximize the
number of edges in $G[\OldSet\symm \SelectSet]$, i.e., we maximize $\abs{E(\OldSet \symm \SelectSet)}$.
For \maxcutkc, the terms $(\frac{1}{2}-\frac{1}{2}x_ix_j) = 1$ if and only if $x_i \neq x_j$, 
i.e., when $i$ and $j$ are on different sides of the cut, otherwise $\frac{1}{2}-\frac{1}{2}x_ix_j = 0$. 
Thus, the objective function sums over all edges with
one endpoint in $\OldSet\symm \SelectSet$ 
and one endpoint in $\overline{\OldSet}\symm \SelectSet$, which is identical to $\cut{\OldSet \symm \SelectSet}$. 


\subsection{SDP-based algorithm}
\label{sec:general:sdp}

Next, we present our \sdp-based algorithm. 
We will show that the \sdp-based algorithm, as applied by
\citet{DBLP:journals/algorithmica/FriezeJ97} to solve \ccmaxcut and by \citet{DBLP:journals/jal/feigel01} to solve \dks, can be adapted to solve our problem \gpkc.


\spara{The semidefinite program.}
We state the {\sdp} relaxation of the integer
program~\eqref{ip:densest-subgraph} as follows, where $\vecv_i \cdot \vecv_j$
denotes the inner product of $\vecv_i$ and $\vecv_j$:
\begin{equation} \label{sdp:densest-subgraph}
\begin{aligned}
	\max_{\vecv_0, \ldots, \vecv_n\in\mathbb{R}^n}~  & 
         \sum_{i < j} w(i,j) \, (\parzero + \parone \vecv_0 \cdot \vecv_i + \partwo \vecv_0 \cdot \vecv_j + \parthree \vecv_i \cdot \vecv_j),\\
    \text{such that }~ 	&
         \sum_i x_i^0 \vecv_i \cdot \vecv_0 = n - 2k, \\ & 
         \sum_{i,j} x_i^0 x_j^0 \vecv_i \cdot \vecv_j = (2k-n)^2, 
             					\text{ and } \\ & 
             					\vecv_i \in \mathcal{S}_{n}, \text{ for } i = 0, \ldots, n.
\end{aligned}
\end{equation}

In the \sdp, we replace each entry $x_i$ with the inner product $\vecv_i\cdot\vecv_0$ and each $x_ix_j$ with the inner product $\vecv_i \cdot \vecv_j$. 
All $\vecv_i$ are vectors from the unit sphere $\mathcal{S}_n$.
For technical reasons, we add the constraint $\smash{\sum_{i,j} x_i^0 x_j^0 \vecv_i \cdot \vecv_j} = (2k-n)^2$, which is a relaxation of the constraint $\smash{(\sum_{i,j} x_i^0 x_{i})^2} = (2k-n)^2$.
This constraint will help us to bound the variance of the random-rounding procedure.

\spara{The algorithm.}
Our algorithm for \gpkc consists of the following three steps:
\begin{enumerate}
\item We solve the {\sdp}~\eqref{sdp:densest-subgraph} to obtain a solution~$\vecv_0, \ldots, \vecv_n$. 
\item We obtain an indicator vector~$\overline{\vecx}\in\{-1,1\}^n$ (which we will later use to obtain~$\SelectSet$) as follows. 
We sample a unit vector $\vecr$ from $\mathcal{S}_n$ uniformly at random and obtain the indicator vector~$\overline{\vecx} \in\{-1,1\}^n$ by setting $\overline{x}_i=1$ if
$(\vecr \cdot \vecv_0)(\vecr \cdot \vecv_i) \geq 0$ and $\overline{x}_i=-1$ otherwise.
We call this step \emph{hyperplane rounding}.
\item As before, we set $\SelectSet=\{i \colon \overline{x}_i \neq x^0_i\}$ and define a temporary set $\TempSet = \{i \colon \overline{x}_i = 1\}$. 
Note that it is possible that $\abs{\SelectSet} \neq k$. 
In this case, we greedily modify $C$ to ensure that $\TempSet$ is a valid solution of \gpkc. We call this last step \emph{fixing $C$}.
\end{enumerate}

To boost the probability of success, 
the second and third steps of the algorithm are repeated
$\bigO(1/\varepsilon \lg(1/\varepsilon))$~times
and we return the solution with the largest objective function value.
The pseudocode of our concrete algorithms for \kdense and \maxcutkc are
presented in Appendices~\ref{appendix:densest-subgraph:sdp:pseudocode}
and~\ref{appendix:maxcutkc:sdp:pseudocode}, resp.

\spara{Our main results}. Before analyzing the \sdp-based algorithm, let us state our main results obtained through the \sdp-based algorithm for \dskc and \maxcutkc. 
For these problems, we provide a complete analysis of the constant factor approximation guarantees for $k=\Omega(n)$.
The specific value of~$k$ determines the constant factor approximation ratios, and we present them in Figure~\ref{fig:plots-approx-ratio}.
Note that, due to our hardness results from Corollary~\ref{cor:densest-subgraph-hardness}, this assumption is inevitable for \dskc if we wish to obtain $\bigO(1)$-approximation algorithms (assuming the Small Set Expansion Hypothesis~\citep{dblp:conf/stoc/raghavendras10}).
Moreover, the assumption $k=\Omega(n)$ is necessary for the analysis of the approximation factor, similar to previous work~\citep{DBLP:journals/jal/feigel01,DBLP:journals/algorithmica/FriezeJ97,han2002improved}.

\begin{theorem}
\label{thm:densest-subgraph-sdp}
Let $|\OldSet|=\Omega(n)$ and $k=\Omega(n)$ where $k \leq \min\{c|\OldSet|, n -
|\OldSet|\}$ for some constant $c\in(0,1)$. There exists an {\sdp}-based
randomized algorithm that runs in time $\bigOtilde(n^{3.5})$ and outputs a
$\bigO(1)$-approximate solution for \dskc with high probability.
\end{theorem}


\begin{theorem}
\label{thm:max-cut-sdp}
	If $|\OldSet|=\Omega(n)$ and $k=\Omega(n)$,
	there exists an {\sdp}-based randomized algorithm that runs in time $\bigOtilde(n^{3.5})$ and returns a
	$\bigO(1)$-approximate solution for \maxcutkc with high probability.
\end{theorem}

We note that the running time is dominated by solving the semidefinite program with $\Theta(n)$ variables and $\Theta(n)$ constraints, and the time complexity is $\bigOtilde(n^{3.5})$~\citep{jiang2020faster}.


\subsection{Useful tools for the analysis}
Let us first introduce additional notation and lemmas
that we will repeatedly use in this section.  We define the constants
\begin{equation}
	\label{eq:const-alpha}
	\alpha = \min_{0 \leq \theta \leq \pi} \frac{2}{\pi} \frac{\theta}{1 - \cos \theta} > 0.87856,
\end{equation}
and
\begin{equation}
	\label{eq:const-beta}
	\beta = \min_{0 \leq \theta < \arccos(-1/3)} \frac{2}{\pi} \frac{2\pi - 3 \theta}{1 + 3 \cos \theta} > 0.79607.
\end{equation}
We also introduce the binary function $\sign \colon \Real \rightarrow \{-1, 1\}$ such that $\sign(x) = 1$ if $x \geq 0$, 
and $\sign(x) = -1$ if $x <0$. Additionally, we will use the following lemmas.

\begin{lemma}[Lemma 2.1 in~\citet{goemans1994approximation}]
\label{lem:alpha}	
It holds for any $y \in [-1, 1]$ that $\arccos(y) \geq \alpha \pi \frac{1}{2}(1-y)$, where $\alpha$ is defined in Equation~\eqref{eq:const-alpha}. 
\end{lemma}


\begin{lemma}[Lemma 1.2 in~\citet{goemans1994approximation}]
	\label{lem:random-projection-1}
	Let $\vecv_i$ and $\vecv_j$ be vectors on the unit sphere $\mathcal{S}_n$,
	let $\vecr$ be a unit vector that is drawn uniformly randomly from $\mathcal{S}_n$,	then $\pr(\sign(\vecv_i \cdot \vecr) \neq \sign(\vecv_j \cdot \vecr)) = \frac{1}{\pi} \arccos(\vecv_i \cdot \vecv_j)$. 
\end{lemma}

\begin{lemma}[Lemma 2.3 in~\citet{goemans1994approximation}]
\label{lem:beta}
Let $\vecv_i$, $\vecv_j$ and $\vecv_k$ be three vectors on the unit sphere $\mathcal{S}_n$, 
let $y_1 = \vecv_i \cdot \vecv_j$, $y_2 = \vecv_j \cdot \vecv_k$ and $y_3 = \vecv_i \cdot \vecv_k$. 
It follows that $1 - \frac{1}{2\pi} (\arccos(y_1) + \arccos(y_2) + \arccos(y_3)) \geq \frac{\beta}{4} (1 + y_1 + y_2 + y_3)$, where $\beta$ is defined in Equation~\eqref{eq:const-beta}. 
\end{lemma}

\subsection{Analysis of the hyperplane rounding}
\label{sec:hyperplane}
In this subsection, we analyze the second step of our algorithm above. In
particular, we derive bounds that hold for the \gpkc after hyperplane rounding. 
Our bounds are presented in Lemma~\ref{lem:cut-bound-z}.
Similar to
\citet{DBLP:journals/algorithmica/FriezeJ97,DBLP:journals/jal/feigel01,han2002improved},
we introduce the random variable~$Z$ that controls the number of nodes that change the position and the objective value. 
Then we derive a lower bound on $Z$ with high probability using Markov's inequality. 
We present the proof at the end of this section. 

In our proof, we use the following notation.
We let $\overline{\vecx} \in \{-1, 1\}^{n}$ be the indicator vector
obtained from the hyperplane rounding;
notice that $\overline{\vecx}$ does not necessarily obey the cardinality constraint. 
We set $\overline{\SdpRnd}$ to the objective value of \gpkc induced by $\overline{\vecx}$.
Let $\OPT$ be the optimal value of \gpkc.
We let $\SdpRatio = \frac{\overline{\SdpRnd}}{\OPT}$ and $\SdpRatioAvg = \frac{\Exp{\overline{\SdpRnd}}}{\OPT}$.
Notice that $\SdpRatio$ is a random variable and $\Exp{\SdpRatio} = \SdpRatioAvg$.
We let $\tau$ be such that $k = \tau n$ and notice that $\tau=\Omega(1)$ since
$k=\Omega(n)$.
Moreover, we let $\SelectSet=\{i \colon \overline{x}_i \neq x^0_i\}$ and $\TempSet = \{i \colon \overline{x}_i = 1\}$.

\begin{lemma}
\label{lem:cut-bound-z}
Let $\gamma$ and $\eta$ be two constants such that $\gamma \in [0.1, 5]$ and $\eta \in [\frac{1-\alpha + 0.01}{\alpha}, +\infty)$.
Let $Z$ be the random variable such that 
$$Z = \SdpRatio + \gamma \eta \frac{n-\abs{\SelectSet}}{n-k} + \gamma \frac{\abs{\SelectSet}(2k - \abs{\SelectSet})}{n^2}.$$
	After repeating the hyperplane rounding at least $N = \frac{1 - p + \epsilon p}{\epsilon p} \log(\frac{1}{\epsilon})$ times, 
 where $p$ is a constant lower bounded by $0.0005$, 
 it holds that with probability at least $1 - \epsilon$, 
 $Z \geq (1-\epsilon) [\SdpRatioAvg + \gamma \eta \alpha + \gamma (\alpha (1 - \tau)^2 - 1 + 2\tau)]$.
\end{lemma}

To prove Lemma~\ref{lem:cut-bound-z}, we need to introduce several bounds on
$\SdpRatioAvg$ and $\abs{\SelectSet}$ in expectation. 
We start with the bound on $\SdpRatioAvg$ and specify it on \kdense and \maxcutkc, respectively. 
As for \vckc and \maxuncutkc, we refer the readers to \citep{DBLP:journals/jal/feigel01}, and we omit the analysis here.
For Lemma~\ref{lem:cut-bound-z} to hold, we will use the fact that
$\SdpRatioAvg>0$ for the parameter settings in Table~\ref{tab:case_distinctions}.

\begin{lemma}
\label{lem:boundobj}
Let $\tilde{\SdpRatioAvg}$ be the maximum value such that the inequality
\begin{equation*}
\begin{aligned}
     &(\parzero + \parone + \partwo + \parthree) - \nicefrac{2}{\pi} \cdot (\parone \arccos(\vecv_0 \cdot \vecv_i) + \partwo \arccos(\vecv_0 \cdot \vecv_j) + \parthree \arccos(\vecv_i \cdot \vecv_j)) \\
     &\geq \tilde{\SdpRatioAvg} \cdot (\parzero + \parone \vecv_0 \cdot \vecv_i + \partwo \vecv_0 \cdot \vecv_j + \parthree \vecv_i \cdot \vecv_j)
\end{aligned}
\end{equation*}
holds for every $\vecv_0, \vecv_i, \vecv_j \in \mathcal{S}_{n}$. It holds that $\SdpRatioAvg \geq \tilde{\SdpRatioAvg}$.
\end{lemma}


\begin{proof}
First, we observe that the condition implies that
\begin{equation*}
\begin{aligned}
     &\sum_{i<j} w(i,j) \left( (\parzero + \parone + \partwo + \parthree) - \nicefrac{2}{\pi} \cdot (\parone \arccos(\vecv_0 \cdot \vecv_i) + \partwo \arccos(\vecv_0 \cdot \vecv_j) + \parthree \arccos(\vecv_i \cdot \vecv_j)) \right) \\
     &\geq \tilde{\SdpRatioAvg} \cdot \sum_{i<j} w(i,j) (\parzero + \parone \vecv_0 \cdot \vecv_i + \partwo \vecv_0 \cdot \vecv_j + \parthree \vecv_i \cdot \vecv_j).
\end{aligned}
\end{equation*}

Next, we show that 
\begin{align*}
    &\Exp{(\parzero + \parone \overline{x}_i + \partwo \overline{x}_j + \parthree \overline{x}_i \overline{x}_j)}\\
    &= (\parzero + \parone + \partwo + \parthree) - \nicefrac{2}{\pi} \cdot (\parone \arccos(\vecv_0 \cdot \vecv_i) + \partwo \arccos(\vecv_0 \cdot \vecv_j) + \parthree \arccos(\vecv_i \cdot \vecv_j))
\end{align*}
by using Lemma~\ref{lem:random-projection-1} as follows, 
\begin{equation*}
    \begin{aligned}
        &\Exp{(\parzero + \parone \overline{x}_i + \partwo \overline{x}_j + \parthree \overline{x}_i \overline{x}_j)} \\
        &=  \parzero + \parone \Exp{\sign((\vecr \cdot \vecv_0)(\vecr \cdot \vecv_i))} + \partwo \Exp{\sign((\vecr \cdot \vecv_0)(\vecr \cdot \vecv_j))} + \parthree \Exp{\sign((\vecr \cdot \vecv_i)(\vecr \cdot \vecv_j))} \\
        &= \parzero + \parone (1 - \nicefrac{2}{\pi}\cdot \arccos{(\vecv_0 \cdot \vecv_i)}) + \partwo (1 - \nicefrac{2}{\pi}\cdot \arccos{(\vecv_0 \cdot \vecv_j)}) + \parthree (1 - \nicefrac{2}{\pi}\cdot \arccos{(\vecv_i \cdot \vecv_j)})\\
        &= c_0 + c_1 + c_2 + c_3 - \nicefrac{2}{\pi}\cdot (\parone \arccos{(\vecv_0 \cdot \vecv_i)} + \partwo \arccos{(\vecv_0 \cdot \vecv_j)} + \parthree \arccos{(\vecv_i \cdot \vecv_j)}).
    \end{aligned}
\end{equation*}

Next, notice that 
$\OPT \leq \sum_{i<j} w(i,j) (\parzero + \parone \vecv_0 \cdot \vecv_i + \partwo \vecv_0 \cdot \vecv_j + \parthree \vecv_i \cdot \vecv_j)$ 
and 
$\Exp{\overline{\SdpRnd}} =\sum_{i<j} w(i,j) \Exp{\parzero + \parone \overline{x}_i + \partwo \overline{x}_j + \parthree \overline{x}_i \overline{x}_j}$, and we get that
\begin{align*}
    \Exp{\overline{\SdpRnd}} \geq \tilde{\SdpRatioAvg} \OPT.
\end{align*}
By definition we get $\SdpRatioAvg \geq \tilde{\SdpRatioAvg}$, and we conclude the lemma. 

\end{proof}

\begin{corollary}
\label{lem:dense-bound-size-3}
For \kdense, it holds that $\SdpRatioAvg = \frac{\Exp{\abs{E[\TempSet]}}}{\OPT} \geq \beta$. 
\end{corollary}

\begin{proof}
    First, we note that $\Exp{\abs{E[\TempSet]}} = \Exp{\overline{\SdpRnd}}$ when the problem is \kdense.
    Next, recall that for \kdense we have $\parzero = \frac{1}{4}$, $\parone =
	\frac{1}{4}$, $\partwo = \frac{1}{4}$, $\parthree = \frac{1}{4}$ in
	Lemma~\ref{lem:boundobj}. Now the ratio $\beta$ is obtained by applying Lemma~\ref{lem:beta}.
\end{proof}

\begin{corollary}
	\label{lem:cut-bound-obj}
	For \maxcutkc, it holds that
 $\SdpRatioAvg = \frac{\Exp{\abs{\cut[\TempSet]}}}{\OPT} \geq \alpha$. 
\end{corollary}
\begin{proof}
    First, we note that $\Exp{\abs{\cut[\TempSet]}} = \Exp{\overline{\SdpRnd}}$ when the problem is \maxcutkc.
    Next, recall that for \maxcutkc we have $\parzero = \frac{1}{2}$, $\parone =
	0$, $\partwo = 0$, $\parthree = -\frac{1}{2}$ in Lemma~\ref{lem:boundobj}.
	Now the ratio $\alpha$ is obtained by applying Lemma~\ref{lem:alpha}.
\end{proof}


Next, we give the analysis on bounding $\Exp{\abs{\SelectSet}}$ and $\Exp{\abs{\SelectSet}(n - \abs{\SelectSet})}$ in Lemma~\ref{lem:dense-bound-size-1-short} and Lemma~\ref{lem:dense-bound-size-2-short}.


\begin{restatable}{lemma}{denseboundsizeone}
\label{lem:dense-bound-size-1-short}
The expected size of $C$ can be bounded by $\Exp{\abs{\SelectSet}} \geq \alpha k$ and $\Exp{\abs{\SelectSet}} \leq n - n \alpha + \alpha k$.
\end{restatable}
\begin{proof}
We start by defining a sequence of Bernoulli random variables
$\{Y_i\}_{i = 1, \ldots, n}$ such that $Y_i = 1$ if $\overline{x}_i \neq x_i^0$
and, otherwise, $Y_i = 0$. 
By definition, $\pr(Y_i = 1) = \pr(\overline{x}_i \neq x_i^0)$. 
Since $x_i^0$ is a given constant that can either be $-1$ 
or $1$, we can write $\pr(Y_i = 1)$ as the summation of joint probability over $x_i^0=1$ and $x_i^0=-1$, that is  
\begin{equation*}
	\begin{aligned}
		\pr(Y_i = 1 ) &= \pr(\overline{x}_i\neq x_i^0, x_i^0 = -1) + \pr(\overline{x}_i\neq x_i^0, x_i^0 = 1). \\
	\end{aligned}
\end{equation*}

Recall that we set $\overline{x}_i = 1$ if $(\vecv_0 \cdot \vecr)(\vecv_i \cdot \vecr) \geq 0$ and 
$\overline{x}_i = -1$ otherwise. 
Thus, $\overline{x}_i = \sign(\vecv_0 \cdot \vecr)\sign(\vecv_i \cdot \vecr)$, 
and $x_i^0 \neq \overline{x}_i$ is equivalent to
$x_i^0 \neq \sign(\vecv_0 \cdot \vecr)\sign(\vecv_i \cdot \vecr)$. 
We make the case distinctions for $x_i^0 = -1$ and $x_i^0 = 1$. 

Case 1: $x_i^0 = -1$. Then $\sign(\vecv_i \cdot \vecr) = -\sign(\vecv_0 x_i^0 \cdot \vecr)$, and 
\begin{equation*}
	\displaystyle
	\begin{aligned}
		\pr(\overline{x}_i\neq x_i^0, x_i^0 = -1) 
		& = \pr(\sign(\vecv_i \cdot \vecr) \sign(\vecv_0 x_i^0 \cdot \vecr) = -1, x_i^0 = -1). \\
	\end{aligned}
\end{equation*}

Case 2: $x_i^0 = 1$. Then $\sign(\vecv_i \cdot \vecr) = \sign(\vecv_0 x_i^0 \cdot \vecr)$, and
\begin{equation*}
	\begin{aligned}
		\pr(\overline{x}_i\neq x_i^0, x_i^0 = 1) 
		& = \pr(\sign(\vecv_i \cdot \vecr) \sign(\vecv_0 x_i^0 \cdot \vecr) = -1, x_i^0 = 1). \\
	\end{aligned}
\end{equation*}

Adding up the above two formulas,
\begin{equation*}
	\begin{aligned}
		\pr(Y_i = 1 ) &= \pr(\sign(\vecv_i \cdot \vecr) \sign(\vecv_0 x_i^0 \cdot \vecr) = -1) 
		&\overset{(1)}{=}  \frac{1}{\pi} \arccos(x_i^0\vecv_0 \cdot \vecv_i),
	\end{aligned}
\end{equation*}
where $(1)$ holds according to Lemma~\ref{lem:random-projection-1}.

Note that $\abs{\SelectSet} = \sum_{i=1}^n Y_i$, and by the linearity of expectation, $\Exp{\abs{\SelectSet}} = \sum_{i=1}^n \Exp{Y_i}$. 
From now on, the analysis is the same as the one that appears in \citet[Lemmas~2.3 and 3.2]{DBLP:journals/jal/feigel01} to obtain lower and upper bounds on $\Exp{\abs{\SelectSet}}$. 
In detail, the lower bound on $\Exp{\abs{\SelectSet}}$ is obtained from
\begin{equation}
	\begin{aligned}
		\Exp{\abs{\SelectSet}} &\overset{(a)}{=} \frac{1}{\pi}\sum_{i=1}^n \arccos(x_i^0 \vecv_0 \cdot \vecv_i) &\overset{(b)}{\geq} \alpha \sum_{i=1}^n \frac{1 - x_i^0 \vecv_0 \cdot \vecv_i}{2} 
		\overset{(c)}{=} \alpha k. 
	\end{aligned}
\end{equation}

Similarly, the upper bound is obtained from 
$\Exp{\abs{\SelectSet}}$:
\begin{equation}
	\begin{aligned}
		\Exp{\abs{\SelectSet}} &\overset{(a)}{=} \sum_{i=1}^n \left(1 - \frac{1}{\pi}\arccos(-x_i^0 \vecv_0 \cdot \vecv_i)\right) \\
		&\overset{(b)}{\leq} n - \alpha \sum_{i=1}^n \frac{1 + x_i^0 \vecv_0 \cdot \vecv_i}{2} 
		\overset{(c)}{=} (1- \alpha) n + \alpha k. 
	\end{aligned}
\end{equation}

In the above formulas, $(a)$ follows from the definition of $\abs{\SelectSet}$, $(b)$ follows from Lemma~\ref{lem:alpha}, and $(c)$ follows from the constraint on the \sdp~\eqref{sdp:densest-subgraph}.


\end{proof}

\begin{restatable}{lemma}{denseboundsizetwo}
\label{lem:dense-bound-size-2-short}
 It holds that $\Exp{\abs{\SelectSet} (n - \abs{\SelectSet})} \geq \alpha (n - k)k$.
\end{restatable}

\begin{proof}
	Let $R_{ij}$ denote the Bernoulli random variable 
	indicating whether the relative locations of nodes~$i$ and~$j$ has changed, that is, 
	\begin{equation*}
	R_{ij} =
	\begin{cases}
	1, & \text{if } (\overline{x}_i \neq \overline{x}_j \text{ and } x_i^0 = x_j^0) \text{ or } (\overline{x}_i = \overline{x}_j \text{ and } x_i^0 \neq x_j^0), \\
	0, & \text{otherwise} .
	\end{cases}
	\end{equation*}
	By definition,  
	$\pr(R_{ij} = 1) = \pr(\overline{x}_i \neq \overline{x}_j, x_i^0 = x_j^0) + \pr(\overline{x}_i = \overline{x}_j, x_i^0 \neq x_j^0)$.

	Notice that $\overline{x}_i = \sign(\vecv_0 \cdot \vecr)\sign(\vecv_i \cdot \vecr)$.
	Now $\overline{x}_i \neq \overline{x}_j$ can be equivalently written as $\overline{x}_i \overline{x}_j = -1$ and, based on the statement before, is also equivalent to  
	$\sign(\vecv_i \cdot \vecr)\sign(\vecv_j \cdot \vecr)\sign(\vecv_0 \cdot \vecr)^2 = -1$. 
	Note that the term $\sign(\vecv_0 \cdot \vecr)^2$ is always equal to $1$ and
	can thus be dropped. 
	The first term of $\pr(R_{ij} = 1)$ can thus be formulated as 
	\begin{equation*}
		\begin{aligned}
			\pr(\overline{x}_i \neq \overline{x}_j, x_i^0x_j^0 = 1)
			&= \pr(\sign(\vecv_i \cdot \vecr) \sign(\vecv_j \cdot \vecr) = -1, x_i^0 x_j^0 = 1) \\
			&= \pr(\sign(x_i^0\vecv_i \cdot \vecr) \sign(x_j^0\vecv_j \cdot \vecr) = -1, x_i^0 x_j^0 = 1). \\
		\end{aligned}
	\end{equation*}

	Similarly, $\overline{x}_i \overline{x}_j = 1$ can be equivalently formulated as $\sign(\vecv_i \cdot \vecr)\sign(\vecv_j \cdot \vecr) = 1$. The second term of $\pr(R_{ij} = 1)$ can thus be formulated as
	\begin{equation*}
		\begin{aligned}
			\pr(\overline{x}_i = \overline{x}_j, x_i^0x_j^0 = -1) 
			&= \pr(\sign(\vecv_i \cdot \vecr) \sign(\vecv_j \cdot \vecr) = 1, x_i^0 x_j^0 = -1) \\
			&= \pr(\sign(x_i^0\vecv_i \cdot \vecr) \sign(x_j^0\vecv_j \cdot \vecr) = -1, x_i^0 x_j^0 = -1). \\
		\end{aligned}
	\end{equation*}

	After summing up the two terms and applying
	Lemma~\ref{lem:random-projection-1} in the last equality, we get that
	\begin{equation*}
		\begin{aligned}
			\pr(R_{ij} = 1) &= \pr(\overline{x}_i \neq \overline{x}_j, x_i^0x_j^0 = 1) + \pr(\overline{x}_i = \overline{x}_j, x_i^0x_j^0 = -1)\\
			&= \pr(\sign(\overline{x}_i\vecv_i \cdot \vecr) \sign(\overline{x}_j\vecv_j \cdot \vecr) = -1)  \\
			&= \frac{\arccos(x_i^0x_j^0 \vecv_i \cdot \vecv_j)}{\pi} .
		\end{aligned}
	\end{equation*}

	Recall that $\SelectSet = \{i \colon \overline{x}_i \neq x^0_i\}$ is the
	set of vertices that changed sides after the hyperplane rounding; thus,
	$\SelectSet$ is a random set and also $\abs{\SelectSet}$ is a random variable.
	Note that by definition, $\abs{\SelectSet}(n - \abs{\SelectSet}) = \sum_{i<j} R_{ij}$. Hence, by the linearity of expectation,  
	$\Exp{\abs{\SelectSet}(n - \abs{\SelectSet})} = \sum_{i<j} \Exp{R_{ij}}$.
	Now we obtain our lower bound as follows:

	\begin{equation}
		\begin{aligned}
			\Exp{\abs{\SelectSet}(n - \abs{\SelectSet})} 
			&\overset{(a)}{=} \sum_{i<j} \frac{\arccos(x_i^0x_j^0 \vecv_i \cdot \vecv_j)}{\pi} \\
			&\overset{(b)}{\geq} \alpha \sum_{i<j} \frac{1 - x_i^0x_j^0 \vecv_i \cdot \vecv_j}{2} \\
			&\overset{(c)}{\geq} \alpha(nk - k^2),
		\end{aligned}
	\end{equation}
 where holds by our derivation above, $(b)$ follows by Lemma~\ref{lem:alpha}, and $(c)$ holds by the constraints in the \sdp~\eqref{sdp:densest-subgraph}.
\end{proof}

\para{Proof of Lemma~\ref{lem:cut-bound-z}.} Now, we are ready to prove our main
result for the hyperplane rounding step of our algorithm.
\begin{proof}[Proof of Lemma~\ref{lem:cut-bound-z}]
We start the proof with a lower bound on $\Exp{Z}$.
By applying Lemma~\ref{lem:dense-bound-size-1-short}, Lemma~\ref{lem:dense-bound-size-2-short} and Lemma~\ref{lem:boundobj}, 
we get
$\Exp{Z} \geq \SdpRatioAvg + \gamma \eta \alpha + \gamma (\alpha (1 - \tau)^2 - 1 + 2\tau)$. 

Next, observe that $Z < 1 + \gamma \eta + \gamma$, as $\SdpRatioAvg < 1$, $\alpha <1$ and $\alpha (1 - \tau)^2 - 1 + 2\tau <1$. 
By Markov's inequality, we get $\pr(Z \leq (1-\epsilon)(\SdpRatioAvg + \gamma \eta \alpha + \gamma (\alpha (1 - \tau)^2 - 1 + 2\tau))) \leq \frac{1-p}{1-(1-\epsilon)p},$ for 
$p = \frac{\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau)}{1 + \gamma \eta + \gamma}$.
More precisely, this inequality can obtained from
\begin{equation*}
	\begin{aligned}
	&\pr(Z \leq (1-\epsilon)(\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau))) \\
	&\leq \frac{1 + \gamma \eta + \gamma - \Exp{Z}}{1 + \gamma \eta + \gamma - (1-\epsilon)(\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau))} \\
	&\leq \frac{1 + \gamma \eta + \gamma - [\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau)]}{1 + \gamma \eta + \gamma - (1-\epsilon)(\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau))} \\
	&\leq \frac{1 - p}{1 - (1 - \epsilon)p}.
	\end{aligned}
\end{equation*}

Now observe that $p < 1$. Additionally, we derive a lower bound on $p$, and we
notice that $p>0.0005$ since
\begin{equation*}
	\begin{aligned}
p &= \frac{\SdpRatioAvg + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau)}{1 + \gamma \eta + \gamma} \\
&\overset{(a)}{>} \frac{ \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau)}{1 + \gamma \eta + \gamma} \\
&\overset{(b)}{>} \frac{ \gamma \eta \alpha + \gamma(\alpha -1)}{1 + \gamma \eta + \gamma} \\
&\overset{(c)}{\geq} \frac{0.01\gamma}{1 + 1.01\gamma /\alpha} \\
&\overset{(d)}{>} 0.0005.
	\end{aligned}
\end{equation*}
Above, $(a)$ holds as $\SdpRatioAvg>0$, $(b)$ holds as $2\alpha \tau < 2\tau$, $(c)$ holds as the formula obtains the minimum value by setting $\eta = \frac{1 - \alpha + 0.01}{\alpha}$, $(d)$ holds as the denominator is at most $2$ and the numerator is at least $0.001$ by setting $\gamma = 0.1$.

As we repeat the hyperplane rounding $N$ times, and pick the largest value of $Z$ for fixed $\gamma$ and $\eta$, 
the probability that $Z \leq (1-\epsilon)(\beta + \gamma \eta \alpha + \gamma(\alpha(1-\tau)^2 - 1 + 2\tau))$ is bounded from above by 
\begin{equation*}
	\begin{aligned}
		\left[ \frac{1-p}{1-(1-\epsilon)p} \right]^N &\leq \left[1 - \frac{1}{1+ \frac{1-p}{\epsilon p}} \right]^{(1+\frac{1-p}{\epsilon p}) \frac{\epsilon p}{1- p + \epsilon p} N} \\
	     &\leq \exp\left[-\frac{\epsilon p}{1 - p + \epsilon p} N\right] .
	\end{aligned}
\end{equation*}

Thus if we choose $N \geq \frac{1 - p + \epsilon p}{\epsilon p} \log(\frac{1}{\epsilon})$,
 we can guarantee the above probability is upper bounded by $\epsilon$, 
 that is, we can guarantee $Z \geq (1 - \epsilon) (\SdpRatioAvg + \gamma \eta \alpha + \gamma (\alpha (1 - \tau)^2 - 1 + 2\tau))$ for a 
small positive value $\epsilon$, with probability at least $1 - \epsilon$.
\end{proof}

\subsection{Analysis of fixing $\SelectSet$}
\label{sec:fix-c}
In this subsection, we analyze the third step of our algorithm above, which
ensures that the cardinality constraint for $\SelectSet$ is met.

Let the size of the selected set of nodes after the hyperplane rounding be
$\mu n$, i.e., we let $\mu$ be such that $\abs{\SelectSet} = \mu n$. 
Notice that $\mu$ is a random variable and $1 \geq \mu > 0$. 
Recall that $k\in \Omega(n)$ and $k = \tau n$, where $\tau$ is an input constant. 
We analyze the cost of fixing $\SelectSet$ to ensure that the cardinality
constraint is met and illustrate how the approximation ratios of \gpkc are
computed. 

Let us start from a direct implication from Lemma~\ref{lem:cut-bound-z}, where we set $\epsilon = \frac{1}{\Theta(n)}$, which is small enough relative to other variables. 
\begin{corollary}
    \label{cor:lambda-tau}
    Let $\OPT$ be the optimal value of the \gpkc problem. 
    After repeating the hyperplane rounding $N = \Theta(n \log n)$ times, with probability at least $1 - \frac{1}{\Theta(n)}$, there exists a solution $\overline{\SdpRnd}$ after the hyperplane rounding such that
    \begin{equation}\label{eq:lambda-tau}
        \frac{\overline{\SdpRnd}}{\OPT} \geq (\SdpRatioAvg + \gamma \eta \alpha + \gamma (\alpha (1 - \tau)^2 - 1 + 2\tau) - \gamma \eta (1-\mu)\frac{1}{1-\tau} - \gamma \mu (2\tau - \mu)) - \frac{1}{\Theta(n)}. 
    \end{equation}
\end{corollary}

Recall that $\SdpRatio = \frac{\overline{\SdpRnd}}{\OPT}$.
Based on Corollary~\ref{cor:lambda-tau}, we can write $\lambda$ as a function of
$\mu$, $\gamma$ and $\eta$ with given $\tau$, and we denote this function by
$\SdpRatio_{\tau}(\mu, \gamma, \eta)$. Then we obtain that
\begin{equation}
	\begin{aligned}
	\frac{\overline{\SdpRnd}}{\OPT}&=\SdpRatio_{\tau}(\mu, \gamma, \eta). 
	\end{aligned}
\end{equation}

Let $\SdpRnd$ be the objective value of \gpkc after fixing $\SelectSet$ through
the greedy procedure. We obtain the following results for \kdense and \maxcutkc,
resp. The proofs are the same as the analysis in
\citep{DBLP:journals/jal/feigel01} with minor changes; we present the detailed proofs in the appendix for completeness. 

\begin{restatable}{lemma}{densemoveall}
	\label{lem:dense-move-all}
 For \kdense, if $\mu > \tau$, $\SdpRnd \geq \frac{\tau^2}{\mu^2}(1 - \frac{1}{\Theta(n)}) \overline{\SdpRnd}$, in other words, the sum of edge weights of the subgraph is lower bounded by $\abs{E[\TempSet]}\frac{\tau^2}{\mu^2}(1 - \frac{1}{\Theta(n)})$. 
 Otherwise, $\SdpRnd \geq \overline{\SdpRnd}$. 
\end{restatable}

\begin{restatable}{lemma}{cutmoveall}
	\label{lemma:cut-move-all}
 For \maxcutkc, $\SdpRnd \geq \min \{\frac{\tau^2}{\mu^2}(1 - \frac{1}{\Theta(n)}),  \frac{(1- \tau )^2}{(1 - \mu )^2} (1 - \frac{1}{\Theta(n)})\} \overline{\SdpRnd}$.
	In particular, when $\mu > \tau$, the cut is lower bounded by 
	$\frac{\tau^2}{\mu^2}(1 - \frac{1}{\Theta(n)})\cut{\TempSet}$;
	when $\mu < \tau$, the cut is lower bounded by 
	$\frac{(1- \tau )^2}{(1 - \mu )^2} (1 - \frac{1}{\Theta(n)})\cut{\TempSet}$.
\end{restatable}

We omit the analysis of \maxuncutkc and \vckc, and the readers may refer to \citet{DBLP:journals/jal/feigel01}.
For all the \gpkc problems, we note that the ratio between $\SdpRnd$ and $\overline{\SdpRnd}$ is a function of $\tau$ and $\mu$. 
Let us introduce a function $\kappa_{\tau}(\mu)$ such that 
\begin{equation}
    \frac{\SdpRnd}{\overline{\SdpRnd}} = \kappa_{\tau}(\mu).
\end{equation}

Then the approximation ratio of \gpkc is given by
\begin{equation*}
     \frac{\SdpRnd}{\OPT} = \frac{\overline{\SdpRnd}}{\OPT} \cdot \frac{\SdpRnd}{\overline{\SdpRnd}} = \kappa_{\tau}(\mu) \SdpRatio_{\tau}(\mu, \gamma, \eta),
\end{equation*}
and the approximation ratio depends on the specific settings of $\gamma$ and $\eta$.  
For the worst case analysis, we can bound it by selecting the most appropriate $\gamma$ and $\eta$ by solving 
\begin{equation}
	\max_{\gamma \in [0.1, 5], \eta \in [\frac{1-\alpha + 0.01}{\alpha}, +\infty)} \min_{\mu \in [0 ,1]} \kappa_{\tau}(\mu) \SdpRatio_{\tau}(\mu, \gamma, \eta).
\end{equation}
In the appendix, we specify the details of solving this optimization problem for \kdense and \maxcutkc. 


\subsection{Heuristic}
\label{sec:densest-subgraph:heuristic}
We note that the time complexity of the \sdp-based algorithms is dominated by
solving the \sdp. By the property of our formulation, the number of constraints
is $\bigO(n)$.  The time complexity of solving the SDP using the
state-of-the-art method is $\bigO(n^{3.5} polylog(1/\epsilon))$~\citep{jiang2020faster}, where $\epsilon$ is an
accuracy parameter.

As this time complexity is undesirable in practice, we also provide an efficient
heuristic for \gpkc.  Our heuristic computes a set~$\SelectSet$, 
by starting with $\SelectSet=\emptyset$
and greedily \emph{adding} $k$~vertices to~$\SelectSet$. 
More concretely, while $|\SelectSet|<k$, we greedily add vertices to $\SelectSet$ 
by computing the value of the objective function $\partition_u = \partition{\OldSet\symm (\SelectSet\cup\{u\}}$,
for each vertex $u\in V\setminus \SelectSet$,
and adding to $\SelectSet$ the vertex~$u^*$ that increases the objective
function the most.
The heuristic terminates when $|\SelectSet|=k$.

\section{Sum-of-squares algorithm}
\label{sec:max-cut:sos}

In this section, we present an optimal approximation algorithm for \maxcutkc under some regimes of $k=\Omega(n)$, assuming the Unique Games Conjecture~\citep{DBLP:conf/stoc/Khot02a}. 
Our main result is stated in Theorem~\ref{theorem:sos-main-theorem}.
Due to Corollary \ref{cor:max-cut-hardness}, the approximation algorithm from
our theorem is optimal for 
$\tau \in (0,0.365) \cup (0.635,1)$, assuming the Unique Games Conjecture~\citep{DBLP:conf/stoc/Khot02a}.
\begin{theorem}
\label{theorem:sos-main-theorem}
Let $\tau\in (0,1)$ be a constant, and consider \maxcutkc with $k=\tau \cdot n$ refinements. Then for every $\varepsilon>0$, there is a randomized polynomial-time algorithm that runs in time $n^{O(1/poly(\varepsilon,\tau))}$, and with high probability, outputs a solution that approximates the optimal solution within a factor of $\alpha_{*} - \varepsilon$, where the value of $\alpha_{*}$ is approximately $0.858$. 
\end{theorem}

Our algorithm relies on the Lasserre hierarchy formulation~\citep{DBLP:journals/siamjo/Lasserre02} of the problem along with the algorithmic ideas introduced by \citet{DBLP:conf/soda/RaghavendraT12} for solving \ccmaxcut. 
We show that their method can be applied in our setting when an initial partition is given.
In addition, we show that their approximation ratio still applies in our setting.
Due to the complexity of the entire algorithm, here we only give an overview of
our main ideas; we present the detailed algorithm and its analysis in Appendix~\ref{sec:sos_detailed}.

Let $\overline{\vecx}$ be the partition obtained by
applying the randomized algorithm introduced by \citet{DBLP:conf/soda/RaghavendraT12}.
Specifically, it is obtained by the rounding scheme on an uncorrelated Lasserre SDP solution, which we illustrate in the appendix. 
Let $\overline{\Rnd}$ be the cut value of the partition induced by $\overline{\vecx}$, and $\OPT$ be the optimal solution of \maxcutkc. 
We state the main result that we obtain from applying the method of~\citet{DBLP:conf/soda/RaghavendraT12} in Lemma~\ref{lem:bound-variance}.
We note that the specific algorithm and its analysis are an extension of this
method, with minor (though not trivial) changes. 

\begin{restatable}{lemma}{boundvariance}
\label{lem:bound-variance}
Let $\tau\in (0,1)$ be a constant, and $k=\tau \cdot n$. 
Let $\delta >0$ be a small constant.
There is a randomized polynomial-time algorithm that runs in time $n^{O(1/poly(\delta,\tau))}$, and with
high probability returns a solution $\overline{\vecx}$  such that 
$\overline{\Rnd} \geq 0.858 \cdot \OPT (1-\delta)^2 $  and such that 
$\left|\sum_{i \in V} x_i^0 \overline{x}_i - (n-2k) \right| \leq \delta^{1/48} |V|$.
\end{restatable}

We note that compared to our analysis from Section~\ref{sec:general-framework},
the main differences are as follows: The \sdp-relaxation in
Section~\ref{sec:general-framework} also obtains a solution with objective
function value approximately $0.858 \cdot \OPT$ after the hyperplane rounding.
However, after the hyperplane rounding we might have that $\abs{\SelectSet}$ is
very far away from the desired size of $\abs{\SelectSet}=k$. This makes the
greedy fixing step (Step~3 in the algorithm) expensive and causes a relatively
larger loss in approximation ratio. This is where the strength of the Lasserre
hierarchy comes in: Lemma~\ref{lem:bound-variance} guarantees that
$\left|\sum_{i \in V} x_i^0 \overline{x}_i - (n-2k) \right| \leq \delta^{1/48} |V|$, 
which implies that $\abs{C}$ is very close to the desired size of $\abs{C}=k$.
Thus, we have very little loss in the approximation ratio when greedily ensuring
the size of~$C$ and obtain a better result overall.

Next, we use Lemma~\ref{lem:bound-variance} to directly prove our main theorem.

\begin{proof}[Proof of Theorem~\ref{theorem:sos-main-theorem}]
Observe that $\overline{\vecx}$ does not necessarily satisfy the constraint on the number of refinements.
We aim to obtain a partition $\vecx$ such that $\sum_{i \in V} x_i^0 x_i = n-2k$. 

Let $s = \sum_{i \in V} x_i^0 \overline{x}_i - (n - 2k)$, and $C$ be the set of vertices that changed their partitions. Observe that $s = (n - 2\abs{C}) - (n - 2k) = 2(k - \abs{C})$.
This implies that we need to move $\frac{s}{2}$
vertices\footnote{We allow negative values of $s$  to mean that we are moving vertices from $C$ to $\overline{C}$.}
from $\overline{C}$ to $C$, where $\overline{C}$ is the complement set of $C$,
i.e., $\overline{C} = V \setminus C$, and
\begin{equation*}
 |s| \leq \delta^{1/48} |V|.
\end{equation*}

Recall that $\overline{\Rnd}$ denotes the cut value induced by $\overline{\vecx}$. For $i \in V$ 
let us denote with $\zeta(i)$ the value that node $i$ contributes to the cut. We have that 
\begin{equation} \label{eq:s_eq}
	\sum_{i\in C} \zeta(i) \leq \sum_{i \in V} \zeta(i) \leq 2 \cdot \overline{\Rnd},
\end{equation}
and also
\begin{equation}\label{eq:sbar_eq}
	\sum_{i\in \overline{C}} \zeta(i) \leq \sum_{i \in V} \zeta(i) \leq 2 \cdot \overline{\Rnd}.
\end{equation}
In case we need to move $|s|$ vertices from $C$ to $\overline{C}$, we move vertices $i \in C$ with the smallest values of $\zeta(i)$. 
Let us use $\left \{x_i\right\}_{i \in V}$ to denote the final assignment obtained by moving $\frac{s}{2}$ vertices from $\overline{C}$  to
$C$, and let us use $\Rnd$ to denote the value of the cut under this assignment.
In this case due to \eqref{eq:s_eq} we have that 
\begin{equation*}
	\Rnd \geq \overline{\Rnd}\cdot (1-|s|/|C|),
\end{equation*}
Similarly, if we need to move $|s|$  vertices from $\overline{C}$ to $C$, we move vertices $i \in \overline{C}$ with the
smallest values of $\zeta(i)$. 
In this case due to \eqref{eq:sbar_eq} we have that 
\begin{equation*}
	\Rnd \geq \overline{\Rnd}\cdot (1-|s|/|\overline{C}|),
\end{equation*}
By the previous discussion, we have that 
\begin{equation*}
	\Rnd \geq \overline{\Rnd}\cdot \left(1-|s|/\min(|C|,|\overline{C}|)\right).
\end{equation*}
Now, since $k = \tau n$ for constant $\tau \in (0, 1)$, we can choose $\delta$
sufficiently small so that $\abs{V} - (2 \delta^{1/96} +  \delta^{1/48}) \abs{V}
\geq k \geq (2 \delta^{1/96} +  \delta^{1/48}) \abs{V}$.
For such chosen $\delta$ we have 
\begin{equation*}
	\Rnd \geq \overline{\Rnd}\cdot \left(1-\frac{\delta^{1/48}n  }{ 2\delta^{1/96} n + \delta^{1/48}n - \delta^{1/48} n}\right) \geq 
	\overline{\Rnd}\cdot (1-\delta^{1/96}).
\end{equation*}
Since $\overline{\Rnd} \geq 0.858 \cdot \OPT (1-\delta)^2$, we have that
\begin{equation*}
	\Rnd \geq (1-\delta^{1/96}) \cdot (1-\delta)^2  \cdot 0.858 \cdot \OPT.
\end{equation*}
This shows that we have an $(1-3\delta^{1/96})\cdot 0.858$-approximation
algorithm. By setting $\delta=(\varepsilon/3)^{96}$, we indeed obtain our
desired $(1-\varepsilon) 0.858$-approximation algorithm.
\end{proof}

\section{Experimental evaluation}
\label{sec:experiments}

We evaluate our approximation algorithms and heuristics on a collection of real-world and synthetic datasets. 
Our implementation is publicly available.\footnote{Our implementation is accessible through \url{https://github.com/SijingTu/2023-OPTRefinement-code/}}
In this section, we only report our findings for \dskc, 
while the results for \maxcutkc are in Appendix~\ref{sec:exp:cut}.
We aim to answer the following questions: 
\begin{description}
\item[{\rm{\bf RQ1}:}] Do our algorithms increase the density of the ground-truth subgraph?
\item[{\rm{\bf RQ2}:}] Do our algorithms restore a good solution after removing some nodes from the ground-truth subgraph?
\item[{\rm{\bf RQ3}:}] What are the differences between the nodes identified by different algorithms?
\item[{\rm{\bf RQ4}:}] Which algorithm has the best performance?
\end{description}

\spara{Datasets.}
We use three types of datasets: 
(1)~Wikipedia politician page networks where the nodes represent 
politicians, who are labeled with their political parties, 
and the edges represent whether one politician appears in the Wikipedia page of
another politician.
(2)~Networks labeled with ground-truth communities collected from
SNAP~\citep{snapnets}.
(3)~Synthetic networks generated using the stochastic block model (SBM). 

As for the \emph{ground-truth subgraphs}, let $G[U]$ be the ground-truth
subgraph induced by the set of vertices $U$; we select $U$ in different ways
depending on the datasets.  For the real-world datasets, we select $U$ according
to the nodes' labels: specifically, for Wikipedia networks, we select all the
nodes from one political party as $U$, and for SNAP networks, we select the
largest community as $U$.  As for the synthetic networks generated by SBM, we
always set one of the planted communities as $U$.  

The statistics of the datasets and the properties of the ground-truth subgraphs
are presented in Table~\ref{tab:not-move-out-10}. More detailed descriptions,
including how we set the parameters to generate our random graphs, can be found
in Appendix~\ref{sec:add-exp:data}.

\begin{table*}[t]
	\centering
	\caption{
	{\small 
	Network statistics and average relative increase of density with respect to the ground-truth subgraphs.
	Here, $n$ and $m$ are the number of nodes and edges of the graph;
  $n_0$ and $\rho_0$ are the number of nodes and density of the ground-truth subgraph;
  $n^*$ and $\rho^*$ are the number of nodes and density of the densest subgraph.
    An algorithm not terminating within $5$ hours is denoted by \_\,;
	X~denotes that \denseSQD does not output a result, as no $\sigma$-quasi-elimination order exists. 
	We set $k = 10\% \, n_0$.}}
	\label{tab:not-move-out-10}
	\resizebox{\textwidth}{!}{%
		\input{tables/dense-not-move-out-10.tex}
	}
\end{table*}

\spara{Algorithms.} 
We evaluate six algorithms for solving \dskc.
We use \denseSDPalgo from Theorem~\ref{thm:densest-subgraph-sdp},
\denseGreedy from Section~\ref{sec:densest-subgraph:heuristic}, 
and two methods based on the black-box reduction from Theorem~\ref{thm:densest-subgraph-reduction}. 
In the black-box reduction, we use two solvers for \dks:  
the peeling algorithm of~\citet{asahiro2000greedily}
and the {\sdp} algorithm of~\citet{DBLP:journals/jal/feigel01},
which we denote as \densePeelMerge and \denseSDPMerge, respectively.
We also compare our methods with the \denseSQD ($\sigma$\emph{-quasi-densify}) 
algorithm~\citep{matakos2022strengthening}, which solves \kdense, where we set $\sigma =5$ 
as in the original paper.
We also consider a baseline that picks $k$ random nodes~(\denserandom);
for \denserandom, we repeat the procedure $5$ times and compute the average results. 

\spara{Evaluation.} 
We evaluate all methods with respect to the relative increase of density,
where we compare to the density of the initial subgraph~$U$.
Formally, assuming that a method $\algo$ returns a subgraph $U_{\!\algo}$,
the \emph{relative increase of density} with respect to the density of $U$ is ${(d(U_{\!\algo}) - d(U))}/{d(U)}$. 
We say that method $\algo$ performs $X$ times better than method $\algoB$, 
if the relative increase of density 
from the subgraph returned by $\algo$ is $X$ times larger than $\algoB$.  

\spara{Initialization via ground-truth subgraphs.}
First, we study whether our algorithms indeed increase the density of a given subgraph. 
To this end, we initialize~$U$ as a ground-truth subgraph from our dataset and apply the algorithms directly to the instance $(G, U, k)$. 
The relative increases of densities with respect to $d(U)$ are reported in Table~\ref{tab:not-move-out-10} and Figure~\ref{fig:density-direct-ratio}.

In both Table~\ref{tab:not-move-out-10} and Figure~\ref{fig:density-direct-ratio}, 
we notice that all algorithms increase the density of the ground-truth subgraph across the majority of datasets.  
Specifically, in Table~\ref{fig:density-direct-ratio}, on the datasets with larger ground-truth subgraphs, 
i.e., \us and \dblp, the relative increase of densities surpass $40\%$. 
The only outlier in which the algorithms decrease the density is \dense; this is because the ground truth 
subgraph is already the densest subgraph.
These findings indicate that our algorithms are capable of identifying denser subgraphs over the input
instances, addressing {\bf RQ1}.


\begin{figure}[t]
    \centering
 	\inputtikz{ds_plots/legend_not_move_out} \\
	\begin{tabular}{cccc}
		\resizebox{0.24\columnwidth}{!}{%
			\inputtikz{ds_plots/wiki_de_results_not_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
			\inputtikz{ds_plots/wiki_gb_results_not_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
			\inputtikz{ds_plots/wiki_us_results_not_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
			\inputtikz{ds_plots/ratio_dblp_results_not_move_out_ratio}
		}\\
		{(a)~\de} &
		{(b)~\gb} &
		{(c)~\us} &
		{(d)~\dblp} \\
	\end{tabular}
	\caption{Relative increase of density for varying values of~$k$.
	   We initialized $U$ as a ground-truth subgraph.
    No results of \SDPalgo and \denseSDPMerge are reported on \dblp due to the prohibitive time complexity.}
	\label{fig:density-direct-ratio}
\end{figure}

\begin{figure}[t]
\centering 	
 \inputtikz{ds_plots/legend_move_out} \\
	\begin{tabular}{cccc}
		\resizebox{0.24\columnwidth}{!}{%
\inputtikz{ds_plots/wiki_de_results_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
\inputtikz{ds_plots/wiki_gb_results_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
\inputtikz{ds_plots/wiki_us_results_move_out_ratio}
		}&
		\resizebox{0.23\columnwidth}{!}{%
\inputtikz{ds_plots/ratio_dblp_results_move_out_ratio}
		}\\
		{(a)~\de} &
		{(b)~\gb} &
		{(c)~\us} &
		{(d)~\dblp} \\
	\end{tabular}
	\caption{Relative increase of density for varying values of~$k$.
	   We initialized $U$ as a ground-truth subgraph and then removed $k$~nodes
	   uniformly at random.
    No results of \SDPalgo and \denseSDPMerge are reported on \dblp due to the prohibitive time complexity.}
	\label{fig:density-out-select-ratio}
\end{figure}



\spara{Initialization via ground-truth subgraphs with $k$ nodes removed.}  
Next, we study whether our algorithms are capable of 
uncovering the same-size subgraphs with densities equal to
or surpassing those of the ground-truth subgraphs.
To this end, we initialize $\hat{U}$ as a ground-truth subgraph $U$ with $k$ random nodes removed.
Then, we apply the algorithms to the instance $(G, \hat{U}, k)$. 
We let \denseinit denote the solution containing the $k$~nodes that were removed to obtain~$\hat{U}$.
We present our results in Figure~\ref{fig:density-out-select-ratio}, which includes the 
average relative increase of density with respect to 
$d(\hat{U})$ across $5$ initialization and the corresponding standard deviations. 

Figure~\ref{fig:density-out-select-ratio} shows that, across various datasets, 
all algorithms consistently find subgraphs that are denser than \denseinit for all values of $k$. 
Additionally, the difference in density between the algorithms and \denseinit increases 
as more nodes are removed (i.e., larger $k$). 
Our observation suggests that our algorithms are as good, if not better, 
at finding subgraphs with comparable densities, and this  
observation addresses {\bf RQ2}.

Next, our goal is to study whether our algorithms can be used to reconstruct ground-truth subgraphs. 
We again take the ground-truth subgraphs and randomly remove $k$ nodes; 
after that we run our algorithms and check whether they find the $k$~nodes 
that are removed or other nodes that are not part of the ground-truth subgraph. 
The results are reported in Figure~\ref{fig:similarity-country}, which illustrates the 
Jaccard similarity among the nodes selected by the algorithms and those identified by \denseinit. 
The Jaccard similarity between two sets $A$ and $B$ is defined as $J(A, B) = \frac{|A \cap B|}{|A \cup B|}$.

Figure~\ref{fig:similarity-country} shows distinct behaviors of the algorithms for 
both the real-world and the synthetic datasets. 
We start by analyzing the results on the Wikipedia datasets. 
We notice that these datasets contain sparse ($\rho_0$ is around half of $\rho^*$) 
and large (around half of the nodes) ground-truth subgraphs, and many nodes within the ground-truth subgraph 
maintain more connections to the nodes outside of this subgraph.
This structure leads all the algorithms to select different nodes than \denseinit.
Moreover, the nodes selected by \denseSDPalgo, \denseSDPMerge, and \densePeelMerge 
have a high degree of similarity in all three datasets, whereas \denseGreedy demonstrates a distinct pattern in \de. 
This distinction can be attributed to the ``peeling'' routine inherent in the prior three algorithms, 
which iteratively \emph{removes} nodes with the lowest degree starting from the
graph from the intermediate step.
On the other hand, \denseGreedy iteratively \emph{adds} nodes with the highest
degrees.
The different procedures thus result in very different node selections in some datasets.  

Next, we analyze the results on the synthetic datasets.
Recall that in \sparse, despite the ground-truth subgraph being sparse, 
each node within this subgraph maintains more connections internally than with
nodes outside (in expectation). 
Under this construction, \denseGreedy (which does not contain the ``peeling'' routine) and 
\densePeelMerge (which highly relies on the ``peeling'' routine) exhibit
significant performance differences. 
\denseGreedy exhibits high similarity to \denseinit, indicating that it finds the same set of $k$ removed nodes. 
However, \densePeelMerge finds a different set of nodes compared to \denseinit. 
Combined with its superior performance over \denseGreedy on the \dblp dataset in 
Figure~\ref{fig:density-direct-ratio} and Figure~\ref{fig:density-out-select-ratio}, 
this behavior suggests that \densePeelMerge tends to find globally denser structures, 
when the ground-truth subgraph is much sparser compared to the densest subgraph.
On the other hand, \denseGreedy effectively uncovers locally dense structures, 
albeit at the expense of overlooking globally denser structures that are less interconnected 
within the local subgraph.

We now look into \dense. All algorithms show high similarity to \denseinit,
which can be explained as the ground-truth subgraph is the densest subgraph and is much denser than other subgraphs.
Hence, any sufficiently good algorithm should recover this ground-truth subgraph.
Lastly, we look into \balanced, where only \densePeelMerge shows a different
selection compared to \denseinit, further implying its ``non-local'' property. 

We remark that performance of \denseSDPalgo and \denseSDPMerge is inbetween
\densePeelMerge and \denseGreedy. 
Specifically, they find the removed nodes for \balanced, 
and find nodes that belong to a denser subgraph for \sparse. 
As \denseSQD's choice is generally different from all the other algorithms, 
we present a more detailed analysis of \denseSQD in Appendix~\ref{sec:add-exp:sqd}. 

As anticipated, the algorithms exhibit varied node selection behaviors across different datasets. 
\denseGreedy favors nodes with stronger connections to the initial subgraph.
In contrast, \densePeelMerge is inclined to identify nodes with globally higher connectivity. 
\denseSDPMerge and \denseSDPalgo offer a balance between these frameworks, addressing {\bf RQ3}.


\begin{figure}[t!]
	\centering 
    \begin{tabular}{ccc}
		\includegraphics[width=0.24\columnwidth]{ds_plots/wiki_de_results_fs50.png}&
		\includegraphics[width=0.24\columnwidth]{ds_plots/wiki_gb_results_fs50.png}&
		\includegraphics[width=0.24\columnwidth]{ds_plots/wiki_us_results_fs50.png}
		\\
		(a)~\de &
		(b)~\gb &
		(c)~\us  \\
		\includegraphics[width=0.24\columnwidth]{ds_plots/sb_model_balanced_results_fs50.png}&
		\includegraphics[width=0.24\columnwidth]{ds_plots/sb_model_dense_results_fs50.png}&
		\includegraphics[width=0.24\columnwidth]{ds_plots/sb_model_sparse_results_fs50.png}
		\\
		(d)~\balanced&
		(e)~\dense&
		(f)~\sparse \\
	\end{tabular}
	\caption{Jaccard similarities between 
 the nodes selected by different algorithms. The similarity matrix is symmetric along the diagonal, to make a clean presentation, we only show its upper triangle. Here we set $k = 50$. }
	\label{fig:similarity-country}
\end{figure}

\spara{Selecting the best algorithm.} 
Last, we summarize the performance of all algorithms and select the best algorithm according to different applications.

For smaller datasets, including Wikipedia and SBM datasets, 
although the differences among the algorithms are marginal, 
\denseSDPalgo and \denseSDPMerge show a slight advantage, 
leading in $6$ out of $7$ datasets as shown in Table~\ref{tab:not-move-out-10}.

For mid-sized datasets such as \dblp and \amazon, 
\denseSDPalgo and \denseSDPMerge fail to produce results 
due to the computational complexity of solving a semidefinite program.
Assessing the output quality, \densePeelMerge is marginally outperformed by 
\denseGreedy on \amazon. Yet, on \dblp, \densePeelMerge's performance is 
$2.5$ times better than that of \denseGreedy, as \densePeelMerge excels at identifying globally denser subgraphs.

For the largest dataset, \youtube, we only apply the \denseGreedy algorithm due to the scalability issues of  
\densePeelMerge, which arises from 
its requirement to 
iteratively remove $\Theta(n)$ nodes. 

To conclude, \denseGreedy emerges as the preferable choice for tasks with 
strict scalability demands. 
In scenarios involving smaller graphs, \denseSDPalgo and \densePeelMerge 
output high-quality results. 
For all other cases, \densePeelMerge is recommended for 
its balance of output quality and scalability, 
thus addressing {\bf RQ4}.


\spara{Summary of the appendix.}
More details on the running times are provided in 
Appendices~\ref{sec:add-exp:running-time-by-k} and \ref{sec:add-exp-running-time-by-n}. 
Additional analysis in Appendix~\ref{sec:add-exp:dense:sparse} 
explores the nodes selected by different algorithms on \sparse.
Figure~\ref{fig:appendix:density-not-out-select-ratio} and 
Figure~\ref{fig:appendix:density-out-select-ratio}, 
illustrate the quality of various algorithms across a broader range of 
$k$ values using synthetic datasets. 
Last, a comprehensive analysis of \denseSQD is presented in 
Appendix~\ref{sec:add-exp:sqd}, where \denseSQD's distinct performances 
in selecting node sets and its consistent underperformance relative 
to other algorithms are discussed. 


\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduce the \emph{OptiRefine framework}, 
where we seek to improve the quality of an existing feasible solution by making a small number of refinements.  
We apply our framework to two classic problems: \emph{densest subgraph} and \emph{maximum cut}.  
We demonstrate that the problems we consider are generalizations of previously-studied problem versions 
with cardinality constraints. 
We further provide novel approximation algorithms and heuristics and show that they are
highly effective in practice.

\section*{Acknowledgment}
We sincerely thank Per Austrin for the discussions and for providing valuable feedback on the sum-of-squares algorithm. 
We appreciate Antonis Matakos for sharing the source code for the SQD algorithm.
This research is supported by the ERC Advanced Grant REBOUND (834862), 
the EC H2020 RIA project SoBigData++ (871042), 
the Vienna Science and Technology Fund (WWTF) [Grant ID: 10.47379/VRG23013], 
the Approximability and Proof Complexity project funded by the Knut and Alice Wallenberg Foundation
and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.

\bibliographystyle{plainnat}
\bibliography{paper}
\clearpage

\appendix
\clearpage
\input{appendix/appendix-dense}
\clearpage
\input{appendix/appendix-cut}
\clearpage
\input{appendix/appendix-general}
\clearpage
\input{appendix/appendix-sos}
\clearpage
\input{appendix/appendix-experiment}

\end{document}
