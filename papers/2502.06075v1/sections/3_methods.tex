\section{Methodology}
\label{sec:method}



To address the limitations of existing methods, including the lack of depth captured by questionnaire instruments \cite{comparison_taherdoost_2022}, the decontextualized nature of social-media posts \cite{social_media_decontext_boyd_2012}, and the burden of manual coding of interview transcripts \cite{coding_manual_saldana_2016}, we propose a novel approach to automatically unraveling depression stigma - a common and representative form of mental-illness stigma - by integrating chatbot interviews (Section \ref{method:datacollection}), AI-assisted qualitative coding (Section \ref{method:auto}), and CKG construction (Section \ref{method:ckg}) (Figure \ref{fig:overview}).


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/figure_overview.pdf}
    \caption{Methodology overview. In this work, we propose this approach to deconstruct depression stigma through two main phases: \textit{I. AI-assisted Data Collection and Analysis Pipeline} (RQ1) and \textit{II. Causal Knowledge Graph Construction} (RQ2).}
    % \caption{\textcolor{blue}{Methodology overview. 
    % In this work, we propose this approach to deconstruct depression stigma. 
    % \textbf{I. Data Preparation} (RQ1). 
    % \textit{Data Collection}: Gather interview data using an AI-powered chatbot. 
    % \textit{Human Coding}: Establish expert codes and develop a codebook. 
    % \textit{AI-assisted Coding}: Expand coding to larger datasets and detect and categorize stigma-related expressions. 
    % \textbf{II. Causal Knowledge Graph Construction} (RQ2). 
    % \textit{Triple Extraction}: Extract entity-relation-entity triplets from participant messages. 
    % \textit{Ontologization}: Map entities to theoretical constructs. 
    % \textit{Entity Resolution}: Merge semantically similar entities. 
    % \textit{Conceptual-model Construction}: Discover emerging themes and interrelationships between constructs.}}
    \Description{Flowchart divided into two main sections. Section I 'AI-assisted Data Collection and Analysis Pipeline (RQ1)' shows a linear sequence of three steps: 'Data Collection,' followed by 'Human Coding', followed by 'AI-assisted Coding.' Section II 'Causal Knowledge Graph Construction (RQ2)' shows a linear sequence of four steps: 'Triple Extraction,' 'Ontologization,' 'Entity Resolution,' and 'Conceptual-model Construction.' All steps are connected by arrows showing the progression of the methodology.}
    \label{fig:overview}
\end{figure*}



\subsection{AI-assisted Data Collection and Analysis Pipeline}



Our three-step workflow (Figure \ref{fig:individual}) combined data collection using AI-assisted interviews, human coding to develop a codebook and expert codes, and AI-assisted coding to automate the analysis to identify attributions of depression stigma.
Each of these three phases is described in turn below.




\subsubsection{Data Collection}
\label{method:datacollection}

\paragraph{\textcolor{darkred}{\textbf{Procedure.}}}
Data collection centered on the participants' respective 20-minute interviews with an AI-powered chatbot, for which they were compensated US\$6.30. 
After obtaining their informed consent, we warned the participants that the upcoming interview scenario related to depression, and reiterated that they had the option to withdraw if this made them uncomfortable. 
Next, they provided their demographic information before proceeding to the main phase of the study, an interview with a chatbot we named \textit{Nova}. 
Its interface can be seen in the \textit{Supplementary Materials}.


Each chatbot interview started with a rapport-building \textit{small-talk} session, followed by Nova's delivery of a depression-related \textit{vignette}, during which it intermittently asked the participant for brief responses. 
After the vignette, Nova posed \textit{open-ended questions} prompting participants to share their opinions and related experiences. 
These core questions were split into two parts by a second brief session of small talk, aimed at re-engaging the participants with lighthearted discussion and relieving their potential emotional burdens. 

After the interaction with Nova ended, all participants were debriefed about common types of stigma and the study's objectives. 
All materials that emanated from the chatbot were refined by a mental-health specialist (a co-author) and an external consulting psychiatrist.



\paragraph{\textcolor{darkred}{\textbf{Participants.}}}
We amassed participants through two online research platforms, Prolific and Qualtrics. 
Our inclusion criteria were that they 1) were at least 21 years old, 2) spoke English as their first language, 3) were willing to read material related to mental illness, and 4) had no immediate or urgent mental-health concerns. 
The last criterion is incorporated because of the potential risk that vignettes about depression could cause distress and/or trauma to participants grappling with such issues \cite{ethic_mental_illness_roberts_2002}.
During recruitment, we clearly outlined our research's scope and duration, and the participants' right to withdraw from it at any time. 
We used IP addresses to filter out potential duplicate participants. 

We recruited 1,002 participants from Western countries, mostly the United States and the United Kingdom, with an average age of 46.4 ($SD = 16.45$). 
They were 53.9\% male, 45.9\% female, and 0.2\% other genders. 
Ethnically, 69.4\% were white, 21.0\% black, 4.7\% Asian, and 6.0\% other ethnicities. 
None of the participants reported having an ongoing mental illness, but 52.5\% said they had close friends or family members with mental-health issues.


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/figure_individual.pdf}
    \caption{Overview of the AI-assisted data collection and analysis pipeline. This pipeline encompasses three main steps: \textit{Data Collection} to gather interview data using an AI-powered chatbot, \textit{Human Coding} to establish expert codes and develop a codebook, and \textit{AI-assisted Coding} to expand coding to larger datasets and detect and categorize stigma-related expressions.}
    \Description{Flowchart diagram organized into three sequential stages to illustrate the AI-assisted data collection and analysis pipeline. The diagram shows three major steps: 'Data Collection,' followed by 'Human Coding,' followed by 'AI-assisted Coding.' The Data Collection section illustrates human-chatbot interaction with dialog boxes. The 'Human Coding' section shows coders interacting with a codebook and participant messages. The 'AI-assisted Coding' section demonstrates the primary prompt-based approach to message classification, with fine-tuning methods shown as an alternative for comparison. The process is illustrated using a sample message about mental health, which is classified as 'Stigma (Responsibility).' All steps are connected by arrows showing the progression of the methodology.}
    \label{fig:individual}
\end{figure*}



\paragraph{\textcolor{darkred}{\textbf{Vignettes.}}}

\textit{Vignettes}, concise fictional narratives, are valuable research tools for exploring attitudes toward mental health \cite{vignette_alem_1999}. 
These stories, grounded in research and real experiences \cite{chatbot_aq27_practice_lee_2023, vignette_alem_1999}, allow research participants to respond to concrete scenarios through various feedback methods, and thus to provide researchers with insights into their perspectives. 
We created a vignette focused on "\textit{Avery}," a person of unspecified age and gender who was experiencing depressive symptoms. 
These symptoms were as set forth in the DSM-5 \cite{dsm5_apa_2013}, except that the more extreme ones such as self-harm and suicide were avoided, and no technical or medical jargon was used. 
The vignette, refined with expert input from our mental-health specialist and consulting psychiatrist, illustrated how such symptoms negatively impacted various aspects of Avery's life, including study, work, relationships, and interactions with others.

Specifically, all participants read the following vignette:

\begin{quote}
    \textit{Avery is employed by a company, and in their spare time, they are dedicated to lifelong learning, doing extensive reading and writing. However, Avery has been diagnosed with depression recently. It has become challenging for them to concentrate, resulting in a decline in work performance and learning outcomes. Interests that once brought them joy have waned. Avery has distanced themself from friends, becoming easily irritated in social interactions with colleagues and feeling judged by others. Avery lives with family and cannot do much, especially household chores. Social media intensifies their feelings of loneliness and anger, leading to frustration about the source of the anger.}
\end{quote}



\paragraph{\textcolor{darkred}{\textbf{Interview Questions.}}}

To elicit the participants' opinions, we designed multiple questions adapted from Corrigan et al.'s attribution model \cite{attribution_model_corrigan_2003} and the Attribution Questionnaire-27 (AQ-27) \cite{aq27_corrigan_2012}, which Nova posed as soon as they had finished reading the vignette. 
The AQ-27, developed from the attribution model, operationalized theoretical constructs into nine measurable items (i.e., \textit{blame}, \textit{fear}, \textit{pity}, \textit{anger}, \textit{helping}, \textit{avoidance}, \textit{coercion}, \textit{segregation}, and \textit{dangerousness}), each assessed through three standardized survey questions.
Following Lee et al. \cite{chatbot_aq27_practice_lee_2023}, we combined the coercion-segregation and fear-dangerousness pairs to create a more concise interview flow and adapted the survey questions into seven chatbot questions corresponding to the attributions of \textit{responsibility} (i.e., \textit{blame}), \textit{fear}, \textit{pity}, \textit{anger}, \textit{helping}, \textit{social distance} (i.e., \textit{avoidance}), and \textit{coercive segregation} (Table \ref{tab:question}).

We randomized the order of the questions to avoid priming effects \cite{priming_effect_molden_2014} and embedded each question within a vivid, relatable scenario to mitigate social-desirability bias and foster more honest responses \cite{sd_indirect_q_fisher_1993}.
All question selections and adaptations were validated by the mental-health specialist and consulting psychiatrist.



\paragraph{\textcolor{darkred}{\textbf{AI-assisted Data-collection Strategies.}}}

To facilitate self-disclosure, we employed three techniques. 
% The first comprised \textit{follow-up questions} \cite{follow_up_q_han_2021}. 
The first is that we formulated one to two \textit{follow-up questions} \cite{follow_up_q_han_2021} for each interview question to probe for underlying reasons, anticipated outcomes, or specific triggering scenarios.
Specifically, when dealing with emotional responses (i.e., \textit{anger}, \textit{fear}, and \textit{pity}) and \textit{responsibility} attributions, Nova prompted the user to give \textit{reasons} for their response if it fell below a minimum-length threshold. 
For behavior-related responses (i.e., \textit{coercive segregation}, \textit{social distance}, and \textit{helping}), it asked for reasons if the response was non-stigmatizing, and for \textit{potential results} if it was stigmatizing. 
For example, if a participant endorsed \textit{coercive segregation}, Nova might ask, \textit{"What do you think would happen if Avery was not involuntarily admitted to a psychiatric hospital?"} 

The second technique was \textit{active listening} \cite{active_strategies_zheng_2023}: i.e., Nova restated and validated the user's viewpoint and expressed understanding of it. 
The third and final technique was \textit{neutral self-disclosure}, in which Nova shared both positive and negative perspectives to elicit participants' disclosure \cite{disclosure_lee_2022} without shifting their attitudes (as further illustrated in the \textit{Supplementary Materials}). 
% This exchange of perspectives is a unique benefit of interpersonal conversations.


\paragraph{\textcolor{darkred}{\textbf{System Implementation.}}}

To create our AI-powered chatbot, we used the UChat platform and adopted a hybrid approach that combined pre-written scripts with AI-generated text. 
The small-talk questions, vignette delivery, questions based on the attribution model \cite{attribution_model_corrigan_2003}, and chatbot's neutral self-disclosure for each question were all pre-scripted and hard-coded. 
All other conversational elements, including follow-up questions and active responses to participants, were generated by the AI. 
We used \texttt{GPT-4-1106-preview} \cite{gpt4_achiam_2023} for generating the chatbot's text, with a maximum token limit of 100 and a temperature setting of 0.2. 
The chatbot interface was integrated into Qualtrics programming, and we ensured that the concurrency rate stayed below 50.




% \begin{table*}[tbp]

% \caption{Chatbot question scripts and their corresponding AQ-27 survey items.}
% \Description{Three-column comparison table showing the relationship between chatbot questions and a standardized questionnaire. The columns are labeled 'Chatbot Question Scripts,' 'AQ-27 Survey Item,' and 'Attribution Types.' Seven rows cover different aspects of depression stigma, with each row mapping a chatbot question to its corresponding survey item and attribution type.}
% \label{tab:question}
% \begin{tabularx}{\columnwidth}{p{0.5\columnwidth}p{0.25\columnwidth}X}
% \toprule
% \textbf{Chatbot Question Scripts} & \textbf{AQ-27 Survey Item} & \textbf{Attribution Types}\\
% \midrule
\begin{table*}[tbp]
\small
\caption{Chatbot question scripts and their corresponding AQ-27 survey items.}
\Description{Three-column comparison table showing the relationship between chatbot questions and a standardized questionnaire. The columns are labeled 'Chatbot Question Scripts,' 'AQ-27 Survey Item,' and 'Attribution Types.' Seven rows cover different aspects of depression stigma, with each row mapping a chatbot question to its corresponding survey item and attribution type.}
\label{tab:question}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{0.45\textwidth}p{0.3\textwidth}p{0.1\textwidth}}
\toprule
\multicolumn{1}{c}{\textbf{Chatbot Question Scripts}} & \multicolumn{1}{c}{\textbf{AQ-27 Survey Items}} & \multicolumn{1}{c}{\textbf{Attribution Types}}\\
\midrule
Just like what is mentioned in the story, Avery is currently facing difficulties in both their relationships with colleagues and their work performance. Do you believe Avery's current situation is primarily \textbf{a result of their actions}? Please share your thoughts. & I would think that it was Harry's \textbf{own fault} that he is in the present condition. & Responsibility \\
\specialrule{0em}{1pt}{1pt}
If you were in the process of selecting a \textbf{tenant} for your home, would you feel comfortable entrusting it to someone like Avery? & If I were a \textbf{landlord}, I probably would rent an apartment to Harry. & Social Distance \\
\specialrule{0em}{1pt}{1pt}
Let's imagine you were one of Avery's friends at the party or other social occasions. Would you feel \textbf{anger} toward them due to their actions, for example, losing their temper and yelling at someone? & How \textbf{angry} would you feel at Harry? & Anger \\
\specialrule{0em}{1pt}{1pt}
I wonder if you were one of Avery's colleagues or friends, then would you be inclined to \textbf{help} them with tasks, such as work projects? & How likely is it that you would \textbf{help} Harry? & Helping \\
\specialrule{0em}{1pt}{1pt}
I'd like to know if you would feel \textbf{concern} and \textbf{sympathy} at Avery's mental state? & How much \textbf{sympathy} and \textbf{concern} would you feel for Harry? & Pity \\
\specialrule{0em}{1pt}{1pt}
Apart from being a colleague or friend, I'm also curious if you are Avery's neighbor, then would you consider it better to \textbf{separate} them from the neighborhood and have them undergo \textbf{hospitalization} for treatment? & How much do you think an \textbf{asylum}, where Harry can be \textbf{kept away from} his neighbors, is the best place for him? & Coercive Segregation \\
\specialrule{0em}{1pt}{1pt}
If you were traveling or engaging in activities with Avery, would you have any feelings of \textbf{frightened} or being \textbf{threatened} regarding the potential implications of your interaction? & I would feel \textbf{frightened} and \textbf{threatened} by Harry. & Fear \\
\bottomrule
\end{tabular}
\end{table*}


\subsubsection{Human Coding}
\label{sec:humancoding}

While AI shows promise in qualitative coding, it is crucial to have humans validate its reliability, as relying solely on AI may lead to biases and overprediction \cite{bias_labelling_llm_ashwin_2023}. 
Thus, we conducted human \textbf{deductive coding} \cite{coding_manual_saldana_2016} to establish expert codes for AI-assisted coding, create a training dataset, and develop a codebook that could guide the research team when giving AI instructions.
We formed a coding team of two members as a triangulation approach \cite{coding_manual_saldana_2016}: one author with a computer-science background and a hired graduate student with social-science expertise.
In line with our interview questions and the attribution model \cite{attribution_model_corrigan_2003}, the coding scheme comprised seven stigma attributions (i.e., \textit{responsibility}, \textit{anger}, \textit{pity}, \textit{fear}, \textit{helping}, \textit{coercive segregation}, and \textit{social distance}) and one code for \textit{non-stigmatization}.
The two coders coded messages from 600 randomly selected participants (a total of 4,200 messages) under the guidance of the main researcher and the mental-health specialist, using Cohen's $\kappa$ as a measure of inter-rater reliability \cite{cohens_kappa_mchugh_2012}.



We developed a preliminary codebook and iteratively revised it through several rounds of coding and discussion.
The two coders independently coded messages with frequent agreement checks. 
Starting with shorter intervals, they first coded 10 participants ($\kappa=0.55$), then another 10 ($\kappa=0.53$), followed by four sets of 20 participants each ($\kappa=0.66$, $0.79$, $0.76$, and $0.72$). 
As consistency improved, they extended the intervals to two sets of 50 participants ($\kappa=0.74$, $0.66$), 100 participants ($\kappa=0.69$), and two sets of 150 participants ($\kappa=0.69$, $0.66$). 
At each checkpoint, the two coders engaged in open discussions with the main researcher and the mental-health specialist about their coding decisions until consensus about the coding rules/specifications was reached.
We achieved \textbf{Cohen's $\kappa$ of 0.71} across all 4,200 messages, reaching a satisfactory level \cite{cohens_kappa_mchugh_2012}; and to further validate the coding quality, we randomly sampled and re-coded 50 messages from the first 700 coded messages, 40 from the next 1,400, and 10 from the last 2,100, which yielded a $\kappa$ of 0.69.


\subsubsection{AI-assisted Coding}
\label{method:auto}

To automatically assign labels indicating the presence of specific stigma attributions in each participant's messages, we employed \texttt{GPT-4-Turbo}. 
The prompt-based method we used to transform our human-developed codebook into a set of instructions for the LLM's qualitative coding, as well as the settings and metrics for the validation tests of this coding, are described below.

\paragraph{\textcolor{darkred}{\textbf{Prompt Curation.}}}
% We used a prompt-based method to transform our human-developed codebook into a set of instructions for the LLM's qualitative coding.
To be effective, our instructions to LLMs had to contain several key elements. 
For \textit{context}, these included the vignette, chatbot questions, and participant messages. 
We also required the LLM to select the most appropriate code from a set of predefined options based on the \textit{constraints} it was given, which comprised information about all codes, including their definitions, keywords, coding rules/specifications derived from our human-developed codebook, and selected examples.


We then determined the structure of the prompts based on the results of our pilot tests and prior guidelines \cite{prompt_guidance_ziems_2024, chatbot_various_task_amin_2023, label_llm_kuzman_2023, label_llm_zhu_2023}. 
We used concise language starting with action verbs and employed a multiple-choice format, transforming the coding task into a single-choice question with a set of lettered answer options. 
\textit{Constraints} were provided before the \textit{context} to promote instruction-following, and the output format was regulated. 
We also asked the LLM to provide explanations for its coding decisions and framed our questions as \textit{what-is-your-prediction} instead of \textit{can-you-predict} \cite{chatbot_various_task_amin_2023}. 
Additionally, we utilized role-playing strategies \cite{prompt_practice_reiss_2023}, casting the LLM as \textit{a competent coder for depression stigma}. 
To ensure reproducibility, we set the temperature to zero and generated five outputs for each message, using a majority-voting mechanism to determine the final code. 
The full prompt text is available in the \textit{Supplementary Materials}.



\paragraph{\textcolor{darkred}{\textbf{Validation Settings.}}}
To validate the AI-assisted coding process, we first assessed \textbf{AI-human agreement} across 4,200 messages with existing human codes. 
We then randomly selected 25 messages per code (a total of 200 messages) from the 2,814 AI-coded messages that had not previously been human-coded and proceeded to code them, allowing us to assess AI-human agreement on previously unseen data.


We also established baselines by fine-tuning two \textbf{computational models} commonly used in social-media analysis: \texttt{RoBERTa-base} \cite{roberta_liu_2019} and \texttt{BERTweet-base} \cite{bertweet_nguyen_2020}. 
The 4,200 human-coded message-code pairs $(m, c)$ were shuffled and split into 80\% training and 20\% test sets using stratified sampling to ensure the code distribution in both sets matched that of the complete dataset. 
We compared the agreement (Cohen's $\kappa$) between human coding and each approach (AI-assisted coding and both baseline models) on the test set.
For baseline models, we identified the optimal configuration through hyperparameter tuning: a training regime of 3 epochs (iterations) with batch sizes of 12 and 5 for \texttt{RoBERTa-base} and \texttt{BERTweet-base} respectively, along with optimized settings for learning rate, AdamW optimizer, and early-stopping criteria.
% For the baseline models, we used a training regime of 3 epochs with batch sizes of 12 and 5 for \texttt{RoBERTa-base} and \texttt{BERTweet-base} respectively, utilizing optimized hyperparameters including learning rate, AdamW optimizer, and early stopping criteria.}




\subsection{Causal Knowledge Graph Construction} % need to highlight this graph is from QUALITATIVE data
\label{method:ckg}

We constructed a CKG using messages collected from all 1,002 participants through our chatbot interviews, along with the results of AI-assisted coding (Figure \ref{fig:graph}). 
By organizing this rich interview data into a graph structure, we aimed to illustrate the mechanisms and causalities underlying stigmatizing behavioral intentions at the macro level. 
The relevant workflow consisted of four parts – triple extraction, ontologization, entity resolution, and conceptual-model construction – each of which is explained in detail below. 

For clarity, we distinguish between three key terms in our paper: \textit{\textbf{entities}} (individual nodes in the CKG representing specific text segments), \textbf{\textit{constructs}} (theoretical categories to which \textit{entities} are mapped), and \textbf{\textit{themes}} (recurring patterns identified within each \textit{construct}).
% Figure \ref{fig:graph} illustrates the first three steps of this process.


\subsubsection{Triple Extraction}

To extract causal relationships, we conducted prompt-based fine-tuning using \texttt{GPT-3.5-Turbo} \cite{kg_carta_2023}. 
Specifically, for each message, we pre-specified a triple connecting stigmatization status (\textit{stigma} or \textit{no stigma}) to the corresponding codes obtained from human/AI coding. 
For example, a message classified as lacking pity would yield the triple \textit{"(stigma, because, no pity)"}. 
Separately, we fine-tuned the model to extract further causal relationships, such as the reasons behind the lack of pity.


To create annotated data, we began by manually extracting triples from 70 messages – comprising five stigmatizing and five non-stigmatizing responses to each of the seven attribution questions. 
We then fine-tuned the model and used it to generate triples for another 70 (random, unseen) messages. 
Two of the authors reviewed, curated, and corrected the model's predictions, and the corrected triples were used to refine the model further. 
By repeating this process for six iterations, with accuracy improving from 0.47 to 0.66, 0.86, 0.90, and 0.93\footnote{We used accuracy instead of Cohen's $\kappa$ because human curation could add or remove triples that the model missed or incorrectly extracted, making the total number of triples inconsistent between human and model outputs. The accuracy was calculated by treating each triple (entity, relationship, entity) as the basic unit of comparison and computing the ratio of matching triples to the total number of triples.}, we covered about 5\% of the dataset. 
During curation, we categorized model errors into five types: cause-effect reversal, logical inconsistencies, wording inaccuracies (incomplete text in entities), redundancies, and omissions. 
We also steered the model to consider longer causal chains.

After curating about 5\% of the messages, we evaluated the model's performance without further fine-tuning by having it extract triples from 70 (new, unseen) messages and then manually assessing these extractions, achieving \textbf{an accuracy of 0.93}.
The final model was then used to process all 7,014 messages based on its knowledge of the 420 curated messages.
% This approach aimed to balance manual effort with scalability while also maintaining reliability.



\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/figure_graph.pdf}
    \caption{Key steps in the construction workflow for a causal knowledge graph of depression stigma: \textit{Triple Extraction}, where we extract entity-relation-entity triplets from participant messages; \textit{Ontologization}, where we map entities to theoretical constructs; and \textit{Entity Resolution}, where we merge semantically similar entities. These steps lay the foundation for \textit{Conceptual-model Construction}, where we discover emerging themes and interrelationships between constructs (not shown in the figure).}
    \Description{Flowchart showing the transformation of interview responses about depression into a structured causal knowledge graph. Starting with chatbot-collected messages in which participants discuss their views on depression and mental health, the workflow visualizes three key steps: (1) Triple Extraction, where messages are broken down into an entity-relationship-entity format; (2) Ontologization, where extracted entities are categorized into color-coded constructs (pink for beliefs, blue for behavioral intentions, gray for potential outcomes, and beige for suggestions); and (3) Entity Resolution, where semantically similar concepts are merged. The figure sets up for, but does not show, the final Conceptual-model Construction step, where emerging themes and relationships between constructs are identified.}
    \label{fig:graph}
\end{figure*}



\subsubsection{Ontologization}
\label{sec:ontology}

In knowledge engineering, an ontologization step is crucial to mapping extracted entities into theoretical constructs \cite{ke_iqbal_2013}.


\paragraph{\textcolor{darkred}{\textbf{Conceptualization.}}}

We began the ontologization with conceptualization, adopting \textbf{a combination of deductive and inductive coding approaches}. 
Two authors first independently examined 50 entities, each extracted from a different message to maximize data coverage. 
% We adopted a combination of deductive and inductive coding approaches. 
We established an initial coding scheme based on Corrigan et al.'s attribution theory, comprising four theory-driven constructs \cite{hybrid_fereday_2006}: \textit{signaling event}, \textit{cognitive mediator}, \textit{affective response}, and \textit{behavioral reaction} \cite{attribution_theory_corrigan_2000}. 
For each entity, two authors analyzed both the entity itself and the participant's full message as context. 
When encountering entities that did not fit within these predefined theoretical constructs, the two authors held regular meetings to conceptualize and develop new constructs, discussing their role in driving stigma. 
These newly identified constructs were then incorporated into the coding scheme.

This iterative process proceeded with 50 new entities each time until no new constructs emerged, which occurred after a total of 200 entities had been examined, achieving a \textbf{Cohen's $\kappa$ of 0.78}, indicating an acceptable level of agreement \cite{cohens_kappa_mchugh_2012}.
Two authors then consulted with the mental-health specialist to align the construct definition with theories (if any), refining terms such as "\textit{intention}" to "\textit{motivation}" to reflect motivational orientation \cite{motivational_reviewer_kvaale_2016}, and "\textit{nature}" to "\textit{personality}" \cite{personality_reviewer_steiger_2022}.
We present the full set of constructs in the next section, distinguishing between theory-driven and data-driven constructs.


\paragraph{\textcolor{darkred}{\textbf{LLM-assisted Construct Assignment.}}}

We then assigned these constructs to all extracted entities via prompt learning with \texttt{Claude -3}\texttt{-Opus}\footnote{Based on our pilot tests where Claude outperformed OpenAI GPT models, we used it for both ontologization and entity resolution tasks in this study.}. 
More specifically, we created LLM instructions that included all identified constructs and their definitions. 
Each construct was illustrated with examples, such as "\textit{easygoing}" for the construct "\textit{personality}". 
The LLM was instructed to output both the assigned construct and a concise justification (< 20 words). 
We evaluated human-LLM agreement on our human-coded 200 entities and obtained a \textbf{Cohen's $\kappa$ of 0.77}.




\subsubsection{Entity Resolution}

To handle variations in phrasing and consolidate semantically similar entities, we performed entity resolution. 


\paragraph{\textcolor{darkred}{\textbf{Identifying Semantically Similar Entities.}}}
The first step was to identify a set of semantically similar entities for each entity in our dataset. 
To calculate semantic similarity, we represented each entity as a vector using word embeddings.
We used multiple embedding methods\footnote{The embedding methods used include GLoVe, BERT, RoBERTa, DistilBERT, ALBERT, XLNet, S-MPNet, S-GTR-T5, S-DistilRoBERTa, and S-Minilm.} to broaden the semantic coverage and potentially reduce biases \cite{embedding_bias_swinger_2019} that may be present in any single embedding method. 
For each entity, we used each embedding method to find the 10 other entities whose semantic meanings were most similar according to their vector representations\footnote{The value of 10 was chosen after experimenting with values of 1, 5, 10, and 20 \cite{entity_matching_2023}, balancing computational resources with coverage of potentially mergeable entities - larger values could identify more potential matches, but they increased computational overhead, while smaller values risked missing valid matches.}. 
We then combined all the similar entities found by different embedding methods into a single candidate set per entity.


\paragraph{\textcolor{darkred}{\textbf{LLM-assisted Entity Matching.}}}
In the second step, we evaluated which entities within these semantically similar sets should be merged. 
For each entity, we filtered its set of semantically similar entities by retaining only those mapped to the same theoretical construct.
For instance, even if two entities were semantically similar, they would not be considered for merging if one was conceptualized as a \textit{cognitive judgment} and the other as a \textit{signaling event}.
After filtering, each entity had an average of 6.74 potential matches ($SD$ = 33.08).

We prompted \texttt{Claude-3-Opus} to determine whether each remaining entity pair should be merged \cite{entity_matching_2023}. 
To assess the performance, we randomly selected 50 pairs that the LLM had deemed mergeable for evaluation by the main researcher. 
This yielded a \textbf{Cohen's $\kappa$ of 0.90} between LLM and human decisions on whether to merge entities, which indicates almost-perfect agreement \cite{cohens_kappa_mchugh_2012}.




\subsubsection{Conceptual-model Construction}
\label{sec:conceptual}


As a final analytic step after building CKG, our aims were to uncover themes within each construct and to map interrelationships between constructs.


\paragraph{\textcolor{darkred}{\textbf{Identifying Themes within Constructs.}}}
To achieve this, we first applied BERTopic \cite{bertopic_grootendorst_2022}, an advanced topic-modeling technique that leverages BERT-based deep-learning models, to each construct separately. 
This technique generated between 60-100 topics per construct, with the optimal number determined by coherence scores \cite{topic_modeling_practice_liu_2024}\footnote{The technical implementation involved BERT for text embedding, UMAP for dimensionality reduction, HDBSCAN for clustering, and TF-IDF for topic extraction.}.


We then interpreted the topic-modeling results using an open-coding approach. 
After closely reading and familiarizing themselves with the topics generated by BERTopic, two authors independently identified a first-level code for each topic from 1) its top 10 keywords and 2) 30 of its representative messages selected at random \cite{topic_modeling_practice_liu_2024}.
Next, through collective discussion with the mental-health specialist, disagreement resolution, and iterative refinement, these first-level codes were aggregated into overarching \textbf{themes} that captured the key patterns within each construct\footnote{We should clarify that this study consists of three "\textit{coding}" phases: 1) \textit{deductive} coding to identify stigma attributions in messages based on the attribution model \cite{attribution_model_corrigan_2003} (Section \ref{sec:humancoding} and Section \ref{method:auto}), 2) \textit{hybrid deductive-inductive} coding to map entities to theoretical constructs guided by attribution theory \cite{attribution_theory_corrigan_2000} (Section \ref{sec:ontology}), and 3) \textit{inductive} coding to identify themes within each construct (Section \ref{sec:conceptual}).}.





\paragraph{\textcolor{darkred}{\textbf{Distilling Key Interrelationships between Constructs.}}}
To further illustrate key interrelationships between constructs, we developed rules and path-selection algorithms using a heuristic approach based on our qualitative data. 
We considered each relationship as a directed \textit{edge} between constructs. 
The \textit{weight} of each edge was determined by its frequency - specifically, how many participant messages contained that particular relationship. 
We first calculated a threshold for each construct by averaging the weights of all its outgoing edges (total weight sum divided by edge count). 
Next, we retained only those edges whose weights exceeded their respective construct's threshold, thus distilling the most critical relationships in our massive graph dataset.

% These stipulated 1) that only relationships observed in at least one participant messages could be included in the conceptual model; 2) that \textit{signaling event}, \textit{past experience}, \textit{personality}, and \textit{situation} could not be led to by other constructs, due to their innate or fixed nature, and that \textit{behavioral intention} could not lead to other constructs, as our focus was on its formation; and 3) that certain constructs were structured into \textit{stimuli}, \textit{cognitive}, \textit{emotional}, and \textit{behavioral} layers, in line with attribution theory \cite{attribution_theory_corrigan_2000}. 


% We considered each extracted relationship as a directed \textit{edge}, whose \textit{weight} was defined as the number of participant messages from which this relationship was extracted. 
% To identify key paths, we selected those whose outgoing edge weights exceeded the average weights of all outgoing edges for that construct (calculated as the sum of all outgoing edge weights divided by the total number of outgoing edges), allowing us to distill the most critical relationships among the constructs in our massive graph dataset.












% --------------- OUTDATED ------------------

% \begin{table}[tbp]
% \small
% \caption{Chatbot question scripts.}
% \Description{This table presents a set of chatbot question scripts designed to assess various types of attributions related to mental illness stigma. The table is divided into two columns: 'Question Scripts' and 'Attribution Types'. It includes seven different question scenarios, each targeting a specific attribution type such as "Responsibility", "Social Distance", "Anger", "Helping", "Pity", "Coercive Segregation", and "Fear". Each question script is crafted to elicit responses that reflect the corresponding attribution type, using a hypothetical character named Avery as the subject.}
% \label{tab:question}
% \begin{tabularx}{\columnwidth}{p{0.75\columnwidth}X}
% \toprule
% Question Scripts & Attribution Types\\
% \midrule
% Just like what is mentioned in the story, Avery is currently facing difficulties in both their relationships with colleagues and their work performance. Do you believe Avery's current situation is primarily \textbf{a result of their actions}? Please share your thoughts. & Responsibility \\
% \specialrule{0em}{1pt}{1pt}
% If you were in the process of selecting a \textbf{tenant} for your home, would you feel comfortable entrusting it to someone like Avery? & Social Distance \\

% \specialrule{0em}{1pt}{1pt}
% Let's imagine you were one of Avery's friends at the party or other social occasions. Would you feel \textbf{anger} toward them due to their actions, for example, losing their temper and yelling at someone? & Anger \\ 

% \specialrule{0em}{1pt}{1pt}
% I wonder if you were one of Avery's colleagues or friends, then would you be inclined to \textbf{help} them with tasks, such as work projects? & Helping \\

% \specialrule{0em}{1pt}{1pt}
% I'd like to know if you would feel \textbf{concern} and \textbf{sympathy} at Avery's mental state? & Pity \\

% \specialrule{0em}{1pt}{1pt}
% Apart from being a colleague or friend, I'm also curious if you are Avery's neighbor, then would you consider it better to \textbf{separate} them from the neighborhood and have them undergo \textbf{hospitalization} for treatment? & Coercive Segregation \\
% \specialrule{0em}{1pt}{1pt}
% If you were traveling or engaging in activities with Avery, would you have any feelings of \textbf{frightened} or being \textbf{threatened} regarding the potential implications of your interaction? & Fear \\ 
% \bottomrule
% \end{tabularx}
% \end{table}


% To achieve this, we first used BERTopic \cite{bertopic_grootendorst_2022} on each construct separately.
% This involved using BERT for text embedding, UMAP for dimensionality reduction, HDBSCAN for clustering, and TF-IDF for topic extraction; and then, choosing our range for the optimal number of topics, 60-100, based on the highest coherence score \cite{topic_modeling_practice_liu_2024}. 

% We subsequently performed qualitative coding to conceptualize and group the generated topics. 
% Two authors independently identified first-level codes for each topic (i.e., construct) from 1) its top 10 keywords and 2) 30 of its representative posts selected at random \cite{topic_modeling_practice_liu_2024}. 
% Through collaborative discussions, these codes were aggregated into umbrella themes.




% Following expert recommendations, the two coders independently coded messages in progressively longer intervals\footnote{The coding process was divided into checkpoints with increasing message volumes: four sets of 10 messages ($\kappa=0.55$, $0.53$, $0.51$, $0.54$), followed by 20 messages ($\kappa=0.66$), two sets of 50 messages ($\kappa=0.68$, $0.65$), 100 messages ($\kappa=0.69$), and two sets of 150 messages ($\kappa=0.67$, $0.71$).}. 
% At each checkpoint, they engaged in open discussions with the main researcher and the social-work specialist about their coding decisions until consensus about the coding rules and specifications was reached.
% In total, 4,200 messages from 600 participants were coded, achieving satisfactory inter-rater reliability (Cohen's $\kappa=0.71$) \cite{hci_qualitative_coding_method_lazar_2017}.

% In line with our interview questions and the attribution model \cite{attribution_model_corrigan_2003}, we developed a coding scheme comprising seven stigma attributions (i.e., \textit{responsibility}, \textit{anger}, \textit{pity}, \textit{fear}, \textit{helping}, \textit{coercive segregation}, and \textit{social distance}) and one code for \textit{non-stigmatization}. 
% To ensure representative sampling, we randomly selected 600 participants from our participant pool, resulting in 4,200 messages for coding (7 messages per participant).

% To enhance coding reliability and mitigate human bias, we employed \textit{triangulation} \cite{hci_qualitative_coding_method_lazar_2017}. 

% Human coders conducted \textit{deductive coding} \cite{coding_manual_saldana_2016} based on the attribution model \cite{attribution_model_corrigan_2003}.

% met with the main researcher and social-work specialist to discuss coding discrepancies and refine the codebook until consensus about the coding rules and specifications was reached.
% The first involved two coders independently coding 700 messages, then engaging in open discussions with a social-work specialist about their coding decisions until consensus about the initial coding rules was reached. 
% To further refine the codebook, we added a third member to the coding team for the formal coding phase. 

% fine-tuned the \texttt{RoBERTa-base} \cite{roberta_liu_2019} and \texttt{BERTweet+SVM} \cite{bertweet_nguyen_2020} models, both of which are commonly used for social-media analysis, using human-coded message-code pairs $(m, c)$ as baselines. 
% Next, we compared the agreement between human-derived and model-generated codes for both these baseline models and our AI-assisted coding approach, and then used the curated prompts to assign codes to all messages. 
% Lastly, human validation was conducted to ensure the accuracy of the AI-generated codes.
% To validate the AI-assisted coding process, 




% \textcolor{blue}{We then assigned these constructs to all extracted entities via prompt learning with \texttt{Claude-3-Opus}\footnote{Based on our pilot tests where Claude outperformed OpenAI GPT models, we employed it for both ontologization and entity resolution tasks in this study.}.
% More specifically, we created LLM instructions incorporating all identified constructs and their definitions, with each construct accompanied by a carefully selected example (e.g., "\textit{easygoing}" for the "\textit{personality}" construct). 
% The LLM was instructed to output both the assigned construct and a concise justification limited to 20 words. 
% We evaluated human-LLM agreement on our human-coded 200 entities, achieving \textbf{a Cohen's $\kappa$ of 0.77}.}

% This procedure allowed us to explore interconnections among psychological constructs and thus gain insights into the factors influencing anticipated discriminatory-behavior formation and perpetuation.
% In this case, it enabled us to illustrate the drivers of anticipated discriminatory-behavior formation by identifying key concepts and constructs within the CKG. 
% While evaluating our model's triple-extraction performance, we manually annotated a set of entities extracted from the 420 curated messages and assigned them ontology types based on attribution theory \cite{attribution_model_corrigan_2003} and insights we had gained from the data itself. 
% 


% \textcolor{blue}{We employed a wide range of semantics-based embedding methods\footnote{The embedding methods used include GLoVe \cite{glove_pennington_2014}, BERT \cite{bert_devlin_2019}, RoBERTa \cite{roberta_liu_2019}, DistilBERT \cite{distilbert_sanh_2019}, ALBERT \cite{albert_lan_2020}, XLNet \cite{xlnet_yang_2019}, S-MPNet \cite{mpnet_song_2020}, S-GTR-T5 \cite{st5_raffel_2020}, S-DistilRoBERTa, and S-Minilm \cite{minilm_wang_2020}.} to generate vector representations for each entity.} 
% This diverse array of embedding techniques was chosen to broaden semantic coverage and potentially reduce biases \cite{embedding_bias_swinger_2019} that might be present in any single embedding method.
% For each embedding method, we generated a vector representation for each entity and performed $k$-nearest neighbor clustering with $k=10$. This created multiple clusters, each of 10 semantically similar entities, per the embedding method.


% For each embedding method, we identified the $k$-nearest neighbors ($k=10$) for every entity\footnote{The value of 10 was chosen after experimenting with $k \in {1,5,10,20}$ \cite{}, balancing computational resources with coverage of potentially mergeable entities. While larger $k$ values could identify more potential matches, they significantly increased computational overhead, while smaller values risked missing valid matches.}. 
% We then combined these nearest-neighbor sets across all embedding methods to create a comprehensive pool of potentially mergeable entities for each original entity. 
% To maintain ontological consistency, we filtered these candidate sets to only retain entities within the same theoretical construct category. 
% For example, even if two entities showed high semantic similarity in their embeddings, they would not be considered for merging if one was categorized as a cognitive judgment and the other as a signaling event.


% To arrive at the final set of potentially mergeable entities for each original entity, we united the candidate clusters across all embedding methods. 
% However, to maintain the integrity of our ontological structure, we only considered merging entities within the same ontology category. 
% For instance, entities categorized as \textit{emotional response} were only merged with other entities from the same category, even if they exhibited semantic similarities with entities from others.


% We instructed \texttt{Claude-3-Opus} to determine whether each possible pair of entities should be merged to select the final candidates for merging from the potentially mergeable entity sets \cite{entity_matching_2023}. 
% To evaluate the performance of the resulting entity-resolution process, we randomly selected 50 pairs of entities that \texttt{Claude-3-Opus} had deemed mergeable and assessed whether we agreed with the LLM's decision.
