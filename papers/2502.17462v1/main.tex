
\documentclass{article} %
\usepackage{iclr2025_conference,times}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularray}

\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amsmath,amsfonts,bm}

\definecolor{Silver}{rgb}{0.752,0.752,0.752}

\title{The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG}


\author{Francesco S. Carzaniga\textsuperscript{\rm 1, 2},
  Gary Hoppeler\textsuperscript{\rm 3},
  Michael Hersche\textsuperscript{\rm 1}, \\
  \textbf{Kaspar A. Schindler\textsuperscript{\rm 2},
  Abbas Rahimi\textsuperscript{\rm 1}}\\
  \textsuperscript{\rm 1}IBM Research -- Zurich, R\"{u}schlikon, Switzerland\\
  \textsuperscript{\rm 2}Department of Neurology, Inselspital, Sleep-Wake-Epilepsy-Center,\\ Bern University Hospital, Bern University, Bern, Switzerland\\
  \textsuperscript{\rm 3 }ETH Z\"{u}rich, Z\"{u}rich, Switzerland\\
  frc@zurich.ibm.com
}

\iclrfinalcopy %
\begin{document}

\maketitle

\begin{abstract}

All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG).
iEEG benefits from a higher signal-to-noise ratio (SNR), as it measures the electrical activity directly in the brain, while EEG is noisier and has lower spatial and temporal resolutions. 
Nonetheless, both EEG and iEEG are important sources of data for human neurology, from healthcare to brain--machine interfaces. They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification.
Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. 
To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor.
We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly.
In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG.
Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain.
This finding is consistent with reports coming from natural language processing, where clean data sources appear to have an outsized effect on the performance of the DL model overall.
BrainCodec also achieves up to a 64$\times$ compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity.
We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. 
Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks.
Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario. The code is available at \url{https://github.com/IBM/eeg-ieeg-brain-compressor}.
\end{abstract}

\section{Introduction}
Collecting high signal-to-noise ratio (SNR) data can prove to be a challenging endeavor in many situations, especially when considering human data.
However, noisier signals are sometimes adequate to perform the task at hand.
Following this principle, different data modalities can be collected from the same source with varying levels of quality.
For instance, the electroencephalogram (EEG) is a multi-variate time-series recording of the electrical activity of the brain, where the quality and SNR vary considerably based on the specific recording setup. 
On the one hand, non-invasive scalp EEG can be easily collected in many environments through the placement of extracranial electrodes on the scalp, but suffers from low spatial and temporal resolutions. The use of ultra long-term non-invasive EEG systems is expected to help improve personalized patient care, for example, by objectively assessing seizure rate, and also macro- and microstructure of sleep~\citep{Yilmaz2024, Casson2010}, which are both treatable risk factors for dementia~\citep{Hanke2022}.
On the other hand, intracranial EEG (iEEG) collected through invasive intracranial electrodes benefits from a higher SNR and a more direct physical connection to the brain. In particular, iEEG is used by neurologists to delineate the seizure onset zone in patients suffering from pharmacoresistant epilepsy, and deep learning (DL) models have been developed to support them~\citep{Kuhlmann2018, Craik2019}. 
As such, both EEG and iEEG are essential data sources available to physicians and researchers for the study and treatment of a variety of neurological diseases.
However, the storage and transmission of EEG signals is often very costly in these highly critical and sensitive environments. 

Data compression is a viable solution to reduce the costs associated with storing and transmitting the large quantities of (i)EEG data that are collected every day.
Data compression significantly predates the current rise of DL; therefore, the most successful algorithms in this field do not belong to the class of DL models, i.e., they are not neural compressors.
However, recent developments~\citep{Dani2021, Webex2022, Defossez2023} have shown that DL models may outperform more traditional approaches.
Nonetheless, no compressor has yet found widespread use for EEG and iEEG data.

Data compressors can be divided into two main classes: lossless and lossy. 
Lossless compression, e.g., lz4, guarantees full signal integrity, at the cost of lower compression ratios. 
Lossy compression, on the contrary, achieves high compression ratios; however, the fidelity of the reconstruction cannot be guaranteed. 
As such, no lossy compression algorithm can be used for manual or automated seizure detection without thorough evaluation in real-world scenarios. 
For time-series, it is usually preferred to use lossy compression, as they often contain information that is noisy and/or not relevant for use by humans. 
For example, lossy audio compressors are ubiquitous and achieve remarkable size reductions by discarding all content that is mostly unnoticeable by humans. 
We can do the same with EEG and iEEG. However, the spectral content and the frequencies of interest in audio and EEG are significantly different. Moreover, lossy compression is strongly affected by noise. For these reasons, we expect the behavior of lossy compression on EEG and iEEG to be notably different between the two modalities, and also from audio.

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\textwidth]{new_figures/figure_1_v10_exp}
    \caption{\textbf{BrainCodec training and usage}. \textbf{a.} BrainCodec can be trained on EEG or iEEG data. \textbf{b.} BrainCodec trained on iEEG can compress both iEEG and EEG data, while BrainCodec trained on EEG can only compress other EEG data. \textbf{c.} BrainCodec's high-fidelity compressed signals can be used to perform downstream classification on iEEG and EEG data.}
    \label{fig:full_arch}
\end{figure}

The state-of-the-art neural model (EnCodec;~\cite{Defossez2023}) currently available for high-fidelity lossy audio signal compression cannot be used directly on EEG signals, since its design is inadequate in preserving the information content relevant for further analysis. 
Moreover, the state-of-the-art neural compressors have not been validated as viable or effective on either EEG or iEEG signals.
As we have seen previously, it is well-known that EEG is overall a noisier data source than iEEG, so a model that is successful on one modality is not guaranteed to work on the other.
At the same time, it has been reported in other fields, such as natural language processing~\citep{Lee2022a,Muennighoff2023,Gunasekar2023}, that training DL models on cleaner and less noisy data can provide increased performance across the board.
Therefore, we also aim to provide concrete guidelines on the performances of neural compressors when transferring between the two data modalities.

To guide our evaluation throughout this work, we propose a set of criteria to define the high-fidelity reconstruction of EEG signals: 1. percentage root-mean-square difference (PRD) lower than 30 as suggested by~\citet{Higgins2010}; and 2. less than 1\% drop in classification performance of downstream tasks such as seizure detection or motor imagery classification for brain--machine interface; and 3. high reconstruction quality as rated by an expert neurologist.
Any EEG compressor that fulfills all the criteria outlined above is considered a high-fidelity compressor.

In this work, we introduce BrainCodec (see Figure~\ref{fig:full_arch}), a high-fidelity quantized autoencoder compressor for EEG and iEEG. BrainCodec has the following features:
\begin{itemize}
    \item universal compression of both iEEG and EEG using the same model;
    \item favorable transfer from cleaner iEEG to noisier EEG;
    \item high-fidelity compression of EEG signals up to a compression ratio as high as 64;
    \item variable compression ratio, depending on the task requirements.
\end{itemize}
Remarkably, a BrainCodec model trained on iEEG signals and used to compress EEG signals consistently achieves better performance at high compression ratios compared to a BrainCodec trained on the same EEG modality. 
This indicates that training with higher quality, higher SNR data generalizes better even across modalities.
This is also consistent with previous body of work on natural language, and highlights the advantage of clean signals with high SNR for the pretraining stage.
In fact, training DL models on such high-quality signals can improve performance even on their noisier counterparts.












\section{Related work}

\textbf{Neural audio compression.} 
Neural network models have recently started gaining popularity in the audio compression domain due to their high compression ratios and design flexibility. Most neural compression architectures consist of the encoder-decoder pair of an autoencoder, together with a quantizer to generate discrete representations. VQ-VAE~\citep{VanDenOord2017} introduced this method---unrelated to the compression objective---by combining variational autoencoders (VAE) with vector quantization (VQ). VQ uses a learnable codebook containing a discrete set of vectors to represent a larger set of input vectors.

GANs have been shown~\citep{Kumar2019, Yamamoto2020, Kong2020} to be an effective solution to drive the overall neural compressor towards better representations. MelGAN~\citep{Kumar2019} introduced a multi-scale discriminator that consists of three convolutional discriminators that operate on different scales of the waveform. This architecture restricts the discriminators to specific frequency bands so that they learn features of different scales. The learned features can be used to train a generator by minimizing the distance between features of real and synthetic data.

Combining autoencoder, quantizer, and GAN, SoundStream~\citep{Zeghidour2022} and EnCodec~\citep{Defossez2023} represent the state-of-the-art audio compression models. They are based on a fully convolutional encoder-decoder network with a residual vector quantizer (RVQ) and a convolutional GAN discriminator, operating on the frequencies of interest of the signal. All components are jointly trained end-to-end by minimizing reconstruction, quantization, as well as perceptual adversarial losses. 

\textbf{Lossless EEG compression.} Standard lossless compression algorithms such as gzip, zstd, and lz4 are used routinely to reduce the storage requirements of large EEG collections. Typical compression ratios for these algorithms on EEG are 1.2$\times$ to 1.5$\times$. Lossless compression models developed specifically for EEG are more scarce~\citep{Alsenwi2018, Hadi2021, AlNassrawy2022} and have not found use in practice.

\textbf{Lossy EEG compression.} Lossy compression has not been adopted for either clinical or research use due to the uncertainty about the fidelity of the reconstructed signal. In particular, standard lossy time-series compressed formats such as mp3 have not been developed for EEG and thus have poor performance. Recently, wavelet transform-based techniques using NLSPIHT~\citep{Xu2015}, arithmetic coding (AAC;~\cite{Nguyen2017}), and artificial neural networks (ANN;~\cite{Hejrati2017}), have shown impressive results on EEG, with compression ratios up to 8$\times$ and high reconstruction fidelity. Given the importance of EEG signals in a variety of clinical tasks, seizure detection~\citep{Nguyen2018} has been used to validate the reconstruction fidelity of lossy compression as well. Finally, DL models equipped with compressed sensing techniques (CS;~\cite{Du2024}) have achieved state-of-the-art compression performance in terms of reconstruction fidelity. Our BrainCodec surpasses existing work by consistently achieving higher compression ratios and higher reconstruction fidelity.

\section{BrainCodec: Quantized autoencoder neural compressor}

This section presents the main contribution of this work, the neural compressor BrainCodec. 

First, we outline a typical use case for BrainCodec. Consider an EEG signal $X \in \mathbb{R}^{C\times T}$ having $C$ channels and a duration $T = d\times f_s$ of $d$ seconds at a sampling frequency of $f_s$. First, each channel of the signal is fed separately to the BrainCodec encoder, which outputs a compressed representation for the given channel. 
The compressed signal can now be transmitted and stored at a fraction of the cost of the original signal. The BrainCodec decoder reconstructs the original EEG signal from the compressed representation, preserving the relevant information content and producing a high-fidelity result. %

We now focus on the design of our neural compressor model, specifically adapted to EEG signals. We adopt the basic quantized autoencoder design of SoundStream~\citep{Zeghidour2022} and EnCodec~\citep{Defossez2023}, and tailor it to the EEG use case by modifying the loss function and the parameters of the architecture. The compressor consists of three components: an encoder, a quantizer, and a decoder. The encoder maps the EEG signal to a latent representation. The quantizer compresses this latent representation to a quantized representation using residual vector quantization (RVQ). Finally, the decoder reconstructs the signal from the RVQ output. We design the compressor to achieve high compression ratios while preserving the information content needed to perform classification on the signal. The model is trained end-to-end together with a discriminator that learns multi-scale features of the input data. We apply multiple losses over both the time and frequency domain to capture different properties of the signal. This allows our compressor to be used in end-to-end classification pipelines as we show in our seizure detection and motor imagery results, providing significant storage savings. 



\textbf{Encoder and decoder.} First, we divide the EEG signal ($X$) channel-wise into short patches $\bm{x}_{i, j} \in \mathbb{R}^W, \; i \in {1, \dots, C}, \; j \in {1, \dots, \frac{T}{W} = T_W}$, with the patch size $W$ in the order of a few seconds. In particular, we choose $W$ to be 4 seconds long at the signal sampling frequency. The patches serve as the input to the encoder. The encoder is composed of a 2D convolutional layer with 1 input channel, $F$ output channels, and a kernel size of $(3, 1)$. This ensures that each channel is treated separately, and allows the encoder to work on signals with varying number of channels. The initial layer is followed by $N$ encoder blocks, where $N$ depends on the compression ratio. Each encoder block comprises a residual block as well as a 2D convolutional layer with a kernel size of $(K, 1)$ and a stride of $(S, 1)$ for down-sampling, with $K$ twice the size of $S$. The number of channels is doubled with each down-sampling layer until there are $256$. The encoder blocks are followed by a final 2D convolutional layer with $D$ output channels and a kernel size of $(3, 1)$. We choose $F = 16, S = 2, D = 64$, and ELU as the activation function. The encoder finally outputs a latent representation $\bm{z}_{i, j}$ for each input patch. The decoder mirrors the encoder, replacing strided convolutions with transposed convolutions. 


\textbf{Quantizer.} We quantize the latent representation ($\bm{z}$) to a compressed representation ($\bm{z}_{q}$) through RVQ. A codebook stores a finite set of learnable prototype vectors that are used to represent a larger set of input vectors. When compressing $\bm{z}$, the quantizer maps the input vector to the closest prototype vector in the codebook. RVQ~\citep{Zeghidour2022} extends this principle to an iterative process. After mapping an input vector onto a prototype vector, RVQ computes the residual and maps it to another prototype vector from a second codebook. By repeating this process, the sum of prototype vectors converges to the original vector.
As suggested in previous literature~\citep{Zeghidour2022, Dhariwal2020}, the selected prototype vectors are updated using an exponential moving average with a decay of 0.99, whereby the entries that have not been assigned to an input vector are replaced by a randomly sampled input vector. To improve the initialization of the codebooks, we apply k-means clustering to the first training batch and use the centroids as prototype vectors. During training, we use a straight-through estimator~\citep{Bengio2013} to pass the gradients from the decoder to the encoder. At the same time, we compute the MSE between $\bm{z}$ and $\bm{z}_q$ and add it to the overall loss. For all our models, we use $4$ codebooks each of size $256$ (i.e., the storage of an index that refers to a codebook entry requires $8$ bits).

\textbf{Discriminator (GAN component).} During training, we use a multi-scale STFT-based (MS-STFT) discriminator~\citep{Defossez2023} to improve the reconstruction of high frequencies. The MS-STFT discriminator is composed of 5 convolutional discriminators operating on different scales of the complex-valued spectrogram. Each discriminator is composed of an initial 2D convolutional layer with 64 output channels and a kernel size of $(3, 3)$. The initial layer is followed by $3$ convolutional layers with increasing dilation in the time dimension of $1$, $2$, and $4$, a kernel size of $(3, 3)$, and a stride of $(1, 2)$. Another 2D convolutional layer with a kernel size of $(3, 3)$ is followed by the final 2D convolutional layer with 1 output channel and a kernel size of $(3, 3)$. We use $(2048, 1024, 512, 256, 128)$ as STFT window lengths and LeakyReLU as the activation function. 























\section{Training setup}\label{sec:training}

We train BrainCodec following the schema of SoundStream~\citep{Zeghidour2022}. To help guide BrainCodec towards reconstructed signals apt for downstream classification, we also add a new loss based on the line length, which is widely considered to be a useful feature for EEG classification~\citep{Schindler2001, Guo2010, Burrello2021}. We observe that the GAN has a significant effect on the reconstruction fidelity of the signal (see App.~\ref{sup:base_vs_gan}); therefore, we train one model with GAN and one without. To avoid the risk of cross contamination between the subjects, we always train the model on one single subject, and test the reconstruction on the remaining subjects.
To provide a direct baseline for the performance of BrainCodec in the seizure detection task, we also train a standard EEGWaveNet on the original signal, and then test it using the reconstructed signal.

\textbf{Validation.} We also provide objective metrics of the reconstruction performance of our BrainCodec in two downstream tasks: the iEEG seizure detection task, and the EEG motor imagery classification task. In particular, for iEEG we train a standard EEGWaveNet~\citep{Thuwajit2022} classifier on the original signal, and evaluate its performance on the reconstructed signal. For EEG, we train an MI-BMInet~\citep{Wang2024} classifier on the original signal, and evaluate its performance on the reconstructed signal. For more information on the detailed training regime of BrainCodec refer to App.~\ref{sup:compressor_training}. 

\textbf{Optimizer and setup.} We apply the 1-cycle learning rate policy, with a learning rate varying from $10^{-5}$ to $10^{-4}$ for the generator and from $10^{-7}$ to $10^{-6}$ for the discriminator. We further use the weights $\lambda_t = 1$ and $\lambda_q = 1$ for the base model. For the GAN model, we choose $\lambda_t = 0.1$, $\lambda_s = 1$, $\lambda_l = 0.1$, $\lambda_f = 3$, $\lambda_g = 3$, and $\lambda_q = 1$. The model is trained with full fp32 precision.


\subsection{Datasets}

\textbf{SWEC iEEG~\citep{Burrello2019}.} This short-term iEEG dataset contains 15 subjects, 14 hours of recording, and 104 ictal events. The iEEG signals were recorded intracranially with a sampling rate of either 512\,Hz or 1024\,Hz. The signals were median-referenced and band-pass filtered between 0.5 and 120\,Hz using a fourth-order Butterworth filter, both in a forward and backward pass. All the recordings were inspected by an expert neurologist for identification of seizure onsets and offsets, and to remove channels corrupted by artifacts.

\textbf{Multi-center (MC) iEEG~\citep{Li2021}.} This iEEG dataset contains iEEG signals around ictal events for 91 subjects for a total of 462 events, with a sampling rate up to 1000\,Hz. The onset and offset times of the seizures are included in the dataset.


\textbf{Brain Treebank iEEG~\citep{Wang2024a}.} This iEEG dataset contains 10 subjects for a total of 43 hours. The subjects have an average of 168 electrodes with a sampling rate of 2048\,Hz. As this is not an ictal dataset, there are no labels for seizures.



\textbf{CHB-MIT~\citep{Shoeb2010, Shoeb2009}.} This EEG dataset contains a total of 24 subjects, 983 hours of recording, and 198 seizures. All signals have a sampling rate of 256\,Hz. The onset and offset times of the seizures are included in the dataset.

\textbf{BONN~\citep{Andrzejak2001}.} This EEG dataset is composed of five sets (Z, O, N, F, S), each containing approximately 40 minutes of single-channel EEG. All signals have a sampling rate of 173.61\,Hz, and they were band-pass filtered between 0.5\,Hz and 40\,Hz. We use all sets for evaluating the compression only.

\textbf{BCI Competition IV-2a~\citep{Tangermann2012}.} This EEG dataset contains a total of 9 subjects. The data was collected for the purpose of 4-class motor imagery classification in brain-machine interfaces, so we have useful labels for downstream classification. All signals have a sampling rate of 250\,Hz, and they were band-pass filtered between 0.5\,Hz and 120\,Hz with an additional notch filter at 50\,Hz to remove line noise. 

\subsection{Baselines}

We compare the performance of BrainCodec with multiple state-of-the-art lossy compression algorithms that have been developed for EEG. We benchmark against the following methods: NLSPIHT~\citep{Xu2015}, a classical algorithm; ANN~\citep{Hejrati2017}, a deep-learning model; AAC~\citep{Nguyen2017}, a wavelet-based model with adaptive arithmetic coding; and CS~\citep{Du2024}, a compressed-sensing based deep learning model.

To showcase the adaptability of BrainCodec, we test it on three iEEG datasets (SWEC, MC, Treebank), which have a high SNR, and also on multiple EEG datasets (CHB-MIT, BONN, and BCI IV-2a), which are noisier. Finally, we evaluate the generalization capabilities of BrainCodec across datasets and modalities. The full comparison with all the baselines is shown in App.~\ref{sup:full_results}.

\section{Results}

To evaluate the compression performance in terms of reconstruction fidelity, we report the percentage root-mean-square distortion (PRD):
\begin{equation}
    \text{PRD} = \frac{\| \bm{x} - \hat{\bm{x}} \|_2}{\| \bm{x} \|_2} \cdot 100, 
\end{equation}
which represents the relative $L_2$-distance between the original and the reconstructed signal. In order to have a comprehensive outlook, we train multiple models at varying compression ratios.

\begin{figure}[ht]
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{new_figures/ieeg_improvement.pdf}
    \caption{Testing cross-modal BrainCodec on scalp EEG.}
    \label{fig:results_cross}
\end{subfigure}\hfill
\begin{subfigure}[t]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{new_figures/eeg_improvement.pdf}
    \caption{Testing cross-modal BrainCodec on intracranial EEG (iEEG).}
    \label{fig:results_cross_conv}
\end{subfigure}
\caption{\textbf{Cross-modality signal reconstruction fidelity of BrainCodec.} BrainCodec trained on iEEG (higher SNR) always performs better at moderate and higher compression ratios compared to BrainCodec trained on scalp EEG (lower SNR), even when compressing EEG.}
\end{figure}

\subsection{BrainCodec cross-modal compression}

Reports from other fields, especially natural language processing, have shown~\citep{Lee2022a,Muennighoff2023,Gunasekar2023} that training with higher-quality data often yields better performance than simply training with more, and more similar, data. We aim to characterise this phenomenon for human iEEG and EEG, where, due to its higher SNR, we consider iEEG to be of higher quality than EEG in the signal processing domain.

To evaluate the role of high-SNR data in human EEG signals, we train an instance of BrainCodec on the SWEC iEEG dataset and use it to compress EEG signals. Figure~\ref{fig:results_cross} shows the median PRD across all tested EEG datasets when training BrainCodec on iEEG or on EEG. The EEG-trained compressor performs slightly better at lower compression ratios, while the iEEG-trained model becomes competitive and even achieves higher performance at higher compression ratios. Aside from the pure compression advantage, variational autoencoders have been shown to produce better representations when the information bottleneck becomes more restrictive and learning pressure increases~\citep{Burgess2018}. In this region of interest, high-SNR iEEG also becomes a better data source than low-SNR EEG. We hypothesize that the increased performance of the cross-modal BrainCodec (i.e., from iEEG to EEG) at above moderate compression ratios can be traced back to the effect of the noise content of the lower SNR EEG with respect to the higher SNR iEEG (see App.~\ref{sup:cross_compression} for more details).

Conversely, we train an instance of BrainCodec on the EEG CHB-MIT dataset and use it to compress the SWEC iEEG dataset. As expected, Figure~\ref{fig:results_cross_conv} shows that the EEG to iEEG cross-modal BrainCodec performs worse than the within-modality model, indicating that a lower-quality signal is less effective at generalizing than a higher-quality one.

In summary, by characterizing the behavior of BrainCodec when trained across modality, we have corroborated previous reports about data quality from natural language processing. In particular, we have shown that training with higher quality data sources yields improved performance even when generalizing to lower quality ones. The converse is, expectedly, not true. App.~\ref{sup:cross_compression} provides more details. 

\begin{figure}[h]
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/mixed_eeg_improvement.pdf}
		\caption{Testing mixed-modal BrainCodec on scalp EEG.}
		\label{fig:results_mixed}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/mixed_ieeg_improvement.pdf}
		\caption{Testing mixed-modal BrainCodec on intracranial EEG (iEEG).}
		\label{fig:results_mixed_conv}
	\end{subfigure}
	\caption{\textbf{Mixed-modality signal reconstruction fidelity of BrainCodec.} BrainCodec trained on both intracranial EEG and scalp EEG maintains the reconstruction fidelity of an iEEG-model when compressing iEEG. At the same time, it improves performance at high compression ratios with respect to a scalp EEG-trained model compressing scalp EEG.}
\end{figure}

\subsection{BrainCodec mixed-modal compression}

Given the promising results shown by BrainCodec when transferring across modalities, we now investigate the performance of our neural compressor when trained with both modalities --- scalp EEG and intracranial EEG --- at the same time.

To evaluate the effect of mixed-modal compression, we train BrainCodec on both the SWEC iEEG dataset and the CHB EEG dataset, for the same overall amount of data as the previous models to keep the evaluation balanced. Figure~\ref{fig:results_mixed} shows that the median PRD of our mixed model is notably superior to the EEG-only model when compressing EEG signals, indicating that the performance benefits of iEEG training have transferred successfully. In line with the previous cross-modal results, this improvement is more marked at high compression ratios. At the same time, the mixed model also performs on par with the EEG-only model at lower compression ratios, mitigating the drawback we had reported in the previous cross-modal results.

On the other hand, we also test mixed BrainCodec on iEEG recordings. In this case, we do not observe any benefit of mixed-modal training in reconstruction fidelity. However, we also do not observe any notable reduction in performance.



\subsection{BrainCodec compression performance}

Next, we train BrainCodec exclusively on iEEG data to compress other iEEG datasets. Likewise, we train BrainCodec on EEG data to compress other EEG datasets, ensuring compression is performed within the same modality. We then compare the results with baseline methods.

\begin{figure}[ht]
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/swec.pdf}
		\caption{Testing on the SWEC dataset.}
		\label{fig:results_swec}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/mc.pdf}
		\caption{Testing on the MC dataset.}
		\label{fig:results_mc}
	\end{subfigure}
	\caption{\textbf{Within-modality signal reconstruction fidelity of BrainCodec on intracranial EEG (iEEG).} BrainCodec trained only on the SWEC dataset shows increased performance across the board both on the SWEC and MC dataset, and also reaches higher compression ratios while maintaining a moderate PRD.}
\end{figure}
\begin{figure}[ht]
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/chb.pdf}
		\caption{Testing on the CHB dataset.}
		\label{fig:results_chb}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{new_figures/bonn_n.pdf}
		\caption{Testing on the BONN N dataset.}
		\label{fig:results_bonn_n}
	\end{subfigure}
	\caption{\textbf{Within-modality signal reconstruction fidelity of BrainCodec on scalp EEG.} BrainCodec trained only on the CHB dataset, shows increased performance across the board both on the CHB and the BONN dataset, and also reaches higher compression ratios while maintaining a moderate PRD.}
\end{figure}

We compress iEEG and EEG signals with a varying compression ratios from 2$\times$ to 64$\times$. We compare BrainCodec with multiple state-of-the-art methods found in the literature, both neural network-based and classical. The full set of results can be found in App.~\ref{sup:full_results}. 

We choose two iEEG datasets to test the performance of BrainCodec with iEEG compression: SWEC and MC. We train all models that require training on a subset of the respective dataset and test on the remaining part. The results of the best performing BrainCodec model on the SWEC dataset are shown in Figure~\ref{fig:results_swec}. At lower compression ratios, BrainCodec improves on the PRD compared to both AAC and NLSPIHT, but the baselines remain competitive. At higher compression ratios, however, BrainCodec is notably better, with a lower PRD at almost twice the compression. In particular, BrainCodec remains in the high-fidelity regime even with a 64$\times$ compression ratio. Figure~\ref{fig:results_mc} paints a similar picture for the iEEG MC dataset. BrainCodec surpasses all baselines at lower compression ratios and reaches much higher ratios overall. Performance is high when trained with the iEEG SWEC dataset, indicating that BrainCodec generalizes well across iEEG datasets within the same modality.

We test BrainCodec's performance on EEG data with three EEG datasets, CHB-MIT, BONN, and BCI IV-2a. Figure~\ref{fig:results_chb} shows that BrainCodec improves on the PRD compared to all other methods on the EEG CHB-MIT dataset. Moreover, it can also achieve much higher compression ratios. The model BrainCodec Base is trained on a subset of the CHB-MIT dataset and tested on the remaining part, to ensure no contamination between the subjects.  The same trend holds for the EEG BONN N dataset (Figure~\ref{fig:results_bonn_n}). The BONN dataset is too small to both train and test the two neural network-based approaches, BrainCodec and ANN. Therefore, we first train them on other EEG datasets and then test them on BONN. As seen previously with iEEG, the performance of BrainCodec is notably better than the baselines even when trained across EEG datasets within the same modality. The results of BrainCodec on BCI can be found in Figure~\ref{fig:results_bci_sup}.

Finally, we also provide a subjective evaluation by an expert neurologist of the SWEC iEEG and the CHB-MIT EEG datasets as compressed by BrainCodec. The evaluation subjectively confirms that BrainCodec achieves a high-fidelity reconstruction both on iEEG and EEG. More details can be found in App.~\ref{app:kaspar}.

We find that BrainCodec shows superior performance compared to all baselines across our suite of benchmarks both on iEEG and EEG, as can be seen in App.~\ref{sup:full_results}. Overall, these results indicate that BrainCodec is an efficient iEEG and EEG compressor, both within and across datasets. 

\subsection{Downstream classification tasks}

As another objective measurement of reconstruction quality, we validate the reconstruction fidelity on two downstream classification tasks: the iEEG seizure detection task, and the EEG motor imagery task.

First, we evaluate BrainCodec on the iEEG seizure detection task, by testing a subject-specific EEGWaveNet~\citep{Thuwajit2022} seizure classifier on the reconstructed signal. We test the EEGWaveNet across all subjects with a leave-one-out cross-validation scheme, training for each subject on all seizures but one and testing on the remaining seizure. Table~\ref{tab:compressor_eegwavenet} provides the compression ratio, PRD, and resulting F1-score for the signals reconstructed by BrainCodec. Even at a 64$\times$ compression, there is no loss of performance in the seizure detection task. Moreover, the F1-score of the BrainCodec GAN model degrades only by 8\% at 256$\times$ compression, likely due to its better reconstruction of the higher frequencies with respect to the Base model (see App.~\ref{sup:base_vs_gan}). Thus, the relevant information content is preserved by BrainCodec while providing significant storage and transmission savings. 

\begin{table}[ht]
\centering
\begin{tblr}{
  row{3} = {Silver},
  cell{1}{3} = {c=4}{c},
  cell{1}{7} = {c=4}{c},
  hline{2} = {2}{1pt},
  hline{2} = {3-6}{1pt, leftpos = -1, rightpos = -1, endpos},
  hline{2} = {7-10}{1pt, leftpos = -1, rightpos = -1, endpos},
}
    & Original iEEG  & BrainCodec Base &       &      & &   BrainCodec GAN &       & &      \\
CR $\uparrow$  & n.a. & 8             & 64    & 128 & 256  & 8              & 64    & 128 & 256  \\
F1 $\uparrow$  & 0.79 & 0.79          & 0.78  & 0.72 & 0.62 & 0.78           & 0.78  & 0.72 & 0.72 \\
PRD $\downarrow$ & n.a  & 1.88          & 15.7 & 26.3 & 40.7   & 2.37           & 17.6 & 30.0    & 47.4
\end{tblr}
\caption{\textbf{Performance of the BrainCodec compressor on the iEEG seizure detection task.} The PRD remains low ($<\!30$) even at 64$\times$ compression, and the F1-score remains high when a standard subject-dependent EEGWaveNet makes inferences with the reconstructed data instead of the original iEEG ($<\!1\%$ drop).}
\label{tab:compressor_eegwavenet}
\end{table}

Second, we evaluate BrainCodec on the EEG motor imagery task using the MI-BMInet~\citep{Wang2024} classifier. The training and testing setup is analogous to the seizure detection task. Specifically, we train the compressor on the first subject of the BCI dataset, and then report the average test accuracy across all remaining 8 subjects. BrainCodec shows high-fidelity reconstructions up to a 4$\times$ compression ratio, and maintains a useful classification performance up to 16$\times$. This rate of compression is expectedly lower than iEEG, as EEG has intrinsically lower SNR and is thus less amenable to compression.

\begin{table}[ht]
\centering
\begin{tblr}{
  row{3} = {Silver},
  cell{1}{3} = {c=4}{c},
  cell{1}{7} = {c=4}{c},
  hline{2} = {2}{1pt},
  hline{2} = {3-6}{1pt, leftpos = -1, rightpos = -1, endpos},
  hline{2} = {7-10}{1pt, leftpos = -1, rightpos = -1, endpos},
}
    & Original EEG  & BrainCodec Base &       &      & &   BrainCodec GAN &       & &      \\
CR $\uparrow$  & n.a. & 4             & 8    & 16 & 64  & 4              & 8    & 16 & 64  \\
Acc. $\uparrow$  & 77\% & 76\%          & 73\%  & 72\% & 63\% & 74\%           & 73\%  & 73\% & 57\% \\
PRD $\downarrow$ & n.a  & 3.44          & 10.4 & 29.0 & 45.5   & 8.04           & 39.2 & 45.3    & 73.8
\end{tblr}
\caption{\textbf{Performance of the BrainCodec compressor on the EEG motor imagery task (BCI IV-2a)}. Subject 1 has been excluded from the evaluation, as it has been used to train BrainCodec. The PRD remains moderate even at high compression ratios, and the accuracy remains high when a standard subject-dependent MI-BMInet makes inferences with the reconstructed data instead of the original EEG.}
\label{tab:compressor_bci}
\end{table}

Overall, we confirm that BrainCodec can achieve high compression ratios both on iEEG and EEG signals without notably impacting downstream task performance. Therefore, we have sufficiently characterized and validated BrainCodec as an efficient iEEG and EEG compressor.


















\section{Discussion}

In this work, we present BrainCodec, a high-fidelity neural compressor for EEG and iEEG signals. BrainCodec is effective across both modalities and a variety of datasets, indicating that it can successfully replace existing methods and be introduced in any EEG processing pipeline.
Compression by BrainCodec up to $64\times$ does not affect downstream seizure detection performance as evaluated both by human experts and deep learning models.
We also observe that BrainCodec performs better when trained with high SNR iEEG. This performance increase is maintained when compressing the noisier EEG signal, compared to the same model trained on the very same EEG modality.
We therefore highlight the importance of training deep learning models on high-quality signals also in the medical domain. 

Overall, BrainCodec is an immediate compression replacement for many EEG and iEEG applications, enabling transmission and storage cost savings in critical clinical environments. We expect the adoption of BrainCodec to increase the feasibility of long-term recordings and wearable devices.

Further work is necessary to assess whether the intermediate representations of BrainCodec can be directly used by other deep learning models, to increase performance and provide an additional speed-up. Architectural changes to BrainCodec could be made, for example by utilising decoder models specifically developed for biosignals~\citep{Zhang2023}. Moreover, the RVQ quantization schema of BrainCodec is known to be not necessarily codebook efficient, and improvements are already being developed in the field~\citep{Kumar2024}. On this front, more venues are being explored to replace RVQ with another quantization schema, such as Finite Scalar Quantization~\citep{Mentzer2024}.


\subsubsection*{Acknowledgments}
This work is supported by the Swiss National Science foundation (SNF), grant no. 200800.

\subsubsection*{Reproducibility}

The training setup, losses, and optimizers are described in detail in Sec.~\ref{sec:training} and App.~\ref{sup:compressor_training}. All datasets used are listed and are publicly available for download, and the data selection for the generation of the results is also explained in detail. Finally, the code is available at \url{https://github.com/IBM/eeg-ieeg-brain-compressor}.

\bibliography{parall}
\bibliographystyle{iclr2025_conference}

\include{supplementary}

\end{document}
