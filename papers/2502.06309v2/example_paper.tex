
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025preprint}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\input{mypackage.sty}
\input{mysymbol.sty}
\input{myacronym.sty}

\newcommand{\WKKT}{W^*_{\texttt{KKT}}}
\newcommand{\TT}{\texttt{Tiki-Taka}}
\newcommand{\AnalogGD}{\texttt{Analog\;GD}}
\newcommand{\AnalogSGD}{\texttt{Analog\;SGD}}
\newcommand{\DigitalSGD}{\texttt{Digital\;SGD}}
\newcommand{\AnalogUpdate}{\texttt{Analog\;Update}}
 

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\TC}[1]{{\color{orange}[\textbf{Q:} #1]}}


\icmltitlerunning{Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions}

\begin{document}

\doparttoc % 
\faketableofcontents % 

\twocolumn[
\icmltitle{Analog In-memory Training on General Non-ideal Resistive Elements: \\Understanding the Impact of Response Functions}




\begin{icmlauthorlist}
\icmlauthor{Zhaoxian Wu}{RPI}
\icmlauthor{Quan Xiao}{RPI}
\icmlauthor{Tayfun Gokmen}{IBM}
\icmlauthor{Omobayode Fagbohungbe}{IBM}
\icmlauthor{Tianyi Chen}{RPI}
\end{icmlauthorlist}

\icmlaffiliation{RPI}{Rensselaer Polytechnic Institute, 
Troy, NY 12180, US}
\icmlaffiliation{IBM}{IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, US}

\icmlcorrespondingauthor{Tianyi Chen}{chentianyi19@gmail.com}

\icmlkeywords{Analog AI; in-memory computing; stochastic gradient descent; stochastic optimization}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. 
    However, the training perspective, especially its training dynamic, is underexplored. 
    In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses.
    Among all the physical properties of resistive elements, the response to the pulses directly affects the training dynamics. 
    This paper first provides a theoretical foundation for gradient-based training on AIMC hardware and studies the impact of response functions.
    We demonstrate that noisy update and asymmetric response functions negatively impact \texttt{Analog\;SGD} by imposing an implicit penalty term on the objective. 
    To overcome the issue, \texttt{Tiki-Taka}, a residual learning algorithm, converges exactly to a critical point by optimizing a main array and a residual array bilevelly.
    The conclusion is supported by simulations validating our theoretical insights.
\end{abstract}


\vspace{-2em}
\section{Introduction}
\label{section:introduction}
The remarkable success of large vision and language models is underpinned by advances in modern hardware accelerators, such as GPU, TPU \citep{jouppi2023tpu}, NPU \citep{esmaeilzadeh2012neural}, and NorthPole chip \citep{modha2023neural}. However, the computational demands of training these models are staggering. For instance, training LLaMA \citep{touvron2023llama} cost \$2.4 million, while training GPT-3 \citep{brown2020language} required \$4.6 million, highlighting the urgent need for more efficient computing hardware. Current mainstream hardware relies on the Von Neumann architecture, where the physical separation of memory and processing units creates a bottleneck due to frequent and costly data movement between them.

In this context, the industry has turned its attention to \emph{analog in-memory computing (AIMC) accelerators} based on resistive crossbar arrays \citep{chen2013comprehensive, Y2020sebastianNatNano, haensch2019next, sze2017efficient}, which excel at accelerating the ubiquitous and computationally intensive \acp{MVM} operations.  In AIMC hardware, the weights (matrices) are represented by the conductance states of the \emph{resistive elements} in analog crossbar arrays \citep{burr2017neuromorphic, yang2013memristive}, while the input and output of MVM are analog signals like voltage and current. Leveraging Kirchhoff's and Ohm's laws, AIMC hardware achieves 10$\times$-10,000$\times$ energy efficiency than GPU \citep{jain2019neural,cosemans2019towards,papistas202122} in the model inference.

Despite its high efficiency, {\em analog training} is considerably more challenging than {\em inference} since it involves frequent weight updates.
Unlike digital hardware, where the weight increment can be applied to the original weight in the memory cell, the weights in AIMC hardware are changed by the so-called \emph{pulse update}. 
When receiving electrical pulses from its peripheral circuits, the resistive elements change their conductance as a response according to the pulse polarity \citep{gokmen2016acceleration}. 
However, the conductance of resistive elements increases and decreases following different response curves, leading to \emph{asymmetric update}. 

\begin{figure*}[t]
    \centering
    \includegraphics[height=4.1cm]{figs/pulse-update-ideal-cycle.pdf}
    \hfil
    \includegraphics[height=4.1cm]{figs/pulse-update-cycle.pdf}
    \caption{
        The weight's response curve. Positive and negative pulses are sent continuously on the left and right half, respectively. One pulse is sent to the resistive element per cycle. 
        Given $w$, the weight becomes $w^+$ or $w^-$ after one positive and negative pulse, respectively. 
        The response factors $q_+(w)$ and $q_-(w)$ are approximately the slope of the curve at $w$, and $\Delta w_{\min}$ is the response granularity.
        \textbf{(Left)} Ideal response functions $q_+(w)=q_-(w)$. Every point is symmetric points.
        \textbf{(Right)} Asymmetric response functions $q_{+}(w) \ne q_{-}(w)$ almost everywhere expect for the symmetric point $w^\diamond$.
    }
    \label{fig:pulse-update}
    \vspace{-0.8em}
\end{figure*}

Ideally, the response to each pulse should remain unchanged regardless of time or conductance state variation, which enables concise control of the weight update. 
A series of works seek various resistive elements that have near-constant or at least symmetric responses. The leading candidates currently include {PCM}~\citep{Burr2016,2020legalloJPD}, {ReRAM}~\citep{Jang2014, Jang2015, stecconi2024analog}, 
{ECRAM}~\citep{tang2018ecram, onen2022nanosecond}, to name a few.
Currently, there is still debate regarding the most suitable resistive element for AIMC hardware.
Anticipating to cover the wide ranges of resistive element candidates with various response functions, this paper studies \emph{generic response} under a set of mild assumptions. The key concept is the \emph{response function} of resistive elements. We are interested in how the resistive elements determine the response functions and how the response functions affect the convergence properties of analog training algorithms.

 


\subsection{Analog training and its challenges}
\textbf{Pulse update and asymmetric issue.}
In AIMC hardware, the weights of a model are stored in crossbar arrays by the conductance states of resistive elements.
To modify the conductance (namely the model weights), a series of electrical pulses need to be sent to resistive elements in consecutive pulse cycles; see Figure \ref{fig:pulse-update}.
Receiving a pulse at each pulse cycle, the conductance is updated by a small amount around $\Delta w_{\min}$, which is called \emph{response granularity}. 
However, resistive elements' response varies from the positive and negative directions, reflected by different slopes in their \emph{response curves}. The slopes, $q_+(w)$ and $q_-(w)$, is called \emph{response functions} in this paper.
All $\Delta w_{\min}$, $q_+(w)$, and $q_-(w)$ are element-dependence parameters or functions, which are set before the training process and hence kept fixed during the training.
The asymmetric update leads to an asymmetric response curve and an imperfect training dynamic.


Supported by pulse update, the gradient-based training algorithms are used to optimize the weights.
Consider a standard training problem with a model parameterized by $W\in\reals^D$
\begin{align}
    \label{problem}
    W^* := \argmin_{W\in\reals^{D}}~ f(W) := \mbE_{\xi}[f(W; \xi)]
\end{align}
where $f(\cdot):\reals^D\to\reals$ is the objective function and $\xi$ is a random variable sampled from an unknown distribution. 

\textbf{Gradient-based training implemented by analog update.}
Similar to stochastic gradient descent (SGD) in digital training (\DigitalSGD), the gradient-based training algorithm on AIMC hardware, \AnalogSGD, updates the weights by the gradient of the objective function. Let $\nabla f(W_k; \xi_k)$ be the stochastic gradient of $f(W_k; \xi_k)$ at $W_k$ with the random variable $\xi_k$. The SGD updates the weight by $W_{k+1} = W_k - \alpha \nabla f(W_k; \xi_k)$ with learning rate $\alpha$. \AnalogSGD~implements it by sending a pulse series with length about $|[\Delta W]_i| / \Delta w_{\min}$ to the $i$-th element, where $\Delta W = -\alpha \nabla f(W_k; \xi_k)$ is the \emph{desired update}. With each pulse incurring roughly by $\Delta w_{\min}$, $W_k$ is updated by $\Delta W$. This process involving multiple pulse cycles to apply desired update $\Delta W$ is called \AnalogUpdate.

However, it is observed that \AnalogSGD~suffers from a serious convergence issue due to the asymmetric update. To alleviate this issue, a family of heuristic variants, \TT, \citep{gokmen2020, gokmen2021, rasch2024fast} is proposed which introduces another crossbar array to accumulate the gradient.
Recently, \cite{wu2024towards} provides a theoretical justification for \TT~and shows that \TT~outperforms \AnalogSGD~by eliminate the asymptotic error in \AnalogSGD~caused by the asymmetric update. 
However, their work is limited to a special case of \emph{linear response}, which are in the form of $q_{+}(w) = 1 - w/{\tau}, q_{-}(w) = 1 + w/{\tau}$ with hardware-specific parameter $\tau>0$. 
Given more general $q_{+}(w)$ and $q_{-}(w)$, the convergence of \TT~does not trivially holds.


\textbf{Challenges with generic response functions.}
It is not necessary for \TT~to outperform \AnalogSGD~on generic resistive elements, even though the response functions are still linear. Consider a more generic linear response setting $q_{+}(w) = (1+c_{\texttt{Lin}})(1 - w/{\tau}), q_{-}(w) = (1-c_{\texttt{Lin}})(1 + w/{\tau})$ with a parameter $c_{\texttt{Lin}}$, which reduces to the setting in \cite{wu2024towards} when $c_{\texttt{Lin}}=0$.
The modification is slight, but it harms the convergence of \TT~significantly.


\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/TT-fails.pdf}
    \vspace{-0.8em}
    \caption{Objective comparison of \AnalogSGD~and \TT~under different parameter $c_{\texttt{Lin}}$. 
    The error plateau in the order $10^{-5}$ comes from the limited response granularity $\Delta w_{\min} = 10^{-4}$.
    }
    \label{fig:TT-fails}
    \vspace{-1em}
\end{figure}
Figure \ref{fig:TT-fails} shows the damage from a non-zero $c_{\texttt{Lin}}$ to \TT. Consistent with the conclusion in \cite{wu2024towards}, \TT~significantly outperforms \AnalogSGD~\allowbreak when $c_{\texttt{Lin}}=0$. However, when $c_{\texttt{Lin}}$ is perturbed from $0.1$ to $0.3$, \TT~degrades dramatically and even becomes worse than \AnalogSGD~does, indicating the non-negligible impact of the response functions. 
Therefore, there is no convergence guarantee for \TT~on generic responses, even with linear response.
It motivates us to study the condition of the response functions for convergence.


\subsection{Main results}
\label{section:main-results}
Complementing existing empirical studies in analog in-memory computing, this paper aims to build a rigorous foundation of analog training with generic response functions.
This paper builds upon the rich study of various resistive elements, including PCM, ReRAM, and so on, where the behavior of the resistive element at each pulse cycle is well studied. 
However, at the algorithm level, we are more interested in the dynamic of \AnalogUpdate, which describes the process of applying a desired update $\Delta W$ to the weight $W$ implemented by sending pulse series with a proper number.
\AnalogUpdate~hides the details on the management of pulse series and enables us to study the convergence behavior easily.
It raises a natural question:
\begin{center}
    \textbf{Q1)} {\em How do the resistive elements determine the dynamic of Analog Update?}
\end{center}
This paper finds that the response functions are the key to connecting pulse update and \AnalogUpdate.
We start by studying the behavior of the resistive element at each pulse cycle. Based on that, we propose an approximated dynamic of \AnalogUpdate~and analyze the approximation error.

The response functions are the main contributor to the difference between digital and analog training. It motivates us to study a class of benign response functions friendly to the convergence of \TT~and raises another question:
\begin{center}
    \textbf{Q2)} {\em What is the impact of generic response functions on analog training? }
\end{center}

To answer the question above, we establish the convergence of \AnalogSGD~and \TT~on generic response functions based on the proposed dynamic.

\textbf{Our contributions.}  
The contributions of the paper include:
\begin{enumerate}[topsep=0.em]
    \itemsep-0.1em 
    \item [\bf C1)] 
    Building upon the equation of pulse update, we propose an approximated dynamic of \AnalogUpdate~which hides non-essential details in the resistive element level while remaining mathematically tractable. Enabled by it, we study the impact of response functions directly without being limited to concrete element candidates.

    \item [\bf C2)] Based on critical physical properties, we focus on a class of benign response functions. Based on that, we show that analog training suffers an implicit penalty. It attracts the weights towards symmetric points and causes inexact convergence of \AnalogSGD. 

    \item [\bf C3)] We demonstrate the method to alleviate the asymmetric update and implicit penalty issues by residual learning, which introduces another sequence, called \emph{residual array}, with stationary point $0$. This method leads to \TT~heuristically proposed in \cite{gokmen2020}. By properly zero-shifting so that the stationary and symmetric points overlap, \TT~provably converges to a critical point.
\end{enumerate}


\section{Dynamic of Non-ideal Analog Update}
\label{section:analog-dynamic}
The primary distinction between digital and analog training is the weight update method. As discussed in Section \ref{section:introduction}, the weight update in AIMC hardware is implemented by \AnalogUpdate, which sends a series of pulses to the resistive elements. 
This section provides an approximated dynamic of \AnalogUpdate~which focuses on the essential details in the resistive element level.



\textbf{Pulse update.}
Consider the response of one resistive element in one cycle, which involves only one pulse.
Given the initial weight $w$, the updated weight increases or decreases by about $\Delta w_{\min}$ depending on the pulse polarity, where $\Delta w_{\min} > 0$ is the \emph{response granularity} determined by elements. The granularity is further scaled by a factor, which varies by the update direction due to the \emph{asymmetric update} property of resistive elements. The notations $q_{+}(\cdot)$ and $q_{-}(\cdot)$ are used to denote the \emph{response functions} on positive or negative sides, respectively, to describe the dominating part of the factor. 
In practice, the analog noise also causes a deviation of the effective factor from the response functions, referred to as \emph{cycle variation}. It is represented by the magnitude $\sigma_c$ times a random variable $\xi_c$ with expectation $0$ and variance $1$. 
Taking all of them into account, with $s\in\{+, -\}$ being the update direction, the updated weight after receiving one pulse is $\tdU_q(w, s)$ where $\tdU_q(\cdot, \cdot) : \reals\times\{+,-\}\to\reals$ is the element-dependent update that implements the resistive element, which can be expressed as
\begin{align}
    \label{analog-pulse-update}
    \tdU_q(w, s) :=&\ 
    w + \Delta w_{\min} \cdot (q_{s}(w) + \sigma_c\xi) \\
    =&\ 
    \begin{cases}
    w + \Delta w_{\min} \cdot (q_{+}(w)+\sigma_c\xi_c),~~~s=+, \\
    w - \Delta w_{\min} \cdot (q_{-}(w)+\sigma_c\xi_c),~~~s=-.
    \end{cases}
    \nonumber
\end{align}
The typical signal and noise ratio $\sigma_c / q_s(w)$ is roughly 5\%-100\% \citep{gong2018signal, stecconi2024analog}, varied by the type of resistive elements.
Furthermore, the response functions also vary by elements due to the imperfection in fabrication, called \emph{element variation} (also referred to as \emph{device variation} in literature \cite{gokmen2016acceleration}).

Equation \eqref{analog-pulse-update} is a resistive element level equation. 
Existing work exploring the candidates of resistive elements usually reports the response curves similar to \Cref{fig:pulse-update}, \citep{gong2022deep, tang2018ecram, stecconi2024analog}.
Taking the difference between weights in two consecutive pulse cycles and adopting statistical approaches \cite{gong2018signal}, all the element-dependent quantities, including $\Delta w_{\min}$, $q_{+}(\cdot)$, $q_{-}(\cdot)$ and $\sigma_c$, can be estimated from the response curves of the resistive elements.


\textbf{Analog update implemented by pulse updates.} Even though the update scheme has evolved over the years \citep{gokmen2016acceleration,gokmen2017cnn}, we discuss a simplified version, called \AnalogUpdate, to retain the essential properties. To update the weight $w$ by $\Delta w$, a series of pulses are sent, whose \emph{bit length (BL)} is computed by $\operatorname{BL} := \left\lceil \frac{|\Delta w|}{\Delta w_{\min}}\right\rceil$.
After received $\operatorname{BL}$ pulses, the updated weight $w'$ can be expressed as the function composition of \eqref{analog-pulse-update} by $\operatorname{BL}$ times
\begin{align}
    \label{recursion:asymmetric-update-implementation}
    w' = \underbrace{\tdU_q\circ \tdU_q \circ\cdots\circ \tdU_q}_{{\times \operatorname{BL}}}(w,s) =: \tdU^{\operatorname{BL}}_q(w,s).
\end{align}
Roughly speaking, given an ideal response $q_+(w)=q_-(w)=1$ and $\sigma_c=0$, $\operatorname{BL}$ pulses, with $\Delta w_{\min}$ increment for each individual pulse, incur the weight update $\Delta w$. Since the response granularity $\Delta w_{\min}$ is scaled by the response function $q_s(w)$, the expected increment is approximately scaled by $q_s(w)$ as well. Accordingly, we propose an approximate dynamic of \AnalogUpdate~is given by $w' \approx U_q(w, \Delta w)$, where $U_q(w, \Delta w)$ is defined by
\begin{align}
    \label{analog-update}
    U_q(w, \Delta w) := \begin{cases}
        w + \Delta w \cdot q_{+}(w),~~~\Delta w \ge 0, \\
        w + \Delta w \cdot q_{-}(w),~~~\Delta w < 0.
    \end{cases}
\end{align}
The following theorem provides estimation of the approximation error. 
It has been shown empirically that the response granularity can be made sufficiently small for updating \citep{rao2023thousands, sharma2024linear}, implying $\Delta w_{\min}\ll \Delta w$.  Therefore, we establish the error estimation of the approximation based on small response granularity condition.

\begin{restatable}[Error from discrete pulse update]{theorem}{ThmPulseUpdateError} \label{theorem:pulse-update-error}
    Suppose the response granularity is sufficiently small such that $\Delta w_{\min}\le o(\Delta w)$.
    With the update direction $s=\sign(\Delta w)$, the error between the true update $\tdU^{\operatorname{BL}}_q(w, s)$ and the approximated $U_q(w, \Delta w)$ is bounded by
    \begin{align}
        \lim_{\Delta w\to 0}\frac{|\tdU^{\operatorname{BL}}_q(w, s) - U_q(w, \Delta w)|}{|\tdU^{\operatorname{BL}}_q(w, s)-w|}
        = 0.
    \end{align}
\end{restatable}
The proof of Theorem \ref{theorem:pulse-update-error} is deferred to Appendix \ref{section:proof-pulse-update-error}. 
In Theorem \ref{theorem:pulse-update-error}, $|\tdU^{\operatorname{BL}}_q(w, s) - U_q(w, \Delta w)|$ is the error between the true update and the proposed dynamic, while $|\tdU^{\operatorname{BL}}_q(w, s)-w|$ is the difference between original weight and the updated one.
Theorem \ref{theorem:pulse-update-error} shows that the proposed dynamic dominates the update, and the approximation error is negligible when $\Delta w$ is small, which holds as $\Delta w$ always includes a small learning rate in gradient-based training.



\begin{tcolorbox}[emphblock]
    \textbf{Takeaway.}
    Theorem \ref{theorem:pulse-update-error} enables us to discuss the impact of response functions directly without dealing with element-specific details like update granularity $\Delta w_{\min}$ and cycle variation $\sigma_c$.
    Response functions are the bridge between the resistive element level equation (pulse update \eqref{analog-pulse-update}) and algorithm level equation (dynamic of \AnalogUpdate~\eqref{analog-update}).
\end{tcolorbox}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/response-example.pdf}
    \vspace{-0.2cm}
    \caption{Examples of response functions from \Cref{assumption:response-factor}; $w^\diamond$ is the symmetric point.
    }
    \label{fig:response-factor}
    \vspace{-0.4cm}
\end{figure*}


\textbf{Compact formulations of vector. }
The update \eqref{analog-update} holds at each resistive element, while the model $W$ contains $N$ resistive elements with different response functions $q_+(\cdot)$ and $q_-(\cdot)$. 
We stack all the weights $w_k$ and expected increment $\Delta w_k$ together into vectors $W_k, \Delta W_k\in\reals^D$, where $k$ is the iteration index\footnote{This paper adopts $w$ to represent the element of the weight $W_k$ without specifying its index. The notation makes the formulations more concise and indicates that all elements are updated in parallel.}.
Similarly, the response functions $q_+(\cdot)$ and $q_-(\cdot)$ are stacked into $Q_+(\cdot)$ and $Q_-(\cdot)$, respectively. 
Let the notation $U_Q(W_k, \Delta W)$ on matrices $W_k$ and $\Delta W$ denote the element-wise operation on
$W_k$ and $\Delta W$, i.e. $[U_Q(W_k, \Delta W)]_i := U_{[Q]_i}([W_k]_i, [\Delta w]_i), \forall i\in[D]$ with $[D]:=\{1, 2, \cdots, D\}$ denoting the index set.
The element-wise update \eqref{analog-update} can be expressed as $W_{k+1} = U_Q(W_k, \Delta W_k)$. Leveraging the symmetric decomposition \citep{gokmen2020,wu2024towards}, we decompose $Q_{-}(W)$ and $Q_{+}(W)$ into symmetric component $F(\cdot)$ and asymmetric component $G(\cdot)$
\begin{align}
    \label{definition:F-G}
    F(W) := (Q_{-}(W)+Q_{+}(W))/2, \\
    G(W) := (Q_{-}(W)-Q_{+}(W))/2,
\end{align}
which leads to a compact form of the \AnalogUpdate
\begin{tcolorbox}[emphblock, width=1\linewidth]
    \small
    \vspace{-1\baselineskip}
    \begin{align}
        \label{biased-update}
        W_{k+1} = W_k + \Delta W_k \odot F(W_k) -|\Delta W_k| \odot G(W_k)
    \end{align}
\end{tcolorbox}
\noindent where $|\cdot|$ and $\odot$ represent the element-wise absolute value and multiplication, respectively.

\textbf{Gradient-based training algorithms on AIMC hardware.} 
In \eqref{biased-update}, the desired update $\Delta W_k$ varies based on different algorithms. Replacing $\Delta W_k$ with the stochastic gradient $\nabla f(W_k; \xi_k)$, we obtain the dynamic of \AnalogSGD~as
\begin{align}
    \label{recursion:analog-SGD}
    W_{k+1} =&\ W_k - \alpha \nabla f(W_k; \xi_k)\odot F(W_k)
    \\
    &\ - \alpha|\nabla f(W_k; \xi_k)|\odot G(W_k)
    \nonumber
\end{align}
where $\alpha>0$ is the learning rate and $\xi_k$ is the noise introduced at time $k$. The noise can come from both data sampling and analog noise, like thermal or shot noise.



\textbf{Saturation.} To avoid reaching arbitrarily high or low conductance states, resistive elements get \emph{saturated} when they keep receiving the same pulses. Constrained by that, the conductance of the resistive element with $q_+(\cdot)$ and $q_-(\cdot)$ is bounded by a \emph{dynamic range} $[\tau^{\min}, \tau^{\max}]$ where $\tau^{\min}$ and $\tau^{\max}$ are the saturation points with zero responses, i.e. $q_+(\tau^{\max})=q_-(\tau^{\min})=0$. Near the saturation points, the asymmetric issue is significant, and thus, the update in one direction is suppressed, considerably impacting the convergence.
On the contrary, if a point $w^\diamond$ satisfies $q_{-}(w^\diamond)=q_{+}(w^\diamond)$, the analog update behaves like a digital update.
We refer to $w^\diamond$ as \emph{symmetric point}. Symmetric points are typically located in the interior of the dynamic range and are far from saturation. 

Stacking all $w^\diamond$ into a vector $W^\diamond\in\reals$. Observe that the function $G(W)$ is large near the saturation points while almost zero around $W^\diamond$, implying it can reflect the degree of saturation. At the same time, $F(W)$ is the average of the response functions in two directions. 
As we will see in Sections \ref{section:convergence-ASGD} and \ref{section:TT}, the ratios $\frac{G(W)}{\sqrt{F(W)}}$ plays a critical role in the convergence behaviors.

\textbf{Response function class.}
Since the behavior of resistive elements is always governed by physical laws, we are interested in a class of response functions that reflect some physical properties.
First, the conductance ends up increasing when receiving a positive pulse and vice versa, leading to positive response functions. 
On top of that, we also assume the response functions are differentiable (and hence continuous) for mathematical tractability.
Taking them and conductance saturation into account, we are interested in the following response function class.
\begin{definition}[Response function class]
    \label{assumption:response-factor}
    $q_+(\cdot)$ and $q_-(\cdot)$ with dynamic range $[\tau^{\min}, \tau^{\max}]$ satisfy
    \vspace{-0.5em}
    \begin{itemize}
        \setlength{\itemsep}{-0.1em}
        \item \textbf{(Positive-definiteness)} 
        $q_+(w) > 0, \forall w < \tau^{\max}$ and $q_-(w) > 0, \forall w > \tau^{\min}$;
        \item \textbf{(Differentiable)} $q_+(\cdot)$ and $q_-(\cdot)$ are differentiable;
        \item \textbf{(Saturation)} $q_+(\tau^{\max})=q_-(\tau^{\min})=0$.
    \end{itemize}
\end{definition}
\Cref{assumption:response-factor} covers a wide range of response functions.
Figure \ref{fig:response-factor} showcases three examples from the response functions class, including linear, non-linear monotonic, and even non-monotonic functions.

\section{Implicit Penalty and Inexact Convergence of \AnalogSGD}
As a critical impact from the response functions, an implicit penalty term is applied to the objective, leading to an asymptotic error in \AnalogSGD.

\subsection{Implicit penalty}
\label{section:implicit-regularization}
We first give an intuition through a situation where $W_k$ is already a critical point, i.e., $\mbE_\xi[\nabla f(W_k; \xi)]=0$.
Recall that stochastic gradient descent on digital hardware (\DigitalSGD) is stable in expectation, i.e.
\begin{align}
    \mbE_{\xi_k}[W_{k+1}] = W_k - \mbE_{\xi_k}[\alpha \nabla f(W_k; \xi_k)] = W_k.
\end{align}
However, this does not work for \AnalogSGD
\begin{align}
    \label{equality:analog-SGD-drift}
    &\ \mbE_{\xi_k}[W_{k+1}] =W_k - \mbE_{\xi_k}[\alpha \nabla f(W_k; \xi_k)\odot F(W_k) 
    \nonumber \\
    &\ \hspace{6em}
    - \alpha|\nabla f(W_k; \xi_k)|\odot G(W_k)] \\
    =&\ W_k - \alpha\mbE_{\xi_k}[|\nabla f(W_k; \xi_k)|\odot G(W_k)]
    \ne W_k.
    \nonumber
\end{align}
Consider a simplified version that the weight is a scalar ($D=1$) and the function $G(W)$ is strictly monotonically decreasing\footnote{It happens when both $q_+(\cdot)$ and $q_-(\cdot)$ are strictly monotonic.} to help us gain intuition on the impact of the drfit in \eqref{equality:analog-SGD-drift}.
Recall $G(W^\diamond)=0$ at the symmetric point $W^\diamond$. $G(W) > 0$ when $W > W^\diamond$ and $G(W) < 0$ otherwise. Consequently, \eqref{equality:analog-SGD-drift} indicates that $\mbE_{\xi_k}[W_{k+1}] < W_k$ when $W_k > W^\diamond$ and $\mbE_{\xi_k}[W_{k+1}] > W_k$ otherwise.
Consequently, $W_k$ suffers from a drift tendency towards $W^\diamond$.
In addition, it can be speculated that the penalty coefficient supposes to be proportional to the noise level since the drift is proportional to $\mbE_{\xi_k}[|\nabla f(W_k; \xi_k)|]$, which is the first moment of noise $\mbE_{\xi_k}[|\nabla f(W_k; \xi_k)-\mbE_{\xi}[\nabla f(W_k; \xi)]|]$ in essence.

The following theorem formalizes the implicit penalty effect. Before that, we define an accumulated asymmetric function $R_c(\cdot) : \reals^D\to\reals^D$, whose gradient is $R(W) := \frac{G(W)}{F(W)}$, i.e. $\nabla R_c(W) = R(W) = \frac{G(W)}{F(W)}$.
Consequently, if $R(W)$ is strictly monotonic, $R_c(W)$ reaches its minimum at the symmetric point $W^\diamond$ where $R(W^\diamond)=0$ so that it penalizes the weight away symmetric point.
\begin{theorem}[Implicit penalty, short version]
    \label{theorem:implicit-regularization-short}
    Suppose $\mbE_{\xi_k}[|\nabla f(W_k; \xi_k)-\mbE_{\xi}[\nabla f(W_k; \xi)]|]$ is a constant $\Sigma\in\reals^D$ and let $D=1$. 
    \AnalogSGD~implicitly optimizes the following penalized objective
    \begin{align}
        \label{problem:implicit-regularization-short}
    	\min_{W} ~ f_{\Sigma}(W) := f(W) + \la \Sigma, R_c(W)\ra.
    \end{align}
\end{theorem}
The full version of Theorem \ref{theorem:implicit-regularization-short} and its proof are deferred to Appendix \ref{section:proof-implicit-regularization}. 
In Theorem \ref{theorem:implicit-regularization-short}, $R_c(W)$ plays the role of penalty to force the weight toward a symmetric point. 
As shown in Appendix \ref{section:proof-implicit-regularization}, $R_c(W)$ has a simple expression on linear response functions when $c_{\texttt{Lin}}=0$, leading \eqref{problem:implicit-regularization-short} to
\begin{align}
    \label{problem:implicit-regularization-linear-device}
    \min_{W} ~ f_{\Sigma}(W) := f(W) + \frac{\Sigma}{2\tau}\|W\|^2
\end{align}
which has the objective with $\ell_2$ regularization. In addition, the implicit penalty has a coefficient proportional to the noise level $\Sigma$ and inversely proportional to the dynamic range $\tau$. It implies that the implicit penalty becomes active only when gradients are noisy, and it is amplified when the noise is large.


\begin{tcolorbox}[emphblock]
    \textbf{Implicit penalty.} 
    When the gradient is noisy, an implicit penalty attracts \AnalogSGD~toward symmetric points. 
\end{tcolorbox}


\subsection{Inexact Convergence of \AnalogSGD}
\label{section:convergence-ASGD}
Due to the implicit penalty, \AnalogSGD~only converges to a critical point inexactly.
Before showing that, We introduce a series of assumptions on the objective, as well as noise.
\begin{assumption}[Objective]
    \label{assumption:Lip}
    The objective $f(W)$ is $L$-smooth and is lower bounded by $f^*$
\end{assumption}


\begin{assumption}[Unbiasness and bounded variance]
    \label{assumption:noise}
    The sample $\{\xi_k : k\in[K]\}$ are independently and identically sampled from a distribution over times $k\in[K]$. Furthermore, the stochastic gradient is unbiased and has bounded variance, i.e., $\mbE_{\xi_k}[\nabla f(W_k;\xi_k)] = \nabla f(W_k)$ and $\mbE_{\xi_k}[\|\nabla f(W_k;\xi_k)-\nabla f(W_k)\|^2]\le\sigma^2$. 
\end{assumption}

\begin{figure*}[t]
    \footnotesize
    \centering
    \includegraphics[width=.44\linewidth]{figs/A03-MNIST-FCN-Pow.pdf}
    \includegraphics[width=0.44\linewidth]{figs/A03-MNIST-CNN-Pow.pdf}
    \vspace{-0.4cm}
    \caption{The test accuracy curves for the model training on MNIST dataset under different $\tau$; \textbf{(Left)} FCN. \textbf{(Right)} CNN. 
    }
    \label{figure:FCN-CNN-MNIST}
    \vspace{-0.6cm}
\end{figure*}

Assumption \ref{assumption:Lip}--\ref{assumption:noise} are standard in non-convex optimization \citep{bottou2018optimization, wu2024towards}. 
Additionally, similar to the setting in \cite{wu2024towards}, we also assume that the saturation degree is bounded, given by a lower bound of response functions.
\begin{assumption}[Bounded saturation]
    \label{assumption:bounded-saturation}
    There exists a constant $H_{\min} > 0$ such that 
    $\min\{Q_+(W)\odot Q_-(W)\}\! >\! H_{\min}$.
\end{assumption}
Assumption \ref{assumption:bounded-saturation} requires that $W_k$ remains far from saturation points, which is a mild assumption in actual training. 
This paper considers the average gradient square norm as the convergence metric, given by
$E_K := \frac{1}{K}\sum_{k=0}^{K-1} \lnorm \nabla f(W_k)\rnorm^2$.
Now, we establish the convergence of \AnalogSGD.
\begin{restatable}[Inexact convergence of \AnalogSGD]{theorem}{ThmASGDConvergenceNoncvx}
    \label{theorem:ASGD-convergence-noncvx}
    Under Assumption 
    \ref{assumption:Lip}--\ref{assumption:bounded-saturation}, 
    if the learning rate is set as 
    $\alpha = O(1/\sqrt{K})$, 
    it holds that
    \begin{align}
    \label{inequality:ASGD-convergence-noncvx}
    E_K
    \le&\ O\lp\sqrt{{\sigma^2}/{K}}
    + \sigma^2 S^{\texttt{ASGD}}_K\rp
    \end{align}
    where $S^{\texttt{ASGD}}_K$ denotes the amplification factor given by
    $S^{\texttt{ASGD}}_K := \frac{1}{K}\sum_{k=0}^{K-1}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty$.
\end{restatable}
The proof of Theorem \ref{theorem:ASGD-convergence-noncvx} is deferred to Appendix \ref{section:proof-ASGD-convergence-noncvx}. Theorem \ref{theorem:ASGD-convergence-noncvx} suggests that the convergence metric $E_K$ is upper bounded by two terms: the first term vanishes at a rate of 
$O(\sqrt{{\sigma^2}/{K}})$,
which matches the \DigitalSGD's convergence rate \citep{bottou2018optimization} up to a constant;
the second term contributes to the \emph{asymptotic error} of \texttt{Analog\;SGD}, which does not vanish with the number of iterations $K$. 
 
\textbf{Impact of saturation/asymmetric update.} 
The exact expression of $S^{\texttt{ASGD}}_K$ depends on the specific noise distribution and thus is difficult to reach. However, $S^{\texttt{ASGD}}_K$ reflects the saturation degree near the critical point $W^*$ when $W_k$ converges to a neighborhood of $W^*$. If $W^*$ is far from the symmetric point $W^\diamond$, $S^{\texttt{ASGD}}_K$ becomes large, leading to a large $E_K^{\texttt{ASGD}}$ and a large asymptotic error. In contrast, if $W^*$ remains close to the symmetric point $W^\diamond$, the asymptotic error is small.



 

\section{Mitigating Implicit Penalty by Residual Learning: \TT}
\label{section:TT}

The asymptotic error in \AnalogSGD~is a fundamental issue that arises from the mismatch between the symmetric point and the critical point. 
An idealistic remedy for the inexact convergence is carefully shifting the weights to ensure the stationary point is close to a symmetric point. However, determining the appropriate adjustment is always challenging, as the critical point is difficult to pinpoint before the actual training. 
Therefore, an ideal solution to address this issue is to jointly construct a sequence with a predictable stationary point and a proper shift of the symmetric point.

\textbf{Residual learning.} Our solution overlaps the algorithmic stationary point and physical symmetric point on the special point $0$. Besides the main analog array, $W_k$, we maintain another array, $P_k$, whose stationary point is $0$. A natural choice is the \emph{residual} of the weight given by the solution of the following problem
\begin{align}
	\label{problem:P-residual}
    P^*(W) \in \argmin_{P\in\reals^D} ~f(W + \gamma P)
\end{align}
where $\gamma\ge 0$ is a mixing coefficient.
Noticing that $0$ is a solution of \eqref{problem:P-residual} if $W$ is already a solution of \eqref{problem}. Furthermore, the gradient of $f(W + \gamma P)$ with respect to $P$, given by $\nabla_P f(W + \gamma P) = \gamma \nabla f(W + \gamma P)$, is accessible with fair expense, enabling us to introduce a sequence $P_k$ to track the residual of $W_k$ by optimizing \eqref{problem:P-residual}
\begin{align}
	\label{recursion:HD-P}
    P_{k+1} = &\ P_k - \alpha\nabla f(\bar W_k; \xi_k)\odot F(P_k) \\
    &\ - \alpha|\nabla f(\bar W_k; \xi_k)|\odot G(P_k).
    \nonumber
\end{align}
Ideally, it holds that $P_k\approx P^*(W_k)$ after \eqref{recursion:HD-P}, and hence it can be expected that $f(W_k+\gamma P_k) \le f(W_k)$ so that one step of transfer leads to descent on $f(W_k)$ 
\begin{align} 
    \label{recursion:HD-W}
    W_{k+1} =&\ W_k + \beta P_{k+1}\odot F(W_k) - \beta|P_{k+1}|\odot G(W_k)
\end{align}
which solves the following problem by approximating the gradient $P^*(W)$ by $P_k$
\begin{align}
    \label{problem:W}
    \argmin_{W\in\reals^D} ~\|P^*(W)\|^2.
\end{align}
The updates \eqref{recursion:HD-P} and \eqref{recursion:HD-W} are performed alternatively until convergence\footnote{In principle, $F(\cdot)$ and $G(\cdot)$ vary from $W_k$ and $P_k$. We adopt the same notations for them just for convenience.}. By solving a bilevel optimization problem \eqref{problem:P-residual} and \eqref{problem:W} approximately, our solution recovers the update of \TT~\cite{gokmen2020}.

On the response functions side, it is naturally required to let zero be a symmetric point, i.e. $G(0)=0$, which can be implemented by zero-shifting technique \citep{kim2019zero} by subtracting a reference array.






\textbf{Convergence properties of \texttt{Tiki-Taka}.}
We begin by analyzing the convergence of \TT~without considering the zero-shift first, enabling us to understand how the zero-shift response function benefit the convergence.

If the optimal point $W^*$ exists and is unique, the solution of \eqref{problem:P-residual} has a closed form $P^*(W) := \frac{W^*-W}{\gamma}$.
At that time, the objective of \eqref{problem:W} is equivalent to $\|W^*-W\|^2$.
However, the solutions of \eqref{problem:P-residual} and \eqref{problem:W} are non-unique in general, especially for non-convex objectives with multiple local minima.
To ensure the existence and uniqueness of $W^*$, we assume the objective is strongly convex.
\begin{assumption}[$\mu$-strong convexity]
\label{assumption:strongly-cvx}
The objective $f(W)$ is $\mu$-strongly convex. 
\end{assumption}
Under the strongly convex assumption, the optimal point $W^*$ is unique. We believe the requirement of strong convexity is non-essence, and the proof can be extended to more general cases, which is left for future work.

Involving two sequences $W_k$ and $P_k$, \TT~converges in different senses, including:
(a) the residual array $P_k$ converges to the optimal point $P^*(W_k)$ of \eqref{problem:P-residual};
(b) $W_k$ converges to the critical point of \eqref{problem:W} or the optimal point $W^*$;
(c) the sum $\bar W_k = W_k + \gamma P_k$ converges to a critical point where $\nabla f(\bar W_k)$. 
Taking all these into account, we define the convergence metric as
\begin{align}
    E^{\texttt{TT}}_K := 
    \frac{1}{K}\sum_{k=0}^{K-1}&\ \mbE\bigg[
    \|\nabla f(\bar W_k)\|^2
    +O(\lnorm P_k - P^*(W_k)\rnorm^2)
    \nonumber
    \\
    &\ 
    + O(\|W_k-W^*\|^2)
    \bigg].
\end{align}
For simplicity, the constants in front of some terms in $E^{\texttt{TT}}_K$ are hidden. 
Now, we provide the convergence of \TT~with generic responses.
\begin{table*}[thb]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
        \toprule
         & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c}{CIFAR100} \\ 
         \cmidrule{2-4}\cmidrule{5-7}
         & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} 
         & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} \\ 
        \midrule
        ResNet18  & 95.43\stdv{$\pm$0.13} & 84.47\stdv{$\pm$3.40} & 94.81\stdv{$\pm$0.09} & 81.12\stdv{$\pm$0.25} & 68.98\stdv{$\pm$1.01} & 76.17\stdv{$\pm$0.23} \\
        ResNet34  & 96.48\stdv{$\pm$0.02} & 95.43\stdv{$\pm$0.12} & 96.29\stdv{$\pm$0.12} & 83.86\stdv{$\pm$0.12} & 78.98\stdv{$\pm$0.55} & 80.58\stdv{$\pm$0.11} \\
        ResNet50  & 96.57\stdv{$\pm$0.10} & 94.36\stdv{$\pm$1.16} & 96.34\stdv{$\pm$0.04} & 83.98\stdv{$\pm$0.11} & 79.88\stdv{$\pm$1.26} & 80.80\stdv{$\pm$0.22} \\
        \bottomrule
    \end{tabular}
    \caption{Analog training with the \emph{power response} for fine-tuning task on CIFAR10/100 using models from ResNet family. The test accuracy is reported. \texttt{DSGD}, \texttt{ASGD}, and \texttt{TT} represent \DigitalSGD, \AnalogSGD, \TT, respectively.}
    \label{table:CIFAR-fine-tune-pow}
    \vspace{-0.5cm}
\end{table*}

\begin{restatable}[Convergence of \TT]{theorem}{ThmTTConvergenceScvx}
    \label{theorem:TT-convergence-scvx}
    Under Assumptions
    \ref{assumption:Lip}--\ref{assumption:bounded-saturation}, and \ref{assumption:strongly-cvx}, 
    with learning rate 
    $\alpha=O\lp \sqrt{{1}/{\sigma^2K}}\rp$, $\beta=O(\alpha\gamma^{3/2})$,
    it holds for \TT~that
    \begin{equation}
        E^{\texttt{TT}}_K
        \le \ 
        O\lp\sqrt{{\sigma^2}/{K}}
        + \sigma^2 S^{\texttt{TT}}_K\rp
    \end{equation}
    \vskip -0.8em\noindent
    where $S^{\texttt{TT}}_K$ denotes the amplification factor of $P_k$ given by
    $S^{\texttt{TT}}_K := \frac{1}{K}\sum_{k=0}^{K}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty$.
\end{restatable}
The proof of Theorem \ref{theorem:TT-convergence-scvx} is deferred to Appendix \ref{section:proof-TT-convergence-scvx}. 
Theorem \ref{theorem:TT-convergence-scvx} claims that \TT~converges at the rate $O\lp\sqrt{{\sigma^2}/{K}}\rp$ to a neighbor of critical point with radius $O(\sigma^2 S^{\texttt{TT}}_K)$, which share almost the same expression with the convergence of \AnalogSGD. The difference lies in the amplification factor $S^{\texttt{TT}}_K$ and $S^{\texttt{ASGD}}_K$, where the former depends on $P_k$ while the latter depends on $W_k$. 
\begin{tcolorbox}[emphblock]
    \textbf{Impact of response functions.} Response function affects the \AnalogSGD~and \TT~similarly. However, attributed to the residual array, constructing response functions to enable exact convergence of \TT~is feasible.
\end{tcolorbox}
As we have discussed, $P_k$ tends to $P^*(W_k)$ which tends to $0$ given $W_k$ tends to $W^*$. Resistive elements with response functions such that $G(P)=0$ when $P=0$ is required for the exact convergence.
\begin{assumption}
    \label{assumption:zero-sp}
    \textbf{(Zero-shifted symmetric point)} $P=0$ is a symmetric point, i.e. 
    $G(0)=0$.
\end{assumption}
Under it and the Lipschitz continuity of the response functions, it holds directly that $\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm_\infty \le L_S \|P_k\|_\infty$ for a constant $L_S\ge 0$. Consequently, when $P_k\to P^*(W_k) \to 0$ as $W_k\to W^*$, the asymptotic error disappears. Formally, the following corollary holds true.
\begin{restatable}[Exact convergence of \TT]{corollary}{ThmTTConvergenceScvxFinal}
    \label{theorem:TT-convergence-scvx-final}
    Under Assumption \ref{assumption:zero-sp} and the conditions in Theorem \ref{theorem:TT-convergence-scvx}, if 
    $\gamma \ge \Omega(H_{\min}^{-1/5})$,
    it holds that $E^{\texttt{TT}}_K
        \le \ O\lp \sqrt{{\sigma^2L}/{K}}\rp$.
\end{restatable}
The proof of Corollary \ref{theorem:TT-convergence-scvx-final} is deferred to Appendix \ref{section:proof-TT-convergence-scvx-final}. 
Corollary \ref{theorem:TT-convergence-scvx-final} demonstrates the failure of \TT~in Figure \ref{fig:TT-fails}. The symmetric point is $w^\diamond = c_{\texttt{Lin}}\tau$ in this example, which violates Assumption \ref{assumption:zero-sp} when $c_{\texttt{Lin}}\ne 0$ and hence introduces asymptotic error into \TT.


 


\section{Numerical Simulations}
\label{section:experiments}
In this section, we verify the main theoretical results by simulations on both synthetic datasets and real datasets.
We use the open source toolkit \ac{AIHWKIT}~\citep{Rasch2021AFA} to simulate the behaviors of \AnalogSGD~and \TT. 
Each simulation is repeated three times, and the mean and standard deviation are reported. 
More details can be referred to in Appendix \ref{section:experiment-setting}.

We consider two types of response functions in our simulations: power and exponential response functions with dynamic ranges 
$[-\tau, \tau]$,
while the symmetric point is 0, as required by Corollary \ref{theorem:TT-convergence-scvx-final}. 

 
 
\textbf{MNIST FCN/CNN.} 
We train fully-connected network (FCN) and convolution neural network (CNN) on the MNIST dataset and compare the performance of \texttt{Analog\;SGD} and \texttt{Tiki-Taka} under various $\tau$ on power responses; see the results in Figure \ref{figure:FCN-CNN-MNIST}.
By tracking residual, \TT~outperforms \AnalogSGD~and reaches comparable accuracy with \DigitalSGD. 
For both architectures, the accuracy of \TT~drops by $<1\%$.
In contrast, \AnalogSGD~takes a few epochs to achieve observable accuracy increment in FCN training, rendering a slower convergence rate than \TT. In CNN training, \AnalogSGD's accuracy increases more slowly than \TT~does and finally gets stuck at about 80\%. It is consistent with the theoretical claims.

\textbf{CIFAR10/CIFAR100 ResNet.} We fine-tune three models from the ResNet family with different scales on CIFAR10/CIFAR100 datasets. The power response functions with the parameter $\gamma_{\text{res}}=3.0$ and $\tau=0.1$ are used, whose results are shown in Table \ref{table:CIFAR-fine-tune-pow}.
The results show that the \TT~outperforms \AnalogSGD~by about $1.0\%$ in most of the cases in ResNet34/50, and the gap even reaches about $10.0\%$ for ResNet18 training on the CIFAR100 dataset. 







\balance

\section{Conclusions and Limitations}

This paper studies the impact of response functions on gradient-based training in analog in-memory computing hardware. We first formulate the dynamic of \AnalogUpdate~based on the pulse update rule, which is a bridge between the equation at the resistive element level and the one at the algorithm level.
Based on this dynamic, we study the convergence of two gradient-based analog training algorithms, \texttt{Analog\;SGD} and \texttt{Tiki-Taka}.
The theoretical results demonstrate that \texttt{Analog\;SGD} converges to a critical point inexactly with asymptotic error, which comes from the noise and asymmetric update.
To overcome this issue, \texttt{Tiki-Taka} introduces a residual array whose stationary point is 0. By properly shifting the symmetric point of the residual array, \TT~provably converges to a critical point exactly.
Numerical simulations verify the claims about the implicit bias and the efficacy of \texttt{Tiki-Taka} against \AnalogSGD.









\bibliography{example_paper.bbl}

\newpage
\appendix
\onecolumn
\begin{center}
\Large \textbf{Appendix for  ``Analog In-memory Training on Non-ideal Resistive Elements: \\Understanding the Impact of Response Functions''} \\
\end{center}

\vspace{-0.5cm}

\addcontentsline{toc}{section}{} % Add the appendix text to the document TOC
\part{} % Start the appendix part
\parttoc % Insert the appendix TOC

\section{Literature Review}
\label{section:review}
This section briefly reviews literature that is related to this paper, as complementary to Section \ref{section:introduction}.

\textbf{Resistive element.}
A series of works seek various resistive elements that have near-constant or at least symmetric responses.
The leading candidates currently include {PCM}~\citep{Burr2016,2020legalloJPD}, {ReRAM}~\citep{Jang2014, Jang2015, stecconi2024analog}, 
{CBRAM}~\citep{Lim2018, Fuller2019}, {ECRAM}~\citep{tang2018ecram, onen2022nanosecond}, MRAM \citep{jung2022crossbar, xiao2024adapting}, FTJ \citep{guo2020ferroic} or flash memory \citep{wang2018three, xiang2020efficient, merrikh2017high}. 

However, the resistive element possessing symmetric updates may not be the best option in terms of manufacturing. 
For example, although ECRAM provides almost symmetric updates, it is still less competitive than ReRAM as ReRAM has a faster response speed and lower pulse voltage \citep{stecconi2024analog}.
The suitability of the resistive elements is evaluated based on metrics across multiple dimensions, such as the number of conductance states, retention, material endurance, switching energy, response speed, manufacturing cost, and cell size. 
Among them, this paper is only interested in the impact of response functions in the training.

\textbf{Gradient-based training on AIMC hardware.} A series of works focuses on implementing back-propagation (BP) and gradient-based training on AIMC hardware.
The seminal work \citep{gokmen2016acceleration, gokmen2017cnn} leverages the rank-one structure of the gradient and implements \AnalogSGD~by a stochastic pulse update scheme, \emph{rank-update}. Rank-update significantly accelerates the gradient descent step by avoiding dealing the gradients with $O(N^2)$ elements directly but using two $O(N)$ vectors for update instead, where $N$ is the numbers of matrix row and column.
To alleviate the {\em asymmetric update issue}, researchers also design various of \AnalogSGD~variants, \texttt{Tiki-Taka} algorithm family \citep{gokmen2020, gokmen2021, rasch2024fast}. The key components of \TT~are introducing a \emph{residual array} to stabilize the training.
Apart from the rank-update, a hybrid scheme that performs forward and backward in the analog domain but computes the gradients in the digital domain has been proposed in \citep{nandakumar2018, nandakumar2020}. Their solution, referred to as \emph{mixed-precision update}, provides a more accurate gradient signal but requires 5$\times$-10$\times$ higher overhead compared to the rank-update scheme \citep{rasch2024fast}.

Attributed to these efforts, analog training has empirically shown great promise in achieving a similar level of accuracy as digital training on chip prototype, with reduced energy consumption and training time \citep{wang2018fully, gong2022deep}.
Simultaneously, the parallel acceleration solution with AIMC hardware is also under exploration \citep{wu2024pipeline}.
Despite its good performance, it is still mysterious about when and why they work.


\textbf{Theoretical foundation of gradient-based training.} The closely related result comes from the convergence study of \TT~\citep{wu2024towards}. Similar to our work, they attempt to model the dynamic and provide the convergence properties of \AnalogSGD~and \TT. However, their work is limited to a special linear response function.
Furthermore, their paper considers a simplified version of \TT, with a hyper-parameter $\gamma=0$ (see \Cref{section:TT}). As we will show empirically and theoretically, \TT~benefits from a non-zero $\gamma$. Consequently,
We compare the results briefly in Table \ref{table:convergence-digital-vs-analog} and comprehensively in Appendix \ref{section:relation-with-wu2024}.

\begin{table*}[!h]
    \centering
    \setlength{\tabcolsep}{1.0em} % for the horizontal padding
    {\renewcommand{\arraystretch}{1.3}% for 
        \begin{tabular}{c | c | c | c }
            \toprule
                 & $\gamma$ & Generic response & Linear response \\
            \midrule
            \TT~\citep{wu2024towards}  & $=0$ & \XSolidBrush & $O\lp\sqrt{\frac{1}{K}}\frac{1}{1-33P_{\max}^2/\tau^2}\rp$  \\
            \cmidrule{1-4}
            \TT~
            [Corollary \ref{theorem:TT-convergence-scvx-final}]
            & $\ne 0$
            & $O\lp\sqrt{\frac{1}{K}}\frac{1}{H^{\texttt{TT}}_{\min}}\rp$ &$O\lp\sqrt{\frac{1}{K}}\frac{1}{1-P_{\max}^2/\tau^2}\rp$ \\
            \bottomrule
            \end{tabular}
    }
    \caption{
        Comparison between our paper and \cite{wu2024towards}.
        Mixing-coefficient $\gamma$ is a hyper-parameter of \TT.
        ``Generic response'' and ``Linear response'' columns are the convergence rates in the corresponding settings.
        $K$ represents the number of iterations.
        $H^{\texttt{TT}}_{\min}$ and $P_{\max}^2/\tau^2 < 1$ measure the saturation while the former one reduces to the latter on linear response functions.
      }
    \label{table:convergence-digital-vs-analog}
\end{table*}
  
\textbf{Energy-based models and equilibrium propagation.}
Apart from achieving explicit gradient signals by the BP, there are also attempts to train models based on \emph{equilibrium propagation} (EP, \cite{scellier2017equilibrium}), which provides a biologically plausible alternative to traditional BP. EP is applicable to a series of energy-based models, where the forward pass is performed by minimizing an energy function \citep{watfa2023energy, scellier2024energy}. The update signal in EP is computed by measuring the output difference between a free phase and an active phase. EP eliminates the need for BP non-local weight transport mechanism, making it more compatible with neuromorphic and energy-efficient hardware \citep{kendall2020training, ernoult2020equilibrium}. We highlight here that the approach to attain update signals (BP or EP) is orthogonal to the update mechanism (pulse update). Their difference lies in the objective $f(W_k)$, which is hidden in this paper. Therefore, building upon the pulse update, our work is applicable to both BP and EP.

\textbf{Physical neural network.}
The model executing on AIMC hardware, which leverages resistive crossbar array to accelerate MVM operation, is a concrete implementation of physical neural networks (PNNs, \cite{wright2022deep, momeni2024training}). 
PNN is a generic concept of implementing neural networks via a physical system in which a set of tunable parameters, such as holographic grating \citep{psaltis1990holography}, wave-based systems \citep{hughes2019wave}, and photonic networks \citep{tait2017neuromorphic}. Our work particularly focuses on training with AIMC hardware, but the methodology developed in this paper can be transferred to the study of other PNNs.





\section{Relation with the result in \cite{wu2024towards}}
\label{section:relation-with-wu2024}
Similar to this paper, \cite{wu2024towards} also attempts to model the dynamic of analog training. They shows that \AnalogSGD~converges to a critical point of problem \eqref{problem} inexactly with an asymptotic error, and \TT~converges to a critical point exactly. In this section, we compare our results with our results and theirs.

As discussed in Section \ref{section:introduction}, \cite{wu2024towards} studies the analog training on special linear response functions
\begin{align}
    q_{+}(w) = 1 - \frac w {\tau}, \quad q_{-}(w) = 1 + \frac w {\tau}.
\end{align}
It can be checked that the symmetric point is $0$ while the dynamic range of it is $[-\tau, \tau]$.
The symmetric and asymmetric components is defined by $F(W)=1$ and $G(W)=\frac{W}{\tau}$, respectively. It indicates $F_{\max}=1$. Furthermore, they assume the bounded weight saturation by assuming bounded weights, i.e., $\|W_k\|_\infty\le W_{\max}, \forall k\in[K]$ with a constant $W_{\max} < \tau$. Under this assumption, the lower bounds of response functions are given by
\begin{align}
    \min\{H(W_k)\} =&\ \min\{Q_+(W_k)\odot Q_-(W_k)\} = 1 - \lp\frac{\|W_k\|_\infty}{\tau}\rp^2 \\
    H^{\texttt{ASGD}}_{\min} =&\ \min\{H(W_k)\} = 1 - \lp\frac{W_{\max}}{\tau}\rp^2.
\end{align} 

\textbf{Convergence of \AnalogSGD.}
As we will show in Remark \ref{remark:ASGD-convergence-wo-saturation} at the end of Appendix \ref{section:proof-ASGD-convergence-noncvx}, inequality \eqref{inequality:ASGD-convergence-noncvx} can be improved when the saturation never happens
\begin{align}
    &\ \frac{1}{K}\sum_{k=0}^{K-1}\mbE[\|\nabla f(W_k)\|^2] \\
    \le&\ \frac{4 F_{\max}^2}{H^{\texttt{TT}}_{\min}} \sqrt{\frac{(f(W_0) -f^*)\sigma^2L}{K}}
    +2F_{\max} \sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1} \left. \lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty \right/ \min\{H(W_k)\} \\
    \nonumber \\
    \le&\ O\lp\sqrt{\frac{
        (f(W_0) - f^*)
        \sigma^2L}{K}}\frac{1}{1-W_{\max}^2/\tau^2}
    \rp
    + 2\sigma^2 \times \frac{1}{K}\sum_{k=0}^{K}\frac{\|W_k\|_\infty^2/\tau^2}{1-\|W_k\|_\infty^2/\tau^2}
    \nonumber
\end{align}
which is exactly the result in \cite{wu2024towards}.

\textbf{Convergence of \TT.}
It is shown that a non-zero $\gamma$ in \eqref{recursion:HD-P} improves the training accuracy \cite{gokmen2020}. However, \cite{wu2024towards} considers a special case with $\gamma=0$ while this paper considers a non-zero $\gamma$. 
As we will discuss latter in this section, different $\gamma$ leads to different convergence behaviors of \TT.

With the linear response, if we also assume the bounded saturation of $P_k$ by letting $\|P_k\|_{\infty} \le P_{\max}$, the minimal average response function is given by $H^{\texttt{TT}}_{\min} = 1 - \lp\frac{P_{\max}}{\tau}\rp^2$. 
The upper bound in Corollary \ref{theorem:TT-convergence-scvx-final} becomes
\begin{align}
    \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla f(\bar W_k)\|^2 \le O\lp \frac{1}{1 - {P_{\max}^2}/{\tau}^2}\sqrt{\frac{(f(W_0) - f^*)\sigma^2L}{K}}\rp.
\end{align}
As a comparison, without introducing a non-zero $\gamma$, \cite{wu2024towards} shows that convergence rate of \TT~is only
\begin{align}
    \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla f(W_k)\|^2 \le O\lp \frac{1}{1 - 33{P_{\max}^2}/{\tau}^2}\sqrt{\frac{(f(W_0) - f^*)\sigma^2L}{K}}\rp.
\end{align}

Even though it is not a completely fair comparison since two paper relies on different assumptions, it is still worthy to compare the analysis in two paper. 
\cite{wu2024towards} assumes the noise should be non-zero, i.e. $[\mbE_{\xi}[|\nabla f(W; \xi)|]]_i\ge c_{\texttt{noise}}\sigma, \forall i\in[D]$ holds for a non-zero constant $c_{\texttt{noise}}$.
Instead, this paper does not make this assumption but assumes that the objective is strongly convex. As mentioned in Section \ref{section:TT}, the strong convexity is introduced only to ensure the existence of $P^*(W_k)$. Therefore, we believe it can be relaxed, and the convergence rate can remain unchanged, which is left as a future work. Taking that into account, we believe the comparison can provide insight of how the non-zero $\gamma$ improves the convergence rate of \TT.

\textbf{Why does non-zero $\gamma$ improve the convergence rate of \TT?}
As discussed in Section \ref{section:TT}, $P_k$ is interpreted as a residual array that optimizes \eqref{problem:P-residual}. In the ideal setting that $F(W)=1$ and $G(W)=0$, it can be shown that $P_k$ converges to $P^*(W_k)$ if $W_k$ is fixed and $P_k$ is kept updated, even though the $W_k \ne W^*$ (hence $\nabla f(W_k)\ne 0$).

Instead, without a non-zero $\gamma$, \cite{wu2024towards} points out that $P_k$ is an approximation of clear gradient by showing
\begin{align}
    \label{inequality:TT-tracking-lemma-nips-paper}
    &\ \mbE_{\xi_k}[\|P_{k+1}-C \nabla f(W_k)\|^2] \\
    {\le}&\ \lp 1-\frac{\beta}{C}\rp\|P_k-C \nabla f(W_k)\|^2 
    + O(\beta C')\|\nabla f(W_k)\|^2
    + \text{remainder}
    \nonumber
\end{align}
where $C, C'$ are constants depending on the resistive element and model dimension, and the ``remainder'' is the non-essential terms. Consider the case that $W_k$ is fixed and \eqref{recursion:HD-P} is kept iterating, in which case the increment on $P_k$ is constant since $\gamma=0$. Telescoping \eqref{inequality:TT-tracking-lemma-nips-paper} we find the upper bound above only guarantee that 
\begin{align}
    \limsup_{k\to\infty}\mbE[\|P_{k+1}-C \nabla f(W_k)\|^2] \le O(C C' \|\nabla f(W_k)\|^2)
\end{align}
which means that $P_k$ tracks the gradient accurately only when $\nabla f(W_k)$ reaches zero asymptotically. Consequently, the less accurate approximation leads to a slower rate than this paper.



\section{Useful Lemmas and Proofs}

\subsection{Lemma \ref{lemma:properties-weighted-norm}: Properties of weighted norm}
\begin{lemma}
    \label{lemma:properties-weighted-norm}
    $\|W\|_S$ has the following properties: 
    (a) $\|W\|_S=\|W\odot\sqrt{S}\|$; 
    (b) $\|W\|_S\le\|W\| \sqrt{\|S\|_{\infty}}$; 
    (c) $\|W\|_S\ge\|W\| \sqrt{\min\{S\}}$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:properties-weighted-norm}]
    The lemma can be proven easily by definition.
\end{proof}


\subsection{Lemma \ref{lemma:bounded-function}: Properties of weighted norm}
A direct property from \Cref{assumption:response-factor} is that all $q_+(\cdot)$, $q_-(\cdot)$, and $F(\cdot)$ are bounded, as guaranteed by the following lemma.
\begin{lemma}
    \label{lemma:bounded-function}
    The following statements 
    are valid for all $W\in\ccalR$.
    (a) $F(\cdot)$ is element-wise upper bounded by a constant $F_{\max} > 0$, i.e., $\|F(W)\|_{\infty}\le F_{\max}$;
    (b) $Q_+(\cdot)$ and $\nabla Q_-(\cdot)$ are element-wise bounded by $L_Q$, i.e., $\|\nabla Q_+(W)\|_\infty\le L_Q$, $\|\nabla Q_-(W)\|_\infty\le L_Q$. 
\end{lemma}

\subsection{Lemma \ref{lemma:lip-analog-update}: Lipschitz continuity of analog update}
\begin{lemma}
    \label{lemma:lip-analog-update}
    The increment defined in \eqref{biased-update} is Lipschitz continuous with respect to $\Delta W$ under any weighted norm $\|\cdot\|_S$, i.e., for any $W, \Delta W, \Delta W'\in\reals^D$ and $S\in\reals^{D}_+$, it holds
    \begin{align}
        &\|\Delta W \odot F(W) -|\Delta W| \odot G(W)-(\Delta W' \odot F(W) -|\Delta W'| \odot G(W))\|_S
        \\
        \le&\ F_{\max}\|\Delta W-\Delta W'\|_S.
        \nonumber
    \end{align}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:lip-analog-update}]
    We prove for the case where $D=1$ and $S=1$, and the general case can be proven similarly. Notice that the absolute value $|\cdot|$ and vector norm $\|\cdot\|$, scalar multiplication $\times$ and element-wise multiplication $\odot$, are equivalent at that situation. We adopt both notations just for readability.
    \begin{align}
        &\ \|\Delta W \odot F(W) -|\Delta W| \odot G(W)-(\Delta W' \odot F(W) -|\Delta W'| \odot G(W))\|
        \\
        =&\ \|(\Delta W-\Delta W') \odot F(W) - (|\Delta W|-|\Delta W'|) \odot G(W)\|
        \nonumber.
    \end{align}
    Since $\|\Delta W-\Delta W'\| \ge \||\Delta W|-|\Delta W'|\|$ and $|G(W)|\le |F(W)|$, we have
    \begin{align}
        &\ |(\Delta W-\Delta W') \odot F(W) - (|\Delta W|-|\Delta W'|) \odot G(W)|
        \\
        \le&\  
        |(\Delta W-\Delta W') \odot (F(W) - |G(W)|)|
        \nonumber\\
        \le&\  
        |\Delta W-\Delta W'| ~ |F(W) - |G(W)||
        \nonumber\\
        \le&\  
        F_{\max}|\Delta W-\Delta W'| 
        \nonumber
    \end{align}
    which completes the proof.
\end{proof}

\subsection{Lemma \ref{lemma:element-wise-product-error}: Element-wise product error}
\begin{lemma}
    \label{lemma:element-wise-product-error}
    Let \( U, V, Q \in \reals^D\) be vectors indexed by \( [D] \). Then the following inequality holds
    \begin{align}
        \la U, V \odot Q \ra 
        \ge C_+ \la U, V\ra 
        - C_- \la |U|, |V|\ra
    \end{align}
    where the constant $C_+$ and $C_-$ are defined by
    \begin{align}
        C_+ :=& \frac{1}{2}\lp\max\{Q\} + \min\{Q\}\rp, \\
        C_- :=& \frac{1}{2}\lp\max\{Q\} - \min\{Q\}\rp.
    \end{align}
\end{lemma}
    
\begin{proof}[Proof of Lemma \ref{lemma:element-wise-product-error}]
    For any vectors $U, V, Q\in\reals^D$, it is always valid that
    \begin{align}
        &\ \la U, V \odot Q \ra 
        = \sum_{i\in[D]} [U]_{i}[V]_i [Q]_i
        \\
        =&\ \sum_{i\in[D], [U]_{i}[V]_i \ge 0} [U]_{i}[V]_i [Q]_i
        \quad
        + \sum_{i\in[D], [U]_{i}[V]_i < 0} [U]_{i}[V]_i [Q]_i
        \nonumber\\
        \ge&\ \min\{Q\}\times \lp\sum_{i\in[D], [U]_{i}[V]_i \ge 0} [U]_{i}[V]_i\rp
        + \max\{Q\}\times \lp\sum_{i\in[D], [U]_{i}[V]_i < 0} [U]_{i}[V]_i\rp
        \nonumber\\
        \eqmark{a}&\ C_+ \lp\sum_{i\in[D], [U]_{i}[V]_i \ge 0} [U]_{i}[V]_i\rp
        - C_- \lp\sum_{i\in[D], [U]_{i}[V]_i \ge 0} |[U]_{i}[V]_i|\rp
        \nonumber\\
        +&\ C_+ \lp\sum_{i\in[D], [U]_{i}[V]_i < 0} [U]_{i}[V]_i\rp
        - C_- \lp\sum_{i\in[D], [U]_{i}[V]_i < 0} |[U]_{i}[V]_i|\rp
        \nonumber\\
        =&\ C_+ \sum_{i\in[D]} [U]_{i}[V]_i 
        - C_- \sum_{i\in[D]} |[U]_{i}[V]_i|
        \nonumber\\
        =&\ C_+ \la U, V\ra 
        - C_- \la |U|, |V|\ra 
        \nonumber
    \end{align}
    where $(a)$ uses the following equality
    \begin{align}
        \min\{Q\} [U]_{i}[V]_i =&\ C_+ [U]_{i}[V]_i - C_- |[U]_{i}[V]_i|,~~~~\text{if }[U]_{i}[V]_i \ge 0, \\
        \max\{Q\} [U]_{i}[V]_i =&\ C_+ [U]_{i}[V]_i - C_- |[U]_{i}[V]_i|,~~~~\text{if }[U]_{i}[V]_i < 0.
    \end{align}
This completes the proof.
\end{proof}


\section{Proof of Theorem \ref{theorem:pulse-update-error}: Error from Pulse Update}
\label{section:proof-pulse-update-error}
\ThmPulseUpdateError*

\begin{proof}[Proof of  Theorem \ref{theorem:pulse-update-error}]
Recall the definition of the bit length is
\begin{align}
    \label{definition:BL}
    \operatorname{BL} := \left\lceil \frac{|\Delta w|}{\Delta w_{\min}}\right\rceil = \Theta\lp\frac{|\Delta w|}{\Delta w_{\min}}\rp
\end{align}
leading to
\begin{align}
    |\operatorname{BL}\Delta w_{\min} - |\Delta w|| \le \Delta w_{\min}
    ~~~~\text{or}~~~~
    |s\operatorname{BL}\Delta w_{\min} - \Delta w| \le \Delta w_{\min}. 
\end{align}
Notice that the update responding to each pulse is a $\Theta(\Delta w_{\min})$ term. Directly manipulating $U^{\operatorname{BL}}_p(w, s)$ and expanding it in Taylor series to the first-order term yields
\begin{align}
    U^{\operatorname{BL}}_p(w, s)
    =&\ w + s\cdot\Delta w_{\min}\sum_{t=0}^{\operatorname{BL}-1} q_s(w + \Theta(t\Delta w_{\min})) 
    + \Delta w_{\min}\sum_{t=0}^{\operatorname{BL}-1} \sigma_c\xi_t
    \\
    =&\ w + s\cdot\Delta w_{\min}\sum_{t=0}^{\operatorname{BL}-1} q_s(w) 
    + \sum_{t=0}^{\operatorname{BL}-1}\Theta(t(\Delta w_{\min})^2)
    + \Delta w_{\min}\sum_{t=0}^{\operatorname{BL}-1} \sigma_c\xi_t
    \nonumber\\
    =&\ w + s\cdot\Delta w_{\min}\cdot \operatorname{BL} \cdot~ q_s(w) 
    + \Theta(\operatorname{BL}^2(\Delta w_{\min})^2)
    + \Delta w_{\min}\cdot \sqrt{\operatorname{BL}}\cdot \sigma_c\xi
    \nonumber\\
    =&\ w + \Delta w \cdot q_s(w) 
    + (s\operatorname{BL}\Delta w_{\min} - \Delta w)
    + \Theta((\Delta w)^2)
    + \sqrt{\Delta w_{\min}}\cdot \sqrt{\Delta w}\cdot \sigma_c\xi
    \nonumber\\
    =&\ U_q(w, \Delta w) + \Theta(\Delta w_{\min})+ \Theta((\Delta w)^2) + \Theta(\sqrt{\Delta w_{\min}}\cdot \sqrt{\Delta w}\cdot \sigma_c)
    \nonumber
\end{align}
where $\xi := \frac{1}{\sqrt{\operatorname{BL}}}\sum_{t=0}^{\operatorname{BL}-1} \xi_t$ is the accumulated noise with variance $1$. The proof is completed.
\end{proof}

\section{Proof of Theorem \ref{theorem:implicit-regularization-short}: Implicit Bias of Analog Training}
\label{section:proof-implicit-regularization}
In this section, we provide a full version of Theorem \ref{theorem:implicit-regularization-short}.
Before that, we formally define the accumulated asymmetric function $R_c(W):\reals^D\to\reals^D$ element-wise.
Let $R(W) := \frac{G(W)}{F(W)}$ be the asymmetric ratio and $R_c(W)$ is defined by
\begin{align}
    [R_c(W)]_i := \int_{\tau_i^{\min}}^{[W]_i} ~ [R(W)]_i ~\diff{[W]_i}
\end{align}
which satisfies
\begin{align}
    \nabla R_c(W) = R(W)
    \quad\text{and}\quad
    \nabla \la \Sigma, R_c(W)\ra = \Sigma \odot R(W).
\end{align}
Since we do not further assume stronger properties for response functions, like monotonicity, it is hard to provide strong claims on the shape of $R(W)$ or $R_c(W)$.
Here we provide the expression of $R_c(W)$ for the linear response functions $Q_{+}(W) = 1 - \frac{W}{\tau}$, $Q_{-}(W) = 1 + \frac{W}{\tau}$. In this case, $F(W) = 1$ and $G(W) = \frac{W}{\tau}$ based on definition \eqref{definition:F-G}; and hence $R(W) = \frac{G(W)}{F(W)} = \frac{W}{\tau}$. Accordingly, the accumulated asymmetric function is given by
\begin{align}
    [R_c(W)]_i =&\ \int_{\tau_i^{\min}}^{[W]_i} ~ [R(W)]_i ~\diff{[W]_i} 
    = \int_{\tau_i^{\min}}^{[W]_i} ~ \frac{[W]_i}{\tau} ~\diff{[W]_i}
    \\
    =&\ \frac{1}{2\tau} ([W]_i)^2 - \frac{1}{2\tau} (\tau_i^{\min})^2.
    \nonumber
\end{align} 
Therefore, the last term in the objective \eqref{problem:implicit-regularization-short} becomes 
\begin{align}
    \la \Sigma, R_c(W)\ra 
    =&\ \sum_{i=1}^D [\Sigma]_i [R_c(W)]_i 
    = \sum_{i=1}^D [\Sigma]_i \lp\frac{1}{2\tau} ([W]_i)^2 - \frac{1}{2\tau} (\tau_i^{\min})^2\rp
    \\
    =&\ \frac{1}{2\tau}\|W\|_{\Sigma}^2 + \text{const.}
    \nonumber
\end{align}
which is a weighted $\ell_2$ norm regularization term. It reduces to \eqref{problem:implicit-regularization-linear-device} in the scalar case.

If the ratio $R(W)$ is monotonic at each coordinate, $R_c(W)$ reaches its minimum at $W^\diamond$.


\begin{restatable}[Implicit Penalty, full version of Theorem \ref{theorem:implicit-regularization-short}]{theorem}{ThmImplicitRegularization}
    \label{theorem:implicit-regularization}
    Let $T(w)$ denote the effective update of \AnalogSGD.
    \begin{align}
    	T(W) := \labs\frac{\mbE_\xi\lB U_q(W, -\alpha f'(W; \xi))\rB - W}{\alpha}\rabs
    	= \labs\mbE_\xi[f'(w; \xi)] \odot F(W) - \mbE_\xi[|f'(W; \xi)|] \odot G(W)\rabs.
    \end{align}
    \AnalogSGD~implicitly optimizes the following penalized objective
    \begin{align}
        \label{problem:implicit-regularization}
    	\min_{W} ~ f_{\Sigma}(W) := f(W) + \la \Sigma, R_c(W)\ra
    \end{align}
    in the sense that there exists a point $W^S$ given by
    \begin{align}
    \label{problem:implicit-regularization-solution}
        W^S := (\nabla^2 f(W^*)-\nabla R(W^\diamond)\Sigma)^{-1}{(\nabla^2 f(W^*) ~W^*-\nabla R(W^\diamond)\Sigma~ W^\diamond)}{}
    \end{align}
    such that $\|\nabla f_{\Sigma}(W^S)\|\le O((W^\diamond-W^*)^2)$ and $T(W^S) \le O((W^\diamond-W^*)^2)$. Both $T(W^S)$ and $\|\nabla f_{\Sigma}(W^S)\|$ are significantly smaller than $T(W^\diamond)=O(|W^\diamond-W^*|)$ and $T(W^*)=O(|W^\diamond-W^*|)$ when $W^\diamond$ is close to $W^*$.
\end{restatable}

If $W$ is a scalar, i.e. $D=1$, \eqref{problem:implicit-regularization-solution} reduces to \eqref{problem:implicit-regularization-linear-device}
\begin{align}
    \min_{W} ~ f_{\Sigma}(W) := f(W) + \frac{\Sigma}{2\tau}\|W\|^2
\end{align}
with its solution
\begin{align}
    W^S := \frac{f''(W^*) ~W^*-R'(W^\diamond)\Sigma~ W^\diamond}{f''(W^*)-R'(W^\diamond)\Sigma}.
\end{align}

\begin{proof}[Proof of Theorem \ref{theorem:implicit-regularization-short} and \ref{theorem:implicit-regularization}]
We separately show that $\|\nabla f_{\Sigma}(W)\|\le O((W^\diamond-W^*)^2)$ and $T(W^S) \le O((W^\diamond-W^*)^2)$.

\textbf{Proof of $\|\nabla f_{\Sigma}(W)\|\le O((W^\diamond-W^*)^2)$.}
The gradient of the penalized objective $f_{\Sigma}(W)$ is given by
\begin{align}
    \nabla f_{\Sigma}(W) = \nabla f(W) + \Sigma \odot R(W).
\end{align}
Leveraging the fact that $\nabla f(W^*)=0$, $\frac{G(W^\diamond)}{F(W^\diamond)}=0$, as well as Taylor expansion given by
\begin{align}
    \label{inequality:implicit-regularization-taylor}
    \nabla f(W^S) =&\ \nabla f(W^*) + \nabla^2 f(W^*)(W^S - W^*) + O((W^S - W^*)^2), \\
    \frac{G(W^S)}{F(W^S)}=&\ \frac{G(W^\diamond)}{F(W^\diamond)} + \nabla R(W^\diamond) (W^S-W^\diamond)  + O((W^S -W^\diamond)^2),
\end{align}
we bound the gradient of the penalized objective as follows
\begin{align}
    \label{inequality:implicit-regularization-S1-T1}
	&\ 
    \|\nabla f_{\Sigma}(W)\|
	= \lnorm \nabla f(W^S) - \Sigma\odot \frac{G(W^S)}{F(W^S)} \rnorm
	\\
	=&\ \lnorm \nabla^2 f(W^*)(W^S - W^*) + O((W^S - W^*)^2) - \Sigma\odot (\nabla R(W^\diamond) (W^S-W^\diamond))  + O((W^S -W^\diamond)^2) \rnorm
	\nonumber\\
	=&\ O((W^S - W^*)^2) + O((W^S -W^\diamond)^2)
	\nonumber\\
	=&\ O((W^* -W^\diamond)^2)
	\nonumber
\end{align}
where the last inequality holds by the definition of $W^S$.

\textbf{Proof of $T(w^S) \le O((w^\diamond-w^*)^2)$.}
By the definition of effective update $T(W^S)$, we have
\begin{align}
    \label{inequality:implicit-regularization-S1}
    &\ \lnorm\frac{\mbE_\xi\lB U_q(W^S, -\alpha \nabla f(W^S; \xi))\rB -W^S}{\alpha}\rnorm \\
    =&\ \lnorm \mbE_\xi[\nabla f(W^S; \xi)] \odot F(W^S) - \mbE_\xi[|\nabla f(W^S; \xi)|] \odot G(W^S) \rnorm
    \nonumber \\
    \le&\ \lnorm \lp \nabla f(W^S) - \mbE_\xi[|\nabla f(W^S; \xi)|] \odot \frac{G(W^S)}{F(W^S)}\rp \rnorm F_{\max}
    \nonumber \\
    \le&\ \lnorm \nabla f(W^S) - \mbE_\xi[|\nabla f(W^S; \xi)-\nabla f(W^S)|] \odot \frac{G(W^S)}{F(W^S)} \rnorm F_{\max}
    \bkeq
    +\lnorm (\mbE_\xi[|\nabla f(W^S; \xi)|]-\mbE_\xi[|\nabla f(W^S; \xi)-\nabla f(W^S)|]) \odot \frac{G(W^S)}{F(W^S)} \rnorm F_{\max}
    \nonumber \\
    \le&\ \|\nabla f_{\Sigma}(W)\| F_{\max}
    +\lnorm (\mbE_\xi[|\nabla f(W^S; \xi)|]-\mbE_\xi[|\nabla f(W^S; \xi)-\nabla f(W^S)|]) \odot \frac{G(W^S)}{F(W^S)} \rnorm F_{\max}
    \nonumber
\end{align}
The first term in the \ac{RHS} of \eqref{inequality:implicit-regularization-S1} is already bounded by \eqref{inequality:implicit-regularization-S1-T1}.
By inequality $||x|-|y|| \le |x-y|$ for any $x, y\in\reals$, the second term in the \ac{RHS} of \eqref{inequality:implicit-regularization-S1} is bounded by
\begin{align}
    \label{inequality:implicit-regularization-S1-T2}
    &\ \lnorm (\mbE_\xi[|\nabla f(W^S; \xi)|]-\mbE_\xi[|\nabla f(W^S; \xi)-\nabla f(W^S)|]) \odot \frac{G(W^S)}{F(W^S)} \rnorm \\
   	\le&\ \lnorm |\nabla f(W^S)| \odot \frac{G(W^S)}{F(W^S)} \rnorm
   	\nonumber \\
   	=&\ \lnorm |\nabla f(W^S)-\nabla f(W^*)| \odot \lp\frac{G(W^S)}{F(W^S)} -\frac{G(W^\diamond)}{F(W^\diamond)} \rp\rnorm
   	\nonumber \\
   	\le&\ O(|W^S-W^*|) \cdot O(|W^S-W^\diamond|)
   	\nonumber \\
   	=&\ O((W^*-W^\diamond)^2)
   	\nonumber
\end{align}
Plugging back \eqref{inequality:implicit-regularization-S1-T1} and \eqref{inequality:implicit-regularization-S1-T2} into \eqref{inequality:implicit-regularization-S1} shows $T(w^S) \le O((w^\diamond-w^*)^2)$. It is trivial to prove $T(W^\diamond)=O(|W^\diamond-W^*|)$ and $T(W^*)=O(|W^\diamond-W^*|)$ by the definition of $W^S$ and \eqref{inequality:implicit-regularization-taylor}.
\end{proof}


\section{Proof of Theorem \ref{theorem:ASGD-convergence-noncvx}: Convergence of \texttt{Analog\;SGD}}
\label{section:proof-ASGD-convergence-noncvx}
\ThmASGDConvergenceNoncvx*



\begin{proof}[Proof of Theorem \ref{theorem:ASGD-convergence-noncvx}]
The $L$-smooth assumption (Assumption \ref{assumption:Lip}) implies that
\begin{align}
    \label{inequality:ASGD-convergence-1}
    &\ \mathbb{E}_{\xi_k}[f(W_{k+1})] \le f(W_k)
    +\underbrace{\mathbb{E}_{\xi_k}[\la \nabla f(W_k), W_{k+1}-W_k\ra]}_{(a)}
    + \underbrace{\frac{L}{2}\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]}_{(b)}.
\end{align}
Next, we will handle the second and the third terms in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1} separately.

\noindent
\textbf{Bound of the second term (a).}
To bound term (a) in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1}, we leverage the assumption that noise has expectation $0$ (Assumption \ref{assumption:noise})
\begin{align}
    \label{inequality:ASGD-convergence-1-2}
    &\ \mathbb{E}_{\xi_k}[\la \nabla f(W_k), W_{k+1}-W_k\ra] \\
    =&\ \alpha\mathbb{E}_{\xi_k}\lB \la 
    \nabla f(W_k)\odot \sqrt{F(W_k)}, 
    \frac{W_{k+1}-W_k}{\alpha\sqrt{F(W_k)}}+(\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot \sqrt{F(W_k)}
    \ra\rB
    \nonumber \\
    =&\ - \frac{\alpha}{2} \|\nabla f(W_k)\odot \sqrt{F(W_k)}\|^2 
    \nonumber \\
    &\ - \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot \sqrt{F(W_k)}\rnorm^2\rB
    \nonumber \\
    &\ + \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha \nabla f(W_k; \xi_k)\odot \sqrt{F(W_k)}\rnorm^2\rB.
    \nonumber
\end{align}
The second term of the \ac{RHS} of \eqref{inequality:ASGD-convergence-1-2} is bounded by
\begin{align}
    \label{inequality:ASGD-convergence-1-2-2}
    &\frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot \sqrt{F(W_k)}\rnorm^2\rB\\
    =&\ \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)}{\sqrt{F(W_k)}}\rnorm^2\rB
    \nonumber\\
    \ge&\ \frac{1}{2\alpha F_{\max}}\mathbb{E}_{\xi_k}\lB\| W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2\rB
    \nonumber.
\end{align}
The third term in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1-2} can be bounded by variance decomposition and bounded variance assumption (Assumption \ref{assumption:noise})
\begin{align}
    \label{inequality:ASGD-convergence-1-2-3}
    &\ \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha \nabla f(W_k; \xi_k)\odot \sqrt{F(W_k)}\rnorm^2\rB
    \\
    =&\ \frac{\alpha}{2}\mathbb{E}_{\xi_k}\lB\lnorm|\nabla f(W_k; \xi_k)|\odot\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2\rB
    \nonumber \\
    \le&\ \frac{\alpha}{2}\lnorm|\nabla f(W_k)|\odot\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty.
    \nonumber 
\end{align}
Define the saturation vector $H(W_k)\in\reals^D$ by
\begin{align}
    H(W_k) :=&\ {F(W_k)^{\odot 2}-G(W_k)^{\odot 2}} 
    = {(F(W_k)+G(W_k))\odot(F(W_k)-G(W_k))} \\
    =&\ {Q_+(W_k)\odot Q_-(W_k)}.
    \nonumber
\end{align}
Note that the first term in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1-2} and the second term in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1-2-3} can be bounded by
\begin{align}
    &\ \label{inequality:ASGD-converge-linear-3}
    - \frac{\alpha}{2} \|\nabla f(W_k) \odot \sqrt{F(W_k)}\|^2 
    + \frac{\alpha}{2} \lnorm|\nabla f(W_k)| \odot \frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2 \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp [F(W_k)]_d-\frac{[G(W_k)]^2_d}{[F(W_k)]_d}\rp\rp
    \nonumber \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp \frac{[F(W_k)]^2_d-[G(W_k)]^2_d}{[F(W_k)]_d}\rp\rp
    \nonumber \\
    \le&\ -\frac{\alpha}{2 F_{\max}}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp [F(W_k)]_d^2-[G(W_k)]^2_d\rp\rp
    \nonumber\\
    =&\ -\frac{\alpha}{2 F_{\max}}\|\nabla f(W_k)\|^2_{H(W_k)}
    \le 0.
    \nonumber
\end{align}
Plugging \eqref{inequality:ASGD-convergence-1-2-2} to \eqref{inequality:ASGD-converge-linear-3} into \eqref{inequality:ASGD-convergence-1-2}, we bound the term (a) by
\begin{align}
    \label{inequality:ASGD-convergence-1-2-final}
    &\ \mathbb{E}_{\xi_k}[\la \nabla f(W_k), W_{k+1}-W_k\ra] \\
    =&\ -\frac{\alpha}{2 F_{\max}}\|\nabla f(W_k)\|^2_{H(W_k)}
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \nonumber \\
    &\ - \frac{1}{2\alpha F_{\max}}\mathbb{E}_{\xi_k}\lB\lnorm W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\rnorm^2\rB.
    \nonumber
\end{align}

\noindent
\textbf{Bound of the third term (b).}
The third term (b) in the \ac{RHS} of \eqref{inequality:ASGD-convergence-1} is bounded by
\begin{align}
    \label{inequality:ASGD-convergence-1-3}
    &\ \frac{L}{2}\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2] \\
    \le&\ L\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2]
    \bkeq
    +\alpha^2 L\mathbb{E}_{\xi_k}[\|(\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2]
    \nonumber\\
    \le&\ L\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2] 
    +\alpha^2 LF_{\max}^2\sigma^2
    \nonumber
\end{align}
where the last inequality leverages the bounded variance of noise (Assumption \ref{assumption:noise}) and the fact that $F(W_k)$ is bounded by $F_{\max}$ element-wise.

Substituting \eqref{inequality:ASGD-convergence-1-2-final} and \eqref{inequality:ASGD-convergence-1-3} back into \eqref{inequality:ASGD-convergence-1}, we have
\begin{align}
    \label{inequality:ASGD-convergence-2}
    &\ \mathbb{E}_{\xi_k}[f(W_{k+1})] \le 
    f(W_k) - \frac{\alpha}{2 F_{\max}} \|\nabla f(W_k)\|^2_{H(W_k)}
    +\alpha^2LF_{\max}^2\sigma^2
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \\
    &\ - \frac{1}{F_{\max}}\lp\frac{1}{2\alpha}-LF_{\max}\rp\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2].
    \nonumber
\end{align}
The third term in the \ac{RHS} of \eqref{inequality:ASGD-convergence-2} can be bounded by
\begin{align}
    \label{inequality:ASGD-convergence-2-T3}
    &\ \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k+\alpha (\nabla f(W_k; \xi_k)-\nabla f(W_k))\odot F(W_k)\|^2]
    \\
    =&\ \alpha^2 \mathbb{E}_{\xi_k}[\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k; \xi_k)| \odot G(W_k)\|^2]
    \nonumber\\
    \ge&\ \frac{1}{2}\alpha^2 \mathbb{E}_{\xi_k}[\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k)| \odot G(W_k)\|^2]
    \nonumber\\
    &\ -\alpha^2 \mathbb{E}_{\xi_k}[\|(|\nabla f(W_k)| - |\nabla f(W_k; \xi_k)|) \odot G(W_k)\|^2]
    \nonumber\\
    \ge&\ \frac{1}{2}\alpha^2 \mathbb{E}_{\xi_k}[\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k)| \odot G(W_k)\|^2]
    \nonumber\\
    &\ -\alpha^2 \mathbb{E}_{\xi_k}[\|(\nabla f(W_k) - \nabla f(W_k; \xi_k)) \odot G(W_k)\|^2]
    \nonumber\\
    \ge&\ \frac{1}{2}\alpha^2 \mathbb{E}_{\xi_k}[\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k)| \odot G(W_k)\|^2]
    -\alpha^2 F_{\max} \sigma^2\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \nonumber
\end{align}
where the first inequality holds because $\|x\|^2\ge\frac{1}{2}\|x-y\|^2-\|y\|^2$ for any $x, y\in\reals^D$, the second inequality comes from $||x|-|y|| \le |x-y|$ for any $x, y\in\reals$, and the last inequality holds because
\begin{align}
    &\ \mathbb{E}_{\xi_k}[\|(\nabla f(W_k) - \nabla f(W_k; \xi_k)) \odot G(W_k)\|^2]
    \\
    =&\ \mathbb{E}_{\xi_k}\lB\lnorm(\nabla f(W_k) - \nabla f(W_k; \xi_k)) \odot \frac{G(W_k)}{\sqrt{F(W_k)}} \odot \sqrt{F(W_k)}\rnorm^2\rB
    \nonumber\\
    \le&\ F_{\max}\mathbb{E}_{\xi_k}\lB\lnorm(\nabla f(W_k) - \nabla f(W_k; \xi_k)) \odot \frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2\rB
    \nonumber\\
    \le&\ F_{\max}
    \mathbb{E}_{\xi_k}\lB\|\nabla f(W_k) - \nabla f(W_k; \xi_k)\|^2\rB 
    \lnorm\frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2_\infty
    \nonumber\\
    \le&\ F_{\max} \sigma^2
    \lnorm\frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2_\infty.
    \nonumber
\end{align}
The learning rate $\alpha\le\frac{1}{4LF_{\max}}$ implies that $\frac{1}{2\alpha}-LF_{\max} \le \frac{1}{4\alpha}$ in \eqref{inequality:ASGD-convergence-2}, which leads \eqref{inequality:ASGD-convergence-1} to
\begin{align}
    \label{inequality:ASGD-convergence-3}
    \mathbb{E}_{\xi_k}[f(W_{k+1})]
    \le&\ f(W_k) - \frac{\alpha}{2 F_{\max}} \|\nabla f(W_k)\|^2_{H(W_k)}
    +\alpha^2LF_{\max}^2 \sigma^2
    +\alpha\sigma^2\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \\
    &\ - \frac{\alpha}{8F_{\max}}\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k)| \odot G(W_k)\|^2.
    \nonumber
\end{align}
Reorganizing \eqref{inequality:ASGD-convergence-3}, taking expectation over all $\xi_K, \xi_{K-1}, \cdots, \xi_0$, and averaging them for $k$ from $0$ to $K-1$ deduce that
\begin{align}
    E_K = 
    &\ \frac{1}{K}\sum_{k=0}^{K}\mbE[\|\nabla f(W_k) \odot F(W_k) + |\nabla f(W_k)| \odot G(W_k)\|^2+4 \|\nabla f(W_k)\|^2_{H(W_k)}]
    \\
    \le&\ 
    \frac{8F_{\max}(f(W_0) - \mbE[f(W_{k+1})])}{\alpha K}
        +8\alpha LF_{\max}^3\sigma^2
        +8F_{\max} \sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \nonumber \\
    \le&\ 
    \frac{8F_{\max}(f(W_0) - f^*)}{\alpha K}
        +8\alpha LF_{\max}^3\sigma^2
        +8F_{\max} \sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1}\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \nonumber \\
    = &\ 
    \saveeq{inequality-RHS:ASGD-convergence-upperbound}{
        16F_{\max}^2\sqrt{\frac{(f(W_0) -f^*)\sigma^2L}{K}}
        +8F_{\max} \sigma^2 S^{\texttt{ASGD}}_K
    }
    \nonumber 
\end{align}
where the last equality chooses the learning rate as $\alpha=\frac{1}{F_{\max}}\sqrt{\frac{f(W_0) - f^*}{\sigma^2LK}}$.
The proof is completed.
\end{proof}

\begin{remark}[Tighter bound without saturation] 
    \label{remark:ASGD-convergence-wo-saturation}
    Assuming the saturation never happens during the training, i.e. $H(W_k) \ge H^{\texttt{TT}}_{\min} > 0$ for all $k\in[K]$, we get a tighter bound in \eqref{inequality:ASGD-convergence-3} by leveraging $\lnorm \nabla f(W_k)\rnorm^2_{H(W_k)} \ge \min\{H(W_k)\} \lnorm \nabla f(W_k)\rnorm^2 \ge H^{\texttt{TT}}_{\min} \lnorm \nabla f(W_k)\rnorm^2$
    \begin{align}
        \mathbb{E}_{\xi_k}[f(W_{k+1})]
        \le&\ f(W_k) - \frac{\alpha}{2 F_{\max}} \|\nabla f(W_k)\|^2_{H(W_k)}
        +\alpha^2LF_{\max}^2 \sigma^2
        +\alpha\sigma^2\lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty
    \end{align}
    which leads to
    \begin{align}
        &\ \frac{1}{K}\sum_{k=0}^{K} [\|\nabla f(W_k)\|^2]
        \\
        = &\ 
        \saveeq{inequality-RHS:ASGD-convergence-upperbound-tighter}{
            \frac{4 F_{\max}^2}{H^{\texttt{TT}}_{\min}} \sqrt{\frac{(f(W_0) -f^*)\sigma^2L}{K}}
            +2F_{\max} \sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1} \left. \lnorm\frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2_\infty \right/ \min\{H(W_k)\}
        }.
        \nonumber 
    \end{align}
    It exactly reduces to the result for the convergence of \AnalogSGD~in \cite{wu2024towards} on special linear repsonse functions, as discussed in Appendix \ref{section:relation-with-wu2024}.
\end{remark}




 


  


  
 

 
 
  
  


  
  




\section{Proof of Theorem \ref{theorem:TT-convergence-scvx}: Convergence of \texttt{Tiki-Taka}}
\label{section:proof-TT-convergence-scvx}
This section provides the convergence guarantee of the \TT~under the strongly convex assumption.
\ThmTTConvergenceScvx*

\subsection{Main proof}
\begin{proof}[Proof of Theorem \ref{theorem:TT-convergence-scvx}]
The proof of the \TT~convergence relies on the following two lemmas, which provide the sufficient descent of $W_k$ and $\bar W_k$, respectively.
\begin{restatable}[Descent Lemma of $\bar W_k$]{lemma}{LemmaTTbarWDescentScvx}
    \label{lemma:TT-barW-descent}
    Suppose Assumptions \ref{assumption:Lip}--\ref{assumption:noise} hold.
    It holds for \TT~that
    \begin{align}
        \label{inequality:TT-barW-descent}
        \mathbb{E}_{\xi_k}[f(\bar W_{k+1})] 
        \le&\  f(\bar W_k)
        -\frac{\alpha}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
        + 2\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
        + 2\alpha^2 L F_{\max}^2 \sigma^2
        \\
        &\ 
        - \frac{\alpha\gamma}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) 
        + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
        \bkeq
        + \frac{F_{\max}}{\alpha}\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}\rB
        + \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
        \nonumber.
    \end{align}
\end{restatable}

\begin{restatable}[Descent Lemma of $W_k$]{lemma}{LemmaTTWDescentScvx}
    \label{lemma:TT-W-descent}
    It holds for \TT~that
    \begin{align}
        \label{inequality:TT-W-descent}
        \|W_{k+1}-W^*\|^2 \le&\ \|W_k-W^*\|^2 - \frac{\beta}{2\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
        \\
        &\ - \frac{\beta\gamma}{2F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
        \bkeq
        + \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}
        + 2\beta^2 \|P_{k+1} - P^*(W_k)\|^2
        \nonumber.
    \end{align}
\end{restatable}
The proof of Lemma \ref{lemma:TT-barW-descent} and \ref{lemma:TT-W-descent} are deferred to Section \ref{section:proof-lemma:TT-barW-descent} and \ref{section:proof-lemma:TT-W-descent}, respectively.

For a sufficiently large $\gamma$, $P^*(W_k)$ is ensured to be located in the dynamic range of analog tile $P_k$ and hence the constraint in \eqref{problem:P-residual} is never active. Therefore, we may assume both $q_+(P_k)$ and $q_-(P_k)$ are non-zero, equivalently, there exists a non-zero constant $H^{\texttt{TT}}_{\min}$ such that $\min\{H(P_k)\}\ge H^{\texttt{TT}}_{\min}$ for all $k$. Under this condition, we have the following inequalities
\begin{align}
    \label{inequality:TT-convergence-scvx-T1.1}
    \frac{\alpha}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
    \ge&\ \frac{\alpha H^{\texttt{TT}}_{\min}}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2, \\
    \label{inequality:TT-convergence-scvx-T1.2}
    \frac{F_{\max}}{\alpha\gamma}\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}
    \le&\ \frac{F_{\max}}{\alpha\gamma H^{\texttt{TT}}_{\min}}\|W_{k+1}-W_k\|^2.
\end{align}
Similarly, we bound the term $\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}$ in \eqref{inequality:TT-barW-descent} by
\begin{align}
    \label{inequality:TT-convergence-scvx-T1.3}
    \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}
    \le \frac{2\beta F_{\max}^3}{\gamma \min\{H(W_k)\}}\lnorm P_{k+1} - P^*(W_k)\rnorm^2.
\end{align}
Notice it is only required to have $\min\{H(W_k)\} > 0$ for the inequality to hold.

By inequality \eqref{inequality:TT-convergence-scvx-T1.2}, the last two terms in the \ac{RHS} of \eqref{inequality:TT-barW-descent} is bounded by
\begin{align}
    \label{inequality:TT-convergence-scvx-T3}
    &\ \frac{F_{\max}}{\alpha}\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}\rB
    + \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    \\
    =&\ \frac{F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2\rB
    + \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    \nonumber\\
    \lemark{a}&\ \frac{2F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2\rB
    = \frac{2\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)\|^2
    \nonumber\\
    \le&\ \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\|^2
    \nonumber\\
    &\ + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)-(P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k))\|^2
    \nonumber\\
    \lemark{b}&\ \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\|^2
    + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|P_{k+1} - P^*(W_k)\|^2.
    \nonumber
\end{align}
where $(a)$ holds if learning rate $\alpha$ is sufficiently small such that $\frac{F_{\max}}{\alpha\gamma H^{\texttt{TT}}_{\min}}\ge 1$; $(b)$ comes from the Lipschitz continuity of the analog update (c.f. Lemma \ref{lemma:lip-analog-update}).


With all the inequalities and lemmas above, we are ready to prove the main conclusion in Theorem \ref{theorem:TT-convergence-scvx} now. 
Define a Lyapunov function by
\begin{align}
    \mbV_k :=&\ f(\bar W_k)-f^*
	+ C\|W_k-W^*\|^2.
\end{align}
By Lemmas \ref{lemma:TT-barW-descent} and \ref{lemma:TT-W-descent}, we show that $\mbV_k$ has sufficient descent in expectation
\begin{align}
    \label{inequality:TT-convergence-Lya-1}
    &\ \mbE_{\xi_k}[\mbV_{k+1}] 
    \\
    =&\ 
    \mbE_{\xi_k}\lB 
        f(\bar W_{k+1})-f^*
        + C\|W_{k+1}-W^*\|^2
    \rB
    \nonumber\\
    \le&\ f(\bar W_k) - f^*
    -\frac{\alpha}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
    + 2\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    + 2\alpha^2 L F_{\max}^2 \sigma^2
    \bkeq
    - \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    \bkeq
    + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\|P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\|^2
    + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
    \nonumber\\
    &\ 
    + C \Bigg(~
        \|W_k-W^*\|^2 - \frac{\beta}{2\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
        + \frac{3\beta F_{\max}^3}{\gamma \min\{H(W_k)\}} \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
        \bkeq\hspace{3em}
        - \frac{\beta\gamma}{2F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    ~\Bigg)
    \nonumber\\
    \le&\ \mbV_k
    -\frac{\alpha H^{\texttt{TT}}_{\min}}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2
    + 2\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    + 2\alpha^2 L F_{\max}^2 \sigma^2
    \bkeq
    - \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    \bkeq
    - \lp\frac{\beta\gamma}{2F_{\max}} C - \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\rp \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \lp \frac{3\beta F_{\max}^3}{\gamma \min\{H(W_k)\}} C + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\rp  \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
    - \frac{\beta}{2\gamma F_{\max}} C \|W_k-W^*\|^2_{H(W_k)}.
    \nonumber
\end{align}


Let $C=\frac{10\beta F_{\max}^2}{\alpha H^{\texttt{TT}}_{\min}\gamma}$, which leads to the positive coefficient in front of $\|P_{k+1} - P^*(W_k)\|^2$, i.e.
\begin{align}
    \label{inequality:TT-convergence-Lya-2}
    &\ \mbE_{\xi_k}[\mbV_{k+1}] 
    \\
    \le&\ \mbV_k
    - \frac{\alpha H^{\texttt{TT}}_{\min}}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2
    + 2\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    + 2\alpha^2 L F_{\max}^2 \sigma^2
    \bkeq
    - \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    \bkeq
    - \frac{\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \lp \frac{30\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\rp  \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
    - \frac{5\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|W_k-W^*\|^2_{H(W_k)}.
    \nonumber
\end{align}
Notice that the $\|P_{k+1} - P^*(W_k)\|^2$ appears in the \ac{RHS} above, we also need the following lemma to bound it in terms of $\|P_k - P^*(W_k)\|^2$.
\begin{restatable}[Descent Lemma of $P_k$]{lemma}{LemmaTTPDescentScvx}
    \label{lemma:TT-P-descent}
    Suppose Assumptions \ref{assumption:response-factor}-\ref{assumption:noise} and \ref{assumption:strongly-cvx} hold.
    It holds for \TT~that
    \begin{align}
        \label{inequality:TT-P-descent}
        &\ \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2] \\
        \le&\ \lp
            1- \frac{\alpha\gamma \mu L}{4(\mu+L)}
        \rp \|P_k-P^*(W_k)\|^2 
        + \frac{2\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
        + \alpha^2 F_{\max}^2 \sigma^2
        \nonumber.
    \end{align}
\end{restatable}
The proof of Lemma \ref{lemma:TT-P-descent} is deferred to Section \ref{section:proof-lemma:TT-P-descent}. By Lemma \ref{lemma:TT-P-descent}, we bound the $\|P_{k+1} - P^*(W_k)\|^2$ in terms of $\|P_k - P^*(W_k)\|^2$ as
\begin{align}
    \label{inequality:TT-convergence-Lya-2-T5}
    &\ \lp \frac{30\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} + \frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}}\rp  \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
    \\
    \lemark{a}&\ \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}}  \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2]
    \nonumber\\
    \le&\ \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} 
    \lp
        1- \frac{\alpha}{4} \frac{\mu L}{\gamma(\mu+L)}
    \rp \|P_k-P^*(W_k)\|^2 
    \bkeq
    + \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} \lp\frac{2\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    + \alpha^2 F_{\max}^2 \sigma^2\rp
    \nonumber\\
    \le&\ \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} \|P_k-P^*(W_k)\|^2 
    + O\lp
        \beta^2\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
        + \alpha\beta^2 F_{\max}^2 \sigma^2
    \rp
    \nonumber\\
    \lemark{b}&\ \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}} \|P_k-P^*(W_k)\|^2 
    + \alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    + \alpha^2L F_{\max}^2 \sigma^2
    \nonumber
\end{align}
where $(a)$ assumes $\frac{4\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \le \frac{2\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}}$ with lost of generality to keep the formulations simple since $\gamma \min\{H(W_k)\} $ is typically small; (b) holds given $\alpha$ and $\beta$ is sufficiently small. 
In addition, the 
{strong convexity of the objective} 
(c.f. Assumption \ref{assumption:strongly-cvx}) implies that
\begin{align}
    \label{inequality:TT-convergence-scvx-T2}
    \frac{\alpha H^{\texttt{TT}}_{\min}}{8 F_{\max}}\lnorm \nabla f(\bar W_k)\rnorm^2
    \ge&\ \frac{\alpha\mu^2 H^{\texttt{TT}}_{\min}}{8 F_{\max}}\lnorm \bar W_k - W^*\rnorm^2
    = \frac{\alpha\mu^2 H^{\texttt{TT}}_{\min}}{8 F_{\max}} \lnorm W_k + \gamma P_k - W^*\rnorm^2
    \\
    =&\ \frac{\alpha\mu^2\gamma^2 H^{\texttt{TT}}_{\min}}{8 F_{\max}} \lnorm P_k - \frac{W^*-W_k}{\gamma}\rnorm^2
    = \frac{\alpha\mu^2\gamma^2 H^{\texttt{TT}}_{\min}}{8 F_{\max}} \lnorm P_k - P^*(W_k)\rnorm^2.
    \nonumber
\end{align}
Substituting \eqref{inequality:TT-convergence-Lya-2-T5} and \eqref{inequality:TT-convergence-scvx-T2} back into \eqref{inequality:TT-convergence-Lya-2} yields 
\begin{align}
    \label{inequality:TT-convergence-Lya-3}
    &\ \mbE_{\xi_k}[\mbV_{k+1}] 
    \\
    \le&\ \mbV_k
    - \frac{\alpha H^{\texttt{TT}}_{\min}}{8 F_{\max}} \|\nabla f(\bar W_k)\|^2
    + 3\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    + 3\alpha^2 L F_{\max}^2 \sigma^2
    \bkeq
    - \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    \bkeq
    - \frac{\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    - \lp \frac{\alpha\mu^2\gamma^2}{8 F_{\max} H^{\texttt{TT}}_{\min}} - \frac{32\beta^2 F_{\max}^5}{\alpha \gamma \min\{H(W_k)\} H^{\texttt{TT}}_{\min}}\rp \lnorm P_k - P^*(W_k)\rnorm^2
    - \frac{5\beta^2 F_{\max}}{\alpha H^{\texttt{TT}}_{\min}} \|W_k-W^*\|^2_{H(W_k)}
    \nonumber\\
    =&\ \mbV_k
    - \frac{\alpha H^{\texttt{TT}}_{\min}}{8 F_{\max}} \|\nabla f(\bar W_k)\|^2
    + 3\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    + 3\alpha^2 L F_{\max}^2 \sigma^2
    \bkeq
    - \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    \bkeq
    - \frac{\alpha\mu^2\gamma^3 \min\{H(W_k)\}}{512 F_{\max}^5 H^{\texttt{TT}}_{\min}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    - \frac{\alpha\mu^2\gamma^2}{16 F_{\max} H^{\texttt{TT}}_{\min}} \lnorm P_k - P^*(W_k)\rnorm^2
    - \frac{5\alpha\mu^2\gamma^3}{512 F_{\max}^5 H^{\texttt{TT}}_{\min}} \|W_k-W^*\|^2_{H(W_k)}
    \nonumber
\end{align}
where the last step chooses the learning rate by
\begin{align}
    \beta = 
    \saveeq{inequality-RHS:beta}{
        \frac{\alpha\mu\gamma^{\frac{3}{2}} \sqrt{\min\{H(W_k)\}}}{16\sqrt{2} F_{\max}^3}
    }.
\end{align}
Rearranging inequality \eqref{inequality:TT-convergence-Lya-1} above, we have
\begin{align}
    \label{inequality:TT-convergence-Lya-4}
    &\ 
    \frac{\alpha}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    + \frac{\alpha}{8F_{\max} H^{\texttt{TT}}_{\min}}\|\nabla f(\bar W_k)\|^2
    \\
    &\ + \frac{\alpha\mu^2\gamma^3 \min\{H(W_k)\}}{512 F_{\max}^5 H^{\texttt{TT}}_{\min}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    +\frac{5\alpha\mu^2\gamma^3 \min\{H(W_k)\}}{512 F_{\max}^5 H^{\texttt{TT}}_{\min}} \|W_k-W^*\|^2_{H(W_k)}
    + \frac{\alpha\mu^2\gamma^2}{16 F_{\max} H^{\texttt{TT}}_{\min}} \lnorm P_k - P^*(W_k)\rnorm^2
    \nonumber\\
    \le&\ 
    \mbV_k - \mbE_{\xi_k}[\mbV_{k+1}]
    + 3\alpha^2 L F_{\max}^2 \sigma^2
    + 3\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty.
    \nonumber
\end{align}
Define the convergence metric $E^{\texttt{TT}}_K$ as
\begin{align}
    \label{definition:E-TT}
    E^{\texttt{TT}}_K := &\ 
    \frac{1}{K}\sum_{k=0}^{K-1}\mbE\bigg[
    \|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    + \frac{1}{H^{\texttt{TT}}_{\min}}\|\nabla f(\bar W_k)\|^2
    \\
    &\ + \frac{\mu^2\gamma^3 \min\{H(W_k)\}}{64 F_{\max}^4 H^{\texttt{TT}}_{\min}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \frac{5\mu^2\gamma^3}{64 F_{\max}^4 H^{\texttt{TT}}_{\min}} \|W_k-W^*\|^2_{H(W_k)}
    + \frac{\mu^2\gamma^2}{2 H^{\texttt{TT}}_{\min}} \lnorm P_k - P^*(W_k)\rnorm^2
    \bigg].
    \nonumber
\end{align}
Taking expectation over all $\xi_K, \xi_{K-1}, \cdots, \xi_0$, averaging \eqref{inequality:TT-convergence-Lya-4} over $k$ from $0$ to $K-1$, and choosing the parameter $\alpha$ as $\alpha = O\lp \frac{1}{F_{\max}}\sqrt{\frac{\mbV_0}{\sigma^2L K}} \rp$ deduce that
\begin{align}
    E^{\texttt{TT}}_K
    \le&\ 8F_{\max}
    \lp
        \frac{\mbV_0 - \mbE[\mbV_{k+1}]}{\alpha K}
        + 3\alpha L F_{\max}^2 \sigma^2
    \rp
    + 24F_{\max}\sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    \\
    \le&\ 8F_{\max}
    \lp
        \frac{\mbV_0}{\alpha K}
        + 3\alpha L F_{\max}^2 \sigma^2
    \rp
    + 24F_{\max}\sigma^2\times \frac{1}{K}\sum_{k=0}^{K-1}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    \nonumber \\
    = &\ 
    O\lp F_{\max}^2\sqrt{\frac{\mbV_0\sigma^2L}{K}}\rp
    + 24F_{\max}\sigma^2 S^{\texttt{TT}}_K.
    \nonumber
\end{align}
The 
{strong convexity of the objective (Assumption \ref{assumption:strongly-cvx})}
 implies that
\begin{align}
    \mbV_0 = f(\bar W_0)-f^* + C\|W_0-W^*\|^2
    \le \lp 1+\frac{2C}{\mu}\rp (f(W_0)-f^*).
\end{align}
Plugging it back to the above inequality, we have
\begin{align}
    E^{\texttt{TT}}_K
    = &\ 
    \saveeq{inequality-RHS:TT-convergence-upperbound}{
        O\lp F_{\max}^2\sqrt{\frac{(f(W_0) - f^*)\sigma^2L}{K}}\rp
        + 24F_{\max}\sigma^2 S^{\texttt{TT}}_K
    }.
\end{align}
The proof is completed.
\end{proof}

\subsection{Proof of Lemma \ref{lemma:TT-barW-descent}: Descent of sequence $\bar W_k$}
\label{section:proof-lemma:TT-barW-descent}
\LemmaTTbarWDescentScvx*
\begin{proof}[Proof of Lemma \ref{lemma:TT-barW-descent}]
The $L$-smooth assumption (Assumption \ref{assumption:Lip}) implies that
\begin{align}
    \label{inequality:TT-convergence-1}
    &\ \mathbb{E}_{\xi_k}[f(\bar W_{k+1})] \le f(\bar W_k)
    +\mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), \bar W_{k+1}-\bar W_k\ra]
    + \frac{L}{2}\mathbb{E}_{\xi_k}[\|\bar W_{k+1}-\bar W_k\|^2] \\
    =&\ f(\bar W_k)
    + \gamma \underbrace{\mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), P_{k+1}-P_k\ra]}_{(a)}
    + \underbrace{\mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), W_{k+1}-W_k\ra]}_{(b)}
    + \underbrace{\frac{L}{2}\mathbb{E}_{\xi_k}[\|\bar W_{k+1}-\bar W_k\|^2]}_{(c)}.
    \nonumber
\end{align}
Next, we will handle the each term in the \ac{RHS} of \eqref{inequality:TT-convergence-1} separately.

\textbf{Bound of the second term (a).}
To bound term (a) in the \ac{RHS} of \eqref{inequality:TT-convergence-1}, we leverage the assumption that noise has expectation $0$ (Assumption \ref{assumption:noise})
\begin{align}
    \label{inequality:TT-convergence-1-T2}
    &\ \mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), P_{k+1}-P_k\ra] \\
    =&\ \alpha\mathbb{E}_{\xi_k}\lB \la 
    \nabla f(\bar W_k)\odot \sqrt{F(P_k)}, 
    \frac{P_{k+1}-P_k}{\alpha\sqrt{F(P_k)}}+(\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot \sqrt{F(P_k)}
    \ra\rB
    \nonumber \\
    =&\ - \frac{\alpha}{2} \|\nabla f(\bar W_k)\odot \sqrt{F(P_k)}\|^2 
    \nonumber \\
    &\ - \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{P_{k+1}-P_k}{\sqrt{F(P_k)}}+\alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot \sqrt{F(P_k)}\rnorm^2\rB
    \nonumber \\
    &\ + \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{P_{k+1}-P_k}{\sqrt{F(P_k)}}+\alpha \nabla f(\bar W_k; \xi_k)\odot \sqrt{F(P_k)}\rnorm^2\rB.
    \nonumber
\end{align}
The second term in the \ac{RHS} of \eqref{inequality:TT-convergence-1-T2} can be bounded by 
\begin{align}
    \label{inequality:TT-convergence-1-T2-T2}
    &\frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{P_{k+1}-P_k}{\sqrt{F(P_k)}}+\alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot \sqrt{F(P_k)}\rnorm^2\rB\\
    =&\ \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{P_{k+1}-P_k+\alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)}{\sqrt{F(P_k)}}\rnorm^2\rB
    \nonumber\\
    \ge&\ \frac{1}{2\alpha F_{\max}}\mathbb{E}_{\xi_k}\lB\| P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\|^2\rB
    \nonumber.
\end{align}

The third term in the \ac{RHS} of \eqref{inequality:TT-convergence-1-T2} can be bounded by variance decomposition and bounded variance assumption (Assumption \ref{assumption:noise})
\begin{align}
    \label{inequality:TT-convergence-1-T2-T3}
    &\ \frac{1}{2\alpha}\mathbb{E}_{\xi_k}\lB\lnorm\frac{P_{k+1}-P_k}{\sqrt{F(P_k)}}+\alpha \nabla f(\bar W_k; \xi_k)\odot \sqrt{F(P_k)}\rnorm^2\rB
    \\
    \le&\ \frac{\alpha}{2}\mathbb{E}_{\xi_k}\lB\lnorm|\nabla f(\bar W_k; \xi_k)|\odot\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2\rB
    \nonumber \\
    \le&\ \frac{\alpha}{2}\lnorm|\nabla f(\bar W_k)|\odot\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty.
    \nonumber
\end{align}

Notice that the first term in the \ac{RHS} of \eqref{inequality:TT-convergence-1-T2} and the second term in the \ac{RHS} of \eqref{inequality:TT-convergence-1-T2-T3} can be bounded together
\begin{align}
    \label{inequality:TT-convergence-1-T2-mixed}
    &\ - \frac{\alpha}{2} \|\nabla f(\bar W_k) \odot \sqrt{F(P_k)}\|^2 
    + \frac{\alpha}{2} \lnorm|\nabla f(\bar W_k)| \odot \frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm^2 \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(\bar W_k)]_d^2 \lp [F(P_k)]_d-\frac{[G(P_k)]^2_d}{[F(P_k)]_d}\rp\rp
    \nonumber \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(\bar W_k)]_d^2 \lp \frac{[F(P_k)]^2_d-[G(P_k)]^2_d}{[F(P_k)]_d}\rp\rp
    \nonumber \\
    \le&\ -\frac{\alpha}{2 F_{\max}}\sum_{d\in[D]} \lp [\nabla f(\bar W_k)]_d^2 \lp [F(P_k)]_d^2-[G(P_k)]^2_d\rp\rp
    \nonumber\\
    =&\ -\frac{\alpha}{2 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
    \le 0.
    \nonumber
\end{align}
Plugging \eqref{inequality:TT-convergence-1-T2-T2} to \eqref{inequality:TT-convergence-1-T2-mixed} into \eqref{inequality:TT-convergence-1-T2}, we bound the term (a) by
\begin{align}
    \label{inequality:TT-convergence-1-T2-2}
    &\ \mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), P_{k+1}-P_k\ra]
    \\
    \le&\ -\frac{\alpha}{2 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    \nonumber \\
    &\ - \frac{1}{2\alpha F_{\max}}\mathbb{E}_{\xi_k}\lB\lnorm P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB.
    \nonumber
\end{align}

\textbf{Bound of the third term (b).} By Young's inequality, we have
\begin{align}
    \label{inequality:TT-convergence-1-T3}
    &\ \mathbb{E}_{\xi_k}[\la \nabla f(\bar W_k), W_{k+1}-W_k\ra]
    \le \frac{\alpha}{4F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)} 
    + \frac{F_{\max}}{\alpha}\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}].
\end{align}

\textbf{Bound of the third term (c).} Repeatedly applying inequality $\|U+V\|^2\le 2\|U\|^2+2\|V\|^2$ for any $U, V\in\reals^D$, we have
\begin{align}
    \label{inequality:TT-convergence-1-T4}
    &\ \frac{L}{2}\mathbb{E}_{\xi_k}[\|\bar W_{k+1}-\bar W_k\|^2]
    \\
    \le&\ L\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    + L\mathbb{E}_{\xi_k}[\|P_{k+1}-P_k\|^2] 
    \nonumber\\
    \le&\ L\mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    + 2L\mathbb{E}_{\xi_k}\lB\lnorm P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \bkeq
    + 2\alpha^2 L\mathbb{E}_{\xi_k}\lB\lnorm (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \nonumber\\
    \le&\ \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    + 2L\mathbb{E}_{\xi_k}\lB\lnorm P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \bkeq
    + 2\alpha^2 L F_{\max}^2 \sigma^2
    \nonumber
\end{align}
where the last inequality comes from the bounded variance assumption (Assumption \ref{assumption:noise})
\begin{align}
    \label{inequality:TT-convergence-1-T4-S1-T3}
    &\ 2\alpha^2 L\mathbb{E}_{\xi_k}\lB\lnorm (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \\
    \le&\ 2\alpha^2 LF_{\max}^2\mathbb{E}_{\xi_k}\lB\lnorm \nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k)\rnorm^2\rB
    \nonumber\\
    \le&\ 2\alpha^2 LF_{\max}^2\sigma^2.
    \nonumber
\end{align}

\textbf{Combination of the upper bound $(a)$, $(b)$, and $(c)$.} Plugging \eqref{inequality:TT-convergence-1-T2-2}, \eqref{inequality:TT-convergence-1-T3}, \eqref{inequality:TT-convergence-1-T4} into \eqref{inequality:TT-convergence-1}, we derive
\begin{align}
    \label{inequality:TT-convergence-2}
    \mathbb{E}_{\xi_k}[f(\bar W_{k+1})] 
    \le&\ f(\bar W_k)
    -\frac{\alpha}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
    +\frac{\alpha\sigma^2}{2}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
    \\
    \hspace{-1em}
    &\ - \lp\frac{1}{2\alpha F_{\max}}-2L\rp~\mathbb{E}_{\xi_k}\lB\lnorm P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \bkeq
    + \frac{F_{\max}}{\alpha}~\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}\rB
    + \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
    + 2\alpha^2 L F_{\max}^2 \sigma^2
    \nonumber.
\end{align}
We bound the fourth term in the \ac{RHS} of \eqref{inequality:TT-convergence-2} using the similar technique as in \eqref{inequality:ASGD-convergence-2-T3}
\begin{align}
    \label{inequality:TT-converge-2-T4}
    &\ \mathbb{E}_{\xi_k}\lB\lnorm P_{k+1}-P_k + \alpha (\nabla f(\bar W_k; \xi_k)-\nabla f(\bar W_k))\odot F(P_k)\rnorm^2\rB
    \\
    \ge&\ \frac{\alpha^2}{2} \|\nabla f(\bar W_k) \odot F(P_k) + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
    -\alpha^2 F_{\max} \sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty.
    \nonumber
\end{align}
Inequality \eqref{inequality:TT-converge-2-T4} as well as the learning rate rule $\alpha\le\frac{1}{4LF_{\max}}$ leads to the conclusion
\begin{align}
    \saveeq{inequality-saved:TT-barW-descent}{
        \mathbb{E}_{\xi_k}[f(\bar W_{k+1})] 
        \le&\  f(\bar W_k)
        -\frac{\alpha}{4 F_{\max}}\|\nabla f(\bar W_k)\|^2_{H(P_k)}
        + 2\alpha\sigma^2\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
        + 2\alpha^2 L F_{\max}^2 \sigma^2
        \\
        &\ 
        - \frac{\alpha\gamma}{8 F_{\max}}\|\nabla f(\bar W_k) \odot F(P_k) 
        + |\nabla f(\bar W_k)| \odot G(P_k)\|^2
        \bkeq
        + \frac{F_{\max}}{\alpha}\mathbb{E}_{\xi_k}\lB\|W_{k+1}-W_k\|^2_{H(P_k)^\dag}\rB
        + \mathbb{E}_{\xi_k}[\|W_{k+1}-W_k\|^2]
        \nonumber.
    }
\end{align}
The proof is completed.
\end{proof}

\subsection{Proof of Lemma \ref{lemma:TT-W-descent}: Descent of sequence $W_k$}
\label{section:proof-lemma:TT-W-descent}
\LemmaTTWDescentScvx*
\begin{proof}[Proof of Lemma \ref{lemma:TT-W-descent}]
The proof begins from manipulating the norm $\|W_{k+1}-W^*\|^2$
\begin{align}
    \label{inequality:TT-convergence-scvx-S1}
    \|W_{k+1}-W^*\|^2 =&\ \|W_k-W^*\|^2 + 2\la W_k-W^*, W_{k+1}-W_k\ra + \|W_{k+1}-W_k\|^2.
\end{align}

Revisit that we interpret $P_k$ as an approximated solution of the constrained optimization problem \eqref{problem:P-residual} with $W_k$ fixed, namely $P^*(W) := \frac{W^*-W}{\gamma}$.
Therefore, we bound the second term in the \ac{RHS} of \eqref{inequality:TT-convergence-scvx-S1} by
\begin{align}
    \label{inequality:TT-convergence-scvx-S1-T2-S1}
    &\ 2\la W_k-W^*, W_{k+1}-W_k\ra
    \\
    =&\ 2\la W_k-W^*, \beta P_{k+1}\odot F(W_k) - \beta|P_{k+1}|\odot G(W_k)\ra
    \nonumber\\
    =&\ 2\beta \la W_k-W^*, P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\ra
    \bkeq
    + 2\beta \la W_k-W^*, P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)-(P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k))\ra.
    \nonumber
\end{align}

The first term in the \ac{RHS} of \eqref{inequality:TT-convergence-scvx-S1-T2-S1} is bounded by
\begin{align}
    \label{inequality:TT-convergence-scvx-S1-T2-S1-T1}
    &\ 2\beta\la W_k-W^*, P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\ra
    \\
    =&\ 2\beta\la (W_k-W^*)\odot \sqrt{F(W_k)}, \frac{P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)}{\sqrt{F(W_k)}}\ra
    \nonumber\\
    =&\ -\frac{2\beta}{\gamma}\la (W_k-W^*)\odot \sqrt{F(W_k)}, (W_k-W^*)\odot \sqrt{F(W_k)}\ra
	\bkeq
	+ \frac{2\beta}{\gamma}\la (W_k-W^*)\odot \sqrt{F(W_k)}, |W_k-W^*|\odot \frac{G(W_k)}{\sqrt{F(W_k)}}\ra
    \nonumber\\
    \eqmark{a}&\ - \frac{\beta}{\gamma} \|(W_k-W^*)  \odot \sqrt{F(W_k)}\|^2 
	+ \frac{\beta}{\gamma} \lnorm |W_k-W^*| \odot \frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2 
    \bkeq
	- \frac{\beta}{\gamma} \lnorm(W_k-W^*) \odot \sqrt{F(W_k)} + |W_k-W^*|\odot \frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2 
    \nonumber\\
    \lemark{b}&\ - \frac{\beta}{\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
	- \frac{\beta}{\gamma} \lnorm(W_k-W^*) \odot \sqrt{F(W_k)} + |W_k-W^*|\odot \frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2 
    \nonumber\\
    \lemark{c}&\ - \frac{\beta}{\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
	- \frac{\beta\gamma}{F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \nonumber
\end{align}
where $(a)$ leverages the equality $2\la U, V\ra = \|U\|^2-\|V\|^2-\|U-V\|^2$ for any $U, V\in\reals^D$, $(b)$ is achieved by similar technique \eqref{inequality:ASGD-converge-linear-3}, and $(c)$ comes from 
\begin{align}
    &\ - \frac{\beta}{\gamma} \lnorm(W_k-W^*) \odot \sqrt{F(W_k)} + |W_k-W^*|\odot \frac{G(W_k)}{\sqrt{F(W_k)}} \rnorm^2 
    \\
    =&\ - \beta\gamma \lnorm \frac{1}{\sqrt{F(W_k)}}\odot\lp\frac{W_k-W^*}{\gamma} \odot {F(W_k)} + \labs\frac{W_k-W^*}{\gamma}\rabs\odot G(W_k)\rp \rnorm^2 
    \nonumber\\
    \le&\ - \frac{\beta\gamma}{F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2.
    \nonumber
\end{align}

The second term in the \ac{RHS} of \eqref{inequality:TT-convergence-scvx-S1-T2-S1} is bounded by the Lipschitz continuity of analog update (c.f. Lemma \ref{lemma:lip-analog-update})
\begin{align}
    \label{inequality:TT-convergence-scvx-S1-T2-S1-T2}
    &\ \frac{2\beta}{\gamma} \la W_k-W^*, P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)-(P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k))\ra
    \nonumber \\
    \le&\ 
    \frac{\beta}{2\gamma F_{\max}}\|W_k-W^*\|^2_{H(W_k)} + \frac{2\beta F_{\max}}{\gamma}
    \\&\ \hspace{-0.5em}
    \times\lnorm P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)-(P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)) \rnorm^2_{H(W_k)^\dag}
    \nonumber \\
    \le&\ 
    \frac{\beta}{2\gamma F_{\max}}\|W_k-W^*\|^2_{H(W_k)}
    + \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}.
    \nonumber 
\end{align}


Substituting \eqref{inequality:TT-convergence-scvx-S1-T2-S1-T1} and \eqref{inequality:TT-convergence-scvx-S1-T2-S1-T2} into \eqref{inequality:TT-convergence-scvx-S1-T2-S1}, we bound the second term in the \ac{RHS} of \eqref{inequality:TT-convergence-scvx-S1} by
\begin{align}
    \label{inequality:TT-convergence-scvx-S1-T2S2}
    &\ 2\la W_k-W^*, W_{k+1}-W_k\ra
    \\
    \le&\ - \frac{\beta}{\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
	- \frac{\beta\gamma}{F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}.
    \nonumber
\end{align}
The third term in the \ac{RHS} of \eqref{inequality:TT-convergence-scvx-S1} is bounded by the Lipschitz continuity of analog update (c.f. Lemma \ref{lemma:lip-analog-update})
\begin{align}
    \label{inequality:TT-convergence-scvx-S1-T3}
    &\ \|W_{k+1}-W_k\|^2
    = \beta^2 \|P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)\|^2
    \\
    \le&\ 2\beta^2 \|P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\|^2
    \nonumber \\
    &\ + 2\beta^2 \|P_{k+1}\odot F(W_k) - |P_{k+1}|\odot G(W_k)-(P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k))\|^2
    \nonumber\\
    \le&\ 2\beta^2 \|P^*(W_k)\odot F(W_k) - |P^*(W_k)|\odot G(W_k)\|^2
    + 2\beta^2 \|P_{k+1} - P^*(W_k)\|^2.
    \nonumber
\end{align}

Plugging \eqref{inequality:TT-convergence-scvx-S1-T2S2} and \eqref{inequality:TT-convergence-scvx-S1-T3} into \eqref{inequality:TT-convergence-scvx-S1} yields
\begin{align}
    \|W_{k+1}-W^*\|^2 \le&\ \|W_k-W^*\|^2 - \frac{\beta}{2\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
    \\
	&\ - \lp\frac{\beta\gamma}{F_{\max}} - 2\beta^2\rp\lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}
    + 2\beta^2 \|P_{k+1} - P^*(W_k)\|^2.
    \nonumber
\end{align}
Notice the learning rate $\beta$ is chosen as $\beta \le \frac{\gamma}{2F_{\max}}$, we have
\begin{align}
\saveeq{inequality-saved:TT-W-descent}{
    \|W_{k+1}-W^*\|^2 \le&\ \|W_k-W^*\|^2 - \frac{\beta}{2\gamma F_{\max}} \|W_k-W^*\|^2_{H(W_k)}
    \\
	&\ - \frac{\beta\gamma}{2F_{\max}} \lnorm P^*(W_k) \odot F(W_k) - |P^*(W_k)|\odot G(W_k) \rnorm^2 
    \bkeq
    + \frac{2\beta F_{\max}^3}{\gamma}\lnorm P_{k+1} - P^*(W_k)\rnorm^2_{H(W_k)^\dag}
    + 2\beta^2 \|P_{k+1} - P^*(W_k)\|^2
    \nonumber
}
\end{align}
which completes the proof. 
\end{proof}

\subsection{Proof of Lemma \ref{lemma:TT-P-descent}: Descent of sequence $P_k$}
\label{section:proof-lemma:TT-P-descent}
\LemmaTTPDescentScvx*
\begin{proof}[Proof of Lemma \ref{lemma:TT-P-descent}]
The proof begins from manipulating the norm $\|P_{k+1} - P^*(W_k)\|^2$
\begin{align}
    \label{inequality:TT-convergence-P-S1}
    \|P_{k+1}-P^*(W_k)\|^2 =&\ \|P_k-P^*(W_k)\|^2 + 2\la P_k-P^*(W_k), P_{k+1}-P_k\ra + \|P_{k+1}-P_k\|^2.
\end{align}
To bound the second term, we need the following equality. 
\begin{align}
    \label{inequality:TT-convergence-P-S1-T2}
    &\ 2\mbE_{\xi_k}[\la P_k-P^*(W_k), P_{k+1}-P_k\ra]
    \\
    =&\ -2\alpha \mbE_{\xi_k}[\la P_k-P^*(W_k), \nabla f(\bar W_k; \xi_k) \odot F(P_k) - |\nabla f(\bar W_k; \xi_k) |\odot G(P_k)\ra]
    \nonumber\\
    =&\ -2\alpha \mbE_{\xi_k}[\la P_k-P^*(W_k), \nabla f(\bar W_k; \xi_k) \odot F(P_k) \ra]
    \bkeq
    + 2\alpha \mbE_{\xi_k}[\la P_k-P^*(W_k), |\nabla f(\bar W_k; \xi_k) |\odot G(P_k)\ra]
    \nonumber\\
    =&\ -2\alpha \la P_k-P^*(W_k), \nabla f(\bar W_k) \odot F(P_k) \ra
    + 2\alpha \la P_k-P^*(W_k), |\nabla f(\bar W_k) |\odot G(P_k)\ra
    \bkeq
    + 2\alpha \mbE_{\xi_k}[\la P_k-P^*(W_k), (|\nabla f(\bar W_k)|-|\nabla f(\bar W_k; \xi_k)|)\odot G(P_k)\ra]
    \nonumber\\
    =&\ -2\alpha \underbrace{\la P_k-P^*(W_k), \nabla f(\bar W_k) \odot F(P_k) - |\nabla f(\bar W_k) |\odot G(P_k)\ra}_{(T1)}
    \bkeq
    + 2\alpha \underbrace{\mbE_{\xi_k}[\la P_k-P^*(W_k), (|\nabla f(\bar W_k)|-|\nabla f(\bar W_k; \xi_k)|)\odot G(P_k)\ra]}_{(T2)}
    \nonumber
\end{align}

\textbf{Upper bound of the first term $(T1)$.}
With Lemma \ref{lemma:element-wise-product-error}, the second term in the \ac{RHS} of \eqref{inequality:TT-convergence-P-S1} can be bounded by
\begin{align}
    \label{inequality:TT-convergence-P-S1-T2-T1-S1}
    &\ -2\alpha \la P_k-P^*(W_k), \nabla f(\bar W_k) \odot F(P_k) - |\nabla f(\bar W_k) |\odot G(P_k)\ra
    \\
    =&\ -2\alpha \la P_k-P^*(W_k), \nabla f(\bar W_k) \odot q_s(P_k)\ra
    \nonumber\\
    \le&\ -2\alpha C_{k, +} \la P_k-P^*(W_k), \nabla f(\bar W_k) \ra
     +2\alpha C_{k, -} \la |P_k-P^*(W_k)|, |\nabla f(\bar W_k)| \ra
    \nonumber
\end{align}
where $C_{k,+}$ and $C_{k,-}$ are defined by
\begin{align}
    C_{k, +} :=& \frac{1}{2}\lp\max_{i\in[D]}\{q_s([P_k]_i)\} + \min_{i\in[D]}\{q_s([P_k]_i)\}\rp, \\
    C_{k, -} :=& \frac{1}{2}\lp\max_{i\in[D]}\{q_s([P_k]_i)\} - \min_{i\in[D]}\{q_s([P_k]_i)\}\rp.
\end{align}

In the inequality above, the first term can be bounded by the strong convexity of $f$. Let $\varphi(P) := f(W+\gamma P)$ which is $\gamma^2 L$-smooth and 
{$\gamma^2\mu$-strongly convex}. 
It can be verified that $\varphi(P)$ has gradient $\nabla \varphi(P_k) = \nabla_{P_k} f(W_k+\gamma P_k) = \gamma \nabla f(\bar W_k)$ and optimal point $P^*(W)$.
Leveraging Theorem 2.1.9 in \cite{Nesterov2003IntroductoryLO}, we have
\begin{align}
    &\ \la \nabla f(\bar W_k), P_k-P^*(W_k)\ra 
    = \frac{1}{\gamma}\la \nabla \varphi(P_k), P_k-P^*(W_k)\ra
    \\
    \ge&\ \frac{1}{\gamma}\lp\frac{\gamma^2\mu \cdot \gamma^2 L}{\gamma^2\mu+\gamma^2 L}\|P_k-P^*(W_k)\|^2 + \frac{1}{\gamma^2\mu+\gamma^2 L}\|\nabla \varphi(P_k)\|^2\rp
    \nonumber\\
    =&\ \frac{\gamma\mu L}{\mu+L}\|P_k-P^*(W_k)\|^2 + \frac{1}{\gamma(\mu+L)}\|\nabla f(\bar W_k)\|^2.
    \nonumber
\end{align}
The second term in the \ac{RHS} of \eqref{inequality:TT-convergence-P-S1-T2-T1-S1} can be bounded by Young's inequality $2\la x, y\ra\le u\|x\|^2 + \frac{1}{u}\|y\|^2$ with any $u>0$ and $x, y\in\reals^D$
\begin{align}
    \label{inequality:TT-convergence-P-S1-T2-T1-S1-T2}
    & 2\alpha C_{k,-} \la |P_k-P^*(W_k)|, |\nabla f(\bar W_k)| \ra
    \\
    \le&\ \frac{\alpha C_{k,-}^2\gamma(\mu+L)}{C_{k,+}} \|P_k-P^*(W_k)\|^2 
    + \frac{\alpha C_{k,+}}{\gamma(\mu+L)} \|\nabla f(\bar W_k)\|^2
    \nonumber
\end{align}
where $u$ is chosen to align the coefficient in front of $\|\nabla f(\bar W_k)\|^2$.
Therefore, $(T1)$ in \eqref{inequality:TT-convergence-P-S1-T2-T1-S1} becomes
\begin{align}
    \label{inequality:TT-convergence-P-S1-T2-T1-S2}
    &\ -2\alpha \la P_k-P^*(W_k), \nabla f(\bar W_k) \odot F(P_k) - |\nabla f(\bar W_k) |\odot G(P_k)\ra
    \\
    \le&\ - \lp\frac{2\alpha \gamma\mu L C_{k, +}}{\mu+L}-\frac{\alpha C_{k,-}^2\gamma (\mu+L)}{C_{k,+}}\rp\|P_k-P^*(W_k)\|^2 - \frac{\alpha C_{k, +}}{\gamma(\mu+L)}\|\nabla f(\bar W_k)\|^2.
    \nonumber
\end{align}

\textbf{Upper bound of the second term $(T2)$.} Leveraging the Young's inequality $2\la x, y\ra\le u\|x\|^2 + \frac{1}{u}\|y\|^2$ with any $u>0$ and $x, y\in\reals^D$, we have
\begin{align}
    & 2\alpha \mbE_{\xi_k}[\la P_k-P^*(W_k), (|\nabla f(\bar W_k)|-|\nabla f(\bar W_k; \xi_k)|)\odot G(P_k)\ra]
    \\
    =&\ 2\alpha \mbE_{\xi_k}\lB\la (P_k-P^*(W_k))\odot \sqrt{F(P_k)}, (|\nabla f(\bar W_k)|-|\nabla f(\bar W_k; \xi_k)|)\odot \frac{G(P_k)}{\sqrt{F(P_k)}}\ra\rB
    \nonumber\\
    \lemark{a}&\ \frac{\alpha \gamma \mu L C_{k, +}}{(\mu+L)F_{\max}} \|(P_k-P^*(W_k))\odot \sqrt{F(P_k)}\|^2 
    \bkeq
    + \frac{\alpha (\mu+L)F_{\max}}{\gamma \mu L C_{k, +}} \mbE_{\xi_k}\lB\lnorm(|\nabla f(\bar W_k)|-|\nabla f(\bar W_k; \xi_k)|)\odot \frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm^2\rB
    \nonumber\\
    \lemark{b}&\ \frac{\alpha \gamma \mu L C_{k, +}}{(\mu+L)F_{\max}} \|(P_k-P^*(W_k))\odot \sqrt{F(P_k)}\|^2 
    \bkeq
    + \frac{\alpha(\mu+L)F_{\max}}{\gamma \mu L C_{k, +}} \mbE_{\xi_k}\lB\lnorm(|\nabla f(\bar W_k) - \nabla f(\bar W_k; \xi_k)|)\odot \frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm^2\rB
    \nonumber\\
    \eqmark{c}&\ \frac{\alpha \gamma \mu L C_{k, +}}{(\mu+L)F_{\max}} \|(P_k-P^*(W_k))\odot \sqrt{F(P_k)}\|^2 
    + \frac{\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L C_{k, +}}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    \nonumber\\
    \lemark{d}&\ \frac{\alpha \gamma \mu L C_{k, +}}{\mu+L} \|P_k-P^*(W_k)\|^2 
    + \frac{\alpha(\mu+L)F_{\max}\sigma^2}{\gamma\mu L C_{k, +}}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    \nonumber
\end{align}
where $(a)$ choose $u>0$ to align the coefficient in front of $\|P_k-P^*(W_k)\|^2$ in the \ac{RHS} of \eqref{inequality:TT-convergence-P-S1-T2-T1-S2}, $(b)$ applies $||x|-|y|| \le |x-y|$ for any $x, y\in\reals$, $(c)$ uses the bounded variance assumption (c.f. Assumption \ref{assumption:noise}), and $(d)$ leverages the fact that $F(P_k)$ is bounded by $F_{\max}$ element-wise.

Combining the upper bound of $(T1)$ and $(T2)$, we bound \eqref{inequality:TT-convergence-P-S1-T2} by
\begin{align}
    \label{inequality:TT-convergence-P-S1-T2-S2}
    &\ 2\mbE_{\xi_k}[\la P_k-P^*(W_k), P_{k+1}-P_k\ra]
    \\
    \le&\ - \lp\frac{\alpha \gamma \mu L C_{k, +}}{\mu+L}-\frac{\alpha C_{k,-}^2 \gamma (\mu+L)}{C_{k,+}}\rp\|P_k-P^*(W_k)\|^2 
    \bkeq
    -  \frac{\alpha C_{k, +}}{\gamma(\mu+L)}\|\nabla f(\bar W_k)\|^2
    + \frac{\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L C_{k, +}}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    \nonumber\\
    \le&\ - \frac{\alpha \gamma \mu L C_{k, +}}{2(\mu+L)}\|P_k-P^*(W_k)\|^2 
    - \frac{\alpha C_{k, +}}{\gamma(\mu+L)}\|\nabla f(\bar W_k)\|^2
    + \frac{\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L C_{k, +}}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    \nonumber
\end{align}
where the last inequality holds when $\gamma$ is sufficiently large, $P_k$ as well as $C_{k,-}$ are sufficiently closed to 0, and the following inequality holds
\begin{align}
    (\mu+L)\frac{C_{k,-}^2}{C_{k,+}^2}
    \le \frac{\mu L}{2(\mu+L)}.
\end{align}
Furthermore, the last term in the \ac{RHS} of \eqref{inequality:TT-convergence-P-S1} can be bounded by the Lipschitz continuity of analog update (c.f. Lemma \ref{lemma:lip-analog-update}) and the bounded variance assumption (c.f. Assumption \ref{assumption:noise})
\begin{align} 
    \label{inequality:TT-convergence-P-S1-T3}
    \mbE_{\xi_k}[\|P_{k+1}-P_k\|^2]
    =&\ \mbE_{\xi_k}[\|\alpha\nabla f(\bar W_k; \xi_k)\odot F(P_k) - \alpha|\nabla f(\bar W_k; \xi_k)|\odot G(P_k)\|^2]
    \\
    \le&\ \alpha^2 F_{\max}^2\mbE_{\xi_k}[\|\nabla f(\bar W_k; \xi_k)\|^2]
    \nonumber\\
    =&\ \alpha^2 F_{\max}^2\|\nabla f(\bar W_k)\|^2 + \alpha^2 F_{\max}^2 \sigma^2
    \nonumber\\
    \le&\ \frac{\alpha C_{k, +}}{\gamma(\mu+L)}\|\nabla f(\bar W_k)\|^2 + \alpha^2 F_{\max}^2 \sigma^2
    \nonumber
\end{align}
where the last inequality holds if $\alpha$ is sufficiently small.

Plugging inequality \eqref{inequality:TT-convergence-P-S1-T2-S2} and \eqref{inequality:TT-convergence-P-S1-T3} above into \eqref{inequality:TT-convergence-P-S1} yields
\begin{align}
    &\ \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2] \\
    \le&\ 
    \lp 1 - \frac{\alpha \gamma \mu L C_{k, +}}{2(\mu+L)}\rp \|P_k-P^*(W_k)\|^2 
    + \frac{\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L C_{k, +}}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    + \alpha^2 F_{\max}^2 \sigma^2.
    \nonumber
\end{align}
By definition of $C_{k,+}$, when the saturation degree of $P_k$ is properly limited, we have $C_{k,+} \ge \frac{1}{2}$. Therefore, we have
\begin{align}
\saveeq{inequality-saved:TT-P-descent}{
    &\ \mbE_{\xi_k}[\|P_{k+1} - P^*(W_k)\|^2] \\
    \le&\ \lp
        1- \frac{\alpha\gamma \mu L}{4(\mu+L)}
    \rp \|P_k-P^*(W_k)\|^2 
    + \frac{2\alpha(\mu+L)F_{\max}\sigma^2}{\gamma \mu L}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}\rnorm_\infty^2
    + \alpha^2 F_{\max}^2 \sigma^2
    \nonumber
}
\end{align}
which completes the proof.
\end{proof}

\subsection{Proof of Corollary \ref{theorem:TT-convergence-scvx-final}: Exact convergence of \TT}
\label{section:proof-TT-convergence-scvx-final}
\ThmTTConvergenceScvxFinal*
\begin{proof}[Proof of Corollary \ref{theorem:TT-convergence-scvx-final}]
    From Theorem \ref{theorem:TT-convergence-scvx}, we have
    \begin{align}
        \|\nabla f(\bar W_k)\|^2 \le O(E^{\texttt{TT}}_K) \le
        O\lp F_{\max}^2\sqrt{\frac{(f(W_0) - f^*)\sigma^2L}{K}}\rp
        + 24F_{\max}\sigma^2 S^{\texttt{TT}}_K.
    \end{align}
    Under the zero-shift assumption (Assumption \ref{assumption:zero-sp}) and the Lipschitz continuity of the response functions, it holds directly that 
    \begin{align}
        \lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
        \le \lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2
        = \lnorm\frac{G(P_k)}{\sqrt{F(P_k)}}-\frac{G(0)}{\sqrt{F(0)}}\rnorm^2
        \le L_S^2 \|P_k\|^2
    \end{align}
    where $L_S\ge 0$ is a constant. Using $\|U+V\|^2\le 2\|U\|^2+2\|V\|^2$ for any $U, V\in\reals^D$, we have
    \begin{align}
        \|P_k\|^2 \le 2\|P_k-P^*(W_k)\|^2 + 2\|P^*(W_k)\|^2
        = 2\|P_k-P^*(W_k)\|^2 + \frac{2}{\gamma^2}\lnorm W_k - W^*\rnorm^2
    \end{align}
    where the last inequality comes from the definition of $P^*(W_k)$, as well as the definition of $P^*(W)$. 
    Recall that convergence metric $E^{\texttt{TT}}_K$ defined in \eqref{definition:E-TT} is in the order of 
    \begin{align}
        E^{\texttt{TT}}_K \ge&\ \Omega\lp \gamma^3\|W_k-W^*\|^2_{H(W_k)}
        + \gamma^2 \lnorm P_k - P^*(W_k)\rnorm^2\rp \\
        \ge&\ \Omega\lp \min\{H(W_k)\} \gamma^3\|W_k-W^*\|^2
        + \gamma^2 \lnorm P_k - P^*(W_k)\rnorm^2\rp
        \nonumber \\
        \ge&\ \Omega\lp \frac{1}{\gamma^2}\|W_k-W^*\|^2
        + \gamma^2 \lnorm P_k - P^*(W_k)\rnorm^2\rp.
        \nonumber
    \end{align}
    Therefore, we have
    \begin{align}
        S^{\texttt{TT}}_K = \frac{1}{K}\sum_{k=0}^{K}\lnorm\frac{G(P_k)}{\sqrt{F(P_k)}} \rnorm^2_\infty
        \le \frac{1}{K}\sum_{k=0}^{K}\lp 2\|P_k-P^*(W_k)\|^2 + \frac{2}{\gamma^2}\lnorm W_k - W^*\rnorm^2\rp
        \le O(E^{\texttt{TT}}_K)
    \end{align}
    where the last inequality holds if $\gamma$ is sufficiently large. Considering that, $E^{\texttt{TT}}_K - S^{\texttt{TT}}_K \ge \Omega(E^{\texttt{TT}}_K) \ge 0$ and the conclusion is reached directly from Theorem \ref{theorem:TT-convergence-scvx}.
\end{proof}

\section{Proof of Theorem \ref{theorem:AGD-converge-non-cvx}: Convergence of \texttt{Analog\;GD}}
\label{section:proof-AGD-converge-non-cvx}
In Section \ref{section:convergence-ASGD}, we showed that \AnalogSGD~converges to a critical point inexactly with asymptotic error proportional to the noise variance $\sigma^2$. Intuitively, without the effect of noise, \texttt{Analog\;GD} converges to the critical point. Define the convergence metric by
\begin{align}
    E_K^{\texttt{AGD}} :=
    \frac{1}{K}\sum_{k=0}^{K-1} \Big(\lnorm \nabla f(W_k) \odot F(W_k) - |\nabla f(W_k)| \odot G(W_k)\rnorm^2 + \lnorm \nabla f(W_k)\rnorm^2_{H(W_k)}\Big).
\end{align}
The convergence is guaranteed by the following theorem.

\begin{theorem}[Convergence of \texttt{Analog\;GD}]
    \label{theorem:AGD-converge-non-cvx}
    Under Assumption \ref{assumption:Lip}--\ref{assumption:noise}, 
    it holds that
    \begin{align}
        E_K^{\texttt{AGD}} \le \frac{8L(f(W_0) - f^*)F_{\max}^2}{K}.
    \end{align}
    Further, if $H^{\texttt{ASGD}}_{\min} := \min_{k\in[K]}\min \{Q_+(W_k)Q_-(W_k)\} > 0$, it holds that
    \begin{align}
        \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla f(W_k)\|^2
        \le
        \frac{2L(f(W_0) - f^*)F_{\max}^2}{K H^{\texttt{ASGD}}_{\min}}.
    \end{align}
\end{theorem}
\begin{proof}[Proof of Theorem \ref{theorem:AGD-converge-non-cvx}]
The $L$-smooth assumption (Assumption \ref{assumption:Lip}) implies that
\begin{align}
    \label{inequality:AGD-converge-linear-1}
    &\ f(W_{k+1}) \le f(W_k)+\la \nabla f(W_k), W_{k+1}-W_k\ra + \frac{L}{2}\|W_{k+1}-W_k\|^2 \\
    =&\ f(W_k) - \frac{\alpha}{2} \|\nabla f(W_k) \odot \sqrt{F(W_k)}\|^2 
    - \frac{1}{F_{\max}}\lp \frac{1}{2\alpha}-\frac{L F_{\max}}{2}\rp 
    \lnorm W_{k+1}-W_k\rnorm^2 
    \nonumber \\
    &\ + \frac{1}{2\alpha}\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha \nabla f(W_k) \odot \sqrt{F(W_k)}\rnorm^2
    \nonumber
\end{align}
where the second inequality comes from 
\begin{align}
    &\ \la \nabla f(W_k), W_{k+1}-W_k\ra 
    = \alpha\la \nabla f(W_k)\odot \sqrt{F(W_k)}, \frac{W_{k+1}-W_k}{\alpha \sqrt{F(W_k)}}\ra
    \\
    =&\ - \frac{\alpha}{2} \|\nabla f(W_k) \odot \sqrt{F(W_k)}\|^2 
    - \frac{1}{2\alpha}\lnorm \frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}\rnorm^2
     \nonumber \\
    &\ + \frac{1}{2\alpha}\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha \nabla f(W_k) \odot \sqrt{F(W_k)}\rnorm^2
    \nonumber
\end{align}
as well as the inequality
\begin{align}
    \lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}\rnorm^2
    \ge \frac{1}{F_{\max}}\|W_{k+1}-W_k\|^2.
\end{align}
The third term in the RHS of \eqref{inequality:AGD-converge-linear-1} can be bounded by
\begin{align}
    \label{inequality:AGD-converge-linear-2}
    \frac{1}{2\alpha}\lnorm\frac{W_{k+1}-W_k}{\sqrt{F(W_k)}}+\alpha \nabla f(W_k) \odot \sqrt{F(W_k)}\rnorm^2
    =&\ \frac{\alpha}{2}\lnorm|\nabla f(W_k)| \odot \frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2.
\end{align}
Define the saturation vector $H(W_k)\in\reals^D$ by
\begin{align}
    H(W_k) :=&\ {F(W_k)^{\odot 2}-G(W_k)^{\odot 2}}
    = {(F(W_k)+G(W_k))\odot(F(W_k)-G(W_k))} \\
    =&\ {q_+(W_k)\odot q_-(W_k)}.
    \nonumber
\end{align}
Notice the following inequality is valid
\begin{align}
    &\ \label{inequality:AGD-converge-linear-3}
    - \frac{\alpha}{2} \|\nabla f(W_k) \odot \sqrt{F(W_k)}\|^2 
    + \frac{\alpha}{2} \lnorm|\nabla f(W_k)| \odot \frac{G(W_k)}{\sqrt{F(W_k)}}\rnorm^2 \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp [F(W_k)]_d-\frac{[G(W_k)]^2_d}{[F(W_k)]_d}\rp\rp
    \nonumber \\
    =&\ -\frac{\alpha}{2}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp \frac{[F(W_k)]^2_d-[G(W_k)]^2_d}{[F(W_k)]_d}\rp\rp
    \nonumber \\
    \le&\ -\frac{\alpha}{2 F_{\max}}\sum_{d\in[D]} \lp [\nabla f(W_k)]_d^2 \lp [F(W_k)]_d^2-G(W_k)]^2_d\rp\rp
    \nonumber\\
    =&\ -\frac{\alpha}{2 F_{\max}}\|\nabla f(W_k)\|^2_{S_k}
    \le 0.
    \nonumber
\end{align}
Substituting \eqref{inequality:AGD-converge-linear-2} and \eqref{inequality:AGD-converge-linear-3} back into \eqref{inequality:AGD-converge-linear-1} yields
\begin{align}
    \frac{1}{F_{\max}}\lp \frac{1}{2\alpha}-\frac{L F_{\max}}{2}\rp\|W_{k+1}-W_k\|^2  
    \le f(W_k) - f(W_{k+1}).
\end{align}

Noticing that $\|W_{k+1}-W_k\|^2=\alpha^2 \|\nabla f(W_k) \odot F(W_k) - |\nabla f(W_k)| \odot G(W_k)\|^2$ and averaging for $k$ from $0$ to $K-1$, we have
\begin{align}
    E_K^{\texttt{AGD}} =&\ 
    \frac{1}{K}\sum_{k=0}^{K-1} \Big(\lnorm \nabla f(W_k) \odot F(W_k) - |\nabla f(W_k)| \odot G(W_k)\rnorm^2 + \lnorm \nabla f(W_k)\rnorm^2_{H(W_k)}\Big)
    \\
    \le&\ \frac{2(f(W_0) - f(W_{K+1}))F_{\max}}{\alpha(1 -\alpha LF_{\max})K}
    \le
    \frac{8L(f(W_0) - f^*)F_{\max}^2}{K}
    \nonumber
\end{align}
where the last inequality choose $\alpha = \frac{1}{2LF_{\max}}$.

Further, if the degree of saturation is bounded,  \eqref{inequality:AGD-converge-linear-1}--\eqref{inequality:AGD-converge-linear-3} implies that
\begin{align}
    \label{inequality:AGD-converge-3}
    \frac{\alpha H^{\texttt{AGD}}_{\min}}{2}\|\nabla f(W_k)\|^2
    \le \frac{\alpha}{2}\|\nabla f(W_k)\|^2_{H(W_k)}
    \le f(W_k) - f(W_{k+1}).
\end{align}
Averaging \eqref{inequality:AGD-converge-3} for $k$ from $0$ to $K$ deduce that
\begin{align}
    \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla f(W_k)\|^2
    \le
    \frac{2(f(W_0) - f(W_{K+1}))F_{\max}}{\alpha K H^{\texttt{AGD}}_{\min}}
    \le
    \frac{2L(f(W_0) - f^*)F_{\max}^2}{K H^{\texttt{AGD}}_{\min}}
\end{align}
where the second inequality holds because the learning rate is selected as $\alpha=\frac{1}{LF_{\max}}$.
\end{proof}

\section{Simulation Details and Additional Results}
\label{section:experiment-setting}
\vspace{-0.25em}
This section provides details about the experiments in Section \ref{section:experiments}.
All simulation is performed under the \pytorch~framework \url{https://github.com/pytorch/pytorch}. 
The analog training algorithms, including \AnalogSGD~and \TT, are provided by the open-source simulation toolkit \ac{AIHWKIT}~\citep{Rasch2021AFA}, which has MIT license; see \url{github.com/IBM/aihwkit}. 

\textbf{Optimizer. } The digital SGD optimizer is implemented by \code{FloatingPointRPUConfig} in \ac{AIHWKIT}, which is equivalent to the SGD implemented in \pytorch. 
The \AnalogSGD~is implemented by selecting \code{SingleRPUConfig} as configation, and \TT~optimizers are implemented by \code{UnitCellRPUConfig} with \code{TransferCompound} devices in \ac{AIHWKIT}.

\textbf{Hardware. }
We conduct our experiments on one NVIDIA RTX 3090 GPU, which has 24GB memory and a maximum power of 350W. 
The simulations take from 30 minutes to 5 hours, depending on model sizes and datasets.

\textbf{Statistical Significance. }
The simulation data reported in all tables is repeated three times. The randomness originates from the data shuffling, random initialization, and random noise in the analog hardware.
The mean and standard deviation are calculated using {\em statistics} library.

\subsection{Power and Exponential Response Functions}

The \emph{power response} is a power function, given by
\begin{align}
  q_{+}(w) = \lp 1 - \frac{w}{\tau}\rp^{\gamma_{\texttt{res}}}
    , 
    \quad\quad
  q_{-}(w) = \lp 1 + \frac{w}{\tau}\rp^{\gamma_{\texttt{res}}}
\end{align}
which can be changed by adjusting the dynamic radius $\tau$ and shape parameter $\gamma_{\texttt{res}}$.
We also consider the \emph{exponential response}, whose response is an exponential function, defined by
\begin{align}
  q_{+}(w) = \frac{\exp\lp \gamma_{\texttt{res}}(1 - {w}/{\tau})\rp-1}{\exp\lp \gamma_{\texttt{res}}\rp-1}
    , 
    \quad\quad
  q_{-}(w) = \frac{\exp\lp \gamma_{\texttt{res}}(1 + {w}/{\tau})\rp-1}{\exp\lp \gamma_{\texttt{res}}\rp-1}.
\end{align}
It could be checked that the boundary of their dynamic ranges are $\tau^{\max}=\tau$ and $\tau^{\min}=-\tau$, while the symmetric point is 0, as required by Corollary \ref{theorem:TT-convergence-scvx-final}.
Figure \ref{fig:response-factor-pow-exp} illustrates how the response functions change with different $\gamma_{\texttt{res}}$. 

\begin{figure*}[!h]
    \includegraphics[width=1.\linewidth]{figs/response_factor_pow_exp.pdf}
    \vspace{-0.2cm}
    \caption{Examples of response functions. The dependence of the response function on the weight $w$ can grow at various rates, including but not limited to power (Left) or exponential rate (Right). $\tau$ is the radius of the dynamic range, and $\gamma_{\texttt{res}}$ is a parameter that needs to be determined by physical measurements. 
    }
    \label{fig:response-factor-pow-exp}
\end{figure*} 

\subsection{Least squares problem}
\label{section:simulation-toy}
In Figure \ref{fig:TT-fails} (see Section \ref{section:main-results}), we consider the least squares problem 
on a synthetic dataset and a ground truth $W^*\in\reals^{D}$.
The problem can be formulated by
\begin{align}
    \min_{W\in\reals^{D}} f(W) := \frac{1}{2}\|AW-b\|^2
    = \frac{1}{2}\|A(W-W^*)\|^2.
\end{align}
The elements of $W^*$ are sampled from a Gaussian distribution with mean $0$ and variance $\sigma^2_{W^*}$.
Consider a matrix $A\in\reals^{D_{\text{out}}\times D}$ of size $D=50$ and $D_{\text{out}}=100$ whose elements are sampled from a Gaussian distribution with variance $\sigma^2_A$.
The label $b\in\reals^{D_{\text{out}}}$ is generated by $b=AW^*$ where $W^*$ are sampled from a standard Gaussian distribution with $\sigma^2_{W^*}$. 
The response granularity $\Delta w_{\min}$=1e-4 while $\tau=3.5$.
The maximum bit length is 8. 
The variance are set as $\sigma^2_A=1.00^2$, $\sigma^2_{W^*}=0.5^2$. 



\subsection{Classification problem}
We conduct training simulations of image classification tasks on a series of real datasets.
In the implementation of \TT, only a few columns or rows of $P_k$ are transferred per time to $W_k$ in the recursion \eqref{recursion:HD-W} to balance the communication and computation. In our simulations, we transfer 1 column every time.
The response granularity is set as $\Delta w_{\min}=$1e-3.

The other setting follows the settings of \ac{AIHWKIT}, including output noise (0.5 \% of the quantization bin width), quantization and clipping (output range set 20, output noise 0.1, and input and output quantization to 8 bits). Noise and bound management techniques are used in~\cite{gokmen2017cnn}.
A learnable scaling factor is set after each analog layer, which is updated using SGD.


\textbf{3-FC / MNIST. }
Following the setting in \cite{gokmen2016acceleration}, we train a model with 3 fully connected layers. The hidden sizes are 256 and 128. The activation functions are Sigmoid.
The learning rates are $\alpha=0.1$ for \DigitalSGD, $\alpha=0.05, \beta=0.01$ for \texttt{Analog\;SGD} and \texttt{Tiki-Taka}. The batch size is 10 for all algorithms.

\textbf{CNN / MNIST. }
We train a convolution neural network, which contains 2-convolutional layers, 2-max-pooling layers, and 2-fully connected layers. The activation functions are Tanh.
The first two convolutional layers use 5$\times$5 kernels with 16 and 32 kernels, respectively. 
Each convolutional layer is followed by a subsampling layer implemented by the max pooling function over non-overlapping pooling windows of size 2 $\times $ 2. 
The output of the second pooling layer, consisting of 512 neuron activations, feeds into a fully connected layer consisting of 128 tanh neurons, which is then connected into a 10-way softmax output layer. 
The learning rates are set as $\alpha=0.1$ for \DigitalSGD, $\alpha=0.05, \beta=0.01$ for \texttt{Analog\;SGD} or \texttt{Tiki-Taka}. The batch size is 8 for all algorithms.

\textbf{ResNet / CIFAR10 \& CIFAR100.}
We train different models from the ResNet family, including ResNet18, 34, and 50. The base model is pre-trained on ImageNet dataset.
The last fully connected layer is replaced by an analog layer. 
The learning rates are set as $\alpha=0.075$ for \DigitalSGD, $\alpha=0.075, \beta=0.01$ for \texttt{Analog\;SGD} or \texttt{Tiki-Taka}. 
\TT~adopts $\gamma=0.4$ unless stated otherwise.
The batch size is 128 for all algorithms. 
Power response with $\gamma_{\texttt{res}}=3.0$ and $\tau=0.1$, and exponential response with $\gamma_{\texttt{res}}=3.0$ and $\tau=0.1$, are used in the simulations.

\subsection{Additional performance on real datasets}
We train different models from the MobileNet family, including MobileNet2, MobileNetV3L, MobileNetV3S. The base model is pre-trained on ImageNet dataset.
The last fully connected layer is replaced by an analog layer. 
The learning rates are set as $\alpha=0.075$ for \DigitalSGD, $\alpha=0.075, \beta=0.01$ for \texttt{Analog\;SGD} or \texttt{Tiki-Taka}. 
\TT~adopts $\gamma=0.4$ unless stated otherwise.
The batch size is 128 for all algorithms. 
Power response function with $\gamma_{\texttt{res}}=4.0$ and $\tau=0.05$ is used in the simulations.

\textbf{CIFAR10/CIFAR100 ResNet.} We fine-tune three models from the ResNet family with different scales on CIFAR10/CIFAR100 datasets. The power response functions with the parameter $\gamma_{\text{res}}=3.0$ and $\tau=0.1$ is used, whose results are shown in Table \ref{table:CIFAR-fine-tune-pow} and \ref{table:CIFAR-fine-tune-exp}, respectively.
The results show that the \TT~outperforms \AnalogSGD~by about $1.0\%$ in most of the cases in ResNet34/50, and the gap even reaches about $10.0\%$ for ResNet18 training on the CIFAR100 dataset. 
An interesting observation is that the gap is closer on the exponential response, at which time \AnalogSGD~achieves a comparable and sometimes higher accuracy with \TT. We speculate it happens because the asymmetric bias, coupled with the impact of a high-dimensional objective landscape, leads to a better local minimum, which is worthy of study in future work.

\begin{table*}[h!]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
        \toprule
            & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c}{CIFAR100} \\
            \cmidrule{2-4}\cmidrule{5-7}
            & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} 
            & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} \\
        \midrule
            ResNet18  & 95.43\stdv{$\pm$0.13} & 94.66\stdv{$\pm$0.11} & 94.70\stdv{$\pm$0.07} & 81.12\stdv{$\pm$0.25} & 73.55\stdv{$\pm$0.22} & 74.64\stdv{$\pm$0.24} \\
            ResNet34  & 96.48\stdv{$\pm$0.02} & 96.19\stdv{$\pm$0.04} & 96.24\stdv{$\pm$0.08} & 83.86\stdv{$\pm$0.12} & 78.10\stdv{$\pm$0.24} & 79.05\stdv{$\pm$0.21} \\
            ResNet50  & 96.57\stdv{$\pm$0.10} & 96.53\stdv{$\pm$0.06} & 96.40\stdv{$\pm$0.13} & 83.98\stdv{$\pm$0.11} & 81.40\stdv{$\pm$0.36} & 79.75\stdv{$\pm$0.10} \\
        \bottomrule
            \end{tabular}
    \caption{Analog training with the \emph{exponential response} for fine-tuning task on CIFAR10/100 datasets using models from ResNet family. The accuracy of the test set is reported. \texttt{DSGD}, \texttt{ASGD}, and \texttt{TT} represent \DigitalSGD, \AnalogSGD, \TT, respectively.}
    \label{table:CIFAR-fine-tune-exp}
\end{table*}

\textbf{CIFAR10/CIFAR100 MobileNet.} We fine-tune three models from the MobileNet family with different scales on CIFAR10/CIFAR100 datasets. The response function are set as the power response with the parameter $\gamma_{\text{res}}=4.0$ and $\tau=0.05$, whose results are shown in Table \ref{table:CIFAR-fine-tune-exp-mobilenet}. In the simulations, the accuracy of \AnalogSGD~drops significantly by about $10\%$ in most cases, while \TT~remains comparable to the \DigitalSGD~with only a slight drop.

\begin{table*}[b]
\small
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
        & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c}{CIFAR100} \\
        \cmidrule{2-4}\cmidrule{5-7}
        & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} 
        & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} \\
    \midrule
        MobileNetV2  & 95.28\stdv{$\pm$0.20} & 94.34\stdv{$\pm$0.27} & 95.05\stdv{$\pm$0.11} & 80.60\stdv{$\pm$0.18} & 63.41\stdv{$\pm$1.20} & 73.33\stdv{$\pm$0.94} \\
        MobileNetV3S & 94.45\stdv{$\pm$0.10} & 80.66\stdv{$\pm$6.18} & 93.65\stdv{$\pm$0.24} & 78.94\stdv{$\pm$0.05} & 51.79\stdv{$\pm$1.05} & 71.14\stdv{$\pm$0.93} \\
        MobileNetV3L & 95.95\stdv{$\pm$0.08} & 80.79\stdv{$\pm$2.97} & 95.39\stdv{$\pm$0.27} & 82.16\stdv{$\pm$0.26} & 66.80\stdv{$\pm$1.40} & 78.81\stdv{$\pm$0.52} \\
    \bottomrule
    \end{tabular}
    \caption{Analog training with exponential response for fine-tuning task on CIFAR10/100 datasets using models from the MobileNet family. The accuracy of the test set is reported. \texttt{DSGD}, \texttt{ASGD}, and \texttt{TT} represent \DigitalSGD, \AnalogSGD, \TT, respectively.}
    \label{table:CIFAR-fine-tune-exp-mobilenet}
\end{table*}

\subsection{Ablation study on cycle variation}
To verify the conclusion of Theorem \ref{theorem:pulse-update-error} that the error introduced by cycle variation is a higher-order term, we conduct a numerical simulation training on image classification task on MNIST dataset using \ac{FCN} or \ac{CNN} network. 
In the pulse update \eqref{analog-pulse-update}, the parameter $\sigma_c$ is varied from 10\% to 120\%, where the noise signal is already larger than the response function signal itself.
The results are shown in Table \ref{table:cycle-variation}. The results show that the test accuracy of both \AnalogSGD~and \TT~is not significantly affected by the cycle variation, which complies with the theoretical analysis.

\begin{table*}[h]
\small
\centering
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
     & \multicolumn{3}{c|}{FCN} & \multicolumn{3}{c}{CNN} \\ 
     \cmidrule{2-4}\cmidrule{5-7}
     & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} 
     & \texttt{DSGD} & \texttt{ASGD} & \texttt{TT} \\ 
    \midrule
    $\sigma_c=10\%$ 
    & \multirow{5}{*}{98.17\stdv{$\pm$0.05}} & 97.22\stdv{$\pm$0.21} & 97.66\stdv{$\pm$0.04} 
    & \multirow{5}{*}{99.09\stdv{$\pm$0.04}} & 92.68\stdv{$\pm$0.45} & 98.74\stdv{$\pm$0.07} \\
    $\sigma_c=30\%$  &  & 96.97\stdv{$\pm$0.12} & 97.07\stdv{$\pm$0.12} &  & 93.36\stdv{$\pm$0.55} & 98.89\stdv{$\pm$0.05} \\
    $\sigma_c=60\%$  &  & 96.33\stdv{$\pm$0.21} & 97.70\stdv{$\pm$0.09} &  & 93.07\stdv{$\pm$0.53} & 98.68\stdv{$\pm$0.09} \\
    $\sigma_c=90\%$  &  & 95.99\stdv{$\pm$0.15} & 97.44\stdv{$\pm$0.15} &  & 91.87\stdv{$\pm$0.48} & 98.92\stdv{$\pm$0.02} \\
    $\sigma_c=120\%$ &  & 96.19\stdv{$\pm$0.20} & 96.97\stdv{$\pm$0.20} &  & 91.57\stdv{$\pm$0.58} & 98.85\stdv{$\pm$0.04} \\ 
    \bottomrule
\end{tabular}
\caption{Test accuracy comparison under different cycle variation levels $\sigma_c$ on MNIST dataset. \texttt{DSGD}, \texttt{ASGD}, and \texttt{TT} represent \DigitalSGD, \AnalogSGD, \TT, respectively}
\label{table:cycle-variation}
\end{table*}

\subsection{Ablation study on various response functions}
We also train a \ac{FCN} model on the MNIST dataset under various response functions.
As shown in the figure, larger $\gamma_{\texttt{res}}$ leads to a steeper response function.
The results are shown in Table \ref{table:MNIST-pow-and-exp}.
The accuracy $<15.00$ in the table implies that \AnalogSGD~fails completely at all trials, which is close to random guess.
The results show that \AnalogSGD~works well only when the asymmetric is mild, i.e. $\gamma_{\texttt{res}}$ is small and $\tau$ is large, while \TT~outperforms \AnalogSGD~and achieves comparable accuracy with \DigitalSGD.

\begin{table*}[h]
\centering
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
&  & \multirow{2}{*}{$\texttt{DSGD}$} 
& \multicolumn{2}{c|}{Power response} & \multicolumn{2}{c}{Exponential response} \\ 
\cmidrule{4-7}
&  &  & \texttt{ASGD} & \texttt{TT} & \texttt{ASGD} & \texttt{TT} \\ 
\midrule
\multirow{3}{*}{$\gamma_{\texttt{res}}=0.5$} 
  & $\tau=0.6$ & \multirow{9}{*}{98.17\stdv{$\pm$0.05}} & 96.01\stdv{$\pm$0.26} & 96.92\stdv{$\pm$0.19} & $<$15.00          & 97.27\stdv{$\pm$0.07} \\
  & $\tau=0.7$ &                                        & 97.40\stdv{$\pm$0.15} & 97.05\stdv{$\pm$0.05} & $<$15.00          & 97.39\stdv{$\pm$0.15} \\
  & $\tau=0.8$ &                                        & 97.38\stdv{$\pm$0.10} & 96.82\stdv{$\pm$0.17} & 94.00\stdv{$\pm$0.63} & 97.16\stdv{$\pm$0.16} \\ 
\cmidrule{1-2}\cmidrule{4-5}\cmidrule{6-7}
\multirow{3}{*}{$\gamma_{\texttt{res}}=1.0$} 
  & $\tau=0.6$ &                                        & $<$15.00          & 97.39\stdv{$\pm$0.05} & $<$15.00          & 97.46\stdv{$\pm$0.08} \\
  & $\tau=0.7$ &                                        & $<$15.00          & 97.33\stdv{$\pm$0.05} & $<$15.00          & 97.49\stdv{$\pm$0.04} \\
  & $\tau=0.8$ &                                        & $<$15.00          & 97.34\stdv{$\pm$0.09} & $<$15.00          & 97.25\stdv{$\pm$0.16} \\ 
\cmidrule{1-2}\cmidrule{4-5}\cmidrule{6-7}
\multirow{3}{*}{$\gamma_{\texttt{res}}=2.0$} 
  & $\tau=0.6$ &                                        & $<$15.00          & 96.93\stdv{$\pm$0.15} & $<$15.00          & 97.19\stdv{$\pm$0.16} \\
  & $\tau=0.7$ &                                        & $<$15.00          & 97.27\stdv{$\pm$0.02} & $<$15.00          & 97.72\stdv{$\pm$0.07} \\
  & $\tau=0.8$ &                                        & $<$15.00          & 97.18\stdv{$\pm$0.04} & $<$15.00          & 97.06\stdv{$\pm$0.10} \\ 
\bottomrule
\end{tabular}
\caption{Test accuracy comparison under different response function parameters $\tau$ and $\gamma_{\texttt{res}}$ for FCN training on MNIST dataset with power or exponential response functions. \texttt{DSGD}, \texttt{ASGD}, and \texttt{TT} represent \DigitalSGD, \AnalogSGD, \TT, respectively.}
\label{table:MNIST-pow-and-exp}
\end{table*}

\subsection{Ablation study on $\gamma$}
We conduct a series of simulations to study the impact of mixing coefficient $\gamma$ in \eqref{recursion:HD-P} on CIFAR10 or CIFAR100 dataset in the ResNet training tasks. The results are presented in Figure \ref{figure:gamma-ablation}, which shows that \TT~achieves a great accuracy gain from increasing $\gamma$ from 0 to 0.1, while the gain saturates after that. Therefore, we conclude that \TT~benefits from a non-zero $\gamma$, and the performance is robust to the $\gamma$ selection.
\begin{figure*}[!h]
    \footnotesize
    \centering
    \includegraphics[width=.4\linewidth]{figs/A12-CIFAR10-resnet-gamma.pdf}
    \includegraphics[width=0.4\linewidth]{figs/A12-CIFAR100-resnet-gamma.pdf}
    \caption{The test accuracy of ResNet family models after 100 epochs trained by \TT~under different $\gamma$ in \eqref{recursion:HD-P}; \textbf{(Left)} CIFAR10. \textbf{(Right)} CIFAR100. 
    }
    \label{figure:gamma-ablation}
\end{figure*}




\end{document}


